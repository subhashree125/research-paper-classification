{
  "Abstract": "Time series analysis finds wide applications in fields such as weather forecasting,anomaly detection, and behavior recognition. Previous methods attempted tomodel temporal variations directly using 1D time series. However, this has beenquite challenging due to the discrete nature of data points in time series and thecomplexity of periodic variation. In terms of periodicity, taking weather and trafficdata as an example, there are multi-periodic variations such as yearly, monthly,weekly, and daily, etc. In order to break through the limitations of the previousmethods, we decouple the implied complex periodic variations into inclusion andoverlap relationships among different level periodic components based on theobservation of the multi-periodicity therein and its inclusion relationships. Thisexplicitly represents the naturally occurring pyramid-like properties in time series,where the top level is the original time series and lower levels consist of periodiccomponents with gradually shorter periods, which we call the periodic pyramid. Tofurther extract complex temporal variations, we introduce self-attention mechanisminto the periodic pyramid, capturing complex periodic relationships by computingattention between periodic components based on their inclusion, overlap, andadjacency relationships. Our proposed Peri-midFormer demonstrates outstandingperformance in five mainstream time series analysis tasks, including short- andlong-term forecasting, imputation, classification, and anomaly detection. The codeis available at",
  "Introduction": "Time series analysis stands as a foundational challenge pivotal across diverse real-world scenarios ,such as weather forecasting , imputation of missing data within offshore wind speed time series ,anomaly detection for industrial maintenance , and classification . Due to its substantial practicalutility, time series analysis has garnered considerable interest, leading to the development of a largenumber of deep learning-based methods for it. Different from other forms of sequential data, like language or video, time series data is continuouslyrecorded, capturing scalar values at each time point. Furthermore, real-world time series variationsoften entail complex temporal patterns, where multiple fluctuations (e.g., ascents, descents, fluctu-ations, etc.) intermingle and intertwine, particularly salient is the presence of various overlappingperiodic components in it, rendering the modeling of temporal variations exceptionally challenging. Deep learning models, known for their powerful non-linear capabilities, capture intricate temporalvariations in real-world time series. Recurrent neural networks (RNNs) leverage sequential data,allowing past information to influence future predictions [6; 7]. However, they face challenges withlong-term dependencies and computational inefficiency due to their sequential nature. Temporalconvolutional neural networks (TCNs) [8; 9] extract variation information but struggle with capturinglong-term dependencies. Transformers with attention mechanisms have gained popularity for sequen-",
  ": Multi-periodicity, inclusion of periodic components, and Periodic Pyramid": "tial modeling [10; 11], capturing pairwise temporal dependencies among time points. Yet, discerningreliable dependencies directly from scattered time points remains challenging . Timesnet innovatively transforms 1D time series into 2D tensors, unifying intra- and inter-period variations in2D space. However, it overlooks inclusion relationships between periods of different scales and isconstrained by limited feature extraction capability of CNNs, hindering its ability to explore complexrelationships within time series. We analyze time series by examining the inclusion and overlap (hereinafter collectively referred to asinclusion) relationships between various periodic components to address complex temporal variations.Real-world time series often show multiple periodicities, like yearly and daily weather variationsor weekly and daily traffic fluctuations. And these periods exhibit clear inclusion relationships, forinstance, yearly weather variations encompass multiple daily weather variations. Besides variationsbetween different period levels, it also occur within periods of the same level. For example, dailyweather variations differ based on conditions like sunny or cloudy. Due to these inclusion andadjacency relationships, different periods show similarities, with short and long periods beingconsistent in overlapping portions, and periods of the same level being similar. Additionally, a longperiod can be decomposed into multiple short ones, forming a hierarchical pyramid structure. In ourstudy, time series without explicit periodicity are treated as having infinitely long periods. Based on the above analysis, we decompose the time series into multiple periodic components,forming a pyramid structure where longer components encompass shorter ones, termed the PeriodicPyramid as shown in , which illustrates the intricate periodic inclusion relationships withinthe time series. Each level consists of components with the same period, exhibiting the same phase,while different levels contain components with inclusion relationships. This transformation convertsthe original 1D time series into a 2D representation, explicitly showing the implicit multi-periodrelationships. Within the Periodic Pyramid, a shorter period may belong to two longer periodssimultaneously, reflecting the complexity of the time series. There is a clear similarity betweencomponents within the same level and those in adjacent levels where inclusion relationships exist.Thus inspired by Pyraformer , we propose the Periodic Pyramid Transformer (Peri-midFormer),which computes self-attention among periodic components to capture complex temporal variations intime series. Furthermore, we consider each branch in the Periodic Pyramid as a Periodic Feature Flow,and aggregating features from multiple flows to provide rich periodic information for downstreamtasks. In experiments, Peri-midFormer achieves state-of-the-art performance in various analytic tasks,including forecasting, imputation, anomaly detection, and classification. 1. Based on the inclusion relationships of multiple periods in time series, this paper proposes atop-down constructed Periodic Pyramid structure, which expands 1D time series variationsinto 2D, explicitly representing the implicit multi-period relationships within the time series. 2. We propose Peri-midFormer, which uses the Periodic Pyramid Attention Mechanism toautomatically capture dependencies between different and same-level periodic components,extracting diverse temporal variations in time series. Additionally, to further harness thepotential of Peri-midFormer, we introduce Periodic Feature Flows to provide rich periodicinformation for downstream tasks. 3. We conduct extensive experiments on five mainstream time series analysis tasks, and Peri-midFormer achieves state-of-the-art across all of them, demonstrating its superior capabilityin time series analysis. The remainder of this paper is structured as follows. briefly summarizes the relatedwork. details the proposed model structure. extensively evaluates our methodsperformance across five main time series analysis tasks. presents ablations analysis, presents complexity analysis, and discusses our results and future directions.",
  "Related Work": "Temporal variation modeling, a crucial aspect of time series analysis, has been extensively investi-gated. In recent years, numerous deep models have emerged for this purpose, including MLP [14; 15],TCN , and RNN [6; 7]-based architectures. Furthermore, Transformers have shown remarkableperformance in time series forecasting [16; 12; 17; 18]. They utilize attention mechanisms to uncovertemporal dependencies among time points. For instance, Wu et al. introduce Autoformer with anAuto-Correlation mechanism, adept at capturing series-wise temporal dependencies derived fromlearned periods. Moreover, to address complex temporal patterns, they adopted a deep decompositionarchitecture to extract seasonal and trend parts from input series. Subsequently, FEDformer enhances seasonal-trend decomposition through a mixture-of-expert design and introduces sparseattention within the frequency domain. Pyraformer constructs a down-top pyramid structurethrough multiple convolution operations on time series to address the issue of long information prop-agation paths in Transformers, significantly reducing both time and space complexity. PatchTST partitions individual data points into patches and uses them as tokens for the Transformer, therebyenhancing its understanding of local information in time series. Additionally, PatchTST innovativelyprocesses each channel separately, making it particularly suitable for forecasting tasks. Additionally, there are some recent innovative works. Timesnet unravels intricate temporalpatterns by exploring the multi-periodicity of time series and captures temporal 2D-variations usingcomputer vision CNN backbones. GPT4TS ingeniously utilizes the large language model GPT2as a pretrained model, fine-tuning some of its structures with time series, achieving state-of-the-artresults. FITS proposes a time series analysis model based on frequency domain operations,requiring very low parameter count and memory consumption. And recent works have consideredmulti-scale information in time series. PDF captures both short-term and long-term variationsby transforming 1D time series into 2D tensors using a multi-periodic decoupling block. It achievessuperior forecasting performance by modeling these decoupled variations and integrating them foraccurate predictions. SCNN decomposes multivariate time series into long-term, seasonal, short-term, co-evolving, and residual components, enhancing interpretability, adaptability to distributionshifts, and scalability by modeling each component separately. TimeMixer uses a novel multiscalemixing approach, decomposing time series into fine and coarse scales to capture both detailed andmacroscopic variations. It employs Past-Decomposable-Mixing to extract historical informationand Future-Multipredictor-Mixing to leverage multiscale forecasting capabilities, achieving greatperformance in forecasting task.",
  "Model Structure": "The overall flowchart of the proposed approach is shown in , it begins with time embeddingof the original time series at the top. Then, we use the FFT to decompose it into multiple periodiccomponents of varying lengths across different levels, with lines indicating the inclusion relationshipsbetween them. Moving down, padding and projection are then applied to ensure uniform dimensions,forming the Periodic Pyramid. Each component is treated as an independent token and receivespositional embedding. Next, the Periodic Pyramid is fed into Peri-midFormer, which consists ofmultiple layers for computing Periodic Pyramid Attention. Finally, depending on the task, twostrategies are employed: for classification, components are directly concatenated and projectedinto the category space; for other reconstruction tasks (since forecasting, imputation, and anomalydetection all necessitate the model to reconstruct the channel dimensions or input lengths, wecollectively refer to such tasks as reconstruction tasks), features from different pyramid branchesare integrated through Periodic Feature Flows Aggregation to generate the final output. Please notethat we referred to for de-normalization and for time series decomposition to maximize theeffectiveness of our method, but we omitted these details from the figure to maintain simplicity. SeeAppendix A for a complete flowchart. Further details are provided below.",
  ": Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism": "ponents with inclusion relationships to explicitly represent implicit periodic relationships. Firstly,as Peri-midFormer is designed to focus on periodic components, we first normalize the originaltime series X RLC that with length L and C channels, then decompose it to obtain the seasonalpart Xs RLC, thus removing the interference of the trend part. For a detailed description ofnormalization and decomposition, please refer to the Appendix A. Then we partition Xs into periodiccomponents, following the approach used in Timesnet . Its important to note that, inspired byPatchTST , we retain the channel dimension C, as it is advantageous for Peri-midFormer incapturing periodic features within each channel (note that we adopt a channel independent strategyand the shows the processing of only one of the channels). The periodic components areextracted in the frequency domain, accomplished specifically through FFT:",
  ", i {1, , k}, (1)": "where FFT() and Amp() denote Fourier Transform and amplitude calculation, respectively. A RL represents the amplitude of each frequency, averaged across C channels using Avg(). Note thatthe j-th value Aj denotes the intensity of the j-th frequency periodic basis function, associated withperiod lengthLj. To handle frequency domain sparsity and minimize noise from irrelevant high frequencies , the top-k amplitude values {Af1, , Afk} corresponding to the most significantfrequencies {f1, , fk} are selected, where k is a hyper-parameter, beginning from 2 to ensure thefundamental pyramid structure. Additionally, to ensure the top level of the pyramid corresponds tothe original time series, we define f1 = 1, with other frequencies arranged in ascending order. Theseselected frequencies correspond to k period lengths {p1, , pk}, arranged in descending order.Due to the frequency domains conjugacy, only frequencies within1, , L 2are considered.Based on the selected frequencies {f1, , fk} and their associated period lengths {p1, , pk}, wepartition the original time series into periodic components for each pyramid level, denoted as C:",
  "C = {C1, C2, , Cn }, {1, , k}, n {1, , fk},(2)": "where Cn denotes the n-th periodic component in the -th pyramid level. Here, is the pyramid levelindex, starting from the top and increasing, with a maximum value of k, indicating the number oflevels determined by the selected periods. Similarly, n represents the component index within a level,increasing from left to right, with a maximum value of fk, indicating the number of components perlevel is determined by the frequency corresponding to that period in the original series. The PeriodicPyramid can thus be represented as:",
  ": Inclusion relationships of periodic components (left) and Periodic Pyramid Attention Mechanism(right)": "where Stack() denotes a stacking operation. The constructed Periodic Pyramid, depicted in theupper right of , displays evident inclusion relationship among different levels, shown byconnections between levels. Let R denote the relationship between pairs of periodic componentsfrom the upper and lower levels, determined by the presence or absence of overlap as follows:",
  "Rn1,n1,=1, Index(Cn11 ) Index(Cn ) > 00, else, {2, , k}, n1, n {1, , fk},(4)": "when R = 1, it signifies an inclusion relationship, while R = 0 indicates no overlap. This isillustrated in the left half of . n denotes the index of the n-th component in the -th level.Index() denotes the positional index of each data point within the periodic component at that level.Indices for points in the first level are contained in {0, . . . , L 1}. For subsequent levels, mostindices match those of the first level. However, due to varying component lengths, there may be slightdifferences in indices for the last portion. Nonetheless, this doesnt impact relationship determinationbetween components across levels. In practice, the relationship between the components is realizedby masking the corresponding elements in the attention matrix. Thanks to the inclusion relationships between periodic components across different levels in thePeriodic Pyramid, complex periodic relationships inherent in 1D time series are explicitly represented.Next, due to the varying lengths of the components, its necessary to map C to the same scale forsubsequent Periodic Pyramid Attention Mechanism, with the equation provided as follows:P = Projection (Padding (Cn )) , {1, , k}, n {1, , fk},(5)",
  "Periodic Pyramid Transformer (Peri-midFormer)": "Once we have the Periodic Pyramid, it can be inputted into the Peri-midFormer, as depicted in. The Peri-midFormer introduces a specialized attention mechanism tailored for the PeriodicPyramid, called Periodic Pyramid Attention Mechanism (PPAM), shown in the right half of . Here, original connections are replaced with bidirectional arrows, and also added within thesame level. These bidirectional arrows signify attention between periodic components. In PPAM,inter-level attention focuses on period dependencies across levels, while intra-level attention focuseson dependencies within the same level. Note that attention occurs among all components within thesame level, not just between adjacent ones. However, for clarity, not all attention connections withinthe same level are depicted. In Periodic Pyramid, a periodic component Cn generally has three types of interconnected relation-ships (denoted as I) with other components: the parent node in the level above (denoted as P), allnodes within the same level including itself (denoted as A), and the child nodes in its next level(denoted as C). Therefore, the relationships of Cn can be expressed by the following equation:",
  "mI(n)expqikm/dK,(7)": "where q, k, and v denote query, key, and value vectors, respectively, as in the classical self-attentionmechanism. m used for selecting components that have interconnected relationships with Cn . kmrepresents the transpose of row m in K. dK refers to the dimension of key vectors, ensuring stableattention scores through scaling. We apply this attention mechanism to each component across all levels of the Periodic Pyramid,enabling the automatic detection of dependencies among all components in the Periodic Pyramid andcapturing the intricate temporal variations in the time series. For a detailed theoretical proof of thePPAM see the Appendix F.",
  "Periodic Feature Flows Aggregation": "Here we explain the Periodic Feature Flows Aggregation used for reconstruction tasks. The outputof the Peri-midFormer retains the original pyramid structure. To leverage the diverse periodiccomponents across different levels, we treat a single branch from the top to the bottom of the pyramidas a periodic feature flow, highlighted by the red line in . Since a periodic feature flow passesthrough periodic components at different levels, it contains periodic features of different scales fromthe time series. Additionally, due to variations among periodic components within each level, eachfeature flow carries distinct information. Therefore, we aggregate multiple feature flows throughPeriodic Feature Flow Aggregation. This involves linearly mapping each feature flow to matchthe length of the target time series and then averaging it across multiple feature flows to obtain theaggregated result Ys, as expressed in the following equation:",
  "Ys = MeanPollingProjectionCn11 , Cn22 , , Cnk, {2, , k}, nk {1, , fk}, (8)": "where Cnkrepresents a specific periodic component in the Peri-midFormers output, and{Cn11 , Cn22 , , Cnk } forms a feature flow, as shown in . Projection() maps each featureflow to match the target output length. Meanpooling() averages the feature flows. Ys indicatesthat this is the output from the seasonal part. Since we retained the channel dimension of the originaltime series, the result obtained after aggregating the periodic feature flows here becomes the shape ofthe final output. Finally, adding the trend part and de-normalization to obtain the ultimate output.",
  "Experiments": "We extensively test Peri-midFormer on five mainstream analysis tasks: short- and long-term fore-casting, imputation, classification, and anomaly detection. We adopted the same benchmarks asTimesnet , see Appendix C for details. Due to space limits, we provide only a summary of theresults here, more details about the datasets, experiment implementation, model configuration, andfull results can be found in Appendix. Baselines The baselines include CNN-based models:TimesNet ; MLP-based models:LightTS , DLinear and FITS ; Transformer-based models: GPT4TS , Time-LLM , iTransformer , TSLANet , Reformer , Pyraformer , Informer ,Autoformer , FEDformer , Non-stationary Transformer , ETSformer , PatchTST .Besides, N-HiTS and N-BEATS are used for short-term forecasting. Anomaly Trans-former is used for anomaly detection. Rocket , LSTNet , TCN and Flowformer are used for classification.",
  "Long-term Forecasting": "Setups Referring to , we adopteight real-world benchmark datasetsfor long-term forecasting, includingWeather , Traffic , Electric-ity , Exchange , and fourETT datasets (ETTh1, ETTh2,ETTm1, ETTm2). Forecast lengthsare set to 96, 192, 336, and 720. Forthe fairness of the comparison, we setthe look-back window for all the meth-ods to 512 (64 on Exchange), the re-sults for other look-back windows canbe found in the Appendix H.3",
  "Average0.308 0.337 0.324 0.346 0.320 0.341 0.308 0.334 0.322 0.344 0.361 0.375 0.331 0.350 0.422 0.402 1.147 0.711": "Results From , it is evident that Peri-midFormer performs exceptionally well, even completelyoutperforms GPT4TS and closely approaching the state-of-the-art method Time-LLM. While Time-LLM demonstrates remarkable capabilities in long-term forecasting, our Peri-midFormer showsclear advantages on the ETTh2, Electricity, and Exchange datasets. Although Time-LLM achievesthe best results, it relies on a very large model, leading to significant computational overheadthat is unavoidable. The same issue exists for GPT4TS. In contrast, our Peri-midFormer achievesperformance close to that of Time-LLM without requiring excessive computational resources, makingit more suitable for practical applications. Further analysis of model complexity is provided in . In addition, our Peri-midFormer exhibits better performance with longer look-back window, asfurther detailed in the Appendix E.6. : Short-term forecasting task on M4. The prediction lengths are in {6, 48} and results areweighted averaged from several datasets under different sample intervals. ( means former, Stationmeans the Non-stationary Transformer.) See for full results. Red: best, Blue: second best.",
  "Short-term Forecasting": "Setups For short-term analysis, we adopt the M4 , which contains the yearly, quarterly andmonthly collected univariate marketing data. We measure forecast performance using the symmetricmean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weightedaverage (OWA), which are calculated as detailed in the Appendix D.1. Results shows that Peri-midFormer outperforms Time-LLM, GPT4TS, TimesNet, and N-BEATS, highlighting its exceptional performance in short-term forecasting. In the M4 dataset, somedata lacks clear periodicity, such as the Yearly data, which mainly exhibits a strong trend. A similarsituation is observed in the Exchange dataset for long-term forecasting task. Peri-midFormer performswell on these datasets due to its time series decomposition strategy. For a detailed analysis, pleaserefer to the Appendix E.7.",
  ": Model comparison in classification. Theresults are averaged from 10 subsets of UEA": "Setups We assessed Peri-midFormers capac-ity for high-level representation learning viaclassification task. Mimicking settings akinto TimesNet , we tested it on 10 multi-variate UEA classification datasets from ,covering tasks like gesture recognition, ac-tion recognition, audio recognition, medicaldiagnoses, and other real-world applications. Results As shown in ,Peri-midFormer achieves an average accuracy of76.6%, surpassing all baselines includingTSLANet (76.0%), GPT4TS (74.0%), Times-Net (73.6%), and all other Transformer-basedmethods. This suggests that Peri-midFormerhas excellent time series representation capa-bilities. See Appendix H.1 for full results.",
  "Imputation": "Setups To validate Peri-midFormers imputation capabilities, we conduct experiment on six real-world datasets, including four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), Electricity ,and Weather . We evaluate different random mask ratios (12.5%, 25%, 37.5%, 50%) for varyinglevels of missing data. Notably, due to the large number of missing values, the time series do notreflect their original periodicity. Therefore, before imputation, we simply interpolate the originalmissing data through a linear interpolation strategy in order to use Peri-midFormer efficiently, whichwe call pre-interpolation. For a description of pre-interpolation and its impact on other methods,please refer to the Appendix E.5. Results demonstrates Peri-midFormers outstanding performance on specific datasets (Elec-tricity and Weather), surpassing other methods significantly and securing the highest average scores.However, its performance on other datasets was ordinary, possibly due to the lack of obvious periodiccharacteristics in them. : Imputation task. We randomly mask {12.5%, 25%, 37.5%, 50%} time points of length-96time series. The results are averaged from 4 different mask ratios. ( means former, Station meansthe Non-stationary Transformer.) See for full results. Red: best, Blue: second best.",
  "Time Series Anomaly Detection": "Setups For anomaly detection, we assess models on five standard datasets: SMD , MSL ,SMAP , SWaT and PSM . To ensure fairness, we exclusively use classical reconstructionerror for all baseline models, aligning with the approach in TimesNet . Specifically, normaldata is used for training, and a simple reconstruction loss is employed to help the model learn thedistribution of normal data. In the testing phase, parts of the reconstructed output that exceed a certainthreshold are considered anomalies. We use a point adjustment technique combined with a manuallyset threshold for this purpose. : Anomaly detection task. We calculate the F1-score (as %) for each dataset. ( meansformer, Station means the Non-stationary Transformer.) A higher value of F1-score indicates a betterperformance. See for full results. Red: best, Blue: second best.",
  "Peri-midFormer0.4090.4300.3170.3770.1520.2490.2330.2710.3910.269": "Setups We performed ablation experiments on key modules of Peri-midFormer for the long-termforecasting task, with results presented in . The table outlines the progression of moduleadditions, from top to bottom. \"Pyraformer\" refers to the Pyraformer , building the pyramid down-top with convolutions and employing a simple two-fold relationship for attention distribution. \"w/operiodic components\" constructs a pyramid top-down by dividing the time series into patches without",
  "Autoformer": "TSLANet 10G FLOPs100G FLOPs : Number of training parameters and FLOPs for Peri-midFormer versus baseline in terms ofclassification accuracy for the UEA Heartbeat dataset (left) and long-term forecasting MSE for theETTh2 dataset (right). In the left graph, the closer to the top left, the better, while in the right graph,the closer to the bottom left, the better. Note that in the long-term forecasting, we did not fully depictthe corresponding sizes due to the oversized FLOPs of Time-LLM, but instead illustrated it with text. considering periodic components. \"w/o PPAM\" divides the time series into periodic components butwithout Period Pyramid Attention Mechanism, using periodic full attention instead, wherein attentionis computed among all periodic components. \"w/o Feature Flows Aggregation\" employs PPAM butwithout Periodic Feature Flows Aggregation. \"Peri-midFormer\" indicates our final approach. Results illustrates the incremental performance enhancement achieved with the integrationof each additional module, validating the effectiveness of Peri-midFormer. Notably, good resultsare achieved even without PPAM. This can be attributed to the models ability to extract periodiccharacteristics inherent in the original time series data by delineating the periodic components.However, without highlighting inclusion relationships through PPAM, periodic full attentions abilityto capture temporal changes is limited, emphasizing the significance of PPAM.",
  "Complexity Analysis": "We conducted experiments on the model complexity of Peri-midFormer using the Heartbeat datasetfor the classification task and the ETTh2 dataset for the long-term forecasting task. We consideredthe number of training parameters, FLOPs, and accuracy (or MSE). The results are depicted in. In the classification task, Peri-midFormer not only achieves a significant advantage inaccuracy but also requires relatively fewer training parameters and FLOPs, much less than manymethods such as TimesNet, GPT4TS, Crossformer, and PatchTST. In the long-term forecasting task,Peri-midFormer achieves the lowest MSE without requiring the enormous FLOPs that Time-LLMdoes. This shows that although Time-LLM has strong long-term forecasting capabilities on mostdatasets, its computational demands are unacceptable. See Appendix E.4 for more analysis.",
  "Conclusions": "In this paper, we introduced a method for general time series analysis called Peri-midFormer. Itleverages the multi-periodicity of time series and the inclusion relationships between differentperiods. By segmenting the original time series into different levels of periodic components, Peri-midFormer constructs a Periodic Pyramid along with its corresponding attention mechanism. Throughextensive experiments covering forecasting, classification, imputation, and anomaly detection tasks,we validated the capabilities of Peri-midFormer in time series analysis, achieving outstanding resultsacross all tasks. However, Peri-midFormer exhibits limitations, particularly in scenarios where theperiodic characteristics are less apparent. We aim to address this limitation in future research tobroaden its applicability.",
  "Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accuratemedium-range global weather forecasting with 3d neural networks. Nature, 619(7970):533538,2023": "Yaoran Chen, Candong Cai, Leilei Cao, Dan Zhang, Limin Kuang, Yan Peng, Huayan Pu,Chuhan Wu, Dai Zhou, and Yong Cao. Windfix: Harnessing the power of self-supervisedlearning for versatile imputation of offshore wind speed time series. Energy, 287:128995, 2024. Haotian Si, Changhua Pei, Hang Cui, Jingwen Yang, Yongqian Sun, Shenglin Zhang, JingjingLi, Haiming Zhang, Jing Han, Dan Pei, et al. Timeseriesbench: An industrial-grade benchmarkfor time series anomaly detection models. arXiv preprint arXiv:2402.10802, 2024. Zhen Liu, Wenbin Pei, Disen Lan, and Qianli Ma. Diffusion language-shapelets for semi-supervised time-series classification. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1407914087, 2024. Hongming Li, Shujian Yu, and Jose Principe. Causal recurrent variational autoencoder formedical time series generation. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 37, pages 85628570, 2023.",
  "Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers:Exploring the stationarity in time series forecasting. In Advances in Neural InformationProcessing Systems, 2022": "Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decompositiontransformers with auto-correlation for long-term series forecasting. In Advances in NeuralInformation Processing Systems, pages 101112, 2021. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:Temporal 2d-variation modeling for general time series analysis. In The Eleventh InternationalConference on Learning Representations, 2023. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time seriesforecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages1112111128, 2023. Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and JianLi. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlpstructures. arXiv preprint arXiv:2207.01186, 2022. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and WancaiZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In35th AAAI Conference on Artificial Intelligence, pages 1110611115, 2021. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer:Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39thInternational Conference on Machine Learning, 2022. Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, and Weiyao Lin. Basisformer: Attention-basedtime series forecasting with learnable and interpretable basis. Advances in Neural InformationProcessing Systems, 36, 2024. Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth64 words: Long-term forecasting with transformers. In The Eleventh International Conferenceon Learning Representations, 2022.",
  "Zhijian Xu, Ailing Zeng, and Qiang Xu. Fits: Modeling time series with 10k parameters. InThe Twelfth International Conference on Learning Representations, 2023": "Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong Jiang, and Shu-Tao Xia.Periodicity decoupling framework for long-term series forecasting. In The Twelfth InternationalConference on Learning Representations, 2024. Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor W Tsang.Disentangling structured components: Towards adaptive, interpretable and scalable time seriesforecasting. IEEE Transactions on Knowledge and Data Engineering, 2024. Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang,and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. InThe Twelfth International Conference on Learning Representations, 2024. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-YuChen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time seriesforecasting by reprogramming large language models. In The Twelfth International Conferenceon Learning Representations, 2024. Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and MingshengLong. itransformer: Inverted transformers are effective for time series forecasting. In TheTwelfth International Conference on Learning Representations, 2023.",
  "Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Expo-nential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381,2022": "Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max MergenthalerCanseco, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time seriesforecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages69896997, 2023. Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neuralbasis expansion analysis for interpretable time series forecasting. In International Conferenceon Learning Representations, 2019.",
  "Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time seriesanomaly detection with association discrepancy. In International Conference on LearningRepresentations, 2021": "Angus Dempster, Franois Petitjean, and Geoffrey I Webb. ROCKET: Exceptionally fastand accurate time series classification using random convolutional kernels. Data Mining andKnowledge Discovery, 34(5):14541495, 2020. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-termtemporal patterns with deep neural networks. In The 41st international ACM SIGIR conferenceon research & development in information retrieval, pages 95104, 2018. Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Lineariz-ing transformers with conservation flows. In International Conference on Machine Learning,pages 2422624242. PMLR, 2022.",
  "Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition:Results, findings, conclusion and way forward. International Journal of Forecasting, 34(4):802808, 2018": "Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, andArtur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. arXivpreprint arXiv:2201.12886, 2022. T. Zhang, Yizhuo Zhang, Wei Cao, J. Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more:Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXivpreprint arXiv:2207.01186, 2022. Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom,Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018.arXiv preprint arXiv:1811.00075, 2018. Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detectionfor multivariate time series through stochastic recurrent neural network. In Proceedings of the25th ACM SIGKDD international conference on knowledge discovery & data mining, pages28282837, 2019. Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Sder-strm. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding.Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &Data Mining, pages 387395, 2018. Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research andtraining on ics security. In 2016 international workshop on cyber-physical systems for smartwater networks (CySWater), pages 3136. IEEE, 2016. Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronousmultivariate time series anomaly detection and localization. In Proceedings of the 27th ACMSIGKDD conference on knowledge discovery & data mining, pages 24852494, 2021. Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and XifengYan. Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting. In NeurIPS, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural InformationProcessing Systems, 30, 2017.",
  ": Full flowchart": "Here, we provide further elaboration on the details of Peri-midFormer. Its full flowchart is depicted in, depicts two strategies for input. The strategy indicated by the red line is for classification task,where de-normalization and time series decomposition are not used. This is because, in classificationtask, there is no need to reconstruct the original data; therefore, the trend part does not need to beextracted and added back. Additionally, it is important to note that the trend part is a significantdiscriminative feature for classification data, so it cannot be separated from the original data beforefeature extraction. In the strategy indicated by the blue line, employed for reconstruction tasks, Peri-midFormer needs to focus on the periodicity in the time series. Therefore, we utilize de-normalizationand time series decomposition to eliminate other influencing factors. To achieve this, we first refer to to address instability factors. We normalize the input X = [x1, x2, ..., xL] RLC to obtainXnorm = [x1, x2, ..., xL] RLC:",
  "D.1Metrics": "We utilize various metrics to evaluate different tasks. For long-term forecasting and imputations,we employ the mean square error (MSE) and mean absolute error (MAE). In anomaly detection,we utilize the F1-score, which combines precision and recall. For short-term forecasting, we utilizethe symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), andoverall weighted average (OWA), with OWA being a unique metric used in the M4 competition.",
  "E.1Hyper Parameter Analysis and Model Limitations": "In Equation (1), we introduced a hyperparameter k to select the most important frequency, whichalso determines the number of levels in the Periodic Pyramid. We conducted sensitivity analysis onit, as shown in . Its evident that our proposed Peri-midFormer exhibits relatively stableperformance across different choices of k for all four tasks. However, there are still some fluctuationin results among different k values depending on the task and dataset, which are determined by theperiodic characteristics in the dataset. To illustrate this, we visualize individual data for long-termforecasting and classification tasks, as shown in . It can be observed that in the long-termforecasting task, the Etth1 dataset exhibits clear periodicity. Therefore, with larger k, Peri-midFormercan capture more periodic information, resulting in better performance. In contrast, the Etth2dataset has less obvious periodicity, so it achieves better results with smaller k, as larger k introduceunnecessary noise, affecting model performance. In the classification task, the EthanolConcentrationdataset is difficult to classify, whereas a larger k allows for the construction of Periodic Pyramid withmore levels, thus extracting more representative features and achieving higher accuracy. In addition,the SelfRegulationSCP1 dataset contains a lot of high-frequency noise, so larger k would focus onirrelevant information, leading to decreased accuracy. Therefore, we adjusted k values differently fordifferent tasks and datasets, as shown in the range in . The above analysis reveals that the limitation of Peri-midFormer lies in its inability to fully leverageits advantages on datasets with poor periodicity characteristics. We plan to address this issue in ourfuture work.",
  "E.2Periodic Pyramid Attention Mechanism Analysis": "To illustrate the PPAM more clearly, we visualize the original time series and attention scores withinthe periodic pyramid for the SelfRegulationSCP2 dataset in the classification task, as shown in . It is evident that the attention scores among components are distributed based on inclusion andadjacency relationships, meaning that components in different levels with inclusion relationshipsor those in the same level have higher attention scores. This demonstrates the rationality of thePeriodic Pyramid structure. Additionally, when the hyperparameter k in Equation (1) is set to 2, thePPAM degenerates into periodic full attention, wherein attention is computed among all periodic Original time seriesPeriodic pyramidAttention score 0 12 3 4 5 6 0.60.40.20-0.2-0.4-0.6-0.8 : Visualization of the original time series and attention scores within the periodic pyramidfor the SelfRegulationSCP2 dataset in the classification task. The left side shows the original data, themiddle displays the corresponding pyramid structure, and the right side depicts the attention scoreswithin the pyramid. In this example, k = 3, f1 = 1, f2 = 2, f3 = 4. 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0246810 12 14 16 18 20",
  ": Original time series of Electricity dataset (left) and Periodic Ptramid Attention score(right)": "components. To illustrate this, we visualize the Electricity dataset in the long-term forecastingtask and its corresponding attention distribution, as shown in . The figure shows that thePeriodic Attention Mechanism can capture the dependencies among the periodic components andidentify which components belong to a longer component (note that k = 2 corresponds to 21 periodiccomponents with larger amplitudes). This is attributed to the separation of the periodic components,which explicitly expresses the hidden periodic inclusion relationships in the time series. The aboveanalysis demonstrates the effectiveness of the PPAM.",
  "E.3Periodic Feature Flows Analysis": "To intuitively understand the Periodic Feature Flows, we visualize it as shown in . Onthe left side is the pyramid representation of the Periodic Feature Flows, with the horizontal axisrepresenting the number of feature flows and the vertical axis representing the length of each featureflow. The feature flows are divided into multiple levels, each containing multiple periodic components.The red box encloses one feature flow, with its position from top to bottom corresponding to thepyramid from top to bottom. It can be observed that some adjacent feature flows are the same at thecorresponding positions, this is because they pass through the same periodic component. On the rightside, the waveform of each feature flow is displayed in different colors, with the position from left toright corresponding to the top to the bottom of the pyramid. It can be observed that each feature flowdiffers, which is why it is necessary to aggregate different feature flows. The aim is to fully utilizethe information from each periodic component to better reconstruct the target sample.",
  "E.4Training/Inferencing Cost": "To further validate the computational complexity and scalability of the proposed method, we con-ducted detailed experiments on Electricity and ETTh1 datasets for the long-term forecasting task.These experiments focused on the complexity and actual time of training and inference, as wellas memory usage, with results presented in Tables 8 and 9. It can be seen that our proposed Peri-",
  "Autoformer202.34G6.34G12981M3216M16M3400s0.0123s0.501": "midFormer demonstrates a significant advantage in computational complexity on the Electricitydataset and achieves the lowest MSE. Similarly, on the ETTh1 dataset, Peri-midFormers computa-tional cost and inference time are not disadvantages. Instead, it achieves a lower MSE, second only toTime-LLM, while having much lower computational cost and inference time compared to it. Theseresults highlight the advantages of our method in terms of computational complexity and scalability.",
  "E.5Pre-interpolation": "Since Peri-midFormer is designed to focus on the periodic components of time series, directlyhandling data with missing values in imputation tasks may prevent it from correctly capturing theperiodic characteristics of the original data without missing values, thus affecting its imputationeffectiveness. Therefore, to adapt Peri-midFormer for imputation tasks, we first apply a linearinterpolation strategy (Equation (13)) to the data with missing values to partially restore the periodiccharacteristics of the original data before inputting it into Peri-midFormer for further imputation. Thevisualization of original data, data with 50% missing values, and pre-interpolated data of Electricitydataset are illustrated in . It is evident that missing values significantly disrupt the periodic",
  ": Visualization of original data, data with 50% missing values, and pre-interpolated data ofElectricity dataset": "characteristics of the original data, while pre-interpolation partially restores them. It is worth notingthat this is not a speculative action but an effort to further explore the potential of deep learningmodels in imputation tasks. Our goal is for deep models to capture subtle variations in time seriesto achieve imputation in the details rather than wasting effort on goals achievable through simplelinear interpolation. Additionally, since we can retain the indices of missing values in practicalapplications, there is no concern about the reliability of evaluating the imputation results. To validatethe effectiveness of the pre-interpolation strategy, we applied it to other methods, as shown in . We conducted experiments on four mask ratios {0.125, 0.25, 0.375, 0.5}, where the first row usesonly the pre-interpolation strategy. From the results, it can be seen that pre-interpolation significantlyimproves the performance of each method across all four mask ratios, demonstrating its importancefor deep learning-based methods in interpolation tasks.",
  "E.6Look-back window length": "Here, we investigate the performance of Peri-midFormer under different look-back window lengthsin long-term forecasting task on the Electricity dataset. As shown in , we experimented notonly with prediction lengths of {96, 192, 336, 720}, but also with longer lengths such as 1000. It canbe observed that there is a clear improvement in performance across all five prediction lengths as thelook-back window length increases. This improvement is particularly notable for longer predictionlengths such as 336, 720, and 1000. It indicates that Peri-midFormer can effectively utilize morehistorical information, as longer look-back windows contain more stable periodic characteristics,which is precisely what Peri-midFormer requires.",
  "E.7Time Series Decomposition Analysis": "Peri-midFormer performs well on the Exchange dataset for long-term forecasting task and the Yearlydataset for short-term forecasting. Although these two datasets lack obvious periodicity, they exhibitstrong trends, as shown in Figure (19)). Peri-midFormer uses a time series decomposition strategy,where the trend part is first separated from the original data before dividing the periodic components.After the output of Peri-midFormer, the predicted trend part is added back, as illustrated in Figure(8). The separated trend part is easier to predict, especially when the trend is strong. This is whyPeri-midFormer achieves strong performance on the Exchange and Yearly datasets.",
  "FProof": "To demonstrate the essence of attention computation among multi-level periodic components, weneed to analyze how the interactions between periodic components at different levels affect the finalfeature extraction. In time series analysis, different periodic components correspond to different timescales. This means that through decomposition, we can capture components of various frequencieswithin the time series. The essence of the periodic pyramid is to capture these different frequencycomponents through its hierarchical structure. Using single-channel data as an example, and given that we adopt an independent channel strategy,this can be easily extended to all channels. Assume the time series x(t) can be decomposed intomultiple periodic components xn(t) :",
  "(19)": "Based on this, considering the periodicity and symmetry of sin(a + b) and sin(a b), when theperiods of two time series components are close / same (intra-layer attention in the pyramid, see theright side of in the original paper) or have overlapping / inclusive parts (inter-layer attentionin the pyramid, see the right side of in the original paper), the values of these two sinefunctions will be highly correlated, resulting in a large QiKTj value. This indicates that the periodicpyramid model can effectively capture similar periodic patterns across different time scales.",
  "where Vj = WV xj(t) = WV Aj cos2tTj + j": "From the above derivation, it can be seen that the attention mechanism can measure the similaritybetween different periodic components. This similarity reflects the alignment between differentperiodic components in the time series, allowing the model to capture important periodic patterns. Bycapturing these periodic patterns, the periodic pyramid can extract key features of the time series,resulting in a comprehensive and accurate time series representation. This representation not onlyincludes information across different time scales but also enhances the representation of importantperiodic patterns.",
  "H.5Full Results of Anomaly Detection ()": ": Full results for the classification task. ( means former, T-LLM means Time-LLM,GPT means GPT4TS, T-Net means TimesNet, Patch means PatchTST, Light means LightTS,Station means the Non-stationary Transformer.) We report the classification accuracy (%) asthe result.The standard deviation is within 1%.We reproduced the results of PatchTSTby reproduced TSLANet by and copied the others from GPT4TS . Red: best, Blue:second best.",
  "Average Accuracy72.5 68.7 70.3 71.9 71.5 70.8 71.1 72.7 70.7 71.0 73.0 67.5 70.4 73.6 74.0 76.076.5": ": Full results for the short-term forecasting task in the M4 dataset. ( means former, T-LLMmeans Time-LLM, GPT means GPT4TS, T-Net means TimesNet, Patch means PatchTST, HiTSmeans N-HiTS, BEATS means N-BEATS, Light means LightTS, Station means the Non-stationaryTransformer.) The standard deviation is within 0.5%. We copied the results of Time-LLM fromTime-LLM , Pyraformer from TimesNet , and the remaining results from GPT4TS . Red:best, Blue: second best.",
  "OWA 0.850 0.859 0.861 0.851 0.869 0.861 0.855 1.172 1.051 1.051 0.918 0.930 0.939 1.480 1.230 1.775": ": Full results of 512 look-back window length in long-term forecasting task (since FITS defaults to a look-back window lengthof 720, its results at 720 length are attached at the end). The standard deviation is within 0.5%. We copied the results of GPT4TSfrom GPT4TS , Time-LLM from TSLANet , reproduced TSLANet by reproduced the others by Red: best, Blue: second best.",
  "Average0.308 0.337 0.324 0.346 0.320 0.341 0.308 0.334 0.322 0.344 0.361 0.375 0.331 0.350 0.422 0.402 1.147 0.711 0.323 0.345": ": Full results of 96 look-back window length in long-term forecasting task. ( means former, Station means the Non-stationaryTransformer.) The standard deviation is within 0.5%. We copied the results of iTransformer from TSLANet , Pyraformer fromTimesNet , reproduced FITS by and copied the others from GPT4TS . Red: best,Blue: second best.",
  "Average0.339 0.349 0.345 0.353 0.373 0.366 0.335 0.358 0.311 0.335 0.381 0.372 0.995 0.709 0.410 0.409 0.458 0.433 0.431 0.408 0.402 0.401 0.429 0.422 0.953 0.803 1.650 0.877": ": Full results for the imputation task. We randomly mask 12.5%, 25%, 37.5% and 50% time points to compare the modelperformance under different missing degrees. ( means former, Station means the Non-stationary Transformer.) The standard deviationis within 0.5%. We reproduced the results of Pyraformer by and copiedthe others from GPT4TS . Red: best, Blue: second best.",
  "Average0.0470.1230.048 0.128 0.050 0.132 0.053 0.159 0.164 0.309 0.123 0.229 0.119 0.225 0.112 0.23 0.057 0.143 0.062 0.151 0.592 0.528 0.165 0.274 0.134 0.241": ": Full results for the anomaly detection task. The P, R and F1 represent the precision, recall andF1-score (%) respectively. F1-score is the harmonic mean of precision and recall. A higher value of P, R andF1 indicates a better performance. (Station means the Non-stationary Transformer.) The standard deviation iswithin 1%. We copied the results from GPT4TS . Red: best, Blue: second best."
}