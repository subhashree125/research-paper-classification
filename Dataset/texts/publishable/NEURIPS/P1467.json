{
  "Shanghai Jiao Tong University2Ant Group": ": Given a few-shot target dataset of a specific domain such as sketches painted by an artist (a),it is usually difficult to directly generate images of this domain using pretrained text-to-image models(b). By using DomainGallery we propose in this work, we can achieve domain-driven generation inintra-category (c); cross-category (d); extra attribute (e); and personalization (f) scenarios.",
  "Abstract": "The recent progress in text-to-image models pretrained on large-scale datasets hasenabled us to generate various images as long as we provide a text prompt describ-ing what we want. Nevertheless, the availability of these models is still limitedwhen we expect to generate images that fall into a specific domain either hard todescribe or just unseen to the models. In this work, we propose DomainGallery, afew-shot domain-driven image generation method which aims at finetuning pre-trained Stable Diffusion on few-shot target datasets in an attribute-centric manner.Specifically, DomainGallery features prior attribute erasure, attribute disentangle-ment, regularization and enhancement. These techniques are tailored to few-shotdomain-driven generation in order to solve key issues that previous works havefailed to settle. Extensive experiments are given to validate the superior perfor-mance of DomainGallery on a variety of domain-driven generation scenarios.Codes are available at",
  "Introduction": "Walking down the street, you see an artist painting portrait sketches for people. You are fascinatedby a couple of masterpieces set by the side, showing his/her unique painting style which you find itdifficult to describe by words. Deeply interested, you are in the mood for seeing more sketches, and itwould be perfect to see him/her painting other things like dogs, especially your favorite ones at home. As a fundamental topic in computer vision, image generation has been attracting enormous researchefforts. However, through the years from VAEs , GANs to diffusion models , generativemodels are becoming more and more data-hungry in order to properly model the distribution ofimages, as the most recent Stable Diffusions have been trained on billions of text-image pairs. Thus unfortunately, it is usually infeasible to directly train a generative model given onlyfew-shot (around ten, or fewer) images of a specific target domain. To tackle such challenging scenarios, one paradigm of solutions is model transfer, which first trains amodel on a relevant source domain and then transfers it to the target domain by finetuning on thefew-shot target dataset. Nevertheless, as Zhao et al. have pointed out, the performance of modeltransfer methods will be significantly influenced by the relevance between source/target domains.Therefore, the applicability of these methods will be limited if we either fail to find a proper sourcedataset, or just do not have enough resources to train a generative model from scratch. With the recent progress in pretrained text-to-image (T2I) models , it seems thatanything can be generated simply by putting a text prompt into an off-the-shelf pretrained T2I model.However, T2I models are still far from once for all solutions to image generation. Sometimes it isdifficult or even impossible to precisely describe certain styles (e.g. sketches by an artist) and contents(e.g. new concepts or personalized subjects), or what we want is simply unseen (thus unknown) tothe model. Fortunately, T2I models can serve as universal source models to be finetuned on specifictarget datasets. Recent works finetuning T2I models have mostly focused on either finetuning withrelatively abundant images (tens, hundreds, or more) , or few-shot subject-driven generationwhose datasets consist of a single person or object . On the contrary, few-shot domain-drivengeneration analogous to the conventional model transfer has rarely been explored. In this work, we analyze and perform few-shot domain-driven image generation from the view ofattributes, as a domain is defined by common attributes shared among images (see Sec. 3). We seekto master four generation cases as illustrated in : Intra-category: The generated images containboth the domain attributes and the categorical attribute of the given target dataset, as in conventionalmodel transfer; Cross-category: While containing non-categorical domain attributes, images ofother categories can be generated through text control, as a feature of T2I models; Extra attribute:Either intra- or cross-category, we can attach additional attributes to the images; Personalization:We hope to combine domain-driven and subject-driven generation for better personalization. Inorder to achieve these goals, we propose DomainGallery, adopting DreamBooth-like finetuningparadigm where the non-categorical domain attributes are learned and bound to an identifier word, sothat the generation can be done via a normal T2I pipeline. DomainGallery features four attribute-centric finetuning techniques which respectively settle four challenges: (1) Prior attribute erasure: The prior attributes of the identifier word may possibly show up even ifwe have bound new domain attributes to it. Therefore, we pre-erase these prior attributes to avoidunexpected elements in images. (2) Attribute disentanglement: The domain/categorical attributes corresponding to the identi-fier/category word may be leaked into each other, causing missing domain attributes and/or un-expected categorical attributes when we change the category word in cross-category generation.Therefore, we explicitly encourage domain-category disentanglement to prevent such leakage. (3) Attribute regularization: The model is prone to overfitting when finetuned on few-shot datasets.Therefore, we regularize the finetuning process (with a strategy to construct paired source/targetlatent codes and a regularization loss) to reduce overfitting caused by excessive presence of domainattributes and possible biases of dataset distributions. (4) Attribute enhancement: Sometimes the strengths of the domain attributes learned on a specificdataset category are insufficient for cross-category generation. Therefore, we adjust the intensity ofthe domain attributes when generating cross-category images for better fidelity. These techniques spreading over pre-finetuning (1)(2), finetuning (2)(3) and inference (4), are tailoredto few-shot domain-driven generation, aiming at solving key issues that previous works have failedto settle. Later in Sec. 5, we conduct thorough experiments on several few-shot datasets. Theseexperiments manifest the superior and satisfying performance of DomainGallery on all of the fourgeneration scenarios, which can serve as a state-of-the-art method of few-shot domain-driven imagegeneration.",
  "Related Work": "Model TransferModel Transfer (of conventional noise-to-image models instead of T2I ones)is a mainstream paradigm of solutions to few-shot image generation. Methods following thisparadigm transfer models trained on related source datasets to target domains by finetuning onfew-shot target datasets. Model transfer has been thoroughly explored using GANs , with a few base on diffusion models . SinceT2I models came to light, people have been freed from choosing proper source datasets/models, andattention has been turned to finetuning T2I models as generic source models. Subject-driven Image GenerationAs one of the most frequently explored finetuning scenarios,subject-driven generation has attracted much research effort since thepioneering works Textual Inversion and DreamBooth . Actually, subject-driven generationcan be categorized as a special case of domain-driven generation, where the domain is defined by aparticular person or object. To preserve the subject identity, fidelity is highly preferred to diversity,as diversity is scarcely evaluated quantitatively by these works. On the contrary, in general domain-driven generation the domains are usually not confined to a specific subject. Therefore diversity is asimportant as fidelity, and we will evaluate both just as model transfer works do. Few-shot Domain-driven Image GenerationWe follow previous works to name our goal asfew-shot domain-driven image generation. Analogous to subject-driven, the term domain-drivenimplies finetuning from T2I models, which enables us to take advantage of the multi-modal capabilityof these models to achieve a variety of generation scenarios (see (cf)). To the best of ourknowledge, there is only one previous work focusing on this topic, namely DomainStudio . Itfinetunes a Stable Diffusion model towards the target domain by learning an identifier similar toDreamBooth , yet equipped with additional losses to enhance diversity and high-frequency details.In Sec. 4, we will analyze some crucial issues in domain-driven generation that previous works havefailed to settle, and accordingly propose attribute-centric solutions to these problems. Other Similar TasksThere are some other works focusing on resembling tasks. For instance,Everaert et al. have focused on finetuning under limited data (tens to hundreds) with per-imagetext prompts. Such requirement of image quantity and prompts has limited its applicability. Anothersimilar topic is T2I style transfer , which usually extracts style information from asingle style image and controls the content via text. A key issue shared by these works is how toclearly defining the boundary between style and content from a single image. Instead, domains canbe naturally delimited as the common attributes shared among multiple images, which also enablesus to learn a domain of certain contents, rather than styles.",
  "Preliminary": "DomainFormally, a domain D can be defined as a sample space X and a data distribution Pdataon X . However, this definition is excessively general as any group of arbitrary images can form adomain. In this work, we would like to provide a rather intuitive definition from the viewpoint ofcommon attributes. We regard an image X to be composed of a set of attributes {ai}Ni=1, where eachai can be either abstract like a certain style, or concrete like a specific category or certain content.Then, an image domain D can be defined as the common attributes shared by all the images of thisdomain: aD = XD X. According to such definition, an image belongs to this domain if and onlyif it contains all the common attributes: X D aD X. Take the few-shot sketches offaces in (a) as an example, aD includes shared categorical attribute of human faces and theattributes of this specific painting style, while the content attributes indicating individuals are notshared. Therefore, any facial sketch of any person in such style belongs to this domain. Since in real-world scenarios, images in few-shot datasets usually share a common category (e.g. facein (a)), it is natural that categorical attribute should be one of the domain attributes. However, toextend domain-driven generation to cross-category scenarios as in (d), in this work we excludethe categorical attribute from aD so that the domain attributes refer to non-categorical attributes only.For instance, the domain in will be referred to as sketches (of anything) in this certain style. Diffusion ModelDiffusion model is a recent genre of generative models. It aims atreversing a diffusion process by recurrently predicting the noises based on noisy data and denoisingthem accordingly till proper images are rendered. For practical usage in high-resolution and condi-tional cases, Latent Diffusion Model (LDM) is often adopted which moves the diffusion processto latent spaces with pretrained VAEs . LDM is commonly trained using a simplified objective asLLDM = El,c,N (0,I),t (lt, t, (c))22,(1)where l, c, and t are respectively latent codes, conditions, ground-truth noises and time steps. Themodule is the encoder of the condition and is the noise-predicting network which is usually aUNet . As special instances of LDM, Stable Diffusion (SD) series are pretrained on large-scaletext-image datasets such as LAION-5B . They serve as state-of-the-art T2I models that are widelyused as base models in many tasks, including our DomainGallery as well. DreamBoothAs a pioneering work in subject-driven image generation, DreamBooth binds theinformation of the subject to an identifier [V], which is a rarely used word such as sks, together witha corresponding category word [N], such as dog. Then images of the target subject can be generatedby using prompts like a [V] [N]. For domain-driven image generation, we inherit such design tobind (non-categorical) domain attributes to [V], so that by changing category words or adding extraattributes via text, DomainGallery is capable of generating various images within the given domain. Low-Rank AdaptationLow-Rank Adaptation (LoRA) is a popular finetuning method fre-quently used on SD models. Instead of finetuning the parameters W Rdindout, LoRA finetunesrank decomposition matrices A Rdinr and B Rrdout as in W = W + A B, where r is verysmall and W is fixed. Finetuned LoRA parameters can be easily shared and used with base modelsdue to much smaller sizes. DomainGallery adopts LoRA when finetuning SD on target datasets.",
  "Prior Attribute Erasure": "Following DreamBooth, we link target domain attributes to an identifier [V]. Although we expect toselect a rarely used word without obvious meaning, this word may have still been bound to certainprior attributes. For instance, the commonly used sks is actually the abbreviation of a rifle , thusimages generated with [V] in prompts will contain military elements, like the helmet in (a). Insubject-driven generation such prior attributes are not problems, since the text condition a [V] [N],as a whole, will gradually overfit to the given subject dataset and override these prior attributes. Also,[V] will never be paired with another category (e.g. a [V] cat when the subject is a dog), while indomain-driven generation we expect [V] to be applicable to any category. According to the resultsin Sec. 5.2 and Appendix B.1, if not pre-erased, these prior attributes will appear in cross-categoryimages, which verifies that these prior attributes are merely concealed rather than eliminated, and it isnecessary to erase them before usage. Since the prior attributes are bound to the identifier in a data-driven manner when T2I models arepretrained, it is difficult to theoretically specify which attributes have been linked to [V]. Therefore,we propose an empirical solution to prior attribute erasure. Based on a noisy source latent lsrc thathas been added noise in the forward process, DomainGallery predicts the added noises src andst using the same LoRA-equipped UNet respectively with source text condition csrc = a [N]and target condition ctgt = a [V] [N]. Then, the prior attribute erasure loss is defined as",
  "erase": ": An overview of DomainGallery. (a) Before finetuning, we erase the prior attributes of theidentifier [V] by matching the predicted noises when using source/target text conditions via Lerase.(b) During fintuning, besides training ordinarily on target datasets (top-left), we additionally imposedomain-category attribute disentanglement loss Ldisen (bottom-left) and transfer-based similarityconsistency loss Lsim (right). (c) When generating cross-category images, we enhance the domainattributes referred by [V] in a CFG-like manner. Dashed arrows indicate gradient stopping. where we omit time step t and text encoder in the UNet , for brevity, indicates LoRAparameters and gs() is the gradient stopping operation that stops the gradient from propagatingthrough or updating the parameters inside. By imposing Lerase, we hope that the model predicts thesame with or without [V], hence the prior attributes in [V] will be removed. Besides Lerase, the prior preservation loss Lprior of DreamBooth is also applied which trains onsource images Isrc generated by the base model of SD itself as training a diffusion model ordinarilyvia Eq. (1). Also, disentanglement loss Ldisen is also included, which will be detailed in Sec. 4.2.After erasure, the learned LoRA parameters will be used to initialize LoRA in the finetuning period.",
  "Finetuning": "With prior attributes of [V] erased, DomainGallery then learns to bind the target domain attributes to[V]. In addition to a standard finetuning on target datasets by Ltgt via Eq. (1), with prior preservationon pre-generated source images, we propose domain-category attribute disentanglement loss Ldisenand transfer-based similarity consistency loss Lsim, as depicted in (b). Domain-category Attribute DisentanglementSince few-shot datasets usually share a commoncategory (e.g. face in (a)), when finetuning on such datasets, the (non-categorical) domainattributes in [V] will always show up together with the categorical attribute in [N], both in targetimages Itgt and target prompts ctgt. As a result, it is possible that certain domain attributes mayleak into [N], and/or conversely the categorical attribute may leak into [V]. Although it is not a problem either for subject-driven generation since [V] and [N] will always be paired when generatingimages, such entanglement between [V] and [N] will harm cross-category scenarios of domain-drivengeneration. As experimental results shown in Sec. 5.2 and Appendix B.1, if we replace [N] withanother category, sometimes domain attributes are partially lost, or elements of the original categorystill appear. To tackle this issue, we try to enhance the disentanglement between [V] and [N], so that all the domainattributes will only be learned into [V] without leaking into [N], and categorical attributes in [N]will not be lost. In other words, attributes of [N] after finetuning should not be different from thosebefore. As we use LoRA, the base model before finetuning is ready to use by simply disenablingLoRA parameters temporarily since the UNet is fixed. Based on noisy source latent lsrc and sourcetext condition csrc, the domain-category attribute disentanglement loss can be formulated as",
  "where without is the base UNet whose LoRA parameters are detached": "Attribute RegularizationAdding regularization is a common practice of model transfer methods to prevent overfitting, where features from paired source/target images generated fromthe same noise are usually required. However, according to the training objective of SD in Eq. (1),no fully denoised latent (i.e. at time step 0) will be generated, let alone paired source/target latents.DomainStudio has proposed a regularization, which applies a similarity consistency loss onbatches of source/target images Isrc/Itgt decoded from denoised latents lsrc/ltgt after a single-stepdenoising from noisy latents lsrc/ltgt. However, there are four drawbacks in this design: (1) single-step denoising usually does not lead to meaningful latents/images unless the timestep is small; (2)decoding latents into images induces significant overhead of computation and storage; (3) computingcosine similarity between pixel-level images is less reasonable; (4) Isrc/Itgt are unpaired as theyderives from unpaired input source/target images Isrc/Itgt, which do not fit the similarity consistencyloss requiring paired images/features. In our DomainGallery, we propose a strategy of constructingpaired source/target latents, followed by a new regularization term named transfer-based similarityconsistency loss, which overcomes the aforementioned drawbacks. First we try to settle (1) and (4) by constructing denoised, meaningful and paired latent codes. Asin the right part of (b), given a batch of lsrc at time step t, we conduct an n-step recurrentdenoising following the accelerated denoising process of DDIM and a linearly decreasing timestep schedule from t to 0. We intuitively set n = 5 to balance denoising quality and speed. We dorecurrent denoising for twice, respectively with source/target text csrc/ctgt, and obtain lsrc/lst. If wedecode them into images Isrc/Ist, we will find that Ist simultaneously have partial target domainattributes after conditioned on ctgt, and share certain similarity with Isrc since they derive from thesame lsrc. Hence lsrc/lst are paired. Next, without actually decoding them into images, we reusethe encoder of UNet as a pretrained feature extractor to directly extract multi-layer features from lsrcand lst, and compute the similarity consistency loss as",
  "(4)": "where f k represents features extracted by the UNet encoder at its k-th layer (of N layers). From theviewpoint of the i-th latent (of B latents in the batch), first its cosine similarities with other latentsare computed, followed by a softmax operation transforming them into a probabilistic distribution pi.Then, Kullback-Leibler Divergence will be computed between two distributions respectively fromlisrc and list, and will be averaged among all i and layers k. In conclusion, Lsim helps to preventoverfitting by matching the similarity distributions among a batch of paired lsrc/lst. By operating onthe features directly extracted from latents rather than on images, our Lsim settles (2) and (3) as well.",
  "Inference": "In preliminary cross-category experiments, the domain attributes are not sufficiently manifestedsometimes. A possible reason is that Lsim has limited the strengths of these attributes to the minimal,just enough to transfer images of the original category, while for cross-category scenarios theseattributes may need enhancing. As shown in (c), we propose an inference-time attributeenhancement based on classifier-free guidance (CFG). Specifically, after applying CFG withdefault weight 1 = 7.5, we additionally increase the strength of [V] by either of",
  "Personalization": "For personalization scenarios, we straightforwardly combine our DomainGallery with DreamBoothin a single stage. Specifically, during the finetuning process in Sec. 4.2, the model is additionallyfinetuned on target subject images and source images of subject category via Eq. (1). In such cases,the objective of finetuning in Eq. (5) will be rewritten as",
  "Experimental Setting": "BaselineOur baseline list includes DreamBooth , as the basis of our method; a LoRA version of DreamBooth, since we utilize LoRA in DomainGallery; and finally DomainStudio , asthe only previous work in few-shot domain-driven image generation. DatasetWe test our method on five widely used 10-shot datasets, including CUFS sketches ([N]: face), FFHQ sunglasses ([N]: face), Van Gogh houses ([N]: house), watercolor dogs ([N]: dog) and wrecked cars ([N]: car). Note that though sunglasses and wrecked cars mayalso be generated by directly mentioning their content attributes in text prompts, we still try on thesedatasets to prove that DomainGallery can also learn content attributes. Experiments are conducted onresolution 512 512 except for DomainStudio which is only capable of 256 256 even on a 40GBVRAM GPU. MetricWe provide quantitative results for intra-category generation since we have dataset imagesas ground truth. For datasets with full sets (CUFS sketches and FFHQ sunglasses), we computeFID between 1,000 samples with the full sets. For the others, we replace FID with KID (103) which better fits few-shot scenarios . Intra-clustered LPIPS of1,000 samples with the few-shot training sets is also reported as a standalone diversity metric.",
  "Experimental Result": "Intra-categoryAs the most basic scenario, we generate target images of the original categories.According to Tab. 1, DomainGallery generally outperforms the baselines w.r.t. both fidelity anddiversity. These scores also match the qualitative results on CUFS sketches in , where Domain-Gallery can precisely capture the painting style of the target domain. Also, due to the effectivenessof our transfer-based similarity consistency loss Lsim, the diversity of DomainGallery surpasses thebaselines by large margins, while achieving competitive or even better fidelity. Refer to inAppendix B.2 for qualitative results on the other datasets. Cross-categoryWe illustrate qualitative results of cross-category generation on Van Gogh housesand watercolor dogs in . Since no previous method has pre-erased the prior attributes of [V] (sks)before usage, prior attributes of military elements can be observed in the samples generated by allthe baselines. Besides, as none of the baselines explicitly imposes disentanglement between [V] and[N], attribute leakage can be observed on both datasets. Some images of DomainStudio still containhouses even if we change [N], manifesting leaked categorical attribute in [V]. On the other hand,many cross-category images of the baselines do not properly depict target domain attributes whiletheir intra-category images do in Appendix B.2. Such phenomenon verifies that domain attributeshave been partially leaked into [N] and will disappear if we change it. By adopting prior attributeerasure and enhancing domain-category attribute disentanglement, our DomainGallery avoids theseissues and performs well. Samples on the other datasets are shown in in Appendix B.2. Extra AttributeIn and (e), we show some images generated by DomainGallery onCUFS sketches with extra attributes added to either intra- or cross-category scenarios. We mayinfer from these results that though we only provide simple prompt (i.e. a [V] [N]) rather thandetailed description for each image of the target dataset, training DomainGallery does not destructthe original text-image structures of SD. The images are still under full control through text prompts,including facial expressions, additional contents (e.g. accessories), sub-category (e.g. a breed ofanimals), background, and specific instances (e.g. celebrities or brands). Besides, the bottom row of illustrates some samples where the extra attributes (blue) providedin the text prompt are in conflict with the domain attributes (colorless). In such case, DomainGalleryis capable of generating images with partially fused attributes. While the images are generally in",
  ": The 10-shot datasets (left) and the cross-category samples generated by the baselines andDomainGallery (right), on Van Gogh houses (top) and watercolor dogs (bottom)": ": Intra-category (top row) and cross-category (middle row) samples with extra attributesgiven by texts generated by DomainGallery, on CUFS sketches. The bottom row additionally showthe case where the text contains conflicting attributes. grayscale, some blue feathers still appear. These results verify the generalization ability of our methodand suggest that it may be open to other generation scenarios such as local editing and style blending. PersonalizationIn the last scenario, DomainGallery is combined with DreamBooth to learn atarget domain and a target subject simultaneously, as described in Sec. 4.5. Results in manifestthat such combination is feasible for both intra-category (the target dataset and subject share the samecategory, e.g. watercolor dogs and the subject of specific dog) and cross-category (otherwise) pairsof datasets. Together with the results of the previous scenario with extra attributes, the satisfyingperformance of DomainGallery shows its potentials to be applied to parallel or downstream tasks.",
  "Conclusion": "In this work, we focus on few-shot domain-driven image generation by analyzing several key issuesthat previous works have failed to settle, and accordingly proposing a new method named Domain-Gallery. DomainGallery features four attribute-centric finetuning techniques that aim at solving theseissues, namely prior attribute erasure, attribute disentanglement, attribute regularization and attributeenhancement. With these designs tailored to domain-driven generation, our DomainGallery achievesconvincing performance on both intra-category and cross-category generation scenarios, while sup-porting extra attributes added by text prompts. Additionally, DomainGallery can be aggregated withsubject-driven generation as well, which further extends its applicability. In Appendix C, we willdiscuss possible limitations and potential future works of DomainGallery.",
  "Acknowledgements": "The work was supported by the National Science Foundation of China (62076162, 62471287, 62302295), andthe Shanghai Municipal Science and Technology Major Project, China (2021SHZDZX0102). This work wasalso supported by Ant Group. Milad Abdollahzadeh, Touba Malekzadeh, Christopher T. H. Teo, Keshigeyan Chandrasegaran, GuimengLiu, and Ngai-Man Cheung. A survey on generative modeling with limited data, few shots, and zero shot.arXiv preprint arXiv:2307.14397, 2023.",
  "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep languageunderstanding. In NeurIPS, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kun-durthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An openlarge-scale dataset for training next generation image-text models. In NeurIPS, 2022. Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen Chang, YuanzhenLi, Irfan Essa, Michael Rubinstein, Yuan Hao, Glenn Entis, Irina Blok, and Daniel Castro Chin. Styledrop:Text-to-image synthesis of any style. In NeurIPS, 2023. Kihyuk Sohn, Albert Shaw, Yuan Hao, Han Zhang, Luisa Polania, Huiwen Chang, Lu Jiang, and IrfanEssa. Learning disentangled prompts for compositional image synthesis. arXiv preprint arXiv:2306.00763,2023.",
  "AImplementation Detail": "ModelOur DomainGallery takes Stable Diffusion (SD) as its base model. For the sake offairness, DomainGallery and all the baselines share a common base model of SD v1.4,2 though Do-mainGallery is probably applicable to newer versions since the attribute-centric techniques proposedin this work are not based on specific structures of the current version. During the periods of prior attribute erasure and finetuning, we apply LoRA of PEFT3 to theUNet of SD, with rank r = 4, and on parameters of to_k, to_q, to_v, to_out.0, add_k_proj andadd_v_proj by default. We do not finetune the text encoder or apply LoRA to it. TrainingFor prior attribute erasure, we train the model for 500 steps, with batch size 4 and learningrate 1 104. While for finetuning, we initialize LoRA with the parameters where prior attributesof the identifier [V] are erased, and train the model for 1,000 steps, with batch size 4 and learningrate 5 105. During both periods, gradient checkpointing and 8bit Adam4 are also applied to saveVRAM. All the experiments running DomainGallery in this work are done on a single NVIDIA RTX4090 GPU with 24GB VRAM.",
  "For finetuning, we additionally apply offset noise5 on CUFS sketches and watercolor dogs, as theirimages are obviously lighter than average": "InferenceWhen generating images during inference period, we apply DDIM scheduler with 50steps and scale of CFG 1 = 7.5. When generating cross-category images, attribute enhancementis also applied as Sec. 4.4. Note that delicately selecting a scheduler and its parameters may renderbetter images, however it is beyond the scope of this work.",
  "B.1Ablation Study": "In this section, we provide ablation studies regarding the four attribute-centric techniques (priorattribute erasure, attribute disentanglement, attribute regularization, and attribute enhancement)proposed in this work, to prove that these techniques are indeed effective to few-shot domain-drivengeneration. Prior Attribute ErasureIn (top) we ablate the process of prior attribute erasure describedin Sec. 4.1, and generate some cross-category images after finetuning. Compared with the fullDomainGallery in (bottom), military elements can be commonly observed, as these priorattributes of the identifier sks have been kept. Hence, pre-erasing the prior attributes of [V] isnecessary before finetuning. Attribute DisentanglementWe remove the domain-category attribute disentanglement loss Ldisenin Sec. 4.2 during finetuning and generate some cross-category images in (middle). WithoutLdisen enhancing disentanglement, the domain attributes are partially lost when we change thecategory word [N], as some images do not present proper styles. On the other hand, the originalcategory has been leaked into [V], as human faces (or patterns like human faces) appear in theimages sometimes, though we have changed [N]. These phenomena necessitate the enhancement ofdisentanglement in DomainGallery. Attribute RegularizationIn , we illustrate the trend of FID and I-LPIPS scores on CUFSsketches when we use different weights sim between 0.0 and 2.0 for the transfer-based similarityconsistency loss Lsim. As the results shown, the diversity of the generated images is indeed improving : The cross-category samples generated by DomainGallery without prior attribute erasure(top), DomainGallery without attribute disentanglement (middle), and the full DomainGallery (bot-tom) on CUFS sketches. 0.450.460.470.480.490.500.510.52 I-LPIPS @ 1k FID @ 1k 0.0 0.10.2 0.5 1.0 1.5 2.0",
  ": FID and I-LPIPS scores achieved by DomainGallery with different sim (annotated abovethe corresponding data points) ranging from 0.0 to 2.0, on CUFS sketches": "as we increase sim. Besides, since Lsim can prevent the model from learning unnecessary attributesinduced by the bias of the few-shot datasets, it also enhances the fidelity of the generated images.However when the weight exceeds 1.0, the regularization inhibits the model from learning necessarydomain attributes as the fidelity begins to deteriorate. Attribute EnhancementAs the last part of the ablation study we investigate the effects of attributeenhancement in Sec. 4.4 during inference time. First we try to apply our attribute enhancement tothe three baselines (DreamBooth , DreamBooth + LoRA, and DomainStudio ). As the topthree rows of show, enhancing [V] alone cannot improve the fidelity of the images, unless we : The cross-category samples generated by the three baselines with attribute enhancement (topthree rows), by DomainGallery without attribute enhancement (fourth row), and by DomainGallerywith attribute enhancement of either VN-N or V-uncond mode (last two rows) on CUFS sketches.",
  "properly learn the target domain attributes into [V] in the first place, as our DomainGallery does (seethe bottom row of )": "Besides, as we have proposed two modes of attribute enhancement in Sec. 4.4, we would like to makea comparison between them. In the last two rows of we illustrate samples generated followingeither mode. Although both modes can enhance the domain attributes to certain extents compared toDomainGallery without attribute enhancement in the fourth row of , the mode of V-uncondgenerally performs better than its counterpart. Therefore, we utilize V-uncond in DomainGallery bydefault when generating cross-category images.",
  "In this section, we present additional qualitative results that are not illustrated in the main paper dueto page limit": "Intra-categoryBesides the intra-category images on CUFS sketches shown in , we depictthose on the other datasets in . Generally, our DomainGallery surpasses the baselines on allthe datasets. It is also worth mentioning that in few-shot domain-driven generation, the domains arenot limited to certain styles (as in CUFS sketches, Van Gogh houses and watercolor dogs). Instead,our method is also applicable to domains of certain contents (FFHQ sunglasses and wrecked cars). Cross-categoryCross-category images of the other datasets not shown in of the main paperare in . Similar to the results in the main paper, elements of the prior attributes of [V], andattribute leakage between [V] and [N] can also be observed in these images, which necessitates priorattribute erasure and attribute disentanglement proposed in DomainGallery.",
  "CLimitation and Future Work": "In this work, we propose DomainGallery, a new method for few-shot domain-driven image generation.Although the experiments in Sec. 5 and Appendix B have validated the capability of DomainGallery,there are still some limitations w.r.t. the availability of our method which indicate directions for futureworks, as discussed below. Our method may not be able to handle the cases where the datasets consist images ofdifferent categories (e.g. a set of paintings of various objects by a certain artist), sinceDomainGallery follows DreamBooth that finetunes on a single category word [N]. However,with minor modification to the DreamBooth-like finetuning pipeline, DomainGallery maybe capable of such cases by using per-image category words. Although we assume that domains should be defined based on obvious common attributes,sometimes there are composite domains that include several sub-domains (e.g. portraitspainted by several artists of the Renaissance). In such cases the common attributes among allthe images may be subtle and hard to tell. Therefore, for few-shot domain-driven methods(not limited to DomainGallery), domains with clear common attributes are preferred. Currently the performance of DomainGallery on domains of contents (e.g. FFHQ sunglasses)is still in need of further improvement, as we admit that the cross-category images on FFHQsunglasses shown in have undergone some cherry-picking. We suppose that it ismuch more difficult to finetune models on few-shot datasets of certain local contents thanglobal styles (e.g. CUFS sketches), since semantic relations between the contents and thebackgrounds (e.g. where to put sunglasses on faces, or even on faces of animals) can only bewell learned through rather adequate data. Therefore, how to make few-shot domain-drivenmethods master on content domains is another direction for future works.",
  "DBroader Impact": "As a new method for few-shot domain-driven image generation, DomainGallery can be used either increative AI applications, or generating image data as a non-traditional data augmentation for variousdownstream tasks. Furthermore, as image generation is a fundamental task in computer vision, theidea of DomainGallery may also be applied to researches in other topics. However, similar to all the methods of image generation (including but not limited to few-shot domain-driven image generation), our method may induce possible societal harms, including fake imagegeneration for misuse and copyright violation, depending on the specific applications. Therefore, wehereby request proper usage of DomainGallery. : The 10-shot datasets (left) and the intra-category samples generated by the baselines andDomainGallery (right), respectively on FFHQ sunglasses (a [V] face), Van Gogh houses (a [V]house), watercolor dogs (a [V] dog) and wrecked cars (a [V] car) (from top to bottom)."
}