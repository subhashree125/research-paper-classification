{
  "Abstract": "Large Language Models still encounter substantial challenges in reasoning tasks,especially for smaller models, which many users may be restricted to due to re-source constraints (e.g. GPU memory restrictions). Inference-time methods toboost LLM performance, such as prompting methods to invoke certain reasoningpathways in responses, have been shown effective in past works, though theylargely rely on sequential queries. The ensemble method, which consists of multi-ple constituent models running in parallel, is a promising approach to achievingbetter inference-time performance, especially given recent developments that en-abled significant speed-ups in LLM batch inference. In this work, we propose anovel, training-free LLM ensemble framework where a single LLM model is fedan optimized, diverse set of prompts in parallel, effectively producing an ensembleat inference time to achieve performance improvement in reasoning tasks. Weempirically demonstrate that our method leads to significant gains on math reason-ing tasks, e.g., on MATH, where our ensemble consisting of a few small models(e.g., three Qwen2-MATH-1.5B-it models) can outperform a larger model (e.g.,Qwen2-MATH-7B-it).",
  "Introduction": "While Large Language Models (LLMs) have demonstrated impressive capabilities in addressinga variety of tasks, they still encounter substantial challenges in reasoning tasks such as multi-steplogical inference or problem-solving . This is especially so for smaller models, which many usersmay be restricted to due to resource constraints (e.g. GPU memory restrictions), posing limitationson their utility in practice. Inference-time methods to boost LLM performance, especially for smallermodels, hold promise in tackling these challenges . However, many of these methods, such asChain-of-Thought (CoT), Reflexion, and other techniques , have focused on sequential queriesto an LLM to improve performance. In contrast, ensemble methods, which involve the use of multiple constituent models in parallel,have been shown to improve models performance and robustness in classical machine-learningsettings and are promising approaches to achieve better inference-time performance, although lesswell-studied in the LLM setting. The prospects of applying such methods to LLMs are increasinglyattractive, given recent developments that have enabled significant speed-ups in parallel, LLM batchinference. These include methods to efficiently handle key-value cache memory and promptcaching to efficiently reuse common prompts for multiple queries , enabling sub-linear (in thenumber of queries) costs for batch inference.",
  "arXiv:2412.15238v1 [cs.CL] 12 Dec 2024": "However, a key challenge in achieving high performing ensembles is how diversity can be appropri-ately injected among its constituents , and this applies to LLM ensembles as well. Recentworks have explored how using hetereogenous model ensembles (i.e. consisting of different modelstypes) could lead to improved performance , although users may often prefer to or be restrictedto using only a single type of LLM model in practice, making such methods not viable in thosecases. While a single LLM may be sampled with the same query multiple times and rely on thestochasticity of the LLM response generation process to essentially form a self-ensemble, thisapproach injects limited diversity to the ensemble which may limit performance improvements. Instead, significantly more diversity could potentially be injected into an LLM ensemble by makinguse of LLMs ability to produce diverse output for a given task with just different prompts. Forexample, adding simple prompt instructions on how the LLM should reason has been shown toresult in performance boosts. This leads to the interesting question of how we could design ensemblemethods that rely on just prompt diversity to produce significant performance boost for a given LLM.Such ensemble methods could be applied during inference to improve the performance of any LLM(e.g. assessed via APIs), together with other types of inference-time methods. In this work, we propose DIPPER, a novel, training-free LLM ensemble framework where a singleLLM model type is fed an optimized, diverse set of reasoning prompts in parallel, effectivelyproducing an ensemble at inference time to improve performance in reasoning tasks. This approachis simple but surprisingly effective and efficient, and could be implemented with any black-box LLM.",
  "Problem setting and related work": "LLMs and prompts. Consider an LLM model M which for our purposes can be viewed as a blackbox that encodes a conditional probability distribution of text responses y over any text input q andadditional prompt w, from which we can autoregressively sample responses y from, i.e.y M(q, w) = pM(y|q, w).(1)Examples of prompt w could include reasoning prompts such as \"Lets think step by step\" in CoT that provide instructions on how the LLMs should derive answers for the query q. LLM ensembles. Ensemble methods involve combining several models to produce a ensemble withbetter performance and lower variance. However, while commonly applied for a wide variety ofmachine learning models , ensemble methods for LLMs have remained relatively unexplored. Pastworks have focused on heterogeneous ensembles involving multiple types of models (e.g. differentLLM API providers) , multi-agent LLM settings that focuses on interactions among agents, or homogeneous ensembles that rely only on stochastic sampling of model responses . However, to the best of our knowledge, we are not aware of any work that focused on designing andanalyzing homogeneous LLM ensembles where their diversity is injected and optimized via promptsto constituents with the same underlying LLM model. Our works focus on such an approach exploitsLLMs unique capabilities of generating diverse output given only changes to its prompts, allowingfor a simple but effective method to boost LLM performance using inference-time compute. Problem formulation. Consider a task T that consists of instances described as tuples t := (qt, ct ),where qt can be represented as a text string query and ct is the corresponding ground truth solution.We have access to a single LLM model M that when provided task queries and a prompt w, willprovide a response y according to Eq. (1). This response will consist of (1) some reasoning outputr, and (2) the final answer c to the query, which we can denote as y := {r, c}. We evaluate theperformance of the model with a specific prompt, denoted as M(, w), on the task by computing itsexpected accuracy over the set of task instances T , i.e., F(M(, w); T ) := EtT [I{ct = ct }], whichin practice is computed over a representative test set. We denote a homogeneous LLM ensemble as E( ; M, n, ), consisting of n instances of the samemodel M and in general has an adjustable inference-time design parameter . The ensembleproduces a final answer when provided a task query, i.e., E(qt; M, n, ) ct, and we can evaluateits performance based on its expected accuracy:F(E, T ) = EtT [I{E(qt; M, n, ) = ct }].(2)Our objective is to design an ensemble framework with an appropriate design parameter suchthat given fixed M, n and a small labeled development set, we can efficiently maximize Eq. (2) byoptimizing for to produce the best performing ensemble without additional training.",
  "Method": "Drawing inspiration from how using different prompts w would result in varying response distributionsin Eq. (1) given the same model M, our DIPPER framework has the set of prompts {wi}ni=1 fed intothe ensemble of n LLM instances as the key ensemble design parameter . DIPPER consists of thefollowing three components: 1. Prompt Generator. First, an LLM generates a large candidate pool of prompts (denoted asW), which can be based on some description of the task and in-context prompt examplesthat we think may be effective, if such prior knowledge is available. The goal is for theprompts to invoke various types of reasoning pathways when addressing queries, henceinjecting diversity into the ensemble. Additional details are in Appendix A.1. 2. Prompt Selector. Then, an optimization process is performed over the candidate pool ofprompts W to select a subset of n prompts (i.e., {wi W}ni=1), based on a diversity metricthat acts as an approximation of the relative performance of each subset (.1).",
  "argmax{wiW}ni=1 F(E(qt; M, n, {wi}ni=1), T ).(3)": "Unfortunately, directly optimizing Eq. (2) is a combinatorial problem that is very challenging, even ifa development/validation set is available for the task of interest. For example, selecting 5 promptsfrom a candidate pool of 200 prompts involves searching over2005 2.5 109 candidates. Instead,we note that the best ensemble composition requires a balance of the two desiderata: fidelity anddiversity. Hence, we propose optimizing Eq. (2) by considering how to prioritize the prompts thathave the best predicted performance on the task T , while maximizing the diversity of the selected setof prompts. Prompt fidelity. First, we can approximate the predicted performance of each prompt by its averageperformance on a task development set Td2. Note that as inference using these various prompts ona small development set can be done in parallel, this process can in practice be significantly spedup by existing batch inference techniques such as those employed by vLLM . Specifically, fora candidate pool of prompts W and development set Td, we can define a prompt fidelity mappingu : W ,u(w) := F(M(, w), Td),(4)where M(, w) is the LLM model conditioned by prompt w W, and F the expected accuracydefined in . In practice, for a candidate pool of size n, u(w) can be represented as an n 1column vector, with the elements representing each prompts expected accuracy. Semantic entropy. Instead, our approach involves prioritizing the prompts that have the best predictedperformance on the task T , while maximizing the diversity of the selected set of prompts. Then, wemeasure prompt diversity by considering how different the semantic meanings of the n role promptsare from each other. We represent each prompts semantic meaning with a mapping R from its textrepresentation w into a normalized continuous vector s Rp in a p-dimensional semantic embeddingspace S through a sentence embedding model Ms , i.e., R(w) := Ms(w). This mapping can berepresented as an n p prompt embedding matrix R = [s1, , sn] where s is a 1 p row vectorrepresenting each prompt.",
  "Without such a development set, an uninformed prior on the performance (e.g. uniform distribution acrossroles), or an informed-prior based on domain knowledge, could also be used": "To quantify prompt diversity of a given set of prompts, we propose to compute the volume enclosedby the selected prompts in semantic space. Intuitively, for n fixed prompts, more diverse promptspoint to more varied directions in semantic space, and enclose larger volume. Specifically, we definethe semantic volume metric V asV := log det(RRT ),(5)",
  "diag(u))R,(6)": "where diag(u) is the diagonal matrix with its ith diagonal element being the corresponding elementui. This essentially scales each row si in R by an exponential factor based on its correspondingpredicted accuracy, exp( 2 ui), where is a scalar hyperparameter influencing the balance betweendiversity and expected performance. Intuitively, prompts with higher expected accuracy would thenbe able to support larger semantic volume and hence be prioritized for inclusion into the ensemble.The adjusted embedding matrix can then be used to compute the semantic volume in Eq. (5). Optimization of semantic entropy. We can now recast Eq. (2) as an optimization of the fidelity-adjusted semantic volume metric V evaluated over the set of candidate prompts. Note that insteadof the expected ensemble performance F(E), which is an objective that can only be optimized byblackbox optimization methods like Bayesian Optimization , our metric V can be approximatedby efficient, well-established heuristics. Specifically, as the semantic volume metric is submodular, we can optimize for the best subset ofroles by incrementally building the subset with a greedy approach up to the desired size n and still beguaranteed a good approximation . This allows us an efficient and theoretically-inspired approachto obtain the best ensemble prompts. Our full algorithm is outlined in Algorithm 1 in the Appendix.",
  "Given the various constituent LLMs responses, the aggregation method determines how muchinformation is used to derive the final ensemble output. We consider two approaches:": "Majority voting (MV). The first involves extracting the final answer c from each LLM responsey = {r, c}, and then selecting the answer that has been proposed the most number of times. Thisapproach does not take into account the reasoning r output produced by the ensemble constituents,but is easily implementable. LLM aggregation (LLMA). The second involves using another LLM instance to evaluate eachconstituent response, aggregate them, and generate a final answer for the task. This approach incursadditional LLM query cost and is dependent on the capabilities of the aggregator LLM, but hasthe advantage of potentially taking into account the various reasoning output r from the ensembleconstituents to further improve overall performance (see .3 for details).",
  "Experiments": "Experimental set-up. We empirically evaluate our framework on mathematically reasoning taskswith the MATH , GSM8K, and MMLU-STEM datasets. We implement our framework byusing the GPT-4o as our prompt generator and Qwen2-MATH-1.5B as the constituent model in theensemble, where the ensemble constituents are run in parallel using vLLM for fast batch inference.Further details of our experiments are in Appx. B. Baselines. We evaluate our DIPPER framework by comparing it against the \"Self-ensemble\" baseline,which lacks prompt diversity but incorporates diversity through repeated response sampling .We also compare our DIPPER implementation based on semantic volume (\"Dipper\") with two othervariants: (1) a nave implementation where prompts are sampled from the candidate pool based on",
  ": Comparison of dif-ferent ensemble methods onMATH": "First, we illustrate the effectiveness of prompt-based ensemble methods by considering a fixed list of7 reasoning prompts inspired by existing works on prompting methods to boost reasoningcapabilities (details in Appx. B.1). Under a fixed ensemble size of 7, shows that the ensembleusing the 7 different prompts (57.31%) significantly outperforms the self-ensemble with no prompt((55.76%)) and the average performance (56.55%) of self-ensemble using any single prompt. To further investigate the impact of prompt diversity, we evaluated all combinations of the 7 promptswhile maintaining a fixed ensemble size of 7. For combinations with fewer than 7 prompts, werandomly sampled responses to reach a total of 7 before applying majority voting. The results in reveals that increasing the number of prompts in the ensemble generally leads to higher accuracy,reduced variance, and fewer unique answers. The 7-prompt ensemble has the highest accuracy andlowest variance, which suggests that employing a diverse set of prompts in an ensemble can enhanceperformance and consistency, especially when we do not know which prompt would perform bestbefore evaluation.",
  "Ensembles with optimized prompt diversity": "Next, we consider our full DIPPER framework. We first generate a pool of prompt candidates(|W| = 200) using the 7 reasoning prompts in the previous section as in-context exemplars (detailsin Appx. B.1) and then perform diversity optimization (Sec. 3.1) to select the best ensemble prompts.Evaluation details are in Appx. B.2. As shown in , our method achieves the highest accuracycompared to all baseline ensemble methods across various ensemble sizes. DIPPER also significantlyoutperforms the single LLM. For example, DIPPER with n = 9 has close to a 10%-pt increase (~20%accuracy gain) compared to the single LLM baseline. In fact, our ensemble that consists of just 3Qwen2-MATH-1.5B model already slightly outperform the next model size class, the Qwen2-MATH-7B model. See more results on GSM8K and MMLU-STEM in Appx. C.2 where DIPPER is shown tobe consistently effective.",
  "LLM aggregation can do better": "Finally, we analyze the effects of using Majority voting (MV) or LLM aggregation (LLMA) for ourresponse aggregator component (see experimental details in Appx. B.3). We consider ensemblesof size n = 5 with randomly selected prompts, and compare their performance on MATH whenusing either majority voting or LLM aggregation. summarizes the results, showing thatLLMA is more accurate than MV on average (i.e., higher F(E)). To better analyze the performancedifference, we computed the Override Ratio which is how often a specific method is correct whenthe two methods disagree. Note that when MV and LLMA disagree, LLMA has a much higher ratiothan MV which is only correct 8% of the time. We attribute LLMAs advantage to its capability ofunderstanding the reasoning r in responses even when the ensembles do not have a majority for the final answers c. This is corroborated when we look at the number of unique answers |C| when onlyone specific method is accurate: |C| for LLMA is higher than that of MV, which suggests that LLMAperforms better than MV when the ensemble produces more unique answers, as expected.",
  "DIPPER combined with other prompting methods like Reflexion": "In addition, we also show that our ensemble framework DIPPER is orthogonal to other establishedprompting techniques (e.g. CoT and Reflexion ), allowing it to stack and bring greater performance.In our experiments, we first use DIPPER to select n agents and query each agent with the questions.Their initial responses will be self-reflected according to the method proposed in Reflexion , beforebeing aggregated into the final answer with MV. The results in shows that DIPPER coupledwith reflection achieves much better results, suggesting that DIPPER has the potential to be extendedfurther or combined with other methods.",
  "Conclusion": "In this work, we have proposed a novel framework, DIPPER, where a single LLM model type isfed an optimized, diverse set of reasoning prompts in parallel, effectively producing an ensembleat inference time to achieve performance improvement in reasoning tasks. Our empirical findingshave demonstrated DIPPERs effectiveness in improving inference performance for a variety ofreasoning tasks, which may inspire future works to investigate additional optimization methods forprompt-based inference-time ensembles to further improve performance gains. This research/project is supported by the National Research Foundation, Singapore under its AISingapore Programme (AISG Award No: AISG2-PhD/2023-01-039J). This research is part of theprogramme DesCartes and is supported by the National Research Foundation, Prime MinistersOffice, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE)programme. This research is supported by the National Research Foundation Singapore and theSingapore Ministry of Digital Development and Innovation, National AI Group under the AI VisitingProfessorship Programme (award number AIVP-2024-001).",
  "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and KarthikNarasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models,December 2023": "Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.Reflexion: Language agents with verbal reinforcement learning. Advances in Neural InformationProcessing Systems, 36, 2024. M. A. Ganaie, Minghui Hu, A. K. Malik, M. Tanveer, and P. N. Suganthan. Ensemble deeplearning: A review. Engineering Applications of Artificial Intelligence, 115:105151, October2022. ISSN 0952-1976. doi: 10.1016/j.engappai.2022.105151. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large lan-guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposiumon Operating Systems Principles, 2023.",
  "Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic LLM-Agent Network:An LLM-agent Collaboration Framework with Agent Team Optimization. October 2023": "Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, HeyangYu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu,Maosong Sun, and Jie Zhou. AgentVerse: Facilitating Multi-Agent Collaboration and ExploringEmergent Behaviors, October 2023. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing. Association for Computational Linguistics, 11 2019.",
  ". Diversity. The prompts should be sufficiently different from one another such that theyelicit various reasoning pathways and provide a diverse pool to select from in the subsequentcomponent": "We first show that LLMs are capable of generating prompts that meet this desideratum, via the mostdirect way of prompting it to generate a pool of candidate prompts while providing it with exemplarsillustrating different reasoning prompts. To do so, we considered a list of 7 reasoning promptsinspired by existing works on prompting methods to boost reasoning capabilities. Giventhese prompts as exemplars, we used GPT-4o to further generate a set of 200 different candidateprompts that each represent a different reasoning approach (details in Appx. B.1). shows thedistribution of average accuracy over a sampled test set of MATH questions for each prompt,when used for the Qwen2-MATH-1.5B model (i.e., F(M(, w); T ) for wi W). Note that thedistribution of accuracy is largely higher than that of the base model without prompts, and similar tothe accuracies achieved by the reasoning prompt exemplars, demonstrating the fidelity requirement.Qualitatively, we see that the prompts are also relatively diverse they generally specify certainreasoning approaches inspired by various subject domains (see Appendix A.1). We quantify thisdiversity in Sec. 3.1 with our proposed metric. Note that when generating the prompts, we did not pass any task description to the LLM promptgenerator. We did so as the reasoning prompts can in general be task-agnostic, even if some maybe inspired by some specific subject matter. In practice, the candidate pool of reasoning promptsneed not be generated on-the-fly, but be drawn from a shared pool prepared beforehand by a morepowerful LLM, to be used by ensembles consisting of much smaller LLMs, as we demonstrated. Theactual selection of relevant prompts can be done by the prompt selector component, which we willdescribed next in Sec. 3.1.",
  ": Examples of reasoning prompts generated based on 7 basic prompts": "Prompt**Break Down the Problem**: Divide the question into smaller, manageable parts and tackleeach part individually before synthesizing the overall answer.**Apply Mathematical Logic**: Use mathematical principles and logic to solve the problem,even if its not a math question.**Use Analogies**: Relate the question to a familiar concept or situation to better understandand solve it.**Consider the Opposite**: Think about what the answer would be if the opposite were true, togain a different perspective.**Consider Cause and Effect**: Identify potential causes and their effects to understand thequestion better.**Think Like a Lawyer**: Build a case for your answer using evidence and logical arguments.",
  ": The table of 7 basic reasoning prompts inspired by existing works": "PromptLets think step-by-step to find the answer.Reflect on the question carefully before answering.Rephrase the question in your own words before responding.Actively reason through the question and answer each part systematically.Answer this question as a scientist would.Eliminate the obviously incorrect answers first and then choose the most likely correct answer.Analyze the context of the question and use relevant information to derive the answer.",
  "B.2Evaluation": "We primarily consider three datasets in our paper. For MATH, we randomly 10% test samples fromeach category and form a fixed subset of size 500. We uniformly randomly sample 20 samples fromthis subset to form a validation dataset and use the rest 480 samples as the hold-out test dataset. ForGSM8K and MMLU-STEAM, we use their official split of test data and uniformly randomly sample20 samples to form a validation dataset for each task, and use the rest samples as the hold-out testdata. In the inference evaluation, we use 4-shot exemplars for MATH, 8-shot for GSM8K, and 5-shot forMMLU-STEM. Those exemplars are adopted from the evaluation setting in Qwen2-MATH andfixed for all questions and all methods.",
  "C.1Performance-adjusted embedding": "To study the effect of accuracy on the performance-adjusted prompt embedding matrix, we report theSpearman correlation between logdet V and the ensemble performance F(E) under different choicesof . We observe that when = 0, the correlation is 0.18, and it increases as becomes larger. Thepositive correlation justifies our approach to maximize prompt diversity. The increasing correlationjustifies our approach of incorporating validation accuracy into the prompt semantic embedding.",
  "C.2Results on More Datasets": "We also evaluate the performance of DIPPER on GSM8K and MMLU-STEM. The results in and demonstrate that our proposed method DIPPER can consistently outperform the self-ensemblebaseline, further demonstrating the benefits of our method. Our proposed DIPPER implementationwith semantic volume optimization also consistently produces better or comparable results comparedto our other variants (Random+ or Top-n) which have more unstable results, showing the usefulnessof prompt diversity optimization in improving inference performance."
}