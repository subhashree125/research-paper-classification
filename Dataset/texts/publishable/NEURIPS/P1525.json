{
  "Abstract": "The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from differentsources. Most existing techniques fail to perform dynamic image fusion whilenotably lacking theoretical guarantees, leading to potential deployment risks inthis field. Is it possible to conduct dynamic image fusion with a clear theoreticaljustification? In this paper, we give our solution from a generalization perspec-tive. We proceed to reveal the generalized form of image fusion and derive a newtest-time dynamic image fusion paradigm. It provably reduces the upper bound ofgeneralization error. Specifically, we decompose the fused image into multiple com-ponents corresponding to its source data. The decomposed components representthe effective information from the source data, thus the gap between them reflectsthe Relative Dominability (RD) of the uni-source data in constructing the fusionimage. Theoretically, we prove that the key to reducing generalization error hingeson the negative correlation between the RD-based fusion weight and the uni-sourcereconstruction loss. Intuitively, RD dynamically highlights the dominant regionsof each source and can be naturally converted to the corresponding fusion weight,achieving robust results. Extensive experiments and discussions with in-depthanalysis on multiple benchmarks confirm our findings and superiority. Our code isavailable at",
  "Introduction": "Image fusion jointly integrates complementary information from multiple sources, aiming to generateinformative and high-quality fused images. With superior scene representation and enhanced visualperception, image fusion significantly benefits downstream vision tasks . Typically, imagefusion can be categorized into multi-modal, multi-exposure, and multi-focus image fusion tasks.Multi-modal image fusion encompasses Visible-Infrared image Fusion (VIF) and Medical ImageFusion (MIF). For VIF , infrared images effectively highlight thermal targets especially underextreme conditions, while visible images provide texture details and ambient lighting. For MIF ,different medical imaging modalities emphasize various focal areas, enhancing diagnostic capabilities.Multi-exposure image Fusion (MEF) bridges the gap between high dynamic range (HDR)natural scenes and low dynamic range (LDR) pictures, ensuring better detail preservation in varyinglighting conditions. Multi-Focus image Fusion (MFF) aims to produce all-in-focus imagesby combining multiple images focused at different depths. Numerous image fusion methods have been introduced, which can be mainly grouped into traditionaltechniques and deep learning approaches. Traditional image fusion methods, such as multi-scaledecomposition-based models and sparse representation-based methods , rely on mathe-matical transformations to fuse images in the transform domain . In contrast, deep learning-based",
  ": We visualized the Relative Dominablity (RD) of each source on four tasks, which effectivelyhighlights the dominance of uni-source in image fusion": "methods employ data-driven schemes to fuse multi-source images, including convolutional neuralnetwork (CNN) based methods , generative adversarial network (GAN) based methods ,and transformer-based methods . The effectiveness of image fusion algorithms hinges on twocritical factors: feature extraction and feature fusion . The aforementioned methods strive toachieve high-quality fused images by learning effective uni-source or multi-source feature representa-tions through complex network structures or feature decomposition schemes. However, they oftenoverlook the complexity of the real world, which necessitates dynamic feature fusion. Recently, some works have highlighted the importance of dynamism in image fusion. For instance, pioneered the combination of image fusion with a Mixture of Experts (MoE), dynamicallyextracting effective and comprehensive information from the respective modalities. utilizedtask-specific routing networks to extract task-specific information from different sources with dy-namic adapters. Despite their empirically superior fusion performance, these dynamic fusion rulesmainly rely on heuristic designs, lacking theoretical guarantees and interpretability. Moreover, theypotentially lead to unstable and unreliable fusion results, especially in complex scenarios. To address these issues, we reveal the generalized form of image fusion and propose a new Test-TimeDynamic(TTD) image fusion paradigm with a theoretical guarantee. Given that the fused imageintegrates comprehensive information from different sources, it can be obtained by weighting theeffective representation of each uni-source. By revisiting the relationship between fusion weightsand image fusion losses from the perspective of generalization error , we decompose the fusedimage into multiple uni-source components and formulate the generalization error upper bound ofimage fusion. Based on generalization theory, we for the first time prove that dynamic image fusionis superior to static image fusion. The key to enhancing generalization lies in the negative correlationbetween fusion weight and uni-source component reconstruction loss. As fusion models are trainedto extract complementary information from each source, the decomposed components representthe effective information from the source data. Thus, the fusion components can be estimated bysource data with the fusion model, the losses of which represent the deficiencies of the source inconstructing fusion images. Accordingly, we derive a pixel-level Relative Dominablity (RD) asthe dynamic fusion weight, which theoretically enhances the generalization of the image fusionmodel and dynamically highlights the changing dominant regions of different sources as shownin . Extensive experiments on multiple datasets and diverse image fusion tasks demonstrate oursuperiority. Overall, our contributions can be summarized as follows: This paper first theoretically proves the superiority of dynamic image fusion over static imagefusion and provides the generalization error upper bound of image fusion by decomposingthe fusion image into uni-source components provably. The proposed generalization theoryreveals that the key to reducing the upper bound lies in the negative covariance between thefusion weight and uni-source reconstruction loss. We proposed a simple but effective test-time dynamic fusion paradigm based on the gener-alization theory. By taking the uni-sources Relative Dominability as the dynamic fusionweight, we theoretically enhance the generalization of the image fusion model and dynami-cally emphasize the dominant regions of each source. Notably, our method does not requireadditional training, fine-tuning, and extra parameters. We conduct extensive experiments on multi-modal, multi-exposure, and multi-focus datasets.The superior performance across diverse metrics demonstrates the effectiveness and applica-bility of our approach. Moreover, an additional exploration of the gradient in constructingfusion weight demonstrates the reasonability of our theory and its expandability.",
  "Related Works": "Image Fusion aims to integrate complementary information of diverse source images. For instance, utilize autoencoders to extract multi-source features and fuse them using a designed strategy.GAN-based methods and transformer-based methods also achieved significant progress. introduced the denoising diffusion probabilistic model (DDPM) to image fusion. and achieve considerable fusion performance by decomposing image features into high-frequency andlow-frequency components. In addition to these static image fusion methods, used a Mixture ofExperts (MoE) to dynamically assign fusion weights, while utilized dynamic adapters to promptvarious fusion tasks within a unified model. These approaches mainly focus on obtaining promisingfeature representations. Although some existing works have explored dynamic image fusion, the lackof theoretical guarantees may result in instability and unreliability in practice. Multimodal Dynamic Learning Although dynamic fusion is not fully studied in existing imagefusion works, numerous methods have leveraged multimodal dynamic learning at the decisionlevel . For example, Han et al. assigned dynamic credible fusion weights to each modalityat the decision level for robust evidence fusion. Xue et al. employs a Mixture of Experts tointegrate the decisions of multiple experts, Zhang et al. combined decision level fusion weightwith uncertainty to conduct a credible fusion. Despite the wide exploration of dynamic fusion at thedecision level, there is still insufficient research on dynamic fusion at the feature level with theoreticalguarantees. In this paper, we focus on the dynamic nature of image fusion, theoretically prove thatdynamic fusion is superior to static fusion, and propose a provable feature-level dynamic fusionstrategy.",
  "Method": "Given the data from M sources for image fusion, the input samples are denoted as {x = x(m) |m = 1, 2, . . . , M}, where x(m) represents the input from the m-th source. Let f be the image fusionmodel, comprising both encoders and decoders. Define E = {E(m)() | m = 1, 2, . . . , M} as theset of encoders within the image fusion network, where E(m)() is the encoder for the m-th source.In early fusion, the encoders in E are constant mapping functions, meaning that multi-source imagesare combined at the image level. Let D() denote the decoder in the image fusion network, and let = {(m) RHW | m = 1, 2, . . . , M} represent the set of image fusion weights. Consequently,the fused image IF can be expressed as:",
  "m=1IF x(m).(2)": "Generalization Error Upper Bound. In machine learning, the concept of Generalization ErrorUpper Bound refers to the theoretical limit on a models performance when applied to unseen data(D) . A smaller upper bound indicates better-expected performance on data from an unknowndistribution. For image fusion tasks, the Generalization Error (GError) of a fusion model f can bedefined as:GError(f) = ExD[(f(x), x)].(3)",
  "Decompose": ": The framework of our TTD. Deriving from the generalization theory, we decompose fusedimages into uni-source components and find the key to reducing generalization error upper bound isthe negative correlation between the fusion weight and reconstruction loss. Accordingly, we proposepixel-wise Relative Dominablity (RD) for each source, which is negatively correlation with thereconstruction loss and highlights the dominant regions of uni-source in constructing fusion images. Considering the GError of image fusion model , (IF , x) can be further deduced as (IF , x) Mm=1 Mi=1 (i) D(E(i)(x(i))) x(m). Therefore, the fused image IF is decomposed into Muni-source components {DE(i) x(i)|i = 1..M}. Based on Eq. (3) and (8), we have: Theorem 3.1 (Decomposition of Generalization Error). The GError for multi-source image fusionmodel f can be decomposed into a linear combination of each uni-source component reconstructionloss under the condition that Mm=1 (m) = 1, the detailed proof is given in Appendix A.1:",
  "x(m).(4)": "Let (m) = DE(m) x(m) x(m), which represents the reconstruction loss between a uni-source component and its corresponding uni-source image. The term Cov((m), (m)) denotes thecovariance between (m) and (m). The essence of reducing generalization error lies in achievingthe lowest possible fusion loss. By leveraging the triangular inequality properties of distance normswithin the fusion loss, we can deduce that the GError is bounded by the covariance term and thedistance between each uni-source component and its source image. It is noteworthy that f(x(m))remains constant during the test phase, emphasizing that the pivotal factor in reducing GeneralizationError Upper Bound (GEB) lies in the covariance between (m) and (m). Superiority of Dynamic Image Fusion over Static Image Fusion. Most existing image fusionapproaches reduce GEB by minimizing (m), indicating an effort to enhance the quality of uni-sourcefeature representations. However, they often overlook the intrinsic significance of fusion weight(m). Fusion strategies employed in static image fusion encompass methods such as maximum,minimum, addition, 1-norm, etc. Nevertheless, none of these fusion weights exhibit a correlationwith uni-source reconstruction loss, i.e. Cov((m)static, (m)) = 0. During the test phase, (m) remainsconstant. If we have: Cov((m)dynamic, (m)) < 0 for all source images, we can derive the conclusion:",
  "This indicates that for a well-trained image fusion model, a dynamic fusion strategy can bring bettergeneralization than a static fusion strategy": "Relative Dominablity. Recalling the Eq. (4), the negative correlation between fusion weight andthe reconstruction loss (m) provably reduces the generalization error upper bound. Therefore, weintroduce a pixel-level Relative Dominablity (RD) as the fusion weight for each source, whichexhibits a negative correlation with the reconstruction loss of the corresponding fusion component.Since fusion models are trained to extract complementary information from each source, the de-composed components of fusion images represent the effective information from the source data.Thus, the uni-source components can be estimated from source data using the fusion model, withthe losses representing the deficiencies of the source in constructing fusion images. For instance, ina given region, the larger the pixel-wise fusion loss between the reconstructed component and itscorresponding uni-source image, the smaller its contribution to image fusion. Intuitively, using RD asthe dynamic fusion weight can capture the dominance of each source in image fusion and enhance itsadvantages in constructing fusion images. Theoretically, according to Thm. 3.1, negatively correlatedwith the pixel-wise fusion loss, RD effectively demonstrates the dominance of each source. Notably,considering the relative nature of multi-source image fusion, the sum of the RDs of different sourcesfor the same pixel should be one. Consequently, by establishing a negative correlation with the lossand implementing normalization, we can obtain the Relative Dominablity of each source for a certainsample as follows:(m) = RD(m) = Softmax(e(m)).(6)",
  "Experimental Setup": "Datasets. We evaluate our proposed method on four image fusion tasks: Visible-Infrared Fusion(VIF), Medical Image Fusion (MIF), Multi-Exposure Fusion (MEF), and Multi-Focus Fusion (MFF). VIF: For VIF tasks, we conduct experiments on two datasets: LLVIP and MSRS . ForLLVIP datasets, we randomly select 70 samples from the test set for evaluation. MIF: We conductexperiments on the Harvard Medical Image Dataset, following the test setting in . MEF:Following the setting in , we verified the performance of our method on MEFB dataset. MFF: For the MFF task, we evaluate our method on MFI-WHU datasets , following the testprotocol in . As a test-time adaption approach, TTD performs adaptive fusion solely duringtesting, without additional training and training data. Competing Methods. For VIF and MIF tasks, we evaluated 12 state-of-the-art methods, encompass-ing both DenseFuse , CDDFuse , U2Fusion , DDFM , DeFusion , PIAFusion, DIVFusion , MUFusion , IFCNN , and SwinFuse , and TC-MoA . ForMEF and MFF tasks, we compared our methods with general image fusion methods and task-specificimage fusion methods. Notably, among these methods, only DDFM is training-free, and othermethods are all pre-trained on VIF datasets. In experiments, we apply our TTD to CDDFuse (CDDFu-sion+TTD), PIAFusion (PIAFusion+TTD), and IFCNN (IFCNN+TTD), separately. Our experimentsare conducted on Huawei Atlas 800 Training Server with CANN and NVIDIA RTX A6000 GPU. Metrics. We selected several evaluation metrics from three aspects , including informationtheory: entropy (EN) , cross entropy (CE), the sum of the correlations of differences (SCD) , image feature: standard deviation (SD), average gradient (AG) , edge intensity (EI) and spatialfrequency (SF) , and structural similarity: structural similarity (SSIM) .",
  "Quantitative Comparisons": "Visible-Infrared Fusion.Tab. 1 reports the performance of competing approaches and TTD-applied methods on LLVIP and MSRS datasets for 7 metrics. Notably, by applying our TTD, theprevious methods have improved on most of the indicators. Also, our TTD strategy outperforms othertraditional static methods, training-free method DDFM, and data-driven dynamic strategy TC-MoA,achieving the SoTA performance on most metrics. Moreover, with particularly high values in SD, AG,EI, and SF, our TTD ensures that fusion images maintain exceptional contrast and detailed texture,highlighting its efficacy in preserving quality. The outstanding performance on EN and SCD indicates : Quantitative performance comparison of different fusion strategies on visible-infrareddatasets. The TTD suffix and gray background indicates our method is applied to this baseline. Thered and blue represent the best and second-best result respectively. The bold indicates the baselinew/ TTD performance better than that w/o TTD. We used to illustrate the amount of improvementour TTD method achieved compared to the baseline.",
  "IFCNN+TTD52.86 15.947.106.3821.99 11.41 IFCNN+TTD54.2219.107.397.7023.517.40Improve1.08 1.46 0.04 0.53 1.58 0.19 Improved0.20 0.39 0.01 0.15 0.66 0.0": "our fusion results embed more information and contain abundant edge information from the sourceimages. Although our approach is a test time adaptation strategy that does not require training, thedesigned dynamic weights based on theoretical principles have led to outstanding fusion performance,achieving SoTA results on VIF tasks. Medical Image Fusion. We report the comparison results on three MIF scenarios: MRI-CT, MRI-PET, and MRI-SPECT. As depicted in Tab. 3, 6 and Tab. 7 in Appendix C.3, our method yieldscompetitive performance on seven evaluation metrics. Specifically, our TTD enhances EN, AG,and SSIM, indicating ample gradient information and structural details in the fusion results. Thesignificant improvements in SD, EI, and SF highlight our high definition and texture quality comparedwith the competing methods, making it exceptionally competitive. Multi-Exposure and Multi-Focus Image Fusion. In the comparisons on MEF and MFF tasks, weapplied our TTD to the general fusion method IFCNN. We compared it with other general fusionmethods and task-specific fusion methods. As depicted in Tab. 2, TTD outperforms existing generalfusion methods and task-specific methods in terms of SD, EI, AG, and SF. This indicates that TTDproduces fused images with clear, abundant edges and exceptional sharpness. Furthermore, thesuperiority in EN and CE suggests that our TTD enables the baseline to preserve more advantagesfrom different sources.",
  "Qualitative Comparisons": "Visible-Infrared Fusion. As shown in and in Appendix C.1, compared with existingmethods on the LLVIP dataset, our TTD effectively combines comprehensive information fromdifferent sources, leading to a significant visual performance. Specifically, the fusion result not onlypreserves the texture details and edge information of visible images but also incorporates high-qualitythermal imaging contrast of infrared images. Additionally, as mentioned in the qualitative analysis,our fusion images exhibit high fidelity and clear contrasts, showing consistent superiority in terms ofimage quality. These experimental results demonstrate the effectiveness of our TTD. Medical Image Fusion. For the MIF task, we present qualitative comparisons of the MRI-PETfusion. As shown in and in Appendix C.1, it is clear that fusion images generated byour method preserve a substantial amount of structural information. Notably, our method maintainsa significant portion of excellent soft tissue contrast details from MRI images and combines thequantitative physiologic information of PET images. With our TTD, the overall structural detailsand sharpness of the fused image are significantly enhanced. Moreover, in the regions where thehigh-intensity color areas of the PET image overlap with the structural information of the MRI, thedetailed information from the original images is well preserved and highlighted in the fused image. Multi-Exposure and Multi-Focus Image Fusion. We also provide a comparison of fusion resultsfor the MEF and MFF tasks in and in Appendix C.1. Notably, our TTD significantlyenhances the clarity and sharpness of texture details. Obvously, after applying our TTD, the fusedimages on the MEF task exhibit higher clarity in both the foreground and background. For the MFFtask, our method accurately utilizes the effective regions from both underexposed and overexposedimages. Compared to other methods, our fusion results achieve more precise exposure and richtexture details, such as the cars in the garage and the textures on the walls. Additionally, our fusionmethod retains high-fidelity colors that are closer to the original images.",
  "Is Negative Correlation Help?": "The negative correlation between RD and (m) is derived from Eq. (4) to reduce the generalizationerror of the image fusion model. To further validate the effectiveness of the theoretical guarantee, wecompare it with a contrast setting: using a new fusion weight, which is positively correlated (PC) to(m), to perform image fusion. As shown in the correlation comparison in (b), loss-positive-correlated weights (yellow line),that conflict with our theory, lead to a decreased performance compared with static fusion (greenline). As a comparison, the results of the loss-negative-correlated fusion strategy (red line), exhibitsuperior performance compared with both static image fusion and positively correlated fusion strategy.These experiments verify that the proposed negative correlation setting can explicitly reduce thegeneralization error, demonstrating the reasonability of the proposed TTD image fusion model.",
  "Relative Dominability": "In this paper, we introduce the pixel-level Relative Dominablity, which indicates the advantages ofeach source. Treating the Relative Dominablity as dynamic fusion weight, TTD achieves an adaptiveand interpretable image fusion. We provide visualizations of each sources pixel-level RelativeDominability obtained using CDD+TTD for VIF and MIF tasks, and IFCNN+TTD for MEF andMFF tasks. Visible-Infrared Fusion. As shown in , it can be observed that Relative Dominablity (RD)accurately reflects the dominance of each source: in visible images, well-lit and properly exposedbright areas contain abundant brightness information, and areas like digits and characters exhibit richtexture details. In contrast, infrared images provide thermal imaging information for areas and objectsin shadow that visible images cannot capture due to visual obstacles. The proposed RD effectivelycaptures the advantageous regions of different source images and assigns larger weights to theseregions, thereby achieving more reliable fusion results. Medical Image Fusion. We visualize RDs in the MRI-PET dataset. Similar findings are also apparentin the MIF task. In PET images, bright regions indicate areas of malignant cell proliferation, whileMRI contains more structural information. As shown in , the RDs of PET stand out in the brightareas while MRIs highlights the structural information. Guided by RD, TTD emphasizes potentiallesion areas while preserving the structural information of these areas effectively. Multi-Exposure and Multi-Focus Image Fusion. For MEF and MFF tasks, the ideal outcome isthat the fused images contain properly exposed or precisely focused regions from each uni-source.As shown in the visualized RD map in , our TTD can effectively capture the dominant areas indifferent sources and assign higher RD values, i.e. dynamic fusion weight, to these regions.",
  "Negative": ": (a) The visualization of RDs obtained by gradient maps of different channels. The 44thgradient map provides wrong dominance information, and the 13th gradient map offers insignificantinformation, while the 58th gradient map performs the proper advantages of the two source images.(b) The radar chart of the gradient-based RD experiment (upper) and the validation of the negativecorrelation (below). Overall, with the integration of our TTD, the baseline model gains the ability to perceive dominantinformation dynamically. Therefore, this interpretable plug-and-play test-time dynamic adaptationfusion strategy can further improve the performance of the existing state-of-the-art methods. Thisfurther validates the effectiveness of RD in our TTD.",
  "Gradient-based Relative Dominability": "In our TTD, the proposed pixel-level fusion weight is computed by pixel-level loss. However, somenumeric losses are limited in directly obtaining the pixel-level weights, making it hard to integratewith TTD flexibly. To overcome this dilemma, we extend our TTD to a more general form andconstruct gradient-based Relative Dominability through any fusion losses for a more fine-grained androbust image fusion. Recalling our optimization objective of the generalization error bound, we aim for a negativecorrelation between the weights and losses of the same modality, i.e., establishing a correlationbetween losses and weights. Inspired by this positive correlation between loss value and the absolutevalue of its gradient for features with any convex loss function, our TTD can be further extended to agradient-based dynamic fusion weight. Specifically, we first calculate the absolute value of gradients |G(m)| RHW C of each uni-sourcefeature. As a test-time adaption approach, TTD does not update the network parameters, meaningthat the unimodal feature space remains fixed for the same baseline. For the same task scenarios, thefeature patterns tend to be similar. Therefore, we can empirically select the gradient channels thatwell represent the advantage areas to compute RDs. Also, as illustrated in (a), gradient mapsof some channels (such as the 44th and the 13th channels) lack significant useful information andfail to capture advantageous regions in the original images. Therefore, we select the gradient map|g(m)| RHW among C channels that best represent the dominance of the uni-source empirically.By replacing (m), we can obtain the RD and the dynamic fusion weight as follows:",
  "(m) = RD(m) = Softmax(e|g|(m)).(7)": "We have also conducted comparisons of loss-based TTD, full gradient-based TTD, and best gradient-based TTD on IFCNN. For the best gradient-based TTD, we choose the 58-th gradient map to obtainthe fusion weight. The results of gradient-based RD in (b) demonstrate that full gradient(yellow line) may bring wrong or useless dominance information to fusion weights, leading to worseperformance compared with loss-based TTD (green line). However, by selecting the empirically bestgradient map (red line), our TTD provides more fine-grained dominance information compared to theglobal loss maps, achieving more detailed dynamic fusion with better performance.",
  "Conclusion": "Image fusion aims to integrate effective information from multiple sources. Despite numerous meth-ods being proposed, research on dynamic fusion and its theoretical guarantees remains significantlylacking. To address these issues, we derive from a generalized form of image fusion and introducea new Test-Time Dynamic (TTD) image fusion paradigm with a theoretical guarantee. From theperspective of generalization error, we reveal that reducing generalization error hinges on the negativecorrelation between the fusion weight and the uni-source component reconstruction loss. Here theuni-source components are decomposed from fusion images, reflecting the effective information ofthe corresponding source image in constructing fusion images. Accordingly, we propose a pixel-levelRelative Dominablity (RD) as the dynamic fusion weight, which theoretically enhances the gener-alization of the image fusion model and dynamically highlights the changing dominant regions ofdifferent sources. Comprehensive experiments with in-depth analysis validate our superiority. Webelieve the proposed TTD paradigm is an inspirational development that can benefit the communityand address the theoretical gap in image fusion research.",
  "Acknowledgements": "This work was sponsored by the National Natural Science Foundation of China (No.s 62476198,62376193, 62106171, 61925602), and CCF-Baidu Open Fund. This work was also sponsored byCAAI-CANN Open Fund, developed on OpenI Community. Yinan Xia and Yi Ding contributedequally to this work. The authors thank anonymous peer reviewers for their helpful suggestions. Robert T Collins, Alan J Lipton, Takeo Kanade, Hironobu Fujiyoshi, David Duggins, Yanghai Tsin, DavidTolliver, Nobuyoshi Enomoto, Osamu Hasegawa, Peter Burt, et al. A system for video surveillance andmonitoring. VSAM final report, 2000(1-68):1, 2000. Li-Jia Li, Richard Socher, and Li Fei-Fei. Towards total scene understanding: Classification, annotationand segmentation in an automatic framework. In 2009 IEEE Conference on Computer Vision and PatternRecognition, pages 20362043. IEEE, 2009.",
  "Jiayi Ma, Yong Ma, and Chang Li. Infrared and visible image fusion methods and applications: A survey.Information fusion, 45:153178, 2019": "Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and Zhongxuan Luo.Target-aware dual adversarial learning and a multi-scenario multi-modality benchmark to fuse infrared andvisible for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 58025811, 2022. Jinyuan Liu, Xin Fan, Ji Jiang, Risheng Liu, and Zhongxuan Luo. Learning a deep multi-scale featureensemble and an edge-attention guidance for image fusion. IEEE Transactions on Circuits and Systems forVideo Technology, 32(1):105119, 2021.",
  "Kede Ma, Kai Zeng, and Zhou Wang. Perceptual quality assessment for multi-exposure image fusion.IEEE Transactions on Image Processing, 24(11):33453356, 2015. doi: 10.1109/TIP.2015.2442920": "K Ram Prabhakar, V Sai Srikar, and R Venkatesh Babu. Deepfuse: A deep unsupervised approach forexposure fusion with extreme exposure image pairs. In Proceedings of the IEEE international conferenceon computer vision, pages 47144722, 2017. Jinyuan Liu, Jingjie Shang, Risheng Liu, and Xin Fan. Attention-guided global-local adversarial learningfor detail-preserving multi-exposure image fusion. IEEE Transactions on Circuits and Systems for VideoTechnology, 32(8):50265040, 2022.",
  "Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin Ling. U2fusion: A unified unsupervised imagefusion network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):502518, 2020": "Bing Cao, Yiming Sun, Pengfei Zhu, and Qinghua Hu. Multi-modal gated mixture of local-to-globalexperts for dynamic image fusion. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 2355523564, 2023. Pengfei Zhu, Yang Sun, Bing Cao, and Qinghua Hu. Task-customized mixture of adapters for generalimage fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 70997108, 2024.",
  "Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Pengfei Li, and Jiangshe Zhang. Didfuse: Deepimage decomposition for infrared and visible image fusion. arXiv preprint arXiv:2003.09210, 2020": "Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte, andLuc Van Gool. Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality imagefusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),pages 59065916, June 2023. Berk Gokberk and Lale Akarun. Comparative analysis of decision-level fusion algorithms for 3d facerecognition. In 18th International Conference on Pattern Recognition (ICPR06), volume 3, pages10181021. IEEE, 2006. Zongbo Han, Changqing Zhang, Huazhu Fu, and Joey Tianyi Zhou. Trusted multi-view classificationwith dynamic evidential fusion. IEEE transactions on pattern analysis and machine intelligence, 45(2):25512566, 2022.",
  "Xingchen Zhang. Benchmarking and comparing multi-exposure image fusion algorithms. InformationFusion, page 111131, Oct 2021. doi: 10.1016/j.inffus.2021.02.005. URL": "Hao Zhang, Zhuliang Le, Zhenfeng Shao, Han Xu, and Jiayi Ma. Mff-gan: An unsupervised generativeadversarial network with adaptive and gradient joint constraints for multi-focus image fusion. InformationFusion, page 4053, Feb 2021. doi: 10.1016/j.inffus.2020.08.022. URL Pengwei Liang, Junjun Jiang, Xianming Liu, and Jiayi Ma. Fusion from decomposition: A self-superviseddecomposition approach for image fusion. In European Conference on Computer Vision, pages 719735.Springer, 2022.",
  "Fang Xu, Jinghong Liu, Yueming Song, Hui Sun, and Xuan Wang. Multi-exposure image fusion techniques:A comprehensive review. Remote Sensing, 14(3):771, 2022": "J Wesley Roberts, Jan A Van Aardt, and Fethi Babikker Ahmed. Assessment of image fusion proceduresusing entropy, image quality, and multispectral classification. Journal of Applied Remote Sensing, 2(1):023522, 2008. V. Aslantas and E. Bendes. A new image quality metric for image fusion: The sum of the correlations ofdifferences. AEU - International Journal of Electronics and Communications, page 18901896, Dec 2015.doi: 10.1016/j.aeue.2015.09.004. URL Guangmang Cui, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Detail preserved fusion of visibleand infrared images using regional saliency extraction and multi-scale image decomposition. OpticsCommunications, page 199209, Apr 2015. doi: 10.1016/j.optcom.2014.12.032. URL",
  "Ahmet M Eskicioglu and Paul S Fisher. Image quality measures and their performance. IEEE Transactionson communications, 43(12):29592965, 1995": "Z. Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: From error visibility tostructural similarity. IEEE Transactions on Image Processing, page 600612, Apr 2004. doi: 10.1109/tip.2003.819861. URL Hao Zhang, Han Xu, Yang Xiao, Xiaojie Guo, and Jiayi Ma. Rethinking the image fusion: A fast unifiedimage fusion network based on proportional maintenance of gradient and intensity. In Proceedings of theAAAI conference on artificial intelligence, volume 34, pages 1279712804, 2020. Qiantong Wang, Weihai Chen, Xingming Wu, and Zhengguo Li. Detail-enhanced multi-scale exposurefusion in yuv color space. IEEE Transactions on Circuits and Systems for Video Technology, 30(8):24182429, 2019.",
  "Kede Ma, Zhengfang Duanmu, Hojatollah Yeganeh, and Zhou Wang. Multi-exposure image fusion byoptimizing a structural similarity index. IEEE Transactions on Computational Imaging, 4(1):6072, 2017": "Xu Song and Xiao-Jun Wu. Multi-focus image fusion with pca filters of pcanet. In Multimodal PatternRecognition of Social Signals in Human-Computer-Interaction: 5th IAPR TC 9 Workshop, MPRSS 2018,Beijing, China, August 20, 2018, Revised Selected Papers 5, pages 117. Springer, 2019. Jia Lei, Jiawei Li, Jinyuan Liu, Shihua Zhou, Qiang Zhang, and Nikola K Kasabov. Galfusion: Multi-exposure image fusion via a globallocal aggregation learning network. IEEE Transactions on Instrumen-tation and Measurement, 72:115, 2023. Boyuan Ma, Yu Zhu, Xiang Yin, Xiaojuan Ban, Haiyou Huang, and Michele Mukeshimana. Sesf-fuse:An unsupervised deep model for multi-focus image fusion. Neural Computing and Applications, 33:57935804, 2021. Jinyuan Liu, Guanyao Wu, Junsheng Luan, Zhiying Jiang, Risheng Liu, and Xin Fan. Holoco: Holisticand local contrastive learning network for multi-exposure image fusion. Information Fusion, 95:237249,2023.",
  "B.1Algorithm": "Here we report the algorithm of the whole dynamic fusion strategy in Algorithm 1. To accomplish our TTD, weinitially feed each uni-source image individually into the fusion network to acquire the respective uni-sourcecomponents. Then we compute the pixel-wise loss between the uni-source components and their correspondinguni-source images. Finally, utilizing Eq. (6) we obtain the dynamic fusion weight and apply dynamic fusionaccordingly.",
  "B.2The Pipeline of TTD": "The detailed pipeline for inference is shown in (c). In stage 1 (black dashed line), we feed each uni-source image individually into the frozen encoder and decoder to acquire the respective decomposed uni-sourcecomponents. Then, we calculate the RDs according to the colored line in Eq. (6) (c). In stage 2 (solid line),we feed multi-source images into the encoder and get their corresponding features. Then, we fuse features bymultiplying the RDs to the respective features and adding them up. Finally, the fused feature is fed into thedecoder for the final fusion results.",
  "C.1Visualization of Relative Dominability": "RD is adaptable to the noise condition. We simulate a noisy situation in which the visible image quality isaffected by contrast perturbation. As shown in , with the corruption severity level increasing, the dominantregions of visible modality are gradually reduced, while the unchanged infrared modality gains an increasingRD. Our RD effectively perceives the dominance changes.",
  "InfraredVisible": ": Visualizations of RD maps at different times within the same scenario in the LLVIP dataset.As time progresses from day to night, an intuitive observation is that the dominance of visible imagesgradually decreases, while the dominance of infrared images increases. RD is adaptable to different data qualities. a) The quality of images also changes with illumination. As shownin , we visualized the RDs of the samples at different times in the same scenario. As it changes from day tonight, the dominance of visible images gradually decreases, while the dominance of infrared images increases. b) Furthermore, to simulate the malfunction of sensors in a real scenario, we masked the infrared image randomly.As shown in (a)(b), the RD of the region being masked is apparently smaller than the surrounding area,while that of the same region in the infrared image is relatively greater.",
  "C.2Ablation Study": "As shown in the ablation study results of our TTD on four tasks in , our TTD can highlight the advantageousregions in four tasks, improve contrast, and preserve detail compared with baselines. For example, in the VIFtask, our method enhances the details of people and the shadow textures of trees in the fused images comparedto the baseline. In the MIF task, our method maintains the bright information from PET while strengthening thetexture details from MRI in the overlapping regions. In the MEF and MFF tasks, the fused images producedby our method have stronger texture details and edge information, as well as higher clarity and color fidelity,compared to the baseline fused images.",
  "(iii) ablation study on the ways to obtain weight: see Sec. 5.3 and": "(iv) ablation study on different forms of fusion weights. We compared different forms of fusion weight: w = 0.5(baseline),w = Softmax(), w = Softmax(Sigmoid()), Softmax(e) over IFCNN on the LLVIPdataset, results are given in Tab. 4, it shows that forms of fusion can be flexible to achieve the negative correlationbetween weight and reconstruction loss. (v) ablation study on the normalization of the weights: we compared three forms of normalization over IFCNNon the LLVIP, results are given in Tab. 5, indicating that as a premise of the generalization theory (see Thm. 3.1),the normalization of the weights is necessary and the ways to normalize have little impact on our method. Overall, we performed complete ablation analyses to validate the effectiveness of TTD (i), the necessity of thenegative correlation between fusion weight and reconstruction loss (ii), the expandability of ways to obtainfusion weight (iii), the flexibility in the form of weights (iv), the significance of normalization (v).",
  "C.4The Effectiveness of TTD on Baselines with Varying Performaces": "In Tab. 1, we have applied TTD to various baselines with different capabilities and all achieved consistentenhancement, TTD can even further improve the performance when combined with current state-of-the-artmethods. To further validate that our TTD is effective on models with different performances, we conductedadditional experiments to apply TTD on models with varying performance levels by adding random Gaussiannoise to the pre-trained model (IFCNN) parameters. The results on the LLVIP dataset are given in Tab. 8,showing that the performance of the baseline decreases with increasing noise added to it. As a comparison, our",
  "C.5Results on Downstream Task": "First, we train the detection model with visible images from the LLVIP dataset. Then we employed our TTD onIFCNN and compared its performance with the baseline on the object detection task. As illustrated in ,the detection results of the baseline fused images exhibit missing detection for hard-recognized fast-movingblurred objects. In contrast, after applying our TTD, all objects were accurately detected. Additionally, the fusedimages obtained using our TTD achieved higher performance in detection tasks compared to the baseline. ourDIF shows improvements over the baseline in P, R, and mAP.5:.95 metrics.",
  "DInference Time": "The inference time of TTD is dependent on the inference time of the baseline. Since TTD is executed in twostages: in the first stage, we calculate the uni-source reconstruction loss and then compute the fusion weights;in the second stage, we perform the fusion based on the weights. As baselines perform the static fusion, theinference time of TTD is approximately double that of the baseline. We measured the average processing timeper image on the test set of the LLVIP dataset. The results of the inference time are given in Tab. 9.",
  "ELimitations and Broader Impacts": "As a test-time dynamic image fusion method, the performance of our TTD significantly depends on theperformance of the baseline models. In the future, we will try to employ the dynamic fusion mechanism in theoptimization of baseline models to guide fusion or design a more effective network, further improving fusionperformance. Moreover, in the gradient-based TTD, we select the best gradient empirically, a more adaptiveselection approach should be explored in the future. As for the potential social impact, our method performsmulti-sensor information fusion, which can be applied to drones, cameras, etc., but it is hard to guarantee theeffectiveness of baseline models, which may be risky in high-risk scenarios such as medical imaging."
}