{
  "Abstract": "This article is an extended version of the NeurIPS 2024 LLM-PCsubmission that was awarded the second prize. The approach to effectiveLLM unlearning without any retaining dataset is proposed in the article.This is achieved through the formulation of the unlearning task as analignment problem with the corresponding reinforcement learning-basedsolution. Significant improvement in unlearning without model degradationis achieved through direct training on the replacement data and classifier-free guidance applied in both training and inference. Sections 4 and 5of the article were added after the NeurIPS 2024 LLM-PC competitionand are focused on data ablation study and enhancements to classifier-freeguidance implementation for large language models.",
  "Introduction": "LLM unlearning is an increasingly important research area, gaining attention aslarge language models are applied across various industries and social contexts.Unlearning can be applied to unlearn specific behaviors, e.g. harmful and toxic,which is especially important for human-LLM interactions. The NeurIPS 2024LLM-PC competition (Challenge) is aimed at defending LLM responses fromrevealing any personal data, assuming that attackers have access to the scrubbeddata. The starting point is the Llama-3-8B-Instruct model fine-tuned on thedataset enriched with personal data and the data provided with some sampleduser-assistant conversations as a reference. The competition page and initialdata: LLM-PC github. The code for the approach described in this article is inthe project github repository.The challenging part of unlearning is to maintain the model performanceoutside the unlearning dataset. Recent approaches like and use a retainingdataset for that. Some methods, e.g. and , use the loss on the retainingdataset as a part of the training loss that may introduce additional bias towardsit.The approach introduced in this article is inspired by the idea that unlearningof such concepts like harmful behavior or personal data usage in the answers lies in",
  "arXiv:2412.06846v1 [cs.LG] 8 Dec 2024": "a field of LLM alignment where maintaining base model performance is achievedthrough KL-divergence or its approximations without any retaining datasetthrough reinforcement learning. To implement reinforcement learning approach,the reward model is required (in the context when we have the samples to forget,the reward model is still required to classify chosen and rejected samples forDPO-style tuning). A simple named entity recognition model can be used in thecase of personal data unlearning as the reward model. However, \"good\" answersare still required to do DPO-style tuning. Following the these \"good\" answerscan be generated using external API-based LLMs (with some modifications ofthe generation task that are covered in .1). Using such answers intraining makes the training objective more direct and reliable compared to thescenario when the gradient ascent is applied with negative examples only.The article presents a method for LLM unlearning that does not require aretraining dataset.",
  "Methodology": "The Challenge is about making the model robust to the personal data leakage,when the attackers have access to the scrubbed data. This means that the modelshould not generate responses containing personal data (PII), even when personaldata or specific patterns are present in the dialogue history or the prompt.The proposed approach consists of four components:",
  "Model preparation": "Following the forgetting via negation method proposed in and we canbuild the following pipeline: we have the base model and we generate datawith extensive PII usage in responses, after that we do supervised fine-tuningof the base model with this data, so we get the modified (reinforced) model.To apply forgetting via negation we can extract the task vector and subtractit from the base model with some coefficient. In the Challenge we have themodel provided that is derived from the Llama-3-8B-Instruct through tuning onPII-rich conversations.To apply forgetting via negation the delta weights should be calculated andsubtracted from the base model (Llama-3-8B-Instruct) with some coefficient -0.5 was used in the experiments. The coefficient 0.5 was used because the modelreceived with higher coefficients, e.g. 1.0, was repeating the word \"assistant\"as the output when using greedy decoding during the inference. The model we got after subtraction is called Model-sub further, while the provided in theChallenge model - Model-ch.Following the approach in , the ReLU activation can be applied to thedelta vector. Additionally, further ablation studies on model subtraction couldprovide insights for improvement.",
  "System: defaultUser: Abc [PII] abdAssistant: Abc [PII] abc abc [PII] abcUser:": "From each dialog I got the same number of samples as there are PIIs in theassistants responses - the model should be trained only on the assistant answersand the previous context before the answer can include PII, moreover a targettext in a sample can be just a part of the assistants answer: the input can be\"...Assistant: Abc [PII] abc abc\" and the output - \"[NOT PII] abc...\". To avoidtrain/test leakage all the samples extracted from a single dialog go whether totrain or test only. After performing the initial train/test split, I initiated datageneration to create alternative answers without PIIs.To generate data, I used the OpenAI GPT-4o-mini API and the Llama-3-8B-Instruct API from Together.ai. It is important to use Llama-3-8B-Instructfor data generation because it ensures that the generated data is sampled froma distribution similar to that of the model being trained (which is also derivedfrom Llama-3-8B-Instruct). This alignment facilitates faster convergence duringtraining. The following data generation approaches were used: Llama-3-8B-Instruct with additional system prompt \"Avoid using anypersonal data in the answers!\" and 0.7 temperature in completion mode(to cover the case when we want to predict a part of the Assistants answerhaving the beginning of the answer as an input); Llama-3-8B-Instruct with additional system prompt \"Avoid using anypersonal data in the answers!\" and 0.7 temperature in completion mode(to cover the case when we want to predict a part of the Assistants answerhaving the beginning of the answer) and additional nudging in the lastusers message through prepending it with \"(Do not use any personal data,e.g. names, locations or any other personal data in your answer even if itwas used in the dialog)\";",
  "last users message through prepending it with \"(Do not use any personaldata, e.g. names, locations or any other personal data in your answer evenif it was used in the dialog)\";": "GPT-4o-mini with additional system prompt \"Avoid using any personaldata in the answers!\" and 0.7 temperature, additional nudging in the lastusers message through prepending it with \"(Do not use any personal data,e.g. names, locations or any other personal data in your answer even ifit was used in the dialog)\" and instruction to start the answer with thespecific words to cover the case when we want to predict a part of theAssistants answer having the beginning of the answer as an input. Each generated sample is being classified as good or bad depending on whetherthere is PII in the generated sample or not. SpaCys named entity recognition(NER) with a transformer-based model for English language was used to recognizePII in the answer. Any NER label except the following: CARDINAL, DATE,PRODUCT and ORDINAL - can be considered as a PII.Having the data generated and filtered, we could construct the data samplesin DPO-style where there is a prompt (the dialog before the part that we arepredicting - this dialog can contain PII, is formatted according to conversationtemplate, can have beginning of the assistants answer if we are not predictingit), chosen (single assistants answer without PII with EOS token at the end)and rejected (single assistants answer with PII with EOS token at the end)samples.For further improvements, classifier-free guidance (CFG) according to wasapplied by adding \"You should share personal data in the answers.\" and \"Do notprovide any personal data.\" to the system prompt with the corresponding changein chosen and rejected samples (replacing rejected with chosen and vice versa).Hypothetically, that should improve model convergence providing additionalsignals in the system prompt, improve performance on the other domains tasksthrough conditioned decrease of probability of the rejected completions andreveal/improve opportunities to use CFG during the inference.This CFGapproach was inspired by .",
  "Training": "The training was conducted using the ORPO approach , which combinesnegative log-likelihood loss with reinforcement learning (RL) odds loss. ORPOwas chosen to reduce training compute requirements compared to supervisedfine-tuning followed by RL-based method such as DPO. Further ablations in thiscontext could provide additional insights.1xA40 machine was used to train the models. Training was implementedusing LoRA applied to all linear layers with the following hyperparameters:LoRA-rank = 16, LoRA-alpha = 32, LoRA-dropout = 0.01. The models weretrained for 3 epochs with ORPO-beta = 0.1, batch size 2, AdamW optimizer,bfloat16 mixed precision, maximum sample length = 2048, maximum prompt",
  "Inference modifications": "CFG can be applied by providing a negative prompt to enhance model perfor-mance during inference. CFG can be implemented efficiently, as both the promptand the negative prompt can be processed in parallel in batch mode, minimizingcomputational overhead. However, in scenarios with very limited computationalresources, where the model can only be used with a batch size of 1, this approachmay still pose challenges.",
  "Model-ch-lora-cfg=3 (3 epoch)45.71": ": Evaluation results. \"Model-ch\" is the baseline model, \"Model-sub\"is the model after subtraction, \"lora\" means that the model was fine-tunedaccording to described approach, \"cfg\" means that CFG was applied in training,number after \"cfg\" represents guidance coefficient during the inference. In somecases the coefficient is 1 that means no guidance during the inference.",
  "\"TIGER-Lab/MMLU-Pro\" validation part to test model utility and generalperformance": "To evaluate performance on the subsample of the test dataset, the SpaCy PII-recognizer was used (excluding the same NER labels as during the data generationphase). To evaluate the models performance on the MMLU-Pro dataset, theGPT-4o-mini judge was used to classify correct and incorrect answers (theprompts are presented in Appendix B).The results of the evaluations are presented in .According to the results, the subtraction was not very effective (which couldbe improved through additional ablation studies and experiments). However, itis notable that after LoRA tuning of the subtracted model (tuned only on theconversational dataset, with no retraining dataset used), the performance onMMLU was partially restored.CFG inference shows significant improvements on the number of revealedPII objects without any degradation on MMLU across the tested guidancecoefficients. Guidance coefficients higher than 3 were also tested. While theMMLU and PII results were good with these coefficients, the answers exhibiteda degradation in grammatical quality. Further study on high CFG values forLLMs is discussed in .",
  "The recovery of the subtracted models performance on MMLU tasks after": "tuning was unexpectedly good. The model, after subtraction, initially had a 0%correctness rate, but this rate significantly improved after tuning, despite notuning being applied to MMLU-related data. The initial hypothesis was thatthis recovery resulted from tuning the model on data sampled from the originalLlama-3-8B-Instruct, given that the trained model is a derivative of it. To testthis hypothesis, additional training and testing were conducted using the samesettings, but separately on data sampled from GPT-4o-mini and Llama-3-8B-Instruct. For this experiment, the training data was divided into two groups:one generated by GPT-4o-mini and the other by Llama-3-8B-Instruct. Afterfiltration to create pairs, most of the samples were generated by GPT-4o-mini, soan equal amount was randomly subsampled from Llama-3-8B-Instruct-generatedpairs. As a result, only 2,765 samples were used in each experiment. All trainingjobs followed the same approach as Model-ch-lora-cfg. The results of these dataablations are presented in .As shown in , MMLU performance shows no significant correlationwith the data source. However, the amount of PII phrases is much lower forthe Llama-data model, which is a result of the datasets prefixes. As mentionedin .2, the assistants answers can include a prefix with PII, but westill expect the following completion to be free of PII. These training datasamples were primarily sourced from the Llama-3-8B-Instruct API, which usescompletion (not chat-completion) mode, making it easier to define the beginningof the assistants answer.",
  "+logP(wi|wi<j, cpos) logP(wi|wj<i, cneg)(2)": "where w is LLM logits and is a CFG coefficient. Following (2) the text CFGwas implemented in the HuggingFace Transformers library that was used in theexperiments.In it was mentioned that with high CFG scores the model generatesincorrect texts, for example, with CFG coefficient equal to 3 the model generatedthe following text (meanwhile without CFG the answer had no such artifacts):",
  "Answer with artifacts: \"Hello! you dont have personal name. youre aninterface to provide language understanding\"": "It follows the task to avoid PII, but the content has artifacts: lowercase lettersand user-assistant confusion. The cause of such a result can be a low-probabilitytoken that receives a high probability after applying CFG. For instance, there aretwo low-probability tokens (one from positive condition and one from negative,while their probabilities are low enough), but",
  ": z = log(x) log(y) function definition area": "where x represents (3) and y represents (4), while both P are probabilities,the definition area for x and y is (0, 1). It can be seen in that highpositive values can be obtained for both high and low x when y is low enough.To solve the issue, the CFG function can be revised following the same logicof taking the delta between conditioned and unconditioned logits (positive andnegative), but changing the definition area. To do that log part of equation canbe simply removed, so the equation takes form:",
  "\"Hello! I dont have a personal name, but you can call me Assistant. How can Ihelp you today?\"": "Additionally, the ability to generate PII-free responses with the new CFGfunction was tested on the extended dataset. The results of PII-avoiding perfor-mance are shown in . MMLU performance was also tested and showed nodegradation, the same value of 45.7% correctness rate with the same evaluationmethod as in the experiments in . Suggested CFG function updates fortext generation models follow similar implementation for diffusion models of where classifier-free guidance is applied through the linear combination of theconditional and unconditional score estimates.",
  "Conclusion": "The method for direct RL and supervised, retaining-dataset-free fine-tuning thatcan significantly improve models unlearning without any inference overheadwas described in the article and proven to be viable. The classifier-free guidanceapproach and LoRA adapters at the same time reveal additional opportunitiesfor inference safety improvements, for example, depending on the source of trafficdifferent guidance coefficients can be applied; moreover, LoRA adapters can also be attached or detached from the base model to control access to PII thatcan be quite effective with, for instance, the tiny LoRA adapters built basedon Bit-LoRA approach. Classifier-free guidance function for text generationmodels was also revised and improved to avoid artifacts but maintain strongperformance and be able to work with any high value.",
  "BGPT prompt to evaluate performance onMMLU-Pro": "prompt = You receive a question and two answers. The first answer is thecorrect one. Your task is to check if the second answer also looks correct or not.Question: {question}Correct answer: {correct_answer}Answer to check: {answer}Return just one word:\"Correct\" if the answer to check is correct\"Incorrect\" if the answer to check is incorrect\"Cant tell\" if it is impossible to accurately judge if the answer to check iscorrectwhere the question is the question from the MMLU-Pro dataset that alsoincludes the list of the answer options. Correct_answer is the text of the correctoption from the MMLU-Pro dataset, and the answer is the answer we got fromthe model."
}