{
  "Abstract": "Most real-world datasets consist of a natural hierarchy between classes or an in-herent label structure that is either already available or can be constructed cheaply.However, most existing representation learning methods ignore this hierarchy,treating labels as permutation invariant. Recent work proposes using thisstructured information explicitly, but the use of Euclidean distance may distort theunderlying semantic context . In this work, motivated by the advantage of hyper-bolic spaces in modeling hierarchical relationships, we propose a novel approachHypStructure: a Hyperbolic Structured regularization approach to accuratelyembed the label hierarchy into the learned representations. HypStructure is asimple-yet-effective regularizer that consists of a hyperbolic tree-based representa-tion loss along with a centering loss. It can be combined with any standard task lossto learn hierarchy-informed features. Extensive experiments on several large-scalevision benchmarks demonstrate the efficacy of HypStructure in reducing distor-tion and boosting generalization performance, especially under low-dimensionalscenarios. For a better understanding of structured representation, we performan eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is availableat",
  "Introduction": "Real-world datasets, such as ImageNet and CIFAR , often exhibit a natural hierarchy oran inherent label structure that describes a structured relationship between different classes in thedata. In the absence of an existing hierarchy, it is often possible to cheaply construct or inferthis hierarchy from the label space directly . However, the majority of existing representationlearning methods treat the labels as permutation invariant, ignoringthis semantically-rich hierarchical label information. Recently, Zeng et al. offer a promisingapproach to embed the tree-hierarchy explicitly in representation learning using a tree-metric-basedregularizer, leading to improvements in generalization performance. The approach uses a computationof shortest paths between two classes in the tree hierarchy to enforce the same structure in the featurespace, by means of a Cophenetic Correlation Coefficient (CPCC) based regularizer. However,their approach uses the 2 distance in the Euclidean space, distorting the parent-child representationsin the hierarchy owing to the bounded dimensionality of the Euclidean space .",
  "arXiv:2412.01023v1 [cs.LG] 2 Dec 2024": "Hyperbolic geometry has recently gained growing interest in the field of representation learning. Hyperbolic spaces can be viewed as the continuous analog of a tree, allowing for embeddingtree-like data in finite dimensions with minimal distortion . Unlike Euclidean spaceswith zero curvature and spherical spaces with positive curvature, the hyperbolic spaces have negativecurvature enabling the length to grow exponentially with its radius. Owing to these advantages,hyperbolic geometry has been used for various applications such as natural language processing, image classification , object detection , action retrieval , andhierarchical clustering . However, the aim of using hyperbolic geometry in these approaches isoften to implicitly leverage the hierarchical nature of the data. In this work, given a label hierarchy, we argue that accurately and explicitly embedding the hierarchicalinformation into the representation space has several benefits, and for this purpose, we proposeHypStructure, a hyperbolic label-structure based regularization approach that extends the proposedmethodology in Zeng et al. for semantically structured learning in the hyperbolic space.HypStructure can be easily combined with any standard task loss for optimization, and enablesthe learning of discriminative and hierarchy-informed features. In summary, our contributions are asfollows: We propose HypStructure and demonstrate its effectiveness in the supervised hierarchicalclassification tasks on three real-world vision benchmark datasets, and show that our pro-posed approach is effective in both training from scratch, or fine-tuning if there are resourceconstraints. We qualitatively and quantitatively assess the nature of the learned representations anddemonstrate that along with the performance gains, using HypStructure as a regularizerleads to more interpretable as well as tree-like representations as a side benefit. Thelow-dimensional representative capacity of hyperbolic geometry is well-known , andinterestingly, we observe that training with HypStructure allows for learning extremelylow-dimensional representations with distortion values lower than even their correspondinghigh-dimensional Euclidean counterparts. We argue that representations learned with an underlying hierarchical structure are beneficialnot only for the in-distribution (ID) classification tasks but also for Out-of-distribution(OOD) detection tasks. We empirically demonstrate that learning ID representations withHypStructure leads to improved OOD detection on 9 real-world OOD datasets withoutsacrificing ID accuracy . Inspired by the improvements in OOD detection, we provide a formal analysis of theeigenspectrum of the in-distribution hierarchy-informed features learned with CPCC-stylestructured regularization methods, thus leading to a better understanding of the behavior ofstructured representations in general.",
  "Background": "Structured representation learning breaks the permutation invariance of flat representationlearning by incorporating a hierarchical regularization term with a standard classification loss. Theregularization term is specifically designed to enforce class-conditioned grouping or partitioning inthe feature space, based on a given hierarchy. More specifically, given a weighted tree T = (V, E, e) with vertices V , edges E and edge weights e,let us compute a tree metric dT for any pair of nodes v, v V , as the weighted length of the shortestpath in T between v and v. For a real world dataset D = {(xi, yi)}Ni=1, we can specify a label treeT where a node vi V , vi corresponds to a subset of classes, and Di D denote the subset of datapoints with class label vi. We denote dataset distance between Di and Dj as (vi, vj) = d (Di, Dj),where d(, ) is any distance metric in the feature space, varied by design. With a collection of tree metric dT and dataset distances , we can use the Cophenetic CorrelationCoefficient (CPCC) , inherently a Pearsons correlation coefficient, to evaluate the correspondencebetween the nodes of the tree, and the features in the representation space. Let dT , denote the meanof the collection of distances, then CPCC is defined as",
  "i<j((vi, vj) )2)1/2 .(1)": "For the supervised classification task, we consider the training set Dintr = {(xi, yi)}Ni=1 and we aimto learn the network parameter for a feature encoder f : X Z, where Z Rd denotes therepresentation/feature space. For structured representation learning, the feature encoder is usuallyfollowed by a classifier gw, and the parameters , w are learnt by minimizing L along with a standardflat (non-hierarchical) classification loss, for instance, Cross-Entropy (CE) or Supervised Contrastive(SupCon) loss, with the structured regularization term as:",
  "(x,y)DFlat(x, y, , w) CPCC(dT , ).(2)": "Using a composite objective as defined in Equation (2), we can enforce the distance relationshipbetween a pair of representations in the feature space, to behave similarly to the tree metric betweenthe same vertices. For instance, consider a simple label tree with a root node, a coarse level, and a finelevel, where subsets of fine classes share the same coarse parent. For this hierarchy, we would expectthe fine classes of the same parents (e.g., apple and banana are fruits) to have closer representationsin the feature space, whereas fine classes with different coarse parents (e.g., an apple is a fruit and atulip is a flower) should be further apart. The learned structure-informed representations reflect thesehierarchical relationships and lead to interpretable features with better generalization .",
  "|Dj|": "xDj f(x)2. The distance between datasets is thus the 2 distancebetween two Euclidean centroids of their class-conditioned representations, which is unsuitable formodeling tree-like data . Additionally, this regularization approach in Zeng et al. is appliedonly to the leaf nodes of T for efficiency. However, this leaf-only formulation of the CPCC offers an approximation of the structured informa-tion, since the distance between non-leaf nodes is not restricted explicitly by the regularization. Thisapproximation, therefore, leads to a loss of information contained in the original hierarchy T . Actually, it is impossible to embed dT into 2 exactly. Or more formally, there exists no bijection such that dT ((zi), (zj)) = zi zj2 irrespective of how large the feature dimension d is. Weprovide two such examples for a toy label tree in , below.",
  ": (left) An unweighted label treewith two coarse nodes: F, G. F containstwo fine classes A, B and G containsthree fine classes C, D, E. We cannotembed this in 2 exactly (right)": "Example 1. We intend to embed all nodes in T , includingpurple internal nodes. Notice that G, C, D, E is a stargraph centered at G. Since CG = DG = 1, CD = 2,by triangle inequality C, D, G must be on the same linewhere G is the center of CD. Similarly, G must be at thecenter of DE. Hence, the location of E must be at C,which contradicts the uniqueness of all nodes in T . Example 2. As an easier problem, let us only embedleaf nodes into the Euclidean space as shown in .Since CD = DE = CE = 2, they must be on a planewith an equilateral triangle CDE in Euclidean geometry.Then all the green classes have the same distance 4 toeach yellow class. Therefore, A, B must be on the lineperpendicular to CDE and intersecting the plane with O, which is the barycenter of CDE. Dueto the uniqueness and symmetry of A, B, we must have AO = BO = 1 to satisfy AB = 2.AO = 1, OE = 2",
  "('transportation','animal')": ": Using 2-CPCC for structured representation on CIFAR10. CIFAR10 hierarchy (left) has athree level structure with 13 vertices. For a 512-dimensional embedding, we apply 2-CPCC eitherfor the full tree (middle) or the leaf nodes only (right) and plot the ground truth tree metric againstpairwise Euclidean centroid distances of the learnt representation. The optimal train CPCC is 1. Since we cannot embed an arbitrary tree T into 2 without distortion, it would also affect theoptimization of the 2-CPCC in a classification problem, where the tree weights encode knowledgeof class similarity. To verify our claims, we consider the optimization of 512-dimensional 2-CPCCstructured representations for CIFAR10 . The CIFAR10 dataset consists of a small label hierarchyas shown in (left). The optimal CPCC is achieved when each tree metric value corresponds to a single 2. However,in (right), even with an optimization of the 2-CPCC loss for the entire tree, we observe asub-optimal train CPCC less than 1, where the distance between two coarse nodes, transportationand animal, is far away from the desired solution. Furthermore, optimization of the CPCC loss foronly the leaf nodes, leads to an even larger distortion of the tree metrics.",
  ": Lines on differentmodels for 2-dimensional hy-perbolic space": "Hyperbolic spaces are non-Euclidean spaces with negative curvaturewhere given a fixed point and a line, there exist infinitely manyparallel lines that can pass through this point. There are severalcommonly used isometric hyperbolic models . For this work, wemainly use the Poincar Ball model. Definition 3.1 (Manifold). A manifold M is a set of points z thatare locally Euclidean. Every point z of the manifold M is attachedto a tangent space TzM, which is a vector space over the reals of thesame dimensionality as M that contain all the possible directionsthat can tangentially pass through z. Definition 3.2 (Poincar Ball Model). Given c as a constant, thePoincar ball model (Bdc, gB) is defined by a manifold of an openball Bdc = {z Rd : cz2 < 1} and metric tensor gB that definesan inner product of TzBdc. The model is equipped with the distance as",
  ".(3)": "For c 0, we can recover the properties of the Euclidean geometry since limc0 dBc(z1, z2) =2z1 z2. Since TzBdc is isomorphic to Rd, we can connect vectors in Euclidean space andhyperbolic space with the bijection between TzBdc and Bdc . For z = 0, the exponential mapexpc0 : TzBdc Bdc and logarithm map logc0 : Bdc TzBdc have the closed form of",
  "v,otherwise,(5)": "so the clipped vector is within the Poincare disk. We set as a small positive number in practice.Definition 3.3 (Klein Model). Klein model (Kdc, gK) consists of an 1/c-radius open ball Kdc ={z Rd : cz2 < 1} and a metric tensor gK different from gB. Similar to the mean computationin Euclidean space, let i = 1/",
  "i=1i.(6)": "We illustrate the relationship between the different hyperbolic models in . The hyperboloidspace models d-dimensional hyperbolic geometry on a d + 1-dimensional space. When d = 2, theKlein model is the tangent plane of the hyperboloid model at (0, 0, 1), and the Poincar disk sharesthe same support as the Klein disk, although shifted downwards and centered at the origin. Given atriangle on the hyperboloid model, its projection on the Klein model preserves the straight sides, butthe projection of a line on the Poincar model is a part of a circular arc or the diameter of the disk.Let zB, zK be coordinates of z under Poincar and Klein model respectively, the prototype operationson Bdc require transformations between Bdc and Kdc as",
  "HypStructure: Hyperbolic Structured Regularization": "At a high level, HypStructure uses a combination of two losses: a Hyperbolic Cophenetic Correla-tion Coefficient Loss (HypCPCC)), and a Hyperbolic centering loss (HypCenter) for embedding thehierarchy in the representation space. Below we describe the two components of HypStructure.The pseudocode of HypStructure is shown in Algorithm 1 in Appendix B.2. HypCPCC (Hyperbolic Cophenetic Correlation Coefficient): We extend the 2-CPCC methodol-ogy in Zeng et al. to the hyperbolic space in HypCPCC. Three major steps of HypCPCC are (i)map Euclidean vectors to Poincar space (ii) compute class prototypes (iii) use Poincar distance forCPCC. Specifically, we first project each zi Rd to Bdc, and compute the Poincar centroid for eachvertex of T using hyperbolic averaging as shown in Equation (6) and Equation (7). Alternatively, wecan also compute Euclidean centroids z =1",
  "zDi z for each vertex, and project each z Rd": "to Bdc either by expc0 or clipc. After the computation of hyperbolic centroids, we use the pairwisedistances between all vertex pairs in T in the Poincar ball, to compute the HypCPCC loss usingEquation (1) by setting = dBc. HypCenter (Hyperbolic Centering): Inspired by Sarkars low-distortion construction thatplaces the root node of a tree at the origin, we propose a centering loss for this positioning, thatenforces the representation of the root node to be close to the center of the Poincar disk, andthe representations of its children to be closer to the border of Poincar disk. We enforce thisconstraint by minimizing the norm of the hyperbolic representation of the root node as center =HypAveB(expc0(z1), . . . , expc0(zN)). Alternatively, for centroids computed in the Euclidean",
  "where Flat is a standard classification loss, such as the CE loss or the SupCon loss": "Time Complexity: In a batch computation setting with a batch size b and the number of classes (leafnodes) as k, the computational complexity for a HypStructure computation to embed the full treewill still be O(d min{b2, k2}), which is the same as the complexity of a Euclidean leaf-only CPCC.The additional knowledge gained from internal nodes allows us to reason about the relationshipbetween higher-level concepts, and the hyperbolic representations help in achieving a low distortionof hierarchical information for better performance in downstream tasks.",
  "Experiments": "We conduct extensive experiments on several large-scale image benchmark datasets to evaluatethe performance of HypStructure as compared to the Flat and 2-CPCC baselines for hierarchyembedding, classification, and OOD detection tasks. Datasets and SetupFollowing the common benchmarks in the literature, we consider three real-world vision datasets, namely CIFAR10, CIFAR100 and ImageNet100 for training, whichvary in scale, number of classes, and number of images per class. We construct the ImageNet100dataset by sampling 100 classes from the ImageNet-1k dataset following . For CIFAR100, athree-level hierarchy is available with the dataset release . Since no hierarchy is available for theCIFAR10 and ImageNet100 datasets, we construct a hierarchy for CIFAR10 manually in .For ImageNet100, we create a subtree from the WordNet given 100 classes as leaves. Moredetails regarding the datasets, network, training and setup are provided in the Appendix B.4.",
  ": Evaluation of distor-tion vs feature dimensions forHypStructure": "First, to assess the tree-likeness of the learnt representations, we measure the Gromovs hyperbolicityrel of the features in . Lower rel indicates higher tree-likeness and a perfecttree metric space has rel = 0 (more details in Appendix B.5). To also evaluate the correspondenceof the feature distances with ground truth tree metrics, we compute CPCC on test sets. We observethat HypStructure reduces distortion of hierarchical information over Flat by upto 59.4% and over2-CPCC by upto 45.4%, while also consistently improving the test CPCC for most datasets. We also perform a qualitative analysis of the learnt representations from HypStructure on theCIFAR10 dataset, and visualize them in a Poincar disk using UMAP in a. We canobserve clearly that the samples for fine classes arrange themselves in the Poincar disk based on thehierarchy tree as seen in , being closer to the classes which share a coarse class parent. To examine the impact of feature dimension on the representative capacity of the hyperbolic space,we vary the feature dimension for HypStructure and compute the rel for each learnt feature.Comparing the distortion of features with the Flat and 2-CPCC settings in , we observe thatrel decreases consistently with increasing dimensions, implying that high dimension features usingHypStructure are more tree-like, and better than Flat and 2-CPCCs 512-dimension baselines.",
  "Classification": "Following Zeng et al. , we treat leaf nodes in the hierarchy as fine classes and their parent nodesas coarse classes. To evaluate the quality of the learnt representations, we perform a classificationtask on the fine and coarse classes using a kNN-classifier following and report theperformance on the three datasets in . We observe that HypStructure leads to upto 2.2%improvements over Flat and upto 0.8% improvements over 2-CPCC on both fine and coarse accuracy.We also visualize the learnt test features from Flat vs HypStructure on the CIFAR100 dataset usingEuclidean t-SNE and show the visualizations in b and c respectively. We observethat HypStructure leads to sharper and more discriminative representations in Euclidean space.Additionally, we see that the fine classes belonging to a coarse class (the same shades of colors)which are semantically closer in the label hierarchy, are grouped closer and more compactly in thefeature space as well, as compared to Flat. We also perform evaluations using the linear evaluationprotocol and observe an identical trend in the accuracy, we report these results in Appendix C.1.",
  "OOD Detection": "In addition to leveraging the hierarchy explicitly for the purpose of learning tree-like ID represen-tations, we argue that a structured separation of features in the hyperbolic space as enforced byHypStructure is helpful for the OOD detection task as well. To verify our claim, we perform anexhaustive evaluation on 9 real-world OOD datasets and demonstrate that HypStructure leads toimprovements in the OOD detection AUROC. We share more details below.",
  "the goal of the OOD detection task is to design a methodology that can solve a binary problem ofwhether an incoming sample x X is from PX i.e. y Yin (ID) or y / Yin (OOD)": "OOD datasets We evaluate on 5 OOD image datasets when CIFAR10 and CIFAR100 are used asthe ID datasets, namely SVHN , Places365 , Textures , LSUN , and iSUN ,and 4 large scale OOD test datasets, specifically SUN , Places365 , Textures andiNaturalist when ImageNet100 is used as the ID dataset. This subset of datasets is preparedby and is created with overlapping classes from ImageNet-1k removed from these datasets toensure there is no overlap in the distributions. OOD detection scores While several scores have been proposed for the task of OOD detection, weevaluate our proposed method using the Mahalanobis score , computed by estimating the meanand covariance of the in-distribution training features. The Mahalanobis score is defined as",
  "Main Results and Discussion": "We report the AUROC averaged over all the OOD datasets (5 datasets for CIFAR10 and CIFAR100,4 datasets for ImageNet100) in a and In addition to the Flat (SupCon) and 2-CPCCbaselines, we also compare our method with other state-of-the-art methods (see Appendix C.3.1 formore details about existing OOD detection methods). We observe that HypStructure leads to aconsistent improvement in the OOD detection score, with upto 2% in average AUROC. We alsoreport the dataset-wise OOD detection results for the CIFAR100 ID dataset in a along withAverage AUROC. To remove the bias in the Average AUROC metric towards any single dataset, wealso evaluate the Borda Count (B.C.) and report the same, along with a detailed comparison withmore OOD detection methods in a, and Tables 6 and 7 in the Appendix C.3. We observe that HypStructure ranks in the highest performing methods consistently, therebydemonstrating a higher Borda Count as well. We additionally visualize the CIFAR100 (ID) vs SVHN(OOD) features learnt from HypStructure, using a hyperbolic UMAP visualization in b.We observe that training with HypStructure leads to an improvement in the separation of ID vsOOD features in the Poincar disk. Additional Experiments, Ablations and Visualizations: More experiments using hyperboliccontrastive losses and hyperbolic networks, ablation studies on each component of HypStructureand additional visualizations can be found in Appendix C.",
  "Flat": "2-CPCC HypStructure : CIFAR100 as in-distribution dataset. Left (a): Hierarchical block pattern of K. Middle (b):Top 100 eigenvalues of K for different representation. Right (c): OOD detection for CIFAR100 vs.SVHN with the top k-th principal component. Naturally, we inspect the eigenvalue properties of (i.e, Z), and observe that K = ZZ Rnnexhibits a hierarchical block structure (a) where the diagonal blocks have a significantlyhigher correlation than other off-diagonal values, leading us to the following theorem. Theorem 5.1 (Eigenspectrum of Structured Representation with Balanced Label Tree). Let Tbe a balanced tree with height H, such that each level has Ch nodes, h [0, H]. Let us denote eachentry of K as rh where h is the height of the lowest common ancestor of the row and the columnsample. If rh 0, h, then: (i) For h = 0, we have C0 C1 eigenvalues 0 = 1 r1. (ii) For0 < h H 1, we have Ch Ch+1 eigenvalues h = h1 + (rh rh+1) C0",
  "Ch . (iii) The lasteigenvalue is H = H1 + C0rH": "We defer the eigenspectrum analysis for an arbitrary label tree to Appendix A. Theorem 5.1 impliesa phase transition pattern in the eigenspectrum. There always exists a significant gap in theeigenvalues representing each level of nodes in the hierarchy, and the eigenvalues correspondingto the coarsest level are the highest in magnitude. CIFAR100 has a balanced three-level labelhierarchy where each coarse label has five fine labels as its children. In b, we visualize theeigenspectrum of CIFAR100 for HypStructure, 2-CPCC and the Flat objective. We observe asignificant drop in the eigenvalues for features learnt from two hierarchical regularization approaches,2-CPCC and HypStructure, at approximately the 20th largest eigenvector (which corresponds tothe number of coarse classes), whereas these phase transitions do not appear for standard flat features.We also observe that the magnitude of coarse eigenvalues are approximately at the same scale. In summary, Theorem 5.1 helps us to formally characterize the difference between flat and structuredrepresentations. CPCC style (eq. (1)) regularization methods can also be treated as dimensionalityreduction techniques, where the structured features can be explained mostly by the coarser levelfeatures. For the OOD detection setting, this property differentiates the ID and OOD samples at thecoarse level itself using a lower number of dimensions, and makes the OOD detection task easier.We visualize the OOD detection AUROC on SVHN (OOD) corresponding to the CIFAR100 (ID)features with the topk principal component for different methods, in c. We observe thatfor features learnt using HypStructure, accurately embedding the hierarchical information leads tothe top 20 eigenvectors (corresponding to the coarse classes) being the most informative for OODdetection. Recall that CIDER is a state-of-the-art method proposed specifically for improvingOOD detection by increasing inter-class dispersion and intra-class compactness. We note that CPCCstyle (eq. (1)) methods can be seen as a generalization of CIDER on higher-level concepts, where thesame rules are applied for coarse labels as well, along with the fine classes. When the ID and OODdistributions are far enough, using coarse level feature might be sufficient for OOD detection.",
  "Related Work": "Learning with Label Hierarchy.Several recent works have explored how to leverage hierarchicalinformation between classes for various purposes such as relational consistency , designingspecific hierarchical classification architectures , hierarchical conditioning of the logits, learning order preserving embeddings , and improving classification accuracy [91, 86, 48, 50, 108, 34]. The proposed structural regularization framework in offers an interestingapproach to embed a tree metric to learn structured representations through an explicit objective term,although they rely on the 2 distance, which is less than ideal for learning hierarchies. Hyperbolic Geometry. first proposed using the hyperbolic space to learn hierarchical repre-sentations of symbolic data such as text and graphs by embedding them into a Poincar ball. Sincethen, the use of hyperbolic geometry has been explored in several different applications. Khrulkovet al. proposed a hyperbolic image embedding for few-shot learning and person re-identification. proposed hyperbolic neural network layers, enabling the development of hybrid architecturessuch as hyperbolic convolutional neural networks , graph convolutional networks , hyperbolicvariational autoencoders and hyperbolic attention networks . Additionally, these hybridarchitectures have also been explored for different tasks such as deep metric learning , objectdetection and natural language processing . There have also been several investigationsinto the properties of hyperbolic spaces and models such as low distortion , small generalizationerror and representation capacity . However, none of these works have leveraged hyperbolicgeometry for explicitly embedding a hierarchy in the representation space via structured regularization,and usually attempt to leverage the underlying hierarchy implicitly using hyperbolic models.",
  "Discussion and Future Work": "In this work, we introduce HypStructure, a simple-yet-effective structural regularization frameworkfor incorporating the label hierarchy into the representation space using hyperbolic geometry. Inparticular, we demonstrate that accurately embedding the hierarchical relationships leads to empiricalimprovements in both classification as well as the OOD detection tasks, while also learning hierarchy-informed features that are more interpretable and exhibit less distortion with respect to the labelhierarchy. We are also the first to formally characterize properties of hierarchy-informed features viaan eigenvalue analysis, and also relate it to the OOD detection task, to the best of our knowledge. Weacknowledge that our proposed method depends on the availability or construction of an externalhierarchy for computing the HypCPCC objective. If the hierarchy is unavailable or contains noise,this could present challenges. Therefore, it is important to evaluate how injecting noisy hierarchiesinto CPCC-based methods impacts downstream tasks. While the current work uses the Poincarball model, exploring the representation trade-offs and empirical performances using other modelsof hyperbolic geometry in HypStructure, such as the Lorentz model is an interesting futuredirection. Further theoretical investigation into establishing the error bounds of CPCC style structuredregularization objectives is of interest as well.",
  "Acknowledgement": "SZ and HZ are partially supported by an NSF IIS grant No. 2416897. HZ would like to thank thesupport from a Google Research Scholar Award. MY was supported by MEXT KAKENHI GrantNumber 24K03004. We would also like to thank the reviewers for their constructive feedback duringthe review process. The views and conclusions expressed in this paper are solely those of the authorsand do not necessarily reflect the official policies or positions of the supporting companies andgovernment agencies.",
  "J. W. Cannon, W. J. Floyd, R. Kenyon, W. R. Parry, et al. Hyperbolic geometry. Flavors ofgeometry, 31(59-115):2, 1997": "M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning ofvisual features by contrasting cluster assignments. Advances in neural information processingsystems, 33:99129924, 2020. I. Chami, A. Wolf, D.-C. Juan, F. Sala, S. Ravi, and C. R. Low-dimensional hyperbolicknowledge graph embeddings. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 69016914, 2020.",
  "M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in thewild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages 36063613, 2014": "E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learningaugmentation strategies from data. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 113123, 2019. E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le. Randaugment: Practical automated data aug-mentation with a reduced search space. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages1861318624. Curran Associates, Inc., 2020. URL",
  "J. Dai, Y. Wu, Z. Gao, and Y. Jia. A hyperbolic-to-hyperbolic graph convolutional network. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages154163, 2021": "J. Davis, T. Liang, J. Enouen, and R. Ilin. Hierarchical classification with confidence usinggeneralized logits. In 2020 25th International Conference on Pattern Recognition (ICPR),pages 18741881. IEEE, 2021. J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy, S. Bengio, Y. Li, H. Neven, and H. Adam.Large-scale object classification using label relation graphs. In D. Fleet, T. Pajdla, B. Schiele,and T. Tuytelaars, editors, Computer Vision ECCV 2014, pages 4864, Cham, 2014. SpringerInternational Publishing. A. Dhall, A. Makarova, O. Ganea, D. Pavllo, M. Greeff, and A. Krause. Hierarchical imageclassification using entailment cone embeddings. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition workshops, pages 836837, 2020. B. Dhingra, C. Shallue, M. Norouzi, A. Dai, and G. Dahl. Embedding text in hyperbolic spaces.In Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural LanguageProcessing (TextGraphs-12), pages 5969, 2018.",
  "D. Hendrycks, M. Mazeika, and T. Dietterich. Deep anomaly detection with outlier exposure.In International Conference on Learning Representations, 2019. URL": "D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan. AugMix:A simple data processing method to improve robustness and uncertainty. Proceedings of theInternational Conference on Learning Representations, 2020. R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, andY. Bengio. Learning deep representations by mutual information estimation and maximization.In International Conference on Learning Representations, 2019. URL D. T. Hoffmann, N. Behrmann, J. Gall, T. Brox, and M. Noroozi.Ranking info noisecontrastive estimation: Boosting contrastive learning via ranked positives. In AAAI Conferenceon Artificial Intelligence, 2022.",
  "E. Jonckheere, P. Lohsoonthorn, and F. Bonahon. Scaled gromov hyperbolic graphs. Journalof Graph Theory, 57(2):157180, 2008": "P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, andD. Krishnan. Supervised contrastive learning. In Advances in Neural Information ProcessingSystems, volume 33, pages 1866118673, 2020. V. Khrulkov, L. Mirvakhabova, E. Ustinova, I. Oseledets, and V. Lempitsky. Hyperbolic imageembeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 64186428, 2020.",
  "A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009": "C. Lang, A. Braun, L. Schillingmann, and A. Valada. On hyperbolic embeddings in objectdetection. In Pattern Recognition: 44th DAGM German Conference, DAGM GCPR 2022,Konstanz, Germany, September 2730, 2022, Proceedings, pages 462476. Springer, 2022. K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information ProcessingSystems, pages 71677177, 2018.",
  "J. Lei, Z. Guo, and Y. Wang. Weakly supervised image classification with coarse and finelabels. In 2017 14th Conference on Computer and Robot Vision (CRV), pages 240247. IEEE,2017": "L. Li, Y. Zhang, and S. Wang. The euclidean space is evil: hyperbolic attribute editing forfew-shot image generation. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 2271422724, 2023. L.-J. Li, C. Wang, Y. Lim, D. M. Blei, and L. Fei-Fei. Building and using a semantivisualimage hierarchy. In 2010 IEEE Computer Society Conference on Computer Vision and PatternRecognition, pages 33363343, 2010. doi: 10.1109/CVPR.2010.5540027.",
  "M. Nickel and D. Kiela. Poincar embeddings for learning hierarchical representations. InAdvances in Neural Information Processing Systems, pages 63386347, 2017": "M. Nickel and D. Kiela. Learning continuous hierarchies in the Lorentz model of hyperbolicgeometry. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conferenceon Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 37793788. PMLR, 1015 Jul 2018. URL K. T. Noor, A. Robles-Kelly, and B. Kusy. A capsule network for hierarchical multi-labelimage classification. In Joint IAPR International Workshops on Statistical Techniques inPattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages163172. Springer, 2022. F. Pinto, H. Yang, S.-N. Lim, P. Torr, and P. K. Dokania. Using mixup as a regularizer cansurprisingly improve accuracy & out-of-distribution robustness. In A. H. Oh, A. Agarwal,D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.URL",
  "E. Ravasz and A.-L. Barabsi. Hierarchical organization in complex networks. Physical reviewE, 67(2):026112, 2003": "J. Ren, P. J. Liu, E. Fertig, J. Snoek, R. Poplin, M. Depristo, J. Dillon, and B. Lakshmi-narayanan. Likelihood ratios for out-of-distribution detection. In Advances in Neural Informa-tion Processing Systems, pages 1468014691, 2019. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual RecognitionChallenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015. doi:10.1007/s11263-015-0816-y.",
  "J. Tack, S. Mo, J. Jeong, and J. Shin. Csi: Novelty detection via contrastive learning ondistributionally shifted instances. In Advances in Neural Information Processing Systems,2020": "F. Taherkhani, H. Kazemi, A. Dabouei, J. Dawson, and N. M. Nasrabadi. A weakly super-vised fine label classifier enhanced by coarse supervision. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 64596468, 2019. Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI16, pages 776794. Springer, 2020.",
  "H. Wei, R. Xie, H. Cheng, L. Feng, B. An, and Y. Li. Mitigating neural network overconfidencewith logit normalization. In ICML, 2022": "H. Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differential-gleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). MathematischeAnnalen, 71(4):441479, Dec. 1912. ISSN 1432-1807. doi: 10.1007/BF01456804. J. Winkens, R. Bunel, A. G. Roy, R. Stanforth, V. Natarajan, J. R. Ledsam, P. MacWilliams,P. Kohli, A. Karthikesalingam, S. Kohl, et al. Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020. Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametricinstance discrimination. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 37333742, 2018. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scenerecognition from abbey to zoo. In 2010 IEEE computer society conference on computer visionand pattern recognition, pages 34853492. IEEE, 2010.",
  "S. Zeng, R. T. des Combes, and H. Zhao. Learning structured representations by embeddingclass hierarchy. In The Eleventh International Conference on Learning Representations, 2022": "J. Zhang, N. Inkawhich, R. Linderman, Y. Chen, and H. Li. Mixture outlier exposure: Towardsout-of-distribution detection in fine-grained environments. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vision (WACV), pages 55315540, January2023. J. Zhang, J. Yang, P. Wang, H. Wang, Y. Lin, H. Zhang, Y. Sun, X. Du, K. Zhou, W. Zhang,et al. Openood v1. 5: Enhanced benchmark for out-of-distribution detection. arXiv preprintarXiv:2306.09301, 2023.",
  "ADetails of Eigenspectrum Analysis": "In this section, we first introduce some notations, discuss the setup for our analysis, followed by pre-liminary lemmas, and then characterize the eigenspectrum of CPCC-based structured representationsin Theorem A.2 for an arbitrary label tree and Theorem 5.1 for a balanced tree presented in the mainbody. Proof SketchThe proof of Theorem A.2 and Theorem 5.1 relies on the important observationof a hierarchical block structure of the covariance matrix of CPCC-regularized features, as shownin a, which will also be supported by Lemma A.1 and Corollary A.1. Theorem A.1 and Lemma A.2 characterize the eigenvalues of a block correlation matrix induced from a basictree where the matrix only has three types of values: diagonal values of 1s, one for within groupentry, and another for across group entry. Larger within group entries lead to the larger eigenvalues.Theorem A.1 and Lemma A.2 are then used as the base case for the induction proof of Theorem 5.1.For an arbitrary tree, in Theorem A.2, we use Weyls Theorem to bound the gap between withingroup entries and across group entries that leads to the phase transition of eigenvalues. Setup detailsAfter training with the HypStructure loss till convergence, let us denote the featurematrix as Z Rnd, where each row of Z is a d-dimensional vector of an in distribution trainingsample, and the CPCC is maximized to 1. We let C0 = n, C1, C2, . . . , CH = 1 be the numberof class labels at height h of the tree T . Following the standard pre-processing steps in OODdetection , we assume that the features are standardized and normalized so that E[Z] = 0 andZi2 = 1, i. Besides, we assume that in T , the distance from root node to each leaf node is thesame. Otherwise, following Santurkar et al. , we can insert dummy parents or children into thetree to make sure vertices at the same level have similar visual granularity. We then apply CPCC toeach node in the extended tree, where each leaf node is one sample. We note that although this isslightly different from the implementation where the leaf nodes are fine class nodes, the distance forsamples within fine classes are automatically minimized by classification loss like cross-entropy andsupervised contrastive loss.",
  "Given these assumptions, we want to analyze the eigenspectrum of the inverse sample covariancematrix1": "n1ZZ, which is the same as investigating the eigenvalues of K = ZZ where Z isordered by classes at all levels, i.e., samples having the same fine-grained labels and coarse labelsshould be placed together. This is because the matrix scaling and permutation will not change theorder of singular values. Since CPCC (eq. (1)) is a correlation coefficient, when it is maximized, the n by n pairwise Poincardistance matrix is perfectly correlated with the ground truth pairwise tree-metric matrix, where eachentry is the tree distance between two samples on the tree, no matter we apply CPCC to leaves or allvertices. This implies that in the similarity matrix K, the relative order of entries are the opposite oftree matrix, and it is trivial to show it as follows",
  "Corollary A.1. If we use the Poincar distance (eq. (3) in CPCC and let the curvature constant c = 1,the statement of cosine distance in Lemma A.1 still holds": "Proof. Since the Poincar distance (eq. (3)) is only defined for vectors with magnitude less than 1, letus consider the case where before the clipping operation, both u and v are outside the unit ball. Afterapplying clip1, u = v = 1 , where is a small constant (105). Then u2 = (1 )2 =1 2 + 2. Define 2 2 as , making u2 := 1 where is also a small constant such thatO(2) is negligible.",
  "= 2 ln 2 u v": "We can see that the Poincar distance monotonically increases with Euclidean distances u v.This property ensures the relative order of any two entries for Euclidean CPCC and Poincare CPCCmatrices in K to be the same. Then, we can argue about the structure of K, either Euclidean orPoincare, to have the hierarchical diagonalized structure as in a. So any statement applied fora Poincar version of CPCC will also hold for the Euclidean CPCC counterpart. For each level of the tree, due to the optimization of CPCC loss, the corresponding off diagonalentries of K, which represent the intra-level-class similarities, are much smaller than inter-level-classvalues. We thus have a symmetric similarity matrix that takes on the following structure, where thered regions are greater than orange regions, which are further greater than the blue regions.",
  ". . .. . .. . .. . .. . .. . .. . .. . .. .": "Each non-diagonal entry is called rhij where i, j are the index of the diagonal block, or the finest labelid of one sample, and h is the height of the lowest common ancestor of the two samples in the rowand the column. Since every two leaves sharing the lowest common ancestor of the same height havethe same tree distance, each entry of K with the same superscript will be the same so we can dropthe i, j subscript in the notation. The size of each block is defined by the number of samples withinone label. Then, the shown submatrix of K corresponds to the following tree in . Next, wepresent several useful lemmas and theorems.",
  "Thus, det(M I) = (1 + (d 1)p)(1 p)d1": "Notice that Lemma A.2 is a special case of Theorem A.1 where the label tree is a two level basic treewith the root node and d leaves in the second label all being the direct children of the root node. Nowwe can leverage Theorem A.1 and Lemma A.2 to investigate the eigenspectrum of K by proving thefollowing theorem:Theorem A.2 (Eigenspectrum of CPCC-based Structured Representation). If T is a tree whoseroot node has height H where each level has Ch nodes, h [0, H]. K = ZZ is a block structuredcorrelation matrix as a result of CPCC optimization, where each off-diagonal entry canbe written as rh and h is the height of the lowest common ancestor of the i-th row and the 00j-thcolumn sample. Let = r1 rh, pi, i [Ch] be the number of children for nodes at height h, andpmax be the maximum. For any h 1, if rh M 0, rh+1 m, then",
  "pmax(Ch1), C0 Ch eigenvalues are all smaller than Ch eigenvalues": "Proof. Part (i) and (ii) can be extended from the proof of Theorem A.1. Let G be the n Ch matrixwhere Gij = 1 if the i-th sample is in group j, otherwise Gij = 0. For any n Ch eigenvectors inthe orthogonal complement of the column space of G, the eigenvector of K is also the eigenvector of",
  "(pmax 1)(r1 rh)pmaxm pmaxmpmaxm(pmax 1)(r1 rh) pmaxm............pmaxmpmaxm (pmax 1)(r1 rh)": "The inequality comes from the effect of the maximization of CPCC that r1 rh rh+1 rHand rh m. The eigenvalues of A1, A2 have the analytical form, where A1s eigenvalues have theform of 1 + (pi 1)rh and A2s eigenvalues can be derived by Lemma A.2. By Weyls inequality, the minimum of these Ch eigenvalues is at least L = (1 + (pmin 1)rh) [(pmax 1) pmaxm + kpmaxm] (1 + (1 1)rh) [(pmax 1) pmaxm + kpmaxm].",
  "To guarantee eigenvalues from Part (ii) are larger, we want L U. We solve this inequality with m,and we will get the desired range of m": "When rh = r1 in Theorem A.2, we have = 0. Therefore, for a three level basic tree withonly r1, r2, if m M/(pmax(C1 1)), C0 C1 eigenvalues are all smaller than C1 eigenvalues.In general, we have shown that when m, i.e., the across group similarity is sufficiently small, theeigenvalue gap always exists. When the label tree T is balanced, we can further specify the expressionof each eigenvalue and the amount of eigenvalue gaps.",
  "We now formally restate the Theorem 4.1 from the main paper and give its proof": "Theorem 5.1 (Eigenspectrum of Structured Representation with Balanced Label Tree). Let Tbe a balanced tree with height H, such that each level has Ch nodes, h [0, H]. Let us denote eachentry of K as rh where h is the height of the lowest common ancestor of the row and the columnsample. If rh 0, h, then: (i) For h = 0, we have C0 C1 eigenvalues 0 = 1 r1. (ii) For0 < h H 1, we have Ch Ch+1 eigenvalues h = h1 + (rh rh+1) C0",
  "Since all statements are presented recursively, we prove the theorem by structural induction on theheight of the tree": "The base case is Lemma A.2 with a two level hierarchy tree where only (i) and (iii) are applicable,and p = r1, C0 = d, C1 = 1. By Lemma A.2, K has C0 1 eigenvalues as 0 = 1 r1, and oneeigenvalue as 1 = 1 + (C0 1)r1 = (1 r1) + r1/C10 .",
  "Let us now assume that the theorem is true for the balanced tree whose root node is at height H 1.Then if we have a tree with height H. We call the resulting matrix KH": "By the first bullet point of Theorem A.1 we directly get 0 from (i). Then by the second bullet pointof Theorem A.1, the rest of the eigenvalues are from the symmetric matrix AH1 RC1C1 whosediagonal elements are = 1 + (C0/C1 1)r1 and whose off diagonal elements are C0/C1 rj forj 2. The key is to observe that AH1 is still a block structured matrix. After AH1 is scaled by , theresulting matrix can be also seen as a result of maximizing CPCC where the off diagonal blocks havesmaller values.",
  "Therefore, we proved the theorem by showing the induction step from KH1 to KH holds": "Note that the true symmetric covariance matrix K might not be having the exact format as K, but itcan be seen as a perturbation of K where K K , is a small constant. By Weyls inequality, the approximation error of each eigenvalue is bounded by [i , i + ].",
  "B.1Broader Impact Statement": "Our work proposes HypStructure, a structured hyperbolic regularization approach to embed hi-erarchical information into the learnt representations. This provides significant advancements inunderstanding and utilizing hierarchical real-world data, particularly for tasks such as representationlearning, classification and OOD detection, and we recognize both positive societal impacts and po-tential risks of this work. The ability to better model hierarchical data in a structured and interpretablefashion is particularly helpful for domains such as AI for science and healthcare, where the learnt rep-resentations will be more reflective of the underlying relationships in the domain space. Additionally,the low-dimensional capabilities of hyperbolic geometry can lead to gains in computational efficiencyand reduce the carbon footprint in large scale machine learning. However, real-world hierarchicaldata often incorporates existing biases which may be amplified by structured representation learning,and hence it is important to incorporate fairness constraints to mitigate this risk.",
  "B.2Pseudocode for HypStructure": "The training scheme for our HypStructure based structured regularization framework is providedin Algorithm 1. At a high level, in HypStructure, we optimize a combination of the followingtwo losses: (1) a hyperbolic CPCC loss to encourage the representations in the hyperbolic space tocorrespond with the label hierarchy, (2) a hyperbolic centering loss to position the representationcorresponding to the root of the node at the centre of the Poincar ball and the children nodes aroundit.",
  "B.3Choice of Flat loss": "We use the Supervised Contrastive (SupCon) loss as the first choice for a flat loss in our experi-mentation. Let qy be the one-hot vector with the y-th index as 1. The Cross Entropy (CE) loss, definedbetween the predictions g f(x) and the labels y, as CE(g f(x), y) := i[k] qi log(g(f(x))i)has been used quite extensively in large-scale classification problems in the literature .However, several shortcoming of the CE loss, such as lack of robustness and poor gener-alization have been discovered in recent research. Contrastive learning has emerged as aviable alternative to the CE loss, to address these shortcomings . The underlyingprinciple for these methods is to pull together embeddings for positive pairs and push apart theembeddings for negative samples, in the feature space. In the absence of labels, positive samplesare created by data augmentations of images and negative samples are randomly chosen from theminibatch. However, when the labels are available, the class information can be leveraged to extendthis methodology as a Supervised Contrastive loss (SupCon) by pulling together embeddings fromthe same class, and pushing apart the embeddings from different classes. This offers a more stablesolution for a variety of tasks .Definition B.1 (SupCon Loss). Given a training sample xi, feature encoder f() and a projectionhead h(), we denote the normalized feature representations from the projection head as:",
  "Nk=1 1(k = i)euTi uk/,(11)": "where Nyi refers to the number of images with label yi in the batch, is the temperature param-eter, refers to the inner product, and ui and uk are the normalized feature representations usingEquation (10) for xi and xk respectively. While the numerator in the formulation in Equation (11) only considers the samples (and its augmen-tations) belonging to the same class, the denominator sums over all the negatives as well. Overall,this encourages the network to closely align the feature representations for all the samples belongingto the same class, while pushing apart the representations of samples across different classes. We note that our proposed method HypStructure is not limited to the choice of euclidean classifica-tion losses as Flat and we report additional results with hyperbolic classification losses in SectionsC.8 and C.9 respectively, demonstrating the wide applicability of our approach.",
  "B.4.2Architecture, Hyperparameters and Training": "We use the ResNet-18 network as the backbone for CIFAR10, and ResNet-34 as the backbonefor CIFAR100 and ImageNet100 datasets. We use a ReLU activated multi layer perceptron withone hidden layer as the projection head h(.) where its hidden layer dimension is the same as inputdimension size and the output dimension is 128. We follow the original hyperparameter settingsfrom for training the CIFAR10 and CIFAR100 models from scratch with a temperature = 0.1,feature dimension 512, and training for 500 epochs with an initial learning rate of 0.5 with cosineannealing, optimizing using SGD with momentum 0.9 and weight decay 104, and a batch size of512 for all the experiments. For ImageNet100, we finetune the ResNet-34 for 20 epochs following with an initial learning rate of 0.01 and update the weights of the last residual block and thenonlinear projection head, while freezing the parameters in the first three residual blocks. We use thesame values as the regularization parameters for the CPCC loss in Equation (2) (2-CPCC) and inEquation (8) (our proposed method HypStructure) for a fair comparison and find that the defaultregularization hyperparameter for the CPCC loss = 1.0 for both 2-CPCC and HypStructureperforms well for the experiments on the CIFAR10 and CIFAR100 datasets. We observe that theexperiments on the IMAGENET100 dataset benefit from a lower = 0.5. Additionally, we set thehyperparameter for the centering loss in our methodology as = 0.01 for all the experiments. Weuse the default curvature value of c = 1.0 for the mapping and distance computations in the Poincarball.",
  ". CIFAR100(). It also consists of 50,000 training images and 10,000 test images, howeverthe images belong to 100 classes. Note that the classes are not identical to the CIFAR10dataset": "3. ImageNet100(). This dataset is created as a subset of the large-scale ImageNet datasetfollowing Ming et al. . The original ImageNet dataset consists of 1,000 classes and1.2 million training images and 50,000 validation images. We construct the ImageNet100dataset from this original dataset by sampling 100 classes, which results in 128,241 trainingimages and 5000 validation images. We mention the specific classes used for samplingbelow. Following , we use the below 100 class ids for creating the ImageNet100 subset: n03877845,n03000684, n03110669, n03710721, n02825657, n02113186, n01817953, n04239074, n02002556,n04356056, n03187595, n03355925, n03125729, n02058221, n01580077, n03016953, n02843684,n04371430, n01944390, n03887697, n04037443, n02493793, n01518878, n03840681, n04179913,n01871265, n03866082, n03180011, n01910747, n03388549, n03908714, n01855032, n02134084,n03400231, n04483307, n03721384, n02033041, n01775062, n02808304, n13052670, n01601694,n04136333, n03272562, n03895866, n03995372, n06785654, n02111889, n03447721, n03666591,n04376876, n03929855, n02128757, n02326432, n07614500, n01695060, n02484975, n02105412,n04090263, n03127925, n04550184, n04606251, n02488702, n03404251, n03633091, n02091635,n03457902, n02233338, n02483362, n04461696, n02871525, n01689811, n01498041, n02107312,n01632458, n03394916, n04147183, n04418357, n03218198, n01917289, n02102318, n02088364,n09835506, n02095570, n03982430, n04041544, n04562935, n03933933, n01843065, n02128925,n02480495, n03425413, n03935335, n02971356, n02124075, n07714571, n03133878, n02097130,n02113799, n09399592, n03594945. In addition to the training and validation images, we also require the label hierarchy for each of thesedatasets for the CPCC computation in 2-CPCC and HypStructure approaches. For CIFAR100, weuse the three-level hierarchy provided with the dataset release3. We show this hierarchy in ,where the top-level is the root of the tree.",
  "Coarse ClassesFine Classes": "aquatic mammalsbeaver, dolphin, otter, seal, whalefishaquarium fish, flatfish, ray, shark, troutflowersorchids, poppies, roses, sunflowers, tulipsfood containersbottles, bowls, cans, cups, platesfruit and vegetablesapples, mushrooms, oranges, pears, sweet peppershousehold electrical devicesclock, computer keyboard, lamp, telephone, televisionhousehold furniturebed, chair, couch, table, wardrobeinsectsbee, beetle, butterfly, caterpillar, cockroachlarge carnivoresbear, leopard, lion, tiger, wolflarge man-made outdoor thingsbridge, castle, house, road, skyscraperlarge natural outdoor scenescloud, forest, mountain, plain, sealarge omnivores and herbivorescamel, cattle, chimpanzee, elephant, kangaroomedium-sized mammalsfox, porcupine, possum, raccoon, skunknon-insect invertebratescrab, lobster, snail, spider, wormpeoplebaby, boy, girl, man, womanreptilescrocodile, dinosaur, lizard, snake, turtlesmall mammalshamster, mouse, rabbit, shrew, squirreltreesmaple, oak, palm, pine, willowvehicles 1bicycle, bus, motorcycle, pickup truck, trainvehicles 2lawn-mower, rocket, streetcar, tank, tractor Since no hierarchy is available for the CIFAR10 and ImageNet100 datasets, we construct a hierarchyfor CIFAR10 manually, as seen in . For ImageNet100, we create a subtree from the WordNet4 hierarchy, given the 100 aforementioned classes as leaves. We consider the classes which are onelevel above the leaf nodes in the hierarchy as the coarse classes, following Zeng et al. . For the task of OOD detection, we use the following five diverse OOD datasets for CIFAR10 andCIFAR100 as ID datasets, following the literature : SVHN , Textures , Places365 ,LSUN and iSUN . When ImageNet100 is used as the ID dataset, we use 4 diverse OODdatasets as the ones in , namely subsets of iNaturalist , SUN , Places andTextures . These datasets have been processed so that there is no overlap with the ImageNetclasses.",
  "(d(x, y) + d(x, z) d(y, z))": "Then, -hyperbolicity can be defined as the minimum value of such that for any four pointsx, y, z, w X, the following condition holds:(x, z)w min((x, y)w, (y, z)w) It can be shown that equivalently, there exists a geometric definition of -hyperbolicity. A geodesictriangle in X is -slim if each of its side is contained in the -neighbourhood of the union of the othertwo sides. We define -hyperbolicity as the minimum that guarantees any triangle in X is -slim.From , when the curvature of the surface increases, the geodesic triangle converges to atree/star graph, and gradually reduces to 0.",
  "C.1Results using Linear Evaluation": "We also perform an evaluation of the fine classification accuracy following the common linearevaluation protocol Khosla et al. where a linear classifier is trained on top of the normalizedpenultimate layer features. We report these accuracies for the models trained on the CIFAR10 andCIFAR100 datasets in for the leaf-only variants of the models. We observe that the relativetrend of accuracies is identical to the ones reported using the kNN evaluation in and ourproposed method HypStructure outperforms the flat and 2-CPCC methods on both the datasets.",
  "C.2Component-wise Ablation Study of HypStructure": "To understand the role of each component in our proposed methodology HypStructure, we performa detailed ablation study with the different components and measure the fine and the coarse accuracieson the CIFAR100 dataset. Specifically, we examine 1. the role of embedding all internal nodes in the label hierarchy (eq. (8) and line 10 inAlgorithm 1), as opposed to only using leaf nodes as in Zeng et al. . We refer to theinclusion of internal nodes as Tint. 2. the role of hyperbolic class centroids computation using hyperbolic averaging (eq. (6) andline 8 in Algorithm 1), as opposed to the Euclidean computation of class prototypes as inZeng et al. . We refer to the hyperbolic class centroid computation as hyp. 3. the role of the hyperbolic centering loss in our proposed methodology (eq. (8) and line 11in Algorithm 1), as opposed to not using a centering loss. We refer to the inclusion of thecentering loss as center. We ablate over the aforementioned settings, where a denotes the inclusion of that setting, and reportthe results on the CIFAR100 dataset in . Firstly, we observe that while the centering loss centerimproves the coarse accuracy only by a small increment, it leads to a significant improvement in thefine accuracy (rows 1 2 and 4 5), indicating that the centering of the root in the poincare diskallows for a better relative positioning of the fine classes within the coarse class groups. Secondly,we observe that both the inclusion of internal nodes Tint, and the hyperbolic computation of theclass centroids hyp is critical for accurately embedding the hierarchy, and removing either of thesecomponents (i.e. rows 5 3 for Tint and rows 5 2 for hyp), leads to a degradation in both thefine as well as the coarse accuracies. The best overall performance is observed when all three of thecomponents are included (row 5).",
  "C.3.1Related Work and Methods": "The goal of prior works in the OOD literature is the supervised setting of learning an accurate classifierfor ID data, along with an ID-OOD detection methodology and this task has been explored in thegenerative model setting , and more extensively in the supervised discriminativemodel setting . The methods in this setting can be categorized into foursub-categories following , primarily:",
  "HypStructure (Ours)92.2190.1297.3395.6193.83": "methods for CIFAR10 and CIFAR100, namely ProxyAnchor , SimCLR CSI , and CIDER respectively. Results for these methods are taken from CIDER where contrastive learningbased OOD detection methods typically outperforms non-contrastive learning ones. For ImageNet100,in the absence of the available class ids used to train the original models in CIDER , we finetunethe ResNet34 models on the created ImageNet100 dataset. For CIDER and SupCon, we use theofficial implementations and hyperparameters provided by the authors. We observe that our proposed method leads to an improvement in the average OOD detectionAUROC over all the ID datasets. In practice, we find that the Euclidean-centroid computationalvariant (first compute the Euclidean centroids and then apply the exponential map) of our proposedmethod performs slightly better than the hyperbolic-centroid computational variant (first apply theexponential map and then compute the hyperbolic average), for the specific task of OOD detection,while having equivalent performance on the ID classification task. Hence, we report the OODdetection accuracy corresponding to the first version.",
  "C.5Effect of Centering Loss and Embedding Internal Node": "Embedding the internal tree nodes in HypStructure Tint (as compared to only leaf nodes in priorwork CPCC) and placing the root node at the center of the Poincar disk with center loss, helps inembedding the hierarchy more accurately. To understand the impact of these components, we firstvisualize the learnt representations from HypStructure, with and without these components - i.e.embedding internal nodes and a centering loss vs leaf only nodes, via UMAP in (CIFAR100)and (ImageNet100). We also provide a performance comparison (fine accuracy) in .",
  ": Fine accuracy comparison of HypStructure with vs. without internal nodes and centeringon CIFAR10, CIFAR100, and ImageNet100 datasets": "First, based on Figures and , one can note that in the leaf-only setting withoutembedding internal nodes and centering loss (figures on the left), the samples belonging to the fineclasses which share the same parent (same color) are in close proximity reflecting the hierarchyaccurately, however the samples are not spread evenly. With the embedding of internal nodes anda centering loss (right), we note that the representations are spread between the center (root) to theboundary as well as across the Poincar disk, which is more representative of the original hierarchy.This also leads to performance improvements as can be seen in .",
  "C.6Effect of Label Hierarchy Weights": "Compared to ranking-based hyperbolic losses , our HypCPCC factors in absolute values of thenode-to-node distances. The learned hierarchy with HypCPCC will not only implicitly encode thecorrect parent-child relations, but can also learn more complex and weighted hierarchical relationshipsmore accurately. To demonstrate this, we modify the CIFAR10 tree hierarchy, and gradually increasethe weight for the left transportation branch to 2 and 4 and use new weighted trees for the CPCCtree distance computation. We visualize the learnt representations in and we can observethat in the learned representations from left to right, the distance between the transportation classes(blue) are larger as compared to other classes, as expected.",
  "C.8Experiments with the Hyperbolic Supervised Contrastive Loss": "We experiment with the Hyperbolic Supervised Contrastive Loss as proposed in as the choiceof the Flat loss and refer to this loss as HypSupCon. We follow the original setup as described bythe authors for the measurement of the HypSupCon, where the representations from the encodersare not normalized directly, instead an exponential map is used to project these features from theEuclidean space to the Poincar ball first. Then, the inner product measurement in the SupCon isreplaced with the negative hyperbolic distances in the Poincar ball to compute the HypSupCon loss.We also experiment with our proposed methodology HypStructure along with the HypSupCon lossand report the classification accuracies and hierarchy embedding metrics for both these settingsin . We further report the OOD detection performance on CIFAR10, CIFAR100 andImageNet100 as in-distribution datasets for both of these settings in Tables 11, 12 and 13 respectively.We observe that using HypStructure with a hyperbolic loss such as HypSupCon as the Flat loss leadsto improvements in accuracy across classification and OOD detection tasks while also improvingthe quality of embedding the hierarchy. This demonstrates the wide applicability of our proposedmethod HypStructure which can be used in conjunction with both euclidean and non-euclideanclassification losses.",
  "C.9Experiments with a Hyperbolic Backbone": "We experiment with Clipped Hyperbolic Neural Networks (HNNs) as a hyperbolic backboneand use our proposed methodology HypStructure in conjunction with the hyperbolic MultinomialLogistic Regression (MLR) loss. We report the classification accuracies and hierarchy embeddingmetrics on the CIFAR10 and CIFAR100 datasets in , and the OOD detection performancesusing CIFAR10 and CIFAR100 as in-distribution datasets in Tables 15 and 16 respectively. We observethat using HypStructure along with a hyperbolic backbone leads to improvements in classificationaccuracies, reduced distortion in embedding the hierarchy, and improved OOD detection performanceoverall, demonstrating the wide applicability of HypStructure with hyperbolic networks.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: Yes, the paper discloses all the necessary details about the implementedarchitectures used, hyperparameters for each setting, algorithm pseudocode and otherexperimental details to reproduce all the experiments in the paper. These details can befound in Section B in the Appendix.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "The answer NA means that the paper does not include experiments": "The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Justification: We discuss the potential positive and negative societal impacts of our work inSection B.1 of the Appendix.Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: The paper uses publicly available datasets, and does not release any data orcode that have a risk of misuse.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  "The authors should state which version of the asset is used and, if possible, include aURL": "The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}