{
  "Abstract": "For classification models based on neural networks, the maximum predicted classprobability is often used as a confidence score. This score rarely predicts well theprobability of making a correct prediction and requires a post-processing calibrationstep. However, many confidence calibration methods fail for problems with manyclasses. To address this issue, we transform the problem of calibrating a multiclassclassifier into calibrating a single surrogate binary classifier. This approach allowsfor more efficient use of standard calibration methods. We evaluate our approachon numerous neural networks used for image or text classification and show that itsignificantly enhances existing calibration methods. Our code can be accessed atthe following link:",
  "Introduction": "The considerable performance increase of modern deep neural networks (DNNs) and their potentialdeployment in real-world applications has made reliably estimating the probability of wrong decisionsa key concern. When such components are expected to be embedded in safety-critical systems (e.g.,medical or transportation), estimating this probability is crucial to mitigate catastrophic behavior. Oneway to address this question is to treat it as an uncertainty quantification problem , where theuncertainty value computed for each prediction is considered as a confidence. This confidence can beused to reject uncertain decisions proposed by the DNN , for out-of-distribution detection , orto control active learning or reinforcement learning based systems . When confidence valuesreliably reflect the true probability of correct decisions, i.e., their accuracy, a predictive system is saidto be calibrated. In this case, confidence values can be used as a reliable control for decision-making. We are interested in producing an uncertainty indicator for decision problems where the input is highdimensional and the decision space large, typically classifiers with tens to thousands of classes. Forthis kind of problem, DNNs are common predictors, and their outputs can be used to provide anuncertainty value at no cost, i.e., without necessitating heavy estimation such as Bayesian sampling or ensemble methods . Indeed, most neural architectures for classification instantiate theirdecision as a softmax layer, where the maximum value can be interpreted as the maximum of theposterior probability and, therefore, as a confidence. Unfortunately, uncertainty values computed inthis way are often miscalibrated. DNNs have been shown to be over-confident , meaning theirconfidence is higher than their accuracy: predictions with 90% confidence might be correct only 80%of the time. A later study suggests that model architecture impacts calibration more than modelsize, pre-training, and accuracy. For ImageNet classifiers, the accuracy and the number of modelparameters are not correlated to calibration, but model families are . These studies show that it is difficult to anticipate the calibration level of confidence values computeddirectly from DNNs and exhibit the benefits of a complementary post-processing calibration. Thiscalibration process can be seen as a learning step that exploits data from a calibration set, distinctfrom the training set, and is used to learn a function that maps classifier outputs into better-calibratedvalues. This process is typically lightweight and decoupled from the issue of improving model",
  "performance. A standard baseline for post-processing calibration is Temperature Scaling , wherethe penultimate logit layer is scaled by a coefficient optimized on the calibration set": "Many post-processing calibration methods have been developed for binary classification models. Applying these methods to multiclass classifiers requires some adaptation. One standardapproach reformulates the multiclass setting into many One-versus-All binary problems (one perclass) . One limitation of this approach is that it does not scale well. When the number of classesis large, the calibration data is divided into highly unbalanced subsets that do not contain enoughpositive examples to solve the One-versus-All binary problems. Other methods based on Platt scaling involve learning a set of parameters whose size grows with the number of classes. For problemswith many classes, they tend to overfit, as we demonstrate in this work. The main idea of our work is to reformulate the multiclass confidence estimation into a single binaryproblem. This problem can be phrased as the unique question: \"Is the prediction correct?\". In thisformulation, the confidence score is defined as the maximum class probability of the binary problemthat outputs 1 if the predicted class is correct and 0 otherwise. The intent is that the confidencescore accurately describes whether the prediction is correct, regardless of the class. We show thatthis novel approach, which we call Top-versus-All (TvA), significantly improves the performanceof standard calibration methods: Temperature and Vector Scaling , Dirichlet Calibration ,Histogram Binning , Isotonic Regression , Beta Calibration , and Bayesian Binning intoQuantiles . We also introduce a simple regularization for Vector Scaling or Dirichlet Calibrationthat mitigates overfitting when the number of classes is high relative to the calibration data size. Weconduct experiments on multiple image and text classification datasets and many pre-trained models.",
  "We discuss four issues of the standard approach to confidence calibration": "To solve these issues, we develop the Top-versus-All approach to confidence calibration ofmulticlass classifiers, transforming the problem into a single binary classifiers calibration.This straightforward reformulation enables more efficient use of existing calibration methods,achieved with minimal modifications to the methods original algorithms. Applied to scaling methods for calibration (such as Temperature Scaling), TvA allows theuse of the binary cross-entropy loss, which is more efficient in decreasing the confidence ofwrong predictions and leads to stronger gradients in the case of Temperature Scaling.Applied to binary methods for calibration (such as Histogram Binning), TvA significantlyimproves their performance and makes them accuracy-preserving. We demonstrate our approachs scalability and generality with extensive experiments on im-age classification with state-of-the-art models for complex datasets and on text classificationwith Pre-trained Language Models (PLMs) and Large Language Models (LLMs).",
  "Related work": "CalibrationThere are various notions of multiclass calibration. One can consider confidence ,class-wise , top-r , top-label , decision , projection smooth , or strong calibration. For recent surveys, we refer to and . In this work, we focus on confidencecalibration and not on the calibration of the full probability vector. Indeed, confidence calibration isuseful for many applications that only require a single confidence value: selective classification ,out-of-distribution detection , or active learning . For these applications, stronger notions ofcalibration are both difficult and useless. Also, class-wise calibration metrics do not appropriatelyscale to large numbers of classes, a setting we consider in this work, as explained in Appendix E. MetricsSeveral metrics have been proposed to quantify calibration error. The most common is theExpected Calibration Error (ECE) (see Equation 2). ECE has flaws: the estimation quality isinfluenced by the binning scheme, and it is not a proper scoring rule . Despite its flaws, itremains the standard comparison metric for confidence calibration. Variants of ECE have also beendeveloped: classwise-ECE , ECE with equal mass bins , or top-label-ECE, which adds aconditioning on the predicted class . The Brier score is also used to measure calibration. Theproximity-informed expected calibration error (PIECE) evaluates the miscalibration due to proximitybias . We mainly use the standard ECE in this work, and the Appendix contains more metrics. Training calibrated networksSeveral solutions have been proposed in the literature to improvecalibration by training neural networks in specific ways, generally by making use of a new loss term. While these methods directly optimize calibration during the training phase of thenetworks, they require a high development time, often compromise accuracy, and are not adapted topre-trained foundation models. That is why we prefer to focus on calibrating already-trained models. Post-processing (or post-hoc) calibration methodsAnother approach is to calibrate already-trained models. This lowers the development time by decoupling accuracy optimization and calibra-tion. In this paper, we divide post-hoc calibration methods into two categories: scaling and binary.Scaling methods are derived from Platt scaling and optimize some parameters to scale the logits.Temperature Scaling is a popular simple post-processing calibration method. The logits vector isscaled by a coefficient, which modifies the probability vector. Vector Scaling is more expressiveand has good performance in many cases . Matrix Scaling can also be considered for moreexpressiveness but is difficult to apply without overfitting . Dirichlet Calibration proposes aregularization strategy for Matrix Scaling. developed Ensemble Temperature Scaling. Scalingcan be combined with binning . Besides logits or probabilities, features can also be used .Another family of methods tackles binary classification. We designate them as binary methods.Histogram Binning divides the prediction into B bins according to the predicted probability. Foreach bin, a calibrated probability is computed from the calibration data. The probability becomesdiscrete: it can only take B values. With some modifications, it outperforms scaling methods .Isotonic Regression learns a piecewise constant function to remap probabilities. BayesianBinning into Quantiles brings Bayesian model averaging to Histogram Binning. Beta Calibration uses a beta distribution to obtain a calibration mapping.Our work reformulates the multiclass calibration problem and allows more efficient use of all thesecalibration methods, with little to no change in their algorithms. Multiclass to BinaryUsing binary calibration methods for a multiclass classifier requires adaptingthe multiclass setting. This is usually done with a One-versus-All approach . The multiclasssetting is decomposed into L One-versus-All independent problems: one binary problem for eachclass. introduce the notion of top-label calibration, i.e., confidence calibration with additionalconditioning on the predicted class (top-label). They describe a general multiclass-to-binary frame-work to develop top-label calibrators. derive L(L 1)/2 pairwise binary problems. The approachrequires training the classifier from scratch, and its performance decreases with the number of classes.Our work tackles this multiclass-to-binary research problem. Contrary to , the One-versus-Allapproach and top-label calibrators , our approach works well for problems with many classes.The methods I-Max and IRM use a shared class-wise strategy to compute a single calibrator.The calibrator is applied to all class probabilities separately, so the class probabilities ranking andprediction might change. In contrast, TvA applied to binary methods rescales the confidence after theclass prediction is made. I-Max and IRM consider the full class probabilities vectors, while TvA onlyconsiders confidence values. Also, they build on top of Histogram Binning and Isotonic Regression,respectively, while we apply our approach to many calibration methods. A concurrent work buildingon an intuition similar to ours derives a calibration method based on a Correctness-Aware Loss .Appendix C discusses the differences between our approach and others.",
  "Background": "Confidence calibration of a classifierWe consider the classification problem where an input xis associated with a class label y Y = {1, 2, ..., L}. The neural network classifier f provides aclass prediction from a final softmax layer that transforms intermediate logits z into probabilities.The classifier prediction is the most probable class y = arg maxkY fk(x) with fk(x) referring tothe probability of class k, and the confidence score defined as s = maxkY fk(x). Note that we usethe term confidence to denote the maximum class probability. With y the real label, we consider theconfidence calibration definition from that says that the classifier f is calibrated if:",
  "P(y = y|s = p) = p,p (1)": "where the probability is over the data distribution. Equation (1) expresses that the probability of beingcorrect when the confidence is around p is indeed p. For instance, if we consider the set of predictions with a confidence of 90%, they should be correct 90% of the time. The conditional probability of (1)is not rigorously defined mathematically (the event {s = p} has zero probability), and interval-basedempirical estimators are often used to define metrics capable of evaluating how well (1) is satisfied.This is the case of ECE, which approximates the calibration error by partitioning the confidencedistribution into B bins. The absolute difference between the accuracy and confidence is computedfor each subset of data in the bins. The final value is a weighted sum of the differences of each bin.",
  "nbN |acc(b) conf(b)|(2)": "Where nb is the number of samples in bin b, N is the total number of samples, acc(b) is the accuracyin bin b, and conf(b) is the average confidence in bin b. ECE can be interpreted visually by looking atdiagrams such as those of : ECE computes the sum of the red bars (difference between binaccuracy and average confidence) weighted by the proportion of samples in the bin. Post-processing calibration methodsWe are considering the scenario where a classifier hasalready been trained, and the objective is to enhance its calibration. Post-processing calibrationmethods aim to remap the classifier probabilities to better-calibrated values without modifying theclassifier. They typically use a calibration set different from the training set to optimize parameters orlearn a function. We note the calibration data Dcal = {(xi, yi)}Ni=1. We focus on post-processingcalibration because it enables better utilization of off-the-shelf models and separates model training(optimized for accuracy) from calibration. These advantages significantly reduce the developmentcost of obtaining a well-performing and well-calibrated model, contrary to optimizing calibrationduring training. We categorize the post-processing calibration techniques considered in this paperinto two groups: scaling methods and binary methods.",
  "Issues related to current approaches": "Behavior of current scaling methodsScaling methods for calibration optimize one or morecoefficients that scale the logits vector to minimize on calibration data the cross-entropy loss definedas lCE = Lk=1 1k=y log(fk(x)) = log(fy(x)). Minimizing lCE therefore increases theprobability of the true class. We can distinguish two cases to understand what happens during theoptimization: whether the prediction y is correct or not. In the first case, the confidence score iss = fy(x): minimizing lCE increases the confidence fy(x). In the second case, the prediction isincorrect, which implies that fy(x) < s. Minimizing lCE increases the probability of the true classfy(x) but does not directly change the confidence (because s = fy(x)). Instead, the confidence(which was attributed to a wrong class) is indirectly lowered through the softmax normalization.",
  ". Issue 2: Vector Scaling and Dirichlet Calibration overfit calibration sets with many classes": "One-versus-All approach for binary methodsThe One-versus-All (OvA) calibration approach allows adapting calibration methods for binary classifiers to multiclass classifiers. To do so, itdecomposes the calibration of multiclass classifiers into sets of L binary calibration problems: onefor each class k. For each problem, the considered probability is fk(x), and the associated label1y=k {0, 1}. When calibrating a classifier from data, each binary problem is highly imbalancedwith a ratio between positive and negative examples equal to1",
  ". Issue 3: OvA approach leads to highly imbalanced binary problems": "At test time, each of the L class probabilities is calibrated by a separate calibration model. Theresulting probability vector can be normalized to ensure a unit norm. Because each probability iscalibrated independently, their ranking can change, thus modifying the predicted class. In ,we see that accuracy is often negatively impacted in practice.",
  "General presentation": "In the calibration definition (1) and the standard ECE metrics, only the confidence, i.e., the maximalprobability, reflects the likelihood of making an accurate prediction. The probabilities of other classesare not taken into account. However, the standard approach to calibration uses the entire set ofprobabilities, not just confidence, which introduces unnecessary complexity. We aim to simplify theprocess by reformulating the problem of calibrating multiclass classifiers into a single binary problem.This problem can be phrased as: \"Is the prediction correct?\". In this setting, we do not calibratethe predicted probabilities vector but only a scalar: the confidence. The remaining probabilities arediscarded. This is equivalent to calibrating a surrogate binary classifier that predicts whether theclass prediction is correct. Since this correctness classifier only considers the maximal probabilityversus all others, we call our approach Top-versus-All (TvA). Replacing the standard approach by TvA is straightforward. Given the standard calibration dataDcal = {(xi, yi)}Ni=1, we add a few data preprocessing steps.First, compute the class pre-dictions y and their correctness: yb = 1y=y.Second, create the surrogate binary classifierf b(x) = maxkY fk(x). Finally, build the calibration set for the surrogate binary classifier:",
  "DTvAcal = {(xi, ybi )}Ni=1(3)": "After this preprocessing, we choose a standard calibration function g, e.g., Temperature Scaling, tocalibrate the surrogate binary classifier. The learning of the calibration function follows its originalunderlying algorithm but uses the modified calibration data DTvAcal. The learned calibration functionis then applied to the confidences of the original multiclass classifier. Algorithm 1 describes ourapproach. In the Appendix, Algorithm 3 provides more details and highlights differences with thestandard approach of Algorithm 2.",
  "lBCE = yb log s + (1 yb) log(1 s)(4)": "Minimizing this loss results in confidence estimates that more accurately describe the probability ofbeing correct, regardless of the L 1 less likely class predictions. Using the binary cross-entropy asa calibration loss makes an important difference compared to the usual multiclass cross-entropy. Thecross-entropy loss takes into account the probability of the correct class, while with TvA the binarycross-entropy takes into account the probability of the predicted class (i.e., the confidence). As for the standard approach, only two cases are possible. When the prediction is correct, lBCE = log(s) = log(fy) = lCE. We get the same result as the cross-entropy loss: minimizing itdirectly increases the confidence. But when the prediction is incorrect, lBCE = log(1 s) = lCE.Minimizing the loss now directly decreases the confidence. This is a key difference compared tousing the multiclass cross-entropy loss.The impact of the reformulation can be seen for Temperature Scaling, which optimizes a coefficientT that scales the logits zk. The reformulation generates stronger gradients when the prediction isincorrect:lBCE",
  "T=1T 2 (zy": "k zk fk). See AppendixD for the mathematical calculations. Because for interesting problems, the confidence verifies s > 0.5most of the time (as shown in ), our approach strengthens the gradients. The optimizationof the temperature T is more efficient as confident incorrect predictions are more heavily penalized.This effect is not mitigated by the choice of learning rate, which does not vary with s. Applyingstandard Temperature Scaling usually results in overconfident probabilities, but our approach limitsthis overconfidence. This is verified experimentally in , which displays the average confidencesfor TS without and with TvA.",
  "i=1(vi 1)2(6)": "This regularization allows these methods to take advantage of their additional expressiveness withoutbeing subject to overfitting. The loss for Vector Scaling becomes lBCE + lreg(v) where is ahyperparameter. The loss for Dirichlet Calibration uses additional matrix regularization terms. isthe only additional hyperparameter introduced by our method, and it applies only to Vector Scalingand Dirichlet Calibration.",
  "Top-versus-All approach for binary methods": "Our TvA approach replaces the One-versus-All approach to apply binary methods to the multiclasssetting. TvA transforms the multiclass setting into a single binary problem that uses the binarycalibration dataset (3). In this dataset, the proportion of positive labels equals the classifiers accuracya. The ratio between negative and positive examples is (1a)N",
  ". Solution for Issue 3: By not dividing the calibration data into class-wise datasets, the TvAapproach yields a much better balanced binary calibration problem": "The Top-versus-All approach operates on confidence alone, not the full class probabilities vector.This means that the class prediction is already done, and the ranking of the class probabilities doesnot change, unlike with One-versus-All. The classifiers prediction and accuracy are unaffected. Thisscheme allows decoupling accuracy improvements (during training time) and calibration (duringpost-processing calibration), thus avoiding compromises and reducing development time.",
  "(h) ViT-B/16 HBTvA": ": Reliability diagrams for ResNet-50 and ViT-B/16 when using Temperature Scaling (TS),Vector Scaling (VS), and Histogram Binning (HB) on ImageNet. The subscript TvA signifies that theTvA reformulation was used, and reg means our regularization (6) was applied. Red bars show thedifferences between bin accuracy (blue bar) and accuracy for perfect calibration (dashed red line).As the methods improve the calibration, these differences are reduced and the average confidence(vertical dotted line) will get closer to the global accuracy (vertical dashed line).",
  "Setting": "Datasets and modelsFor image classification, we used the datasets CIFAR-10 (C10) and CIFAR-100 (C100) with 10 and 100 classes respectively, ImageNet (IN) with 1000 classes, andImageNet-21K (IN21K) with 10450 classes. For text classification, we used Amazon Fine Foods(AFF) and DynaSent (DF) for sentiment analysis with 3 classes, MNLI for naturallanguage inference with 3 classes, and Yahoo Answers (YA) for topic classification on 10 classes.Experiment results are averaged over five random seeds that randomly split the concatenation of theoriginal validation and test sets into calibration and test sets.We used the following models for image classification: ResNet , Wide-ResNet-26-10 (WRN) ,DenseNet-121 , MobileNetV3 (MN3) , ViT , ConvNeXt , EfficientNet , Swin, and CLIP which matches input images to text descriptions in a shared embedding space,assigning labels based on the highest similarity score. For text classification, we used the PLMsRoBERTa and T5 .More details about datasets, calibration sets sizes, and model weights can be seen in Appendix F. BaselinesOur Top-versus-All (TvA) reformulation and regularization (reg) can be applied to differentcalibration methods. We have tested the following scaling methods: Temperature Scaling (TS) andVector Scaling (VS) , and Dirichlet Calibration (DC) with the best-performing variantDir-ODIR, which regularizes off-diagonal and bias coefficients. We also tested the following binarymethods: Histogram Binning (HB) using for each case the best-performing variant betweenequal-mass or equal-size bins, Isotonic Regression (Iso) , Beta Calibration (Beta) , andBayesian Binning into Quantiles (BBQ) . For comparison, we include methods with state-of-the-art results on problems with many classes: I-Max and IRM . See Appendix C for moredetails on these methods. More details on code implementations can be seen in Appendix F. : ECE in % (lower is better). The subscript TvA denotes that our reformulation was applied tothe calibration method. IRM and I-Max are competing methods. The best method for a given modelis in bold. Methods in purple impact the model prediction, potentially degrading accuracy; methodsin teal do not. Values are averaged over five random seeds. Results are averaged over models of thesame family. Detailed results for all models can be seen in Tables 5 and 6 of the Appendix.",
  "Top-versus-All": "For visual qualitative results, displays reliability diagrams . We observe that initially,ResNet-50 is highly underconfident and ViT-B/16 a bit underconfident. Applying TS and VS solvesthe underconfidence and makes the models slightly overconfident. TvA improves these methods, andthe average confidence gets closer to the accuracy. HBTvA makes the calibration almost perfect. shows the results of applying the Top-versus-All reformulation to several calibration methods.For clarity, results are averaged over families of models (models based on the same architecture) andthe full results are available in Tables 5 and 6 of the Appendix. In most cases, the TvA reformulationsignificantly lowers the ECE by dozens of percent. Without TvA, binary methods often perturb theprediction and degrade the classifiers accuracy (see ), making them inapplicable in a practicalsetting. TvA solves the issue as it only scales the confidence (after the prediction is made) and makesbinary methods outperform scaling methods. Improvements due to TvA are consistent across models. However, exceptions are observed for CLIP:it is the model family with the lowest ECE pre-calibration, but the highest ECE post-calibration forImageNet. CLIPs multimodal training regime, zero-shot adaptation as a classifier, and very largetraining dataset might cause this different behavior. CLIPs low ECE was also observed in . specifically tackles the calibration of fine-tuned CLIP, a setting not considered here. We also found that DC is sensitive to hyperparameter tuning, and its performance is usually not muchbetter than VS, which is consistent with . In some cases, the optimization diverges, leading tovery poor results, e.g., for CLIP on ImageNet. Improvements due to TvA are also consistent across datasets, although they tend to increase withthe number of classes. Improvements on ImageNet are usually better than on CIFAR-100, whoseimprovements are usually better than on CIFAR-10. This is notable with e.g., TS or HB. Themagnitude of improvement is usually higher for binary methods, e.g., HB, than scaling methods, e.g.,TS, especially for many classes. This indicates that Issue 3 is more serious than Issue 1. For text datasets with only three classes (AFF, DS, and MNLI), TS does not benefit from TvA, butother methods do, despite the small number of classes. According to , TS is among the bestcalibration methods for the text classification tasks considered here, even compared to ones thatretrain the model. Even so, our method HBTvA significantly outperforms it. Some methods current implementations could not handle the large scale of ImageNet-21K, resultingin out-of-memory errors written as \"err.\" in the Table. For I-Max and IRM, this is because theyconsider the full probability vectors while TvA efficiently uses data by considering only confidencevalues. Indeed, TvA handles this scale without difficulty. Additional results are included in Appendix H. Tables 5 and 6 contain the full results for ECE, withthe standard deviations in . reveals that ImageNet networks are mostly underconfident.This is aligned with and goes against previous knowledge on overconfidence, which was initiallybelieved to be linked to network size . provides the accuracies after calibration. exhibits that ECE with equal-mass bins has similar values as standard ECE. shows that TvAmostly lowers the Brier score, except for Iso, which has the lowest score overall. Calibration methods can also be applied to Large Languages Models (LLMs) using In-ContextLearning (ICL) to tackle text classification tasks . The primary goal of thesemethods is to improve model accuracy. TvA was not designed for this objective, but it can still beapplied on top of an existing method that improves the accuracy. TvA then lowers the calibrationerror while keeping the accuracy gain. Results for GPT-J and Llama-2 are in . To summarize the results for practical use, our experiments show that Histogram Binning (within theTvA or I-Max setting) is the best calibration method overall, providing ECE values mostly below1%. This is the method we advise using. However, suppose the underlying application requires aconfidence with continuous values, e.g., to rank the predictions for selective classification. In thatcase, we advise using a method that improves the AUROC, shown in Appendix G, such as TS or Iso.",
  "Solving overfitting with regularization and TvA": "On ImageNet, VS and DC overfit the calibration set, degrading the calibration on the test set. Thelower performance of VS relative to TS indicates this overfitting. As visualized in , combiningthe binary cross-entropy loss used in the TvA reformulation and an additional regularization termprevents overfitting. We fixed the value = 0.01 as it works well across models. Initializing thevector coefficients to 1",
  "Influence of the calibration set size": "The size of the calibration set influences the performance of the different methods, as seen in .TS and TSTvA do not benefit from more data due to their low expressiveness. VS does not improvethe ECE because of the overfitting problem. In contrast, VSreg_TvA benefits from more calibration data.With enough data ( 15000), it outperforms TSTvA. Binary methods using the standard One-versus-Allapproach have poor performance and need a large amount of data to be competitive. Using TvA, theyget excellent performance with little data.",
  "Limitations": "Our approach tackles confidence calibration and is unlikely to improve performance for strongernotions of calibration, such as class-wise calibration. However, confidence calibration is useful formany practical cases, such as selective classification , out-of-distribution detection , or activelearning . Also, calibration improvements are less significant for problems with few classes( 10) than for problems with many classes, but our approach still provides the best results.",
  "Conclusion": "Reducing the miscalibration of neural networks is essential to improve trust in their predictions.This can be done after the model training with an optimization using calibration data. However,many current calibration methods do not scale well to complex datasets: binary methods underthe One-versus-All setting do not have enough per-class calibration data, and scaling methods areinefficient. We demonstrate that reformulating the confidence calibration of multiclass classifiers as asingle binary problem significantly improves the performance of baseline calibration techniques. Thecompetitiveness of scaling methods is increased, and binary methods use per-class calibration datamore efficiently without altering the models accuracy. In short, our TvA reformulation enhancesmany existing calibration methods with little to no change in their algorithm. Extensive experimentswith state-of-the-art image classification models on complex datasets and with text classificationdemonstrate our approachs scalability and generality.",
  "and Disclosure of Funding": "We thank Tahar Nabil, Houssem Ouertatani, and Pol Labarbarie for their constructive feedback onthe paper draft.This work has been supported by the French government under the \"France 2030 program, as part ofthe SystemX Technological Research Institute.This work was granted access to the HPC/AI resources of IDRIS under the allocation 2024-AD011013372R1 made by GENCI. Momin Abbas, Yi Zhou, Parikshit Ram, Nathalie Baracaldo, Horst Samulowitz, Theodoros Salonidis, andTianyi Chen. Enhancing in-context learning via linear probe calibration. arXiv preprint arXiv:2401.12406,2024. Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh,Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Na-havandi. A review of uncertainty quantification in deep learning: Techniques, applications and challenges.Information Fusion, 76:243297, December 2021. Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, BinBao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, AlbanDesmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh,Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang,Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, MarkSaroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang,William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, PengWu, and Soumith Chintala. PyTorch 2: Faster Machine Learning Through Dynamic Python BytecodeTransformation and Graph Compilation. In 29th ACM International Conference on Architectural Supportfor Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). ACM, April 2024.",
  "Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review,78(1):13, 1950": "Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A Close Look into the Calibrationof Pre-trained Language Models. In Proceedings of the 61st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 13431367, Toronto, Canada, 2023. Associationfor Computational Linguistics. Jiacheng Cheng and Nuno Vasconcelos. Calibrating Deep Neural Networks by Pairwise Constraints. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1370913718, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchicalimage database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255,June 2009.",
  "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, andZhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image isworth 16x16 words: Transformers for image recognition at scale. In International Conference on LearningRepresentations, 2020. Telmo Silva Filho, Hao Song, Miquel Perello-Nieto, Raul Santos-Rodriguez, Meelis Kull, and Peter Flach.Classifier Calibration: A survey on how to assess and improve predicted class probabilities. MachineLearning, 112(9):32113260, September 2023. Ido Galil, Mohammed Dabbah, and Ran El-Yaniv. What Can we Learn From The Selective PredictionAnd Uncertainty Estimation Performance Of 523 Imagenet Classifiers? In The Eleventh InternationalConference on Learning Representations, February 2023. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt,Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, WenYang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in deep neural networks. ArtificialIntelligence Review, July 2023.",
  "Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-DistributionExamples in Neural Networks. In International Conference on Learning Representations, 2017": "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of theIEEE/CVF international conference on computer vision, pages 13141324, 2019. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.Densely connectedconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 47004708, 2017.",
  "A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009": "Meelis Kull, Telmo M. Silva Filho, and Peter Flach. Beyond sigmoids: How to obtain well-calibratedprobabilities from binary classifiers with beta calibration. Electronic Journal of Statistics, 11(2):50525080,January 2017. Meelis Kull, Miquel Perello Nieto, Markus Kngsepp, Telmo Silva Filho, Hao Song, and Peter Flach.Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration.Advances in neural information processing systems, 32, 2019.",
  "Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. Advances in NeuralInformation Processing Systems, 32, 2019": "Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable Calibration Measures for Neural Networks fromKernel Mean Embeddings. In Proceedings of the 35th International Conference on Machine Learning,pages 28052814. PMLR, July 2018. Fabian Kppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidencecalibration for object detection. In The IEEECVF Conference on Computer Vision and Pattern Recognition(CVPR) Workshops, June 2020. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictiveuncertainty estimation using deep ensembles. Advances in neural information processing systems, 30,2017.",
  "Mingkun Li and I.K. Sethi. Confidence-based active learning. IEEE Transactions on Pattern Analysis andMachine Intelligence, 28(8):12511261, August 2006": "Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration for Deep Neural Networks. In The Eleventh International Conference on LearningRepresentations, September 2022. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,prompt, and predict: A systematic survey of prompting methods in natural language processing. ACMComputing Surveys, 55(9):135, 2023. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXivpreprint arXiv:1907.11692, 2019.",
  "Yuchi Liu, Lei Wang, Yuli Zou, James Zou, and Liang Zheng. Optimizing calibration by gaining aware ofprediction correctness. arXiv preprint arXiv:2404.13016, 2024": "Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 1200912019, 2022. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 1001210022, 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. Aconvnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 1197611986, 2022.",
  "TorchVision maintainers and contributors. Torchvision: Pytorchs computer vision library. 2016": "Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of userexpertise through online reviews. In Proceedings of the 22nd International Conference on World WideWeb, WWW 13, page 897908, New York, NY, USA, 2013. Association for Computing Machinery. Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, DustinTran, and Mario Lucic. Revisiting the Calibration of Modern Neural Networks. In Advances in NeuralInformation Processing Systems, volume 34, pages 1568215694. Curran Associates, Inc., 2021. Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania.Calibrating Deep Neural Networks using Focal Loss. In Advances in Neural Information ProcessingSystems, volume 33, pages 1528815299. Curran Associates, Inc., 2020. Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated ProbabilitiesUsing Bayesian Binning. Proceedings of the AAAI Conference on Artificial Intelligence, 29(1), February2015. Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. InProceedings of the 22nd International Conference on Machine Learning, ICML 05, pages 625632, NewYork, NY, USA, August 2005. Association for Computing Machinery.",
  "John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihoodmethods. Advances in large margin classifiers, 10(3):6174, 1999": "Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. DynaSent: A dynamic benchmarkfor sentiment analysis. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 23882404, Online,August 2021. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pages 87488763. PMLR,2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.Journal of Machine Learning Research, 21(140):167, 2020. Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for themasses. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and BenchmarksTrack (Round 1), 2021. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.In Proceedings of the 2013 conference on empirical methods in natural language processing, pages16311642, 2013.",
  "Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International conferenceon machine learning, pages 1009610106. PMLR, 2021": "Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. OnMixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks. In Advancesin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation andfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and ThomasSchn. Evaluating model calibration in classification. In The 22nd International Conference on ArtificialIntelligence and Statistics, pages 34593467. PMLR, 2019. Ellen M. Voorhees and Dawn M. Tice. Building a question answering test collection. In Proceedings of the23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,SIGIR 00, page 200207, New York, NY, USA, 2000. Association for Computing Machinery.",
  "David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classification: Aunifying framework. Advances in neural information processing systems, 32, 2019": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentenceunderstanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings ofthe 2018 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long Papers), pages 11121122, New Orleans, Louisiana, June2018. Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, PierricCistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages3845, Online, October 2020. Association for Computational Linguistics. Miao Xiong, Ailin Deng, Pang Wei W Koh, Jiaying Wu, Shen Li, Jianqing Xu, and Bryan Hooi. Proximity-informed calibration for deep neural networks. Advances in Neural Information Processing Systems,36:6851168538, 2023. Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision treesand naive Bayesian classifiers. In Proceedings of the Eighteenth International Conference on MachineLearning, ICML 01, pages 609616, San Francisco, CA, USA, June 2001. Morgan Kaufmann PublishersInc. Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probabilityestimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining, KDD 02, pages 694699, New York, NY, USA, July 2002. Association for ComputingMachinery.",
  "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,2016": "Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methodsfor uncertainty calibration in deep learning. In International conference on machine learning, pages1111711128. PMLR, 2020. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification.In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural InformationProcessing Systems, volume 28. Curran Associates, Inc., 2015. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification.In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural InformationProcessing Systems, volume 28. Curran Associates, Inc., 2015. Shengjia Zhao, Michael Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating predictions todecisions: A novel approach to multi-class calibration. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,pages 2231322324. Curran Associates, Inc., 2021. Xujiang Zhao, Shu Hu, Jin-Hee Cho, and Feng Chen. Uncertainty-based Decision Making Using DeepReinforcement Learning. In 2019 22th International Conference on Information Fusion (FUSION), pages18, July 2019. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improvingfew-shot performance of language models. In International conference on machine learning, pages1269712706. PMLR, 2021. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy.Batch calibration: Rethinking calibration for in-context learning and prompt engineering. arXiv preprintarXiv:2309.17249, 2023.",
  "AAppendix contents": "B discusses the broader impacts of the work.C discusses the proposed approach in more details, and compares with other methods.D contains a theoretical justification for Top-versus-All in the case of Temperature Scaling.E discusses the limits of classwise-ECE and top-label-ECE for a high number of classes.F describes implementation details, including the computing time in .G shows the impact of different calibration methods on selective classification.H provides additional results: full ECE results in and 6, standard deviations in ,confidences in , accuracies in , equal-mass bins ECE in , Brier score in , experiments for in-context learning of LLMs in .",
  "BBroader impacts": "Our reformulation of the confidence calibration of multiclass classifiers as a binary problem is bothsimple and general. It has several benefits. On the theoretical side, it might lead to new perspectiveson the confidence calibration problem and the development of new calibration methods. On thepractical side, existing calibration methods can be adapted to our problem reformulation by addingjust a few lines of code. This is an easy and quick way to improve the calibration of classificationmodels. Better-calibrated models are more trustworthy: potential incorrect predictions are moreeasily identifiable and preventable. However, this also comes with potential risks. The knowledgethat a model is well-calibrated might lead to undue trust in the system and the tendency to overlookprediction errors. Even well-calibrated models are not entirely reliable, and developers and usersmust remember this. Post-processing calibration requires data not included in the training set, whichleaves less data available for a thorough evaluation of the model. Calibration does not fix biases inthe data. Finally, we tested the calibration improvement only on in-distribution data, but real systemsmight receive out-of-distribution data (e.g., an image of a new class) or adversarial examples. Forsuch inputs, the classifier predicted probabilities (and thus the confidence) are unreliable, even forwell-calibrated models, and a pipeline to filter such data is necessary.",
  "end ifInference:Use g to calibrate confidences from f": "Comparison with the standard approachAlgorithm 2 describes the standard approach to post-processing calibration, and Algorithm 3 describes our approach in more details and shows in bluethe differences with the standard approach. Our approach adds a preprocessing step to keep only theconfidences instead of the full probabilities vector. It can be seen as creating a surrogate \"correctness\"classifier and its associated calibration data. The calibrator is learned for the surrogate classifierand applied to the original classifier at inference time. Also, we add regularization for some scalingmethods and we have only one binary calibrator instead of one per class. Comparison with IRM and I-MaxIRM and I-Max are, like TvA, multiclass-to-binaryreductions. This is why TvA cannot be applied on top of them: they already transform the multiclassproblem into a binary one using a different strategy. The shared class-wise strategy of and the data ensemble strategy of are described very brieflyin subsections 3.2 and 3.3.2 of their respective papers and not rigorously justified. Our understandingis that these two strategies do exactly the same thing. To build the calibration set, they concatenateall the class probability vectors so that we get a big probability vector of size N.L (N samples andL classes) as predictions and similarly concatenate the one-hot embedding of the target class (a bigvector with N ones and N.(L 1) zeros) as targets. Then, they learn a single calibrator. For eachexample, this calibrator aims to simultaneously increase the probabilities for the target class (target is1) and decrease all the other class probabilities (target is 0). The single calibrator is applied to eachclass probability separately, meaning that the ranking of class probabilities can change, modifyingthe classifier prediction. Our strategy derives from transforming the multiclass calibration into a single binary problem. Theintuition is to learn the calibrator on a surrogate binary classifier and apply this calibrator to theoriginal classifier. This binary classifier is built on top of the original classifier (by applying themax function to the class probabilities vector). They thus share their confidence. However, thebinary classifier aims to solve a different task: predicting the correctness of the original classifier. Tobuild the calibration set, we concatenate all the confidences (a vector of size N) as predictions andconcatenate all the correctnesses as targets (also a vector of size N). The correctness value of a givenexample is 1 if the class prediction is correct; otherwise, it is 0. Then, we learn a single calibrator,similar to the strategy above. However, there is a key difference: this calibrator aims to increasethe probabilities for correct predictions and decrease them for incorrect predictions. Note that ourprobabilities are all confidences (the maximum class probabilities), meaning we only consider theconfidences, which the calibrator directly increases or decreases. In the strategy from and ,the calibrator has to manage all class probabilities (L times more), even the ones that do not matter,including the lowest class probabilities close to 0. This is less efficient (actually, while this can surelybe fixed, the original implementations of IRM and I-Max could not run on ImageNet-21K). This pointis closely linked to the analysis of the binary cross entropy loss for scaling methods in Subsection 4.2:when the prediction is incorrect, increasing the probability of the correct class indirectly decreasesthe confidence (strategy from and ) while our strategy directly decreases the confidence. I-Max is more complex because it modifies the Histogram Binning algorithm, while our approachdoes not. Additionally, found that I-Max produces unusable probability vectors. Indeed, they donot sum up to 1, and normalizing them degrades the methods performance. We wrote our paper with practicality and generality in mind. Contrary to and , we demon-strate the generality of our strategy by applying it on top of existing calibration baselines of differentnatures (scaling and binary). One of our main goals is that practitioners can easily and quickly tryour TvA approach, using just a few lines of code, which can significantly improve the calibrationperformance of their existing calibration pipeline while having no impact on the predicted class bydesign (except for VS and DC). Comparison with ProCalAnother recent calibration method is ProCal . However, its primaryobjective differs from ours: it \"focuses on the problem of proximity bias in model calibration, aphenomenon wherein deep models tend to be more overconfident on data of low proximity\". Its goalis to lower the difference in the confidence score values between regions of low and high density,i.e., to make the confidence score independent of a local density indicator called \"proximity.\" Thereis no theoretical guarantee, however, that minimizing the proximity bias improves the confidencecalibration, the focus of our work. Theorem 4.2 about the PIECE metric is a direct consequenceof Jensens inequality and is true for any random variable D, not necessarily a proximity score. Theorem 5.1 is an interesting bias/variance decomposition of the Brier score. However, as this typeof decomposition usually states, the error may come from bias (here, a wrong initial calibration) orhigh estimation variance (which can be related to low density but is not expressed as such in thedecomposition). We experimentally compare our approach to the ProCal algorithm using the codeprovided by its authors and observe in that our approach gives much better ECE confidencecalibration and, for half of the models, also better PIECE values. ProCal aims to achieve three goals: mitigate proximity bias, improve confidence calibration, andprovide a plug-and-play method. We share the last two goals. Concerning improving confidencecalibration, our approach has better results, as shown in . Both approaches are plug-and-play,but they apply very differently. ProCal is applied after existing calibration methods to further improvecalibration. It thus does not solve any of the four issues we identified (e.g., cross-entropy loss isstill inefficient, and One-versus-All still leads to highly imbalanced problems). Our Top-versus-Allapproach is a reformulation of the calibration problem that uses a surrogate binary classifier. Existingapproaches are applied to this surrogate classifier, which is how the four issues are solved. We donot propose a new method but a new way of applying existing methods. Our approach does notintroduce new hyperparameters (except in the particular case of regularizing scaling methods). ProCalintroduces several new hyperparameters, such as the choice of the distance, the number of KNNneighbors, or a shrinkage coefficient. Comparison with Correctness-Aware LossA concurrent work builds a calibration method ontop of an intuition similar to ours: binarize the calibration problem. However, what the authors do withthis intuition differs vastly from our approach. They derive a Correctness-Aware Loss (Eq. 7 of theirpaper), which is almost the standard binary cross-entropy loss we use for scaling methods but withouta logarithm. They use this loss to learn a separate model that predicts a sample-wise temperaturecoefficient. This is a new calibration method, which is not straightforward to implement due to thenumerous hyperparameters (network architecture, image transformations...). It also requires multipleinferences at test-time, which can be problematic in some production models. Our approach is, again,not a calibration method but a general reformulation of the calibration problem that enhances existingmethods. By looking at their , they get an ECE of 2.22 on ImageNet (in-distribution), while ourapproach achieves values around 0.5 for most models in our papers . Their method, contraryto TvA, improves the AUROC, but in our understanding, it seems mostly due to the use of imagetransformations, not from their proposed loss. Their method seems to work best in out-of-distributionscenarios, which is not the main objective of our paper. However, these good results for AUROC andout-of-distribution scenarios make this method complementary to our approach, and combining thetwo in some way could be promising. : ECE, MCE, ACE, and PIECE in %. The experimental setting is the one used for of. The baselines are no calibration (conf), Temperature Scaling (TS), Multi Isotonic Regression(MIR), and Histogram Binning (HB). The ProCal calibration method is applied after one ofthe baselines, as symbolized by the \"+\" symbol. Our approach, Top-versus-All, changes what thebaselines optimize and is symbolized by \"TvA\". We apply it for TS, Isotonic Regression (Iso), andHB. The best values for each model and metric are in bold.Overall, HBTvA is the best calibration method as it always gets the lowest ECE and ACE. Our TvAapproach lowers PIECE and even achieves the lowest value for half of the models.",
  "DTheoretical justification for Top-versus-All in the case of TemperatureScaling": "We define L the number of classes, fk(x) the classifier estimated probability for class k and datasample x, y the correct class, and the confidence s(x) := maxk fk(x). The cross-entropy loss islCE(x, y) = Lk=1 1{k = y} log(fk(x)) = log(fy(x)). Because the last layer of the classifieris a softmax function, fy(x) =ezy",
  "(7)": "For our TvA approach, the problem becomes binary. The classification output becomes the confidences(x) = maxjY fj(x) and the ground truth label becomes a binary representation of the predictioncorrectness: yb = 1y=y with y(x) = arg maxkY fk(x) and 1 the indicator function. The loss weuse is the binary cross entropy lBCE(x, y) = yb log s(x) + (1 yb) log(1 s(x))",
  "s1| = . In practice, s is not close enough to 1 to generate explodinggradients, so it just means that as confidences for wrong predictions gets higher, so does the gradientto reduce the confidences": "The conclusion is that for correct predictions, our approach does not change the optimization, butfor incorrect predictions, the gradient is stronger and penalizes more heavily confident predictionsthat are wrong. This is also proven experimentally by looking at where we see that average confidences of temperature scaling with the TvA approach (TSTvA) are lower than the ones using thestandard approach (TS), for almost all networks. This makes the average confidences closer to theaccuracy, showing reduced overconfidence.",
  "j=1ECEj": "Classwise-ECE considers the full probabilities vectors: all the class probabilities for each prediction.However, this metric does not scale to large numbers of classes. Let us see why with an example. Let us use a test set of N samples, N/L for each of the L classes (the dataset is balanced), and ahigh-accuracy classifier fairly calibrated. The classifier predicts N probability vectors of length L.Predicted probabilities for class j are all the values of the vector at dimension j. Because the classifierhas a high accuracy and is fairly calibrated, around N/L values are close to 1 (corresponding tomostly correct predictions), and the remaining ones, around N N/L, are close to 0 (because thepredicted class is not class j, and the predicted probability is high for another class). To compute ECEj with equal size 15 bins, the predicted probabilities for class j are partitioned into15 bins. The first bin (with probabilities close to 0), contains n1 N N/L samples while the lastone (with probabilities close to 1) contains nB N/L samples. The remaining bins are usually evenmore empty. That means that the calibration error in the first bin is weighted n1/nB = L 1 timesmore than the last one. For the 1000 classes of ImageNet, L 1 = 999. shows the numberof samples (nb) in each bin for an ImageNet classifier. Because the impact of the calibration error in the bin with the high probabilities is negligible relativeto the bin with the low probabilities, the classwise-ECE mostly measures whether probabilities closeto 0 are calibrated. We argue this is not what we are interested in: what matters more is the calibrationof higher values of the probabilities. Top-label ECE is another interesting metric that does not scale to large numbers of classes either.Top-label-ECE divides data into subsets according to the predicted class, computes the ECEs of thesesubsets, and averages them. For an ImageNet test set of 25000 samples (25 per class), data is dividedinto 1000 subsets of 25 samples each (the classifier is high-accuracy, most of the time the predictedclass is equal to the true class). The ECE is computed for each subset containing only 25 samples.To compute the ECE, samples are typically partitioned into 15 bins. The number of samples per bindoes not allow a correct estimation of the average confidence or accuracy.",
  "ImageNet (IN) contains 1.3 million images from 1000 classes. Following , werandomly split the original validation test of size 50000 into a calibration set and a test set,both of size 25000": "ImageNet-21K (IN21K) , in its winter21 version, contains 11 million images in the trainset, and 522500 in the test set (50 for each of the 10450 classes). We randomly split the testset into equal-sized calibration and test set (261250 samples each, 25 per class). Amazon Fine Foods is a collection of customer reviews for fine foods sold on Amazon.Reviews are categorized into bad, neutral, and good. The original validation set size is 78741and test size 91606. We randomly split them into 78741 samples for calibration and 91606for test. DynaSent is a dynamic benchmark for sentiment analysis consisting of sentencesannotated as positive, neutral, and negative. The original validation set size is 11160 andtest size 4320. We randomly split them into 11160 samples for calibration and 4320 for test. MNLI contains pairs of sentences labeled as contradiction, neutral, and entailment. Theoriginal validation set size is 19635 and test size 9815. We randomly split them into 19635samples for calibration and 9815 for test. Yahoo Answers (YA) contains question-answers pairs corresponding to 10 differenttopics. The original validation set size is 14000 and test size 60000. We randomly split theminto 14000 samples for calibration and 60000 for test.",
  "GImpact on selective classification": "Selective classification aims to improve a models prediction performance by trading-off coverage: areject option allows to discard data that might result in wrong predictions, thus improving the accuracyon the remaining data. A strong standard baseline uses thresholding on the maximum softmaxprobability outputted by the classifier . Improving confidence calibration means uncertainty isbetter quantified and should result in better selective classification. Results in show the superiority of Histogram Binning (applied with the right framework) inreducing the calibration error ECE. Unfortunately, it does not translate into improvements in selectiveclassification. AUROC is a standard metric for selective classification . shows thatHistogram Binning actually degrades the AUROC, while the best method is Isotonic Regression. OurTvA framework does not significantly impact the AUROC. This paper addresses confidence calibration, usually measured by ECE. AUROC is a global rank-based metric for selective classification: it relies on the relative values of the scores, not their absolutevalues. Even though calibration and selective classification are related, improvement in calibrationdoes not directly translate to better selective classification. This has been clearly demonstratedexperimentally by .A good example of that difference is the behavior of HBTvA: it is the best calibration method overallbut actually degrades the AUROC in most cases. Such a difference can be explained by the fact thatselective classification benefits from a continuous score able to discriminate between certain anduncertain examples finely, but HB quantizes the confidences into, e.g., 10 different values. : AUROC in % (higher is better). Methods in purple impact the model prediction, potentiallydegrading accuracy; methods in teal do not. Improvements from the uncalibrated model are coloredin blue and degradations in orange.",
  "HAdditional results": ": ECE in % (lower is better, best in bold) full results for image classification datasets.Averages on 5 seeds. Mean relative improvements from TvA are shown (negative values for reductionsof ECE). Methods in purple impact the model prediction, potentially degrading accuracy; methods inteal do not. Values are averaged over five random seeds.",
  "Mean improvement-38%-20%-12%err.-90%err.-98%": ": ECE in % (lower is better, best in bold) full results for text classification datasets. Averageson 5 seeds. Mean relative improvements from TvA are shown (negative values for reductions ofECE). Methods in purple impact the model prediction, potentially degrading accuracy; methods inteal do not. Values are averaged over five random seeds.",
  "RoBERTa-large72.892.573.173.085.685.689.575.689.376.193.272.674.872.973.272.973.073.0": ": Accuracy in % (higher is better). Methods in purple impact the model prediction, potentiallydegrading accuracy; methods in teal do not. Because classifiers can be well calibrated when notaccurate (by having low accuracy and low confidence), it is important to monitor the accuracy. It iseven better when the methods preserve the accuracy by design.",
  "Uncalibrated70.05.255.07.094.75.9ConC73.712.644.022.894.33.9LinC73.59.450.214.495.33.9LinC+HBTvA73.57.050.24.295.32.1": "Large Language Models (LLMs) exhibit an in-context learning (ICL) capability, meaning they canlearn from just a few examples in the context. It works by constructing a prompt that includesinput-output pairs demonstrating the considered task, followed by a query for a new input. See fora survey. Recent works develop calibration methods whose main goal is to improve the performanceof ICL for LLMs, without requiring a complicated model fine-tuning. uses a customized variantof Platt scaling (more specifically, Vector Scaling). Their method infers good values of the vectorscaling parameters in a data-free procedure. The idea is that for a \"content-free\" input, e.g., \"N/A\",the calibrated probability has a 50% chance (for a binary classification task) of removing a biastoward the positive or negative class. In our paper, we denote this method as ConC. builds on topof this work but uses a calibration set to learn the scaling parameters by minimizing the cross-entropyloss. This can be considered as Matrix Scaling. We denote this method as LinC. proposes aper-class normalization of the probabilities on a given batch. estimates the in-context modellabel marginal p(y) from limited data and uses it to calibrate the model probabilities. Paper usesa Gaussian mixture model. In our experiments, we have tested a two-step calibration. First, we use the state-of-the-art methodLinC to maximize the accuracy by learning scaling parameters on a calibration set. Then, we applyHBTvA to scale the confidences to lower the calibration error ECE, while preserving the accuracy gains.We use the same calibration set for the two methods. LinC performance depends on hyperparametervalues, but to keep the experiments simple, we fixed the following values: 100 epochs, a learning rateof 0.001, and 300 calibration samples. It means that the reported performance of LinC is suboptimaland could be enhanced even more. We used the same experimental setting as . We used the modelsGPT-J with 6B parameters and Llama-2 with 13B parameters . The text classification datasetsare TREC for question classification with 6 classes, SST-5 for sentiment analysis with 5classes, and DBpedia for topic classification with 14 classes. The 0-shot, 1-shot, 4-shot, and8-shot learning settings were tested. Five different sets of 300 test samples were randomly selected,and results are averaged over 5 seeds. We evaluated the accuracy and ECE for each configuration.Please see for the results. In most cases, LinC+HBTvA achieves the best accuracy and ECE.",
  ". Claims": "Question: Do the main claims made in the abstract and introduction accurately reflect thepapers contributions and scope?Answer: [Yes]Justification: Our main claims in the abstract and introduction are that our approach solvesthe failure of many calibration methods when applied to problems with many classes andthat we conduct extensive experiments. This reflects our analyses in Sections 3 and 4 andexperimental results in and Appendices G and H.Guidelines:",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  "If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact": "Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}