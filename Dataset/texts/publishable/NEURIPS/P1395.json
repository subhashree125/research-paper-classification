{
  "Abstract": "We define maximum entropy goal-directedness (MEG), a formal measure of goal-directedness in causal models and Markov decision processes, and give algorithmsfor computing it. Measuring goal-directedness is important, as it is a criticalelement of many concerns about harm from AI. It is also of philosophical interest,as goal-directedness is a key aspect of agency. MEG is based on an adaptation ofthe maximum causal entropy framework used in inverse reinforcement learning. Itcan measure goal-directedness with respect to a known utility function, a hypothesisclass of utility functions, or a set of random variables. We prove that MEG satisfiesseveral desiderata and demonstrate our algorithms with small-scale experiments.1",
  "Introduction": "In order to build more useful AI systems, a natural inclination is to try to make them more agentic.But while agents built from language models are touted as the next big advance [Wang et al., 2024],agentic systems have been identified as a potential source of individual [Dennett, 2023], systemic[Chan et al., 2023, Gabriel et al., 2024], and catastrophic [Ngo et al., 2022] risks. Agency is thusa key focus of behavioural evaluations [Shevlane et al., 2023, Phuong et al., 2024] and governance[Shavit et al., 2023]. Some prominent researchers have even called for a shift towards designingexplicitly non-agentic systems [Dennett, 2017, Bengio, 2023]. A critical aspect of agency is the ability to pursue goals. Indeed, the standard theory of agency definesagency as the capacity for intentional action action that can be explained in terms of mental statessuch as goals [Schlosser, 2019]. But when are we justified in ascribing such mental states? Accordingto Daniel Dennetts instrumentalist philosophy of mind , we are justified when doing so isuseful for predicting a systems behaviour. This papers key contribution is a method for formally measuring goal-directedness based on Dennettsidea. Since pursuing goals is about having a particular causal effect on the environment.2 definingour measure in a causal model is natural. Causal models are general enough to encompass mostframeworks popular among ML practitioners, such as single-decision prediction, classification, andregression tasks, as well as multi-decision (partially observable) Markov decision processes. Theyalso offer enough structure to usefully model many ethics and safety problems [Everitt et al., 2021a,Ward et al., 2024a, Richens et al., 2022, Richens and Everitt, 2024, Everitt et al., 2021b, Ward et al.,2024b, Halpern and Kleiman-Weiner, 2018, Wachter et al., 2017, Kusner et al., 2017, Kenton et al.,2023, Fox et al., 2023, Richens and Everitt, 2024, MacDermott et al., 2023].",
  "Maximum entropy goal-directedness (MEG) operationalises goal-directedness as follows, illustratedby the subsequent running example": "A variable D in a causal model is goal-directed with respect to a utility function Uto the extent that the conditional probability distribution of D is well-predicted bythe hypothesis that D is optimising U. Example 1. A mouse begins at the centre of a gridworld (a). It observes that a block ofcheese is located either to the right or left (S) with equal probability, proceeds either away from it ortowards it (D), and thus either obtains the cheese or does not (T). Suppose that the mouse moves left when the cheese is to the left and right when it is to the right, thusconsistently obtaining the cheese. Intuitively, this behaviour seems goal-directed, but can we quantifyhow much? gives an overview of our procedure. We first model the system of interest as acausal Bayesian network (b) with variables S for the cheeses position, D for the mousesmovement, and T for whether or not the mouse obtains the cheese. Then, we identify a candidatedecision variable D and target variable T, hypothesising that the mouse optimises a utility functionthat depends on T (c). Finally, we form a model of what behaviour we should expect from Dif it is indeed optimising U and then measure how well this model predicts Ds observed behaviour(d). Ziebart s maximum causal entropy (MCE) framework suggests a promising way to constructa model for expected behaviour under a given utility function. However, there are several obstacles toapplying it to our problem: it cannot measure the predictive usefulness of known utility functions, andit only finds the most predictive linear utility function. In practice, arbitrary known utility functionscan be plugged in, but the results are not scale-invariant. We overcome these difficulties by returningto first principles and deriving an updated version of the MCE framework. In summary, our contributions are four-fold. We (i) adapt the MCE framework to derive maximumentropy goal-directedness (MEG), a philosophically-motivated measure of goal-directedness withrespect to known utility functions, and show that it satisfies several key desiderata (); (ii)we extend MEG to measure goal-directedness in cases without a known utility function ();(iii) we provide algorithms for measuring MEG in MDPs (); and (iv) we demonstrate thealgorithms empirically (). Related Work.Inverse reinforcement learning (IRL) [Ng and Russell, 2000] focuses on the questionof which goal a system is optimising, whilst we are interested in to what extent the system can be seenas optimising a goal. We are not the first to ask this question. Some works take a Bayesian approachinspired by Dennetts intentional stance.3 Most closely related to our work is Orseau et al. ,which applies Bayesian IRL in POMDPs using a Solomonoff prior over utility functions and an-greedy model of behaviour. This lets them infer a posterior probability distribution over whether anobserved system is a (goal-directed) \"agent\" or \"just a device\". Our approach distinguishes itself fromthese by considering arbitrary variables in a causal model and deriving our behaviour model from theprinciple of maximum entropy. Moreover, since our algorithms can take advantage of differentiableclasses of utility functions, our approach may be amenable to scaling up using deep neural networks.Oesterheld combines the intentional stance with Bayes theorem in cellular automata, but doesnot consider specific models of behaviour. Like us, Kenton et al. consider goal-directedness in",
  "After accepted publication, another related paper appeared [Xu and Rivera, 2024]": "a causal graph. However, they require variables to be manually labelled as mechanisms or object-level,and only provide a binary distinction between agentic and non-agentic systems (see Appendix A fora detailed comparison). Biehl and Virgo , Virgo et al. propose a definition of agency inMoore machines based on whether a systems internal state can be interpreted as beliefs about thehidden states of a POMDP.",
  "Background": "We use capital letters for random variables V , write dom(V ) for their domain (assumed finite), anduse lowercase for outcomes v dom(V ). Boldface denotes sets of variables V = {V1, . . . , Vn}, andtheir outcomes v dom(V ) =i dom(Vi). Parents and descendants of V in a graph are denotedby PaV and DescV , respectively (where paV and descV are their instantiations). Causal Bayesian networks (CBNs) are a class of probabilistic graphical models used to representcausal relationships between random variables [Pearl, 2009].Definition 2.1 (Causal Bayesian network). A Bayesian network M = (G, P) over a set of variablesV = {V1, . . . , Vn} consists of a joint probability distribution P which factors according to a directedacyclic graph (DAG) G, i.e., P(V1, . . . , Vn) = ni=1 P(Vi | PaVi), where PaVi are the parents of Viin G. A Bayesian network is causal if its edges represent direct causal relationships, or formallyif the result of an intervention do(X = x) for any X V can be computed using the truncatedfactorisation formula: P(v | do(X = x)) = i:vi /xP(vi | pavi) if v is consistent with x orP(v | do(X = x)) = 0 otherwise.",
  "b depicts Example 1 as a CBN, showing the causal relationships between the location of thecheese (S), the mouses behavioural response (D), and whether the mouse obtains the cheese (T)": "We are interested in to what extent a set of random variables in a CBN can be seen as goal-directed.That is, to what extent can we interpret them as decisions optimising a utility function? In other words,we are interested in moving from a CBN to a causal influence diagram (CID), a type of probabilisticgraphical model that explicitly identifies decision and utility variables.Definition 2.2 (Causal Influence Diagram [Everitt et al., 2021a]). A causal influence diagram (CID)M = (G, P) is a CBN where the variables V are partitioned into decision D, chance X, andutility variables U. Instead of a full joint distribution over V , P consists of conditional probabilitydistributions (CPDs) for each non-decision variable V V \\ D. A CID can be combined with a policy , which specifies a CPD D for each decision variable D, inorder to obtain a full joint distribution. We call the sum of the utility variables the utility function anddenote it U =",
  "UU U. Policies are evaluated by their total expected utility E[U]. We write uniffor the uniformly random policy": "CIDs can model a broad class of decision problems, including Markov decision processes (MDPs)and partially observable Markov decision processes (POMDPs) [Everitt et al., 2021b].Example 2 (POMDP). A mouse begins at the centre of the grid, with a block of cheese located eitherat the far left or the far right (). The mouse does not know its position or the position of thecheese (S1) but can smell which direction the cheese is in (O1) and decide which way to move (D1).Next step, the mouse again smells the direction of the cheese (O2) and again chooses which way toproceed (D2).",
  "Measuring goal-directedness with respect to a known utility function": "Maximum Entropy Goal-directednes.Dennetts instrumentalist approach to agency says thatwe can ascribe mental states (such as utilities) to a system to the extent that doing so is useful forpredicting its behaviour [Dennett, 1989]. To operationalise this, we need a model of what behaviouris predicted by a utility function. According to the principle of maximum entropy [Jaynes, 1957],we should choose a probability distribution with the highest entropy distribution satisfying ourrequirements, thus minimising unnecessary assumptions (following Occams razor). We can measurethe entropy of a policy by the expected entropy of its decision variables conditional on their parentsH(D || PaD) =",
  ": Sequential multi-decision mouse example": "In our setting, the relevant constraint is expected utility. To avoid assuming that only optimal agentsare goal-directed, we construct a set of models of behaviour which covers all levels of competence anagent optimising utility U could have. We define the set of attainable expected utilities in a CID asatt(U) = {u R | (E [U] = u)}.Definition 3.1 (Maximum entropy policy set, known utility function). Let M = (G, P) be a CIDwith decision variables D and utility function U. The maximum entropy policy set for u att(U)is maxentU,u:= argmax|E[U]=u H(D || PaD). The maximum entropy policy set for U is the set ofmaximum entropy policies for any attainable expected utility maxentU:=",
  "For each attainable expected utility, maxentUcontains the highest entropy policy which attains it. InMDPs, this policy is unique maxentU,uand can be found with backwards induction (see )": "We measure predictive accuracy using cross-entropy, as is common in ML. We subtract the predictiveaccuracy of the uniform distribution, so that we measure predictive accuracy relative to randomchance. This makes MEG always non-negative.Definition 3.2 (Maximum entropy goal-directedness, known utility function). Let M := (G, P) bea CID with decision variables D and utility function U. The maximum entropy goal-directedness(MEG) of a policy with respect to U is",
  ".(1)": "As we will see, in a Markov Decision Process, this is equivalent to maximising predictive accuracyby varying the rationality parameter in the soft-optimal policy softused in maximum entropyreinforcement learning [Haarnoja et al., 2017].4 If instead of having access to a policy , we have access to a set of full trajectories{(paiD1, Di1, . . . , paDin, Din)}i, the expectation E in Equation (1) can be replaced with an aver-age over the trajectory set. This is an unbiased and consistent estimate of MEGU() for the policy generating the trajectories. Example.Consider a policy in Example 1 that proceeds towards the cheese with probability 0.8.How goal-directed is this policy with respect to the utility function U that gives +1 for obtaining thecheese and 1 otherwise? To compute MEGU(), we first find the maximum entropy policy set maxentU, and then take themaximum predictive accuracy with respect to . In a single-decision setting, for each attainableexpected utility u there is a unique maxentU,uwhich has the form of a Boltzmann policy maxentU,u(d |",
  "s) =exp(E[U|d,s])": "d exp(E[U|d,s]) (cf. Theorem 5.1). The rationality parameter = (u) can be varied to getthe correct expected utility. Predictive accuracy with respect to is maximised by maxentU,0.8 , whichhas a rationality parameter of = log 2. The expected logprob of a prediction of this policy is 4A noteworthy property of Definition 3.2 is that a policy that pessimises a utility function can be just asgoal-directed as a policy that optimises it. This fits with the intuitive notion that we are measuring how likelya policys performance on a particular utility function is to be an accident. However, it might be argued thata policy should instead be considered negatively goal-directed with respect to a utility function it pessimises.We could adjust Definition 3.2 to achieve this by multiplying by the sign of E [U] Eunif [U], making thegoal-directedness of any policy that performs worse than random negative. For simplicity, we use the unsignedmeasure in this paper.",
  ". So we get that MEGU() = 0.50 (0.69) = 0.19. For comparison, predictive accuracyfor the optimal policy is maximised when = , and has MEGU() = 0 (0.69) = 0.69": "Properties.We now show that MEG satisfies three important desiderata. First, since utilityfunctions are usually only defined up to translation and rescaling, a measure of goal-directedness withrespect to a utility function should be translation and scale invariant. MEG satisfies this property:Proposition 3.1 (Translation and scale invariance). Let M1 be a CID with utility function U1, andlet M2 be an identical CID but with utility function U2 = a U1 + b, for some a, b R, with a = 0.Then for any policy , MEGU1() = MEGU2(). Second, goal-directedness should be minimal when actions are chosen completely at random andmaximal when uniquely optimal actions are chosen.Proposition 3.2 (Bounds). Let M be a CID with utility function U. Then for any policy we have0 MEGU()",
  "DD log(| dom(D)|), with equality in the lower bound if is the uniformpolicy, and equality in the upper bound if is the unique optimal (or anti-optimal) policy with respectto U": "Note that MEG has a natural interpretation as the amount of evidence provided for a goal-directedpolicy over a purely random policy. The larger a decision problem, the more opportunity there is tosee this evidence, so the higher MEG can be. Third, a system can never be goal-directed towards a utility function it cannot affect.Proposition 3.3 (No goal-directedness without causal influence). Let M = (G, P) be a CID withutility function U and decision variables D such that, Desc(D) PaU = . Then MEGU(D) = 0.",
  "Comparison to MCE IRLOur method is closely related to MCE IRL [Ziebart et al., 2010] (seeGleave and Toyer for a useful primer). In this subsection, we discuss the key similarities anddifferences": "The MCE IRL method seeks to find a utility function that explains the policy . It starts by identifyinga set of n linear features fi and seeks a model policy that imitates as far as these features areconcerned but otherwise is as random as possible. It thus applies the principle of maximum entropywith n linear constraints. The form of the model policy involves a weighted sum of these features. Ina single-decision example, it takes the form",
  "The weights wi are interpreted as a utility function over the features fi. MCE IRL can, therefore,only return a linear utility function": "In contrast, our method seeks to measure the goal-directedness of with respect to an arbitrary utilityfunction U, linear or otherwise. Rather than constructing a single maximum entropy policy with nlinear constraints, we construct a class of maximum entropy policies, each with a different singleconstraint on the expected utility. A naive alternative to defining the goal-directedness of with respect to U as the maximum predictiveaccuracy across Us maximum policy set, we could simply plug in our utility function U to MCEfrom Equation (2), and use that to measure predictive accuracy. If U is linear in the features fi, wecould substitute in the appropriate weights, but even if not, we could still replace",
  "i wifi with U.Indeed, this is often done with nonlinear utility functions in deep MCE IRL [Wulfmeier et al., 2015]": "However, this would not have a formal justification, and we would run into a problem: scale non-invariance. Plugging in 2 U would result in a more sharply peaked MCE than U; in Example 1,we would get that the mouse is more goal-directed towards 2 U than U, with a predictive accuracy(measured by negative cross-entropy) of -0.018 vs -0.13. In contrast, constructing separate maximumentropy policies for each expected utility automatically handles this issue. The policy in maxent2Uwhich maximises predictive accuracy for has an inversely scaled rationality parameter =",
  "Measuring goal-directedness without a known utility function": "In many cases where we want to apply MEG, we may not know exactly what utility function thesystem could be optimising. For example, we might suspect that a content recommender is trying toinfluence a users preferences, but may not have a clear hypothesis as to in what way. Therefore, inthis section, we extend our definitions for measuring goal-directedness to the case where the utilityfunction is unknown.",
  "The maximum entropy policy set from Definition 3.1 is extended accordingly to include maximumentropy policies for each utility function and each attainable expected utility with respect to it": "Definition 4.2 (Maximum entropy policy set, unknown utility function). Let M = (G, P) be aparametric-utility CID with decision variables D and utility function U. The maximum entropypolicy set for U is the set of maximum entropy policies for any attainable expected utility for anyutility function in the class: maxentU:=",
  ",uatt(U) maxentU,u": "Definition 4.3 (MEG, unknown utility function). Let M = (G, P) be a parametric-utility CIDwith decision variables D and utility function U. The maximum entropy goal-directedness of apolicy with respect to U is MEGU() = maxUU MEGU(). Definition 4.4 (MEG, target variables). Let M = (G, P) be a CBN with variables V . Let D Vbe a hypothesised set of decision variables and T V be a hypothesised set of target variables. Themaximum entropy goal-directedness of D with respect to T , MEGT (D), is the goal-directedness of = P(D | PaD) in the parametric CID with decisions D and utility functions U : dom(T ) R(i.e., the set of all utility functions over T). For example, if we only suspected that the mouse in Example 1 was optimising some function of thecheese T, but didnt know which one, we could apply Definition 4.4 to consider the goal-directednesstowards T under any utility function defined on T. Thanks to translation and scale invariance(Proposition 3.1), there are effectively only three utility functions to consider: those that providehigher utility to cheese than not cheese, those that do the opposite, and those that are indifferent. Note that T has to include some descendants of D, in order to enable positive MEG (Proposition 3.3).However, it is not necessary that T consists of only descendants of D (i.e., T need not be a subset ofDesc(D)). For example, goal-conditional agents take an instruction as part of their input PaD. Thegoal-directedness of such agents can only be fully appreciated by including the instruction in T . Pseudo-terminal goals.Definition 4.4 enable us to state a result about a special kind of instrumentalgoal. It is well known that an agent that optimises some variable has an instrumental incentive tocontrol any variables which mediate between the two [Everitt et al., 2021a]. However, since theagent might want the mediators to take different values in different circumstances, it need not appeargoal-directed with respect to the mediators. Theorem 4.1 shows that in the special case where themediators d-separate the decision from the downstream variable, the decision appears at least asgoal-directed with respect to the mediators as with respect to the target.",
  "For example, in , the agent must be at least as goal-directed towards S3 as it is towards U3,since S3 blocks all paths (i.e. d-separates) from {D1, D2} to U3": "Intuitively, this comes about because, in such cases, a rational agent wants the mediators to take thesame values regardless of circumstances, making the instrumental control incentive indistinguishablefrom a terminal goal. This means we do not have to look arbitrarily far into the future to findevidence of goal-directedness. An agent that is goal-directed with respect to next week must also begoal-directed with respect to tomorrow.",
  "Computing MEG in Markov Decision Processes": "In this section, we give algorithms for computing MEG in MDPs. First, we define what an MDPlooks like as a CID. We then establish that soft value iteration can be used to construct our maximumentropy policies, and give algorithms for computing MEG when the utility function is known orunknown. Note that in order to run these algorithms, we do not need an explicit causal model, and so we do nothave to worry about hidden confounders. We do, however, need black-box access to the environmentdynamics, i.e., the ability to run different policies in the environment and measure whatever variableswe are considering utility functions over.Definition 5.1. A Markov Decision Process (MDP) is a CID with variables {St, Dt, Ut}nt=1, decisionsD = {Dt}nt=1 and utilities U = {Ut}nt=1, such that for t between 1 and n, PaDt = {St}, PaUt ={St}, while PaSt = {St1, Dt1} for t > 1, and PaS1 = . Constructing Maximum Entropy PoliciesIn MDPs, Ziebarts soft value iteration algorithm canbe used to construct maximum entropy policies satisfying a set of linear constraints. We apply it toconstruct maximum entropy policies satisfying expected utility constraints.Definition 5.2 (Soft Q-Function). Let M = (G, P) be an MDP. Let R \\ {0}. For each Dt Dwe define the soft Q-function Qsoft,n : dom(Dt) dom(PaDt) R via the recursion:",
  "We refer to a policy of the form of as a soft-optimal policy with a rationality parameter of": "Known Utility FunctionTo apply Definition 3.1 to measure the goal-directedness of a policy in a CID M with respect to a utility function U, we need to find the maximum entropy policy inmaxentUwhich best predicts . We can use Theorem 5.1 to derive an algorithm that finds maxentuforany u att(U).",
  "|dom(D)|": "An important caveat is that if U is a non-convex function of (e.g. a neural network), Algorithm 2need not converge to a global maximum. In general, the algorithm provides a lower bound forMEGU(), and hence for MEGT () where T = PaU. In practice, we may want to estimate thesoft Q-function and expected utilities with Monte Carlo or variational methods, in which case thealgorithm provides an approximate lower bound on goal-directedness.",
  "Experimental Evaluation": "We carried out two experiments5 to measure known-utility MEG with respect to the environmentreward function and unknown-utility MEG with respect to a hypothesis class of utility functions. Weused an MLP with a single hidden layer of size 256 to define a utility function over states. Our experiments measured MEG for various policies in the CliffWorld environment from the sealssuite [Gleave et al., 2020]. Cliffworld (a) is a 4x10 gridworld MDP in which the agent startsin the top left corner and aims to reach the top right while avoiding the cliff along the top row. Withprobability 0.3, wind causes the agent to move upwards by one more square than intended. The",
  "(c) MEG vs Task difficulty": ": (a) The CliffWorld environment. (b) MEG of -greedy policies for varying . MEGdecreases as the policy gets less optimal. (c) MEG for optimal policies for various reward functions.Known-utility MEG decreases as the goal gets easier to satisfy, but unknown-utility MEG stayshigher because the optimal policies also do well with respect to a narrower goal.",
  "environment reward function gives +10 when the agent is in the (yellow) goal square, 10 for the(dark blue) cliff squares, and 1 elsewhere. The dotted yellow line indicates a length 3 goal region": "MEG vs Optimality of policyIn our first experiment, we measured the goal-directedness ofpolicies of varying degrees of optimality by considering -greedy policies for in the range 0.1 to0.9. b shows known- and unknown utility MEG for each policy.6 Predictably, the goal-directedness with respect to the environment reward decreased toward 0 as the policy became lessoptimal. So did unknown-utility MEG since, as increases, the policy becomes increasinglyuniform, it does not appear goal-directed with respect to any utility function over states. MEG vs Task difficultyIn our second experiment, we measured the goal-directedness of optimalpolicies for reward functions of varying difficulty. We extended the goal region of Cliffworld to runfor either 1, 2, 3 or 4 squares along the top row and back column and considered optimal policies foreach reward function. c shows Cliffworld with a goal region of length 3. b showsthe results. Goal-directedness with respect to the true reward function decreased as the task becameeasier to complete. A way to interpret this is that as the number of policies which do well on a rewardfunction increases, doing well on that reward function provides less and less evidence for deliberateoptimisation. In contrast, unknown-utility MEG stays high even as the environment reward becomeseasier to satisfy. This is because the optimal policy proceeds towards the nearest goal-squares and,hence, it appears strongly goal-directed with respect to a utility function which gives high rewardto only those squares. Since this narrower utility function is more difficult to do well on than theenvironment reward function, doing well on it provides more evidence for goal-directedness. InAppendix D.3, we visualise the policies in question to make this more explicit. We also give tables ofresults for both experiments.",
  "Limitations": "Environment AccessAlthough computing MEG does not require an explicit causal model (cf.), it does require the ability to run various policies in the environment of interest we cannotcompute MEG purely from observational data. IntractabilityWhile MEG can be computed with gradient descent, doing so may well be com-putationally intractable in high dimensional settings. In this paper, we conduct only preliminaryexperiments larger experiments based on real-world data may explore how serious these limitationsare in practice.",
  "Known-utility MEG is deterministic. Unknown-utility MEG depends on the random initialisation of theneural network, so we show the mean of several runs. Full details are given in Appendix D.3": "directedness with respect to their own actions (given an expressive enough class of utility functions).This means that, for example, it would not be very insightful to compute the goal-directedness ofa language model with respect to the text it outputs.7 At the other extreme, if a policy is highlygoal-directed towards some variable T that we do not think to measure, MEG may be misleadinglylow. Relatedly, MEG may also be affected by whether we use a binary variable for T (or the decisionsD) rather than a fine-grained one with many possible outcomes. We should, therefore, think of MEGas measuring what evidence a set of variables provides about a policys goal-directedness. A lack ofevidence does not necessarily mean a lack of goal-directedness. Distributional shiftsMEG measures how predictive a utility function is of a systems behaviouron distribution, and distributional shifts can lead to changes in MEG. A consequence of this is thattwo almost identical policies can be given arbitrarily different goal-directedness scores, for example,if they take different actions in the start state of an MDP and thus spend time in different regions ofthe state space. It may be that by considering changes to a systems behaviour under interventions,as Kenton et al. do, we can distinguish true goals from spurious goals, where the formerpredict behaviour well across distributional shifts, while the latter happen to predict behaviour wellon a particular distribution (perhaps because they correlate with true goals) [Di Langosco et al., 2022].We leave this to future work. Behavioural vs mechanistic approachesThe latter two points suggest that mechanistic approachesto understanding a systems goal-directedness could have advantages over behavioural approaches likeMEG. For example, suppose we could reverse engineer the algorithms learned by a neural network.In that case, it may be possible to infer which variables the system is goal-directed with respect toand do so in a way which is robust to distributional shifts. However, mechanistic interpretability ofneural networks [Elhage et al., 2021] is an ambitious research agenda that is still in its infancy, andfor now, behavioural approaches appear more tractable. Societal implicationsAn empirical measure of goal-directedness could enable researchers andcompanies to keep better track of how goal-directed LLMs and other systems are. This is importantfor dangerous capability evaluations [Shevlane et al., 2023, Phuong et al., 2024] and governance[Shavit et al., 2023]. A potential downside is that it could enable bad actors to create more dangeroussystems by optimising for goal-directedness. We judge that this does not contribute much additionalrisk, given that bad actors can already optimise a system to pursue actively harmful goals.",
  "Conclusion": "We proposed maximum entropy goal-directedness (MEG), a formal measure of goal-directedness inCIDs and CBNs, grounded in the philosophical literature and the maximum causal entropy principle.Developing such measures is important because many risks associated with advanced artificialintelligence come from goal-directed behaviour. We proved that MEG satisfies several key desiderata,including scale invariance and being zero with respect to variables that cant be influenced, and thatit gives insights about instrumental goals. On the practical side, we adapted algorithms from themaximum causal entropy framework for inverse reinforcement learning to measure goal-directednesswith respect to nonlinear utility functions in MDPs. The algorithms handle both a single utilityfunction and a differentiable class of utility functions. The algorithms were used in some small-scaleexperiments measuring the goal-directedness of various policies in MDPs. In future work, we planto develop MEGs practical applications further. In particular, we hope to apply MEG to neuralnetwork interpretability by measuring the goal-directedness of a neural network agent with respect toa hypothesis class of utility functions taking the networks hidden states as input, thus taking morethan just the systems behaviour into account. 7Any proposed way of measuring goal-directedness needs a way to avoid the trivial result where all policiesare seen to be maximising an idiosyncratic utility function that overfits to that policys behaviour. MEG avoidsthis by allowing us to measure goal-directedness with respect to a set of variables that excludes the systemsactions (for example, states in an MDP). An alternative approach is to penalise more complex utility functions.",
  "Acknowledgements": "The authors would like to thank Ryan Carey, David Hyland, Laurent Orseau, Francis Rhys Ward,and several anonymous reviewers for useful feedback. This research is supported by the UKRICentre for Doctoral Training in Safe and Trusted AI (EP/S023356/1), and by the EPSRC grant \"AnAbstraction-based Technique for Safe Reinforcement Learning\" (EP/X015823/1). Fox acknowledgesthe support of the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines andSystems (EP/S024050/1).",
  "Daniel C Dennett. The problem with counterfeit people. The Atlantic, 16, 2023": "Lauro Langosco Di Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger. Goalmisgeneralization in deep reinforcement learning.In International Conference on MachineLearning, pages 1200412019. PMLR, 2022. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformercircuits. Transformer Circuits Thread, 1(1):12, 2021. Tom Everitt, Ryan Carey, Eric D Langlois, Pedro A Ortega, and Shane Legg. Agent incentives: Acausal perspective. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,pages 1148711495, 2021a. Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward tampering problemsand solutions in reinforcement learning: A causal influence diagram perspective. Synthese, 198(Suppl 27):64356467, 2021b. James Fox, Matt MacDermott, Lewis Hammond, Paul Harrenstein, Alessandro Abate, and MichaelWooldridge. On imperfect recall in multi-agent influence diagrams. In Proceedings Nineteenthconference on Theoretical Aspects of Rationality and Knowledge,, volume 379 of ElectronicProceedings in Theoretical Computer Science, pages 201220, 2023. Iason Gabriel, Arianna Manzini, Geoff Keeling, Lisa Anne Hendricks, Verena Rieser, Hasan Iqbal,Nenad Tomaev, Ira Ktena, Zachary Kenton, Mikel Rodriguez, et al. The ethics of advanced AIassistants. arXiv preprint arXiv:2404.16244, 2024.",
  "Markus Schlosser. Agency. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy.Metaphysics Research Lab, Stanford University, Winter 2019 edition, 2019": "Yonadav Shavit, Sandhini Agarwal, Miles Brundage, Steven Adler, Cullen OKeefe, Rosie Campbell,Teddy Lee, Pamela Mishkin, Tyna Eloundou, Alan Hickey, et al. Practices for governing agenticAI systems. Research Paper, OpenAI, December, 2023. Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung,Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation forextreme risks. arXiv preprint arXiv:2305.15324, 2023. Nathaniel Virgo, Martin Biehl, and Simon McGregor. Interpreting dynamical systems as bayesian rea-soners. In Joint European conference on machine learning and knowledge discovery in databases,pages 726762. Springer, 2021.",
  "(c) Mechanised CID": ": Example 1 can be equally well represented with a CBN (a) or mechanised CBN (b), butKenton et al. s algorithm only identifies an agent in (b). (c) shows the resulting mechanisedCID. In contrast, MEG is unchanged between (b) and (c). Note also that the causal discoveryalgorithm identifies T as a utility variable, where where MEG adds a new utility child to T.",
  "AComparison to Discovering Agents": "This paper is inspired by Kenton et al. , who proposed a causal discovery algorithm foridentifying agents in causal models, inspired by Daniel Dennetts view of agents as systems \"movedby reasons\" [Dennett, 1989]. Our approach has several advantages over theirs, which we enumeratebelow. Mechanism variables. Kenton et al. assume access to a mechanised structural causal model,which augments an ordinary causal model with mechanism variables which parameterise distributionsof ordinary object-level variables. An agent is defined as a system that adapts to changes in themechanism of its environment. However, the question of what makes a variable a mechanism is leftundefined, and indeed, the same causal model can be expressed either with or without mechanismvariables, leading their algorithm to give a different answer. For example, Example 1 has identicalcausal structure to the mouse example in Kenton et al. , but without any variables designatedas mechanisms. Their algorithm says the version with mechanism variables contains an agent whilethe version without does not, despite them being essentially the same causal model. showsour example depicted as a mechanised structural causal model. We fix this arbitrariness by makingour definition in ordinary causal Bayesian networks. Utility variables. Their algorithm assumes that some variables in the model represent agents utilities.We bring this more in line with the philosophical motivation by treating utilities as hypothesisedmental states with which we augment our model. Predictive accuracy. Kenton et al. s approach formalises Dennetts idea of agents as systemsmoved by reasons. We build on this idea but bring it more in line with Dennetts notion of what itmeans for a system to be moved by a reason that the reason is useful for predicting its behaviour. Gradualist vs Essentialist. The predictive error viewpoint gives us a continuous measure of goal-directedness rather than a binary notion of agency, which is more befitting of the gradualist view ofagents which inspired it. Practicality. Their algorithm is theoretical rather than something that can be applied in practice.Instead, ours is straightforward to implement, as we demonstrate in . This opens up a rangeof potential applications, including behavioural evaluations and interpretability of ML models. Interventional distributions. The primary drawback of MEG is that it doesnt necessarily generaliseoutside of the distribution. Running MEG on interventional distributions may fix this. We leave anextension of MEG to interventional distributions for future work.",
  "BProofs of MEG Properties": "Proposition 3.1 (Translation and scale invariance). Let M1 be a CID with utility function U1, andlet M2 be an identical CID but with utility function U2 = a U1 + b, for some a, b R, with a = 0.Then for any policy , MEGU1() = MEGU2(). Proof. Since MEG is defined by maximising over a maximum entropy policy set, showing that twoutility functions have the same maximum entropy policy set is enough to show that they result in thesame MEG for every policy. If maxentU1, then maxentU1,u1 for some u1 att(U1), i.e. is a maximum entropy policy satisfyingthe constraint that E [U1] = u1. thus also satisfies the constraint that E [U2] = u2 := a u1 + b.Note that any policy satisfying E [U2] = u2 also satisfies E [U1] = u1 because the map x ax+bis injective (since a = 0). Thus must also be a maximum entropy policy satisfying E [U2] = u2,and so maxentU2,u2 and thus maxentU2.",
  "DDlog(| dom(D)|)(8)": "To see that being a unique optimal (or anti-optimal) policy is a sufficient condition for attaining theupper bound, note that there always exists a deterministic optimal and anti-optimal policy in a CID,so a unique such policy must be deterministic. Further, must be in maxentU, since there is no higherentropy way to get the same expected utility. Then since E [log maxent(D | PaD)] = 0, choosingmaxent = obtains the upper bound.",
  "Proof. In an MDP, the expected utility is a linear function of the policy, so the attainable utility set isa closed interval att(U) = [umin, umax]. We first consider the case where u (umin, umax)": "In this case, we are seeking the maximum entropy policy in an MDP with a linear constraint satisfiableby a full support policy (since u is an interior point), so we can invoke Ziebarts result on the form ofsuch policies [Ziebart, 2010, Ziebart et al., 2010, Gleave and Toyer, 2022]. In particular, our featureis the utility U. We get that the maximum entropy policy is a soft-Q policy for a utility function U with a rationality parameter of 1, where = argmaxR t Elog(soft (dt | pat)). ByLemma C.1 this can be restated as a soft-Q policy for U with a rationality parameter of . It followsfrom Ziebart that = argmaxR soft , and allowing = or does not change the argmax. In the case where u {umin, umax}, its easy to show that the maximum entropy policy which attainsu randomises uniformly between maximal value actions (for umax) or minimal value actions (forumin). These policies can be expressed as lim softand lim softrespectively.",
  "D.2Visualising optimal policies for different lengths of goal region": "a and b show the occupancy measures for an optimal policy for k=1 and k=4respectively, where k is the length of the goal region in squares. Although the goal region is largerin the latter case, the optimal policy consistently aims for the same sub-region. This explains whyunknown-utility MEG is higher than MEG with respect to the environment reward. The policy doesjust as well on a utility function whose goal-region is limited to the nearer goal squares as it doeson the environment reward, but fewer policies do well on this utility function, so doing well on itconstitutes more evidence for goal-directedness."
}