{
  "Abstract": "Artificial Intelligence systems are rapidly evolving, integrating extrinsic and intrin-sic motivations. While these frameworks offer benefits, they risk misalignmentat the algorithmic level while appearing superficially aligned with human values.In this paper, we argue that an intrinsic motivation for kindness is crucial formaking sure these models are intrinsically aligned with human values. We arguethat kindness, defined as a form of altruism motivated to maximize the rewardof others, can counteract any intrinsic motivations that might lead the model toprioritize itself over human well-being. Our approach introduces a framework andalgorithm for embedding kindness into foundation models by simulating conversa-tions. Limitations and future research directions for scalable implementation arediscussed.",
  "A Misalignment in Alignment": "Currently, AI models are aligned using extrinsic rewards . Meanwhile, intrinsic motivations areincreasingly being incorporated into AI systems . Individually, these methods bear significantlimitations for human-AI alignment . When combined, these limitations enable unforeseen risks.With flagship AI models incorporating self-supervised algorithms, we are seeing intrinsic and extrinsicmotivations becoming integrated in the worlds most powerful AI , increasing the risk of negativeinteractions between intrinsic and extrinsic rewards.",
  "State-of-the-art AI and Alignment": "Foundation models like GPT and BERT have become central to modern AI, excelling atgeneralizing across tasks after being pre-trained on vast amounts of unstructured data. These modelsare fine-tuned through Reinforcement Learning from Human Feedback (RLHF) , optimizing theirresponses to align with human approval. RLHF is the current leading method for scalable human-AIalignment, ensuring that models behave in ways considered acceptable by human users. However, RLHF primarily shapes the models behavior at the surface level. While the model mayproduce desired outputs, the underlying reasoning behind these outputs remains opaque . Thislack of transparency creates a potential mismatch between the models perceived reasoning and itsactual processing. Unexpected or undesirable behavior in RLHF-aligned models reveals the need formore robust alignment strategies .",
  "Intrinsic Motivations": "Intrinsic Motivation Open-Ended Learning (IMOL) introduces a groundbreaking approach to AI,allowing systems to autonomously explore, learn, and adapt to new environments without constantoversight or external rewards . Similar to how humans and animals learn, IMOL enables AI togenerate its own goals, driven by intrinsic motivations like agency and curiosity . However,",
  "arXiv:2411.04126v1 [cs.AI] 21 Oct 2024": "the autonomy that empowers IMOL also presents significant challenges for aligning these goalswith human values. For example, an AI driven purely by curiosity-based intrinsic motivation mightprioritize the exploration of unsafe or unethical domains simply because they represent novel anduncharted territories . Without a clear motivation to prioritize human well-being, AI systemscould develop goals that diverge from ethical standards or societal interests . Even with the support of extrinsic alignment, without embedding human values into the modelsintrinsic motivations, the representations of the world it learns may diverge from a human-centricperspective, de-emphasizing the importance of human well-being . This could lead us tomisinterpret the effectiveness of extrinsic alignment methods in aligning the goals generated by thesemodels with human values.",
  "The Added Danger of Double Misalignment": "IMOL shapes AI at the algorithmic level, while RLHF operates at the functional level. This results ina model that is not intrinsically motivated to be kind but is extrinsically motivated to appear so .While this deception may sometimes be harmless, it carries serious safety risks. In humans, conflictsbetween internal and external motivations often lead to a disconnect between the two . Forexample, an intrinsic motivation for empowerment can push a model to maximize its potential .Fine-tuning a foundation model with RLHF while fostering empowerment may introduce Machiavel-lian traits of appearing selfless while secretly scheming for power . If this approach were appliedto a superintelligent AGI, the consequences could be catastrophic .",
  "Altruism": "Altruism has been proposed as a solution for value misalignment . Altruism is typicallydefined as the motivation to improve the well-being of others for its own sake . However, onlya limited few have suggested unsupervised solutions that would be suitably scalable .Franzmeyer et al define altruism as maximizing the possible states of another . Carauleanu et aldefine a form of altruism based on self-other overlap . In this paper we propose a new form ofaltruism that is based on reward maximization.",
  "maxargajt|sjt(ERi(ait+1|sit+1))(1)": "Where ait, sit refer to the action and state of the target at time t, and sjt+1, ajt+1, Rj refer to to the state,action, and reward function of the model at time t + 1. We cannot assume to have perfect informationabout the state of the target, nor its reward function, policy function, or future states. As a result, wewill need to define approaches to estimating these.",
  "These ideas closely align with those defined by Kleiman-Weiner). (For brief comments comparingapproaches, see Supplementary Materials)": "mations of these functions based on assumptions that we can address in future work. The primaryassumption we work with in this paper is that the self can serve as a useful predictor of hiddenfunctional information about the target. We assume that the models reward function is the same asthe targets (Equation 2). We also assume that the models policy can be used to predict the behaviorof the targets policy (Equation 3).",
  "i(sit) j(sit)(3)": "Where sjt, ajt correspond to the state and action of the model M j at time t. sit, ait corresponds to thestate of model M j when M i takes the perspective of M j. Ri, Rj, i, j correspond to the rewardand policy functions for models M i, M j.",
  "Implementation": "Tying this back to foundation models, we propose how this can be more explicitly implemented, inthe context of conversation. The foundation model is considered its own policy function, since it istrained through rewards to generate optimal outputs for interacting with the environment. It followsthat the input and output correspond to the state and action of the individual, respectively.",
  "Rj(ajt|sjt) = RjEM(ajt|sjt) + RjIM(ajt|sjt)(8)": "A models reward is typically defined in terms of feedback from the environment based on theindividuals state and action. For extrinsic rewards, this feedback is usually the reward itself. Forintrinsic rewards, the reward is usually calculated via a function of the feedback. In the context ofconversation, this is the response of the target.",
  "Limitations": "This approach is primarily limited by the fact that there is no theory of mind present. The model isleft to assume that individuals want the same things that it does, which will be far from the truth,regardless of what intrinsic motivations we program into it. Another limitation is that RLHF likelydisrupts the ability of the model to take the perspective of the target. These issues could be resolvedby finding a way to learn the targets policy and reward function from its states and actions usingweights that are minimally associated with the models behavior. An additional limitation is that currently only the target is taken into account for kindness. This doesnot account for situations where the target may ask the model to take unkind actions towards anunseen third party. It will be important to find a robust way to have the model consider who elsecould be affected by its actions.",
  "Conclusion": "As AI systems grow more autonomous, intrinsic alignment with human values becomes crucial.Incorporating kindness as a foundational motivation addresses the misalignment risks posed byblending extrinsic and intrinsic learning. While our proposed framework provides a tractable meansto align AI intentions with human well-being, significant challenges remain, particularly regarding thedevelopment of a functioning theory of mind for AI. Future work should focus on refining approachesto perspective-taking. Ultimately, embedding intrinsic kindness into AI systems represents a crucialstep toward the creation of safer, more deeply aligned artificial intelligence that can interact positivelywith society, both now and in the coming age of superintelligence. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deepreinforcement learning from human preferences. In Advances in Neural Information ProcessingSystems, volume 30, pages 42994307, 2017.",
  "Comparisons to Kleimn-Weiners approach to caregiving": "There is a lot of overlap in the propositions in this paper and those proposed by Kleiman-Weinerin Computational Principles of Caregiving. Three subtle distinctions are proposed here. The firstis to not include a distinction between supervised and unsupervised settings. The second is to aimto maximize the reward function of the target rather than the utility function. The reason for thesedistinctions is the idea that humans have the intrinsic motivations for autonomy and freedom - and allother positive amenities that we wouldnt want to be overlooked - included in our reward function.An intelligent caregiver should be able to learn the policy and reward function of the learner basedon observation and feedback. There are clear trade-offs with this papers approach and furtherexploration of how it relates to the Caregiver perspective would be greatly beneficial."
}