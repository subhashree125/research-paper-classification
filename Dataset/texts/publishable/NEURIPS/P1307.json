{
  "Abstract": "Neuromorphic computing aims to replicate the brains capabilities for energy ef-ficient and parallel information processing, promising a solution to the increasingdemand for faster and more efficient computational systems. Efficient training ofneural networks on neuromorphic hardware requires the development of trainingalgorithms that retain the sparsity of spike-based communication during training.Here, we report on the first implementation of event-based backpropagation on theSpiNNaker2 neuromorphic hardware platform. We use EventProp, an algorithmfor event-based backpropagation in spiking neural networks (SNNs), to computeexact gradients using sparse communication of error signals between neurons. Ourimplementation computes multi-layer networks of leaky integrate-and-fire neu-rons using discretized versions of the differential equations and their adjoints, anduses event packets to transmit spikes and error signals between network layers. Wedemonstrate a proof-of-concept of batch-parallelized, on-chip training of SNNsusing the Yin Yang dataset, and provide an off-chip implementation for efficientprototyping, hyper-parameter search, and hybrid training methods.",
  "Introduction": "Neuromorphic computing seeks to emulate the unparalleled efficiency of biological neural net-works by implementing spiking neural networks which use sparse, spike-based communicationbetween neurons. This could enable artificial neuronal systems to process temporal, spike-baseddata with efficiency similar to biological brains. At the same time, backpropagation proved tobe a pivotal method in machine learning, allowing for efficient gradient computation and en-abling the recent advances in training non-spiking, artificial neural networks on challenging tasks[Goodfellow et al., 2016]. This suggests that implementing gradient-based learning algorithms onneuromorphic hardware could enable similar achievements while surpassing traditional hardwarein terms of energy efficiency. Such learning algorithms should make use of the temporal sparsityafforded by spike-based processing. Buildingonpreviousworkongradient-basedlearninginspikingneuralnetworks[Bohte et al., 2000], the EventProp algorithm [Wunderlich and Pehle, 2021] uses event-basedbackpropagation to compute exact gradients, retaining the advantages of temporal sparsity duringnetwork training. In contrast, non-spiking neural networks typically require a dense sampling ofneuronal variables for backpropagation which introduces a memory bottleneck, limits network sizeand increases energy consumption due to the required storage and transfer of dense state variablesamples [Pleiss et al., 2017, Ojika et al., 2020, Kumar et al., 2019].Harnessing the potential ofevent-based algorithms such as EventProp requires their implementation on suitable event-basedhardware architectures. Gradient-based learning using neuromorphic hardware has been previously implemented using in-the-loop approaches, where neuromorphic hardware computes spike times and state variables in aninference phase (the forward pass) while a classical computer computes gradients and implementsbackpropagation for the computation of surrogate gradients [Cramer et al., 2022, Esser et al., 2015]or exact gradients [Goltz et al., 2021, Pehle et al., 2023]. Previous work implemented the E-propalgorithm [Bellec et al., 2020] on SpiNNaker1 [Perrett et al., 2022] and an FPGA-based prototypeof SpiNNaker2 [Rostami et al., 2022].This algorithm computes surrogate gradients using lo-cal eligibility traces and a global error signal, without error backpropagation through networklayers, which might alter scalability.Besides SpiNNaker, the GeNN code generation frame-work [Knight et al., 2021] has been used to create a GPU-based implementation of EventProp[Nowotny et al., 2022]. This manuscript presents the first implementation of event-based backpropagation on SpiNNaker2.SpiNNaker2 is a novel digital neuromorphic hardware platform providing a scalable event-basedand asynchronous computational substrate [Gonzalez et al., 2024]. While our results are obtainedusing a single SpiNNaker2 chip, the platform can scale to large numbers of interconnected chips(e.g., more than 5 million compute cores in the Dresden SpiNNcloud platform [Mayr et al., 2019]).",
  "SpiNNaker2 Details": "SpiNNaker2 is a massively parallel compute architecture for event-based computing. It is basedon a digital, many-core GALS (Globally Asynchronous Locally Synchronous) configuration, whereevery chip is composed of 152 ARM-based Processing Elements (PEs) with dedicated acceleratorsfor both neuromorphic (e.g., true random number generators, exponential and logarithmic acceler-ators) and deep learning applications (e.g., multiply-and-accumulate arrays). The PEs are arrangedin a two-dimensional array and communicate via a dedicated high-speed Network-On-Chip (NoC).Communication to a host computer is established via 1 Gbit ethernet (UDP) and 2 GB of DRAM onthe board can be accessed via two LPDDR4 interfaces. For scalable, system-wide communication,each chip has a dedicated SpiNNaker2 packet router, containing configurable routing tables, andsix links to neighbouring chips. Different packet types with up to 128-bit payload allow for effi-cient communication between PEs, chips and boards. This will allow for SpiNNaker2 systems withup to 65 000 chips and over 10 million cores [Mayr et al., 2019]. Importantly, these ARM CortexM4F cores provide flexibility for the implementation of arbitrary neuron dynamics and event-basedmodels (e.g., [Subramoney et al., 2022]). A Python-based software package, PY-SPINNAKER2 [Vogginger et al., 2024], allows user to definenetwork architectures and parameters (py-spinnaker2) and is the main entry point for running ex-periments on-chip. The Python module interacts with the chip through an intermediate layer imple-mented in C++ that sends and receives data and loads up PEs with the memory files needed to runsimulations. Interactions between host and chip are detailed in .",
  "Programs on Processing Elements": "Our implementation comprises four programs running on different PEs of a chip: the first injectsinput spikes, the second simulates a layer of leaky integrate-and-fire neurons, the third computes atime-to-first-spike loss and the fourth accumulates gradients and computes weight updates. A common regularization method in machine learning is to process a subset of the training batch inparallel: gradients are averaged across a mini-batch [Goodfellow et al., 2016]. Mini-batch trainingcan achieve better generalization while speeding up training by processing training examples in agiven mini-batch in parallel. In the case of training on Spinnaker2, we can take advantage of theinherently parallel nature of the architecture to deploy identical copies of the network on multiplecores. Different copies of a network process different input samples, implementing mini-batch train-ing. Except for the optimisation program, all described programs are duplicated on the chip for eachtraining example processed in parallel. We detail each programs implementation next, and provide pseudo-code for each in 6.4. The archi-tecture and software stack is summarised in . This implementation showcases the flexibilityof SpiNNaker2 to implement custom training algorithms on neuromorphic hardware. While ourimplementations only uses 128 kB of SRAM available on each PE, future work will interface with2 GB of DRAM on the board to enable larger networks and more complex models.",
  "Host : High Level (python)Host : Low Level (C++)": ": Software and Hardware stack: A.1 shows the software architecture stack. The host experi-ment runner handles network creation, simulation parameters, initial data and mapping, all of whichare detailed in A.2. Necessary data is then sent on-chip. B. details the 4 main programs responsiblefor executing the eventprop algorithm on-chip, detailed later on. C. shows a potential mapping of amulti-batch simulation on the SpiNNaker2 chip. Input Spikes Program: This program injects input spikes representing samples of the input dataset into neurons of the first layer. We use a dedicated PE running this program to inject spikes intothe network for clarity and to optimise memory usage. Spike times for every sample are loadedat the beginning of the simulation and injected using event packets via the network-on-chip to PEs representing target layers. To process different samples of a training mini-batch in parallel, differentcores run the same program, but transmit different input samples to neuronal input layers during thesimulation. Neuron Layer Program: This program simulates a layer of leaky integrate-and-fire neurons bycomputing the forward and backward pass. It uses a clock-driven simulation based on the dis-cretized differential equations and their adjoints [Wunderlich and Pehle, 2021]. In a given time step,different network layers are computed asynchronously on different PEs and event packets represent-ing spikes (forward pass) or error signals (backward pass) are transmitted by the network-on-chipand buffered by the receiving PEs to update variables in the next time step. PEs store dense weightmatrices (in contrast to the original synapse-based implementation of SpiNNaker) and use incomingspikes payloads to retrieve the synaptic weight corresponding to the arriving spike and to updatethe synaptic current of the target neuron. During the backward pass, spikes are distributed in reverse order, and carry state variables repre-senting error signals within the 32 bit payload of event packets. At this point, the network runs inreverse-time and follows the adjoint dynamics, with error signals being transmitted at the spiketimes computed during the forward pass. In this way, the backward pass implements a spikingneural network with graded spikes (representing the error signal) at fixed times, making it suitedfor an event-based implementation on the neuromorphic platform. The backward pass implementsa discretization of the adjoint system [Wunderlich and Pehle, 2021], following an optimize-then-discretize approach. In this way, the program uses event-based backpropagation to compute a dis-cretized approximation to the exact gradients of the simulated spiking neural network. The completeset of discretized equations is provided in section 6.1. Time-to-First-Spike Loss Program: The loss program receives spikes from the output layer andcomputes the loss as well as the initial error signals sent to the output layer during the backward pass.At the beginning of the simulation, cores running the loss program are loaded with classificationlabels. After receiving spikes from the output layers, loss cores compute derivatives of a cross-entropy loss based on first spike times as given by",
  ",(1)": "where tsk is time step of the first spike of the kth neuron, l is the index of the neuron that should firefirst and 0, 1 and are hyper-parameters of the loss function. Loss cores compute error signalscorresponding to the derivative of eq. (1) which are sent back to the output layer at the respectivespike time steps using spike payloads. Optimisation Program: After the backward pass, each neuron layer has accumulated gradients. Asingle PE is then tasked with gathering gradients and optimisation of the weights. The PE runningthis program gathers gradients using direct memory access (DMA) from PEs processing differentsamples of a batch and sums them locally, implementing a mini-batch gradient update. The summedgradients are used to update an internal state which implements the Adam optimization algorithm[Kingma and Ba, 2017]. This state is used to compute a new set of weights which is written back toeach PE using DMA. The processing of the next sample is then triggered for all cores using a globalinterrupt signal, synchronizing the processing elements.",
  "Yin Yang Dataset": "We present learning results using the Yin-Yang dataset and networks dimensions . Totaltraining-set size is 5000, processing in parallel mini-batches of size 22 . We train for 40 epochs intotal, and we average results over 10 different random seeds (, left). These results demon-strate successful on-chip training using event-based backpropagation on SpiNNaker2 as well as aclose match between on- and off-chip simulations after training. In addition to batch-parallelised processing, we report on results where the networks are trained on asingle sample at a time, for a single epoch, on a limited training-set of 300 samples (, right).Such a scenario is particularly relevant for on-line learning on neuromorphic hardware, where thenetwork is trained on a continuous stream of data, which would be critical for embedded systems andautonomous agents. Again, the results show a close match between on-chip and off-chip simulations,and satisfying accuracy considering the heavy limitations.",
  "Off-chip simulation": "shows the differences of voltage traces (forward and backward) between off-chip and on-chip implementations when processing a single sample. The minor deviations, which are likelycaused by numerical differences, imply that the off-chip implementation can be used to optimisehyperparameters for on-chip deployment. we can see in that the resulting gradients andweights end up almost perfectly identical, only deviating of minor numerical values.",
  "Dataset": "traintest : Accuracy comparison between on- and off-chip simulations. On the left, we show thefinal accuracy of the models after training for 40 epochs on the complete dataset, for both testingand training sets. On the right we show the final accuracies reached in the online setting, afteronly seeing 300 samples one by one. Results are displayed using a standard boxplot, displaying themedian (labelled) and quartiles values of the distribution. All individual points are also overlayedon top. Naker implementation maintained real-time processing capabilities, successfully processing sam-ples within the 61ms (forward + backward + 1 optimization step) window. Power efficiency mea-surements revealed substantial differences between the implementations. The SpiNNaker imple-mentation exhibited minimal power consumption of under 0.5W even without leveraging dynamicpower management, whereas power measurements for the GPU-based implementation indicated aconsumption of 13.5 W for the RTX4070 GPU device alone. It is important to note that these GPUmeasurements exclude the power consumption of other system components such as the CPU, mem-ory, and peripheral devices, suggesting that the total system power draw would be notably higher.These GPU measurements were collected on a shared workstation environment where other sec-ondary processes may have slightly influenced power draw. A more rigorous comparison left forfuture work could include embedded GPU platforms such as the Jetson Nano, which would provideisolated testing conditions and precise power monitoring capabilities. Such platforms would allowfor measurement of both isolated GPU power consumption and total system power draw, potentiallyoffering a more edge-oriented comparison with the SpiNNaker implementations full-system mea-surements. A more comprehensive comparison would then require controlled testing environmentsand dedicated power monitoring infrastructure to draw definitive conclusions about the relative en-ergy efficiency of these approaches. However, the current measurements already suggest a superiorenergy efficiency for the SpiNNaker implementation, even when additional optimizations could stillbe applied. For instance, incorporating further optimizations associated with intrinsic mechanismsavailable in the hardware such as core-specific Dynamic Voltage and Frequency Scaling (DVFS)[Hoppner et al., 2019]. The profiling results for both platforms are shown in table 1.",
  "Discussion": "This work reports on the first, proof-of-concept implementation of event-based backpropagation onSpiNNaker2. While our results are limited by the intentional desire of using the SRAM memoryof each processing element, future work will leverage the 2 GB of DRAM available on the hostboard to enable larger networks and the processing of more complex data sets. However, scalingthe current implementation to larger networks and multi-chip systems will require addressing otheralgorithmic challenges. For instance, the discretization used in this implementation of EventPropmay impose constraints on the scalability and performance of multi-chip systems. Discretizationerrors accumulate and likely limit the scalability of this approach, especially for deep networks",
  "or networks with long temporal dependencies. This challenge could be addressed by using otherneuron models and numerical schemes that allow for scalable event-based neural networks": "While our work is based on spiking neurons, its applicability is not confined to this modelclass. Advances in event-based machine learning, such as the event-based gated recurrent unit[Subramoney et al., 2022], suggest the possibility of extending neuromorphic event-based back-propagation to models that use event-based processing with graded spikes. Hybrid hardware suchas SpiNNaker2 that supports both event-based communication and the acceleration of conventionaldeep neural networks facilitates the integration of dynamic sparsity benefits into broader machinelearning applications, leading to order-of-magnitude improvements in energy efficiency comparedto traditional computing [Nazeer et al., 2024]. Our on-line learning results also suggest that event-based backpropagation could be used for on-chiplearning in applications where training data arrives continuously. The computational complexity ofthe backward pass corresponds to that of the forward pass, making it feasible to use EventProp insuch a scenario even if the algorithm is not online strictly speaking. Moreover, it is possible toimplement reinforcement learning methods such as policy gradient [Sutton and Barto, 2018] in anon-line fashion, since EventProp only needs reward signals and not labels. Our framework could usethe off-chip simulation to train a base model that is then deployed and fine-tuned on the neuromor-phic hardware using data arriving in real time. To achieve efficient and adaptive on-chip fine-tuningacross a range of tasks, the off-chip simulation could implement an outer meta-training loop. Forexample, model agnostic meta learning (MAML) can be used to find an optimal initialisation thatensures quick and effective adaptation when deployed on-chip, and has been shown to be success-fully applicable to SNNs [Stewart and Neftci, 2022]. Such a hybrid approach could leverage andcombine the advantages of conventional and neuromorphic computing, and enable the deploymentof autonomous and adaptive agents on edge-devices. Our work provides a proof-of-concept implementation of event-based backpropagation using Event-Prop on SpiNNaker2. Event-based backpropagation reduces the demand for memory compared tobackpropagation-through-time in non-spiking neural networks and therefore allows for larger net-work sizes given a fixed amount of memory. At the same time, implementing event-based backprop-agation on natively event-based computational substrates such as SpiNNaker2 enables higher energyefficiency due to temporally sparse data transfers between neurons. Realising these advantages willrequire addressing the challenges outlined above, and incorporating gradient-based learning in spik-ing neural networks, which remains an active area of research. Our work demonstrates that the flex-ibility afforded by SpiNNaker2 can be used to implement future advances in event-based trainingmethods for spiking neural networks, and outline its applicability to other more generic event-basedMachine Learning models [Subramoney et al., 2022].",
  "Acknowledgement": "This project is partially funded by the EIC Transition under the SpiNNode project (grant num-ber 101112987), and by the Federal Ministry of Education and Research of Germany in the pro-gramme of Souveran. Digital. Vernetzt.. Joint project 6G-life, project identification number:16KISK001K. This work was partially funded by the German Federal Ministry of Education andReseach (BMBF) within the KI-ASIC project (16ES0996), and by the BMBF and the free state ofSaxony within the ScaDS.AI center of excellence for AI research. The authors would like to ac-knowledge the 2022 Telluride Neuromorphic Cognition Engineering Workshop where this projectwas initiated. [Bellec et al., 2020] Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., andMaass, W. (2020). A solution to the learning dilemma for recurrent networks of spiking neurons.Nature Communications, 11(1):3625.",
  "[Bohte et al., 2000] Bohte, S. M., Kok, J. N., and La Poutre, J. A. (2000). Spikeprop: backpropa-gation for networks of spiking neurons. In ESANN, volume 48, pages 419424. Bruges": "[Cramer et al., 2022] Cramer, B., Billaudelle, S., Kanya, S., Leibfried, A., Grubl, A., Karasenko, V.,Pehle, C., Schreiber, K., Stradmann, Y., Weis, J., Schemmel, J., and Zenke, F. (2022). Surrogategradients for analog neuromorphic computing. Proceedings of the National Academy of Sciences,119(4):e2109194119. [Esser et al., 2015] Esser, S. K., Appuswamy, R., Merolla, P., Arthur, J. V., and Modha, D. S.(2015). Backpropagation for energy-efficient neuromorphic computing. In Cortes, C., Lawrence,N., Lee, D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information ProcessingSystems, volume 28. Curran Associates, Inc. [Goltz et al., 2021] Goltz, J., Kriener, L., Baumbach, A., Billaudelle, S., Breitwieser, O., Cramer,B., Dold, D., Kungl, A. F., Senn, W., Schemmel, J., Meier, K., and Petrovici, M. A. (2021).Fast and energy-efficient neuromorphic deep learning with first-spike times. Nature MachineIntelligence, 3(9):823835. [Gonzalez et al., 2024] Gonzalez, H. A., Huang, J., Kelber, F., Nazeer, K. K., Langer, T., Liu, C.,Lohrmann, M., Rostami, A., Schone, M., Vogginger, B., Wunderlich, T. C., Yan, Y., Akl, M.,and Mayr, C. (2024). Spinnaker2: A large-scale neuromorphic system for event-based and asyn-chronous machine learning.",
  "[Ojika et al., 2020] Ojika, D., Patel, B., Reina, G. A., Boyer, T., Martin, C., and Shah, P. (2020).Addressing the Memory Bottleneck in AI Model Training. CoRR, abs/2003.08732": "[Paszke et al., 2019] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch:An imperative style, high-performance deep learning library.",
  "[Pleiss et al., 2017] Pleiss, G., Chen, D., Huang, G., Li, T., van der Maaten, L., and Weinberger,K. Q. (2017). Memory-Efficient Implementation of DenseNets. CoRR, abs/1707.06990": "[Rostami et al., 2022] Rostami, A., Vogginger, B., Yan, Y., and Mayr, C. G. (2022). E-prop onspinnaker 2: Exploring online learning in spiking rnns on neuromorphic hardware. Frontiers inNeuroscience, 16. [Stewart and Neftci, 2022] Stewart, K. M. and Neftci, E. O. (2022). Meta-learning spiking neu-ral networks with surrogate gradient descent.Neuromorphic Computing and Engineering,2(4):044002. Publisher: IOP Publishing.",
  "V j,lt+1 = V V j,lt (1 (V j,lt 1)) + (1 V )Ij,lt+1,(3)": "where V , I are the respective decay factors, W l(jk) is the weight matrix of the lth layerand () is the Heaviside theta function. The number of neurons in the lth layer is given by Nl andthe initial conditions are Ij,l0= V j,l0= 0 for all neurons. The decay factors are given by"
}