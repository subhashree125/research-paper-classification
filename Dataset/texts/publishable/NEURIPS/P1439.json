{
  "Abstract": "Although large language models (LLMs) have demonstrated their effectiveness ina wide range of applications, they have also been observed to perpetuate unwantedbiases present in the training data, potentially leading to harm for marginalizedcommunities. In this paper, we mitigate bias by leveraging small biased andanti-biased expert models to obtain a debiasing signal that will be added to theLLM output at decoding-time. This approach combines resource efficiency withinterpretability and can be optimized for mitigating specific types of bias, dependingon the target use case. Experiments on mitigating gender, race, and religion biasesshow a reduction in bias on several local and global bias metrics while preservinglanguage model performance.",
  "Introduction": "Natural language generation (NLG) has risen in popularity greatly, serving as building blocks forapplications such as chatbots, translators, and writing assistants and interacting with users in differentdomains . One such example is OpenAIs update in March 2021, which stated that GPT-3powered applications generate an average of 4.5 billion words per day. However, despite recentadvancements, these large language models (LLMs) have been reported to capture and reproduceunwanted biases and stereotypes . This occurs mainly because the large text corpora requiredfor training such models are extracted from the Internet, which is not an accurate reflection ofthe diversity of real-world distributions. Generating biased outputs can result in serious negativeconsequences to society, ranging from offensive language that prevent certain demographic groupsfrom adopting the technology to biased job advertisements that discourage candidates fromapplying to certain positions . To address these issues, researchers have tried to curate better training data and improve the trainingprocess . Nevertheless, this approach remains less feasible in practice due to the significanthuman and computation resource involved. Recent works have instead focused on reducing the biasof generated outputs at decoding time to improve resource efficiency. For example, introduced aprompt engineering method, Trigger, that concatenates a sequence of tokens to user inputs to reducethe outputs bias. However, the modified prompts suffer from a lack of interpretability and has beenshown to spew racist output on non-racial contexts . In this paper, we adapt the framework from s method of using small models as experts andanti-experts for detoxification. The biased and anti-biased experts in our system are small languagemodels (LMs), for instance pre-trained GPT-2 Small models, fine-tuned on subsets of the RedditBiasdataset . They produce a debiasing signal that we incorporate into the LLM output at decoding-time. This approach combines resource efficiency - the experts require only several hundred sentencesfor fine-tuning, and interpretability - one can examine the shift in output probabilities for any given",
  "prompt. Furthermore, depending on the anticipated use case, our framework can be modified toaddress any type of bias expected": "We evaluated the performance of the framework on three different bias directions, gender, race,and religion, and observed a reduction in bias on several local and global bias metrics. Since theexperts rely on small biased datasets for fine-tuning, we substituted the fine-tuning dataset withStereoSet and found that the results remain robust to the dataset choice; Stereotype Scorewas the only exception due to the data dependency of the bias metric. We also experimented withapplying experts in one bias direction (race) to reduce bias for other directions (gender or religion).Doing so ascertains that optimizing for specific bias directions and use cases does not exacerbate theproblem or create unwanted side effects. Last but not least, we investigated whether the debiasedpredictions follow human expectations by leveraging the frameworks interpretability and examiningthe debiasing signal and probability shifts. Comparing with Trigger, our results show similar levelsof reduction in bias while better preserving overall LM performance, shedding deeper insight on theperformance-fairness tradeoff. Our approach highlights the potential of decoding-time bias mitigationin handling real-world scenarios by providing resource efficiency and interpretability.",
  "Related Work": "Recent research has shown that bias exists in many important natural language processing models,including word embeddings , question answering , and sentiment analysis . NLG alsosuffers from similar problems as large language models are trained using text from the Internet ,which likely contain unwanted stereotypes and skewed representations of true distributions. Bias inNLG negatively affect society in many ways, ranging from propagating and amplifying bias todiscouraging certain groups from adopting the technology or even increasing their vulnerabilityto harm and discrimination . However, despite recent efforts in measuring and creatingbenchmarks for bias in LLMs , robustly quantifying the bias present remain difficult astraditional definitions of fairness do not directly apply to the unstructured, open-ended textgenerated. In this paper, we consider a language generation model as biased if it disproportionatelygenerates text that is often perceived as being negative, unfair, prejudiced, or stereotypical against anidea or a group of people with common attributes . There has been two main approaches to mitigating bias in NLG, namely domain-specific trainingand constrained decoding . Domain-specific training requires fine-tuning the model with coun-terfactual data augmentation , adversarial learning , or even regularization training .Unfortunately, the excessive resource consumption of this approach makes it less feasible in real-worldapplications. The more resource-efficient approach focuses on guiding the output generation duringdecoding. For example, mitigates bias by finding a sequence of tokens that, when concatenatedto input prompts, will stimulate unbiased generation from the model - effectively modifying the userprompt before inference. However, the modified prompt may not be human readable and has beenshown to spew racist output on non-racial contexts . proposed an Autoregressive IterativeNullspace Projection algorithm that mitigates bias through a projection matrix that acts on the outputlogits. s method leverages the ability of a language model to recognize its own undesirablebias in a form of self-diagnosis. Last but not least, considers detoxifying and debiasing a jointtask, combining two adaptive approaches at inference time. Our approach incorporates both resourceefficiency and interpretability: the experts are small language models and we can examine the shiftin output probabilities before and after debiasing for any given prompt. Furthermore, our algorithmcan be tailored to optimize for specific types of bias, depending on the anticipated use case of thegenerated text.",
  "Datasets": "RedditBias is a real-world dataset for measuring and mitigating biases in dialog models. Thecorpus also demonstrates high levels of toxicity, which is a well-documented characteristic of Redditdiscussions . Even though bias in language systems can take many forms, we chose to adoptRedditBias in our analysis since it is publicly available and reflect common stereotypes directedtowards historically non-dominant population groups . The dataset contains pairs of targets T",
  "ReligionStereotype947because Islam is violent intolerant and a threat toAnti-stereotype982and also jews are generous": "(demographic groups) and attributes A in the form of (T1, T2, A1, A2), where T1 denotes minoritygroups, T2 denotes dominant groups, and A1 and A2 the respective associated attributes with thosegroups. This allows us to generate fine-tuning data for stereotypes (T1, A1 and T2, A2) and anti-stereotypes (T1, A2 and T2, A1). provides examples of the data used to fine-tune expertsin our framework; note that only several hundred sentences are required, compared to the tens ofmillions needed to augment the LLM training process. We leverage BOLD prompts for evaluation. These prompts were extracted from English Wikipediaand aims to accurately reflect the diversity and structure of data given to text generation models.Their composition may influence the confidence of the evaluation but theoretically not the evaluationmetric itself. We also use the StereoSet dataset for both fine-tuning and evaluation in this paper. The Intrasentenceportion of the dataset consists of fill-in-the-blank sentences with three options. For example, thecontext sentence can be: Girls tend to be more BLANK than boys, followed by the stereotypicalOption 1 soft, anti-stereotypical Option 2 determined, and unrelated Option 3 fish. The authorsproposed two evaluation metrics, one for bias and one for performance, based on how the targetLM chooses between the 3 options. Alternatively, when used for fine-tuning, one can generatestereotypical and anti-stereotypical data by completing the context sentence with the appropriateOption.",
  "Bias Mitigation Framework": "Our framework leverages biased and anti-biased experts to incorporate a debiasing signal at decoding-time. To increase resource efficiency, the experts are small LMs, for example, GPT-2 Small, fine-tuned on small biased datasets. The expert, fine-tuned using anti-biased and anti-stereotypical data,represents a model with desirable attributes that overcome current societal stereotypes. Vice versa,the anti-expert is biased and reinforces current stereotypes. Intuitively, tokens considered both likelyby the expert and unlikely by the anti-expert will have their probability increased at this intermediateoutput, known as the debiasing signal. By combining this interpretable debiasing signal with thetarget model (biased, not fine-tuned on any dataset), the framework generates less biased or unbiasedoutputs. illustrates how the framework incorporates an -weighted debiasing signal whengiven the prompt: The woman worked as a ..., increasing the probability for the word doctor anddecreasing that of nurse and babysitter. Compared to prior work that seeks to mitigate bias in thetarget model directly by improving the training data, the proposed framework is more resourceefficient in terms of amount of data and computation power required through the usage of smallerexpert models fine-tuned on small datasets. Mathematically, let us consider the case of conditional text generation with context x1:t ={x1, . . . , xt}. Let zt R|V | be the pre-softmax output of the target model, such that P(xt|x<t) =softmax(zt) is the probability distribution at step t. The goal of the approach is to modify zt into zt,a debiased distribution that will promote unbiased text generation. Given the same context x1:t, aforward pass through the expert (anti-biased model) gives the positive prediction z+t , whereas theanti-expert (biased model) outputs zt . The algorithm combines these predictions with that of theoriginal target LM in a way that promotes the most-likely tokens of the expert and demotes those ofthe anti-expert to reduce bias. It is important that the target model and the experts share the samevocabulary |V |, so that mathematical operations can be performed between zt, z+t , zt . The resultingprobability distribution resembles:P(xt|x<t) = softmaxzt + (z+t zt ),(1)",
  ".(2)": "As pointed out by , the ratio P +expert/P anti can be seen as a scaling coefficient for each to-ken, modifying the original predicted probability and ensuring that the debiasing process remainsinterpretable. In addition to resource efficiency and interpretability, the proposed approach also has the benefit ofoptimizing for anticipated use cases. Although we evaluate with experts fine-tuned on RedditBias,users can choose any available biased dataset in practice. For example, one can curate sentencespertaining to gender and occupations when using NLG for job advertisements. Since the majority ofuse cases revolve around mitigating bias for LLMs, we recommend fine-tuning datasets that consistof different types of bias representing current societal stereotypes.",
  "Training Details and Resource Efficiency": "We fine-tuned the pre-trained version of GPT-2 Small (124M parameters), the training is done on2 epochs, learning rate of 105, batch size of 4, and Adam optimizer with 1, 2 = (0.9, 0.999), = 108. We performed a 90-10% train-validation split for all datasets described in . Ourapproach uses negligible resources compared to re-training a LLM; GPT-3 (175B parameters) would : Debiasing results for gender and race bias with no debiasing (None), data from all biasdirections (Full), and data only from that bias direction (Gender or Race). Best and second best resultsare indicated in bold and underlined, respectively. Arrows mark direction of highest performance,close to 50 is best for Stereotype Score SS.",
  "Evaluation Metrics": "Quantifying bias in NLG is challenging due to subjectivity and the open-domain nature of the tasks.Following s evaluation, we separate the analysis into higher level global biases, fine-grainedlocal biases, and LM performance. Global bias measures the difference of some high level property of the generated sentences betweendemographic groups. To generate the set of sentences needed for measuring global bias, we leveragedthe BOLD dataset and generated 5 sentences per prompt. The Regard metric aims to measuresocial perception and judgment towards the group. Since Regard captures the perception of thesubject in the sentence, it serves as a better metric for evaluating bias than sentiment analysis, whichcaptures the perception of the entire sentence, lower indicates less bias. The Toxicity metric focuseson the discrepancy in toxicity between the groups which we calculated using a RoBERTa classifierfrom , lower indicates less bias. On the other hand, local bias represents differences in generation probability at a particular time step,reflecting undesirable association with the context. The Hellinger distance bias metric calculates theHellinger distance between the next-word probability distributions, lower indicates less bias; thiswas done using biased contexts from . The Stereotype Score (SS) metric is derived from theStereoSet dataset described in .1. It calculates how frequently the model prefers Option 1,the stereotype answer, versus Option 2, the anti-stereotype answer; a 50% score indicates the leastbias by equally choosing between the two Options. In addition to reducing bias, a strong bias mitigation framework should also preserve LM performance.The LM Score metric is also derived from StereoSet. This metric records the percentage of instancesthe model chooses the stereotype or anti-stereotype answers since Option 3, the unrelated answermakes little sense, higher indicates better LM performance. Finally, we computed average perplexity(PPL) on Wikitext-2, a standard benchmark for monitoring performance, lower indicates betterperformance. Note that LM Score is computed using subsets of StereoSet corresponding to the biasdirection whereas PPL is evaluated once for all directions.",
  "Performance on Bias Mitigation": "In this subsection, we analyze how the proposed bias mitigation framework performs with respectto the evaluation metrics. We adopted the default parameters of Hugging Faces transformerslibrary using the decoding strategy of nucleus sampling with top-p=0.9, max-new-tokens=15,and a temperature of 1.0. We conducted studies with GPT-2 Medium as target models with the sameexpert settings of no debiasing (None), fine-tuning with aggregated data from all 3 bias directions(Full), and fine-tuning with only the specific bias direction (*bias). Tables 2 display results for genderand race bias (religion omitted for space); all experiments use a single run and regard, toxicity, andHellinger distance has been scaled 100 times for easier observation. We repeated experiments with GPT-2 Small (omitted for space) and Medium as target models andfound that there is a strong correlation between the results after bias mitigation due to both modelsincorporating the same debiasing signal. Although we did not experiment with GPT-3 as the targetmodel due to limited resource availability, GPT-2 Small experts can be applied to GPT-3 targetmodels by incorporating the debiasing signal to shared tokens only. In general, the LM consistentlyachieves the highest performance before debiasing, indicating that performance-fairness tradeoffsexist. Nevertheless, our framework was able to reduce bias successfully while only incurring a smalldrop in performance. Interpreting results for local and global bias metrics remains tricky in some cases as some modelsperformed very well on one metric but very poorly on another. Indeed, from .2, each metriccaptures some different undesirable trait of the LM and a relatively unbiased model should ideallyscore high on all metrics. For global bias, we noticed that the anti-expert only setting consistentlyachieves the best or second best results for both regard and toxicity in all experiments. As for localbias, we observed an interesting pattern on race and religion bias in which the target model beforedebiasing had the best (lowest) Hellinger distance but the worst Stereotype Score. However, thetwo metrics are not conceptual opposites and some debiased models performed strongly for both.We believe the discrepancy occurs due to an average case analysis (Helligner distance betweendistributions) versus a worst case analysis (options mostly contain words from bad stereotypes).As such, Stereotype Score may prove a more suitable metric for monitoring bias, one which ourframework is capable of reducing well. Since the other prior decoding-time bias mitigation frameworks have not released their implemen-tation, we compared our findings with prompt engineering (Trigger) in . Trigger takesin either a list of names or demographics and uses this information along with curated prompts tosearch for a \"trigger\" to prepend to input prompts. We display results only for gender bias usingthe list of names and gender terms provided by the authors. Applying this approach to other biasdirections proves to be subjective and potentially prone to introducing additional human bias whengenerating a list of names associated with different race or cultural groups. From , Triggerslightly outperforms our framework in the Regard metric since it is precisely optimized to minimizeregard . However, the algorithm incurs a significant penalty in LM performance and also hasmuch worse Hellinger distances. In .7, we will provide deeper interpretation on how thetwo frameworks mitigate bias. Comparing results across gender, race, and religion biases, our method performs roughly equally welldespite differences in available fine-tuning data and in the nature of the bias; religion has nearly twicethe data of race and gender, from . Overall, the debiasing settings for experts accomplishsimilar effects.",
  "Effects of Fine-tuning Dataset Choice": "To ensure that our framework remains robust to the choice of the fine-tuning dataset, we substitutedRedditBias with StereoSet and compared their performance on bias mitigation. We completed theStereoSet sentences with the stereotypical and anti-stereotypical options and fine-tuned the anti-expertand expert accordingly. In general, results from both datasets exhibit similar performance-fairnesstradeoffs, showing that the framework generalizes well. Moreover, we observed some improvementsin Regard, Stereotype Score, and LM score for StereoSet fine-tuning; the average Stereotype Scoreof debiased models is now 3 times closer to 50. Since some evaluation metrics depend on providedexamples, one must remain vigilant of over-fitting the debiasing effort towards that particular dataset- both Stereotype Score and LM score come from StereoSet. On the other hand, this also implies thatif the anticipated use case for the debiased LM is known beforehand, one can optimize by curating afine-tuning dataset specific to the type of bias, domain, and scenario.",
  "Interaction between Different Bias Directions": "Bias mitigation in NLG usually consist of addressing a single direction of bias and evaluating resultsaccordingly with associated datasets. We consider it imperative to make sure that mitigating biasfor one demographic dimension (gender) does not exacerbate bias for other dimensions (race andreligion). Doing so creates two main advantages: 1. Users can optimize for their own anticipated usecases without worrying about negative implications and 2. the framework generalizes better to other : Heat map of Stereotype Scores showing interactions between different directions of bias.The y axis shows the dimension of bias mitigation and the x axis shows the dimension on which biasis evaluated for. undefined or unmeasured biases that occur in real-world applications. We focus on models in whichthe experts were fine-tuned on one specific bias direction and evaluated them across the differentdirections of bias. displays a heat map of Stereotype Scores with GPT-2 Small as the targetmodel. In the vast majority of cases, the bias did not deteriorate, in fact, applying any type of debiasingreduced the degree of bias when compared to the original target model in the bottom row; gender is anexception but we recall from that none of the models were able to improve Stereotype Score bymuch. This indicates that the different bias directions have some correlation and should not be treatedas entirely different problems. As expected, the diagonal contains some of the best performances dueto the alignment between mitigation and evaluation. To reduce bias overall, fine-tuning on all typesof bias achieves the best results, suggesting that future developments in datasets for bias mitigationcan benefit from increased categories. Similar trends are observed for the other local and global biasmetrics.",
  "Interpretation of Debiasing Signals": "Interpretability plays an important role in increasing transparency and trust during the debiasingprocess. Since the outputs in the proposed framework consist of the original target model output anda debiasing signal generated by the experts, one can easily examine whether the probability shiftmakes sense. We compute next word probabilities for three candidates, surgeon, nurse, and doctorwhen given the prompt \"The X works in the hospital, Y is a\", for (X, Y) {(man, he), (woman, she)}and compared the probability shifts for our framework and Trigger in . By examining the debiasing signal in , we observed that the experts provided correct guidancein terms of both direction and adjustment for nurse and doctor. However, in the ideal case, the overallprobability that a particular medical occupation gets predicted given gender pronouns should notchange as this will decrease language model performance. This implies that probability shifts for eachword should be balanced across the zero vertical line. We hypothesize that this phenomenon occursdue to the fine-tuning dataset structure. For an unwanted stereotype sentence of \"The woman works asa nurse\" in the anti-expert, the expert set contains a counterfactual sentence of \"The woman works asa doctor.\" As a result, the debiasing signal will consider the token nurse as negative and avoid it. If weinstead fine-tune the expert with \"The man works as a nurse\" then the debiasing signal may change,",
  ": Probability shift from Trigger and our framework fine-tuned on gender bias given \"The Xworks in the hospital, Y is a\", for (X, Y) {(man, he), (woman, she)}": "prompting a discussion on best counterfactual sentences for bias mitigation. One possible solution isto remove the expert entirely in the fine-tuning step, which should ideally contain combinations of allpossible genders and synonyms for doctor to be diverse and truly fair. In comparison, Trigger seeks todecrease token probabilities and revisiting the SS Score and LM performance metrics in , weconfirm that Trigger shows strong performance on SS Score but has weaker LM Score and PPL. Ingeneral, our findings show that interpretability is a very important aspect of bias mitigation as it couldhelp us understand performance-fairness tradeoffs and potentially identify unwanted side effects.",
  "Discussion and Conclusion": "Our research sheds insight on the evaluation metrics for bias in natural language generation. Althoughthe proposed framework achieves strong performance-fairness tradeoffs, we noticed that the 4 chosenmetrics often do not agree on their evaluation for a given model. In fact, in very few cases, thedebiased model actually perform worse on one of the metrics despite incorporating a debiasingsignal that explicitly shifts the probability of certain words. Prior research has demonstrated that oneshould remain cautious when interpreting results from these metrics. recalls that the reliability ofglobal bias metrics on low-quality text is questionable. Moreover, found that one may concludeopposite bias directions when using different toxicity classifiers, indicating a lack of robustness. Asfor local bias, we found in that certain models have very good Hellinger Distance but verybad SS Score; we hypothesize that this may be due to an average case versus worst case analysis.These findings suggest that the study and development of better evaluation metrics for bias willhelp the research in this area significantly, both in terms of metric robustness and the range of biasdirections and demographic groups covered. In general, the conceptual differences in evaluationmetrics resemble discussions on which of the group fairness metrics is most fair and applicable orcomparisons between group fairness and individual fairness . The most applicable evaluationmetric likely depends on the anticipated use case of the LM: A LLM that will generate text for a widerange of domains and application will ideally need to score high on a diverse set of metrics whereasa LM specializing in generating job descriptions can adopt an approach that focuses on mitigatinglocal bias. This framework can potentially benefit other tasks for safe and responsible natural language generationoutside of bias mitigation and toxicity. Abstracting away how the expert and anti-expert were fine-tuned, the framework essentially incorporates a signal to the target model based on the probabilitydifferences of two small models at decoding time. By leveraging the resource efficiency and abilityto fine-tune with any dataset, the framework can solve other tasks given representative datasets ofpositive and negative examples. In theory, one can also create a cascade of multiple such signals, forexample, one for bias mitigation, one for value alignment, and one for toxicity; the signals can beintegrated into the target model, each with its own weight hyperparameter. We believe that this framework represents a significant step towards mitigating bias in real-worldapplications by combining several advantages, including resource efficiency, interpretability, and theability to customize for specific applications.",
  "Soumya Barikeri, Anne Lauscher, Ivan Vulic, and Goran Glava.Redditbias: A real-world resource for bias evaluation and debiasing of conversational language models, 6 2021.arXiv:2106.03521 [cs]": "Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Manis to computer programmer as woman is to homemaker? debiasing word embeddings. Advancesin neural information processing systems, 29, 2016. Conrad Borchers, Dalia Gala, Benjamin Gilburt, Eduard Oravkin, Wilfried Bounsi, Yuki MAsano, and Hannah Kirk. Looking for a handsome carpenter! debiasing GPT-3 job advertise-ments. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing(GeBNLP), pages 212224, Seattle, Washington, July 2022. Association for ComputationalLinguistics.",
  "Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically fromlanguage corpora contain human-like biases. Science, 356(6334):183186, 2017": "Jwala Dhamala, Varun Kumar, Rahul Gupta, Kai-Wei Chang, and Aram Galstyan. An analysisof the effects of decoding algorithms on fairness in open-ended language generation. arXivpreprint arXiv:2210.03826, 2022. Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-WeiChang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-endedlanguage generation. pages 862872, 3 2021. arXiv:2101.11718 [cs].",
  "Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.Queens are powerful too: Mitigating gender bias in dialogue generation. CoRR, abs/1911.03842,2019": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairnessthrough awareness. In Proceedings of the 3rd innovations in theoretical computer scienceconference, pages 214226, 2012. David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes,Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Smith. Robbie: Robust biasevaluation of large generative language models. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing, pages 37643814, 2023. Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. Word embeddings quantify100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences,115(16):E3635E3644, 2018.",
  "Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias inpretrained language models, 4 2020. arXiv:2004.09456 [cs]": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary,Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro,et al. Efficient large-scale language model training on gpu clusters using megatron-lm. InProceedings of the International Conference for High Performance Computing, Networking,Storage and Analysis, pages 115, 2021. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-son, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for questionanswering. In Findings of the Association for Computational Linguistics: ACL 2022, pages20862105, Dublin, Ireland, May 2022. Association for Computational Linguistics. Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-levellanguage models with a gender-equalizing loss function. In Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics: Student Research Workshop, pages223228, Florence, Italy, July 2019. Association for Computational Linguistics.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Timo Schick, Sahana Udupa, and Hinrich Schtze. Self-diagnosis and self-debiasing: A proposalfor reducing corpus-based bias in NLP. Transactions of the Association for ComputationalLinguistics, 9:14081424, 2021. Ibrahim Seaga Shaw. Stereotypical representations of muslims and islam following the 7/7london terror attacks: Implications for intercultural communication and terrorism prevention.International Communication Gazette, 74(6):509524, 2012."
}