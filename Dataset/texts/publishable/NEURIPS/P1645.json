{
  "Abstract": "Solving societal problems on a global scale requires the collection and processingof ideas and methods from diverse sets of international experts. As the numberand diversity of human experts increase, so does the likelihood that elements inthis collective knowledge can be combined and refined to discover novel and bettersolutions. However, it is difficult to identify, combine, and refine complementaryinformation in an increasingly large and diverse knowledge base. This paper arguesthat artificial intelligence (AI) can play a crucial role in this process. An evolu-tionary AI framework, termed RHEA, fills this role by distilling knowledge fromdiverse models created by human experts into equivalent neural networks, whichare then recombined and refined in a population-based search. The framework wasimplemented in a formal synthetic domain, demonstrating that it is transparent andsystematic. It was then applied to the results of the XPRIZE Pandemic ResponseChallenge, in which over 100 teams of experts across 23 countries submittedmodels based on diverse methodologies to predict COVID-19 cases and suggestnon-pharmaceutical intervention policies for 235 nations, states, and regions acrossthe globe. Building upon this expert knowledge, by recombining and refining the169 resulting policy suggestion models, RHEA discovered a broader and moreeffective set of policies than either AI or human experts alone, as evaluated basedon real-world data. The results thus suggest that AI can play a crucial role inrealizing the potential of human expertise in global problem-solving.",
  "Introduction": "Integrating knowledge and perspectives from a diverse set of experts is essential for developing bettersolutions to societal challenges, such as policies to curb an ongoing pandemic, slow down and reverseclimate change, and improve sustainability . Increased diversity in human teamscan lead to improved decision-making , but as the scale of the problem and size of theteam increases, it becomes difficult to discover the best combinations and refinements of availableideas . This paper argues that artificial intelligence (AI) can play a crucial role in this process,making it possible to realize the full potential of diverse human expertise. Though there are many AIsystems that take advantage of human expertise to improve automated decision-making ,an approach to the general problem must meet a set of unique requirements: It must be able toincorporate expertise from diverse sources with disparate forms; it must be multi-objective sinceconflicting policy goals will need to be balanced; and the origins of final solutions must be traceableso that credit can be distributed back to humans based on their contributions. An evolutionary AIframework termed RHEA (for Realizing Human Expertise through AI) is developed in this paper tosatisfy these requirements. Evolutionary AI, or population-based search, is a biologically-inspiredmethod that often leads to surprising discoveries and insights ; it is also a natural fithere since the development of ideas in human teams mirrors an evolutionary process .Implementing RHEA for a particular application requires the following steps ():",
  ". Distill4. Evolve": ": The RHEA (Realizing Human Expertise through AI) framework. The framework consistsof four components: Defining the prediction and prescription tasks, gathering the human solutions,distilling them into a canonical form, and evolving the population of solutions further. a, The predictormaps context and actions to outcomes and thus constitutes a surrogate, or a digital twin, of thereal world. For example, in the Pandemic Response Challenge experiment, the context consistedof data about the geographic region for which the predictions were made, e.g., historical data ofCOVID-19 cases and intervention policies; actions were future schedules of intervention policiesfor the region; and outcomes were predicted future cases of COVID-19 along with the stringencyof the policy. b, Given a predictor, the prescriptor generates actions that yield optimized outcomesacross contexts. c, Humans are solicited to contribute expertise by submitting prescriptors usingwhatever methodology they prefer, such as decision rules, epidemiological models, classical statisticaltechniques, and gradient-based methods. d, Each submitted prescriptor is distilled into a canonicalneural network that replicates its behavior. e, This population of neural networks is evolved further,i.e., the distilled models are recombined and refined in a parallelized, iterative search process. Theybuild synergies and extend the ideas in the original solutions, resulting in policies that perform betterthan the original ones. For example, in the Pandemic Response Challenge, the policies recommendinterventions that lead to minimal cases with minimal stringency.",
  ". Evolve. Recombine and refine the distilled solutions using a population-based search torealize the complementary potential of the ideas in the expert-developed solutions": "RHEA is first illustrated through a formal synthetic example below, demonstrating how this processcan result in improved decision-making. RHEA is then put to work in a large-scale internationalexperiment on developing non-pharmaceutical interventions for the COVID-19 pandemic. The resultsshow that broader and better policy strategies can be discovered in this manner, beyond those thatwould be available through AI or human experts alone. The results also highlight the value ofsoliciting diverse expertise, even if some of it does not have immediately obvious practical utility: AImay find ways to recombine it with other expertise to develop superior solutions. To summarize, the main contributions of this paper are as follows: (1) Recognizing that bringingtogether diverse human expertise is a key challenge in solving many complex problems; (2) Identifyingdesiderata for an AI process that accomplishes this task; (3) Demonstrating that existing approachesdo not satisfy these desiderata; (4) Formalizing a new framework, RHEA, to satisfy them; (5)Instantiating a first concrete implementation of RHEA using standard components; and (6) Evaluatingthis implementation in a global application: The XPRIZE Pandemic Response Challenge.",
  "Illustrative Example": "In this section, RHEA is applied to a formal synthetic setting where its principles and mechanics aretransparent. It is thus possible to demonstrate how they can lead to improved results, providing aroadmap for when and how to apply it to real-world domains (see App. B for additional details). Consider a policy-making scenario in which many new reasonable-sounding policy interventions areconstantly being proposed, but there are high levels of nonlinear interaction between interventionsand across contexts. Such interactions are a major reason why it is difficult to design effective policiesand the main challenge that RHEA is designed to solve. They are unavoidable in complex real-worlddomains such as public health (e.g., between closing schools, requiring masks, or limiting internationaltravel), traffic management (e.g., adding buses, free bus tokens, or bike lanes), and climate policy(e.g., competing legal definitions of net-zero or green hydrogen, and environmental feedbackloops) . In such domains there exist diverse expertse.g., policymakers, economists,scientists, local community leaders, and other stakeholderswhose input is worth soliciting beforeimplementing interventions. In RHEA, this policy-making challenge can be formalized as follows: Define. Suppose we are considering policy interventions a1, . . . , an. A policy action A consistsof some subset of these. Suppose we must be prepared to address contexts c {c1, . . . , cm},and we have a black-box predictor (c, A) to evaluate utility (a). In practice, will be acomplex dynamical model such as an agent-based or neural-network-based predictor. In this example,to highlight the core behavior of RHEA, is a simple-to-define function containing the kinds ofchallenging nonlinearities we would like to address, such as context dependence, synergies, anti-synergies, threshold effects, and redundancy (the full utility function is detailed in Eq. 1). Similarly, is a simple cost function, defined as the total number of prescribed policy interventions. A prescriptoris a function (c) = A (b). The goal is to find a Pareto front of prescriptors across the outcomesof utility and cost . Note that the search space is vast: There are 2mn possible prescriptors. Gather. Suppose prescriptors of unknown functional form have been gathered (c) from threeexperts: one generalist, providing general knowledge that applies across contexts (see cfor an example); and two specialists, providing knowledge that is of higher quality (i.e. lowercost-per-utility) but applies only to a few specific contexts (a-b). Distill. Datasets for distillation can be generated by running each expert prescriptor over all contexts.The complete behavior of a prescriptor can then be visualized as a binary grid, where a black cellindicates the inclusion of an intervention in the prescription for a given context (a-c). This datacan be used to convert the expert prescriptors into rule sets or neural networks (d, App. B.2).",
  "Evolve. These distilled models can then be injected into an initial population and evolved usingmulti-objective optimization (e). The full optimal Pareto front is obtained as a result": "With this formalization, it is possible to construct a synthetic example of RHEA in action, as shownin . It illustrates the optimal Pareto front. Importantly, this front is discoverable by RHEA, butnot by previous machine learning techniques such as Mixture-of-Experts (MoE) or WeightedEnsembles , or by the experts alone. RHEA is able to recombine the internal structure of expertsacross contexts (e.g., by adding a3, a4, a5 to a1, a2 in c1). It can innovate beyond the experts byadding newly-applicable interventions (a6). It can also refine the results by removing interventionsthat are now redundant or detrimental (a5 in c2), and by mixing in generalist knowledge. In contrast,the discoveries of MoE are restricted to mixing expert behavior independently at each context, andWeighted Ensemble solutions can only choose a single combination of experts to apply everywhere. The domain also illustrates why it is important to utilize expert knowledge in the first place. Thehigh-dimensional solution space makes it very difficult for evolution alone (i.e. not starting fromdistilled expert prescriptors) to find high-quality solutions, akin to finding needles in a haystack.Experimental results confirm that RHEA discovers the entire optimal Pareto front reliably, even",
  "Total Utility (Methods A.1)Total Utility (App. Eq. 1)": ": An Illustration of RHEA in a Synthetic Domain. The plots show the Pareto front of prescrip-tors discovered by RHEA vs. those of alternative prescriptor combination methods, highlighting thekinds of opportunities RHEA is able to exploit. The specialist expert prescriptors a and b and thegeneralist expert prescriptor c are useful but suboptimal on their own (purple s). RHEA recombinesand innovates upon their internal structure and is able to discover the full optimal Pareto front (blues). This front dominates that of Mixture-of-Experts (MoE; green s), which can only mix expertbehavior independently in each context. It also dominates that of Weighted Ensembling (yellow+s), which can only choose a single combination of experts to apply everywhere. Evolution alone(without expert knowledge) also struggles in this domain due to the vast search space (App. ),as do MORL methods (App. ,8). Thus, RHEA unlocks the latent potential in expert solutions. as the number of available interventions increases, while evolution alone does not (App. ).Multi-objective reinforcement learning (MORL) methods also struggle in this domain (App. ,8).Thus, RHEA harnesses the latent potential of expert solutions. It uses pieces of them as buildingblocks and combines them with novel elements to take full advantage of them. This ability can beinstrumental in designing effective policies for complex real-world tasks. Next, RHEA is put to workon one particularly vexing task: optimizing pandemic intervention policies.",
  "The XPRIZE Pandemic Response Challenge": "The XPRIZE Pandemic Response Challenge presented an ideal opportunity for demonstratingthe RHEA framework. XPRIZE is an organization that conducts global competitions, fueled bylarge cash prizes, to motivate the development of underfunded technologies. Current competitionstarget wildfires, desalination, carbon removal, meat alternatives, and healthy aging . In 2020 and2021, the XPRIZE Pandemic Response Challenge was designed and conducted , challengingparticipants to develop models to suggest optimal policy solutions spanning the tradeoff betweenminimizing new COVID-19 cases and minimizing the cost of implemented policy interventions. Define. The formal problem definition was derived from the Oxford COVID-19 government responsetracker dataset , which was updated daily from March 2020 through December 2022. Thisdataset reports government intervention policies (IPs) on a daily basis, following a standardizedclassification of policies and corresponding ordinal stringency levels in Z5 (used to define IP cost)to enable comparison across geographical regions (geos), which include nations and subnationalregions such as states and provinces. The XPRIZE Challenge focused on 235 geos (App. Fig 9) andthose 12 IPs over which governments have immediate daily control : school closings, workplaceclosings, cancellation of public events, restrictions on gathering size, closing of public transport, stay at home requirements, restrictions on internal movement, restrictions on international travel,public information campaigns, testing policy, contact tracing, and facial covering policy. Submissionsfor Phase 1 were required to include a runnable program (predictor) that outputs predicted casesgiven a geo, time frame, and IPs through that time frame (a). Submissions for Phase 2 wererequired to include a set of runnable programs (prescriptors), which, given a geo, time frame,and relative IP costs, output a suggested schedule of IPs (prescription) for that geo and timeframe (b). By providing a set of prescriptors, submissions could cover the tradeoff spacebetween minimizing the cost of implementing IPs and the expected number of new cases. Sincedecision makers for a particular geo could not simultaneously implement multiple prescriptionsfrom multiple teams, prescriptions were evaluated not in the real world but with a predictor (fromPhase 1), which forecasts how case numbers change as a result of a prescription. The formal problemdefinition, requirements, API, and code utilities are publicly available . Teams were encouragedto incorporate specialized knowledge in geos with which they were most familiar. The current studyfocuses on the prescriptors created in Phase 2. There are 10620 possible schedules for a single geofor 90 days, so brute-force search is not an option. To perform well, prescriptors must implementprincipled ideas to capture domain-specific knowledge about the structure of the pandemic. Gather. Altogether, 102 teams of experts from 23 countries participated in the challenge. Some teamswere actively working with local governments to inform policy ; other organizations servedas challenge partners, including the United Nations ITU and the City of Los Angeles . The set ofparticipants was diverse, including epidemiologists, public health experts, policy experts, machinelearning experts, and data scientists. Consequently, submissions took advantage of diverse methodolo-gies, including epidemiological models, decision rules, classical statistical methods, gradient-basedoptimization, various machine learning methods, and evolutionary algorithms, and exploited variousauxiliary data sources to get enhanced views into the dynamics of particular geos (c).The Phase 2 evaluations showed substantial specialization to different geos for different teams, astrong indication that there was diversity that could be harnessed. Many submissions also showedremarkable improvement over strong heuristic baselines, indicating that high-quality expertise hadbeen gathered successfully. Detailed results of the competition are publicly available ; this studyfocuses on the ideas in them in the aggregate. Distill. A total of 169 prescriptors were submitted to the XPRIZE Challenge. After the competition,for each of these gathered prescriptors i, an autoregressive neural network (NN) i with learnableparameters i was trained with gradient descent to mimic its behavior, i.e. to distill it (d;App. C.1). Each NN was trained on a dataset of 212,400 input-output pairs, constructed by queryingthe corresponding prescriptor nq times, i.e., through behavioral cloning:",
  "i(qj) i(q, i(qj), ); i1 ,(2)": "where q Q is a query and is a function that maps queries (specified via the API in Define) toinput data, i.e., contexts, with a canonical form. Each (date range, geo) pair defines a query q, withi(q) Z90125the policy generated by i for this geo and date range, and (q, i(q)) R90 thepredicted (normalized) daily new cases. Distilled models were implemented in Keras and trainedwith Adam using L1 loss (since policy actions were on an ordinal scale) (see App. C.1). Evolve. These 169 distilled models were then placed in the initial population of an evolutionary AIprocess (e). This process was based on the same Evolutionary Surrogate-assisted Prescription(ESP) method previously used to evolve COVID-19 IP prescriptors from scratch . In standardESP, the initial population (i.e., before any evolution takes place) consists only of NNs with randomlygenerated weights. By replacing random neural networks with the distilled neural networks, ESPstarts from diverse high-quality expert-based solutions, instead of low-quality random ones. ESPcan then be run as usual from this starting point, recombining and refining solutions over a series ofgenerations to find better tradeoffs between stringency and cases, using Pareto-based multi-objectiveoptimization (App. C.2). Providing a Pareto front of policy strategy options is critical, becausemost decision-makers will not simply choose the most extreme strategies (i.e. IPs with maximumstringency, or no IPs at all), but are likely to choose a tradeoff point appropriate for their particularpolitical, social and economic scenario (d shows the real-world distribution of IP stringencies).",
  "REMRUN": ": Quantitative comparison of solutions. a, Objective values for all solutions in the finalpopulation of a single representative run of each method. b, Pareto curves for these runs. Distilledprovides improved tradeoffs over Random and Evolved (from random), and RHEA pushes the frontout beyond Distilled. c, Overall Pareto front of the union of the solutions from these runs. The vastmajority of these solutions are from RHEA. d, The distribution of actual stringencies implementedin the real world across all geos at the prescription start date, indicating which Pareto solutionsreal-world decision makers would likely select, i.e., which tradeoffs they prefer. e, Given thisdistribution, the proportion of the time the solution selected by a user would be from a particularmethod (the REM metric); almost all of them would be from RHEA. f, The same metric, but basedon a uniform distribution of tradeoff preference (RUN) g, Domination rate (DR) w.r.t. Distilled,i.e. how much of the Distilled Pareto front is strictly dominated by another methods front. WhileEvolved (from scratch) sometimes discovers better solutions than those distilled from expert designs,RHEA improves 75% of them. h, Max reduction of cases (MCR) compared to Distilled acrossall stringency levels. i, Dominated hypervolume improvement (HVI) compared to Distilled. Foreach metric, RHEA substantially outperforms the alternatives, demonstrating that it creates improvedsolutions over human and AI design, and that those solutions would likely be preferred by humandecision-makers. (Bars show mean and st.dev. See App. C.3 for technical details of each metric.) Evolution from the distilled models was run for 100 generations in 10 independent trials to producethe final RHEA models. As a baseline, evolution was run similarly from scratch. As a second baseline,RHEA was compared to the full set of distilled models. A third baseline was models with randomlyinitialized weights, which is often a meaningful starting point in NN-based policy search . Allprescriptor evaluations, including those during evolution, were performed using the same referencepredictor as in the XPRIZE Challenge itself; this predictor was evaluated in depth in prior work . Results. The performance results are shown in . As is clear from the Pareto plots (a-c)and across a range of metrics (e-i), the distilled models outperform the random initial models,thus confirming the value of human insight and the efficacy of the distillation process. Evolution thenimproves performance substantially from both initializations, with distilled models leading to thebest solutions. Thus, the conclusions of the illustrative example are substantiated in this real-worlddomain: RHEA is able to leverage knowledge contained in human-developed models to discoversolutions beyond those from the AI alone or humans alone. The most critical performance metric isthe empirical R1-metric (REM; ), which estimates the percentage of time a decision-maker witha fixed stringency budget would choose a prescriptor from a given approach among those from allapproaches. For RHEA, REM is nearly 100%. In other words, not only does RHEA discover policiesthat perform better, but they are also policies that decision-makers would be likely to adopt.",
  "RHEA": ": Dynamics of IP schedules discovered by RHEA. a, UMAP projection of geo IP schedulesgenerated by the policies (App. C.4). The schedules from high-performing submitted expert mod-els are concentrated around a 1-dimensional manifold organized by overall cost (seen as a yellowarc). This manifold provides a scaffolding upon which RHEA elaborates, interpolates, and expands.Evolved policies, on the other hand, are scattered more discordantly (seen as blue clusters), un-grounded by the experts. b, To characterize how RHEA expands upon this scaffolding, five high-levelproperties of IP schedules were identified and their distributions were plotted across the schedules. Foreach, RHEA finds a balance between the grounding of expert submissions (i.e., regularization) andtheir recombination and elaboration (i.e., innovation), though this balance manifests in distinct ways.For swing and separability, RHEA is similar to real schedules, but finds that the high separabilityproposed by some expert models can sometimes be useful. RHEA finds the high focus of the expertmodels even more attractive; in practice, they could provide policy-makers with simpler and clearermessages about how to control the pandemic. For focus, agility, and periodicity, RHEA pushesbeyond areas explored by the submissions, finding solutions that humans may miss. The exampleschedules shown in a(i-v) illustrate these principles in practice (rows are IPs sorted from top tobottom as listed in Sec. 3; column are days in the 90-day period; darker color means more stringent).(i) Real-world examples demonstrate that although agility and periodicity require some effort toimplement, they have occasionally been utilized (e.g. in Portugal and France); (ii) a simple exampleof how RHEA generates useful interpolations of submitted non-Pareto schedules, demonstrating howit realizes latent potential even in some low-performing solutions, far from schedules evolved fromscratch; (iii) another useful interpolation, but achieved via higher agility than Pareto submissions;(iv) a high-stringency RHEA schedule that trades swing and separability for agility and periodicitycompared to its submitted neighbor; and (v) a medium-stringency RHEA schedule with lower swingand separability and higher focus than its submitted neighbor. Overall, these analyses show howRHEA realizes the latent potential of the raw material provided by the human-created submissions. UMAP to visualize the distribution of their behavior (a). Note that the schedules from thehighest-performing (Pareto) submitted policies form a continuous 1D manifold across this space,indicating continuity of tradeoffs. This manifold serves as scaffolding upon which RHEA recombines,refines, and innovates; these processes are the same as in the illustrative example, only more complex.Evolution alone, on the other hand, produces a discordant scattering of schedules, reflecting itsunconstrained exploratory nature, which is disadvantageous in this domain. What kind of structuredoes RHEA harness to move beyond the existing policies? Five high-level properties were identifiedthat characterize how RHEA draws on submitted models in this domain: swing measures thestringency difference between the strictest and least strict day of the schedule; separability measuresto what extent the schedule can be separated into two contiguous phases of different stringencylevels; focus is inversely proportional to the number of IPs used; agility measures how often IPschange; and periodicity measures how much of the agility can be explained by weekly periodicity",
  "cde": ": Dynamics of evolutionary discovery process. a, Sample ancestries of prescriptors on theRHEA Pareto front. Leaf nodes are initial distilled models; the final solutions are the root. Thehistory of recombinations leading to different solutions varies widely in terms of complexity, withapparent motifs and symmetries. The ancestries show that the search is behaving as expected, in thatthe cost of the child usually lands between the costs of its parents (indicated by color). This propertyis also visualized in b (and c), where child costs (and cases) are plotted over all recombinations fromall trials (k-NN regression, k = 100). d, From ancestries, one can compute the relative contributionof each expert model to the final RHEA Pareto front (App C.5). This contribution is remarkablyconsistent across the independent runs, indicating that the approach is reliable (mean and st.dev.shown). e, Although there is a correlation between the performance of teams of expert models andtheir contribution to the final front, there are some teams with unimpressive quantitative performancein their submissions who end up making outsized contributions through the evolutionary process.This result highlights the value of soliciting a broad diversity of expertise, even if some of it does nothave immediately obvious practical utility. AI can play a role in realizing this latent potential. (b; App. C.4). Some ideas from submitted policies, e.g., increased separability and focus, arereadily incorporated into RHEA policies. Others, e.g. increased focus, agility, and periodicity, RHEAis able to utilize beyond the range of policies explored by the human designs. The examples ina illustrate these properties in practice. Example (i) shows a number of real policies, suggestingthat geos are capable of implementing diverse and innovative schedules similar to those discoveredby RHEA; e.g., weekly periodicity was actually implemented for a time in Portugal and France.Examples (ii-v) show RHEA schedules and their nearest submitted neighbors, demonstrating howinnovations can manifest as interpolations or extrapolations of submitted policies. For instance, oneopportunity is to focus on a smaller set of IPs; another is to utilize greater agility and periodicity.This analysis shows how RHEA can lead to insights on where improvements are possible. Second, to understand how RHEA discovered these innovations, an evolutionary history can bereconstructed for each solution, tracing it back to its initial distilled ancestors (). Some finalsolutions stem from a single beneficial crossover of distilled parents, while others rely on morecomplex combinations of knowledge from many ancestors (a). While the solutions are morecomplex, the evolutionary process is similar to that of the illustrative example: It proceeds in aprincipled manner, with child models often falling between their parents along the case-stringencytradeoff (b-c). Based on these evolutionary histories, one can compute the relative contributionof each expert model to the final RHEA Pareto front (App C.5). These contributions are highlyconsistent across independent runs, indicating that the approach is reliable (d). Indeed in theXPRIZE competition, this contribution amount was used as one of the quantitative metrics of solutionquality . Remarkably, although there is a correlation between the performance of expert modelsand their contribution to the final front, there are also models that do not perform particularly well, butend up making outsized contributions through the evolutionary process (e; see also a(ii)).This result highlights the value of soliciting a broad diversity of expertise, even if some of it does nothave immediately obvious practical utility. AI can play a role in realizing this latent potential.",
  "Discussion": "Alternative Policy Discovery Methods.Our implementation of RHEA uses established methods inboth the Distill and Evolve steps; the technical novelty comes from their unique combination in RHEAto unlock diverse human expertise. Popular prior methods for combining diverse models includeensembling and Mixture-of-Experts , but, as highlighted in , although multi-objectivevariants have been explored in prior work , neither of these methods can innovate beyond thescaffolding provided by the initial experts. Evolution is naturally suited for this task: Crossover is apowerful way to recombine expert models, mutation allows innovating beyond them, and population-based search naturally supports multiobjective optimization. Other approaches for policy optimizationinclude contextual bandits , planning-based methods , and reinforcement learning ,and an interesting question is how they might play a role in such a system. One approach could be touse evolutionary search for recombination and use another method for local improvement, akin tohybrid approaches used in other settings (See App. A for a longer discussion). Theory.It is intuitive why expert knowledge improves RHEAs search capability. However, anytheoretical convergence analysis will depend on the particular implementation of RHEA. The presentimplementation uses NSGA-II, the convergence of which has recently been shown to depend criticallyon the size of jumps in the optimization landscape, i.e. roughly the maximum size of non-convexregions . On the ONEJUMPZEROJUMP benchmark, the tightest known upper-bound forconvergence to the full ground truth Pareto front is O(N 2nk/(k)k), where k is a measure of thejump size, n is the problem dimensionality, and N is the (sufficiently large) population size. In otherwords, a smaller jump size leads to a drastic convergence speed up. Distilling useful, diverse expertsis conceptually analogous to decreasing the jump size. This effect is apparent in the illustrativedomain, where the experts provide building blocks that can be immediately recombined to discoverbetter solutions, but that are difficult to discover from scratch (). This interpretation is borneout in the experiments: RHEA continues to converge quickly as the action space (i.e. problemdimensionality) increases, whereas evolution regresses to only being able to discover the most convex(easily-discoverable) portions of the Pareto front (App. ). Generalizability.RHEA can be applied effectively to policy-discovery domains where (1) theproblem can be formalized with contexts, actions, and outcomes, (2) there exist diverse experts fromwhich solutions can be gathered, and (3) the problem is sufficiently challenging. In contrast, RHEAwould not be effective, (1) if the problem is too easy, so that the input from human experts would notbe necessary, (2) if the problem is hard, but no useful and diverse experts exist, and (3) if there is noclear way to define context and/or action variables upon which the experts agree. The modularity of RHEA allows different implementations of components to be designed for differentdomains, such as those related to sustainability, engineering design, and public health. One particularlyexciting opportunity for RHEA is climate policy, which often includes complex interactions betweenmultiple factors . For example, given the context of the current state of the US energy grid andenergy markets, the green hydrogen production subsidies introduced by the Inflation Reduction Actwill in fact lead to increases in carbon emissions, unless the Treasury Department enacts three distinctregulations in the definition of green hydrogen . It is precisely this kind of policy combinationthat RHEA could help discover, and such a discovery process could be an essential part of a climatepolicy application. For example, the En-Roads climate simulator supports diverse actions acrossenergy policy, technology, and investment, contexts based on social, economic, and environmentaltrajectories, and multiple competing outcomes, including global temperature, cost of energy, andsea-level rise . Users craft policies based on their unique priorities and expertise. RHEA could beused with a predictor like En-Roads to discover optimized combinations of expert climate policiesthat trade-off across temperature change and other the outcomes that users care about most. Ethics and Broader Impact.As part of the UN AI for Good Initiative, we are currently buildinga platform for formalizing and soliciting expert solutions to SDG goals more broadly . Ethicalconsiderations when deploying such systems are outlined below. See App. D for further discussion. Fairness. In such problems with diverse stakeholders, breaking down costs and benefits by affectedpopulations and allowing users to input explicit constraints to prescriptors can be crucial for generatingfeasible and equitable models. In this platform, RHEA could take advantage of knowledge thatlocal experts provide and learn to generalize it; by treating each contributed model as a black box,",
  "it is agnostic to the type of models used, thus helping to make the platform future-proof. Fairnessconstraints can also be directly included in RHEAs multiple objectives": "Governance and Democratic Accountability. An important barrier in the adoption of AI by real-worlddecision-makers is trust . For example, such systems could be used to justify the decisionsof bad actors. RHEA provides an advantage here: If the initial human-developed models it uses areexplainable (e.g. are based on rules or decision trees), then a user can trust that suggestions generatedby RHEA models are based on sensible principles, and can trace and interrogate their origins. Evenwhen the original models are opaque, trust can be built by extracting interpretable rules that describeprescriptor behavior, which is feasible when the prescriptors are relatively compact and shallow, as in the experiments in this paper. That is, RHEA models can be effectively auditedacritical property for AI systems maintained by governments and other policy-building organizations. Data Privacy and Security. Since experts submit complete prescriptors, no sensitive data they mayhave used to build their prescriptors needs to be shared. In the Gather step in Sec. 3, each expert teamhad an independent node to submit their prescriptors. The data for the team was generated by runningtheir prescriptors on their node. The format of the data was then automatically verified, to ensure thatit complied with the Defined API. Verified data from all teams was then aggregated for the Distill& Evolve steps. Since the aggregated data must fit an API that does not allow for extra data to bedisclosed, the chance of disclosing sensitive data in the Gather phase is minimized. External Oversight. Although the above mechanisms could all yield meaningful steps in addressing abroad range of ethical concerns, they cannot completely solve all issues of ethical deployment. So, itis critical that the system is not deployed in an isolated way, but integrated into existing democraticdecision-making processes, with appropriate external oversight. Any plan for deployment shouldinclude a disclosure of these risks to weigh against the potential societal benefits. Sustainability and Accessibility. Due to the relatively compact model size, RHEA uses orders ofmagnitude less compute and energy than many other current AI systems, which is critical for creatinguptake by decision-makers around the world who do not have access to extensive computationalresources or for whom energy usage is becoming an increasingly central operational consideration. Limitations.Understanding the limitations of the presented RHEA implementation is critical forestablishing directions for future work. The cost measure used in this paper was uniform over IPs,an unbiased way to demonstrate the technology, but, for a prescriptor to be used in a particular geo,costs of different IPs should be calibrated based on geo-specific cost-analysis. The geo may also havesome temporal discounting in its cases and cost objectives. For consistency with the XPRIZE, theywere not included in the experiments in this paper but can be naturally incorporated into RHEA inthe future. When applying surrogate-developed policies to the real world, approximation errors cancompound over time. Thus, user-facing applications of RHEA could benefit from the inclusion ofuncertainty measures , inverse reinforcement learning , as well as humans-in-the-loopto prevent glaring errors. Distillation could also be limited in cases where expert models use externaldata sources with resulting effects not readily approximated by the inputs specified in the defined API.If this were an issue in future applications, it could be addressed by training models that generalizeacross domain spaces . RHEA prescriptors were evaluated in the same surrogate settingas prescriptors in the XPRIZE, but not yet in hands-on user studies. Hands-on user evaluation is acritical step but requires a completely different kind of research effort, i.e. one that is political andcivil, rather than computational. Our hope is that the publication of the results of RHEA makes thereal-world incorporation of these kinds of AI decision-assistants more likely. Conclusion.This paper motivated, designed, and evaluated a framework called RHEA for bringingtogether diverse human expertise systematically to solve complex problems. The promise of RHEAwas illustrated with an initial implementation and an example application; it can be extended to otherdomains in future work. The hope is that, as a general and accessible system that incorporates inputfrom diverse human sources, RHEA will help bridge the gap between human-only decision-makingand AI-from-data-only approaches. As a result, decision-makers can start adopting powerful AIdecision-support systems, taking advantage of the latent real-world possibilities such technologiesilluminate. More broadly, the untapped value of human expertise spread across the world is immense.Human experts should be actively encouraged to continually generate diverse creative ideas andcontribute them to collective pools of knowledge. This study shows that AI has a role to play inrealizing the full value of this knowledge, thus serving as a catalyst for global problem-solving.",
  "Acknowledgements": "We would like to thank XPRIZE for their work in instigating, developing, publicizing, and adminis-tering the Pandemic Response Challenge, as well as the rest of the Cognizant AI Labs research groupfor their feedback on experiments and analysis. We would also like to thank Conor Hayes for adviceon running the MORL comparisons, and Benjamin Doerr for advice on NSGA-II theory. L. N. Alegre, A. L. Bazzan, D. M. Roijers, A. Now, and B. C. da Silva. Sample-efficientmulti-objective learning via generalized policy improvement prioritization. In Proceedingsof the 2023 International Conference on Autonomous Agents and Multiagent Systems, pages20032012, 2023.",
  "Buchanan and Smith. Fundamentals of expert systems. Annu. Rev. Comput. Sci., 1988": "F. Chicano, D. Whitley, G. Ochoa, and R. Tins. Optimizing one million variable NK landscapesby hybridizing deterministic recombination and local search. In Proceedings of the Genetic andEvolutionary Computation Conference, GECCO 17, pages 753760, July 2017. F. Chicano, D. Whitley, G. Ochoa, and R. Tins. Optimizing one million variable nk landscapesby hybridizing deterministic recombination and local search. In Proceedings of the genetic andevolutionary computation conference, pages 753760, 2017.",
  "M. M. Drugan and A. Nowe. Designing multi-objective multi-armed bandits algorithms: Astudy. In The 2013 international joint conference on neural networks (IJCNN), pages 18. IEEE,2013": "F. Felten, L. N. Alegre, A. Nowe, A. Bazzan, E. G. Talbi, G. Danoy, and B. C da Silva. A toolkitfor reliable benchmarking and research in multi-objective reinforcement learning. Advances inNeural Information Processing Systems, 36, 2024. O. Francon, S. Gonzalez, B. Hodjat, E. Meyerson, R. Miikkulainen, X. Qiu, and H. Shahrzad.Effective reinforcement learning through evolutionary surrogate-assisted prescription. In Proc.of the Genetic and Evolutionary Computation Conference, June 2020.",
  "R. B. Freeman and W. Huang. Collaboration: Strength in diversity. Nature, 513(7518):305,Sept. 2014": "J. Gawlikowski, C. R. N. Tassi, M. Ali, J. Lee, M. Humt, J. Feng, A. Kruspe, R. Triebel,P. Jung, R. Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprintarXiv:2107.03342, 2021. T. Hale, N. Angrist, R. Goldszmidt, B. Kira, A. Petherick, T. Phillips, S. Webster, E. Cameron-Blake, L. Hallas, S. Majumdar, and H. Tatlow. A global panel database of pandemic policies(oxford COVID-19 government response tracker). Nature Human Behaviour, 5(4):529538,Mar. 2021. M. P. Hansen and A. Jaszkiewicz. Evaluating the quality of approximations to the non-dominatedset. Technical Report IMM-REP-1998-7, Institute of Mathematical Modelling, TechnicalUniversity of Denmark, 1994. C. F. Hayes, R. Radulescu, E. Bargiacchi, J. Kllstrm, M. Macfarlane, M. Reymond, T. Ver-straeten, L. M. Zintgraf, R. Dazeley, F. Heintz, et al. A practical guide to multi-objectivereinforcement learning and planning. Autonomous Agents and Multi-Agent Systems, 36(1):26,2022.",
  "C. Le Goues, S. Forrest, and W. Weimer. The case for software evolution. In Proceedings of theFSE/SDP workshop on Future of software engineering research, pages 205210, Nov. 2010": "J. Lehman, J. Clune, D. Misevic, C. Adami, L. Altenberg, J. Beaulieu, P. J. Bentley, S. Bernard,G. Beslon, D. M. Bryson, N. Cheney, P. Chrabaszcz, A. Cully, S. Doncieux, F. C. Dyer, K. O.Ellefsen, R. Feldt, S. Fischer, S. Forrest, A. Frenoy, C. Gagne, L. Le Goff, L. M. Grabowski,B. Hodjat, F. Hutter, L. Keller, C. Knibbe, P. Krcah, R. E. Lenski, H. Lipson, R. MacCurdy, C. Maestre, R. Miikkulainen, S. Mitri, D. E. Moriarty, J.-B. Mouret, A. Nguyen, C. Ofria,M. Parizeau, D. Parsons, R. T. Pennock, W. F. Punch, T. S. Ray, M. Schoenauer, E. Schulte,K. Sims, K. O. Stanley, F. Taddei, D. Tarapore, S. Thibault, R. Watson, W. Weimer, andJ. Yosinski. The surprising creativity of digital evolution: A collection of anecdotes from theevolutionary computation and artificial life research communities. Artif. Life, 26(2):274306,Apr. 2020.",
  "J. K. Pugh, L. B. Soros, and K. O. Stanley. Quality diversity: A new frontier for evolutionarycomputation. Frontiers in Robotics and AI, 3:40, 2016": "K. Pulkkinen, S. Undorf, F. Bender, P. Wikman-Svahn, F. Doblas-Reyes, C. Flynn, G. C. Hegerl,A. Jnsson, G.-K. Leung, J. Roussos, T. G. Shepherd, and E. Thompson. The value of values inclimate science. Nat. Clim. Chang., 12(1):46, Jan. 2022. X. Qiu, E. Meyerson, and R. Miikkulainen. Quantifying point-prediction uncertainty in neuralnetworks via residual estimation with an i/o kernel. In International Conference on LearningRepresentations, 2019.",
  "N. Riquelme, C. Von Lcken, and B. Baran. Performance metrics in multi-objective optimization.In 2015 Latin American Computing Conference (CLEI), pages 111, Oct. 2015": "Rock and Grant. Why diverse teams are smarter. Harv. Bus. Rev., 2016. M. Romanello, A. McGushin, C. Di Napoli, P. Drummond, N. Hughes, L. Jamart, H. Ken-nard, P. Lampard, B. Solano Rodriguez, N. Arnell, S. Ayeb-Karlsson, K. Belesova, W. Cai,D. Campbell-Lendrum, S. Capstick, J. Chambers, L. Chu, L. Ciampi, C. Dalin, N. Dasandi,S. Dasgupta, M. Davies, P. Dominguez-Salas, R. Dubrow, K. L. Ebi, M. Eckelman, P. Ekins,L. E. Escobar, L. Georgeson, D. Grace, H. Graham, S. H. Gunther, S. Hartinger, K. He,C. Heaviside, J. Hess, S.-C. Hsu, S. Jankin, M. P. Jimenez, I. Kelman, G. Kiesewetter, P. L.Kinney, T. Kjellstrom, D. Kniveton, J. K. W. Lee, B. Lemke, Y. Liu, Z. Liu, M. Lott, R. Lowe,J. Martinez-Urtaza, M. Maslin, L. McAllister, C. McMichael, Z. Mi, J. Milner, K. Minor,N. Mohajeri, M. Moradi-Lakeh, K. Morrissey, S. Munzert, K. A. Murray, T. Neville, M. Nilsson,N. Obradovich, M. O. Sewe, T. Oreszczyn, M. Otto, F. Owfi, O. Pearman, D. Pencheon, M. Rab-baniha, E. Robinson, J. Rocklv, R. N. Salas, J. C. Semenza, J. Sherman, L. Shi, M. Springmann,M. Tabatabaei, J. Taylor, J. Trinanes, J. Shumake-Guillemot, B. Vu, F. Wagner, P. Wilkinson,M. Winning, M. Yglesias, S. Zhang, P. Gong, H. Montgomery, A. Costello, and I. Hamilton.The 2021 report of the lancet countdown on health and climate change: code red for a healthyfuture. Lancet, 398(10311):16191662, Oct. 2021.",
  "Siau and Wang. Building trust in artificial intelligence, machine learning, and robotics. Cutterbusiness technology journal, 2018": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-brenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484489, Jan. 2016.",
  "K. O. Stanley, J. Clune, J. Lehman, and R. Miikkulainen. Designing neural networks throughneuroevolution. Nature Machine Intelligence, 1(1):2435, Jan. 2019": "F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune. Deep neuroevo-lution: Genetic algorithms are a competitive alternative for training deep neural networks forreinforcement learning. arXiv preprint arXiv:1712.06567, Dec. 2017. R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. G. Swamy, D. Wu, S. Choudhury, D. Bagnell, and S. Wu. Inverse reinforcement learningwithout reinforcement learning. In International Conference on Machine Learning, pages3329933318. PMLR, 2023.",
  "University of Oxford.Codebook for the Oxford COVID-19 government responsetracker. github.com/OxCGRT/covid-policy-tracker/blob/master/documentation/codebook.md, 2020": "P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski,P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman,N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero,C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0Contributors. SciPy 1.0: fundamental algorithms for scientific computing in python. Nat.Methods, 17(3):261272, Mar. 2020.",
  "A.1Harnessing diversity in AI": "Machine learning (ML) models generally benefit from diversity in the data on which they are trained. At a higher level, it has long been known that diverse models for a single task may be usefullycombined to improve performance on the task. Methods for such combination usually fall under thelabel of ensembling . By far, the most popular ensembling method is to use a linear combinationof models. Mixture-of-Experts (MoE) approaches use a more sophisticated approach of conditionallyselecting which models to use based on the input . However, as highlighted in , althoughsome multi-objective variants have been explored in prior work , neither of these methods areinherently sufficient for the kind of policy discovery required by RHEA. In particular, such methodsare not multi-objective, and provide no method of innovating beyond the scaffolding provided by theindividual experts. An orthogonal approach to harnessing diversity within a single task is to exploit regularities acrossmultiple tasks, by learning more than one task in a single model . In the extreme case, a singlemodel may be trained across many superficially unrelated tasks with the goal of learning sharedstructure underlying the problem-solving universe . In this paper, it was possible to specializethe expert models to different regions and pandemic states, but the input-output spaces of the modelswere uniform to enable a consistent API. Future work could generalize RHEA to cases where theexpert models are trained on different, but related, problems that could potentially benefit from oneanother. Finally, there is a rich history of managing and exploiting diverse solutions in evolutionary algorithms:from early work on preserving diversity to prevent premature convergence and well-establishedwork on multi-objective optimization , to more recent research on novelty search and diversity fordiversitys sake , and to the burgeoning field of Quality Diversity, where the goal is to discoverhigh-performing solutions across an array of behavioral dimensions . RHEA is different fromthese existing methods because it is not about discovering diverse solutions de novo, but rather aboutharnessing the potential of diverse human-created solutions. Nonetheless, the scope and successof such prior research illustrate why evolutionary optimization is well-suited for recombining andinnovating upon diverse solutions.",
  "A.2Alternative approaches to policy discovery": "In this paper, evolutionary optimization was used as a discovery method because it is most naturallysuited for this task: crossover is a powerful way to recombine expert models, mutation allowsinnovating beyond them, and population-based search naturally supports multiobjective optimization.Other approaches for policy optimization include contextual bandits , planning-based methods, and reinforcement learning , and an interesting question is whether they could be used inthis role as well. Although less common than in evolutionary optimization, multi-objective approaches have beendeveloped for such methods . However, because they aim at improving a single solution ratherthan a population of solutions, they tend to result in less exploration and novelty than evolutionaryapproaches . One approach could be to use evolutionary search for recombination and use one ofthese non-evolutionary methods for local improvement. Such hybrid approaches have been used inother settings , and would be an interesting avenue of future work with RHEA.",
  "(c, A) =": "1,if c = c1 A = {a1, a2}2,if c = c1 A = {a1, a2, a3, a4, a5}3,if c = c1 A = {a1, a2, a3, a4, a5, a6}4,if c = c2 A = {a1, a2, a3, a4, a5, a6}5,if c = c2 A = {a1, a2, a3, a4, a6}1,if c = c2 A = {a3, a4, a5}1,if A = {a7, a8, a9, a10}0,otherwise.",
  "(1)": "In this definition, the non-zero-utility cases represent context-dependent synergies between policyinterventions; they also represent threshold effects where utility is only unlocked once enough ofthe useful interventions are implemented. The interventions that are not present in these cases yieldanti-synergies, i.e. they negate any positive policy effects. The contexts c1 and c2 represent similarbut distinct contexts in which similar but distinct combinations of interventions are useful and caninform one another. In c2, a5 becomes redundant once a6 is included.",
  "B.2Analytic Distillation": "Since the context and action spaces are discrete in this domain, prescriptors can be analyticallydistilled based on the dataset describing their full behavior (i.e., the binary grids in ). Forexample, these prescriptors can be distilled into rule-based or neural network-based prescriptors. Consider rule-based prescriptors of the form: = [C1 A1, . . . , Cr Ar], where Ci {c1, . . . , cm} and Ai {a1, . . . , an} are subsets of the possible contexts and policy interven-tions, respectively. These prescriptors have a variable number of rules r 0. Given a contextc, (c) prescribes the first action Ai such that c Ci, and prescribes the empty action Ao = if no Ci contains c. Then, the gathered expert prescriptors with behavior depicted in a-c can be compactly distilled as 1 = [{c1} {a1, a2}], 2 = [{c2} {a3, a4, a5}], and3 = [{c1, c2, c3, c4, c5, c6, c7} {a7, a8, a9, a10}], respectively. Similarly, consider neural-network-based prescriptors with input nodes c1, . . . , cm, output nodesa1, . . . , an, and hidden nodes with ReLU activation and no bias. For every unique action Aiprescribed by a prescriptor , let Ci be the set of contexts c for which (c) = Ai. Add a hiddennode hi connected to each input c Ci and each output a Ai. Let all edges have weight one.When using this model, include a policy intervention ai in the prescribed action if its activation ispositive. Then, distilled versions of the expert prescriptors can be compactly described by their setsof directed edges: 1 = {(c1, h1), (h1, a1), (h1, a2)}, 2 = {(c2, h1), (h1, a3), (h1, a4), (h1, a5)},and 3 = {(c1, h1), (c2, h1), . . . , (c7, h1), (h1, a7), (h1, a8), (h1, a9), (h1, a10)}.",
  "B.3Evolution of Analytically Distilled Models": "For experimental verification, the distilled rule-set models were used to initialize a minimal multi-objective evolutionary AI process. This process was built from standard components including amethod for recombination and variation of rule sets, non-dominated sorting , duplicate removal,and truncation selection. In the RHEA setup, the distilled versions of the gathered expert prescriptorswere used to initialize the population and were reintroduced every generation. In the evolution alonesetup, all instances of distilled models were replaced with random ones. The Python code for runningthese experiments can be found at",
  "Time for RHEA to discover full Pareto front % of full Pareto front found by Evolution in 500 generations": ": Experimental results comparing RHEA vs. Evolution alone (i.e., without knowledge ofgathered expert solutions) in the illustrative domain. Whiskers show 1.5IQR; the middle bar isthe median. a, RHEA exploits latent expert knowledge to reliably and efficiently discover the fulloptimal Pareto front, even as the number of available policy interventions n increases (there are 2npossible actions for each context; 100 trials each). b, Evolution alone does not reliably discover thefront even with 10 available interventions, and its performance drops sharply as the number increases(100 trials each). Thus, diverse expert knowledge is key to discovering optimal policies.",
  "B.4Comparison to multi-objective reinforcement learning": "Multi-objective reinforcement learning (MORL) is a growing area of research that aims to deploythe recent successes of reinforcement learning (RL) to multi-objective domains . A naturalquestion is: Is RHEA needed, or can MORL methods be directly applied from scratch (without expertknowledge) and reach similar or better performance? To answer this question, comparisons were performed with a suite of state-of-the-art MORL tech-niques in the Illustrative domain. Preliminary tests were run with several of the recent algorithms,namely, GPI-LS , GPI-PD , and Envelope Q-Learning . The hyperparameters were thosefound to work well in the most similar discrete domains in the benchmark suite1. Due to computa-tional constraints, the comparisons then focused on GPI-LS for scaling up to larger action spacesbecause (1) it has the best recorded results in this kind of domain , and (2) none of the otherMORL methods in the suite were able to outperform GPI-LS in the experiments. Note that the moresophisticated GPI-PD yields essentially the same results as GPI-LS in this discrete context and actiondomain. In short, even the baseline multi-objective evolution method strongly outperforms MORL (,8).The reason is that evolution inherently recombines blocks of knowledge, whereas MORL techniquesstruggle when there is no clear gradient of improvement.",
  "abc": ": Convergence curve comparisons. a-c, Convergence curves for 10, 30, and 50 actions,respectively, in the Illustrative domain. RHEA converges to the full Pareto front in all cases, whereasthe other methods converge to lower values as the action space grows. Evolution substantiallyoutperformed the MORL baselines in all cases. With 10 actions, all MORL baselines convergedrelatively quickly to the same performance. Due to computational limitations, only the most relevantcomparison, GPI-LS (which is state-of-the-art in discrete domains), was run in the experiments withmore actions (lines are means; shading is standard deviation). : MORL scaling comparison. GPI-LS discovered less of the true Pareto front than theEvolution baseline (100 trials each). The performance of both methods decreases as the problemcomplexity, i.e., the number of actions, increases. This plot complements b. Recall from athat RHEA discovers the entire Pareto front in all trials. where q Q is a query, i is the initial solution, i is the distilled model with learnable parametersi, and is a function that maps queries (which may be specified via a high-level API) to inputdata, i.e., contexts, with a canonical form that can be used to train i. In practice, i is trained byoptimizing i with stochastic gradient descent using data derived from the nq queries for which datais available. In the Pandemic Response Challenge experiment, prescriptors were distilled into an evolvable neuralnetwork architecture based on one previously used to evolve prescriptors from scratch in this domain, with the following changes: (1) In addition to the IPs used in that previous work, new IPswere used that were released in the Oxford data set since that work , and that were used inthe XPRIZE Pandemic Response Challenge; (2) Instead of a case growth rate, the case data inputto the models were presented as cases per 100K residents. This input was found to allow distilledmodels to fit the training data more closely than the modified growth rate used in previous work. Thereason for this improvement is that cases per 100K gives a more complete picture of the state of thepandemic; the epidemiological-model-inspired ratio used in prior work captures the rate of changein cases explicitly but makes it difficult to deduce how bad an outbreak is at any particular moment.Since many diverse submitted prescriptors took absolute case numbers into account, including thesevalues in the distillation process allows the distilled prescriptors to align with their source modelmore closely.",
  "Data for training a distilled model i was gathered by collecting the prescriptions made by i in theXPRIZE Pandemic Response Challenge. Data was gathered for all prescriptions made with uniform": "Afghanistan, Albania, Algeria, Andorra, Angola, Argentina, Aruba, Australia, Austria, Azerbaijan, Bahamas, Bahrain, Bangladesh, Barbados,Belarus, Belgium, Belize, Benin, Bermuda, Bhutan, Bolivia, Bosnia and Herzegovina, Botswana, Brazil, Brunei, Bulgaria, Burkina Faso,Burundi, Cambodia, Cameroon, Canada, Cape Verde, Central African Republic, Chad, Chile, China, Colombia, Comoros, Congo, CostaRica, Cote dIvoire, Croatia, Cuba, Cyprus, Czech Republic, Democratic Republic of Congo, Denmark, Djibouti, Dominica, DominicanRepublic, Ecuador, Egypt, El Salvador, Eritrea, Estonia, Eswatini, Ethiopia, Faeroe Islands, Fiji, Finland, France, Gabon, Gambia, Georgia,Germany, Ghana, Greece, Greenland, Guam, Guatemala, Guinea, Guyana, Haiti, Honduras, Hong Kong, Hungary, Iceland, India, Indonesia,Iran, Iraq, Ireland, Israel, Italy, Jamaica, Japan, Jordan, Kazakhstan, Kenya, Kosovo, Kuwait, Kyrgyz Republic, Laos, Latvia, Lebanon,Lesotho, Liberia, Libya, Lithuania, Luxembourg, Macao, Madagascar, Malawi, Malaysia, Mali, Mauritania, Mauritius, Mexico, Moldova,Monaco, Mongolia, Morocco, Mozambique, Myanmar, Namibia, Nepal, Netherlands, New Zealand, Nicaragua, Niger, Nigeria, Norway,Oman, Pakistan, Palestine, Panama, Papua New Guinea, Paraguay, Peru, Philippines, Poland, Portugal, Puerto Rico, Qatar, Romania, Russia,Rwanda, San Marino, Saudi Arabia, Senegal, Serbia, Seychelles, Sierra Leone, Singapore, Slovak Republic, Slovenia, Solomon Islands,Somalia, South Africa, South Korea, South Sudan, Spain, Sri Lanka, Sudan, Suriname, Sweden, Switzerland, Syria, Taiwan, Tajikistan,Tanzania, Thailand, Timor-Leste, Togo, Trinidad and Tobago, Tunisia, Turkey, Uganda, Ukraine, United Arab Emirates, United Kingdom/ England, United Kingdom / Northern Ireland, United Kingdom / Scotland, United Kingdom / Wales, United Kingdom, United States /Alabama, United States / Alaska, United States / Arizona, United States / Arkansas, United States / California, United States / Colorado,United States / Connecticut, United States / Delaware, United States / Florida, United States / Georgia, United States / Hawaii, United States/ Idaho, United States / Illinois, United States / Indiana, United States / Iowa, United States / Kansas, United States / Kentucky, UnitedStates / Louisiana, United States / Maine, United States / Maryland, United States / Massachusetts, United States / Michigan, United States /Minnesota, United States / Mississippi, United States / Missouri, United States / Montana, United States / Nebraska, United States / Nevada,United States / New Hampshire, United States / New Jersey, United States / New Mexico, United States / New York, United States / NorthCarolina, United States / North Dakota, United States / Ohio, United States / Oklahoma, United States / Oregon, United States / Pennsylvania,United States / Rhode Island, United States / South Carolina, United States / South Dakota, United States / Tennessee, United States /Texas, United States / Utah, United States / Vermont, United States / Virginia, United States / Washington, United States / Washington DC,United States / West Virginia, United States / Wisconsin, United States / Wyoming, United States, Uruguay, Uzbekistan, Vanuatu, Venezuela,Vietnam, Yemen, Zambia, Zimbabwe",
  ": List of the 235 geos (i.e., countries and subregions) whose data (from the Oxford dataset) was used in XPRIZE competition and in experiments in this paper": "IP weights. This data consisted of ten date ranges, each of length 90 days, and 235 geos (Fig 9),resulting in 212,400 training samples for each prescriptor, a random 20% of which was used forvalidation for early stopping. More formally, each (date range, geo) pair defines a query q, withi(q) Z90125the policy generated by i for this geo and date range. The predicted daily newcases for this geo and date range given this policy is (q, i(q)) R90. Let h be the vector of dailyhistorical new cases for this geo up until the start of the date range. This query leads to 90 trainingsamples for i: For each day t, the target is the prescribed actions of the original prescriptor i(q)t,and the input is the prior 21 days of cases (normalized by 100K residents) taken from h for prior daysbefore the start of the date range and from (q, i(q)) for days in the date range. Distilled models were implemented and trained in Keras using the Adam optimizer . Meanabsolute error (MAE) was used as the training loss (since policy actions were on an ordinal scale),with targets normalized to the range . The efficacy of distillation was confirmed by computingthe rank correlations between the submitted expert models in the XPRIZE challenge and their distilledcounterparts with respect to the two objectives: For both cases and cost, the Spearman correlation was 0.7, with p < 1020, demonstrating that distillation was successful. In such a real-world scenario,a correlation much closer to 1.0 is unlikely, since many solutions are close together in objective space,and may have different positions on the Pareto front depending on the evaluation context.",
  "C.2Evolution": "In the Pandemic Response Challenge experiment, the evolution component was implemented insideusing the Evolutionary Surrogate-assisted Prescription (ESP) framework , which was previouslyused to evolve prescriptors for IP optimization from scratch, i.e., without taking advantage of distilledmodels . The distillation above results in evolvable neural networks 1 . . . n which approximate1 . . . n, respectively. These distilled models were then placed into the initial population of a runof ESP, whose goal is to optimize actions given contexts. In ESP, the initial population (i.e., beforeany evolution takes place) usually consists of neural networks with randomly generated weights.By replacing random neural networks with the distilled neural networks, ESP starts from diversehigh-quality solutions, instead of low-quality random solutions. ESP can then be run as usual fromthis starting point. In order to give all distilled models a chance to reproduce, the population removal percentageparameter was set to 0%. Also, since the experiments were run as a quantitative evaluation of teamsin the XPRIZE competition , distilled models were selected for reproduction inversely",
  "proportional to the number of submitted prescriptors for that team. This inverse proportional samplingcreates fair sampling at the team level": "Baseline experiments were run using the exact same algorithm but with initial populations consistingentirely of randomly initialized models (i.e., instead of distilled models). The population size was200; in RHEA, 169 of the 200 random NNs in the initial population were replaced with distilledmodels. Ten independent evolutionary runs of 100 generations each were run for both the RHEA andbaseline settings. The task for evolution was to prescribe IPs for 90 days starting on February 12, 2021, for the 20regions with the most total deaths at that time. Internally, ESP uses the Pareto-based selectionmechanism from NSGA-II to handle multiple objectives . The current experiments were implemented with ESP because it is an already established method inthis domain. Note, however, that such distillation followed by injecting in the initial population couldbe used in principle to initialize the population of any multi-objective evolution-based method thatevolves functions.",
  "C.3Pareto-based Performance Metrics": "This section details the multi-objective performance evaluation approach used in this paper. It is basedon comparing Pareto fronts, which are the standard way of quantifying progress in multi-objectiveoptimization. While there are many ways to evaluate multi-objective optimization methods, the goalin this paper is to do it in a manner that would be most useful to a real-world decision-maker. That is,ideally, the metrics should be interpretable and have immediate implications for which method wouldbe preferred in practice. In the Pandemic Response Challenge experiment, each solution generated by each method m inthe set of considered methods M yields a policy with a particular average daily cost c and a corresponding number of predicted new cases a 0 . Each method returns a set of Nmsolutions which yield a set of objective pairs Sm = {(ci, ai)}Nmi=1. Following the standard definition,one solution s1 = (c1, a1) is said to dominate another s2 = (c2, a2) if and only if",
  "(c1 < c2 a1 a2) (c1 c2 a1 < a2),(4)": "i.e., it is at least as good on each metric and better on at least one. If s1 dominates s2, we writes1 s2. The Pareto front Fm of method m is the subset of all si = (ci, ai) Sm that are notdominated by any sj = (cj, aj) Sm. The following metrics are considered: Hypervolume Improvement (HVI)Dominated hypervolume is the most common general-purposemetric used for evaluating multi-objective optimization methods . Given a reference point in theobjective space, it is the amount of dominated area between the Pareto front and the reference point.The reference point is generally chosen to be a worst-possible solution, so the natural choice in thispaper is the point with maximum IP cost and number of cases reached when all IPs are set to 0. Callthis reference point so = (co, ao). Formally, the hypervolume is given by",
  "R2 1 s Fm : s s s sods,(5)": "where 1 is the indicator function. Note that HV can be computed in time linear in the cardinality ofFm. HVI, then, is the improvement in hypervolume compared to the Pareto front Fmo of a referencemethod mo:HVI(m) = HV(m) HV(mo).(6)The motivation behind HVI is to normalize for the fact that the raw hypervolume metric is ofteninflated by empty unreachable solution space. Domination Rate (DR)This metric is a head-to-head variant of the Domination Count metricused in Phase 2 evaluation in the XPRIZE, and goes by other names such as Two-set Coverage .It is the proportion of solutions in the Pareto front Fmo of reference method mo that are dominatedby solutions in the Pareto front of method m:",
  "|Fmo| {so Fmo : ( s Fm : s so)}.(7)": "The above generic multi-objective metrics can be difficult to interpret from a policy-implementationperspective, since, e.g., hypervolume is in units of cost times cases, and the domination rate can beheavily biased by where solutions on the reference Pareto front tend to cluster. The following threemetrics are more interpretable and thus more directly usable by users of such a system.",
  "MCR(m) = maxao a (so = (co, ao) Fmo, s = (c, a) Fm) : s so.(8)": "In other words, there is a solution in Fmo such that one can reduce the number of cases by MCR(m),with no increase in cost. If MCR is high, then there are solutions on the reference front that can bedramatically improved. The final two metrics, RUN and REM, are instances of the R1 metric for multi-objective evaluation, which is abstractly defined as the probability of selecting solutions from one set versusanother given a distribution over decision-maker utility functions. R1 Metric: Uniform (RUN)This metric captures how often a decision-maker would prefersolutions from one particular Pareto front among many. Say a decision-maker has a particular costthey are willing to pay when selecting a policy. The RUN for a method m is the proportion of costswhose nearest solution on the combined Pareto front F (the Pareto front computed from the union ofall Fm m M) belong to m:",
  "where s = (c, a). Here, cmin = 0, and cmax = 34, since that is the sum of the maximum settingsacross all IPs. Note that RUN can be computed in time linear in the cardinality of F": "RUN gives a complete picture of the preferability of each methods Pareto front, but is agnostic as tothe real preferences of decision-makers. In other words, it assumes a uniform distribution over costpreferences. The final metric adjusts for the empirical estimations of such preferences, so that theresult is more indicative of real-world value. R1 Metric: Empirical (REM)This metric adjusts the RUN by the real-world distribution of costpreferences, estimated by their empirical probabilities p(c) at the same date across all geographies ofinterest:",
  "c c Fmdc.(10)": "In this paper, p(c) is estimated with Gaussian Kernel Density Estimation (KDE; d), usingthe scipy implementation with default parameters . For the metrics that require a referencePareto front against which performance is measured (HVI, DR, and MCR), Distilled is used as thisreference; it represents the human-developed solutions, and the goal is to compare the performanceof Human+AI (i.e. RHEA) to human alone. All of the above metrics are used to compare solutions in of the main paper. They all consistentlydemonstrate that RHEA creates the best solutions and that they also would be likely to be preferredby human decision makers.",
  "C.4Analysis of Schedule Dynamics": "The data for the analysis illustrated in is from all submitted prescriptors and single runsof RHEA, evolution alone, and real schedules. Each point in a corresponds to a scheduleS Z90125produced by a policy for one of the 20 geos used in evolution. For visualization, eachS was reduced to S 12 by taking the mean of IPs across time, and these 12-D S vectorswere processed via UMAP with n_neighbors=25, min_dist=1.0, and all other parametersdefault.",
  "C.5Pareto Contributions": "To measure the contribution of individual models, the ancestry of individuals on the final Pareto frontof RHEA is analyzed. For each distilled model i, the number of final Pareto front individuals whohave i as an ancestor is counted, and the percentage of genetic material on the final Pareto frontthat originally comes from i is calculated. Formally, these two metrics are computed recursively.Let Par() be the parent set of in the evolutionary tree. Individuals in the initial population havean empty parent set; individuals in further generations have two parents. Let F be the set of allindividuals on the final Pareto front. Then, the ancestors of are",
  "C.6Energy Estimates": "The relatively compact model size in RHEA makes it accessible and results in a low environmentalimpact. Each run of evolution in the Pandemic Response experiments ran on 16 CPU cores, consumingan estimated 3.9 106J. This computation is orders-of-magnitude less energy intensive than manyother current AI systems: For instance, training AlphaGo took many limited-availability and expensiveTPUs, consuming 8.8 1011J ; training standard image and language models on GPUs canconsume 6.7 108J and 6.8 1011J , respectively. Specifically:",
  "This section considers ethical topics related to deploying RHEA and similar systems in the real world": "Fairness. Fairness constraints could be directly incorporated into the system. RHEAs multi-objectiveoptimization can use any objective that can be computed based on the systems behavior, so a fairnessobjective could be used if impacts on the subgroups can be measured. A human user might alsointegrate this objective into their calculation of a unified Cost objective, since any deviation from idealfairness is a societal cost. In deployment, an oversight committee could interrogate any developedmetrics before they are used in the optimization process to ensure that they align with declaredsocietal goals. Governance and Democratic Accountability. This is a key topic of Project Resilience , whosegoal is to generalize the framework of the Pandemic Response Challenge to SDG goals more broadly.We are currently involved in developing the structure of this platform. For any decision-makingproject there are four main roles: Decision-maker, Experts, Moderators, and the Public. The goal isto bring these roles together under a unified governance structure. At a high-level, the process for any project would be: the Decision-maker defines the problem forwhich they need help; Experts build models for the problem and make them (or data to produce Dis-tilled versions) public; Moderators supervise (transparently) what Experts contribute (data, predictorsor prescriptors); The Public comments on the process, including making suggestions on what to doin particular contexts, and on ways to improve the models (e.g., adding new features, or modifyingobjectives); Experts incorporate this feedback to update their models; after sufficient discussion, theDecision-maker uses the platform to make decisions, looking at what the Public has suggested andwhat the models suggest, using the Pareto front to make sense of key trade-offs; The Decision-makercommunicates about their final decision, i.e., what was considered, why they settled on this set ofactions, etc. In this way, key elements of the decision-making process are transparent, and decision-makers can be held accountable for how they integrate this kind of AI system into their decisions. Byenabling a public discussion alongside the modeling/optimization process, the system attempts tomove AI-assisted decision-making toward participative democracy grounded in science. The closestexample of an existing platform with a similar interface is but itis for predictions, not prescriptions, and problems are not linked to particular decision-makers. It is important that the public has access to the models via an app, giving them a way to directlyinvestigate how the models are behaving, as in existing Project Resilience proof-of-concepts234,along with a way of flagging any issues/concerns/insights they come across. A unified governanceplatform like the one outlined above also would enable mechanisms of expert vetting by the public,decision-makers, or other experts. The technical framework introduced in this paper provides a mechanism for incorporating demo-cratically sourced knowledge into a decision-making process. However, guaranteeing that sourcedknowledge is democratic is a much larger (and more challenging) civil problem. The concepts ofpower imbalances and information asymmetry are fundamental to this challenge. Our hope is that, bystarting to formalize and decompose decision-making processes more clearly, it will become easier toidentify which components of the process should be prioritized for interrogation and modification,toward the goal of a system with true democratic accountability. For example, the formal decomposition of RHEA into Define, Gather, Distill, Evolve, enables eachstep to be interrogated independently for further development. The implementation in the paper startswith the most natural implementation of each step as a proof-of-concept, which should serve as afoundation for future developments. For example, there is a major opportunity to investigate thedynamics of refinements of the Distill step. In the experiments in this paper, classical aggregatedmachine learning metrics were used to evaluate the quality of distillation, but in a more democraticplatform, experts could specify exactly the kinds of behavior they require the distillation of theirmodels to capture. By opening up the evaluation of distillation beyond standard metrics, we couldgain a new view into the kinds of model behavior users really care about. That said, methods couldalso be taken directly from machine learning, such as those discussed App. A. However, we do notbelieve any of these existing methods are at a point where the humans can be removed from the loopin the kinds of real-world domains the approach aims to address. Data Privacy and Security. Since experts submit complete prescriptors, no sensitive data they mayhave used to build their prescriptors needs to be shared. In the Gather step, each expert team hadan independent node to submit their prescriptors. The data for the team was generated by runningtheir prescriptors on their node. The format of the data was then automatically verified, to ensure thatit complied with the Defined API. Verified data from all teams was then aggregated for the Distill& Evolve steps. Since the aggregated data must fit an API that does not allow for extra data to bedisclosed, the chance of disclosing sensitive data in the Gather phase is minimized. One mechanismfor improving security is to allow the user of each role to rate sources, data, and models for a quality,reliability, and security standpoint, similar to established approaches in cybersecurity5. External Oversight. Although the above mechanisms all could yield meaningful steps in addressing abroad range of ethical concerns, they cannot completely solve all issues of ethical deployment. So, itis critical that the system is not deployed in an isolated way, but integrated into existing democraticdecision-making processes, with appropriate external oversight. Any plan for deployment shouldinclude a disclosure of these risks to weigh against the potential societal benefits.",
  "EData Availability": "The data collected from the XPRIZE Pandemic Response Challenge (in the Define and Gatherphases) and used to distill models that were then Evolved can be found on AWS S3 at (i.e., in the public S3 bucket namedcovid-xprize-anon, so it is also accessible via the AWS command line). This is the raw data fromthe Challenge, but with the names of the teams anonymized. The format of the data is based on theformat developed for the Oxford COVID-19 Government Response Tracker .",
  "FCode Availability": "The formal problem definition, requirements, API, and code utilities are for the XPRIZE, includingthe standardized predictor, are publicly available . The prediction and prescription API,as well as the standardized predictor used in the XPRIZE and the evolution experiments can befound at The Evolve step in theexperiments was implemented in a proprietary implementation of the ESP framework, but thealgorithms used therein have been described in detail in prior work . Code for the illustrativedomain was implemented outside of the proprietary framework and can be found at"
}