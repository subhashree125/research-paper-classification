{
  "Abstract": "Graph neural networks (GNNs) have attracted considerable attention due to theirdiverse applications. However, the scarcity and quality limitations of graph datapresent challenges to their training process in practical settings. To facilitate thedevelopment of effective GNNs, companies and researchers often seek externalcollaboration. Yet, directly sharing data raises privacy concerns, motivating dataowners to train GNNs on their private graphs and share the trained models. Un-fortunately, these models may still inadvertently disclose sensitive properties oftheir training graphs (e.g., average default rate in a transaction network), leadingto severe consequences for data owners. In this work, we study graph propertyinference attack to identify the risk of sensitive property information leakage fromshared models. Existing approaches typically train numerous shadow models fordeveloping such attack, which is computationally intensive and impractical. Toaddress this issue, we propose an efficient graph property inference attack by lever-aging model approximation techniques. Our method only requires training a smallset of models on graphs, while generating a sufficient number of approximatedshadow models for attacks. To enhance diversity while reducing errors in the ap-proximated models, we apply edit distance to quantify the diversity within a groupof approximated models and introduce a theoretically guaranteed criterion to evalu-ate each models error. Subsequently, we propose a novel selection mechanism toensure that the retained approximated models achieve high diversity and low error.Extensive experiments across six real-world scenarios demonstrate our methodssubstantial improvement, with average increases of 2.7% in attack accuracy and4.1% in ROC-AUC, while being 6.5 faster compared to the best baseline.",
  "Introduction": "Graph data, encapsulating relationships between entities across various domains such as socialnetworks, molecular networks, and transaction networks, holds immense value . Graph neuralnetworks (GNNs) have proven effective in modeling graph data , yielding promising resultsacross diverse applications, including recommender systems , molecular prediction , andanomaly detection . While training high-quality GNN models may necessitate a substantialamount of data, graphs may be scarce or of low quality in practice , prompting companiesand researchers to seek additional data from external sources .",
  "arXiv:2411.03663v1 [cs.LG] 6 Nov 2024": "However, directly obtaining data from other sources is often difficult due to privacy concerns . As an alternative, sharing models rather than raw data has become increasingly common .Typically, data owners train a model on their own data and subsequently release it to the communityor collaborators . For instance, a larger bank may train a fraud detection model on its extensivetransaction network and share it with partners, allowing them to use their own customer data toidentify risks. Despite the benefits, this model-sharing strategy sometimes remains vulnerable to data leakage risks.Given access to the released model, one may infer sensitive properties of the data owners graph,which are not intended to be shared. In the context of releasing a fraud detection model, if anadversarial bank can determine the average default rate of all customers in the transaction network,the data owner banks financial status can potentially be revealed. Another example is releasinga recommendation model trained on a companys product network . If a competitor can inferthe distribution of co-purchase links between different products, he may determine which items arefrequently promoted together and deduce the companys marketing tactics. Such attacks are possiblebecause released models may inadvertently retain and expose sensitive information from the trainingdata . We refer to such sensitive information related to the global distribution in a graph asgraph sensitive properties, and we aim to investigate the problem of graph property inference attack. Previous property inference attacks primarily focus on text or image data, assuming modelstrained on different properties exhibit differences in parameters or outputs. For GNNs modelinggraph data, the inherent relationships and message-passing mechanisms can magnify distribution bias, making them more vulnerable to attacks. Although a few studies extend property inferenceto graphs and GNNs , they typically involve creating shadow models that replicate thereleased models architecture and are trained on shadow graphs with varying sensitive properties.The parameters or outputs of shadow models are used to train an attack model to classify the propertyof the data owners graph. A major limitation of these attacks is the need to train a large number ofshadow models (e.g., 4,096 models , 1,600 models ), resulting in significant computationalcost and low efficiency. In this paper, we explore the feasibility of avoiding the training of numerous shadow models bydesigning an efficient yet effective graph property inference attack. Our key insight is to train only asmall set of models and then generate sufficient approximated shadow models to support the attack.To this end, we leverage and extend model approximation techniques. For a given dataset and a modeltrained on it, when the training data changes (e.g., removing a sample), model approximation allowsthe efficient estimation of new model parameters for the updated dataset without retraining. Thistechnique, often called unlearning , enables the efficient generation of multiple approximatedshadow models from a single trained model. Specifically, given a small set of graphs and theircorresponding trained models, we perturb each graph to alter sensitive properties (e.g., changing thenumber of nodes corresponding to high default rate users) and then apply model approximation toproduce a sufficient number of approximated models corresponding to the perturbations, therebyreducing the total attack cost. illustrates our approach compared to the traditional attack. Nevertheless, achieving this goal presents several challenges. The first challenge is to ensure thediversity of approximated models, which provides a broader range of training samples for the attackmodel and enhances its generalization capability. To tackle this, we develop structure-aware randomwalk sampling graphs from distinctive communities and introduce edit distance to quantify thediversity of a set of approximated models. The second challenge is to ensure that the errors in theapproximated shadow models are sufficiently small. Otherwise, these models may fail to accuratelyreflect differences in graph properties, thereby diminishing attack performance. To address this, weestablish that different graph perturbations can lead to varying approximation errors, which offersa theoretical-guaranteed criterion for assessing the errors of each approximated model. Finally, wepropose a novel selection mechanism to reduce errors while enhancing the diversity of approximatedmodels, formulated as an efficiently solvable programming problem. Our contributions are as follows:",
  "We propose an efficient and effective graph property inference attack that requires training only afew models to generate sufficient approximated models for the attack": "We propose a novel selection mechanism to retain approximated models with high diversity andlow error, using edit distance to measure the diversity of approximated models and a theoreticalcriterion for assessing the errors of each. This diversity-error optimization is formulated as anefficiently solvable programming problem. Experiments on six real-world scenarios demonstrate the efficiency and effectiveness of ourproposed attack method. On average, existing attacks require training 700 shadow models toachieve 67.3% accuracy and 63.6% ROC-AUC, whereas our method trains only 66.7 models andobtains others by approximation, achieving 69.0% attack accuracy and 66.4% ROC-AUC.",
  "Problem Definition": "The scenario of a graph property inference attack first involves a data owner who trains a GNN modelusing his graph data, referred to as the target model and target graph in the following text. Oncetrained, the target models parameters or output posterior probabilities can be released in communities. For example, the data owner can upload the pre-trained parameters to GitHub tofacilitate downstream tasks. Or he may upload the posterior probabilities from a recommender systemto a third-party online optimization solver , such as Gurobi *. Collaborative machine learning isanother potential attack scenario . For instance, consider two banks aiming to collaborativelytrain a fraud detection model. While sharing raw transaction networks poses risks to privacy andcommercial confidentiality, they can employ model-sharing strategies . With access to the target model, curious users may launch inference attacks to obtain some sensitiveproperties of the target graph, which can reveal secrets not intended to be shared. For example,the ratio of co-purchase links between particular products in a product network may relate to thepromoting tactics of a sales company, or the average default rate in a transaction network may revealthe financial status of a bank. Such confidential information may further impact business competition. In the rest of this section, we first define the privacy, i.e., the sensitive properties referred to in thiswork. Then, we introduce the knowledge of the attack. Finally, we formulate the problem of propertyinference attacks. Graph sensitive property.In this paper, we consider an attributed target graph where nodes areassociated with multiple attributes. The sensitive property is defined based on one specific type ofattribute, called the property attribute. Specifically, the sensitive property is defined as a certainstatistical value of the property attributes distribution. We consider two types of properties that theattacker may infer: (1) node properties, specified by the ratio of nodes with a particular propertyattribute value, and (2) link properties, specified by the ratio of links where the end nodes haveparticular property attribute values. Note that the property attribute can be either discrete or continuous. For instance, a node propertydefined on the discrete category attribute in a product network can be the ratio of co-purchaselinks between luxury items. An edge property defined on the continuous default rate attribute in atransaction network can be the average default rate of all customers. The inferred sensitive propertiesmay reveal data owners secrets such as commercial strategies; see for further discussion.",
  "Attackers knowledge.The attackers background knowledge is assumed to be as follows:": "Auxiliary graph: We assume the attacker has an auxiliary graph from the same domain as the targetgraph but does not necessarily intersect with the latter. In practice, the auxiliary graph can besourced from publicly available data or derived directly from the adversarys own knowledge. Target model: We consider two types of knowledge on the target model: the white-box setting,where the adversary knows the architecture and parameters of the target GNN, and the black-boxsetting, where the adversary only knows the target GNNs output posterior probabilities. Property inference attack.Formally, let Gtar denote the target graph. And let P(Gtar) denote theproperty value of Gtar. Note that P can represent either node properties or link properties. Given thatthe attacker has an auxiliary graph Gaux from the same domain as Gtar, we define graph propertyinference attack as follows: Problem 1 (Graph property inference attack) Given the auxiliary graph Gaux, and assume theattacker has either the white-box knowledge of the target GNN parameters or the black-box knowledgeof target GNNs output posterior probabilities, the objective of the graph property inference attack isto infer the property P(Gtar) without access to it.",
  "Methodology": "This section provides a detailed description of the proposed graph property inference attack. We startwith an overview of our method and then delve into the technical aspects of model approximation.Finally, we describe how to ensure the diversity of approximated models while reducing their errors.The overall algorithm and complexity analysis are summarized in Appendix A.3.",
  "Overview": "As shown in (a), given the auxiliary graph, the conventional approach is to first samplenumerous shadow graphs, ensuring that graphs with different properties are adequately represented.Each is then used to train a shadow model with the same structure as the target model. Once trained,parameters (white-box) or output posterior probabilities (black-box) of shadow models are collected,along with the corresponding properties of shadow graphs. Finally, an attack model (e.g. linearclassifier) is trained to classify properties based on parameters or posteriors. Since the number ofshadow models is usually hundreds or thousands, their training can be computationally expensive.",
  "(4) We collect parameters or posteriors of all approximated models and train the attack model in asimilar manner as previous attacks": "Here, we mainly face two challenges: ensuring that the approximate error associated with augmen-tations is relatively small, and ensuring that the approximated models are sufficiently diverse. Toaddress them, we derive a theoretical criterion for calculating approximation errors across differentaugmented graphs (see 3.2) and design a diversity enhancement strategy in 3.3.",
  "We proceed by introducing the techniques of model approximation, which include generating aug-mented graphs, obtaining approximated models, and conducting theoretical analysis for error criterion": "Generating augmented graphs and identifying influenced nodes.First, we aim to ensure thatmultiple perturbations produce distinctive augmented graphs. This is essential because highly similaraugmentations reduce the distinction in the corresponding graph properties and model features,providing minimal benefit to the overall attack. For this purpose, we propose removing both nodesand edges from the reference graph. Formally, Let the reference graph be denoted by Gref = (V, E)sampled from Gaux, where V is the node set, E V V is the edge set. For one perturbation, we remove V R V and ER E to obtain the augmented graph Gaug. In GNNs, the neighborhoodaggregation makes the removal inevitably influence the state of other remaining nodes. Given al-layer GNN, the influenced nodes of removing a single node v V R is the l-hop neighborhood of v,denote as Nl(v). And the influenced nodes of removing a single edge e ER, connecting nodes vand u, is denoted as Nl(e) = Nl1(v) Nl1(u) {v, u}. With these in mind, we next define thetotal influenced nodes for removing V R and ER.",
  "Note that V I is exclusive of V R; we omit the set difference for simplicity": "Generating Approximated Models.Subsequently, we generate the approximated model basedon the perturbation. While existing graph unlearning may offer potential solutions,they are either limited to specific model architectures or only support the removal of nodes oredges individually, making them unsuitable for direct application. To address this, we extend theirmechanisms to suit our scenario. Let the reference model be parameterized by ref Rm. In thispaper, we consider cross-entropy loss as the loss function, and ref is obtained as follows:",
  "vV/V R (ref; v, E/ER).The detailed derivation can be found in Appendix A.2": "In practice, the Hessian may be non-invertible due to the non-convexity of GNNs. We address this byadding a damping term to the Hessian .To reduce computation, we also follow to convert theinverse Hessian calculation into quadratic minimization. See Appendix A.3 for complexity analysis. Analyzing the approximation error.Eventually, we aim to quantitatively assess the error in theapproximated model, as this directly determines whether graph properties can be effectively reflected,thereby influencing the attack. To achieve this, we investigate how specific removal choices of V Rand ER affect the approximation error in Eq. (4). Note that vV/V R (; v, E/ER) = 0 onlywhen aug is the exact minimizer, thus the gradient norm vV/V R (; v, E/ER)2 can reflectthe approximation error. The following theorem provides an upper bound on this gradient norm.",
  "vV/V R(aug; v, E/ER)2 C(|V R| + 2|V I|)2 = C (V R, ER),(5)": "where | | denotes the cardinality of a set, and (, ) denotes the square of the number of nodesremoved and influenced, given V R and ER. C denotes a constant depending on the GNN model, seeAppendix A.2 for detail proof. Theorem 3.2 indicates that the error bound for the approximation is related to both the number ofremoved nodes and influenced nodes. Next, we demonstrate how this can serve as an error criterionto select augmented graphs that result in minimal approximation errors.",
  "Diversity enhancement": "Following the above, we detail the designed diversity enhancement strategy. To develop a well-generalized attack model capable of distinguishing different sensitive properties, we first applya structure-aware random walk for sampling diverse reference graphs. We then propose a novelselection mechanism to ensure that multiple perturbations on the reference graphs further enhancediversity while considering the reduction of approximation error. Sampling diverse reference graphs.Inspired by community detection, where diverse communitiesare identified on a graph, we design a structure-aware random walk for sampling reference graphs.Specifically, we incorporate Louvain community detection to partition the auxiliary graph intoseveral similarly sized communities. During random walks, the starting nodes are chosen fromdifferent communities. And we assign different weights to neighboring nodes: w for those withinthe same community and 1 w for those from different communities, where w is a hyper-parameter. The transition probabilities are then obtained by normalizing these weights. This strategyencourages sampling reference graphs within distinct communities, thus boosting their diversity. Ensuring diverse augmented graphs.To ensure perturbations on reference graphs can enhancethe diversity, we further design a perturbation selector. Based on 3.2, it is easy to see that eachapproximated model can be considered as a result of the specific perturbation. Thus, improvingthe diversity of approximated models is essentially improving the diversity of augmented graphs.Formally, for each reference graph we generate k augmented graphs Gaug = {Gaug1, Gaug2, . . . , Gaugk}by randomly removing k different sets of nodes and edges. The diversity for Gaug is defined as:",
  "defined as the sum of all pair-wise graph distances in Gaug, that is, ki=1kj=1 dGaugi, Gaugj": "Since stochastic augmentations may not all contribute to total diversity, our objective is to select adiverse subset of Gaug, namely, a subset of diverse perturbations to enhance the diversity of augmentedmodels. However, it is important to note that solely maximizing diversity may lead to relatively largeapproximation errors, which may worsen the attack performance. Fortunately, utilizing the errorcriterion from Eq. (5), we can ensure that augmentations enhance diversity while minimizing totalapproximation error, which can be formulated as a quadratic integer programming task. Given k available perturbations, we aim to select q of them, such that the diversity among theseselected is maximized while keeping the approximation error minimal. We here introduce decisionvariables xi {0, 1} to represent whether the i-th augmentation is selected. Let i represent theapproximation error in the i-th augmentation (cf. Eq. (5)). The optimization problem is as follows:",
  "i=1ixi ,(6)": "where is a constant that imposes the budget on the total approximation error of the selected qaugmentations, ensuring that it does not exceed . Here, we select graph edit distance as the distancemetric, which can be efficiently calculated since all k augmented graphs Gaug are derived from onereference Gref. We utilize Gurobi Optimizer , a state-of-the-art solver, to solve this quadraticinteger programming problem, which is known for its efficiency and effectiveness.",
  "Datasets and sensitive properties.We conduct property inferences on three real world datasets:Facebook , Pubmed , and Pokec . Appendix A.4 details the datasets and properties": "Facebook and Pokec are social networks where nodes represent users and edges denote friendships.Following , we select gender as the property attribute, set node property as whether the malenodes are dominant, and edge property as whether the same-gender edges are dominant. Pubmed is a citation network where nodes are publications and edges are citations. We select thekeyword Insulin (IS) as the property attribute. Node property is whether publications with ISare dominant. Edge property is whether citations between publications with IS are dominant. Allused properties are summarized in . Training and testing data.For fairness, we evaluate our method and baselines on the same targetgraphs. To ensure there is no overlap between the target graph and the auxiliary graph, for eachdataset we first use Louvain community detection to split the original graph into two similarly sizedparts. One part is used as the auxiliary graph, and the other part is used to sample multiple targetgraphs. Sizes and numbers of reference graphs (our method), shadow graphs (baselines), and targetgraphs are provided in Appendix A.4. Target GNN.For target GNN, We use a widely recognized GNN model, GraphSAGE ,configured as per with 2 layers, 64 hidden sizes, and 1,500 training epochs with an early stoptolerance of 50. The Adam optimizer is used with a learning rate of 1e-4 and a weight decay of 5e-4. Implementation details.For the attack model, We use a linear classifier with the deepest trick ;For hyper-parameter settings, we perform grid searches of reference graphs numbers in (0, 100] (stepsize 25), and augmented graphs numbers in (0, 10] (step size 2), across all datasets. Experiments arerepeated 5 times to report the averages with standard deviations. See appendix A.4 for more details.Our codes are available at Baselines.We adopt four state-of-the-art baseline models to compare against the proposed attackmodel: (1) GPIA : An attack method designed for graphs and GNNs in both white-box and black-box settings, following the traditional attack framework. (2) PIR-S/PIR-D : Two permutationequivalence methods designed for white-box attacks, PIR-S using neuron sorting and PIR-D usingset-based representation. (3) AIA : Property inference method based on attribute inference attack,which first predicts the property attribute based on embeddings/posteriors and then predicts property,suitable for both white-box/black-box attacks. See Appendix A.4 for details.",
  "Evaluation of efficiency and effectiveness (RQ1)": "We first focus on white-box settings and evaluate the accuracy and ROC-AUC for effectiveness andruntime for efficiency. Note that the reported runtime throughout this work encompasses the entireattack process for both the proposed method and baselines, starting from sampling the reference(shadow) graphs to inferring the properties of the target graphs. presents the averageaccuracy and runtime of the proposed attack method compared to other baseline methods on thesix aforementioned sensitive properties. We provide the corresponding standard deviations andROC-AUCs results in Appendix A.5. The results reveal several key insights: (1) Traditional attacksincur significantly high runtime. The slight differences mainly depend on the different strategies intheir attack models. (2) PIR-D achieves better accuracy among the baselines, possibly due to theirconsideration of permutation equivalence. AIA shows lower performance, which may be because oftheir limited ability to conduct attribute inference, thus affecting the classification of properties. (3)The proposed attack model outperforms all baseline methods across all datasets, achieving an averageincrease of 2.7% in accuracy and being 6.5 faster compared to the best baseline, demonstratingits remarkable efficiency and efficacy. The significant margin by which our method outperforms thebaselines is primarily due to our specific mechanisms that ensure diversity in both reference andaugmented graphs, which are essential for training a robust attack model. In contrast, conventionalattacks lack such designs for shadow graph diversity, resulting in sub-optimal performance.",
  "Evaluation of influencing factors (RQ2)": "Ablation study.To ensure effectiveness, our method includes two main mechanisms: samplingdiverse reference graphs and selecting diverse augmented graphs. Here, we conduct ablation studiesto demonstrate their necessity, including four variants: (1) w/o structure: We discard structure-awaresampling and use simple random walks to sample reference graphs. (2) w/o selector: We discard theaugmentation selector and use random removal to obtain augmented graphs. (3) w/o error: In theaugmentation selector (cf. Eq. (6)), we ignore the approximation error and only select augmentationsthat maximize diversity. (4) w/o diversity: We ignore diversity in the augmentation selector (cf.Eq. (6)) and only select augmentations that minimize the approximation error. (a) showsthe attack results on Facebooks node property. Notably, the complete model consistently surpassesthe performance of all variants, showing the effectiveness and necessity of simultaneously samplingdiverse reference graphs and selecting diverse augmented graphs. Hyper-parameter analysis.We next evaluate the impact of two important hyper-parameters on ourmethod: (1) the number of reference graphs and (2) the number of selected augmented graphs. Bothdirectly affect the diversity of approximated models. We tune the number of reference graphs among{25, 50, 75, 100} and the number of selected augmented graphs among {2, 4, 6, 8, 10}. The results in (b) and 2 (c) show that as both hyper-parameters increase, the attack performance initiallyimproves and then stabilizes. This indicates that a relatively small number of reference graphs andaugmented graphs are sufficient to ensure diversity, thereby maintaining good attack performance.",
  "Acc.Rt.Acc": ": (a) Evaluation of the necessity of considering diversity while minimizing the approximationerror. (b) and (c) Impact of the number of augmented graphs (per reference graph) and referencegraphs on attack accuracy, respectively. (d) Accuracy and runtime comparison in black-box settings. To test the applicability of our method, we evaluate its performance under various conditions,including scenarios with black-box adversary knowledge, on different types of GNN models, onlarge-scale graph datasets, and when the target and auxiliary graphs are distinct. Performance on black-box knowledge.In the black-box setting, we use model outputs, specificallyposterior probabilities, to train attack models for our method and baselines. Since PIR-D and PIR-Sonly support white-box settings, we included another state-of-the-art black-box attack, PIA-MP ,as detailed in Appendix A.4. The results on Facebooks node property in (d) show that ourmethod improves accuracy by 11.5% compared to the best baselines while being 7.3 faster.",
  "Ours58.3 267 57.3 236 65.7 233 59.3 177": "Performance on other GNNs.We conductproperty inference attacks on other three funda-mental GNNs: GCN , GAT , and SGC. For GCN and GAT, hyper-parameters areconfigured according to , while for SGC,we set the number of hops to 2. We reportthe attack accuracy and runtime of our methodalongside other baselines on Facebooks nodeproperty, as illustrated in (a)-(c). Itis observed that the overall attack accuracy forSGC is comparatively lower, potentially due tothe SGC models inherent limitations in captur-ing property information effectively. Moreover,our method consistently achieves the highestaccuracy, also demonstrating a runtime that is4.4 faster on GCN, 4.0 faster on GAT, and 4.3 faster on SGC compared to the best baseline. Performance on scalability.We further conducted property inference attacks on a large-scalegraph dataset, Pokec-100M, which contains 1,027,956 nodes and 27,718,416 edges. This graph issampled from the original dataset by retaining nodes with relatively complete features. Wetargeted the same node property as in the Pokec dataset, with the number of nodes in the referencegraphs, shadow graphs, and target graphs set to 52,600, 50,000, and 50,000, respectively. All othersettings remain consistent with previous experiments. We compare the attack accuracy and runtime ofour method against other baselines. As shown in (d), conventional attacks incur significantcomputational costs on this dataset, whereas our method is 10.0 faster. Additionally, our attackaccuracy is significantly higher than those of the baselines. Performance with distinct target and auxiliary graphs.In the above experiments, the targetand auxiliary graphs are splits of the same original graph. However, in real-world scenarios, thisassumption may not hold. Therefore, we evaluate the performance of our attack under a more practicalcondition, where distinct graphs (from the same domain) are used as the target and auxiliary graphs.Specifically, we select Facebook and Pokec, as they are both social networks, and consider two cases:using Facebook as the target and Pokec as the auxiliary graph, and vice versa. Since the featuredimensions of these two datasets differ, the parameters of the approximated model and the targetmodel are not directly compatible, so we apply PCA dimension reduction to align the parameters.",
  "Literature Review": "Property inference attack.The concept of property inference attack is first introduced by ,demonstrating the leakage of sensitive properties from hidden Markov models and support vectormachines in systems like speech-to-text. Building on this, attacks on various machine learning modelsare studied, including feed-forward neural networks, convolutional neural networks, and generativeadversarial networks . Some works also consider multi-party collaborative learningscenarios or incorporate data poisoning . Specifically, proposes an efficientattack based on distinguishing tests, achieving faster performance than traditional shadow training.Their setting differs from ours by the additional adversarial capability of data poisoning. Recently,with the increasing use of graphs and GNNs, security and privacy concerns are emerging .While efforts have been made to investigate property inference attacks on GNNs , theyfollow the shadow training framework, which requires training a relatively large number of shadowGNN models, leading to high computational costs and reduced feasibility . assumes accessto the embedding of whole graphs and targets at graph-level properties, which is beyond our scope. GNN model approximation.GNN model approximations are primarily based on the influencefunction or Newton update . Except for , these methods are utilizedin the context of graph unlearning. Studies explore model approximation for edge ornode removal and analyze the corresponding approximation error bounds, yet they are limited tospecific model architectures (e.g., simple graph convolution, graph scattering transform). Furtherefforts extend model approximation to generic GNNs. introduces a framework for edgeunlearning, while proposes a general unlearning framework for removing either nodes, edges,or features individually. Our model approximation differs from above by enabling the simultaneousremoval of nodes and edges across generic GNN architecture. A concurrent work addresses asimilar model approximation as our attack; however, the additional theoretical assumptions could failwhen removing a combination of nodes and edges, and their corresponding solution may significantlycompromise the efficiency. Other studies also employ the graph shard approach. However,they may have poor efficiency in batch removal, which involves multiple retraining of sub-models.",
  "Conclusion": "In this paper, we focus on the problem of graph property inference attacks. We utilize modelapproximation to efficiently generate approximated models after initially training a small set ofmodels, which replaces the costly shadow training in traditional attacks. To overcome the challengeof ensuring the diversity of approximated models while reducing the approximation error, we firstderive a theoretical criterion to quantify the impact of different augmentations on approximation error.Next, we propose a diversity enhancement strategy, including a structure-aware random walk forsampling diverse reference graphs and a selection mechanism to retain optimal approximated models,utilizing edit distance to measure diversity and the theoretical criterion to assess approximation error.The retained approximated models are finally used to train an attack classifier. Extensive experimentsacross six real-world scenarios demonstrate our attacks outstanding efficiency and effectiveness.",
  "Ziniu Hu et al. Gpt-gnn: Generative pre-training of graph neural networks. In: Proceedingsof the 26th ACM SIGKDD international conference on knowledge discovery & data mining.2020, pp. 18571867": "Da Zheng et al. DGL-KE: Training Knowledge Graph Embeddings at Scale. In: Proceedingsof the 43rd International ACM SIGIR Conference on Research and Development in InformationRetrieval. SIGIR 20. New York, NY, USA: Association for Computing Machinery, 2020,pp. 739748. Jiezhong Qiu et al. Gcc: Graph contrastive coding for graph neural network pre-training. In:Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &data mining. 2020, pp. 11501160.",
  "Wanrong Zhang, Shruti Tople, and Olga Ohrimenko. Leakage of dataset properties in {Multi-Party} machine learning. In: 30th USENIX security symposium (USENIX Security 21). 2021,pp. 26872704": "Xiuling Wang and Wendy Hui Wang. Group property inference attacks against graph neu-ral networks. In: Proceedings of the 2022 ACM SIGSAC Conference on Computer andCommunications Security. 2022, pp. 28712884. Karan Ganju et al. Property inference attacks on fully connected neural networks usingpermutation invariant representations. In: Proceedings of the 2018 ACM SIGSAC conferenceon computer and communications security. 2018, pp. 619633. Giuseppe Ateniese et al. Hacking smart machines with smarter ones: How to extract meaning-ful data from machine learning classifiers. In: International Journal of Security and Networks10.3 (2015), pp. 137150.",
  "A.3Training algorithm and complexity analysis": "Complexity of generating approximated models.As the computation of gradients can be effi-ciently handled by the PyTorch Autograd Engine, the primary operation is solving the inverse ofthe Hessian (cf. Eq. (4)). To mitigate the high computational cost, we follow in converting theinverse computation into finding the minimizer of a quadratic function, resulting in an approximatedsolution. By leveraging efficient Hessian-vector products and the conjugate gradient method, thiscan be solved with time complexity of O(t||), where || denotes the number of parameters, and trepresents the number of iterations in conjugate gradient method.",
  "Pokec40,478531,7361972Facebook4,309176,4681,2822Pubmed19,71788,6485003": "Facebook : This dataset consists of 4,039 nodes and 176,468 edges. Nodes have features likebirthday, education, work, name, location, gender, hometown, and language, all anonymized forprivacy. The target GNNs task is to classify users education types. PubMed : This dataset includes 19,717 scientific publications related to diabetes, with acitation network of 88,648 links. Each publication is described by a TF/IDF weighted word vectorfrom a dictionary of 500 unique words, such as male, female, children, cholesterol, and insulin.The target GNNs task is to classify the topic categories of the publications.",
  ": end for16: Train an attack model based on the r q parameters (or posterior) and properties, classify P(Gtar)of the target graph Gtar": "Pokec : This online social network dataset is from Slovakia. Each node has anonymizedfeatures such as gender, age, and hobbies. We follow to sample nodes with relatively completefeatures, resulting in a graph with 40,478 nodes and 531,736 edges, using gender, age, height,weight, and region as node features. The target GNNs task is to classify whether a users allfriendships are public. Details of sensitive properties.For each dataset, we design one node property and one link propertyto be targeted in our attacks. For Facebook and Pokec we select gender as the property attribute. ForPubMed, we select the keyword Insuli as the property feature, as it has the highest TF-IDF weight.These properties are summarized in .",
  "Pokec50525Facebook503200Pubmed1004250": "For all baselines, we follow the settings specified in to sample shadow graphs: the size of eachshadow graph is 20%, 25%, and 30% of Pokec, Facebook, and Pubmed, respectively, and the numberof shadow graphs is 700 for all datasets. For target graphs, we sample 300 shadow graphs for each dataset; the size of each shadow graph is20%, 25%, and 30% of Pokec, Facebook, and Pubmed datasets, respectively. To ensure fairness, weevaluate our method and baselines on the same target graphs. More implementation details.All experiments are conducted on a machine of Ubuntu 20.04system with AMD EPYC 7763 (756GB memory) and NVIDIA RTX3090 GPU (24GB memory). Allmodels are implemented in PyTorch version 2.0.1 with CUDA version 11.8 and Python 3.8.0.",
  "A.6Limitation and future work": "In this work, we consider two settings: white-box and black-box, which encompass many real-worldscenarios. However, stricter cases exist where the attacker can make only a limited number of queriesor access only model predictions (i.e., classification results). We acknowledge that our methoddoes not yet address these cases. Additionally, some studies explore scenarios where attackers haveenhanced capabilities, such as data poisoning. We leave the investigation of efficient attacks underthese conditions for future work.",
  "A.7Potential impacts": "While the proposed method is designed to infer properties of specific graph data, our primary objectiveis to raise awareness of the privacy and security concerns associated with GNNs and to encourage theimplementation of protective measures in model design. Traditional property inference methods areoften inefficient, and despite efforts to illuminate potential threats, less practical attack scenarios maynot receive adequate attention. Nonetheless, the privacy risks persist. We seek to bring this threat tothe forefront and advocate for the adoption of more robust protective measures.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: See 4.1 and Appendix A.4.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: See 4.1 and Appendix A.4.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: See the standard deviation in .Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: See the code link provided in implementation details.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This work does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}