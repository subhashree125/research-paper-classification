{
  "Abstract": "We introduce a novel framework, Online Relational Inference (ORI), designed toefficiently identify hidden interaction graphs in evolving multi-agent interactingsystems using streaming data. Unlike traditional offline methods that rely on afixed training set, ORI employs online backpropagation, updating the model witheach new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter,optimized through a new adaptive learning rate technique called AdaRelation,which adjusts based on the historical sensitivity of the decoder to changes in theinteraction graph. Additionally, a data augmentation method named TrajectoryMirror (TM) is introduced to improve generalization by exposing the model tovaried trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantlyimproves the accuracy and adaptability of relational inference in dynamic settingscompared to existing methods. This approach is model-agnostic, enabling seamlessintegration with various neural relational inference (NRI) architectures, and offersa robust solution for real-time applications in complex, evolving systems. Code isavailable at",
  "Introduction": "Multi-agent interacting systems have been studied in various fields, including particle-based physicalsimulations , traffic systems , and social networks . Interaction among agents arecrucial information to accurately model such systems and providing the interpretability in agentbehaviors as well . However, external observers can only access the trajectory of agents withoutknowing interaction graphs. Accordingly, identifying unknown interaction graphs from observabletrajectories of agents has been emerged as a specific problem referred to as relational inference . In recent years, neural relational inference (NRI) and its variants have shown promising performancein synthetic and real-world environments . Prior studies mostly aimed to present betternetwork for NRI based on variational autoencoder (VAE) built with graph neural networks (GNN). These methods involve an encoder to infer an interaction graph as an adjacency matrixfrom observed trajectories and a decoder to predict the future trajectories employing the inferredinteraction graph. They generally perform training offline assuming the well-aligned distribution intraining and testing data. Unfortunately, such assumption is frequently violated in practice due to theshifts in test condition, including sudden changes in the interaction, evolving system parameters andeven dynamics itself. Building a relational inference model generalizable to all the different scenariosis challenging . In this case, online learning is an attractive approach to continuously adaptthe model to the newly observed environments . However, online learning for relational inferencehas been rarely explored.",
  "indicates the presence of a feature, and indicates the absence of a feature": "Online backpropagation using gradient descent is a widely used online learning method as it iscompatibility to various neural network designs . However, online backpropagation on existingNRI models significantly degrades the accuracy on relational inference since their decoder quicklylearns the trajectory prediction even before the encoder generates reasonable interaction graphs.It is important to note that while the models are trained in self-supervised manner (i.e., trajectoryprediction), true labels of interaction graphs are never provided to the models, indicating thatidentifying interaction graphs is essentially unsupervised. That is, optimizing the unsupervisedencoder is more challenging than the self-supervised decoder in nature. The key problem is how tomatch the learning speed between the interaction identification and the trajectory prediction so thatboth the tasks can be collaboratively optimized. In this paper, we propose a novel framework named Online Relational Inference (ORI) to effi-ciently identify hidden interaction graphs in evolving multi-agent interacting systems. Our methodstrategically allocates an adjacency matrix representing the interaction as a trainable parameter inthe model and directly optimizes it through online backpropagation on the predicted trajectories.It effectively accelerates the update of the adjacency matrix than the encoder-based approach andensures the following decoder to be learned with reasonable adjacency matrices from the early stageof training. ORI can seamlessly integrate with prior NRI models, offering architectural flexibility (i.e.,model-agnostic). Moreover, we developed an adaptive learning rate technique named AdaRelationparticularly designed for relational inference in the evolving systems. It employs the historical adja-cency matrix to indirectly estimate the decoders sensitivity over the adjacency matrix and determinewhether the learning needs to be accelerated. In addition, we introduce a data augmentation techniquenamed Trajectory Mirror (TM) to expose various trajectories by flipping the axis in the systems. Weexperimentally demonstrate the effectiveness of ORI on various NRI models in both synthetic andthe real-world (CMU MoCap for human motion ) dataset. Our key contributions are as follows: To the best of our knowledge, ORI is the first model-agnostic online relational inferenceframework for evolving multi-agent interacting systems. ORI employs online backpropaga-tion to optimize an adjacency matrix from the trajectory information without any assumptionson the model architecture. We experimentally demonstrate that ORI identifies unknown interaction graphs in variousevolving multi-agent interacting systems, such as sudden changes in interaction (),parameters in the dynamics (), and even dynamics itself (), outperformingexisting NRI models (). We propose AdaRelation, a novel adaptive learning rate technique particularly designed forrelational inference in evolving multi-agent systems. It automatically tunes the learning ratefor the adjacency matrix when interaction or/and dynamics in the system suddenly change(). We propose Trajectory Mirror, a data augmentation technique to ensure the reasonablerelational inference regardless of the trajectory axis. It significantly improves the conver-gence speed and overall interaction prediction accuracy in the several evolving scenarios(Supplementary).",
  "Neural relational inference.NRI is the first work that formulated the problem of an unsuper-vised relational inference from observed agent trajectories. They implemented a VAE-based graph": "neural network that encodes trajectories into an adjacency matrix, representing relation types, andthen decodes the trajectory using the predicted adjacency matrix. This approach has been establishedas a standard in several follow-up works. For example, dNRI and EvolveGraph introducedgraph recurrent network-based architectures to encode and generate dynamic priors for predictingevolving interactions. Additionally, a memory-augmented architecture has been proposed for en-hanced long-term prediction using external associative memory pools . More recently, finergranularities in relation modeling have been considered, such as edge to edge interaction , relationpotentials using an energy-based function , and disentangled edge features . However, all ofthese method commonly rely on an encoder-decoder pair in an offline setting, supported by a largeamount of batched training samples, without considering various evolving scenarios. Evolving multi-agent systems.Apart from relational inference, trajectory learning of evolvingmulti-agent interacting systems have actively explored using graph neural networks . Inparticular, graph neural ordinary different equations (ODE) have been applied to various evolvingscenarios, including evolving environments (e.g., temperature and pressure) and stochasticmotion . However, these approaches assume a known graph structure, focusing on acceleratingthe simulation of physical dynamical systems . Additionally, each model addresses a specifictype of evolution, raising questions about its generalizability to other scenarios. Moreover, adaptingneural ODEs, potentially through online backpropagation, is a challenging problem since they areoptimized from initial values . Online learning for multi-agent systems.Online learning for relational inference in multi-agentsystems is a rarely explored research problem despite its significance. One primary related work also treats the adjacency matrix as a trainable parameter without using an encoder, optimizing it withan online expert mixture algorithm, a type of online convex optimization algorithm. Consequently,the loss function needs to be convex over the adjacency matrix to guarantee optimization. Theirmodel architecture is specifically designed to place message passing in the last layer of the modelto ensure the model output is a linear combination of hidden states and the adjacency matrix,maintaining the convex property. However, this simple architecture significantly differs from recentarchitectures (e.g., NRI and its variants), making it challenging to model nonlinear and complexinteractions. Additionally, this work only report trajectory prediction errors without providing anyaccuracy metrics on relational inference. Please note that comparison with ORI is provided in thesupplementary material.",
  "Background": "Neural relational inference.Relational inference aims to identify an unknown interaction graphrepresented by an adjacency matrix I RNNm, where N is the number of agents and m is thenumber of interaction types, from their trajectory in a given time period xt:t+t, which generallyinclude positions and velocities of agents. Existing approaches are mostly designed with a neuralnetwork-based encoder and decoder pair, where encoder (g) predicts the adjacency matrix It froman observed trajectory (i.e., g(xtt:t) and then decoder predicts the future trajectory (xt:t+t) byf(xtt:t, It). That is, the encoder is optimized to return the adjacency matrix for a lower error onthe predicted trajectory (decoder output), assuming I = arg minIt L(xt:t, f(xtt:t, It). Online learning.Online learning problems have traditionally been formulated within the onlineconvex optimization (OCO) framework, as introduced in . Conventional gradient descent-basedonline optimization projects the updated variables onto a convex set at each step . Such learningsettings provide theoretical convergence guarantees when the loss function is convex with respectto the optimization variables. However, applying online gradient descent to deep neural networks(DNNs) is challenging due to the nonconvex nature of the loss function, and standard backpropagationperforms poorly in an online setting . Two primary directions of study have emerged to tackle thechallenges of online backpropagation. One approach involves employing a flexible network structure,where the DNN architecture evolves over time . The other approach focuses on using anadaptive learning rate, where the learning rate is adjusted over time . Since our primary focus",
  "Motivation of ORI": "Our objective is to discover hidden relations between agents in evolving multi-agent interactingsystems using their streaming trajectories. The motivation of this work is from the primary challengesto apply existing methods to implement a fast-adapting, accurate, and stable online relational inferenceframework. Accordingly, we summarize our key motivations into three categories. Why we consider adjacency matrix as trainable parameter?It may not be effective to simplyapply the encoder and decoder-based existing methods to evolving multi-agent systems in the onlinesetting. The primary challenge is that, the intricate encoder is slowly trained with streaming andevolving data. It eventually degrades the decoder as well since the encoder and decoder are jointlytrained, influencing each other. Whereas, ORI allocates the adjacency matrix as a trainable parameter,significantly enhancing the training speed in both the matrix and decoder. While such allocation ismotivated from , this work still faces following crucial challenges. Why we need model-agnostic learning?The relation inference is performed through the trajectoryprediction without an explicit supervision on graph structures (i.e., true relation is not observable).This means the only supervision is defined by the predicted trajectories from the decoder, and hencelargely depending on how effectively the decoder responds to changes in the embedding adjacencymatrix. If a learning method is particular constrained to a specific decoder design, like , theperformance achievable by the method can be constrained by the decoder design. Ideally, the learningmethod should offer the flexibility to seamlessly integrate to various decoder designs. Why we need adaptive learning rate?The choice of a learning rate is particularly important in theevolving systems since the loss landscape can significantly vary with the evolution in the systems. Forexample, a low learning rate may be suitable for a slowly evolving dynamics A, but may a high oneis needed for stable operation in a fast evolving dynamics B. A constant (or decaying) learning rateleads to a slow convergence or/and potentially a sub-optimal performance as the dynamics evolvesover time. Ideally, the learning rate needs to be automatically tuned over time, avoiding a trade-offbetween faster convergence and unstable learning.",
  "Training Procedures of ORI": "First, the key difference in our training setup compared to the offline learning setup is both batch andepoch are 1 (i.e., streaming data). There is no separate validation or test dataset in online learning.Training of ORI is performed by online backpropagation on each individual training sample everyiteration. describes the propose approach. Note that the input and output to the decoder inthe figure (denoted as GNN) and its optimization are essentially the same as existing methods. Our",
  "dI(t)(1)": "where h(DI(t), (t)) R1 is the relation-aware adaptive learning rate, elaborated in the followingparagraph. Initially, the adjacency matrix (I(t) RNNm; Ii,j(t) ) is filled with 0.5. Eachtraining sample given to the model involves a single trajectory for a time period of t t : t + t(i.e., xtt:t+t). The decoder (f) observes xtt:t and then predicts the future trajectory byxt:t+t = f(xtt:t, I(t)) in an autoregressive manner. It also reconstructs the trajectory xtt+1:tduring the observation. ORI estimates a MSE loss on the predicted and reconstructed trajectory att + t (i.e., Lmse(xtt+1:t+t)) and then updates the decoder and adjacency matrix using gradientdescent. Note that, the adjacency matrix is updated only once at each iteration. The model infersthe relation in the given trajectory at the last time step. While the training objective is to minimizethe overall MSE loss throughout the streaming data, our primary evaluation criterion is the relationaccuracy representing the proportion of true positive and true negative in the adjacency matrix. Adaptive relation-aware learning rate.We propose AdaRelation, a learning rate adaptationtechnique specifically designed for relational inference. AdaRelation adjusts the learning rate forthe adjacency matrix (not the decoder) based on changes in the norm of the adjacency matrix. Forexample, if the norm exceeds a certain threshold, it gradually decrease the learning rate within adefined range. The mechanism is intuitive: a good trajectory predictor should show large output variance dependingon the adjacency matrix, meaning the quality of a predicted trajectory should vary clearly withchanges in the adjacency matrix. We observe that the norm of gradient (|| dLmse dI(t) ||1) indeed increasesas the model adapted to the evolved system, often making the adjacency matrix unstable (see (1)).Conversely, it decreases when the system suddenly evolves, slowing down the update of the adjacencymatrix. Therefore, the gradient norm indicates when the update of the adjacency matrix needs to bestabilized or accelerated. Given the adjacency matrix is the sum of this gradient over time, comparingthe current matrix with a past one essentially reflects changes in the gradient norm. Accordingly, wedefine DI(t) to estimate the evolution in the L1 norm of the adjacency matrix over w time steps asfollows:DI(t) =1",
  "N 2m||I(t) I(t w)||1(2)": "This measures how much the predicted interaction strength (Ii,j,k) changes on average over w timesteps. We define a threshold parameter to constrain changes in Ii,j,k to be remain near this threshold.The update of (t) involves adding or subtracting , an adaptation step size, determined by the rangeof DI(t) and the threshold:",
  "h(DI(t), (t)) = clipmin;max((t) + (t)))(4)": "where clipmin;max limits the learning rate within the lower bound (min) and upper bound (max).This effectively controls the learning rate to automatically stabilize and accelerate the update of theadjacency matrix. More intuition behind AdaRelation is discussed in the supplementary material. Trajectory Mirror.The models trained by the online backpropagation are often prone to be biasedto certain training samples. It also happens in multi-agent interacting systems. For example, thecoordinates and velocities of agents in the currently observed data samples can be biased. Suchscenario has not been discussed much in literature because existing works expose the model to thehuge amount of simulations with short-term trajectories, ensuring several different initial positionsand velocities. However, our problem addresses streaming trajectories in a much longer-term, wherewe do not have an access to initialize their positions and velocities. Ideally, the relation betweenagents should be correctly inferred regardless where the model observes them. Accordingly, weconsider a data augmentation technique named Trajectory Mirror, which flips the axis and generate, for example in a two-dimensional space, three additional trajectories (see ). It is simpleyet effective to avoid the model bias and significantly enhance the convergence speed of the modelwithout storing the past trajectories. Ablation studies are available in the supplementary material. Technical novelty.ORI is a novel model-agnostic online relational inference framework, incor-porating a strategic combination of two different learning methods for the adjacency matrix anddecoder. ORI introduces AdaRelation, an adaptive learning rate technique designed for relationalinference, and Trajectory Mirror, a simply yet effective data augmentation technique proved in severalevolving scenarios (results are in supplementary). Our approach clearly differs from existing methodswhich perform end-to-end gradient descent on the encoder or online convex optimizationconstrained to a specific decoder design .",
  "Experimental Results": "Dataset.We experimented with three common benchmarks, synthetic springs and charged systems,and CMU MoCap (human motion) . The synthetic datasets were generated using the open-sourcecode from NRI . For the springs and charged systems, 10 simulations with 90k time steps eachwere created, involving 10 agents with different random interactions (p = 0.5). These simulations aresequentially presented to models. The evolving relation dataset features different interaction graphsin each simulation with fixed spring k = 0.1 and ke = 1.0. Another dataset varies these constants,generated from uniform distributions [0.1, 0.2] for springs and for charged constants. Theevolving relation and dynamics dataset randomly selects each simulation from either the springs orcharged systems. The CMU MoCap dataset, processed using dNRIs open-source code , includeshuman motion data with 31 joints and various actions, captured at 120Hz. Baseline and Implementation Detail.ORI is a model-agnostic online learning framework in-volving the trainable adjacency matrix. The adjacency matrix is initialized only at the initializationstage. The same adjacency matrix is used throughout the entire samples and simulations withoutassuming that we know when the interaction evolves. ORI can integrate with various models aslong as they use the adjacency matrix as input. Most prior works use decoders based on graphmulti-layer perceptron (MLP) and graph recurrent neural networks (RNN), as seen in NRI ,incorporating node-to-edge and edge-to-node message passing. Additionally MPM introduces agraph RNN and an attention-based graph RNN for more efficient edge-to-edge message passing. Todemonstrate ORIs effectiveness, we use four different trajectory predictors: NRIm (MLP-based),NRIr (RNN-based), MPMr (RNN-based), and MPMa (attention-based). Since ORI employs thedecoders in NRI and MPM, we primarily compare our approach with the original NRI and MPM. Wealso include dNRI , which is specifically designed for dynamically evolving interaction graphs.As these works analyzed the performance in the offline setup, we evaluated their models in the onlinesetup. We follow their default implementation but replace the encoder with the trainable adjacencymatrix. More details on the training setup is available in the supplementary material.",
  "Inferring Relation in Evolving Interaction Graph": "We explore relation inference accuracy in the synthetic systems incorporating evolving interactiongraphs. These systems involve constant parameters and no switching in the dynamics. demonstrates the relation accuracy over time and predicted trajectories in the springs system. Therelation accuracy is evaluated based on the number of true positives and true negatives excludingself-interaction (i.e., total 109 relations). Our approach is able to quickly recover the relationaccuracy when the model encounters a new interaction graph. For example, in the bottom of (a), the target interaction matrix suddenly changes at the 15k-th iteration. While it fails to adapt in asingle iteration, the target and predicted matrices are aligned well after 100 iterations. In contrast,the baseline (MPM)s accuracy slowly increases throughout the entire training iterations, showinga significant gap in the average relation accuracy. (b) compares the target and predictedtrajectories at the 15k-th iteration (i.e., right after the new interaction) and 18k-1-th iteration (i.e., rightbefore another interaction), indicating the trajectory quality also improves along with the accuracy. showcases the average accuracy and average MSE loss during entire iterations in the springsand charged systems. As the interaction graph changes over time, simply reporting the final accuracyonly represents how well the model adapts to the last graph. Accordingly, we report the average",
  "Interaction graph evolves every 3k iters": ": Prediction results of ORI with MPMr decoder and the baseline MPM in the springs system.(a) the relation accuracy in the two models throughout the training (top) and visualization of the targetand predicted adjacency matrix in our model (bottom). (b) target and predicted trajectories in ourmodel. : Comparison with offline learning models in springs and charged systems with evolvinginteractions. Acc and mse stand for the relation accuracy and mse on the predicted trajectory averagedover the entire training iterations. The number following mse (e.g., mse 10) denotes the mse at the10-th prediction time step.",
  "Acc (%)mse 1mse 10mse 20mse 30Acc (%)mse 1mse 10mse 20mse 30": "dNRI 51.14.34e-58.07e-42.40e-35.34e-350.32.02e-35.00e-39.44e-35.40e-2NRI 57.41.70e-41.36e-34.49e-31.11e-252.03.80e-31.13e-22.40e-24.63e-2MPM 61.62.76e-41.10e-34.10e-39.15e-351.76.16e-31.21e-22.60e-24.86e-2 Ours w/ NRIm74.51.45e-48.19e-42.71e-37.32e-388.66.60e-31.61e-23.79e-27.33e-2Ours w/ NRIr94.21.24e-46.02e-41.71e-33.44e-390.94.87e-31.47e-23.54e-26.85e-2Ours w/ MPMr95.22.80e-44.54e-41.43e-33.16e-395.36.98e-31.35e-23.06e-25.91e-2Ours w/ MPMa96.42.59e-43.74e-41.17e-32.62e-387.16.85e-31.42e-23.25e-26.26e-2 accuracy over entire iterations to understand the models accuracy on the multiple graphs and howfast it adapts to the change in the graphs. ORI with four different decoders consistently outperformthe existing encoder and decoder-based methods with respect to the accuracy. Note that TM isapplied to both existing methods and ours for the fair comparison. It is crucial to emphasizethat, in the charged system, our results with almost 40% higher accuracy does not exhibit lowertrajectory errors. For example, NRI reaching at only 52.0% accuracy still shows the lower MSE of3.80103 4.63102 over all the prediction steps than our results. This implies the comparisonsolely using the MSE loss may not indicate the quality of inferred relations at all. The lower MSE inexisting methods is probably achieved by the larger capacity than our methods due to the encoder,overfitting to the trajectory modeling, not the relation inference. However, in spring systems, themethods with higher accuracy exhibits lower trajectory errors.",
  "Inferring Relation in Evolving Interaction Graph and Dynamics": "ORI is evaluated on the two additional evolving scenarios in the springs (spr) and charged (cha)systems, where 1) interaction graph and parameter evolve and 2) interaction graph and the dynamicsitself evolve. In addition, we analyse the relation learning rate to understand how the relation accuracyresponds depending on the learning rate. The yellow, red, and blue curves correspond to ORI withAdaRelation, ORI with constant learning rate (lower bound), and ORI with constant learning rate(upper bound). showcases the relation accuracy and relation learning rate over 30k trainingiterations in both the evolution scenarios. Although the models tend to slowly converge comparedto the springs systems, they can still adapt to the systems with evolving parameters or switchingdynamics, reaching to the 1.0 accuracy given enough training iterations.",
  "(a)(b)": ": Prediction results of ORI with NRIr decoder in the charged system with evolving interactionand parameters (a) and ORI with MPMr decoder in the springs and charged systems with evolvinginteraction and dynamics (b). 1-st row compares the relation accuracy between constant learningrates and AdaRelation. 2-nd row shows changes in the relation learning rate throughout the training.",
  "(b)(c)(a)": ": Comparison between ORI and existing methods with respect to the relation accuracy (a),variance in the adjacency matrix (b), and variance in the predicted trajectory (c) depending on encodercomplexity. The number in the MPM () represents the dimension of hidden states in the encoder. decreases over time, and then increases again at the 15k-th iteration. This means that AdaRelationnotices a change in the interaction graph of the systems in an unsupervised manner and hence increasesthe learning rate for a while. This ensures not only the fast adaptation to a new environment but alsothe stability (i.e., less fluctuation in the accuracy) after the relation accuracy converges. Moreover, (right bottom) demonstrates that AdaRelation controls the learning rate depending on thedynamics as well. The models with a high learning rate are stable enough in the springs systems.However, the fluctuation in the accuracy emerges in the charged systems, particularly when theaccuracy almost reaches at 1. AdaRelation effectively suppresses such fluctuation without sacrificingthe convergence speed. For example, between the 18k-th and 21k-th iterations, the accuracy ofAdaRelation converges as fast as the high learning rates one while having much less fluctuation in thelater iterations. Thus, AdaRelation effectively enhances the convergence speed, stability, and overallaccuracy in evolving multi-agent interacting systems. The related accuracy and ablation studies areavailable in the supplementary material.",
  "We discuss how existing methods fail to clarify the benefit of ORI over them. Since they share thesame trajectory predictors, the primary reason should be studied with respect to the encoder side": "Lightweight encoders in existing works.One of the key features in ORI is the encoder-less design,having much fewer trainable parameters, excluding the trajectory predictor, than existing works. Toclarify whether the performance gain is from the less trainable parameters, we demonstrate howexisting methods perform when their encoder is significantly lightened while having the same decoder.(a) showcases the relation accuracy depending on the encoder complexity in the springsystem with evolving interaction. However, the performance gain from their lightweight encodersare not significant, indeed still much lower than ORI. Accordingly, we explore the following twoadditional experiments.",
  "TargetPrediction": ": Prediction results of ORI with MPMr decoder and MPM in CMU MoCap dataset. 1-strow represents the last frame in the predicted and target trajectory from ORI. 2-nd and 3-rd rowsvisualize the top-30 stongest interaction edges in the corresponding frame from ORI and MPM. Notethat MPM allocate higher relation strengths in the front foot while ORI focuses on the foot behind. Changes in inferred relations.(b) describes changes in the adjacency matrix (DI(t))throughout the training in the models with different encoder complexity. Note that DI(t) estimateshow much the current adjacency matrix differs from the past one (i.e., ||I(t) I(t w)||1). First,the range of DI(t) in ours is consistent throughout the entire iterations, effectively updating theadjacency matrix from the early stage of training. In contrast, MPMs even with the smaller encoder,such as MPM (64) and MPM (32), exhibit sudden increases in DI(t) after few thousands iterations.This means that the predicted adjacency matrix is not responding much to the observed trajectory,implying their encoders completely fail to discover the relationship between the observed trajectoryand the adjacency matrix. The larger models, MPM (256) and MPM (128), show worse results suchas slow increases in DI(t) and almost no changes in the first 10k training iterations. That is, thelightweight encoder still shows the totally different behavior to ORI and does not effectively updatethe adjacency matrix in the early stage. Output variance given true and wrong relations.Following the above paragraph, we study howthe slow optimization of the encoder influences the trajectory predictor. (c) displays thegap in MSE losses between the output with true interaction graph and one with completely wronginteraction graph over the training iterations. This essentially represents how sensitive the trajectorypredictor is to the input interaction graph. ORI demonstrates a clearly higher gap in the MSE lossesthan all the other MPM models. Apart from ORI, the smaller encoders increase the MSE gap inthe trajectory predictor. That is, the failure in the encoder degrades the trajectory predictor, whichpotentially influence the encoder again. In summary, allocating the trainable adjacency matrix in ORIensures the stable update in the embedding adjacency matrix to the trajectory predictor and enhanceits output variance depending on the interaction graph.",
  "Real-world Application": "ORI is assessed in the real-world human motion dataset (CMU MoCap) against existing offlinemethods. showcases the target and predicted trajectories on walking motion from ORI (1-strow) and the top-30 strongest relations between joints in a skeleton model for the corresponding frame(2-nd row). Additionally, the figure incorporates the inferred relations from MPM (3-rd row). Thevisual comparison illustrates that ORIs predicted joint trajectories closely align with the target, yetORI exhibits higher MSE loss compared to MPM (see the supplementary material). However, similarwith the observation in the charged systems (), ORI appears to offer more interpretable relationinference on the joints. For example, in 3-rd row of the figure, MPM simplifies the relational inferenceby cyclically focusing on right foot, left foot, and right foot again. In contrast, ORI introduces anadditional layer of shifts in the relation, emphasizing primary connections between left foot, rightknee, and right foot. This additional detail enhances the interpretability of the walking motion.",
  "Conclusion": "Summary.We introduced Online Relational Inference (ORI), a novel framework for online re-lational inference in evolving multi-agent interacting systems. ORI employs an adaptive learningrate technique, AdaRelation, allowing it to adapt dynamically to changing environments throughonline backpropagation. Our approach also includes the Trajectory Mirror (TM) data augmentationmethod to enhance generalization. This model-agnostic framework seamlessly integrates with variousneural relational inference architectures, providing a robust solution for real-time applications in com-plex, evolving systems. Future work will focus on enhancing the adaptability in more fast-evolvinginteraction and exploring its applicability to a wider range of multi-agent systems. Limitation and future work.Our current experiments do not evaluate ORI in non-ideal environ-ments, incorporating directed interaction graphs or/and variable number of nodes. This limits thepotential of ORI in relatively ideal environments. We expect that ORI can be extended to scenarioswhen agents are added or deleted, provided we are aware of which agent is added and deleted byadding and deleting corresponding row and column in the adjacency matrix. While the experimentsin the paper do not explicitly include directed interactions, ORI does not assume any symmetry(undirected graph) in the adjacency matrix. In other words, there is no technical limitation to applyORI in directed interaction.",
  "Acknowledgements": "This work is partly supported by the Office of Naval Research under Grant Number N00014-20-1-2432 and National Science Foundation under Grant Number 2328962. The views and conclusionscontained in this document are those of the authors and should not be interpreted as representingthe official policies, either expressed or implied, of the Office of Naval Research, National ScienceFoundation, or the U.S. Government. Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, andPeter Battaglia. Learning to simulate complex physics with graph networks. In Internationalconference on machine learning, pages 84598468. PMLR, 2020.",
  "Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learningmesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020": "Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer,Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, et al. Threedworld: Aplatform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954,2020. Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, and Noseong Park. Graph neural con-trolled differential equations for traffic forecasting. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 36, pages 63676374, 2022. Maosen Li, Siheng Chen, Yanning Shen, Genjia Liu, Ivor W Tsang, and Ya Zhang. Online multi-agent forecasting with interpretable collaborative graph neural networks. IEEE Transactions onNeural Networks and Learning Systems, 2022. Zijie Huang, Yizhou Sun, and Wei Wang. Coupled graph ode for learning interacting systemdynamics. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &data mining, pages 705715, 2021.",
  "Chengxi Zang and Fei Wang. Neural dynamics on complex networks. In Proceedings of the26th ACM SIGKDD international conference on knowledge discovery & data mining, pages892902, 2020": "Zijie Huang, Jeehyun Hwang, Junkai Zhang, Jinwoo Baik, Weitong Zhang, Dominik Wodarz,Yizhou Sun, Quanquan Gu, and Wei Wang. Causal graph ode: Continuous treatment effectmodeling in multi-agent dynamical systems. arXiv preprint arXiv:2403.00178, 2024. Beomseok Kang, Harshit Kumar, Minah Lee, Biswadeep Chakraborty, and Saibal Mukhopad-hyay. Learning locally interacting discrete dynamical systems: Towards data-efficient andscalable prediction. arXiv preprint arXiv:2404.06460, 2024. Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, PeterBattaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning.Advances in neural information processing systems, 30, 2017. Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neuralrelational inference for interacting systems. In International conference on machine learning,pages 26882697. PMLR, 2018.",
  "Enna Sachdeva and Chiho Choi.Dider: Discovering interpretable dynamically evolvingrelations. IEEE Robotics and Automation Letters, 7(4):1182311830, 2022": "Jiachen Li, Fan Yang, Masayoshi Tomizuka, and Chiho Choi. Evolvegraph: Multi-agenttrajectory prediction with dynamic relational reasoning.Advances in neural informationprocessing systems, 33:1978319794, 2020. Sindy Lwe, David Madras, Richard Zemel, and Max Welling. Amortized causal discovery:Learning to infer causal graphs from time-series data. In Conference on Causal Learning andReasoning, pages 509525. PMLR, 2022. Longyuan Li, Jian Yao, Li Wenliang, Tong He, Tianjun Xiao, Junchi Yan, David Wipf, andZheng Zhang. Grin: Generative relation and intention network for multi-agent trajectoryprediction. Advances in Neural Information Processing Systems, 34:2710727118, 2021. Xiao Luo, Haixin Wang, Zijie Huang, Huiyu Jiang, Abhijeet Gangan, Song Jiang, and YizhouSun. Care: Modeling interacting dynamics under temporal environmental variation. Advancesin Neural Information Processing Systems, 36, 2024. Suresh Bishnoi, Jayadeva Jayadeva, Sayan Ranu, and NM Anoop Krishnan.Brognet:Momentum-conserving graph neural stochastic differential equation for learning browniandynamics. In The Twelfth International Conference on Learning Representations, 2023. Zijie Huang, Yizhou Sun, and Wei Wang. Generalizing graph ode for learning complex systemdynamics across environments. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, pages 798809, 2023. Doyen Sahoo, Quang Pham, Jing Lu, and Steven CH Hoi. Online deep learning: learningdeep neural networks on the fly. In Proceedings of the 27th International Joint Conference onArtificial Intelligence, pages 26602666, 2018.",
  "CMU. Carnegie-mellon motion capture database, 2003. URL": "Siyuan Chen, Jiahai Wang, and Guoqing Li. Neural relational inference with efficient mes-sage passing mechanisms. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 35, pages 70557063, 2021. Dong Gong, Frederic Z Zhang, Javen Qinfeng Shi, and Anton Van Den Hengel. Memory-augmented dynamic neural relational inference. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1184311852, 2021. Armand Comas, Yilun Du, Christian Fernandez Lopez, Sandesh Ghimire, Mario Sznaier,Joshua B Tenenbaum, and Octavia Camps. Inferring relational potentials in interacting systems.In International Conference on Machine Learning, pages 63646383. PMLR, 2023. Xiao Luo, Jingyang Yuan, Zijie Huang, Huiyu Jiang, Yifang Qin, Wei Ju, Ming Zhang, andYizhou Sun. Hope: High-order graph ode for modeling interacting dynamics. In InternationalConference on Machine Learning, pages 2312423139. PMLR, 2023.",
  "Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends inOptimization, 2(3-4):157325, 2016": "Andri Ashfahani and Mahardhika Pratama. Autonomous deep learning: Continual learningapproach for dynamic environments. In Proceedings of the 2019 SIAM international conferenceon data mining, pages 666674. SIAM, 2019. Stefan Duffner and Christophe Garcia. An online backpropagation algorithm with validationerror-based adaptive learning rate. In International Conference on Artificial Neural Networks,pages 249258. Springer, 2007. Rui Zhang, Zong-Ben Xu, Guang-Bin Huang, and Dianhui Wang. Global convergence of onlinebp training with dynamic learning rate. IEEE Transactions on neural networks and learningsystems, 23(2):330341, 2012.",
  "ATraining Setup": "Training setup.For the synthetic datasets, the model observed the first 30 time steps (xt30:t) andthen predict the next 30 time steps (xt:t+30). The next prediction window is defined on xt+30:t+90.Similarly, for the CMU MoCap data, the model observed the first 10 time steps and then predict thenext 10 time steps. We trained the models with a single GTX 2080Ti GPU. Implementation details.We directly use the implemented decoders from NRI and MPM for ORI with NRI and ORI with MPM. The hidden size on the decoders are set to 256 as default. Thelearning rate for the decoders is 1e-4. The initial learning rates ((0)) for the adjacency matrix is 100 for ORI with NRI decoders and 20for ORI with MPM decoders. The lower and upper bound for learning rates in AdaRelation aremin = 100 max = 200 for ORI with NRI decoders, min = 20 max = 50 for ORI with MPMdecoders. The threshold and adaptation step size in AdaRelation are set to 0.05 and 1. For theCMU MoCap dataset, we observe that the gradient on the adjacency matrix is relative small; in orderto ensure more dynamic updates in the adjacent matrix, we largely increase the (0) to 100k.",
  "BAdditional Discussion": "Computational complexity.In terms of trainable parameters, NRI has 721.4k for encoder and727.3k for decoder; MPM has 1724.9k for encoder and 1071.7k for decoder; dNRI has 883.7k forencoder and 269.8k for decoder. In terms of FLOPs per iteration, NRI shows 177.8MFLOPs forencoder and 5040.5MFLOPs for decoder; MPM shows 3.9GFLOPs for encoder and 10.9GFLOPs fordecoder. It indicates that the decoder consumes more computation than the encoder even though theyare with the similar level of trainable parameters.",
  "Intuition behind the norm of gradient || dLmse": "dI(t) ||1.The norm of gradient indicates that how thetrajectory error (Lmse) changes by the adjacency matrix (I(t)). Ideally, we expect this normbeing high enough so that the model learns the strong correlation between the trajectory of agentsand their relation. In other words, the low norm of gradient means that, the model returns the similartrajectory regardless of the relation (i.e., adjacency matrix), which is undesirable. Intuition behind the deviation DI(t).The deviation is a function of ||I(t)I(tw)||1, where bothI(t) and I(t w) are the learned adjacency matrix, not the actual one. Hence, the significant changein the actual adjacency matrix used for generating the observed time series, may not necessarily leadto large values of the deviation. This depends on the how quickly ORI learns the new adjacencymatrix as discussed below. Consider a scenario when the actual adjacency matrix significantly changes between the time stept w and t. Assume ORI has learned the actual adjacency matrix at time step t w. Now, at timestep t, ORI can respond in two possible ways. First, ORI may quickly identify the new adjacency matrix at the time step t. In this case, I(t) andI(t w) will be related to the new and previous actual adjacency matrices, respectively. Assumingthe two actual adjacency matrices are significantly different, the deviation between two learnedadjacency matrices will also be large. Hence, based on equation (3), the learning rate will decrease.This will make the learned adjacency matrix stable, thereby helping ORI to stay at the new learnedadjacency matrix at time t, which is desirable as that is also the actual adjacency matrix. Let us now consider the second case where ORI does not quickly learn the new actual adjacencymatrix and hence, the learned adjacency matrix at time t stays close to the one learned at time stept w. In other words ||I(t) I(t w)||1 remains low even if the actual adjacency matrices havechanges. In this case, following equation (2), the learning rate increases to rapidly update the learnedadjacency matrix, which is desirable to quickly move the learned matrix to the actual one.",
  "CAdditional Experimental Results": "Correlation between dissimilarity in graph and relational accuracy.Even though we randomlyevolved the interaction graphs, there exists a slight similarity between the graphs. A dissimilarity isdefined on two interaction graphs to understand how it influences relational accuracy. We first sum theelement-wise difference between two interaction graphs and then divide by the number of elements.This dissimilarity is compared with the number of iterations required to reach 90% accuracy since theinteraction graph evolves. For example, in ORI with MPMr decoder in the springs system (in the main paper), in the early stage of training, we do not observe much correlation between thedissimilarity as the model is not matured yet. However, after approximately 18k iterations, moredissimilar interaction graphs require more training iterations to reach 90% accuracy (see ).Note that, higher dissimilar interaction graphs indicate the higher accuracy drop when the interactiongraph evolves. This leads to the lower initial accuracy and hence requires less iterations. (see thesecond column in (a)). However, we consider the variation of the require training iterationsis still marginal (e.g., 45-55 iterations in (b)).",
  "Case 5: interaction graph changes after 3k, 1k, 1k, 2k, 3k, 2k, 2k, 1k, 3k, 2k iterations": "From Case 1 to Case 5, the accuracy is 93.9%, 92.2%, 93.8%, 93.1%, and 92.5%. Overall, thevariation in the accuracy is marginal, and hence, the performance of ORI is not significantly influencedby the irregular evolutions. However, since we only considered 1k, 2k, and 3k iterations, more extremescenarios, such as few-thousands iterations would be an interesting future study. MSE in CMU MoCap. summarizes the MSE loss on the predicted trajectory in the CMUMoCap dataset. Overall, the difference in the MSE losses are not significant. However, the originalMPM method works better than ORI with NRI and ORI with MPM.",
  "Ours w/ NRIm0.5651.6550.6720.5780.5630.5650.6310.8631.0481.151Ours w/ MPMr1.0281.6810.3590.2530.2281.0280.6410.4610.4590.466Ours w/ MPMa1.6531.6100.2790.1810.1651.6530.6140.3590.3290.337": "Comparison with prior online work.The primary related work for online learning in multi-agentinteracting systems is . They also considered springs systems with 20 agents, but their code,implementation details, and relation accuracy are not available, making direct comparison difficult.Instead, we provide an indirect comparison using similar data (springs system with 20 agents andevolving interaction). This work compared their method with NRI and dNRI, so we normalizetheir results with their NRI and dNRI, and ours with our NRI and dNRI, and then compare theseresults as relative MSE (). We observe that in the very early prediction steps (e.g., steps 1 and2), their method performs better, but ours achieves a clearly lower relative MSE loss in the later timesteps. Ablation study.We perform ablation studies on AdaRelation and Trajectory Mirror in the threedifferent evolving systems. For AdaRelation, the models with constant learning rates, lower or upperbound learning rate in AdaRelation, are compared. , , and summarize theresults. We bold the top accuracy (or others with less than 0.5% difference). Overall, the largeaccuracy gain is resulted from Trajectory Mirror, and AdaRelation further enhances the accuracy."
}