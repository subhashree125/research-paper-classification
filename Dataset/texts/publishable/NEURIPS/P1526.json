{
  "Abstract": "Multi-modal contrastive learning with language supervision has presented aparadigm shift in modern machine learning. By pre-training on a web-scale dataset,multi-modal contrastive learning can learn high-quality representations that exhibitimpressive robustness and transferability. Despite its empirical success, the theoret-ical understanding is still in its infancy, especially regarding its comparison withsingle-modal contrastive learning. In this work, we introduce a feature learningtheory framework that provides a theoretical foundation for understanding thedifferences between multi-modal and single-modal contrastive learning. Basedon a data generation model consisting of signal and noise, our analysis is per-formed on a ReLU network trained with the InfoMax objective function. Througha trajectory-based optimization analysis and generalization characterization ondownstream tasks, we identify the critical factor, which is the signal-to-noise ratio(SNR), that impacts the generalizability in downstream tasks of both multi-modaland single-modal contrastive learning. Through the cooperation between the twomodalities, multi-modal learning can achieve better feature learning, leading toimprovements in performance in downstream tasks compared to single-modallearning. Our analysis provides a unified framework that can characterize theoptimization and generalization of both single-modal and multi-modal contrastivelearning. Empirical experiments on both synthetic and real-world datasets furtherconsolidate our theoretical findings.",
  "Introduction": "Large-scale pre-trained models have achieved unprecedented success, including GPT series ,LLaMa , among many others. CLIP as a typical example, uses a multi-modal contrastivelearning framework to learn from a massive scale of image-caption data. The multi-modal contrastivelearning in CLIP has shown significant capabilities to learn high-quality representations, which areready to be adapted to a wide range of downstream tasks, forming the backbone of generative models",
  "arXiv:2411.02837v1 [cs.LG] 5 Nov 2024": "like DALL-E2 , prompt learning as well as general purpose multi-modal agents .Given the huge success of models like CLIP that have stellar zero-shot and few-shot capabilitieson a wide range of out-of-distribution (OOD) benchmarks, they have been widely recognized asfoundation models (FMs). More similar examples are given by ALIGN , Florence , BLIP, Flamingo . Despite the unprecedented success achieved by multi-modal contrastive learning, the fundamentalmechanism that leads to greater performance, especially compared to single-modal contrastivelearning is still under-explored. Recently, several seminal works provided theoretical explanations foreither single-modal or multi-modal contrastive learning .For example, studied how single-modal contrastive learning learns the feature representations forneural networks by analyzing its feature learning process. As for multi-modal contrastive learning, provided explanations for why multi-modal contrastive learning demonstrates zero-shottransferability, and robustness to distribution shifts, than supervised learning, which offer valuableinsights. Although both lines of the existing works provide valid theoretical insights under therespective settings, rare work has compared the optimization and generalization of the two types ofcontrastive learning under a unified framework. This motivates us to establish a systematic featurelearning analysis for both single-modal and multi-modal contrastive learning. In particular, we consider a data generation model that contains two modalities of data, which aregenerated from signal and noise features. The signal feature correlates in different modalities, whilethere is no correlation between noise features among modalities. We then study the optimizationof single-modal and multi-modal contrastive learning under gradient descent training. By studyingthe trajectories of signal learning and noise memorization, we establish the convergence conditionsand further characterize the generalization ability in the downstream tasks. The results show that,through the cooperation between modalities, multi-modal contrastive learning can achieve bettergeneralization in the downstream task. In contrast, without the help of the second modality, single-modal contrastive learning concentrates on learning noise from the data, and thus generalizes poorlyon the downstream tasks. The main contributions of this work are summarized as follows: This work establishes the first systematic comparative optimization analysis for single-modaland multi-modal contrastive learning under gradient descent training in non-convex settings. Weshow that both single-modal and multi-modal can achieve near-zero training error under InfoMaxcontrastive loss after polynomial number of iterations, by overcoming the non-convex difficulty. By a trajectory-based analysis of the signal learning and noise memorization of the ReLU networkfrom the data, we successfully characterize the difference in generalization between single-modaland multi-modal contrastive learning. The distinct SNRs of different modalities lead to a divergencein the generalization of downstream tasks for the two contrastive learning frameworks. Our theory suggests that the advantage of multi-modal over single-modal contrastive learningcomes from the high quality of the second modality and the cooperation between the two modalitiesthrough contrastive learning. This divergence is ultimately reflected in the difference in featurelearning and the final gap in downstream task generalization. Experimental results on both syntheticand real-world datasets confirm our theoretical findings and understanding.",
  "Related Work": "Theoretical Understanding of Single-modal Contrastive Learning. The seminal work startedtheoretical research on single-modal contrastive learning. They assumed that different positivesamples are independently drawn from the same latent class, making a connection to supervisedlearning. identified two key properties related to the contrastive loss: alignment and uniformity.Alongside, illustrated that predicting auxiliary prediction tasks helps in learning representationseffective for downstream prediction tasks, and provided a theoretical analysis of contrastivelearning in the multi-view setting. Besides, proposed a theoretical framework to understandcontrastive self-supervised learning from an optimization perspective. proposed a loss thatperforms spectral decomposition on the population augmentation graph and can be succinctly writtenas a contrastive learning objective on neural net representations. pointed out the importance ofinductive biases of the function class and training algorithm in understanding contrastive learning.The most related work to us is the work by . Similar to them, this work studies ReLU networksand considers the signal-noise data model. However, we do not require the adjustable bias term in",
  "the activation function, which plays a critical role in . Furthermore, this work adopts a unifiedframework to compare with multi-modal contrastive learning, which is out of scope in": "Understanding of Multi-modal Contrastive Learning. As the multi-modal contrastive learningapproaches such as CLIP received great success, recent works have been proposing explanationsfrom empirical perspective. empirically showed that high train-test similarity is insufficient toexplain CLIPs OOD performance. illustrated that CLIP behaves similarly to Bags-of-words inlanguage-based image retrieval, i.e., the order of words in the input sentence does not largely affectCLIP to find the corresponding image. Besides, demonstrated that training data diversity and theability to leverage the diversity as supervised learning is the key to the effective robustness of CLIP.Theoretically, proved that multi-modal contrastive learning can block-identify latent factorsshared between modalities by the a generative data model. analyzed the training dynamics ofa simple multi-modal contrastive learning model and show that contrastive pairs are important forthe model to efficiently balance the learned representations. Furthermore, showed that eachstep of loss minimization by gradient descent can be seen as performing SVD on a contrastivecross-covariance matrix. Similar to us, tried to answer why multi-modal learning is better thansingle model learning. However, they did not consider contrastive learning and thus cannot explainthe success of multi-modal contrastive multi-modal learning. Data Quality Matters for Multi-modal Contrastive Learning. Aligned with our theoreticalresults, there is a lot of empirical evidence showing that improving the alignment quality withmore descriptive captions improves multi-modal contrastive learning. show that the trainingdistribution mostly determines the generalizability of CLIP. Furthermore, find filteringpoorly aligned image-caption samples used for training leads to further improvements. Besides, demonstrate that improving the descriptiveness of the captions could further boostthe performance of CLIP. Besides, demonstrated that the caused by a combination of modelinitialization and contrastive learning optimization. However, their results do not take neural networkarchitecture into consideration, and do not provide an analysis of test errors either.",
  "Problem Setting": "Notation.We use bold-faced letters for vectors and matrices otherwise representing scalar. Weuse 2 to denote the Euclidean norm of a vector or the spectral norm of a matrix, while denoting F as the Frobenius norm of a matrix. For a neural network, we denote () as the activationfunction and we adopt ReLU activation where (x) = max{0, x} in this work. To simplify, wedenote [n] = {1, 2, . . . , n}.",
  "x = [x(1), x(2)] = [y, ],y unif({1, 1}).(1)": "where x R2d is the input feature and y {1, 1} is the corresponding label generated fromRademacher distribution. In particular, x(1) = y Rd is the task-relevant signal vector, andx(2) = N(0, 2I) Rd is the task-irrelevant noise vector. Intuitively, if a network learnsprimarily from signal, it can effectively generalize to unseen data and vice versa. Similar data modelshave been adopted in recent theoretical works on supervised learning and self-supervised learning .",
  "x = [x(1), x(2)] = [y, ],y unif({1, 1}),(2)": "where the input feature x R2 d and the label y is shared with the first modality. Besides, the signalis a given vector R d, and noise follows N(0, 2I) R d. The linear data models formulti-modal learning have also been studied in previous work . To simplify the analysis, we setd = d, = . However, we highlight that extensions to deal with unmatched dimension and noiselevel is possible.",
  "eSimh(xi,xi)/ + Mj=i eSimh(xi,xj)/ ),(6)": "where is the temperature parameter, n is the number of training samples, and M is the number ofnegative pairs. In this work, to efficiently optimize the loss to near zero, we require negative samplepairs do not share the same label, i.e., yj = yi in (6). Note that this setting is aligned with supervisedcontrastive learning .",
  "eSimh(xi,xi)/ + Mj=i eSimh(xi,xj)/ . (8)": "Intuitively, when the similarity between positive pair is high, and the similarity between negative timeis low, we can see (t)i 1 and (t)i,j 0, for i [n] and j [M]. Therefore, the gradient descentin Eq. (7) is close to zero, indicating the near convergence result. Furthermore, from Eq. (7), weobserve that the evolution direction of weight is composed of signal vector and noise vectors i fori [n]. This observation plays a critical role in our following theoretical analysis.",
  "eSimg,h(xi,xi)/ + Mj=i eSimg,h(xi,xj)/ ).(9)": "Same to the single-modal learning whose objective function is governed by Eq. (6), the objectivefunction for multi-modal contrastive learning adopt one positive pair and M negative pairs. Besides,we require the negative pairs do not share the same label. To optimize the objective function (9) formulti-modal learning, gradient descent is applied to train two encoders simultaneously. The gradientdescent rule for the first modal network is governed by the following expression.",
  "j=i(t)i,j g(t)r (x(2)j )h(t)r(x(2))i.(10)": "Here with a slight abuse of notation, we use (t)i, (t)i,j to represent the loss derivatives for bothmodalities. Compared to signal-modal learning, the main difference for the multi-modal learningis that the corresponding embedding is from another modality. The gradient update for the secondmodality can be derived similarly, which we omit here for clarity.",
  "To evaluate the out-of-distribution generalization of single-modal and multi-modal contrastive learningfor downstream task, we consider a test distribution Dtest, where a sample xtest = [y , ]": "Dtest is generated as follows. The test signal satisfies , = O(22d1/2) and the testnoise follows N(0, 2I) and y follows Rademacher distribution. After the training is complete,we introduce a linear head on top of the learned embedding h(xtest) for adapting to test distribution,i.e., f(xtest) = w, h(xtest). Specifically, we consider the task of classification and define thepopulation 0-1 test error as LDtest = PxtestDtestyf(xtest) < 0.",
  "Main Results": "In this section, we introduce our key theoretical findings that elucidate the optimization and general-ization result for both single-modal and multi-modal contrastive learning through the feature learninganalysis. We use a trajectory-based analysis for the iterations induced by gradient descent, followinga post-training analysis for the performance on the downstream test set. Below we provide the mainassumption and main theorems.",
  "(5) min{(2), /(1)}. (6) n SNR2 = (1). (7) C2 = 2, where C 2.66is a constant": "(1) We adopt a high dimensional setting to ensure enough over-parameterization. (2,3) The learningrate and the strength of initialization are chosen to make sure the that gradient descent can effectivelyminimize the contrastive loss. (4) The choice of hidden size m and number of training sample nis to provide adequate concentration. (5) The strength of augmentation is set to keep the similaritybetween two positive samples. (6) The relation between number of sample and SNR is to distinguishthe feature learning process between single-modal and multi-modal contrastive learning. (7) Todifferentiate single-modal and multi-modal contrastive learning, we introduce a constant C, whichenables the cooperation between the two modalities in multi-modal contrastive learning.Theorem 4.2 (Single-Modal Contrastive Learning). Under the single-modal learning setup, supposeAssumption 4.1 holds. Then after T = (1mn2 d1 + 1mn2 d11), the with proba-bility at least 1 1/d, it holds that (1) Training error L(T ) and (2) Test error at down-streamtask LDtest(T ) = (1). Theorem 4.2 states that despite the small training error achieved by single-modal contrastive learning,the test error is large in the downstream task.Theorem 4.3 (Multi-Modal Contrastive Learning). Under the single-modal learning setup, supposeAssumption 4.1 holds. Then after T = (1mn2 d1 + 1mn2 d11), the with proba-bility at least 1 1/d, it holds that (1) Training error L(T ) and (2) Test error at down-streamtask LDtest(T ) = o(1). Theorem 4.3 demonstrates that trained multi-modal contrastive learning can achieve both smalltraining error and downstream test error. Compared to Theorem 4.2, Theorem 4.3 shows that thegeneralization of multi-modal contrastive learning in downstream tasks is better than single-modalcontrastive learning. The reason behind this difference is that the two modalities can cooperate witheach other; the higher quality in one modality can boost the feature learning in the target modality,helping to generalize to the downstream task. On the contrary, augmentation often maintains thesame SNR as the original data, so single-modal learning hardly benefits from the augmentation andcan only memorize the noise from the data, which is not applicable to downstream tasks.",
  "Proof Sketch for Single Modal Contrastive Learning": "The proof is constructed by a optimization analysis followed by a generlization analysis in thedownstream task. Through the application of the gradient descent rule outlined in Eq. (7), we observethat the gradient descent iterate w(t)ris a linear combination of its random initialization w(0)r , thesignal vector and the noise vectors in the training data i for i [n]. Consequently, for r [m],the decomposition of weight vector iteration can be expressed:",
  "Lemma 5.1 tells how the coefficients evolve under gradient descent update. In the following, weintroduce a two-stage dynamics to characterize the whole training process based on Eq 12 and Eq 13": "First Stage: Exponential growth. During the first stage, we show before (t)ror (t)r,i grow to (1),the embedding (3) is close to zero, suggesting the similarity is bounded by 1 Simh(x, x) Cfor some constant C > 1. The loss derivatives defined in (8) can thus be bounded within someconstant range. Signal learning.According to the update for signal learning in (12), we see the prop-agation can be simplified based on the hard-negative sampling strategy, i.e., the negativepairs do not share the same labels.This suggests the negative term is always zero asni=1Mj:yj=yi (w(t)r , yj)(w(t)r , yi) = 0.The resulting update of (t)rreduces to",
  "(t+1)r= (t)r +": "nmni=1(1 (t)i)(w(t)r , yi)(w(t)r , yi)yi22. Examining the prop-agation of (t)r , we can divide the dynamics into two groups depending on the sign of weightinitialization w(0)r , . Let U(t)+ {r : w(t)r , > 0} and U(t) {r : w(t)r , < 0}. Then forr U(0)+ , we can show (t)r 0 increases exponentially and thus the sign of inner produce staysinvariant with U(t)+ = U(0)+for all t 0. On the other hand, for r U(0) , we can show (t)r 0 anddecreases exponentially with U(t) = U(0) for all t 0. Noise memorization. Compared to signal learning, the behaviour of noise memorization requires moredetailed analysis. This is mainly because the negative pairs can not be eliminated simply based onlabel difference, as the noise patch i is generated independent of label yi. In addition, the added noisei by augmentation can also contribute to the noise dynamics. We first show when the noise level is much smaller compared to , the dynamics of noise memorization is largely remains unaffected.By the sign of w(t)r , i, we partition the samples into two sets, i.e., I(t)r,+ = {i : w(t)r , i > 0},",
  "B(t)r,+,": "where the coefficient of 1/2 appears as a result of the randomness of the sign of initialization. Thisresult suggests, individual (t)r,i:yi=1 cannot grow too slow compared to the (t)r,i:yi=1. Following a similar induction argument, we are able to show (t)r,i:yi=1 has an exponential growth lower bound. Onthe other hand, for samples with yi = 1 but with different neuron, we can use the same strategy toshow an exponential growth lower bound for some neurons that satisfy the initialization conditions.",
  "nm, we have(t)r= O(1/n) for all r [m] and 0 t T1 and maxr (T1)r,i= (1) for all i [n]": "Second stage: convergence and scale difference. At the end of first stage, the noise grows to aconstant order while signal learning remains negligible. As a result, the loss derivatives are no longerbounded within some constant range. In the second stage, we aim to show the loss is able to convergeto an arbitrarily small value . Despite the unsupervised learning setup, we are still able to show lossconvergence thanks to the hard negative samples. Let F0(W, xi) = Sim(xi, xi) be the similarityto the argumentation and Fj(W, xi) = Sim(xi, xj) for j = 1, ..., M be the similarity betweenthe negative pairs. Then we can show there exists some W such that F0(W(t), xi), W 2 log(2M/) while Fj(W(t), xi), W log(2M/) for all j = 1, ..., M. Then we can boundLS(W(t)), W(t) W 1",
  "(W(t) W2F W(t+1) W2F ) + whichguarantees convergence by telescoping over the inequality. Upon the convergence, we can also show": "the scale difference obtained at the end of the first stage is maintained, i.e., (t)r= O(1/n) whilemaxr (T1)r,i= (1) for all i [n]. This suggests the non-linearly separability for the resultingembeddings and thus the downstream test error is non-vanishing. The formal convergence result isestablished in Lemma C.18. Combined with the generalization error demonstrated in Appendix C.3,this completes the proof of Theorem 4.2.",
  "First Stage: Exponential growth The first stage of multi-modal learning shares similar characteristicsas single-modal learning": "Signal learning. For signal learning, we analyze the trajectories for both (t)rand (t)r . To thisend, we partition the neurons depending on their initialization status. Apart from U(t)+ and U(t)defined in the single-modal learning, we additionally define U(t)+ {r : w(t)r , > 0}, U(t){r : w(t)r , < 0} for the other modality. Then for r U(0)+ U(0)+ , we can show (t)r 0,(t)r 0 and are increasing. For neurons r U(t) U(t) , we can show (t)r 0, (t)r 0 and aredecreasing. Furthermore, the sign of inner product stays invariant, i.e., U(t)+ U(t)+ = U(0)+ U(0)+and U(t) U(t) = U(0) U(0) . For neurons with only one of the modalities activated initially, i.e.,r U(0)+ U(0) or r U(0) U(0)+ , we can show there exists some time t 0 such that the neurons",
  "m, wehave (T1)r,i= O(1/n) for all r [m], i [n], and 0 t T1 and maxr (T1)r= (1)": "Second Stage: Convergence and scale difference. The second stage presents similar patternscompared to single-modal learning. Thanks to the correlation between the two modality duringgradient descent training, the two neural network converge at the same time, minimizing the trainingloss. Besides, The scale difference at the end of the first stage is carried over throughout the secondstage until convergence. Therefore, it allows to show a monotonic decrease in the loss functionas L(W(t), W(t)) W(t) W2F + W(t) W2F W(t+1) W2F W(t+1) W2F + 2, which guarantees convergence by telescoping over the inequality. At the same time,until convergence, we can show the scale difference obtained at the end of the first stage is maintained,namely maxr,i (t)r,i = O(1/n) and maxr (t)r= (1). This suggests the signal learning dominatesthe noise memorization and thus the resulting embeddings are linearly separable, which guarantees asmall test error for downstream tasks. The formal convergence result is established in Lemma D.15.Combined with the generalization error demonstrated in Appendix D.3, this completes the proof ofTheorem 4.3.",
  "Experiments": "Synthetic experimentsWe conduct synthetic experiments to verify the theoretical results obtainedin the previous sections. We generate samples following the theoretical setups, where we set the datadimension d = 2000, number of training samples n = 100, number of test samples ntest = 200,and the hidden size of all encoders as m = 50. We adopt gradient descent with a learning rateof 0.01 as the optimizer to train the model by 200 epochs. In the single-modal setting, the is set to be [5, 0, ..., 0]T and the N(0, I) for the in-distribution data, and the augmentationvector N(0, 0.01 I). For the multi-modal setting, = [0, 15, 0, ..., 0]T . In addition, for theOOD test data xtest = , we set = [2, 0, ..., 0] and N(0, I). We perform logisticregression based on the learned features h(xtest) and apply the learned classifier head to evaluateOOD generalization error in terms of prediction accuracy.",
  "Results. In , we see the training loss of both single-modal and multi-modal learning convergesrapidly. At the same time, OOD test accuracy of multi-modal learning converges to nearly 1.0 while": "that of single-modal learning stagnates around 0.5. This is primarily because under the setup wherethe other modality has a higher SNR, signal learning of is lifted. This can be verified from thethird plot of , where the signal learning of multi-modal framework is significantly higherthan single-modal. Further, it can be observed that single-modal contrastive learning exhibits moresevere noise memorization, which suppresses signal learning. In contrast, multi-modal contrastivelearning exhibits less severe noise memorization which would further encourage signal learning.These phenomena again support and align with our theoretical results. Real-world experimentsWe now extend the comparison of single-modal and multi-modal learningto realistic image data, ColoredMNIST , which is a typical benchmark studying the generaliza-tion capability under distribution shifts. The ColoredMNIST dataset is a variation of the standardMNIST dataset, where each digit is assigned a specific color based on its label. The two modalitiesare image, and text that describes the images. The task is a 10-class classification that recognizesthe number of the colored MNIST images. Specifically, we have 10 colors to color 10 digits, andintroduce spurious correlations via label noises following the literature: For the training set, 10% of labels will be clipped to a random class. For images with class 0 (or1), they will be colored as red (or green) with a probability of 77.5%, and as another randomcolor with a probability of 22.5%. The coloring scheme introduces a spurious correlation. For the test set, 10% of labels will be clipped to a random class. For images with class 0 (or 1),they will be colored as green (or red) with a probability of 77.5%, and as another random colorwith a probability of 22.5%. The coloring scheme can be considered as reversing the trainingspurious correlations. Therefore, the evaluation on test set can reflect to what extent the modellearns to use the spurious features, i.e., colors, to classify images. We implement the multi-modal learning following the practice in , where we consideran ideal language encoder that successfully encodes the caption of the images into one-hot labels of colors and digits.For single-modal learning, we follow the implementa-tion of the SimCLR to construct a set of augmentations to learn the representations.",
  "Single-modal88.43%12.68%Multi-modal87.77%82.13%": "Results. Under the distribution shift, weverify that multi-modal learning archivesan out-of-distribution test accuracy of82.13%, which outperforms that of single-modal learning 12.68%. As a result, wecan claim that the effective SNR of invari-ant features (the shape of the digit) will bedegraded under the impact of the injectedcolor. Therefore, the performance of single-modal may be suboptimal as it cannot effectively utilizethe information of the digits shape. On the other hand, multi-modal demonstrates a better capacityfor handling this scenario.",
  "Conclusions": "In this work, we have established a comprehensive comparison of the optimization differences duringthe pre-training stage and the generalization gap between single-modal and multi-modal contrastivelearning for downstream tasks. With the cooperation between modalities, multi-modal contrastivelearning can achieve better feature learning and generalization on downstream tasks compared tosingle-modal learning. On the other hand, data augmentation alone can hardly improve data qualityand thus cannot boost the performance of single-modal contrastive learning. Together, these resultsquantitatively demonstrate the superiority of multi-modal learning over single-modal learning andemphasize the importance of data quality in multi-modal contrastive learning.",
  "and Disclosure of Funding": "We thank the anonymous reviewers for their insightful comments to improve the paper. Wei Huang issupported by JSPS KAKENHI Grant Number 24K20848. Yuan Cao is supported by NSFC 12301657and HK RGC-ECS 27308624. Taiji Suzuki is partially supported by JSPS KAKENHI (24K02905)and JST CREST (JPMJCR2015). Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visuallanguage model for few-shot learning. Advances in Neural Information Processing Systems,35:2371623736, 2022.",
  "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini-mization. arXiv preprint arXiv:1907.02893, 2019": "Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and NikunjSaunshi. A theoretical analysis of contrastive unsupervised representation learning. arXivpreprint arXiv:1902.09229, 2019. Randall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learn-ing recover global and local spectral embedding methods. Advances in Neural InformationProcessing Systems, 35:2667126685, 2022. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, MateuszLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. InAdvances in Neural Information Processing Systems, 2020. Vivien Cabannes, Bobak Kiani, Randall Balestriero, Yann LeCun, and Alberto Bietti. The sslinterplay: Augmentations, inductive bias, and generalization. In International Conference onMachine Learning, pages 32523298. PMLR, 2023.",
  "Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving CLIPtraining with language rewrites. In Advances in Neural Information Processing Systems, 2023": "Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave,and Ludwig Schmidt. Data determines distributional robustness in contrastive language imagepre-training (CLIP). In International Conference on Machine Learning, 2022. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T Toshev, andVaishaal Shankar. Data filtering networks. In NeurIPS 2023 Workshop on Distribution Shifts:New Frontiers with Foundation Models, 2023. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, ThaoNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, RahimEntezari, Giannis Daras, Sarah M Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe,Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, OlgaSaukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.Datacomp: In search of the next generation of multimodal datasets. In Thirty-seventh Conferenceon Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, ElenaBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neuralinformation processing systems, 33:2127121284, 2020.",
  "Wenlong Ji, Zhun Deng, Ryumei Nakada, James Zou, and Linjun Zhang. The power ofcontrast for feature learning: A theoretical analysis. Journal of Machine Learning Research,24(330):178, 2023": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-HsuanSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learningwith noisy text supervision. In International conference on machine learning, pages 49044916.PMLR, 2021. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, AaronMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neuralinformation processing systems, 33:1866118673, 2020.",
  "Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already knowhelps: Provable self-supervised learning. Advances in Neural Information Processing Systems,34:309323, 2021": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In InternationalConference on Machine Learning, pages 1288812900. PMLR, 2022. Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mindthe gap: Understanding the modality gap in multi-modal contrastive representation learning.Advances in Neural Information Processing Systems, 35:1761217625, 2022.",
  "Yifei Ming and Yixuan Li. Understanding retrieval-augmented task adaptation for vision-language models. arXiv preprint arXiv:2405.01468, 2024": "Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang.Understanding multimodal contrastive learning and incorporating unpaired data. In InternationalConference on Artificial Intelligence and Statistics, pages 43484380. PMLR, 2023. Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt.Improving multimodal datasets with image captioning. In Thirty-seventh Conference on NeuralInformation Processing Systems Datasets and Benchmarks Track, 2023. Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Qualitynot quantity: On the interaction between dataset design and robustness of CLIP. In Advances inNeural Information Processing Systems, 2022. OpenAI. Gpt-4 technical report, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021.",
  "Yunwei Ren and Yuanzhi Li. On the importance of contrastive loss in multimodal learning.arXiv preprint arXiv:2304.03717, 2023": "Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori Hashimoto. Isa caption worth a thousand images? a study on representation learning. In InternationalConference on Learning Representations, 2023. Nikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, ShamKakade, and Akshay Krishnamurthy. Understanding contrastive learning requires incorporatinginductive biases. In International Conference on Machine Learning, pages 1925019286.PMLR, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman,Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, PatrickSchramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmar-czyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generationimage-text models. In Thirty-sixth Conference on Neural Information Processing SystemsDatasets and Benchmarks Track, 2022. James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and JoshuaAlbrecht. On the stepwise nature of self-supervised learning. In International Conference onMachine Learning, pages 3185231876. PMLR, 2023.",
  "Yihao Xue, Siddharth Joshi, Dang Nguyen, and Baharan Mirzasoleiman. Understandingthe robustness of multi-modal contrastive learning to distribution shift.arXiv preprintarXiv:2310.04971, 2023": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, HoudongHu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model forcomputer vision. arXiv preprint arXiv:2111.11432, 2021. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. Whenand why vision-language models behave like bags-of-words, and what to do about it? arXive-prints, pages arXiv2210, 2022.",
  "ALimitations and broader impact": "While our theoretical analysis is novel in terms of optimization and generalization, the data model canbe further modified to be more practical. Our theoretical analysis may be further used for empiricaland theoretical studies of contrastive learning, especially multi-modal contrastive learning. However,we do not foresee a direct social impact from our theory.",
  "which completes the proof": "According to the behavior of the defined loss derivative (8), we split the entire training dynamics intotwo phases. In the first stage, the loss derivative remains close to its initial value as the similarity issmall from initialization. Later, as the similarity grows to a constant value, the loss derivative is nolonger close to the initial value, and the dynamics transition to the second stage. In this stage, thesimilarity increases logarithmically, and the empirical loss converges.",
  "where inequality (a) follows from Lemma C.1; we use (t+1)r 0 and Lemma B.2 in derivinginequality (b). This completes the induction for r U(0)": "With Lemma C.3 at hand, we are ready to demonstrate the upper bound of the growth rate for signallearning.Lemma C.4. With the same condition as in Lemma C.2 and Lemma C.3 and n 2500 log(4/),define A(t)r= (t)r+ w(0)r , for r U(0)+ ; and A(t)r= (t)r w(0)r , for r U(0) . Withprobability at least 1 , we have",
  "(t)r,i (t)r,i + w(0)r , i, with yi = 1, i I(t)r,+": "(t)r,i (t)r,i + w(0)r , i, with yi = 1, i I(t)r,+With all the results (lemmas) and definitions outlined above at hand, we are ready to state the lemmasthat provide the lower bound for noise memorization as follows.Lemma C.11. Under the same condition as Theorem 4.2, then with probability at least 1 ,",
  "D.1.1Dynamics of Signal Learning: Lower Bound": "We first analyze the dynamics of signal learning for both two modalities. Similar as in the single-modal learning, we partition the neurons, depending on the initialization, i.e., U(t)+= {r [m] :w(t)r , > 0} and U(t)= {r [m] : w(t)r , < 0}, U(t)+= {r [m] : w(t)r , > 0} andU(t) = {r [m] : w(t)r , < 0}.",
  "j=1(t)i,j (w(t)r , yj)22 > (t)r .(38)": "Then it is clear that the change of (t)r , (t)rdoes not align with the sign of its initialization. Thereforethere exists some time t 0 such that either r U(t)+ U(t)+or r U(t) U(t) . We provethis claim by contradiction. Suppose for all t 0, r U(t)+ U(t) , then by (37) and (38), wesee (t+1)r (t)r 0 and (t+1r (t)r 0. Because w(t)r , 1.01w(0)r , + (t)randw(t)r , 0.99w(0)r , + (t)r , this raises a contradiction as either w(t)r , 0 or w(t)r , 0. With Lemma D.3 at hand, we are ready to demonstrate the lower bound of the growth rate for signallearning.Lemma D.4. With the same condition as in Lemma C.2 and Lemma C.3 and n 2500 log(4/),define A(t)r= (t)r+ w(0)r , for r U(0)+ ; and A(t)r= (t)r w(0)r , for r U(0) . Similarly,we define A(t)r= (t)r+ w(0)r , for r U(0)+and A(t)r= (t)r w(0)r , for r U(0) . Thenwith probability at least 1 , we have",
  "D.1.3Signal Learning: Proof of Lemma 5.4": "Before proving Lemma 5.4, we require the following lower bound for the initialization. Recall thedefinition that A(t)r= (t)r+ w(0)r , for r U(0)+ ; and A(t)r= (t)r w(0)r , for r U(0) .Similarly, we have A(t)r= (t)r+ w(0)r , for r U(0)+and A(t)r= (t)r w(0)r , for r U(0) ."
}