{
  "Abstract": "Average-link is widely recognized as one of the most popular and effective meth-ods for building hierarchical agglomerative clustering. The available theoreticalanalyses show that this method has a much better approximation than other popularheuristics, as single-linkage and complete-linkage, regarding variants of Dasguptascost function [STOC 2016]. However, these analyses do not separate average-linkfrom a random hierarchy and they are not appealing for metric spaces since everyhierarchical clustering has a 1/2 approximation with regard to the variant of Das-guptas function that is employed for dissimilarity measures [Moseley and Yang2020]. In this paper, we present a comprehensive study of the performance ofaverage-link in metric spaces, regarding several natural criteria that captureseparability and cohesion, and are more interpretable than Dasguptas cost func-tion and its variants. We also present experimental results with real datasets that,together with our theoretical analyses, suggest that average-link is a better choicethan other related methods when both cohesion and separability are importantgoals.",
  "Introduction": "Clustering is the task of partitioning a set of objects/points so that similar ones are grouped togetherwhile dissimilar ones are put in different groups. Clustering methods are widely used for exploratoryanalysis and for reducing the computational resources required to handle large datasets. Hierarchical clustering is an important class of clustering methods. Given a set of X of n points, ahierarchical clustering is a sequence of clusterings (Cn, Cn1, . . . , C1), where Cn is a clustering withn unitary clusters, each of them corresponding to a point in X, and the clustering Ci, with i < n, isobtained from Ci+1 by replacing two of its clusters with their union Ai. A hierarchical clusteringinduces a strictly binary tree with n leaves, where each leaf corresponds to a point in X and the ithinternal node, with i < n, is associated with the cluster Ai; the points in Ai correspond to the leavesof the subtree rooted in Ai. Hierarchical clustering methods are often taught in data science/MLcourses, are implemented in many machine learning libraries, such as scipy, and have applicationsin different fields as evolution studies via phylogenetic trees [Eisen et al., 1998], finance [TUM, 2010]and detection of closely related entities [Kobren et al., 2017, Monath et al., 2021]. Average-link is widely considered one of the most effective hierarchical clustering algorithms. Itbelongs to the class of agglomerative methods, that is, methods that start with a set of n clusters,corresponding to the n input points, and iteratively use a linkage rule to merge two clusters. Due toits relevance, we can find some recent works dedicated to improving average-link efficiency andscalability [Yu et al., 2021, Dhulipala et al., 2021, 2022, 2023] as well as recent theoretical work thattry to understand its success in practice [Cohen-Addad et al., 2019, Charikar et al., 2019a, Moseleyand Wang, 2023, Charikar et al., 2019b].",
  "arXiv:2411.05097v1 [cs.LG] 7 Nov 2024": "Most of the available theoretical works give approximation bounds for average-link regarding thecost function introduced by [Dasgupta, 2016] as well as for some variants of it. Let D be the treeinduced by a hierarchical clustering. Dasguptas cost function and its variation for dissimilaritiesconsidered in [Cohen-Addad et al., 2019] are, respectively, given by",
  "a,bXdiss(a, b) |D(a, b)|,(1)": "where sim(a, b) (diss(a, b)) is the similarity (dissimilarity) of points a and b; D(a, b) is the subtreeof D rooted at the least common ancestor of the leaves corresponding to a and b, and |D(a, b)| isthe number of leaves in D(a, b). In general, the existing results show that average-link achievesconstant approximation for variants of Dasguptas function while other linkage methods do not. However, there is significant room for further analysis due to the following reasons. First, Das-guptas cost function, despite its nice properties, is less interpretable than traditional cost functionsthat measure compactness and separability. Second, although the analyses based on Dasg and itsvariants allow to separate average-link from other linkage methods as single-linkage andcomplete-linkage in terms of approximation, they do not separate average-link from a randomhierarchy [Cohen-Addad et al., 2019, Moseley and Wang, 2023, Charikar et al., 2019b]. Moreover, forthe case in which the points lie in a metric space every hierarchical clustering has 1/2 approximationfor the maximization of CKMM [Wang and Moseley, 2020], so this cost function is less appealing inthis relevant setting. Finally, to the best of our knowledge, Dasg does not reveal how good are theclusters generated for a specific range of k. As an example, small k are important for exploratoryanalysis while large k is important for de-duplication tasks [Kobren et al., 2017].",
  "Our results": "Motivated by this scenario, we present a comprehensive study of the performance of average-linkin metric spaces, with regards to several natural criteria that capture separability and cohesion ofclustering. In a nutshell, these results, as explained below, show that average link has much betterglobal properties than other popular heuristics when these two important goals are taken into account.",
  "sepmin(C)(4)": "Let Ak be a k-clustering produced by average-link. We first prove through a simple inductiveargument that cs-ratioAV(Ak) 1. This result does not assume that the points in X lie in a metricspace and it is tight in the sense that there are instances in which cs-ratioAV(C) = 1 for every k-clustering C. For the related cs-ratioDM criterion, we present a more involved analysis whichshows that cs-ratioDM(Ak) as well as the approximation of average-link regarding OPT (theminimum possible cs-ratioDM) are O(log n); these bounds are nearly tight since there exists aninstance for which cs-ratioDM(Ak) and cs-ratioDM(Ak)/OPT are (log n log log n). Both cs-ratioAVand cs-ratioDM allow an exponential separation between average-link and other linkage methods,as single-linkage and complete-linkage. Interestingly, in contrast to CKMM (Eq. 1), our criteriaalso allow a very clear separation between average-link and the clustering induced by a randomhierarchy.",
  "Next, we focus on separability criteria. Let OPTSEP(k)be the maximum possible sepavof ak-clustering for (X, dist). We show that sepav(Ak) is at least OPTSEP(k)": "k+2 ln n and that this result isnearly tight. Furthermore, we argue that any hierarchical clustering algorithm that has boundedapproximation regarding max-diam or max-avg does not have approximation better than 1/k tosepav. Regarding single-linkage and complete-linkage, we present instances that show thattheir approximation with respect to sepav are exponentially worse than that of average-link, forthe relevant case that k is small. We also investigate the cohesion of average-link. For a k-clustering C, let avg-diam be theaverage diameter of the k clusters in C. Let OPTDM(k) and OPTAV(k) be, respectively, the min-imum possible max-diam and avg-diam of a k-clustering for (X, dist). We prove that forall k, max-diam(Ak) min{k, 1 + 4 ln n}klog2 3OPTAV(k). This result together with the in-stance given by Theorem 3.4 of [Dasgupta and Laber, 2024] allow to separate average-linkfrom single-linkage, in terms of approximation, when k is (log2.41 n). We also show thatmax-diam(Ak) is (k)OPTDM(k), which is, to the best of our knowledge, the first lower bound onthe maximum diameter of average-link. Finally, to complement our study, we present some experiments with 10 real datasets in whichwe evaluate, to some extent, if our theoretical results line up with what is observed in practice.These experiments conform with our theoretical results since they also suggest that average-linkperforms better than other related methods when both cohesion and separability are taken intoaccount.",
  "There is a vast literature about hierarchical agglomerative clustering methods. Here, we focus onworks that provide provable guarantees for average-link and some other well-known linkagemethods": "Average-link. There are works that present bounds on the approximation of average-link re-garding some criteria [Cohen-Addad et al., 2019, Charikar et al., 2019b,a, Moseley and Wang,2023, Dasgupta and Laber, 2024]. All these works but [Dasgupta and Laber, 2024] analyse theapproximation of average-link regarding variants of Dasguptas cost function. [Moseley andWang, 2023] assumes that the proximity between the points in X is given by a similarity matrix.They show that average-link is a 1/3-approximation with respect to the \"dual\" of Dasguptas costfunction. [Cohen-Addad et al., 2019], as in our work, assumes that the proximity between points inX is given by a dissimilarity measure and shows that average-link has 2/3 approximation for theproblem of maximizing CKMM (Eq. 1). [Charikar et al., 2019b] show that these approximation ratiofor average-link are tight. These papers also show that a random hierarchy obtained by a divisiveheuristic that randomly splits the set of points in each cluster matches the 1/3 and 2/3 bounds. [Dasgupta and Laber, 2024] presents an interesting approach to derive upper bounds on cohesioncriteria for a certain class of linkage methods that includes average-link. They show that avg(A) k1.59OPTAV(k) for every cluster A Ak. Our bound on the maximum diameter of a cluster in Akincurs an extra factor of min{k, 1 + 4 ln n} to this bound and its proof combines their approach withsome new ideas/analyses. Other Linkage Methods. There are also works that give bounds on the diameter of the clus-tering built by complete-linkage and single-linkage on metric spaces [Dasgupta andLong, 2005, Ackermann et al., 2010, Growendt and Rglin, 2015, Arutyunova et al., 2023,Dasgupta and Laber, 2024]. Let C and S be the k-clustering built by these methods, respec-tively.[Arutyunova et al., 2023] shows that max-diam(C) is (kOPTDM(k)) while [Dasgupta and Laber, 2024] shows that max-diam(C) is O(min{k1.30OPTDM(k), k1.59OPTAV(k)}). Regard-ing single-linkage, max-diam(S) is (kOPTDM(k)) [Dasgupta and Long, 2005, Arutyunovaet al., 2023] and (k2OPTAV(k)) [Dasgupta and Laber, 2024]. [Ackermann et al., 2010, Growendtand Rglin, 2015] give bounds for the case in which dist is the Euclidean metric. In terms of separability criteria, it is well known that single-linkage maximizes the minimumspacing of a clustering [Kleinberg and Tardos, 2006][Chap 4.7]. Recently, [Laber and Murtinho,2023] observed that it also maximizes the cost of the minimum spanning tree spacing, a strongercriterion. These criteria, in contrast to ours, just take into account the minimum distance betweenpoints in different clusters and then they can be significantly impacted by noise.",
  "Theorem 3.1. Let Ak be a k-clustering built by average-link.Then, for every k,cs-ratioAV(Ak) 1": "We note that the above result does not assume the triangle inequality and it is tight in the sense thatfor the instance (X, dist), in which the n points of X have pairwise distance 1, every clustering hascs-ratioAV equal to 1. In Section B.2, we present instances which show that cs-ratioAV can be (n), (n) and un-bounded in terms of n for single-linkage, complete-linkage and a random hierarchy, respec-tively. Interestingly, all the k-clustering, with 2 < k n/2, induced by the hierarchical clustering ob-tained by these methods satisfy these bounds. Furthermore, since cs-ratioDM(C) cs-ratioAV(C)for every clustering C, these bounds also hold for the cs-ratioDM criterion. A natural question that arises is whether average-link has a \"good\" approximation with respect tocs-ratioAV. Unfortunately, the answer is no. In fact, in Section B.3 we show an instance where theapproximation is unbounded in terms of n. However, as we show in the next section, average-linkhas a logarithmic approximation with respect to cs-ratioDM.",
  "We analyze the cs-ratioDM of average-link. The results of this section will have an importantrole in the analysis of both the separability and cohesion of average-link presented further": "First, we show that for every cluster X in Ak, the average distance of a point x X to the otherpoints in X x is at most a logarithmic factor of the average distance between any two clusters Yand Z. The proof can be found in Section B.5. Let Ti1 be the cluster that contains x before theith merge involving x and let Si be the cluster that is merged with Ti1. We prove by inductionthat avg(x, Ti x) ln H|Ti|1avg(Y, Z), which implies on the desired result because Tt = X forsome t. To establish the induction, we use the triangle inequality to write avg(x, Ti x) as a functionof both avg(x, Ti1 x) and avg(Ti1, Si), and also argue that avg(Ti1, Si) avg(X, Y ).",
  "where the first inequality follows from the triangle inequality and the second one due to Lemma3.2": "The next theorem shows that cs-ratioDM(Ak) 2Hn and that average-link has a logarithmic ap-proximation for the cs-ratioDM criterion. The first upper bound is a simple consequence of Theorem3.3. Let OPT be the minimum possible cs-ratioDM. To prove the bound on the approximation weconsider two cases. If OPT 1/3 the result holds because cs-ratioDM(Ak) 2 ln n 6OPT ln n.If OPT < 1/3, we argue that the clusters in the optimal clustering are \"well separated\" and, hence,average-link builds the optimal clustering. Theorem 3.4. For all k, the k-clustering Ak built by average-link satisfies cs-ratioDM(Ak) 2Hn. Furthermore, for all k, cs-ratioDM(Ak) is O(log n) OPT where OPT is cs-ratioDM of thek-clustering with minimum possible cs-ratioDM. Proof. The inequality cs-ratioDM(Ak) 2Hn is obtained by using Theorem 3.3, with X beingthe cluster with the largest diameter in Ak and Y and Z being the clusters in Ak that satisfyavg(Y, Z) = sepmin(Ak).",
  "Claim 1. Let C, C be two clusters in C(k) and let a, b be two closest points in C and C, that is,dist(a, b) = min{dist(x, y)|(x, y) C C}. Thus, dist(a, b) > max{diam(C), diam(C)}": "Proof of the claim. We assume w.l.o.g. that diam(C) diam(C). For the sake of reaching acontradiction, assume that dist(a, b) diam(C). Then, it follows from the triangle inequality thatthe maximum distance between a point in C and C is at most 3diam(C). Thus, sepmin(C(k)) avg(C, C) 3diam(C) and so cs-ratioDM(C(k)) diam(C)/3diam(C) = 1/3, which contra-dicts our assumption. . Now, we argue that average-link constructs the clustering C(k) when cs-ratioDM(C(k)) < 1/3,so its approximation is 1 in this case. For the sake of reaching a contradiction, let us assumeAk = C(k). Hence, at some iteration average-link merges two clusters, say A and B, that satisfythe following properties: A C and B C, where C and C are two different clusters in C(k).Let t be the first iteration of average-link when it occurs. Case 1) A C or B C. Let us assume w.l.o.g. that A C. In this case, there is a cluster Aat the beginning of iteration t such that A A C. We have that avg(A, A) diam(C) and bythe above claim the minimum distance between A and B is larger than max{diam(C), diam(C)}.Thus, avg(A, B) > max{diam(C), diam(C)} avg(A, A), which contradicts the choice ofaverage-link. Case 2) A = C and B = C. If k = 2 we are done. Otherwise, there exists a cluster C C(k)and two clusters X and Y at the beginning of iteration t such that X Y C. Thus, it followsfrom the condition OPT < 1/3 that avg(X, Y ) diam(C) < 1",
  "3avg(C, C) avg(C, C), which again contradicts the choice of average-link": "It is noteworthy that, in contrast to Theorem 3.1, the assumption that the points lie in a metric space isnecessary to prove Theorem 3.4. In Section B.4 we present an instance that supports this observation. Now, we present an instance, denoted by ICS, that shows that the above results are nearly tight. Thisinstance with small modifications will also be used to investigate the tightness of our results regardingthe separability () and the cohesion () of average-link. We note that in mostof the instances presented here, including ICS, will have more than one possible execution for themethods we analyze. In these cases, we will always consider the execution that is more suitable forour purposes. These multiple executions can be avoided at the price of more complicated descriptionsthat involve the addition of small values to the distance or points to break ties.",
  "Let t be an integer that satisfies t! = n; note that t = (log n": "log log n). Moreover, let A0 be a set containinga single point located at position p0 in the real line and Ai, for 0 < i t 1, be a set of (i + 1)! i!points that are located at position pi of the real line. We define B0 = A0 and Bi = Bi1 Ai, fori 1. Set p0 = 0, p1 = 1 and, for i > 1, pi = pi1 + avg(Ai1, Bi2). The set of points for ourinstance ICS is Bt1 and the distance between a point in Ai and a point in Aj is |pi pj|. Thefollowing lemma gives properties of ICS and, in particular, how average-link behaves on it. Lemma 3.5. For i 0, we have that |Bi| = (i+1)! and for i 2, we have diam(Bi2) = i(i1)/2,avg(Bi2, Ai1) = i + 1 and pi = i(i + 1)/2. Furthermore, for k t, average-link obtains thek-clustering Ak = (Btk, Atk+1, . . . , At1) and, in particular, for k = 2 it obtains the clusteringA2 = (Bt2, At1).",
  "k+2 ln n and that this bound is nearly tight. We also show that there are instances inwhich the sepav of single-linkage and complete-linkage are exponentially smaller than thatof average-link": "Theorem 4.2 gives an upper bound on sepav for average-link and its complete proof can befound in Section D.2. Here, we give an overview of the proof for the case k > 2, which is themost involved one. The proof uses the fact established by Proposition 4.1 that there exists a setof k points P X that satisfies avg(P) OPTSEP(k). This holds because a set of k randomlyselected points that intersect all clusters of a k-clustering with maximum sepav satisfies the thedesired property (in expectation). Having this result in hands, it is enough to show that avg(P) isO((k + Hn1)sepav(Ak)). This bound on avg(P) is obtained by relating the distance of each pair of points p, p P with theaverage distance between clusters in Ak. Let p, p P and let A and A be clusters in Ak such thatp A and p A. Moreover, let S be a cluster in Ak, with S / {A, A}. From the triangle inequalitywe have that dist(p, p) = avg(p, p) avg(p, A) + avg(A, S) + avg(S, A) + avg(A, p). Then,by bounding both avg(p, A) and avg(A, p) via Lemma 3.2, with Y and Z satisfying avg(Y, Z) sepav(Ak), we conclude that dist(p, p) 2Hnsepav(Ak) + avg(A, S) + avg(S, A). In generallines, the result is then established by averaging this inequality for all S / {A, A} and for allp, p P. Proposition 4.1. There is a set of points P X with the following properties: |P| = k andavg(P) OPTSEP(k).Theorem 4.2. For every k, the k-clustering Ak obtained by average-link satisfies sepav(Ak) OPTSEP(k)k+2Hn . We present two instances that, together, show that the previous theorem is nearly tight. The first is theinstance ICS presented right after Theorem 3.4. For ICS, the clustering A2 = (At1, Bt2) built byaverage-link satisfies sepav(A2) = avg(At1, Bt2) = t + 1. On the other hand, Eq. (5) showsthat sepav(A) = (t1)2",
  "log n)": "Now, we present our second instance, denoted by Isepk. Let k be an odd number and let D and bepositive numbers. The set of points of Isepkis given by S1 S2 S3, where |S1| = |S2| = (k 1)/2and S3 = {si|1 i k 2}. We have dist(x, y) = for x, y S1, dist(x, y) = for x, y S2,dist(x, y) = 1 for x, y S3 and dist(x, y) = D if x and y are not in the same set. For Isepk, when D is sufficiently large and is sufficiently small, Ak = (S1, S2, s1, . . . , sk2) andsepav(Ak) = O(D/k). On the other hand, the sepav of the kclustering that has the clusterS3 and k 1 singletons corresponding to the points in S1 S2 is (D). Thus, sepav(Ak) isO(OPTSEP(k)/k). We note that single-linkage and complete-linkage also obtain the k-clustering Ak for Isepk,so the upper bound OPTSEP(k)/k also holds for them. In Section D.3 we present instances that showthat sepav is O( OPTSEP(k)n) for both single-linkage and complete-linkage.",
  "In this section, we prove that max-diam(Ak) min{k, 1 + 4 ln n}k1.59OPTAV(k) and we alsopresent an instance which shows that max-diam(Ak) kOPTDM(k)": "Dasgupta and Laber presented an interesting approach to devise upper bounds on cohesioncriteria for a class of linkage methods that includes average-link. Although this approach was usedto show that the maximum pairwise average distance of a cluster in Ak is at most k1.59OPTAV(k), itcannot be employed, at least directly, to bound the maximum diameter of a cluster in Ak. Thus, toobtain our (1 + 4 ln n)k1.59OPTAV(k) bound we combine the results of [Dasgupta and Laber, 2024]with Theorem 3.4 while for the k1+1.59OPTAV(k) bound we add some new ideas/analysis on top ofthose from [Dasgupta and Laber, 2024]. The analysis in Dasgupta and Laber keeps a dynamic partition of the clusters produced bythe linkage method under consideration. Each group in the partition is a set of clusters denoted byfamily. A point p belongs to a family F if it belongs to some cluster in F. Thus, diam(F) is givenby the maximum distance among the points that belong to F. The approach bounds the diameterof each family F as (essentially) a function of the clusters that F touches in a target k-clusteringT = (T1, . . . , Tk). The bound on diam(F) is then used to upper bound the diameter of the clustersin F. For a k-clustering C, let avg-diam(C) := 1",
  "kki=i diam(Ci). As in Dasgupta and Laber ,we use as the target clustering the one with minimum avg-diam": "We explain how the families evolve along the execution of a linkage method, in particularaverage-link. Initially, we have k families, F1, . . . , Fk, where Fi is a family that contains |Ti|clusters, each one being a point from Ti. Furthermore, the families are organized in a directed forestD that initially consists of k isolated nodes, where the ith node corresponds to family Fi. We specify how the families and the forest D are updated when the linkage method merges theclusters g and g belonging to the families F and F , respectively. Assume w.l.o.g. |F| |F |. Wehave the following cases:",
  "We are ready to establish the main result of this section.Theorem 5.3. Every cluster S in Ak satisfies diam(S) min{k, 4 ln n + 1}klog2 3OPTAV(k)": "Proof. Let V = {T T |S T = } be the set of clusters of the target clustering T that inter-sect S. We build a graph G whose nodes correspond to the clusters in V . At the beginning ofaverage-links execution, G contains the set of nodes V and no edges.",
  "Case 1) (g g) S = . In this case, G is not updated": "Case 2) (g g) S. Let x and y be points in g and g such that dist(x, y) is minimum and let T xand T y be the clusters in T that contain x and y, respectively. We add an edge of weight dist(x, y)between T x and T y. We say, in this case, that x and y are associated with the edge that links T x toT y.",
  "Claim 3. For a cluster C, let VC := {T T |T C = }. Let S be a cluster generated byaverage-link that is a subset of S. Then, when S is created, the subgraph of G induced by VS isconnected": "Proof of the claim If |S| = 1 the property holds. Let S be a cluster obtained by merging S1 and S2.By induction, the property holds for S1 and S2. Since an edge is added between nodes in VS1 andVS2 then the property also holds for S. Thus, at the end of the algorithm, G is connected and each of its edges has weight at mostklog2 3OPTAV(k). Let x and y be points in S such that dist(x, y) = diam(S) and let T x =v1 . . . v = T y be a path in G from T x to T y.",
  "(k 1)klog2 3OPTAV(k) + kOPTAV(k)": "For the logarithmic bound, let S1 and S2 be the two clusters that are merged to form S. At thebeginning of the iteration in which S1 and S2 are merged, Proposition 5.1 assures that there existsa regular family, say H. Let h and h be two clusters in H. By Proposition 5.2, avg(h, h) diam(H) klog2 3OPTAV(k). Thus, by Theorem 3.3, diam(S1) 2 ln n avg(h, h) 2 ln n klog2 3OPTAV(k) and diam(S2) 2 ln n klog2 3OPTAV(k). Let s1 S1 and s2 S2 be such thatdist(s1, s2) = min{dist(p, q)|(p, q) S1 S2). Since S1 and S2 are merged we have thatdist(s1, s2) avg(S1, S2) avg(h, h) klog2 3OPTAV(k). Thus, diam(S) diam(S1) +dist(s1, s2) + diam(S1) (1 + 4 ln n)klog2 3OPTAV(k). Theorem 3.4 of Dasgupta and Laber presents an instance with n = 2k 2 points forwhich single-linkage builds a k-clustering that has a cluster whose diameter is (k2OPTAV(k)).Thus, this result together with Theorem 5.3 show a separation between average-link andsingle-linkage when k is (log2.41 n). Our last theoretical result is a lower bound on the maximum diameter of the clustering built byaverage-link. Its proof can be found in the Section E and it employs an augmented version ofinstance ICS, presented right after Theorem 3.4.",
  "Experiments": "In this final section, we briefly present an experiment in which we evaluate whether average-link,in addition to having better theoretical bounds, it also has a better performance in practice for thestudied criteria. We employed 10 datasets and used the Euclidean metric to measure distances. Foreach of them, we executed average-link, complete-linkage and single-linkage, for thefollowing sets of values of k: Small={k|2 k 10}, Medium={k|n 4 k n + 4} andLarge={k|k = n/i and 2 i 10}. More details, as well as the results of our experiment withother distances, can be found in Section F. shows the average ratio between the result of a method and that of the best one, grouped bycriterion and set of k. Each entry is the average of 90 ratios (9 ks and 10 datasets) and each of theseratios for a method M is a value between 0 and 1 that is obtained by dividing the minimum betweenthe result of M and that of the best method by the maximum between them. The letters A, C and Sare the initials of the evaluated methods. Concerning separability criteria, single-linkage and average-link have the best results forsepav. The latter has some advantage when k is small, which is in line with its better worst-casebound for small k (results from ). For sepmin, average-link has a huge advantage, whichis not surprising since its linkage rule tries to increase sepmin at each step by merging the the clustersA and B for which avg(A, B) = sepmin(C), where C is the current clustering. Regarding cohesion criteria, complete-linkage and average-link were the best methods. Theyhad close results for max-avg while for max-diam the former had a strong dominance. Theseresults align with ours and those from [Dasgupta and Laber, 2024], in the sense that they showthat these linkage methods present better worst-case upper bounds than single-linkage whenthe comparison is made against OPTAV(k). Moreover, the advantage of complete-linkage formax-diam is also expected since it is the \"natural\" greedy rule to minimize the maximum diameter(See Proposition 2.1 of Dasgupta and Laber ). For cs-ratioDM, average-link and complete-linkage present the best results, with the for-mer being slightly superior for the small k and the latter being slightly superior when k is notsmall. average-link has a huge dominance for the cs-ratioAV criterion, which lines up with thetheoretical results from .1.",
  "In summary, these experiments, together with our theoretical results, provide evidence thataverage-link is a better choice when both cohesion and separability are relevant": "Acknowledgements The work of the first author is partially supported by CNPq (grant 310741/2021-1). This study was financed in part by the Coordenao de Aperfeioamento de Pessoal de NvelSuperior - Brasil (CAPES) - Finance Code 001 Limitations. We have not identified a major limitation in our work. That said, the assumption thatthe points lie in a metric space used in our results (except Theorem 3.1) could be seen as a limitation.On the experimental side, having more than 10 datasets would give our conclusions more robustness.",
  "Correlation, hierarchies, and networks in financial markets. Journal of Economic Behavior &Organization, 75(1):4058, 2010. ISSN 0167-2681. doi: Transdisciplinary Perspectives on Economic Complexity": "Ari Kobren, Nicholas Monath, Akshay Krishnamurthy, and Andrew McCallum.A hierarchi-cal algorithm for extreme clustering.In Proceedings of the 23rd ACM SIGKDD Interna-tional Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August13 - 17, 2017, pages 255264. ACM, 2017. doi: 10.1145/3097983.3098079. URL Nicholas Monath, Kumar Avinava Dubey, Guru Guruganesh, Manzil Zaheer, Amr Ahmed, AndrewMcCallum, Gkhan Mergen, Marc Najork, Mert Terzihan, Bryon Tjanaka, Yuan Wang, and YuchenWu. Scalable hierarchical agglomerative clustering. In Feida Zhu, Beng Chin Ooi, and ChunyanMiao, editors, KDD 21: The 27th ACM SIGKDD Conference on Knowledge Discovery andData Mining, Virtual Event, Singapore, August 14-18, 2021, pages 12451255. ACM, 2021. doi:10.1145/3447548.3467404. URL Shangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, and Julian Shun. Parchain: A framework forparallel hierarchical agglomerative clustering using nearest-neighbor chain. Proc. VLDB Endow.,15(2):285298, 2021. doi: 10.14778/3489496.3489509. URL Laxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab S. Mirrokni, and Jessica Shi. Hierarchicalagglomerative graph clustering in nearly-linear time. In Marina Meila and Tong Zhang, editors,Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 26762686.PMLR, 2021. URL Laxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab Mirrokni, and Jessica Shi.Hierar-chical agglomerative graph clustering in poly-logarithmic depth.In Sanmi Koyejo, S. Mo-hamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neu-ral Information Processing Systems 35:Annual Conference on Neural Information Pro-cessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - Decem-ber 9, 2022, 2022.URL Laxman Dhulipala, Jakub Lacki, Jason Lee, and Vahab Mirrokni. Terahac: Hierarchical agglomerativeclustering of trillion-edge graphs. Proc. ACM Manag. Data, 1(3):221:1221:27, 2023. doi:10.1145/3617341. URL",
  "Benjamin Moseley and Joshua R. Wang. Approximation bounds for hierarchical clustering: Averagelinkage, bisecting k-means, and local search. J. Mach. Learn. Res., 24:1:11:36, 2023. URL": "Moses Charikar, Vaggos Chatziafratis, and Rad Niazadeh.Hierarchical clustering better thanaverage-linkage. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAMSymposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9,2019, pages 22912304. SIAM, 2019b. doi: 10.1137/1.9781611975482.139. URL Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. In Daniel Wichs andYishay Mansour, editors, Proceedings of the 48th Annual ACM SIGACT Symposium on Theory ofComputing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 118127. ACM, 2016.doi: 10.1145/2897518.2897527. URL Yuyan Wang and Benjamin Moseley. An objective for hierarchical clustering in euclidean space and itsconnection to bisecting k-means. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI2020, New York, NY, USA, February 7-12, 2020, pages 63076314. AAAI Press, 2020. doi:10.1609/AAAI.V34I04.6099. URL",
  "B.1Proof of Theorem 3.1": "Proof. When k = n the result is valid because avg(An) = 0 for every A An. We assume byinduction that the result holds for k + 1 and we prove that it also holds for k. Let A and B be theclusters in Ak+1 that are merged to obtain Ak, so Ak = Ak+1 (A B) {A, B}. Let S, T and Ube clusters in Ak, with T = U. It is enough to prove that avg(S) avg(T, U).",
  "Case 1) A B / {S, T, U}. In this case, S, T, U Ak+1 and, then, by induction, avg(S) avg(T, U)": "Case 2) A B = S and S / {T, U}. Since A, B, T, U Ak+1, the induction hypothesis assuresthat avg(A) avg(T, U) and avg(B) avg(T, U) and the average-link rule ensures thatavg(A, B) avg(T, U). Since avg(S) is a convex combination of avg(A), avg(B) and avg(A, B),the above inequalities imply that avg(S) = avg(A B) avg(T, U). Case 3) A B = S and S {T, U}.We assume w.l.o.g.that S = T.The inductionhypothesis and the average-link rule guarantee that max{avg(A), avg(B), avg(A, B)} min{avg(A, U), avg(B, U)} Since avg(S, U) is a convex combination of avg(A, U) and avg(B, U)and avg(S) is a convex combination of avg(A), avg(B) and avg(A, B), the above inequality impliesthat avg(S) = avg(A B) avg(T, U). Case 4) S = A B and A B {T, U}. We assume w.l.og. that T = A B. SinceS, A, B, U Ck+1, the induction hypothesis assures that avg(S) min{avg(A, U), avg(B, U)}Since avg(T, U) is a convex combination of avg(A, U) and avg(B, U), the above inequality assuresthat avg(S) avg(T, U).",
  "The following examples show that the cs-ratioAV of complete-linkage, single-linkage anda random hierarchy can be much higher than that of average-link in metric spaces": "single-linkage. Consider the instance with n points x1, . . . , xn in the real line, where xi = 1, ifi = 1, and xi = xi1 + 1 i, for i > 1. For sufficiently small, single-linkage builds thek-clustering C = (x1, x2, . . . , xk1, {xk, . . . , xn}). We have that avg({xk, . . . , xn}) is (n k)while avg(x1, x2) = 1 , so that cs-ratioAV(C) is (n k). complete-linkage. Let t = 2m 1, where m is a positive integer and let p = 2(t2 + t). We build aninstance whose set of points X = A B C D E has n = 2p points, where A, B, C, D and Eare sets of points in Rp+1 that satisfy the following properties:",
  "A = {a1, . . . , at} and the first coordinate of ai is equal to i + 1/2;": "B = {b1, . . . , bt} and the first coordinate of bi is equal to (i + 1/2); C has t2 points and all have the first coordinate 1/2; D has t2 points and all have the first coordinate 1/2; E = {e1, . . . , ep}, where the value of the first coordinate of ei is t2, the (i+1)th coordinatehas value 1.5t and all other coordinates have value equal to 0. The distance between any two points in X is given by the 1 metric. Hence, the distance between anytwo points in E is 3t, the distance between points in AB C D is at most 2t+1 and the distancebetween a point in A B C D and a point in E is at least t2. For i p, let Ei = {ei, . . . , ep}.",
  "Since max-avg(C) avg(Ek2) = 3t, we get that cs-ratioAV(Ck) is (t) and, hence, (n)": "random hierarchy. To analyze a random hierarchy, we first need to define how it is generated. Westart with a random permutation of the points in X and a clustering C containing initially the clustercomprised by all points in X. Let x1, . . . , xn be the points in X according to the order given by thepermutation. Then, we perform the following steps until we have n clusters:",
  "After t splits we have a clustering with n t clusters": "Now, we consider an instance with n points and 3 groups X, Y and Z, that satisfy |X| = |Y | =(n 1)/2 and Z = {z}. The distance between any two points in X is 1 and the same holds forY . Moreover, the distance between points in X and Y is 2. The distance of z to any other point isD >> n2. Any k-clustering, with k 3, has sepmin 2 because at least two clusters do not containz. Let k n/2. The probability that z is a singleton in the k-clustering when z / {x1, xn} isn3k3",
  "(n 1)(n 2) < 1": "Then, with probability at least 3/4, there will be a cluster C that contains z and a point in X Y ,which implies that E[avg(C)] D/4n2. Thus, with probability at least 3/4 the k-clusteringinduced by the random hierarchy has sepav (D/4n2), when z / {x1, xn}. Since the probability ofz / {x1, xn} is (n 2)/n, the same bound holds when we drop this constraint.",
  "B.3On the approximation of average-link for cs-ratioAV": "Let n be an even number, k = 2 and a positive number very close to 0. Consider 4 set of pointsS1, S2, S3 and S4, where S1 = {s1},S2 = {s2} and S3 and S4 have n/2 1 points each. We havedist(x, y) = for x, y S3, dist(x, y) = for x, y S4, dist(s1, s2) = T and dist(x, y) = Tfor (x, y) S3 S4. In addition, we have dist(s1, x) = 2T for x = s2 and dist(s2, y) = 2T fory = s1. Clearly, the 4-clustering obtained by average-link is (S1, S2, S3, S4). Then, to obtain a 2-clustering, it merges the clusters S1 and S2 and, next, S3 and S4, so that the final 2-clusteringis A2 = (S1 S2, S3 S4), which satisfies max-avg(A2) = T and sepmin(A) = 2T. On theother hand, for the clustering S = (S1 S3, S2 S4), we have that max-avg(S) is O(T/n2) andsepmin(S) T. Thus, the approximation of average-link is (n2)",
  "We present an instance that shows that the assumption that points lie in a metric space is necessary toestablish Theorem 3.4": "Let A and B be sets with n/21 and n/2 points, respectively. We have dist(a, a) = 1 if a, a A;dist(b, b) = 1 if b, b B and dist(a, b) = 4 if (a, b) A B. Moreover, let p be a point thatis not in A B. There is a point a A for which dist(a, p) = n/2 2 and for all other pointsa A {a}, dist(a, p) = 2. Moreover, dist(p, b) = 4 for b B. For this instance average-link builds the 2-clustering A2 = (A{p}, B). We have that diam(Ap) = n/2 2 and avg(A p, B) = 4, Thus, cs-ratioDM(A2) is (n). On the other hand, for theclustering A = (A, B p), cs-ratioDM(A) is O(1), so the approximation of average-link is(n).",
  "where the inequality follows from the triangle inequality": "Let us consider the beginning of the iteration in which Ti1 and Si are merged. At this point wehave 1 clusters Y1, . . . , Y such that Y = Y1 Y and clusters Z1, . . . , Z such thatZ = Z1 Z. Note that there exist i and j such that avg(Yi, Zj) avg(Y, Z). Thus, we musthave ei avg(Y, Z), otherwise average-link would merge Yi and Zj rather than Ti1 and Si.",
  "diam(Bi1) = pi1 p0 = pi1 = i(i 1)": "Now we show that at the beginning of the step (n t) + i average-link keeps a clustering thatcontains the cluster Bi1 and the clusters Aj, for i j t 1. First, we observe that after n tsteps average-link produces a t-clustering (A0, . . . , At1) since points in the same group Ai arelocated at the same position. We analyze what happens in the remaining t 1 steps.",
  "D.1Proof of Proposition 4.1": "Proof. Let C = (C1, . . . , Ck) be a k-clustering that maximizes sepav. Let Q be the family of setsof points Q such that |Q| = k and Q intersects all clusters C1, . . . , Ck. Let P = {p1, . . . , pk} be aset in Q that satisfies avg(P) avg(Q), for every Q Q. Moreover, let U = {u1, . . . , uk} be a",
  "log n": "For single-linkage, consider the instance X = A B {p}, where A contains n 1 npoints and B contains n points b1, . . . , bn. Moreover, we have dist(x, y) = , for x, y A,dist(bi, x) = i for every point x A and dist(bi, bj) = |ij|. Moreover, dist(p, x) = 1+, forevery point x A. and dist(p, bi) = 1 + + i In this case, single-linkage builds the clustering(A B, {p}). We have that sepav(A B, p) 2, while sepav(A p, B) is (n). Regarding complete-linkage, we consider the instance presented at Section B.2, but without theset E, that is, the set of points is X = A B C D. When k = 2, complete-linkage buildsthe clustering (A C, B D) that has sepav O(1) while the clustering (A, C D B) satisfies",
  "D.4Separability and cohesion can be conflicting": "Recall that for instance Isepkaverage-link builds the k-clustering Ak = (S1, S2, s1, s2, . . . , sk2).Note that max-diam(Ak) = max-avg(Ak) = . Let A be a k-clustering different from Ak. We arguethat max-diam(A) 1 and max-avg(A) is (1/k2). In fact, if A has a cluster A that satisfies|A| 2 and |A S3| 1, then max-diam(A) 1 and max-avg(A) is (1/k2). Otherwise, if A does not have such a cluster, then all points in S3 must be singletons in A. Since A = Ak, thereis a cluster in A that contains both a point in S1 and a point in S2. Thus, max-diam(A) = D andmax-avg(A) is (D/k2). Let M be the class of methods with bounded approximation regarding max-diam or to max-avg.Then any method M M builds the clustering Ak. Since sepav(Ak) is O(D/k) and there is ak-clustering for Isepkwhose sepav is (D), we conclude that the approximation factor of any methodM M regarding sepav is O(1/k). Now, we consider sepmin. We have that sepmin(Ak) = 1. Let B = (B1, . . . , Bk) be a k-clusteringwith the following properties: (i) |Bi S3| 1 for each i k 2; (ii) each Bi, with i 2, hasexactly one point in S1 S2 (iii) Bk1 has a point in S1 and Bk has a point in S2. We have thatsepmin(B) is (D). Thus, any method M M has approximation O(1/D) to sepmin, that is, theapproximation is unbounded in terms of n.",
  "EProof of Theorem 5.4": "Proof. Let I be the instance obtained by augmenting the instance ICS, presented right after Theorem3.4, with the points x0, . . . , xt1, where dist(xi, Ai) = t + 1 + and for i = j, dist(xi, xj) =|pj pi| + 2(t + 1 + ) and dist(xi, Aj) = |pj pi| + t + 1 + . Consider t = k. We argue that the (k + 1)-clustering obtained by average-link for I consistsof the clusters (Bk1, {x0}, . . . , {xk1}). In fact, in its first steps average-link obtains the 2k-clustering (A0, . . . , Ak1, x0, . . . , xk1) since the distance between points in Ai is 0. In the nextk 1 steps, average-link does not make a merge involving a point xi because the average distanceof xi to any other cluster is larger k + 1 and, by Lemma 3.5, the average distance between Bi2 andAi is i + 1 k + 1. Thus, the execution of average-link for I merges the same clusters that aremerged in the instance ICS and, then, ends up with the (k+1)-clustering (Bk1, {x0}, . . . , {xk1}).",
  "presents our datasets with their main characteristics": "Figures (1)-(6) show the results obtained by single-linkage, complete-linkage andaverage-link, for all datasets and the different criteria considered in the paper. For a givendataset D, method M and criterion , the height of the bar is given by the average of mk for everyk considered in our experiments, where mk is the ratio between the value of criterion achievedby method M on dataset D divided by the best value for criterion , among those achieved bysingle-linkage, average-link and complete-linkageon dataset D.",
  "and 4 show the results for the experiment described in , when the Euclidean distanceis replaced with the 1 and norm, respectively. The observations made in also holdwhen these metrics are used": "Finally, we note that the variance of the results for average-link is small. Indeed, an entry (average)close to 1 (e.g. 0.96) cannot have an underlying large variance because 1 is the maximum possiblevalue for an entry. Since most entries for average-link are close to 1, one can conclude that thevariance of its results is usually small. In the supplemental material, we have .csv files with our fullresults. airfoil banknote collins concrete digits mice qsarfish tripadvisor vowel geomusic 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 max_avg singlecompleteaverage"
}