{
  "Abstract": "We present CALICO, a method to fine-tune Large Language Models (LLMs) tolocalize conversational agent training data from one language to another. Forslots (named entities), CALICO supports three operations: verbatim copy, literaltranslation, and localization, i.e. generating slot values more appropriate in thetarget language, such as city and airport names located in countries where thelanguage is spoken. Furthermore, we design an iterative filtering mechanism todiscard noisy generated samples, which we show boosts the performance of thedownstream conversational agent. To prove the effectiveness of CALICO, webuild and release a new human-localized (HL) version of the MultiATIS++ travelinformation test set in 8 languages. Compared to the original human-translated(HT) version of the test set, we show that our new HL version is more challenging.We also show that CALICO out-performs state-of-the-art LINGUIST (which relieson literal slot translation out of context) both on the HT case, where CALICOgenerates more accurate slot translations, and on the HL case, where CALICOgenerates localized slots which are closer to the HL test set.",
  "models such as encoder-only Transformers (Chen et al., 2019; Xu et al., 2020) are still useful for cost-and latency-sensitive applications that support very high throughput": "Synthetic Data Generation (SDG) from Large Language Models (LLMs) has become a popular trendto address the data scarcity problem (Rosenbaum et al., 2023). SDG approaches relevant to the ICand ST tasks include back-translation, (Bannard and Callison-Burch, 2005; Sennrich et al., 2016;Edunov et al., 2018; Xie et al., 2020) paraphrasing (Kumar et al., 2020; Cho et al., 2019; Malandrakiset al., 2019; Jolly et al., 2020; Panda et al., 2021) word replacement (Zhang et al., 2020; Dai andAdel, 2020; Wei and Zou, 2019), and carrier phrase regeneration (Kumar et al., 2022). A relatedthread is in-context generation of multilingual semantic parsing data (Rosenbaum et al., 2022a) andmulti-party dialogs (Chen et al., 2023). In terms of cross-lingual SDG for IC+ST, Machine Translation with Slot Alignment (MT-SA) is astrong baseline (Xu et al., 2020), however the separate alignment step a posteriori introduces noise,which negatively impacts the quality of the generated data and the downstream task model. Recently proposed LINGUIST (Rosenbaum et al., 2022b) avoids the alignment problem by firstmachine-translating the slot values (out-of-context entities like new orleans or december six-teenth), and then generating a slot-annotated utterance in the target language incorporating themachine-translated slot values. (An example of LINGUIST output is book a flight to [1 neworleans ] on [2 december sixteenth ], where 1 and 2 indicate slot labels to_city anddate respectively. )",
  "In this work, we propose CALICO for cross-lingual SDG of IC+ST training data, which resolves twoimportant limitations of LINGUIST (see ):": "(i) Contextualized Slot Value Translation: LINGUIST translates the slots a priori and out of context,which can lead to cascading errors, due to the translation model choosing the wrong grammaticalform in morphologically inflected languages, or choosing the wrong semantic translation altogether. (For example, light can be a noun, synonym of lamp; or an adjective, opposite of heavy; or averb.) By contrast, CALICO translates the slot values and carrier phrase text jointly, while producingthe same slot-annotated output format as LINGUIST to avoid the alignment problem of MT-SA. (ii) Slot Value Localization: in real-world systems, users are more likely to ask for entities specificto their locale, e.g. in German booking flights to or from kln on lufthansa instead of just askingfor English entities and their translations, like denver or a literal translation of united airlines.CALICO introduces a localization operator which instructs the model to replace the value in thesource language with a localized version of the slot while translating the rest of the text around it. To demonstrate the utility of slot value localization, we create a new human-localized (HL) versionof the MultiATIS++ test set in all 9 languages1, and benchmark CALICO compared to LINGUISTon the 6 languages shared with our data generation model. We show that our HL test set is morechallenging that the original human-translated (HT) test set. We also show that CALICO out-performsLINGUIST both on the original HT version, by producing more accurate slot translations with the fullsentence context, and on the new HL version by producing more relevant training data with localizedslot values like city and airport names. Furthermore, we improve the process of selecting from among the n-best generated CALICO outputs:instead of taking the output with lowest perplexity, we design an Iterative Filtering Mechanism (IFM)inspired by data augmentation through weak supervision (Chen et al., 2022). We use the downstreamtask model (IC+ST encoder fine-tuned on real data plus selected CALICO-generated synthetic data)to re-select from among the n-best outputs based on matching the intent and slots requested in theprompt. We show that the IFM improves the final IC+ST performance on the MultiATIS++ test set. In summary, our contributions are threefold: (1) We propose CALICO to localize IC+ST trainingdata with controls to either copy, translate, or localize slot values; (2) we create a new version of theMultiATIS++ non-English test set, which includes updated text and annotation with human-localizedslot values such as city and airport names, benchmark our models on it, and release the test set;and (3) we design an iterative filtering mechanism to select model generated data and show thatit improves IC+ST performance on the MultiATIS++ test set (both original and human-localizedversions) compared to selecting the output with lowest perplexity.",
  "CALICO Prompt Design": "The CALICO prompt () differs from LINGUIST by adding controls at the slot level for threeoperations: unchanged, indicating a verbatim copy (e.g. for flight numbers), literal translation,and localization, i.e. replacement with a value more appropriate in the target language. The translation operation of CALICO improves the slot translation quality compared to out-of-context MT applied a priori to LINGUIST. For example, without any context, the word secondin English could be reasonably translated to Spanish as either segundo, segunda, segundos,or segundas, depending on the plurality and grammatical gender of the Spanish noun it modifies.Furthermore, if second is part of the phrase the second of september, then it should be translatedas dos, meaning two. CALICO attends to the entire input English sentence when generatingtranslated values, and therefore can disambiguate such cases.",
  "Models": "We fine-tune AlexaTM 5B seq2seq model (Rosenbaum et al., 2022b; Soltan et al., 2022; FitzGeraldet al., 2022a) as the CALICO data generaiton model. For the downstream task IC+ST model anditerative filtering model, we fine-tune xlm-roberta-base (Conneau et al., 2020) (12 layers, 768hidden dimension), from the HuggingFace (Wolf et al., 2020) implementation.",
  "Datasets": "We fine-tune CALICO on cross-lingual prompts extracted from MASSIVE, which contains parallelIC+ST annotated data in 51 languages. We fine-tune on 6 languages: German, Spanish, French,Hindi, Japanese, and Portuguese, each parallel to English. After the CALICO data generation model is trained, we apply it on two IC+ST datasets, MultiATIS++and MultiSNIPS, to localize training data from English into the target languages. The CALICO modelhas never seen the specific intent and slot names, annotation scheme, or data conventions of the targetdownstream tasks, and therefore must generalize at inference.",
  "MASSIVE": "MASSIVE (FitzGerald et al., 2022b), Multilingual Amazon SLURP (SLU resource package) forSlot Filling, Intent Classification, and Virtual-Assistant Evaluation, contains 19,521 English realistic,labeled virtual-assistant utterances spanning 18 domains, 60 intents, and 55 slots. It is a paralleldataset, where each English utterance is localized or translated into 50 typologically diverse lan-guages. The dataset includes annotations on the human-chosen replacement method for each slot(i.e. translation or localization or unchanged) for each pair of English and parallel target languageutterance. Crucially, we use these slot-level annotations in the prompts for CALICO fine-tuning sothat the model learns to follow instructions like localization ().",
  "MultiATIS++": "MultiATIS++ dataset extends the English-only Air Travel Information Services dataset ATIS to 9languages via human translation. Our work focuses on the 7 languages MultiATIS++ shares with ourpre-trained model: English (EN), German (DE), Spanish (ES), French (FR), Hindi (HI), Japanese(JA), Portuguese (PT), with 4488 (HI: 1440) training utterances, 490 (HI: 160) validation utterances,and 893 test utterances, over 18 (HI: 17) intents and 84 (HI: 75) slots. (The test set release alsocontains Turkish and Chinese.) Since the original test set is human-translated from English, it contains only translated slot values(e.g. for new orleans in English, the Spanish version would be nueva orleans), but not localizedentities. To showcase the effectiveness of CALICO, we create a new version of the MultiATIS++test set in all 8 non-English languages, by asking human experts to localize the slot values for8 slot types. Specifically, the human experts replace the original human-translated English slotvalue (e.g. nueva orleans in Spanish) with a value more appropriate value in the target language(e.g. madrid). The localized slot types are airline_code, airline_name, airport_code,airport_name, city_name, country_name, state_code, state_name. We also ask the experts to modify the carrier phrase text as needed to make the entire requestgrammatically correct. For example, in French the experts might change the form of the definitearticle le, la, les, or l (all meaning the) to match the plurality, gender, and pronunciationof the newly chosen slot value. The rest of the slot types and text remain as they are in the original.",
  "Results": "MultiATIS++ Results are shown in Tables 1 and 2, where the upper block reports IC Accuracy,and the lower block reports ST F1 Score. is reported on the original test set, which containshuman-translated slot values, whereas is on our new version of the test set, where the slotsare human-localized to versions more common in the target language. Lower bound indicates the IC+ST model trained only on the English training data. Upper boundindicates the IC+ST model trained on MultiATIS++ training data for all 7 languages. The remainingcolumns indicate IC+ST models trained on the concatenation of original English training data withsynthetic training data for the other 6 languages from one or more methods. LINGUIST (our repro)is our reproduction of LINGUIST. CALICO (IFM) is our candidate approach, where we apply the localization, translation, andcopy operations to specific slot values as shown in (Appendix C), along with post-generationiterative filtering mechanism. CALICO (All Transl.) is our candidate model with the translationoperation applied to all slots at inference time. In CALICO (No IFM), we do an ablation setupon the iterative filtering mechanism, where we do not perform iterative filtering. And finally inthe column IFM Comb all, we include the synthetic data from CALICO (IFM) combined withLINGUIST (our repro) and CALICO (All Translate). We primarily focus on AVG non EN, which is the average across the non-English languages. Onthe original test set (), we find that CALICO (IFM) is the best performing single method onaverage, surpassing LINGUIST by 2.95 points absolute on IC (from 94.01 to 96.96) and 2.04 pointsabsolute on ST F1 (from 83.54 to 85.58). The improvement of CALICO over LINGUIST is reflected on most languages, however Japanese (JA)shows the most improvement, having +7.55 / +5.56 (from 89.30 to 96.85 / from 85.15 to 90.71) pointsimprovement on IC / ST. This suggests that Japanese was being limited the most by the drawbacks ofLINGUIST, e.g. making translation mistakes out of context. On the new localized test data (), both versions of CALICO improve over LINGUIST, and IFMimproves even further. With CALICO plus IFM, we improve on IC from 93.94 LINGUIST to 96.15(for 2.21 points absolute) and ST from 80.49 LINGUIST to 84.81 (i.e. 4.32 points absolute). As withthe HT test set, the improvement is particularly large for Japanese (JA).",
  "On both settings, we combine data from LINGUIST and both versions of CALICO, however find thatthe gains are not consistently synergistic": "We see the performance improvement of CALICO over LINGUIST is directly correlated with theSuccess Rate ( in Appendix B, filtering to keep only those outputs which pass string-matchingheuristics and IC hypothesis filtering), indicating that the data produced from CALICO is cleaner andmore usable than that of LINGUIST. All data generation models and even the Upper Bound of including human-translated training dataperform significantly worse on the test data with human-localized slots compared to the originalhuman-translated test data, indicating that the human-localized test set is more challenging, andmotivating future work on conversational agent localization. MultiSNIPS Results are show in . Here, in the AVG non EN there are small differencesoverall compared to LINGUIST: CALICO (All Translate) is 0.74 points absolute worse on IC (from98.62 to 97.88) and +1.55 points absolute better on ST (from 85.86 to 87.41). However, similarly tothe MultiATIS++ results, CALICO (All Translate) out-performs LINGUIST. All models are close tothe upper bound, however, indicating that this dataset may not be particularly challenging.",
  "Conclusion and Future Work": "We introduced CALICO, a novel pipeline for synthetic annotated data generation in new languages,via fine-tuning a largescale pre-trained multilingual seq2seq model. We demonstrated that unlikeprior techniques that would translate slots out of context, CALICO can generate annotated slots basedon the context and localize them with values more appropriate to the target language . In future, weplan to extend and leverage a reward model into a reinforcement learning setup to further improvethe quality of the generated data. We would also like to explore ways to combine the positive effectsof LINGUIST paraphrasing with CALICO localization. Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. InProceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05),pages 597604, Ann Arbor, Michigan. Association for Computational Linguistics. Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, YangLiu, Zhou Yu, and Dilek Hakkani-Tur. 2023. PLACES: Prompting language models for socialconversation synthesis. In Findings of the Association for Computational Linguistics: EACL 2023,pages 844868, Dubrovnik, Croatia. Association for Computational Linguistics. Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Andy Rosenbaum, Seokhwan Kim, YangLiu, Zhou Yu, and Dilek Hakkani-Tr. 2022. Weakly supervised data augmentation throughprompting for dialogue understanding. In NeurIPS 2022 Workshop on SyntheticData4ML.",
  "Alice Coucke, Mohammed Chlieh, Thibault Gisselbrecht, David Leroy, Mathieu Poumeyrol, andThibaut Lavril. 2019. Efficient keyword spotting using dilated convolutions and gating": "Xiang Dai and Heike Adel. 2020. An analysis of simple data augmentation for named entity recogni-tion. In Proceedings of the 28th International Conference on Computational Linguistics, pages38613867, Barcelona, Spain (Online). International Committee on Computational Linguistics. Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translationat scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing, pages 489500, Brussels, Belgium. Association for Computational Linguistics. Jack FitzGerald, Shankar Ananthakrishnan, Konstantine Arkoudas, Davide Bernardi, AbhishekBhagia, Claudio Delli Bovi, Jin Cao, RAKESH CHADA, Amit Chauhan, Luoxin Chen, AnuragDwarakanath, Satyam Dwivedi, Turan Gojayev, Karthik Gopalakrishnan, Thomas Gueudre, DilekHakkani-Tur, Wael Hamza, Jonathan Hueser, Kevin Martin Jose, Haidar Khan, Beiye Liu, JianhuaLu, Alessandro Manzotti, Pradeep Natarajan, Karolina Owczarzak, Gokmen Oz, Enrico Palumbo,Charith Peris, Chandana Satya Prakash, Stephen Rawls, Andy Rosenbaum, Anjali Shenoy, SalehSoltan, Mukund Harakere, Liz Tan, Fabian Triefenbach, Pan Wei, Haiyang Yu, Shuai Zheng,Gokhan Tur, and Prem Natarajan. 2022a. Alexa teacher model: Pretraining and distilling multi-billion-parameter encoders for natural language understanding systems. In KDD 2022. Jack FitzGerald, Christopher Leo Hench, Charith S. Peris, Scott Mackie, Kay Rottmann, A. Pa-tricia Domnguez Snchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, SwethaRanganath, L. Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and P. Natarajan. 2022b. Massive:A 1m-example multilingual natural language understanding dataset with 51 typologically-diverselanguages. ArXiv, abs/2204.08582. Shailza Jolly, Tobias Falke, Caglar Tirkaz, and Daniil Sorokin. 2020. Data-efficient paraphrasegeneration to bootstrap intent classification and slot labeling for new features in task-orienteddialog systems. In Proceedings of the 28th International Conference on Computational Linguistics:Industry Track, pages 1020, Online. International Committee on Computational Linguistics. Manoj Kumar, Yuval Merhav, Haidar Khan, Rahul Gupta, Anna Rumshisky, and Wael Hamza.2022. Controlled data generation via insertion operations for NLU. In Proceedings of the 2022Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies: Industry Track, pages 5461, Hybrid: Seattle, Washington +Online. Association for Computational Linguistics. Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. Data augmentation using pre-trainedtransformer models. In Proceedings of the 2nd Workshop on Life-long Learning for SpokenLanguage Systems, pages 1826, Suzhou, China. Association for Computational Linguistics. Nikolaos Malandrakis, Minmin Shen, Anuj Goyal, Shuyang Gao, Abhishek Sethi, and AngelikiMetallinou. 2019. Controlled text generation for data augmentation in intelligent artificial agents.In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 9098, HongKong. Association for Computational Linguistics. Subhadarshi Panda, Caglar Tirkaz, Tobias Falke, and Patrick Lehnen. 2021. Multilingual ParaphraseGeneration For Bootstrapping New Features in Task-Oriented Dialog Systems. In Proceedings ofthe 3rd Workshop on Natural Language Processing for Conversational AI, pages 3039, Online. Soham Parikh, Mitul Tiwari, Prashil Tumbade, and Quaizar Vohra. 2023. Exploring zero and few-shottechniques for intent classification. In Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 5: Industry Track), pages 744751, Toronto, Canada.Association for Computational Linguistics.",
  "Andy Rosenbaum, Saleh Soltan, and Wael Hamza. 2023. Using large language models (llms) tosynthesize training data. Amazon Science": "Andy Rosenbaum, Saleh Soltan, Wael Hamza, Marco Damonte, Isabel Groves, and Amir Saffari.2022a. CLASP: Few-shot cross-lingual data augmentation for semantic parsing. In Proceedings ofthe 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguisticsand the 12th International Joint Conference on Natural Language Processing (Volume 2: ShortPapers), pages 444462, Online only. Association for Computational Linguistics. Andy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese. 2022b. LIN-GUIST: Language model instruction tuning to generate annotated utterances for intent classificationand slot tagging. In Proceedings of the 29th International Conference on Computational Linguis-tics, pages 218241, Gyeongju, Republic of Korea. International Committee on ComputationalLinguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translationmodels with monolingual data. In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 8696, Berlin, Germany. Associationfor Computational Linguistics. Saleh Soltan, Shankar Ananthakrishnan, Jack G. M. FitzGerald, Rahul Gupta, Wael Hamza, HaidarKhan, Charith S. Peris, Stephen Rawls, Andrew Rosenbaum, Anna Rumshisky, Chandan Prakash,Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, and Premkumar Natarajan.2022. Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model. ArXiv,abs/2208.01448. Asa Cooper Stickland, Sailik Sengupta, Jason Krone, Saab Mansour, and He He. 2023. Robustificationof multilingual language models to real-world noise with robust contrastive pretraining.InProceedings of the 17th Conference of the European Chapter of the Association for ComputationalLinguistics. Association for Computational Linguistics.",
  "Gokhan Tur and Renato De Mori. 2011. Spoken Language Understanding: Systems for ExtractingSemantic Information from Speech. Wiley": "Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance ontext classification tasks. In Proceedings of the 2019 Conference on Empirical Methods in NaturalLanguage Processing and the 9th International Joint Conference on Natural Language Process-ing (EMNLP-IJCNLP), pages 63826388, Hong Kong, China. Association for ComputationalLinguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrickvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-artnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations, pages 3845, Online. Association forComputational Linguistics. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised dataaugmentation for consistency training. In Advances in Neural Information Processing Systems,volume 33, pages 62566268. Curran Associates, Inc."
}