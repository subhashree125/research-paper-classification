{
  "Google DeepMind, 2The University of Chicago": "Current RLHF frameworks for aligning large language models (LLMs) typically assume a fixed promptdistribution, which is sub-optimal and limits the scalability of alignment and generalizability of models.To address this, we design a general Open-Ended RLHF framework that casts alignment as an asymmetricgame between two players: (i) a creator that generates increasingly informative prompt distributionsusing reward signals, and (ii) a solver that learns to produce more preferred responses on promptsproduced by the creator. This framework of Evolving Alignment via Asymmetric Self-Play (eva), results ina simple and efficient approach that can utilize any existing RLHF algorithm for scalable alignment. evaoutperforms state-of-the-art methods on widely-used benchmarks, without the need of any additionalhuman crafted prompts. Specifically, eva improves the win rate of gemma2-9b-it on Arena-Hard from51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7% with SimPO, and from54.8% to 60.3% with ORPO, surpassing its 27B version and matching claude-3-opus. This improvementis persistent even when new human crafted prompts are introduced. Finally, we show eva is effectiveand robust under various ablation settings.",
  "| eva.Open-Ended RLHF viaAsymmetric Self-Play. The creator is theprompt generation policy and the solveris the response generation policy |": "SFTDPODPO-eva 8.55 8.70 8.85 Rating (0-10) 8.57 8.66 8.90 MT-Bench SFTDPODPO-eva Win rate (%) 41.3 51.6 60.1 Arena-Hard SFTDPODPO-eva Win rate (%) 38.39 51.68 55.53 AlpacaEval | Results. Our method eva achieves concrete perfor-mance gain especially on hard alignment benchmarks, withoutrelying on any additional human prompts. Here, we report re-sults for DPO-eva; see more in 4.1.",
  ". Introduction": "Long-lived artificial intelligence must deal with an ever-evolving, open-ended world, yet currently face con-straints in both the scale and quality of available data, and the growth rate at which new, useful information iscreated. High-quality human data, crucial for scaling large language model based intelligence, is projected torun out in the next few years (Villalobos et al., 2024); the quality of such data is also expected to stagnate:as those large models become more capable, they need to identify and solve new and increasingly complexchallenges, requiring training data beyond the capabilities of humans to create. This necessitates a newfundamental training mechanism, where models can continue to self-improve by generatively exploring andsolving new problems. We thereby investigate the research question below:",
  "solver": "| Pipeline: We generalize classical RLHF with open-ended RLHF, optimized with a creator-solvergame for self-improving language models. Our proposed eva strategically evolves prompt distributions with acreator policy, which synthesizes prompts with an easy-to-implement estimate, sample then evolve procedure;specifically, it estimates the informativeness for each prompt by how contrastive the self-generated responsesare to the prompt, from the reward signals it receives. The creator evolves new prompts from highly informativeprompts, which the solver uses for continual training. The solver and creator policy can share the same networkor operate independently. See more on our minimax-regret objective that drives the above design in 3. Many preference optimization algorithms (Christiano et al., 2017; Guo et al., 2024; Liu et al., 2023a; Rafailovet al., 2023; Zhao et al., 2023) have been proposed to improve the alignment of language models, however,they all default to fixed prompt training distributions. Such fixed training paradigm inevitably leads to: (i)generalization issues (models may underperform or hack on instructions that are insufficiently represented inthe fixed set) and (ii) efficiency issues (data annotation and model training are costly, however not all promptprovide the same utility; it is wasteful to invest in sub-optimal fixed set, while identifying informative promptsthrough human efforts is expensive and time-consuming) (Dubey et al., 2024; Team et al., 2023, 2024). The objective thereby should not only be to optimize over a specific, static distribution of prompts, yet todevelop an agent that can autonomously evolve the training data distribution for self-improvement, to alignwell across unseen, novel environments or tasks (instantiated by prompts). Thus, we develop eva (Evolving Alignment via Asymmetric Self-Play), as illustrated in . Centralto our approach is a game with the minimax-regret objective, achieved through alternating optimizationbetween creating prompts and solving them. The interplay encourages evolving curricula (Parker-Holderet al., 2022), potentially benefits both generalization and efficiency (see also 3.4). Orthogonal to manyrecent self-play studies in LLM alignment (Choi et al., 2024; Munos et al., 2023; Wu et al., 2024), eva isasymmetric (Sukhbaatar et al., 2017), with two policies of different goals:",
  "Our main contributions are summarized as:": "A new principle: We propose a generalized Open-Ended RLHF objective for aligning language models,which seeks to jointly optimize the prompt distribution and the response policy, thus incentivizes modelsto self-improve to generalize well on new, unseen tasks beyond the initial training prompt distribution foralignment, as in Definition 1. A new algorithm: To optimize the objective, we design a practical algorithm via asymmetric self-play,which is implemented through alternating optimization in a creator-solver game, and can be easilyplugged into any existing alignment pipeline, as in Algorithm 1. State-of-the-art performance: We empirically validate eva on public alignment benchmarks andpresent general strong performance improvement when plugged in with different preference optimizationalgorithms (i.e., DPO, SPPO, SimPO, ORPO). We also conduct extensive ablation studies that providefurther insights on choices of informativeness metrics, reward models and training schedules, as in 4.",
  "Scalable Alignment via Asymmetric Self-Play": "G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahri-ari, A. Ram, et al.Gemma 2: Improving open language models at a practical size.arXiv preprintarXiv:2408.00118, 2024. O. E. L. Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz, M. Jaderberg,M. Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808,2021. J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deepneural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligentrobots and systems (IROS), pages 2330. IEEE, 2017.",
  ". The Principle: Open-Ended RLHF for Joint Self-Improvement": "Intuition.Classical RLHF (cf., Eq. 1) optimizes over a static prompt distribution, meaning that the agentis only aligned to a fixed prompt set D, making it brittle when it is evaluated on new problems from theever-changing real world. Our Open-Ended RLHF breaks away from this static framework, with the goal todevelop an agent that generalizes well across unseen, novel environments (where the tasks entailed in promptsmay not have been explicitly encountered during training). To achieve this, we must design a new objectivethat agents can generate its own problems for self-improvement.",
  ".(8)": "Here, ref(x) represents an underspecified, potentially intractable probability distribution over possible tasks(instantiated via prompts) in the wild, as a realizable open-ended reference that covers the full diversity andcomplexity of tasks agents may encounter, not the initial static prompt set D. The joint regularization towardsref(x, y) captures the objective for agents to generalize on alignment in ref(x) with broader open-endedprompts, while being close to the SFT policy SFT(y|x).",
  ". The Mechanism: Asymmetric Self-Play via Creator-Solver Games": "Intuition.It can be hard to directly solve Eq. 7, due to (i) the intractability of the underspecified refer-ence (Dennis et al., 2020), (ii) the instability of joint differentiation (Goodfellow et al., 2014). We presenta heuristic of iterative alternating optimization by casting it as an asymmetric creator-solver game, thatimplicitly achieves the conceptual probability matching of KL[(x) ref(x)] by iteratively creating asequence of prompt distributions to expand over the task space.",
  "XxXRegret(x, Y|X).(10)": "To illustrate, open-ended RLHF allows for the creation of evolving prompt distributions that challenge theagent progressively for better generalization; the introduced minimax regret objective further adds robustnesson top of such evolving curricula by incentivizing agents to perform well in all cases. However, while it is often straightforward for the solver to minimize the regret (e.g., by direct policy opti-mization), the optimal policy remains unknown during optimization, thus regret as the decision signal is oftenintractable to the creator regret approximation is needed. We design the proxy below for creators regretapproximation (see H for further illustration), also as a metric for prompt informativeness:",
  ". The Creator Step: Estimate, Sample then Evolve": "Plainly, the creator finds most useful prompts and generate variants of them for preference optimization. Onemay relate this to evolution strategies (Schwefel, 1977) which find the most promising species, then mutate andcrossover, or to curriculum RL (Parker-Holder et al., 2022) which finds environments with high-regret levels,then edits within some distance. In this work, we do not seek a differentiable creator in this work; the creatoris implemented in three steps as in to approximate regret maximization. See more in H.",
  "Step 1: info() estimate the informativeness.For each x in the prompt set X, we generate responses,annotate rewards and estimate a informativeness metric to x by Eq. 11 (see also )": "Step 2: sample() weighted sampling for an informative subset.By using the informativeness metric as theweight, we sample an informative prompt subset Xinfoto be evolved later. This is similar to finding high-regretlevels in curriculum RL (Parker-Holder et al., 2022). Step 3: evolve() evolving for a proximal region of high-advantage prompts.Our algorithm is agnostic toand does not rely on any specific evolving method (see empirical evidence in D.1). We take EvolInstruct (Xuet al., 2023a) as a default off-the-shelf method, which conducts in-depth (i.e., adding constraints, deepening,concretising, complicating) and in-breadth evolving (i.e., mutation) for prompts. Specifically, we iterate overeach prompt in the Xinfo, where each one is evolved to multiple variations, then optionally mix the newlygenerated prompts with a uniformly sampled buffer from X to create X+1.",
  ". The Solver Step: Solve then Optimize": "This step is the classical preference optimization (Rafailov et al., 2023), where responses are generated andthe gradient descent is performed. Take the pointwise reward model setting as an example, for every prompt,we sample responses with reward annotated for each; we take the responses with the maximal and theminimal reward to construct the preference pairs, then optimize upon. This implicitly minimizes the regret tothe KL-optimal policy, which we present in more details at H. Put together, eva can unify existing iterative optimization pipeline (Tran et al., 2023) with a new creatormodule, which can either share the same network as the solver policy or operate independently.",
  ". Understanding the Informativeness Proxy in Different Intuitive Ways": "Learning potential.Our metric intuitively identifies the learning potential of a prompt by measuring the gapbetween the best and worst response to it from the solver. We reason, that prompts eliciting both high-rewardand low-reward outcomes, reflect learnable tasks where the model is capable of improving but has not yetmastered, thereby implying learning potential (cf., Jiang et al. (2021b)). Worst-case guarantees.The minimax-regret objective, by design, leads to solvers that perform robustlyacross the prompt space, thus gives the worst-case guarantee. While exact equilibrium may not be attainablewith approximation, our empirical results in 4.2.1 demonstrate robustness. Auto-curricula for the players.We visualize the curriculum induced by eva in E. With the stochastic policy,the advantage may be heuristically understood as the reward difference between a base solver and a referencesolver. Rather than optimizing separate solvers (Dennis et al., 2020), we sample multiple times from the samepolicy to create the pair. In this way, the creator is incentivized to produce new prompts that are just out of thecomfort zone of solvers (Chaiklin et al., 2003): For overly challenging prompts, both solutions perform poorly, leading to a low proxy. For overly easy prompts, the base solution already performs well, again giving a low proxy. The optimal strategy is to find prompts that are just beyond the solvers current capability. Auto-curricula inherent to Contrastive Optimization.Contrastive preference optimization generalizes DPOand a family of algorithms (c.f., Hejna et al. (2023); Rafailov et al. (2023); Tang et al. (2024)), many of whoselosses monotonically decrease as the contrastive ratio increases. Here, by Eq. 2 and Eq. 6, the contrastive ratiocan be written via the advantage-based proxy:",
  "min(x) = x; ref.(13)": "By our proxy, we implicitly incentivize the creator to generate prompts that bring the most contrastive responses,which decrease the loss the most. This matches the curriculum learning literature, which prioritizes (in eva,generatively prioritizes) examples with smaller losses for better convergence and generalization (Bengio et al.,2009). We hence suggest the Contrastive Curriculum Hypothesis: in contrastive preference optimization,prioritizing prompts with higher contrastive ratio improves sample efficiency and generalization. We showinitial empirical results on this in 4.2.1 and 4.2.4.",
  ". Experiments": "Datasets and models for training.We use UltraFeedback (Cui et al., 2023) as the training dataset, whichcontains diverse high-quality prompts that are primarily human-generated. We use the instruction-finetunedGemma-2-9B (Team et al., 2024) as the primary model, which is a strong baseline for models of its size.Detailed experimental setting can be found in A. Evaluation settings.We choose: (i) AlpacaEval 2.0 (Dubois et al., 2024), which assesses general instructionfollowing with 805 questions; (ii) MT-Bench (Zheng et al., 2023), which evaluates multi-turn instructionfollowing with 80 hard questions in 8 categories; (iii) Arena-Hard (Li et al., 2024b), which is derived from200K user queries on Chatbot Arena with 500 challenging prompts across 250 topics. We use gpt-4-1106 asthe judge and gpt-4-0314 as the baseline for win rate.",
  ". Main Results": "In general, eva brings notable gains in alignment without relying on any human-crafted data, thus offeringmore efficiency. In the base setup, building on the one-iteration finetuned model (01), eva adds a creatorto self-evolve the prompt set of the initial iteration and uses any preference optimization algorithm for anadditional open-ended RLHF iteration, resulting in 11",
  "We conduct in-depth ablation studies on eva, with findings below to be elaborated on later:": "4.2.1 - informativeness metric: our regret-based metric outperforms other alternatives. 4.2.2 - sample-then-evolve procedure: our method outperforms greedy selection. 4.2.3 - scaling w/ reward models: the alignment gain of eva scales with reward models. 4.2.4 - continual training : our method has monotonic gain with incremental training; the evolveddata and schedule by eva serves as an implicit regularizer for better local minima.",
  "Advantage as the informativeness metric outperforms baselines. As in , eva offers an effectivecurriculum by the advantage-based proxy as the informativeness metric (bottom row):": "Comparing w/ uniform evolving (brown): Existing baselines generate prompts in a uniform manner (Yuanet al., 2024) (cf., the principle of insufficient reason (Keynes, 1921; Tobin et al., 2017)). eva concretelyoutperforms, corroborating Das et al. (2024) that uniform learners can suffer from sub-optimality gaps. Comparing w/ other heuristics (blue): Prior practices (Team et al., 2023) tried heuristics like prioritizingprompts w/ the most variance in its rewards or w/ the lowest/highest average. We find our advantagebased methods (red) outperforms those heuristics; see F for more. Comparing w/ the inverse advantage (purple): Contrary to curriculum learning, a line of works conjecturethat examples w/ higher losses may be prioritized (Jiang et al., 2019; Kawaguchi and Lu, 2020), whichcan be done by inverting our metric. We find it significantly hurt the alignment gain, corroborating Min-dermann et al. (2022) that those examples can be unlearnable or irrelevant, meaning our curriculum iseffective and practical. Among our advantage variants (green): We designed variants of our default advantage-based metric, as in; the default min remains competitive among its peers. Together, the advantage-based principleprovides a robust guideline for prompt sampling and evolving. The lesson is that we must be selective about which are the promising to evolve, otherwise unlearnable, noisyor nave prompts may hinder learning. Our regret-inspired metric represents a solid baseline.",
  ":+ eva-sample (our default)60.18.909.048.7555.3555.53": "| Effect of evolving. The blue are those training w/ only the informative subset and w/o evolving); wedenote -sample for the default weighted sampling procedure in Algo 1, while using -greedy for the variantfrom the classical active data selection procedure (cf., a recent work (Muldrew et al., 2024) and a pre-LLMwork (Kawaguchi and Lu, 2020)), which selects data by a high-to-low ranking via the metric greedily. We showevolving brings a remarkable alignment gain (the red v.s. the blue); and as we evolve, sampling is more robustthan being greedy (cf., Russo et al. (2018)).",
  ". Scaling Pointwise and Pairwise Reward Models": "DPODPO-eva DPO-eva 54.0 54.5 55.0 55.5 56.0 56.5 LC Win rate (%) 55.01 55.35 56.12ArmoRM-8B SkyworkRM-27B SPPOSPPO-evaSPPO-eva 51.00 51.25 51.50 51.75 52.00 52.25 52.50 52.75 53.00 LC Win rate (%) 51.58 51.86 52.71PairRM-0.4B PairRM-8B | eva scales with quality of reward models,under pointwise RMs w/ DPO (left) and pairwise RMsw/ SPPO (right). Note SPPO handles general prefer-ences thus requires pairwise RMs, and DPO relies onthe Bradley-Terry assumption, for which pointwiseRMs are suitable. presents the length-controlled win rate ofeva on AlpacaEval using pointwise and pairwise re-ward models of varying scales. The results give aclear trend: as the quality of reward models improve,eva brings higher alignment gain. The scaling obser-vation shows the effectiveness of eva in exploitingmore accurate reward signals to choose informativeprompts for better alignment. One takeaway is inter-action w/ the external world is essential for intelli-gence. The more accurate reward signals observed,the better the agent incentivize themself to improve(cf., Silver et al. (2021)).",
  ". eva Improves Both Sample Efficiency and Generalization": "We continuously run the default incremental training (i.e., trainining from the last checkpoint w/ the evolvedset in each iteration), as in Fig 5 and D.2, eva presents monotonic performance gain over iterations, andsurpasses that trained w/ new human prompts, implying the generalization benefit. We conjecture that behaviors of the dashed/dotted lines relate to loss of plasticity (Abbas et al., 2023; Ash andAdams, 2019; Dohare et al., 2023; Nikishin et al., 2022; Xue et al., 2024). Classical works resolve it by theoptimization perspective (e.g., weight perturbing), whereas eva provides a new data perspective, potentiallymimicing an implicit regularizer for better generalization.",
  "|Continualtraining.eva stays robust w/ more iterationsin incremental training": "The solutions found by eva cannot be recovered by training longer w/a fixed distribution (the dashed), nor by navely sourcing new promptsw/o examining informativeness (the gray dotted), thus our generativedata schedule is effective. In , we ablate eva in scratch training, i.e., training w/ the full set(the evolved and the original data). eva is competitive in incrementaltraining, thus learns more effective with less data a nice bonus viaminimax regret (Jiang et al., 2021a).",
  ". Related Works": "Self-improving algorithms and iterative optimization.This line of work focuses on iteratively generatingsamples from the response policy and continuously re-training the policy by selected self-generated samples.Major works include ReST (Gulcehre et al., 2023; Singh et al., 2023), STaR (Zelikman et al., 2022), RFT (Yuanet al., 2023), RAFT (Dong et al., 2023), self-improving LLMs (Huang et al., 2022; Yuan et al., 2024); in thecontext of preference optimization, iterative DPO (Pang et al., 2024; Tajwar et al., 2024; Tran et al., 2023;Xiong et al., 2024; Xu et al., 2023b) has proven effective. Most works focus on self-training by improving inY | X, while we jointly optimize both responses and prompts via generative exploration in the (X, Y) space.Among them, we also distinctly present a game-theoretic framework for continual training. Prompt synthesis for language models.Existing works include Self-Instruct (Wang et al., 2022), Wiz-ardLM (Luo et al., 2023; Xu et al., 2023a), Self-Align (Sun et al., 2024), Glan (Li et al., 2024a), EvoPrompt (Guoet al., 2023), Magpie (Xu et al., 2024) and others (Long et al., 2024). eva is an orthogonal contributionsince any synthesis method can be plugged in as the evolve() for the creator. Importantly, our work presentsa new reward-related metric to endow prompt the notion of informativeness. We also focus on preferenceoptimization algorithms, while those existing works primarily use synthesized prompts in an SFT-only way. Self-play and curriculum RL.Agents trained on a fixed data distribution are often brittle and may struggleto adapt to the real world (Hughes et al., 2024a). Self-play (Goodfellow et al., 2014; Samuel, 1959; Silver et al.,2016) addresses this by having the agent learn through self-interaction, thus creating more diverse experiencesand automatic curricula. In asymmetric self-play, the paradigm centers on Alice proposing a task, and Bobdoing it (Beukman et al., 2024a; Dennis et al., 2020; Samvelyan et al., 2023; Sukhbaatar et al., 2017). Werevive the classical asymmetric self-play (Sutton et al., 2011) in optimizing language models. Unlike traditionalcurriculum RL (Parker-Holder et al., 2022), which renders environments by specifying levels (Dennis et al.,2020), our approach is generative by nature, as we directly generate states from auto-regressive languagemodels. We believe such generative exploration in the joint state-action space presents a new training paradigm. Self-play in RLHF.A growing line of research frames RLHF as a symmetric self-play game, where both playersare response players (Choi et al., 2024; Munos et al., 2023; Rosset et al., 2024; Wu et al., 2024). However, thesemethods still rely on a fixed prompt distribution thus is sub-optimal. In contrast, we solve this by asymmetricself-play, enabling evolving prompt distributions for more generalizable language agents. During our work,we notice one concurrent paper adopting the asymmetric two-player setup (Zheng et al., 2024), however(i) it applies to adversarial attack tasks instead of general alignment benchmarks, (ii) it is incompatible w/direct preference optimization, and (iii) it relies on the maxmin principle (which may produce unlearnableenvironments (Dennis et al., 2020)) instead of the minimax regret principle (Fan, 1953; Savage, 1951) as wedo. We also first precisely define the new problem of open-ended RLHF, generalizing over classical RLHF.",
  ". Concluding Remarks": "Future directions.eva defines a new paradigm for reinforcement learning from human feedback, openingup many new directions, e.g., (i) extending to differentiable creator policies, combining w/ other evolve()methods; (ii) evolving for more iterations w/ on-policy solvers like RLOO (Ahmadian et al., 2024); (iii) investi-gating exploration bonuses for distribution diversity and coverage, and the self-consuming loop (Gerstgrasseret al., 2024); (iv) extending the game with more modality (Bruce et al., 2024), and/or with more playersfor full automation (e.g., rewarders, critics, rewriters, verifiers, retrievers); (v) extending from alignment toreasoning (e.g., auto-conjecturing for theorem proving (Poesia et al., 2024) can be cast as an asymmetric game),or from the bandits to the trajectories w/ process reward models and hierarchical search for creator and solvergenerations; (vii) further scaling up w/ millions of prompts with continual RL training (Abel et al., 2024). Conclusions.eva is a new, simple and scalable framework for aligning language models, and can be pluggedinto any existing alignment pipeline. The primary takeaway may be that RLHF can be made open-ended: (i)self-evolving joint data distributions can bring significant gain (as shown across various optimization algorithms),and (ii) reward advantage acts as an effective metric informing the collection and creation of future promptsfor alignment. eva presents a new view of alignment by framing it as an asymmetric game between a creatorgenerating new and learnable states (i.e., prompts) and a solver producing preferred actions (i.e., responses).eva also incentivizes agents to create problems rather than to simply solve problems, which is a key feature ofintelligence, yet classical bandit/RL works often neglect.",
  "Acknowledgements": "We extend our sincerest gratitude to Bilal Piot for his thoughtful reviews and valuable advice on this paper.We are also grateful to Chenkai Kuang, Dustin Tran, Albert Webson, Hanzhao Lin, Clara Huiyi Hu, JeremiahLiu, Luheng He, Chenjie Gu, Yong Cheng, Pei Sun, and Heng-Tze Cheng for their fun discussions and notes onSelf-Play. We also thank David Abel, Yuxin Chen, Ziniu Hu, Guohao Li, Rylan Schaeffer, Haifeng Xu, ChaoqiWang and Yifei Wang for their initial helpful discussions and references on fine-tuning, contrastive learning,data synthesis, open-ended learning, and continual reinforcement learning.",
  "Reproducibility Statement": "We hope to open-source all the code, datasets (synthesized prompts/responses) and models, upon approval. Be-fore then, we are more than happy to provide any clarification requested to help re-implement eva and replicateour results. Our code base is made to be simple to use for practitioners, requiring only a creator module additionto the commonly adopted Alignment Handbook pipeline.",
  "Societal Impact": "eva enables scalable training of language agents through open-ended training, improving AI alignment withhuman values, which may ultimately contribute to social welfare (Arrow, 1952; Pigou, 1920; Zhi-Xuan et al.,2024). This could democratize the development of more generally capable AI agents, benefiting a wide range ofapplications from education to scientific discovery. We recognize that eva relies on self-exploration guided by reward signals. If these signals are inaccurate ormisaligned, the trained agents may exhibit undesirable behaviors such as reinforcing biased hallucination.Mitigating these risks requires continued research into robust reward models, transparent evaluation protocols,open collaboration within the AI research community, and more. We are committed to supporting these effortsby sharing our findings and implementations to promote open and responsible research and development.",
  "The appendix is organized as follows:": "A - Details On Reproducibility B - Plug-In Loss Functions Used In Main Results C - Extended Results for Experiments in the Main Paper D - Additional Experiments H - Illustration on Methodology E, F and I - Illustrations on Prompts, Responses and Relevant Distributions G - Additional Literature Review",
  "Hyperparameter () / Loss ()DPOORPOSimPOSPPO": "learning rate5e-75e-78e-75e-7learning rate schedulercosinecosinecosinelinear0.05/100.001//5//0.5//no. epochs per iter2116warmup ratio per iter0.10.10.10.1effective batch size88328max length2048204820481024max prompt length102410241024512optimizeradamwadamwadamwrmsprop Iterative Training Settings.By default (Tran et al., 2023; Yuan et al., 2024), we train with equal-size promptsubset in each iteration. Unless otherwise specified, we use 10K prompts from the UltraFeedback dataset (Cuiet al., 2023) per iteration. The incremental training proceeds as follows:",
  ": initialize with 01; trained with the prompt split X2 via by self-generated responses from theinitial model 01": "For evolving prompts (e.g., evolving X1 to X1), with the calculated informativeness metric for each prompt,we normalize them as the weight to do weighted sampling for a 25% informative subset to get Xinfo1. Wethen iterate over in Xinfo1and call EvolInstrut (Xu et al., 2023a) as the plug-in evolving method (withthe number of evolutions as 4) using the default mutation templates for (i) in-depth evolving (constraints,deepening, concretizing, increased reasoning steps) and (ii) in-breadth evolving (extrapolation) as imple-mented in tasks/evol_instruct/utils.py of distilabel==1.3.2. Next we uniformly select 80% promptsfrom this evolved dataset and 20% from the original dataset (i.e., the buffer) to form X1. We do not seekextensive parameter search (e.g., the number of evolutions, the evolving ratio) in this stage and encouragefuture works on exploring this and other plug-in evolving methods. For solver we generate 6 responses perprompt. We use 42 as the random seed.",
  "As an addition to , we have experimented with three different evolve() methods, including:": "SelfInstruct (Wang et al., 2022): Given seed prompts, variations are created based on criteria such asverb diversity and style blending (mixing interrogative and imperative styles). Unlike EvolInstruct (Xuet al., 2023a), which generates prompt variations sequentially, this approach generates independently.We follow the one-shot implementation in self_instruct.py of distilabel==1.4.1 and modified theinstruction on conciseness so that those newly generated prompts have similar lengths compared to theseed prompts. EvolQuality and EvolComplexity (Liu et al., 2023b): The two methods use the same evolutionaryapproach (i.e., sequentially generating), but with slightly different meta-instructions for prompt genera-tion, where EvolQuality asks to improve the quality (i.e., helpfulness, relevance, etc) of the seed promptand EvolComplexity asks to improve the complexity (i.e., increased reasoning steps, etc) of the seedprompt. We follow the implementation in evol_quality/utils.py and evol_complexity/utils.pyof distilabel==1.4.1.",
  "| Results of using different evolving methods": "eva is effective under different evolving methods.As shown in , our method brings strongperformance gain without training with additional human prompts. Among the experimented methods, wefind EvolComplexity shows better results. We believe the main strength of such method is its simplicity. Viewing the evolving process as x ( |x, meta_prompt), one can easily tune the meta prompt in natural language for improved performance. However,such simplicity comes at a price: (i) the main weakness is that the default method does not take environmentalfeedback into account (e.g., rewards received, verbal critique on responses, etc) and relies on the pre-definedmeta prompt, thus the evolving may be less directional; we encourage practitioners to consider incorporatingmore richer feedback during evolving (one way to formulate this is by generative optimization (Cheng et al.,2024; Nie et al., 2024; Yuksekgonul et al., 2024)); (ii) another weakness is that existing method is single-shot(i.e., we evolve based on a single x each time), thus the diversity of the generation may be limited weanticipate future works improving this with multi-shot evolving by graph-based sampling. In this regard, theevolving process can be viewed as {x}=1 ( | {x}=1, meta_prompt, env_feedback).",
  "| Results of using 60k prompts per iteration (DPO + length-penalized NLL loss)": "eva can bring robust gains with multiple iterations.As shown in , 12, and 13 below, our methodpresents persistent performance gain over iterations, and concretely surpasses the performance by default DPOtraining with true human prompts. However, there exist diminishing marginal gains in iterative off-policy training. We ground eva in the iterative(off-policy) preference alignment paradigm due to its efficiency and ease of integration. However, such paradigmsinherently face diminishing returns, where performance gains decrease with successive iterations, as previouslyobserved in (Nikishin et al., 2022; Setlur et al., 2024; Wu et al., 2024; Yuan et al., 2024). While the generativedata schedule in eva mitigates these challenges and extends beyond default training with human prompts(see also 4.2.4), the gains can weaken over iterations. We summarize potential reasons as: (i) the off-policysignal decay as the number of examples increases, signals from the off-policy data become weaker due todistributional shift; (ii) the loss of plasticity, where the agents ability to learn good policies decreases incontinuing training with more iterations (Nikishin et al., 2022); (iii) the ability of the solver as we evolvemore harder prompts, it is harder for the solver to produce preferred response (thus more explicit reasoningtechniques may be needed); (iv) the ability of the reward model to correctly provide reward signals toresponses and thus informativeness signals to prompts, as there may exists distributional mismatch.",
  "D.2.1. Bonus Experiments on rewriter() In The Loop": "We present the basic idea here for practitioners to build upon. The motivation comes from the hypothesesderived from D.2: as the prompts gets harder by evolving, there may be greater demands on the solverscapabilities compared to earlier iterations. As such, the solver may not be naively treated the same. One mayaddress this by either scaling up response sampling or introducing meta-instructions to explicitly enhance thesolvers reasoning. We design a proof-of-concept experiment w.r.t the latter by adding rewriter in evas solver step. Previously, asin Algo. 1 and 3.3.2, for each prompt x, we generate multiple responses, and choose the best as y+ and theworst as y for preference optimization. Now, we add one more rewriting step that attempts to enhance y+ tobe y+, by applying a rewriting instruction (Liu et al., 2023b) that asks the solver to alter y+ with imporvedhelpfulness, relevance, reasoning depths, creativity and details while keeping the similar length. We thentrain with (x, y+, y) for preference optimization. shows that adding the rewriter yields concreteperformance gains over the default training method, while keeping the training budget and only slightlyincreasing cost for offline data generation.",
  ": The weak correlation between reward variance and reward advantage shows that variancecannot serve as a substitute for advantage as a proxy for informativeness": "We have discussed the contrastive curriculum hypothesis in 3.4 to support using reward advantage in thesense that the induced samples tend to decrease the loss the most in the contrastive optimization. Furthermore,assuming the optimization algorithm can converge to the more optimal responses, neither reward mean norvariance directly capture the learning potential of such responses one may easily construct cases with identicalvariance yet differ much in reward range thus variance fails to distinguish such scenarios. By contrast, rewardadvantage estimate inherently captures the relative improvement towards better response, and is sensitive todifferences in reward range; variants of advantage estimate are commonly used in literature, and we discussunderlying principles in H.",
  "G. Extended Literature Review for Open-Ended Learning": "The design of our game-theoretic framework for language model post-training is inspired from many priorworks in open-ended learning. The central idea of open-ended learning is not to optimize for a specific, staticdistribution, but to develop an agent that can generalize well across unseen, novel environments, which are theenvironments that the agent has not been explicitly trained on. To achieve this, unsupervised environmentdesign proposes to generate environments that present a curriculum of increasing complexity for the agentto evolve, which ensures that the agents learning is not narrow, but broad enough to handle the diversity ofcomplexity of future environments. In such curriculum, as the agent solves simpler environments, it moveson to more difficult ones, thus progressively builds more sophisticated strategies. Furthermore, by adopting aminimax regret framework, this approach adds a layer of robustness by minimizing the agents performancegap in worst-case (i.e., most adversarial) environments. In addition to distinctions discussed in 5, we herelist several foundational works in this line, and encourage the LLM community to explore with more rigorand depth: Schmidhuber (1991) presents an initial investigation into open-ended learning via self-supervisedcuriosity-driven exploration; Wang et al. (2019) emphasize co-evolution of environments and agent policiesby training a population of agents that adapt to and solve progressively complex challenges; Dennis et al.(2020) formally introduce the notion of Unsupervised Environment Design (UED), where a protagonist andantagonist agent pair simulates regret by competing in shared environments, driving the protagonist (the mainlearner) to adapt to increasingly challenging scenarios; Jiang et al. (2021b) introduce Prioritized Level Replay(PLR), which uses a rolling buffer of high-regret levels to dynamically adjust the training curriculum, andselects levels with the higher learning potential; Parker-Holder et al. (2022) further propose improvements byediting previously high-regret levels; Hughes et al. (2024b) present a formal definition for open-ended systemwith respect to novelty and learnability, that generalizes various systems, e.g., AlphaGo (Silver et al., 2016),AdA (Team et al., 2021), etc.",
  "H. Extended Illustration on the Methodology": "This is an extended version of 3. In H.1, we re-present the open-ended RLHF principle in Definition 1,and discuss the intuition under the KL regularization. In H.2, we show heuristic approaches in open-endedlearning to approximate this objective, with a focus on minimax game formulation. In H.3, we formalizethe regret objective in our RLHF setting, and discuss the regret minimization for the solver and the regretmaximization for the creator.",
  "This KL (and preference optimization) is explicitly achieved in plug-in algorithms (e.g., DPO) inAlgo. 1. We later show how it relates to solvers regret minimization": "KL(x) ref(x): this probability matching term captures the intuition on optimizing (x) toapproach the conceptualized ref(x), in the sense that a language model optimizes itself by adaptingits training distributions with newly generated prompts for self-training to develop increasingly generalcapabilities, directing its learning towards informative, new tasks (Jiang, 2023), instead being constrainedin a static, pre-defined set of tasks. This conceptual KL is implicitly achieved by the creator step in the current eva setting by training ona sequence of informative prompt sets. We later show how it relates to creators regret maximization.As illustrated in 3.3.1, we start from the seed prompt set, choose those high-regret prompts andgenerate variations upon them by EvolInstruct, then mixing with a buffer of the original set toform the new training distribution at each iteration. This approach resembles classical open-endedlearning in H.2, and we hope it can serve as a small step for future works to build upon. A common misunderstanding among readers may be to confuse the open-ended reference ref(x)with the initial seed prompt distribution D, which is static. In contrast, ref(x) represents a broaderspace of tasks (e.g., user prompts in the real wild world), as a conceptual target derived from theunderspecified distribution (Dennis et al., 2020), i.e., an environment with free parameters thatcontrol. Lets use an illustrative example with : the prompt distribution may be defined alongseveral dimensions (e.g., the number or complexity of coding problems); a potential creator can bedesigned to modify these dimensions, steering the initial D to new training distributions, by certaindecision rules (e.g., minimax regret, which offers worst-case guarantees) that forms a sequence ofinformative prompts for training.",
  "H.2.1. The Asymmetric Game Formulation for Unsupervised Environment Design": "While we cannot directly train the agent with the intractable ref(x) of the open-ended world, it is possible tocurate a curriculum of prompt distributions to improve over the static distribution and support the continualtraining of the policy (|x), for it to keep improving and succeed over the full task space, thus conceptuallyapproaching ref(x). This is often framed as an asymmetric two-player game. Dennis et al. (2020) first formally define this problem as Unsupervised Environment Design (UED). The ideais that while the real-world environments are inexhaustible and hard to tract, there may exist some freeparameters (e.g., height and roughness in a maze) which one may control to generate new environments;UED then concerns about designing a distribution of those free parameters (i.e., settings) to create new fullyspecified environments, that can be used to train the agents. In this setup, one player, the creator, generates new environments based on some specific decision rules (seethe following), while the other player, the solver, optimizes its policy within these training environments, andthe process continues iteratively. Common heuristic strategies include:",
  "Maximin: the creator generates environments that minimize the solvers maximum possible reward,which can often lead to unsolvable scenarios (Khirodkar and Kitani, 2018)": "Minimax regret: The creator targets environments that maximize the solvers regret, defined as thedifference between the optimal return achievable and that of the solvers current policy (Beukman et al.,2024b). The regret is often conceived as the creators utility. Among them4, the minimax regret approach presents a sweet spot where the creator can create hard yetsolvable environments, and is often empirically better. The minimax regret strategy also implies that the agentspolicy is trained to perform well under all levels/settings, thus enjoys a worst-case guarantee. However, whileit is often straightforward for the solver to minimize the regret (e.g., through direct policy optimization, aswe discuss in H.3), the optimal policy remains unknown during the optimization process, thus regret asthe decision signal is often intractable to the creator which requires approximation (this is described as theAchilles heel of those curriculum RL methods by Parker-Holder et al. (2022)).",
  "Jiang et al. (2021a) further propose a double-creator setting based on (Jiang et al., 2021b), where one creatoris actively generating new environments, and the other is retrieving from the buffer": "Parker-Holder et al. (2022) propose to sample high-regret levels and generate new environments by makingedits on existing ones. The regret approximation is the same as (Jiang et al., 2021b) the positive value loss.For environment generation, the authors suggest a general editing/mutation mechanism, where the creatorchooses from high-regret levels and make small variations within an edit distance. There is an additionalfiltering step: they do not directly train on newly generated levels, but evaluate on those levels first, then addonly the high-regret ones to the training buffer.",
  "Regret minimization for the solver: we avoid calculating regret and use direct policy optimization(e.g., DPO) to equivalently achieve regret minimization": "Regret maximization for the creator: similarly to (Jiang et al., 2021b; Parker-Holder et al., 2022),we first find an approximation of regret, then curate new environments for the solver to train on by (i)sampling from a replay buffer of existing prompts, and (ii) making variations (through EvolInstruct (Xuet al., 2023a)) on those high-regret prompts. Specifically, we use advantage-based estimates of thecurrent policy, as summarized in . This asymmetric game serves as one potential modeling choice to implicitly achieve the open-ended RLHFprinciple that we proposed in Definition 1. We look forward to exploring more principled solutions in future.",
  "RegretKL(x, ) = y(|x)(x, y) yKL(|x)(x, y).(14)": "Regret Minimization for the Solver.It is rather straightforward/trivial to understand the objective of thesolver ( | x) as regret minimization, since the goal is to align the policy ( | x) with the KL-optimal solutionKL( | x), which directly minimizes the KL-regularized regret by design. This formulation allows flexibility inthe plug-in preference optimization algorithms for the solvers step in Algorithm 1, and ensures the alignmentproblem is well-defined. In practice, we use Direct Preference Optimization (DPO) and its variants, whichapproximate the KL-optimal solution by iteratively adjusting to reflect preference differences. Regret Maximization for the Creator.As discussed previously, while it is often trivial for the solver tominimize the regret through direct policy optimization, the optimal policy remains unknown during theoptimization process, thus we cannot directly calculate the regret we must approximate it when using it asthe utility for the creator. Similarly to heuristics proposed by prior works (Jiang et al., 2021a,b; Parker-Holderet al., 2022), we use the advantage-based estimate:",
  "ybaseline := arg miny (x, y) or ybaseline := avgy (x, y),(17)": "and {y}=1 is a set of responses sampled from ( | x) and (, ) is the reward oracle. We use arg miny (x, y)by default due to its simplicity and efficiency (see also 3.4 for additional interpretation) and consistent strongempirical gains we observed in vast experiments. As the policy optimizes, the proxy should approximate thetrue regret better. We leave exploration of other informativeness proxy designs in eva to future work. For new environment generation, as illustrated in 3.3.1, we start from the seed prompt set, choose thosehigh-regret prompts and generate variations upon them, then mixing with a buffer of the original set to formthe new training distribution at each iteration.",
  "I. Examples on Prompts and Model Generations": "| The initial prompt distribution of AlpacaEval by bart-large-mnli with 0-shot classification, whichis imbalanced. For the reward distribution, the category with lowest average reward has the highest rewardgap (i.e., the default informativeness proxy), implying the potential to improve. initial prompt Write me the code for a distributed transaction manager.\\nThinkstep by step and use pseudo code first.\\nThen, define interfacesfor all involved actors and entities.\\nUse Rational Unifiedapproach for this part.\\n\\nOnly then move on to the actualimplementation, class-by-class, and method-by-method.\\nMake thecode be implemented in C# and follow SOLID principles. evolved #1 Craft a suite of syntax for a distributed transaction coordinator.Start with a sequential breakdown in pseudocode format.Followingthat, establish the protocols for communication and interactionamongst the various participants and components, incorporatingthe Rational Unified Process methodology.\\n\\nProceed thereafter tothe concrete creation, detailing each class and function.Ensurethat the final C# code adheres to the precepts of SOLID and isannotated for clarification and maintainability purposes.evolved #2 Devise a comprehensive set of directives and structures for adistributed transaction coordinator architecture.Initiate byformulating a step-by-step algorithmic decomposition in pseudocode.Subsequently, delineate the frameworks for dialog and cooperationbetween the numerous entities and elements, utilizing the tenetsof the Rational Unified Process methodology.\\n\\nContinue to theactual generation of the code, meticulously detailing every classand corresponding method.Guarantee that the culminating C# codeis in strict compliance with SOLID principles and is supplementedwith descriptive commentary to enhance future clarity and upkeep,while also validating the code against a set of unit tests toensure robust functionality.evolved #3 Commence by constructing an algorithm in pseudocode thatmeticulously breaks down the sequential stages for a distributedtransaction coordinator system.Progress by outlining thecommunication protocols and collaborative mechanisms among themyriad components, adhering to the Rational Unified Processapproach.\\n\\nAdvance to crafting the code in C#, ensuring eachclass and method is elaborated with precision, aligning with SOLIDdesign principles.Integrate thorough comments for heightenedmaintainability and conduct comprehensive unit testing for assuredoperational integrity.evolved #4 Design a detailed flowchart that represents the intricatesteps of an orchestration engine for managing microservicesin a cloud-native environment.Begin by detailing the servicediscovery, load balancing, and fault tolerance strategies, whileincorporating the DevOps philosophy.\\n\\nProceed by developing theimplementation in Java, focusing on clean, modular code followingobject-oriented programming best practices.Accompany yourcode with clear documentation for easy understanding and performrigorous integration testing to guarantee robustness. | Examples of evolved prompts from AlpacaEval. We suggest practitioners to include generatedresponses with oracle rewards and self-critique in the context for better prompt evolving, which is helpful formitigating the effect of potential self-consuming loops in synthetic data training.",
  "* Since 5*k is a multiple of 4, theremainder when twice the number is dividedby 4 is 2.Let me know if youd like to try anotherexample!": "Heres how to solve this problem:1.Understand the first statement:- \"When a number is divided by 10, theremainder is 4\" means the number can berepresented as 10*k + 4, where k is aninteger.2.Double the number:- Twice the number is 2*(10*k + 4) = 20*k +83.Divide by 4:- When (20*k + 8) is divided by 4, the 20*kterm is perfectly divisible by 4.- The remainder will be the same as theremainder when 8 is divided by 4, which is0.Therefore, the remainder when twice thenumber is divided by 4 is 0.",
  "2nd-turn prompt: What about when twice the number is divided by 5?": "Answer from gemma-2-9b-it-dpoAnswer from gemma-2-9b-it-dpo-evaHeres how to solve that:1.We know:Twice the number is 20*k + 8(from the previous calculation).2.Divide by 5:(20*k + 8) divided by 5is (20*k)/5 + 8/5 = 4*k + 1.63.Find the remainder:Since 4*k is amultiple of 5, the remainder when twice thenumber is divided by 5 is 1.Let me know if you have any more numbertheory puzzles!",
  "Judgment (by gpt-4)": "Judgment for gemma-2-9b-it-dpoJudgment for gemma-2-9b-it-dpo-evaThe assistants response is incorrect.The assistant incorrectly calculated theremainder when twice the number is dividedby 5.The assistant calculated the decimalpart of the division operation, not theremainder.The correct calculation shouldbe (20n + 8) % 5 = (20n % 5) + (8 % 5) =0 + 3 = 3.Therefore, the remainder whentwice the number is divided by 5 is 3, not1 as the assistant stated. The corrected response would be:1.We know:Twice the number is 20*k + 8(from the previous calculation).2.Divide by 5:(20*k + 8) divided by 5gives us a remainder of (20*k % 5) + (8 %5) = 0 + 3 = 3.3.Find the remainder:The remainder whentwice the number is divided by 5 is 3.",
  "Generative adversarial nets. Advances in neural information processing systems, 27, 2014": "C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang,C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, and Y. Yang. Connecting large language modelswith evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023.",
  "S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and automaticcurricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017": "Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan. Principle-driven self-alignment oflanguage models from scratch with minimal human supervision. Advances in Neural Information ProcessingSystems, 36, 2024. R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: A scalable real-timearchitecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th InternationalConference on Autonomous Agents and Multiagent Systems-Volume 2, pages 761768, 2011. F. Tajwar, A. Singh, A. Sharma, R. Rafailov, J. Schneider, T. Xie, S. Ermon, C. Finn, and A. Kumar. Preferencefine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367, 2024. Y. Tang, Z. D. Guo, Z. Zheng, D. Calandriello, R. Munos, M. Rowland, P. H. Richemond, M. Valko, B. . Pires,and B. Piot. Generalized preference optimization: A unified approach to offline alignment. arXiv preprintarXiv:2402.05749, 2024."
}