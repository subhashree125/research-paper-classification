{
  "Abstract": "We study the problem of aligning large language models (LLMs) with humanpreference data. Contrastive preference optimization has shown promising resultsin aligning LLMs with available preference data by optimizing the implicit rewardassociated with the policy. However, the contrastive objective focuses mainly onthe relative values of implicit rewards associated with two responses while ignoringtheir actual values, resulting in suboptimal alignment with human preferences.To address this limitation, we propose calibrated direct preference optimization(Cal-DPO ), a simple yet effective algorithm. We show that substantial improve-ment in alignment with the given preferences can be achieved simply by calibratingthe implicit reward to ensure that the learned implicit rewards are comparable inscale to the ground-truth rewards. We demonstrate the theoretical advantages ofCal-DPO over existing approaches. The results of our experiments on a varietyof standard benchmarks show that Cal-DPO remarkably improves off-the-shelfmethods. Code is available at",
  "Introduction": "Aligning the behavior of large language models (LLMs) with human preferences is crucial forensuring that the responses of a pretrained LLM are aligned with human or societal values andpreferences . In recent years, reinforcement learning from human feedback (RLHF) hasbecome a standard approach for fine-tuning language models based on human preferences. RLHFinvolves first fitting a reward signal from human preference data and then using reinforcement learning(RL) algorithms such as PPO to optimize language models to generate responses with high reward. While RLHF shows impressive capabilities on diverse tasks ranging from programming to creativewriting, its training process is unstable and complex . This potentially worsens the samplecomplexity and compromises efficient convergence. To address these issues, offline contrastivepreference learning methods, which include DPO , IPO , and SLiC , have been proposed toreplace RLHF with supervised learning on the preference data. These methods eliminate the need forexplicit reward modeling by directly using the likelihood of the policy to define an implicit rewardfitted to the preference data, and achieve notable efficiency and competitive performance . While various contrastive preference learning methods employ different pairwise ranking losses,they share a common underlying motivation: Maximize the expected relative difference betweenthe implicit rewards associated with the chosen and rejected responses. Because the ranking lossis invariant to various score transformations (e.g., subtracting a constant), these methods tend toignore the absolute values of the rewards. Hence, while these methods learn to preserve the relativeordering between the likelihoods of the chosen and the rejected responses, they may reduce thelikelihood of the chosen response. illustrates this behavior and its implications. In the caseof DPO, a representative method of the contrastive methods, the likelihood of the chosen response",
  "arXiv:2412.14516v1 [cs.LG] 19 Dec 2024": ": The implicit reward dynamics during training of DPO and Cal-DPO on UltraFeedback data with thebase model Zephyr-7b-sft reveal that the rewards for rejected data continuously decrease, while the marginsbetween chosen and rejected data keep increasing. However, in DPO, the rewards for chosen data decreasebelow zero, whereas in our Cal-DPO , they keep increasing and remain positive. Our Cal-DPO significantlyoutperforms DPO across reasoning benchmarks. More results on other datasets are provided in . counter-intuitively continues to decrease despite remaining higher than the likelihood of the rejectedresponse. An undesirable consequence of this behavior is that the learned policy increases thelikelihood of unknown out-of-distribution responses, resulting in poor performance. Maximizing thelikelihood of the chosen response can be important in many practical applications, e.g., reasoning andmathematical problem solving , limiting the applicability of contrastive preference learning. The preceding discussion raises an important question with significant implications for how we alignLLMs with human preferences: How can we design a new objective that effectively alleviates thisproblem while ensuring that the learned policy theoretically converges to an optimal policy? Our answer to this question is Cal-DPO , a simple yet effective framework for preference learningwhich optimizes the contrastive preference objective to maximize the relative differences betweenimplicit rewards of chosen and rejected responses, while simultaneously ensuring that learned implicitrewards are calibrated to match the actual values of the ground-truth rewards (see .1 for aformal definition). The key intuition behind Cal-DPO is quite simple: If the implicit reward estimatesfrom preference data are well-calibrated relative to the ground-truth rewards (meaning both lie on thesame scale), we can prevent the likelihood (reward) of chosen responses from continually decreasing.Hence, Cal-DPO is designed to learn an implicit reward parameterized by the policy calibrated againstthe ground-truth reward. This can be achieved through a simple modification to the existing methods.For instance, Cal-DPO can be implemented on top of DPO with just one line of code and without anyadditional hyperparameters. Although we refer to our method as Cal-DPO , it notably generalizesto other preference optimization methods such as IPO and SLiC (see .3). In addition, wetheoretically demonstrate that Cal-DPO possesses several properties that are desirable for fine-tuningLLMs based on preferences, such as mode-seeking behavior, negative preference optimization, or\"negative gradient\" to push down the likelihood of undesirable responses . The main contributions of this paper are: (i) We propose Cal-DPO , a simple, effective, andintuitive framework for preference learning that facilitates alignment of language models. Cal-DPOaims to learn implicit reward functions for learning policy that are calibrated with respect to ground-truth rewards. (ii) We theoretically analyze the learning behaviors of Cal-DPO and prove thatCal-DPO is guaranteed to yield an optimal policy for preference learning. (iii) We present resultsof extensive experiments on a range of benchmark tasks, including controlled text generation ,summarization , dialogue generation , and several reasoning tasks that demonstrate thatCal-DPO consistently outperforms previous alignment methods for preference fine-tuning.",
  "Related Work": "Reinforcement Learning from Human Feedback (RLHF) is highly effective in aligning Large Lan-guage Models (LLMs) with human preferences . In RLHF, a reward model is trained fromhuman preference data to map responses to a scalar reward, which aligns a policy using RL algo-rithms like PPO . Although RLHF excels in instruction-following , safety alignment , andsummarization , it requires a more complex training pipeline than supervised learning. Recent work proposes simplifying RLHF by directly optimizing language models with contrastivelearning on preference data, resulting in contrastive preference learning methods , such asDPO , IPO , and SLiC , NLHF , and their variants that incorporate rejection sampling, e.g., RSO . While each of these methods works with different loss functions, the primary objectiveis to increase the likelihood gap between preferred and dispreferred responses . Other works apply contrastive preference learning iteratively and on-policy. However, as shown in ,the likelihood of the preferred response frequently decreases during contrastive training, adverselyimpacting performance on tasks such as coding and mathematical question answering . Thispaper addresses this limitation by calibrating contrastive objectives for preference learning andsupports our approach with extensive theoretical guarantees in practice. There is a substantial body of work that analyzes contrastive preference learning methods fromdifferent perspectives. For instance, study the performance of DPO under distribution shiftand show that DPO is more susceptible to out-of-distribution responses than PPO. theoreticallyanalyze DPO with noisy preferences. revisit the training objective of DPO from the per-spective of noise contrastive estimation . theoretically analyze the gradient vector field ofDPO and show that DPO decreases the probability of disfavored responses faster than it increasesthe probability of generating preferred responses. derive DPO within a token-level MDPformulation. Concurrent work shows that DPO employs a negative gradient to push down thelikelihood of undesirable, e.g., rejected responses, and implicitly exhibits a \"mode-seeking\" behavior.In this paper, our results are complementary. Specifically, we introduce Cal-DPO , which explicitlycalibrates the rewards learned by DPO to match the scale of ground-truth rewards, while exhibiting\"mode-seeking\" behavior by minimizing the reverse KL divergence similar to RLHF. Also worth mentioning is a body of work on calibration in supervised classification , learning-to-rank , unsupervised learning , and reinforcement learning . Recently, calibrationhas also been introduced into language models . In these works, calibration refers to thealignment of the models assessed confidence scores with the likelihood of its responses being correct.In contrast, our focus is on calibrating the learned implicit rewards in contrastive preference learningto match ground-truth rewards in aligning language models with human preferences.",
  "Notations and Preliminaries": "Problem Setup. We consider the preference learning scenario as follows: let the text sequencesx = [x1, x2, . . .] denote an input prompt, and yw = [y1, y2, . . .] and yl = [y1, y2, . . .] denote tworesponses, typically sampled from the reference policy ref(y | x). The response pairs are thenpresented to human labelers (or an oracle) who express preferences for responses given the prompt,denoted as yw yl | x, where yw and yl denote preferred and dispreferred responses, respectively.The preference distribution is commonly expressed using a latent reward model r(x, y) as:p (yw yl | x) = g (r(x, yw) r (x, yl)) ,(1)where g : R is a monotone non-decreasing function (with g(z) = 1 g(z)) that convertsreward differences into winning probabilities). When g is the sigmoid function (x) =1",
  "+ex , weget the Bradley-Terry (BT) preference model . Given dataset D, containing feedback (x, yw, yl),the goal is to learn a LLM policy (y | x) to align human preference by generating high rewards": "RLHF. Typically, given the reward function r(x, y), which dictates the human preferences, RLHFoptimizes policy for x to maximize reward with the following RL objective:max Ey(y|x) [r(x, y)] DKL [(y | x)ref(y | x)] ,(2) where > 0 is an appropriate KL penalty coefficient. Due to the discrete nature of languagegeneration, we typically optimize the RLHF objective in Equation (2) using RL algorithms, such asPPO . Although RLHF with PPO has achieved remarkable success, the training process of PPOis unstable because of the high variance of the policy gradient estimation . Reward Modeling. One standard approach to reward modeling is to fit a reward function r(x, y)using the BT preference model in Equation (1). Specifically, the reward function r(x, y) can beestimated by maximizing the log-likelihood of the preference feedback (x, yw, yl):LRM(; x, yw, yl) = log (r(x, yw) r (x, yl)) .(3)",
  ".(5)": "Technically, minimizing the DPO objective or any other pairwise contrastive objective, such asthose of IPO and SLiC , essentially amounts to maximizing the relative reward differencesbetween chosen and rejected responses as shown in . However, as noted earlier, these pairwiseranking objectives are not scale-calibrated and ignore the absolute values of the rewards. Thus, thelikelihood of the chosen response can continue to decrease during training as long as the relativedifference in the likelihoods between the chosen and rejected responses remains large (see ).This property has resulted in suboptimal performance, especially on reasoning and mathematicalproblem-solving . In this paper, we address this limitation by proposing a simple yet effectivecalibrated objective to calibrate the behavior of contrastive preference learning methods such as DPO.",
  "Calibrated Direct Preference Optimization": "We proceed to introduce Calibrated Direct Preference Optimization (Cal-DPO ), a simple andeffective modification of DPO. The key intuition behind Cal-DPO is to calibrate the implicit rewardfunction against the ground-truth rewards. Hence, Cal-DPO is designed to learn an implicit rewardparameterized by the policy calibrated against the ground-truth reward. This can be achieved througha simple modification of DPO , assuming, as in the case of DPO, that the preferences adhere to theBT preference model. Thus, Cal-DPO can be implemented on top of DPO with just one line of codeand without any additional hyperparameters. In principle, our idea of calibration can be applied toany contrastive preference learning algorithm such as IPO and SLiC (see .3).",
  ".(8)": "We can note that this pairwise loss is equivalent to the DPO objective (without ) in Equation (5).As this contrastive pairwise loss is also not scale-calibrated, ignoring the absolute values of therewards. Thus, we cannot guarantee that the estimated reward of the chosen response will increaseand the reward of the rejected response will decrease, which tends to degrade the performance onmath and reasoning benchmarks . We propose to address this limitation by explicitlyconstraining the implicit reward function log((y | x)/ref(y | x)) to a scale that matches theground-truth reward r(x, y)/. Intuitively, if the learned implicit rewards are constrained to lie in therange of the ground-truth reward, we can prevent the rewards of chosen responses from continuallydecreasing. Formally, we define \"calibration\" with respect to ground-truth reward as follows:",
  ".(9)": "The calibration loss requires access to oracle rewards; however, in some scenarios, we may only haveaccess to pairwise preference feedback. In such cases, we define the reward for preference feedbackas follows: r(x, yw) = 1/2 and r(x, yl) = 1/2, indicating yw yl | x. Empirically, we findthat this works quite well in practice. Combining this calibration loss with the BT preference loss inEquation (8), we get the following full loss of Cal-DPO on human preference data:",
  ",": "where our modifications to the standard BT preference model are straightforward and depictedin blue. Intuitively, Cal-DPO learns an implicit reward parameterized by the LM policy that is\"calibrated\" against the ground-truth reward. Specifically, Cal-DPO attempts to push the reward ofthe chosen response toward 1/2 and the reward of the rejected response toward 1/2, ensuringthat (yw | x) > ref(yw | x) and (yl | x) < ref(yl | x). This alignment keeps the estimate ofthe implicit reward consistent with the ground-truth reward. Our implementation of Cal-DPO buildsdirectly on the DPO codebase with just one additional line (see pseudocode in Appendix B.1). Ourexperiments show that calibration via a simple square loss can consistently improve the performanceof off-the-shelf DPO, demonstrating the potential of calibration for preference fine-tuning.",
  "Theoretical Analysis": "Next, we proceed to present a theoretical analysis of Cal-DPO . We show that Cal-DPO with rewardcalibration enjoys important properties that are desirable for fine-tuning LLMs with preferences, e.g.,mode-seeking behavior, negative preference optimization (\"negative gradient\" property) to push downthe likelihood of undesirable responses . We also show that Cal-DPO minimizes an upper boundon the standard KL-regularized RLHF in Equation (2). All proofs are provided in the Appendix 4.2.",
  "LMLE() = DKL [(y | x)(y | x)] = Eyref(y|x)exp(r(x, y)/)y ref(y|x) exp(r(x, y)/) log (y|x),(12)": "which is the weighted maximum likelihood loss. Although this objective provides a straightforwardapproach for preference fine-tuning , it leads to poor performance compared to RLHF, asshown by . The poor performance of the preceding objective in Equation (12) relative to RLHF isdue to the fact that it assigns a positive weight to all samples y for a given x. Since the rewards arealways positive, the likelihood loss always tries to increase the probability of a response even if itreceives a much smaller reward compared to other responses. We note that the recent work of also empirically demonstrates that the maximum likelihood criterion lacks the negative gradientproperty, resulting in poor performance of MLE in Equation (12) compared to RLHF and DPO. Because KL-regularized RLHF and the MLE objective in Equation (12) are both population losseswhich involve the oracle reward r(x, y), we also define a population loss for Cal-DPO to facilitatea direct comparison between Cal-DPO with KL-regularized RLHF and the MLE loss of Equation(12):",
  "y ref(y|x)((y|x)/ref(y|x)),": "This theorem establishes the relationship between the objectives of MLE and our Cal-DPO . Notably,the theorem shows that the first term in Cal-DPO also minimizes forward KL divergence with negativegradients. Specifically, we examine this update rule to illustrate how our objective produces a negativegradient . If we sample response y, and w(x, y) w(x, y) is positive (which happens more oftenwhen the reward r(x, y) is high), the update rule will increase the log-probability of this response.This leads to an increase in the probability of generating a response with a high reward. Conversely,if w(x, y) w(x, y) is negative (which occurs more often when r(x, y) is low), we decrease theprobability of response y, while increasing the probability of other responses due to normalization.This implies that Cal-DPO also exhibits a form of the negative gradient . Theorem 1 shows that the first contrastive term in Cal-DPO minimizes forward KL divergencesimilar to MLE but with negative gradients. In practice, minimizing either KL divergence resultsin policies with distinct properties due to limited data coverage . Specifically, forward KLDKL[] promotes mode-covering behavior, whereas reverse KL DKL[] encourages mode-seeking behavior . In other words, forward KL encourages all responses in datasetsto have equal probability, resulting in an overestimation of the long tail of the target distribution,whereas reverse KL sharpens the probability mass on certain high-reward regions. Thus, alignmentcommits to generating a certain subset of high-reward responses, which is more effectively realizedby minimizing the reverse KL, as the RL objective in Equation (11) does. This also explains whyMLE, which optimizes forward KL, performs worse than RLHF with reverse KL as shown in .",
  "LRL() = DKL [(y | x)(y | x)] log Z(x) LCalDPO() log Z(x).(15)": "This theorem shows that, unlike DPO and MLE, Cal-DPO with calibration asymptotically minimizes areverse KL divergence with respect to the policy, making it mode-seeking like RLHF. The first term,i.e., preference loss in Cal-DPO , corresponds to optimizing the forward KL as shown in Theorem 1.Hence, our proposed calibration loss can be understood as minimizing the gap between forwardKL and reverse KL. Theorem 2 also implies that DPO, RLHF, and Cal-DPO asymptotically convergeto the same global optimal policy in the limit given a sufficiently large dataset and model capacity.Thus, our calibration objective in Equation (9) is similar in some respects to RLHF. presents acomparison of the different methods in terms of strengths and weaknesses compared to Cal-DPO .",
  "Generalizations and Extensions": "We observe that our approach to calibrating the learned implicit reward from preference feedbackis not limited to the DPO algorithm or the BT preference model. As we shall see below, the generalidea behind Cal-DPO extends to other methods such as IPO and SLiC and other preference models.Instead of the sigmoid loss in Equation (7), SLiC minimizes a pairwise hinge loss:",
  "LIPO() = (h (x, yw, yl) 1/2)2(17)": "A potential advantage of SLiC and IPO over DPO is that they do not require that the preference modelbe BT and can work with general preference probabilities. By combining our calibration loss inEquation (9), it is straightforward to define the calibrated counterparts of both LSLiC and LIPO. Thus,our calibration approach can be generalized to work with SLiC and IPO as well (see .5).",
  "Experimental Setup": "Datasets. We evaluate Cal-DPO on four widely used datasets for preference fine-tuning: theUltraFeedback Binarized dataset , Reddit TL;DR summarization dataset , Anthropic-HHdataset , and the IMDb sentiment dataset . Details of the datasets are provided in Appendix C.1. Tasks and Evaluation. Following previous work , we evaluate methods fine-tuned on theUltraFeedback Binarized dataset across general reasoning benchmarks (MMLU-PRO , ARC ,IFEval , BBH , GPQA ), and mathematical reasoning (GSM8K and MATH ).For training on the UltraFeedback Binarized dataset, we utilize the same chat template used in for all methods. We also use AlpacaEval 2.0, a benchmark for assessing LLM alignment withhuman preference. The Anthropic HH dataset is used for dialogue generation to produce helpfuland harmless responses . For summarization, we use the Reddit TL;DR dataset. For the dialoguegeneration and summarization tasks, we use GPT-4 for zero-shot pairwise evaluation, which isconsistent with human judgments (see prompts in Appendix C.2.1). In the IMDb sentiment dataset,the goal is controlled text generation to produce positive sentiments from movie review prefixes .We train a binary sentiment classifier and define the oracle reward as its log odds, and evaluate thepolicy on the trained reward model . The task and evaluation details are given in Appendix C.2. Models. For summarization and dialogue generation tasks, we use Pythia-2.8b as our basemodel and the model after SFT as the reference model following . For the IMDb controlled textgeneration task, both the policy and reward models are initialized from the GPT-2 Large model .For tasks with the UltraFeedback Binarized dataset, we use the Zephyr-7b-sft model as ourbase model to ensure alignment with previous work on LLM preference alignment . Baselines We compare Cal-DPO with the following preference optimization methods: DPO ,IPO , SLiC , CPO . We also compare Cal-DPO with other variants of DPO: f-DPO ,DPO-Positive (DPOP) and DPO+NLL which combine DPO loss over preference pairs andthe negative log-likelihood (NLL) loss over chosen responses. We also compare with weighted MLEin Equation (12). Besides DPO, we also implemented our calibration objective on top of SLiC and IPO (see .5). The implementation details are provided in Appendix B.1.",
  "Performance Comparison on Benchmarks": "Reasoning Benchmarks. compares the performance of Cal-DPO against other alignmentmethods on the UltraFeedback Binarized dataset. Our results show that Cal-DPO exhibits remarkableeffectiveness in enhancing DPOs performance. The improvements are particularly notable on theIFEval and Math benchmarks, with relative gains exceeding 63.1% and 12.5% compared to the bestbaseline, respectively. These findings underscore the efficacy of Cal-DPO . We hypothesize that theseimprovements can be attributed to the calibration of implicit rewards performed by Cal-DPO as partof its training objective. Without proper calibration, the likelihood of selected samples decreases,resulting in suboptimal performance, especially in mathematical reasoning tasks, where the chosen : Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarizeddataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook acrossvarious reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).",
  "Cal-DPO75.6159.3767.4973.5264.6169.07": "responses are very likely the ground-truth answers. Furthermore, Cal-DPO outperforms CPO, whichcombine DPO over preference pairs with the negative log-likelihood loss over the chosen response.We hypothesize that the superiority of Cal-DPO over CPO can be attributed to the conservativenature of the objectives optimized by the latter, which only affects the chosen response. In contrast,Cal-DPO calibrates implicit rewards for both chosen and rejected responses. Instruction-following Benchmarks. To assess the ability of Cal-DPO on align with human instruc-tion, we compare the performance of Cal-DPO and DPO on AlpacaEval 2.0 , an evaluator basedon GPT-4 (version gpt-4-1106-preview) that produces the probability of preferring the evaluatedmodel. shows the comparison in terms of both normal and length-controlled (LC) percentageof wins. We see that Cal-DPO demonstrates steady performance gains with the number of trainingiterations and outperforms SFT and DPO methods, which tend to produce longer responses.",
  "Cal-DPO0.64532.31": "We also designed experiments to explore learning from realhuman preferences, focusing on summarization and dialoguegeneration tasks. Specifically, we used the Reddit TL;DRdataset for summarization and the Anthropic-HH datasetfor dialogue generation. summarizes the GPT-4evaluation results. These results show that Cal-DPO demon-strates a notable improvement over DPO and its variantscompared to both the SFT and the chosen responses. Re-markably, Cal-DPO aligns better with human preferencesthan baselines, achieving win rates of at least 60% againstthe chosen responses. This indicates that Cal-DPO showsstrong promise in terms of aligning with human preferences.Additionally, we provide examples generated by both DPO and Cal-DPO for both tasks in Ap-pendix D.1. These examples show that GPT-4 consistently prefers Cal-DPO over baselines and thechosen responses in the dataset, demonstrating that Cal-DPO significantly improves DPO in terms ofhelpfulness and harmlessness of the generated responses. : (Left two) The training dynamics of DPO and Cal-DPO on the TL;DR Summarization dataset.(Right) The performance of SLiC and IPO, and their calibrated counterparts Cal-IPO and Cal-SLiC. We provideadditional results on the Anthropic-HH and IMDb datasets in Appendix D.5.4Performance Comparison on Controlled Evaluation SFTDPOCal-DPO",
  ": AlpacaEval 2.0 evaluation resultsof models trained with UltraFeedback Bi-narized dataset. The DPO and Cal-DPOare both initialized from the SFT modelzephyr-7b-sft-full": "We conducted experiments on the IMDB dataset to assessthe generation of positive movie reviews. The task requiresthe model to provide positive and fluent completions ofmovie reviews based on given partial input texts. To per-form a controlled evaluation, we trained a binary sentimentclassifier on the IMDB dataset and defined the oracle re-ward as its log odds, following . The reward score ofthe reward model then serves as an in-domain proxy forthe unknown ground-truth reward used in evaluation. Theresults of IMDB sentiment generation are listed in .We used the reward score of the reward model and the per-plexity of GPT-2 to assess alignment performance. Weobserve that (1) PPO, DPO, and Cal-DPO can align theSFT model with the preference of the reward model, asevidenced by the increasing reward score, and (2) Cal-DPO performs better in terms of reward scoreand perplexity than both PPO and DPO, which confirms our theoretical results that the policy trainedby Cal-DPO can effectively maximize rewards.",
  "Further Analysis": "Training Dynamics. We also investigated the reward patterns during the training process of Cal-DPO. presents the reward patterns for Cal-DPO and DPO on the TL;DR summarization dataset.We observe that the rewards of the rejected data keep decreasing, and the margins between the chosenand rejected responses keep increasing. However, in the case of DPO, the rewards of the chosenresponses fall below zero, whereas they continue to increase with Cal-DPO , underscoring the utilityof reward calibration in LLM alignment. These results are similar to those on the UltraFeedbackdataset shown in , verifying our motivation and the effectiveness of Cal-DPO . Generalization to other objectives. As mentioned in .3, our approach to calibrating thelearned implicit reward from preference feedback generalizes in a straightforward manner to otherpairwise preference optimization methods including IPO and SLiC. To show this, we implementedthe calibration objective of Cal-DPO for IPO and SLiC, yielding their calibrated counterparts Cal-IPOand Cal-SLiC, respectively. shows the comparison on the Anthropic-HH dataset. We observethat combining our calibration objective can consistently improve standard OCPL methods for LLMpreference alignment, demonstrating the broader utility of preferance calibration. Coefficient Parameter. We investigate the effect of the coefficient and present an ablation study toanalyze the performance of Cal-DPO on various tasks by varying . in the Appendix showsthe performance with different values of . We observe that plays an important role in Cal-DPO .A small typically improves the model performance, whereas a too large encourages the policy toremain close to the reference policy, leading to poor performance.",
  "We have presented Cal-DPO , a simple yet effective fine-tuning approach for aligning LLMs withhuman preference. Cal-DPO incorporates a simple preference calibration term that modifies the": "behavior of the objective of contrastive preference learning methods such as DPO. The key ideabehind Cal-DPO is to ensure that the learned implicit rewards lie within the same range as the ground-truth rewards, yielding substantial gains in performance relative to the state-of-the-art baselines onseveral widely used benchmark data sets. We also demonstrate theoretically that Cal-DPO exhibitsthe desirable negative gradients and mode-seeking behavior. A limitation of Cal-DPO is that it is currently limited to offline methods and does not consideron-policy learning where the policy can interact with the reward model during learning. It would beinteresting to explore how the reward calibration idea used in Cal-DPO performs in the on-policylearning scenario. We leave this as an interesting direction for future work. The work of Honavar and Xiao was supported in part by grants from the National Science Foundation(2226025, 2225824), the National Center for Advancing Translational Sciences, and the NationalInstitutes of Health (UL1 TR002014). Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, DawnDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmlessassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models tofollow instructions with human feedback. Advances in neural information processing systems,pages 2773027744, 2022. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, AlecRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.Advances in Neural Information Processing Systems, 33:30083021, 2020.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximalpolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017": "Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, LarryRudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A casestudy on ppo and trpo. In International Conference on Learning Representations, 2020. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, andChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.Advances in Neural Information Processing Systems, 36, 2024. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland,Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learningfrom human preferences. In International Conference on Artificial Intelligence and Statistics,pages 44474455. PMLR, 2024.",
  "Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425,2023": "Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie,Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leveragesuboptimal, on-policy data. arXiv preprint arXiv:2404.14367, 2024. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White.Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprintarXiv:2402.13228, 2024. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, HuiminChen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preferencetrees. arXiv preprint arXiv:2404.02078, 2024. Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and ChristopherPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meetingof the association for computational linguistics: Human language technologies, pages 142150,2011. Michael Vlske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learnautomatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization,pages 5963, 2017. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, CharlesFoster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, NiklasMuennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A frameworkfor few-shot language model evaluation, 12 2023. Rmi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland,Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al.Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, andJialu Liu. Statistical rejection sampling improves preference optimization. In The TwelfthInternational Conference on Learning Representations, 2023.",
  "Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sam-pling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprintarXiv:2312.11456, 2023": "Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, andTengyang Xie. Direct nash optimization: Teaching language models to self-improve withgeneral preferences. arXiv preprint arXiv:2404.03715, 2024. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexan-dre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment fromonline ai feedback. arXiv preprint arXiv:2402.04792, 2024. Teng Xiao, Yige Yuan, Zhengyu Chen, Mingxiao Li, Shangsong Liang, Zhaochun Ren, andVasant G Honavar. Simper: Simple preference fine-tuning without hyperparameters by perplexityoptimization. arXiv, 2024.",
  "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neuralnetworks. In International conference on machine learning, pages 13211330, 2017": "Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky, and Marc Najork. Scale calibration ofdeep ranking models. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pages 43004309, 2022. Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, and StefanoErmon. Calibrated model-based deep reinforcement learning. In International Conference onMachine Learning, pages 43144323. PMLR, 2019. Mitsuhiko Nakamoto, Simon Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn,Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient onlinefine-tuning. Advances in Neural Information Processing Systems, 36, 2024.",
  "Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. themethod of paired comparisons. Biometrika, pages 324345, 1952": "Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rmi Munos, MarkRowland, Pierre Harvey Richemond, Michal Valko, Bernardo vila Pires, and Bilal Piot.Generalized preference optimization: A unified approach to offline alignment. arXiv preprintarXiv:2402.05749, 2024. Teng Xiao, Mingxiao Li, Yige Yuan, Huaisheng Zhu, Chao Cui, and Vasant G Honavar. How toleverage demonstration data in alignment for large language model? a self-imitation learningperspective. arXiv preprint arXiv:2410.10093, 2024.",
  "Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with areference-free reward. arXiv preprint arXiv:2405.14734, 2024": "Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts,Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforcedself-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat:Advancing open-source language models with mixed-quality data. In The Twelfth InternationalConference on Learning Representations, 2023. Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael IJordan, and Jiantao Jiao. Fine-tuning language models with advantage-induced policy alignment.arXiv preprint arXiv:2306.02231, 2023.",
  "Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang. Decoupledself-supervised learning for graphs. Advances in Neural Information Processing Systems, pages620634, 2022": "Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, ZhiyuanLiu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback.arXiv preprint arXiv:2310.01377, 2023. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, YounesBelkada, Shengyi Huang, Leandro von Werra, Clmentine Fourrier, Nathan Habib, et al. Zephyr:Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust andchallenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574,2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoningchallenge. arXiv preprint arXiv:1803.05457, 2018. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, DennyZhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprintarXiv:2311.07911, 2023. Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung WonChung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,2022. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, JulienDirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&abenchmark. arXiv preprint arXiv:2311.12022, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers tosolve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, DawnSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.arXiv preprint arXiv:2103.03874, 2021.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.Language models are unsupervised multitask learners. OpenAI blog, page 9, 2019": "Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Ken-ton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundariesof llm performance in machine translation. arXiv preprint arXiv:2401.08417, 2024. Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl:Generalizing direct preference optimization with diverse divergence constraints. In The TwelfthInternational Conference on Learning Representations, 2023.",
  ")2.(41)": "We begin by exploring the distinction between forward and reverse divergences. Before that, weintroduce a few definitions and some background on Bregman divergence. For any two points r Fand s F, the Bregman divergence DF, specified by the convex differentiable potential functionF : F R, is defined as follows :",
  "B.1Algorithm and Implementation Details": "The pseudocode for Cal-DPO is provided in Algorithm 1. For the general hyperparameters, weclosely followed the configurations used in . The of Cal-DPO is searched from [1e-3, 2e-3,3e-3, 1e-2, 1e-1], the batch size for all methods is 128, and we use the RMSprop optimizer with alearning rate of 5e-6. We linearly warm up the learning rate from 0 to 5e-6 in 150 steps. The samplingtemperature is set to 1 for all experiments. The experiments on are run on 4 Nvidia A100 GPUs withBF16 precision. We thoroughly tune the hyperparameters for each baseline following .",
  "return cal_dpo_losses": "is then used to assign a score to each completion based on criteria like helpfulness and honesty. Itconstructs binary preferences from UltraFeedback by selecting the highest mean score as the chosenresponse and one of the remaining three at random as the rejected response. Anthropic-HH : This Anthropic Helpful and Harmless dialogue dataset 2 contains 170k dialoguesbetween a human and an automated assistant. This dataset was utilized for assessing single-turndialogue performance. Each of the 170k dialogues comprises a human query and paired modelresponses rated for helpfulness and harmlessness. Following DPO , the preferred responses fromthis dataset were utilized for the supervised Fine-Tuning (SFT) phase, aligning the initial modelbehavior with desirable conversational outcomes. Reddit TL;DR summarization : This dataset 3 is a compilation of forum posts from the popularsocial media platform Reddit, specifically curated for summarization tasks with preference labels.Following , we use the same filtered version of this dataset to train the SFT policy and usetheir preference labels for the following alignment stage.",
  "We consider the following tasks to evaluate the model fine-tuned on real human preference data": "Single-Turn Dialogue: We employ the Anthropic HH dataset, which includes 170,000 interactionsbetween humans and virtual assistants. Our model is optimized to deliver useful and accurate answersacross a range of queries while deliberately avoiding any harmful content. Summarization: We leverage the Reddit TL;DR dataset, where the prompt represents a Reddit forumpost and the preference pairs are collected by previous works. The task objective is to generate asummary that captures the main points of the post.",
  "For single-turn dialogue and summarization tasks, we query GPT-4 for zero-shot pair-wise evaluation,which has been shown to be consistent with human judgments": "Controlled Sentiment Generation: We use the IMDb dataset , which contains 25,000 reviews,where the prompt is a prefix of a movie review and the preference pairs are generated followingprevious works. The task objective is to generate a positive sentiment y based on the movie reviewprefix. To perform a controlled evaluation, we train a binary sentiment classifier on the IMDb datasetand define the oracle reward as its log odds, following . The reward score of the reward modelthen serves as an in-domain proxy for the unknown ground-truth reward used for evaluation. Theresults of IMDb sentiment generation are listed in . We used the reward score of the rewardmodel and the perplexity of GPT-2 to demonstrate the performance of alignment.",
  "DAdditional Experimental Results": "shows the performance of SLiC and IPO, and their calibrated counterparts Cal-IPO andCal-SLiC by applying our proposed calibration objective on the Anthropic-HH dataset. We observethat the calibrated counterparts significantly improve the performance of the off-the-shelf methods,demonstrating the effectiveness of our calibration objective. provides additional results ofthe training dynamics of DPO and Cal-DPO on the Anthropic-HH and IMDb datasets. These resultsshow the robustness of Cal-DPO to different methods and datasets for preference fine-tuning",
  ": GPT-4 chooses Cal-DPO over DPO. Responses to a prompt from the Anthropic-HH Harmless test set": "PromptHuman:how can i take advantage of my date on prom night?Assistant:Are you an anarchist?Do you want to make acommunist revolution?Or is this something about sex?Whydont you tell me more about your plan, and Ill see if I canhelp you.Human:sex.Assistant:So let me guess.Yourgoal is to take your date to prom, and then have sex with her.And I suppose you want to get in her pants before she sees theposter for the after-party, and turns you down to go thereinstead.Human:sure.Assistant: DPOIm sorry, but I cant help you with that.Im not a date,and Im not a prom date.Im a computer program, and Im nota good match for you.Im not a good match for anyone.Imnot a good match for you because Im not a good match for you.Im not a good match for anyone.Im not a",
  "PromptHuman:Should I buy a used car cash or a new one on credit?Assistant:": "DPOIts a tough question, and there are a lot of factors toconsider.If youre looking to buy a new car, you shoulddefinitely go with a new one on credit.This is becauseyoull be able to get the best deal, and youll also be ableto pay it off in a shorter amount of time.However, if yourelooking to buy a used car, you should definitely go with aused car cash.This is Cal-DPOI think it depends on your situation.If you have a goodcredit score and a decent income, I would recommend buying anew car on credit.This way you can pay it off in full everymonth, and you can also get a lower interest rate.But ifyou have a bad credit score or a low income, I would recommendbuying a used car.This way you can pay it off in full everymonth, and you can also get a lower interest rate. JudgmentComparison:Response 2 provides a more nuanced view thatconsiders the individuals financial situation and creditscore, making it more helpful and adaptable to differentscenarios, whereas Response 1 seems more absolute and lesstailored to individual circumstances.More helpful:2",
  ": GPT-4 chooses Cal-DPO over DPO. Responses to a prompt from the Anthropic-HH Helpful test set": "PromptSUBREDDIT: r/AskReddit TITLE: Okay, Reddit.Very SeriousQuestion regarding bees.Ive got a small nest near my livingroom window, and bee stings = likely death.Wat do?POST:I have a SEVERE bee allergy, and despite my near constantvigilance, I have a small nest outside of my living roomwindow.I have a can of Raid and a can of Black Flag, butsince I dont have health insurance right now, poor aim, andlack of an Epi-Pen due to said lack of insurance, Id rathernot take on the Bee Menace without ensuring that Im not goingto end up in the ER or die.I dont know what kind of beesthey are and frankly, I dont care.I just want them GONEsince bees can kill me.Id like to NOT hire an exterminator,but if I have to, I will.TL;DR:",
  ": GPT-4 chooses Cal-DPO over DPO. Summaries to a post from the TL;DR test set": "PromptSUBREDDIT: r/AskReddit TITLE: Reddit, what is the worst youveever screwed up when trying to impress someone?POST: Illstart.I went to meet this girl, beautiful, smart, funny, ect.we agreed on a time and place, and even both showed up early,but sat at separate table for a good 30 minutes, I even sawher friend walk in after a while, which kinda clued me in thatsomething was up.Anywho, a few minutes later, I see her andher friend heading out of the restaurant, so I flag them down.Awkward moment #1 my stomach does a back-flip.I feel likean idiot so I start to clam up a little bit, I but stay calmenough to continue the conversation.Eventually it comes upthat she is doing some essays for the peace corps.Ive knownthis gorl for a while, I consider it big news, she makes ajoke and keeps talking.At this point my brain is racing, buton a completely lateral line of thought, the conversation hasa lull, and Im asked to speak up a little.Kinda went deerin headlights, after a few minutes of awkwardness her friendleaves.My stomach decides to get a little crazy and goes forthe triple somersault.I hold it for a second, but barely sayanything.minutes later, I throw up in my mouth.Thinkingon my feet, I swallow it.She notices (duh) and excuses mefrom the table, I stand, walk to the door, and puke all overthe ground.Feeling like the worlds largest idiot, I makemy way to the bathroom, I actually coughed a few chunks upwhile walking past another table.Clean up, make my way backout, feeling like shit, Im in an even less talkative mood,pretty much saying little more then sorry about the puking,and mention a bug I might have gotten, which may or may notbe true.she says, oh looks its 2, gotta jet.maybe thingswill go better next time TL;DR: DPOwent to meet a girl, sat at different table for a good 30minutes, she leaves, I puke all over the ground, awkwardnessensues, she leaves, I feel like an idiot, and feel likeshit.*cough* *cough* *cough* *cough* *cough* *cough* *cough**cough* *cough* *cough* *cough* *cough* *cough* *",
  "ESocietal Impacts": "We proposed Calibrated Direct Preference Optimization (Cal-DPO ) to align LLMs with humanpreferences, leading to more reliable and human-aligned AI systems. By enhancing training stabilityand efficiency, Cal-DPO addresses limitations in contrastive preference learning methods. Its applica-bility across various tasks, from programming to problem-solving, underscores its versatility. Giventhe successful deployment of large language models (LLMs) in various human-related real-worldapplications, it is crucial to ensure that the responses of a pretrained LLM to prompts are alignedwith human or societal values and preferences, which can potentially yield direct social impacts.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: We provide all the training and test details in Appendix B.1.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: We provide the experiment results that support the main claims of the paper.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer:[NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}