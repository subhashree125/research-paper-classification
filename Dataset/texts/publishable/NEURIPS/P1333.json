{
  "Abstract": "Large Language Models (LLMs), while being increasingly dominant on a myriadof knowledge-intensive activities, have only had limited success understandinglengthy table-text mixtures, such as academic papers and financial reports. Recent advances of long-context LLMs have opened up new possibilitiesfor this field. Nonetheless, we identify two roadblocks: (1) Prior benchmarksof table question answering (TableQA) have focused on isolated tables withoutcontext , making it hard to evaluate models in real-world scenarios. (2) Priorbenchmarks have focused on some narrow skill sets of table comprehension suchas table recognition , data manipulation/calculation , table summarization etc., while a skilled human employs those skills collectively. In this work, weintroduce TableQuest, a new benchmark designed to evaluate the holistic tablecomprehension capabilities of LLMs in the natural table-rich context of financialreports. We employ a rigorous data processing and filtering procedure to ensurethat the question-answer pairs are logical, reasonable, and diverse. We experimentwith 7 state-of-the-art models, and find that despite reasonable accuracy in locatingfacts, they often falter when required to execute more sophisticated reasoning ormulti-step calculations. We conclude with a qualitative study of the failure modesand discuss the challenges of constructing a challenging benchmark. We make theevaluation data, judging procedure and results of this study publicly available tofacilitate research in this field.",
  "Introduction": "In many fields, ranging from finance to healthcare, tables play a crucial role in organizing andinterpreting vast amounts of data. They allow individuals to discern patterns, extract meaningfulinsights, and make informed decisions by presenting information in a concise, structured format.However, the tabular format poses visible challenges for large language models (LLMs), whichare fundamentally designed to process data in sequential, text-based formats rather than navigatingthe complex spatial relationships inherent in tables. Despite growing interest and progress innatural language processing, LLMs still struggle with extracting, understanding and reasoning aboutinformation presented in tabular form, thereby limiting their use in industrial applications. While numerous approaches have been proposed to enhance LLMs performance on tasks involvingtabular data, there is a significant gap in comprehensive evaluation frameworks that capture thebreadth and complexity of real-world use cases. Existing benchmarks tend to be narrowly focused,",
  ": A sample of our TableQuest benchmark showcasing the different aspects of table compre-hension. The sample image on the left was for demonstration, which was rendered from the sourceHTML file": "often concentrating on specific aspects like table-based question answering or informationretrieval , and fail to represent the full spectrum of challenges that professionals encounterwhen working with tabular data. To address this gap, we introduce TableQuest, a novel benchmark specifically designed to evaluatethe proficiency of LLMs in understanding and reasoning with tabular data across a diverse set ofscenarios. TableQuest is a question-answering dataset over public traded companies annual filingswhich contain ample text and tables. We carefully construct the questions such that on answering thequestions correctly, the models demonstrate the following capabilities: Extraction (Easy level). Given the popularity of long-context retrieval benchmarks (e.g., needle-in-a-haystack ), we adapt this format by focusing on retrieving information from table cells withinreal-world documents, rather than from randomly injected facts. It not only enables evaluating modelsto distinguish similar pieces of information under an ecologically valid condition, but also requiresmodels to extract insights of two dimensions (rows and columns) from linear inputs. Calculations (Medium level). Finance reports, the essential tools to make informed financialdecisions and strategize for future growth, provide the perfect ground for a wide range of numericaltasks, including data analysis, forecasting, budgeting, and performance tracking. Prior works haveexplored this area in a wide range of heavily-engineered systems , breaking down the probleminto multiple steps for multiple models specializing in each. In our benchmark, we present thischallenge in a monolithic fashion, pressuring-testing the models ability to spot evidence in tables aswell as conduct accurate calculations. Analytics (Hard level). One step further from accurate calculations, we ask models to uncoverinsights from numbers they extracted or calculated and output in words. The aim is that models coulddigest the reports and output summaries in a query-focused manner, effectively aiding humans atdocument comprehension. In our evaluation of TableQuest, we explored the performance of a wide array of both proprietary andopen-source LLMs, employing an automated evaluation system to assess the quality and correctness ofthe models responses. Our initial results indicate that while some models can handle straightforwarddata extraction tasks with reasonable accuracy, they often falter when required to execute moresophisticated reasoning or multi-step calculations. Common issues include difficulty maintainingcontext over longer sequences, following instructions (particularly format), inconsistencies in applyingdomain-specific knowledge and reasoning, highlighting areas where current models fall short andwhere future improvements are necessary.",
  "Data Preparation": "Source. We collect the 10-K reports of S&P 500 companies directly from the SECs EDGAR database2, which offers comprehensive HTML annotations for each table, capturing hierarchical structuresand detailed cell information, including text content and formatting. This structured format allowsfor the extraction and post-processing of data using HTML tags, enabling a thorough representationof the complex relationships and structures present within the tables. Despite a lack of consensus onthe best format for table serialization , we employed HTML due to two reasons: 1. it canpreserve table structures better than other languages (e.g. natural text, markdown, json etc.), and 2.the internet being a critical source for web-scale LLM pre-training corpora . Question-Answer Synthesis. Existing question-answering datasets often exhibit internal homogene-ity, such as focusing solely on tabular answers derived from multiple tables . To address thislimitation, we implemented a progressive three-round chat format, where each round builds uponthe previous one, resulting in a sequence of question-answer pairs. In each round, we introduce anew question that demands an additional capability, expanding the models range of responses. Thisiterative process ensures that each prior question-answer pair informs and enhances the subsequentone, fostering greater diversity in the questions generated. Additionally, we incorporated explicitchain-of-thought (CoT) prompting to guide the model in articulating its reasoning steps, whichfacilitated more thorough quality inspection. We present an validation study on the quality of thequestion-answer generation pipeline with a self-supervised data source, which we detail in AppendixF. The multi-round chat prompts can be found in Appendix A. Quality filtering. Given the difficulty of questions, we employ a hybrid human-machine verificationpipeline to validate the questions and answers generated were relevant, reasonable and valid. Wecollaborated closely with a team of finance experts to develop the initial guidelines for constructingthe questions. In Appendix B, you can find the data processing and quality assurance specifics.",
  "Experiments": "Inputs and Outputs. We sample 240 questions from our synthesized dataset, 80 of each difficultylevel, as the evaluation subset of our benchmark. Each question comes with the context on whichit was generated, which was included into the prompt for generation, along with instructions and aone-shot prompt for answer formatting purposes, the details of which can be found in Appendix A.The metadata of the sample set can be found in .",
  "Configurations. We experiment with 7 models, 4 proprietary and 3 open-source, as detailed in table2. We employ the following configuration for the models included in our test:": "We use a vLLM backend for serving the open-sourced models (including the Qwen, and theLlama families). We use a temperature of 0.05, with a token limit of 16,384 or 8,192 as determinedby the underlying LLM3 For proprietary models, we use their official API services with a temperatureof 0.05, with their maximum input token limit provided respectively. We truncate the input exceedingthe input limits, which accounts for only <2% of all samples.",
  "our tested models, only Meta-Llama-3-70B-Instruct uses an 8,192 limit": "Evaluation Framework. In traditional question-answering (QA) tasks, a range of rule-based andword-matching evaluation metrics have been commonly used to assess model performance. Someof the most prevalent metrics include Exact Match (EM), F1 Score, BLEU (Bilingual EvaluationUnderstudy) , ROUGE (Recall-Oriented Understudy for Gisting Evaluation) , METEOR(Metric for Evaluation of Translation with Explicit ORdering) , and BERTScore . Despite their widespread usage, these metrics exhibit significant limitations, particularly whenapplied to analytical or open-ended questions. Therefore, in addition to word-matching based metrics,we employ a machine judge for a more interpretable and flexible judging of long-form answers.Specifically, for analytics tasks, we compare each models response to a baseline (in our case, gpt-4ousing text input, with a 1000 base score), ask the judging model (gpt-4-turbo) to reason about whichmodel is more accurate, logical, and clear in its output. The prompts we used are detailed in AppendixA. For extraction and calculation tasks, we also provide accuracy as a metric in Appendix D.",
  "demonstrate model performance using ELO scores, with shades of blue and red demonstratingbetter and worse scoring": "Closed-source models like gpt-4-turbo and claude-3-5-sonnet clearly outperform open-sourcemodels, particularly on harder tasks, as evidenced by their significantly higher ELO ratings. GPT-4-turbo leads with an overall ELO rating of 1164.35, excelling in medium difficulty tasks whilemaintaining strong performance in easier tasks. Claude-3-5-sonnet also performs well across alldifficulty levels, showing a more balanced profile with a particularly strong performance in easytasks. We observe that among open-source model, However, Meta-Llama-3.1-70B-Instruct, anopen-source model, shows promise by delivering competitive performance in medium tasks (ELO1008.80) and a decent overall ELO rating (869.27), standing out as a notable open-source optiondespite trailing behind the closed-source leaders. We present a more in-depth analysis on what causesthe performance gap in Appendix C with the statistics in Appendix D. Models can excel at easier tasks, but be significantly weaker at hard tasks. Notably, models likegemini-1.5-pro and gemini-1.5-flash excel in simpler tasks, with Easy task ELO ratings reachingup to 967.86 and 880.19 respectively, but struggle with harder tasks, as indicated by their lowerHard ELO ratings (both at 779.98). Similarly, Meta-Llama-3.1-70B-Instruct demonstrates moderateperformance in Easy and Medium tasks (ELO 880.19 and 1008.80 respectively) but falls significantlyshort in Hard tasks, with an ELO rating of 703.98. These patterns highlight the specialization ofsome models for simple tasks while exposing weaknesses in their ability to handle more complexscenarios. As a side note, we found certain multi-modal models exhibit this pattern as well, which wedemonstrate in the complete evaluation results in Appendix D, but we call for future work for furtherinvestigation in multi-modal table comprehension benchmarking.",
  "Conclusion": "In this paper, we introduce TableQuest, a new benchmark, that addresses the gaps by evaluatingLLMs ability to reason with tables in natural, table-rich context of financial reports. Our resultswith 7 state-of-the-art models reveal that, while LLMs can locate facts with reasonable accuracy,they struggle with complex reasoning and multi-step calculations. This highlights the need forfuture improvements in LLMs to enhance their proficiency in tasks requiring comprehensive tablecomprehension. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, HaiyangYu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. LeaveNo Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA, June2024. arXiv:2406.17419 [cs].",
  "Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and BertieVidgen. FinanceBench: A New Benchmark for Financial Question Answering, November 2023.arXiv:2311.11944 [cs, stat]": "Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon,Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang.FinQA: A Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021Conference on Empirical Methods in Natural Language Processing, pages 36973711, Onlineand Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. Xinyi Zheng, Doug Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global TableExtractor (GTE): A Framework for Joint Table Identification and Cell Structure RecognitionUsing Visual Context, December 2020. arXiv:2005.00589 [cs]. Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, KangliXu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, and Jie Tang. TableLLM:Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios, April 2024.arXiv:2403.19318 [cs].",
  "Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, DiyiYang, and Dipanjan Das. ToTTo: A Controlled Table-To-Text Generation Dataset, October2020. arXiv:2004.14373 [cs]": "Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, WojciechKryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand,Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev,and Dragomir Radev. FeTaQA: Free-form Table Question Answering. Transactions of theAssociation for Computational Linguistics, 10:3549, January 2022. Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, andRada Mihalcea. Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMsand MLLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of theAssociation for Computational Linguistics ACL 2024, pages 407426, Bangkok, Thailand andvirtual meeting, August 2024. Association for Computational Linguistics.",
  "Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, and Arman Cohan. FinanceMath:Knowledge-Intensive Math Reasoning in Finance Domains, November 2023": "Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, FuliFeng, and Tat-Seng Chua. TAT-QA: A Question Answering Benchmark on a Hybrid ofTabular and Textual Content in Finance. In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the 11th International Joint Conference onNatural Language Processing (Volume 1: Long Papers), pages 32773287, Online, 2021.Association for Computational Linguistics.",
  "Kamradt G.Needle in a haystack. 2024": "Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, ZehuiChen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, JiayeGe, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, TingHuang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, LinyangLi, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu,Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma,Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song,Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, JiaqiWang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, FanWu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, HaochenYe, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang,Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou,Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. InternLM2Technical Report, March 2024. arXiv:2403.17297 [cs]. Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach,Diego Socolinsky, Srinivasan Sengamedu, and Christos Faloutsos. Large Language Mod-els(LLMs) on Tabular Data: Prediction, Generation, and Understanding A Survey, June 2024.arXiv:2402.17944 [cs].",
  "Weizheng Lu, Jiaming Zhang, Jing Zhang, and Yueguo Chen. Large Language Model for TableProcessing: A Survey, February 2024. arXiv:2402.05121 [cs]": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer, September 2023. arXiv:1910.10683 [cs, stat]. Guilherme Penedo, Hynek Kydlcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell,Colin Raffel, Leandro Von Werra, and Thomas Wolf. The FineWeb Datasets: Decanting theWeb for the Finest Text Data at Scale, June 2024. arXiv:2406.17557 [cs].",
  "Vaishali Pal, Evangelos Kanoulas, Andrew Yates, and Maarten de Rijke. MultiTabQA: Generat-ing Tabular Answers for Multi-Table Question Answering": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large lan-guage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposiumon Operating Systems Principles, 2023. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a Method for AutomaticEvaluation of Machine Translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for ComputationalLinguistics.",
  "Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for ComputationalLinguistics": "Satanjeev Banerjee and Alon Lavie. METEOR: An Automatic Metric for MT Evaluation withImproved Correlation with Human Judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin,and Clare Voss, editors, Proceedings of the ACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan,June 2005. Association for Computational Linguistics.",
  "[User]": "You will be presented with a question, the context, a reference answer marked with \"gt\", and twomodel predictions marked by \"pred_1\" and \"pred_2\" in JSON format. Compare the two predictions andevaluate which one better answers the question based on the reference answer. Output your rationale(marked by \"rationale\" in JSON) for your decision, and then your decision (marked by \"better_model\"in JSON, with possible values \"model_1\", \"model_2\", or \"tie\") in JSON format.Please use the following listed aspects and their descriptions as evaluation criteria:",
  "Prompt 4": "Youre an experienced university professor designing test questions for corporate finance.Based on the material provided, generate an analytical question that focuses on a fraction ofthe content. Then, provide a detailed step-by-step reference reasoning steps. Finally, providea concise reference answer which should be less than 3 sentences long for the ease of grading....Output:",
  "BQuality assurance on the data synthesis pipeline": "Our process involved generating an initial set of questions, followed by iterative feedback sessionswith these experts to refine and enhance the quality of the questions. To ensure a thorough evaluation,we selected a representative sample of 50 questions from each difficulty level for human annotation.For this task, we engaged four STEM graduates who were responsible for locating the sources ofanswers, verifying the alignment of human-provided answers with the synthetic ones, and assessingwhether the quality of the questions appropriately matched their designated difficulty levels. Through-out this iterative process, we maintained a high standard of accuracy. We identified the most commonerror cases, which were then used as in-context examples to further fine-tune the performance ofGPT-4-Turbo, ensuring it could accurately re-evaluate the question-answer pairs. This approachallowed us to systematically address and rectify discrepancies, enhancing the overall robustness andvalidity of our benchmark. Finally, the refined set of question-answer pairs demonstrated high fidelityto real-world scenarios, reflecting the domain knowledge and quality standards initially set out.",
  "CCase Study": "Given the gap in open-source/proprietary model performance, we conduct a qualitative analysis withone model from each world as representatives, llama-3.1-70B-Instruct and GPT-4-Turbo. We employthe most recent reasoning models to date, o1-preview, to judge and critique both responses on thesame question for comparison. On a 3-point scale scoring system, GPT-4-Turbo leads by 0.34 onaverage. We identified 12.5% of the evaluation samples on which GPT-4-Turbo scored 2 or higherthan Llama-3.1-70B-Instruct. The performance disparity between Llama and GPT-4-Turbo in the financial analysis benchmarkreveals limitations in Llamas capabilities. Firstly, the prevalence of empty responses, particularlyin questions requiring straightforward fact retrieval, suggests deficiencies in Llamas long-contextinformation retrieval and instruction following output generation mechanisms. Secondly, the cases of calculation errors and misinterpretation of numerical data, indicates weaknesses in its mathematicalreasoning abilities and financial data comprehension. These errors ranged from failing to calculatespecific percentages to misinterpreting financial trends, highlighting a need for enhanced numericalprocessing capabilities. Lastly, Llama demonstrated difficulties with complex questions requiringmulti-step reasoning. In these instances, Llama often provided incomplete analyses or failed toaddress key aspects of multi-faceted queries, suggesting limitations in its ability to decompose andaddress complex problems. These findings collectively point to areas of improvement for all opensourced models, particularly in domains requiring precise numerical analysis and multi-step reasoning.Below we showcase five representative examples of the differences in the judgement from the judgeto the two models.",
  "Question and Ground Truth": "Question: Analyze the effectiveness of the cash flow hedge strategy for the cross-currencyswaps by comparing the fair values of the contracts between 2022 and 2023.Ground Truth Answer: The fair value of the cross-currency swaps decreased by $13.2million from 2022 to 2023, indicating reduced effectiveness or negative economic impact onthe hedge during this period.",
  "LLaMA Prediction": "{ \"steps\":\"To analyze theeffectiveness of the cashflow hedge strategy for thecross-currency swaps, we needto calculate the change in fairvalue of the contracts between2022 and 2023.The change infair value is calculated as thedifference between the fair valuein 2023 and the fair value in 2022.Then, we calculate the percentagechange in fair value by dividingthe change in fair value by theoriginal fair value in 2022 andmultiplying by 100.\", \"answer\":{ \"change_in_fair_value\":\"-$13.2 million\",\"percentage_change_in_fair_value\":\"-85.16%\" } }",
  "EDataset Details": "We detail the input length by token count in . As demonstrated by the green line, the inputsexceeding the 16k token limits are the outliers. Given that all but one model tested in our experimentsaccept 16k tokens, we believe that the truncation employed to fit context limit does not pose asignificant disadvantage to open-source models with a smaller context limit.",
  "FSelf-supervised question-answer synthesis validation": "Despite extensive efforts from both human and machine annotators, the complexity and the domainknowledge required to validate whether the questions are meaningful, or whether the answer aptlyresponds to the question, i.e. whether our external supervision setup was, in fact, factual - remainsa challenging point to prove. Therefore, in an attempt to verify the efficacy of the data synthesispipeline, we adapted it for a new domain, diversifying our data sources. More importantly, thisadaptation enabled us to leverage self-supervision signals within the context document to fact-checksynthesized answers reliably. Specifically, we selected recent peer-reviewed papers from reputable artificial intelligence conferencesas source material for our data synthesis pipeline. Our selection was based on three reasons: (1)Conference papers in this domain often contain rich tabular content, which is frequently referencedin the main text with detailed analysis; (2) The expert peer-review process ensures a certain quality ofcontent; and (3) The AI field has a high volume of contributions, providing a solid foundation forscaled analysis. We collected 936 academic papers from January 2022 to May 2024, utilizing a subset of a recentlyreleased dataset focused on long-context reasoning. Since the data was obtained in markdownformat through OCR from the original PDFs, it was incompatible with our existing synthesis pipeline,which was designed for HTML-format handling. We employed several preprocessing steps: (a)splitting each markdown paper into chunks according to its native formatting and extracting tablesand headers using regular expressions; (b) gathering all chunks that explicitly reference each table toserve as context for question synthesis; and (c) converting all markdown tables into canonical HTML form using the markdownify package. These steps transformed the original data source into a formatcompatible with our synthesis pipeline and provided high-quality evidence for verifying syntheticanswers. All other configurations, including model settings and prompts, were kept consistent withthe original setup to preserve the validity of the validation process. Synthetic questions are generatedwith a requirement to cite a grounding sentence from the evidence pool, while synthetic answers arecreated based solely on the corresponding tables and questions, without referencing direct evidencefrom the paper. Following quality filtering procedures, we obtained 2,100 question-answer pairs, with700 representing each difficulty level. To evaluate the faithfulness of synthetic answers to the source, we used recall as our evaluation metric.Specifically, we leveraged the nltk package to tokenize each answer string and the correspondingevidence string, which theoretically contains the answer. We then identified the common tokensbetween the two and calculated the recall rate based on the total tokens in the answer string. Theresults (as shown in ) indicate an average recall rate of 0.74, demonstrating our synthesismodels (GPT-4-turbo) strong capability to produce accurate answers that align with the source. In conclusion, our adaptable data synthesis pipeline demonstrates robust performance in generatingfactually grounded question-answer pairs, particularly through its effective handling of domain-specific content like academic papers with tabular data leveraging a self-supervision signal. Thisversatility highlights its potential for applications across diverse domains that require structured andunstructured data synthesis, making it a valuable tool for generating and verifying knowledge in areassuch as long-context reasoning and fact-checking. Future extensions of this pipeline could furtherenhance its applicability by incorporating additional domains and refining its adaptability to varyingdata formats."
}