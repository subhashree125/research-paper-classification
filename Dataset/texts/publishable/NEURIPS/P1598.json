{
  "Abstract": "Molecular property prediction (MPP) is integral to drug discovery and materialscience, but often faces the challenge of data scarcity in real-world scenarios. Ad-dressing this, few-shot molecular property prediction (FSMPP) has been developed.Unlike other few-shot tasks, FSMPP typically employs a pre-trained molecularencoder and a context-aware classifier, benefiting from molecular pre-trainingand molecular context information. Despite these advancements, existing methodsstruggle with the ineffective fine-tuning of pre-trained encoders. We attribute this is-sue to the imbalance between the abundance of tunable parameters and the scarcityof labeled molecules, and the lack of contextual perceptiveness in the encoders. Toovercome this hurdle, we propose a parameter-efficient in-context tuning method,named Pin-Tuning. Specifically, we propose a lightweight adapter for pre-trainedmessage passing layers (MP-Adapter) and Bayesian weight consolidation for pre-trained atom/bond embedding layers (Emb-BWC), to achieve parameter-efficienttuning while preventing over-fitting and catastrophic forgetting. Additionally, weenhance the MP-Adapters with contextual perceptiveness. This innovation allowsfor in-context tuning of the pre-trained encoder, thereby improving its adaptabil-ity for specific FSMPP tasks. When evaluated on public datasets, our methoddemonstrates superior tuning with fewer trainable parameters, improving few-shotpredictive performance.",
  "Introduction": "In the field of drug discovery and material science, molecular property prediction (MPP) stands as apivotal task . MPP involves the prediction of molecular properties like solubility and toxicity,based on their structural and physicochemical characteristics, which is integral to the developmentof new pharmaceuticals and materials. However, a major challenge encountered in real-world MPPscenarios is data scarcity. Obtaining extensive molecular data with well-characterized properties canbe time-consuming and expensive. To address this, few-shot molecular property prediction (FSMPP)has emerged as a crucial approach, enabling predictions with limited labeled molecules .",
  "arXiv:2411.01158v1 [cs.LG] 2 Nov 2024": "structures into vectorized representations , and then the classifier uses theserepresentations to predict molecular properties. In the context of few-shot scenarios, two significantdiscoveries have been instrumental in advancing this task. Firstly, pre-trained molecular encodershave demonstrated consistent effectiveness in FSMPP tasks . This indicates the utility ofleveraging pre-acquired knowledge in dealing with data-limited scenarios. Secondly, unlike typicalfew-shot tasks such as image classification , FSMPP tasks greatly benefits from molecularcontext information. This involves comprehending the seen many-to-many relationships betweenmolecules and properties , as molecules are multi-labeled by various properties. Thesetwo discoveries have collectively led to the development of the widely used FSMPP framework thatutilizes a pre-trained encoder followed by a context-aware classifier, as shown in (b).",
  "Train-from-ScratchPretrain-then-FreezePretrain-then-Finetune": ":Comparison of molecular encoderstrained via different paradigms: train-from-scratch,pretrain-then-freeze, and pretrain-then-finetune.The evaluation is conducted across two datasetsand three encoder architectures . Theresults consistently demonstrate that while pretrain-ing outperforms training from scratch, the currentmethods do not yet effectively facilitate finetuning. Despite the progress, there are observed lim-itations in the current approaches to FSMPP.Notably, while using a pre-trained molecularencoder generally outperforms training fromscratch, fine-tuning the pre-trained encoder of-ten leads to inferior results compared to keepingit frozen, which can be observed in . The observed ineffective fine-tuning can be at-tributed to two primary factors: (i) Imbalance be-tween the abundance of tunable parameters andthe scarcity of labeled molecules: fine-tuningall parameters of a pre-trained encoder with fewlabeled molecules leads to a disproportionate ra-tio of tunable parameters to available data. Thisimbalance often results in over-fitting and catas-trophic forgetting . (ii) Limited contextualperceptiveness in the encoder: while molecularcontext is leveraged to enhance the classifier , the encoder typically lacks the explicit capabil-ity to perceive this context, relying instead on implicit gradient-based optimization. This leads tothe encoder not directly engaging with the nuanced molecular context information that is critical inFSMPP tasks. In summary, while significant strides have been made, the challenges of imbalancebetween the number of parameters and labeled data, along with the need for contextual perceptivenessin the encoder, necessitate more sophisticated methodologies in this domain. Based on the aforementioned analysis, we propose the parameter-efficient in-context tuning method,named Pin-Tuning, to address the two primary challenges in FSMPP. To overcome the parameter-data imbalance, we propose a parameter-efficient chemical knowledge adaptation approach forpre-trained molecular encoders. A lightweight adapters (MP-Adapter) are designed to tune the pre-trained message passing layers efficiently. Additionally, we impose a Bayesian weight consolidation(Emb-BWC) on the pre-trained embedding layers to prevent aggressive parameter updates, therebymitigating the risk of over-fitting and catastrophic forgetting. To address the second challenge, wefurther endow the MP-Adapter with the capability to perceive context. This innovation allows forin-context tuning of the pre-trained molecular encoders, enabling them to adapt more effectively tospecific downstream tasks. Our approach is rigorously evaluated on public datasets. The experimentalresults demonstrate that our method achieves superior tuning performance with fewer trainableparameters, leading to enhanced performance in few-shot molecular property prediction.",
  "The main contributions of our work are summarized as follows:": "We analyze the deficiencies of existing FSMPP approaches regarding the adaptation of pre-trainedmolecular encoders. The key issues include an imbalance between the number of tunable parametersand labeled molecules, as well as a lack of contextual perceptiveness in the encoders. We propose Pin-Tuning to adapt the pre-trained molecular encoders for FSMPP tasks. Thisincludes the MP-Adapter for message passing layers and the Emb-BWC for embedding layers,facilitating parameter-efficient tuning of pre-trained molecular encoders.",
  "Related work": "Few-shot molecular property prediction. Few-shot molecular property prediction aims to accuratelypredict the properties of new molecules with limited training data . Early research applied generalfew-shot techniques to FSMPP. IterRefLSTM is the pioneer work to leverage metric learningto solve FSMPP problem. Following this, Meta-GGNN and Meta-MGNN introducemeta-learning with graph neural networks, setting a foundational framework that subsequent studieshave continued to build upon . It is noteworthy that Meta-MGNN employs a pre-trainedmolecular encoder and achieves superior results through fine-tuning in the meta-learning processcompared to training from scratch. In fact, pre-trained graph neural networks haveshown promise in enhancing various graph-based downstream tasks , including molecularproperty prediction . Recent efforts have shifted towards leveraging unique naturein FSMPP, such as the many-to-many relationships between molecules and properties arising fromthe multi-labeled nature of molecules, often referred to as the molecular context. PAR initiallyemploys graph structure learning to connect similar molecules through a homogeneouscontext graph. MHNfs introduces a large-scale external molecular library as context to augmentthe limited known information. GS-Meta further incorporates auxiliary task to depict themany-to-many relationships. Parameter-efficient tuning. As pre-training techniques have advanced, tuning of pre-trained modelshas become increasingly crucial. Traditional full fine-tuning approaches updates all parameters, oftenleading to high computational costs and the risk of over-fitting, especially when available data fordownstream tasks are limited . This challenge has led to the emergence of parameter-efficienttuning . The philosophy of parameter-efficient tuning is to optimize a small subset ofparameters, reducing the computational costs while retaining or even improving performance ondownstream tasks . Among the various strategies, the adapters have gainedprominence. Adapters are small modules inserted between the pre-trained layers. During the tuningprocess, only the parameters of these adapters are updated while the rest remains frozen, which notonly improves tuning efficiency but also offers an elegant solution to the generalization .By keeping the majority of the pre-trained parameters intact, adapters preserve the rich pre-trainedknowledge. This attribute is particularly valuable in many real-world applications including FSMPP.",
  "Problem formulation": "Let {T } be a collection of tasks, where each task T involves the prediction of a property p. Thetraining set comprising multiple tasks {Ttrain}, is represented as Dtrain = {(mi, yi,t)|t {Ttrain}},with mi indicating a molecule and yi,t its associated label for task t. Correspondingly, the test setDtest, formed by tasks {Ttest}, ensures a separation of properties between training and testing phases,as the property sets {ptrain} and {ptest} are disjoint ({ptrain} {ptest} = ). The goal of FSMPP is to train a model using Dtrain that can accurately infer new properties from alimited number of labeled molecules in Dtest. Episodic training has emerged as a promising strategyin meta-learning to deal with few-shot problem. Instead of retaining all {Ttrain} tasks inmemory, episodes {Et}Bt=1 are iteratively sampled throughout the training process. For each episodeEt, a particular task Tt is selected from the training set, along with corresponding support set St andquery set Qt. Typically, the prediction task involves classifying molecules into two classes: positive(y = 1) or negative (y = 0). Then a 2-way K-shot episode Et = (St, Qt) is constructed. Thesupport set St = {(msi, ysi,t)}2Ki=1 includes 2K examples, each class contributing K molecules. Thequery set containing M molecules is denoted as Qt = {(mqi , yqi,t)}Mi=1.",
  "Encoder-classifier framework for FSMPP": "Encoder-classifier framework is widely adopted in FSMPP methods. As illustrated in (a),given a molecule m whose property need to be predicted, a molecular encoder f() first learns themolecules representation based on its structure, i.e., hm = f(m) Rd. The molecule m is generallyrepresented as a graph m = (V, A, X, E), where V denotes the nodes (atoms), A represents theadjacent matrix defined by edges (chemical bonds), and X, E denote the original feature of atoms and bonds, then graph neural networks (GNNs) are employed as the molecular encoders .Subsequently, the learned molecular representation is fed into a classifier g() to obtain the predictiony = g(hm). The model is trained by minimizing the discrepancy between y and the ground truth y. Further, two key discoveries have been pivotal for FSMPP. The first is the proven effectiveness ofpre-trained molecular encoders, while the second is the significant advantage gained from molecularcontext. Together, these discoveries have further reshaped the widely adopted FSMPP framework,which combines a pre-trained encoder followed by a context-aware classifier, as shown in (b).",
  "Pre-trained molecular encoders (PMEs)": "Due to the scarcity of labeled data in molecular tasks, molecular pre-training has emerged as acrucial area, which involves training encoders on extensive molecular datasets to extract informativerepresentations. Pre-GNN is a classic pre-trained molecular encoder that has been widely usedin addressing FSMPP tasks . The backbone of Pre-GNN is a modified version of GraphIsomorphism Network (GIN) tailored to molecules, which we call GIN-Mol, consisting ofmultiple atom/bond embedding layers and message passing layers. Atom/Bond embedding layers. The raw atom features and bond features are both categorical vectors,denoted as (iv,1, iv,2, . . . , iv,|En|) and (je,1, je,2, . . . , je,|Ee|) for atom v and bond e, respectively.These categorical features are embedded as:",
  "where EmbAtoma()a{1,...,|En|} and EmbBondb()b{1,...,|Ee|} represent embedding operations that": "map integer indices to d-dimensional real vectors, i.e., h(0)v , h(l)e Rd, l {0, 1, . . . , L 1}represents the index of encoder layers, and L is the number of encoder layers. The atom embeddinglayer is present only in the first encoder layer, while an bond embedding layer exists in each layer.",
  ",(2)": "where u N(v) {v} is the set of atoms connected to v, and h(l)v Rd is the learned representationof atom v at the l-th layer. MLP() is implemented by 2-layer neural networks, in which the hiddendimension is d1. After MLP, batch normalization is applied right before the ReLU. The molecule-levelrepresentation hm Rd is obtained by averaging the atom representations at the final layer.",
  "Context Encoder": ": (a) The vanilla encoder-classifier framework for MPP. (b) The framework widely adoptedby existing FSMPP methods, which contains a pre-trained molecular encoder and a context-awareproperty classifier. (c) Our proposed framework for FSMPP, in which we introduce a Pin-Tuningmethod to update the pre-trained molecular encoder followed by the property classifier. (d) Thedetails of our proposed Pin-Tuning method for pre-trained molecular encoders. In (b) and (c), we usethe property names like SR-HSE to denote the molecular context in episodes.",
  "MP-Adapter: message passing layer-oriented adapter": "For message passing layers in PMEs, the number of parameters is disproportionately large comparedto the training samples. To mitigate this imbalance, we design a lightweight adapter targeted at themessage passing layers, called MP-Adapter. The pre-trained parameters in each message passinglayer include parameters in the MLP and the following batch normalization. We freeze all pre-trainedparameters in message passing layers and add a lightweight trainable adapter after MLP in eachmessage passing layer. Formally, the adapter module for l-th layer can be represented as:",
  "h(l)v= LayerNorm(h(l)v + h(l)v ) Rd,(5)": "where FeedForward() denotes feed forward layer and LayerNorm() denotes layer normalization.To limit the number of parameters, we introduce a bottleneck architecture. The adapters downscalethe original features from d dimensions to a smaller dimension d2, apply nonlinearity , then upscaleback to d dimensions. By setting d2 smaller than d, we can limit the number of parameters added. Theadapter module has a skip-connection internally. With the skip-connection, we adopt the near-zeroinitialization for parameters in the adapter modules, so that the modules are initialized to approximateidentity functions. Therefore, the encoder with initialized adapters is equivalent to the pre-trainedencoder. Furthermore, we add a layer normalization after skip-connection for training stability.",
  "Emb-BWC: embedding layer-oriented Bayesian weight consolidation": "Unlike message passing layers, embedding layers contain fewer parameters. Therefore, we directlyfine-tune the parameters of the embedding layers, but impose a constraint to limit the magnitude ofparameter updates, preventing aggressive optimization and catastrophic forgetting. The parameters in an embedding layer consist of an embedding matrix used for lookups based on theindices of the original features. We stack the embedding matrices of all embedding layers to form REd, where E represents the total number of lookup entries. Further, i Rd denotes the i-throws embedding vector, and i,j R represents the j-th dimensional value of i.",
  "where H(DP, i) Rdd is the Hessian of the log likelihood LP of pre-training dataset DP at i": "Details on the theoretical derivation of Eq. (6) are given in Appendix A. Since H(DP, i) isintractable to compute due to the great dimensionality of , we adopt the diagonal approximationof Hessian. By approximating H as a diagonal matrix, the j-th value on the diagonal of H can beconsidered as the importance of the parameter i,j. The following three choices are considered.",
  "Ei=1dj=1(i,j i,j)2, assigning equal importance to each parameter. Thisloss function is also known as L2 penalty with pre-trained model as the starting point (L2-SP)": "Diagonal of Fisher information matrix. The Fisher information matrix (FIM) F is the negationof the expectation of the Hessian over the data distribution, i.e., F = EDP[H], and the FIM canbe further simplified with a diagonal approximation. Then, the Eq. (6) is simplified to LFIMEmb-BWC =12Ei=1 Fi(i i)2, where Fi Rd is the diagonal of F(DP, i) Rdd and the j-th value in Fiis computed as EDP(LP/i,j)2. This is equivalent to elastic weight consolidation (EWC) . Diagonal of embedding-wise Fisher information matrix. In different property prediction tasks, theimpact of the same atoms and inter-atomic interactions may be significant or negligible. Therefore,we propose this choice to assign importance to parameters based on different embeddings, rather thantreating each parameter individually. By defining i =",
  "Enabling contextual perceptiveness in MP-Adapter": "For different property prediction tasks, the decisive substructures vary. As shown in , theester group in the given molecule determines the property SR-HSE, while the carbon-carbon triplebond determines the property SR-MMP. If fine-tuning can be guided by molecular context, encodingcontext-specific molecular representations allows for dynamic representations of molecules tailoredto specific tasks and enables the modeling of the context-specific significance of substructures.",
  ": Convert the context information of a 2-shot episode into a context graph": "Extracting molecular context information. Ineach episode, we consider the labels of the sup-port molecules on the target property and seenproperties, as well as the labels of the querymolecules on seen properties, as the context ofthis episode. We adopt the form of a graph todescribe the context. demonstrates thetransformation from original context data to acontext graph. In the left table, the labels ofmolecules mq1, m22 for property pt are the pre-diction targets, and the other shaded values arethe available context. The right side shows thecontext graph constructed based on the available context. Specifically, we construct context graphGt = (Vt, At, Xt) for episode Et. It contains M molecule nodes {m} and P property nodes {p}.Three types of edges indicate different relationships between molecules and properties.",
  "Then we employ a GNN-based context encoder: C = ContextEncoder(Vt, At, Xt), where C R(M+P )d2 denotes the learned context representation matrix for Et. Vt and At denote the node set": "and the adjacent matrix of the context graph, respectively, and Xt denotes the initial features of nodes.The features of molecule nodes are initialized with a pre-trained molecular encoder. The propertynodes are randomly initialized. When we make the prediction of molecule ms target property p, wetake the learned representations of the this molecule cm and of the target property cp as the contextvectors. Details about the context encoder are provided in Appendix F.2. In-context tuning with molecular context information. After obtaining the context vectors, weconsider enabling the molecular encoder to use the context as a condition, achieving conditionalmolecular encoding. To achieve this, we further refine our adapter module. While neural conditionalencoding has been explored in some domains, such as cross-attention and ControlNet forconditional image generation, these methods often come with a significant increase in the numberof parameters. This contradicts our motivation of parameter-efficient tuning for few-shot tasks. Inthis work, we adopt a simple yet effective method. We directly concatenate the context with theoutput of the message passing layer, and feed them into the downscaling feed-forward layer in theMP-Adapter. Formally, the downscaling process defined in Eq. (3) is reformulated as:",
  "Evaluation setups": "Datasets. We use five common few-shot molecular property prediction datasets from the Molecu-leNet : Tox21, SIDER, MUV, ToxCast, and PCBA. Standard data splits for FSMPP are adopted.Dataset statistics and more details of datasets can be found in Appendix D. Baselines.For a comprehensive comparison, we adopt two types of baselines: (1) methodswith molecular encoders trained from scratch, including Siamese Network , ProtoNet ,MAML , TPN , EGNN , and IterRefLSTM ; and (2) methods which leverage pre-trained molecular encoders, including Pre-GNN , Meta-MGNN , PAR , and GS-Meta .More details about these baselines are in Appendix E. Metrics. Following prior works , ROC-AUC scores are calculated on the query set for eachmeta-testing task, to evaluate the performance of FSMPP. We run experiments 10 times with differentrandom seeds and report the mean and standard deviations. : ROC-AUC scores (%) on benchmark datasets, compared with methods trained from scratch(first group) and methods that leverage pre-trained molecular encoder (second group). The bestis marked with boldface and the second best is with underline. Improve. indicates the relativeimprovements over the baseline models in percentage.",
  "Performance comparison": "We compare Pin-Tuning with the baselines and the results are summarized in , , and. Our method significantly outperforms all baseline models under both the 10-shot and 5-shotsettings, demonstrating the effectiveness and superiority of our approach. Across all datasets, our method provides greater improvement in the 10-shot scenario than in the5-shot scenario. This is attributed to the molecular context constructed based on support molecules.When there are more molecules in the support set, the uncertainty in the context is reduced, providingmore effective adaptation guidance for our parameter-efficient tuning. Among benchmark datasets, our method shows significant improvement on the SIDER dataset,increasing by 10.73% in the 10-shot scenario and by 8.81% in the 5-shot scenario. We consider thisis related to the relatively balanced ratio of positive to negative samples, as well as the absence ofmissing labels in the SIDER dataset (). A balanced and low-uncertainty distribution can betterbenefit addressing the FSMPP task from our method. We also observe that the standard deviations of our methods results under 10 seeds are slightly higherthan that of baseline models. However, our worst-case results are still better than the best baselinemodel. For example, in 10-shot experiments on the Tox21 dataset, the performance of our method is91.56 2.57. However, our 10 runs yield specific results with the worst-case ROC-AUC reaching88.02, which is also better than the best baseline model GS-Metas result of 86.67 0.41. Therefore,a high standard deviation does not mean our method is inferior to baseline models.",
  "Ablation study": "For MP-Adapter, the main components consist of: (i) bottleneck adapter module (Adapter), (ii)introducing molecular context to adatpers (Context), and (iii) layer normalization (LayerNorm). Theresults of ablation experiments are summarized in . The bottleneck adapter and the modeling ofmolecular context are the most critical, having the most significant impact on performance. Removingthem leads to a noticeable decline, which underscores the importance of parameter-efficient tuningand context perceptiveness in FSMPP tasks. Layer normalization is used to normalize the resultingrepresentations, which is also important for improving the optimization effect and stability.",
  "--89.7090.1270.7680.24-90.1792.0672.3780.74LIMEmb-BWC91.5693.4173.2281.26LFIMEmb-BWC90.9390.0972.1780.78LEFIMEmb-BWC91.3290.3172.7881.22": "For Emb-BWC, we verify the effectiveness of fine-tuning the embedding layers and regularizingthem with different approximations of LEmb-BWC(). Since the embedding layers have rela-tively few parameters, direct fine-tuning can alsoenhance performance. Applying our proposedregularizers to fine-tuning can further improvethe effects. Among the three regularizers, theLIMEmb-BWC is the most effective. This indicates that keeping pre-trained parameters to some extent canbetter utilize pre-trained knowledge, but the parameters worth keeping in fine-tuning and the importantparameters in pre-training revealed by Fisher information matrix are not completely consistent.",
  ": ROC-AUC (%) and number oftrainable parameters of Pin-Tuning with var-ied value of d2 and full Fine-Tuning method(e.g., GS-Meta) on the Tox21 dataset": "Effect of weight of Emb-BWC regularizer . Emb-BWC is applied on the embedding layers to limit themagnitude of parameter updates during fine-tuning. We vary the weight of this regularization from{0.01, 0.1, 1, 10}. The first subfigure in shows that the performance is best when = 0.1 or1. When is too small, the parameters undergo too large updates on few-shot downstream datasets,leading to over-fitting and ineffectively utilizing the pre-trained knowledge. Too large causes theparameters of the embedding layers to be nearly frozen, which prevents effective adaptation. Effect of hidden dimension of MP-Adapter d2. The results corresponding to different values of d2from {25, 50, 75, 100, 150} are presented in the second subfigure of . On the Tox21 dataset,we further analyze the impact of this hyper-parameter on the number of trainable parameters. Asshown in , the number of parameters that our method needs to train is significantly less thanthat required by the full fine-tuning method, such as GS-Meta, while our method also performs betterin terms of ROC-AUC performance due to solving over-fitting and context perceptiveness issues.When d = 50, Pin-Tuning performs best on Tox21, and the number of parameters that need to train isonly 14.2% of that required by traditional fine-tuning methods.",
  "Conclusion": "In this work, we propose a tuning method, Pin-Tuning, to address the ineffective fine-tuning ofpre-trained molecular encoders in FSMPP tasks. Through the innovative parameter-efficient tuningand in-context tuning for pre-trained molecular encoders, our approach not only mitigates the issuesof parameter-data imbalance but also enhances contextual perceptiveness. The promising resultson public datasets underscore the potential of Pin-Tuning to advance this field, offering valuableinsights for future research in drug discovery and material science. This work is jointly supported by National Science and Technology Major Project (2023ZD0120901),National Natural Science Foundation of China (62372454, 62236010) and the Excellent YouthProgram of State Key Laboratory of Multimodal Artificial Intelligence Systems.",
  "Jianyuan Deng, Zhibo Yang, Hehe Wang, Iwao Ojima, Dimitris Samaras, and Fusheng Wang.A systematic study of key elements underlying molecular property prediction. Nature Commu-nications, 14, 2023": "Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu,Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu,Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: Acomprehensive study of parameter efficient methods for pre-trained language models. arXiv,abs/2203.06904, 2022. Ning Ding, Yujia Qin, Guang Yang, Fu Wei, Zonghan Yang, Yusheng Su, Shengding Hu,Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu,Haitao Zheng, Jianfei Chen, Y. Liu, Jie Tang, Juanzi Li, and Maosong Sun. Parameter-efficientfine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5:220235,2023.",
  "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.Neural message passing for quantum chemistry. In ICML, 2017": "Jonathan Godwin, Michael Schaarschmidt, Alexander L. Gaunt, Alvaro Sanchez-Gonzalez,Yulia Rubanova, Petar Velickovic, James Kirkpatrick, and Peter W. Battaglia. Simple GNNregularisation for 3d molecular property prediction and beyond. In ICLR, 2022. Renxiang Guan, Zihao Li, Wenxuan Tu, Jun Wang, Yue Liu, Xianju Li, Chang Tang, andRuyi Feng. Contrastive multiview subspace clustering of hyperspectral images based on graphconvolutional networks. IEEE Transactions on Geoscience and Remote Sensing, 2024.",
  "Ying Song, Shuangjia Zheng, Zhangming Niu, Zhang-Hua Fu, Yutong Lu, and Yuedong Yang.Communicative representation learning on attributed molecular graphs. In IJCAI, 2020": "Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mondal, and Jyoti Prakash Sahoo. A com-prehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities.ACM Computing Surveys, 2023. Megan Stanley, John Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, MarwinH. S. Segler, Nadine Schneider, and Marc Brockschmidt. Fs-mol: A few-shot learning datasetof molecules. In NeurIPS Datasets and Benchmarks, 2021. Hannes Strk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, StephanGnnemann, and Pietro Li. 3d infomax improves gnns for molecular property prediction. InICML, pages 2047920502, 2022."
}