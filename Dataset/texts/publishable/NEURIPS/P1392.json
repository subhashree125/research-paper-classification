{
  "Abstract": "A critical question about Large Language Models (LLMs) is whether their apparentdeficiency in mathematical reasoning is inherent, or merely a result of insufficientexposure to high-quality mathematical data. To explore this, we developed anautomated method for generating high-quality, supervised mathematical datasets.The method carefully mutates existing math problems, ensuring both diversity andvalidity of the newly generated problems. This is achieved by a neuro-symbolic datageneration framework combining the intuitive informalization strengths of LLMs,and the precise symbolic reasoning of math solvers along with projected Markovchain Monte Carlo sampling in the highly-irregular symbolic space. Empiricalexperiments demonstrate the high quality of data generated by the proposed method,and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with thegenerated data, surpass their state-of-the-art counterparts.",
  "Introduction": "Despite recent progress , both proprietary and open-source LLMs are still far from satisfactoryin mathematical reasoning . It is an open question whether LLMs subpar reasoning capabilityis inherent or due to the the extreme scarcity of high-quality mathematical datasets . As aninitial step towards answer this question, a data generation framework that could create high-qualitymath datasets is required. To this end, current two lines of research struggle in the diversity-validitydilemma: (1) to produce diverse math data, the prompt-based method effectively rephrases mathproblems using LLMs, but may induce errors thus ruining the validity, especially considering therigor of maths; (2) to ensure the validity, template-based methods are often used by rewriting mathproblems with certain rules, sacrificing the diversity and thus confining data scale. To address this dilemma, we propose a novel neuro-symbolic framework that automatically generateshigh-quality, supervised mathematical data. The merit of this paradigm lies in leveraging both neuraland symbolic strengths: (1) the math problem is generated in the symbolic space, achieving diversitythrough systematic sampling, while maintaining validity through symbolic solvers; (2) the translationfrom the symbolic space back to the natural language space can be effectively supported by LLMs,ensuring the consistency between newly generated formal problems and their corresponding naturallanguage versions. Our framework, as illustrated in , initiates with the formalization of the original problemexpressed via the math symbolic tools. Next, it mutates the formal problem into an evolved version,and then derives a new natural language problem by informalization. Specifically, we design amutation mechanism, including various simplification and complication strategies, such that thenew problems can be generated with a controllable complexity. As shown in , our mutation",
  "Mutate": "Q: A rectangle has a perimeter of 30 units. The length of the rectangle is 14 units more than twice its width. However, due to a mistake in measurement, the actual perimeter is 4 units less than twice the sum of its width and length. What is the width of the rectangle? Q: A rectangle has a width and a length. The perimeter of the rectangle is 17 units. The length is calculated using the expression: first, multiply the width by 8, then add 31, subtract 38, and finally, multiply the result by 16. The perimeter is also given by another expression: add 76 to twice the sum of the width and length, then subtract 63 and 30. What is the width of the rectangle?A: 1",
  "Validity of formal mutates guaranteed by solversDiversity of formal mutates achieved by projected MCMC": ": The overview of our neuro-symbolic data generation framework. The framework comprisesthree steps: (1) Formalize the seed problem into its symbolic version. (2) Mutate the symbolicproblem to create new variants. (3) Translate the variants in symbolic form back to the naturallanguage version. Additionally, we prompt GPT-4 to generate reasoning paths, which are verified bysymbolic solvers, as part of the supervision. mechanism can properly adjust the complexity of generated problems, and the exposure to morecomplex math problems can improve the LLMs reasoning capability. Moreover, to ensure the datavalidity and achieve higher generation diversity, we combine the symbolic solving with the randomsampling through the projected Markov chain Monte Carlo technique . #Reasoning-steps in Training 0.00 0.03 0.06 0.09 0.12 0.15 Reasoning-steps Density OriginLevel-1Level-3Level-5 Various Difficulty Levels of Mutation #Reasoning-steps in Training Reasoning-errors (%) in Test : The performance of our proposed mutation mechanism. The first figure illustrates thatthe generated problems with higher difficulty levels lead to more reasoning steps of GPT-4. Thesecond figure shows that the gradual incorporation of more difficult problems consistently improvesthe LLMs reasoning capability. Empirical evaluation on GSM8K and MATH demonstrates the effectiveness of the proposedmethod. Particularly, we use the proposed framework to generate a mathematical dataset of 620Kexamples for supervised fine-tuning. The experimental results show that, the fine-tuned models onLLaMA-2 and Mistral-7B significantly outperform the existing open-source counterpartson both GSM8K and MATH datasets, as well as two out-of-domain datasets SVAMP andASDiv . On the GSM8K dataset, the model fine-tuned on Mistral-7B even outperforms GPT-3.5-Turbo (by 2.4%), a proprietary model with an order of magnitude larger parameters. Additionally, weevaluate the scalability of our method, and observe consistent performance improvements, as the sizeof training data increases. This upward trajectory suggests a promising avenue for further enhancingLLMs mathematical capabilities.",
  "Mutation": "Compared to existing data generation methods, the key feature of our framework lies in the mutationof math problems within the symbolic space, enabling systematic sampling and symbolic solving.Technically, our mutation mechanism includes several simplification and complication strategies,to control the complexity of the generated math problems. The overall framework of our problemmutation method is summarized in Algorithm 1.",
  "Formalization": "We first provide the formalization of math problems, based on which the mutation mechanism isoperated. Specifically, we adopt the SMT-LIB language , a standard notation compatible withprevalent SMT solvers (e.g., Z3 , CVC5 , and MathSAT ). It can also be easily extendedfor symbolic calculators (e.g., SymPy ) and numerical solvers (e.g., SciPy ). With SMT-LIBlanguage, the math problem in the following structure is enabled: Goalg := min | max | solve f(x)Constraintsh := h1 h2 | h1 h2 | ite(h1, h2, h3) |x. e1(x) e2(x) | x. e1(x) e2(x) |e1 e2, {, , >, <, =, =}Expressionse := c | x := (x1, . . . , xn) | foo(x) |e1 e2, {+, , , }",
  "DomainsD := N | N+ | R | C": "where c denotes a constant, x denotes an n-dimensional variable, ite denotes the if-then-else structure,foo refers to an interpreted function (e.g., trigonometric, logarithmic or user-defined ones) on thedomain, and g and h represent any function of interest (can include quantifiers). In particular, wepre-defined a series of interpreted functions, such as summation, binomial, gcd, lcm, derivate,and integral, which facilitate the formalization of most high-school level mathematical problems(excluding geometry) within the above SMT-LIB language.",
  "We perform simplification by systematically considering expression reduction and constraint reduc-tion, which can be attained through heuristic tactics provided by standard symbolic solvers": "Specifically, we apply the simplify tactic for expression reduction, which involes operations suchas constant or variable folding (e.g., x + 0 x or y + x x y), expression expansion (e.g.,(x + 1)2 x2 + 2x + 1), and function application (e.g., (x = 2) (y = log(x)) y = log(2));we also perform symbolic and numerical computations for further reductions (e.g., gcd(2x, 6y) 2gcd(x, 3y) and sin(/6) 0.5). For constraint reduction, we mainly employ the Gaussian elimination tactic gaussian_elim (e.g.,x = 2 y x + z y 2 + z). To handle the if-then-else term, we apply the elim_term_itetactic to decompose it by introducing a fresh variable (e.g., ite(x > y, x, y) > z (k > z) (x >y k = x) (x y k = y)). For constraints involving quantifiers, we strive to eliminate themusing the qe tactic (e.g., y.(y > 0) (x = y + 2) x > 2). Appendix B provides more examplesillustrating these simplifications.",
  "a(b + c) + d = 152b(c + a) e = 162c(a + b) = 170d + e = 150d e = 78a, b, c, d, e N+": "However, such a strategy is non-trivial in practice. The first challenge lies in the validity aspect, i.e.,the math problem is often carefully designed, and thus a random mutation may ruin their well-definedstructure. Consider the running example problem (M1), which has been normalized for simplicity.In this problem, a reckless mutation can easily violate the positive integer constraints, causing theproblem ill-defined and unsolvable. To address this issue, we equip each mutation with an auxiliary variable, followed by symbolicsolvers to ensure the problem remains well-defined. Continuing with the previous example, weintroduce three auxiliary variables, denoted by z1, z2, z3, and then mutate the problem as (M2),where 1, 2, 3 {+, , , } represent three random operators. Furthermore, we instantiatee1, e2, e3 by interpreted functions, i.e., e1 = foo1(z1), e2 = foo2(z2), and e3 = foo3(z3), wherefoo1, foo2, foo3 are randomly selected from foo(z) = z | log(z) | exp(z) | arcsin(z) | .For our running example, we simply choose the identity function for foo1, foo2, foo3, and set1 = , 2 = +, 3 = . Using symbolic solvers to compute a feasible solution of (z1, z2, z3),we derive a new and well-defined problem (M3). The subsequent challenge is to ensure the diversity of the mutated problems, which now becomeshow to make the solutions of auxiliary variables sufficiently diverse. This is essentially a modelcounting problem , and current symbolic solvers still underperform in this regard . Tothis end, we instead opt for auxiliary variable solution generation via the projected Markov chainMonte Carlo (projected MCMC) . Simply put, projected MCMC first perturbs a subsetof variables (projected random walk), and then resolves the remaining part (inverse projection viasymbolic solvers), which ensures both diversity and validity of the variable solutions. Finally, to complicate the constraints, one can easily reverse the process of simplification. For ourrunning example, we can reverse the Gaussian elimination with refreshed variables, obtaining thefinal form (M4), which is then included as a new problem in the dataset.",
  "Informalization": "Informalization aims to translate a formal problem back to natural language without the loss ofsoundness . As shown in Example 1, a simple, one-line instruction follows the formally posedSMT-LIB problem, serving as the input. Then, GPT-4 interprets the formal problem as a new mathword problem. The key challenge of informalization lies in ensuring a consistent conversion, i.e., the natural languageproblem informalized by GPT-4 should align with the formal solution given by symbolic solvers.Since it is difficult to directly measure this consistency, we instead use GPT-4 to generate a solutionfor each informalized problem, and then calculate the consistency rate between the solutions fromGPT-4 and those from symbolic solvers as a surrogate metric. Furthermore, we observe that, if theproblem is incorrectly informalized, GPT-4s solutions almostly cannot be confirmed by symbolicsolvers (i.e., zero false positive). Therefore, the surrogate consistency rate can be regarded as a lowerbound to the true consistency rate.",
  "GPT-4 output:Sara bought a pair of shoes for $50.00 and a dress for $200.00. If Rachel has twice theamount that Sara spent in total, how much is Rachels budget?": "To further improve conversion consistency, in addition to the basic zero-shot learning template inExample 1, we investigate the effects of the following operations, whose detailed examples areavailable in Appendix C. : Consistency rate of six different opera-tions used in informalization: (1) Mutation; (2)Few-shot learning; (3) Comment generation; (4)Math-word instruction; (5) Problem modification;(6) Variable refresh. We recommend two patterns(i.e., P1: 1-5 and P2: 1-3&6), both of which canachieve satisfactory results.",
  "(6) Variable refresh. We standardize the naming of all introduced variables (e.g., rachel_budget x_1), to eliminate the impact of math word problems": "Different combinations of the above operations result in different patterns. The effects of sometypical patterns are shown in , where the results are evaluated on 1,000 problems randomlysampled from GSM8K. The basic pattern in Example 1 yields a consistency rate of 75.6%. Themutation operation alone indeed degrades the consistency, but its combination with other operatorscan further boost the informalization performance. In practice, we use two different patterns fordifferent informalization styles: the first pattern (P1) tends to generate math word problems, whereasthe problems generated by the second pattern (P2) tend to be pure math problems.",
  "Experimental Setup": "Dataset. We conduct our data generation on the training sets of two popular mathematical reasoningbenchmarks: GSM8K and MATH . GSM8K is a dataset comprising high-quality gradeschool math problems, which contains 7,473 training data and 1,319 testing data. MATH is adataset comprised of challenging competition math problems, spanning seven subjects includingPrealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, andPrecalculus. There are 7,500 training data and 5,000 testing data in the MATH dataset. Additionally,we include two mathematical reasoning datasets, i.e., SVAMP and ASDiv , to evaluate theout-of-domain generalizability of the models fine-tuned on the data generated from GSM8K andMATH datasets. Comparison Methods. In our experiments, we compare the models trained using our generated datawith existing state-of-the-art open-source mathematical reasoning models, including WizardMath ,MuggleMATH , MAmmoTH , and MetaMath . We also conduct a thorough comparisonbetween our math generation method and the bootstrapping method employed in MetaMathQA ,which is presently the most extensive open-source dataset for mathematical reasoning. Data Generation Details. We use our mutation mechanism to generate a series of problems withvarying levels of difficulty, and the specifics are as follows. Starting with a problem from the originaldataset as a seed, we first perform simplification to the problem, and define this new version as level-0.Then, we randomly apply one expression complication step and one constraint complication step tothe level-0 version, deriving a more difficult problem (level-1 version); and such complications canbe repeated to obtain more difficult problems. For the GSM8K dataset, we create datasets across fivelevels of difficulty, with 30K examples at level-0 and 100K examples for the remaining four levels.As for the MATH dataset, we establish four levels of difficulty, where level-0, level-1, level-2, andlevel-3 contain 70K, 120K, 120K, and 120K examples, respectively. Particularly, for some problemsin the MATH dataset that cannot be solved by symbolic solvers, we directly prompt GPT-4 to rephrasethe problem and ignore the solution verification. The number of generated problems without solutionverification varies across problem categories, and the details can be referred to Appendix D. In total,we generated 860K math problems based on the proposed framework to construct our dataset. Each generated math problem consists of a natural language problem description informalized byGPT-4 (version 0710), a final answer outputted by the symbolic solver, and a reasoning path from theproblem to the final answer. The reasoning path for each problem is also generated by GPT-4, whichis further verified by the corresponding answer derived from symbolic solvers.",
  "Empirical Results": "RQ1: Efficacy. Using the generated math dataset, we fine-tune the LLaMA-2 base models of 7B and13B parameter sizes, as well as the Mistral 7B base model. The fine-tuned models, as well as thecomparison methods, are evaluated on the GSM8K and MATH datasets. : Performance comparison among existing mathematical reasoning models fine-tuned on threebase models (LLaMA-2 7B, LLaMA-13B, and Mistral 7B). The best performance is in bold. Thedelta performance between our model and other SOTA LLMs on each dataset is also reported.",
  "Model performance is re-evaluated using Pass@1 of CoT prompt": "As shown in , our approach achieves the best performance among the baseline modelsacross different model scales. Compared to LLMs with the LLAMA-2 7B base model, our modeldemonstrates a fair improvement in accuracy, surpassing them by at least 10.6% on the two datasets.For our model fine-tuned using the LLAMA-2 13B base model, our model achieves an accuracy of84.1% and 33.7%, outperforming existing SOTA model by 10.1% and 11.3%. When fine-tuned onthe Mistral 7B base model, our model still attains the best performance with an increase in accuracyof 3.6% and 4.3%, respectively. Notably, our model even slightly outperforms GPT-3.5-Turbo (80.8%and 34.1%) by 6.0% on the GSM8K dataset and 3.2% on the MATH dataset. In addition to the above competitors, we also compare our models with tool-based models, andprovide the results in Appendix E.2. We summarize two observations here. First, tool-based modelstend to over-rely on external tools, and thus do not necessarily improve the inherent reasoning abilityof LLMs. Second, although the tool-based models perform better on the MATH dataset (whichfrequently entails complex calculations), they still underperform on the GSM8K dataset (whichemphasizes knowledge reasoning but involves simpler calculations). RQ2: Efficiency. To illustrate the data efficiency of our method, we carry out a comparativeexperiment with current SOTA method MetaMathQA . The MetaMathQA dataset comprises240K data bootstrapped from the GSM8K training dataset and 155K data from the MATH trainingdataset. To ensure a fair comparison with MetaMathQA, we use the same data budget. Additionally,we expand the MATH data of the MetaMathQA dataset to 430K, aligning its size with that of ourgenerated data. Then, we fine-tune LLaMA-2-7B models on the 240K GSM8K augmented data, aswell as the 155K and 430K MATH augmented data, respectively. The performance of fine-tuned models are given in . We also evaluate the models on twoout-of-domain datasets, SVAMP and ASDiv. The results confirm the efficiency of our framework.With an equal generation budget of 240K for the GSM8K dataset and 155K for the MATH dataset, ourmethod exhibits accuracy improvements, ranging from 2.1% to 24.4% across the four datasets. Thissuperiority is consistent with the model trained on 430K MATH generation data, with improvementsof 19.9%, 3.2%, 12.7%, and 19.2%, respectively. RQ3: Generability. Despite we carefully ensure that our mutations on the training set do not accessthe test set, we still provide a series of analysis about the potential data contamination or overfittingissues. We first use the memorization detection method introduced in Minerva . Specifically, weselect 150 problems with the highest majority vote score and then compute the BLUE score on solutions of trained Mistral 7B, GPT-4, and the ground-truth. The results of our method andMetaMath are provided in , which show that our BLUE score on the test set is (1) muchlower than our BLUE score on the training set; (2) is consistent with that of MetaMath. Hence, thereis no evidence that the mutation contaminates the test set. Second, in addition to the two out-of-distribution datasets SVAMP and ASDiv, we conduct exper-iments on another benchmark DyVal , which avoids the leak of test set through dynamicallygenerating new benchmarks. The results of our models, alongside those of the comparison models,are provided in Appendix E.3. In summary, our models demonstrated superior performance in 11 out : Comparison between our method andMetaMathQA (MMQA) with the same data bud-gets. The models are fine-tuned using LLaMA-2-7B base model, and evaluated on GSM8K, MATH,SVAMP, and ASDiv datasets. The results illusratethe high quality of our generated data.",
  "3.2 12.7 19.2": ": BLUE scores between the outputof our fine-tuned model, versus the ground-truth solution and GPT-4 output. The model isfine-tuned on the Mistral 7B base model, andMetaMath Mistral 7B model is also included asa reference. The results show that our methoddoes not induce data contamination. 0.0 0.2 0.4 0.6 0.8 BLUE 0.4 0.6 0.50.60.6 0.8 0.50.5",
  "Test Set": "MMQAOurs of 12 cases and delivered competitive results in the remaining case. Particularly, as the complexity ofthe tasks increased, our models exhibited a relatively robust performance compared to other models. Finally, we include an additional experiment on the Hungarian High School National Finals Examdataset , whose problems are newly collected at 2023. We manually check 33 testing problemsbased on the provided ground-truth answer, and our model correctly solved 14 problems and partiallysolved 6 problems, resulting in an exam score of 44. The result is comparable to GPT3.5-turbo (i.e.,41 exam score), conforming the generalizability of our method. 070 140 210 280 350 GSM8k 070 140 210 280 350",
  "ASDiv": "070 140 210 280 350 MATH Size of Training Dataset (Thousands) Accuracy (%) MMQAOurs : Performance curves of the LLaMA-2-7B models fine-tuned on various scales of datasets.The two datasets are generated by our approach and MetaMath (MMQA). The performance can beconsistently enhanced by increasing the amount of data generated using the proposed framework. RQ4: Scability. To explore the scalability of our framework, we fine-tune the LLaMA-2 7B modelusing our generated datasets of various sizes and difficulties. To be specific, we progressivelyincorporate a 30K dataset, along with four additional 70K datasets generated from GSM8K. Note thatthese five datasets are randomly sampled from five different levels of difficulty. Five LLaMA-2-7Bmodels are fine-tuned based on these datasets, and the scalability curves are shown in . Wealso include MetaMathQA with the same data settings as a reference. Since MetaMathQA cannotinherently group the dataset into various difficult levels, we construct five datasets by incrementallyrandom sampling from the GSM8K subset of the MetaMathQA dataset. The results presented in indicate the promising scalability of our method. That is, as thesize of data increases, the accuracy of the model consistently improves. In contrast, the performanceenhancement observed in MetaMathQA is limited and starts to diminish as the data size reaches 70K.We also present the models performance on the other three out-of-domain datasets in this case, i.e.,SVAMP, ASDiv, and MATH datasets. The results demonstrate that the scalability of our method isrobust and generalizable, while MetaMathQA hardly guarantees such consistency. We also investigate the diversity gain relative to the original dataset for each difficulty level. Theresults are provided in Appendix E.4. It is observed that the dataset consisting of the same difficultylevel cannot further improve the diversity with a larger data budget. On the contrary, the diversitygain of the dataset comprising all difficulty levels continues to increase as the data budget grows.",
  "Related Work": "Recent surveys have comprehensively discussed the current advances in the mathematicalreasoning of LLMs. Here, we review three main lines of existing work on enhancing the mathematicalreasoning for LLMs related to our study: prompt-based methods, rephrasing-based methods, andtool-based methods. Prompt-based Method. Prompt-based methods aim to harness the inherent capabilities of LLMs bycarefully designing appropriate input prompts without tuning the model parameters. This line of workstarts from the observation that LLMs can effectively tackle more math problems when provided witha simple Chain-of-Thought (CoT) prompt, i.e., Lets think step by step . Building upon theCoT prompt, Wang et al. further propose to consolidate multiple reasoning paths based on theself-consistency of correct answers. Later, several researchers propose to prompt LLMs to decomposecomplex problems. For example, Zhou et al. introduce the least-to-most strategy that promptsLLMs to break down the original problem into a series of sub-problems. Khot et al. furtherboost this strategy, by assigning each sub-problem to the corresponding LLM that is specificallyoptimized for it. Finally, few-shot prompting, e.g., Few-shot CoT and Complex CoT , hasalso been studied to enhance the reasoning performance of LLMs. To further improve the few-shotprompting, the prompt retrieval is proposed to automatically select high-quality examples ,while the prompt compression is explored to include more examples in restricted context by pruningeach example . Rephrasing-based Method. The second line of existing work aims to generate additional math data,based on which the mathematical reasoning capability of LLMs can be established via supervisedfine-tuning. To address the data scarcity issue, current research mainly focuses on rephrasing theproblem or the answer. For the answer rephrasing, Magister et al. adopt the PaLM and GPT-3to generate CoT math data, resulting in improved performance of the T5 model on math reasoningtasks. To mitigate the inclusion of incorrect answers during the supervised fine-tuning, RFT introduces a rejection sampling strategy, whereas AFT trains an LLM to categorize them.Regarding the problem rephrasing, WizardMath proposes a reinforced evol-instruct method. Itinstructs ChatGPT and trains a new LLM to rephrase the problem, equipped by a reward model forevaluating the quality of generated problems. Combining the rephrasing of problems and answerstogether, MuggleMATH builds the AugGSM8K dataset based on prompting GPT-3.5 and GPT-4.MetaMath develops a question bootstrapping method based on LLMs, unifying rephrasing,self-verification , FOBAR , and answer augmentation strategies, obtaining the MetaMathQA.Xwin-Math is a peer study that significantly enhances the reasoning capacity of LLMs usingproblems generated by GPT-4 Turbo. In contrast, our work focuses on generating verifiable problemsthrough controllable mutations, rather than relying entirely on the GPT model. Our proposed method also falls into this category. In contrast to existing methods directly promptingLLMs to rephrase the problem, we mutate the problem in the formal symbolic space, resulting in amore controllable mutation mechanism that ensures both the validity and diversity of the generatedproblems. Moreover, the quality of reasoning paths is also guaranteed by the symbolic solvers. Tool-based Method. Tool-based methods aim to enhance the math solving performance of LLMsby instructing them to use external math tools. For instance, PoT (Program of Thought) andPAL propose to prompt the LLMs to delegate the computation to a program interpreter, whichcan be executed to obtain the final answer. To further improve the tool-using ability of LLMs,MathCoder constructs a math dataset containing problems and their code-based solutions forthe supervised fine-tuning; MAmmoTH builds a dataset that combines CoT and PoT reasoning,enabling LLMs to perform hybrid inference. Since the interaction with math tools can furtherboost the performance of LLMs, TVA includes the Isabelle theorem prover to check eachreasoning step and guide the reflection of LLMs; Tora generates interactive tool-use trajectorieson mathematical datasets and then performs imitation learning on the annotations.",
  "Limitations": "The Capability of Symbolic Solvers. The effectiveness of our approach significantly hinges on thesymbolic solvers. However, existing mathematical tools (e.g., Z3 , SymPy , and SciPy )face limitations when it comes to expressing and solving a wide array of mathematical problems.For instance, the Z3 SMT solver struggles with expressing higher-order concepts like gcd and lcm,while the SymPy encounters difficulties in solving inequalities involving multiple variables. Inour framework, we integrate five mathematical tools, i.e., Z3, CVC4 , MathSAT , SymPy,and SciPy, and employ SMT-LIB as a unified formal language to enhance the performance ofsymbolic solving. The Expressiveness of Mutations. The mutation operators used within our framework remainlimited, especially in generating more difficult problems (e.g., college- and even IMO-level mathproblems). One of our future work is to introduce more mutation operators, further increasing theproblem difficulty. A possible strategy is the problem fusion , which fuses two formal problemsinto a single, new problem, rather than merely modifying an individual problem. Moreover, theinformalization facilitated by LLMs can effectively mitigate the unnaturalness issue stemming frombrute-force fusion. The Dependence on GPT-4. GPT-4 is involved in our framework to carry out the informalizationand generate the reasoning paths. We also consider the possible solutions that the dependence onGPT-4 can be gradually removed. First, by leveraging our generated formal-informal pairs, wecan fine-tune a new LLM specifically for the informalization. Second, it is possible to bypassthe generation of reasoning paths, through curriculum learning instead of supervised fine-tuning. Particularly, the reward in the curriculum learning can be determined by whether the generatedsolution is consistent with symbolic solvers , and the curriculum progresses by incorporating problemsof various difficulty levels.",
  "Conclusion": "This paper explores the question of whether sufficient exposure to high-quality mathematical datacould enhance LLMs inherent mathematical reasoning capability. We identify a key challenge inbalancing diversity and validity in current math problem generation methods. To tackle this challenge,we propose a neuro-symbolic framework that initially generates formal mathematical problems andthen informalizes them back into natural language versions. By casting the data generation into theformal language space, the diversity and validity of the generated math problems can be effectivelyensured by the systematic sampling and symbolic solvers. Building upon this, we carefully devisea mutation mechanism, establishing the math dataset encompassing various difficulty levels, andprompt the LLMs to accomplish informalization. Through empirical experiments, we demonstratethat our neuro-symbolic data generation framework significantly enhances the performance of variousLLMs in mathematical reasoning tasks, surpassing the current state-of-the-art open-source models.The results also suggest a promising pathway for further enhancing LLMs mathematical capabilities. In future work, we intend to expand the expressiveness of mutations and enhance the capability ofsymbolic solvers to support more types of problems, such as inequality problems. Our goal is tooffer a data generation framework to automatically generate high-quality, supervised datasets forLLMs. We expect that our neuro-symbolic data generation framework can provide a potential solutionfor LLMs to solve the problem of data scarcity, and thereby facilitate in building more LLMs indownstream tasks. Further, our framework has the potential to be integrated with recent studies ,which only require problems and final answers. We appreciate the anonymous reviewers for their valuable insights and helpful comments. This workis supported by the National Natural Science Foundation of China (Grants #62025202), the FrontierTechnologies R&D Program of Jiangsu (BF2024059), and the Key Program of Jiangsu Science Foun-dation (BK20243012). Xian Zhang () and Xiaoxing Ma ()are the corresponding authors. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu,Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problemswith language models. In Advances in Neural Information Processing Systems 35, 2022.",
  "Artur dAvila Garcez and Lus C. Lamb. Neurosymbolic AI: the 3rd wave. Artificial IntelligenceReview, 56(11):1238712406, 2023": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXivpreprint arXiv:2303.18223, 2023. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomousagents. arXiv preprint arXiv:2308.11432, 2023. Enkelejda Kasneci, Kathrin Seler, Stefan Kchemann, Maria Bannert, Daryna Dementieva,Frank Fischer, Urs Gasser, Georg Groh, Stephan Gnnemann, Eyke Hllermeier, et al. Chatgptfor good? on opportunities and challenges of large language models for education. Learningand individual differences, 103:102274, 2023. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large languagemodels. ACM Transactions on Intelligent Systems and Technology, 2023. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun RLoomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-levelscientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635,2023.",
  "Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. Dyval:Graph-informed dynamic evaluation of large language models. CoRR, abs/2309.17167, 2023": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers tosolve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, DawnSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. InAdvances in Neural Information Processing Systems Track on Datasets and Benchmarks, 2021.",
  "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simplemath word problems? arXiv preprint arXiv:2103.07191, 2021": "Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open languagemodel for mathematics. arXiv preprint arXiv:2310.10631, 2023. Weiming Feng, Kun He, and Yitong Yin. Sampling constraint satisfaction solutions in the locallemma regime. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory ofComputing, pages 15651578, 2021. Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxing Ma, L Jian, et al. Softenedsymbol grounding for neuro-symbolic systems. In The Eleventh International Conference onLearning Representations, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, LukasBlecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, AnthonyHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, MadianKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, ZhengYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurlienRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundationand fine-tuned chat models. CoRR, abs/2307.09288, 2023. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-lot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,Llio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,Thomas Wang, Timothe Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825,2023. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating anddeveloping english math word problem solvers. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages 975984, 2020.",
  "Leonardo De Moura and Nikolaj Bjrner. Z3: An efficient smt solver. In Internationalconference on Tools and Algorithms for the Construction and Analysis of Systems, pages337340. Springer, 2008": "Haniel Barbosa, Clark Barrett, Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann,Abdalrhman Mohamed, Mudathir Mohamed, Aina Niemetz, Andres Ntzli, et al. cvc5: Aversatile and industrial-strength smt solver. In International Conference on Tools and Algorithmsfor the Construction and Analysis of Systems, pages 415442. Springer, 2022. Roberto Bruttomesso, Alessandro Cimatti, Anders Franzn, Alberto Griggio, and RobertoSebastiani. The mathsat 4 smt solver: Tool paper. In Computer Aided Verification: 20thInternational Conference, CAV 2008 Princeton, NJ, USA, July 7-14, 2008 Proceedings 20,pages 299303. Springer, 2008. Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondrej Certk, Sergey B Kirpichev,Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K Moore, Sartaj Singh, et al. Sympy:symbolic computing in python. PeerJ Computer Science, 3:e103, 2017. Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Courna-peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0:fundamental algorithms for scientific computing in python. Nature methods, 17(3):261272,2020.",
  "Mark Jerrum and Alistair Sinclair. The markov chain monte carlo method: an approach toapproximate counting and integration. Approximation Algorithms for NP-hard problems, PWSPublishing, 1996": "Stefano Ermon, Carla Gomes, and Bart Selman. Uniform solution sampling using a constraintsolver as an oracle. In Proceedings of the Twenty-Eighth Conference on Uncertainty in ArtificialIntelligence, pages 255264, 2012. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik,and Christian Szegedy. Autoformalization with large language models. Advances in NeuralInformation Processing Systems, 35:3235332368, 2022. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical rea-soning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583,2023. Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, ChuanqiTan, Xiang Wang, and Chang Zhou. Query and response augmentation cannot help out-of-domain math reasoning generalization. CoRR, abs/2310.05506, 2023.",
  "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and WenhuChen. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR,abs/2309.05653, 2023": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematicalquestions for large language models. arXiv preprint arXiv:2309.12284, 2023. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, VinayRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solvingquantitative reasoning problems with language models.Advances in Neural InformationProcessing Systems, 35:38433857, 2022. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automaticevaluation of machine translation. In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics, pages 311318, 2002.",
  "Keiran Paster. Testing language models on a held-out high school national finals exam. 2023": "Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large languagemodels for mathematical reasoning: Progresses and challenges. In Proceedings of the 18thConference of the European Chapter of the Association for Computational Linguistics, pages225237, 2024. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learningfor mathematical reasoning. In Proceedings of the 61st Annual Meeting of the Association forComputational Linguistics, pages 1460514631, 2023. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, HaoCheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematicalreasoning of foundation models in visual contexts. In Proceedings of the 12th InternationalConference on Learning Representations, 2024. Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.Large language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal,D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems,pages 2219922213, 2022. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, AakankshaChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in languagemodels. In Proceedings of the 11th International Conference on Learning Representations,2023. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, DaleSchuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most promptingenables complex reasoning in large language models. In Proceedings of the 11th InternationalConference on Learning Representations, 2023. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, andAshish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. InProceedings of the 11th International Conference on Learning Representations, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi,Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large languagemodels. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,Advances in Neural Information Processing Systems, pages 2482424837, 2022. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-basedprompting for multi-step reasoning. In Proceedings of the 11th International Conference onLearning Representations, 2023. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought promptingin large language models. In Proceedings of the 11th International Conference on LearningRepresentations, 2023. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, PeterClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structuredmathematical reasoning. In Proceedings of the 11th International Conference on LearningRepresentations, 2023.",
  "Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu,and Zhifang Sui. Making large language models better reasoners with alignment. CoRR,abs/2309.02144, 2023": "Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu,and Jun Zhao. Large language models are better reasoners with self-verification. In Findings ofthe Association for Computational Linguistics: EMNLP 2023, pages 25502575, 2023. Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T Kwok.Forward-backward reasoning in large language models for mathematical verification. arXivpreprint arXiv:2308.07758, 3, 2023.",
  "Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, andHouwen Peng. Common 7b language models already possess strong math capabilities. CoRR,abs/2403.04706, 2024": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompt-ing: Disentangling computation from reasoning for numerical reasoning tasks. Transactions onMachine Learning Research, 2023. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,and Graham Neubig. PAL: program-aided language models. In Proceedings of the 40thInternational Conference on Machine Learning, pages 1076410799, 2023. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, LinqiSong, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms forenhanced mathematical reasoning. arXiv preprint arXiv:2310.03731, 2023. Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy, Kilian Q Weinberger, andYuhuai Wu. Dont trust: Verify grounding LLM quantitative reasoning with autoformalization.In The Twelfth International Conference on Learning Representations, 2024. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen,et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprintarXiv:2309.17452, 2023. Clark Barrett, Christopher L Conway, Morgan Deters, Liana Hadarean, Dejan Jovanovic, TimKing, Andrew Reynolds, and Cesare Tinelli. Cvc4. In Computer Aided Verification: 23rdInternational Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23,pages 171177. Springer, 2011. Dominik Winterer, Chengyu Zhang, and Zhendong Su. Validating smt solvers via semanticfusion. In Proceedings of the 41st ACM SIGPLAN Conference on programming language designand implementation, pages 718730, 2020.",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficientfinetuning of quantized llms. In Advances in Neural Information Processing Systems, 2023": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,Lu Wang, and Weizhu Chen.Lora: Low-rank adaptation of large language models.InProceedings of the 1th International Conference on Learning Representations, 2022. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, PercyLiang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model.Technical report, 2023.",
  "Marco Gario and Andrea Micheli. Pysmt: a solver-agnostic library for fast prototyping ofsmt-based algorithms. In SMT Workshop 2015, 2015": "Charles R Harris, K Jarrod Millman, Stfan J Van Der Walt, Ralf Gommers, Pauli Virtanen,David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. Arrayprogramming with numpy. Nature, 585(7825):357362, 2020. Zhoulai Fu and Zhendong Su. Xsat: a fast floating-point satisfiability solver. In ComputerAided Verification: 28th International Conference, CAV 2016, Toronto, ON, Canada, July 17-23,2016, Proceedings, Part II 28, pages 187209. Springer, 2016. Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and MartinVechev. Dl2: training and querying neural networks with logic. In International Conference onMachine Learning, pages 19311941. PMLR, 2019. Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, L Jian, et al. Learn-ing with logical constraints but without shortcut satisfaction. In The Eleventh InternationalConference on Learning Representations, 2022.",
  "GPT-4 output:Sara went shopping and bought a dress for $200.00 and a pair of shoes for $50.00. What isthe total amount Sara spent on her shopping trip?": "We elucidate the simplification strategy through Example 2, which is selected from the GSM8Kdataset. In this formal problem, we can conduct one step of Gaussian elimination, i.e., randomlysolving and removing a variable (e.g., rachel_budget = 500). Then, we derive the new problemwith its informalized version. Furthermore, we present another example, specifically Example 3,sourced from the MATH dataset. Here, the new problem is generated through the simplificationstrategy that randomly performs some calculations.",
  "GPT-4 output:": "(declare -fun num () Real)(declare -fun den () Real)(declare -fun fraction () Real); Define the numerator and denominator expressions(assert (= num (+ 2 4 (- 8) 16 32 (- 64))))(assert (= den (+ 4 8 (- 16) 32 64 (- 128)))); Define the fraction as the numerator divided by the denominator(assert (= fraction (/ num den )))(check -sat)(get -value (fraction ))",
  "Mutation & Informalization:": "(declare -fun num () Real)(declare -fun den () Real)(declare -fun fraction () Real); Define the numerator and denominator expressions(assert (= num (+ (- 2) 16 ( -32))))(assert (= den (+ (- 4) 32 (- 64)))); Define the fraction as the numerator divided by the denominator(assert (= fraction (/ num den )))(check -sat)(get -value (fraction ))",
  "DAdditional Experimental Details": "Generation Details. In the GSM8K dataset, each problem can be accurately formalized into theSMT-LIB format and successfully mutated into a new version. However, some problems, particularlyin the precalculus and geometry categories of the MATH dataset, cannot be formalized or mutatedeffectively. Overall, out of a total of 7,500 problems, 822 cannot be formalized into the SMT-LIBformat, and approximately 3,600 formalizations are inaccurate although they remain usable for thedata generation. To address this issue, we strategically added a proportional number of mutatedproblems derived by directly prompting GPT-4, bypassing solution verification. The detailed countsof these problems are presented in . Training Details. In this paper, we fully fine-tune the LLAMA-2-7B and LLAMA-2-13B modelsusing four H800 NVIDIA GPUs. Each model is trained for 3 epochs with a batch size of 128 and alearning rate of 2e-5. For the fine-tuning of the LLAMA-2-70B model, we adopt the QLoRA method with a learning rate of 1e-4. The rank and alpha of LoRA are set to 96 and 16,respectively, with a dropout rate of 0.05 between the two matrices. The LoRA modules are added toboth the attention and MLP layers. The 70B model is fine-tuned using eight A800 NVIDIA GPUs.",
  "For answer extraction and accuracy calculation, we follow the code of WizardMath to extractthe answer after the phrase The answer is": "Symbolic Solvers. We integrate five symbolic solvers, Z3, CVC4, MathSAT, SymPy, and SciPy basedon the PySMT framework . To be specific, the PySMT intrinsically includes Z3, CVC4, andMathSAT, and we further extend its support to encompass SMT-LIB version 2.5, which incorporatesmore commands like define-rec. Next, we proceed to serialize the SMT-LIB format into SymPyexpressions, and attempt to find solutions using SymPys solve function. In addition, the SymPyexpressions are also encoded as NumPy functions, thereby enabling the using of SciPysoptimization modules, such as differential_evolution and minimize. Note that we introduce afuzzy-logic-like strategy in the encoding, which combines the equalities and inequalities intoa loss function, subsequently enabling optimization methods for problem-solving tasks.",
  "E.1Detailed results on MATH dataset": "We present detailed results across different categories in the MATH dataset in . The numbers1-7 correspond to Algebra, Counting and Probability, Geometry, Intermediate Algebra, NumberTheory, Prealgebra, and Precalculus, respectively. The results indicate that Algebra is easier toimprove, as evidenced by its higher mutation success rate. It is worth noting that improvements inCounting and Number Theory are reasonable because related mutation operators (e.g., binomial,gcd, lcm, etc.) are included in our framework. However, Precalculus and Geometry are still notwell-supported. For example, the concept of triangle cannot currently be correctly expressed in theSMT-LIB format, resulting in relatively low improvement rates.",
  "E.2Comparison to tool-based methods": "We compare our model with the tool-based models in . Although tool-based models achievegood performance with tools, they meet severe performance degradation when tools are not available.This result indicates that training a model using code-based and language-based rationales does notnecessarily enhance the intrinsic reasoning ability; instead, it often promotes excessive dependenceon external tools. For datasets that involve complex calculations, such as the MATH dataset, tool-based methods offer certain advantages due to their utilization of the strong capabilities of externaltools. For datasets that emphasize knowledge reasoning but involve simpler calculations, such as the",
  "E.3Experiments on DyVal Datasets": "We also compare our models and existing mathematical reasoning models using DyVal datasetsto further evaluate the generalization ability of our models. DyVal is a flexible evaluation protocol fordynamic evaluation of LLMs, which generates evaluation samples with controllable complexitiesusing directed acyclic graphs (DAGs). We focus on Arithmetic tasks using three DAGs orders:topological (TOPO), reversed topological (REVERSED), and random orders (RAND). Following thesame setting, we generate testing four increased complexity levels {D1, D2, D3, D3}, with tree depthsand widths set to (2, 2), (3, 2), (3, 3), (4, 2). The performance comparison between models fine-tunedon LLaMA2-7B and Mistral-7B are respectively shown in and . The results showthat our models achieve the best performance in 11/12 cases and give a competitive performancein the remaining one case. As the complexity increases, our models achieves a relatively robustperformance compared with the other models.",
  "E.4Diversity Gain across Various Difficulty Levels": "To illustrate the need for various difficulty levels, we calculate the diversity gain relative to the originaldataset for each difficulty level. We apply the same method as in MetaMath to compute diversity gain but use the BERT model as a feature extractor instead. We select data budgets of 35K,50K, and 100K, respectively, and investigate the diversity across four levels, from level-1 to level-4.Additionally, we create a mixed version by sampling data from all levels. The results are shownin , and we can observe that: (1) The higher the difficulty level, the greater the diversityof generated data; (2) The mixed version increases with the growing data budget, and achieves thehighest diversity gain. 35K Data Budget70K Data Budget100K Data Budget4",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: Details of data generation, training and inference process are discussed inAppendix D. We will public the code, as well as the fine-tuned models, for the reproducibility.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: Training and test settings are detailed in Appendix D.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [No]Justification: We cannot provide statistical significance of the experiments due to the limitedGPU resources.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [NA]Justification: The paper does not release new assets up to now.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}