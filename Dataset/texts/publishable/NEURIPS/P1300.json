{
  "Abstract": "AI deployed in the real-world should be capable of au-tonomously adapting to novelties encountered after deploy-ment. Yet, in the field of continual learning, the relianceon novelty and labeling oracles is commonplace albeit un-realistic. This paper addresses a challenging and under-explored problem: a deployed AI agent that continuouslyencounters unlabeled data - which may include both un-seen samples of known classes and samples from novel (un-known) classes - and must adapt to it continuously.Totackle this challenge, we propose our method COUQ Con-tinual Open-world Uncertainty Quantification, an itera-tive uncertainty estimation algorithm tailored for learningin generalized continual open-world multi-class settings.We rigorously apply and evaluate COUQ on key sub-tasksin the Continual Open-World: continual novelty detection,uncertainty guided active learning, and uncertainty guidedpseudo-labeling for semi-supervised CL. We demonstratethe effectiveness of our method across multiple datasets,ablations, backbones and performance superior to state-of-the-art. We will release our code upon acceptance.1",
  ". Introduction": "Real-world AI systems frequently face evolving data dis-tributions due to changes in operating conditions and theemergence of new classes after deployment. To ensure arobust response, AI systems should ideally be able to de-tect these novelties and continuously learn from them, whileminimizing computing and labeling costs. Early researchon continual learning (CL) focused on the important prob-lem of catastrophic forgetting but relied on a so-called Oracle for two critical functions: (i) identifyingnovel test samples and (ii) providing labels for these sam-",
  ". A general open-world, continual learning pipeline. Un-certainty estimation plays a crucial role in various points": "ples. While beneficial for advancing initial CL research,the assumption of an omniscient oracle is unrealistic forreal-world applications. In practice, adaptive AI systemsshould be capable of automatically identifying novelties,a challenging task known as novelty detection or out-of-distribution (OOD) detection. Further, learning from nov-elties usually requires labels for the novel samples (e.g.open-world classification ); yet, annotation of all suchsamples is expensive and impractical. Therefore, for data-efficient model updates, it is advantageous to label only aa small subset of informative data samples judiciously cho-sen from the larger pool of novel samples; a task known assample selection in active learning .Although both these problems of novelty-detection andactive sample-selection have been widely explored, most re-search has been in non-continual settings. There is exten-sive research on cost-effective active labeling for conventional non-continual training, but surprisinglythis has found little adoption in CL models .Moreover, solutions for unsupervised and semi-supervisedCL are scarce and often operate with a verystrong oracle assumption:that in unlabeled input, pastclasses do not appear in conjunction with newly introducedclasses - bypassing the need for novelty detection.For",
  "arXiv:2412.16409v1 [cs.LG] 21 Dec 2024": "such methods, removing the oracle causes significant per-formance degradation. Methods under Generalized Cate-gory Discovery (GCD) do address the scenario inwhich both old and new classes co-occur in the test data.However, they are designed for single-task setups wherethe entire unlabeled dataset is presented to the model atonce for categorization, rather than in a continual, class-incremental manner. These solutions are ill-suited for (post-deployment) continual learning as they assume full dataavailability (labeled and unlabeled) and perform expen-sive full re-trainings poorly scalable to CL. Similarly, mostsolutions for novelty or out-of-distribution (OOD) detec-tion were developed for and evaluatedagainst a single fixed binary partition of known (old) versusnovel classes and not on continual splits. Such conventionalOOD models are not designed to continually integrate andlearn from the detected novel data.Overall, both novelty detection and active learning de-pend on the availability of reliable, high-quality uncertaintyestimates, see figure 1. A variety of uncertainty quantifica-tion techniques have been studied: softmax probability andits temperature-scaled variants ; predictive uncer-tainty from Bayesian Neural Networks ; feature-basedMahalanobis distances and its variants .Thesemethods work well in fixed settings where the model istrained once. However, in CL settings where the modelparameters are continually updated, the quality of uncer-tainty estimates deteriorates as knowledge from novel tasksand classes is incrementally integrated. Such estimates, ifused either for novelty detection or for active sample se-lection, will lead to poor prediction of novelties and/or apoor choice of samples selected for CL. Furthermore, er-rors tend to accumulate as new tasks are encountered, re-sulting in progressively poorer performance over time. Arecent model, incDFM , attempted to address this prob-lem by proposing a solution to continual novelty detectionand integrated it into the broader pipeline of unsupervisedclass incremental learning. However, the number of novelclasses introduced at each task was limited to one, so thatany detected novelty could then be trivially labeled as be-longing to a single novel class. The more general scenarioof class increment learning with multiple novel classes wasnot addressed, which introduces significant new challenges:the number of new classes is not known a priori, and error-propagation is exacerbated since a novel class sample maynot only be misidentified as an old-class sample (the onlytype of wrong prediction possible in incDFM), but couldalso be labeled incorrectly. Contribution:We present an iterative uncertainty estima-tion technique suited for learning in an open-world, contin-ual multi-class incremental environment wherein, at eachcontinual task, a model is exposed to a mix of data fromboth known and an arbitrary number of unknown classes. To the best of our knowledge, this is the first work to ad-dress uncertainty estimation in such a general scenario. Weshow that unlike traditional one-shot uncertainty estima-tion methods, the quality of our uncertainty estimates doesnot degrade as new classes are iteratively encountered andlearned. We demonstrate its effectiveness by applying thetechnique to various tasks in a continual learning pipelinesuch as (i) continuous novelty detection in unsupervisedand semi-supervised continual settings, and (ii) sample se-lection for active learning and for pseudo-labeling in semi-supervised continual learning. One advantage of our formu-lation is that it enables us to distinguish between confidentlynovel samples (which are confidently identified as novel) asopposed to ambiguously novel samples. This allows us toexperiment with and analyze the effectiveness of various ac-tive sample selection strategies. Through exhaustive exper-iments and ablations, we demonstrate the superiority of ourmethod for both these tasks across multiple classificationdatasets and various DNN backbones. Results are bench-marked against multiple competitive baseline methods.",
  ". Problem Statement - Background": "2.1.Problem Setting: Consider a deep neural networkmodel y = (x) expected to learn from a sequence of con-tinual tasks. can be partitioned into a backbone networkthat produces features u = g(x) followed by a classifiery = f(u) operating on those features to produce the fi-nal prediction y, i.e. = f g(x). At each continualtask, t, the model is presented with an initially unlabeledset of samples U(t) comprising a mixture of unseen sam-ples of old/learned classes Uold(t) and unseen samplesfrom new/unlearned classes Unew(t),",
  "k=1Dk}, Unew(t) = {X|X Dt},": "where Dt comprises data from the set of new classesCtnew = {cti}, i = 1, . . . , Nt, introduced at task t. For nota-tional convenience, we denote Ctold = t1k=0 Cknew, whichis the collection of classes observed up to and includingtask t 1. Note that samples in Uold(t) belong to previousclasses but are unseen, i.e., were never used in trainingduring prior tasks. The goal is to accurately differentiate be-tween Uold(t) and Unew(t) and simultaneously learn to pro-cess/classify the novel classes present in Unew(t). This im-plies gaining the ability to classify/distinguish among dif-ferent novel classes in Unew(t). To accomplish this, wepropose a multi-class continual uncertainty quantificationalgorithm that is able to generate uncertainty scores per de-tected novel class. The uncertainty scores can also be usedfor (i) selecting informative novel samples for active label-",
  "ing, and (ii) selecting confidently novel samples for unsu-pervised pseudo-labeling": "2.2. Other Relevant Solutions:The described problemsetting is complex and can be broken down into a series ofsub-problems (and sub-solutions) that will be outlined be-low. Some of the (sub-)solutions will be used as comparisonmethods to benchmark our approach in 3.3. 2.2.1. Learning from Novelties:Early works on OpenWorld Classification were essentially the same asOOD detection and did not focus on learning from the de-tected novel data. More recent works on Category Dis-covery attempt to do this by estimating the number ofnovel classes and assign each novel sample to the appropri-ate novel class. They assume, however, that the unlabeledset contains only novel-class data (U = Unew), thus notrequiring novelty detection. Work on Generalized Cate-gory Discovery (GCD) removes this restriction andpermit U to include both known and novel classes. Whilethis represents a significant advancement, it remains a chal-lenging problem that, to the best of our knowledge, has onlybeen addressed in non-continual settings , withoutthe added complexities inherent to continual learning. 2.2.2.Continual Learning:Most CL approaches thatfocus on mitigating catastrophic forgetting are fully su-pervised and assume access to fully labeled data streams.More recent approaches,though, do not make this assumption and explore unsu-pervised, semi-supervised, and few-shot continual learningmethods . However, these too assume that the in-coming data contains only novel classes (U(t) = Unew(t)),thus bypassing the need for novelty detection and avoidingerror propagation. In this category, we compare to CCIC, a semi-supervised CL method that leverages the Mix-Match technique to learn more efficiently from both la-beled and unlabeled samples. As shown in 3.3, approacheslike CCIC scale poorly to our generalized setting where theincoming data can include both old and novel classes.Lastly,continual novelty detection (CND) remains under-explored, with a notable exception of incDFM , which con-strained Unew(t) to have only one new class at a time. Wecompare our approach to incDFM in the results 3.3 andshow that it does not generalize well when Unew(t) maycontain an arbitrary number of novel classes. 2.2.3.Active Learning:Active learning aims to learnfrom a small set of informative data samples judiciouslychosen from a larger unlabeled dataset. Diverse strategieshave been used for selecting the samples based on uncer-tainty or diversity . We refer to for ex-haustive surveys of the methods, which overwhelmingly op-erate in an offline fashion. Defining an effective AL heuris-tic in the generalized setting of U(t) is challenging as will be shown in 3.3. An early attempt was made by the au-thors of GBCL . Their method integrates AL with few-shot continual learning by selecting samples that are mostdistant from a continually updated Gaussian mixture modelof all old classes. We compare to GBCL in 4.3.",
  ". Our Approach: COUQ": "Choice of elemental uncertainty metric:We refer to ourmethod as COntinual Uncertainty Quantification (COUQ).Our method is agnostic to the choice of the underlying un-certainty measure used within our algorithm, as long as itcan reliably estimate uncertainty per novel class or per oldclass. However, this is not an easy feat since many exist-ing static novelty detection approaches make for very poorper-class uncertainty estimators. In our current formula-tion, we leverage the feature reconstruction error (FRE)metric introduced in , which has been shown to effec-tively estimate per-class uncertainty in the non-continualsetting. For each in-distribution class, FRE learns a PCA(principal component analysis) transform {Tm} that mapshigh-dimensional features u from a pre-trained deep-neural-network backbone g(x) onto lower-dimensional subspaces.During inference, a test-feature u = g(x) is first trans-formed into a lower-dimensional subspace by applying Tmand then re-projected back into the original higher dimen-sional space via the inverse T m. The FRE measure is calcu-lated as the 2 norm of the difference between the originaland reconstructed vectors:",
  "FREm(u) = f(x) (T m Tm)u2.(1)": "Intuitively, FREm measures the distance of a test-feature tothe distribution of features from class m. If a sample doesnot belong to the same distribution as that mth class, it willusually result in a large reconstruction score FREm. FREis particularly well suited for continual settings since foreach new class an additional PCA transform can be trainedwithout disturbing the ones learnt for previous classes.",
  ". Algorithmic Steps": "Initial training and deployment:At the outset (i.e. taskt = 0), we assume that the main model (x) has beentrained to classify among an initial fixed set of classes C0new(following notation from Sec.2).An initial set of PCAtransforms {T 0m}, m C0new have also been learnt.Continual Learning and Adaption:At any task t, t > 0,as unlabeled data arrives, COUQ will follow an iterativeprocedure to derive uncertainty scores that can be used todetect novelties and classify them if present. Classifica-tion of novelties can be performed in an unsupervised man-ner using clustering techniques such as K-means, or in asemi-supervised manner via a combination of active andPseudo labeling. We do not prescribe a particular approach;rather, we show that our uncertainty estimation algorithm can work well with both semi-supervised and unsupervisedapproaches. In the former case, the selection of samples foractive labeling itself can be guided by the uncertainty scorefrom COUQ. Finally, note that this iterative procedure is aninner-loop iteration (indexed by i) employed at each task,which is different from the outer-loop iteration over tasks(indexed by t). Each iteration i utilizes the scores and nov-elty class predictions from the previous iteration to progres-sively obtain better uncertainty scores St,i(u). To simplifythe notation, we index only w.r.t iteration i, with the under-standing that uncertainties are recalculated at every task.",
  "S0(u) = minjCtoldFRE0j (u),(2)": "FRE0 indicates scores at the 0th iteration of the tth task.These reflect the distance to the detected past classes en-countered till t 1. This score can be used for noveltydetection by noting that samples with high S0(u) valueshave a significant probability of being novel. Once confi-dently novel samples have been identified, these require anovelty mapper M i(u) to assign labels or IDs for the sub-sequent iterations. This mapper can be obtained from anunsupervised approach such as K-means clustering by se-lecting the id of the closest cluster centroid. Alternately, wecan choose a small number b0 samples with high S0(u) val-ues for active querying and train a pseudo-labeler (such as asmall MLP) on these b0 samples to predict the class-ids forthe remaining confidently novel samples. Either way, thenovel classes that are identified are appended to Ctnew andare used for computing initial estimates of per-class PCAtransforms for the new classes {T t,i=0m}, m Ctnew. i > 0: Iterative Training.For all subsequent iterationsof the same task, COUQ computes a per-novel-class un-certainty score, relying on the previous iterations mappernovel class-id predictions and the corresponding previousiterations per-novel-class PCA transforms {T t,i1m}. Theoverall multiclass uncertainty score for a given unlabeledsample u is defined in eq 3. A novel class-id pseudo-labelm is predicted by the novelty mapper from the previous iter-ation M i1(u), and we select the corresponding PCA trans-form T t,i1mto calculate the score.",
  "classes Ctold, while a low value of the denominator im-plies low distance from novel class m. Such a samplelikely belongs to Unew(t) and is a strong candidate to bepseudo-labeled as class m": "2. Identify as old-class with high-confidence: This is theopposite of the previous case: low score values corre-sponding to low numerator (low distance w.r.t Ct1old ) andhigh numerator value (high-distance from the novel classm). Such a sample likely belongs to Uold(t) and is notneeded any further for novelty detection (since we areassuming no distribution-shift for old classes). 3. Ambiguous:Samples for which the score is neitherdefinitively high or low. These could be old-class sam-ples having relatively high scores, or novel-class sampleshaving relatively low scores. Owing to this ambiguity,a clear determination cannot be made, and hence thesesamples would most benefit from active labeling.We iteratively improve the quality of our multiclassuncertainty measure, Si(u), via a pseudo-labeling and/oractive-labeling at each iteration. Using the novel class-idpredictions from iteration i 1 (un-supervised or semi-supervised), we separate and sort the scores per predictednovel class m Ctnew. We select the topmost percent perpredicted-class. These are the samples predicted as novelclass m with highest-confidence. If active querying is alsoused, then we additionally select bi most ambiguous sam-ples per predicted novel class to actively label, so long asthe tiny active label budget has not been exhausted. To-gether, the accumulated selected active or pseudo-labeledsamples are used for (i) computing all novel PCA transfor-mations {T t,im }, (ii) re-training the novelty mapper - eitherthe unsupervised K-means or the small MLP pseudo-labeler- in preparation for the next iteration, and (iii) updating themain model classifier (or only the classifier f if using afrozen backbone g). Further details on COUQ iterations,such as stopping criteria, as well as a detailed Algorithmflow box are included in supplementary. 3.2. Application to Continual Novelty DetectionAs described, uncertainty estimation in COUQ can be useddirectly for novelty detection. Importantly, because the ad-dition of new PCA transforms (per detected novel class)does not impact those already stored in memory, noveltydetection performance does not significantly degrade, e.g.does not catastrophically forget.Furthermore, the useof an iterative approach results in higher-quality and moreconsistent uncertainty estimates than other one-shot ap-proaches, thus also minimizing continual error propagation. 3.3. Applications to Active Sample SelectionWe describe next how the uncertainty score from Eq. (2)can be used for efficient active annotation. The samplesthus labeled can be used not only for improving the qual-ity of the multiclass uncertainty measure as desribed above, but also for updating the weights of a downstream continualclassifier f. One possible sample selection strategy wouldbe to prioritize novel samples for annotation. This couldbe done by selecting the most confident novel samples foractive annotation. However, we find that including samplesof ambiguous novelty, scores Si(u) which are neither toohigh or too low, is more informative. We set our defaultAL strategy to pick 1:1 between ambiguous and confidentnovel samples per detected novel class. Details of how am-biguous samples are determined can be found in the supple-mentary. In 4.3.1, we compare various sample selectionstrategies using different uncertainty scores and show thatour proposed AL strategy using Si(u) uncertainty scoressignificantly outperform others.",
  ". Experimental Setup": "Model:The backbone g used in our method and all base-lines is a frozen, pre-trained deep model. This is a commonpractice in transfer learning in CL , and is theoret-ically based on the principle that low-level visual featuresobtained from a frozen model are thought to be task non-specific and do not need to be constantly re-learned dur-ing CL tasks. We tested over 3 different pretrained founda-tion backbones: ResNet50 pre-trained on ImageNet1Kvia unsupervised SwAV and ViTs16 or ViTb16 pre-trained on Imagenet1K via unsupervised DINO .The features u are used for computation of the uncertaintyscores in Eqs. (2) and (3) and also inputted to the classifierf for eventual continual class-prediction. f is implementedas a one hidden-layer perceptron (of size 4096).As described in 3.1, we test two variants of the nov-elty mapper M i(u) to assign class-ids to confidently novelsamples: (1) a K-means clustering in a fully unsupervisedscenario that is trained to cluster the confident novel sam-ples, and (2) a fully-connected layer (different from themain classifier f) on top of the frozen backbone in the semi-supervised scenario. Here, the mapper is trained with cross-entropy loss on the few actively labeled samples in additionto the confident novel samples pseudolabeled samples fromthe previous inner-loop iteration.Finally, while COUQ is agnostic to which CL method isused to prevent catastrophic forgetting of the continual clas-sifier, we showcase results employing a conventional CLtechnique termed Experience Replay (ER) . InER methods, a limited number of exemplars from the old-classes must be stored in a buffer of fixed size B. These are used in conjunction with the novel samples to train the clas-sifier without catastrophic forgetting. We set the B to 5000for Im21K-OOD and Places, and 2500 for Cifar100, Eu-rosat, and Plants. Further details about COUQ being usedwith ER are shared in the supplementary. Experiments:Overall, we test Continual Novelty De-tection and Continual Open-World Classification on 5 di-verse datasets:Imagenet21K-OOD (Im21K-OOD) ,Places365-OOD (Places) , Eurosat , iNaturalist-Plants-20 (Plants) and Cifar100-superclasses . Allof the aforementioned datasets were constructed to haveclass orthogonality (be out-of-distribution) with respect toImagenet1K, which was used to pretrain the backbone (w/exception of Cifar100). Further details on datasets are in thesupplementary. At each incoming unlabeled pool (task), wefix a mixing ratio of 2:1 of old to new classes per task, withold classes drawn from a holdout set (0.35% of each datasetprepared at experiment onset). We set pseudolabeling selec-tion to = 20% of samples predicted as novel, as describedin 3.1 . After iterative updates of COUQ or baselines, forfair evaluation, we measure AUROC on an independent testset with the same ratio of old to new class samples as inthe training pool (see 4.1). For experiments not purpos-edly varying the continual class increments, we set defaultvalues as follows: 5 for Im21K-OODD, 5 for Places, 3 forboth cifar100-superclasses and Plants, 2 for eurosat. Moredetails can be found in the supplementary. Baselines:Since we test our approach in various settings,we describe the relevant baselines chosen for each setting.(A) Baselines for only Continual Novelty Detection (CND):(1) incDFM , a method that includes an updatable con-tinual novelty detector, but assumes single class novelties(see 2.2); (2) DFM , a precursor of incDFM originallyproposed for non-continual novelty detection;(B) Baselines for only Continual Open-World NoveltyLearning (CONL): (3) CCIC a semi-supervised CLmodel that adapts MixMatch to the CL setting; (4)GBCL, a few-shot continual active learning approach ;(C) Baselines used for both CND and CONL: (5) (ER-variants) Semi-supervised baselines built upon the CL tech-nique of Experience Replay based on for ac-tive continual learning. For these, uncertainty metrics suchas (5.1) Entropy, (5.2) Margin, (5.3) Softmax are used to ac-tively select the most uncertain samples for labeling whichwill be used in continual training updates. The same un-certainty metrics are then used to output scores for evalu-ation; (6) For further comparison, we introduce additionalbaselines built upon the previous item ER-variants but usingthe corresponding uncertainty quantification (e.g. Entropy,Margin or Softmax) to iteratively pseudolabel the most con-fident samples during the inner loop akin to our approach. . (Left A.1,B.1) AUROC of Novelty Detection at each continual task. Number of novel classes per task is in parenthesis. COUQ(green) clearly outperforms baselines both in both semi-supervised (solid line) and unsupervised versions (dashedline); (Center A.2,B.2)Results varying the supervision budget; (Right A.3,B.3) Results varying Novel Class Increment per task. For (left,right) Supervision budgetis 1.25% and all plots show results implemented with a Resnet50 backbone. Equivalent plots for other datasets in appendix.",
  ". Continual Novelty Detection": "We first demonstrate the effectiveness of COUQ within thegeneral problem of continual novelty detection (CND). Re-call that at each task the novelty detector should be ableto detect novelties reliably from an unknown and multiplenumber of novel classes. We evaluate CND performanceusing the common threshold-agnostic Area-under-receiver-operating-curve (AUROC) score. We show results for ourmethod and all the relevant baselines over all 5 datasets and3 different foundation model backbones in table 1. We testCND performance in both semi-supervised and unsuper-vised setups as described earlier. For the semi-supervisedcase, we fix AL budget at a default value of 1.25% forall baselines (including our own method) and across alldatasets, as detailed in .1. Default class-increment the number of novel classes introduced at each task is indicated in parenthesis by the dataset name in the ta-ble. Entries in are averaged AUROC scores overall continual tasks. We plot CND performance (AUROC)over continual tasks (time) in (left, A.1, B.1). Keytakeways from are as follows: (1) Both our actively-supervised method version, COUQ (AL+P), and our unsu-pervised version, COUQ-Unsup (P), outperform compet-ing methods by large margins over all experimental vari-ations. In fact, our unsupervised variant overperforms evenother methods that rely on semi-supervision. To note, semi- supervised methods are included in the first rows of the ta-ble, separated by a line. Naturally, COUQ-AL outperformsCOUQ-Unsup. (2) Traditional uncertainty metrics En-tropy, Margin, Softmax which are computed from the con-tinual classification decision boundary perform poorly over-all. One likely reason is the compounded negative effectof error-propagation (miss-identified samples) together withthe pressures of catastrophic forgetting on the classificationdecision boundary. Note that in both ER- and PseudoER-variants, its corresponding uncertainty metric is used to se-lect active samples with highest uncertainty and pseudola-bels of high-confidence (in case of PseudoER). (3) Finally,note that PseudoER variants fail to consistently outperformER. This is because, unlike our method, they are unable toproduce high-quality, high-confidence pseudolabels. Thishighlights the importance of our COUQ uncertainty metric3 in measuring pseudolabel confidence. Varying experimental parameters:Next, we explorethe impact of varying experimental parameters. First, wetest with different AL budgets (from 0.625% to 5%) forwhich the results are shown in (Center, A.2-B.2).Our method both supervised and unsupervised continuesto outperform baselines over a wide range of AL-budgets.Finally, we vary the class-increments from their default val-ues with results plotted in Fig 2 (right, A.3 and B.3). Wesee that the compared SOTA novelty detector incDFM",
  ". AUROC results from ablations of COUQ on CND": "performs quite well for the increment of one novel classper task, for which it was originally proposed. However,when the class increment increases, this method degradesin performance because it groups multiple novel classes to-gether without distinction, which severely hurts detectioncapacity. We note a similar pattern in DFM. In comparison,due the clustering-based novelty mapper, our unsupervisedCOUQ-Unsup variant is able to significantly better modelthe novelty distribution along time and class increments. Ablations:Next, we perform ablations to highlight theimpact of various components of our proposed method. Theresults are presented in . In the first row (GT-Sup orground-truth supervision), all the confident novel samplesidentified in a task are sent to a labeler with ground-truth la-bel instead of a limited number of judiciously chosen ones.This is unrealistic in a real-world setting given high costsof labeling. Hence, it represents a conceptual upper-boundof performance, and there is no error-propagation betweentask transitions. For the remaining variants described next,an active budget of 1.25% is assumed as before. These in-clude: AL-Amb - Querying samples ambiguous uncertaintyscores as described in 3.3 for active labeling. This is thedefault strategy; (2) AL-Top - Querying samples with high-est uncertainty scores (i.e.most-confidently novel sam-ples) for active labeling rather than ambiguous samples as inCOUQ; (3) AL-Random - Using a random selection of sam-ples for active labeling; (4) No-Iters - performing COUQ(with default stretgy) in oneshot rather than over multipleinner-loop iterations. In this case, we use all supervisionbudget upfront at i = 0 and then also pseudolabel in one-shot prior to updating S(i) 3. We additionally use P todenote when pseudolabeling is used in addition to activesample selection. Hence, the last row in containsresults with AL only without pseudolabeling. First, we ob-serve a small drop (2.2%) in performance of our methodrelative to the fully-labeled GT-Sup case.This indicatesthat for CND, COUQ manages to minimize the impact oferror propagation. Next, we observe that other active label-ing strategies AL-Top or AL-Rand decrease performance by4.9% and 2.9% respectively, underscoring the informative-ness of querying ambiguous samples for AL with the goalof continual novelty detection. We observe the importanceof minimizing error propagation via our methods iterative-ness since No-Iters results in an significant 8.3% decrease inperformance. Finally, we show that pseudolabeling among",
  ". Continual Open-World Novelty Learning": "Here we apply COUQ to Continual Open-World classifica-tion/learning (which we term CONL). The setting is identi-cal to that described in 4.1. However, in addition to noveltydetection, the goal is also to learn to continuously classifyand thus incorporate novel class samples into knowledgecontinuously. Within this general framework, we demon-strate two ways in which COUQ can be applied to helpsolve CONL. The first is using COUQ to decide which sam-ples to actively label at each task. The second is to use itto reliably rank samples by their confidence of being of agiven novel class m, from which it can be derived a reli-able pseudolabeling algorithm. We finally combine the twoaforementioned uses (Active and Pseudo) to offer a com-prehensive and robust response to the CONL problem.",
  ". Effect of the AL strategy in CONL performance; Mea-sured as average continual accuracy over all tasks and classes": "We first isolate the effects of using COUQ for Active la-beling only, with results in . We do so by removingpseudolabeling in Eq. 3, and using only the actively labeledsamples for (i) updating the iterative metric, and (ii) updat-ing the downstream continual classifier. The first three rowsof show variants of AL using COUQ, same as thoseused in 4.2. Note however that P is omitted from theirnames since pseudolabeling is not used. Similar to CNDablations, here too we see an improvement of AL-Amb overAL-Top and No-iters. Overall, COUQ with ambiguous se-lection strategy consistently outperforms other SOTA con-tinual active learning baselines. The last row contains thelower-bound of random sample selection.",
  ". Pseudo Labeling": "Next, we isolate the effect of COUQ when used only forpseudolabeling by adopting a random selection strategyfor labeling instead. shows that for pseudolabel-ing, COUQ overperforms conventional uncertainty metricssuch as Margin.Note that for COUQ, the pseudolabelsare used to update both Eq. (3) and the downstream con-tinual classifier.In the case of baselines, it is only the",
  ". Effect of Uncertainty scoring in Pseudolabeling in CONL;Measured as average CL accuracy over all tasks and classes": "latter. Similar to previous results, removing the iterative-ness (COUQ(P;oneshot) leads to an 8% decrease in per-formance. Most importantly, only COUQ pseudolabelingis consistently superior to abstaining from using pseudola-beling (lowerbound Rand) and updating via only the fewrandomly labeled samples. Lastly, we want to emphasizethat there are several semi-supervised learning techniques,which exploit the use of unlabeled data akin to pseudola-beling, and which are orthogonal to our method. Some ex-amples are Consistency propagation, semi-supervised con-trastive losses , etc. These approaches can be used intandem with COUQ and we leave that for future work.",
  ". Combining Active and pseudolabeling": ". (Row 1) Continual classification accuracy over contin-ual tasks during Continual Open-World Learning. The number ofnovel classes introduced per task for each dataset is in parenthesis.(Row 2) Results varying AL budget. Finally, we assess the performance of COUQ on a com-plete open-world CL pipeline comprising active labeling,pseudolabeling, and novelty detection. shows thecumulative average accuracy of the continual-learning clas-sifier at the end of all tasks for all 5 datasets and 3 architec-ture variations. The Oracle method constitutes an upper-bound. It has perfect knowledge of old and new class labels(100% supervision) and is trained using the same architec-ture and experience replay hyper-parameters as COUQ andall baselines. Overall, our method COUQ(AL+P), whichincludes both uncertainty-aware active and pseudolabelingvia 3, outperforms all baselines by a large margin, even withstringent labeling budgets of 2.5 5%. When comparingCOUQ(AL+P) to the ablated COUQ(AL) and COUQ(P) insections 4.3.1 and 4.3.2, we see a clear boost in perfor-mance. By focusing AL strategy on including ambiguousnovel samples and pseudolabeling on confident novel sam-ples, COUQ(AL+P) is better qualified for the difficult prob-lem of CONL. With respect to SOTA baselines, we see theclassic Offline AL strategies adapted for ER (e.g.Margin,Entropty, Max) overall performs poorly. The main reasonis that when datasets are more challenging and classes arepresented in continual order, pressures from decreased ac-curacy and catastrophic forgetting diminish the trustworthi-ness of these AL metrics computed from the logit decisionboundary. Moreover, as can be gauged by varying the ALbudget, at low AL ratios, ER-AL variants decay abruptly.Additionally, baseline GBCL which was specifically devel-oped for few-shot active open-world CL and is also trainedsolely from Actively labeled samples also under-performs. (row-2) analyzes the effect of varying the tinyactive labeling budget.We show COUQ over-performsthe other methods over a large interval of supervision bud-gets (all tested). (row-1) shows cumulative contin-ual accuracy results over all tasks. Note that CCIC (blueline or star symbol) drastically under-performs all other ap-proaches even with a large supervision budget of 10%. Asmost semi-supervised CL methods, it originally assumedold and new classes would not co-occur and cannot properlyquantify uncertainty when this assumption is lifted. Equiv-alent plots for other datasets are in the supplementary. 5. ConclusionWe present an uncertainty quantification method specifi-cally designed for continual open-world learning.Withour method, we are able to assign high-confidence pseudo-labels which are reliable and that we show can signifi-cantly reduce labeling costs. Furthermore, also with ouruncertainty method, we demonstrate that active queryingbased on novelty ambiguity may be more informative thanmerely selecting the most-likely novel samples. Our ap-proach outperforms baselines across multiple datasets andexperiments. Yet, several challenges remain, which we aimto address in future work.An example is how to con-tinually query and update when distribution shifts of pastclasses (e.g., noise, illumination changes) co-occur withnovel classes. Eduardo Aguilar, Bogdan Raducanu, Petia Radeva, and JoostVan de Weijer. Continual evidential deep learning for out-of-distribution detection. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV) Work-shops, pages 34443454, 2023. 1 Nilesh A. Ahuja, Ibrahima J. Ndiour, Trushant Kalyanpur,and Omesh Tickoo. Probabilistic modeling of deep featuresfor out-of-distribution and adversarial detection. In BayesianDeep Learning workshop, NeurIPS, 2019. 2",
  "Benedikt Bagus, Alexander Gepperth, and Timothee Lesort.Beyond supervised continual learning:a review.arXivpreprint arXiv:2208.14307, 2022. 1, 3": "David Berthelot, Nicholas Carlini, Ian Goodfellow, NicolasPapernot, Avital Oliver, and Colin A Raffel. Mixmatch: Aholistic approach to semi-supervised learning. Advances inneural information processing systems, 32, 2019. 3, 5 Matteo Boschini, Pietro Buzzega, Lorenzo Bonicelli, AngeloPorrello, and Simone Calderara. Continual semi-supervisedlearning through contrastive interpolation consistency. Pat-tern Recognition Letters, 162:914, 2022. 3, 5, 8 Matteo Boschini, Pietro Buzzega, Lorenzo Bonicelli, AngeloPorrello, and Simone Calderara. Continual semi-supervisedlearning through contrastive interpolation consistency. Pat-tern Recognition Letters, 162:914, 2022. 1, 3",
  "Klaus Brinker. Incorporating diversity in active learning withsupport vector machines.In International Conference onMachine Learning, 2003. 1, 3": "Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Si-mone Calderara.Rethinking experience replay: a bag oftricks for continual learning. In 2020 25th International Con-ference on Pattern Recognition (ICPR), pages 21802187.IEEE, 2021. 5 Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-otr Bojanowski, and Armand Joulin. Unsupervised learningof visual features by contrasting cluster assignments. Ad-vances in Neural Information Processing Systems, 33:99129924, 2020. 5 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 96509660, 2021. 5",
  "Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deepbayesian active learning with image data. In InternationalConference on Machine Learning, pages 11831192. PMLR,2017. 1": "Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learningto discover novel visual categories via deep transfer cluster-ing. In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision, pages 84018409, 2019. 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 5",
  "Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, andSerge Belongie. The inaturalist species classification and de-tection dataset, 2018. 5": "Zhiqi Kang, Enrico Fini, Moin Nabi, Elisa Ricci, and Kar-teek Alahari. A soft nearest-neighbor framework for con-tinual semi-supervised learning.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1186811877, 2023. 1, 3 James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, JoelVeness, Guillaume Desjardins, Andrei A Rusu, KieranMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neu-ral networks. Proceedings of the national academy of sci-ences, 114(13):35213526, 2017. 3",
  "Jitendra Parmar, Satyendra Chouhan, Vaskar Raychoudhury,and Santosh Rathore. Open-world machine learning: appli-cations, challenges, and opportunities. ACM Computing Sur-veys, 55(10):137, 2023. 1": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, GeorgSperl, and Christoph H Lampert. icarl: Incremental classifierand representation learning. In Proceedings of the IEEE con-ference on Computer Vision and Pattern Recognition, pages20012010, 2017. 3 Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, RyanPoplin, Mark Depristo, Joshua Dillon, and Balaji Lakshmi-narayanan. Likelihood ratios for out-of-distribution detec-tion. In Advances in Neural Information Processing Systems,pages 1470714718, 2019. 2",
  "Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisser-man. Generalized category discovery. In IEEE Conferenceon Computer Vision and Pattern Recognition, 2022. 2, 3": "Thuy-Trang Vu, Shahram Khadivi, Mahsa Ghorbanali, DinhPhung, and Gholamreza Haffari. Active continual learning:On balancing knowledge retention and learnability. arXivpreprint arXiv:2305.03923, 2023. 1, 3, 5 Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang.Vim: Out-of-distribution with virtual-logit matching. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2022. 2 Shixian Wen, Amanda Rios, Yunhao Ge, and Laurent Itti.Beneficial perturbation network for designing general adap-tive artificial intelligence systems.IEEE Transactions onNeural Networks and Learning Systems, 2021. 3"
}