{
  "Abstract": "When evaluating stimuli reconstruction results it is tempting to assume that higherfidelity text and image generation is due to an improved understanding of thebrain or more powerful signal extraction from neural recordings. However, inpractice, new reconstruction methods could improve performance for at leastthree other reasons: learning more about the distribution of stimuli, becomingbetter at reconstructing text or images in general, or exploiting weaknesses incurrent image and/or text evaluation metrics. Here we disentangle how much ofthe reconstruction is due to these other factors vs. productively using the neuralrecordings. We introduce BrainBits, a method that uses a bottleneck to quantifythe amount of signal extracted from neural recordings that is actually necessaryto reproduce a methods reconstruction fidelity. We find that it takes surprisinglylittle information from the brain to produce reconstructions with high fidelity. Inthese cases, it is clear that the priors of the methods generative models are sopowerful that the outputs they produce extrapolate far beyond the neural signalthey decode. Given that reconstructing stimuli can be improved independently byeither improving signal extraction from the brain or by building more powerfulgenerative models, improving the latter may fool us into thinking we are improvingthe former. We propose that methods should report a method-specific randombaseline, a reconstruction ceiling, and a curve of performance as a function ofbottleneck size, with the ultimate goal of using more of the neural recordings.",
  "Introduction": "Applying powerful generative models to decoding images and text from the brain has become anactive area of research with many proposed methods of mapping brain responses to model inputs. Arace between publications is driving down the reconstruction error to produce higher fidelity imagesand text . It could be easy to assume that as the field gets better at reconstructing stimuli,we will simultaneously be getting better at modeling vision and language processing in the brain. Weargue that this is not necessarily the case. There are several reasons why a method might have higher quality reconstructions yet actuallyrequire the same or less signal from the brain. For example, a much larger model can learn astronger prior over the space of images and text, so even if it were given less information fromthe brain, it might produce better reconstructions. In particular, a generative model might becomemore fine-tuned toward the distribution of images and text that are used in standard datasets. This isproblematic because so few open neuroscience datasets exist, and even fewer at the scale that would",
  "arXiv:2411.02783v1 [cs.LG] 5 Nov 2024": ": BrainBits bottlenecking framework as applied to BrainDiffuser. The goal of imagereconstruction is to generate an image based on brain signal. The brain signal is mapped to a hiddenvector (gold) by a compression mapping gL, which is then used to predict VDVAE, CLIP-text,and CLIP-vision latents via a mapping fL. As in , these latents are used to produce the finalreconstruction. In our studies, we restrict the information available from the brain by varying thedimension of the hidden vector. enable this research. It is easy to inadvertently overfit a model and over-optimize for the particularbiases of the standard benchmarks, never mind explicitly tuning the parameters. Finally, there is theseparate, confounding issue of how to best evaluate the reconstructions. Even the best intentionedmodeling approaches on novel data can run afoul of the extremely limited image and text evaluationmethods that we have today. Later in the manuscript, we demonstrate the importance of appropriatelycalibrating and understanding the shortcomings of these methods. Given that better decoding need not explain more of the brain, we create the first metric to measure this:BrainBits. BrainBits measures how reconstruction performance varies as a function of an informationbottleneck. We learn linear mappings from the neural recordings to a smaller-dimensional space,optimizing the reconstruction objective of each method. The result of running BrainBits on state-of-the-art reconstruction methods is striking: a bottleneckthat is a small percentage of the full brain data size is sufficient to guide the generative models towardsimages of seemingly high fidelity. For fMRI, the entire brain volume often has on the order of 100Ktotal voxels and about 14K voxels in the visual area, which is what the methods we report here use.We find that a reduction through a bottleneck of only 30 to 50 dimensions provides the vast majorityof the performance of a reconstruction method depending on the metric. BrainBits enables us to disentangle the contributions of the generative models prior and the signalextracted from neural recordings when evaluating models. This is critical to soundly using stimulireconstruction as a tool for making neuroscientific progress. We would like reconstruction methodsthat explain more of the brain rather than merely relying on better priors. In particular, we proposethree components: to produce a method-specific random baseline that uses no neural recordings, tocompute a method-specific reconstruction ceiling, and to compute reconstruction performance as afunction of the bottleneck size. Ideally, models would achieve near full performance only with large bottlenecks, showing that theyare relying on the neural signal for their performance. Models that have high random baselines andthen exploit only a few bits of information from the brain in order to achieve their, at first glance,impressive performance, are doing so largely from their prior. And while all the examples of BrainBitswe provide here use fMRI, the method can be applied to any neural recording modality. BrainBitsalso provides interpretability, by showing which brain areas contribute to decoding as a function ofthe bottleneck size and making the activity of these regions available for probing via decoder.",
  "Groundtruthwere both from up north were both kind of newish to the neighborhood this is inflorida we both went to college not great colleges but man we graduated and...(c) Tang et al. 2023": ": High quality stimuli can be reconstructed from a fraction of the data. Shown hereare images and text reconstructed for several bottleneck sizes using our BrainBits approach. Imagesand text are shown for subject 1 for all three methods. Examples where the original methods couldreasonably reconstruct the stimuli were chosen; the same images for both visual methods are shownin the appendix. As the bottleneck dimension increases, the accuracy of the reconstruction increases.Although there are differences between the full and bottlenecked (d = 50) results, the reconstructionsare surprisingly comparable, despite the fact that the full reconstruction methods have > 14, 000voxels available to them. Text reconstructions are harder to evaluate in this qualitative manner, laterwe present a quantitative evaluation.",
  "(c) Tang et al. 2023 with insets zooming in for clarity": ": Quantifying the fraction of the data needed to reconstruct stimuli. While differentmetrics present slightly different pictures of model performance, most performance is reached byabout 20 32-bit floating point numbers and essentially all performance is reached by about 50. Visionreconstruction methods (a,b) are significantly better than language reconstruction methods (c). Andalthough language methods appear to use very large bottlenecks, as a fraction of the data available,they are comparable (vision methods presented here use only voxels in the visual cortex). Relative tovision, language methods are much closer to the random baseline and have a longer way to go, asshown by the inset. This also reveals a limitation of the resolution of the BrainBits approach: there isnot much room for bottlenecking when performance is near the random baseline and metrics lacka well calibrated scale. Across different metrics, both low level metrics (like pixel correlation andword error rate) and high level metrics (like DreamSim and BERT) the message is the same: modelsasymptote quickly.",
  "Related work": "Several recent works have focused on predicting the latent features of deep, pretrained, generativemodels from fMRI data in order to reconstruct corresponding stimuli. Han et al. introduced atechnique that projects fMRI recordings to the bottleneck layer of an image-pretrained variationalautoencoder and reconstructs the stimulus via the decoder network. Similarly, learnsprojections to the input space of the generator network belonging to a pretrained generative adversarialnetwork . Given the success of models such as Dall-E , more recent work has focused onlearning mappings to the latent space of large diffusion models . Furthermore,approaches such as and leverage recent generative models for the multimodal case, decodingboth images and captions. This family of methods has been facilitated by the growth of fMRI datasets containing pairs of stimuliand recorded neural data, the current largest of which is the publicly available Natural Scenes Dataset(NSD) . The Natural Scenes Dataset contains fMRI recordings of multiple subjects cumulativelyviewing tens of thousands of samples from the Microsoft CoCo dataset . Given its increased sizerelative to previous similar datasets , it presents more potential for data-driven neural decoding.As a result, it is a popular choice for many recent methods , and we select it for our analysis. Numerous metrics for measuring reconstruction fidelity have been proposed. In the visual domainthese include pixel correlation, SSIM, CLIP similarity, and DreamSim . For language these includeword error rate (WER), BLEU, METEOR, and BERTScore . None take into account the priorknowledge that modern models have built into them.",
  "Approach": "Given a reconstruction method f that maps brain data X to images Y , we seek to determine howmuch the quality of the images Y = f(X) depends on the brain signal. We do this by placingrestrictions on information flow, and then examining the resulting reconstructions. This restrictionis operationalized by a bottleneck mapping gL that compresses the brain data to a vector of smallerdimension. Specifically, let Y = {yi} where yi is an individual original image corresponding to thebrain data response xi. Then, as stated, our aim is to find the best reconstruction achievable for agiven restriction L, where reconstruction quality is scored by some metric s(, ):",
  "s(f(gL(xi)), yi)(1)": "This allows us to produce a curve of reconstruction quality as a function of L. We restrict our attentionto linear transformations, gL, to find the interpretable mappings that yield the best reconstructionquality. Model performance lies in a range between model-specific randomly generated images and a model-specific ceiling. To compute a models random performance, we run bottleneck training and recon-struction, substituting the original brain data with synthetic data generated according to N(0, 1). Thepurpose of this baseline is to obtain a set of images that reflect the generative prior of the modelwithout any input from the brain. To compute a ceiling on the image reconstructions, BrainDiffuser and Takagi et al 2023, we runthe complete original reconstruction pipeline, but substitute the ground truth image latents insteadof using the latents as predicted from the brain data. We do this to obtain the reconstructions asproduced by the generative models, had the target latents been predicted perfectly. No analogousceiling procedure exists for the language reconstruction method, Tang et al 2023, which is based onscoring word predictions via an encoder model.",
  "Experiments": "We adapt three state-of-the-art methods BrainDiffusers , Takagi & Nishimoto , and Tanget al. to compute BrainBits; the first two are vision reconstruction methods and the last is alanguage reconstruction approach. In each case, BrainBits is computed in the same way, but it iscomputed jointly with the optimization for each method. This means that BrainBits is not a simplepre- or post-processing step or function call, it must be integrated into the method, which at times can require updates to the optimizer being used, effectively calling for a port from standard regressionlibraries to a deep learning framework. We optimize reconstruction for varying bottleneck sizes, andevaluate the resulting reconstructions on the standard metrics, including the ones used by the authors,as well as a new metric that has since been proposed, DreamSim, in order to show that BrainBitsproduces the same message regardless of the metric chosen or modality of the reconstruction. Below,we describe each method and how BrainBits was computed.",
  "BrainDiffusers": "The original BrainDiffusers uses fMRI data from the Natural Scenes Dataset (see ), inwhich the brain volumes have been masked specifically to only include the visual areas ( = 13930voxels). In the BrainDiffusers approach, regressions are fitted to map the fMRI data to latentrepresentations of the corresponding images, namely VDVAE and CLIP embeddings of theimages. An additional regression is fitted to predict CLIP embeddings of the corresponding COCOcaptions. The predicted VDVAE latent is used to produce a coarse version of the image. Then,the predicted VDVAE image, the predicted CLIP-text embedding, and the predicted CLIP-visionembeddings are given as input to versatile-diffusion , which produces the final predicted image.Complete details are given in the original paper . Our approach to bottlenecking BrainDiffusers is shown in . For a given bottleneck size L,we learn a mapping gL from the fMRI input to a L-dimensional vector. From this vector, we learn amapping to the image and text embedding targets. See (Appendix A.1: Training Bottlenecks) fortraining details.",
  "Takagi & Nishimoto": "This approach is broadly similar to BrainDiffuser in that the same dataset is used, and the sameapproach of mapping fMRI signal to embedding targets is used. However, there are a few keydifferences. First, separate mappings are learned for different parts of the brain. The early visual areais mapped to the latent representation space of a VAE, and the outputs of this mapping are passedthrough the VAEs decoder to produce a course reconstruction of the image stimuli. Another mapping is fit from a concatenation of the early, ventral, midventral, midlateral, lateral, andparietal regions to BLIP embeddings of the image stimuli. Predictions of these embeddingsare decoded into text that is then used along with the coarse image reconstruction to guide imagegeneration with Stable Diffusion . Since two separate mappings are learned, we insert twobottlenecks trained separately and keep their size the same.",
  "Tang et al": "We show how our benchmark framework can be extended to other modalities by also making a studyof an fMRI-to-language reconstruction approach. In the original approach, an encoding model is fitto map GPT embeddings to brain activity. Then, at inference time, the decoder takes the brainactivity as input and uses GPT to auto-regressively propose candidate predictions for the next word.The word with the highest likelihood, as computed by the encoding model, is accepted as the nextword in the sequence. We follow the original method in separating the training and decoding steps.We insert the information bottleneck by first learning a mapping from brain activity to a compressedvector that is mapped to a GPT text embedding. Instead of using the raw brain activity as input, weuse the resulting bottleneck representations, and run the rest of the pipeline without modification.This method has the fewest changes and simplest adaptation to BrainBits. Tang et al. reconstructlanguage from perceived speech, imagined speech, perceived movie, and perceived multi-speakerspeech (see ), and we report performance averaged across these tasks. Additional details aregiven in the appendix.",
  "Effective dim. of vision bottlenecksEffective dim. of language bottlenecks": ": How large are the bottlenecks? Even though the bottleneck representations have Ldimensions, it is not necessarily the case that all dimensions are used by the bottleneck mapping. Forboth language and vision, we can measure the effective dimensionality to get a sense for how muchof the channel capacity is being used. For BrainDiffuser, the effective dimensionality is comparableto the bottleneck size, showing that information is being extracted from the neural recordings up toabout 15-20 dimensions For language bottlenecks, effective dimensionality remains low showing thatlittle of the channel capacity, and therefore little of the neural signal, is being used. We also consider similarity as measured by DreamSim . Including additional metrics demonstratesthat the BrainBits message is independent of the metric used: models use relatively little of the neuralrecordings to achieve the vast majority of their performance. As described above, we insert information bottlenecks into two vision and one language recon-struction method and vary the bottleneck size while optimizing on the original objectives. Havinglearned these bottleneck mappings, we then investigate the resulting reconstructions. We can alsostudy the representations learned by the bottleneck mappings themselves, and we compute the effec-tive dimensionality of the bottleneck representations, the types of information decodable from therepresentations, and the weight that each bottleneck mapping places on different regions of the brain. How much information is needed to reconstruct an image or text? Qualitative results are shownin while quantitative results are shown in . For BrainDiffuser a bottleneck of size50 achieves 75%, 95%, 100%, and 89% of the original performance as measured by DreamSim,CLIP cosine similarity, SSIM, and pixel correlation. This is a reduction of a factor of approximately300, given that the method starts with approximately 14,000 voxels, to achieve most to all of thereconstruction performance. A similar trend holds for the Takagi & Nishimoto method. Chance performance is high for both visual reconstruction methods because the models learn strongpriors over the data. More surprising is the relatively low ceiling that both methods have. Eveninserting the best possible latents, methods are very limited in their ability to maximize these metrics. For language reconstruction, the original uses whole-brain fMRI; approximately 90,000 floating pointnumbers. A bottleneck of size 1000 is sufficient to recover 50% of the performance, averaged acrosssubjects, as measured by BERT. This bottleneck achieves 26% and 20% of the original performance,as measured by BLEU and METEOR respectively, and the WER is comparable. Chance performanceis similarly surprisingly good, with a WER of approximately 1.1, BLEU of approximately 0.18,METOR of approximately 0.14, and BERT score of approximately 0.79; these vary slightly bysubject. As discussed in the conclusion, a limitation of BrainBits is that the bottleneck size may beexaggerated in cases like these where performance is relatively close to chance.",
  "How effectively are the bottlenecks used?": "The bottleneck dimension is an upper bound on the information being extracted from the brain. A50-dimensional bottleneck may contain a much lower dimensional signal; see . We use thenumber of principal components needed to explain at least 95% of the variance, as a measure ofthe effective dimensionality of the bottlenecked representations . For BrainDiffuser, a bottleneckof size 50 has an effective dimension of about 16, averaged across subjects. For comparison, theaverage effective dim of the fMRI inputs is 2, 257 (see supplementary ). Finally, languagemodels have a much smaller effective dimension: a bottleneck of size 1,000 has roughly an effectivedimensionality of 5 to 20 depending on the subject.",
  "What regions of the brain matter most?": "We plot the weights of the bottleneck mapping back onto the brain for BrainDiffuser; see .The vast majority of the weight is assigned to voxels in the periphery of the early visual cortex. Asthe model has access to larger bottlenecks, it continues to assign more importance to these areasrather than including new areas. One would ideally like models to expand the brain areas that theyuse effectively as the bottleneck size increases. : What areas of the brain help reconstruction the most? Models quickly zoom in onuseful areas even at low bottleneck sizes. Note that for clarity the color bar cuts off at 1e-6, valuesabove that are all orange. In this case BrainDiffuser on subject 1 attends to peripheral areas of theearly visual system. As the bottleneck size goes up models exploit those original areas but do notmeaningfully expand to new areas. Ideally, one would hope to see more of the brain playing animportant role with larger bottleneck sizes; this is not what BrainBits uncovers.",
  "(c) RMS Contrast(d) Average gradient magnitude": ": What information do bottlenecks contain? For the BrainDiffusers approach we computethe decodability of four different features (object class, brightness, RMS contrast, and the averagegradient magnitude) as a function of bottleneck size. Object class refers to decoding the class of thelargest object in the image; often the focus of the image. The average gradient magnitude is a proxyfor the edge energy in the image. Dashed lines in plot (a) indicate 1-out-of-61 classification chance,1.6%. Dashed lines on plots (b, c, d) indicate the metrics MSE distance from the average metricvalue on the training set. Larger bottlenecks are needed to extract more object class informationabove chance. Edge energy, brightness and contrast are mostly exhausted early. Looking at featuresas a function of bottleneck size can reveal what types of interpretable features models learn, offeringsome explanation as to why performance goes up as a function of bottleneck size.",
  "Limitations": "BrainBits has a number of limitations. It requires rerunning the decoding process several times. Thiscan be expensive depending on the method. It is also not plug-and-play, since it must be optimizedjointly with the reconstruction method. While a simple fixed compression scheme such as PCA canbe used for a rough estimate, jointly optimizing the bottleneck and the reconstruction method can bea more difficult optimization problem requiring some manual attention. Depending on the method,BrainBits may be inserted at different points, for example, a method may have several steps thatextract information from the brain. All of this prevents BrainBits from being a simple library call.Models must be adapted to compute BrainBits instead, although this adaptation is generally simple. The resolution of BrainBits depends in part on sweeping the bottleneck size, but it also dependson precisely how that bottleneck is computed. We only consider a linear bottleneck to avoidadding meaningful computations to the model. One could also consider methods that employ vectorquantization, which we intend to do in the future. The linear approach we take here has difficultytraining with bottleneck sizes that are smaller than one float. A vector quantization method wouldlikely work better for such small bottlenecks. Although, small bottlenecks are perhaps not thatinteresting given that the goal is to explain more of the brain. In general, current image and text metrics are limited and make understanding reconstruction per-formance difficult. This situation is made worse by the fact that random performance can be highwhen models have strong priors. And even more so when those priors allow models to perform wellwith little added information from the brain. Computing these quantities is critical for understandingwhere we are in terms of absolute performance and explaining representations in the brain.",
  "Discussion and Conclusion": "The fidelity of a reconstruction depends on the priors of the generative model being used andthe amount of useful information extracted from the brain. Through visual inspection or fromreconstruction metrics, one can easily be fooled into thinking that because the results appear highfidelity, they must leverage large amounts of brain signal to recover such details. BrainBits revealsotherwise. Relatively little of the neural recordings are used, and many of the produced details canbe attributed to the generative prior of the diffusion model. To get a clearer understanding of howto evaluate these reconstructions, we propose a realistic random baseline based on the generativeprior, as well as a reconstruction ceiling based on what it is possible to decode with perfect latentprediction. The random baseline achieved by generative models is far higher that most would expectand the reconstruction ceiling on some metrics is far lower than expected. The priors create a much narrower range of performance than one might expect. For example,BrainDiffusers has an effective range of approximately 0.15 to 0.75 DreamSim, 0.48 to 0.88 CLIP,0.15 to 0.3 SSIM, and 0.05 to 0.7 Pixel Correlation. These ranges depend entirely on the modelsemployed, and are a reflection of the priors of the models. Reporting at least the floor is important forcontextualizing results; this is not reported in most prior work. Bottlenecks appear to exploit information in order, from low-level features, brightness and contrast,to mid-level features, edge energy, to high level features, object class. Vision models focus on thesame region of the brain regardless of bottleneck: early visual cortex. Higher-level features shouldbecome more disentangled and easier to take advantage of in later parts of the visual system. Thisdoes not appear to be useful to current models. Goodharts law states: When a measure becomes a target, it ceases to be a good measure. With theadvent of high resolution reconstructed image stimuli, we as a field may be tricked into believingthat we have become better at understanding visual processing in the brain. We may be tempted intofurther optimizing the quality of the reconstructed images in this service. But this is an inappropriatetarget if neuroscientific insight is our goal. We emphasize the importance of a BrainBits analysis forall neuroscientific studies of stimulus reconstruction to quantify the true contribution of the brain toreconstructions.",
  "We would like to thank Colin Conwell and Brian Cheung for their helpful feedback and discussionabout our experiments": "This work was supported by the Center for Brains, Minds, and Machines, NSF STC award CCF-1231216, the NSF award 2124052, the MIT CSAIL Machine Learning Applications Initiative, theMIT-IBM Watson AI Lab, the CBMM-Siemens Graduate Fellowship, the DARPA Artificial SocialIntelligence for Successful Teams (ASIST) program, the DARPA Mathematics for the DIscoveryof ALgorithms and Architectures (DIAL) program, the DARPA Knowledge Management at Scaleand Speed (KMASS) program, the DARPA Machine Common Sense (MCS) program, the UnitedStates Air Force Research Laboratory and the Department of the Air Force Artificial IntelligenceAccelerator under Cooperative Agreement Number FA8750-19-2-1000, the Air Force Office ofScientific Research (AFOSR) under award number FA9550-21-1-0399, the Office of Naval Researchunder award number N00014-20-1-2589 and award number N00014-20-1-2643, and this materialis based upon work supported by the National Science Foundation Graduate Research FellowshipProgram under Grant No. 2141064. This material is based on work supported by The DefenseAdvanced Research Projects Agency under Air Force Contract No. FA8721-05-C-0002 and/orFA8702-15-D-0001. Any opinions, findings, and conclusions or recommendations expressed in thismaterial are those of the author(s) and do not necessarily reflect the views of The Defense AdvancedResearch Projects Agency. The views and conclusions contained in this document are those of theauthors and should not be interpreted as representing the official policies, either expressed or implied,of the Department of the Air Force or the U.S. Government. The U.S. Government is authorized toreproduce and distribute reprints for Government purposes notwithstanding any copyright notationherein. Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan TDowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t fmri datasetto bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116126,2022.",
  "Matteo Ferrante, Tommaso Boccato, Furkan Ozcelik, Rufin VanRullen, and Nicola Toschi.Through their eyes: multi-subject brain decoding with simple alignment techniques. ImagingNeuroscience, 2024": "Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, andPhillip Isola. Dreamsim: Learning new dimensions of human visual similarity using syntheticdata. Advances in Neural Information Processing Systems, 2023. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherjilOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communicationsof the ACM, 63(11):139144, 2020. Kuan Han, Haiguang Wen, Junxing Shi, Kun-Han Lu, Yizhen Zhang, Di Fu, and ZhongmingLiu. Variational autoencoder: An unsupervised model for encoding and decoding fmri activityin visual cortex. NeuroImage, 198:125136, 2019.",
  "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Second InternalConference on Learning Representations, 19, 2014": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Internationalconference on machine learning, pages 1288812900. PMLR, 2022. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ComputerVisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part V 13, pages 740755. Springer, 2014.",
  "Furkan Ozcelik and Rufin VanRullen. Natural scene reconstruction from fmri signals usinggenerative latent diffusion. Scientific Reports, 13(1):15666, 2023": "Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, and Rufin VanRullen. Recon-struction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans. In 2022 International Joint Conference on Neural Networks (IJCNN), pages18. IEEE, 2022. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, JoePenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution imagesynthesis. arXiv preprint arXiv:2307.01952, 2023.",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving languageunderstanding by generative pre-training. 2018": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, MarkChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference onmachine learning, pages 88218831. PMLR, 2021. Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, ethan cohen,Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, andTanishq Abraham. Reconstructing the mind's eye: fmri-to-image with contrastive learning anddiffusion priors. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,editors, Advances in Neural Information Processing Systems, volume 36, pages 2470524728.Curran Associates, Inc., 2023. URL",
  "Katja Seeliger, Umut Gl, Luca Ambrogioni, Yagmur Gltrk, and Marcel AJ Van Ger-ven. Generative adversarial networks for reconstructing natural images from brain activity.NeuroImage, 181:775785, 2018": "Jingyuan Sun, Mingxiao Li, Zijiao Chen, Yunhao Zhang, Shaonan Wang, and Marie-FrancineMoens. Contrast, attend and diffuse to decode high-resolution images from brain activities.Advances in Neural Information Processing Systems, 36, 2024. Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusionmodels from human brain activity. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1445314463, 2023.",
  "Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. Semantic reconstruction ofcontinuous language from non-invasive brain recordings. Nature Neuroscience, 26(5):858866,2023": "Alexis Thual, Yohann Benchetrit, Felix Geilert, Jrmy Rapin, Iurii Makarov, Hubert Banville,and Jean-Rmi King. Aligning brain functions boosts the decoding of visual semantics in novelsubjects. arXiv preprint arXiv:2312.06467, 2023. Weihao Xia, Raoul de Charette, Cengiz Oztireli, and Jing-Hao Xue. Dream: Visual decodingfrom reversing human visual system. In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, pages 82268235, 2024. Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi.Versatilediffusion: Text, images and variations all in one diffusion model. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pages 77547765, 2023.",
  "A.1Training bottlenecks": "Case study: BrainDiffuersThe original BrainDiffusers method learns separate mappings to theVDVAE, CLIP-text and CLIP-vision latents. We predict all embeddings simultaneously. We use anMSE objective, and weight the loss on the predicted VDVAE, CLIP-text, and CLIP-vision targetswith respectively. We train our network with a batch size b = 128, an AdamW optimizer, a weight decay of wd = 0.1 and a learning rate of lr = 0.01. We train for 100 epochs and usethe weights with the best validation loss at test time. Case study: Tang et al. 2023Tang et al. use the fMRI data to both predict the timing and thecontent of language. Because we are interested in the semantic decoding, we use the original modelsof timing, and use the bottleneck representations to predict the words itself. We train our bottleneckwith a batch size of b = 512, learning rate lr = 5e 4, and the AdamW optimizer. We train for 100epochs and use the weights with the best validation loss at test time. For evaluation, we report theaverage of scores across windows, (see for details). Takagi et al 2023Takagi et al. learn mappings from the early visual area to VAE latents of thestimuli . They also learn mappings from the early, ventral, midventral, midlateral, laterial, andparietal regions to the latents of a BLIP image encoder . We learn a separate bottleneck for eachmapping with the MSE objective. We use the AdamW optimizer and perform hyperparametersearch over learning rates in [1e 5, 1e 4, 1e 3, 1e 2] and weight decays in [1e 3, 1e 2, 1e 1, 5e 1] .",
  "BottleneckText output": "1name but she didnt answer i went to the bathroom and i was crying i had no ideawhat to do i didnt know what i could do to help her i just wanted to get her out ofthere i was so scared i couldnt even look at her i just 5damn movie theater and watch my wife die right there and i feel so horrible but imjust so angry and so incredibly sad i hate her for not telling me that i am the best andthe most beautiful and that she should never have to work so hard and so 50the manager talking to a guy i wasnt familiar with on the phone we made it to thebar and there were at least of us and i remember some of the conversations we hadand the whole place seemed to be filled with the same stuff the only difference was that 100in a large city with about hundred students from a variety of major and regionaluniversities all graduating with degrees in physics history and math in engineeringand biology all of this was based around a very big and complicated system ofprogramming that had been built by very very small and 500a university which was in a suburb of a city with a small town of about millionstudents all graduating with degrees in physics i was one of the youngest of them allof a sudden a very powerful and intelligent individual with a phd in physics from asmall liberal arts 1000in a city with about thousand students all graduating from a prestigious college in themiddle east there are hundreds of universities all over the country where i was raisedfrom what i saw was a very large area with lots of very liberal arts majors and anaverage high school level Fullhe was born in a city that was mostly rural but was in the south of the us and was anengineer and a teacher all of which i had a good relationship with she was also veryinterested in working in a computer science program but had a job in another Groundtruthwere both from up north were both kind of newish to the neighborhood this is inflorida we both went to college not great colleges but man we graduated and imactually finding myself a little jealous of her because she has this really cool jobwashing dogs she had horses back home and she really",
  "A.5Two-Way Evaluation": ": BrainDiffuser identification accuracy. We evaluate the agreement between latent em-beddings of of ground-truth and decoded images of the BrainDiffuser method using the identificationaccuracy protocol described in . : Takagi identification accuracy. We evaluate the agreement between latent embeddings ofof ground-truth and decoded images of the Takagi method using the identification accuracy protocoldescribed in ."
}