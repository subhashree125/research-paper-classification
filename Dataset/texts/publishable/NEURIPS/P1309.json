{
  "Abstract": "In a graph bisection problem, we are given a graph G with two equally-sized unlabeled com-munities, and the goal is to recover the vertices in these communities. A popular heuristic,known as spectral clustering, is to output an estimated community assignment based on theeigenvector corresponding to the second smallest eigenvalue of the Laplacian of G. Spectralalgorithms can be shown to provably recover the cluster structure for graphs generated fromcertain probabilistic models, such as the Stochastic Block Model (SBM). However, spectral clus-tering is known to be non-robust to model mis-specification. Techniques based on semidefiniteprogramming have been shown to be more robust, but they incur significant computationaloverheads.In this work, we study the robustness of spectral algorithms against semirandom adversaries.Informally, a semirandom adversary is allowed to helpfully change the specification of themodel in a way that is consistent with the ground-truth solution. Our semirandom adversariesin particular are allowed to add edges inside clusters or increase the probability that an edgeappears inside a cluster. Semirandom adversaries are a useful tool to determine the extent towhich an algorithm has overfit to statistical assumptions on the input.On the positive side, we identify classes of semirandom adversaries under which spectralbisection using the unnormalized Laplacian is strongly consistent, i.e., it exactly recovers theplanted partitioning. On the negative side, we show that in these classes spectral bisection withthe normalized Laplacian outputs a partitioning that makes a classification mistake on a constantfraction of the vertices. Finally, we demonstrate numerical experiments that complement ourtheoretical findings.",
  "Introduction": "Graph partitioning or clustering is a fundamental unsupervised learning primitive.In a graphpartitioning problem, one seeks to identify clusters of vertices that are highly internally connectedand sparsely connected to the outside. This task is of particular significance when the given graphpresents a latent community structure.In this setting, the goal is to recover the communitiesas accurately as possible. Various statistical models that attempt to capture this situation have University of Utah. Email: of Chicago. Email: Polytechnique Fdrale de Lausanne. Email: Technological Institute at Chicago. Email: Polytechnique Fdrale de Lausanne. Email: Polytechnique Fdrale de Lausanne. Email: .",
  "been proposed and studied in the literature. Perhaps the most popular of these is the SymmetricStochastic Block Model (SSBM) [HLL83]": "Following the notation of previous works [AFWZ20; DLS21], in this paper we describe an SSBMwith specifications n, P1, P2, p, q, where n is an even positive integer, P1 and P2 are a partitioningof the vertex set V = {1, . . . , n} into subsets of equal size, and p and q are probabilities. Withoutloss of generality, we may assume that the partitions P1 and P2 consist of vertices 1, . . . , n/2and n/2 + 1, . . . , n, respectively. Hence, with a mild abuse of notation, we write an SSBM withparameters n, p, q only and write it as SSBM(n, p, q). Now, let SSBM(n, p, q) be a distribution overrandom undirected graphs G = (V, E) where each edge (v, w) P1 P1 and (v, w) P2 P2(which we refer to as internal edges) appears independently with probability p, and each edge(v, w) P1 P2 (which we refer to as crossing edges) appears independently with probability q.When p q, there should be many more internal edges than crossing edges. Hence, we expect thecommunity structure to become more evident as p tends away from q. In such scenarios, our general algorithmic goal is to efficiently identify P1 and P2 when given Gwithout any community labels. This task is hereafter referred to as the graph bisection problem.In this work, we will be interested in exact recovery, also known as strong consistency, in whichwe want an algorithm that, with probability at least 1 1/n over the randomness of the instance,exactly returns the partition {P1, P2} for all n sufficiently large. Other approximate notions ofrecovery (such as almost exact, partial, and weak recovery) are also well-studied but are beyondthe scope of this work. Although the SSBM(n, p, q) distribution over graphs is a useful starting point for algorithm designand has led to a deep theory about when recovery is possible and of what nature [Abb18], it may notbe representative of all scenarios in which we should expect our algorithms to succeed. To remedythis, researchers have proposed several different random graph models that may be more reflectiveof properties satisfied by real-world networks. These include the geometric block model [GNW24],the Gaussian mixture block model [LS24], and others. In this paper, we take a different perspective to graph generation by considering various semiran-dom models. At a high level, a semirandom model for a statistical problem interpolates between anaverage-case input (for example produced by a model such as the SSBM) and a worst-case input,in a way that still allows for a meaningful notion of ground-truth solution. In our context of graphbisection, this can be achieved by an adversary adding internal edges or by the distribution ofinternal edges itself being nonhomogeneous (i.e., every internal edge (v, w) appears independentlywith probability pvw p, where the pvw may be chosen adversarially for each internal edge). Re-searchers have studied similar semirandom models for graph bisection [FK01; MMV12; MPW16;Moi21; CdM24] and other statistical problems such as classification under Massart noise [MN06],detecting a planted clique in a random graph [FK01; CSV17; MMT20; BKS23], sparse recovery[KLLST23], and top-K ranking [YCOM24]. These modeling modifications are not necessarily meant to capture a real-world data generationprocess. Rather, they are a useful testbed with which we can determine whether commonly usedalgorithms have overfit to statistical assumptions present in the model.In particular, observethat these changes in model specification are ostensibly helpful, in that increasing the number ofinternal edges should only enhance the community structure. Perhaps surprisingly, it is knownthat a number of natural algorithms that succeed in the SSBM setting no longer work under suchhelpful modifications [Moi21]. Therefore, it is natural to ask which algorithms for graph bisectionare robust in semirandom models.",
  ":return {S, V \\ S}": "various semirandom models [FK01; MMV12; MPW16; Moi21; CdM24]. However, in practice, it isimpractical to run such an algorithm due to computational costs. Another class of algorithms, thatwe call spectral algorithms, is more widely used in practice. Loosely speaking, a spectral algorithmconstructs a matrix M that is a function of the graph G and outputs a clustering arising fromthe embedding of the vertices determined by the eigenvectors of M. Popular choices of matricesinclude the unnormalized Laplacian LG and the normalized Laplacian LG (we will formally defineand intuit these notions in the sequel) [Von07]. This is because structural properties of both LGand LG imply that the second smallest eigenvalue of each, denoted as 2(LG) and 2(LG), servesas a continuous proxy for connectivity, and the corresponding eigenvector, u2(LG) and u2(LG),has entries whose signs reveal a lot of information about the underlying community structure. Thismotivates Algorithm 1. It can be run, for example, with Matrix(G) := LG or Matrix(G) := LG.Following this discussion, we arrive at the question we study in this paper.",
  "Main contributions.Our results show a surprising difference in the robustness of spectralbisection when considering the normalized versus the unnormalized Laplacian. We summarize ourresults below:": "Consider a nonhomogeneous symmetric stochastic block model with parameters q < p < p,where every internal edge appears independently with probability puv [p, p] and everycrossing edge appears independently with probability q. We show that under an appropriatespectral gap condition, the spectral algorithm with the unnormalized Laplacian exactly re-covers the communities P1 and P2. Moreover, this holds even if an adversary plants npinternal edges per vertex prior to the edge sampling phase. Consider a stronger semirandom model where the subgraphs on the two communities P1 andP2 are adversarially chosen and the crossing edges are sampled independently with probabil-ity q. We show that if the graph is sufficiently dense and satisfies a spectral gap condition,then the spectral algorithm with the unnormalized Laplacian exactly recovers the communi-ties P1 and P2. We show that there is a family of instances from a nonhomogeneous symmetric stochasticblock model in which the spectral algorithm achieves exact recovery with the unnormalizedLaplacian, but incurs a constant error rate with the normalized Laplacian. This is surprisingbecause it contradicts conventional wisdom that normalized spectral clustering should befavored over unnormalized spectral clustering [Von07].",
  "We also numerically complement our findings via experiments on various parameter settings": "Outline.The rest of this paper is organized as follows. In , we more formally defineour semirandom models, the Laplacians L and L, and formally state our results. In ,we give sketches of the proofs of our results. In , we show results from numerical trials suggested by our theory. In Appendices A.1 and A.5 we prove important auxiliary lemmas we needfor our results. In Appendix A.6, we prove our robustness results for the unnormalized Laplacian.In Appendix A.8, we prove our inconsistency result for the normalized Laplacian. In Appendix B,we give additional numerical trials and discussion.",
  "In this paper, we study unnormalized and normalized spectral clustering in several semirandomSSBMs. These models permit a richer family of graphs than the SSBM alone": "Matrices related to graphs.Throughout this paper, all graphs are to be interpreted as beingundirected, and we assume that the vertices of an n-vertex graph coincide with the set {1, . . . , n}.With this in mind, we begin with defining various matrices associated with graphs, building upto the unnormalized and normalized Laplacians, which are central to the family of algorithms weanalyze (Algorithm 1).",
  "Next, we define the spectral bisection algorithms. We will discuss some intuition for why thesealgorithms are reasonable heuristics in": "Definition 2.5 (Unnormalized and normalized spectral bisection). Let G = (V, E) be a graph, andlet its unnormalized and normalized Laplacians be L and L, respectively. We refer to the algorithmresulting from running Algorithm 1 on G with Matrix(G) := LG as unnormalized spectral bisection.We refer to the algorithm resulting from running Algorithm 1 on G with Matrix(G) = LG asnormalized spectral bisection.",
  "Our goal is to understand when the above algorithms, applied to a graph with a latent communitystructure, achieve exact recovery or strong consistency, defined as follows": "Definition 2.6. Let {P1, P2} be a partitioning of V = {1, . . . , n}, and let D := D({P1, P2}) be adistribution over n-vertex graphs G = (V, E). We say that an algorithm is strongly consistent orachieves exact recovery on D if given a graph G D it outputs the correct partitioning {P1, P2}with probability at least 1 1/n over the randomness of G.",
  "Our first model is a family of nonhomogeneous symmetric stochastic block models, defined below": "Model 1 (Nonhomogeneous symmetric stochastic block model). Let n be an even positive integer,V = {1, . . . , n}, {P1, P2} be a partitioning of V into two equally-sized subsets, and q < p pbe probabilities. Let D be any probability distribution over graphs G = (V, E) such that for every(v, w) P1 P1 and (v, w) P2 P2, the edge (v, w) appears in E independently with someprobability pvw [p, p], and for every (v, w) P1 P2, the edge (v, w) appears in E independentlywith probability q. We call such D a nonhomogeneous symmetric stochastic block model (which wewill abbreviate as NSSBM). We call the set of all such D the family of nonhomogeneous stochasticblock models with parameters p, p, q, written as NSSBM(n, p, p, q).",
  ",": "where the leftmost matrix denotes the expected adjacency matrix of SSBM(n, p, q), the rightmostmatrix denotes the expected adjacency matrix of SSBM(n, p, q), Jk denotes the kk all-ones matrix,and PP1 and PP2 denote the edge probability matrices for edges internal to P1 and P2, respectively. The above also shows that the rank of the expected adjacency matrix for SSBM(n, p, q) is 2.However, the rank for the expected adjacency matrix for some NSSBM distribution may be as largeas (n). Perhaps surprisingly, this will turn out to be unimportant for our entrywise eigenvectorperturbation analysis. In particular, the tools we use were originally designed for low-rank signalmatrices or spiked low-rank signal matrices [AFWZ20; DLS21; BV24], but we will see that theycan be adapted to the signal matrices we consider. The NSSBM family generalizes the symmetric stochastic block model described in the previoussection this is attained by setting pvw = p for all internal edges (v, w). However, it can alsoencode biases for certain graph properties. For instance, a distribution from the NSSBM familymay encode the idea that certain subsets of P1 are expected to be denser than P1 as a whole.",
  "With this definition in hand, we are ready to formally state our first technical result in Theorem 1": "Theorem 1. Let p, p, q be probabilities such that q < p p and such that := p/(p q) is anarbitrary constant. Let D NSSBM(n, p, p, q). Let n N() where the function N() only dependson . There exists a universal constant C > 0 such that if",
  "then unnormalized spectral bisection is strongly consistent on D": "Proof of Theorem 2. In this proof, let L be the Laplacian matrix that agrees with L on all internaledges and agrees with E [L] on all crossing edges. Let L(cross) denote the Laplacian matrix corre-sponding to the cross edges, so we can write L = L L(cross) + EL(cross). Although L = E [L]due to the adaptive adversary, by Lemma A.14, we still have Lu2 = 2u2 = nqu2. Moreover,(L L)u2 is the vector whose entries are of the form 2(dout[v] E [dout[v]])/n. Thus, we will beable to apply Lemma A.16 and Lemma A.12 later on. Finally, observe that i(L) i(L) for alli 3 and 2(L) = 2(L) = nq. Thus, one can use the spectral gap 3(L) 2(L) to reason about3 2.",
  "Deterministic clusters model": "Given Theorem 1, it is natural to ask what happens if we allow the adversary full control overthe structure of the graphs in P1 and P2 instead of simply allowing the adversary to perturb theedge probabilities. In this section, we answer this question. We first describe a more adversarialsemirandom model than the NSSBM family. We call this model the deterministic clusters model,defined as follows. Model 2 (Deterministic clusters model). Let n be an even positive integer, V = {1, . . . , n}, {P1, P2}be a partitioning of V into two equally-sized subsets, q be a probability, and din be an integer degreelower bound. Consider a graph G = (V, E) generated according to the following process.",
  ". The adversary arbitrarily adds edges (v, w) P1P1 and (v, w) P2P2 to E after observingthe edges sampled by nature": "We call a distribution D of graphs generated according to the above process a deterministic clus-ters model (DCM). We call the set of all such D the family of deterministic clusters models withparameters din and q, written as DCM(n, din, q). The DCM graph generation process is heavily motivated by the one studied by Makarychev,Makarychev, and Vijayaraghavan [MMV12]. This model is much more flexible than the SSBM andNSSBM settings in that the graphs the adversary draws on P1 and P2 are allowed to look very farfrom random graphs. This means the DCM is a particularly good benchmark for algorithms toensure they are not implicitly using properties of random graphs that might not hold in the worstcase.",
  "Within the DCM setting, we have Theorem 2": "Theorem 2. Let q be a probability and din be an integer, and let D DCM(n, din, q). For G D,let L denote the expectation of L after step (2) but before step (3) in Model 2. There exists constantsC1, C2, C3 > 0 such that for all n sufficiently large, if",
  "In Theorem 3, we prove that there is a subfamily of instances belonging to NSSBM(n, p, p, q) with": "p = 6p, q = p/2 on which unnormalized spectral bisection is strongly consistent (following fromTheorem 1) but normalized spectral clustering is inconsistent in a rather strong sense. Thus, onecannot obtain results similar to Theorem 1 and Theorem 2 for normalized spectral bisection. Theorem 3. For all n sufficiently large, there exists a nonhomogeneous stochastic block model suchthat unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection (bothsymmetric and random-walk) incurs a misclassification rate of at least 24% with probability 11/n.",
  "We prove Theorem 3 in Appendix A.8. Furthermore, we expect that it is straightforward to adaptthe example in Theorem 3 to prove an analogous result for our DCM setting": "The result of Theorem 3 may run counter to conventional wisdom, which suggests that normalizedspectral clustering should be favored over the unnormalized variant [Von07].Perhaps a morenuanced view in light of Theorem 1 and Theorem 2 is to acknowledge that the normalized Laplacianand its eigenvectors enjoy stronger concentration guarantees [SB15; DLS21], but the unnormalizedLaplacians second eigenvector is more robust to monotone adversarial changes.",
  "Open problems": "Perhaps the most natural follow-up question inspired by our results is to determine whether therestriction that every internal edge probability pvw p can be lifted entirely while still maintainingstrong consistency of the unnormalized Laplacian (Theorem 2).Another exciting direction forfuture work is to lower the degree and/or spectral gap requirement present in our results in theDCM setting (Theorem 2). Finally, we only study insertion-only monotone adversaries, as crossingedge deletions change the second eigenvector of the expected Laplacian. It would be illuminatingto understand the robustness of Laplacian-based spectral algorithms against a monotone adversarythat is also allowed to delete crossing edges. We are optimistic that the answers to one or more ofthese questions will further improve our understanding of the robustness of spectral clustering tohelpful model misspecification.",
  "Analysis sketch": "First, let us give some intuition as to why one may expect that unnormalized spectral bisectionis robust against our monotone adversaries. Here and in the sequel, let u2 = [1n/2 1n/2]/n,where 1k denotes the all-1s vector in k dimensions and denotes vector concatenation. Let Lbe the unnormalized Laplacian of the graph we want to partition, L := E [L], E := L L, andi := i(L) for 1 i n. For an edge (v, w), let evw := ev ew, so that evw is an edge incidencevector corresponding to the edge (v, w). Let pvw be the probability that the edge (v, w) appears inG and observe that L can be written as",
  "(v,w)Ecrossingq evweTvw ,": "where Einternal = (P1P1)(P2P2) and Ecrossing = P1P2. We can verify that u2 is an eigenvectorof L indeed, we do so in Lemma A.14. And, for now, assume that u2 does correspond to thesecond smallest eigenvalue of L (in our NSSBM family, this is easily ensured by enforcing p > q).Moreover, for every internal edge (v, w) Einternal, we have evw, u2 = 0. Hence, any changes ininternal edges do not change the fact that u2 is an eigenvector of the perturbed matrix. Thus, ifthe sampled L is close enough to L, then it is plausible that the second eigenvector of L, denotedas u2, is pretty close to u2. In fact, the following conceptually stronger statement holds. If thesubgraph formed by selecting just the crossing edges of G is regular, then u2 is an eigenvector of L.This follows from the fact that u2 is an eigenvector of the unnormalized Laplacian of any regularbipartite graph where both sides have size n/2 and the previous observation that every internaledge is orthogonal to u2. To make this perturbation idea more formal, we recall the Davis-Kahan Theorem. Loosely, it statesthat u2 u22 (L L)u22 /(3 2) (we give a more formal statement in Lemma A.15).Expanding the entrywise absolute value |(L L)u2| reveals that its entries can be expressed as2 |dout[v] E [dout[v]]| /n, where dout[v] denotes the number of edges incident to v crossing to theopposite community as v. This is unaffected by any increase in the number of edges incident tov that stay within the same community as v, denoted as din[v]. Hence, regardless of how manyinternal edges we add before sampling or what substructures they encourage/create, if we have2 3, then we get u2 u22 o(1). This immediately implies that u2 is a correct classifieron all but an o(1) fraction of the vertices. Entrywise analysis of u2 and NSSBM strong consistency.In order to achieve strongconsistency, we need that for all n sufficiently large, u2 is a perfect classifier. Unfortunately, theabove argument does not immediately give that. In particular, in the density and spectral gapregimes we consider, the bound of o(1) yielded by the Davis-Kahan theorem is not sufficientlysmall to directly yield u2 u22 1/n. Instead, we carry out an entrywise analysis of u2. Ageneral framework for doing so is given by Abbe, Fan, Wang, and Zhong [AFWZ20] and is adaptedto the unnormalized and normalized Laplacians by Deng, Ling, and Strohmer [DLS21]. At a high level, we adapt the analysis of Deng, Ling, and Strohmer [DLS21] to our setting. We con-sider the intermediate estimator vector (D 2I)1 Au2. This is a natural choice because we canverify (D2I)1Au2 = u2. We will see that it is enough to show that this intermediate estimatorcorrectly classifies all the vertices while satisfying |(D 2I)1A(u2 u2)| | (D 2I)1 Au2|(again, the absolute value is taken entrywise). With this in mind, taking some entry indexed byv V and multiplying both sides by d[v]2 (which we will show is positive with high probability),we see that it is enough to show",
  "where av denotes the v-th row of A. The advantage of this rewrite is that the right hand side canbe uniformly bounded, so it is enough to control the left hand side": "To argue about the left hand side of (3.1), it may be tempting to use the fact that av is a Bernoullirandom vector and use Bernsteins inequality to argue about the sum of rescalings of these Bernoullirandom variables. Unfortunately, we cannot do this since u2 and av are dependent. To resolvethis, we use a leave-one-out trick [AFWZ20; BV24]. We can think of this as leaving out the vertexv corresponding to the entry we want to analyze and sampling the edges incident to the rest of thevertices. The second eigenvector of the resulting L(v), denoted as u(v)2 , is a very good proxy for u2and is independent from av. Hence, we may complete the proof of Theorem 1.",
  ": A for Theorem 3 is defined to have the above block structure": "One of our main observations is that although this style of analysis was originally built for low-rank signal matrices [AFWZ20; BV24], it can be adapted to handle the nonhomogeneity inside P1and P2. In particular, the nonhomogeneity we permit in the NSSBM family may make L lookvery far from a spiked low-rank signal matrix. Furthermore, our entrywise analysis of eigenvectorsunder perturbations is one of the first that we are aware of that moves beyond analyzing low-ranksignal matrices or spiked low-rank signal matrices. Extension to deterministic clusters.To prove Theorem 2, we start again at (3.1). Analternate way to upper bound the left hand side is to use the Cauchy-Schwarz inequality. A variantof the Davis-Kahan theorem gives us control over u2 u22 while av2 = d[v]. The advantageof this is that we get a worst-case upper bound on the left hand side of (3.1) it holds no matterwhat edges orthogonal to u2 are inserted before or after nature samples the crossing edges (whichare precisely the internal edges). Combining these and using the fact that the right hand side of(3.1) is increasing in din[v] (and increases faster than av2 =",
  "d[v]) allows us to complete theproof of Theorem 2": "Inconsistency of normalized spectral bisection.Finally, we describe the family of hardinstances we use to prove Theorem 3. To motivate this family of instances, recall that by the graphversion of Cheegers inequality, the second eigenvalue of L and the corresponding eigenvector can beused to find a sparse cut in G. Thus, if we create sparse cuts inside P1 that are sparser than the cutformed by separating P1 and P2, then conceivably the normalized Laplacians second eigenvectormay return the new sparser cut. To make this formal, consider the following graph structure.Let n be a multiple of 4.LetL1 consist of indices 1, . . . , n/4, L2 consist of indices n/4 + 1, . . . , n/2, and R consist of indicesn/2 + 1, . . . , n. Consider the block structure induced by the matrix A = E [A] shown in . Intuitively, as K gets larger, the cut separating L1 from V \\ L1 becomes sparser. From Cheegersinequality, this witnesses a small 2(L) and therefore the corresponding u2(L) may return the cutL1, V \\ L1. We formally prove that this is indeed what happens when K is a sufficiently largeconstant and then Theorem 3 follows.",
  "Numerical trials": "We programmatically generate synthetic graphs that help illustrate our theoretical findings usingthe libraries NetworkX 3.3 (BSD 3-Clause license), SciPy 1.13.0 (BSD 3-Clause License), andNumPy 1.26.4 (modified BSD license) [HSS08; VGO+20; HMvdW+20]. We ran all our experimentson a free Google Colab instance with the CPU runtime, and each experiment takes under one hourto run. In this section we focus on a setting that allows relating Theorem 1 and Theorem 3, anddefer more experiments that investigate both NSSBM and DCM graphs to Appendix B.",
  "To put Theorem 1 and Theorem 3 in perspective, we consider graphs generated following theprocess outlined in the proof of Theorem 3, which gives rise to the following benchmark distribution": "Benchmark distribution.Let n be divisible by 4 and let {P1, P 2} be a partitioning ofV = [n] into two equally-sized subsets. Let {L1, L2} be a bipartition of P1 such that |L1| = |L2| =n/4 and call L = P1, R = P2 for convenience as in the proof of Theorem 3.Then, for somep, p, q such that q p p, consider the distribution Dp,p,q over graphs G = (V, E) obtainedby sampling every edge (u, v) (L1 L1) (L2 L2) independently with probability p, everyedge (u, v) (L1 L2) (R R) independently with probability p, and every edge (u, v) L Rindependently with probability q. One can see that Dp,p,q is in fact in the set NSSBM(n, p, p, q). Setup.Let us fix n = 2000, p = 24 log n/n, q = 8 log n/n. For varying values of p in therange [p, 1], we sample t = 10 independent draws G from Dp,p,q. For each of them, we run spectralbisection (i.e. Algorithm 1) with matrices L, Lsym, Lrw, A. Then, we compute the agreement of thebipartition hence obtained (with respect to the planted bisection), that is the fraction of correctlyclassified vertices. We average the agreement across the t independent draws. The results are shownin the top left plot of . Another natural way to get a bipartition of V from the eigenvectoris a sweep cut. In a sweep cut, we sort the entries of u2 and take the vertices corresponding to thesmallest n/2 entries to be on one side of the bisection and put the remaining on the other side.The average agreement obtained in this other fashion is shown in the bottom left plot of .Theoretical framing.As per Theorem 1, we expect unnormalized spectral bisection to achieveexact recovery (i.e. agreement equal to 1) whenever p pmax, where",
  "n log n(4.1)": "is obtained by rearranging the precondition of Theorem 1, ignoring the constants and disregardingthe fact that should be O(1). On the contrary, the proof of Theorem 3 shows that normalizedspectral bisection misclassifies a constant fraction of vertices provided that p/q 2 (which ourchoice of parameters satisfies) and p pthr, where",
  "pthr = 3 p2/q .(4.2)": "In , the solid vertical line corresponds to the value of pthr on the x-axis, and the dashedvertical line corresponds to the value of pmax on the x-axis. In particular, observe that in our settingpthr < pmax, so there is an interval of values for p where we expect Theorem 1 and Theorem 3 toapply simultaneously. Empirical evidence: consistency.One can see from the top left plot in that theagreement of unnormalized spectral bisection is 100% for all values of p, even beyond pthr andpmax. On the other hand, the agreement of the bipartition obtained from all other matrices (henceincluding normalized spectral bisection) drops below 70% well before the threshold pthr predictedby Theorem 3. From the right plot in , we see that computing the bipartition by taking asweep cut of n/2 vertices does not change the results u2 of the unnormalized Laplacian continuesto achieve 100% agreement, while for all other matrices the corresponding u2 remains inconsistent. Empirical evidence: embedding variance.From the setting of the experiment we justillustrated, observe that as we increase p, we expect the subgraph G[L] to have increasing volume.As illustrated in , this seems to correlate with a decrease in the variance of the secondeigenvector u2 of the unnormalized Laplacian with respect to the ideal second eigenvector u2.More precisely, we compute the average distance squared of the embedding of a vertex in u2 fromits ideal embedding in u2, i.e. the quantity",
  "mins{1}1n u2 s u222 .(4.3)": ": Top left, bottom left: Agreement with the planted bisection of the bipartition ob-tained from several matrices associated with an input graph generated from a distribution inNSSBM(n, p, p, q) for fixed values of n, p, q and varying values of p. In the top left plot, the bi-partition is the 0-cut of the second eigenvector, as in Algorithm 1. In the bottom left plot, thebipartition is the sweep cut of the first n/2 vertices in the second eigenvector. The dashed verti-cal line corresponds to pmax = pmax(n, p, q) (see (4.1)), and the solid vertical line corresponds topthr = pthr(n, p, q) (see (4.2)). Top middle, top right, bottom middle: Embedding of the ver-tices given by the second eigenvector u2 of several matrices associated with a graph sampled fromDp,p,q with p = pthr. Horizontal dashed lines, from top to bottom, correspond to 1/n, 0, 1/nrespectively.Bottom right: Variance of the embedding in the second eigenvector u2 of the unnormalizedLaplacian with respect to the ideal eigenvector u2 (see (4.3)), for input graphs generated from adistribution in NSSBM(n, p, p, q) with fixed values of n, p, q and varying values of p.",
  "This suggests that not only does the second eigenvector of the unnormalized Laplacian remainrobust to monotone adversaries, but it actually concentrates more strongly around the ideal em-bedding u2": "Empirical evidence: example embedding.Let us fix the value p = pthr, for which we see in that all matrices except the unnormalized Laplacian fail to recover the planted bisection. Wegenerate a graph from Dp,p,q, and plot how the vertices are embedded in the real line by the secondeigenvector of all the matrices we consider. The result is shown in , where the three horizontaldashed lines, from top to bottom, respectively correspond to the value of 1/n, 0, 1/n on they-axis.",
  "literature, see the survey by Abbe [Abb18]. In what follows, we discuss the works we believe aremost related to what we study in this paper": "As mentioned in the introduction, perhaps the most fundamental and well-studied model is thesymmetric stochastic block model (SSBM), due to [HLL83]. The celebrated work of Abbe, Bandeira,and Hall [ABH16] gives sharp bounds on the threshold for exact recovery for the SSBM setting.They complement their result by showing that SDP based methods can achieve the informationtheoretic lower bound for the planted bisection problem, even with a monotone adversary [Moi21].A line of work [AFWZ20; DLS21] demonstrates that natural spectral algorithms achieve exactrecovery for the SSBM all the way to the information-theoretic threshold. Generalizations of the symmetric stochastic block model.Since the introduction ofSBMs [HLL83], numerous variants have been proposed that are designed to better reflect real-worldgraph properties. For instance, real-life social networks are likely to contain triangles. To addressthis, Sankararaman and Baccelli [SB17] introduced a spatial stochastic block model, sometimesknown as the geometric stochastic block model (GSBM). Other variations were introduced in theworks of [GPMS18; GMPS19]. Subsequent work studies the performance of spectral algorithms oncertain Gaussian or Geometric Mixture block models [ABRS20; ABD21; LS24; GNW24]. Studying community detection with a semirandom model approaches this modeling question dif-ferently.Rather than implicitly encouraging a particular structure within the clusters like themodels just mentioned, a semirandom adversary (including the ones we study in this paper) canmore directly test the robustness of the algorithm to specially designed substructures. Semirandom and monotone adversaries.As far as we are aware, Blum and Spencer [BS95]were the first to introduce a semirandom model. Within this model, they studied graph coloringproblems.Feige and Kilian [FK01] demonstrated that semidefinite programming methods canaccurately recover communities up to a certain threshold, even in the semi-random setting. Otherproblems, such as detecting a planted clique [Jer92; Ku95; BHKKMP19], have also been studied inthe semi-random model of [FK01]. In the setting of planted clique, a natural spectral algorithm failsagainst monotone adversaries [MMT20; BKS23]. Monotone adversaries and semirandom modelshave also been extensively studied for other statistical and algorithmic problems [VA18; KLLST23;GC23; BGLMSY24]. Finally, [SL17] shows that a spectral heuristic due to Boppana [Bop87] isrobust under a monotone adversary that is allowed to both insert internal edges and delete crossingedges. However, as far as we are aware, this algorithm does not fit in the framework of Algorithm 1. We remark that the models we study in this paper are most closely related to models studied by[MN06] and [MMV12]. In particular, allowing increased internal edge probabilities is analogous toMassart noise in classification problems, and our model with adversarially chosen internal edgescan be seen as the same model as that studied in [MMV12] (although without allowing crossingedge deletions). Finally, note that Cohen-Addad, dOrsi, and Mousavifar [CdM24] give a near-linear time algorithm for graph clustering in the model of [MMV12], though they do not explicitlyshow their algorithm is strongly consistent on instances that are information-theoretically exactlyrecoverable. Acknowledgments.AB was partially supported by the National Science Foundation underGrant Nos. CCF-2008688 and CCF-2047288. NSM was supported by a National Science Foun-dation Graduate Research Fellowship. We thank Avrim Blum and Yury Makarychev for helpfuldiscussions. We thank Nirmit Joshi for pointing us to the reference [DLS21]. [Abb18]Emmanuel Abbe. Community detection and stochastic block models: recent de-velopments. Journal of Machine Learning Research, 18(177):186, 2018. arXiv:1703.10146 [math.PR]. url: 480.html(cited on pages 2, 12). [ABH16]Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in thestochastic block model. IEEE Transactions on Information Theory, 62(1):471487,2016. doi: 10.1109/TIT.2015.2490670. arXiv: 1405.3267 [cs.SI] (cited onpages 5, 12, 42). [ABRS20]Emmanuel Abbe, Enric Boix-Adser, Peter Ralli, and Colin Sandon. Graph pow-ering and spectral robustness. SIAM Journal on Mathematics of Data Science,2(1):132157, 2020. doi: 10.1137/19M1257135. arXiv: 1809.04818 [cs.DS]. url: (cited on page 12). [AFWZ20]Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywiseeigenvector analysis of random matrices with low expected rank. Annals of statis-tics, 48(3):1452, 2020. arXiv: 1709.09565 [math.ST] (cited on pages 2, 5, 6, 8, 9,12, 26, 27). [ABD21]Konstantin Avrachenkov, Andrei Bobu, and Maximilien Dreveton. Higher-orderspectral clustering for geometric graphs. Journal of Fourier Analysis and Ap-plications, 27(2):22, March 2021. doi: 10.1007/s00041- 021- 09825- 2. arXiv:2009.11353 [cs.LG]. url: on page 12). [BHKKMP19]Boaz Barak, Samuel Hopkins, Jonathan Kelner, Pravesh K Kothari, Ankur Moitra,and Aaron Potechin. A nearly tight sum-of-squares lower bound for the plantedclique problem. SIAM Journal on Computing, 48(2):687735, 2019. arXiv: 1604.03084 [cs.CC] (cited on page 12). [BV24]Abhinav Bhardwaj and Van Vu. Matrix perturbation: davis-kahan in the infinitynorm. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Al-gorithms (SODA). January 2024, pages 880934. arXiv: 2304.00328 [math.PR](cited on pages 5, 8, 9). [BGLMSY24]Avrim Blum, Meghal Gupta, Gene Li, Naren Sarayu Manoj, Aadirupa Saha, andYuanyuan Yang. Dueling optimization with a monotone adversary. In Proceedingsof Thirty Fifth Conference on Algorithmic Learning Theory (ALT), February 2024.arXiv: 2311.11185 [cs.DS] (cited on page 12).",
  "[BS95]Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorablegraphs. Journal of Algorithms, 19(2):204234, 1995. issn: 0196-6774. doi: url: (cited on page 12)": "[Bop87]Ravi B. Boppana. Eigenvalues and graph bisection: an average-case analysis.In 28th Annual Symposium on Foundations of Computer Science (sfcs 1987),pages 280285, 1987. doi: 10.1109/SFCS.1987.22 (cited on page 12). [BKS23]Rares-Darius Buhai, Pravesh K. Kothari, and David Steurer. Algorithms approach-ing the threshold for semi-random planted clique. In Proceedings of the 55th An-nual ACM Symposium on Theory of Computing, STOC 2023, pages 19181926, Or-lando, FL, USA. Association for Computing Machinery, 2023. isbn: 9781450399135.arXiv: 2212.05619 [cs.DS] (cited on pages 2, 12). [CSV17]Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusteddata. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory ofComputing, STOC 2017, pages 4760, Montreal, Canada. Association for Comput-ing Machinery, 2017. isbn: 9781450345286. arXiv: 1611.02315 [cs.LG] (cited onpage 2). [CdM24]Vincent Cohen-Addad, Tommaso dOrsi, and Aida Mousavifar. A near-linear timeapproximation algorithm for beyond-worst-case graph clustering. In Forty-first In-ternational Conference on Machine Learning, 2024. arXiv: 2406.04857 [cs.DS].url: (cited on pages 2, 3, 12). [DLS21]Shaofeng Deng, Shuyang Ling, and Thomas Strohmer. Strong consistency, graphlaplacians, and the stochastic block model. Journal of Machine Learning Research,22(117):144, 2021. arXiv: 2004.09780 [stat.ML] (cited on pages 2, 58, 12, 31,40). [FK01]Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. J. Comput.Syst. Sci., 63(4):639671, December 2001. issn: 0022-0000. doi: 10.1006/jcss.2001.1773. url: (cited on pages 2,3, 5, 12). [GMPS19]Sainyam Galhotra, Arya Mazumdar, Soumyabrata Pal, and Barna Saha. Connec-tivity of Random Annulus Graphs and the Geometric Block Model. In DimitrisAchlioptas and Lszl A. Vgh, editors, Approximation, Randomization, and Com-binatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2019),volume 145 of Leibniz International Proceedings in Informatics (LIPIcs), 53:153:23, Dagstuhl, Germany. Schloss Dagstuhl Leibniz-Zentrum fr Informatik,2019. isbn: 978-3-95977-125-2. arXiv: 1804.05013 [cs.DM] (cited on page 12). [GPMS18]Sainyam Galhotra, Soumyabrata Pal, Arya Mazumdar, and Barna Saha. The geo-metric block model and applications. In 2018 56th Annual Allerton Conference onCommunication, Control, and Computing (Allerton), pages 11471150, 2018. doi:10.1109/ALLERTON.2018.8635938 (cited on page 12).",
  "[GC23]Xing Gao and Yu Cheng. Robust matrix sensing in the semi-random model. InThirty-seventh Conference on Neural Information Processing Systems, 2023. url: (cited on page 12)": "[GNW24]Julia Gaudio, Xiaochun Niu, and Ermin Wei. Exact community recovery in the geo-metric sbm. In Proceedings of the 2024 Annual ACM-SIAM Symposium on DiscreteAlgorithms (SODA). 2024, pages 21582184. doi: 10.1137/1.9781611977912.78.arXiv: 2307.11196 [cs.SI]. url: (cited on pages 2, 12). [HSS08]Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure,dynamics, and function using NetworkX. Technical report, Los Alamos NationalLab.(LANL), Los Alamos, NM (United States), 2008 (cited on page 9). [HMvdW+20]Charles R. Harris, K. Jarrod Millman, Stfan J. van der Walt, Ralf Gommers,Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg,Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. vanKerkwijk, Matthew Brett, Allan Haldane, Jaime Fernndez del Ro, Mark Wiebe,Pearu Peterson, Pierre Grard-Marchant, Kevin Sheppard, Tyler Reddy, War-ren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Ar-ray programming with NumPy. Nature, 585(7825):357362, September 2020. doi:10.1038/s41586-020-2649-2. url: (cited on page 9). [HLL83]Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochas-tic blockmodels: first steps. Social Networks, 5(2):109137, 1983. issn: 0378-8733.doi: 8733(83)90021- 7. url: (cited onpages 2, 12).",
  "[Jer92]Mark Jerrum. Large cliques elude the metropolis process. Random Struct. Algo-rithms, 3(4):347360, 1992. doi: 10.1002/RSA.3240030402. url: (cited on page 12)": "[KLLST23]Jonathan Kelner, Jerry Li, Allen X. Liu, Aaron Sidford, and Kevin Tian. Semi-random sparse recovery in nearly-linear time. In Gergely Neu and Lorenzo Rosasco,editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195of Proceedings of Machine Learning Research, pages 23522398. PMLR, July 2023.arXiv: 2203.04002 [cs.DS]. url: (cited on pages 2, 12). [Ku95]Ludk Kuera. Expected complexity of graph partitioning problems. Discrete Ap-plied Mathematics, 57(2):193212, 1995. issn: 0166-218X. doi: https : / / doi .org/10.1016/0166- 218X(94)00103- K. url: Combinatorial optimization 1992(cited on page 12). [LLV17]Can M Le, Elizaveta Levina, and Roman Vershynin. Concentration and regular-ization of random graphs. Random Structures & Algorithms, 51(3):538561, 2017.arXiv: 1506.00669 [math.PR] (cited on page 20).",
  "[MN06]Pascal Massart and lodie Ndlec. Risk bounds for statistical learning. TheAnnals of Statistics, 34(5), October 2006. issn: 0090-5364. doi: 10 . 1214 /009053606000000786. url: on pages 2, 12)": "[MMT20]Theo McKenzie, Hermish Mehta, and Luca Trevisan. A new algorithm for therobust semi-random independent set problem. In Proceedings of the Thirty-FirstAnnual ACM-SIAM Symposium on Discrete Algorithms, SODA 20, pages 738746, Salt Lake City, Utah. Society for Industrial and Applied Mathematics, 2020.arXiv: 1808.03633 [cs.DS] (cited on pages 2, 12). [Moi21]Ankur Moitra. Semirandom stochastic block models. In Beyond the Worst-CaseAnalysis of Algorithms. Tim Roughgarden, editor. Cambridge University Press,2021, pages 212233. doi: 10.1017/9781108637435.014 (cited on pages 2, 3, 12). [MPW16]Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruc-tion thresholds for community detection? In Proceedings of the Forty-Eighth AnnualACM Symposium on Theory of Computing, STOC 16, pages 828841, Cambridge,MA, USA. Association for Computing Machinery, 2016. isbn: 9781450341325. doi:10.1145/2897518.2897573. arXiv: 1511.01473 [cs.DS]. url: (cited on pages 2, 3). [SB17]Abishek Sankararaman and Franois Baccelli. Community detection on euclideanrandom graphs. In 2017 55th Annual Allerton Conference on Communication,Control, and Computing (Allerton), pages 510517, 2017. doi: 10.1109/ALLERTON.2017.8262780 (cited on page 12). [SB15]Purnamrita Sarkar and Peter J. Bickel. Role of normalization in spectral clusteringfor stochastic blockmodels. The Annals of Statistics, 43(3), June 2015. issn: 0090-5364. doi: 10 . 1214 / 14 - aos1285. arXiv: 1310 . 1495 [stat.ML]. url: http ://dx.doi.org/10.1214/14-AOS1285 (cited on page 7). [SL17]Martin R. Schuster and Maciej Liskiewicz. New Abilities and Limitations of Spec-tral Graph Bisection. In Kirk Pruhs and Christian Sohler, editors, 25th AnnualEuropean Symposium on Algorithms (ESA 2017), volume 87 of Leibniz Interna-tional Proceedings in Informatics (LIPIcs), 66:166:15, Dagstuhl, Germany. SchlossDagstuhl Leibniz-Zentrum fr Informatik, 2017. isbn: 978-3-95977-049-1. doi:10.4230/LIPIcs.ESA.2017.66. url: (cited on page 12).",
  "[Ver18]Roman Vershynin. High-dimensional probability: An introduction with applicationsin data science, volume 47. Cambridge university press, 2018 (cited on page 17)": "[VA18]Aravindan Vijayaraghavan and Pranjal Awasthi. Clustering semi-random mixturesof Gaussians. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35thInternational Conference on Machine Learning, volume 80 of Proceedings of Ma-chine Learning Research, pages 50555064. PMLR, July 2018. arXiv: 1711.08841[cs.DS]. url: (cited on page 12). [VGO+20]Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy,David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, JonathanBright, Stfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman,Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Har-ris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt,and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Com-puting in Python. Nature Methods, 17:261272, 2020. doi: 10.1038/s41592-019-0686-2 (cited on page 9).",
  "A.1Concentration inequalities": "Our proof strategy for Theorem 1 and Theorem 2 is to appeal to Lemma A.23, which guaranteesstrong consistency provided that d[v] 2 > 0, din[v] > dout[v], and |av, u2 u2| (din[v] dout[v])/n for all vertices v.Proving that the first two conditions hold is relatively easy.Inthe setting of Theorem 1, it essentially follows from concentration of the degrees, which is provedin Appendix A.2. In the setting of Theorem 2, it follows from the assumptions of the Theorem.Proving that the third condition holds is the main technical challenge.",
  "Taking a union bound over all n vertices completes the proof of Lemma A.4": "Combining the above two lemmas, we obtain a lower-bound on din[v] dout[v]. In particular, thefollowing lemma implies that in the setting of Theorem 1, we have din[v] > dout[v]. This will berequired for applying Lemma A.23. Lemma A.5. There exists a universal constant C > 0 such that with probability 1 , in thesame settings as Lemma A.3 and Lemma A.4 and assuming the gap condition in Theorem 1, ifp q, then for all v V we have",
  "For the matrix concentration lemmas, we need a result due to Le, Levina, and Vershynin [LLV17].We reproduce it below": "Lemma A.9 ([LLV17, Theorem 2.1]). Consider a random graph from the model G(n, {pij}). Letd = maxij npij. For any r 1, the following holds with probability at least 1 nr for a universalconstant C. Consider any subset consisting of 10n/d vertices, and reduce the weights of the edgesincident to those vertices in an arbitrary way. Let d be the maximal degree of the resulting graph.Then, the adjacency matrix A of the new weighted graph satisfies",
  "completing the proof of Lemma A.10": "By applying the above lemma, we can show that there is a gap between 3 and 2, which willallow us to apply Davis-Kahan style bounds. More concretely, Lemma A.11 and Lemma A.12,together with Lemma A.16, show that u2 u22 is small. This will be useful for proving that inthe context for Theorem 1, the condition |av, u2 u2| (din[v] dout[v])/n in Lemma A.23 issatisfied.",
  "In the setting of Theorem 2, we have 3(L) 2(L) > nq, by the theorem assumption. Since L": "is obtained from L by adding the adversarial edges, we have i(L) i(L) for all i. In particular,we have 3(L) 3(L) = 2(L) + (3(L) 2(L)) > nq, where the last inequality is usingthe fact 2(L) 0. Therefore, nq must be the second eigenvalue of L, completing the proof ofLemma A.14.",
  "(L L)u223(L) 2(L)": "Proof of Lemma A.15. One can get this sort of guarantee from variants of the Davis-Kahan theo-rem, but it is more illuminating to write an eigenvalue decomposition and observe it from there.Without loss of generality, assume that u2, u2 0 (indeed, otherwise we can always negate u2 ifthis is not the case). Notice that",
  "np log (n/) + log (n/)": "To appeal to Lemma A.15, we need to understand the entries of the matrix L L(v). It is easyto see that this matrix only has nonzero entries on the diagonal and in the vth row and column.There, the vth row and column of L L(v) are exactly equal to those of L L. Moreover, thew = vth diagonal entry of L L(v) is exactly 1 {(v, w) E} pvw.",
  "Unfortunately, since u2 depends on av av, we cannot express this inner product as the sum ofindependent random variables": "To resolve this, we use the leave-one-out method. Let u(v)2be the second eigenvector of the leave-one-out Laplacian L(v) of A(v), where A(v) is chosen to agree with A everywhere except for the vthrow and vth column. The vth row and vth column of A(v) are replaced with those of A. Now, avdoes not depend on L(v) and therefore u(v)2 .",
  "p q u2 +av av, u(v)2 u2": "To bound the rightmost term of the RHS, we use Lemma 7 of [AFWZ20], reproduced inLemma A.20. In that, let w := u(v)2 u2. Let a =1np log (20n/) so that 2 exp(2anp) /(10n).Note that for the deterministic entries, we have av av = 1 1 = 0, so in Lemma A.20, we canset Xw Ber(0) for these entries. Now, by Lemma A.20, with probability 1 /n, we have",
  "At a high level, the proof plan is as follows": "1. We first establish a sufficient condition for a particular vertex to be classified correctly. Wecan think of this as simultaneously showing that the intermediate estimator (D 2I)1Au2is strongly consistent and that the corresponding noise term (D 2I)1A(u2 u2) is alower-order term in comparison to this. For a more formal way to see this, see Lemma A.23. 2. For the proof of Theorem 1, the main technical challenge in showing that the noise termabove is small amounts to analyzing the random quantity |av, u2 u2|. This is where wewill have to use the leave-one-out method to decouple the dependence between av and u2.The relevant lemmas for the leave-one-out analysis are Lemma A.21 and Lemma A.22.",
  "A.6.1A sufficient condition for exact recovery and proof": "The main result of this subsection is Lemma A.23, which gives a general condition under which aparticular vertex will be classified correctly. The proofs of Theorem 1 and Theorem 2 will followby invoking Lemma A.23. We remark that the point of this lemma is mostly conceptual; the cruxof the analysis lies in establishing that these conditions are satisfied our models.",
  "(D 2I)1 Au2": "At a high level, our goal is to show that this correctly classifies all the vertices with high probabilityand also is very close to u2 in norm with high probability. Deng, Ling, and Strohmer [DLS21]used this intermediate estimator to prove the strong consistency of unnormalized spectral bisectionfor SBM(n, p, q) instances.",
  "We are finally ready to prove Theorem 1. For convenience, we reproduce its statement here": "Theorem 1. Let p, p, q be probabilities such that q < p p and such that := p/(p q) is anarbitrary constant. Let D NSSBM(n, p, p, q). Let n N() where the function N() only dependson . There exists a universal constant C > 0 such that if",
  "For convenience, we reproduce the statement of Theorem 2 here": "Theorem 2. Let q be a probability and din be an integer, and let D DCM(n, din, q). For G D,let L denote the expectation of L after step (2) but before step (3) in Model 2. There exists constantsC1, C2, C3 > 0 such that for all n sufficiently large, if",
  "A.8Inconsistency of normalized spectral bisection": "In this section, we design a family of problem instances on which unnormalized spectral bisectionis strongly consistent whereas normalized spectral bisection is inconsistent. Specifically, our goal isto prove Theorem 3. Theorem 3. For all n sufficiently large, there exists a nonhomogeneous stochastic block model suchthat unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection (bothsymmetric and random-walk) incurs a misclassification rate of at least 24% with probability 11/n.",
  "A.8.1The nested block example": "We first state the family of instances on which we will prove our inconsistency results. Let n bea multiple of 4. Let L1 consist of indices 1, . . . , n/4, L2 consist of indices n/4 + 1, . . . , n/2, and Rconsist of indices n/2 + 1, . . . , n.",
  ": A is defined to have the above block structure": "We will draw our instances from the nonhomogeneous stochastic block model according to theprobabilities prescribed above. Note that within the two clusters L := L1 L2 and R, each edgeappears with probability at least p. Moreover, each edge in LR appears with probability exactly q.However, there are also two subcommunities L1 and L2 that appear within L. Furthermore, observethat unnormalized spectral bisection is consistent on this family of examples with probability 1 1/n by Theorem 1.",
  "p2(K + 3) + 4pq + 2q2": "Proof of Lemma A.27. As we can see from Lemma A.26, I L is a matrix whose rank is at most3, since it can be constructed by carefully repeating 3 distinct column vectors. Thus, it can haveat most 3 nonzero eigenvalues. In what follows, we consider the case where K > 1 so that thereare exactly 3 nonzero eigenvalues.",
  "We are now ready to prove the inconsistency of normalized spectral bisection on the nested blockexamples": "Proof of Theorem 3. Let G be a graph drawn from the nested block example. We choose p andq such that p log n/n and p/q = 2 where is some constant and such that p and q bothsatisfy the conditions of Theorem 1. Let K 3. Observe that the true communities are L and R.We will show that bisection based on u2 of I L (corresponding to the eigenvector associated withthe second smallest eigenvalue of L) will attain a large misclassification rate. In particular, basedon our calculation in Lemma A.27, we expect that u2 will output a bisection that places L1 and L2into separate clusters. On the other hand, by Theorem 1, for all n large enough, the unnormalizedspectral bisection algorithm will be strongly consistent.",
  "D1/2u2 = D1AD1/2u2 = D1A(D1/2u2),": "which shows that D1/2u2 must be the eigenvector of the random-walk normalized LaplacianID1A corresponding to eigenvalue 2. Since D is a positive diagonal matrix, it does not changethe signs of u2 and therefore the output of the normalized spectral bisection algorithm is the same. Our general approach to prove the inconsistency is to use the Davis-Kahan Theorem, a boundon L Lnlop, and a bound on the gap 2 3. Let dmin be the minimum degree of the graphgiven by adjacency matrix A and let dmin be the minimum weighted degree of the graph given bythe adjacency matrix A. First, using [DLS21, Theorem 3.1], we have with probability 1 nr forsome constant r 1 and constants C(r) and C (the latter of which does not depend on r), for alln sufficiently large,",
  "Cg(, K)np C4(r, , K)np": "Now, consider the subset of coordinates of u2 belonging to L1. Suppose m of these coordinates donot agree in sign with u2. To maximize m, each of these coordinates in u2 should be 0, so using thisreasoning and applying Lemma A.29 means the total 2 error can be bounded (using Lemma A.28)as",
  "B.1Varying edge probabilities in an NSSBM": "In , we investigated the behavior of an NSSBM model by fixing the values of p, q andvarying the largest edge probability p. Here, we take an alternative approach, and instead fix pand vary the values of p and q. : Agreement with the planted bisection of the bipartition obtained from unnormalizedspectral bisection, for graphs generated from a distribution in NSSBM(n, p, p, q) for fixed values ofn, p and varying values of p > q. The left plot uses p = 1/2, the right plot uses p = 1. The solidred curves plot the function pthr(q) (see (B.1)), and the dashed red curves plot the function pinfo(q)(see (B.2)). Setup.Let us fix n = 2000, p {1/2, 1}.For varying p, q in the range [1/n, 9/20] suchthat p > q, we sample t = 3 independent draws G from the same benchmark distribution Dp,p,qused in . For each of them, we compute the agreement of the bipartition obtained byunnormalized spectral bisection with respect to the planted bisection. For each (p, q), we plot theaverage agreement across the t independent draws. The results are shown in , where in theleft and right plot we ran the experiments with p = 1/2 and p = 1 respectively. The lower diagonalof these plots, where p q, is artificially set to 0. Theoretical framing.According to Theorem 1, fixing the value of p {1/2, 1}, we obtainthat unnormalized spectral bisection achieves exact recovery provided that for q [1/n, 9/20] onehas p pthr(q) where",
  "The dashed red curve in plots pinfo(q) as a function of q": "Empirical evidence.From , one can see that our experiments reflect the behaviorpredicted by Theorem 1 quite closely, although empirically we achieve 100% agreement slightlyabove pthr(q) (i.e. the solid red curve). However, this is likely due to the constant factors fromTheorem 1 that we ignored, and also n = 2000 is plausibly too small to show asymptotic behaviors.Nevertheless, we do achieve 100% agreement consistently as soon as we surpass the information-theoretic threshold pinfo(q): in the regime of our experiment, it appears that the unnormalized : Agreement with the planted bisection of the bipartition obtained from several matricesassociated with an input graph generated from a distribution DG1,G2q DCM(n, din, q) for fixedvalues of n, q and varying the size of the planted clique S. In the left plot, the bipartition is the0-cut of the second eigenvector, as in Algorithm 1. In the right plot, the bipartition is the sweepcut of the first n/2 vertices in the second eigenvector.",
  "B.2Varying the size of a planted clique in a DCM": "In some sense, the experiments from and Appendix B.1 can be thought of as experimentsfor the deterministic clusters model too. This is because each realization of the internal edges givesrise to a different DCM distribution (see ). We complement our previous discussion byillustrating the behavior of certain families of DCM distributions that are conceptually differentthan those considered in . Benchmark distribution.Let n be divisible by 4 and let {P1, P 2} be a partitioning ofV = [n] into two equally-sized subsets. Fix p . For some set S P1 such that S = {1, . . . , |S|}(for simplicity), let G2 = (P2, E2) ER(n/2, p) be a graph drawn from the Erds-Rnyi distributionwith sampling rate p, and let G1 = (P1, E1) ERPC(n/2, p, S) be also a graph drawn from theErds-Rnyi distribution with sampling rate p where we additionally plant a clique on the verticesS. Fixing G1, G2, for q we consider the distribution DG1,G2qover graphs G = (V, E) whereG[P1] = G1, G[P2] = G2, and every edge (u, v) P1P2 is sampled independently with probabilityq. One can see that DG1,G2qis in fact in the set DCM(n, din, q) for some din. Setup.Let us fix n = 2000, p = 9/n, q = 1/n. For varying values of |S| in the range[|P1|/10, |P1|], we sample G1 = (P1, E1) ERPC(n/2, p, S) and G2 = (P2, E2) ER(n/2, p), andthen draw t = 10 independent samples G from DG1,G2q.For each sample G, we run spectralbisection (i.e. Algorithm 1) with matrices L, Lsym, Lrw, A. Then, we compute the agreement of thebipartition hence obtained with respect to the planted bisection, and average it out across the tindependent draws. The results are shown in the left plot of . Again, another natural wayto get a bipartition of V from the eigenvector is a sweep cut, and the average agreements that thisresults in are shown in the right plot of .",
  "Theoreticalframing.Ignoringtheconstants,Theorem2guaranteesthatexactrecoveryisachievedbyunnormalizedspectralbisectionaslongasdin nq + nand": ": The minimum in-cluster degree din and the spectral gap 3(L) 2(L) of distributionsDG1,G2q DCM(n, din, q) with fixed values of n, q and varying the size of the planted clique S. Thered horizontal line on the left corresponds to the value nq +n, the red horizontal line on the rightcorresponds to the value n + nq + nq log n + log n. 3(L) 2(L) n + nq + nq log n + log n, whereL is the expected Laplacian of DG1,G2q.For each clique size that we consider, shows the minimum in-cluster degree of thegraphs G1, G2 that we draw (in the left plot), and the spectral gap 3(L) 2(L).The redhorizontal lines in the left and right plot respectively correspond to the value of nq + n andn + nq + nq log n + log n on the y-axis, indicating the lower bound on din and 3(L) 2(L)demanded by Theorem 2. Empirical evidence: consistency.From , one can see that all the distributions DG1,G2qthat we use roughly meet the requirement of Theorem 2. Indeed, in the left plot of one seesthat unnormalized spectral bisection consistently achieves exact recovery for all clique sizes. Onthe contrary, the bipartition obtained by running spectral bisection with the adjacency matrix Amisclassifies a fraction of the vertices for certain sizes of the planted clique. Nevertheless, the sweepcut obtained from all the matrices recovers the planted bisection exactly. Empirical evidence: example embedding.Let us fix the value |S| = 800 for the sizeof the planted clique, for which we see in that the adjacency matrix fails to recover theplanted bisection. We generate a graph from a distribution DG1,G2qwith clique size |S| = 800, andplot how the vertices are embedded in the real line by the second eigenvector of all the matriceswe consider. The result is shown in , where the three horizontal dashed lines, from top tobottom, respectively correspond to the value of 1/n, 0, 1/n on the y-axis. Graphically, onecan see that the embedding in the unnormalized Laplacian is indeed the one that moves the leastaway from the values 1/n, and in fact the vertices {1, . . . , 800} P1 where we plant the cliqueconcentrate even more around 1/n. This is a phenomenon related to the one illustrated by .Finally, one can see from the embedding that splitting vertices around 0 does result in misclassifyinga fraction of the vertices for the adjacency matrix. However, taking a sweep cut that splits thevertices into two equally sized parts recovers the planted bisection for all matrices. This reflectsthe results shown in . : Embedding of the vertices given by the second eigenvector u2 of several matrices as-sociated with a graph sampled from a distribution DG1,G2q DCM(n, din, q), with the size of theplanted clique set to |S| = 2/5 n. Horizontal dashed lines, from top to bottom, correspond to1/n, 0, 1/n respectively."
}