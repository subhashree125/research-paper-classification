{
  "Abstract": "Existing research has made impressive strides in reconstructing human facialshapes and textures from images with well-illuminated faces and minimal externalocclusions. Nevertheless, it remains challenging to recover accurate facial texturesfrom scenarios with complicated illumination affected by external occlusions, e.g.a face that is partially obscured by items such as a hat. Existing works based onthe assumption of single and uniform illumination cannot correctly process thesedata. In this work, we introduce a novel approach to model 3D facial texturesunder such unnatural illumination. Instead of assuming single illumination, ourframework learns to imitate the unnatural illumination as a composition of multipleseparate light conditions combined with learned neural representations, namedLight Decoupling. According to experiments on both single images and videosequences, we demonstrate the effectiveness of our approach in modeling facialtextures under challenging illumination affected by occlusions. Please check for our videos and codes.",
  "Introduction": "Recently, 3D face reconstruction has made significant progress with the rapiddevelopment of digital human and meta-universe technologies. These techniques have becomeincreasingly proficient in recovering details of face shapes and textures from single images or videosequences. Their performances have been particularly commendable on well-illuminated faces. Despite the advancements, the real world presents more complex scenarios. As shown in the blue andred rectangled regions of input images in , self occlusions from facial parts such as the nose,or external occlusions such as hats or hairs introduce illumination changes and produce shadowsin certain regions. Although recent works replace the linear models withnon-linear GAN-inversion-based textured models to greatly enhance the quality of textures andcomplete occluded facial areas, they still rely on the assumption that the environment illuminationis single and uniform. As the illumination affected by self and external occlusions is unnatural, itmay deviate drastically from the single and uniform illumination assumption, which would bringunexpected influence to the textures modelled with existing methods. As shown in -a, methods based on the diffuse-only texture and mesh render bake both shadows caused by self occlusion of the nose and external occlusion of the hat onto thetexture, where the re-rendered output also appears unrealistic and lacks authenticity. Recent worksproposed by Dib et al. combine local reflectance texture model incorporating diffuse,specular, and roughness albedos with ray-tracing render to implicitly model influence of the selfocclusion. As shown in -b, these methods can create more realistic rendered output and avoidthe shadows of self occlusion on the recovered texture in the blue rectangle, while the influence fromexternal occlusion in the red rectangle remains unresolved.",
  "arXiv:2412.08524v1 [cs.CV] 11 Dec 2024": ": Blue and red rectangles mark regions affected by self and external occlusions, respectively.(a) Texture modeling with diffuse-only texture map. (b) Texture modeling based on diffuse, specular,and roughness albedos from local reflectance model , while optimizing with ray-tracing render.(c) Our method learns neural representations to decouple the original illumination into multiple lightconditions, where the influence from external occlusions can be modeled as one of the conditions.White and black regions in the masks denote 1 and 0, respectively. To mitigate the above-mentioned issues, we propose a face texture modeling framework for facesunder complicated and unnatural illumination affected by external occlusions. Given the limitedphysical information about external occlusions presented in a single facial image or video sequence,accurately modeling how occlusion impacts illumination becomes nearly unattainable. Regardingthis challenge, we propose to use multiple separate light conditions to imitate the local illuminationof different facial areas after being affected by external occlusions. As shown in -c, our methodcan model the influence from external occlusion as one of the light conditions and eliminate its effecton the recovered texture as highlighted in the red rectangle, where the rendered output is also morerealistic and closer to the input image especially on the forehead and eyes. Specifically, we use aspatial-temporal neural representation to predict masks for different light conditions, which are usedto combine rendered results under multiple light conditions into final output. The number of lightmasks is adpatively adjusted during optimization. As external occlusions may directly cover parts ofthe face, we also introduce another neural representation to provide a continuous prediction for theavailable face region. Furthermore, our approach incorporates realistic constraints by introducingpriors from the statistical model and pre-trained face perceptual models to ensure our extractedtextures construct lifelike human faces.",
  "The 3D Morphable Model (3DMM) is a widely employed linear statistical frameworkused for modeling the geometry and texture of faces. It is constructed based on aligned face images": "using Principal Component Analysis (PCA). Within the 3DMM framework, the coordinates and colorsof facial mesh vertices are computed through a linear combination of numerical parameters. Theseparameters can be estimated to enable face reconstruction through optimization-based fitting orlearning-based approaches , with the goal of minimizing the disparities between therendered and original face images. Traditional 3DMMs, although effective, are limited by their linearPCA basis. Recent advancements have introduced non-linear basis by incorporatingpre-trained mesh or texture decoder networks. In these methods, latent codes for these decoders areoptimized to align with the original face images. This approach significantly enhances the robustnessof the model when faced with issues such as occlusions and missing textures. However, such methodsdemand access to large, well-preprocessed datasets for effective pre-training.",
  "Face Texture and Illumination Modeling": "Texture and illumination modeling plays a pivotal role in 3D face reconstruction, directly impactingthe color rendering in generated images. Accurate modeling for 3D face textures and environmentalillumination is vital for subsequent applications such as face relighting or animation . To achieve high-resolution rendering, recent approaches usually opt for UVmapping to model facial texture, as opposed to the earlier methods that focused on vertex colors .The UV map can either be initialized with a PCA basis or estimated through the use of apre-trained non-linear texture decoder . There are two commonly used models to estimatehuman skin reflectance for the texture modeling framework: the Lambertian model and the Blinn-Phong model . The Lambertian model is more computationally efficient, while the Blinn-Phongmodel produces more realistic rendering results by accounting for specular attributes in textures. As capturing environmental illumination directly can be challenging, most existing methods assume itas primarily uniform Spherical Harmonics . This approach, however, tends to bake\" shadowscast by facial characters or external occlusions into the textures. Although method introduceimplicit texture modeling to achieve better face rendering performances, it requires multi-view imagesunder uniform illumination for optimization, which is not appropriate for face images affected byocclusions. 2D shadow removal methods try to eliminate the influence of shadows directlywith networks. These methods have higher inference efficiency, while the size of their training setmay greatly affect their performances. Notably, Dib et al. have introduced techniquesthat implicitly model self-shadows through ray-tracing, demonstrating exceptional performance onfaces with severe self occlusions. Despite these advancements, existing methods usually encounterchallenges in handling shadows caused by external occlusions such as hats or hair, tending to take theshadows as part of textures. To address these issues, we propose a novel framework to recover clear textures under challengingillumination affected by external occlusions. Our approach achieves this by decoupling the unnaturalaffected illumination into multiple light conditions using learned neural representations.",
  "Problem Definition": "This work focus on the texture modeling problem of human faces under challenging environmentillumination affected by external occlusions. Given an input image or video sequences Iin taken fromaffected illuminations, our task is to recover clear and accurate texture T from Iin and ensure that Tcan synthesize results Iout close to Iin.",
  "Overall Illustration": "As illustrated in , instead of making the assumption that the face is exposed to a single anduniform illumination, we propose to model n possible separate light conditions 1 n in theenvironment, where the masks for possible regions MN are predicted with a neural representationf(). Effective masks ML and rendered faces IRs are selected from original MN and rendered IRnduring optimization with the Adaptive Condition Estimation (ACE) strategy. The final rendered faceunder multiple light conditions is IR = IRs ML, which is merged with the input images toconstruct the output Iout = IR Mo + Iin (1 Mo). The face shape is modeled with statistical : Illustration of our framework. The pipeline is proposed to recover texture T and 3DMMstatistical coefficients , , , p, from the input image Iin. The statistical coefficient is used toinitialized T. Render mask MR and Faces IRn under n light conditions = {1 n} are acquiredthrough ray-tracing rendering. f() and g() are neural representations predicting light masks MNand facial region mask Mo. ACE is introduced to select effective masks ML and rendered facesIRs. IRs are combined into IR with ML, where IR is merged with surroundings in Iin with Mo toconstruct output image Iout. Lpho and Llan are photometric loss and landmark loss, respectively.",
  "Light Decoupling": "Light Condition Initialization.Following using B = 9 bands Spherical Harmonics (SH)and ray-tracing rendering to model the illumination under self occlusions, we use n separate SH tomodel n possible light conditions. The coefficients are then simply initialized as i = 2 i nI 1for i = {1, .., n}, where I is a all-one matrix with the same shape as the SH coefficient.Please note that we use each SH to imitate the local illumination after being affected by the externalocclusion, instead of the global natural illumination. This means that we do not need to considerphysical influence of occlusion in each SH. Instead, we optimize each SH independently to directlyimitate the illumination in different face regions.Neural Representations for Face Segment.To decouple the illumination into multiple lightconditions, we design a pair of spatial-temporal continuous neural representations to segment theface into regions for different light conditions. As illustrated in , spatial positions x, y andtemporal position t of pixels are normalized and embedded into a coordinates system. t is decided bythe number of frames in the input image/video sequence. For the single image reconstruction, t isset as a constant 0, where it would be i/k for the ith frame of a k frames video sequences. For theconvenience of learning, (x, y) are normalized into [1.0, 1.0] and t is normalized into [0.0, 1.0]. A Multi-Layer Perceptron (MLP) f() is then introduced to predict the probability of assignmentto each light condition. Given the render mask MR in the ray-tracing-assisted rendering, the lightmasks can be obtained as MN = MR f(x, y, t). Effective masks ML are selected from MN withAdaptive Condition Estimation (ACE) to provide the final segment for different light conditions. Similarly, we use another MLP g() to predict the probability that each pixel belongs to the faceregions to avoid the influence from direct occlusion such as hat or hair. The Face mask is thus givenby: Mo = MR g(x, y, t). A pre-trained semantic segmentation network is introduced fordistillation to g(), while the probability for labels of all face components such as eyes, mouth, ornose are added together to construct a classifier h() to predict the association of a pixel at x, y, t tothe face. The distillation loss Lseg can be simply defined as:",
  "|Iin|g(x, y, t) h(x, y, t)2.(3)": "Note that g() is co-optimized together with the Lpho and Lseg to further distinguish some occlusionshard to be fitted with the 3DMM statistical model.Adaptive Condition Estimation (ACE).As the complexity of illumination is mutative in differentsurroundings, we design a strategy to estimate the number of light conditions existing in thisenvironment during optimization. Specifically, light masks with larger area than a pre-definedthreshold in MN are preserved in ML, while smaller ones are dropped with corresponding lightconditions and not further optimized in later iterations. Given the number of light masks MN same aslight condition n, then MN = {M iN}ni=1, ML = {M iL}nLi=1 = {M iN|1",
  "where m M iL denotes each pixel value in ith light mask ML. nL is the number of masks in ML": "Note that Larea is different from Lbin as it pushes the mask values M iN far from the mean of allmasks, while Lbin pushes mask values away from mean of different positions in the same mask M iL.ACE is executed at a specific iteration iter0 to select ML from MN. We use Larea to encourage areaconcentration before iter0, while using Lbin to push the mask to 0 or 1 after iter0.",
  "Realistic Constraints": "To ensure the reconstructed texture is reasonable and lifelike, we propose global prior constraintLGP , local prior constraint LLP , and human prior constraint LHP by introducing both priors from3DMM statistical model and the pre-trained perceptual model.Global Prior Constraint.The global prior constraint is used to ensure the consistency of overallhue between optimized texture T and initialized texture T 0 from the 3DMM statistical modelcalculated with . As the colors mainly come from diffuse albedo in T, we estimate the overall hueof T 0 with a K-means algorithm on its diffuse albedo T 0D and get a 4 4 color matrices C. Giventhe diffuse albedo of Texture T as TD, this term is given by:",
  "|Iin|(ND N 0D + NS N 0S + NR N 0R).(7)": "Human Prior Constraint.Human prior constraint is introduced to enhance the optimization oftexture T in Stage 3 with a face recognition network FaceNet pre-trained on large dataset suchas VGGFace2 or Casia-Webface . The recognition model pre-trained on VGGFace2 andCasia-webface tends to classify the input images to 8,631 and 10,575 identities with different genders,ethnicity, etc. In this work, we propose a human prior constraint by maximizing the probability thatthe rendered face is recognized on one of the identities by FaceNet. Defining the FaceNet as fr(),the human prior constraint can then be written as:",
  "Training Pipeline": "The pipeline of the entire training process of our proposed framework is presented in Alg. 1. Asillustrated in , the training of our proposed method consists of 3 stages, which is similar toNextFace . In Stage 1, the expression , shape , and pose p coefficients are optimized toconstruct the basic face shape. In Stage 2, all coefficients, and the neural representations f() andg() are optimized together to reconstruct the face with statistical model. In Stage 3, the texture T isdirectly optimized without coefficient , while and f() are optimized with small learning rates.0 7 are pre-defined weights. We use Adam optimizer for optimization.",
  "Source": ": Comparison results on the video sequences from Voxceleb2. Ours and Ours+ denote ourrendered results IR directly overlapped onto original images, and results combined with environments:Iout = Mo IR + (1 Mo) Iin, respectively. The symbols are defined following Sec. 3. the same, while the results optimized without ACE contain multiple redundant and inaccurate lightconditions in ML. It confirms that ACE can help remove these unnecessary light conditions and keepeffective ones. With ACE, our method can decouple the original illumination affected by occlusionsinto light conditions more consistent with actual observations of input images.",
  "Dataset and Implementation Details": "We follow NextFace for the ray-tracing rendering and multi-stage organization of optimizationpipeline. Detailed hyper-parameter settings can be found in our supplementary. For our experiments,we utilize two datasets: Voxceleb2 and CelebAMask-HQ . VoxCeleb2 is a diversedataset encompassing numerous videos collected from interviews, movies and videos, where thesame person may have multiple separate videos. CelebAMask-HQ is a large scale face imagedataset with fine attributes annotation and high resolution, widely used in face editing and generation.Evaluation.We construct a collection of evaluation data with challenging illumination affectedby external occlusions for comparative analysis. This collection includes 38 pairs of single images,24 pairs of videos from Voxceleb2 with 256 256 resolution, and 62 single images fromCelebAMask-HQ with 512 512 resolution. Each pair consists of source images affected byexternal occlusions and target images without occlusion, both from the same person, where eachvideo is sampled to 8 frames for sequence-based comparisons. To quantitatively assess the quality of recovered textures, face textures extracted from the occludedsource images are leveraged to synthesize the unoccluded target images. Specifically, we optimizesource and target images separately following Sec. 3. Then, keeping the face shape, pose, andillumination invariant, we replace the target images texture with the sources and re-render it to thesynthetic result. This allows us to measure texture quality by quantifying the differences betweenthe synthesized target images and the original target images. We also introduce differences betweenoriginal and reconstructed source images within facial regions acquired by face parsing as anassistant metric to see if textures restore source images well. Our quantitative comparison employs three metrics: PSNR, SSIM, and perceptual error LPIPS of AlexNet . State-of-the-art methods D3DFR , CPEM , NextFace , and RGBFitting methods proposed in FFHQ-UV are introduced for comparison. For D3DFR, we adopt itsenhanced PyTorch version. As NextFace does not distinguish the face and external occlusionwhen optimizing the texture, we introduce NextFace* in subsequent comparison by adding faceparsing to select the face region during optimization.",
  "Ours": ": Comparison results on the CelebAMask-HQ dataset. Ours and Ours+ denote our renderedresults IR directly overlapped onto original images, and results combined with environments: Iout =Mo IR + (1 Mo) Iin, respectively. : Quantitative comparison on sin-gle images from Voxceleb2.Source andTarget denote differences evaluation on re-constructed source images and synthetic tar-get images. LPIPS is multiplied with 102.Underline and bold mark the suboptimal andoptimal results, respectively.",
  "Evaluation on Voxceleb2": "Evaluation on Single Images.We conduct an evaluation about the performance of our method incollected single images sourced from the Voxceleb2 dataset . As shown in , our approachdemonstrates a marked improvement in recovering clearer textures from the original images takenunder challenging illumination affected by external occlusions. Our method also surprisingly recoversclear textures under strong colorful lights, which may benefit from the realistic constraints to keeptextures reasonable. In contrast, existing methods tend to incorporate these occlusions, shadows, orcolorful lights directly into the texture, resulting in less satisfactory outcomes. Furthermore, our methods achieves superior results on the synthesis of target images and comparableperformances on the reconstruction of source images as shown in . It confirms our methodconsistently recovers both accurate and clear textures. Although NextFace performs a little betteron PSNR and SSIM of source images, it performs the worst on synthetic target images as it actuallyseverely over-fits source images. As shown in , it bakes shadows and occlusions to the textures,which confirms it is not appropriate for faces affected by external occlusions.",
  "PSNR 25.1927.5229.22SSIM 0.870.890.91LPIPS 9.167.756.36": "to video sequences, we share texture, illumination and shape coefficients across all frames duringoptimization. As D3DFR and FFHQ-UV do not provide support for sequences, we recur-rently apply them to each single image. Quantitative comparison in demonstrates that ourmethod constantly performs superior to existing methods on texture modeling from continuous videosequences. Please check our Supplementary for corresponding qualitative results.",
  "Evaluation on CelebAMask-HQ": "We extend our comparisons on CelebAMask-HQ . As this dataset lacks multiple images fromthe same identity, we focus on evaluating the performance based on the recovered textures and thereconstructed results of source images. The outcomes of our assessment are visually depicted in. We observe that our method continues to perform well in the task of recovering clear texturesfrom challenging input. Additionally, our reconstructed results exhibit a higher degree of realism onthe reconstructed results when compared to the outcomes produced by other methods.",
  "Evaluation on Images with Diverse Shadows": "To validate the performances of our method more sufficiently, we further evaluate our methodon the single image dataset proposed by , which includes 100 images affected by manuallycreated external shadows, as well as corresponding ground truths. The quantitative and qualitativecomparisons are presented in and . We can observe that our method still outperformsother methods under faces with diverse shadows.",
  "Ablation Study": "Ablation Study on Losses.In this section, we analyze the impact of the proposed losses: LGP ,LLP , and LHP by comparing the rendered results under relative brighter light conditions. Asillustrated in , it becomes evident that the inclusion of the LGP loss plays a significant role ineliminating the abnormal colors on the texture, where LLP loss effectively remove large artifactsobviously different from human faces. LHP can further reduce slight unreasonable defects withpriors from the pre-trained recognition model . Quantitative comparisons in also confirmthat each proposed loss contributes to the final performances. Ablation Study on Neural Representations.To ascertain the significance of the neural repre-sentations employed, we perform an analysis where we eliminate both the neural representations:f() and g() from the light decoupling pipeline. As depicted in , the neural representation forthe decoupling of light conditions f() notably contributes in the removal of shadows artifacts fromthe results. Furthermore, the neural representation g() displays its efficacy in minimizing existingirregularities in the synthetic results, which achieves smoother and cleaner results. We also presentcorresponding quantitative comparisons in , which demonstrates that both f() and g() haveobvious influence on the final performances.",
  "Conclusion": "In this paper, we present a novel framework dedicated to recover clear facial textures from imagestaken under challenging illumination affected by external occlusion. Our approach makes use ofneural representations to decouple the original illumination into multiple separate light conditionsacross various facial regions. Then the affected complicated illumination can be modelled with thecombination of different light conditions. Furthermore, we introduce well-established human facepriors through the realistic constraints to enhance the realism of our results. According to experimentson single images and video sequences, our method consistently surpasses existing techniques torecover clearer and more accurate textures from faces under affected unnatural illumination.",
  "Edward Angel. nteractive computer graphics: a top-down approach with shader-based opengl. 2011": "Haoran Bai, Di Kang, Haoxian Zhang, Jinshan Pan, and Linchao Bao. Ffhq-uv: Normalized facialuv-texture dataset for 3d face reconstruction. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 362371, 2023. Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang, Sheng Wang, Xuefei Zhe, Di Kang, HaozhiHuang, Xinwei Jiang, Jue Wang, et al. High-fidelity 3d digital human head creation from rgb-d selfies.ACM Transactions on Graphics (TOG), 41(1):121, 2021.",
  "James F Blinn. Models of light reflection for computer synthesized pictures. In Proceedings of the 4thannual conference on Computer graphics and interactive techniques, pages 192198, 1977": "James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan Ponniah, and David Dunaway. A 3d morphablemodel learnt from 10,000 faces. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 55435552, 2016. Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset forrecognising faces across pose and age. In 2018 13th IEEE international conference on automatic face &gesture recognition (FG 2018), pages 6774. IEEE, 2018.",
  "Wei-Chieh Chung, Jian-Kai Zhu, I-Chao Shen, Yu-Ting Wu, and Yung-Yu Chuang. Stylefaceuv: A 3d faceuv map generator for view-consistent face image synthesis. 2022": "Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruc-tion with weakly-supervised learning: From single image to image set. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition workshops, pages 00, 2019. Abdallah Dib, Junghyun Ahn, Cedric Thebault, Philippe-Henri Gosselin, and Louis Chevallier. S2f2:Self-supervised high fidelity face reconstruction from monocular image. arXiv preprint arXiv:2203.07732,2022. Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, Cdric Thbault, Philippe Gosselin, Marco Romeo, andLouis Chevallier. Practical face reconstruction via differentiable ray tracing. In Computer Graphics Forum,volume 40, pages 153164. Wiley Online Library, 2021. Abdallah Dib, Cedric Thebault, Junghyun Ahn, Philippe-Henri Gosselin, Christian Theobalt, and LouisChevallier. Towards high fidelity monocular face reconstruction with rich reflectance using self-supervisedlearning and ray tracing. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 1281912829, 2021. Bernhard Egger, Sandro Schnborn, Andreas Schneider, Adam Kortylewski, Andreas Morel-Forster,Clemens Blumer, and Thomas Vetter. Occlusion-aware 3d morphable models and an illumination prior forface image analysis. International Journal of Computer Vision, 126:12691287, 2018. Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler,Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al. 3d morphable face modelspast,present, and future. ACM Transactions on Graphics (ToG), 39(5):138, 2020. Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. Ganfit: Generative adversarialnetwork fitting for high fidelity 3d face reconstruction. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 11551164, 2019. Yingqing He, Yazhou Xing, Tianjia Zhang, and Qifeng Chen. Unsupervised portrait shadow removalvia generative priors. In Proceedings of the 29th ACM International Conference on Multimedia, pages236244, 2021. Andrew Hou, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Face relighting with geometri-cally consistent shadows. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 42174226, 2022.",
  "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutionalneural networks. Advances in neural information processing systems, 25, 2012": "Alexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos Ploumpis, Vasileios Triantafyllou,Abhijeet Ghosh, and Stefanos Zafeiriou. Avatarme: Realistically renderable 3d facial reconstruction\"in-the-wild\". In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages 760769, 2020. Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, and StefanosZafeiriou. Fitme: Deep photorealistic 3d morphable model avatars. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 86298640, 2023. Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Abhijeet Ghosh, and StefanosZafeiriou. Avatarme++: Facial shape and brdf inference with photorealistic rendering-aware gans. IEEETransactions on Pattern Analysis and Machine Intelligence, 44(12):92699284, 2021.",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba) dataset.Retrieved August, 15(2018):11, 2018": "Langyuan Mo, Haokun Li, Chaoyang Zou, Yubing Zhang, Ming Yang, Yihong Yang, and Mingkui Tan.Towards accurate facial motion retargeting with identity-consistent and expression-exclusive constraints.In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 19811989, 2022. Thomas Nestmeyer, Jean-Franois Lalonde, Iain Matthews, and Andreas Lehrmann. Learning physics-guided face relighting under directional light. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 51245133, 2020. Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model forpose and illumination invariant face recognition. In 2009 sixth IEEE international conference on advancedvideo and signal based surveillance, pages 296301. Ieee, 2009. Ravi Ramamoorthi and Pat Hanrahan. An efficient representation for irradiance environment maps. InProceedings of the 28th annual conference on Computer graphics and interactive techniques, pages497500, 2001. Ravi Ramamoorthi and Pat Hanrahan. A signal-processing framework for inverse rendering. In Proceedingsof the 28th annual conference on Computer graphics and interactive techniques, pages 117128, 2001. Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for facerecognition and clustering. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 815823, 2015. Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo, and David W Jacobs. Sfsnet: Learning shape,reflectance and illuminance of facesin the wild. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 62966305, 2018. William AP Smith, Alassane Seck, Hannah Dee, Bernard Tiddeman, Joshua B Tenenbaum, and BernhardEgger. A morphable face albedo model. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 50115020, 2020. Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li,Zhi-Qi Cheng, Ying Tai, et al. Facechain-imagineid: Freely crafting high-fidelity diverse talking facesfrom disentangled audio. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 12921302, 2024.",
  "Xuaner Zhang, Jonathan T Barron, Yun-Ta Tsai, Rohit Pandey, Xiuming Zhang, Ren Ng, and David EJacobs. Portrait shadow manipulation. ACM Transactions on Graphics (TOG), 39(4):781, 2020": "Zhenyu Zhang, Renwang Chen, Weijian Cao, Ying Tai, and Chengjie Wang. Learning neural proto-facefield for disentangled 3d face modeling in the wild. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages 382393, June 2023. Zhenyu Zhang, Yanhao Ge, Ying Tai, Xiaoming Huang, Chengjie Wang, Hao Tang, Dongjin Huang,and Zhifeng Xie. Learning to restore 3d face from in-the-wild degraded images. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 42374247, June2022. Mingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen. Imface: A nonlinear 3d morphable facemodel with implicit neural representations. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 2034320352, 2022. Mingwu Zheng, Haiyu Zhang, Hongyu Yang, and Di Huang. Neuface: Realistic 3d neural face renderingfrom multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1686816877, 2023.",
  "A.1Limitation": "We use AlbedoMM to initialize the face texture and optimize neural representations. Limited bythe capacity of AlbedoMM, the initialized face texture may not be accurate enough for subsequentoptimization in some occasions. We can explore to resolve this problem by combining our methodwith non-linear texture modeling methods such as in our future work. Furthermore, theoptimization of multiple light conditions makes our method slower than other single illuminationmethods . It costs 340s for a 256 256 image on 2080ti, which is slower than 240s ofFFHQ-UV but still affordable.",
  "A.2Details of Parameter Settings": "In , we present details about the hyper-parameters mentioned in Sec. 3. Although it seemsthat there are multiple parameters, the setting is robust for both images and video sequences fromadopted datasets. We conduct all experiments on a Nvidia 2080ti GPU with a 2.9Ghz i5-9400 CPU.It costs 340s for a 256 256 image.",
  "A.3Comparisons against the combination of 2D Shadow Removal and 3D Texture Modeling": "Although the former mentioned existing 3D face texture modeling methods cannot process facialshadows from external occlusions directly, there are some 2D shadow removal methods trying to eliminate the shadows directly from the images. Therefore, another alternative simplebaseline is to pre-process the image with 2D shadow-removal networks before texture modeling.In this section, we introduce the most recent method with a pre-trained model to pre-processthe images before feeding them to compared methods mentioned in Sec. 4.1. The quantitative andqualitative comparison are presented in , and , respectively.",
  "Pre-processing with 2D shadow removal methods indeed improves the performances of baselines,while our method still outperforms them. From qualitative results, we observe that the shadow": ": Discussion about the effect of g(). w2 is the weight to constrain g(), defined in Alg. 1.The red and black rectangles mark shadow-affected regions and detailed textures, respectively. g()will weaken both shadows and details from textures when reducing w2 to loose its constraint. : Ablation study for Larea and Lbin in ACE. NA denotes removing both Larea and Lbin.Larea can remove redundant light conditions as shown by the blue rectangle, while Lbin ensures thelight condition shown in the red rectangle region is consistent as our observation of the input. MLand IRs are predicted masks and rendered faces defined in , respectively. removal model cannot fully remove the external shadows in cases where the external shadows coverrelatively large regions. Although modifying the shadow removal model to be more powerful mayfurther improve performances, it goes beyond the range of this work. We can explore it in the future.",
  "A.4Comparisons with Deocclusion methods": "Besides the former mentioned shadow removal baselines, learning-based deocclusion methods can remove external occlusions from faces by predicting the occluded regions. Such operationsalso have the potential to deal with external shadows by directly treating the shadow regions asocclusions. In this section, we conduct a brief comparison with the most recent open sourceddeocclusion method . The quanlitative and qualitative results are presented in and, respectively. The metrics are evaluated on the target images mentioned in Sec. 4.1. Wecan observe that the method lose some facial details such as beards. The reason is that thedeocclusion method actually divides occlusion regions by the distances between input images",
  "A.5Discussion about the number of Sphere Harmonics (SH) bands": "In this work, we follow NextFace to use 9-bands SH to model the local illumination, which canactually capture quite fine details. However, SH with more bands would have stronger ability for themodeling of illumination. To verify if the external shadows can be directly modelled with more bandsSH, we also provide the quantitative results of our method modelled with only one single global SHin 9, 12, 15, 18 bands in , where we remove our light decoupling framework. We observethat increasing the number of bands in a single global SH yields quite limited improvements. Apossible reason is that the external occluded shadows on human faces represent drastic illuminationchanges in relatively small areas, which may not be appropriately modeled as a single global SHduring optimization.",
  "A.6Discussion about the number of lighting conditions n": "In this work, we initialize n different lighting conditions to imitate the illumination affected byexternal occlusion at the beginning, where some of them would be removed by ACE in subsequentprocessing. To explore the influence of different numbers of n, we conduct a ablation study for thenumber of n used for initialization of lighting conditions in . Single images from VoxCeleb2are used for evaluation. We observe that n = 3 produces sub-optimal results, likely because thenumber of lighting condition candidates is insufficient to model images with complex illuminations.In contrast, n = 5, 7, 9 yield good and similar results as there are enough initial lighting conditions,and any redundant ones are removed. The result for n = 5 is slightly better. While introducing largern and further adjusting the hyper-parameters might improve performance, it would also increase theoptimization burden.",
  ": Qualitative Comparisons with the Deocclusion method": "We cannot use perceptual loss here because we do not have **corresponding ground truth imagesunder the multiple decoupled lighting conditions**, where HP does not need such ground truths.Nonetheless, the perceptual loss can be indeed applied between the final rendered result Iout andinput image Iin. We present a comparison between such perceptual loss implementation and the HPin . We can see that HP still has better performances, which can confirm it provides moreeffective constraints for the textures through rendered faces under multiple lighting conditions.",
  "A.8Discussion about g()": "Both f() and g() have impacts to remove artifacts on the recovered textures. To confirm that theyhave principled differences, we implement a discussion about the effect of g(). As presented inthe third column of , g() cannot fully avoid the influence of external shadows on the faces.As presented in Sec. 3.3, Lseg is introduced to constrain g() to ensure that g() predicts relativelyaccurate face regions. Lseg is weighted by w2 in optimization as defined in Alg. 3. Although theshadow effect can decrease when we reduce w2 to allow g() to filter out more face regions, thedetailed textures are equally weaken and may be fully removed as shown in the last row of . Itconfirms that f() is still essential for this framework.",
  "A.9Analysis about Lbin and Larea in ACE": "In ACE mentioned in Sec 3.3, two regularization constraints Larea and Lbin are introduced. As theblue rectangle input face of is not affected by any external occlusion, its illumination can bemodelled with only one light condition. Larea helps remove redundant light conditions. However,only using Larea may create decoupled light conditions obviously different from the observationof input image, as illustrated in the red rectangle regions of . Adding Lbin can ensure the",
  "A.11Visualization about Mo": "We present an ablation study to confirm the effect of g() against direct segmented mask fromface parsing to predict Mo. The circled parts of show that the parsed masks may beinappropriate due to the limitation of generalizability. We can see in the first row of that therendered result IR may have rough edges and artifact colors from the occlusion, while our methodcan avoid this problem by refining the parsed mask with g(). Moreover, some parts may be missingon the parsed mask. As shown in the second and third rows of , artifacts show up in theseunconstrained regions. Our method can complete these missing regions for more reasonable results.",
  "A.12Discussion about the failure cases": "Except the efficiency problem mentioned in Sec. A.1, our primary limitation is the initialization withAlbedoMM. As shown in , for faces with many high-frequency details, such as wrinkles, ourmethod may lose these details during reconstruction. Replacing AlbedoMM with more powerful facerepresentations could address this issue. We plan to explore this further in future work.",
  "A.13Effect of Adaptive Condition Estimation": "As described in Sec. 3.3, the Adaptive Condition Estimation (ACE) is proposed to select effectiveML and IRs from the initialized MN and IRn. To remove ACE, we use initialized MN and IRnas ML and IRs directly. As shown in , we can see that the rendered results Iout are almost",
  "A.14More Visualized Results": "In this section, we present three representative examples from each sequence in . It is evidentthat our method performs the best in generating synthetic results close to the target sequences,exhibiting a high degree of realism. In contrast, other methods still produce less convincing outcomesdue to negative effects from external occlusions. More results on images sourced from Voxceleb2 and CelebAMask-HQ are presented in and . Our method still performs better.Please refer to the attached video for more results on sequences from Voxceleb2 . D3DFRFFHQ-UVSourceCPEMNextFace NextFace*OursTarget : More results on single images from Voxceleb2 . We can see that our method cansynthesize more accurate target images based on textures from the source images, which validate thequality of our acquired textures."
}