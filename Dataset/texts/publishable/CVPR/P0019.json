{
  "Second FRCSyn-onGoing:Winning Solutions and Post-Challenge Analysis toImprove Face Recognition with Synthetic Data": "Ivan DeAndres-Tame, Ruben Tolosana, Pietro Melzi, Ruben Vera-Rodriguez, Minchul Kim, Christian Rathgeb,Xiaoming Liu, Luis F. Gomez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Zhizhou Zhong, YugeHuang, Yuxi Mi, Shouhong Ding, Shuigeng Zhou, Shuai He, Lingzhi Fu, Heng Cong, Rongyu Zhang, ZhihongXiao, Evgeny Smirnov, Anton Pimenov, Aleksei Grigorev, Denis Timoshenko, Kaleb Mesfin Asfaw, Cheng YawLow, Hao Liu, Chuyi Wang, Qing Zuo, Zhixiang He, Hatef Otroshi Shahreza, Anjith George, AlexanderUnnervik, Parsa Rahimi, Sebastien Marcel, Pedro C. Neto, Marco Huber, Jan Niklas Kolf, Naser Damer, FadiBoutros, Jaime S. Cardoso, Ana F. Sequeira, Andrea Atzori, Gianni Fenu, Mirko Marras, Vitomir Struc, Jiang Yu,Zhangjie Li, Jichun Li, Weisong Zhao, Zhen Lei, Xiangyu Zhu, Xiao-Yu Zhang, Bernardo Biesseck, Pedro Vidal,Luiz Coelho, Roger Granada, David Menotti AbstractSynthetic data is gaining increasing popularity forface recognition technologies, mainly due to the privacy concernsand challenges associated with obtaining real data, includingdiverse scenarios, quality, and demographic groups, amongothers. It also offers some advantages over real data, such asthe large amount of data that can be generated or the abilityto customize it to adapt to specific problem-solving needs. Toeffectively use such data, face recognition models should also IvanDeAndres-Tame,RubenTolosana,PietroMelzi,RubenVera-Rodriguez, Aythami Morales, Julian Fierrez, and Javier Ortega-Garcia arewith the Universidad Autonoma de Madrid, SpainMinchul Kim and Xiaoming Liu are with the Michigan State University, USAChristian Rathgeb is with the Hochschule Darmstadt, GermanyZhizhou Zhong, Yuxi Mi and Shuigeng Zhou are with the Fudan University,ChinaYuge Huang and Shouhong Ding are with the Tencent Youtu Lab, ChinaShuai He, Lingzhi Fu, Heng Cong, Rongyu Zhang, and Zhihong Xiao arewith the Interactive Entertainment Group of Netease Inc, ChinaEvgeny Smirnov, Anton Pimenov, Aleksei Grigorev, and Denis Timoshenkoare with the ID R&D Inc., USAKaleb Mesfin Asfaw is with the Korea Advanced Institute of Science &Technology, Korea Cheng Yaw Low is with the Institute for Basic Science,KoreaHao Liu, Chuyi Wang, Qing Zuo, and Zhixiang He are with the China TelecomAI, ChinaHatef Otroshi Shahreza, Anjith George, Alexander Unnervik, Parsa Rahimi,and Sebastien Marcel are with the Idiap Research Institute, SwitzerlandHatef Otroshi Shahreza, Alexander Unnervik, and Parsa Rahimi are also withthe EPFL, SwitzerlandSebastien Marcel is also with the Universite de Lausanne, SwitzerlandPedro C. Neto, Jaime S. Cardoso, and Ana F. Sequeira are with the INESCTEC, and the Universidade do Porto, PortugalMarco Huber, Jan Niklas Kolf, Naser Damer, and Fadi Boutros are with theFraunhofer IGD, GermanyAndrea Atzori, Gianni Fenu, Mirko Marras are with the University of Cagliari,ItalyVitomir Struc is with the University of Ljubljana, SloveniaJiang Yu, Zhangjie Li, and Jichun Li are with the Samsung Electronics (China)R&D Centre, ChinaZhangjie Li is also with the University of Science and Technology, ChinaWeisong Zhao and Xiao-Yu Zhang are with the IIE, CAS, ChinaZhen Lei and Xiangyu Zhu are with the MAIS, CASIA, ChinaBernardo Biesseck, Pedro Vidal, and David Menotti are with the FederalUniversity of Parana, BrazilBernardo Biesseck is also with the Federal Institute of Mato Grosso, BrazilLuiz Coelho and Roger Granada are with the unico - idTech, BrazilManuscript received Month 00, 2024; revised Month 00, 2024. be specifically designed to exploit synthetic data to its fullestpotential. In order to promote the proposal of novel GenerativeAI methods and synthetic data, and investigate the applicationof synthetic data to better train face recognition systems, weintroduce the 2nd FRCSyn-onGoing challenge, based on the2nd Face Recognition Challenge in the Era of Synthetic Data(FRCSyn), originally launched at CVPR 2024. This is an ongoingchallenge that provides researchers with an accessible platformto benchmark i) the proposal of novel Generative AI methodsand synthetic data, and ii) novel face recognition systems thatare specifically proposed to take advantage of synthetic data. Wefocus on exploring the use of synthetic data both individually andin combination with real data to solve current challenges in facerecognition such as demographic bias, domain adaptation, andperformance constraints in demanding situations, such as agedisparities between training and testing, changes in the pose, orocclusions. Very interesting findings are obtained in this secondedition, including a direct comparison with the first one, in whichsynthetic databases were restricted to DCFace and GANDiffFace.",
  "I. INTRODUCTION": "Face biometrics is a very popular area within ComputerVision and Pattern Recognition, finding applications acrossvarious domains such as person recognition , , healthcare, , and e-learning , among others. In recent years,with the fast development of deep learning, significant ad-vances have been made in areas like face recognition (FR), , surpassing previous benchmarks. However, FR tech-nology still faces challenges in several research directions,including explainability , demographic bias , ,privacy , and robustness against adverse conditions, such as aging , pose variations , , illuminationchanges , and occlusions .Synthetic data has gained popularity as a good solution tomitigate some of these drawbacks , , allowing thegeneration of i) a large number of facial images from different",
  ". Examples of synthetic identities and variations for different demographic groups using GANDiffFace": "non-existent identities, and ii) variability in terms of demo-graphic attributes and scenario conditions. Several GenerativeAI approaches have been presented in the last couple of yearsfor the synthesis of face images, considering state-of-the-artdeep learning methods. Two of the most popular methods areGenerative Adversarial Networks (GANs) and DiffusionModels . Popular generative methods can be SynFace and SFace , based on GANs, DCFace , IDiff-Face and ID3PM , based on Diffusion models, GANDiff-Face , based on the combination of GANs and Diffusionmodels, or alternative methods such as DigiFace-1M andITI-GEN , based on computer graphics techniques anda vision-language pre-trained CLIP model, respectively.Some examples of synthetic face images generated usingGANDiffFace are shown in .Beyond the generation of synthetic faces, another criticalaspect lies in understanding the potential applications andbenefits of synthetic data in enhancing FR technology. Re-cent studies have highlighted a performance gap between FRsystems trained only with synthetic data and those trained onreal data , . Nevertheless, the results achieved in the1st edition of the Face Recognition Challenge in the Era ofSynthetic Data (FRCSyn) , , emphasize the relevanceof synthetic data, either alone or merged with real data, inmitigating challenges in FR, such as demographic bias ,. Notably, in the 1st FRCSyn-onGoing, only syntheticdata from DCFace and GANDiffFace methods wereallowed for training FR systems. Additionally, together withnovel generative methods, improving FR technology involvesrefining the design and training processes to address domaingaps between real and synthetic data in certain scenarios. Forinstance, observations from the 1st FRCSyn-onGoing revealedthat most teams considered similar deep learning architectures(e.g., ResNet-100 ) and loss functions (e.g., AdaFace ),commonly used in FR systems trained with real data.In order to promote the development of novel face genera-tive methods and the creation of synthetic face databases, aswell as investigate the application of synthetic data to bettertrain FR systems, we have organized the 2nd FRCSyn-onGoingChallenge, which is based on the 2nd FRCSyn Challenge aspart of CVPR 20241 . In this 2nd edition, we introducenew sub-tasks allowing participants to train FR systems using synthetic data generated with their preferred generative frame-works, offering more flexibility compared to the 1st edition, . Additionally, new sub-tasks with varied experimen-tal settings are included to explore how FR systems can betrained under both constrained and unconstrained scenariosregarding the amount of synthetic training data. The FRCSynChallenge aims to address the following research questions:",
  ") Can synthetic data help alleviate current limitations inFR technology?": "These questions have become increasingly relevant after thediscontinuation of popular real FR databases due to privacyconcerns2 and the introduction of new regulatory laws3.The foundation of the present article was established in anearlier publication , with the current version notably ex-tending it through: i) a more extensive description and analysisof the top synthetic face generation methods and FR systemspresented so far in this 2nd FRCSyn-onGoing, including keygraphical representations of the proposed systems to improvethe understanding of the reader, ii) incorporating additionalmetrics in the evaluation of the proposed FR systems in orderto analyze different operational scenarios, iii) presenting anin-depth analysis of the performance achieved for various de-mographic groups and databases used for evaluation, togetherwith novel figures and tables, and iv) a direct comparisonbetween the results obtained in this 2nd edition and the onesobtained in 1st edition , , highlighting very interestingfindings.The remainder of the article is organized as follows. Sec-tion II describes the databases considered at the 2nd FRCSyn-onGoing. Section III explains the experimental setup of thechallenge, including the different tasks and sub-tasks, theexperimental protocol, metrics, and restrictions. In Section IV,we describe the approaches proposed by the top-6 participatingteams. Section V presents the best results achieved so far inthe different tasks and sub-tasks of 2nd FRCSyn-onGoing, em-phasizing the key results of the challenge. Finally, in SectionVI, we provide some conclusions, highlighting potential futureresearch directions in the field.",
  "A. Synthetic Databases": "One of the main novelties of the 2nd FRCSyn-onGoingis that there are no restrictions in terms of the generativemethods used to create synthetic data. Unlike the 1st FRCSyn-onGoing, where only synthetic data created using DCFace and GANDiffFace was available, in this 2nd edition weallow participants to use any generative framework of theirchoice to create synthetic data, limiting in some sub-tasksthe number of synthetic face images used to train the FRsystems (more details in Section III-A). As a reference, afterthe registration in the challenge, we provide all the participantswith a list of possible state-of-the-art generative frameworks.For completeness, we summarize next and in Table I the mostpopular approaches available at the beginning of the challenge: DCFace4 : This framework is entirely based onDiffusion models, composed of a sampling stage for thegeneration of synthetic identities XID, and a mixingstage for the generation of images XID,sty with the sameidentities XID from the sampling stage and the styleselected from a style bank of images Xsty. GANDiffFace5 : This framework combines Style-GAN and a Diffusion Model, i.e., DreamBooth ,to generate fully synthetic FR databases with desiredproperties such as human face realism, controllable de-mographic distributions, and realistic intra-class varia-tions (e.g., changes in pose, expression, and occlusions).Graphical examples are shown in . IDiff-Face6 : This framework uses a Diffusion Modelconditioned on identity context, which allows the modelto either generate variations of existing authentic imagesby using authentic embeddings or to generate novel syn-thetic identities by using synthetic face embeddings. Theauthors presented two distinct datasets: one by generatingidentity context in a two-stage process, and the otherthrough a synthetic uniform representation.",
  "CASIA-WebFace Real10.5K47500KBUPT-BalancedFace Real24K451MAgeDB Real5702917KCFP-FP Real500147KROF Real180316K": "DigiFace-1M7 : This framework can generate large-scale synthetic face images with many unique subjectsbased on 3D parametric model rendering. It considersthe method introduced by Wood et al. , tacklingthe ethical and labeling problems associated with thegeneration of synthetic data. ID3PM : This framework considers a DiffusionModel to perform an inversion of a FR model gen-erating new images from Gaussian noise with variousbackgrounds, lighting, poses, and expressions while pre-serving the identity.",
  "SYNFace9:ThisframeworkusesDiscoFace-GAN to generate face images with different identitiesfrom a Mixup Face Generator": "ITI-GEN10 : This framework uses CLIP togenerate embeddings to translate the visual attribute dif-ferences into natural language differences and perform aText-to-Image generation that is inclusive.These are just some possible generative frameworks, withthe corresponding synthetic databases available, that can beused by participants. But, as indicated before, the purposeof the 2nd FRCSyn-onGoing is to promote the proposal ofnovel generative methods and the creation of better syntheticdatabases to improve the performance of FR systems. Itis important to mention that in the 2nd FRCSyn-onGoing,synthetic data is exclusively used in the training stage of FRtechnology, replicating realistic operational scenarios.",
  "B. Real Databases": "For the training of the FR systems participants are allowedto use only the CASIA-WebFace as real data (dependingon the sub-task, please see Section III-A). This database con-tains 494, 414 face images of 10, 575 real identities collectedfrom the web. For the final evaluation of the proposed FRsystems, we consider the same four real databases used atthe 1st FRCSyn Challenge , , as they consider keychallenges in FR such as demographic bias, pose variations,",
  "aging, and occlusions. We summarize next and in Table II eachof them:": "BUPT-BalancedFace is designed to address per-formance disparities across different ethnic groups. Werelabel it according to the FairFace classifier , whichprovides labels for ethnicity (White, Black, Asian, Indian)and gender (Male, Female). We then consider the eightdemographic groups obtained from all possible combina-tions of four ethnic groups and genders. We are aware thatthese groups do not comprehensively represent the entirespectrum of real world ethnic diversity. Nevertheless, theselection of these categories, while imperfect, is primarilydriven by the need to align with the demographic catego-rizations used in BUPT-BalancedFace to facilitate easierand more consistent evaluation.",
  "III. SECOND FRCSYN-ONGOING: SETUP": "Due to the success of the 1st FRCSyn-onGoing , ,we also decided to run the 2nd edition in Codalab11, an open-source framework designed for conducting scientific competi-tions and benchmarks. On this platform, participants can findthe competitions requirements and limitations and can submittheir scores to automatically obtain i) the evaluation metrics oftheir system, and ii) the position on the challenge leaderboard.",
  "Similar to the 1st FRCSyn-onGoing , , in this 2nd": "edition we also explore the application of synthetic data fortraining FR systems, with a specific focus on addressing twocritical aspects in current FR technology: i) mitigating de-mographic bias, and ii) enhancing overall performance underchallenging conditions that include variations in age and pose,the presence of occlusions, and diverse demographic groups.To investigate these two areas, we consider two different tasks,each comprising three sub-tasks. Each sub-task considersdifferent types (real/synthetic) and amounts of data for trainingthe FR systems. Consequently, the 2nd edition comprises 6different sub-tasks. We summarize in Table III the key aspectsof the experimental protocol, metrics, and restrictions for eachsub-task.Task 1: The first task focuses on using synthetic data tomitigate demographic biases within FR systems. To evaluatethe performance of these systems, we create sets of matedand non-mated comparisons using subjects from the BUPT-BalancedFace database . We consider the eight demo-graphic groups defined in Section II-B, which result from thecombination of four ethnicities (White, Black, Asian, and In-dian) and two genders (Male and Female), ensuring a balancedrepresentation across these groups in the comparison lists. Fornon-mated comparisons, we exclusively pair subjects withinthe same demographic group, as these hold greater relevancecompared to non-mated comparisons involving subjects fromdifferent demographic groups.Task 2: The second proposed task focuses on usingsynthetic data to enhance the overall performance of FR sys-tems under challenging conditions. To assess the effectivenessof the proposed systems, we use lists of mated and non-mated comparisons selected from subjects from the differentevaluation databases, each one designed to address specificchallenges in FR. Specifically, BUPT-BalancedFace is used toconsider diverse demographic groups, whereas AgeDB, CFP-FP, and ROF to assess age, pose, and occlusion challengesrespectively.",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20215": "exclusively, i.e., CASIA-WebFace , and ii) following thespecific requirements of the chosen sub-task, as summarized inTable III. According to this protocol, participants must provideboth the baseline system and the proposed system for eachspecific sub-task. The baseline system plays a critical rolein evaluating the impact of synthetic data on training andserves as a reference point for comparing the proposed modelagainst the conventional practice of training only with realdatabases. To maintain consistency, the baseline FR system,trained exclusively with real data, and the proposed FR system,trained according to the specifications of the selected sub-task,must have the same architecture and training protocol.Evaluation: In each sub-task, participants received thecomparison files comprising both mated and non-mated com-parisons, which are used to evaluate the performance of theirproposed FR systems. Task 1 involves a single comparisonfile containing balanced comparisons of different demographicgroups of the BUPT database, while Task 2 comprisesfour comparison files, each corresponding to every specificreal-world databases considered (i.e., BUPT, AgeDB ,CFP-FP , and ROF ). During the evaluation of eachsub-task, participants are required to submit two files perdatabase through the Codalab platform: i) the scores of thebaseline system, and ii) the scores of the proposed system.Finally, for each sub-task, participants must submit a fileincluding the decision threshold for each FR system (i.e.,baseline and proposed). The submitted scores must fall withinthe range of , with lower scores indicating non-matedcomparisons, and vice versa.",
  "C. Evaluation Metrics": "We evaluate the FR systems using a protocol based onlists of mated and non-mated comparisons for each sub-task and database. From the scores and thresholds providedby participants, we calculate the binary decision and theverification accuracy. Additionally, we calculate the gap to real(GAP) as follows: GAP = (REAL SYN) /SYN, withREAL representing the verification accuracy of the baselinesystem and SYN the verification accuracy of the proposedsystem, trained with synthetic (or real + synthetic) data. Othermetrics such as False Non-Match Rate (FNMR) at a fixedoperational point, or the Area Under the ROC Curve whichare very popular for the analysis of FR systems in real-worldapplications, are also computed from the scores provided byparticipants. Next, we explain how participants are ranked inthe different tasks.Task 1: To rank participants and determine the winnersof Sub-Tasks 1.1, 1.2, and 1.3, we closely examine the trade-off between the average (AVG) and standard deviation (SD)of the verification accuracy across the eight demographicgroups defined in Section II-B. We define the trade-off metric(TO) as follows: TO = AVG SD. This metric involvesplotting the average accuracy on the x-axis and the standarddeviation on the y-axis in a 2D space. Multiple 45-degreeparallel lines are drawn to identify the winning team, whoseperformance is located on the far right of these lines. Withthis proposed metric, we reward FR systems that achieve good levels of performance and fairness simultaneously, unlike com-mon benchmarks based only on recognition performance. Thestandard deviation of verification accuracy across demographicgroups is a common metric for assessing bias and should bereported by any work addressing demographic bias mitigation.Task 2: To rank participants and establish the winners inSub-Tasks 2.1, 2.2, and 2.3, we examine the average verifica-tion accuracy from the four different databases designated forevaluation, as described in Section II. This approach enablesus to assess four main challenges of FR technologies: i) therepresentation of diverse demographic groups, ii) the impact ofaging on recognition, iii) the variations in facial pose, and iv)the challenges made by occlusions. This evaluation providesa comprehensive overview of FR systems in real operationalscenarios.",
  "D. Restrictions": "Participants have the freedom to choose the FR system foreach task as long as the number of Floating Point OperationsPer Second (FLOPs) of the system does not exceed 50GFLOPs. This threshold has been established to facilitatethe exploration of innovative architectures and encourage theuse of diverse models while preventing the dominance ofexcessively large models. Participants are also free to use theirpreferred training modality, with the requirement that only thespecified databases are used for training. Generative modelscannot be used to generate supplementary data. Participantsare allowed to use non-face databases for pre-training purposesand use traditional data augmentation techniques using theauthorized training databases. To maintain the integrity ofthe evaluation process, the organizers reserve the right todisqualify participants if anomalous results are detected or ifparticipants fail to adhere to the challenges rules.",
  "IV. SECOND FRCSYN-ONGOING: SYSTEMS DESCRIPTION": "In this 2nd FRCSyn-onGoing, we encourage participantsto propose novel Generative AI methods for the creation ofsynthetic data. Besides, we also give participants the freedomto choose the FR architecture and training methods. Table IVsummarizes for each team the key information in terms of theproposed synthetic data and FR system. Teams are arrangedby their average ranking in the 6 sub-tasks from the 2nd FRCSyn-onGoing. In general, we can see that most teamshave decided to use synthetic data from DCFace andIDiff-Face databases, improving also the original datathrough cleaning and selection approaches, among other moresophisticated techniques. Also, regarding the FR technologies,most of them are based on ResNet and IResNet architectures, with AdaFace and ArcFace as the mainused losses. However, some teams proposed their own methodsto generate synthetic facial images, as well as to train their FRmodels. Next, we describe the specific details of the top-11proposed systems in the 2nd FRCSyn-onGoing.ADMIS (All sub-tasks): This team comprises membersfrom Fudan University and Tencent Youtu Lab, China. Theyused a Latent Diffusion Model (LDM) based on IDiff-Face to synthesize faces. The LDM was conditioned",
  ". Framework proposed by the OPDAI team": "using identity embeddings as contexts, extracted from facesby a pretrained ElasticFace model with an IResNet-101 backbone. They trained the LDM with the CASIA-WebFace database. As the LDM takes the ID embed-dings as context, they considered an unconditional DenoisingDiffusion Probabilistic Model (DDPM) trained on the FFHQdatabase as a context generator. Specifically, they usedthe DDPM to generate 400K faces with arbitrary identities,known as context faces. They exploited the same ElasticFacemodel to extract the embeddings from the context faces. Toencourage the quality and the distinctiveness of identity fromlater LDM-generated faces, they filtered the embeddings bysetting a minimum cosine similarity threshold of 0.3 betweenarbitrary pairs of embeddings. This yields 30K embeddingswith discriminative identities. Furthermore, they acceleratedthe sampling process of the LDM by Denoising DiffusionImplicit Models (DDIM) . For the training of the FRmodel, they generated 49 images for each context. Theyadopted the ID oversampling strategy from DCFace andperformed it five times for each ID to enhance consistency. Asa result, 10K contexts were used for Sub-Tasks 1.1 and 2.1,while 30K for Sub-Tasks 1.2 and 2.2. For Sub-Tasks 1.3 and2.3, they expanded Sub-Tasks 1.1 and 2.1 with the CASIA-WebFace database. Both the baseline and proposed FR modelsused IResNet-101 architectures. They applied the ArcFace loss with a batch size of 64 and an initial learning rate of 0.1for 40 epochs. The learning rate was divided by 10 at epochs22, 28, and 32. They also used random cropping augmentationduring training. Their proposed architecture is described in.Code: FRCSyn ADMIS",
  ". Framework proposed by the ID R&D team": "OPDAI (All sub-tasks): This team comprises membersfrom Interactive Entertainment Group of Netease Inc, China.They initially used the DCFace database, generating then10 more face images for each ID with large pose variations andocclusions using Photomaker . They randomly replacedthese images in the original DCFace data to ensure that thetotal number of samples meets the requirement of 500K.During the Photomaker inference, they adopted a batch sizeof 1 and used random prompts including age, pose, andimage quality to ensure the diversity of the generated samples.For Sub-Tasks 1.2 and 2.2, they combined this data withthe 1.2M version of DCFace, while for Sub-Tasks 1.3 and2.3, it was merged with CASIA-WebFace . For Sub-Tasks 1.2, 1.3, 2.2, and 2.3 they did not merge nor denoisesamples from different databases, following the Partial FCapproach . Regarding the FR model, they obtained theloss of different databases in independent AdaFace heads,calculating the final loss as the average of the multiple heads.Both baseline and proposed models are based on IResNet-100 architectures, with horizontal flipping. Their proposedarchitecture is described in .Code: cvpr2024.git ID R&D (All sub-tasks): This team comprises membersfrom ID R&D Inc, USA. To generate the synthetic data, theyused two models trained on WebFace42M . The first modelwas based on Hourglass Diffusion Transformers (HDT) .It was trained focusing on conditional flow matching andfollowing the classifier-free guidance approach . Identityembeddings were used directly, whereas style embeddingswere processed through a Vector Quantised-Variational Au-toEncoder (VQVAE) . Specifically, for head position, 32embeddings were allocated for VQVAE processing, whileage and facial expression attributes were represented with 8embeddings each. During inference, combinations of theseembeddings were randomly selected. The second model usedto generate synthetic data was based on StyleNAT ,enhanced with a FR model. This network was trained usingauxiliary sources of supervision: a pre-trained FR networkwith Prototype Memory (for identity supervision) anda pre-trained face attribute classification network (for stylesupervision). To create their synthetic data, they used classifierweights of the trained Prototype Memory to get 50K identityembeddings, of which 20K were randomly selected and 30Kwere uniformly sampled from the 1K clusters obtained usingk-means, to get demographic diversity. For each identity, they",
  ". Framework proposed by the K-IBS-DS team": "generated 5 face images using each of the two generativemodels. In the first stage, identity embeddings were used bythe HDT model to get 10 images for each identity. Then 5of these images were included in the training dataset. Identityand style embeddings were taken from the remaining 5 imagesand used as a condition to generate 5 different images withthe StyleNAT model. These images were also put into thetraining dataset. Regarding the FR model, they trained anIResNet-200 with UniFace loss for 28 epochs. Onenetwork was trained with color, geometric augmentations,and FaceMix-B , and the other one using only randomhorizontal flipping. These two networks were combined in anensemble, where the first one received the original image,and the second one a mirrored copy. They used the samemodel for Sub-Tasks 1.1, 1.2, 2.1 and 2.2. For Sub-Tasks 1.3and 2.3, they combined the synthetic data with the CASIA-WebFace data, training two models, one on the mixeddata, and the other on the CASIA-WebFace. Their proposedarchitecture is described in .K-IBS-DS (All sub-tasks): This team comprises mem-bers from Korea Advanced Institute of Science & Technologyand Institute for Basic Science, South Korea. They usedDCFace with 500K and 1.2M face images (dependingon the sub-task). Regarding the FR model, they used severalIResNet models of 50, 100, and 152 layers with Squeeze-and-Excitation (SE) blocks . Inspired by SlackedFace ,they made two modifications to enhance the AdaFace FR classifier: i) they used renormalized uniform initializationas a more reliable weight initialization for uniformity acrossidentity prototypes in the unit sphere and ii) replaced the L2-norm with the powered-norm (p-norm), or face recognizabilityindex from , which integrates the L2-norm with the learnedembedding proximity. The training stage was in line with and , including optimizer, learning rate, etc. For Sub-Tasks 1.3 and 2.3, the first 10K subjects of the CASIA-WebFace were assigned for training, and the remainingones for performance validation using random pairs withchallenging conditions (identified based on the poorest L2- StyleGAN",
  ". Framework proposed by the Idiap-SynthDistill team": "norm values ). The final score was obtained by aggregatingthe comparison scores of the different IResNet models, alongwith the horizontally flipped instances through score fusion.All training and test sets were realigned using RetinaFace ,followed by a similarity transformation. For all sub-tasks,aggressive data augmentations were applied, including randomhorizontal flipping, photometric operations, cropping, resizing,and the addition of sunglasses and masks. Their proposedarchitecture is described in .Code: frcsyn CTAI (All sub-tasks): This team comprises membersfrom China Telecom AI, China. By analyzing popular syn-thetic data, they found that intra-class and inter-class noise waswidely present. Data cleaning can effectively remove the badexamples of synthetic data and retain important images from alarge amount of synthetic data. In order to select the optimalsynthetic data, they first trained an IResNet-100 modelwith SE blocks using CASIA-WebFace to extractfeatures of synthetic images from DCFace , GANDiff-Face , and DigiFace-1M . Subsequently, they used DB-SCAN clustering to segregate intra-class noise and removedIDs with a class center feature cosine similarity greater than0.5. Finally, they used the cleaned synthetic data merged withCASIA-WebFace to finetune the IResNet-100 for a seconddata refinement. From the final refined synthetic dataset, theysampled 500K face images while retaining as many IDs aspossible to build their synthetic training set. Regarding the FRmodel, in particular Sub-Task 2.3 in which they achieved theirhighest position among all sub-tasks, they trained IResNet-100with AdaFace loss (A1) and CosFace loss (A2) withmask and occlusion augmentation on CASIA-WebFace and therefined synthetic data. They used an ensemble of A1, A2, anda model trained with only synthetic data. Furthermore, dataaugmentation was considered to enhance all features.Code: Idiap-SynthDistill (Sub-Tasks 1.2 and 2.2): This teamcomprises members from Idiap Research Institute, EPFL, andUniversite de Lausanne, Switzerland. The proposed methodwas based on SynthDistill , which is an end-to-end ap-",
  ". Architecture proposed by the INESC-IGD team": "proach, generating synthetic images and training the FR modelin the same training loop. Instead of using the pre-trainedmodel in a separate step, they directly used it in the trainingloop for supervision, while a new student FR model wastrained fully using synthetic data generated from a StyleGANmodel . For generating synthetic images, they trainedStyleGAN2 with the CASIA-WebFace database andthen dynamically generated 20M synthetic images duringtraining based on the training loss. For the dynamic imagegeneration, they used the training loss from every iteration asfeedback to find the most difficult synthetic image in eachbatch and then re-sampled a new batch of synthetic images inthe intermediate latent space W of StyleGAN near the latentvector of the most difficult sample. If the loss value was high,they re-sampled with a relatively small standard deviationaround the difficult sample and generated similar images, butif the loss value was small they re-sampled with a higherstandard deviation, generating images with more variations.Throughout the process, the generated images were resized to112112 before being fed to the FR models. They used a pre-trained FR model with IResNet-101 architecture trainedwith CosFace loss on a subset of the WebFace260Mdatabase and trained a new model as a student networkwith the same architecture using synthetic data with theirdynamic synthetic image generation approach. They used theAdam optimizer with an initial learning rate of 0.001 andtrained their student model with the same loss function asin . For thresholding, a subset of DCFace was usedto determine the optimal threshold for maximizing verificationaccuracy, using a 10-fold cross-validation approach based ona random selection of identities and comparison pairs. Theirproposed architecture is described in .Code: synthdistill INESC-IGD (All sub-tasks): This team comprises mem-bers from INESC TEC and Universidade do Porto, Portugal,and Fraunhofer IGD, Germany. For the training dataset, theymerged DCFace , IDiff-Face Uniform, and IDiff-FaceTwo-stage databases and then labeled the data withethnicity labels using a similar approach to . For Sub-Tasks 1.1 and 2.1, they created a synthetic training datasetcontaining 500K face images by sampling 7K balancedidentities, in terms of ethnicity labels. For Sub-Tasks 1.2 and2.2, they created a synthetic training dataset containing 2.1Mface images by sampling 50K identities from the trainingdatasets. For Sub-Tasks 1.3 and 2.3, two instances of ResNet-",
  ". Framework proposed by the UNICA-IGD-LSI team": "100 were trained, one on CASIA-WebFace and theother on a subset of synthetic datasets (e.g., 400K images of9K identities). For all sub-tasks, they trained a ResNet-100with ElasticCosFac-Plus loss using the settings presentedin . During the testing phase of Sub-Tasks 1.3 and 2.3,feature embeddings were obtained from trained models andthe weighted sum of 0.5 score-level fusion was used. Duringthe FR training of all sub-tasks, the training datasets wereaugmented using RandAug and occluded augmentation with probabilities of 0.4. The occluded augmentation followedprotocol 4 proposed in , leading to occlusions on theeyes, lower face, upper face, or a combination of the eyesocclusion with the others. Occluded augmentations boosted theperformance, as synthetic data has a lower frequency of naturalocclusions such as beard and makeup . Their proposedarchitecture is described in .Code: UNICA-IGD-LSI (All sub-tasks): This team comprisesmembers from Fraunhofer IGD, Germany, University ofCagliari, Italy, and University of Ljubljana, Slovenia. Theyused the DCFace synthetic database as it led to remark-able performance gains under well-known evaluation bench-marks for face verification, while combined with real data .Also, they considered synthetic data generated with IDiff-Face and ExFaceGAN in Sub-Tasks 1.1, 1.2, 2.1,and 2.2. The ExFaceGAN data was generated using an identitydisentanglement approach on pretrained GAN-Control .Regarding the FR model, they trained a ResNet-100 network using CosFace loss with a margin penalty of 0.35and a scale term of 64. The similarity mean difference betweenreal-only and synthetic-only samples was scaled and added tothe loss value. They trained the FR model for 40 epochs witha batch size of 512 and an initial learning rate of 0.1, whichwas divided by 10 after 10, 22, 30, and 40 epochs. Duringthe training phase, the synthetic samples were augmentedusing RandAugment with 4 operations and a magnitude of 16,following , . For Sub-Tasks 1.3 and 2.3, the selectedsynthetic dataset was combined with CASIA-Webface ,obtaining a total of 1M images from 20, 572 identities. Theirproposed architecture is described in .Code:",
  ". Framework proposed by the SRCN AIVL team": "University of Science and Technology, IIE, CAS, and MAIS,CASIA, China. They selected 400K samples from the DC-Face database and labeled the ethnicity of each subject,as the racial distribution gap may lead to bad performance intesting. Based on this approach, they trained IDiff-Face with CASIA-WebFace database generating 100K syn-thetic face images of specific races. Regarding the FR system,they used two custom ResNet-101 trained with AdaFaceloss function. The models were trained for 60 epochswith an initial learning rate of 0.1 and a batch size of 512,which was adjusted at predefined milestones. Their trainingdata underwent further preprocessing, including padding cropaugmentation, low-resolution augmentation, photometric aug-mentation, random grayscale, and normalization. The thresh-old was determined by the 10-fold optimal threshold in thevalidation set. For the inference, data preprocessing involvedan MTCNN for the alignment and resizing all data to112 112. After cropping and alignment, they fed the imageand the flipped image into the two models. Finally, after ob-taining the two feature embeddings, they combined them andperformed the similarity calculation with these embeddings.Their proposed architecture is described in .Code: CBSR-Samsung (Sub-Tasks 1.3 and 2.3): This teamcomprises members from Samsung Electronics (China) R&DCentre, IIE, CAS, and MAIS, CASIA, China. They first traineda FR model using CASIA-WebFace . Then, they usedit to de-overlap DCFace from CASIA, as DCFace wastrained using that real database. For the synthetic dataset,they compared the performance of models trained with threesynthetic databases, including GANDiffFace , DCFace,and IDiff-Face , and finally selected DCFace as theonly synthetic training set. They created a validation datasetincluding three subsets for three different testing scenarios: i)random sample pairs from DCFace, simulating age variabil-ity and demographic groups as in AgeDB and BUPT-Balanced databases, respectively; ii) randomly positionedvertical bar masks to the images to simulate the self-occlusiondue to as considered in CFP-FP database ; and iii) addmask and sunglasses to the images by detecting the land-marks via FaceX-Zoo , simulating the ROF database . De-overlap",
  ". Framework proposed by the BOVIFOCR-UFPR team": "This is done following . All validation subsets consistof 6K positive pairs and 6K negative pairs. Finally, theyconcatenated these subsets as the validation set. Subsequently,they conducted an intra-class clustering for all datasets usingDBSCAN (0.3 threshold) and removed the samples that wereseparated from the class center. Regarding the FR model, theymerged the refined datasets and trained IResNet-100 withAdaFace loss . In addition, they adopted two augmentationstrategies, i.e., photometric augmentation and rescaling. Afterthat, they trained two FR models using occlusion augmentationwith 10% and 30% probability, respectively. Finally, theysubmitted the average similarity score of the two models. Theirproposed architecture is shown in .BOVIFOCR-UFPR (Sub-Tasks 1.2 and 2.1): This teamcomprises members from Federal University of Parana, Fed-eral Institute of Mato Grosso, and unico - idTech, Brazil. Theychose DCFace as the synthetic database and randomlyremoved 910 identities with 55 images per ID to reduce thenumber to follow the rules. For the FR model, they used aResNet-100 as the backbone, trained with the ArcFace loss function. The images used for training were augmentedusing a Random Flip with a probability of 0.5. They alsoapplied random erasing and RandAugment as additional aug-mentations. To validate their model they subsampled imagesfrom DCFace, generating genuine and impostor pairs, and usedthese pairs to select the best threshold to classify the proposedmodel output scores. Their proposed architecture is describedin .Code:",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202111": "TABLE VRESULTS OF THE TEAMS THAT RANKED AMONG THE TOP-6 IN AT LEASTONE SUB-TASK, ORDERED BY THE AVERAGE RANK IN ALL THESUB-TASKS. FOR EACH TEAM, WE REPORT THE RANKING METRIC ANDTHE POSITION ACROSS ALL THE SUB-TASKS. THE BEST RESULT OF EACHSUB-TASK IS HIGHLIGHTED IN BOLD. WE MARK WITH A - IF THE TEAMDID NOT PARTICIPATE IN A SUB-TASK. TO = TRADE-OFF, AVG =AVERAGE ACCURACY",
  "TeamTask 1.1TO [%]Task 1.2TO [%]Task 1.3TO [%]Task 2.1AVG [%]Task 2.2AVG [%]Task 2.3AVG [%]": "ADMIS94.30 (2)95.72 (2)96.50 (1)91.19 (3)92.92 (2)94.15 (5)OPDAI93.75 (4)94.12 (3)95.96 (4)91.93 (1)92.04 (3)95.23 (2)ID R&D96.73 (1)96.73 (1)86.73 (8)91.86 (2)91.86 (4)94.05 (6)K-IBS-DS92.91 (6)93.72 (5)96.17 (2)91.05 (4)91.61 (5)95.42 (1)CTAI93.21 (5)93.21 (6)95.41 (7)90.59 (5)90.59 (6)94.56 (3)Idiap-SynthDistill-89.70 (9)--93.50 (1)-INESC-IGD92.28 (7)94.05 (4)95.65 (5)83.16 (8)85.40 (7)89.43 (8)UNICA-IGD-LSI91.89 (8)91.89 (7)96.00 (3)87.80 (7)87.80 (8)92.79 (7)SRCN AIVL94.06 (3)-----CBSR-Samsung--95.57 (6)--94.20 (4)BOVIFOCR-UFPR-90.48 (8)-89.97 (6)--",
  "A. Task 1: Bias Mitigation": "Table VI shows the results achieved by participants in Task1, focused on demographic bias mitigation. Teams are rankedby descending order of TO, which tends to correlate to theascending order of SD (i.e., from less to more biased FRsystems). Notably, the winner of Sub-Tasks 1.1 and 1.2, IDR&D (96.73% TO), demonstrates a significant negative GAPvalue (-5.31%), showing a higher performance when trainingthe FR system with synthetic data compared to real data(i.e., CASIA-WebFace ). Furthermore, in Sub-Task 1.1, wecan observe that teams with negative GAP values consideredDiffusion Models for the generation of synthetic data (i.e.,ID R&D uses HDT, SRCN AIVL combines DCFace andIDiff-Face, and CTAI combines DCFace and GANDiffFace),showing that this generation method may work better than realdata in scenarios with limited data. Next, after removing thelimitation on the number of synthetic images (i.e., Sub-Task1.2), the TO value of most FR systems increases, which leadsto performance and fairness improvement simultaneously. Forinstance, for the ADMIS team (ranked top-2 in both Sub-Task 1.1 and 1.2), the TO value increases to 95.72% in Sub-Task 1.2 (i.e., 1.42% TO improvement compared to Sub-Task 1.1). Also, the GAP value decreases from 1.47% to -0.56%, obtaining better results when increasing the amount ofsynthetic data in comparison to limited real data (i.e., CASIA-WebFace). Another example is the OPDAI team, which raisedfrom top-4 to top-3 positions between Sub-Task 1.1 and 1.2. ItsTO value increases to 94.12% (i.e., 0.37% TO improvementfrom Sub-Tasks 1.1 to 1.2), and the GAP value is reducedfrom 1.02% to 0.71%. These findings emphasize the potentialof generating large number of synthetic face images fromdifferent demographic groups to mitigate bias in existing FRtechnology. Finally, we analyze in Sub-Task 1.3 the case ofusing both, real and synthetic data, in the FR training process.In general, we can observe considerable improvements in TABLE VIRANKING FOR THE THREE SUB-TASKS CONSIDERED IN TASK 1. FOR EACHSUB-TASK, WE HIGHLIGHT IN BOLD THE BEST TEAM ACCORDING TO THETRADE-OFF. TO = TRADE-OFF, AVG = AVERAGE ACCURACY, SD =STANDARD DEVIATION OF ACCURACY, FNMR = FALSE NON-MATCHRATE, FMR = FALSE MATCH RATE, AUC = AREA UNDER CURVE, GAP= GAP TO REAL.",
  "Pos.TeamTO [%]AVG [%]SD [%]FNMR@FMR=1%AUC [%]GAP [%]": "1ADMIS96.5097.250.753.90 (1)99.72 (1)-1.332K-IBS-DS96.1796.920.755.88 (4)99.54 (2)-1.373UNICA-IGD-LSI96.0096.700.705.90 (5)99.49 (3)-5.334OPDAI95.9696.800.844.90 (2)99.54 (2)-0.035INESC-IGD95.6596.330.676.15 (6)99.18 (5)-0.126CBSR-Samsung95.5796.540.975.00 (3)99.41 (4)-24.43 terms of TO values, along with higher negative GAP valuesfor all the top-6 teams, e.g., ADMIS (96.50% TO, -1.33 GAP),K-IBS-DS (96.17% TO, -1.37% GAP), and UNICA-IGD-LSI(96.00% TO, -5.33% GAP). These results highlight that thecombination of synthetic and real data achieves superior FRperformance compared to the case of only training with realdata. Finally, it is noteworthy to compare the best resultsachieved in Sub-Task 1.2, i.e., unconstrained synthetic data,and Sub-Task 1.3, i.e., constrained synthetic + real data. TheID R&D team achieves 96.73% TO in Sub-Task 1.2, whereasADMIS achieves 96.50% TO in Sub-Task 1.3, showing thatunlimited synthetic data for training can even outperform FRsystems trained with limited synthetic + real data. Theseresults motivate the use of synthetic data for demographicbias mitigation, improving at the same time privacy as no realidentities are seen by the network. B. Task 2: Overall ImprovementTable VII provides the results achieved by participants inTask 2, focusing not only on bias mitigation but also otherchallenges in FR such as age, pose, and occlusions. Teams areranked in descending order based on the average verificationaccuracy across the four databases. Notably, in all sub-tasks,the AVG is lower than the achieved in Task 1 for the BUPT-BalancedFace database, showing the additional challengesintroduced by the AgeDB , CFP-FP , and ROF databases. This trend can also be observed in GAP results,which tend to be worse for Sub-Tasks 2.1 and 2.2 comparedto Sub-Tasks 1.1 and 1.2, suggesting that it is far more difficult",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202112": "False Match Rate (%) False Non-Match Rate (%) Sub-Task 1.1: Bias Mitigation White Male: 97.90% Acc White Female: 96.50% Acc Black Male: 97.80% Acc Black Female: 97.20% Acc Asian Male: 96.90% Acc Asian Female: 96.80% Acc Indian Male: 99.20% Acc Indian Female: 98.10% Acc False Match Rate (%) False Non-Match Rate (%) Sub-Task 1.2: Bias Mitigation White Male: 97.90% Acc White Female: 96.50% Acc Black Male: 97.80% Acc Black Female: 97.20% Acc Asian Male: 96.90% Acc Asian Female: 96.80% Acc Indian Male: 99.20% Acc Indian Female: 98.10% Acc False Match Rate (%) False Non-Match Rate (%) Sub-Task 1.3: Bias Mitigation White Male: 97.90% Acc White Female: 98.60% Acc Black Male: 97.10% Acc Black Female: 97.30% Acc Asian Male: 97.30% Acc Asian Female: 96.20% Acc Indian Male: 97.40% Acc Indian Female: 96.20% Acc",
  "Pos.TeamAVG [%]FNMR@FMR=1%AUC [%]GAP [%]": "1K-IBS-DS95.429.49 (5)98.14 (6)-2.152OPDAI95.237.54 (1)98.70 (1)-0.523CTAI94.568.85 (4)98.41 (3)-6.014CBSR-Samsung94.208.62 (3)98.17 (4)-4.405ADMIS94.1510.99 (6)98.46 (2)-1.106ID R&D94.058.00 (2)98.16 (5)0.07 to emulate the conditions of these real databases with syntheticdata by itself. For example, in Sub-Task 2.1, although the top-3 teams achieve high AVG results, they exhibit a considerablepositive GAP value (i.e., OPDAI 91.93% AVG, 3.09% GAP;ID R&D 91.86% AVG, 2.99% GAP; ADMIS 91.19% AVG,2.78% GAP), showing that a FR model trained only with realdata (i.e., CASIA-WebFace) adapts better to adverse imageconditions such as aging, pose, or occlusions. Focusing onSub-Task 2.2, the team ranked top-1, i.e., Idiap-SynthDistill,achieves much better results compared to the best result ofSub-Task 2.1 (i.e., 93.50% vs. 91.93% AVG), proving thatunlimited synthetic data can further improve the performanceof the system. Finally, in Sub-Task 2.3, most teams reportbetter AVG and higher negative GAP values (e.g., K-IBS-DS achieves 95.42% AVG, -2.15% GAP), proving again that",
  "C. Demographic Groups and Evaluation Databases": "This section provides an in-depth analysis of the resultsin terms of the different demographic groups and individualdatabases considered in the 2nd FRCSyn-onGoing. shows the Detection Error Tradeoff (DET) curves of Sub-Tasks 1.1, 1.2, and 1.3, including the results achieved for thetop-1 team in each demographic group. For completeness, theinformation and graphical representations for all the teams canbe found on the challenge Codalab platform12.For Sub-Tasks 1.1 and 1.2, the team that achieves thefirst place, ID R&D, demonstrates high performance acrossthe different demographic groups considered (above 96.50%Accuracy for all demographic groups). However, a slightgender bias can be observed (improvements of 1% betweenMale and Female labels for some ethnicities). Regardingthe ethnicity, the proposed FR model showed better resultsfor subjects from the Indian ethnicity (99.20% Accuracy forIndian Male; 98.10% Accuracy for Indian Female). Finally,for Sub-Task 1.3, the winning team, ADMIS, also performswell across all demographic groups (all above 96% Accuracy).However, there exists variability in performance between dif-ferent demographic groups. For example, Asian Females andIndian Females have the lowest Accuracy (96.20%) whileWhite Females have the highest Accuracy (98.60%). shows the DET curves of Sub-Tasks 2.1, 2.2,and 2.3, including the results achieved for the top-1 teamin each database. Analyzing the FR model proposed by theOPDAI team for Sub-Task 2.1, the spread of the curvesindicates variability in the system performance across differentdatabases, with the results from AgeDB (94.54% Accuracy)outperforming others. Moreover, in Sub-Task 2.2 the Idiap-SynthDistill FR model significantly improves the performanceof Sub-Task 2.1 for AgeDB and CFP-FP databases (i.e.,96.72% and 96.14% Accuracy, respectively). Finally, for Sub-Task 2.3, the curves from the K-IBS-DS FR model areclosely aligned for the AgeDB (96.89% Accuracy), BUPT(96.88% Accuracy), and CFP-FP (97.51% Accuracy), showingconsistent and reliable performance across these databases.However, the curve from the ROF database remains the worst",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202113": "False Match Rate (%) False Non-Match Rate (%) Sub-Task 2.1: Overall Improvement BUPT: 93.42% AccROF: 87.22% AccAGEDB: 94.54% AccCFP-FP: 92.51% Acc False Match Rate (%) False Non-Match Rate (%) Sub-Task 2.2: Overall Improvement BUPT: 93.85% AccROF: 87.31% AccAGEDB: 96.72% AccCFP-FP: 96.14% Acc False Match Rate (%) False Non-Match Rate (%) Sub-Task 2.3: Overall Improvement BUPT: 96.88% AccROF: 90.39% AccAGEDB: 96.89% AccCFP-FP: 97.51% Acc . DET curves of Sub-Tasks 2.1 (left), 2.2 (Middle), and 2.3 (right), including the results achieved for the top-1 team (i.e., OPDAI, Idiap-SynthDistill,K-IBS-DS, respectively) in each evaluation database.",
  "D. Post-Challenge Analysis and Comparison with 1st Edition": "Analyzing the contributions of all eleven top teams, wecan observe the prevalence of well-established methodologies.Notably, most teams used DCFace either independentlyor in conjunction with other synthetic databases such asGANDiffFace , DigiFace-1M , or IDiff-Face . Fur-thermore, several teams, including CTAI and CBSR-Samsung,adopted interesting approaches involving synthetic data clean-ing and selection. It is interesting to highlight the ID R&D andIdiap-SynthDistill teams as they considered novel methods togenerate synthetic data. Specifically, the ID R&D team usedan HDT to generate synthetic facial images along withidentity and style embeddings, wich were used by a Style-NAT model to generate more variability in the syntheticdata. Another example is the Idiap-SynthDistill team, whichproposed an end-to-end method that dynamically generatedfacial images through StyleGAN2 and trained a FR modelthrough model distillation. Regarding the backbone architec-ture, all teams opted for either ResNet or IResNet ,mainly for their widespread adoption in state-of-the-art FRmethodologies. The selection of the loss functions was alsosimilar among the teams, with AdaFace and ArcFace the prevalent choices. Nevertheless, there were exceptions suchas for the ID R&D team that used the recent UniFace , orthe UNICA team that considered CosFace .Finally, we compare the results achieved in the 2nd FRCSyn-onGoing with the results of the 1st edition . Table VIIIshows the best results achieved in the 1st and 2nd editions ofthe challenge, including also the GAP values. It is important toremark that Sub-Tasks 1.2 and 2.2 of the 2nd FRCSyn-onGoingare not included in the analysis as they are novel sub-tasksonly available in the 2nd edition. Notably, two observationscan be made: i) the main metric for ranking teams (i.e., TOand AVG) shows improvements across all cases in this 2nd edition for both Task 1 (e.g., 96.73% vs. 92.25% TO in Sub-Task 1.1 and 96.50% vs. 95.25% TO in Sub-Task 1.3) and Task2 (e.g., 91.93% vs. 90.50% AVG in Sub-Task 2.1 and 95.42%vs. 94.95% AVG in Sub-Task 2.3), and ii) in terms of theGAP value, the FR models of this 2nd edition follow a similar TABLE VIIIDESCRIPTION OF THE BEST RESULTS ACHIEVED IN THE 1ST AND 2NDFRCSYN-ONGOING. SUB-TASKS 1.2 AND 2.2 OF THE 2NDFRCSYN-ONGOING ARE NOT INCLUDED IN THE TABLE AS THEY ARENOVEL SUB-TASKS ONLY AVAILABLE IN THE 2ND EDITION.",
  "BOVIFOCR94.95-3.69K-IBS-DS95.42-2.15": "trend compared to the 1st edition, achieving in most sub-tasksnegative GAP values, remarking the benefits for training usingsynthetic data. In particular, for Sub-Task 1.1, a much highernegative GAP value is observed in this 2nd edition (i.e., -5.31%vs. -0.74%). This result, together with the higher TO value,seems to be due to the generation of better synthetic databy the ID R&D team, with the proposal of novel generativemethods, as indicated before. In addition, several conclusionscan be drawn. First, the improvement in the main metric canbe associated with the freedom to select the methodology togenerate the synthetic data to train the FR models, as well",
  "VI. CONCLUSION": "The 2nd FRCSyn-onGoing has presented a comprehensiveexploration of the applications of synthetic data in FR, ef-fectively addressing existing limitations in the field. In this2nd edition, two additional sub-tasks have been introduced,showing that impressive results can be achieved using un-limited synthetic data, even outperforming in some cases thescenario of training with only real data. With an increasednumber of participants in this last edition, we have witnesseda considerable performance improvement in all sub-tasks incomparison to the 1st edition , . This has been possiblethanks to the proposal of novel methods to generate and selectbetter synthetic data, as well as FR models and loss functions.These approaches can be compared across a variety of sub-tasks, with many being reproducible thanks to the materialsmade available by the participating teams.Future studies will include recent AI techniques , tomake sure that only the databases available by the challengeare used by participants. We will also perform a more detailedanalysis of the results and comparison with recent challengesin the topic, such as SDFR , or evaluate over a morediverse set of databases that include other FR challenges, likequality, surveillance, or large distance images . Finally,we plan to focus on the explainability of these FR modelsand the frameworks that generate synthetic images . Debi-asing face recognition models using synthetic datasets is animportant task and in this challenge, we found that the use ofsynthetic data can furhter increase the performance. However,the concept of bias itself is complex and often subjective.What constitutes a fair representation can vary significantlydepending on cultural context, individual experiences, andeven personal beliefs. Therefore, debiasing efforts should beapproached with an accurate understanding of the multifacetednature of bias. Simply generating synthetic data to reflect aparticular demographic distribution might not fully address thecomplexities of real-world inequalities. Rather than seeking toeliminate bias, perhaps a more productive approach is to pur-sue research in the direction of transparency, interpretability,and controllability. This translates to research questions thatallow researchers to easily define what bias should be andallow them to fine-tune their models accordingly. Ultimately,the goal should be to develop face recognition systems thatare not only accurate but also fair and ethical.",
  "This study has received funding from the European Unions Horizon2020 TReSPAsS-ETN (No 860813) and is supported by INTER-ACTION": "(PID2021-126521OB-I00 MICINN/FEDER), Catedra ENIA UAM-VERIDASen IA Responsable (NextGenerationEU PRTR TSI-100927-2023-2), R&DAgreement DGGC/UAM/FUAM for Biometrics and Cybersecurity, and Pow-erAI+ (SI4/PJI/2024-00062, funded by Comunidad de Madrid through thegrant agreement for the promotion of research and technology transfer at UAM). It is also supported by the German Federal Ministry of Education andResearch and the Hessian Ministry of Higher Education, Research, Science,and the Arts within their joint support of the National Research Center forApplied Cybersecurity ATHENE. K-IBS-DS was supported by the Institutefor Basic Science, Republic of Korea (IBS-R029-C2). UNICA-IGD-LSI was",
  "H. Du, H. Shi, D. Zeng, X.-P. Zhang, and T. Mei, The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances, ACMComput. Surv., vol. 54, no. 10s, pp. 142, 2022. 1": "C. Bisogni, A. Castiglione, S. Hossain, F. Narducci, and S. Umer,Impact of Deep Learning Approaches on Facial Expression Recognitionin Healthcare Industries, IEEE Transactions on Industrial Informatics,vol. 18, no. 8, pp. 56195627, 2022. 1 L. F. Gomez, A. Morales, J. R. Orozco-Arroyave, R. Daza, and J. Fierrez,Improving Parkinson Detection Using Dynamic Features From EvokedExpressions in Video, in Proc. IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, 2021. 1 R. Daza, L. F. Gomez, A. Morales, J. Fierrez, R. Tolosana, R. Cobos, andJ. Ortega-Garcia, MATT: Multimodal Attention Level Estimation for e-learning Platforms, in Proc. AAAI Workshop on Artificial Intelligencefor Education, 2023. 1 J. Deng, J. Guo, N. Xue, and S. Zafeiriou, ArcFace: Additive AngularMargin Loss for Deep Face Recognition, in Proc. IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, 2019. 1, 5, 6, 7, 10,13",
  "M. Kim, A. K. Jain, and X. Liu, AdaFace: Quality Adaptive Margin forFace Recognition, in Proc. IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2022. 1, 2, 5, 6, 7, 8, 10, 13": "I. Deandres-Tame, R. Tolosana, R. Vera-Rodriguez, A. Morales, J. Fier-rez, and J. Ortega-Garcia, How Good Is ChatGPT at Face Biometrics?A First Look Into Recognition, Soft Biometrics, and Explainability,IEEE Access, vol. 12, pp. 34 39034 401, 2024. 1, 14 C. R. Crum, P. Tinsley, A. Boyd, J. Piland, C. Sweet, T. Kelley,K. Bowyer, and A. Czajka, Explain To Me: Salience-Based Explain-ability for Synthetic Face Detection Models, IEEE Transactions onArtificial Intelligence, pp. 112, 2023. 1 Y. Shen, C. Yang, X. Tang, and B. Zhou, InterFaceGAN: Interpretingthe Disentangled Face Representation Learned by GANs, IEEE Trans-actions on Pattern Analysis and Machine Intelligence, vol. 44, no. 4,pp. 20042018, 2022. 1 P. Terhorst, J. N. Kolf, M. Huber, F. Kirchbuchner, N. Damer, A. M.Moreno, J. Fierrez, and A. Kuijper, A Comprehensive Study onFace Recognition Biases Beyond Demographics, IEEE Transactionson Technology and Society, vol. 3, no. 1, pp. 1630, 2021. 1 P. Melzi, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, A. Morales,D. Lawatsch, F. Domin, and M. Schaubert, Synthetic Data for theMitigation of Demographic Biases in Face Recognition, in Proc. IEEEInternational Joint Conference on Biometrics, 2023. 1 A. Morales, J. Fierrez, R. Vera-Rodriguez, and R. Tolosana, Sensi-tiveNets: Learning Agnostic Representations with Application to FaceImages, IEEE Transactions on Pattern Analysis and Machine Intelli-gence, vol. 43, no. 6, pp. 21582164, 2021. 1 P. Melzi, H. O. Shahreza, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez,J. Fierrez, S. Marcel, and C. Busch, Multi-IVE: Privacy Enhancementof Multiple Soft-Biometrics in Face Embeddings, in Proc. IEEE/CVFWinter Conference on Applications of Computer Vision Workshops,2023. 1",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202115": "S. P. Mudunuri and S. Biswas, Low Resolution Face RecognitionAcross Variations in Pose and Illumination, IEEE Transactions onPattern Analysis and Machine Intelligence, vol. 38, no. 5, pp. 10341040, 2016. 1 H. Qiu, D. Gong, Z. Li, W. Liu, and D. Tao, End2End Occluded FaceRecognition by Masking Corrupted Features, IEEE Transactions onPattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 69396952, 2022. 1 P. Melzi, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, D. Lawatsch,F. Domin, and M. Schaubert, GANDiffFace: Controllable Generationof Synthetic Datasets for Face Recognition with Realistic Variations,in Proc. IEEE/CVF International Conference on Computer Vision Work-shops, 2023. 2, 3, 6, 8, 10, 13",
  "F. Boutros, V. Struc, J. Fierrez, and N. Damer, Synthetic Data for FaceRecognition: Current State and Future Prospects, Image and VisionComputing, vol. 135, p. 104688, 2023. 1": "I. Joshi, M. Grimmer, C. Rathgeb, C. Busch, F. Bremond, andA. Dantcheva, Synthetic Data in Human Analysis: A Survey, IEEETransactions on Pattern Analysis and Machine Intelligence, vol. 46,no. 7, pp. 49574976, 2024. 1 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,S. Ozair, A. Courville, and Y. Bengio, Generative Adversarial Nets,in Proc. Advances in Neural Information Processing Systems, 2014. 2",
  "H. Qiu, B. Yu, D. Gong, Z. Li, W. Liu, and D. Tao, SynFace: FaceRecognition with Synthetic Data, in Proc. IEEE/CVF InternationalConference on Computer Vision, 2021. 2, 3": "F. Boutros, M. Huber, P. Siebke, T. Rieber, and N. Damer, SFace:Privacy-friendly and Accurate Face Recognition using Synthetic Data,in Proc. IEEE International Joint Conference on Biometrics, 2022. 2, 3 M. Kim, F. Liu, A. Jain, and X. Liu, DCFace: Synthetic Face Gen-eration with Dual Condition Diffusion Model, in Proc. IEEE/CVFConference on Computer Vision and Pattern Recognition, 2023. 2, 3, 5,6, 7, 8, 9, 10, 13 F. Boutros, J. H. Grebe, A. Kuijper, and N. Damer, IDiff-Face:Synthetic-based Face Recognition through Fizzy Identity-ConditionedDiffusion Model, in Proc. IEEE/CVF International Conference onComputer Vision, 2023. 2, 3, 5, 6, 9, 10, 13 M. Kansy, A. Rael, G. Mignone, J. Naruniec, C. Schroers, M. Gross, andR. M. Weber, Controllable Inversion of Black-Box Face RecognitionModels via Diffusion, in Proc. IEEE/CVF International Conference onComputer Vision Workshops, 2023. 2, 3 G. Bae, M. de La Gorce, T. Baltrusaitis, C. Hewitt, D. Chen, J. Valentin,R. Cipolla, and J. Shen, DigiFace-1M: 1 Million Digital Face Imagesfor Face Recognition, in Proc. IEEE/CVF Winter Conference on Ap-plications of Computer Vision, 2023. 2, 3, 8, 13",
  "C. Zhang, X. Chen, S. Chai, C. H. Wu, D. Lagun, T. Beeler, andF. De la Torre, ITI-GEN: Inclusive Text-to-Image Generation, in Proc.IEEE/CVF International Conference on Computer Vision, 2023. 2, 3": "A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,Learning Transferable Visual Models From Natural Language Super-vision, in Proc. 38th International Conference on Machine Learning,2021. 2, 3 P. Melzi, R. Tolosana, R. Vera-Rodriguez et al., FRCSyn-onGoing:Benchmarking and Comprehensive Evaluation of Real and SyntheticData to Improve Face Recognition Systems, Information Fusion, vol.107, p. 102322, 2024. 2, 3, 4, 13, 14",
  "K. He, X. Zhang, S. Ren, and J. Sun, Deep Residual Learning for ImageRecognition, in Proc. IEEE/CVF Conference on Computer Vision andPattern Recognition, 2016. 2, 5, 6, 9, 10, 13": "I. DeAndres-Tame, R. Tolosana, P. Melzi, R. Vera-Rodriguez, M. Kim,C. Rathgeb, X. Liu, A. Morales, J. Fierrez, J. Ortega-Garcia, Z. Zhong,Y. Huang, Y. Mi, S. Ding, S. Zhou, S. He, L. Fu, H. Cong, R. Zhang,Z. Xiao, E. Smirnov, A. Pimenov, A. Grigorev, D. Timoshenko, K. M.Asfaw, C. Y. Low, H. Liu, C. Wang, Q. Zuo, Z. He, H. O. Shahreza,A. George, A. Unnervik, P. Rahimi, S. Marcel, P. C. Neto, M. Huber,J. N. Kolf, N. Damer, F. Boutros, J. S. Cardoso, A. F. Sequeira, A. Atzori,G. Fenu, M. Marras, V. Struc, J. Yu, Z. Li, J. Li, W. Zhao, Z. Lei,X. Zhu, X.-Y. Zhang, B. Biesseck, P. Vidal, L. Coelho, R. Granada, andD. Menotti, Second Edition FRCSyn Challenge at CVPR 2024: Face",
  "Recognition Challenge in the Era of Synthetic Data, in Proc. IEEE/CVFConference on Computer Vision and Pattern Recognition, 2024. 2": "T. Karras, M. Aittala, S. Laine, E. Harkonen, J. Hellsten, J. Lehtinen,and T. Aila, Alias-Free Generative Adversarial Networks, Advancesin Neural Information Processing Systems, vol. 34, pp. 852863, 2021.3 N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman,DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, in Proc. IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2023. 3 E. Wood, T. Baltrusaitis, C. Hewitt, S. Dziadzio, T. J. Cashman, andJ. Shotton, Fake It Till You Make It: Face Analysis in the Wild UsingSynthetic Data Alone, in Proc. IEEE/CVF International Conference onComputer Vision, 2021. 3 Y. Deng, J. Yang, D. Chen, F. Wen, and X. Tong, Disentangledand Controllable Face Image Generation via 3D Imitative-ContrastiveLearning, in Proc. IEEE/CVF Conference on Computer Vision andPattern Recognition, 2020. 3",
  "M. Wang and W. Deng, Mitigating Bias in Face Recognition UsingSkewness-Aware Reinforcement Learning, in Proc. IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, 2020. 3, 4, 5, 10,11": "S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, andS. Zafeiriou, AgeDB: The First Manually Collected, In-The-Wild AgeDatabase, in Proc. IEEE/CVF Conference on Computer Vision andPattern Recognition Workshops, 2017. 3, 4, 5, 10, 11 S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, andD. W. Jacobs, Frontal to Profile Face Verification in the Wild, inProc. IEEE/CVF Winter Conference on Applications of Computer Vision,2016. 3, 4, 5, 10, 11",
  "I. C. Duta, L. Liu, F. Zhu, and L. Shao, Improved Residual Networks forImage and Video Recognition, in Proc. 25th International Conferenceon Pattern Recognition, 2021. 5, 6, 7, 8, 9, 10, 13": "R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,High-Resolution Image Synthesis With Latent Diffusion Models, inProceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, 2022. 5, 6 F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper, ElasticFace:Elastic Margin Loss for Deep Face Recognition, in Proc. IEEE/CVFConference on Computer Vision and Pattern Recognition Workshops,2022. 6, 7, 9",
  "J. Song, C. Meng, and S. Ermon, Denoising Diffusion Implicit Models,in Proc. International Conference on Learning Representations, 2021.6, 7": "Z. Li, M. Cao, X. Wang, Z. Qi, M.-M. Cheng, and Y. Shan, Pho-toMaker: Customizing Realistic Human Photos via Stacked ID Embed-ding, in Proc. IEEE/CVF Conference on Computer Vision and PatternRecognition, 2024. 6, 7 K. Crowson, S. A. Baumann, A. Birch, T. M. Abraham, D. Z.Kaplan, and E. Shippole, Scalable High-Resolution Pixel-Space Im-age Synthesis with Hourglass Diffusion Transformers, arXiv preprintarXiv:2401.11605, 2024. 6, 7, 13",
  "H. O. Shahreza, A. George, and S. Marcel, SynthDistill: Face Recog-nition with Knowledge Distillation from Synthetic Data, in Proc. IEEEInternational Joint Conference on Biometrics, 2023. 6, 8, 9, 13": "F. Boutros, M. Klemt, M. Fang, A. Kuijper, and N. Damer, ExFace-GAN: Exploring Identity Directions in GANs Learned Latent Space forSynthetic Identity Generation, in IEEE International Joint Conferenceon Biometrics, 2023. 6, 9 X. An, X. Zhu, Y. Gao, Y. Xiao, Y. Zhao, Z. Feng, L. Wu, B. Qin,M. Zhang, D. Zhang, and Y. Fu, Partial FC: Training 10 MillionIdentities on a Single Machine, in Proc. IEEE/CVF InternationalConference on Computer Vision Workshops, 2021. 7 Z. Zhu, G. Huang, J. Deng, Y. Ye, J. Huang, X. Chen, J. Zhu, T. Yang,J. Lu, D. Du, and J. Zhou, WebFace260M: A Benchmark Unveilingthe Power of Million-Scale Deep Face Recognition, in Proc. IEEE/CVFConference on Computer Vision and Pattern Recognition, 2021. 7, 9 A. Tong, K. FATRAS, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, G. Wolf, and Y. Bengio, Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport, Transac-tions on Machine Learning Research, 2024. 7",
  "E. Smirnov, N. Garaev, V. Galyuk, and E. Lukyanets, PrototypeMemory for Large-Scale Face Representation Learning, IEEE Access,vol. 10, pp. 12 03112 046, 2022. 7": "J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou, RetinaFace:Single-Shot Multi-Level Face Localisation in the Wild, in Proc.IEEE/CVF Conference on Computer Vision and Pattern Recognition,2020. 8 T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,Analyzing and Improving the Image Quality of StyleGAN, in Proc.IEEE/CVF Conference on Computer Vision and Pattern Recognition,2020. 9 P. C. Neto, E. Caldeira, J. S. Cardoso, and A. F. Sequeira, CompressedModels Decompress Race Biases: What Quantized Models Forget forFair Face Recognition, in Proc. International Conference of the Bio-metrics Special Interest Group, 2023. 9 P. C. Neto, F. Boutros, J. R. Pinto, N. Damer, A. F. Sequeira,J. S. Cardoso, M. Bengherabi, A. Bousnat, S. Boucheta, N. Hebbadj,M. E. Erakn, U. Demir, H. K. Ekenel, P. B. De Queiroz Vidal, andD. Menotti, OCFR 2022: Competition on Occluded Face Recognitionfrom Synthetically Generated Structure-Aware Occlusions, in Proc.IEEE International Joint Conference on Biometrics, 2022. 9 P. C. Neto, R. M. Mamede, C. Albuquerque, T. Goncalves, and A. F.Sequeira, Massively Annotated Datasets for Assessment of Syntheticand Real Data in Face Recognition, arXiv preprint arXiv:2404.15234,2024. 9 A. Atzori, F. Boutros, N. Damer, G. Fenu, and M. Marras, If Its NotEnough, Make It So: Reducing Authentic Data Demand in Face Recog-nition through Synthetic Faces, in Proc. 18th International Conferenceon Automatic Face and Gesture Recognition, 2024. 9",
  "M. L. Ngan, P. J. Grother, and K. K. Hanaoka, OnGoing FaceRecognition Vendor Test (FRVT) Part 6b: Face Recognition Accuracywith Face Masks Using Post-Covid-19 Algorithms, 2020. 10": "D. DeAlcala, A. Morales, G. Mancera, J. Fierrez, R. Tolosana, andJ. Ortega-Garcia, Is my Data in your AI Model? Membership InferenceTest with Application to Face Images, arXiv preprint arXiv:2402.09225,2024. 14 H. O. Shahreza, C. Ecabert, A. George, A. Unnervik, S. Mar-cel, N. D. Domenico, G. Borghi, D. Maltoni, F. Boutros, J. Vo-gel, N. Damer, Angela Sanchez-Perez, EnriqueMas-Candela, J. Calvo-Zaragoza, B. Biesseck, P. Vidal, R. Granada, D. Menotti, I. DeAndres-Tame, S. M. L. Cava, S. Concas, P. Melzi, R. Tolosana, R. Vera-Rodriguez, G. Perelli, G. Orr`u, G. L. Marcialis, and J. Fierrez, SDFR:Synthetic Data for Face Recognition Competition, in Proc. 18th IEEEInternational Conference on Automatic Face and Gesture Recognition,2024. 14 F. Liu, R. Ashbaugh, N. Chimitt, N. Hassan, A. Hassani, A. Jaiswal,M. Kim, Z. Mao, C. Perry, Z. Ren, Y. Su, P. Varghaei, K. Wang,X. Zhang, S. Chan, A. Ross, H. Shi, Z. Wang, A. Jain, and X. Liu,FarSight: A Physics-Driven Whole-Body Biometric System at LargeDistance and Altitude, in Proc. IEEE/CVF Winter Conference onApplications of Computer Vision, 2024. 14"
}