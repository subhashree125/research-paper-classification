{
  "Abstract": "While diffusion models have significantly advanced thequality of image generation, their capability to accuratelyand coherently render text within these images remains asubstantial challenge. Conventional diffusion-based meth-ods for scene text generation are typically limited by theirreliance on an intermediate layout output. This dependencyoften results in a constrained diversity of text styles andfonts, an inherent limitation stemming from the determinis-tic nature of the layout generation phase. To address thesechallenges, this paper introduces SceneTextGen, a noveldiffusion-based model specifically designed to circumventthe need for a predefined layout stage. By doing so, Scene-TextGen facilitates a more natural and varied representa-tion of text. The novelty of SceneTextGen lies in its inte-gration of three key components: a character-level encoderfor capturing detailed typographic properties, coupled witha character-level instance segmentation model and a word-level spotting model to address the issues of unwanted textgeneration and minor character inaccuracies. We validatethe performance of our method by demonstrating improvedcharacter recognition rates on generated images across dif-ferent public visual text datasets in comparison to both stan-dard diffusion based methods and text specific methods. 1",
  ". Introduction": "The text-to-image task has gained popularity withadvancements in diffusion models , significantlyenhancing the quality of image generation. However, seam-lessly integrating clear and contextually appropriate textinto images is a persistent challenge.Text plays a vitalrole in many domains, including media content creation andartistic design, yet current diffusion models often struggle toproduce text that is lexically correct, in valid font style, andthat naturally complements the overall image aesthetics.Traditional methods of creating scene text",
  "(b) Layout-Agnostic SceneTextGen": ". Models reliant on predefined text layouts for input ex-hibit limitations such as constrained font diversity and static textpositioning during each inference, leading to a lack of variabilityin style and arrangement. images typically formulate this problem as scene text edit-ing which only involves editing or adding text to an ex-isting scene image. These methods have challenges han-dling complex backgrounds, font styles and lighting vari-ations. While recent models have madestrides in addressing these limitations by enhancing text en-coding strategies or employing predefined text layouts, they still face significant constraints in generating visualtext. These constraints become apparent when observingthe limited diversity in font styles and the static positioningof text, as shown in . Such rigidity in layout and fontselection hampers the capacity of generative models to pro-duce text that is stylistically varied and contextually alignedwith the image content.To address these issues, we propose SceneTextGen, anovel framework that capitalizes on the capabilities of la-tent diffusion models to infuse text into scene images withgreater diversity and authenticity. Our approach is specifi-cally engineered to transcend the limitations of predefinedlayouts, enabling more flexible text placement and an ex-pansive assortment of text styles.The two primary contributions of this work are: (i) theintegration of character-level encoder to capture the typo-graphic properties of visual text and carefully injecting it",
  "arXiv:2406.01062v5 [cs.CV] 15 Sep 2024": "into the cross attention layers of the diffusion model, and (ii)introducing a novel word spotting loss using a pre-trainedOCR along with a character segmentation loss to make thenetwork more faithful in generating visual text. The charac-ter level encoder naturally blends with the existing diffusionmodel architecture and therefore allows the network to learnthe layout of visual text implicitly rather than constrainingitself to some pre-defined form. Our comprehensive evalu-ations confirm that SceneTextGen surpasses contemporarymethods, facilitating the generation of images with text thatis both aesthetically pleasing and rich in variety.",
  ". Text 2 Image Generation Models": "In the realm of text-to-image generation, diffusion mod-els represent a sig-nificant leap forward.Different from GAN-based mod-els , diffusion models employ a stochastic pro-cess that iteratively adds noise to an image and learn toreverse this process to generate images from textual de-scriptions.Their capacity to produce high-quality, de-tailed visual content from text prompts has been well doc-umented. The Latent Diffusion Model (LDM) furtherenhances this approach by operating in a compressed im-age latent space, improving both efficiency and image qual-ity. It also facilitates high-quality conditional generationthrough cross-attention text conditioning, leading to vari-ous downstream applications . DALL-E, renowned for its novel approach of combining discreteVAEs with transformer language models, has shown re-markable ability in generating diverse and complex imagesfrom textual descriptions, showcasing the potential of trans-former architectures in creative generative tasks. Deepfloyd, utilizing the robust T5 text encoders , not onlyenhances image quality but also facilitates nuanced under-standing of complex prompts, thereby allowing for moreaccurate and context-aware visual representations. Control-Net , has demonstrated exceptional performance in con-ditioned image generation by providing the model with ref-erences such as skeleton, canny edge images, and segmen-tation maps. However, these models frequently encounterdifficulties when it comes to the generation of text withinimages (see ), a task requiring the text to be notonly visually integrated but also contextually pertinent tothe image content. This challenge stems from the complex-ity of modeling the fine-grained interplay between visualand textual elements, ensuring that the generated text is leg-ible, aesthetically fitting, and semantically in sync with theimage. Prior attempts to refine this aspect have led to im-provements , yet the generation of contextuallycoherent text in images remains a largely unsolved problem,underscoring the need for more focused research.",
  ". Scene Text Generation": "The success of adversarial networks such as GANs for image generation and style transfer gave rise to scene text generation methods which can gen-erate text at the granularity of glyphs or individualwords . Many previous works focused on theparticular task of scene text editing where the model learnsa style from a reference image and renders the target contentin that style. Methods such as SRNet , SWAPText decomposes the problem into: (1) learning the foregroundtext using style and content, (2) background in-painting net-work to remove existing text, and (3) a blending networkto merge foreground and background. These methods of-ten fail in learning the correct style and removing existingtext from complex backgrounds. TextStyleBrush pro-poses a self-supervised approach to disentangle content andstyle and generate word images in a one-shot manner. Allthese previous methods are limited to generating individualwords, requires reference word style images and does notgeneralize to generate the entire scene text image.Spatial fusion GAN (SF-GAN) generates text im-ages by superimposing a foreground content image, trans-formed to match the style and geometry of a backgroundimage. This approach is aimed more at pure text synthesison image regions without text, whereas in this work we aimto generate both image and text in a manner that reflects itsnatural appearance in real-world contexts.With the significant progress of diffusion models in text-to-image generation, recent methods in scene text genera-tion adapt these models towards producing more legible vi-sual text. DIFFSTE enhances scene text editing witha dual encoder design in diffusion models, promoting textlegibility and style control. It is adept at mapping text in-structions to images, showcasing zero-shot capabilities forrendering text in novel font variations and interpreting infor-mal natural language instructions. This method is however ascene text editing method which generates single keywordson specific regions defined by a mask, whereas we proposea scene text generation network. One of the closest workin this space is TextDiffuser which address the problemof scene text generation by decomposing the problem intotwo-stages. In the first stage, a transformer model createthe layout mask for keywords extracted from text prompts.This is then taken as condition data while formulating thediffusion model to generate scene text images. They alsointroduce the character segmentation loss which helps ingenerating legible text. ControlNet , though not orig-inally designed for scene text image rendering, has been ef-fectively adapted for this purpose. It utilizes canny edgemaps as conditional inputs, sourced from printed text im-ages generated by a layout model, to fine-tune the diffusionmodels output. The use of pre-defined layout for visual textgeneration in , seriously limits the diversity of text",
  "rounded cursive": ". Scene Text Generation. Qualitative samples of scene text images generated by our model are presented. These images containvisually appealing texts that are coherent with the background and are created without relying on any spatial information or predefinedlayouts as input, thereby enhancing the Text-to-Image (T2I) Diffusion Models capability to generate text. styles, fonts and even the layouts. We believe this is due tothe inherent difficulty in predicting layouts independentlywithout the general image guidance. In our work, we avoidthe need of pre-defined layout and make the network im-plicitly learn layout along with image generation using ournovel way of injecting character level features. We also in-troduce a word spotting loss which augments the charactersegmentation loss proposed in to generate more legiblevisual texts.",
  ". Scene Text Recognition": "Advances in computer vision have laid a foundation for so-phisticated analytical techniques in various domains .Especially in scenetext image recognition, most existing works split the pro-cess into two stages: a text detection module to detect words or characters from complex back-grounds, and a text recognition modulewhich transcribes the text into unicode characters givena cropped word image.More recently, end-2-end meth-ods have become popular due to thebenefits of joint training of detector and recognizer to sharecontextual information.In the nexus of scene text generation and recognition,leveraging pre-trained scene text recognition or word spot-ting models as guidance during diffusion-based text-to-image synthesis has surfaced as an innovative strategy. Forsimplicity, in this paper we refer to a scene text recogni-tion module as OCR (optical character recognition). Theintegration of OCR-derived losses enables the refinementof generative models to produce text that is not just visually coherent but also contextually accurate. This confluence ofgenerative modeling prowess with OCR accuracy paves theway for novel research avenues to generate images with textthat is both authentic to read and visually integrated.",
  ". Motivation": "Our objective is to facilitate layout-free text image genera-tion with diverse layouts and styles. A straightforward ap-proach would be to directly fine-tune an existing latent dif-fusion model with text images. However, our early inves-tigation suggests that this strategy did not yield significantimprovements compared to the original LDM. We hypothe-size that this limitation is due to two primary factors. First,the language encoder of the LDM, primarily designed forsemantic interpretation, fails to capture character-specificinformation adequately for text rendering.This encodertends to provide more semantic than structural informationabout the text, necessitating a dedicated network for encod-ing the text and guiding the model on its visual representa-tion. Second, the conventional denoising loss used in diffu-sion models seems insufficient for accurately rendering textin images, often leading to text regions resembling textualpatterns without the distinct features of text strokes. To ad-dress these challenges, we propose integrating a character-level encoder and a hierarchical cross-attention mechanismto learn character-level context information. Additionally,we introduce two auxiliary losses at the word and characterlevels to emphasize the text presentation. The subsequentsections will detail each of these components and their con-",
  ". Preliminaries": "The proposed method is trained on a corpus of visual textimages. Given an image I with textual caption c, we denotewi, where i [0, N], as the visual text (words) present inthis image. We also assume that for each word, we knowits location, given in terms of the bounding box. For allpractical purposes, we assume this can be pre-computed us-ing a pre-trained OCR detection and recognition net-work . Please note, the OCR bounding box and tran-scriptions are typically noisy. presents an overviewof our proposed method. A latent diffusion model has threekey components: a CLIP encoder for semantic embeddingof text and images, an autoencoder for dimensionality re-duction and feature extraction, and a UNet-based structurefor effective image-text synthesis and manipulation. Thecore architecture of our method follows the latent diffu-sion model, which is adapted to incorporate the proposedcharacter-level features and content-based loss functions.",
  ". Character-Level Encoding": "Our approach begins by extracting and ordering the visualtext words from the image based on its locations as givenby the bounding box.We sort the word boxes following the conventional read-ing pattern, i.e., from left to right and top to bottom. Thisprovides us the naive ordering of visual text which is thentokenized at the character level to capture its typographicinformation. Then, a character encoder is used to encodethese tokenized characters into a high-dimensional fea-ture space, allowing an accurate transcription of the textsspelling and appearance. The derived text features encap-sulate the typographical details and are poised for the sub-sequent integration with the images latent features, whichhave already been contextually primed by the initial cross-attention with the encoded caption features obtained fromthe CLIP text encoder.",
  ". Hierarchical Text Integration Process": "We have employed a sequential cross-attention mechanismto integrate character information into the U-Net architec-ture effectively. This method is inspired by the observationthat the cross-attention of captioning features is pivotal inshaping the overall spatial structure of an image, a notionsupported by prior studies . Leveraging this concept,our model initially constructs the general spatial layout ofthe image content, guided by the caption, using the firstcross-attention layer. It then focuses on the precise render-ing of characters in a subsequent cross-attention phase. Thisapproach establishes a hierarchical text integration process,facilitating the development of a preliminary visual scaf-fold that is both thematically and contextually coherent. It",
  ". OCR-Guided Diffusion for Text Accuracy": "To ensure the textual accuracy of generated images, ourmodel incorporates an OCR loss, utilizing the predictionsfrom the end to end pre-trained GLASS model . Uponeach iteration of the diffusion process, the UNet predicts adenoising step, from which we derive an estimation of cleanimage x0. This estimated x0 is then decoded through a Vari-ational Autoencoder (VAE) to reconstruct an image.The GLASS OCR model performs inference on this re-constructed image to produce word-level recognition re-sults, which are represented as a tensor P with shape[N, L, K], where N is the number of words detected, L isthe maximum length of any word, and K is the size of thecharacter set. The ground truth for these detections is rep-resented as a tensor G with shape [N, L], where each entryis the character index for the corresponding position, andnon-character positions are marked with zero.The OCR loss (LOCR) is computed using a masked cross-entropy function, which is formulated as follows:",
  "exp(Pij[Gij])Kk=1 exp(Pij[k])": "(1)Here, M is a binary mask tensor that has the same shapeas G and indicates the valid character positions (i.e., whereGij = 0). Mij represents the binary value of the mask at thei-th word and j-th character position, Pij is the predictedprobability distribution over the character set, and Gij isthe ground truth character index at that position.By integrating this OCR loss into the training regime, weguide the diffusion model to produce text that is not only vi-sually coherent but also textually accurate, as recognized bythe GLASS OCR model, thereby enhancing the overallfidelity of the generated images.",
  ". Refinement of Text Generation with Character-Level Constraints": "During the iterative refinement of our diffusion model, weobserved an unintended consequence of the OCR loss; themodel tended to generate images with repetitive words.This issue is potentially attributed to the blurry nature of im-ages at higher noise levels during training, which could ren-der the OCR loss counterproductive. The word-level OCRloss, while ensuring textual accuracy, imposes no explicitconstraint on the quantity of text within the image, inadver-tently encouraging the model to generate excessive text.To address this, we augmented our loss function witha character-level segmentation loss, which acts directly on",
  "KV : OCR Text Features": ". Model Framework: SceneTextGen employs a character-level encoder to extract detailed character-specific features. During losscomputation, the model leverages both word-level and character-level supervisions to guide the recovery of the image, in addition to thestandard denoising loss. This dual-level supervision enhances the models ability to accurately generate and refine text within scenes. the latent space rather than the recovered image. After ob-taining the predicted latent features of a image x0 from theUNet, we proceed in two directions: we decode x0 usingthe VAE to compute the word-level OCR loss on the re-covered image (as explained earlier), and we also apply x0to a pre-trained character-level segmentation model basedon U-Net adapted from.This model outputs a 96-dimensional feature map (corresponding to the length ofthe alphabet plus one for non-character pixels) with a spa-tial resolution of 6464. The character-aware loss is thencomputed via cross-entropy between this feature map and aresized character-level segmentation mask C.Thus, the total loss function is a composite of the denois-ing loss, the word-level recognition loss, and the character-level segmentation loss, expressed as:",
  ". Implementation Details": "DatasetsFor the training of our model, we utilizedthe publicly available MARIO dataset from , exclud-ing MARIO-TMDB, and MARIO-OpenLibrary subsets asthey are not publicly accessible.Upon removing anycorrupted images, our final dataset comprised 7,249,449image-caption pairs. In addition, to bolster our models ca-pability in generating a broader range of concepts (beyondtext-centric images), we integrated 2,110,745 non-text im-ages. These additional images, accompanied by text pairs,were selected based on a minimum predicted aestheticsscore of 6.25, allowing for joint training to enhance over-all performance. BaselinesWe conducted quantitative comparisons of ourSceneTextGen method against several leading approaches,includingLDM,ControlNet,TextDiffuser,GlyphControl, and DeepFloyd, utilizing the pub-licly available code and pre-trained models for fairness.Notably, DeepFloyd is distinguished by its dual super-resolution modules, enabling it to produce high-resolutionimages at 1024x1024 pixels, in contrast to the 512x512pixel images generated by the other models. For Control-Net comparisons, we employed Canny edge maps of printedtext images created by the initial model stage of TextDif-fuser as conditioning inputs. However, due to the unavail-ability of APIs, open-source code or checkpoints, we couldnot extend our comparative analysis to include Imagen,eDiff-i, or GlyphDraw. Evaluation CriteriaWe assess text rendering qual-ity using the MARIO-7M-Eval, MARIO-TMDB-Eval, andMARIO-OpenLibrary-Eval datasets.Our evaluation istwofold: firstly, through CLIPScore, which measures thecosine similarity between the image and text representa-tions from CLIP; and secondly, via OCR Evaluation, whichleverages existing OCR tools to detect and recognize textregions within the generated images. Metrics such as Av-erage Precision, Average Recall, F1 Score, and Accuracyare employed to determine the presence of keywords in thegenerated images. During training, the input of the charac-ter encoder is the ground truth OCR texts. During the in-ference process, the character encoder receives as its inputthe text from captions provided by the user. These captionsare enclosed in quotation marks as specified in the usersprompts, and the input of the CLIP text encoder is the fullcaption. For each generated and ground truth image pair, weutilize the easy-ocr library for OCR detection and recogni-tion, followed by Hungarian Matching between the sets oftexts, applying the Levenshtein distance (or edit distance)on the matched text pairs for the OCR evaluation.",
  ": end for9: return AVERAGE(Scores)": "Pseudo Code for OCR Performance EvaluationThemethodology described in Algorithm 1 demonstrates thesteps taken to assess the performance of the text-to-imageconversion. In the initial step, the algorithm applies OCR toboth the ground truth and the generated images. This pro-cess results in two sets of text outputs, which are then pairedfor comparison. The Hungarian algorithm is employed hereto find the optimal matching between elements (words) ofthese two sets, minimizing the overall difference betweenthe matched pairs. This is crucial for an objective and accu-rate comparison. For each matched pair, we calculate a costmatrix, which serves as the input for the Hungarian algo-rithm. The output of this step is a matching matrix M which represents the best possible alignment between the text el-ements in the ground truth and the generated images. Sub-sequently, the algorithm computes key performance metricsfor each pair: Precision, Recall, F1 Score, and Accuracy.Precision focuses on the accuracy of the replicated text, re-call measures the completeness, F1 score provides a balancebetween precision and recall, and accuracy gives an overalleffectiveness of the text replication. Finally, the algorithmaverages these scores across all image pairs to provide anoverall performance evaluation of the text-to-image conver-sion process.",
  ". Quantitative Results": "Our experimental analysis in Tab. 1 provides a direct com-parison of the OCR based recognition scores among variousmodels as measured in terms of Precision, Recall, F1 scores,and Accuracy. Our results indicate that SceneTextGen con-sistently outperforms competing models in most metrics.Latent Diffusion Model, lacking a sophisticated mechanismfor text comprehension, typically under-performs, leadingto lower OCR scores. In contrast, DeepFloyd incor-porates a T5 encoder which aids in textual understanding,thereby enhancing the quality of the generated text. How-ever, its performance is still limited due to an insufficientcharacter-level understanding.ControlNet, TextDiffuser, and GlyphControl,which utilize predefined text layouts or spatial information,show mixed results. While the explicit introduction of textinformation allows ControlNet to achieve high OCR scores,the resulting text often appears artificial and lacks seamlessintegration within the images (see ). Cross-dataset Generalization AbilityAs demonstratedin Tab. 1, SceneTextGen-7M, despite being trained solelyon the MARIO-7M dataset, exhibits strong generalizationcapabilities. It maintains robust OCR scores across evalu-ation sets from both the TMDB and OpenLibrary datasets,underscoring its adaptability and the efficacy of its trainingmethodology.",
  ". Measuring Font Style Diversity": "To quantitatively assess the diversity of font styles gener-ated by SceneTextGen, we utilized a pretrained VGG-basedfont recognition model trained on synthesized text images.This approach involved first extracting text image patchesusing a pretrained Optical Character Recognition (OCR)model. These patches were then processed through the fontrecognition model to retrieve features from the penultimatelayer. By applying t-SNE for dimensionality reduction, wevisualized the feature space to examine the proximity anddiversity of text generated by different methods.As illustrated in , the rendered text images which serve as a baseline were created by printing text",
  "TextDiffuser-10M 0.61350.42890.46830.34250.44010.42430.39940.28890.56170.38160.42420.2916GlyphControl-10M* 0.50750.41180.41400.28830.36350.40820.34570.23620.47340.37620.38360.2534": "Latent Diffusion Model0.14820.16900.12960.07530.17170.25790.17030.09740.19110.28730.19230.1106DeepFloyd 0.24670.27880.22060.13660.22840.37380.24490.15000.25550.39100.26350.1610ControlNet 0.51020.44440.42380.29810.30750.46670.32840.21940.40500.42730.36900.2415TextDiffuser-7M 0.47780.34470.36820.25120.31980.33620.29610.19300.42570.31460.33180.2108SceneTextGen-7M(Ours)0.52740.44200.44240.30880.38130.47160.37900.26020.41360.45190.39450.2571 . Comparative analysis of OCR based text recognition scores across different models. Note, 7M denotes models that were trained onthe MARIO-7M dataset (7 million images with texts). In contrast, the TextDiffuser-10M category includes models trained on an expandeddataset collection that encompasses MARIO-7M, MARIO-TMDB, and MARIO-OpenLibrary. * denotes LAION-Glyph dataset.",
  "LDMLDM-X": ". Comparative visualization of generated images. We present a side-by-side comparison of images generated from the same textprompt across different existing methods (with results generated by as a proxy. Human faces are blurred for ethical considerations.).Each row corresponds to a unique prompt, showcasing the visual quality, text clarity, and contextual coherence achieved by each method. onto a white canvas using the layout generator from withthe default Arial font. This process establishes a refer-ence for the feature distribution of Artificial Texts. Con-trolNet , constrained by predefined text canny edges,exhibits a feature distribution closely aligned with the ren-dered text images. This proximity suggests its limitationsin integrating text seamlessly within images, a finding cor-roborated by the qualitative results shown in . TextD-iffuser , although performing well in most instances, stillshares some feature space with ControlNet, indicating oc-casional production of artificial or unnatural texts. In con-trast, SceneTextGen operating independently of any pre-defined canny edges or layouts demonstrates a distinct distribution with minimal overlap with the rendered textimages, signifying its robustness in generating naturalistictext within the context of scene images. Example visual-izations are in . In addition, to better understand thetext layout given by each model, we also show in theoverall distribution of visual text in the generated imagesand ground truth images.",
  ". Conclusion": "SceneTextGen, incorporating a character-level encoder andhierarchical text integration, offers advancements in scenetext image generation.Despite improved text rendering,limitations arise in generating complex visuals and handlinglengthy text. These challenges highlight the ongoing dif-ficulty in reconciling textual accuracy with broader imagesynthesis. This work furthers our understanding of text-image generation, paving the way for future exploration.",
  "AcknowledgementsThis research project has been par-tially funded by research grants to Dimitris N. Metaxasthrough NSF: 2310966, 2235405, 2212301, 2003874, andFA9550-23-1-0417": "Samaneh Azadi, Matthew Fisher, Vladimir G Kim, ZhaowenWang, Eli Shechtman, and Trevor Darrell.Multi-contentgan for few-shot font style transfer. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 75647573, 2018. 2 Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park,Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwal-suk Lee. What is wrong with scene text recognition modelcomparisons? dataset and model analysis. In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 47154723, 2019. 3 Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun,and Hwalsuk Lee. Character region awareness for text de-tection.In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 93659374,2019. 3 Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-imagediffusion models with an ensemble of expert denoisers. arXivpreprint arXiv:2211.01324, 2022. 1, 5",
  "Darwin Bautista and Rowel Atienza. Scene text recognitionwith permuted autoregressive sequence models. In EuropeanConference on Computer Vision, pages 178196. Springer,2022. 4": "Michal Busta, Yash Patel, and Jiri Matas. E2e-mlt-an uncon-strained end-to-end method for multi-language scene text.In Computer VisionACCV 2018 Workshops: 14th AsianConference on Computer Vision, Perth, Australia, Decem-ber 26, 2018, Revised Selected Papers 14, pages 127143.Springer, 2019. 3 Qi Chang, Zhennan Yan, Mu Zhou, Di Liu, Khalid Sawalha,Meng Ye, Qilong Zhangli, Mikael Kanski, Subhi AlAref,Leon Axel, et al. Deeprecon: Joint 2d cardiac segmentationand 3d volume reconstruction via a structure-specific gener-ative method. In International Conference on Medical ImageComputing and Computer-Assisted Intervention, pages 567577. Springer, 2022. 2",
  "Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, QifengChen, and Furu Wei. Textdiffuser: Diffusion models as textpainters. arXiv preprint arXiv:2305.10855, 2023. 1, 2, 3, 5,6, 7, 8": "Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, ShaotingZhang, and Dimitris N Metaxas. A data-scalable transformerfor medical image segmentation: architecture, model effi-ciency, and benchmark. arXiv preprint arXiv:2203.00131,2022. 3 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. Advances inneural information processing systems, 27, 2014. 2 Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, BoZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-tor quantized diffusion model for text-to-image synthesis. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1069610706, 2022. 2 Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,Dimitris Metaxas, and Feng Yang. Svdiff: Compact param-eter space for diffusion fine-tuning. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 73237334, 2023. 2 Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, KunpengSong, Mengwei Ren, Ruijiang Gao, Anastasis Stathopou-los, Xiaoxiao He, Yuxiao Chen, Di Liu, Qilong Zhangli,Jindong Jiang, Zhaoyang Xia, Akash Srivastava, and Dim-itris Metaxas.Proxedit: Improving tuning-free real im-age editing with proximal guidance. In Proceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision (WACV), pages 42914301, 2024. 2 Xiaoxiao He, Chaowei Tan, Bo Liu, Liping Si, Weiwu Yao,Liang Zhao, Di Liu, Qilong Zhangli, Qi Chang, Kang Li,et al. Dealing with heterogeneous 3d mr knee images: A fed-erated few-shot learning method with dual knowledge dis-tillation.In 2023 IEEE 20th International Symposium onBiomedical Imaging (ISBI), pages 15. IEEE, 2023. 3 Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt imageediting with cross-attention control. In The Eleventh Inter-national Conference on Learning Representations, 2022. 2,4 Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,Mohammad Norouzi, and Tim Salimans. Cascaded diffusionmodels for high fidelity image generation. The Journal ofMachine Learning Research, 23(1):22492281, 2022. 2 Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei AEfros. Image-to-image translation with conditional adver-sarial networks. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 11251134,2017. 2 Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou,Zhifei Zhang, Brian Price, and Shiyu Chang.Improvingdiffusion models for scene text editing with dual encoders.arXiv preprint arXiv:2304.05568, 2023. 2",
  "Diederik P Kingma and Max Welling. Auto-encoding varia-tional bayes. arXiv preprint arXiv:1312.6114, 2013. 4": "Praveen Krishnan, Rama Kovvuri, Guan Pang, Boris Vas-silev, and Tal Hassner. Textstylebrush: Transfer of text aes-thetics from a single example. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2023. 2 Chen-Yu Lee and Simon Osindero. Recursive recurrent netswith attention modeling for ocr in the wild. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 22312239, 2016. 3 Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, YijuanLu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei.Trocr: Transformer-based optical character recognition withpre-trained models. In Proceedings of the AAAI Conferenceon Artificial Intelligence, pages 1309413102, 2023. 3 Wei Li, Yongxing He, Yanwei Qi, Zejian Li, and YongchuanTang. Fet-gan: Font and effect transfer via k-shot adaptiveinstance normalization. In Proceedings of the AAAI confer-ence on artificial intelligence, pages 17171724, 2020. 2 Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.Gligen: Open-set grounded text-to-image generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2251122521, 2023. 2 Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and XiangBai. Real-time scene text detection with differentiable bina-rization. In Proceedings of the AAAI conference on artificialintelligence, pages 1147411481, 2020. 3, 4 Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman,Shai Mazor, and R Manmatha.Scatter:selective con-text attentional scene text recognizer.In proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1196211972, 2020. 3 Di Liu, Yunhe Gao, Qilong Zhangli, Ligong Han, Xiaox-iao He, Zhaoyang Xia, Song Wen, Qi Chang, Zhennan Yan,Mu Zhou, et al. Transfusion: multi-view divergent fusionfor medical image segmentation with transformers. In In-ternational Conference on Medical Image Computing andComputer-Assisted Intervention, pages 485495. Springer,2022. 3 Di Liu, Xiang Yu, Meng Ye, Qilong Zhangli, Zhuowei Li,Zhixing Zhang, and Dimitris N Metaxas. Deformer: Inte-grating transformers with deformable models for 3d shapeabstraction from a single image.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1423614246, 2023. Di Liu, Anastasis Stathopoulos, Qilong Zhangli, Yunhe Gao,and Dimitris Metaxas. Lepard: Learning explicit part dis-covery for 3d articulated shape reconstruction. Advances inNeural Information Processing Systems, 36, 2024. 3 Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan,Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mo-hammad Norouzi, and Noah Constant.Character-awaremodels improve visual text rendering.arXiv preprintarXiv:2212.10562, 2022. 1 Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang,Yingbin Zheng, and Xiangyang Xue.Arbitrary-orientedscene text detection via rotation proposals. IEEE transac-tions on multimedia, 20(11):31113122, 2018. 3 Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu,Haonan Lu, and Xiaodong Lin.Glyphdraw: Learning todraw chinese characters in image synthesis models coher-ently. arXiv preprint arXiv:2303.17870, 2023. 5 Carlos Martn-Isla, Vctor M Campello, Cristian Izquierdo,Kaisar Kushibar, Carla Sendra-Balcells, Polyxeni Gkontra,Alireza Sojoudi, Mitchell J Fulton, Tewodros WeldebirhanArega, Kumaradevan Punithakumar, et al.Deep learn-ing segmentation of the right ventricle in cardiac mri: The",
  "m&ms challenge. IEEE Journal of Biomedical and HealthInformatics, 2023. 3": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. Glide: Towards photorealistic image generationand editing with text-guided diffusion models. arXiv preprintarXiv:2112.10741, 2021. 2 Siyang Qin, Alessandro Bissacco, Michalis Raptis, YasuhisaFujii, and Ying Xiao. Towards unconstrained end-to-end textspotting. In Proceedings of the IEEE/CVF international con-ference on computer vision, pages 47044714, 2019. 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 4 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J Liu. Exploring the limits of transfer learning witha unified text-to-text transformer. The Journal of MachineLearning Research, 21(1):54855551, 2020. 2 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.Zero-shot text-to-image generation. In International Confer-ence on Machine Learning, pages 88218831. PMLR, 2021.1, 2 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 2, 5 Roi Ronen, Shahar Tsiper, Oron Anschel, Inbal Lavi, AmirMarkovitz, and R Manmatha.Glass: Global to local at-tention for scene-text spotting. In European Conference onComputer Vision, pages 249266. Springer, 2022. 3, 4 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2250022510, 2023. 2 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. 1, 5 Lohrasb Ross Sayadi, Usama S Hamdan, Qilong Zhangli,and Raj M Vyas. Harnessing the power of artificial intelli-gence to teach cleft lip surgery. Plastic and ReconstructiveSurgeryGlobal Open, 10(7):e4451, 2022. 3 Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao,and Xiang Bai.Robust scene text recognition with auto-matic rectification. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 41684176,2016. 3 Baoguang Shi, Mingkun Yang, Xinggang Wang, PengyuanLyu, Cong Yao, and Xiang Bai. Aster: An attentional scenetext recognizer with flexible rectification. IEEE transactionson pattern analysis and machine intelligence, 41(9):20352048, 2018. 3",
  "Peng Wang, Cheng Da, and Cong Yao. Multi-granularityprediction for scene text recognition. In European Confer-ence on Computer Vision, pages 339355. Springer, 2022.3": "Song Wen, Hao Wang, Di Liu, Qilong Zhangli, and DimitrisMetaxas. Second-order graph odes for multi-agent trajectoryforecasting. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, pages 51015110,2024. 3 Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jing-tuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild.In Proceedings of the 27th ACM international conference onmultimedia, pages 15001508, 2019. 1, 2 Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, and Ani-mesh Garg. Slotdiffusion: Object-centric generative model-ing with diffusion models. Advances in Neural InformationProcessing Systems, 36:5093250958, 2023. 2 Zhaoyang Xia, Yuxiao Chen, Qilong Zhangli, Matt Huener-fauth, Carol Neidle, and Dimitris Metaxas. Sign languagevideo anonymization. In Proceedings of the LREC2022 10thWorkshop on the Representation and Processing of Sign Lan-guages: Multilingual Sign Language Resources, Marseille,France, 25 June 2022, 2022. 2 Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generativeadversarial networks. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 13161324, 2018. 1",
  "Fangneng Zhan, Hongyuan Zhu, and Shijian Lu.Spa-tial fusion gan for image synthesis.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 36533662, 2019. 1, 2": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Addingconditional control to text-to-image diffusion models.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 38363847, 2023. 2, 5, 6, 7, 8 Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris NMetaxas, and Jian Ren. Sine: Single image editing with text-to-image diffusion models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 60276037, 2023. 2 Qilong Zhangli, Jingru Yi, Di Liu, Xiaoxiao He, ZhaoyangXia, Qi Chang, Ligong Han, Yunhe Gao, Song Wen, Haim-ing Tang, et al. Region proposal rectification towards robustinstance segmentation of biological images. In InternationalConference on Medical Image Computing and Computer-Assisted Intervention, pages 129139. Springer, 2022. 3 Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, ShuchangZhou, Weiran He, and Jiajun Liang. East: an efficient andaccurate scene text detector. In Proceedings of the IEEE con-ference on Computer Vision and Pattern Recognition, pages55515560, 2017. 3 Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei AEfros.Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEEinternational conference on computer vision, pages 22232232, 2017. 2"
}