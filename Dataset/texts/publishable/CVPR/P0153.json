{
  "Abstract": "Monocular 3D lane detection has become a fundamen-tal problem in the context of autonomous driving, whichcomprises the tasks of finding the road surface and locat-ing lane markings. One major challenge lies in a flexiblebut robust line representation capable of modeling complexlane structures, while still avoiding unpredictable behav-ior. While previous methods rely on fully data-driven ap-proaches, we instead introduce a novel approach LaneCPPthat uses a continuous 3D lane detection model leverag-ing physical prior knowledge about the lane structure androad geometry. While our sophisticated lane model is ca-pable of modeling complex road structures, it also showsrobust behavior since physical constraints are incorporatedby means of a regularization scheme that can be analyti-cally applied to our parametric representation. Moreover,we incorporate prior knowledge about the road geometryinto the 3D feature space by modeling geometry-aware spa-tial features, guiding the network to learn an internal roadsurface representation. In our experiments, we show thebenefits of our contributions and prove the meaningfulnessof using priors to make 3D lane detection more robust. Theresults show that LaneCPP achieves state-of-the-art perfor-mance in terms of F-Score and geometric errors.",
  ". Introduction": "Robust and precise lane detection systems build one of themost essential components in the perception stack of au-tonomous vehicles. While some approaches utilize LiDARsensors or multi-sensor setups, the application of monoc-ular cameras has become more popular due to their lowercost and the high-resolution visual representation that pro-vides valuable information to detect lane markings.In the past, lane detection was mainly treated as a 2D de-tection task. Deep learning based methods achieved goodresults by treating the problem as a segmentation task inpixel space , used to classify andregress lanes using anchor-based representations, or as key-points on a grid structure . How-ever, due to the lack of depth information, these 2D repre-sentations fail to model lane markings and road geometry in3D space, which forms an important prerequisite for laterfunctionalities like trajectory planning. Consequently, ap-proaches for monocular 3D lane detection were introduced,which adapted lane representations for the 3D domain bymodeling vertical anchors or local segments on a grid in a Birds-Eye-View (BEV) oriented 3D-frame.A crucial topic for the application of lane detection al-gorithms in autonomous systems is safety, which requirespredictable and robust behavior in any traffic situation. Onerisk of learning-based methods is the tendency to show un-predictable behavior in cases of rarely observed scenarios.Since obtaining large amounts of data with high-quality an-notations is cumbersome and expensive, publicly available3D datasets are limited in size and accuracy. Hence, theydo not reflect the variability of real-world scenarios suffi-ciently. This makes learning-based models prone to overfit-ting, and eventually, diminishes predictability.One common way to deal with such problems is the inte-gration of prior knowledge. Physics provides us a profoundunderstanding of the 3D world, allowing us to make validassumptions about the lane structure and road surface ge-ometry. Therefore, we introduce physically motivated pri-ors into the lane detection objective to cope with the limiteddata problem and achieve robust and predictable behavior.There are certain geometric properties that should gen-erally hold for detected lane lines. For instance, we knowthat most lines progress parallel to each other, reside on asmooth surface and should not exceed certain thresholds interms of curvature and slope. However, integrating suchassumptions into prevailing discrete representations is notstraight forward as strong simplifications are necessary. Incontrast, continuous 3D lane representations directly pro-vide parametric curves using polynomials or moresophisticated B-Splines .These allow for analyticalcomputations on the curve function, which enables the inte-gration of such priors into the lane representation. By mod-eling these priors explicitly instead of learning them from data, the model can focus its full capacity on learning richerfeatures for the lane detection task.We can further use physical knowledge about the roadgeometry to support the model in learning an internal trans-formation from image features to 3D space. While methodsbased on Inverse Perspective Mapping (IPM) make false flat-ground assumptions, learning basedtransformations completely ignore road proper-ties. In contrast, integrating prior knowledge about the roadsurface allows us to model 3D features geometry-aware andhelps the network to focus on the 3D region of interest.Thus, we propose a novel 3D lane detection approachnamed LaneCPP that leverages valuable prior knowledge toachieve accurate and robust perception behavior. It intro-duces a new sophisticated continuous curve representation,which enables us to incorporate physical priors. In addition,we present a spatial transformation component for learninga physically inspired mapping from 2D image to 3D spaceproviding meaningful spatial features.Our main contributions can be summarized as follows: We propose a novel architecture for 3D lane detectionfrom monocular images using a more sophisticated flexi-ble parametric spline-based lane representation.",
  ". Related work": "Different Lane Representations.An important designchoice in deep learning based lane detection is the rep-resentation that the network uses to model lane line ge-ometry, which can be categorized as follows: 1) Pixel-wise representations, which formulate lane detection as asegmentation problem, were used mainly in 2D methods and were adopted in 3Dby SALAD combining line segmentation with depth-prediction.These representations come with high com-putational load since a large amount of parameters is re-quired.2) Grid-based approaches divide the space intocells and model lanes using local segments or key-points . 3D-LaneNet+ suggests to use localline-segments and BEV-LaneDet defines key-points ona BEV grid representation. Both depend on the grid res-olution and require costly post-processing to obtain lines.3) Anchor-based representations modellines as straight anchors with positional offsets at prede-fined locations.They are widely used in 3D detectionapproaches including 3D-LaneNet and Gen-LaneNet, which use vertical anchors in the top-view, and An- chor3DLane , introducing anchor projection with it-erative regression. Similar to grid-based representations,it requires subsequent curve-fitting to obtain smooth lines.4) Continuous curve representations in-stead directly model smooth curves without requiring costlypost-processing. While CLGO and CurveFormer use simple polynomials, 3D-SpLineNet proposes B-Splines . Since B-Splines offer local control over curvesegments, they are compatible to model complex shapeswith low-degree basis functions, while polynomials andBezier curves show global dependence and thus requirehigher degrees causing expensive computation. Although3D-SpLineNet achieves superior detection performance onsynthetic data, it unfortunately lacks flexibility as the curveformulation is limited to monotonically progressing lanes,making it hardly applicable to real-world data. To resolvethis issue, we propose a more flexible representation basedon actual 3D B-Splines. In contrast to discrete grids and an-chors, continuous representation even allow us to integrateprior knowledge in an analytical manner. Geometry Priors.Several approaches suggest to in-corporate prior knowledge into learning-based methods,e.g. by integrating invariance into the model architecture or task-specific transformations as for trajectoryplanning . In the field of lane detection, line par-allelism has been formulated as a hard constraint to resolvedepth ambiguity and determine camera parameters .Deep declarative networks offer a general frameworkto incorporate arbitrary properties as constraints, by solv-ing a constrained optimization problem in the forward pass.While such methods are appropriate when hard constraintsmust be enforced, our goal is rather to guide the networkin learning typical geometric lane properties by formulat-ing soft constraints in a regularization objective. Such aregularization only affects training and does not require re-solving an optimization problem in the forward pass, andthus, comes without additional computational cost duringinference. Following this paradigm, SGNet pro-poses to penalize the deviation of lateral distance from aconstant lane width in the IPM warped top-view, but ignoresthat the property does not hold for lines deviating from theground plane. GP presents a parallelism loss that en-forces constant distance between nearest neighbors locally,which depends on the number of anchor points. In contrast,our method presents a way to learn parallelism globally andindependent of resolutions of discrete lane representations.We propose an elegant way to learn parallelism as well asother geometry priors using analytical formulations of tan-gents and normals, which are well-defined on our continu-ous spline representation.",
  "Leveraging 3D Features.An important model com-ponent consists in the extraction of 3D features, encodingvaluable information to detect lanes along the road surface": ". Our approach: First, front-view image I is propagated through the backbone extracting multi-scale feature maps. These aretransformed to 3D using our spatial transformation and then fused to obtain a single 3D feature map. Feature pooling is applied to obtainfeatures for each line proposal that are propagated through fully connected layers to obtain the parameters for our line representation.Finally, prior knowledge is exploited to regularize the lane representation and to produce surface hypotheses for the spatial transformation. While some works predict 3D lanes directly from the front-view, e.g. by utilizing pixel-wise depth estimation or3D anchor-projection mechanisms , prevalent methodsemploy an intermediate 3D or BEV feature representationwith an internal transformation from the front-view to the3D space. 3D-LaneNet proposes to utilize IPM toproject front-view features to a flat road plane due to thespatial correlation between the warped top-view image and3D lane geometry and was adopted in several other works. However, IPM causes visual distortions inthe top-view representation when the flat road assumptionis violated. In related fields like BEV semantic segmen-tation, BEV transformations are learned via Multi-Layer-Perceptrons (MLPs) , depth prediction or transformer-based attention mechanisms . In3D lane detection, PersFormer utilizes attention betweenfront- and top-view, CurveFormer introduces dynamic3D anchors that model queries as parametric curves andBEV-LaneDet uses MLPs for the spatial transforma-tions. However, these learned transformations do not nec-essarily provide a 3D feature representation since they arenot guided by valuable priors about the road surface ge-ometry, which potentially results in unforeseen behaviorfor out-of-distribution data. Our approach instead aims forcarefully modeling a geometry-aware feature space usinga depth classification method inspired by that exploitsknowledge about the distribution of the road surface.",
  ". Methodology": "The following section describes our 3D lane detection ap-proach. An overview of the overall architecture is describedand illustrated in . The main focus lies on our contin-uous 3D lane line representation, our regularization mecha-nism using physical priors and our prior-based spatial trans- . Our 3D lane line representation: For each proposal f(purple lines), line geometry is described by 3D B-Splines withcontrol points ck (green dots). Each control point is determinedby the offsets k, k from the control points of the initial proposalin normal direction (orange vectors). Additionally, visibility v(t)is modeled by splines with 1D control points k.",
  ". Lane line representation": "Inspired by prior work in 3D lane detection , we lever-age the benefits of continuous representations and employ aparametric model based on B-Splines. However, modelingonly lateral (x-) and vertical (z-) components with spline-based functions (as done in previous approaches) is limitedto lanes that merely progress along the longitudinal (y-) di-rection. Instead, we propose the first full 3D lane line rep-resentation modeling each component (x, y, z) such that we",
  "k=1ck Bk,d(t)(1)": "with curve argument t and K control points ck =xk, yk, zkT . Each control point ck weights the respectivebasis function Bk,d(t) (recursive polynomials of degree d)controlling the curve shape.Due to the ambiguity of curves using 3D B-Splines (thesame spline curve can be described by different configura-tions of its control points), regressing all three dimensionsper control point results in strong overfitting during training.We resolve this issue by limiting the degrees of freedom percontrol point to two and constraining the control points de-flection to one direction in the x-y-plane and one directionin the y-z-plane as illustrated in . More precisely, thedegrees of freedom per control point are specified by thedirections of the normals Nxy and Nz of an initial curveproposal f with control points ck =xk, yk, zkT . Thecontrol points are then defined as",
  ",(2)": "where Nx, Ny describe the x- and y-component of the nor-mal vector Nxy in the x-y-plane. As shown in Eq. (2) andillustrated in , modeling splines as deflections in nor-mal direction of its underlying initial line proposal only re-quires two parameters k, k per control point to describethe 3D shape. We use a wide variety of orientations for theinitial proposals f (see ), which allows us to detectany kind of lines with this formulation. More details aboutthe initial proposals are provided in the supplementary.While models the curve range using start- and end-points that are learned by means of regression, we insteadpropose to model visibility1 using a continuous representa-tion v(t) and treat the visibility estimation as a classificationproblem. We obtain probability values applying sigmoidactivation and consider v(t)> 0.5 the visible range.While in theory any kind of function can be utilized, wefound that B-Splines with the same configuration as f(t)are well-suited and introduce spline control points k defin-ing the shape of v(t).Eventually, binary cross-entropy is used as a classifica-tion loss to learn visibility",
  ". Regularization using physical priors": "In this section, we describe our regularization method to in-tegrate prior knowledge about lane structure and surface ge-ometry into our parametric line representation (see ).Line parallelism. In order to reinforce parallel lines, thetangents at point pairs located in opposite normal directionon neighboring lines must be similar (see left). Werealize this by penalizing the cosine distance of the unit tan-gents T(t) on neighboring lines i and j for normal pointpairs. More precisely, for each point p P(i) on line i weselect the normal pair point p on neighbor line j that mini-mizes the distance to the normal plane, which is defined bythe plane equation T(i)(t)T (x, y, z)T f (i)(t)= 0.In the normal planes are visualized in a 2D top-viewas lines (orange) for simplicity. Hence the respective curveargument tp for point p on line j is given as",
  "tp = argminpP(j) T(i)(tp)T f (j)(tp) f (i)(tp),(5)": "where P(j) denotes the points on line j. While in theoryEq. (5) can be solved analytically, the simpler way is tosample the set of points P(j) instead. (Note that our contin-uous representation allows us to choose high sampling rateswithout losing precision as no interpolation is required.)With the normal point pairs, we define the parallelismloss for a neighbor line pair based on the cosine distance oftheir tangents as",
  "Since the criterion of line parallelism should not hold forall normal point pairs of neighboring lines (e.g. mergingor splitting lines), 1(ij)p {0, 1} represents the indicator": "function determining whether the parallelism loss is appliedto the point pair. More precisely, the function ensures thatonly the overlapping range of neighboring lines is taken intoaccount. Furthermore, it determines whether the line pairshould be considered as a parallel pair based on the stan-dard deviation of euclidean distances between normal pointpairs, i.e. high deviations indicate that the line pair mightbelong to a merge or split structure. In our experiments, weachieve state-of-the-art performance on test sets containingmerges and splits, proving that our model is also capable oflearning non-parallel line pairs using this indicator function.Surface smoothness. Since the lines reside on a smoothroad, the surface normals of neighboring lanes should besimilar. Analogously to Lpar, we express this with the co-sine distance between surface normals N(ih) and N(ij) as",
  "T(i)(tp)f (h)(tp)f (i)(tp)": "||f (h)(tp)f (i)(tp)||. For the normal between linei and right neighbor j the sign is flipped to obtain upwardspointing normal vectors.Curvature. We determine lane curvature by computingthe second order derivatives as the difference of tangentsat consecutive points divided by their euclidean distanceas T(tp) =T(tp)T(tpt) ||f(tp)f(tpt)||. The maximum curvaturein x-y-plane (inverse curve radius) and in z (rate of slopechange) have very different value ranges and are thereforerestricted by different limits.Hence, we define the twothresholds xy and z and formulate the curvature loss online i as",
  "(10)": "with individual weights par, sm, curv.Note that allthese properties are expressible by means of tangents andnormals, which can be computed analytically on our para-metric representation in continuous space. Consequently,minimization of the herein introduced prior losses doesnot depend on numerical approximations as is the case foranchor-, grid- or key-point representations. . Our proposed spatial transformation module. First, sev-eral road surface hypotheses are defined (a) to which front-viewfeatures are lifted (b) and weighted according to the predicteddepth distribution. Afterwards, point features are aggregated ina weighted manner to obtain the 3D feature map (c).",
  ". Spatial transformation": "In this section, we describe our spatial transformation(shown in ) that is leveraging valuable physical knowl-edge about surface geometry. We know that the road sur-face typically shows small deviations from the ground level(z = 0) in the near-range and stronger deviations in the far-range. Based on this knowledge, we sample ground surfacehypotheses that reflect the distribution of the road surfaceheight profile (a). While in theory different types ofsurface functions could be utilized as hypotheses, we decideto merely rely on planes, since this facilitates the computa-tion of ray intersections described in the following step.Next, the multi-scale front-view feature maps extractedby the backbone are lifted to 3D space (b). Our ap-proach is inspired by , where front-view features arespreading along rays throughout the space of the road sur-face. These rays intersect with the surface hypotheses atdifferent depths spanning a frustum-like point cloud in 3Dspace, where each point is affiliated with a C-dimensionalfeature vector and additionally attached with its height valuez, hence, each point in the cloud has dimension (C + 1).The front-view feature map is propagated through a depthbranch with a channel-wise softmax applied to obtain a cat-egorical distribution for each ray, resulting in a tensor ofsize H W S, where H, W denote height and width andchannel size S the number of surface hypotheses.In order to aggregate the information in 3D space, a BEVgrid of size X Y is defined. Features from points map-ping to the same grid cell are weighted by the categoricaldepth distribution for the respective ray and accumulated interms of a weighted sum (c). Since the z-component",
  ". Experimental setup": "We evaluate our method on two different datasets: Open-Lane and Apollo 3D Synthetic - both containing 3D laneground truth as well as camera parameters per frame.OpenLane is a real-world dataset containing 150,000images in the training and 40,000 in the test set from 1000different sequences. In order to evaluate different drivingscenarios the test set is divided into different situations,namely Up & Down, Curve, Extreme Weather, Night, In-tersection and Merge & Split. For ablation studies we usethe smaller version OpenLane300 including 300 sequences.Apollo 3D Synthetic is a small synthetic dataset,consisting of only 10,500 examples from rather simple sce-narios of highway, urban and rural environments. The datais split into three subsets, (1) Standard (simple) scenarios,(2) Rare Scenes and (3) Visual Variations.Evaluation metrics. For the quantitative evaluation bothdatasets utilize the evaluation scheme proposed in .It evaluates the euclidean distance at uniformly dis-tributed points in the range of 0-100 m along the y-direction.Based on the mean distance and range, F1-Score is com-puted, as well as the mean x- and z-errors in near- (0-40 m)and far-range (40-100 m) to evaluate geometric accuracy.Baseline.Our approach builds up on 3D-SpLineNet.Since it was applied on synthetic data only, it showedpoor performance on real data. We applied some straight-forward design adaptations - e.g. larger backbone, multi-scale features (see supplementary) - and use this modified3D-SpLineNet as our baseline (first row ).",
  "LaneCPP (Ours)60.30.2640.3100.0770.11753.664.456.754.952.058.7": ". Quantitative comparison on OpenLane . Best performance and second best are highlighted. The scenario categories are Upand Down (U&D), Curve (C), Extreme Weather (EW), Night (N), Intersection (I), Merge and Split (M&S). PersFormer* denotes the latestperformance reported on the official code base, Anchor3DLane-T represents the temporal multi-frame method of .",
  ", 1": "8,116]. The final 3D feature map has size26 16 with 64 channels. We use M = 64 initial line pro-posals and B-Splines of degree d = 3 and K = 10 controlpoints. We apply Adam optimizer with an initial learn-ing rate of 2 104 for OpenLane and 104 for Apolloand a dataset specific step-wise scheduler. We train for 30epochs on OpenLane and 300 epochs on Apollo with batchsize 16. For more details we refer to the supplementary.",
  ". Ablation studies": "indicates the effect of our proposed prior-based reg-ularization. It is evident that each prior improves the F1-Score as well as geometric errors. While the surface andcurvature priors result in better far-range estimates, line par-allelism supports X-regression in the near-range. Besides,using surface smoothness loss results in lowest Z-far errors.Finally, a combination of priors yields a good balance of F1-Score and geometric errors. The positive effect of par-allelism is confirmed by , where reinforcing paral-lel lane structure leads to better estimates in the near-range(a) and far-range (b) compared to the unregularized model.Learning parallel lines also is evidently beneficial in casesof poor visibility (b) and occlusions (a). In the latter case,the regularized model even shows better predictions thanthe noisy ground truth. This emphasizes the high relevanceof priors for more robust behavior for real-world datasets,where 3D ground truth often comes with inaccuracies.For the spatial transformation (see ), too lownumbers of surface hypotheses result in worse score,presumably as 3D geometry is not captured sufficiently,whereas larger numbers tend to decreasing performance dueto the higher complexity. The best F1-Score is obtainedwith 5 hypotheses, which is chosen for further experiments.While the improvement over IPM is already considerable,we think that with the simplifications of plane hypothesesprevent the component from developing its full potential.We see ways to enhance the 3D transformation even further",
  ". Quantitative comparison of best methods on Apollo 3D Synthetic . Best performance and second best are highlighted": "using more sophisticated spatial representations in future.The impact of our different contributions is summarizedin , where the first row shows our baseline (seeSec. 4.1). More than two percent in F1-Score are gainedwith our novel lane representation compared to the simpli-fied one from . Moreover, it is clear that both, the regu-larization using combined priors and the spatial transforma-tion using 5 hypotheses result in significant improvement.Eventually, combining all components yields the best modelconfiguration, which we choose for further evaluation.",
  ". Evaluation on OpenLane": "On the real-world OpenLane benchmark our model evi-dently outperforms all other methods with respect to F1-Score as well as geometric errors as shown in .Compared to BEV-Lanedet, which achieves a high detec-tion score, our model gains +1.9 %, while reaching sig-nificantly lower geometric errors. In comparison to An-chor3DLane the improvements with respect to X-errors areless substantial, however, our approach surpasses the F1-Score by a large gap of +6.6 %. Analyzing the detectionscores among different scenarios, outstanding performancegain is observed on the up- and down-hill test set (+5.9 %)that highlights the capability of our approach to capture 3Dspace proficiently, which is supported by the low Z-errors.Apart from quantitative results, we show qualitative ex-amples in . In up-hill scenarios like b our modelmanages to estimate both lateral and height profile accu-rately, since our assumptions about road surface and lineparallelism are satisfied. In contrast, PersFormer lacks spa-tial features and does not use any kind of physical regular-ization. Consequently, it fails to estimate the 3D lane ge-ometry and even collapses in c, whereas our surfaceand curvature priors always prevent such a behavior. Note-worthy is also the top performance on the merges and splitsset. This proves that our soft regularization is even capableto handle situations containing non-parallel lines, which isalso confirmed by d. However, we rarely observe limi- tations with our formulation for line pairs with a similar ori-entation but weakly converging course as shown in e.In such cases the indicator function might erroneously de-cide for parallelism loss during training. One possible solu-tion for future work would be to consider ground truth forthe indicator function to identify such situations.",
  ". Evaluation on Apollo 3D Synthetic": "The Apollo 3D Synthetic dataset is very limited in size andonly consists of simple situations in contrast to OpenLane.While we find the results on OpenLane more meaningful,we would like to still provide and discuss the quantitativeresults on the Apollo dataset. Due to the simplicity of thedataset, our model cannot benefit that significantly from ourpriors but still achieves competitive results to state of the artwith the highest F1-Score on the balanced scenes datasetand comparable error metrics (second best for most errors).",
  ". Conclusions and future work": "In this work, we present LaneCPP, a novel approach for3D lane detection that leverages physical prior knowl-edge about lane structure and road geometry.Our newcontinuous lane representation overcomes previous defi-ciencies by allowing arbitrary lane structures and enablesus to regularize lane geometry based on analyticallyformulated priors.We further introduce a novel spatialtransformation module that models 3D features carefullyconsidering knowledge about road surface geometry.Inour experiments,we demonstrate state-of-the-art per-formance on real and synthetic benchmarks.The fullcapability of our approach is revealed on real-worldOpenLane, for which we prove the relevance of priorsquantitatively and qualitatively. In future, priors could beindividualized for different driving scenarios and mightsupport to learn inter-lane relations to achieve betterscene understanding in a global context.We also seeways to leverage the full potential of the spatial transfor-mation by using more sophisticated surface representations. Yifeng Bai, Zhirong Chen, Zhangjie Fu, Lang Peng, Peng-peng Liang, and Erkang Cheng. Curveformer: 3d lane detec-tion by curve propagation with curve queries and attention.In Proc. IEEE International Conf. on Robotics and Automa-tion (ICRA), 2023. 1, 2, 3, 7, 8 Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu,Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi,Yu Qiao, et al. Persformer: 3d lane detection via perspectivetransformer and the openlane benchmark.In Proc. of theEuropean Conf. on Computer Vision (ECCV), 2022. 2, 3, 4,6, 7, 8, 9, 10, 11",
  "Steffen Hagedorn, Marcel Milich, and Alexandru P. Con-durache. Pioneering se (2)-equivariant trajectory planningfor automated driving. arXiv:2403.11304, 2024. 2": "Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen ChangeLoy. Learning lightweight lane detection cnns by self atten-tion distillation. In Proc. of the IEEE International Conf. onComputer Vision (ICCV), 2019. 1, 2 Shaofei Huang, Zhenwei Shen, Zehao Huang, Zi han Ding,Jiao Dai, Jizhong Han, Naiyan Wang, and Si Liu.An-chor3dlane: Learning to regress 3d anchors for monocular3d lane detection. In Proc. IEEE Conf. on Computer Visionand Pattern Recognition (CVPR), 2023. 2, 3, 7, 8 Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, WillSong, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Ra-jpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando A.Mujica, Adam Coates, and Andrew Y. Ng.An em-pirical evaluation of deep learning on highway driving.arXiv/1504.01716, 2015. 1, 2",
  "YeongMin Ko, Jiwon Jun, Donghwuy Ko, and Moongu Jeon.Key points estimation and point instance segmentation ap-proach for lane detection. arXiv/2002.06604, 2020. 1, 2": "Seokju Lee, Junsik Kim, Jae Shin Yoon, Seunghak Shin,Oleksandr Bailo, Namil Kim, Tae-Hee Lee, Hyun SeokHong, Seung-Hoon Han, and In So Kweon. Vpgnet: Van-ishing point guided network for lane and road marking de-tection and recognition. In Proc. of the IEEE InternationalConf. on Computer Vision (ICCV), 2017. 1, 2 Chenguang Li, Jia Shi, Ya Wang, and Guangliang Cheng.Reconstruct from top view: A 3d lane detection approachbased on geometry structure prior. In Proc. IEEE Conf. onComputer Vision and Pattern Recognition (CVPR), 2022. 2,3, 8",
  "Hanspeter Mallot, Heinrich Bulthoff, J.J. Little, and SBohrer. Inverse perspective mapping simplifies optical flowcomputation and obstacle detection. Biological Cybernetics,1991. 3": "Davy Neven, Bert De Brabandere, Stamatios Georgoulis,Marc Proesmans, and Luc Van Gool. Towards end-to-endlane detection: an instance segmentation approach. In Proc.IEEE Intelligent Vehicles Symposium (IV), 2018. 1, 2 Marcos Nieto, Luis Salgado, Fernando Jaureguizar, and JonArrospide.Robust multiple lane road modeling based onperspective analysis. In Proc. IEEE International Conf. onImage Processing (ICIP), 2008. 2",
  "Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encodingimages from arbitrary camera rigs by implicitly unprojectingto 3d. In Proc. of the European Conf. on Computer Vision(ECCV), 2020. 3, 5, 6": "Maximilian Pittner, Alexandru Condurache, and Joel Janai.3d-splinenet: 3d traffic line detection using parametric splinerepresentations. In Proc. of the IEEE Winter Conference onApplications of Computer Vision (WACV), 2023. 1, 2, 3, 4,6, 8, 5 Fabio Pizzati, Marco Allodi, Alejandro Barrera, and Fer-nando Garca. Lane detection and classification using cas-caded cnns. In Proc. of the International Conf. on ComputerAided Systems Theory (EUROCAST), 2019. 1, 2 Zhan Qu, Huan Jin, Yang Zhou, Zhen Yang, and Wei Zhang.Focus on local: Detecting lane marker from bottom up viakey point. In Proc. IEEE Conf. on Computer Vision and Pat-tern Recognition (CVPR), 2021. 1, 2 Matthias Rath and Alexandru Paul Condurache. Invariant in-tegration in deep convolutional feature space. In Proc. of Eu-ropean Symposium on Artificial Neural Networks, Computa-tional Intelligence and Machine Learning (ESANN), 2020.2 Matthias Rath and Alexandru Paul Condurache. Improvingthe sample-complexity of deep classification networks withinvariant integration. In Proc. of International Joint Conf. onComputer Vision, Imaging and Computer Graphics Theoryand Applications (VISIGRAPP), 2022. 2",
  "Bingke Wang, Zilei Wang, and Yixin Zhang. Polynomialregression network for variable-number lane detection. InProc. of the European Conf. on Computer Vision (ECCV),2020. 2": "Jinsheng Wang, Yinchao Ma, Shaofei Huang, Tianrui Hui,Fei Wang, Chen Qian, and Tianzhu Zhang. A keypoint-basedglobal association network for lane detection. In Proc. IEEEConf. on Computer Vision and Pattern Recognition (CVPR),2022. 1, 2 Ruihao Wang, Jian Qin, Kaiying Li, Yaochen Li, Dong Cao,and Jintao Xu. Bev-lanedet: An efficient 3d lane detectionbased on virtual camera via key-points. In Proc. IEEE Conf.on Computer Vision and Pattern Recognition (CVPR), 2023.2, 3, 7, 8",
  "Yuping Wang and Jier Chen. Eqdrive: Efficient equivari-ant motion forecasting with multi-modality for autonomousdriving. arXiv:2310.17540, 2023. 2": "Lu Xiong, Zhenwen Deng, Peizhi Zhang, and Zhiqiang Fu.A 3d estimation of structural road surface based on lane-lineinformation. IFAC Conf. on Engine and Powertrain Control,Simulation and Modeling (E-COSM), 2018. 2 Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen,Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eqmo-tion: Equivariant multi-agent motion prediction with invari-ant interaction reasoning. In Proc. IEEE Conf. on ComputerVision and Pattern Recognition (CVPR), 2023. 2 Fan Yan, Ming Nie, Xinyue Cai, Jianhua Han, Hang Xu,Zhen Yang, Chaoqiang Ye, Yanwei Fu, Michael Bi Mi, andLi Zhang. Once-3dlanes: Building monocular 3d lane detec-tion. In Proc. IEEE Conf. on Computer Vision and PatternRecognition (CVPR), 2022. 2, 3 Tu Zheng, Hao Fang, Yi Zhang, Wenjian Tang, Zheng Yang,Haifeng Liu, and Deng Cai. RESA: recurrent feature-shiftaggregator for lane detection. In Proc. of the Conf. on Artifi-cial Intelligence (AAAI), 2021. 2 Tu Zheng, Yifei Huang, Yang Liu, Wenjian Tang, ZhengYang, Deng Cai, and Xiaofei He. Clrnet: Cross layer re-finement network for lane detection. In Proc. IEEE Conf. onComputer Vision and Pattern Recognition (CVPR), 2022. 2 Qin Zou, Hanwen Jiang, Qiyu Dai, Yuanhao Yue, LongChen, and Qian Wang. Robust lane detection from continu-ous driving scenes using deep neural networks. IEEE Trans.on Vehicular Technology (VTC), 2020. 1, 2",
  "Supplementary Material": ". Height distribution (z) along the longitudinal direction(y) of ground truth line points (blue points) on OpenLane dataset.Height deviations in the near-range (left side) tend to be smallerthan in the far-range (right side) spanning a triangle-like region ofinterest in the y-z-profile. For the spatial transformation, we sam-ple surface hypotheses (green) of different pitch angles to coverthis region.",
  "A.1. Backbone": "Similar to , we use a modified version of EfficientNet as our backbone. More precisely, we extract a specificlayer as the following modules input. Then, several con-volution layers are applied, such that the backbone moduleoutputs four different scaled front-view feature maps. Theirresolutions are 180240, 90120, 4560, 2230. Eachof the front-view feature maps is then fed into the spatialtransformation module. The total number of parameters ofthe backbone is 10.28 M.",
  "A.2. Spatial transformation": "The depth branch consists of two convolution layers eachwith 128 kernels and zero-padding, followed by batch normand ReLU activation. An additional convolution layer usesS (number of surface hypotheses) kernels of size 1 1 fol-lowed by a channel-wise softmax to obtain the depth distri-bution. Since the depth distribution should be similar for allfront-view feature maps of different scales, only one featuremap needs to be propagated through the depth-branch. Weuse the feature map with lowest resolution 22 30 and re-peat the resulting depth distribution of shape 22 30 S(with S the number of surface hypotheses) at the neighbor-ing feature cells to match the higher resolutions. Conse-quently, we obtain depth distributions for all scales of front-view feature maps sharing the same depth information.To model the road surfaces region of interest, we select",
  ". Different orientations of surface hypotheses": "surface hypotheses such that the distribution of lane heightis covered (see ). The surface hypotheses are planescrossing the origin of the 3D coordinate system with differ-ent orientations with respect to the pitch angle. The differ-ent configurations that we use in the experimental sectionare listed in .After the front-view features are lifted to 3D space theyare accumulated on BEV grids. Analogously to the multi-scale front-view feature maps, we also model multi-scaleBEV feature maps. The different resolutions are 208128,104 64, 52 32, 26 16.",
  "A.3. BEV feature fusion": "The BEV feature fusion module consists of convolutionlayers operating on each scale to down-sample the higherresolutions to the lowest resolution feature map of shape26 16. Afterwards, all feature maps are simply concate-nated and fed through several layers preserving the resolu-tion. Each contains a convolution with zero-padding, batchnorm and ReLU activation. The last convolution layer uses64 channels, thus, the input to the detection head is of shape26 16 64.",
  "A.4. Detection head": "The detection head operates on a BEV feature map of shape26 16 64 covering a range of [10 m, 10 m] in lateralx-direction and [3 m, 103 m] in longitudinal y-direction.Based on the location of initial line proposals, features arepooled from the BEV feature map for each line proposalas illustrated in . More precisely, we step througha proposal inside the BEV feature grid with a small stepsize and determine the nearest cells, where the maximumnumber of cells is limited to max cells. We then take the64-dimensional features of the set of selected cells and flat-ten it to a feature vector of size 64 max cells. If less thanmax cells are pooled for the proposal, the remaining entriesof the feature vector are simply masked out. The result-ing feature vector for each line proposal is then propagatedthrough the fully-connected layers as depicted in . Im-portant to notice is also that the fully connected layers share . The detection head of our model: First, features arepooled from the BEV feature map for each proposal.After-wards, pooled features are flattened and fed through several fully-connected (FC) layers, which share weights for all proposals, tofinally obtain the lane parameters. weights among all proposals to learn the same patterns fordifferent line orientations from the BEV feature map. Fi-nally, for each proposal the model yields parameters to de-scribe lane line geometry and visibility ({k, k, k}Kk=1),as well as a line presence probability ppr and a probabilitydistribution pcat for different line categories.",
  "B.1. Initial proposals and Matching": "We use several initial line proposals to cover a wide vari-ety of lane geometries. More precisely, the proposals arestraight lines with different orientations and different posi-tions in the x-y-plane. After investigations of different setconfigurations, we found the best set of proposals to be theone with M = 64 proposals that is illustrated in . The matching of ground truth lines to the line proposalsis inspired by , which choose the unilateral chamfer dis-tance (UCD) as a matching criterion. However, we foundthat a combination of the unilateral chamfer distance (nor-malized, thus UCD ) and an orientation cost basedon the cosine distance (CosD ) better reflects howwell a line proposal f resembles a ground truth line de-scribed by the set of ground truth points PGT . Thus, thepair-wise matching cost between a proposal with index i(with i M) and a ground truth line with index j (withj MGT and MGT the number of ground truth lines) is",
  "B.2. Losses and ground truth": "We provide more details regarding losses and ground truth.Indicator function for prior regularization.The par-allelism loss uses an indicator function 1(ij)pdeciding,whether the loss is applied to the point pair consisting ofpoint p on line i and the best matching point in normal di-rection p on line j. The indicator function is defined as",
  "OD(ij)p = T(i)(tp)T f (j)(tp) f (i)(tp).(17)": "Hence, only point pairs are considered for the parallelismloss, which actually lie in opposite normal direction. Thisis implied by the orthogonal distance having a small enoughvalue, i.e.if the value is lower than a certain thresholdODthr. For instance, if two neighboring lines have differentranges, the non-overlapping range has no neighbor pointsthat have an orthogonal distance smaller than the threshold.Thus, the condition ensures that only point pairs are consid-ered, which are actual neighbors in normal direction.The second condition (ij) < thr guarantees that par-allelism is not reinforced for line pairs, which presumablybelong to lanes of different orientations, e.g.for mergeand split scenarios. The distinction between parallel andnon-parallel line pairs can be determined by evaluating thestandard deviation (ij) of the euclidean distances D(ij)pofpoint pairs of neighboring lines i and j. The standard devi-ation is defined as",
  "pP(i)D(ij)p,(19)": "and the euclidean distance for one point pair as D(ij)p=f (i)(tp) f (j)(tp)2. For lines of different orientations(as for merging and splitting lines) this standard deviationis rather high and more likely surpasses the threshold thrin contrast to lines belonging to the same lane, where (ij) is rather small.Ground truth generation for surface loss. For the surfaceloss computation, height ground truth zuv needs to be pro-vided on the X Y BEV grid. We approximate this surfaceground truth by interpolation of the 3D lane ground truth.For this, we simply compute the convex hull of ground truthlines and interpolate the height value at each cell inside theconvex hull. Only cells inside the convex hull are consid-ered for the surface loss, whereas cells outside the convexhull are simply masked out. This is reflected by the indi-cator function 1uv, hence 1uv = 1 if cell (u, v) is insidethe hull, else 1uv = 0. The result of the grid-wise heightground truth generation is visualized in for an up-hill and a down-hill scenario.Lane presence and category classification losses.Forboth classification losses, we apply focal loss . For linepresence, which only considers the two classes present andnot present, the loss is given as",
  "(b) Up-hill scenario": ".Examples of the surface ground truth generation.Ground truth lines are visualized as blue lines and height groundtruth per cell as blue dots. The black dots correspond to cells out-side the convex hull of 3D lines and are not considered for thesurface loss. with predicted line presence probability p(i)pr for line i andline presence ground truth p(i)pr = {0, 1}. f 0 denotesthe focusing parameter introduced in to handle classimbalance.The category classification loss is applied for datasets,which provide lane category information in the groundtruth. Analogously to Eq. (21), the loss is given as",
  "p(i)cat[c]f logp(i)cat[c],(23)": "with the predicted category probability vector p(i)catRCcat, which represents the categorical distribution for linei, and the ground truth one-hot vector p(i)cat {0, 1}Ccat.Moreover, p(i)cat[c] denotes the cth entry of the vector p(i)cat.Regression loss. For both, the regression and visibility loss,the curve argument tp has to be determined for a respec-tive point in the ground truth p PGT . Since our modellearns to predict orthogonal offsets from the assigned lineproposal, the points are projected orthogonal onto the lineproposal as illustrated in . After having obtained the",
  "(24)": "with v(i)pthe ground truth visibility information and(x(i)p , y(i)p , z(i)p )T the 3D position of a ground truth point pon line i. w R3 is a vector with weighting factors for each3D component providing for a more balanced regression ineach dimension. As shown in Eq. (24) and illustrated ina, only visible points are utilized. The total regres-sion loss for all lines is given as",
  "C.2. Losses": "The weights for the different losses are pr = 20, cat = 2,reg = 0.5, par = 10, sm = 0.01, curv = 1,prior = 1, surf = 0.1. The focusing parameter for theclassification losses is f = 6.0 and the vector to weighteach dimension for the regression loss is w = (2, 10, 1)T .The thresholds for the indicator function used for the priorlosses are thr = 2 m and ODthr = 1 m and the thresholdsfor the maximum curvatures are xy = 5 and z = 0.1.The set of ground truth points considered for the visibilityand regression losses has size |PGT | = 20. For the par-allelism and surface smoothness loss we sample |P| = 20points from the predictions and |P| = 100 points for thecurvature loss.",
  "C.3. Training procedure": "In the training, we use Adam optimizer , with an initiallearning rate of 2104 for OpenLane and 104 for Apollo.We use a dataset specific scheduler: We train for 30 epochson OpenLane, where the learning rate is decreased to 5 105 after 27 epochs, and for 300 epochs on Apollo, wherethe learning rate is divided by two every 100 epochs.",
  "D.1. Ablation studies": "shows the performance of 3D-SpLineNet onOpenLane300 and the effect of different design adaptations.It is clearly evident that these modifications result in largeimprovements that were necessary to make the approach ap-plicable to real-world data.In we compare two different strategies to drawsamples from the camera rays to investigate the effect ofusing priors in form of surface hypotheses for this compo-nent. The samples determine the frustum-like pseudo point",
  ". Effect of the sampling strategy used in the spatial trans-formation on OpenLane300. Uniform ray sampling is compared tosamples obtained from intersections of rays with surface hypothe-ses": "cloud in 3D space as described in Sec. 3.3 in the main paper.For the uniform sampling (comparable to ), the samplesare drawn along the rays with equal step size in the range[3 m, 110 m] to guarantee that the whole space of interest iscovered. We compare this method to our sampling based onprior-incorporated surface hypotheses as proposed and de-scribed in the main paper. As shown in , the perfor-mance gaps between the two strategies are significant. Thishighlights the importance of modeling geometry-aware 3Dfeatures by generating samples in the space of interest us-ing knowledge about the surface geometry. The differencesin F1-Score for varying sampling rates also imply that auniform sampling strategy requires high sampling rates toachieve comparable performance. In contrast, using surfacehypotheses, lower sampling rates are sufficient which keepsthe computational costs lower.",
  "D.3. Qualitative results": "We show additional qualitative results on OpenLane in. Considering the top rows, it is clearly evident in allexamples that our LaneCPP detects lanes more accuratelycompared to 3D-SpLineNet, which performs poorly on real-world data. The bottom row shows a direct comparison ofLaneCPP and PersFormer. Particularly in curves (a -b) and up- or down-hill scenarios (d - f)our model shows high-quality detections compared to Pers-Former. For the intersection scenario (c) with manydifferent line instances, LaneCPP shows overall good re-sults but still leaves room for improvement with respect togeometrical precision. A possible solution to improve thebehavior in such cases could be to model lane line relationsexplicitly to better capture global context as mentioned inour future work section. Moreover, we prove that our modelis able to classify line categories accurately as illustrated in the middle row plots.We further demonstrate the results of our model onApollo 3D Synthetic illustrated in . As shown, ourmodel achieves accurate detection results in simple scenar-ios from the Balanced Scenes test set (a - b),in more challenging up- and down-hill scenarios from theRare Scenes test set (c - d) as well as in caseof visual variations (e - f). A very challeng-ing scene is shown in f, where our model managesto capture the overall line structure well but still could beimproved slightly with respect to close-range x-errors."
}