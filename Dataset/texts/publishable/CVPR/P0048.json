{
  "Abstract": "Development of new materials in hard drive designsrequires characterization of nanoscale materials throughgrain segmentation. The high-throughput quickly chang-ing research environment makes zero-shot generalization anincredibly desirable feature. For this reason, we explorethe application of Metas Segment Anything Model (SAM)to this problem. We first analyze the out-of-the-box use ofSAM. Then we discuss opportunities and strategies for im-provement under the assumption of minimal labeled dataavailability. Out-of-the-box SAM shows promising accu-racy at property distribution extraction. We are able to iden-tify four potential areas for improvement and show prelimi-nary gains in two of the four areas.",
  ". Introduction": "Data is being generated and utilized at an ever-increasingpace, and this demand for data necessitates cheap, mass ca-pacity data storage. Data storage has not been able to keepup with the pace of data creation, and by 2025, world-widestorage capacity will only be able to store 10% of the digitaldata created . New Hard Disk Drive (HDD) advancessuch Heat Assisted Magnetic Recording (HAMR) can in-crease the density of information on each disk, promisingcapacity increases of 2-3 times current HDD offerings .HAMR requires nanoscale characterization and control ofmaterials to generate structures with the right magnetic,plasmonic, photonic, and electronic properties.The grain structure is the same length scale as the nan-odevices and can impact the materials properties , sohigh-throughput techniques with nanoscale grain sensitiv-ity are needed for both research and production.High-resolution grain characterization tools such as Atom ProbeTomography and Scanning Transmission Electron Mi-croscopy lack the throughput for high volume data collec-tion and entail destructive FIB cuts of the head . Bulktechniques like X-Ray Diffraction average across a sam-ple and miss spatial information. Scanning Electron Mi-croscopy has an optimal balance of spatial resolution and",
  "throughput for grain characterization , but there remaingrain boundary contrast challenges due to the verticallyaligned grains": "Quantifying the grain structure requires segmentationof the grains, but rule-based segmentation models fail dueto weak grain boundary contrast and contrast fluctuationsacross the image.Previous work has gotten around thisproblem by training neural networks on hundreds of hand-labeled grains . This method works well for seg-menting grain images if the material and image condi-tions are not changing, but in the fast-paced semiconductorresearch environment, processing conditions and materialcomposition may be changing on even a wafer-to-wafer ba-sis. A robust grain segmentation model is needed to avoidre-labelling and re-training for each new change in the pro-cess. For this reason, we chose to investigate the use of MetasSegment Anything Model (SAM), a large pre-trained gen-eral segmentation model . SAM has been shown to havesome zero-shot segmentation performance in microscopyimages . This gives us the flexibility of a neural net-work across different imaging conditions while minimiz-ing the need for hand-labeled images. The characteristicof Segment Anything as a prompt-based, ambiguity-awareinstance segmentation model allows us to optimize our fi-nal output based on domain-specific patterns. We investi-gate the performance of SAM at grain property extractionand discuss paths for improvements in prompt point selec-tion, prompt engineering, post-processing, and weight fine-tuning.",
  ". Data": "A dataset of 5 SEM images from one gold film wafer wasused for this analysis. The gold film was deposited by Phys-ical Vapor Deposition in Ultra-High Vacuum and then im-aged on a KLA eDR7380 Scanning Electron Microscope.Backscattered electron imaging mode was used to collectthe images.The labels were obtained by a single annotator hand trac-ing over the images using a drawing tablet. Multiple annota-tors tend to disagree, so hand-drawn labels are not a perfecttruth. The hand-labeling results in 783 grains.",
  ". Segmentation Method": "SAM takes a prompt and image and outputs a boolean seg-mentation mask. The prompt can be any combination ofa set of foreground and background points, a box, and alow resolution mask. It can return one output or three. Themulti-mask output mode allows it to handle ambiguity inthe output of simple prompts. The most common prompttype is a single foreground prompt point used to identifythe object at that location on the image.The Automatic Mask Generator (AMG) is Metasmethod for full-image fully automatic instance segmenta-tion using SAM. It consists of three main steps. First, isthe extraction of segmentation masks over multiple over-lapping zoomed-in crops using a grid of single foregroundpoints as the prompt input. The zoom-in crops strategy isdone to improve mask quality for smaller masks. The sec-ond step is mask post-processing to remove known com- mon errors, namely small spurious components and smallspurious holes.Finally, is the use of mask quality met-rics namely predicted intersection over union (IoU) andstability.IoU is a commonly used metric for segmen-tation tasks.Stability is the sensitivity of the output tochanges in the cutoff value used to binarize the predic-tion. These quality metrics are used to filter out bad masksand resolve duplicates using box-level non-maximum su-pression (NMS). This code is provided at their repo We use the default settings of AMG with the pre-trainedmodel weights for the ViT-h architecture. The resultingmasks are then filtered with mask-level NMS with SAMspredicted IoU scores as the criteria. This is a stricter con-straint on overlapping objects than the AMG provides, but isnecessary to a guarantee maximum overlap threshold con-sistent with grain boundary expectations. The inputs andresulting output can be seen in a-c. Holes in thefinal output happen for two reasons. Some small grains arenot detected, and the model asymmetrically struggles withsome low contrast boundaries. This means that boundariesare only detected from foreground point prompts on oneside of the boundary. Other miss-segmentation is due tolow-contrast boundaries being missed or intra-grain texturebeing recognized as a boundary.",
  ". Comparison of ability of SAM to capture hand-labeled grains under increasingly generous conditions": "perimeter between hand-labeled and model results. The re-sults are shown in d-f. There is good alignment be-tween the two methods. However, there are some consistentbiases in the prediction across all images. The lower tail ofarea, lower tail of perimeter, and upper tail of elongatednessare shorter as these grains are missing.",
  ". SAMs Limitations": "We consider four reasons for the inability of the segmenta-tion method to capture every grain and outline a method forquantifying the number of grains not captured attributableto each reason. First, a good mask for the grain may havebeen found but filtered out by the post-processing steps.We save these masks before post-processing. Second, thegrain may not have contained any of the prompt points. Weconsider the masks of 50 random prompt points per grain.Third, we consider grains that could be captured with a bet-ter prompt input.These are extracted via a box promptaligned with the bounding box of the hand-labeled grain.They represent an approximate upper limit on the perfor-mance of the model via prompting without changing anymodel weights. Finally, there are those can not be directlycaptured by the default SA model. In , we comparethe percent of grains falling into each of these categoriesat different IoU thresholds. At a 0.7 IoU threshold, thereare 27.34% of grains which could potentially be recoveredwithout changing SAMs weights.",
  ". Training-Free Improvements": "The AMG pipeline allows for three areas to optimize with-out adjusting model weights. One can change the input im-age, the input prompt(s), and how output masks are post-processed. We believe that incorporating domain specificknowledge into these model pipeline choices can improveour results.There are many inherent characteristics ofthe grain structure that can be leveraged in making thesechoices. We have listed some in . In this section, wepresent our initial investigation into improvements via input",
  ". Prompt-Selection": "Our analysis in .4 shows 2-8% of grains can befound if an optimal foreground prompt point is selected.Due to the expected long-tailed distribution of grain size,grid prompt point selection is ill-suited to guaranteeing eachgrain will have at least one prompt point within its bound-aries.We test a process of iteratively selecting promptpoints where holes were left in the previous step.In , we compare iterative and grid prompt se-lection methods. To obtain the mean IoU (mIoU) we usethe same segmentation pipeline as described in .2changing only the prompt points. The mIoU value is cal-culated over all 783 grains across the five images. Iterativepoint selection is able to achieve a higher mIoU in less orthe same number of prompt points.In .4, we also show that an additional 1-13% ofgrains can be found by prompts other than single foregroundpoints. Methods based on optimizing prompt embeddings or incorporating semantic information into prompts via text have potential for recovering those grains.",
  ". Comparison of mask scoring methods for overlap resolu-tion via Non-Maximum Supression": "Alignment, which is the overlap coefficient between themask perimeter and the noisy boundary. To compare EdgeAlignment with SAMs predicted IoU score, we use SAMto generate a set of 50 masks per grain each based on a ran-dom foreground prompt point within the grain. For each setof masks we perform NMS to obtain the expected IoU pergrain. These results can be seen in . The mIoU val-ues are higher than in the previous section due to not com-peting with masks from prompt points outside each grain.Edge Alignment performs similarly to predicted IoU as aNMS criteria despite not being correlated. However, it doesnot lead to a large improvement in mIoU.We have anecdotally seen cases where cropping masksand filling holes can result in good masks not predictableby SAM from single foreground point prompts.This islikely due to the asymmetric boundary detection discussedin .2. A SAM independent NMS criteria, like EdgeAlignment, allows those crops and hole fills to be addedpost model inference but pre-filtering. However, we havenot determined a method for efficiently creating these cropor fill masks.",
  ". Training-Based Improvements": "There are many works on fine-tuning SAM .These are a great option if you have a representative labeleddataset to train on. Unfortunately, in the design space theexpectation of novel images means no training dataset canbe truly representative. Furthermore, in this setting, hand-labeled data is potentially more valuable for testing thantraining. This does not wholly rule out fine-tuning on hand-labeled data but does encourage us to explore alternative approaches. Three are discussed here.First is data augmentation for teaching specific concepts.Two common errors we have seen qualitatively in otherdatasets are miss-segmentation due to obscuring contami-nation and lighting effects. Synthetically introducing theseconcepts to a diverse training dataset allows for fine-tuningthe model without overfitting to a specific domain and los-ing zero-shot performance.Second is style-transfer for synthetic data.Previousworks have shown success using style transfer todo unsupervised generation of segmentation training datain microscopy domains. This would allow for quick adap-tation to new variations in imaging conditions and materialtexture, eliminating the need for a model with good zero-shot performance and allowing for the use of more tradi-tional fine-tuning methods.Third is using SAM for weakly-supervised pre-trainingof a smaller segmentation model. In this scheme, the modelis first trained on a large set of in-domain images automati-cally labeled by SAM then fine-tuned on a few hand-labeledgrains. Previous works have shown less than 100images are needed in training U-Net for segmentationof grains. Domain-specific losses such as clDice fur-ther reduce the amount of training data needed.",
  ". Conclusion": "This paper presents an evaluation of Segment Anything as atool for grain characterization in the hard drive design pro-cess. We show experimental results over a limited dataset.The Segment Anything Automatic Mask Generator showspromising out-of-the-box performance at property extrac-tion for our images, but has systematic biases regarding im-portant properties that need to be further explored over amore diverse and representative dataset before deploying itat a larger scale. We also introduce a basic method for deter-mining potential for training-free improvements. Improvedprompting and domain-specific scoring show promise forimproving segmentation performance and mitigating biasesof the AMG pipeline.Future work will refine training-free methods for images of a similar quality to our datasetand explore training-based strategies for poorer quality im-ages.",
  "Bertrand Chauveau and Pierre Merville. Segment anythingby meta as a foundation model for image segmentation: anew era for histopathological images.Pathology, 55(7):10171020, 2023. 1": "Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao,Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, YingZang, and Papa Mao. Sam-adapter: Adapting segment any-thing in underperformed scenes. In 2023 IEEE/CVF Interna-tional Conference on Computer Vision Workshops (ICCVW).IEEE, 2023. 4 Shen J. Dillon, Kaiping Tai, and Song Chen. The impor-tance of grain boundary complexions in affecting physicalproperties of polycrystals. Current Opinion in Solid Stateand Materials Science, 20(5):324335, 2016. 1 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In International Conference on Learning Representa-tions, 2021. 2 M. Herbig, P. Choi, and D. Raabe. Combining structural andchemical information at the nanometer scale by correlativetransmission electron microscopy and atom probe tomogra-phy. Ultramicroscopy, 153:3239, 2015. 1",
  "Michael C. Kautzky and Martin G. Blaber.Materials forheat-assisted magnetic recording heads. MRS Bulletin, 43(2):100105, 2018. 1": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment anything, 2023. 1 Rasmus Larsen, Torben L. Villadsen, Jette K. Mathiesen,Kirsten M. . Jensen, and Espen D. Boejesen. Np-sam: Im-plementing the segment anything model for easy nanoparti-cle segmentation in electron microscopy images, 2023. 1",
  "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmen-tation, pages 234241.Springer International Publishing,2015. 4": "Bastian Ruhle, Julian Frederic Krumrey, and Vasile-DanHodoroaba.Workflow towards automated segmentationof agglomerated, non-spherical particles from electron mi-croscopy images using artificial neural networks. ScientificReports, 11(1), 2021. 4 Yoshinobu Sato, Shin Nakajima, Nobuyuki Shiraga, HidekiAtsumi, Shigeyuki Yoshida, Thomas Koller, Guido Gerig,and Ron Kikinis. Three-dimensional multi-scale line filterfor segmentation and visualization of curvilinear structuresin medical images. Medical Image Analysis, 2(2):143168,1998. 3 Peng Shi, Mengmeng Duan, Lifang Yang, Wei Feng, Lian-hong Ding, and Liwu Jiang. An improved u-net image seg-mentation method and its application for metallic grain sizestatistics. Materials, 15(13):4417, 2022. 1, 4 Suprosanna Shit, Johannes C. Paetzold, Anjany Sekuboyina,Ivan Ezhov, Alexander Unger, Andrey Zhylka, Josien P. W.Pluim, Ulrich Bauer, and Bjoern H. Menze. cldice - a noveltopology-preserving loss function for tubular structure seg-mentation. In 2021 IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR). IEEE, 2021. 4 Haoxiang Wang, Pavan Kumar Anasosalu Vasu, FartashFaghri, Raviteja Vemulapalli, Mehrdad Farajtabar, SachinMehta, Mohammad Rastegari, Oncel Tuzel, and HadiPouransari.Sam-clip: Merging vision foundation modelstowards semantic and spatial understanding, 2023. 3"
}