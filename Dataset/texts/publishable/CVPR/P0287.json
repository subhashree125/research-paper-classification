{
  "Abstract": "Open-vocabulary object detection (OvOD) has trans-formed detection into a language-guided task, empower-ing users to freely define their class vocabularies of interestduring inference. However, our initial investigation indi-cates that existing OvOD detectors exhibit significant vari-ability when dealing with vocabularies across various se-mantic granularities, posing a concern for real-world de-ployment. To this end, we introduce Semantic HierarchyNexus (SHiNe), a novel classifier that uses semantic knowl-edge from class hierarchies. It runs offline in three steps:i) it retrieves relevant super-/sub-categories from a hierar-chy for each target class; ii) it integrates these categoriesinto hierarchy-aware sentences; iii) it fuses these sentenceembeddings to generate the nexus classifier vector.Ourevaluation on various detection benchmarks demonstratesthat SHiNe enhances robustness across diverse vocabu-lary granularities, achieving up to +31.9% mAP50 withground truth hierarchies, while retaining improvements us-ing hierarchies generated by large language models. More-over, when applied to open-vocabulary classification onImageNet-1k, SHiNe improves the CLIP zero-shot baselineby +2.8% accuracy.SHiNe is training-free and can beseamlessly integrated with any off-the-shelf OvOD detector,without incurring additional computational overhead dur-ing inference. The code is open source.",
  "Definition of Nexus, Oxford Dictionary1. Introduction": "Open-vocabulary object detection (OvOD) transforms the object detection task into a language-guided matching problem between visual regions and classnames.Leveraging weak supervisory signals and a pre-aligned vision-language space from Vision-Language Mod-els (VLMs) , OvOD methods ex-tend the ability of models to localize and categorize objectsbeyond the trained categories. Under the OvOD paradigm,target object classes are described using text prompts like\"a {Class Name}\", rather than class indices. By alter-",
  "Sub-categories": ". (Top) Classifier comparison for open-vocabulary ob-ject detectors: (Left) standard methods use solely class names inthe vocabulary specified by the user to extract text embeddings;(Right) our proposed SHiNe fuses information from super-/sub-categories into nexus points to generate hierarchy-aware represen-tations. (Bottom) Open-vocabulary detection performance at dif-ferent levels of vocabulary granularity specified by users: A stan-dard Baseline under-performs and presents significant variability;SHiNe allows for improved and more uniform performance acrossvarious vocabularies. Results are on the iNatLoc dataset. ing the \"{Class Name}\", OvOD methods enable users tofreely define their own Classes of Interest (CoIs) using nat-ural language. This allows new classes of interest to be de-tected without the need for model re-training.Yet,recent studies for open-vocabulary classifica-tion highlight a key challenge: open-vocabularymethods are sensitive to the choice of vocabulary. For in-stance, Parashar et al. enhanced CLIPs zero-shot per-formance by substituting scientific CoI names, like \"Rosa\",with common English names, such as \"Rose\".RecentOvOD models have improved performance by better align-ing object features with the VLM semantic space .However, a pivotal question remains:Are off-the-shelf",
  "arXiv:2405.10053v1 [cs.CV] 16 May 2024": "OvOD detectors truly capable of handling an open vocabu-lary across various semantic granularities?In practical scenarios, Classes of Interest (CoIs) are inthe eyes of the beholder. For example, consider a regioncrop of a \"Dog\": one user may be interested in the specificbreed (e.g., \"Labrador\"), while another might only beconcerned about whether it is an \"Animal\". Thus, the CoIis defined at varying levels of semantic granularity. Ideally,since these CoIs refer to the same visual region, the perfor-mance of an OvOD detector should be consistent across dif-ferent granularities. However, our initial experiments (illus-trated in ) reveal that the performance of an OvOD de-tector (see Baseline) fluctuates based on the vocabularygranularity. This inconsistency in performance across gran-ularities presents a significant concern for deploying off-the-shelf OvOD models in real-world contexts, especiallyin safety-critical areas like autonomous driving .Although the same physical object, a \"Labrador\",can be classified at varying levels of granularity, the in-herent fact that a \"Labrador is a dog, which is ananimal\" remains constant.This knowledge is readilyavailable from a semantic hierarchy. Guided by this ratio-nale, we aim to enhance the robustness of existing OvODdetectors to vocabularies specified at any granularity byleveraging knowledge inherent in semantic hierarchies. Re-cent research in open-vocabulary classification hasexplored using super-/sub-categories of CoIs from hierar-chies to improve accuracy.However, these methods in-volve searching through sub-categories or both super-/sub-categories at inference time, leading to additional computa-tional overhead and limiting their use in detection tasks.We introduce the Semantic Hierarchy Nexus (SHiNe),a novel classifier designed to enhance the robustness ofOvOD to diverse vocabulary granularities.SHiNe istraining-free, and ensures that the inference procedureis linear in complexity relative to the number of CoIs.SHiNe first retrieves relevant super(abstract)-/sub(specific)-categories from a semantic hierarchy for each CoI in a vo-cabulary. It then uses an Is-A connector to integrate thesecategories into hierarchy-aware sentences, while explicitlymodeling their internal relationships. Lastly, it fuses thesetext embeddings into a vector, termed nexus, using an aggre-gator (e.g., the mean operation) to form a classifier weightfor the target CoI. SHiNe can be directly integrated withany off-the-shelf VLM-based OvOD detector. As shown in, SHiNe consistently improves performance across arange of CoI vocabulary granularities, while narrowing per-formance gaps at different granularities.We evaluate SHiNe on various detection datasets ,that cover a broad range of label vocabulary granularities.This includes scenarios with readily available hierarchiesand cases without them.In the latter, we utilize largelanguage models to generate a synthetic three- level hierarchy for SHiNe.Our results demonstrate thatSHiNe significantly and consistently improves the perfor-mance and robustness of baseline detectors, and showcaseits generalizability to other off-the-shelf OvOD detectors.Additionally, we extend SHiNe to open-vocabulary classifi-cation and further validate its effectiveness by comparing itwith two state-of-the-art methods on the ImageNet-1k dataset. The key contributions of this work are: We show that the performance of existing OvOD detec-tors varies across vocabulary granularities.This high-lights the need for enhanced robustness to arbitrary gran-ularities, especially for real-world applications. We introduce SHiNe, a novel classifier that improvesthe robustness of OvOD models to various vocabularygranularities using semantic knowledge from hierarchies.SHiNe is training-free and compatible with existing andgenerated hierarchies. It can be seamlessly integrated intoany OvOD detector without computational overhead. We demonstrate that SHiNe consistently enhances theperformance of OvOD detectors across various vocabu-lary granularities on iNatLoc and FSOD , withgains of up to +31.9 points in mAP50.On open-vocabulary classification, SHiNe improves the CLIP zero-shot baseline by up to +2.8% on ImageNet-1k .",
  ". Related Work": "Open-vocabulary object detection (OvOD) israpidly gaining traction due to its practical significance,allowing users to freely define their Classes of Interest(CoIs) during inference and facilitating the detection ofnewly specified objects in a zero-shot way. With the aidof weak supervisory signals, OvOD surpasses zero-shotdetectors by efficiently aligning visual region fea-tures with an embedding space that has been pre-alignedwith image and text by contrastive vision-language mod-els (VLMs) .This process is approached fromeither the vision or text side to bridge the gap betweenregion-class and image-class alignments. To this end, meth-ods based on region-aware training , pseudo-labeling , knowledge distillation ,and transfer learning are explored. In our study,we apply our method to pre-trained region-text alignedOvOD detectors, improving their performance and robust-ness to vocabularies of diverse granularities. Our methodshares conceptual similarities with the work of Kaul etal. , where they develop a multi-modal classifier thatmerges a text-based classifier enriched with descriptors from GPT-3 and a vision classifier grounded in imageexemplars. This classifier is then used to train an OvODdetector with an extra learnable bias. In contrast, ourproposed SHiNe is training-free, enabling effortless inte-gration with any OvOD detector. Prompt engineering has been extensively studied asa technique to enhance VLMs . Prompt enrich-ment methods have focused on aug-menting frozen VLM text classifiers by incorporating addi-tional class descriptions sourced from large language mod-els (LLMs) . In contrast, our work explores the acquisi-tion of useful semantic knowledge from a hierarchy. Prompttuning methods introduced learn-able token vectors into text prompts, which are fine-tunedon downstream tasks.In contrast, our proposed methodis training-free.Our work is mostly related to two re-cent methods, CHiLS and H-CLIP , that improveCLIPs zero-shot classification performance by rely-ing on a semantic hierarchy. CHiLS searches for higherlogit score matches within the sub-categories, using the maxscore found to update the initial prediction. H-CLIP runsa combinatorial search over related super-/sub-categoriesprompt combinations for higher logit scores. However, bothapproaches incur additional computational overhead due totheir search-on-the-fly mechanism during inference, con-straining their use to classification tasks. In contrast, SHiNeoperates offline and adds no overhead at inference, makingit applicable to both classification and detection tasks.Semantic hierarchy is a tree-like taxon-omy or a directed acyclic graph that structuressemantic concepts following an asymmetric and transitiveIs-A relation . Previous works have used such hierar-chies to benefit various vision tasks .Cole et al. introduce the extensive iNatLoc dataset witha six-level hierarchy to enhance weakly supervised objectlocalization, showing that appropriate label granularity canimprove model training. Shin et al. and Hamamci etal. develop hierarchical architectures that incorporatemultiple levels of a label hierarchy for training, enhancingmulti-label object detection in remote sensing and dental X-ray images, respectively. Our work distinguishes itself fromprevious studies in two key ways: i) We focus on multi-modal models; ii) We improve OvOD detectors using labelhierarchies as an external knowledge base, without requir-ing hierarchical annotations or any training. Furthermore,SHiNe does not rely on a ground-truth hierarchy and canwork with an LLM-generated hierarchy.",
  ". Method": "Our objective is to improve the robustness of off-the-shelfopen-vocabulary object detectors to diverse user-definedClasses of Interest (CoIs) with varying levels of seman-tic granularity. We first provide an introduction of open-vocabulary object detection (OvOD). Sec. 3.1 introducesour method of developing the Semantic Hierarchy Nexus(SHiNe) based classifier for OvOD detectors to improvetheir vocabulary granularity robustness. Once established,the SHiNe classifier can be directly integrated with existing trained OvOD detectors and transferred to novel datasets ina zero-shot manner as discussed in Sec. 3.2.Problem formulation. The objective of open-vocabularyobject detection is to localize and classify novel objectclasses freely specified by the user within an image, with-out any retraining, in a zero-shot manner. Given an inputimage I R3hw, OvOD localizes all foreground ob-jects and classifies them by estimating a set of boundingbox coordinates and class label pairs {bm, cm}Mm=1, withbm R4 and cm Ctest, where Ctest is the vocabulary setdefined by the user at test time. To attain open-vocabularycapabilities, OvOD uses a box-labeled datasetDdet with a limited vocabulary Cdet and an auxiliary datasetDweak as weak supervisory signals. Dweak features fewerdetailed image-class or image-caption annotation pairs, yetit encompasses a broad vocabulary Cweak (e.g., ImageNet-21k ), significantly expanding the detection lexicon.Open-vocabulary detector. Predominant OvOD detectors,such as Detic and VLDet , follow a two-stagepipeline. First, given an image, a learned region proposalnetwork (RPN) yields a bag of M region proposals by{zm}Mm=1 = RPN(I), where zm RD is a D-dimensionalregion-of-interest (RoI) feature embedding. Then, for eachproposed region, a learned bounding box regressor predictsthe location coordinates by bm = REG(zm). An open-vocabulary classifier estimates a set of classification scoressm(c, zm) = wc, zm for each class, where wc is a vec-tor in the classifier W R|Ctest| and , is the cosinesimilarity function. W is the frozen text classifier, createdby using a VLM text encoder (e.g., CLIP ) to encodethe names of CoIs in Ctest specified by the user. The CoIthat yields the highest score is assigned as the classificationresult. During training, OvOD detectors learn all model pa-rameters except for the frozen text classifier. This allowsthem to achieve region-class alignment by leveraging thevision-language semantic space pre-aligned by VLMs forthe open-vocabulary capability. Our work aims to improveexisting pre-trained OvOD detectors, so we omit further de-tails, and refer the reader to dedicated surveys .",
  ". SHiNe: Semantic Hierarchy Nexus": "Here, we describe SHiNe, our proposed semantic hierar-chy nexus-based classifier for improving OvOD. As illus-trated in (top), for each target CoI c Ctest (e.g.,\"Bat\") in the user-defined vocabulary, we construct a nexuspoint nc RD by incorporating information from relatedsuper-/sub-categories derived from a semantic hierarchy H.SHiNe is training-free. Upon constructing the nexus pointsfor the entire vocabulary offline, the nexus-based classifierN is directly applied to an off-the-shelf OvOD detector forinference. This replaces the conventional CoI name-basedclassifier W with our hierarchy-aware SHiNe classifier.This enables the classification score sm(c, zm) = nc, zm",
  "Constructed Nexus Classifier": ". Overview of our method. (Top) SHiNe constructs the semantic hierarchy nexus classifier in three steps offline: (1) For each targetclass (e.g., \"Bat\" in green) in the given vocabulary, we query the associated super-(in blue)/sub-(in pink) categories from a semantichierarchy. (2) These retrieved categories along with their interrelationships are integrated into a set of hierarchy-aware sentences using ourproposed Is-A connector. (3) These sentences are then encoded by a frozen VLM text encoder (e.g., CLIP ) and subsequently fusedusing an aggregator (e.g., mean-aggregator) to form a nexus classifier vector for the target class. (Bottom): The constructed classifier isdirectly applied to an off-the-shelf OvOD detector for inference, enhancing its robustness across various levels of vocabulary granularity. to be high when the proposed region closely aligns with thesemantic hierarchy theme embodied by the nexus point.This point represents the fusion of a set of hierarchy-awaresemantic sentences from specific to abstract that are relevantto the CoI c. Next, we detail the construction process. Querying the semantic hierarchy. To obtain related super-/sub-categories, a semantic hierarchy H is crucial for ourapproach. In this study, we investigate two types of hierar-chies: i) dataset-specific class taxonomies , and ii)hierarchies synthesized for the target test vocabulary usinglarge language models (LLM). To generate the synthetic hi-erarchy, we follow Novack et al. and query an LLMsuch as ChatGPT to generate super-categories (p = 3)and sub-categories (q = 10) for each CoI c Ctest, creatinga three-level hierarchy H (see App. B for details). With thehierarchy available, as depicted in (1), for each tar-get CoI c, we retrieve all the related super-/sub-categories,which can assist in distinguishing c from other concepts inthe vocabulary across granularities . Note that we ex-clude the root node (e.g., \"entity\") from this process, asit does not help differentiate c from other categories. Hierarchy-aware semantic sentence integration.Thecollected categories contain both abstract and specific se-mantics useful for guiding the classification process. How-ever, methods like simple ensembling or concatena-tion overlook some valuable knowledge implicitly pro- vided by the hierarchy, namely the inherent internal rela-tionships among concepts. Inspired by the hierarchy struc-ture definition , we propose an Is-A connector to explic-itly model these interrelationships. Specifically, for eachtarget CoI c, the Is-A connector integrates the retrieved cat-egories into sentences from the lowest sub-category (morespecific) to the highest super-category (more abstract), in-cluding the target CoI name.As depicted in (2),this process yields a set of K hierarchy-aware sentences{eck}Kk=1. Each sentence eck contains knowledge that spansfrom specific to abstract, all related to the target CoI andcapturing their inherent relationships, as",
  "where the sub-categories, target category, and super-categories are color-coded in red, green, and blue": "Semantic hierarchy Nexus construction. A nexus nc RD serves as a unifying embedding that fuses the hierarchy-aware knowledge contained in the integrated sentences{eck}Kk=1.As shown in (3), we employ a frozenVLM text encoder Etxt to translate the integrated sen-tences into the region-language aligned semantic spacecompatible with the downstream OvOD detector. The se-mantic hierarchy nexus for the CoI c is then constructed by",
  "(1)": "where, by default, we employ a straightforward but ef-fective mean-aggregator to compute the mean vector ofthe set of sentence embeddings.The goal of the aggre-gation process is to fuse the expressive and granularity-robust knowledge into the nexus vector, as a theme, fromthe encoded hierarchy-aware sentences. Inspired by textclassification techniques in Natural Language Processing(NLP) , we also introduce an alternative aggre-gator, where we perform SVD decomposition of the sen-tence embeddings and replace the mean vector with theprincipal eigenvector as nc. We study its effectiveness inSec. 4.1 and provide a detailed description in App. C.4.",
  "cm = arg maxcCtest nc, zm(2)": "where zm is the m-th region embedding. Given that nc RD, it becomes evident from Eq. 2 that SHiNehas the samecomputational complexity as the vanilla name-based OvODclassifier. Let us note that SHiNe is not limited to detec-tion, it can be adapted to open-vocabulary classification bysubstituting the region embedding zm with an image one.We validate this claim by also benchmarking on ImageNet-1k . We provide the pseudo-code and time complexityanalysis of SHiNe in App. C.2 and App. C.3, respectively.",
  "Actinopterygii115Food15Chordata": "Evaluation protocol and datasets. We primarily followthe cross-dataset transfer evaluation (CDTE) protocol in our experiments. In this scenario, the OvOD detectoris trained on one dataset and then tested on other datasetsin a zero-shot manner.This enables a thorough evalua-tion of model performance across diverse levels of vocab-ulary granularity. We conduct experiments on two detec-tion datasets: iNaturalist Localization 500 (iNatLoc) . Training signal combinations. LVIS and COCO are used as strong box-level supervision. ImageNet-21k (IN-21k) and the 997-class subset (IN-L) of ImageNet-21k that over-laps with LVIS are used as weak image-level supervision.",
  "ILVISN/AIILVISIN-LIIILVISIN-21kIVLVIS & COCOIN-21k": "and Few-shot Object Detection dataset (FSOD) , whichhave ground-truth hierarchies for evaluating object labelingat multiple levels of granularity. iNatLoc is a fine-graineddetection dataset featuring a consistent six-level label hier-archy based on the biological tree of life, along with bound-ing box annotations for its validation set. FSOD is assem-bled from OpenImages and ImageNet , structuredwith a two-level label hierarchy. For a more comprehensiveevaluation, we use FSODs test split and manually constructone more hierarchy level atop its existing top level, result-ing in a three-level label granularity for evaluation. Tab. 1outlines the number of label hierarchy levels and the cor-responding category counts for both datasets, accompaniedby examples to demonstrate the semantic granularity. De-tailed dataset statistics and their hierarchies are availablein App. A. We use the mean Average Precision (mAP) atan Intersection-over-Union (IoU) threshold of 0.5 (mAP50)as our main evaluation metric. Additional experiments onCOCO and LVIS under the open-vocabulary pro-tocol are provided in App. I. Baseline detector.In our experiments, we use the pre-trained Detic method as the baseline detector, given itsopen-source code and strong performance. Detic is a two-stage OvOD detector that relies on CenterNet2 and in-corporates a frozen text classifier generated from the CLIPViT-B/32 text encoder using a prompt of the form:\"a {Class}\". Detic uses both detection and classificationdata (image-class weak supervisory signals) for training. Inour experiments, we explore and compare with Detic underfour variants of supervisory signal combinations as shownin Tab. 2. We study a ResNet-50 and a Swin-B backbone pre-trained on ImageNet-21k-P . SHiNe implementation details.To directly apply ourmethod to the baseline OvOD detector, we use the CLIPViT-B/32 text encoder to construct the SHiNe classifierand directly apply it to the baseline OvOD detector, follow-ing the pipeline described in Sec. 3.1. We use the mean-aggregator by default. In our experiments, we employ andstudy two sources for the hierarchy: the ground-truth hi-erarchy structure provided by the dataset and a synthetichierarchy generated by an LLM. We use the gpt-3.5-turbomodel as our LLM via its public API to produce a sim-ple 3-level hierarchy (comprising one child and one parent",
  "mAP50 (%)": ". Study of hierarchy-aware sentence integration methods (left) and aggregators (right) across various label granularity levels onthe iNatLoc dataset. Detic with a Swin-B backbone is used as the baseline. Darker background color indicates higher mAP50. The defaultcomponents of SHiNe are underlined. Note that the experiment in (a) omits sub-categories and the aggregation step.",
  ". Analysis of SHiNe": "We first study the core components of SHiNe on the iNat-Loc using its ground-truth hierarchy. Consistent find-ings on the FSOD dataset are reported in App. D.The Is-A connector effectively integrates hierarchyknowledge in natural sentences. To assess the effective-ness of our Is-A connector, we design control experimentsfor constructing the OvOD classifier with a single sentence,omitting sub-categories and the aggregation step. Specifi-cally, for a target CoI like \"Baseball bat\", we retrieveonly its super-categories at each ascending hierarchy level.We then explore three ways to integrate the CoI with its as-cending super-categories in natural language and create theclassifier vector as follows: Ensemble (Ens): {\"baseball bat\", \"bat\", \"sportsequipment\"}",
  "sports equipment\"": "Is-A (Ours): \"A baseball bat, which is a bat,which is a sports equipment\"where the super-categories are colored in blue. For Con-cat and Is-A, we create the classifier vector for the targetCoI by encoding the single sentence with the CLIP text en-coder. For the Ens method, we use the average embeddingof the ensembled names as:13(Etxt(\"baseball bat\") +Etxt(\"bat\") + Etxt(\"sports equipment\")).Next, weconduct control experiments to evaluate the three integra-tion methods as well as the standard CoI name-based base-line methods. As shown in (a), except for the top lev-els where all methods degrade to the standard baseline (nosuper-category nodes), all methods outperform the baselineacross all granularity levels by directing the models fo- cus towards more abstract concepts via the included super-categories. Among the methods compared, our Is-A con-nector excels across all granularity levels, boosting thebaseline mAP50 by up to +39.4 points (see last row andsecond column in (a-L5)). This underscores the ef-fectiveness of our Is-A connector, which integrates relatedsemantic concepts into sentences and explicitly models theirrelationships, yielding hierarchy-aware embeddings.A simple mean-aggregator is sufficient for semanticbranch fusion.We explored two aggregation methods:mean-aggregator (M-Agg) and principal eigenvector aggre-gator (PE-Agg). Note that in this experiment, all methodsuse the proposed Is-A connector to create a set of hierarchy-aware sentences to aggregate, ranging from each retrievedsub-category to the super-categories, as elaborated in Sec. 3.As (b) shows, both methods improve performanceover the baseline across various models and label granular-ities. Note that these aggregators revert to the simple Is-Amethod at the leaf level where no sub-categories are avail-able for aggregation. The benefits of aggregation methodsare more pronounced with coarser granularity, significantlyoutperforming the baseline and the Is-A method, with gainsup to +9.8 on iNatLoc (see third row and second column in(b-L1). Notably, M-Agg generally outperforms PE-Agg despite its simplicity, making it the default choice forSHiNe in the subsequent experiments. Nonetheless, we aimto highlight the effectiveness of PE-Agg: to the best of ourknowledge, this is the first study using the principal eigen-vector as a classifier vector in vision-language models.",
  ". SHiNe on Open-vocabulary Detection": "SHiNe operates with different hierarchies. In this sec-tion, we broaden our investigation to assess the effective-ness and the robustness of SHiNe with different semantichierarchy sources.Tab. 3 shows the comparative analy-sis across various levels of label granularity between the . Detection performance across varying label granularity levels, ranging from finest (F) to coarsest (C), on iNatLoc (upper) andFSOD (lower) datasets. SHiNe is directly applied to the baseline detector (BL) with ground-truth (GT-H) and LLM-generated (LLM-H) hierarchies. ResNet-50 (left) and Swin-B (right) backbones are compared. Four types of supervisory signal combinationsare investigated. Note (): At the L1-/L6-level of GT-H, no super-/sub-categories categories are used, respectively. mAP50 (%) is reported.",
  "L1 19.939.7(+19.8) 25.4(+5.5)20.841.6(+20.8) 26.2(+5.4)": "baseline OvOD detector and our method, using either theground-truth hierarchy or the LLM-generated hierarchy asproxies.We observe that our approach consistently sur-passes the baseline by a large margin across all granular-ity levels on both datasetsand this holds true whetherwe employ the ground-truth or LLM-generated hierarchy.Averaged across all models and granularity levels on iNat-Loc, our method yields an improvement of +16.8 points us-ing the ground-truth hierarchy and +13.4 points with theLLM-generated hierarchy. For the FSOD dataset, we ob-serve gains of +10.3 and +2.9 points, respectively.Al-though the performance gains are smaller with the LLM-generated hierarchy, they nonetheless signify a clear en-hancement over the baseline across label granularities onall examined datasets. This shows that SHiNe is not relianton ground-truth hierarchies. Even when applied to noisy,synthetic hierarchies, it yields substantial performance im-provements. Additional results are in App. F and App. G.",
  "SHiNe operates with other OvOD detectors. To eval-": "uate SHiNes generalizability, we apply SHiNe to addi-tional OvOD detectors: CoDet and VLDet (VLD) .The evaluation results showcased in Tab. 4 affirm thatSHiNe consistently improves the performance of CoDet andVLDet significantly across different granularities on bothdatasets, with both hierarchies. Further, we assess SHiNeon another DETR-style detector, CORA , in App. H.",
  ") and the expanded mis-specified () vocabularies. SHiNeemploys the LLM-generated hierarchy for both vocabularies. Wereport mAP50, highlighting the performance drop ()": "SHiNe is resilient to mis-specified vocabularies. In real-world applications, an authentic open vocabulary text clas-sifier may be constructed using a vocabulary comprising awide array of CoIs, even though only a subset of those spec-ified classes appear in the test data. We define these as mis-specified vocabularies. Studying resilience in this challeng-ing scenario is essential for practical applications. To thisend, we gathered 500 class names from OpenImages . ImageNet-1k zero-shot classification. We comparewith two state-of-the-art hierarchy-based methods under WordNet(WDN) and LLM-generated hierarchies. Vanilla CLIP servesas the baseline. We report top-1 accuracy, and FPS measured onthe same NVIDIA RTX 2070 GPU with a batch size 1 and aver-aged over 10 runs. : For fair comparison, we reproduce H-CLIPsresults without its uncertainty estimation step and its refined Word-Net hierarchy. In the original H-CLIP paper, a top-1 accuracy of67.78% on ImageNet-1k was achieved using ViT-B/16 encoders.",
  "H-CLIP55.8(-3.1)260.1(-3.8)266.9(-5.1)1CHiLS61.1(+2.2)2666.1(+2.2)2773.4(+1.4)23SHiNe61.6(+2.7)14166.7(+2.8)14973.6(+1.6)81": "and 1203 from LVIS , resulting in 1466 unique classesafter deduplication. These are added as noisy CoIs tothe iNatLoc and FSOD leaf label vocabularies, creating ex-panded sets with 1966 and 1570 CoIs, respectively. Us-ing ChatGPT, SHiNe generates simple 3-level hierarchiesfor each class in these expanded vocabularies. As shownin , mis-specified vocabularies cause a decrease inbaseline detector performance, dropping an average of -4.1points on iNatLoc and -4.2 points on FSOD. However, in-terestingly, SHiNe not only continues to offer performancegains over the baseline detector but also mitigates the per-formance drop to -1.4 on iNatLoc and -3.3 on FSOD, re-spectively. This suggests that SHiNe not only improves therobustness but also enhances the resilience of the baselinedetector when confronted with a mis-specified vocabulary.",
  ". SHiNe on Open-vocabulary Classification": "In this section, we adapt SHiNe to open-vocabulary clas-sification, by simply substituting the region embedding inEq. 2 with an image embedding from the CLIP image en-coder . We evaluate it on the zero-shot transfer classifi-cation task using the well-established ImageNet-1k bench-mark .We compare SHiNe with two state-of-the-arthierarchy-based methods: CHiLS and H-CLIP ,which are specifically designed for classification.ImageNet-1k Benchmark. In Tab. 5, we compare methodson ImageNet in terms of accuracy and frames-per-second(FPS). We observe that our approach consistently outper-forms related methods.Comparing to the baseline thatonly uses class names, SHiNe improves its performance byan average of +1.2% and +2.4% across different modelsizes using WordNet and LLM-generated hierarchies, re-spectively. Note that both CHiLS and H-CLIP introducesignificant computational overheads due to their search-on-the-fly mechanism, resulting in a considerable decrease ininference speed. Consequently, this limits their scalability",
  "Level# ClassesCLIPH-CLIP CHiLS SHiNe": "L11056.267.9 (+11.7)73.8 (+17.6)50.4(-5.8)L22956.869.3 (+12.5)67.2 (+10.4)60.9(+4.1)L312843.362.4 (+19.1)62.2 (+18.9)54.7(+11.4)L446655.269.6 (+14.4)70.1 (+14.9)70.3(+15.1)L559162.465.9 (+3.5)64.5 (+2.1)69.1(+6.7)L69873.175.4 (+2.3)73.5 (+0.4)78.9(+5.8) to detection tasks that necessitate per-region proposal infer-ence for each image. For example, when processing detec-tion results for one image with 300 region proposals, theoverhead caused by CHiLS and H-CLIP would increase by300. In contrast, SHiNe maintains the same inferencespeed as the baseline, preserving its scalability.BREEDS ImageNet Benchmark. Next, we analyze dif-ferent granularity levels within ImageNet as organized byBREEDS . In Tab. 6, we observe that CHiLS and H-CLIP surpass SHiNe at coarser granularity levels (L1 toL3).This is largely attributed to the BREEDS-modifiedhierarchy, where specific sub-classes in the hierarchy pre-cisely correspond to the objects present in the test data. Yet,our method yields more substantial performance improve-ments at finer granularity levels (L4 to L6). Overall, the per-formance gains exhibited by all three methods underscorethe benefits of using hierarchy information for improvingopen-vocabulary performance across granularities.",
  ". Conclusion": "Given the importance of the vocabulary in open-vocabularyobject detection, the robustness to varying granularities be-comes critical for off-the-shelf deployment of OvOD mod-els. Our preliminary investigations uncovered notable per-formance variability in existing OvOD detectors across dif-ferent vocabulary granularities.To address this, we in-troduced SHiNe, a novel method that utilizes semanticknowledge from hierarchies to build nexus-based classifiers.SHiNe is training-free and can be seamlessly integratedwith any OvOD detector, maintaining linear complexity rel-ative to the number of classes. We show that SHiNe yieldsconsistent improvements over baseline detectors acrossgranularities with ground truth and LLM-generated hierar-chies. We also extend SHiNe to open-vocabulary classifica-tion and achieve notable gains on ImageNet-1k . Acknowledgements. E.R. is supported by MUR PNRR projectFAIR - Future AI Research (PE00000013), funded by NextGen-erationEU and EU projects SPRING (No. 871245) and ELIAS(No. 01120237). M.L. is supported by the PRIN project LEGO-AI(Prot. 2020TA3K9N). We thank Diane Larlus and Yannis Kalan-tidis for their helpful suggestions. M.L. thanks Zhun Zhong andMargherita Potrich for their constant support. ReljaArandjelovic,AlexAndonian,ArthurMensch,Olivier J. Henaff, Jean-Baptiste Alayrac, and Andrew Zis-serman. Three ways to Improve Feature Alignment for OpenVocabulary Detection. arXiv:2303.13518, 2023. 2",
  "Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and LihiZelnik-Manor. ImageNet-21K Pretraining for the Masses.In NeurIPS, 2021. 5": "Karsten Roth, Jae Myung Kim, A. Sophia Koepke, OriolVinyals, Cordelia Schmid, and Zeynep Akata.Wafflingaround for Performance: Visual Classification with RandomWords and Broad Concepts. In ICCV, 2023. 3 Michael A. Ruggiero, Dennis P. Gordon, Thomas M. Or-rell, Nicolas Bailly, Thierry Bourgoin, Richard C. Brusca,Thomas Cavalier-Smith, Michael D. Guiry, and Paul M.Kirk. A Higher Level Classification of All Living Organ-isms. PLOS ONE, 10(4):e0119248, 2015. 3",
  "Chufeng Tan, Xing Xu, and Fumin Shen.A Survey ofZero Shot Detection: Methods and Applications. CognitiveRobotics, 1:159167, 2021. 2": "Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, andSerge Belongie. The iNaturalist Species Classification andDetection Dataset. In CVPR, 2018. 3 Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-ona, and Serge Belongie.The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, Cali-fornia Institute of Technology, 2011. 3",
  "Supplementary Material": "In this appendix, we begin by detailing the detection datasets inApp. A. Then, App. B delves into the process of synthetic seman-tic hierarchy generation using LLMs, providing the LLM promptsand a thorough summary of the generated hierarchies statistics.We provide in App. C additional implementation specifics ofSHiNe. We present in App. D an extended analysis of SHiNescomponents on the FSOD dataset. App. F and App. G extendthe main detection experiments, offering comprehensive summarystatistics. App. I includes additional investigation of SHiNes per-formance on the COCO and LVIS datasets. Finally, App. J show-cases some qualitative detection results.Our code is publiclyavailable at",
  "A. Dataset Details": ". Summary of datasets. FSOD and iNatLoc offerthree and six levels of label hierarchies, respectively, with varyingsemantic granularity. This feature allows evaluating models onthese datasets at different granularity level in our experiments. Toevaluate, for FSOD, we use its official test split, and for iNatLoc,we use the combined test and validation splits.",
  "# of Levels36# of ClassesL3L2L1L6L5L4L3L2L1per Level200461550031718464185# of Images1415225000# of BBoxes3510225000": "In Tab. 7, we present a comprehensive summary of thetwo detection datasets used in our evaluation, FSOD andiNatLoc , under the cross-dataset transfer open-vocabularyevaluation protocol. Given that the original FSOD dataset provides only a two-level hierarchy, we manually constructed theL1 level of the label hierarchy (the most abstract one) for a morecomprehensive evaluation, under which we grouped the categoriesof level L2 (which corresponds to the upper level category in theoriginal label space). The L1 level consists of the following 15label categories:",
  "{\"liquid\", \"instrument\", \"food\", \"art\",\"plant\", \"component\", \"animal\", \"body\",\"wearable item\", \"infrastructure\",\"vehicle\", \"furnishing\", \"fungi\",\"equipment\", \"beauty product\"}": "For FSOD and iNatLoc, we use their official test splits in ourexperiments, respectively. Notably, FSOD and iNatLoc offer threeand six levels of label hierarchies, respectively, each characterizedby distinct semantic granularities. Specifically, for the same set ofevaluation images and their associated box-label annotations, theactual label used for evaluation can be mapped to different linked labels at each granularity level of the hierarchy. For example, inFSOD, a box region labeled as \"watermelon\" at the L3-levelcould be mapped to label \"fruit\" at the L2-level or \"food\" atthe L1-level. This hierarchical approach to labeling facilitates theevaluation of these datasets at various granularity levels. See theannotation files in our codebase.Evaluation level. During the evaluation, we consider only onehierarchy level at a time, where the label class vocabulary corre-sponding to the evaluation level of granularity serves as the targettest (user-defined) vocabulary for both the methods being com-pared and our proposed SHiNe. This means that model has toassign labels solely from the given hierarchy level.",
  "B. Semantic Hierarchy Generation": "Being a hierarchy-based method, validating SHiNes effectivenesswith various hierarchy sources is crucial. In real-world applica-tions, an ideal semantic hierarchy for the target data might notalways be available. Therefore, our study focuses on evaluatingSHiNe using not only the dataset-specific class taxonomies (the ground-truth hierarchies provided by the datasets as de-scribed in Sec. A) but also hierarchies synthesized for the targettest vocabulary via large language models (LLMs). Our key idea isthat encyclopedic textual information about semantic categories isreadily available on the Internet. Contemporary LLMs like Chat-GPT , trained on vast internet-scale corpora, inherently en-code the necessary semantic class taxonomic information in theirweights. Similar to the approach used in CHiLS , we employan LLM to automatically generate simple three-level semantic hi-erarchies for the target vocabularies. We use the ChatGPT gpt-3.5-turbo model as our LLM via its public API to generate thesynthetic semantic hierarchy with a temperature parameter of 0.7.See our codebase for the generated hierarchies.In the following subsections, we first detail the process ofprompting LLMs to generate hierarchies (Sec. B.1) and then sum-marize the statistics of the hierarchies we generated (Sec. B.2).",
  "B.1. Prompting LLMs": "In scenarios where a ground-truth hierarchy is unavailable, andgiven a label vocabulary Ctest representing the target Classes of In-terest (CoIs) at a specific granularity level of the evaluation dataset,the true super-/sub-categories for each CoI are unknown. To gener-ate a simple 3-level hierarchy for Ctest, we first use ChatGPT to generate a list of super-categories for each CoI c Ctest usingthe following super-category prompt:",
  "where p = 3. Subsequently, following Novack et al. , foreach CoI c Ctest, we query ChatGPT to generate a list ofsub-categories using the following sub-category prompt:": ". Summary statistics of synthetic hierarchies generated by the LLM for our experiments. We present the number of label classesin the target vocabulary, the total number of generated super-categories and sub-categories, and the average number of generated super-categories and sub-categories per target class (CoI) for each dataset at each label vocabulary level. Additionally, links are provided to theexperiments using these LLM-generated hierarchies. Note: N/A indicates that only one level of label vocabulary exists in the dataset. :At the most abstract (coarsest) level L1 of iNatLoc, all target classes belong to the single super-category Kingdom \"Animalia\".",
  "Generate a list of q types of the following[context] and output the list separated by&:c": "where q = 10. The [context] prompt is consistently setto object across all datasets, except for iNatLoc , wherecontext-specific prompts like species or genus are used, align-ing with its biological tree of life structure.The & symbolserves as a separator prompt, facilitating the formatting of Chat-GPTs responses for easier post-parsing of category names. More-over, the final lists of super-categories and sub-categories are theunion of results from t = 3 LLM queries. To be more specific,we employ the same super-/sub-category prompts for querying theLLM t = 3 times for each target CoI, and then amalgamate theseLLM responses to form the final results.In order to generate hierarchies for all datasets, we fix p = 3,q = 10, and t = 3. It is important to note that we did not performany extensive hyperparameter tuning for p, q, and t, as our goalis to construct hierarchies automatically and validate SHiNes ef-fectiveness with open and noisy hierarchies. Apart from parsingcategory names from ChatGPTs responses, we do not perform anyadditional cleaning or organizing of the query results, ensuring anunbiased evaluation of our methods inherent efficacy. The hierar-chies generated for the evaluation datasets are directly employedas LLM-generated hierarchies by SHiNe in our experiments to as-sess its performance.Discussion:Differences between our hierarchy generationprocess and the one from CHiLS . For any given targetvocabulary, CHiLS uses GPT-3 to generate only sub-categories,forming a two-level hierarchy. In our work, we adopt the sub-category prompt from CHiLS for generating sub-categories.However, our hierarchy generation strategy significantly differsfrom CHiLS in three key respects: i) We generate both super-categories and sub-categories, creating a more comprehensivethree-level hierarchy. ii) We query our LLM three times (t = 3) and use the union of the outcomes of these queries as the final set,aiming to enrich and diversify the category sets with varied catego-rization principles. iii) As a result of merging and de-duplicatingthe generated category names from three LLM queries, we do nothave a predetermined (fixed) number of super-/sub-categories foreach target CoI (class). Thus, our generated hierarchies are morevaried and imbalanced, aligning more closely with real-world sce-narios.Discussion: The rationale behind generating p = 3 super-categories instead of just one. In real-world contexts, there is nosingle optimal hierarchy for any given vocabulary set. A singlevocabulary can have multiple, equally valid hierarchical arrange-ments, depending on the categorization principles applied. Forexample, \"Vegetable salad\" might be classified under vari-ous super-categoriessuch as \"Appetizer\", \"Cold dish\",\"Side dish\", or simply \"Vegetable\"based on culturalor contextual differences. Therefore, a truly robust and effec-tive hierarchy-based method should function with hierarchiesopen to diverse categorization principles. In such open hier-archies, categories are open to multiple categorization principles(i.e. , one class may link to several super-category nodes). Thus,we choose to generate p = 3 super-categories per target CoI (cat-egory) in Ctest at each single LLM query. In our 3-level synthetichierarchies, each target CoI falls under multiple super-categoriesgenerated from three times of LLM queries, reflecting various anddiverse categorization principles. This approach allows us to rig-orously evaluate the efficacy of our proposed SHiNe in realistic,diverse yet noisy categorization scenarios.",
  "B.2. Summary statistics of the LLM hierarchies": "In Tab. 8, we present comprehensive summary statistics for thesynthetic hierarchies generated by the LLM across each dataset atevery label vocabulary level. All synthetic hierarchies are createdusing p = 3 and q = 10, with the final super-/sub-categories a de- duplicated union of results from t = 3 LLM queries. As shown inTab. 8, the hierarchies synthesized are both highly open (each CoIis linked to multiple super-categories) and noisy (sub-categoriesmight not be present in the dataset). Despite these challenges, asshown in Tab. 3, Tab. 4, Tab. 5, Tab. 12, and , SHiNe per-forms effectively using such open and noisy synthetic hierarchies,consistently improving the baseline results. This underlines theadaptability and robustness of SHiNe in using open and noisy se-mantic hierarchies when the ground-truth hierarchies are not avail-able.",
  "C.1. Hierarchy-aware Sentences Integration": "This section provides a further explanation of SHiNes process forintegrating hierarchy-aware sentences with different hierarchicalstructures, as illustrated in .Single super-category path hierarchy case (ground-truth hier-archy structures). (a) illustrates the case where the targetClass of Interests (CoI) is linked to a unique super-category at eachhigher hierarchical level and multiple sub-categories at each lowerlevel. In this case, SHiNe employs the Is-A connector to formhierarchy-aware sentences by integrating the lowest linked sub-category, the target CoI, and the highest super-category, followingtheir hierarchical relationships in a bottom-up manner. As a result,the total number of constructed sentences in this case equals thenumber of the lowest linked sub-categories.Multiple super-category path hierarchy case (LLM-generatedhierarchy structures). (b) displays the case where the tar-get CoI is linked to multiple super-categories at the upper leveland several sub-categories at the lower level. Here, SHiNe buildshierarchy-aware sentences by iterating through all combinations ofthe linked sub-categories, super-categories, and the target CoI. TheIs-A connector is used to connect these categories in a specific-to-abstract order. The resulting number of constructed sentences inthis case equals the product of the counts of the lowest linked sub-categories and the linked super-categories.",
  "C.3. Time Complexity Analysis of SHiNe": "Let c be the number of Classes of Interest (CoIs) in a given vo-cabulary, and let p and q represent the average number of re-lated super-categories and sub-categories per CoI, respectively, ina hierarchy. Our proposed method, SHiNe, aggregates hierarchy-aware information from both super-categories and sub-categoriesinto c nexus-based embeddings (offline). Consequently, at infer-ence, both memory and time complexity of SHiNe scale linearlyas O(c). It is important to note that this scalability at inference isunaffected by the number of related super-/sub-categories, becausethey are only used offline to generate nc. The offline pipeline toconstruct SHiNe OvOD classifier needs to run only once.In contrast, the time and memory complexities for CHiLS scale at inference as O(c(1 + q)), because image-text similar-ity scores are computed for vocabulary nodes and all their chil-",
  "Retrieved Super-/Sub-categoriesHierarchy-aware SentencesRetrieved Super-/Sub-categoriesHierarchy-aware Sentences": ". Examples of integrating hierarchy-aware sentences with different hierarchy structures. We use \"Bat\" as an example of the targetClass of Interest (CoI) for example. The retrieved super-/sub-categories and the target CoI are color-coded in blue and red, and green,respectively. (a) The target CoI is linked to a unique super-category at each higher hierarchy level and multiple sub-categories at eachlower level, akin to the ground-truth hierarchy structure of the datasets. (b) The target CoI is associated with multiple super-categories atthe upper hierarchy level and multiple sub-categories at the lower level, akin to the simple three-level LLM-generated hierarchy structures. dren. H-CLIP , on the other hand, involves a search proce-dure online across p (c + 1) prompt combinations for the top k(e.g. , k = 5) predicted CoIs, resulting in a time complexity ofO(c + p (q + 1) k). Crucially, the operations for p (q + 1) konly commence after the prediction based on the first c standardprompts. Unlike SHiNe and CHiLS , for which the embed-dings are precomputed and the class predictions can be fully paral-lelized, H-CLIP requires encoding the latter p(q+1)k CLIP text embeddings at test time on-the-fly. Furthermore, it employsa search-on-the-fly mechanism, resulting in significant computa-tional overheads. This makes H-CLIP a sub-optimal candidatefor many applications, particularly those like detection and seg-mentation tasks that require per-box, per-mask, or even per-pixelprediction.Given the extensive number of super-/sub-categories in the hi-erarchy employed in our experiments, as detailed in Tab. 8, thesubstantial computational overheads imposed by CHiLS and H-CLIP become evident.",
  "k=1Etxt (eck) ,(3)": "where Etxt is the frozen CLIP text encoder, and {eck}Kk=1 rep-resents the K hierarchy-aware sentences, which are built by in-tegrating all super-/sub-categories related to the target class (CoI)c using our proposed Is-A connector. This aggregator, which wecall the mean-aggregator, calculates the mean of the encoded sen-tences embeddings to form the final nexus-based classifier weightvector for c. This mean vector is the centroid represented withinCLIPs embedding space, summarizing the general characteristicsof the hierarchy-aware embeddings related to the target CoI. Atinference, the classification decision for a region is based on thecosine similarity between the visual embedding of the region andthe hierarchy-aware representation defined by the mean vector n,which we call nexus. This approach renders the decision-making process less sensitive to variations in the semantic granularity ofthe name c. Note that all the embeddings are l2-normalized.Principal Eigenvector Aggregator Drawing inspiration fromtext classification techniques in Natural Language Processing(NLP) , we introduce an alternative aggregationapproach, called the principal eigenvector aggregator.Thismethod uses the principal eigenvector of the sentence embeddingsmatrix as the classifier weight vector nc. Specifically, for a set ofhierarchy-aware sentences {eck}Kk=1, we first apply a Singular Vec-tor Decomposition (SVD) operation on their embedding matrix as:",
  "USVT = SVDconcatKk=1 {Etxt (eck)},(4)": "where U and V are orthogonal matrices representing the left andright singular vectors, respectively, and S is a diagonal matrixwith singular values in descending order. Subsequently, we canderive the principal eigenvector, corresponding to the largest sin-gular value in the sentence embedding matrix, by selecting the firstcolumn of matrix V as:",
  "nc = V[:, 0] ,(5)": "where nc serves as the nexus-based classifier vector for the tar-get class c.In contrast to the mean-aggregator, the principaleigenvector aggregator captures the dominant trend in the sen-tence embeddings (as known as their theme, to maintain NLPterminology), to effectively represent the CoIs. Note that all theembeddings are l2-normalized.Next, we explain the rationale behind this aggregator design.In high-dimensional semantic spaces like the 512-dimensionalvision-language aligned embedding space of CLIP ViT-B/32, theprincipal eigenvector is able to capture the most significant seman-tic patterns or trends within the embeddings. This approach stemsfrom the understanding that the direction of greatest variance inthe space contains the most informative representation of seman-tic embeddings. Projecting the high-dimensional hierarchy-awaresentence embeddings of a target class (CoI) onto this principaleigenvector yields a condensed yet information-rich representa-tion, preserving the essence of the original hierarchy-aware sen-tences. Consequently, during inference, classification decisionsfor a region are based on the cosine similarity between the regions embedding and the semantic pattern or trend depicted by the prin-cipal eigenvector. This differs from the representation centroidapproach used by the mean-aggregator.We compare the mean-aggregator and the principal eigen-vector aggregator in Sec. 4.1 of the main paper. While the princi-pal eigenvector aggregator shows slightly lower performance com-pared to the mean-aggregator in general, its potential applicationin VLM tasks might be interesting for future research. In gen-eral, given the intimate connection between computer vision andNLP in open-vocabulary models, we believe in the importance ofenabling more connections between the two fieldsin this case,drawing from the NLP field of topic modeling.",
  "D. Extended Analysis of SHiNe on FSOD": "In , we present an expanded study of the core componentsof SHiNe, examining their effectiveness across various levels oflabel granularity on both the iNatLoc and FSOD datasets. Theresults from FSOD align with those observed in the iNatLoc-onlystudy shown in of the main paper. Next, we provide furtheranalysis of SHiNes core components.Extended discussion: the Is-A connector effectively integrateshierarchy knowledge in natural sentences. The effectiveness ofthe proposed Is-A connector is studied in (a). Excludingthe top (abstract) levels where all methods, including Ens, Con-cat, and Is-A, revert to the plain baseline due to the absence offurther parent nodes, the methods leveraging super-category in-formation consistently outperform the baseline across nearly alllevels of granularity.This improvement is attributed to direct-ing the models focus towards more general concepts via super-category-inclusive classifiers. An exception occurs at the secondlevel of FSOD ((a-FSOD-L2)), where no method exceedsthe baseline. We speculate that at this level, target categories like\"Fruit\" are already highly abstract, rendering the addition ofmore abstract parent categories like \"Food\" redundant in clari-fying ambiguities. Nevertheless, this challenge is alleviated whensub-categories are also included in the aggregation step. In com-parative terms, the Is-A and Concat connectors yield greater gainsthan Ens, highlighting the advantage of capturing internal seman-tic relationships for distinguishing between classes. Notably, ourIs-A connector surpasses Concat at all levels of granularity in bothdatasets, improving the baseline mAP50 by up to +39.4 points oniNatLoc ((a-iNat-L5)) and +2.5 points on FSOD ((a-FSOD-L3)). This indicates the superior effectiveness of Is-As ex-plicit modeling of category relationships compared to the mere se-quential ordering of class names from specific to abstract by Con-cat. Overall, the integration of more abstract concepts proves ben-eficial in object detection across diverse label granularities, withour Is-A connector particularly excelling due to its effective in-corporation of hierarchical knowledge into natural language sen-tences, achieved by explicitly modeling internal category relation-ships.Extended discussion: A simple mean-aggregator is sufficientfor hierarchy-aware sentences fusion. The impact of the aggre-gation step is analyzed in (b), focusing on both the mean-aggregator (M-Agg) and the principal eigenvector aggregator (PE-Agg).These aggregators consistently outperform the baselineacross various models and levels of label granularity in all datasets. Notably, their advantage becomes more pronounced with increas-ingly abstract target vocabularies, surpassing the benchmarks setby the Is-A method. This is especially evident in cases involvinghighly abstract label vocabularies, where these aggregation meth-ods significantly improve baseline performance, achieving gainsof up to +9.8 points in iNatLoc ((b-iNat-L1)) and +20.5points in FSOD ((b-FSOD-L1)).These results underscore the effectiveness of the aggregationstep in fusing hierarchy-aware sentences into semantic nexus-based classifiers.This fusion allows the nexus-based classifierto use both specific knowledge from sub-categories and abstractknowledge from super-categories, thereby improving the baselinedetectors ability to discriminate visual object robustly.",
  "E. Comparison with Additional Baselines": "To further validate the effectiveness of the proposed Is-A prompt-ing method, we further compare SHiNe with two additionalbaselines: i) Root-Stmt prompting, which explicitly states theroot (target) class and its super/sub-classes using the template like\"A bat, which is a sports equipment and canbe instantiated in a wooden baseball bat ora baseball bat\"; ii) 80-Prompts, where we embed thetarget class name into the 80 hand-crafted prompts from CLIP and average the scores. As shown in Tab. 9, methods leveraging ahierarchy consistently surpass the 80-prompt ensemble baseline,demonstrating the benefits of leveraging hierarchy knowledge.Moreover,SHiNes superior performance to the Root-Stmtbaseline suggests that Is-A prompting and nexus aggregation ismore effective for combining hierarchy information.",
  "F. Extended Main Experimental Results": "In Tab. 10, we present additional experimental results from apply-ing our proposed SHiNe to Detic with a Swin-B back-bone, trained using only LVIS and LVIS combined with IN-L as auxiliary weak supervisory signals. This observation is consis-tent with those in Tab. 3 from the main paper, demonstrating thatSHiNe consistently and substantially improves the performance ofthe baseline OvOD detector on both iNatLoc and FSOD datasets.This improvement spans across various label vocabulary granular-ities and is evident with both the ground-truth hierarchy (GT-H)and a synthetic hierarchy generated by LLM (LLM-H).",
  "FSOD: 3 levels": ". Further study of hierarchy-aware sentence integration methods (left) and aggregators (right) across various label granularitylevels on both iNatLoc and FSOD datasets. Darker color indicates higher mAP50. Components used by default in SHiNe are underlined.Detic with Swin-B backbone, trained using various combinations of supervisory signals described in Tab. 2, serves as the baselineopen-vocabulary detector for all methods evaluated. To evaluate the effectiveness of hierarchy-based components, we use the ground-truthhierarchy for all methods that rely on hierarchies.",
  "ResNet-50 BackboneSwin-B Backbone": ". Additional summary statistics across all levels for the main experimental results in Tab. 3 for iNatLoc (upper) and FSOD (lower),respectively. This summary includes various measures for mAP50, such as arithmetic mean (AM), harmonic mean (HM), geometric mean(GM), minimum value (Min), median (Med), and maximum value (Max), calculated across all granularity levels within each dataset. Alarger area indicates better performance across various metrics. Gray dashed gridlines are scaled from 10 (innermost) to 100 (outermost). . Additional results are provided for Detic with aSwin-B backbone, trained using both I-LVIS and II-LVIS+IN-Lsupervisory signal combinations. Detection performance acrossvarying label granularity levels on iNatLoc (upper) and FSOD(lower) datasets are reported. SHiNe is directly applied to thebaseline detector (BL) with ground-truth (GT-H) and LLM-generated (LLM-H) hierarchies. mAP50 (%) is reported.",
  "L1 18.0 38.5(+20.5) 23.7(+5.7)18.2 35.9(+17.7) 22.3(+4.1)": "ing various summary statistical metrics. As shown in , ourproposed SHiNe consistently and markedly enhances the baselineOvOD detectors performance across a range of summary met-rics, including arithmetic mean (AM), harmonic mean (HM), geo-metric mean(GM), on both datasets. The harmonic and geometricmeans are employed to present the evaluation results from diverseperspectives, particularly in contexts where extreme values mightskew the interpretation. These means are less influenced by ex-treme values, such as exceptionally high or low mAP50 scores atspecific granularity levels. The enhancement from SHiNe is ap-parent when employing both the ground-truth hierarchy and a syn-thetic hierarchy generated by the LLM. Notably, SHiNe most sig-nificantly improves the baselines weakest performance (minimummAP50), suggesting a notable improvement in performance con-sistency by improving the minimum achieved performance acrossgranularity levels. These results demonstrate that SHiNe not onlyboosts overall performance but also enhances consistency acrossdifferent vocabulary granularities, a crucial aspect for real-worldapplications.",
  "I. Further Experiments on COCO/LVIS": "This section extends the evaluation of SHiNe to COCO andLVIS , following the open-vocabulary evaluation (OVE) pro-tocol as described in . According to the OVE protocol, datasetsare divided into base and novel classes; models are trained on baseclasses with bounding box annotations and then evaluated on novelclasses and their union. The base classes are disjoint from the . Comparison with CORA and VLDet (VLD) on iNatLoc and FSOD. SHiNe is applied to the baseline meth-ods, respectively. All methods employ ResNet-50 as back-bone.Note that CORA uses only box-annotated COCO base split for training, while VLDet uses box-annotated LVIS and image-caption-annotated CC3M as supervisory signals.mAP50 (%) is reported.",
  "L1 11.626.2(+14.6) 14.1(+2.5)12.831.0(+18.2) 17.3(+4.5)": ". Comparison of detection performance on COCO andLVIS benchmarks using the OVE protocol. We use Detic with a ResNet-50 backbone as the baseline detector (BL). SHiNeis applied to the baseline using hierarchies generated by LLM. Allmodels receive strong supervision on the base class partitions ofboth datasets, with box-class annotations. A comparison of differ-ent weak supervisory signals is also included. mAP50novel andmAPnovel denote performance evaluated on the novel class par-titions (17 classes for COCO and 337 classes for LVIS), whilemAP50all and mAPall represent evaluations on both base andnovel classes (65 classes for COCO and 1203 classes for LVIS).",
  "Conceptual Captions19.321.5(+2.2)33.433.5(+0.1)": "novel classes. We follow the base/novel class partitions for COCOand LVIS as used in . Both datasets have a single, flat classvocabulary: COCO with 65 classes (48 base, 17 novel) and LVISwith 1203 classes (866 base, 337 novel). We use Detic witha ResNet-50 backbone, trained on the box-class annotatedbase classes with various weak supervisory signals, as the base-line OvOD detector in this experiment. Specifically, the baselineis trained on COCO-base with 48 classes or LVIS-base with 866classes. We explore three types of weak supervisory signals asproposed in : i) N/A, using only strong supervisory signals;ii) IN-L, a 997-class subset of ImageNet-21k intersecting withthe LVIS vocabulary; iii) Conceptual Captions dataset; andiv) COCO Captions dataset. For Conceptual Captions andCOCO Captions, nouns are parsed from the captions, and bothimage labels and captions are used for weak supervision . Wereport mAP50 for COCO and the official mask mAP metric forLVIS as suggested in .",
  "L1": ". Qualitative detection results of SHiNe applied to Detic with Swin-B , evaluated on the FSOD dataset across threedifferent label granularity levels. All models are trained using the LVIS + IN-L dataset as strong and weak supervisory signals, respectively.It is advisable to zoom in for a clearer view. We evaluate and compare SHiNe with the baseline under theOVE protocol. In the absence of available ground-truth hierarchyinformation, we use the LLM to generate simple 3-level synthetichierarchies for the target vocabularies of COCO and LVIS, as de-scribed in Tab. 8. Consequently, SHiNe is constructed using thesegenerated hierarchies. As shown in the OVE evaluation resultsin Tab. 12, SHiNe notably improves the performance of the base-line detector on both COCO and LVIS benchmarks under the OVEprotocol. Interestingly, SHiNe yields a greater performance gainon the novel class partitions. However, this advantage becomesless pronounced when assessing combined base and novel classes.This is attributed to the model overfitting on the base classes tothe text classifier based on the standard \"a {Class Name}\"prompts during strongly supervised training. Replacing this over-fit classifier with the SHiNe classifier leads to significant gains onnovel class partitions, but slightly reduces performance on baseclass partition test data.Nevertheless, the consistent improve-ments achieved by SHiNe across most cases in Tab. 12 underscoreits effectiveness on the COCO and LVIS benchmarks."
}