{
  "Query video: 5253B (Action Type), 3.2 (Difficulty Degree), 75.2 (Action Score)": ". An overview of fine-grained spatial-temporal action parser (FineParser). It enhances human-centric foreground action repre-sentations by exploiting fine-grained semantic consistency and spatial-temporal correlation between video frames, improving the AQAperformance. Green, red, yellow, and blue dashed lines represent the fine-grained alignment of target actions between query and exemplarvideos in time and space within the same semantics.",
  "Abstract": "Existing action quality assessment (AQA) methodsmainly learn deep representations at the video level forscoring diverse actions. Due to the lack of a fine-grainedunderstanding of actions in videos, they harshly sufferfrom low credibility and interpretability, thus insufficientfor stringent applications, such as Olympic diving events.We argue that a fine-grained understanding of actions re-quires the model to perceive and parse actions in both timeand space, which is also the key to the credibility and inter-pretability of the AQA technique. Based on this insight, wepropose a new fine-grained spatial-temporal action parsernamed FineParser. It learns human-centric foreground ac-tion representations by focusing on target action regionswithin each frame and exploiting their fine-grained align-ments in time and space to minimize the impact of in-valid backgrounds during the assessment. In addition, we",
  "*Corresponding author": "construct fine-grained annotations of human-centric fore-ground action masks for the FineDiving dataset, calledFineDiving-HM. With refined annotations on diverse targetaction procedures, FineDiving-HM can promote the devel-opment of real-world AQA systems. Through extensive ex-periments, we demonstrate the effectiveness of FineParser,which outperforms state-of-the-art methods while support-ing more tasks of fine-grained action understanding. Dataand code are available at",
  "arXiv:2405.06887v1 [cs.CV] 11 May 2024": "support in action quality assessment (AQA). This consider-ably impacts sports analysis, helping evaluate athlete per-formance, designing targeted training programs, and pre-venting sports injuries.Unlike general videos, sports videos are sequential pro-cesses with explicit procedural knowledge. Athletes have tocomplete a series of rapid and complex movements. Takingdiving as an example, athletes will stretch, curl, and movetheir limbs and joints to finish different somersaults withthree body positions, including straight, pike, and tuck, in-terspersed with varying twists. Then, the referee will assessthe scores based on the athletes take-off, somersault, twists,and entry. To achieve better competitive performance, ath-letes (1) take off decisively and forcefully at the right angleand with a proper height; (2) perform beautiful body posi-tions, quick somersaults, and twists in the flight; (3) enterthe water with a posture perpendicular to the surface, avoid-ing splashing water around. According to the diving rules,just a few degree differences in the take-off angle/height andthe verticality of entry into the water can affect the numberof points deducted. The difficulty lies in whether the humaneye can accurately discern such subtle differences.To address this issue, many video understanding-basedAQA methods lack a fine-grained under-standing of actions in videos. They cannot solve the prob-lem of limitations of human eye judgment and lack credibil-ity, which is inadmissible in real-world applications. Thereis an urgent need for a fine-grained understanding of ac-tions, i.e., parsing the internal structures of actions in timeand space with semantic consistency and spatial-temporalcorrelation, to obtain precise action representations and im-prove the usefulness of the AQA system.To this end, we present a new framework for fine-grained action understanding, which learns human-centricforeground action representations with context informationby developing a new fine-grained spatial-temporal actionparser named FineParser. FineParser consists of four com-ponents: (1) spatial action parser (SAP); (2) temporal ac-tion parser (TAE); (3) static visual encoder (SVE); (4) fine-grained contrastive regression (FineReg). Given query andexemplar videos, SAP first models the intra-frame featuredistribution of each video by capturing multi-scale repre-sentations of human-centric foreground actions. The criticalregions are concentrated around the athletes body, spring-board (or platform), and splash, guaranteeing the spatialparsing to be credible and visually interpretable.Then,TAP models semantic and temporal correspondences be-tween videos by learning their spatial-temporal representa-tions and parsing the actions into consecutive steps. Com-bining TAP and SAP, FineParser learns the target actionrepresentations at the fine-grained level, ensuring seman-tic consistency and spatial-temporal correspondence acrossvideos. In addition, SVE enhances the above target action representations by capturing more contextual details. Fi-nally, FineReg can quantify the quality differences in pair-wise steps between query and exemplar videos and assessthe action quality.To promote the evaluation of credibility and visual inter-pretability of FineParser, we densely label human-centricforeground action regions of all videos in the FineDivingdataset and construct additional mask annotations, namedFineDiving-HM. Experimental results demonstrate that ourfine-grained actions understanding framework accuratelyassesses diving actions while focusing on critical regionsconsistent with human visual understanding.The contributions of this paper are summarized as fol-lows: (1) We propose a new fine-grained spatial-temporalaction parser, FineParser, beneficial to the AQA task viahuman-centric fine-grained alignment. (2) FineParser cap-tures the human-centric foreground action regions withineach frame, minimizing the impact of invalid backgroundin AQA. (3) We provide human-centric foreground actionmask annotations for the FineDiving dataset, FineDiving-HM, which we will release publicly to facilitate the evalu-ation of credibility and visual interpretability of the AQAsystem.(4) Extensive experiments demonstrate that ourFineParser achieves state-of-the-art performance with sig-nificant improvements and better visual interpretability.",
  ". Related Work": "Fine-grained Action Understanding. With ongoing ad-vancements in action understanding, analyzing actions infiner granularity has become inevitable. Current endeav-ors in fine-grained action understanding mainly encompasstasks such as temporal action detection , ac-tion recognition , video question answering, and video-text retrieval . Recently, Shao etal. constructed FineGym that provides coarse-to-fineannotations temporally and semantically for facilitating ac-tion recognition. Chen et al. proposed SportsCap thatestimates 3D joints and body meshes and predicts actionlabels. Li et al. introduced MultiSports with spatio-temporal annotations of actions from four sports. Zhanget al. constructed a temporal query network to answerfine-grained questions about event types and their attributesin untrimmed videos. Li et al. presented a hierarchi-cal atomic action network that models actions as combi-nations of reusable atomic ones to capture the common-ality and individuality of actions. Zhang et al. intro-duced a fine-grained video representation learning methodto distinguish video processes and capture their temporaldynamics. These methods mainly concentrated on a fine-grained understanding of the temporal dimension. In con-trast, our FineParser captures human-centric action repre-sentations by simultaneously building a fine-grained under-standing in both time and space.",
  "Exemplar": ". The architecture of the proposed FineParser. Given a pair of query and exemplar videos, spatial action parser (SAP) and temporalaction parser (TAP) extract spatial-temporal representations of human-centric foreground actions in pairwise videos, as well as predict bothtarget action masks and step transitions. The static visual encoder (SVE) captures static visual representations combined with the targetaction representation to mine more contextual details. Finally, fine-grained contrastive regression (FineReg) utilizes the representations topredict the action score of the query video. Action Quality Assessment.In early pioneering work,Pirsiavash et al. formulated the AQA task as a re-gression problem from action representations to scores, andParisi et al. adopted the correctness of performed ac-tion matches to assess action quality. Parmar et al. demonstrated the effectiveness of spatio-temporal featuresfor estimating scores in various competitive sports.Re-cently, Tang et al. introduced an uncertainty-awarescore distribution learning method to alleviate the ambi-guity of judges scores. Yu et al. developed a con-trastive regression based on video-level features, enablingthe ranking of videos and accurate score prediction. Wanget al. introduced TSA-Net to generate action repre-sentations using the outputs of the VOT tracker, improv-ing AQA performance.Xu et al. contributed to afine-grained sports video dataset for AQA and proposeda new action procedure-aware method to improve AQAperformance. Zhang et al. proposed a plug-and-playgroup-aware attention module to enrich clip-wise represen-tations with contextual group information. In contrast, ourFineParser parses action in space and time to focus on thehuman-centric foreground action, improving AQAs credi-bility and visual interpretability.",
  ". Problem Formulation": "Given a pair of query and exemplar videos with the sameaction type, denoted as (X, Z), our approach is formu-lated as a fine-grained understanding framework that pre-dicts the action score of the query video X. Inspired by fine-grained contrastive regression , our framework consid-ers fine-grained quality differences between human-centricforeground actions in both time and space perspectives tomodel variations in their scores. The core is a new fine-grained action parser, FineParser F, represented as",
  ". Fine-grained Spatio-temporal Action Parser": "FineParser is composed of four core components. In short,SAP, TAP, and SVE collaborate to learn fine-grained targetaction representations, and FineReg then uses these repre-sentations to predict the final score. Spatial Action Parser (SAP). SAP parses the target actionfor each input video at a fine-grained spatial level. Inspiredby I3D and its fully convolutional version , trans-posed convolution layers are introduced before each maxpooling layer to upsample the outputs of I3D submodules,and the rest after the last average pooling layer is discarded.These operations facilitate capturing multi-scale visual andsemantic information that spans from short-term local fea-tures obtained from lower layers to long-term global seman-tic context derived from the last few layers.Concretely, taking the query video X={Xi}Ni=1 as anexample, the first I3D submodule B1 encodes each snip-pet Xi to capture short-term local features, as B1(X) ={B1(Xi)}Ni=1. Similarly, other three submodules encode Xito obtain middle representations, as Bj(X)=Bj(Bj1(X)),with j. For each Bj(X), two upsampling blocks arefurther inserted, denoted as Bupj,1 and Bupj,2. Both compriseconvolution layers performed on the feature dimension andtranspose convolution layers performed on both spatial andtemporal dimensions. They can be presented as",
  "Mfusei= Conv3d(Concat({Mup1j,i }4j=1)),(3)": "where {Mup2j,i }4j=1 are the predicted target action masksfrom different scales for optimizing SAP. These masks cap-ture multi-scale human-centric foreground action informa-tion, from short-term local features obtained from lowerlayers (small scale) to long-term global semantic contextderived from the last few layers (large scale).Mfuseiisthe final target action mask of Xi by fusing {Mup1j,i }4j=1.SAP generates the above five target action masks and onetarget action mask embedding B5(X), where the formerare used to anticipate the human-centric foreground actionmask, and the latter facilitates learning target action repre-sentations. With mask embedding B5(X) and video em-bedding B(X), target action representations XV are cal-culated by elements-wise multiplication, as XV =B(X) sigmoid(B5(X)). For the exemplar video Z, the target ac-tion representations ZV can be obtained similarly.Temporal Action Parser (TAP). TAP parses each actionprocedure into consecutive steps with semantic and tempo-ral correspondences. Specifically, PSNet is adopted toparse XV and ZV , which identifies the temporal transitionwhen the step switches from one sub-action type to another.Supposed that L step transitions are needed to be identi-fied in the action, the submodule S predicts the probabil-ity of the k-th step transiting at the t-th frame, denoted asS(XV )[t, k]R. By",
  "the timestamp tk of the k-th step transition is predicted foreach k [1, L]. Based on {tk}Lk=1, each action procedure": "is parsed into L+1 consecutive steps, i.e., {XlV }L+1l=1 and{ZlV }L+1l=1 , where l is the index of step. While the lengthsof the above consecutive steps may differ in nature, they arefixed to the same size via downsampling or upsampling op-erations f along the temporal axis, ensuring that the dimen-sions of query and key are matched in the attention model.Therefore, the target action representations of query and ex-emplar videos become {f(XlV )}L+1l=1 and {f(ZlV )}L+1l=1 .Static Visual Encoder (SVE). SVE captures more contex-tual information to further enhance the action representa-tions, especially for high-speed and complex actions likediving. It consists of two submodules: a ResNet model Tand a set of projection functions {fl}L+1l=1 . For the inputvideo X, the outputs of T can be obtained by",
  "(5)": "Through post-projection, the static visual representations ofX can be written as {fl(XlS)}L+1l=1 . Similarly, the static vi-sual representation of Z are {fl(ZlS)}L+1l=1 .Fine-grained Contrastive Regression (FineReg). It lever-ages the sequence-to-sequence representation ability of thetransformer to learn powerful representations from pairwisesteps and static visual representations via cross-attention.Specifically, the target action representations of pairwisesteps f(XlV ) and f(ZlV ) interact with each other, helpingthe model focus on the consistent regions of motions in thecross-attention to generate the new features DVl . Similarly,cross-attention between the static visual representations ofpairwise steps fl(XlS) and fs(ZlS) generates the new fea-tures DSl . Based on these two generated representations ofthe l-th step pairs, FineReg quantifies step quality differ-ences between the query and exemplar by learning relativescores. This guides the framework to assess action qualityat the fine-grained level with contrastive regression R. Thepredicted score yX of the query video X is calculated as",
  "t(pk(t)log St,k+(1pk(t)) log(1St,k)), (10)": "where St,k=S(XV )[t, k] is the predicted probability of thek-th step transiting at the t-th frame, and pk is a binarydistribution encoded by the ground truth timestamp tk ofthe k-th step transition, with pk(tk)=1 and pk(tm)|m=k=0. LReg is used to optimize RV and RS by minimizingthe mean squared error between the ground truth yX andprediction yX, which is written as",
  ". Datasets": "FineDiving-HM. FineDiving contains 3,000 videoscovering 52 action types, 29 sub-action types, 23 difficultydegree types, fine-grained temporal boundaries, and offi-cial action scores.To evaluate the effectiveness of ourFineParser and make the results more credible and inter-pretable visually, we provide additional human-centric ac-tion mask annotations for the FineDiving dataset in thiswork, called FineDiving-HM. FineDiving-HM contains312,256 mask frames covering 3,000 videos, in whicheach mask labels the target action region to distinguish thehuman-centric foreground and background.FineDiving-HM mitigates the problem of requiring frame-level annota-tions to understand human-centric actions from fine-grainedspatial and temporal levels. We employ three workers withprior diving knowledge to double-check the annotationsto control their quality. shows some examples ofhuman-centric action mask annotations, which precisely fo-cus on foreground target actions. There are 312,256 fore-ground action masks in FineDiving-HM, where the num-ber of action masks for individual diving is 248,713 andthat for synchronized diving is 63,543. As shown in ,the largest number of action masks is 35,287, belonging tothe action type 107B; the second largest number of actionmasks is 34,054, belonging to the action type 407C; and",
  ". Comparisons of performance with state-of-the-art AQAmethods on the FineDiving-HM Dataset. Our result is highlightedin the bold format": "the smallest number of action masks is 101, correspondingto the action types 109B, 201A, 201C, and 303C. Coachesand athletes can use the above statistics to develop compe-tition strategies, for instance, what led to the rise of 107Band 407C and how athletes gain a competitive edge.MTL-AQA. It is a multi-task action quality assessmentdataset consisting of 1,412 samples collected from 16different world events, with annotations containing the de-gree of difficulty, scores from each judge (7 judges), type ofdiving action, and the final score.",
  ". Evaluation Metrics": "Action Quality Assessment. Following previous efforts, we utilize Spearmans rank correlation(, the higher, the better) and Relative 2 distance (R2, thelower, the better) for evaluating the AQA task.Temporal Action Parsing. Given the ground truth bound-ing boxes and a set of predicted temporal bounding boxes,we adopt the Average Intersection over Union (AIoU) to evaluate the performance of TAP. The higher the value ofAIoU@d, the better the performance of TAP.Spatial Action Parsing. We adopt three evaluation metricsfor comparison: MAE , F-measure F ( = 0.3) ,and S-measure Sm . MAE (the lower, the better) mea-sures the average pixel-wise absolute error between the bi-nary ground truth mask and normalized saliency predictionmap. F-measure (the higher, the better) comprehensivelyconsiders precision and recall by computing the weightedharmonic mean. S-measure (the higher, the better) evaluatesthe structural similarity between the real-valued saliencymap and the binary ground truth, considering object-aware(So) and region-aware (Sr) structure similarities (=0.5). .The distribution of human-centric foreground actionmasks. The largest number of mask instances is 35,287, belongingto the action type 107B. The smallest number of mask instances is101, containing the action types 109B, 201A, 201C, and 303C.",
  ". Implementation Details": "We adopted the I3D model pre-trained on the Kinetics as the backbone of the SAP and TAP modules, where SAPis composed of {Bj}5j=1 and {Bupj,1, Bupj,2}4j=1 with the ini-tial learning rate 103 and TAP consists of B and S withthe initial learning rate 104. SAP and TAP did not shareparameters. Besides, we set the initial learning rates of T(i.e., ResNet34) in SVE as 103. We utilized Adam optimizer and set weight decay as 0. In SAP and TAP, fol-lowing previous works , we extracted 96 framesfor each video and split them into 9 snippets, where eachsnippet contains 16 continuous frames with a stride of 10frames. We set L as 3 and the weights {l}Ll=1 as {3, 5, 2}.Furthermore, we followed the exemplar selection criterionin and on the FineDiving-HM and MTL-AQAdatasets, respectively. Following the experiment settings in, we selected 75 percent of samples for trainingand 25 percent for testing in all the experiments.",
  ". Comparison with the State-of-the-Arts": "FineDiving-HM. Tab. 1 summarized the experimental re-sults of state-of-the-art AQA methods on the FineDiving-HM dataset.Our FineParser significantly improved theperformance of Spearmans rank correlation and Relative2-distance compared to all methods.The advantagesof FineParser stemmed from a fine-grained understand-ing of human-centric foreground actions, which requiresthe model to parse actions in time and space, making themodel credible and interpretable visually.Compared toC3D-LSTM, C3D-AVG, MSCADC, I3D+MLP, USDL, and",
  ". Comparisons of performance with representative AQAmethods on the MTL-AQA dataset. Our result is highlighted inthe bold format": "MUSDL, FineParser outperformed them significantly andachieved 24.66%, 10.64%, 17.47%, 6.59%, 6.05%, and1.94% performance improvements in terms of Spearmansrank correlation as well as 0.8165, 0.3649, 0.6725, 0.2365,0.2198, and 0.0872 in Relative 2-distance. Compared toCoRe, FineParser obtained 1.27% and 0.0546 performanceimprovements on Spearmans rank correlation and Relative2-distance. FineParser further improved the performanceof TSA on Spearmans rank correlation and Relative 2-distance, which also can be observed in the TAP metric.MTL-AQA. Tab. 2 reported the experimental results ofrepresentative AQA methods on the MTL-AQA dataset.Our FineParser outperformed other methods on Spearmansrank correlation. For instance, FineParser achieved betterAQA performance than CoRe and TSA-Net, demonstratingthe effectiveness of additional human-centric foregroundaction masks and the meticulous design of a fine-grainedaction understanding of FineParser.",
  ". Ablation Study": "We conducted an ablation study on the FineDiving-HMdataset to demonstrate the effectiveness of individual partsof FineParser by designing different modules, differentbackbones of SVE, and varied step durations of the pro-jection function in SVE.Different Modules in FineParser. We summarized the ex-perimental results in Tab. 3. Under Spearmans rank corre-lation, the AQA performance of the model with SVE andTAP can be improved from 0.9334 to 0.9351. Significantimprovements on and are directlyproportional to the accuracy of action quality assessment,demonstrating that SVE can help the model perform moreaccurate temporal action parsing in the TAP module. Fur-ther introducing the SAP module into the model, the AQAperformance can be further enhanced to 0.9435 in Spear-mans rank correlation, demonstrating that incorporatingSAP allows for capturing more characteristics of target ac-tion, achieving more accurate action quality assessment. Ifonly SAP or SVE were introduced, Spearmans rank cor-",
  ". Ablation study on different modules in FineParser onFineDiving-HM. The results of unavailable methods are omitted": "relations would be 0.9313 or 0.9328, respectively, whichcannot achieve the AQA performance of our final version.Different Step Durations in SVE. We studied the influenceof different step durations used in the projection function ofSVE on the AQA performance. As shown in Tab. 4, we setthe step duration as 2, 4, and 8 and then observe that theAQA performance of FineParser is optimal when set to 4. Itis attributed to proper step duration that can benefit miningmore valuable information from human-centric foregroundaction and static visual representations.Different Backbones of SVE. We conducted several exper-iments on the FineDiving-HM dataset to investigate the ef-fects of different backbones of SVE on the performance ofaction quality assessment. In Tab. 5, ResNet34 outperformsother ResNet architectures while slightly inferior to ViT-S/16. For one thing, ResNet34 has a deeper network depththan ResNet18, allowing it to capture more global and high-level semantic information, whereas ResNet50 may lead tooverfitting on the steps with relatively short durations (e.g.,four frames). In addition, ViT allows the model to capturelong-term dependencies among video frames rather than lo-cal relationships, which is beneficial to learning target ac-tion representations by capturing global features, furtherimproving the AQA performance (i.e., R-2) of FineParser.",
  ". Ablation study on different backbones in SVE": "fine-grained representations for target actions via integrat-ing spatial action parser, temporal action parser, static vi-sual encoder, and fine-grained contrastive regression andachieved state-of-the-art. To understand human-centric ac-tions from fine-grained spatial and temporal levels, we alsoprovided human-centric foreground action mask annota-tions for the FineDiving dataset, named FineDiving-HM, toprovide three quantitative metrics for the credibility and vi-sual interpretability of the AQA model. We hope FineParsercould be a baseline for fine-grained human-centric AQAand facilitate more tasks that require a fine-grained under-standing of sports.Limitations. The human-centric foreground action masksneed to be manually adjusted and labeled. This work con-tributes new human-centric annotations for the dataset ondiving events, while they are challenging to transfer to othercompetitive sports directly."
}