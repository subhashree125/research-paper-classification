{
  "Abstract": "In recent years there has been enormous in-terest in vision-language models trained usingself-supervised objectives. However, the use oflarge-scale datasets scraped from the web fortraining also makes these models vulnerable topotential security threats, such as backdooringand poisoning attacks. In this paper, we pro-pose a method for mitigating such attacks oncontrastively trained vision-language models.Our approach leverages external knowledge ex-tracted from a language model to prevent mod-els from learning correlations between imageregions which lack strong alignment with ex-ternal knowledge. We do this by imposing con-straints to enforce that attention paid by themodel to visual regions is proportional to thealignment of those regions with external knowl-edge. We conduct extensive experiments usinga variety of recent backdooring and poisoningattacks on multiple datasets and architectures.Our results clearly demonstrate that our pro-posed approach is highly effective at defendingagainst such attacks across multiple settings,while maintaining model utility and withoutrequiring any changes at inference time 1.",
  "Introduction": "Recent years have seen enormous interest in vision-language models trained on web-scale image-captioning data using contrastive objectives (Rad-ford et al., 2021; Li et al., 2021a) and text genera-tion objectives (Yu et al., 2022). These models havedrawn great attention due to their superior perfor-mance in many downstream tasks such as zero-shotimage classification (Radford et al., 2021), imagegeneration (Ramesh et al., 2022; Li et al., 2022),and video recognition (Akbari et al., 2021) com-pared to methods trained on smaller superviseddatasets.",
  "Defended Model": ": We defend against both backdooring and poi-soning attacks on vision-language models by encour-aging models to attend to visual regions which alignwith external knowledge. Because the attack does notconsistently appear in patches aligned with the sameknowledge and because the KEs are shared by non-targeted categories, the defended model does not learnan association between the attack signal and the targetedcategory. Although such image-text foundation modelshave demonstrated remarkable performance, sev-eral recent studies have demonstrated that they areparticularly vulnerable to adversarial attacks (Yanget al., 2023c,a; Li et al., 2023) by introducing asmall amount of malicious data (e.g. 75 instancesout of 3 million (Yang et al., 2023c)) into the train-ing data. Practically, this can be achieved by insert-ing imperceptible noise or a backdoor patch intosome images, as shown in 1, and pairing the im-ages with proxy captions controlled by the attacker.The backdoored data is then released on the webin the hope it will be scraped and used for train-ing. Similarly, these models are also susceptible topoisoning attacks, which insert many image-proxycaption pairs into training data leading to unex-pected model behavior (Yang et al., 2023c). Suchattacks are practical and achievable by attackersand pose a serious threat against vision-languagefoundation models.To defend against such attacks, a number of",
  "arXiv:2411.15673v1 [cs.CV] 23 Nov 2024": "a methods have been proposed.For example,Anti-backdoor learning (Li et al., 2021b) proposesto defend against backdoored samples on objectrecognition tasks by using the unique gradientsof these samples to isolate them, but does not ad-dress vision-language (VL) models. More simi-lar to our work, CleanCLIP (Bansal et al., 2023b)proposes a method for defending contrastive VLmodels against backdooring, but does not addressnon-backdoored poisoning attacks as we do. While(Yang et al., 2023c) propose to clean labeled datato mitigate the impact of poisoning, no prior workhas proposed a unified defense mechanism forcontrastively trained VL models that is effectiveagainst both backdooring and poisoning attacks.To address this urgent need, we propose a de-fense method for VL models that defends againstboth backdooring and poisoning attacks.Ourmethod can also be deployed in object recognitionsettings, by casting it as a text retrieval problem fol-lowing (Radford et al., 2021). Our method is moti-vated by the following insight. We note that attacksrely on having models learn correlations between aparticular visual signal and target. However, thesetargeted images share lower-level semantic con-cepts with other, non-targeted categories (See 1).As a consequence, the attack tends not to affect themodels representation of these concepts.Moreover, in the case of backdooring, the attacksignal is applied to various images whose semanticschange in the region on which the attack is applied.For example, in one image the attack may cover abatch associated with paw, while in another imagethe signal is associated with sharp teeth. Thus,the model fails to learn an association betweenthe attack signal and these lower-level semantics.We refer to these lower-level semantic conceptsassociated with objects or captions as KnowledgeElements (KEs). KEs consist of semantic attributes(e.g. round), but also sub-objects (e.g. paw), andrelations. Our defense mechanism aligns with howhumans understand semantics of objects or sen-tences: as collections of semantic units which com-bine together to form higher-level concepts that aremore abstract, compositional and include actions(running) and proto-objects (four-legged ani-mal). We propose to encourage models to relymore heavily on relevant lower level semanticswhen producing their representations. As a con-sequence, our models are much more resistant toattacks.Our method works by learning an alignment be- tween image patches from images and a set of KEsassociated with each image caption. To discoverassociated KEs, prior to training our model weprompt a large language model (Vicuna (Chianget al.)) to list possible KEs for each caption. Wenext perform contrastive image-caption training,but add several new objectives. First, we enforce analignment between image patches and KEs usinga novel multi-instance learning based constraint,since we do not know which patches go with whichKEs. While this aligns image patches and KEs,it does not prevent the model from relying on theattackers visual signal when computing its repre-sentation. Thus, we also propose a second con-straint which enforces that the models attention topatches is proportional to each patchs alignmentwith a KE. That is, if a patch has a low alignmentwith all KEs, the patch should have a low effecton the models representation. Finally, we observethat for attacked samples, the overall patch-KEalignment is much lower. We thus introduce a dy-namic per-sample weight term on the contrastiveloss based on the overall alignment of the KEswith the images patches. This has the effect ofdownweighting the effect of poisoned samples dur-ing training. We evaluate our defense method, Se-mantic Shield, against multiple recent attacks anddefenses on multiple datasets. We observe thatSemantic Shield significantly outperforms prior de-fenses across multiple settings. Our defense tech-nique adds very little overhead at train time, whilemaking models significantly more robust to a widevariety of attacks. The major contributions of thispaper are as follows:",
  "Vision-language contrastive learning": "In recent years, large-scale contrastively trainedvision-language foundation models have demon-strated remarkable performance on a number ofdownstream tasks, even surprassing the perfor-mance of supervised models in some cases (Rad-ford et al., 2021; Yu et al., 2022; Li et al., 2021a;Zhai et al., 2022). While contrastive approacheshave been used to align visual and textual embed-dings for years (Feng et al., 2014; Zhang et al.,2014; Zhang and Lu, 2018; Thomas and Kovashka,2020), recent approaches such as CLIP (Radfordet al., 2021) and ALIGN (Jia et al., 2021) havedemonstrated how training on hundreds of millionsof image-caption pairs scraped from the web canyield powerful generalist image-text foundationmodels which can be applied to many downstreamtasks. CLIP-inspired contrastively trained mod-els have found widespread use in many security-critical applications, including navigation (Dorbalaet al., 2022; Huang et al., 2023; Majumdar et al.,2022), healthcare (Zhang et al., 2022b; Wang andChen, 2022), worksite safety (Tsai et al., 2022),disinformation detection (Zhou et al., 2023; Wanget al., 2023), and many others (Gonzlez-Pizarroand Zannettou, 2023; Shin et al., 2022). Giventheir widespread use, it is critical that contrastivelytrained vision-language models perform in safe andexpected ways. Our work adopts the standard two-stream contrastive architecture proposed in (Rad-ford et al., 2021) and demonstrates how such mod-els can be defended against potential attacks lurk-ing within webly-harvested data.",
  "Poisoning and backdoor attacks": "Data poisoning attacks (Biggio et al., 2012; Zhaoand Lao, 2022; Tolpegin et al., 2020; Xiao et al.,2015), which have been proposed in both super-vised (Koh and Liang, 2017) and unsupervised(Kloft and Laskov, 2010; Biggio et al., 2013b)settings, involve introducing mislabeled (or mis-aligned) data into the models training set. At testtime, models behave in unexpected and attacker-influenced ways when presented with the poisonedexamples seen during training. While targeted poi-soning attacks target specific examples introducedduring training, backdoor attacks can be applied toany image. Backdooring attacks are a type of datapoisoning attack where an attacker introduces a spu-rious signal, such as patches (Saha et al., 2020; Gu et al., 2019) or imperceptible perturbations (Doanet al., 2021b; Nguyen and Tran, 2021; Doan et al.,2021a; Phan et al., 2022) into an image. Mod-els learn to associate the introduced signal withthe targeted concept. While poisoning and back-door attacks have traditionally targeted supervisedlearning settings, recent work has shown that con-trastively trained vision-language models are par-ticularly vulnerable (Zhang et al., 2022a; Carliniand Terzis, 2022). (Carlini and Terzis, 2022) showthat by introducing as few as 3 out of 3 millionsamples, an attacker can execute a successful at-tack. This is a highly practical attack, as an attackercan release large amounts of poisoned data on theinternet in the hopes that it will be scraped and laterused for training. In our work, we demonstrate thatour method is highly effective against a number ofrecent backdooring methods and poisoning attackson contrastive models.",
  "Defending against attacks": "Given the large potential risks posed by attacks tomodels, extensive research has been conducted onapproaches for defending models against both poi-soning (Weerasinghe et al., 2021; Chen et al., 2021)and backdooring (Hayase et al., 2021; Wang et al.,2022a; Huang et al., 2021) attacks. Defenses canbe broadly categorized into methods for detectingand removing attacked samples from training (Tranet al., 2018; Chen et al., 2019; Tang et al., 2021),those that remove backdoors already learned bymodels (Liu et al., 2022; Zeng et al., 2021; Wu andWang, 2021), and those that seek to prevent mod-els from learning backdoors by decreasing theireffectiveness (Qiu et al., 2021; Bansal et al., 2023a;Li et al., 2021b). Unfortunately, detection-basedmethods often fail to detect all backdoors and giventhe particular vulnerability of contrastive models,imperfect filtering could still result in model poi-soning. Unlike our approach, model de-poisoningmethods often fail to achieve similar performanceto clean models (Liu et al., 2023).Of particular relevance to our work are methodsaimed at defending against poisoning and back-dooring for vision-language contrastive learning(Bansal et al., 2023b). (Bansal et al., 2023b) pro-pose to independently realign representations fromdifferent modalities. Unlike this approach, ourmethod learns a fine-grained alignment between ex-ternal knowledge extracted from a large languagemodel and visual regions. These alignments arethen used as a penalty to prevent models from at-",
  "Threat model": "Adversary objective. Given a vision-languge con-trastive learning model M, an adversary aims tocompromise the model by injecting a small amountof poisoned data Dp into a clean dataset Dc, bothof which constitute the training data D. The modeltrained on the poisoned training data is denotedas Mp. In this paper, we consider two types ofattacks: 1) backdooring and 2) poisoning. In abackdoor attack, the adversary overlays either asmall patch or some visually imperceptible noiseon an image, causing the backdoored image to bemisclassified or incorrectly retrieved by a retrievalmodel. During testing, the adversary cause themodel to misclassify or retrieve a specific class byinserting the backdoor into test images. In contrast,in a poisoning attack, the goal is to cause the modelMp to associate a targeted set of text with imagesof a specified class by inserting many training in-stances which incorrectly associate visual contentwith concepts controlled by the adversary. In bothcases, the poisoned model is expected to maintainsimilar utility (performance) compared to the cleanmodel.Adversary capabilities. We consider an adver-sary capable of injecting a small number of poi-sonous samples into the training dataset, similarto prior work (Biggio et al., 2013a). In traditionalsupervised attacks (Shafahi et al., 2018; Saha et al.,2022), adversaries were required to modify a largeamount of the training data - an impractical settingfor vision-language models trained on web-scaledata. Our setting is more realistic, because achiev-ing a high poisoning rate is improbable when poi-soned data is released on the internet with the hopeof it being scraped for training. Thus, we focus onthe more feasible scenario and assume a relativelylow poisoning rate. We assume a black-box set-ting, where the adversary lacks knowledge of thetarget models architecture and hyperparameters.Additionally, the adversary lacks control over thetraining process.",
  "Model training. We denote our training data as(i, t) D = I T , where D, I, and T represent": "the training set, image set, and text set, respectively.Within a collection of N image-text pairs, we iden-tify (ij, tk) as a positive pair if j = k; otherwise,it is considered a negative pair. The contrastivelearning model concurrently optimizes the imageencoder Ei and the text encoder Et to maximize thesimilarity between the embeddings of positive pairsin a batch while minimizing that of negative pairs.Specifically, for a given batch of N image-textpairs, we obtain the image embedding Iej = Ei(ij)and the corresponding text embedding T ek = Et(tk)for each pair, normalizing both embeddings usingthe L2 norm. The cross-modal contrastive lossLCL is then computed as follows:",
  "k=1logexp((Iek, T ek)/)Nj=1 exp((Iej , T ek)/)": "(1)where (., .) is the product between the image andtext embeddings (their similarity) and denotesthe temperature.Backdoor attack. A successful backdoor at-tack introduces a trigger into a model so that whenthe trigger is present in the input image (dog),the model incorrectly associates the image withthe specific target class (boat caption) controlledby the attacker.We applied backdoor attacksto poison multimodal contrastive learning mod-els, following the approach in (Carlini and Terzis,2022).We consider two types of backdoor at-tacks: a) overlaying a backdoor trigger, such asa (16 16 patch), on a small subset of trainingimages, and b) injecting imperceptible noise intoa limited subset of images. The latter is consid-ered a stealthy backdoor attack. We classify theBPP (Wang et al., 2022b) and Wanet (Nguyen andTran, 2021) attacks as stealthy, because they posea challenge for human identification due to theirsubtle and imperceptible nature. To perform ourbackdoor attack, we construct the poisoning datasetDp =(Ii bd), T yi: Ii Dsubset, by embed-ding a backdoor trigger bd (e.g. a 16 16 patch orimperceptible noise) in a small subset of trainingimages, Dsubset D, T yi T y, where y is targetclass.Single target label attack. In this poisoningattack, an adversary aims to associate images fromone class e.g. (dog) with captions from anotherclass e.g. (boat). The attack can be formulated as (i, t)|i IAtrain, t T Btrain, where A and B are theoriginal and the target classes respectively. Givena caption t T Btest, we expect the model to retrieveimages from IAtest as the most relevant. We poisonthe model to build a strong relationship betweenimages in class A and captions in class B, even ifthe test images and captions are unseen at trainingtime.Multiple target label attack. An adversary canextend the single target label\" attack by poison-ing multiple target classes simultaneously, i.e. im-ages from multiple original classes can be mappedto multiple target classes in captions.In thissetting, the poisoning goal is defined as Dp =(A1, B1), (A2, B2), ..., (An, Bn) where Ai IA",
  "Approach": "In this section, we introduce our framework formitigating backdooring and poisoning attacks onvision-language models. Backdoor attacks on mul-timodal contrastive learning are effective becausemodels learn a correlation between the backdoortrigger either in a form of patch or imperceptiblenoise added to the image and the target concept inthe paired captions. The core intuition behind ourapproach stems from human perception, where setsof lower level semantic concepts play a key rolein distinguishing objects. See 1. These semanticconcepts consist of semantic attributes (e.g. thickfur\", rough green texture\"), but also parts of ob-jects (e.g. paws, whiskers). We term these identi-fiable properties knowledge elements (KEs). Ourcore intuition is that backdooring and poisoningattacks are effective because models learn spuriouscorrelations between the visual content and the tar-get label. However, because other non-backdooredclasses also share some of the same KEs, modelswill not learn an association between the KEs andthe spurious visual signal. Thus, we propose toleverage KEs to prevent models from relying onsuch correlations in their representations.",
  "Aligning patches to knowledge elements": "The traditional contrastive learning objective en-courages image embedding Iei and text embeddingT ei to be close. However, in addition to this, weenforce that image patch embeddings Ipatchiand as-sociated KE embeddings KEei to also be close. Ourkey observation is that because backdoor signals are injected in random locations of the image whichdo not necessarily contain a KE, the similarity be-tween these patches and KE embeddings should belower compared to others. Even if by chance thearea covered by the attack does contain KEs, theaffected KEs will not be the same when the attackis performed on a different image, preventing themodel from learning an association between theattack perturbation and the KEs. Based on this in-tuition, our model first learns to align patches andKEs using a contrastive constraint, LKE. Thislearned alignment will later be used to preventthe model from attending to potentially attackedpatches. To learn the patch-KE alignment, we firstcompute the maximum and minimum patch-KEsimilarity per category per sample as",
  "(3)": "where n is the number of patches per image, mis the number of KEs per object category, andc C, where C is the number of object cate-gories. (KEcq)e is the per KE embedding per cate-gory. Note that our approach also extends to image-text datasets without any defined object categoriesor labels. In this case, we treat each image-captionpair as its own category with a set of knowledgeelements and C is the same as the batch size. Theobjective function for patch-KE similarity is there-fore given by",
  "(4)": "where is the sigmoid function and yci is the multi-label ground truth information per sample per cate-gory. Note that, summation over batch is omittedfor brevity. In 2 and 3 all patches of every im-age compute their similarity with all KEs from thebatch. We perform max/min to select either thebest aligned KEs (for paired captions) or worstaligned KEs (for non paired) to prevent false nega-tives. We thus can fine-tune our model via a linearcombination of these two objectives:",
  "Visual knowledge elements for caption": ": Semantic Shield prompts a LLM to extract potential visual knowledge elements (KEs) from a caption.Image patches are aligned with KEs via the patch-KE loss. These patch-KE alignments are used to penalize themodels attention to patches which do not align well with KEs. We also use the overall alignment to weight theimage-text contrastive loss (not shown).",
  "Knowledge element-guided attention": "Next, we observe that the attention mechanismwithin the vision transformer (ViT) attends to bothattacked patches and unaffected patches. This is un-desirable because attention paid to attacked patchesrenders the output embeddings more dependent onthe attack signal, and thus more vulnerable. Thus,it is imperative for ViT to allocate reduced attentionto attacked patches relative to unaffected patches.Our intuition is that the model should pay moreattention to image regions that align well with KEsthan patches with low alignment. Thus, we lever-age our patch-KE similarity scores to modulateViTs attention by enforcing a constraint betweenViTs attention and the patch-KE similarity scores.Given ViTs query, key, and value denoted asQ, K, V respectively, the attention weight is com-puted as = softmax( QKT dk ), where dk is thedimensionality of the key vectors. Now, the penal-ized attention weight can be computed based on themaximum and minimum similarity computed in 2,3 (ci)max = ci ci . (ci)min = ci ci Since the similarity scores between a targeted visual regionand KE are less compared to unaffected patch andKE, ViT pays less attention to attacked patches.The resulting objective function which penalizesattention values which deviate from the patch-KEsimilarity scores is:",
  "Knowledge element weighted contrastiveloss": "Note that during the fine-tuning process of 5 and7, the contrastive learning objective 1, seeks toalign representations from each modality whichhas the effect of pulling attacked images and cap-tions closer in the embedding space. Therefore,we introduce a dynamic weighting function whichweights each sample in the contrastive objectivefunction. Our intuition is that attacked sampleswill have lower similarity scores between imagepatches and KEs, since the attack does not explicit target the KEs. Thus, we penalize the contrastiveobjective for each sample with the average sim-ilarity score, so that the contrastive objective isdownweighted for attacked samples compared tobenign samples. We compute the maximum simi-larity scores per sample across categories following2, where i = maxcC ci , i N, 1, 2 = 1:",
  "Knowledge element (KE) generation": "Our approach requires external knowledge abouteach image in addition to a paired caption. Forexample, a caption of dog image might be \"Adog is running in the park\". In this case, suit-able knowledge elements might be paws, sharpnails, furry animal, trees. We follow incontext learning approach by prompting a largelanguage model (Vicuna (Chiang et al.)) for gener-ating KEs for each image. Note that the KEs aregenerated purely from the caption or object labeland thus are only potentially relevant to the image.Our approach accounts for this by generating 25KEs per caption/category. Then, we take the top 5KEs per caption based on the similarity scores be-tween image and generated KEs. For COCO (Linet al., 2014), we prompt Vicuna with What areuseful visual features for distinguishinga category name in a photo?. Since COCOhas 80 categories we choose this prompt follow-ing (Menon and Vondrick, 2023). For Flickr30k(Young et al., 2014), we design prompts that gen-erate KEs for each caption, since we do not haveany predefined object classes. Additional detailsare included in our supplementary.",
  "Experimental Setup": "Models and datasets. We follow (Carlini andTerzis, 2022)s setting by attacking CLIP-like mod-els (Radford et al., 2021). We adopt ViT-B/16 as im-age encoder, pretrained on ImageNet-21k (Steineret al., 2022) and fine-tuned on ImageNet-1k. As atext encoder, we adopt a BERT-style (Devlin et al.,2019) encoder following (Radford et al., 2021).We cap the max sequence length of text to 100.We use AdamW with weight decay using a co-sine scheduler from 104 with decay rate 0.2. Wetrain for 30 epochs with a batch size of 128 on theCOCO (Lin et al., 2014) and Fickr30k (Young et al.,2014) datasets. While COCO has 80 defined ob-ject categories, Flickr30k has no label information.Additional details are included in supplementary.Backdoor settings.We tested out defenseagainst three recent backdoor attacks. To do so,we couple backdoored samples with a caption men-tioning the target class. Adversaries only requirea very small amount of poisoned samples for poi-soning contrastive models (e.g. , CLIP) (Carliniand Terzis, 2022). Following this, we inject a verysmall amount of poisoned samples (0.01% of thetrain dataset for both COCO and Flickr30k).Poisoning settings. We performed two types ofpoisoning attacks following (Yang et al., 2023b).For single target label attack, the poisoning goalis dog2boat for both Flickr30k and COCO. Weevaluate them on test samples that are unseen in thetraining process. For example, we take an clean im-age of dog and associate it with a proxy caption ofboat. The poisoning rate for this attack is 0.065%for Flickr30k and 0.24% for COCO. For the multi-target label attack, we take two classes. The poi-soning goals are dog2boat and train2zebra forCOCO. For Flickr30k, the poisoning goals aredog2boat and bird2sofa. The poisoning rate forCOCO and Flickr30k are 0.52% and 0.34% respec-tively.",
  "Experimental Results": "Backdoor Attack. In 1, we compared ablations ofour method (CL+ KE, CL + Attention) with otherbaselines e.g.Cleanlip (Bansal et al., 2023b),Anti-Backdoor Learning (ABL) (Li et al., 2021b).Finally, our model Semantic Shield (Weighted CL+ Attention), outperforms all baselines with signifi-cant margins. Note that, at test time, we used 100backdoor images (patch, BPP, Wanet) for the text",
  "retrieval task. At test time, our model retrieves nocaption associated with poisoned categories for anybackdoored image on Flickr30k": "Poisoning Attack. Similarly, to the above, attest time, we use 100 poisoned images for both sin-gle and multi-target settings for both datasets. Ourmodel outperforms all existing work significantlywith large margins, particularly on the multi-targetlabel setting. We observe that the unweighted ver-sion of our approach slightly outperforms SemanticShield for dog2boat at Hit@1, but Semantic Shieldsignificantly outperforms for Hit@5 and Hit@10,suggesting significantly reduced poisoning overall. Utility evaluation. We evaluate model utilityfor image-caption retrieval. 4 shows the perfor-mance (Recall@10) of the poisoned model on eachattack type as well as the clean model on the testdata. We observe that the utility of the poisonedmodel is at the same level or slightly less than theclean model e.g.BPP in COCO dataset. Thisimplies that despite being trained on poisoned data, models maintain their performance. We show themodel utility after being defended with SemanticShield and its variants (CL + KE, CL + Attention,weighted CL + Attention) in 3. We largely observea similar utility compared to the models from 4.On the Flickr30k dataset, single target or multipletarget attack scenario, for TR task, the utility isslightly less than the clean model (4, 3).",
  "Ablations": "Poisoning rate. We compare the performance ofpoisoning attacks at different poisoning rates onthree backdoor attacks. We conduct these attacksagainst the victim model with four different poison-ing rates (0.001 to 0.01%) on the COCO dataset(3). We observe that attack performance signifi-cantly improves with increased poisoning rate, eventhough the rate is quite low, which demonstrates thevulnerability of contrastively trained VL models toattacks.Fine-tuning epoch. In 4 we use the max poi-",
  "Qualitative analysis": "In 5, we present the contrast between a modeldefended by Semantic Shield and an undefendedmodels attention map. 5b shows that poisonedmodel pays attention to the patch (bottom right cor-ner). In contrast, the defended model 5c does notpay any attention to the patch. Next, in 5d and 5gtwo imperceptible noises are injected e.g.BPP,Wanet. We wanted to see what happens if we injectthe noise randomly throughout the entire images.Poisoned models in 5e and 5h show spurious visual 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0 # Epochs Hit@k Backdoor-patch for COCO Hit@1Hit@5Hit@10",
  "Conclusion": "In this paper, we introduced Semantic Shield, anapproach for defending against attacks on con-trastively trained VL models. Our approach worksby leveraging external knowledge to guide themodels attention to non-attacked visual regionsand samples. We evaluated Semantic Shield againstrecent backdooring and poisoning attacks and de-fenses on two benchmarks. Our experiments showthat Semantic Shield substantially outperforms ex-isting defenses across all settings. In future work,we will explore a tighter integration of the LLMusing prompting by dynamically producing KEsonline based on the defended models current state.In addition, we will explore how multimodal largelanguage models could be used to extract more rel-evant KEs. While Semantic Shield is successfulat defending against attacks on natural images forwhich there is a meaningful visual-KE alignment,it may be less successful for images such as chartsor more abstract text for which clear KEs cannotbe extracted. Moreover, it does not preclude the",
  "The authors acknowledge Advanced ResearchComputing at Virginia Tech for providing compu-tational resources and technical support that havecontributed to the results reported within this paper.URL:": "Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang,Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT: trans-formers for multimodal self-supervised learning from rawvideo, audio and text. In Advances in Neural Informa-tion Processing Systems 34: Annual Conference on NeuralInformation Processing Systems 2021, NeurIPS 2021, De-cember 6-14, 2021, virtual, pages 2420624221, 2021. Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, AdityaGrover, and Kai-Wei Chang. Cleanclip: Mitigating datapoisoning attacks in multimodal contrastive learning. InICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models, 2023a. Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, AdityaGrover, and Kai-Wei Chang. Cleanclip: Mitigating datapoisoning attacks in multimodal contrastive learning. InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 112123, 2023b. Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoningattacks against support vector machines. In Proceedings ofthe 29th International Coference on International Confer-ence on Machine Learning, pages 14671474, 2012. Battista Biggio, Ignazio Pillai, Samuel Rota Bul, DavideAriu, Marcello Pelillo, and Fabio Roli. Is data cluster-ing in adversarial settings secure? In Proceedings of the2013 ACM Workshop on Artificial Intelligence and Secu-rity, page 8798, New York, NY, USA, 2013a. Associationfor Computing Machinery. Battista Biggio, Ignazio Pillai, Samuel Rota Bul, DavideAriu, Marcello Pelillo, and Fabio Roli. Is data cluster-ing in adversarial settings secure? In Proceedings of the2013 ACM workshop on Artificial intelligence and security,pages 8798, 2013b. Nicholas Carlini and Andreas Terzis. Poisoning and back-dooring contrastive learning. In The Tenth InternationalConference on Learning Representations, ICLR 2022, Vir-tual Event, April 25-29, 2022. OpenReview.net, 2022. Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, HeikoLudwig, Benjamin Edwards, Taesung Lee, Ian Molloy, andBiplav Srivastava. Detecting backdoor attacks on deepneural networks by activation clustering. In Workshop onArtificial Intelligence Safety. CEUR-WS, 2019. Jian Chen, Xuxin Zhang, Rui Zhang, Chen Wang, and LingLiu. De-pois: An attack-agnostic defense against data poi-soning attacks. IEEE Transactions on Information Foren-sics and Security, 16:34123425, 2021. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, ZhanghaoWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, YonghaoZhuang, Joseph E Gonzalez, et al. Vicuna: An open-sourcechatbot impressing gpt-4 with 90%* chatgpt quality. 2023.URL org/blog/2023-03-30-vicuna, 1(2):3. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: pre-training of deep bidirectional trans-formers for language understanding. In Proceedings ofthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: HumanLanguage Technologies, NAACL-HLT 2019, Minneapolis,MN, USA, June 2-7, 2019, Volume 1 (Long and Short Pa-pers), pages 41714186. Association for ComputationalLinguistics, 2019.",
  "Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack withimperceptible input and latent modification. Advances inNeural Information Processing Systems, 34:1894418957,2021a": "Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira:Learnable, imperceptible and robust backdoor attacks. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1196611976, 2021b. Vishnu Sashank Dorbala, Gunnar A Sigurdsson, Jesse Thoma-son, Robinson Piramuthu, and Gaurav S Sukhatme. Clip-nav: Using clip for zero-shot vision-and-language naviga-tion. In Workshop on Language and Robotics at CoRL2022, 2022.",
  "Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and SiddharthGarg. Badnets: Evaluating backdooring attacks on deepneural networks. IEEE Access, 7:4723047244, 2019": "Jonathan Hayase, Weihao Kong, Raghav Somani, and Se-woong Oh. Spectre: Defending against backdoor attacksusing robust statistics. In International Conference onMachine Learning, pages 41294139. PMLR, 2021. Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Bur-gard. Visual language maps for robot navigation. In 2023IEEE International Conference on Robotics and Automa-tion (ICRA), pages 1060810615. IEEE, 2023.",
  "Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and KuiRen. Backdoor defense via decoupling the training process.In International Conference on Learning Representations,2021": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In Internationalconference on machine learning, pages 49044916. PMLR,2021. Marius Kloft and Pavel Laskov. Online anomaly detection un-der adversarial impact. In Proceedings of the thirteenth in-ternational conference on artificial intelligence and statis-tics, pages 405412. JMLR Workshop and ConferenceProceedings, 2010.",
  "Pang Wei Koh and Percy Liang. Understanding black-boxpredictions via influence functions. In International con-ference on machine learning, pages 18851894. PMLR,2017": "Changjiang Li, Ren Pang, Zhaohan Xi, Tianyu Du, Shouling Ji,Yuan Yao, and Ting Wang. An embarrassingly simple back-door attack on self-supervised learning. In Proceedingsof the IEEE/CVF International Conference on ComputerVision, pages 43674378, 2023. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, ShafiqJoty, Caiming Xiong, and Steven Chu Hong Hoi. Align be-fore fuse: Vision and language representation learning withmomentum distillation. Advances in neural informationprocessing systems, 34:96949705, 2021a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.Hoi. BLIP: bootstrapping language-image pre-trainingfor unified vision-language understanding and generation.In International Conference on Machine Learning, ICML2022, 17-23 July 2022, Baltimore, Maryland, USA, pages1288812900. PMLR, 2022. Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li,and Xingjun Ma. Anti-backdoor learning: Training cleanmodels on poisoned data. In Advances in Neural Informa-tion Processing Systems 34: Annual Conference on NeuralInformation Processing Systems 2021, NeurIPS 2021, De-cember 6-14, 2021, virtual, pages 1490014912, 2021b. Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects incontext. In Computer VisionECCV 2014: 13th EuropeanConference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part V 13, pages 740755. Springer, 2014.",
  "Min Liu, Alberto Sangiovanni-Vincentelli, and Xiangyu Yue.Beating backdoor attack at its own game. In Proceedingsof the IEEE/CVF International Conference on ComputerVision, pages 46204629, 2023": "Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma,Li Wang, and Jianfeng Ma. Backdoor defense with ma-chine unlearning. In IEEE INFOCOM 2022-IEEE Confer-ence on Computer Communications, pages 280289. IEEE,2022. Arjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, JudyHoffman, and Dhruv Batra.Zson: Zero-shot object-goal navigation using multimodal goal embeddings. Ad-vances in Neural Information Processing Systems, 35:3234032352, 2022. Sachit Menon and Carl Vondrick. Visual classification via de-scription from large language models. In The Eleventh In-ternational Conference on Learning Representations, ICLR2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,2023.",
  "Tuan Anh Nguyen and Anh Tuan Tran. Wanet - imperceptiblewarping-based backdoor attack. In International Confer-ence on Learning Representations, 2021": "Huy Phan, Cong Shi, Yi Xie, Tianfang Zhang, ZhuohangLi, Tianming Zhao, Jian Liu, Yan Wang, Yingying Chen,and Bo Yuan. Ribac: Towards r obust and i mperceptibleb ackdoor a ttack against c ompact dnn.In EuropeanConference on Computer Vision, pages 708724. Springer,2022. Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, MeikangQiu, and Bhavani Thuraisingham. Deepsweep: An evalua-tion framework for mitigating dnn backdoor attacks usingdata augmentation. In Proceedings of the 2021 ACM AsiaConference on Computer and Communications Security,pages 363377, 2021. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,Gabriel Goh, Sandhini Agarwal, Girish Sastry, AmandaAskell, Pamela Mishkin, Jack Clark, Gretchen Krueger, andIlya Sutskever. Learning transferable visual models fromnatural language supervision. In International Conferenceon Machine Learning, 2021.",
  "Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pir-siavash. Hidden trigger backdoor attacks. In Proceedingsof the AAAI conference on artificial intelligence, pages1195711965, 2020": "Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Kooh-payegani, and Hamed Pirsiavash. Backdoor attacks onself-supervised learning.In IEEE/CVF Conference onComputer Vision and Pattern Recognition, CVPR 2022,New Orleans, LA, USA, June 18-24, 2022, pages 1332713336. IEEE, 2022. Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu,Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poi-son frogs! targeted clean-label poisoning attacks on neuralnetworks. In Proceedings of the 32nd International Con-ference on Neural Information Processing Systems, page61066116, Red Hook, NY, USA, 2018. Curran AssociatesInc. Wonyoung Shin, Jonghun Park, Taekang Woo, Yongwoo Cho,Kwangjin Oh, and Hwanjun Song. e-clip: Large-scalevision-language representation learning in e-commerce. InProceedings of the 31st ACM International Conference onInformation & Knowledge Management, pages 34843494,2022. Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, RossWightman, Jakob Uszkoreit, and Lucas Beyer. How totrain your vit? data, augmentation, and regularization invision transformers. Trans. Mach. Learn. Res., 2022, 2022. Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang.Demon in the variant: Statistical analysis of {DNNs} forrobust backdoor contamination detection. In 30th USENIXSecurity Symposium (USENIX Security 21), pages 15411558, 2021. Christopher Thomas and Adriana Kovashka. Preserving se-mantic neighborhoods for robust cross-modal retrieval. InComputer VisionECCV 2020: 16th European Confer-ence, Glasgow, UK, August 2328, 2020, Proceedings,Part XVIII 16, pages 317335. Springer, 2020. Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and LingLiu. Data poisoning attacks against federated learningsystems.In Computer SecurityESORICS 2020: 25thEuropean Symposium on Research in Computer Security,ESORICS 2020, Guildford, UK, September 1418, 2020,Proceedings, Part I 25, pages 480501. Springer, 2020.",
  "Brandon Tran, Jerry Li, and Aleksander Madry. Spectralsignatures in backdoor attacks. Advances in neural infor-mation processing systems, 31, 2018": "Wei Lun Tsai, Jacob J Lin, and Shang-Hsien Hsieh. Generat-ing construction safety observations via clip-based image-language embedding. In European Conference on Com-puter Vision, pages 366381. Springer, 2022. Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, andZhangyang Wang. Trap and replace: Defending backdoorattacks by trapping them into an easy-to-replace subnet-work. Advances in neural information processing systems,35:3602636039, 2022a.",
  "Lin Wang and Jie Chen. Improving radiology report genera-tion with adaptive attention. In Multimodal AI in health-care: A paradigm shift in health intelligence, pages 293305. Springer, 2022": "Longzheng Wang, Chuang Zhang, Hongbo Xu, Yongxiu Xu,Xiaohan Xu, and Siqi Wang.Cross-modal contrastivelearning for multimodal fake news detection. In Proceed-ings of the 31st ACM International Conference on Multi-media, pages 56965704, 2023. Zhenting Wang, Juan Zhai, and Shiqing Ma.Bppattack:Stealthy and efficient trojan attacks against deep neuralnetworks via image quantization and contrastive adversar-ial learning. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, CVPR 2022, New Orleans, LA,USA, June 18-24, 2022, pages 1505415063. IEEE, 2022b. Sandamal Weerasinghe, Tansu Alpcan, Sarah M Erfani, andChristopher Leckie. Defending support vector machinesagainst data poisoning attacks. IEEE Transactions on In-formation Forensics and Security, 16:25662578, 2021.",
  "Dongxian Wu and Yisen Wang. Adversarial neuron pruningpurifies backdoored deep models. Advances in NeuralInformation Processing Systems, 34:1691316925, 2021": "Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera,Claudia Eckert, and Fabio Roli. Is feature selection se-cure against training data poisoning?In Internationalconference on machine learning, pages 16891698. PMLR,2015. Wenhan Yang, Jingdong Gao, and Baharan Mirzasoleiman.Robust contrastive language-image pretraining against datapoisoning and backdoor attacks. In Thirty-seventh Confer-ence on Neural Information Processing Systems, 2023a. Ziqing Yang, Xinlei He, Zheng Li, Michael Backes, MathiasHumbert, Pascal Berrang, and Yang Zhang.Data poi-soning attacks against multimodal encoders. In Interna-tional Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, pages 3929939313.PMLR, 2023b. Ziqing Yang, Xinlei He, Zheng Li, Michael Backes, MathiasHumbert, Pascal Berrang, and Yang Zhang.Data poi-soning attacks against multimodal encoders. In Interna-tional Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, pages 3929939313.PMLR, 2023c. Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-maier. From image descriptions to visual denotations: Newsimilarity metrics for semantic inference over event de-scriptions. Trans. Assoc. Comput. Linguistics, 2:6778,2014. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastivecaptioners are image-text foundation models. Transactionson Machine Learning Research, 2022.",
  "Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, andRuoxi Jia. Adversarial unlearning of backdoors via implicithypergradient. In International Conference on LearningRepresentations, 2021": "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.Lit: Zero-shot transfer with locked-image text tuning. InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1812318133, 2022. Hanwang Zhang, Yang Yang, Huanbo Luan, Shuicheng Yang,and Tat-Seng Chua. Start from scratch: Towards automati-cally identifying, modeling, and naming visual attributes.In Proceedings of the 22nd ACM international conferenceon Multimedia, pages 187196, 2014.",
  "Ying Zhang and Huchuan Lu. Deep cross-modal projectionlearning for image-text matching. In Proceedings of theEuropean conference on computer vision (ECCV), pages686701, 2018": "Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher DManning, and Curtis P Langlotz. Contrastive learning ofmedical visual representations from paired images and text.In Machine Learning for Healthcare Conference, pages225. PMLR, 2022b. Bingyin Zhao and Yingjie Lao. Towards class-oriented poi-soning attacks against neural networks. In Proceedingsof the IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 37413750, 2022. Yangming Zhou, Yuzhou Yang, Qichao Ying, Zhenxing Qian,and Xinpeng Zhang. Multimodal fake news detection viaclip-guided learning. In 2023 IEEE International Confer-ence on Multimedia and Expo (ICME), pages 28252830.IEEE, 2023."
}