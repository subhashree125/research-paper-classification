{
  "Abstract": "Mitigating bias in machine learning models is a criticalendeavor for ensuring fairness and equity. In this paper,we propose a novel approach to address bias by leverag-ing pixel image attributions to identify and regularize re-gions of images containing significant information aboutbias attributes. Our method utilizes a model-agnostic ap-proach to extract pixel attributions by employing a convolu-tional neural network (CNN) classifier trained on small im-age patches. By training the classifier to predict a propertyof the entire image using only a single patch, we achieveregion-based attributions that provide insights into the dis-tribution of important information across the image. Wepropose utilizing these attributions to introduce targetednoise into datasets with confounding attributes that bias thedata, thereby constraining neural networks from learningthese biases and emphasizing the primary attributes. Ourapproach demonstrates its efficacy in enabling the trainingof unbiased classifiers on heavily biased datasets.",
  ". Introduction": "In the domain of computer vision, various methodologiesare employed to mitigate bias and uphold fairness in ma-chine learning models operating on sensitive attributes.Data bias can manifest in various ways, such as whenthe data representation disproportionately favors a spe-cific group of subjects . Alternatively, bias may arisefrom substantial differences between the training and pro-duction environments .Moreover, biases within thedata can be either explicit and acknowledged beforehandor concealed.We specifically focus on situations wherebias is pre-existing and originates from disparities betweenthe training and evaluation datasets due to a confoundingvariable heavily influencing the former. For instance, inthe CelebA dataset, a significant majority of womenare depicted with blond hair, whereas only a minority ofmen exhibit this trait. Consequently, a model trained onsuch data may exhibit unequal performance across men and Proportionate results",
  "Low": "confidence Input image Region-based classifier Blond HairBiased data Targetednoise forBlond Hair Tackling dataset bias . Our strategy to address biased learning. We utilize aregion-based classifier to classify individual image patches basedon attributes that introduce bias to the data. By leveraging theconfidence of the trained model, we identify the regions within animage where attribute-related information is most concentrated.Subsequently, we introduce targeted noise to these areas in thetraining data to prevent models from overfitting to the confoundingattributes. women in a production setting where this distribution doesnot hold true as it has learned to associate having blond hairwith being a woman.One prevalent technique to mitigate the learning of bi-ases is dataset re-sampling, where instances from minor-ity or majority classes are either over-sampled or under-sampled to achieve a more equitable distribution. However,in scenarios such as facial data, where numerous attributesare present, achieving a perfectly balanced dataset may beunattainable as this would require an equal number of sam-ples for each subcategory. In the case of CelebA, whichcontains 40 binary attributes, this would mean you requirean equal amount of samples for each of the 240 possiblecombinations. Another approach is to utilize dataset aug-mentation , which involves altering the training data to",
  "arXiv:2405.05031v1 [cs.CV] 8 May 2024": "rectify imbalances in underrepresented classes or counterbiases inherent in the training dataset, for example by gen-erating synthetic data . Our solution falls under thiscategory as well. We propose a solution where we iden-tify critical pixel areas for classifying attributes that intro-duce a bias in the data (e.g., blond hair in face data). Bystrategically introducing targeted noise to these regions inthe training data, we aim to prevent the model from overfit-ting to these confounding attributes. A conceptualisation ofour technique can be seen in .Attributing pixels that are important for classification istypically referred to as saliency mapping. Saliency mappingtechniques were originally developed to elucidate machinelearning models rather than datasets. Consequently, they at-tempt to identify pixel regions crucial for a specific modelsclassification. However, for mitigating biases being learned,its more pertinent to determine which regions potentiallycontribute to classification. In other words, its essential todiscern which image regions harbour valuable informationfor a particular class (e.g., identifying facial features aidingin ethnicity classification). This will not always align withspecific model attribution maps, as not all networks learnin the same way, and some may lay more importance on acertain pixel region than others.To tackle this issue, we introduce data attribution throughclassification. We employ a small-scale classifier that pre-dicts a sensitive label for the entire image from a singleimage patch. By identifying regions where the classifierconfidently predicts the label, assuming the model is wellcalibrated, we establish a measure of information contentregarding the sensitive label within that patch. As a result,this methodology allows us to attribute specific image re-gions to particular classes.The resulting attributions could be used for a number ofpurposes, but we see the most promise in fields where it isundesirable that certain attributes are learned. The most no-table of these is learning using a biased dataset. We proposeto use our attributions as a way to regularize the trainingprocess, if some biases in the data are known a priori (e.g.due to a high correlation between labels), then we can com-pel the model learning to classify desired attributes to divertfocus away from those regions important for classifying theconfounder.We demonstrate the effectiveness of our attribution tech-nique on two prominent face datasets, namely FairFace and CelebA .These datasets are selected due totheir comprehensive annotations and consistent alignmentof posture, facilitating the validation of our attributionsutility through averaged attributions across multiple images.Subsequently, we explore the application of our attributionsin training unbiased perceived gender classifiers on biaseddata, leveraging the CelebA dataset. By creating biased sub-sets where specific attributes are prominent for women but absent for men, we evaluate subgroup accuracy on a bal-anced dataset. Our experiments reveal that incorporatingnoise based on our attributions to the training data provesbeneficial in mitigating the adverse effects of dataset biason classifier performance.We make the following contributions in this paper:",
  "We developed a general model-independent data attribu-tion technique that can foster data understanding": "We showcase the use of this attribution technique for dataaugmentation to train neural networks under known bi-ases and experiment with different noise addition typesand settingsThe remainder of this paper is organized as follows. Weposition related works in . Following this, we in-troduce our region classifier and show how we obtain attri-butions in . Lastly, we experiment with using theattributions for developing robust classifiers in .We conclude our work in .",
  ". Dataset bias and fairness": "Correlation is a widely studied problem in fair research,mainly concerning spurious correlations in the datasetthrough societal biases or dataset construction. For exam-ple, for the CelebA dataset, it is known that the attractiveattribute is biased towards women and that a majority of thewomen have blonde hair. Rajabi et al. demonstrate thatmitigating bias between gender and attractiveness for theCelebA dataset influences classification of several other at-tributes related to attractiveness as well. Similarly, Dentonet al. found that manipulating non-hair attributes suchas heavy makeup resulted in a change in hair style/and orcolor.Addressing these spurious correlations and biases rep-resents an active area of investigation within the re-search community. Some studies concentrate on improv-ing datasets to ensure a more equitable distribution of datasamples, either through resampling techniques or augment-ing the dataset with new data generated, for example, usinggenerative models . Others employ techniques dur-ing model training to guide the learning process, facilitatingthe acquisition or elimination of specific biases . For in-stance, Kim et al. utilize a regularization loss based on mu-tual information to prevent the model from learning biasedattributes . Additionally, certain approaches leveragecausal learning methodologies to achieve unbiased recogni-tion .",
  "Pixel attribution techniques are utilized to identify the areasor pixels within an image that significantly contribute to amodels prediction. These techniques typically fall into two": "categories: occlusion or perturbation-based, and gradient-based methods. The former, being model-agnostic, assessesthe impact on prediction when certain regions are omitted,while the latter computes gradients concerning the inputimage . Noteworthy gradient-based techniques includeGrad-CAM , Integrated Gradients , and XRAI .For occlusion-based approaches, LIME and SHAP are widely recognized.These methods primarily serve to enhance the inter-pretability of deep neural networks.However, they alsosee use beyond, in improving the generalization of deeplearning models , such as those used in facial expressionrecognition and person detection tasks , includ-ing scenarios involving thermal imagery . Particularlyrelevant to our work, Huang et al. employ saliency tech-niques to address biases in facial recognition models. Theirapproach employs gradient attention maps, ensuring con-sistent attention patterns across diverse racial backgrounds.However, our work differs in that we utilize saliency to mit-igate confounding variables being learned, whereas they fo-cus on ensuring uniform learning across different subgroupsregardless of what is learned.",
  ". Region classifier": "In our efforts to alleviate learned biases, our focus is di-rected towards identifying the regions containing vital in-formation for classification. While saliency techniques arecommonly used for this purpose, they often focus solely onpinpointing important pixels for a specific model, poten-tially overlooking broader regions of interest. To addressthis, we deploy a region classifier capable of analyzing im-age patches and attributing characteristics of the entire im-age, such as determining a persons age. By assessing theconfidence levels of a well-trained classifier, we gain valu-able insights into the specific areas where significant infor-mation is concentrated.",
  ". Training setup": "Our region classifier employs a ResNet18 model architec-ture , which operates on patches of size k k. Addition-ally, the network takes in a region indicator, which indicatesthe region from which the patch was taken in the originalimage. This region indicator is translated into an embed-ding akin to the vision transformer approach . This em-bedding is then concatenated to the input image before ex-ecution. The embeddings and the classification network aretrained simultaneously. During the training process, a singlepatch per image is randomly selected for training. Patchesare randomly selected from p possible positions, wherepatch i corresponds to having ((i mod p)s, i/ps)as the top-left coordinates of the patch. Here, the stride s iscalculated as I k/p, with I being the image size. OurexperimentsutilizetheFairFaceandCelebA dataset,providing centred and alignedface images, which we resized to 128 128 pixels. Weemploy the AdamW optimizer with an initial learningrate of 103, reduced by a factor of 10 every 40 epochsduring training, for a total of 90 epochs. Patch dimensions kare set to 32, and the number of possible patch positions p is2041. A batch size of 256 is utilized, and the cross-entropyloss function with label smoothing of 0.1 is employed.",
  ". Confidence estimation": "To effectively utilize our region classifier for determiningwhether a patch contains relevant information regarding thesubject, it is imperative that our classifier can provide anindication of its confidence level. There are three prevalentmethods for estimating confidence when working with clas-sification networks. Each relies on softmax values derivedfrom the output logits l, which are computed as follows:",
  ". Calibration": "When using our classifier for attribution, its important thatits confidence outputs are grounded in how accurately itcan predict and, thus, how much information there is. Acalibrated model is a model whose estimated confidence isclose to its accuracy. The most common metrics of cali-bration are the Expected Calibration Error (ECE) , andreliability diagrams .",
  "ibi||(pi ci)||(5)": "Here, N represents the number of bins used for calibra-tion, bi is the proportion of samples falling into bin i, pidenotes the average predicted confidence, and ci representsthe average true accuracy. ECE measures the average dif-ference between predicted confidence and actual accuracyacross different confidence levels. It provides a holistic as-sessment of the calibration performance of a classifier.Reliability diagrams, also known as calibration dia-grams, graphically illustrate the alignment between pre-dicted confidence levels and actual accuracy. They plot pre-dicted confidence values against observed accuracy withinequally spaced bins.Ideally, a well-calibrated classifiershows a diagonal line, indicating accurate confidence es-timates. Deviations from this line highlight areas of over-confidence or underconfidence. Reliability diagrams offera visual tool for evaluating a classifiers calibration perfor-mance.Calibration diagrams, along with the corresponding Ex-pected Calibration Error (ECE) scores, for networks trainedto classify the three FairFace attributes are depicted in Fig-ure 2. These diagrams were generated using all possiblepatches from the validation set using 100 bins. We observethat our networks are well-calibrated without having madeany specific modifications. This is likely attributed to thehigh variability in data quality. Many regions may lack suf-ficient information for accurate classification but are stillincluded, thereby mitigating overfitting and ensuring con-sistent calibration. Notably, for the age attribute, calibra-tion appears to decrease significantly at high confidences,although such instances are infrequent, as evidenced by thelower plots.",
  ". Region-based attributions": "For the final step, we can use our patch-based classifiers,which we have shown to be well-calibrated, to calculateattribution maps. To attribute an image with respect to aspecific attribute, we employ our region classifier to assessconfidence across numerous regions. This is achieved byiteratively collecting regions in a sliding window manner,as can also be seen in . Subsequently, the obtainedregions are batched and classified using the region classi-fier. Finally, a confidence score, such as negative entropy,is computed for each patch. The resulting attribution can bevisualized as a p p map, with each pixel (x, y) rep-resenting a region whose top left coordinate in the originalimage is (sx, sy). Illustrative examples of our region at-tributions for the FairFace dataset are showcased in Figure 3. Note that we solely rely on confidence scores to calculateattributions, allowing a trained model to be applied to noveldata without the need for ground-truth labels. One limita-tion of this attribution technique is its computational cost,as it requires classifying p image patches per image.By averaging over all samples, mean confidence mapscan be generated. These maps offer valuable insights intothe spatial distribution of important attribute informationwithin an image and can be used to relate these attributes toone another. The calculated maps for the FairFace attributesare presented in , revealing distinct differences be-tween the attributes. In addition to this, we trained regionclassifiers on select attributes of the CelebA dataset, forwhich we can see the mean attributions in . We ob-serve that the attribution maps for attributes such as BlondHair and Eyeglasses correspond with human intuition.For many applications, a pixel-space attribution map ismore practical.We can achieve this by converting theregion-based map to pixel space using Algorithm 1. Thealgorithm employs the confidences and locations of allpatches present on the original image to produce a pixel-level confidence map matching the original images dimen-sions. It calculates the confidence for each pixel by averag-ing the confidences of all patches containing that pixel. Forall experiments in , we used pixel space attribu-tions, which were normalized to a range.",
  ". Regularized training on biased data": "To develop a robust training algorithm, we aim to lever-age our attribution maps to address biases in the trainingdataset by introducing targeted noise to regions critical fordetecting confounding attributes. Our attribution techniquecould enable manipulation of the training data to mitigatethe models reliance on these confounders. An illustrativefigure can be seen in .",
  ". Mean attribution maps for the FairFace attributes": "the CelebA dataset. Specifically, the dataset is perfectly bal-anced for the classification attribute (3000 instances of men,3000 instances of women), but contains bias in terms of an-other attribute. Within the subset of men, none exhibit theconfounding attribute, whereas among women, 2000 indi-viduals possess the confounder while 1000 do not. We cre-ated datasets in this manner using Blond Hair, Eyeglasses,Smiling and Wearing Hat as confounding attributes. Sub-sequently, we assess performance on a test dataset that isbalanced with respect to gender as well as the hidden at-",
  ". Example of regularizing the data based on attributionsfor hair color. From left to right: Original, General Mask, SpecificMask, General Noise, Specific Noise": "tribute. By introducing this bias into the dataset, our hy-pothesis posits a significant discrepancy in test accuracy be-tween male and female subjects, owing to the disparate dis-tribution of data instances across gender categories in thetest set.To cultivate a robust classifier, we conducted experi-ments exploring various methods of utilizing attributionmaps to alter the training data. We investigated the incorpo-ration of both image-specific and general attributions (cal-culated over a large number of samples) of confounding at-tributes, as exemplified in . For both mask types,we experimented with greying out as well as additive noise.An illustration of adding noise for the Hair Color attribute inall manners (General Mask, Specific Mask, General Noise,Specific Noise) is provided in . When introducingnoise, we drew samples from a Gaussian distribution witha mean of 0 and a standard deviation of 0.5. In all cases,we masked out regions deemed most crucial for classifyingunintended biases, such as the top 30% most confident re-gions within an image. The selection of the noise additionscheme and the quantile used for masking out regions weretreated as hyperparameters in our analysis.Several metrics are available for evaluating model fair-ness, with Demographic Parity and Equality of Opportunity being the most prominent. In our assessment, focusing onperceived gender classification, we deemed it appropriate toexamine accuracy per subgroup. This approach allows us todetermine if any particular category (in this case, men orwomen) is disproportionately affected by the known bias.We employed a customized Convolutional Neural Net-work (CNN) architecture for gender classification, as weobserved that deeper network architectures such as VGG and ResNet were susceptible to overfitting. OurCNN architecture comprised three convolutional layers,supplemented with dropout regularization and batchnormalization . To optimize the model parameters, weutilized the AdamW optimizer with an initial learningrate set to 105, followed by exponential decay with a de-cay rate () of 0.95. We conducted training with a batch sizeof 128. Robust models incorporating noise were trained for20 epochs, while those utilizing masking techniques weretrained until convergence for 10 epochs.",
  ". Results": "We conducted experiments using all attributes illustrated in as confounding attributes, with the results summa-rized in . The table presents overall accuracy, as wellas accuracy for men and women separately on a balancedtest set, with the gap column indicating the difference be-tween these two. Notably, our experiments encompassedmultiple quantiles, yet we only included those yielding themost significant results for brevity. Additionally, we com-pared these results against classifiers trained on a datasetof equivalent size but balanced with respect to the attribute.Its worth noting that the data was not fully balanced in thecase of Blond Hair as the confounder for men but rather54/46 in favor of non-blond hair, as the original CelebAtrain set contains only 1387 men with blond hair.First of all, we can observe that all models have somediscrepancy between the accuracy for men and women, withthose trained on the biased dataset without any modifica-tions having the largest gap. Nevertheless, we observe thatfor each confounding attribute, there exists a specific com-bination of noise type and quantile for regularizing thatyields notably improved equality in accuracy, despite thesignificant divergence in the training distribution of mencompared to the balanced test data. Surprisingly, even whentrained on balanced data concerning the attribute, we stillobserve a performance gap between men and women, al-beit significantly smaller than with highly unbalanced data.This discrepancy likely persists due to other confoundersthat remain present, as we did not balance the data on theremaining 38 attributes. Furthermore, in the case of smiling,there is almost no discernible difference between balancedand unbalanced data. This observation may be attributedto the inherent difficulty in classifying smiling comparedto gender, resulting in a lesser impact of this bias on the",
  "Accuracy Accuracy Men Accuracy Women Gap TypeOriginalBalanced Ours OriginalBalanced Ours OriginalBalanced Ours OriginalBalancedOursAttributeNoiseQuantile": "Blond HairGeneral Mask0.600.74+0.090.00.6+0.2+0.020.88-0.03-0.030.280.050.22General Noise0.700.77+0.06-0.020.64+0.16+0.080.9-0.05-0.120.260.050.09Specific Mask0.600.74+0.09+0.010.6+0.2+0.090.88-0.03-0.080.280.050.11Specific Noise0.800.79+0.04-0.050.67+0.13+0.010.9-0.05-0.10.230.050.15EyeglassesGeneral Mask0.600.71+0.06-0.020.61+0.11+0.030.82+0.0-0.070.210.100.12General Noise0.600.72+0.05+0.010.61+0.11+0.050.84-0.02-0.040.230.100.14Specific Mask0.600.71+0.06+0.030.61+0.11+0.130.82+0.0-0.080.210.100.05Specific Noise0.800.72+0.05-0.010.61+0.11+0.10.84-0.02-0.130.230.100.09SmilingGeneral Mask0.800.81+0.03-0.040.71+0.06-0.020.9+0.01-0.050.190.130.16General Noise0.600.85-0.01-0.050.79-0.02-0.040.91-0.0-0.070.120.130.09Specific Mask0.950.81+0.03-0.020.71+0.06-0.020.9+0.01-0.010.190.130.20Specific Noise0.600.85-0.01-0.050.79-0.02+0.010.91-0.0-0.10.120.130.05Wearing HatGeneral Mask0.800.71+0.08+0.050.63+0.11+0.080.79+0.05+0.030.160.100.10General Noise0.950.73+0.06-0.020.63+0.11+0.030.84+0.0-0.070.210.100.11Specific Mask0.950.71+0.080.00.63+0.11+0.010.79+0.05-0.010.160.100.15Specific Noise0.950.73+0.06-0.030.63+0.11+0.040.84+0.0-0.10.210.100.08 . Summary of the noise addition experiment across multiple attributes of the CelebA dataset. Models are trained on a highly biaseddataset regarding each attribute, leading to a disparity in performance between men and women. This disparity between performance formen and women is denoted in the gap column. Accuracies for models trained on a balanced dataset, and those using our noise additionregularization are shown relative to those trained on the original, biased dataset. Through the strategic addition of noise to regions crucialfor each attribute based on a specified noise type, we diminish the discrepancy in accuracies between genders. model . However, our technique notably reduces the ac-curacy gap between men and women, approaching the per-formance achieved with balanced data, suggesting that ourmethod is a more practical approach than obtaining a bal-anced dataset on all attributes.Subsequently, we zoom in on the choice of the two hy-perparameters. illustrates a comparison of variousnoise addition strategies for the Blond Hair attribute, wherewe removed the 30% most influential information relatedto the attribute from all training data. All robust trainingschemes either enhance or maintain the accuracy for men,while diminishing the accuracy for women, thus aligningsubgroup accuracies and mitigating the effect of trainingon a biased dataset. Notably, the incorporation of GeneralNoise brings both subset accuracies closest to parity. AccuracyAccuracy MenAccuracy Women",
  "Next to the type of noise, the quantile of informationblocked is an important hyperparameter as well.Figure": "8 shows how the accuracy of the subgroups evolves withthe differing percentages. In the case of Specific Mask forthe Blond Hair attribute, the optimal point seems to be the70% mark. Masking out more reduces accuracy for womenfurther while more specific noise additions increase the in-equality in performance to that of unmodified data. 0.600.700.800.900.95",
  ". Conclusion": "In conclusion, our paper introduces a novel method for mit-igating bias in machine learning models, crucial for ensur-ing fairness and equity. We propose leveraging pixel imageattributions to identify and regulate regions within imagescontaining significant information about biased attributes.Our approach, employing a model-agnostic technique to ex-tract pixel attributions through a CNN classifier trained onsmall image patches, enables the identification of critical in-formation distribution across images. By utilizing these at- tributions to introduce targeted noise into datasets with con-founding attributes, we effectively prevent neural networksfrom learning biases and prioritize primary attributes. Ourmethod demonstrates its effectiveness in training unbiasedclassifiers on heavily biased datasets, offering promise forenhancing fairness and equity in machine learning applica-tions. Sander De Coninck receives funding from the Spe-cial Research Fund of Ghent University under grantno.BOF22/DOC/093.ThisresearchreceivedfundingfromtheFlemishGovernmentundertheOnderzoeksprogrammaArtificieleIntelligentie(AI)Vlaanderen programme. Wilbert G Aguilar, Marco A Luna, Julio F Moya, VanessaAbad, Hugo Ruiz, Humberto Parra, and William Lopez. Cas-cade classifiers and saliency maps based people detection. InAugmented Reality, Virtual Reality, and Computer Graph-ics: 4th International Conference, AVR 2017, Ugento, Italy,June 12-15, 2017, Proceedings, Part II 4, pages 501510.Springer, 2017. 3 Aidan Boyd, Kevin W. Bowyer, and Adam Czajka. Human-aided saliency maps improve generalization of deep learning.In Proceedings of the IEEE/CVF Winter Conference on Ap-plications of Computer Vision (WACV), pages 27352744,2022. 3",
  "Emily Denton, Ben Hutchinson, Margaret Mitchell, TimnitGebru, and Andrew Zaldivar. Image counterfactual sensi-tivity analysis for detecting unintended bias. arXiv preprintarXiv:1906.06439, 2019. 2": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. ICLR, 2021. 3 Debasmita Ghose, Shasvat M. Desai, Sneha Bhattacharya,Deep Chakraborty, Madalina Fiterau, and Tauhidur Rahman.Pedestrian detection in thermal images using saliency maps.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR) Workshops, 2019. 3",
  "Melissa Hall, Laurens van der Maaten, Laura Gustafson,Maxwell Jones, and Aaron Adcock. A systematic study ofbias amplification. arXiv preprint arXiv:2201.11706, 2022.7": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3, 6 Linzhi Huang, Mei Wang, Jiahao Liang, Weihong Deng,Hongzhi Shi, Dongchao Wen, Yingjie Zhang, and Jian Zhao.Gradient attention balance network: Mitigating face recog-nition racial bias via gradient attention. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 3847, 2023. 3 Sergey Ioffe and Christian Szegedy. Batch normalization:Accelerating deep network training by reducing internal co-variate shift. In International conference on machine learn-ing, pages 448456. pmlr, 2015. 6 Andrei Kapishnikov, Tolga Bolukbasi, Fernanda Viegas, andMichael Terry. Xrai: Better attributions through regions. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 49484957, 2019. 3 Kimmo Karkkainen and Jungseock Joo.Fairface: Faceattribute dataset for balanced race, gender, and age forbias measurement and mitigation.In Proceedings of theIEEE/CVF winter conference on applications of computervision, pages 15481558, 2021. 2, 3",
  "Medronha, Luis V Moura, Gabriel S Simoes, and Rodrigo CBarros. Fairness in deep learning: A survey on vision andlanguage research. ACM Computing Surveys, 2023. 1": "Amirarsalan Rajabi, Mehdi Yazdani-Jahromi, Ozlem OzmenGaribay, and Gita Sukthankar. Through a fair looking-glass:mitigating bias in image datasets.In International Con-ference on Human-Computer Interaction, pages 446459.Springer, 2023. 2 Vikram V Ramaswamy, Sunnie SY Kim, and Olga Rus-sakovsky. Fair attribute classification through latent spacede-biasing. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 93019310,2021. 2 Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. why should i trust you? explaining the predictions of anyclassifier. In Proceedings of the 22nd ACM SIGKDD interna-tional conference on knowledge discovery and data mining,pages 11351144, 2016. 3 Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.Grad-cam:Visual explanations from deep networks viagradient-based localization. In Proceedings of the IEEE in-ternational conference on computer vision, pages 618626,2017. 3",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomaticattribution for deep networks. In International conference onmachine learning, pages 33193328. PMLR, 2017. 3": "Tan Wang, Chang Zhou, Qianru Sun, and Hanwang Zhang.Causal attention for unbiased visual recognition. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 30913100, 2021. 2 Seyma Yucer, Samet Akcay, Noura Al-Moubayed, andToby P Breckon. Exploring racial bias within face recog-nition via per-subject adversarially-enabled data augmenta-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition Workshops, pages 1819, 2020. 2 Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell.Mitigating unwanted biases with adversarial learning.InProceedings of the 2018 AAAI/ACM Conference on AI,Ethics, and Society, pages 335340, 2018. 2"
}