{
  "Abstract": "Vision foundation models are increasingly employed inautonomous driving systems due to their advanced capabil-ities. However, these models are susceptible to adversarialattacks, posing significant risks to the reliability and safetyof autonomous vehicles. Adversaries can exploit these vul-nerabilities to manipulate the vehicles perception of its sur-roundings, leading to erroneous decisions and potentiallycatastrophic consequences. To address this challenge, wepropose a novel Precision-Guided Adversarial Attack (PG-Attack) framework that combines two techniques: Preci-sion Mask Perturbation Attack (PMP-Attack) and Decep-tive Text Patch Attack (DTP-Attack). PMP-Attack preciselytargets the attack region to minimize the overall perturba-tion while maximizing its impact on the target objects rep-resentation in the models feature space. DTP-Attack in-troduces deceptive text patches that disrupt the models un-derstanding of the scene, further enhancing the attacks ef-fectiveness. Our experiments demonstrate that PG-Attacksuccessfully deceives a variety of advanced multi-modallarge models, including GPT-4V, Qwen-VL, and imp-V1.Additionally, we won First-Place in the CVPR 2024 Work-shop Challenge: Black-box Adversarial Attacks on VisionFoundation Models and codes are available at",
  "*indicates equal contributions.indicates corresponding author.https : / / challenge . aisafety . org . cn / # /competitionDetail?id=13": "systems . These advanced models possesspowerful perception, decision-making, and control capabil-ities, which can greatly improve the performance and safetyof self-driving cars . In complex road environments,they can process massive amounts of data from multiplesensors in real-time, accurately identify surrounding ob-jects, vehicles, and pedestrians, and make appropriate driv-ing decisions . However, despite the impressive performance of visionfoundational models, they face a significant challenge: thethreat of adversarial attacks . Some maliciousadversaries may exploit vulnerabilities in these models bycarefully designing adversarial examples, and manipulatingthe models perception and understanding of the surround-ing environment. Therefore, improving the robustness andsafety of vision foundational models in autonomous driv-ing scenarios and defending against the risks of adversar-ial attacks has become crucial. Developing effective attackmethods can help us better understand the threat patterns ofadversarial attacks and develop targeted defense measures.However, creating effective adversarial examples for thesevision foundational models faces numerous challenges. Forinstance, vision foundational models consume significantmemory, making them difficult to use directly for infer-ring adversarial attacks, which poses major challenges fordeploying such models for attack inference. Additionally,when attacking models like GPT-4V , it is crucial toconsider the contents integrity; if perturbations cause im-ages to be misclassified as violent or other negative content,OpenAIs policy will prevent their evaluation. To address this challenge, we propose a novel Precision-Guided Adversarial Attack Framework (PG-Attack) thatseamlessly integrates two innovative approaches: PrecisionMask Patch Attack (PMP-Attack) and Deceptive Text PatchAttack (DTP-Attack).PMP-Attack leverages a maskedpatching approach to pinpoint the attack region, maximiz-",
  ". Visualization of the impact of PG-Attack on the VQAperformance of GPT-4V, imp-V1, and Qwen-VL": "ing the representation discrepancy of the target object in themodels feature space while minimizing overall perturba-tion. DTP-Attack, on the other hand, introduces deceptivetext patches to disrupt the models scene understanding, fur-ther augmenting the attacks efficacy. This integrated at-tack methodology effectively enhances the attack successrate across a wide spectrum of tasks and conditions. Bystrategically integrating PMP-Attack and DTP-Attack, ourapproach aims to maximize the attack success rate whilemaintaining high SSIM scores, effectively addressing thecompetitions requirements and constraints.",
  "The main contributions of this paper can be summarizedas follows:": "By integrating masked patch with adversarial attacks, wepropose PMP-Attack, a novel attack method that enablesprecise localization of attack regions while balancing at-tack effectiveness with structural similarity between pre-and post-attack images. We innovatively introduce Deceptive Text Patch Attack(DTP-Attack). DTP-Attack synergistically complementsPMP-Attack, disrupting the models scene understandingand further enhancing the attacks efficacy. Our experiments demonstrate that PG-Attack success-fully deceives a variety of advanced multi-modal largemodels, including GPT-4V , Qwen-VL , and imp-V1 .Additionally, we won the First-Place win-ner in the CVPR 2024 Workshop Challenge:Black-box Adversarial Attacks on Vision Foundation Models,fully demonstrating the effectiveness and impact of thismethod.",
  ". Vision Foundation Models": "Motivated by the success of large language models ,the field of computer vision has similarly embraced equallypowerful models. Qwen-VLs visual encoder uses theVision Transformer (ViT) architecture with pre-trainedweights from Openclips ViT-bigG. It resizes inputimages to a specific resolution, splits them into patcheswith a stride of 14, and generates a set of image fea-tures.Imps visual module employs the SigLIP-SO400M/14@384 as its pretrained visual encoder,enabling it to obtain fine-grained visual representationsthrough large-scale image-text contrastive learning. Addi-tionally, GPT-4V offers a more profound understand-ing and analysis of user-provided image inputs, highlight-ing the significant advancements in multimodal capabilitieswithin computer vision.",
  ". Adversarial Attack": "Adversarial attacks are classified into white-box attacks and black-box attacks based on the at-tackers knowledge of the target model. In white-box at-tacks, the attacker has full access to the target models de-tails, such as its network architecture and gradients.Inblack-box attacks, the attacker does not have access tothe internal information of the target model. Adversarialtransferability describes how effectively an attack devel-oped on a source model performs when applied to a dif-ferent target model. In computer vision, common adver-sarial attack methods include FGSM , I-FGSM ,PGD , etc. In natural language processing, attacks suchas TextFoole , BAE , and BERT-Attack manip-ulate the text by adding, altering, or deleting specific com-ponents to achieve the desired attack performance.In the attack on the multimodal large model, Zhang etal. combines visual and textual bimodal informationand proposes the first white-box attack, Co-attack, by uti-lizing the synergistic effect between images and text in theVLP model. Then, SGA first explores the black-box at-tacks and use data augmentation to generate multiple groupsof images, match them with multiple text descriptions, andcomprehensively utilize cross-modal guidance informationto improve the transferability of adversarial examples inblack-box models. CMI-Attack enhances modality in-teraction by using Embedding Guidance and Interaction En-hancement modules, significantly boosting the attack suc-cess rate of transferring adversarial examples to other mod-els. Based on this, we adopt CMI-Attack as the baselinemethod for our Precision Mask Perturbation Attack. Ourapproach further refines this by using mask patches to pre-cisely locate attack regions and removing the text attackcomponent, thereby focusing on enhancing the efficacy and",
  ". Problem Formulation": "Crafting effective adversarial examples that can disrupt amodels performance across multiple taskscolor judg-ment, image classification, and object countingis ex-tremely challenging. The key difficulty lies in optimizingperturbations that can subtly alter the models perceptionfor each individual task, while maintaining high cross-tasktransferability and image similarity under diverse condi-tions. Specifically, the adversarial examples must inducemisclassification, color confusion, and counting errors si-multaneously, without compromising spatial consistency orraising human suspicion. Optimizing for such diverse goalsrisks getting trapped in local optima, making the design ofhighly transferable and robust adversarial attacks an intri-cate endeavor. Furthermore, directly employing multimodallarge models to infer adversarial attacks poses a significantchallenge due to their immense memory footprint, render-ing the direct utilization of such models for attack inferencearduous. These challenges require careful planning, experi-mentation, and a deep understanding of both the target mod- els and the nature of adversarial perturbations. With limitedsubmission opportunities and a need for high naturalness inthe adversarial examples, efficient use of resources and iter-ative refinement are crucial for success in the competition.To address the aforementioned challenges, we haveadopted the following measures: Strategic Problem Transformation: We first view theentire task as a black-box transfer attack problem in thevisual question answering (VQA) domain, which canthen be transformed into an adversarial attack problem onvision-and-language models that have been widely usedto solve VQA tasks.Specifically, we aim to generateinput-based adversarial examples that cause the modelunder evaluation to fail to accurately answer the threetypes of task questions mentioned above. Optimized Transferability and Effectiveness: Visual-Language Pre-training (VLP) models such as CLIP and TCL , which leverage large-scale multimodalpre-training, offer several advantages for generating ad-versarial examples.Compared to multimodal largemodels, VLP models require significantly less memory,achieve faster inference speeds, and adversarial examplesgenerated from them exhibit strong transferability. Forthese reasons, we leverage a VLP model as the source",
  ". Framework": "Our proposed method consists of three phases, as illustratedin . Phase I is the modality expansion phase, wherewe input the initial dataset into the YOLOv8 model to com-pute the binary images with the key objects masked. Simi-larly, to obtain the textual modality of the dataset, we inputthe dataset into the BLIP model and generate imagecaptions through its Image Captioning task. Phase II repre-sents the first attack stage of our method, employing the im-age attack component from the CMI-Attack framework andfurther enhancing its effectiveness through data augmenta-tion. Notably, considering the challenges specific SSIMvalue requirements, we confine the attack range to the tar-get region to achieve optimal performance. We refer to thisprocess as the Precision Mask Perturbation Attack. PhaseIII constitutes the second attack stage of our method, wherewe incorporate disruptive text information into the imagesobtained from the previous stage in a bullet-chat-like man-ner to further enhance the attacks effectiveness against theVQA task of the black-box model. The disruptive text isdesigned based on the content of the specific VQA task be-ing attacked, aiming to mislead the models understanding.We refer to this attack process as the Deceptive Text PatchAttack. The whole description of the PG-Attack is shownin Algorithm 1.",
  ". Precision Mask Perturbation Attack": "This involves combining the CMI-Attack with mask patchmethod. The CMI-Attack enhances the overall effec-tiveness and robustness of the attack by ensuring the pertur-bations are subtle yet impactful. The mask patch method,on the other hand, targets specific areas of the image to im-prove the attacks precision and focus.The original CMI-Attack framework incorporates text-based attacks; however, since the competition does not in-volve text attacks, we have modified the optimization ob-jective of CMI-Attack. The overall optimization goal of ourframework is to maximize the semantic distance betweenthe adversarial image ImgAdv generated by the sourcemodel in the feature space of the image encoder EI and thecaption in the feature space of the text encoder ET . This isformally represented by Equation 1:",
  "Algorithm 1 Precision-Guided Adversarial Attack": "Input: Image i, Caption t, Multi-scale Image set Si ={i1, i2, . . . , im}, Caption set St = {t1, t2, . . . , tm},Image encoder EI, Text encoder ET , iteration steps T,interactively iterations N, joint gradient parameters ,attack step size and loss function J, maximum pertur-bation i, matrix M from the mask image and deceptivetext D.",
  ". Deceptive Text Patch Attack": "DTP-Attack further deceives models by adding a text patchattack to the image. This stage leverages textual elementsto further deceive the models, exploiting any weaknesses inhandling mixed content (visual and textual).The main algorithmic formula for the DTP-Attack is rep-resented as follows: ImgadvDT P Img + RenderText(D, DColor, DSize),(3)where ImgadvDT P represents the adversarial image after ap-plying the DTP-Attack. RenderText(D, DColor, DSize)is the function responsible for rendering the text patch onto",
  ". The Impact of Disruptive Text Quantity": "the image. D represents the textual content, DColor signi-fies the color of the text, and DSize denotes the size of thetext.The incorporation of textual elements into the adversar-ial attack expands the attack surface and increases the com-plexity of the deception, making it more challenging for themodel to discern between genuine and manipulated content.",
  ". Dataset": "The dataset is provided by the CVPR 2024 Workshop Chal-lenge and generated using the CARLA simulator.Thedataset for Phase I of the competition encompasses 461 im-ages, encompassing key objects such as cars, pedestrians,motorcycles, traffic lights, and road signals. Notably, thecars exhibit a diverse array of colors, including but not lim-ited to red, black, white, alternating green and white, alter-nating purple and white, alternating black and white, andothers.Interestingly, the traffic lights display a reddish-orange hue instead of the typical red, along with yellow andgreen colors. For Phase II, the dataset consists of 100 im-ages featuring similar key objects to Phase I.",
  "i=1ASRi[ + (1 ) SSIM(xi, xadv)],(4)": "where ASRi is the Attack Success Rate for the ith image,SSIM(xi, xadv) quantifies the structural similarity betweenthe original image xi and adversarially perturbed imagexadv, and (set to 0.5) determines the relative weightingbetween ASR and SSIM. A higher final score indicates bet-ter performance, as it signifies both a high success rate inmisleading the target models and a high degree of visualsimilarity preservation compared to the original images.",
  ". Implementation Details": "Reproduction Process. The reproduction of the attack pro-cess requires strictly following the procedures outlined in. First, the modality expansion phase is conductedto obtain the captions and target mask images.Subse-quently, the captions, original images, and target mask im-ages are utilized in the CMI-Attack framework to generatethe adversarial images from the first attack stage. Finally, inthe last phase, disruptive text is added to the images, furtherenhancing the attack capability against the VQA task.Hyperparameter Settings. Regarding the hyperparametersettings, we first followed the image augmentation schemeproposed in SGA . Additionally, we further enhancedthe CMI-Attack attack settings by applying scaling factorsof [1, 1/2, 1/4, 1/8, 1/16] to the images.We also aug-mented text by replicating each text three times and feedingit into the CMI-Attack attack setting. The attack step wasset to 2/255 and the number of attack steps was set to 60.Environment Configuration. Our proposed method is im-plemented using PyTorch, and the experiments are con-ducted on an NVIDIA GeForce RTX 3090 GPU.",
  ". Ablation Study": "In this section, we conduct ablation experiments to analyzevarious parameters of our approach. These parameters in-clude the perturbation range of the CMI-Attack on themask part, the color of the disruptive text, the quantity ofdisruptive text, and the font of the disruptive text. The ablation study shows that increasing the perturbationrange on the mask part significantly boosts the attack suc-cess rate, indicating that larger perturbations are more effec-tive in deceiving the model, as shown in . Addition-ally, the text color plays a crucial role in the attacks effec-tiveness, with black and contrasting color schemes, such asa white background with a black frame, resulting in highersuccess rates, as demonstrated in . The effective-ness of disruptive text quantity varies, with six text elementsachieving the highest attack success rate, followed by sevenand five, suggesting an optimal quantity for maximum dis-ruption, as illustrated in . Finally, the choice of fontdoes impact the attack success rate, with Times New Romanoutperforming Calibri and Arial in misleading the model, asshown in .Through these ablation experiments, we identify key fac-tors that influence the success rate of our proposed attack,providing insights for further optimization.",
  ". Conclusion": "This study highlights the vulnerabilities of vision founda-tion models in autonomous driving systems by demonstrat-ing the effectiveness of our Precision-Guided AdversarialAttack Framework (PG-Attack). Extensive experimentationshowed that adversarial attacks could significantly com-promise advanced multi-modal models, including GPT-4V,Qwen-VL, and imp-V1. Our approach achieved first placein the CVPR 2024 Workshop Challenge: Black-box Adver-sarial Attacks on Vision Foundation Models, setting a newbenchmark for attack efficacy and robustness. These find-ings underscore the critical need for more robust defensesand security measures to protect vision foundation modelsagainst adversarial threats.Broader impacts.Our work indicates that downstreamtasks of vision foundation models are currently exposed tosecurity risks. PG-Attack aids researchers in understandingvision foundation models from the perspective of adversar-ial attacks, thereby facilitating the design of more reliable,robust, and secure vision foundation models. By expos-ing these vulnerabilities, we hope to encourage the devel-opment of enhanced security measures and defenses, ulti-mately contributing to the safer deployment of autonomousdriving technologies and other critical applications relianton vision foundation models. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, SinanTan, Peng Wang, Junyang Lin, Chang Zhou, and JingrenZhou. Qwen-vl: A frontier large vision-language model withversatile abilities. CoRR, abs/2308.12966, 2023. 2",
  "autonomous driving systems. CoRR, abs/2312.06701, 2023.1": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, LinyiYang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang,Yidong Wang, et al. A survey on evaluation of large languagemodels. ACM Transactions on Intelligent Systems and Tech-nology, 15(3):145, 2024. 2 Zhaoyu Chen, Bo Li, Shuang Wu, Jianghe Xu, ShouhongDing, and Wenqiang Zhang.Shape matters: deformablepatch attack. In Computer Vision - ECCV 2022 - 17th Eu-ropean Conference, Tel Aviv, Israel, October 23-27, 2022,Proceedings, Part IV, pages 529548, 2022. 2",
  "Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, ShouhongDing, and Wenqiang Zhang. Content-based unrestricted ad-versarial attack. 2023. 2": "Mehdi Cherti, Romain Beaumont, Ross Wightman, MitchellWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-ing laws for contrastive language-image learning. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 28182829, 2023. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 2 Jiyuan Fu, Zhaoyu Chen, Kaixun Jiang, Haijing Guo, JiafengWang, Shuyong Gao, and Wenqiang Zhang. Improving ad-versarial transferability of vision-language pre-training mod-els through collaborative multimodal interaction.CoRR,abs/2403.10883, 2024. 2, 4, 5 Siddhant Garg and Goutham Ramakrishnan.BAE: bert-based adversarial examples for text classification. In Pro-ceedings of the 2020 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2020, Online,November 16-20, 2020, pages 61746181. Association forComputational Linguistics, 2020. 2 Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.Explaining and harnessing adversarial examples. In 3rd In-ternational Conference on Learning Representations, ICLR2015, San Diego, CA, USA, May 7-9, 2015, ConferenceTrack Proceedings, 2015. 2",
  "Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. IsBERT really robust? A strong baseline for natural language": "attack on text classification and entailment. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI2020, The Thirty-Second Innovative Applications of Artifi-cial Intelligence Conference, IAAI 2020, The Tenth AAAISymposium on Educational Advances in Artificial Intelli-gence, EAAI 2020, New York, NY, USA, February 7-12, 2020,pages 80188025. AAAI Press, 2020. 2 Wei-Bin Kou, Qingfeng Lin, Ming Tang, Sheng Xu, Rong-guang Ye, Yang Leng, Shuai Wang, Zhenyu Chen, GuangxuZhu, and Yik-Chung Wu. pfedlvm: A large vision model(lvm)-driven and latent feature-based personalized federatedlearning framework in autonomous driving, 2024. 1 Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Ad-versarial machine learning at scale. In 5th International Con-ference on Learning Representations, ICLR 2017, Toulon,France, April 24-26, 2017, Conference Track Proceedings.OpenReview.net, 2017. 2 Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.Hoi. BLIP: bootstrapping language-image pre-training forunified vision-language understanding and generation. In In-ternational Conference on Machine Learning, ICML 2022,17-23 July 2022, Baltimore, Maryland, USA, pages 1288812900. PMLR, 2022. 4 Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, andXipeng Qiu.BERT-ATTACK: adversarial attack againstBERT using BERT.In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural Language Processing,EMNLP 2020, Online, November 16-20, 2020, pages 61936202. Association for Computational Linguistics, 2020. 2 Yanze Li, Wenhua Zhang, Kai Chen, Yanxin Liu, PengxiangLi, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao,Zhenguo Li, Dit-Yan Yeung, Huchuan Lu, and Xu Jia. Au-tomated evaluation of large vision-language models on self-driving corner cases. CoRR, abs/2404.10595, 2024. 1",
  "Jiaqi Liu, Shiyu Fang, Xuekai Liu, Lulu Guo, Peng Hang,and Jian Sun.A decision-making gpt model augmentedwith entropy regularization for autonomous vehicles. arXivpreprint arXiv:2406.13908, 2024. 1": "Dong Lu,Zhiqiang Wang,Teng Wang,Weili Guan,Hongchang Gao, and Feng Zheng.Set-level guidance at-tack: Boosting adversarial transferability of vision-languagepre-training models. In IEEE/CVF International Conferenceon Computer Vision, ICCV 2023, Paris, France, October 1-6,2023, pages 102111. IEEE, 2023. 2, 5 Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,Dimitris Tsipras, and Adrian Vladu. Towards deep learningmodels resistant to adversarial attacks. In 6th InternationalConference on Learning Representations, ICLR 2018, Van-couver, BC, Canada, April 30 - May 3, 2018, ConferenceTrack Proceedings. OpenReview.net, 2018. 2",
  "Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, LihaoZheng, Zhenbiao Gai, Mingyang Wang, and Jiajun Ding.Imp: Highly capable large multimodal models for mobiledevices, 2024. 2": "Jiafeng Wang, Zhaoyu Chen, Kaixun Jiang, Dingkang Yang,Lingyi Hong, Pinxue Guo, Haijing Guo, and WenqiangZhang.Boosting the transferability of adversarial attackswith global momentum initialization. Expert Systems withApplications, page 124757, 2024. 1 Ningfei Wang, Yunpeng Luo, Takami Sato, Kaidi Xu, andQi Alfred Chen. Does physical adversarial example reallymatter to autonomous driving? towards system-level effectof adversarial object evasion attack. In IEEE/CVF Interna-tional Conference on Computer Vision, ICCV 2023, Paris,France, October 1-6, 2023, pages 43894400. IEEE, 2023.1 Licheng Wen, Xuemeng Yang, Daocheng Fu, XiaofengWang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, LinranXu, Dengke Shang, et al. On the road with gpt-4v (ision):Early explorations of visual-language model on autonomousdriving. arXiv preprint arXiv:2311.05332, 2023. 1 Dingkang Yang, Shuai Huang, Zhi Xu, Zhenpeng Li, ShunliWang, Mingcheng Li, Yuzheng Wang, Yang Liu, Kun Yang,Zhaoyu Chen, et al. Aide: A vision-driven multi-view, multi-modal, multi-tasking dataset for assistive driving perception.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 2045920470, 2023. 1 Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda,Liqun Chen, Belinda Zeng, Trishul Chilimbi, and JunzhouHuang. Vision-language pre-training with triple contrastivelearning. In IEEE/CVF Conference on Computer Vision andPattern Recognition, CVPR 2022, New Orleans, LA, USA,June 18-24, 2022, pages 1565015659. IEEE, 2022. 3 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, andLucas Beyer. Sigmoid loss for language image pre-training.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 1197511986, 2023. 2 Jiaming Zhang, Qi Yi, and Jitao Sang. Towards adversar-ial attack on vision-language pre-training models. In MM22: The 30th ACM International Conference on Multime-dia, Lisboa, Portugal, October 10 - 14, 2022, pages 50055013. ACM, 2022. 2 Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, XiaoleiWang, Yupeng Hou, Yingqian Min, Beichen Zhang, JunjieZhang, Zican Dong, et al. A survey of large language mod-els. arXiv preprint arXiv:2303.18223, 2023. 2"
}