{
  "{kostas,spiros,nick}@moverse.ai": ". Given a single motion sequence our improved GAN learns to generate motion variations in minutes using mini-batch trainingand transfer learning, without compromising the quality or the diversity of synthesized motion. Here we generate variations of the Mixamosequences (raw-major order): a) breakdance freezes , b) dancing, c) swing dancing, and d) salsa dancing .",
  "Abstract": "Despite the recent advances in the so-called cold startgeneration from text prompts, their needs in data and com-puting resources, as well as the ambiguities around intel-lectual property and privacy concerns pose certain coun-terarguments for their utility. An interesting and relativelyunexplored alternative has been the introduction of uncon-ditional synthesis from a single sample, which has led tointeresting generative applications. In this paper we focuson single-shot motion generation and more specifically onaccelerating the training time of a Generative AdversarialNetwork (GAN). In particular, we tackle the challenge ofGANs equilibrium collapse when using mini-batch train-ing by carefully annealing the weights of the loss functionsthat prevent mode collapse. Additionally, we perform sta-tistical analysis in the generator and discriminator mod-els to identify correlations between training stages and en-able transfer learning. Our improved GAN achieves com-petitive quality and diversity on the Mixamo benchmarkwhen compared to the original GAN architecture and asingle-shot diffusion model, while being up to 6.8 faster",
  ". Introduction": "Since the advent of the Large Language Models (LLMs),the so-called cold-start generation has attracted impres-sive attention as a viable path to artificial general intelli-gence. In fact, LLMs have demonstrated proficiency in var-ious domains transforming text information to im-ages, scenes, and more recently to human pose and mo-tion even coupled with denoising diffusion model variants . However, the said models require massivecomputational resources and are trained on vast amountsof annotated data, which can include personal and sensi-tive information. Their lack of interpretability and explain-ability poses a certain risk that they may inadvertentlymemorize and reproduce this sensitive data during genera-tion, which raises ethical and intellectual property barriers.",
  "arXiv:2406.01136v2 [cs.CV] 4 Jun 2024": "Inference, or using a pre-trained LLM for generating text,also demands significant computational resources. LLMsmay reflect and potentially amplify biases present in thetraining data, leading to biased or unfair outputs. Address-ing bias requires careful curation of training data and modeldesign, which can be resource-intensive and challenging.An interesting alternative to cold-start generation and itschallenges are the single sample generative models, whichcan serve as powerful editing tools as they provide a goodbalance between pluralism and context preservation. Pio-neered in the domain of images, they have been used toremap , composite and edit images, increase theirresolution , and also generate and expand tex-tures. Even though they are very important as they can helpovercome intellectual property and data privacy/sensitivityissues, they still remain a relatively unexplored topic. In thecontext of 3D content, they are even more important as compared to text, images, and/or video 3D data are morechallenging to acquire in quantity. Specifically for 3D mo-tion, two variants have been recently introduced, GANima-tor and SinMDM that represent the two dominantclasses of approaching this task, namely using generativeadversarial networks (GANs) and denoising diffusionprobabilistic models (DDPMs) . While both are hyper-parameter and architecture sensitive approaches, the latter(SinMDM) was shown to be faster to train than the former(GANimator), as well as support more applications with-out re-training. On the other hand, GANimator relies on asingle forward pass, and thus, exhibits much faster infer-ence performance compared to the iterative nature of dif-fusion. Content editing applications need to support inter-active workflows (i.e. real-time inference) but at the sametime the workflows are on-demand, which also imposesconstraints on the set up time (i.e. fast training).In this work we focus on improving the training time ofGANimator that already delivers real-time inference, tak-ing a step towards the practical realization of single motiongenerative editing. We find that a major limitation of sin-gle sample GANs compared to DDPMs is the lack of mini-batch training. The latter is a challenge for single sampletraining, especially in the unconditional case, a task thatneeds to balance adversarial training with a latent anchor-ing objective. Further, we show that the hierarchical natureof GANimator is an unexplored trait that can be exploited toimprove training time and realize more editing applicationsin a unified manner and without retraining.Summarizing, our contribution is two-fold: We study the challenges for mini-batch and hierarchicaltraining in the single sample GAN regime and show in-crease its performance by a factor of 10 through combin-ing mini-batch training and cross-stage transfer learning.Our GANimator variant trains faster than SinMDM andsimultaneously offers real-time inference performance.",
  ". Prior Art": "Data-driven motion generation. Synthesizing human mo-tion is a long standing problem with some of the early ap-proaches being based on statistical modelling , exemplar or graph walk based composition, and learningthe distribution of novel constructs like motion textons .Given larger datasets and modern learning techniques it isnow possible to generate motion from text , of-fer a GPT-like interface for motion and use said mod-els to perform editing tasks . More general motionsynthesis and editing frameworks have also been presented due to the wider availability of large scale motiondatasets. Still, large dataset acquisition is challenging, es-pecially when considering text prompt annotations. It alsocomes with barriers related to the sensitivity of the data ei-ther from a personal or creative point of view.Single-shot generation. Simple sample generative modelsare a promising alternative to overcome these barriers asthey train specialized models that generate variations of aspecific sample only. InGAN first showed that it ispossible to train a conditional GAN model on a single im-age for the task of remapping, with follow-up works focus-ing on texture generation and editing . Followinga progressive learning scheme across multiple stages, eachoperating on a different scale, SinGAN is the first un-conditional GAN trained on a single image. Crucially itrelies on a patch-based discriminator restricting itsreceptive field, which is coupled with a reconstruction ob-jective that anchors its latent space protect the model train-ing from a mode collapse.Nonetheless, this increases the training time, which when considering the single sample context is a big ob-stacle for practical application use. Different variants fol-lowed, with ExSinGAN using external priors to im-prove structural and semantic performance of the model,while ConSinGAN focused on improving the train-ing time by training stages in parallel and reducing theirnumber. To improve the preservation of samples context,OneShotGAN introduced a dual discriminator to su-pervise both the global context, as well as the patch-basedlayout. PetsGAN improves training time by leverag-ing external priors whereas HP-VAE-GAN opts for ahybrid VAE-GAN scheme to enable single video genera-tion. The challenge of quickly training single sample gen-erative models mostly stems from the nature of the adver-sarial game, and thus, novel approaches that reformulate thetask to a reconstruction or nearest neighbor retrieval one manage to greatly accelerate training time, but atthe expense of generation variance. Using diffusion mod- els, like SinDDM , is another alternative to reducingtraining time due to mini-batch training. The interactionbetween the added reconstruction objective with the adver-sarial game is difficult to balance, and is the reason why sin-gle sample GANs are typically trained with a batch size ofone as training destabilizes when increasing the batch size.Single-shot 3D content generation. More recently, scarceefforts have been made to demonstrate single sample gener-ation to 3D content. Sin3DM trains a diffusion modelon a single sample, and leverages and intermediate latentrepresentation to overcome memory and runtime perfor-mance issues. SinGRAF bridges a neural renderingrepresentation to accomplish single sample variation gen-eration of specific 3D scenes. For 3D motion single sam-ple generation there exist two approaches, GANimator and SinMDM , using adversarial training and denoisingdiffusion respectively. GANimator consists of 7 stages ofskeleton-aware convolutions forming 4 pyramid levels(2-2-2-1 stages) emulating the pyramidal design of SinGANin the temporal domain, where each stage learns to gener-ate a sequence of different length (up to the inputs one).Changing the core of the aforementioned works, Raab etal. present a motion diffusion model that learns to gen-erate variations of a single motion sequence. SinMDM fol-lows the structure of the UNet-based diffusion model pre-sented in but with a significant detail; they add shift-invariant local attention layers to decrease the recep-tive field of UNet and avoid overfitting in the single sam-ple scenario. GANimator is slower to train but significantlyfaster when generating samples, whereas SinMDM exploitsmini-batch training to reduce training time but requires aniterative diffusion process at inference. Further, SinMDMsupports more applications without requiring retraining, animportant advantage from a practical point of view.",
  ". Background": "We present the background of GANimator and formal-ize the notations to set the stage for our improvements in theGAN training process.Data representation. Li et al. form a motion rep-resentation MT RT (JQ+C+3), where T indicates thenumber of frames, J is the number of skeleton joints, Q = 6corresponds to the 6D rotation representation of the joints,and C indicates the foot contact labels followed by the 3Drepresentation of the root joint including the x- and z-axisvelocity and the y-axis position. . A schematic representation of the GANimator grad-ual training architecture. From top to bottom, each pyramid levelis learning to generate motion features at time scale ( T). When apyramid level is trained it serves as a frozen feature extractor forthe next level. Note that the last pyramid level (L4) consists ofonly one {G(), D()} pair. GAN architecture. GANimator follows a coarse-to-finemotion feature learning approach, with S stages of genera-tors G() and discriminators D() pairs. The GAN modeldoes not train all stages in an end-to-end manner, but fol-lows a gradual learning approach, as shown in . Thestages are groups of pairs of {G(), D()} forming 4 pyra-mid levels L - except for L4 that contains only S7. Eachlevels stage learns to generate the motion features of in-creasing temporal resolution. For the rest of the paper weuse the subscript to denote stage levels and the superscriptto denote the pyramid levels (e.g. T 12 corresponds to thegenerated motion features from the second G() of the firstpyramid level). Each generator G() and discriminator D()consists of 4 skeleton-aware convolutions followed byleaky ReLU activations (except for the final convolutionlayer). As depicted in , G11 is responsible for learn-ing the mapping between the sampled noise and the motionrepresentation denoted as T +1, i.e. T 11 = G11(z1), while therest G{2,...,S}() form a hierarchical auto-regressive processthat progressively upsamples the generated sequence:",
  "T i = Gi( T i1, zi), i {2, . . . , S}, {1, . . . , L}, (1)": "where zi is sampled from an i.i.d. normal distributionN(0, I) and multiplied with an decreasing amplitude i.Losses. The model is trained with multiple losses for se-curing an equilibrium in the adversarial game between gen-erators and discriminators, having the constraint that thereis only a single sample to train the model. Both G() andD() are supervised with the Wasserstein variant from :",
  "||Di( Ti)||2 12,": ". GANimators pyramid level 1 in detail: G1 is responsi-ble for learning the mapping between sampled noise z1 N(0, I)(multiplied by a predefined amplitude) and the motion representa-tion T, which is then upsampled ( T1) and sent to D1; before G2we add extra noise to the predicted motion features T1 to force themodel to learn variations of the input sample. where P denotes a learned distribution and Ti = Ti +(1)Ti is a linear combination of the generated and groundtruth motion features, respectively.The last term of theequation, i.e. the gradient penalty regularization, enforcesLipschitz continuity and stabilizes the training. To preventmode collapse due to the single-sample training the G() areadditionally supervised by an L1 reconstruction loss:",
  "One major advantage of the single-shot diffusion models vssingle-shot GANs is the exploitation of mini-batch training": ".Representation similarities across stages and levels.Each generator (Gj) exhibits low representation similarity scoreswith the following generator (Gj+1) (orange dashed arrows &plots).Yet, we find that the corresponding stages generators(Gj Gj+2) across levels (Li) exhibit higher similarity scoresfor the early layers only (1 & 2 purple dashed arrows & plots).Transferring the trained generators (Gj) layers weights to thenext levels (Li+1) corresponding stages generators (Gj+2) be-fore training them, improves convergence rate. In fact, finding the equilibrium in the adversarial game ofa single-shot GAN is challenging and the mode collapse isa common result when trying to set the batch size largerthan 1. As discussed in , in a data-driven adversarialgame mini-batching helps the discriminator to understandwhen the generator produces samples of very low variationand avoid mode collapse due to this side information. Insingle-shot generation, the outputs of the generator are bydefinition minor variations of the single input sample, whilethe narrow receptive field of the patch-based discriminatorand the reconstruction loss Lrec are responsible for prevent-ing the mode collapse. Naturally, increasing the batch sizecounters the intuition that the narrow receptive field of thediscriminator will prevent mode collapse, while the recon-struction loss tends to reduce the coverage by forcing theaverage of the batch to be similar to the input sequence.Since mini-batching critically improves the training time,we performed an ablation study on the weights of Lrec andLadv in a quest for finding the correct combination that pre-serves the equilibrium in the adversarial game. However,note that this is not a straight-forward weight tuning processsince each loss operates on different parts of the trainingprocess (i.e. different optimizer). Starting from the originalweight values adv = 1 and rec = 50 we choose a stage-based linear annealing of: a) adv, b) rec, c) both adv andrec. As shown in , we achieve the best results with(c) when we boost Ladv in the early stages where mappingfrom sampled noise to motion representation takes place,while the later stages are dominated by the reconstructionloss to ensure that no rare mini-motions are omitted. . Ablation study of the model parameters for the GANimator architecture. The demonstrated results correspond to motionsalsa dance of the Mixamo benchmark. Although the GAN variant with batch size 24 (Abl #10) is the fastest to train, Abl #9 exhibits thebest trade-off between quality/diversity and train time.",
  ". Cross-Stage Transfer Learning": "Apart from increasing convergence through larger mini-batch training, the hierarchical structure of single sampleGANs can be exploited to improve convergence. ConSin-GAN trained multiple stages in parallel and re-usedthe discriminator from the previous stage to improve perfor-mance by transferring its weights to initialize the next stagediscriminator. GANimator already trains two stages inthe same pyramid level but does not perform discrimina-tor transfer across stages or pyramid levels, even though thesame discriminator is used in terms of architecture and ca-pacity - as in ConSinGAN and in contrast to SinGAN thatincreases discriminator capacity at each stage.Curiously, re-using the generator weights does not leadto improved performance. To study this we turn to study-ing the neural network representations using finer grainedlayer-wise analysis. It has been shown that the similarity ofrepresentations across layers can be measured despite thehigher dimensionality of the representations . We per-form linear centered kernel alignment (CKA) for all stagecombinations across all pyramid levels.Our results, depicted in , show that despite the hier-archical approach of using the same model and capacity foreach stage, most layers exhibit low similarity scores. Yetwe find that the early layer representations across the stagesof each pyramid level, exhibit higher levels of similarity.Contrary to ConSinGAN that outputs features at each levelapart from the last, GANimator reconstructs the output mo-tion at each level. Therefore the early convolutional lay-ers operate on similar features and, as indicated by theirCKA scores, extract similar representations, whereas thelatter layers apply the levels motion motifs, style and de-tails. Based on this analysis, we design a generator transferlearning scheme across levels and stages where each gener-ators (Gij) early layer weights are initialized from the earlylayer weights of the previous level generator (Gi1j2). Thisensures that each next training stage is closer to the con-verged state allowing us to reduce the number of iterationssignificantly, further boosting training time.",
  ". Quantitative analysis": "Fist, we briefly report the metrics used in the literature forassessing the quality and the diversity of single-sample gen-eration, present our implementation details, and discuss ourresults compared to similar models .Metrics. For a fair comparison we use the metrics pre-sented in , which try to measure the local and globaldiversity of the generated sequences, as well as their qual-ity in terms of plausibility and coverage. We give a briefdescription for each metric, commenting on its usefulness.Quality: The combination of the coverage and the plau-sibility expressed as distance from a distribution form a ro-bust pair for understanding how realistic is the generatedmotion. Li et al. consider a temporal window Tw ofthe input sequence covered if its distance from its nearestneighbor Qw in the generated sequence is less than a pre-defined threshold ; on the other hand, SinMDM adopts theFrechet Inception Distance (FID) variant from , whichuses the deep features of an earlier convolutional layer ofthe Inception Network in order to compute the FIDstatistics between the input and the generated sequences.As noted in , coverage has been experimentally shownto be sensitive, thus we choose to interpret it jointly with thesingle sample FID (SiFID) to truly describe quality.Diversity: and compute the local and globaldiversity of the generated sequence following different ap-proaches; Li et al. use high-level features (i.e. rota-tion angles) to compute distances either from the nearestneighbors of the input sequence, while Raab et al. useembeddings of motion features from a pre-trained motionencoder for computing the corresponding distances. Fromour experiments, we confirm the superiority of deep fea- . Quality, diversity and performance comparison between our improved GAN, GANimator and SinMDM . We present theaverage results on the Mixamo benchmark. The presented train and inference times are measured on a NVIDIA RTX 3060 GPU on thatsalsa dance with character Joe sample from Mixamo. Following we compute the harmonic mean of the 6 metrics to provide abalanced overall result for each experiment.",
  ".505h30m5.2ms0.941.421.001.081.431.930.581h24m5000msOurs0.891.461.351.991.751.570.520h48m5.2ms": ". Body-part composition: (top) snapshots of 7 generated swing dancing motion variants with the lower body masked (shadedarea) to preserve the original sequence pose, while the upper body is randomly generated; (bottom) snapshots of 7 generated swingdancing motion variants with the upper body masked to preserve the original sequence pose, while the lower body is randomly generated.Note that the root joint is considered as part of the lower body, thus the top samples retain the global orientation of the original motion. tures over raw input features in interpreting the inter-diversity and the intra-diversity of the generated sequences.However, we run our evaluation using all presented metrics,as well as the harmonic mean from that attempts to de-scribe both quality and diversity with one value.Data. We use the Mixamo1 sequences presented in to evaluate our improved GAN against the GANimator andthe SinMDM in terms of training time, inference time, qual-ity and diversity using the aforementioned metrics.Implementation details. We use the provided Py-Torch implementations for GANimator and SinMDMand adopt the pretrained motion encoder from the SinMDMcodebase to extract the motion embeddings for SiFID andinter/intra diversity metrics computation. All experimentsare conducted on a NVIDIA RTX 3060 GPU.Results. details the performed ablation study forimproving the training performance of GANimator by val-idating the two presented techniques, i.e. mini-batch train-ing with equilibrium preservation and transfer learning be- tween pyramid levels. As shown in the results, increasingthe batch size significantly improves the training time, butshould be accompanied with an increase of training itera-tions, adv, and rec. However, larger batch sizes seem tohurt the G() D() adversarial game and leads to infe-rior performance despite decreasing the training time. Af-ter having improved training time and demonstrated similarperformance with the baseline, we move to exploit the cor-relations between pyramid levels. From , we can seethat the combination of cross-stage transfer for the gener-ators G(), and annealing adv and rec leads to the bestresults, while exhibiting the best improvement in trainingtime. As a next step, we compare our best model with GAN-imator and SinMDM . From the results in ,we conclude that our improved GAN achieves slightly bet-ter results that its baseline when tested on the Mixamodataset, while also approaching SinMDM . However,the results showcase that our GAN exhibits a significant im-provement in training time compared to both SoTA (almost7 and 2 respectively), while being extremely faster . Full-body motion composition example: (top) we select the region of interest (ROI), i.e. a clock-wise spin, from the salsadancing sequence of the Mixamo corpus; (middle) a variat of the original sequence is generated with the selected ROI removed and thetemporal space being filled with motion features generated from noise; (bottom) a new salsa dancing variant is composed with 2 spinsbeing placed at user-selected time steps - the non-ROI region are generated by sampled noise. We depict more frames of the composedsalsa variant (bottom) to demonstrate the 2 spins in the same row.",
  ". Applications": "Apart from the training time performance increase, we in-troduce new applications that can be performed with asingle-shot GAN-based model, such as the body-part andfull-body motion composition, in addition to showcasing re-sults on applications introduced by GANimator, like motionre-styling and crowd generation. Note that contrary to we focus solely on applications that do not need re-training,as they pose the main challenge and exploit the superiorityof GANs over other approaches in terms of performance.Body-part motion composition. As detailed in Sec. 3.1,the motion feature MT of each input sequence T includesa 6D representation about each skeleton joint. This allow usto define body binary masks M for the upper and the lowerbody, which can be applied on the motion features duringinference and force the masked area to retain its originalvalues, while the rest of the bodys movement will be gen-erated based on randomly sampled noise. As depicted in (top) we use the Mixamo motion swing dancing asinput sequence T and we choose to keep the lower-body un-altered, while generating alternative - but natural - versionsof the upper-body. To achieve that, we use the G() trainedwith this sequence; the hierarchical structure of the model allows us to choose at which level L to apply the prede-fined mask M lb that will keep the lower-body (lb) unaltered.We choose to apply M lb at L2 as it leads to the smoothestblending of the body parts. (bottom) demonstrates theapplication of M ub to the upper-body (ub) of the same mo-tion sample, which preserves its pose despite the generatedglobal rotation and lower body pose.Full-body motion composition. Assuming a referencemotion T of arbitrary length, we consider the followingoptions: a) remove mini-clips of T and use the GAN toinpaint them with generated content, and b) select one(or more) mini-clip(s) from T and compose a new mo-tion with the mini-clip(s) placed at the temporal spot(s) ofinterest. To perform this options, we use a binary maskM BT (JQ+C+3) applied to the whole motion featureand not on specific joint-related features. As depicted in, we use M to select the region of interest (ROI) ofthe salsa dance sequence, i.e. M values are ones for theframes that correspond to a clock-wise spin. Then, for op-tion (a) we remove the ROI and inpaint the missing part ofthe motion as:",
  ". An exampled of crowd generation using the model train-ing on the dancing Mixamo benchmark sample": "in the downsampled () salsa sequence. The result of theinpainting is presented in the middle of . For option(b) we use the ROI as a standalone mini-clip T ROI whichwe downsample to the L2 input level and concatenate a pre-defined numbers of ROIs with generated motion featuresfrom L1 level. For example, as depicted in (bottom),two T ROI - each representing a spin - are concatenated withT 12 generated by generators trained with the salsa dancesample. The two spins are smoothly blended into the gen-erated salsa dance sequence in the desired time steps.Crowd generation & motion expansion. Single-shotlearning enables the generation of motions with commonlow-frequency features, i.e. same motion base, and smallvariations in the high-level features.This means thatby sampling multiple codes from a Gaussian distributionN(0, I), we can generate a crowd performing similar mo-tions. An example of crowd generation is depicted in .Another straight-forward application is the motion expan-sion. Since the skeleton-aware convolutions can be appliedto a motion feature of arbitrary size, we can concatenategenerated features T 24 to the downsampled original motionfeatures T 24 at the temporal dimension and use them as inputto the corresponding G{2,...,S}.Re-styling. This has been the most discussed applica-tion in single-shot generation as it is relatively trivial for theimage domain, however applying style on a motion is chal-lenging. In the image domain, transferring style is the pro-cess of applying texture encoded information on image con-tent (e.g. applying a style of another artist on a painting asin ). In the motion domain, style transfer is realized asapplying high-frequency details on low-frequency featuresthat correspond to a certain motion. This means that onecannot apply a dancing style from a stationary (i.e. minortranslation) motion to a walking one with single-shot gen-eration. However, even with some restrictions, re-stylinga motion in real-time is still valuable and leads to interest-ing results, as the example in . To re-style motion Txwith style from motion Ty, we use the generators Gyi withi 2, . . . S, i.e. the stages that learn the high-level features",
  ". Conclusion": "In this work we investigate the performance of the single-sample generation GANs and address its key challenges.Building on prior work for learning motion generationfrom a single sample, we propose an loss weight annealingtechnique for enabling mini-batch training without compro-mising the adversarial equilibrium. To further minimize therequired training iterations we propose a certain cross-stageweight initialization process based on a statistical analysisthat exposes correlations between GAN stages.Overall,having similar performance in quality and diversity asanchor, our GAN improves GANimator and SinMDMtrain time and achieves impressive results in real-timeapplications without the need for re-training. Next stepsinclude the integration of prior knowledge to further speedup training performance without compromising quality ordiversity, as well as the investigation for a more specializedmetric that will combine coverage with quality to be able toindicate when we sacrifice tail mini-motions for generatedmotion smoothness or variation.",
  "Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.Image style transfer using convolutional neural networks.In Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR),pages 24142423, 2016. 8": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-nications of the ACM, 63(11):139144, 2020. 2 Niv Granot, Ben Feinstein, Assaf Shocher, Shai Bagon, andMichal Irani. Drop the GAN: In defense of patches nearestneighbors as single image generative models. In Proc. IEEEConf. Comput. Vis. Pattern Recog. (CVPR), pages 1346013469, 2022. 2",
  "Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, andOlga Sorkine-Hornung. GANimator: Neural motion synthe-sis from a single sequence. ACM Trans. Graph. (TOG), 41(4):138, 2022. 2, 3, 4, 5, 6, 7": "Yan Li, Tianshu Wang, and Heung-Yeung Shum. Motiontexture: A two-level statistical model for character motionsynthesis. In Proc. Annual Conf. on Comp. Graph. and Inter.Tech. (CGIT), pages 465472, 2002. 2 Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,Pranav Shyam,Pamela Mishkin,Bob McGrew,IlyaSutskever, and Mark Chen. GLIDE: Towards photorealis-tic image generation and editing with text-guided diffusion"
}