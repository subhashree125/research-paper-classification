{
  "Abstract": "Vision and Language Models (VLMs) continue to demon-strate remarkable zero-shot (ZS) performance across vari-ous tasks. However, many probing studies have revealedthat even the best-performing VLMs struggle to captureaspects of compositional scene understanding, lacking theability to properly ground and localize linguistic phrases inimages. Recent VLM advancements include scaling up bothmodel and dataset sizes, additional training objectives andlevels of supervision, and variations in the model architec-tures. To characterize the grounding ability of VLMs, suchas phrase grounding, referring expressions comprehension,and relationship understanding, Pointing Game has beenused as an evaluation metric for datasets with boundingbox annotations. In this paper, we introduce a novel suiteof quantitative metrics that utilize GradCAM activationsto rigorously evaluate the grounding capabilities of pre-trained VLMs like CLIP, BLIP, and ALBEF. These metricsoffer an explainable and quantifiable approach for a moredetailed comparison of the zero-shot capabilities of VLMsand enable measuring models grounding uncertainty. Thischaracterization reveals interesting tradeoffs between thesize of the model, the dataset size, and their performance.",
  ". Introduction": "Foundational Vision and Language Models (VLMs) havedemonstrated impressive performance in various visionand language tasks, including visual question answering(VQA), retrieval, image-text matching, referring expressioncomprehension, or captioning. For instance, CLIP has be-come a widely-used backbone in various applications, rang-ing from open-vocabulary object localization & referringexpression grounding , to in-context learning in gen-erative Multimodal Large Language Models (MLLMs) suchas LLaVa , and Embodied AI for navigation-relatedtasks . Despite that, state-of-the-art VLMs still strug-gle to capture aspects of compositional scene understand-",
  "GT BBoX of her headBLIP PG UncertaintyBLIP NMS Analysis": ". Uncertainty in Pointing Game (PG) accuracy, when hav-ing multiple top-k identical activations with inconsistent PG binarylabels (Scenario 1). As depicted in the bottom-right figure, threetop high-confidence activations exist, each with a value of 1.0,after our NMS analysis. One falls outside the bounding box, oneinside, and one at the border. In these cases, PG lacks any addi-tional clues or heuristics to determine which one to select. ing and lack proper grounding for noun phrases, verbs, orrelations . Earlier model architectures localizeobjects of interest by predicting bounding box locationsand use traditional IoU evaluation metrics with respect toground truth datasets . Recent MLLMs forgo thebounding box prediction and tackle the location predictionon the language side . The ability to ground and lo-calize the concepts continues to be of central interest ,and GradCAM visualizations or Pointing Game accu-racy are often used to quantify models grounding abil-ity. Pointing Game (PG) considers grounding successfulif the highest GradCAM activation falls inside the ground",
  ". Method": "Consider GradCAM activation map Ai,j, obtained by pass-ing an image and a corresponding text prompt to the model,and Mi,j are pixel locations of the binary ground-truthbounding box mask. For computing IoUSoft and DiceSoft,weve followed the formulas used for semantic segmenta-tion, according to . We first apply the threshold of 0.5to each pixel value in Ai,j and pass the thresholded binaryactivation maps to compute the IoUBinary and DiceBinary.Distance Maps. Given the bounding box coordinates as(y0, x0, y1, x1), distance maps Di,j is computed as for all(i, j) where 0 i < H, 0 j < W:",
  "SoftBinarySoftBinarySoftBinaryLogSigAccuracy Uncertainty": "Flickr30K EntitiesBLIPbase0.120.090.210.150.950.790.4360.08317 / 14481(TEST)BLIPlarge0.140.110.230.170.940.770.4471.43965 / 14481CLIPgScoreCAM0.210.230.330.340.920.560.4475.410 / 14481ALBEFAMC0.190.160.310.250.860.600.6287.6910 / 14481 RefCOCO+BLIPbase0.100.060.180.100.980.750.3967.90198 / 5726(TESTA)BLIPlarge0.100.060.180.100.980.760.3867.27336 / 5726CLIPgScoreCAM0.170.150.280.240.980.680.3463.360 / 5726ALBEFAMC0.140.090.240.150.960.660.5478.8114 / 5726 RefCOCO+BLIPbase0.110.060.200.110.980.860.3146.96201 / 4889(TESTB)BLIPlarge0.110.060.200.110.980.870.3046.71449 / 4889CLIPgScoreCAM0.180.190.290.290.970.850.3049.020 / 4889ALBEFAMC0.160.110.270.190.960.760.4564.3416 / 4889 SpatialSenseBLIPbase0.110.120.200.180.970.750.2946.9950 / 1811(TRIPLETS)BLIPlarge0.120.120.210.190.970.730.3151.68100 / 1811CLIPgScoreCAM0.160.200.260.300.950.810.2850.570 / 1811ALBEFAMC0.170.160.270.260.930.710.4267.643 / 1811 SpatialSenseBLIPbase0.100.080.170.140.970.760.2939.8142 / 1811(SUBJECTS)BLIPlarge0.110.110.200.170.970.740.3049.75116 / 1811CLIPgScoreCAM0.170.230.280.340.950.690.3160.070 / 1811ALBEFAMC0.160.150.270.240.890.700.5270.014 / 1811 SpatialSenseBLIPbase0.110.070.190.120.920.690.4250.0237 / 1811(OBJECTS)BLIPlarge0.120.080.210.130.920.740.4054.55158 / 1811CLIPgScoreCAM0.200.210.320.320.900.650.4164.930 / 1811ALBEFAMC0.160.120.270.190.810.650.6378.243 / 1811 . Quantitative results comparison for all settings where numbers are reported as mean IoU (mIoU) across each setting. For eachsetting, the top performance across all models is highlighted as bold, and the second-best with underline.",
  "(5)": "Pointing Game Uncertainty Analysis:We extract thelocal maxima (v) of the activation map Ai,j higher thanthe activation threshold () of 0.7 as the set of V={v1, v2, ..., vn}. V is then sorted, followed by an additionalNMS step of suppressing maxima that are within Euclideandistance 2 of larger extremum. Vnms and Cnms, representremaining activation values and their corresponding coordi-nates. For every point in Cnms, we check whether it fallsinside the ground-truth bounding box and count the numberof cases where top-k equal activations in Vnms are not alleither inside or outside of the bounding box.",
  "= 50 in our experiments": "BLIPlarge, CLIP gScoreCAM, and the AMC variation ofALBEF) on a wide range of grounding tasks, varied bythe text prompt granularity, and with both in-distribution(ID) and out-of-distribution (OOD) data. The quantitativeresults are summarized in , and sample score dis-tributions are shown in .For phrase groundingand referring expression comprehension, we used the testsplit of Flickr30K Entities and RefCOCO+ testA &testB datasets that are considered from in-domain dis-tribution for the models. In order to investigate the out-of-distribution generalizability of these models, we ran thesame experiment on the SpatialSense dataset test split,which has both visual domain shift due to the inclusion ofNYU dataset images with a total of 3,679 unique ob-jects, including small/long-tail concepts, and the languagedomain shift by the annotation of 17,498 triplets includingspatial relations. The dataset is designed for spatial relation-ship recognition and is annotated in the triplet {SUBJECT-RELATION-OBJECT} format with the ground-truth OBJECTand SUBJECT bounding boxes. We ran our experiments inthree different settings for TRIPLET, SUBJECT, and OB-JECT grounding. An instance of the SpatialSense datasetis demonstrated in . Experiment details for all set-tings can be found in the Appendix 6. Additional qualitativeresults can be found in Appendix 7, where another case ofScenario 1 uncertainty is shown in , obtained byBLIPbase in the top-left sub-figure of the first example. . Histogram of IoUSoft and IOratio distributions for ID vs. OOD. Note that the histograms are more peaked for in-distributiondatasets, as shown in blue on the left, and for better-performing models, they are shifted to the right. The out-of-distribution experimentsfor all models have less peaked, flatter histograms, where shown in orange on the right. Full visualizations can be found in Appendix 8.",
  ". Discussion": "According to , ALBEFAMC is the winner, consid-ering the combination of PGAccuracy, PGUncertainty, andIOratio metrics.This highlights the importance of fine-tuning ALBEF bounding box-level supervision, comparedto scaling models size and training set size using oftennoisy image-text pairs. Regarding the similarity betweenthe activations and ground-truth masks, CLIPgScoreCAMand ALBEFAMC are the best and second-best perform-ing models according to IoU and Dice, while in termsof WDP, it is reversed in most cases.This suggeststhat CLIP has more spurious GradCAM activations. Fur-thermore, we believe that due to the prevalent noisi-ness in GradCAM activations, the WDPSoft penalizationis much more strict than WDPBinary variation, makingWDPBinary a more practical metric to use. PGUncertaintyis zero in CLIPgScoreCAM, which we believe stems fromthe difference in the vision backbone and nuances in howthe GradCAM is being computed in . ALBEFAMC isthe second-best in PGUncertainty, marginally. In contrast,BLIPlarge has shown the highest relative PGUncertaintyacross all the settings. Our IOratio metric has a strong pos-itive correlation with PGAccuracy while being more strict.This makes it a suitable standalone metric for assessing themodels grounding performance, as it considers both inside& outside activations, in addition to PGAccuracy. Note that PGAccuracy is not positively correlated with PGUncertaintyin all models. This finding is insightful as it demonstratesthat not always the better performing model in terms ofPGAccuracy has the lowest PGUncertainty. Our OOD ex-periments also demonstrate the applicability of our met-rics to the evaluation of triplet-based spatial understandinggrounding , and shifted visual domains.Model size vs.training data impact.Apart fromthe ALBEFAMC superiority, our experiments show thatBLIPlarge, which has 446M parameters and pre-trainedon 129M image-text pairs, under-performs BLIPbase,which has 223M parameters and pre-trained on 14Mimage-text pairs, in terms of the PGUncertainty in all 6dataset splits, and in 2 dataset splits in terms of PGAccuracy.Considering CLIP, which is pre-trained on 400M image-text pairs, as the highest end, and both ALBEF & BLIPbaseas the lowest end of the model & data size spectrum inour experiments, the performance of ALBEFAMC againhighlights the effectiveness of finer-grained fine-tuning. Wesuggest running our IOratio, PGUncertainty, WDPBinary,and either of IoUSoft or DiceSoft metrics, in addition toPGAccuracy, for a thorough grounding evaluation.",
  ". Conclusion": "We first demonstrate two scenarios that the Pointing Game(PG) evaluation metric fails to handle properly, and intro-duce a new set of metrics for evaluation of models ground-ing ability that captures finer-grained differences betweenmodels. According to our experiments, ALBEFAMC hasshown its superiority quantitatively, compared to the otherthree models, and demonstrated better performance qualita-tively in terms of the sharpness of the activations it predictsinside the ground-truth bounding box, as characterized byIOratio. The proposed metrics enable a finer-grained eval-uation of grounding ability for phrase grounding and refer-ring expression comprehension, and how it varies across in-distribution and out-of-distribution datasets. Hassan Akbari, Svebor Karaman, Surabhi Bhargava, BrianChen, Carl Vondrick, and Shih-Fu Chang. Multi-level multi-modal common semantic space for image-phrase grounding.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1247612486, 2019. 2 Assaf Arbelle, Sivan Doveh, Amit Alfassy, Joseph Shtok,Guy Lev, Eli Schwartz, Hilde Kuehne, Hila Barak Levi,Prasanna Sattigeri, Rameswar Panda, et al.Detector-freeweakly supervised grounding by separation. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 18011812, 2021. 2",
  "Peijie Chen, Qi Li, Saad Biaz, Trung Bui, and Anh Nguyen.gscorecam: What objects is clip looking at? In Proceedingsof the Asian Conference on Computer Vision, pages 19591975, 2022. 1, 2, 4": "Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja,Devi Parikh, and Ajay Divakaran. Align2ground: Weaklysupervised phrase grounding guided by image-caption align-ment. In Proceedings of the IEEE/CVF international confer-ence on computer vision, pages 26012610, 2019. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 2 Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang,Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann Le-Cun, Nanyun Peng, et al.Coarse-to-fine vision-languagepre-training with fusion in the backbone. Advances in neuralinformation processing systems, 35:3294232956, 2022. 1 Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang,Jan Kautz, and Derek Hoiem.Contrastive learning forweakly supervised phrase grounding. In European Confer-ence on Computer Vision, pages 752768. Springer, 2020.2 Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, andDerek Hoiem. Towards general purpose vision systems: Anend-to-end task-agnostic vision-language architecture.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1639916409,2022. 1",
  "Lisa Anne Hendricks and Aida Nematzadeh.Probingimage-language transformers for verb understanding. arXivpreprint arXiv:2106.09141, 2021. 1": "Chenguang Huang, Oier Mees, Andy Zeng, and WolframBurgard. Visual language maps for robot navigation. In 2023IEEE International Conference on Robotics and Automation(ICRA), pages 1060810615. IEEE, 2023. 1 Aishwarya Kamath, Mannat Singh, Yann LeCun, GabrielSynnaeve, Ishan Misra, and Nicolas Carion.Mdetr-modulated detection for end-to-end multi-modal understand-ing. In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision, pages 17801790, 2021. 1",
  "tographs of natural scenes. In Proceedings of the 2014 con-ference on empirical methods in natural language processing(EMNLP), pages 787798, 2014. 3": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-tidis, Li-Jia Li, David A Shamma, et al.Visual genome:Connecting language and vision using crowdsourced denseimage annotations. International journal of computer vision,123:3273, 2017. 4, 1 Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, SilvioSavarese, and Steven C.H. Hoi. LAVIS: A one-stop libraryfor language-vision intelligence. In Proceedings of the 61stAnnual Meeting of the Association for Computational Lin-guistics (Volume 3: System Demonstrations), pages 3141,Toronto, Canada, 2023. Association for Computational Lin-guistics. 1 Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation. Advances in neural infor-mation processing systems, 34:96949705, 2021. 2 Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for uni-fied vision-language understanding and generation. In In-ternational Conference on Machine Learning, pages 1288812900. PMLR, 2022. 2 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 1",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, ShaohanHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-ing multimodal large language models to the world. arXivpreprint arXiv:2306.14824, 2023. 1": "Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazeb-nik. Flickr30k entities: Collecting region-to-phrase corre-spondences for richer image-to-sentence models.In Pro-ceedings of the IEEE International Conference on ComputerVision (ICCV), 2015. 3, 1 Mengxue Qu, Yu Wu, Wu Liu, Qiqi Gong, Xiaodan Liang,Olga Russakovsky, Yao Zhao, and Yunchao Wei. Siri: Asimple selective retraining mechanism for transformer-basedvisual grounding. In European Conference on Computer Vi-sion, pages 546562. Springer, 2022. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International Conference on Machine Learning,pages 87488763. PMLR, 2021. 2",
  "Navid Rajabi and Jana Kosecka. Towards grounded visualspatial reasoning in multi-modal vision language models.arXiv preprint arXiv:2308.09778, 2023. 4": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.Grad-cam:Visual explanations from deep networks viagradient-based localization. In Proceedings of the IEEE in-ternational conference on computer vision, pages 618626,2017. 1 Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and RobFergus.Indoor segmentation and support inference fromrgbd images. In Computer VisionECCV 2012: 12th Eu-ropean Conference on Computer Vision, Florence, Italy, Oc-tober 7-13, 2012, Proceedings, Part V 12, pages 746760.Springer, 2012. 3, 1 Sanjay Subramanian, Will Merrill, Trevor Darrell, MattGardner, Sameer Singh, and Anna Rohrbach.Reclip: Astrong zero-shot baseline for referring expression compre-hension. arXiv preprint arXiv:2204.05991, 2022. 1 Tristan Thrush, Ryan Jiang, Max Bartolo, AmanpreetSingh, Adina Williams, Douwe Kiela, and Candace Ross.Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 52385248, 2022. 1 Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang,and Dong Yu. Improving weakly supervised visual ground-ing by contrastive knowledge distillation. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1409014100, 2021. 2",
  "Kaiyu Yang, Olga Russakovsky, and Jia Deng. Spatialsense:An adversarially crowdsourced benchmark for spatial rela-tion recognition. In International Conference on ComputerVision (ICCV), 2019. 3, 1": "Ziyan Yang, Kushal Kafle, Franck Dernoncourt, and Vi-cente Ordonez. Improving visual grounding by encouragingconsistent gradient-based explanations.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1916519174, 2023. 1, 2 Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-maier. From image descriptions to visual denotations: Newsimilarity metrics for semantic inference over event descrip-tions.Transactions of the Association for ComputationalLinguistics, 2:6778, 2014. 1 Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,Dan Jurafsky, and James Zou.When and why vision-language models behave like bag-of-words models, and whatto do about it? arXiv preprint arXiv:2210.01936, 2022. 1 Jianming Zhang, Sarah Adel Bargal, Zhe Lin, JonathanBrandt, Xiaohui Shen, and Stan Sclaroff. Top-down neu-ral attention by excitation backprop. International Journalof Computer Vision, 126(10):10841102, 2018. 1",
  ". Experiments Details": "For the phrase grounding experiments, we used the testsplit of Flickr30K Entities dataset, specifically, themerged version which has also been used by MDETR and SiRi .In terms of the pre-trained checkpoints,we used blip-image-text-matching checkpoint forBLIP (both variations of base and large), throughthe LAVIS codebase.For ALBEFAMC, we usedthe best-flickr checkpoint released by AMC where sample qualitative results are shown in & 2. For all CLIPgScoreCAM experiments, we used theRESNET5016 variation of CLIP with the average pool-ing of top 300-channel activation maps, as the best-performing setting reported by .For the referring expressions comprehension experi-ments, we used the same checkpoints as phrase ground-ing ones for both BLIP experiments,but used thebest-refcoco checkpoint released by , where sam-ple qualitative results are shown in & 6. Also,an additional example of PG Uncertainty (Scenario 1) isdemonstrated in the first example of , in the top-left GradCAM activations of the first obtained by runningBLIPbase. Again, there are two top activations with a valueof 1.0 each, while one of them falls inside and the otherone outside of the ground-truth bounding box.In thesecases, PG becomes indecisive in picking the maximum tocompute the accuracy, as this part of the evaluation becomesstochastic and directly affects the evaluation reliability.For the Out-of-Ditribution (OOD) experiments, we ranthe models on the SpatialSense dataset.We con-sider the Spatial Sense dataset OOD because it has a do-main shift in both visual side (due to including new im-ages from Flickr and NYU , instead of widely-used pre-training/fine-tuning datasets like MSCOCO and Visual Genome ), and language side, by includ-ing spatial clauses and small/long-tail objects for increas-ing the detection/grounding difficulty. To be more specific,we ran this experiment on the instances with True labelsin the test set, with a total number of 1811 instances inthree settings. The first setting grounds the entire triplet{SUBJECT-RELATION-OBJECT} and we consider the SUB-JECT ground-truth bounding box as the correct boundingbox supervision for the entire triplet, since according totheir convention, the SUBJECT acts as the target and OB-JECT as reference.Also, we have conducted two sepa-rate experiments for individual object grounding for SUB-JECTS and OBJECTS using their corresponding ground- truth bounding boxes each. In addition to confirming thehigh degree of difficulty we have seen during the SpatialSense experiments, in which we have also shown a samplequalitative example in , weve noticed that in someof the instances, grounding the SUBJECTS is more compli-cated due to the underlying ambiguities/distractions whengrounding the SUBJECT as a standalone phrase. This hap-pens because of the dependency on the RELATION and OB-JECT as the context required for disambiguation, which isshown in as an example. Therefore, the TRIPLETand OBJECT grounding settings results seem more reliablein general, but since we are comparing different models onexactly the same footing, our SUBJECT grounding resultsshould also be considered fair among all the models."
}