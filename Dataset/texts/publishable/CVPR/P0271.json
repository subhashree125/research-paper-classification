{
  "Abstract": "Wepresentpersonalizedresidualsandlocalizedattention-guided sampling for efficient concept-drivengeneration using text-to-image diffusion models.Ourmethod first represents concepts by freezing the weights ofa pretrained text-conditioned diffusion model and learninglow-rank residuals for a small subset of the models layers.The residual-based approach then directly enables appli-cation of our proposed sampling technique, which appliesthe learned residuals only in areas where the conceptis localized via cross-attention and applies the originaldiffusion weights in all other regions. Localized samplingtherefore combines the learned identity of the concept withthe existing generative prior of the underlying diffusionmodel.We show that personalized residuals effectivelycapture the identity of a concept in 3 minutes on a singleGPU without the use of regularization images and withfewer parameters than previous models, and localizedsampling allows using the original model as strong priorfor large parts of the image.",
  ". Introduction": "Large-scale text-to-image diffusion models have demon-strated the ability to generate high-quality images that fol-low the constraints of the input text . How-ever, these models do not inherently encode any informa-tion about the identity of a specific concept, thus limitingthe control over specifying a particular instance to appearin the generated image. To address this, recent approachespropose techniques to personalize these models such thatthey can generate specific concepts in novel environmentsand styles.Given a set of images depicting the desired concept,",
  "*Work performed during an internship at Adobe Research": "personalization approaches differ in which parameters theytrain and whether they are specific to a single concept (i.e.,they need to be separately trained for each new concept) orcan generalize to new concepts without retraining. To en-able personalization of arbitrary concepts, one can finetunethe models parameters or its inputs directly suchthat it can reconstruct the training data. These approachescan be applied to any kind of concepts, but the finetun-ing needs to be done on a per-concept basis and differentparameters need to be stored for each. Other approachestrain an encoder specific to a particular domain (e.g., faces)and finetune the diffusion model once to use the encodersembeddings to reconstruct specific concepts within that do-main . The advantage of the latter approach isthat it does not require retraining for every concept and caninstead be used to instantly generate new concepts from thegiven domain. However, this approach is limited to a singledomain and requires a large dataset to train the encoder. Our approach follows the former setting, i.e., it finetunesthe models parameters for each concept so that there areno constraints on the domain (see for examplesusing our proposed method). The main challenges of open-domain approaches is the need for regularization to mitigateforgetting of concepts learned in the models original train-ing, and the computational overhead in finetuning a new setof parameters for each concept. The most common regular-ization approach is to use images from the same domain asthe target concept with the reference images during the fine-tuning of parameters. The choice of regularization imagesaffects the quality of the final outputs and, as such, is usuallymodel-, training-, and sometimes even concept-dependent.Finally, to address the large overhead of finetuning a wholenew model for each concept, many approaches only fine-tune a subset of parameters (e.g., attention layers weights) or the input to the text-to-image model (e.g., the textembedding representing a specific concept ).",
  "Concept": ". (Top) Given a set of reference images, we learn personalized residuals for a subset of a pretrained diffusion models weights forefficient concept-driven text-to-image generation. (Bottom) The residuals can be combined with our proposed localized attention-guided(LAG) sampling, which leverages the cross-attention maps from the diffusion models to localize the application of the residuals and usesthe original, unchanged, diffusion model for generating everything else. parameters and does not rely on regularization images.While most approaches focus on finetuning the key andvalue weights of the cross-attention layers, we instead pre-dict a low-rank residual to the weights of the outputprojection conv layer after each cross-attention layer. Thisallows us to finetune even fewer parameters (about 0.1%of the base model) than previous approaches. Furthermore,we find that this approach does not require any regulariza-tion images which makes our approach both simpler, sincewe do not need to find appropriate strategies to obtain reg-ularization images, and faster, since we do not need addi-tional training iterations for learning from the regularizationimages. We also show that the choice of macro class forpersonalizing a given image affects the performance, e.g.,using car instead of Lamborghini as the macro class in affects the quality of the outcome (see supplemen-tary). Based on this, removing the need for regularizationimages removes an additional dependency and decreasesthe need for manual selections. Additionally, many personalization approaches struggleto render specific backgrounds or add new objects often dueto some degree of overfitting to the target concept.Forthese scenarios, we propose a novel localized attention-guided (LAG) sampling scheme, which allows us to usethe finetuned residuals with the original model to generate the target concept and the rest of the image, respectively.To achieve this, we use the attention maps from the cross-attention layers of the diffusion model at each timestep topredict the location of the concept in the generated imageand then apply the features, produced using the personal-ized residuals, only in the predicted region such that the restof the image (e.g., background and other objects) is gener-ated by the original model. Thus, we ensure that we donot lose the capability of generating specific backgrounds orunrelated objects due to overfitting. Furthermore, this sam-pling approach does not require any additional training ordata, and does not increase sampling time as no additionalmodel evaluations are needed. We evaluate our approach and sampling technique onthe CustomConcept101 dataset , which was specificallydesigned to evaluate personalization approaches. We useCLIP and DINO scores to evaluate the text-image align-ment (i.e., how well the personalized model can generatethe concept in novel scenes and environments) and identitypreservation of the personalized model (i.e., how well it cangenerate the desired concept). We also perform a user studyto evaluate human preference for text-image alignment andidentity preservation. Our results show that our model per-forms on par or better compared to current state-of-the-artbaselines while using significantly fewer parameters, not re- lying on regularization images, and being faster to train.To summarize, our key contributions are a novel andmore efficient low-rank personalization approach for text-to-image diffusion models that works for arbitrary domainsand concepts, uses fewer parameters than previous ap-proaches, does not rely on regularization images and is,therefore, faster and simpler to train. We also introduce anovel localized attention-guided (LAG) sampling approachthat allows us to flexibly combine the original pretrainedand the finetuned model on the fly to generate differentparts of the image, without increasing the sampling timeand without requiring additional training or user inputs.Our user study and quantitative evaluations show that ourmethod performs comparably or better than other baselines,and our proposed sampling approach can address challengeswith certain types of recontextualization scenarios, such asbackground changes.",
  ". Personalization of text-to-image models": "The task of text-to-image personalization was proposed by, where a few example images of the given concept areused to finetune a personalized token embedding whileall other parameters of the model frozen. Instead of tryingto find an embedding within the existing text conditioningspace to represent a concept, DreamBooth finetunesthe diffusion models parameters to directly inject the con-cept into the learned prior, leading to better performance.Custom Diffusion only finetunes the cross-attentionweights in addition to the token embedding to achieve moreefficient personalization compared to DreamBooth. Basedon these works, other aim to improve the performance andefficiency of personalizing text-to-image models throughapproaches such as, but not limited to, learning multiplepersonalized tokens , imposing constraints on thetrainable parameters (e.g., key-locking , orthogonality, low-rank , singular values only ), training hy-pernetworks and domain-specific encoders ,and injecting of visual features .",
  ". Attention-guided text-to-image synthesis": "Attention layers have been shown to play an impor-tant role in the success of text-conditioned image synthesisusing diffusion models. Recent works propose to manipu-late attention maps from these layers for guided synthesisand editing. modifies cross-attention values to guide thegeneration process so that the subjects specified in an inputprompt appear and the attributes are associated to its cor-responding subject. enable conditioning on a user-provided layout by guiding the localization of objects viacross-attention manipulation. Given an existing image and aprompt that describes the image, synthesize/edit im- ages by manipulating the cross-attention map correspond-ing to the editing target. Similarly, performs edits on ex-isting images albeit through instructions and modificationswithin self-attention layers.",
  ". Approach": "Our method consists of two components:1) Personal-ized residuals, which encode the identity of a given con-cept through a set of learned offsets applied to a subset ofweights within a pretrained text-to-image diffusion model,and 2) Localized attention-guided (LAG) sampling, whichleverages attention maps to localize where the residuals areapplied, essentially allowing a single image to be efficientlygenerated by leveraging both the base diffusion model andthe personalized residuals.",
  ". Preliminaries": "Diffusion models. Diffusion models consist of a fixedforward noising process that gradually adds noise to animage, and a learned denoising process that iteratively re-moves noise to produce a valid image. The denoising pro-cess is learned through a U-Net , parameterized by ,and is conditioned on an image xt noised to timestep t, andt itself. Text guidance can be incorporated through condi-tioning on embeddings c = (y) of input prompts y from atext encoder , such as CLIP .In this work, we leverage Stable Diffusion, a text-conditioned latent diffusion model (LDM) . An LDMis a variant of a diffusion model that operates in the latentspace of a variational autoencoder . The encoder E em-beds an input image x into a latent representation z = E(x)and a decoder D maps z back into pixel space x = D(z).The diffusion portion of LDM operates on z and is trainedusing the following objective:",
  "LLDM = EzE(x),y,N (0,1),tzt, t, (y)22. (1)": "Low rank adaptation (LoRA). Low rank adaptation(LoRA) is an efficient method originally proposed forupdating large language models through learned residualsinstead of directly finetuning their parameters. For a givenlayer of the pretrained model with weight matrix W0 Rmn, LoRA learns two matrices A and B whose productforms a residual W = AB Rmn, where A Rmr,B Rrn, and r min(m, n) is the rank. The updatedweight matrix is then defined as W = W0 + W. Withsmall values of r, LoRA has been shown to significantly re-duce the number of learnable parameters while retaining oreven improving performance.",
  "The goal of personalizing text-to-image models is to faith-fully capture the identity of a target concept while simulta-": "neously avoiding overfitting so that the concept can be re-contextualized into new settings and configurations. Sinceconcepts are often learned using only a few reference im-ages, directly finetuning the weights of a very large genera-tive model can easily lead to overfitting and/or overwritingunnecessary parts of the learned language prior. Instead wepropose to use a LoRA-based approach to learn low-rankoffsets for a small subset of the diffusion model weightswhich will represent the target concept. Thus, we are ableto recover the full generative capacity of the original modelby simply not applying the learned residuals at inference.The diffusion model contains multiple transformerblocks, which consist of self- and cross-attention layers with a 11 conv projection layer on either end (see Fig-ure 2). While several approaches primarily target the cross-attention layers due to their learning of relationships be-tween text and images, we choose to learn offsets for theoutput projection conv layers because these localized oper-ations can capture finer details than the global operations ofcross-attention.We illustrate the process of learning personalized resid-uals in .Given a pretrained text-to-image dif-fusion model containing L transformer blocks, we learnWi = AiBi Rmimi for the output projection layerlproj out,i with weight matrix Wi Rmimi1 within eachtransformer block i, where Ai Rmiri and Bi Rrimi.We reshape the residual such that Wi Rmimi1 andadd to the original weights Wi to produce W i = Wi+Wi.The Wis are updated using the original diffusion objec-tive in Equation (1).Similar to other works, we associate the concept with aunique identifier token (e.g., V*), which is initialized us-ing a rarely occurring token embedding. During training,we use the unique token and macro class of the concept in afixed template for the prompt associated with each referenceimage (e.g., a photo of a V* macro class). Person-alization approaches that involve direct updates to the dif-fusion models weights are susceptible to overwriting partsof the existing generative prior with the new concept andthus explicitly require prior preservation through regular-ization images during training . Since our methoddoes not directly update the diffusion model, we avoid thisissue entirely and eliminate the burden on the user to de-termine an effective set of regularization images, which isnot always straightforward. Additionally, the low-rank con-straint on the residuals reduces the number of trainable pa-rameters, making our method a simpler and more efficientapproach for personalization.",
  "With our residual-based personalization approach, we haveadditional flexibility in how the offsets are applied at infer-ence. We introduce a new localized attention-guided (LAG)": "sampling method to better combine a newly learned conceptwith the original generative prior of the diffusion model.As shown in , within every transformer block ofthe diffusion model is a cross-attention layer, which aims tolearn the correspondence between text tokens and image re-gions. Each cross-attention layer computes attention mapsAyi for each token yi in the prompt, indicating where thetoken will affect the generated image. The attention mapsare produced using the following equation:",
  ",(2)": "where Q = W Qx is the query, K = W Ky is the key,and dk is the dimension of the query and key.Given the indices C of the unique identifier and macroclass tokens specifying the concept (e.g., V* and dog),we sum the values of the corresponding attention mapsAi,C =",
  "fi = (1 Mi) fi + Mi f i,(3)": "where fi = Wix is the feature produced using the origi-nal conv weight Wi, and f i = W ix is the feature producedusing the updated weight from the personalized residualW i = Wi + Wi. Thus, the identity represented throughthe personalized residuals is only being applied in the re-gions corresponding to the target concept, and the remain-ing regions are generated by the original diffusion model.The proposed LAG sampling technique is visualized in Fig-ure 4.While there exist personalization works using attentionguidance (e.g., ), they often rely on object masksand/or additional losses at train time to focus on the relevantobject location in the reference images, whereas manually-provided object masks or specific training are not neededto enable LAG. Additionally, LAG sampling explicitlymerges the features of two layers (personalized/finetunedand original/non-finetuned) on-the-fly based on the cross-attention maps obtained during inference and has negligibleimpact on the sampling speed. In contrast, other synthe-sis/editing works (see .2) use cross-attention val-ues to up- or down-weight the influence of specific tokensat specific image locations.LAG sampling can be beneficial in scenarios where thelearned residuals overfit to the reference images and havenot effectively disentangled the target concept from thebackground, which can occur as a consequence of ambi-guities of the target concept given the reference images ormodel biases (e.g., furniture often photographed indoors).By leveraging the attention maps from the tokens denoting Self-",
  "Guided Sampling": ". Overview of our proposed work. (1) Personalized residuals: We learn low-rank residuals for the output projection layer withineach transformer block in the diffusion model. The residuals contain relatively few parameters, are fast to train, and do not require anyregularization images during training. (2) Localized attention-guided sampling: We optionally apply the personalized residuals only inthe areas that the cross-attention layers have localized the concept via predicted attention maps. Thus, we can combine the newly learnedconcept with the original generative prior of the base diffusion model within a single image.",
  ". Training details": "We build upon Stable Diffusion v1.4 . For each trans-former block i, we compute the rank ri for its outputprojection convolution layer with weight matrix WiRmimi1 as ri = 0.05mi, totalling 1.2M trainable pa-rameters (0.1% of Stable Diffusion). Each of the low-rankmatrices are randomly initialized. We train our method for150 iterations with a batch size of 4 and learning rate of1.0e-3 on 1 A100 GPU (3 minutes) across all experiments.",
  ". Baselines": "We focus on comparisons to open-domain (i.e., does not re-quire encoders limited to a single given domain) approacheswith publicly available code.Specifically, we compareour method against four baselines: Textual Inversion ,DreamBooth , Custom Diffusion , and ViCo .Textual Inversion freezes the entire diffusion model and op-timizes only the unique identifier token V* for each concept.ViCo optimizes V* as well as newly added cross-attentionlayers to the diffusion model to incorporate visual informa-tion from the reference images while keeping the rest of the model frozen. DreamBooth finetunes the entire diffusionmodel using the reference images and a set of regulariza-tion images, which are generated within the same domain asthe target concept using the original model. While Dream-Booth was originally proposed using Imagen , we usean open-source version built on Stable Diffusion1. CustomDiffusion finetunes only the key and value weights of thecross-attention layers in addition to the identifier token em-bedding, and uses a set of real regularization images sam-pled from LAION-400M .We use the recommended settings described by each pa-per. For Textual Inversion and ViCo, which initialize theidentifier token embedding to a single word that best rep-resents the concept, we use our best discretion to pick aword most similar to the macro class given by CustomCon-cept101.",
  ". Evaluation metrics": "Following the protocol described in , we leverage theCustomConcept101 dataset, consisting of 101 conceptsacross 16 broader categories. For every concept we gen-erate 50 samples for each of the 20 prompts given by thedataset. We use DDIM sampling with N = 50 steps, = 0.0, and a guidance scale of 6.0 for all methods. Weset the same random seed for sampling across each methodso that the choice of starting noise does not impact theresults. Results of our method with LAG sampling are ex-plicitly labeled as such.We evaluate each method for text alignment and image",
  "Image61.9662.1151.3363.2726.264.76%5.80%4.65%5.59%4.91%": "alignment. Text alignment is measured as the similarity be-tween the CLIP text feature of the input prompt and theCLIP image feature of the resulting generated image. Imagealignment is measured as the similarity between image fea-tures from either CLIP or DINO of the reference imagesand corresponding generated images.Additionally, we evaluate both text and image alignmentusing human evaluations through user studies on AmazonMechanical Turk (AMT). For each text alignment case, wedisplay a text prompt and a pair of corresponding gener-ated images, and ask users Which image is more consistentwith the given text prompt?. For each image alignmentcase, we display 3 reference images for a concept and a pairof corresponding generated images, and ask Which imagebetter preserves the identity of the subject in the providedreference images?. For both studies, each pair of imagescontains one from {Textual Inversion, ViCo, DreamBooth,Custom Diffusion, Ours w/ LAG sampling} and one fromours with normal DDIM sampling. Users can select eitherimage or neither (Not sure).",
  ". Results": "We visualize samples generated by each method for vari-ous types of prompts in . Textual Inversion failsto reliably capture the concepts identity and/or the promptwhereas all other methods, including ours, are able to betterpreserve the concepts identity while also adhering to theprompt. We highlight that our method is able to achievethese results while having significantly fewer learnable pa-rameters and requiring less training time compared to ViCo,DreamBooth, and Custom Diffusion, as well as not leverag- ing regularization images.We compare examples using our proposed personalizedresiduals with and without localized attention-guided sam-pling in . We illustrate how LAG sampling affectsthe output image by using the same starting noise map zT tosample each pair of {w/o LAG, w/ LAG} images. We high-light scenarios where LAG sampling performs better thannormal sampling in a and vice versa in b.Quantitative evaluations for text and image alignment us-ing CLIP and DINO are shown in . We include re-sults using the original Stable Diffusion model, which hasno notion of any of the concepts, for reference. We showthat our method performs similarly with and without LAGsampling averaged across the whole dataset, demonstratinghigher image alignment and slightly lower text alignmentthan the more computationally-heavy baselines.However, as seen by the results of 1250 responses col-lected through AMT user studies for both text and imagealignment in , we show that the CLIP text alignmentscores do not necessarily correlate to human preference. Weobserve that our method performs similarly to Custom Dif-fusion for text alignment, which was assigned the highestCLIP text score, and outperforms all baselines for imagealignment. Again, we note that our method achieves similarperformance to the better performing baselines while beingsignificantly more computationally efficient. We also com-pare our method with and without LAG sampling in the userstudies and show that LAG is preferred for image alignmentbut not text alignment. Further analysis comparing the twosampling approaches can be found in the supplementary.We also train and evaluate our method using CLIP simi-larity to select the most representative macro class amongthe 117k nouns in WordNet for each concept. In Ta-ble 4, we show that using the WordNet macro class leads tofurther improvements in image alignment while decreasingtext alignment, the latter of which may not necessarily re-flect human preference as previously demonstrated. See thesupplementary for additional discussions.Ablation studies.We perform ablation studies onchanging the targets for where the residuals are applied, re-moving the macro class from the prompt, including regu-larization images (sampled from LAION) during training,updating the concept identifier token embedding V*, andvarying the rank of the residuals. Results are shown in Ta-ble 3 (see for results on changing the rank).We show that changing where the residuals are applied toeither the key and value weights of the cross-attention layers(like Custom Diffusion) or the input projection conv layer(rather than the output) slightly decreases the scores acrossall three metrics compared to our proposed approach. Wehypothesize that the output projection layer achieves no-ticeably higher identity preservation because it refines thefeature map at the end of each block. Additionally, learning ConceptOursTextual InversionDreamBoothCustom Diffusion V* penguin plushie in Grand Canyon V* backpack on a caf table with a steaming cup of coffee nearby A pink V* chair Georgia OKeeffe style V* dog painting An origami art of V* houseplant V* barn in snowy ice Gold colored V* shoes Marigold flowers in the V* vase ViCo",
  "residuals for multiple layers simultaneously leads to overfit-ting to the reference images as demonstrated by the higherimage alignment scores and lower text alignment": "Omitting the macro class leads to significant dropsacross all metrics, demonstrating that the additional infor-mation is useful to our method for knowing what withinthe reference images is important to model. Similar to theeffect of using regularization images for DreamBooth andCustom Diffusion, regularization images slightly improvestext alignment but decreases image alignment. On the otherhand, updating the token embedding for V* leads to over-fitting as shown by the increase in image alignment and de-crease in text alignment.",
  ". Conclusion": "We introduce personalized residuals, a method for concept-driven synthesis using text-to-image diffusion models. Pre-vious approaches to personalization are often slow to train,have high computational demands, require regularizationimages, and/or have difficulty recontextualizing the targetconcept. Through our proposed LoRA-based approach thatlearns a small set of residuals to represent the identity of aconcept, we reduce the number of learnable parameters andtraining time and remove the reliance on domain regulariza-tion while maintaining flexibility with editing. We also in-troduce localized attention-guided sampling which appliesthe personalized residuals only in regions where the conceptis localized via the cross-attention mechanism. We evaluateour method across several metrics to show that we are ableto efficiently enable personalization. Limitations and future work. We show that localizedsampling is not always the best choice (e.g., changing thecolor of a concept) and relies on the cross-attention layersto produce high-quality attention maps, which is not alwaysthe case. Our approach can be sensitive to the choice ofmacro class and inherits the pretrained models biases andlimitations, such as mixing up the relationship between at-tributes in the prompt. Finally, we leave multi-concept gen-eration through LAG sampling as future work. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-imagediffusion models with an ensemble of expert denoisers. arXivpreprint arXiv:2211.01324, 2022. 3 Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-structpix2pix: Learning to follow image editing instructions.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1839218402, 2023.3 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 96509660, 2021. 6 Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, andDaniel Cohen-Or.Attend-and-excite: Attention-based se-mantic guidance for text-to-image diffusion models. ACMTransactions on Graphics (TOG), 42(4):110, 2023. 3",
  "Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, andAleksander Holynski. Diffusion self-guidance for control-lable image generation. arXiv preprint arXiv:2306.00986,2023. 3": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.An image is worth one word: Personalizing text-to-image generation using textual inversion.arXiv preprintarXiv:2208.01618, 2022. 1, 3, 5 Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,Gal Chechik, and Daniel Cohen-Or. Encoder-based domaintuning for fast personalization of text-to-image models. ACMTransactions on Graphics (TOG), 42(4):113, 2023. 1, 3",
  "George A Miller. Wordnet: a lexical database for english.Communications of the ACM, 38(11):3941, 1995. 6": "Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, YaoFeng, Zhen Liu, Dan Zhang, Adrian Weller, and BernhardScholkopf. Controlling text-to-image diffusion by orthogo-nal finetuning. arXiv preprint arXiv:2306.07280, 2023. 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 3, 6",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,and Mark Chen. Hierarchical text-conditional image gener-ation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 1": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1, 3, 5 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III18, pages 234241. Springer, 2015. 3 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2250022510, 2023. 1, 3, 4, 5 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,and Kfir Aberman. Hyperdreambooth: Hypernetworks forfast personalization of text-to-image models. arXiv preprintarXiv:2307.06949, 2023. 1, 3 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. 1, 5 Christoph Schuhmann, Richard Vencu, Romain Beaumont,Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, TheoCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:Open dataset of clip-filtered 400 million image-text pairs.arXiv preprint arXiv:2111.02114, 2021. 5 James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, TingHua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual dif-fusion: Continual customization of text-to-image diffusionwith c-lora. arXiv preprint arXiv:2304.06027, 2023. 3",
  "Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.Key-locked rank one editing for text-to-image personaliza-tion.In ACM SIGGRAPH 2023 Conference Proceedings,pages 111, 2023. 3, 1": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 3, 4 Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, LeiZhang, and Wangmeng Zuo. Elite: Encoding visual con-cepts into textual embeddings for customized text-to-imagegeneration. arXiv preprint arXiv:2302.13848, 2023. 3 Guangxuan Xiao, Tianwei Yin, William T Freeman, FredoDurand, and Song Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention.arXivpreprint arXiv:2305.10431, 2023. 1, 3, 4",
  ". Additional experimental results": "We explore the difference in normal and LAG sampling byusing ChatGPT to categorize each prompt into {add ob-ject(s), artistic style, change attribute, change background,identity, object in style of V*}. We note that a prompt mayfall into multiple categories, but we only use one as deter-mined by ChatGPT. We split the AMT evaluations for textalignment by category in . We observe that LAGsampling performs best for identity, change background,and add object(s), which are tasks in which the target objectis somewhat independent of the rest of the image. Tasksthat require modifying the target (artistic style, change at-tribute, object in style of V*) perform better with normalDDIM sampling.In Figures 6 to 11 we directly compare examples fromeach of the six prompt categories using the two samplingmethods by generating corresponding pairs using the samestarting noise maps. Additional qualitative samples can befound in Figures 12 and 13.We plot CLIP/DINO image alignment scores againstCLIP text scores, averaged across concepts within the the16 categories of CustomConcept101, for each method from.Additionally, we compare our method to an unofficialimplementation2 of Perfusion (an official version is notpublicly available). We followed the experimental setup andhyperparameter values described by the original authors,but note that we were unable to reproduce the quality of theresults shown in the paper: CLIP text 0.6879, CLIP image0.5669, DINO image 0.2228.",
  "WordNet NNCustomConcept1010.66260.77980.5904WordNet NN0.68690.77980.5904": "calculate the cosine similarity against the CLIP text em-bedding for each of the 117k nouns within WordNet. Wetrain our method and/or sample using the WordNet nounwith the highest similarity and compare with using the pro-vided macro class from CustomConcept101 during train-ing and/or sampling in . We observe that using theWordNet nearest neighbor as the macro class leads to higherimage alignment and lower text alignment compared to theCustomConcept101-provided macro class.Selecting the best macro class for concepts can bechallenging and given that it can lead to noticeable changesin alignment metrics, an automatic heuristic for choosing asuitable macro class would be helpful to users. We leave thedesigning of such a heuristic as future work.",
  "Ours (0.05mi)0.71930.75940.5671": "We evaluate different values for the rank of the learnedresiduals in and observe that text alignment is in-versely proportional to the rank and image alignment isdirectly proportional.Since the dimensions of the convweight matrix varies across the transformer blocks withinthe U-Net, we believe that calculating the rank with respectto the dimensions is the better approach over setting a fixedvalue across all layers, which is empirically validated by theresults with our proposed formula achieving a better balanceof image and text alignment."
}