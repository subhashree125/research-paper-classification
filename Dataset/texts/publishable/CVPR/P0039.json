{
  "Haolin Li, Yuhang Zhou, Ziheng Zhao, Siyuan Du, Jiangchao Yao, Weidi Xie, Ya Zhang, and Yanfeng Wang": "AbstractThe widespread adoption of large-scale pre-training techniques has significantly advanced the development of medicalfoundation models, enabling them to serve as versatile tools across a broad range of medical tasks. However, despite their stronggeneralization capabilities, medical foundation models pre-trained on large-scale datasets tend to suffer from domain gaps betweenheterogeneous data, leading to suboptimal performance on specific tasks compared to specialist models, as evidenced by previousstudies. In this paper, we explore a new perspective called Knowledge Decomposition to improve the performance on specific medicaltasks, which deconstructs the foundation model into multiple lightweight expert models, each dedicated to a particular anatomicalregion, with the aim of enhancing specialization and simultaneously reducing resource consumption. To accomplish the aboveobjective, we propose a novel framework named Low-Rank Knowledge Decomposition (LoRKD), which explicitly separates gradientsfrom different tasks by incorporating low-rank expert modules and efficient knowledge separation convolution. The low-rank expertmodules resolve gradient conflicts between heterogeneous data from different anatomical regions, providing strong specialization atlower costs. The efficient knowledge separation convolution significantly improves algorithm efficiency by achieving knowledgeseparation within a single forward propagation. Extensive experimental results on segmentation and classification tasks demonstratethat our decomposed models not only achieve state-of-the-art performance but also exhibit superior transferability on downstreamtasks, even surpassing the original foundation models in task-specific evaluations. Moreover, these compact expert models significantlyreduce resource consumption, making them more suitable and efficient for practical deployment. The code is available at here.",
  "INTRODUCTIONM": "EDICAL image analysis powered by deep learningplays a fundamental role in numerous clinical appli-cations, including computer-aided diagnosis, disease pro-gression monitoring, and treatment planning , , , ,. Traditional deep learning models are typically tailoredfor specific tasks, such as brain tumor segmentation , ,. These models excel only in identifying specific regionsof interest (ROI) and exhibit weak adaptability to new tasks,thus can be referred to as specialist models. Recently, theresearch paradigm of medical image analysis has shiftedtowards universal pretraining , , , , , result-ing in the development of foundation models, which arepre-trained on large-scale datasets encompassing variousanatomical structures and imaging modalities. These foun-dation models possess robust transfer and generalizationcapabilities, allowing them to handle a variety of tasksacross different anatomies and modalities.While foundation models exhibit impressive general fea-ture extraction capabilities, two critical challenges remain in",
  "This work has been submitted to the IEEE for possible publication.Copyright may be transferred without notice, after which this version mayno longer be accessible": "Haolin Li and Siyuan Du are with the School of Computer Science,Fudan University, Shanghai 200437, China, and also with Shang-hai AI Laboratory, Shanghai 200032, China. (E-mail: {23110240025,23110240011}@m.fudan.edu.cn) Yuhang Zhou, Ziheng Zhao, Jiangchao Yao, Weidi Xie, Ya Zhangand Yanfeng Wang are with Shanghai Jiao Tong University, Shanghai200240, China, and also with Shanghai AI Laboratory, Shanghai 200032,China. (E-mail: {zhouyuhang, Zhao Ziheng, sunarker, weidi, ya zhang,wangyanfeng}@sjtu.edu.cn).",
  "Jiangchao Yao and Yanfeng Wang are the corresponding authors": "the medical field. 1) The significant anatomical differencesacross various regions of human body, such as the abdomenand brain, incur substantial domain gaps between imagesfrom different anatomical structures. The cost of pretrainingon such heterogeneous data usually comes with sacrificingthe performance of individual regions. Specifically, recentstudies in medical field , , have shown that theperformance of foundation models remains inferior to thatof specialist methods, implying that current medical foun-dation models may not be able to well guarantee both gener-ality and specialization simultaneously. 2) Foundation mod-els, characterized by their extensive parameters and highcomputational demands, are impractical for deploymentin diverse resource-constrained medical environments ,, , . For example, as highlighted in , ,, medical foundation models require high-performancehardware that is often difficult for hospitals to acquire,particularly for hospitals in underdeveloped areas.To address the aforementioned issues, we propose a newperspective called knowledge decomposition, which aimsto offer potential solutions for the practical application ofcumbersome medical foundation models. The objective ofknowledge decomposition is to decompose the foundationmodel into multiple lightweight expert models, where eachexpert model concentrates exclusively on a specific region,following the department taxonomy of a hospital (as shownin ). 1) In contrast to specialist models that aredesigned to handle single specific task of a particular region(such as segmenting lung tumors in thoracic imaging), thedecomposed expert models we decompose are capable ofmanaging all tasks within their respective regions. For in-stance, an expert model dedicated to the thorax can perform",
  "Expert1Expert2Expert3Expert4": ": Knowledge decomposition is employed to breakdown the foundation model into multiple lightweight ex-pert models, each tailored to a specific domain. The goal ofthis paradigm is to improve the specialization of deploy-ment models within a specific domain, while simultane-ously reducing deployment costs. segmentation tasks across various organs and conditionswithin the thoracic region, such as those involving the lungs,heart, and other thoracic structures. 2) Compared to foun-dation models that tackle all tasks across all regions, decom-posed expert models effectively mitigate conflicts arisingfrom heterogeneous data, leading to enhanced specializationand reduced deployment costs. To the best of our knowledge,there has been no research conducted in the medical fieldon how to decompose a foundation model into multipleexpert models. The most related study in the field of naturalimages, KF , has made preliminary explorations into thisproblem. KF factorizes the pre-trained model into a com-mon knowledge network (CKN) and several task-specificnetworks (TSNs) by manipulating the mutual informationbetween models. After decomposition, the CKN can be com-bined with each TSN to form task-specific expert models.However, the indefinite primary-secondary structure designrequires trivial training and cannot effectively decoupleknowledge from different regions solely by means of the lossfunction. Regarding the lightweight aspect, the introductionof TSNs also results in significant resource overhead, mak-ing the approach inefficient for practical applications.In this work, we propose Low-Rank Knowledge Decom-position (LoRKD), a method for decomposing the medicalfoundation model into lightweight, task-specific experts.Our LoRKD consists of two main components: low-rank ex-pert modules and efficient knowledge separation convolu-tion. Concretely, the low-rank expert modules comprise twomain modules: a primary common-shared backbone andsecondary task-specific low-rank expert modules attachedto the backbone. The common-shared backbone, whichhouses the majority of the model parameters, is utilizedto learn generic knowledge shared across all tasks. Theregion-specific low-rank expert modules employ low-rankadapters (LoRA) to assimilate domain-specific knowl-edge, explicitly segregating gradients from different regionsinto their corresponding modules. This architecture effi-ciently controls parameter growth while resolving conflictsin heterogeneous data. However, such a design also comesalong with a critical technical challenge: when a mini-batch contains data from multiple tasks, the forward operationshould be performed multiple times, which greatly increasestraining time. To address this issue, we introduce efficientknowledge separation convolution to achieve knowledgeseparation at the convolutional level. This approach enablesgradients to be separated into their corresponding expertmodules in a single forward propagation, while simultane-ously accumulating them in the shared backbone. Further-more, considering the varying difficulty of tasks in differentregions, we present two variants, LoRKD and LoRKD*. Theformer sets the same rank for each regions low-rank expertmodule. The latter implements an automated, imbalanceddesign for the ranks of different expert modules. Specifically,for regions with more challenging tasks, the rank of theirexpert modules is set higher to enhance the representationability; conversely, for regions with relatively simpler tasks,the rank of their expert modules is set lower to reduce costs.During inference, the low-rank expert modules can beintegrated into the backbone, further reducing inferencelatency and computational overhead. For scenarios neces-sitating targeted analysis of a particular region, only therelevant low-rank module needs to be fused with the com-mon backbone to create an expert model. For instance, inthe thoracic surgery department, only the thorax expertmodule is required. This integrated model, compared tothe original foundation model, boasts fewer parameters andsuperior performance, thereby achieving cost reduction andperformance improvement simultaneously. In a nutshell,our contributions are summarized as follows: Knowledge Decomposition. Given the significantdata heterogeneity in medical area, we introduceknowledge decomposition to broaden the applica-tion of medical foundation models, which decom-poses foundation models into multiple lightweightexperts to reduce costs and enhance specialization. Novel Framework. We introduce a novel methodLoRKD, which comprises two components: low-rank expert modules and the efficient knowledgeseparation convolution. LoRKD injects task-specificknowledge into the corresponding expert modulesvia efficient explicit gradient separation. Superior Performance. Extensive experiments onboth segmentation and classification tasks demon-strate the superiority of our method. LoRKD candecompose the foundation models into lighter yetstronger expert models, leading to superior spe-cialization and transferability to downstream tasks.Comprehensive analysis further verifies the potentialand applicability of knowledge decomposition.",
  "Medical Foundation Models": "Medical foundation models powered by universal pre-training have emerged as a crucial advancement in medicalimage analysis, driving significant progress across varioustasks. Pretrained on large-scale diverse datasets, medicalfoundation models exhibit remarkable performance andgenerality. These models can be broadly categorized intothose designed for segmentation tasks and those for diag-nosis tasks. To advance segmentation foundation models,",
  ": The resource consumption of foundation modelsis growing at an exponential rate. The size of the circlerepresents the models parameters": "researchers have undertaken preliminary explorations. Sev-eral methods have concentrated on fine-tuning SAM onmedical data , , , , 3DSAM-adapter andSAM-Med3D introduce novel methods to adapt SAMfrom 2D natural images to 3D volumetric images, fullyleveraging spatial information. Other works have exploredalternative pre-trained models , . SAT employedknowledge-enhanced representation learning to pre-trainuniversal segmentation model with text prompts, whileUniverSeg exploited a CrossBlock mechanism to learnprecise segmentation maps without additional training.Similarly, foundation models for disease diagnosis ex-hibit strong generality and transferability to downstreamclassification tasks , , . Recent advancementsin classification foundation models have explored variouspre-training methods. Some studies attempt to develop amedical version of ImageNet to facilitate the pre-trainingof medical image classification models , , ,thereby enhancing the transferability of pre-trained modelsto downstream tasks. Other works concentrate on designingself-supervised pre-training methods tailored for medicalimages , , , , . Furthermore, some studiesleverage text information, including medical records andterminology descriptions, to develop advanced multimodalpre-training algorithms , , , , , .Despite these advances, all these foundation modelsface challenges such as gradient conflicts and high com-putational costs, particularly when trained on large-scalemedical image datasets spanning various body regions andanatomies , , .",
  "Knowledge Decomposition": "Different from the previous disentangled representationlearning that is usually done through variational auto-encoder , , or adversarial learning , , ,, the goal of knowledge decomposition is to break downthe pre-trained foundation model into multiple region-specific experts. Recently, in the field of natural images,KF conducted early exploration of knowledge de-composition by promoting modularization of knowledgethrough optimizing mutual information loss , , .It factorizes a pre-trained model into a common knowledge",
  "Low-Rank Adaptation": "Low-Rank adaptation (LoRA) is a parameter-efficient fine-tuning method for large language models . During fine-tuning, LoRA utilizes low-rank matrices to approximate thechanges in pre-trained weights. The low-rank matrices canbe re-parameterized into the pre-trained weights to avoidinference latency. Due to its impressive performance andefficiency, many LoRA variants have been proposed ,, , . QLoRA combined LoRA with 4-bitNormalFloat quantization to further reduce computationalcosts. DoRA decomposed the weight change into mag-nitude and direction components and utilized LoRA to fine-tune the direction component. Galore implemented agradient low-rank projection method to reduce optimizermemory usage, allowing full-parameter training under lim-ited resources. These LoRA variants are designed solely forparameter-efficient fine-tuning, while our LoRKD employslow-rank structures as knowledge carriers for specific tasksto alleviate conflicts between heterogeneous data and simul-taneously maintain minimal growth in model parameters.",
  "METHODOLOGY": "In this section, we first present the problem formulation andmotivation in 3.1 and 3.2 respectively. Then, we introducethe details of our LoRKD in three parts: 3.3 describesthe low-rank expert modules; 3.4 presents the efficientknowledge separation convolution; the training objectiveof LoRKD is shown in 3.5; we provide the decompositionprocedure and analysis algorithm complexity in 3.6.",
  "EKS Conv.Shared Conv.ForwardBackwardFrozenTrainableLow-rank Module": ": The illustration of LoRKD for medical foundation models on segmentation. The low-rank expert modules controlthe number of parameters and efficient knowledge separation convolution (EKS Conv) achieves computationally efficientgradient separation. Decomposed models can replace medical foundation model in specific domains and can switch taskknowledge conveniently between departments. The case for classification tasks holds by turning the decoders as classifiers. method can be naturally compatible with 2D cases by sim-ply degenerating the input dimension. Assuming we havea universal pretraining dataset D = {(x1, y1), ..., (xn, yn)},where n is the number of data, xi RCHW D representsthe input volumetric image, and yi is the prediction target.C, H, W, D is the channel, height, width, and depth ofthe feature maps, respectively. For the segmentation task,yi RKHW D is the binary segmentation masks of theanatomical targets and K stands for the number of segmen-tation targets. For the classification task, yi {0, 1, ...K 1}is the class label of xi and K is the number of classes.Given a foundation model F pre-trained on heteroge-neous datasets covering multiple anatomical regions, ourgoal is to decompose F into several lightweight modelsF1, ..., FT , where each lightweight model is an expert modelcorresponding to a specific anatomical region. Specifically,our decomposed model Fd consists of a common-sharedbackbone Fs and T low-rank expert modules E1, ..., ET ,with each expert module specializing in a particular region,such as the brain or abdomen. An expert model Fi can beobtained by compositing the low-rank expert module Eiwith the shared backbone, namely, Fi = Fs Ei.",
  "Motivation": "The increasing size of foundation models has led to sig-nificant challenges regarding computational resources andefficiency. illustrates the growth trend in the num-ber of parameters and computational requirements (mea-sured in FLOPs) for well-known medical models. As it isshown, while these models excel at general feature extrac-tion, their massive parameter counts demand substantialcomputational power, making them impractical for manyreal-world scenarios. Additionally, despite their general-ity, foundation models often underperform compared tospecialist models on specific medical tasks. As shown in, we evaluated the performance of a state-of-the-art foundation model MedSAM against a state-of-the-artspecialist model nnUNet on 20 distinct datasets. Our results showed that the specialist model outperformed thefoundation model in most cases, achieving superior resultson 16 out of 20 datasets. This further demonstrates the lackof specialization in foundation models for medical tasks.To reduce costs and enhance specialization, we proposeour LoRKD, which tackles these two issues from the per-spective of knowledge decomposition. LoRKD consists oftwo main components: the low-rank expert modules and theefficient knowledge separation convolution. We explicitlyseparate the gradients from different anatomical regions intocorresponding low-rank expert modules. Our intuition isthat the expert modules can then learn task-specific knowl-edge while the shared backbone can acquire general knowl-edge, thus resolving gradient conflicts between heteroge-neous data. To handle the computational challenge posed bymultiple expert modules, we introduce efficient knowledgeseparation convolution, which enables gradient separationto be accomplished in a single forward pass, significantlyreducing computational overhead. Besides, during the infer-ence for specific regions, the composition of expert modulesand shared backbone makes the parameter size in a tolerablescale compared to medical foundation models. The overallframework of our LoRKD is illustrated in .",
  "Low-Rank Expert Modules": "Considering the limited computational resources and thescalability required for numerous tasks, expert modulescarrying region-specific knowledge need to strike a balancebetween the number of parameters and feature representa-tion capability. LoRA , a widely used fine-tuning methodin large language models, has been demonstrated to beparameter-efficient , . Inspired by this, we proposeto use a similar low-rank structure as the carriers for knowl-edge decomposition, named low-rank expert modules.Given a shared convolution W0 RCoutCinkkk",
  "gt = (W0 + BtAt)ht,(1)": "where, for brevity, we omit the reshape operation, andht, gt represent the input features and output featuresrespectively. It is worth noting that, different from previousscenarios where W0 remains frozen in LoRA, in our knowl-edge decomposition scenario, W0, as a carrier of commonknowledge, requires to be updated along with the low-rankfactors At and Bt. 3.3.1Task disparity requires imbalanced rank designThe intrinsic differences between various anatomical re-gions present substantial challenges in medical image anal-ysis using neural networks. These variations arise fromseveral factors, including distinct anatomical features, tissuedensities, potential pathologies, and the specific imagingmodalities employed . For instance, the differences be-tween medical imaging of the brain and the thorax are sig-nificant. Brain imaging is predominantly performed usingMRI, which provides detailed images of soft tissues andis crucial for identifying neurological conditions , .In contrast, imaging of the thorax often employs CT or X-ray modalities, which are better suited for visualizing densestructures and detecting conditions related to the lungs ,. Additionally, the data employed for universal pre-training is highly imbalanced, with most images comingfrom a few regions, as illustrated in . This imbalanceexacerbates the difficulty of tasks associated with underrep-resented regions, as the neural networks training is skewedtowards more frequently imaged areas. Therefore, the dif-ficulty level of tasks across different anatomical regionsis markedly disparate, necessitating tailored approaches toadaptively address this unique challenge.Specifically, for regions with a large loss reduction dur-ing the warmup phase, the corresponding low-rank expertmodules are assigned larger rank values. A large loss re-duction indicates significant optimization space, necessitat-ing a larger rank for sufficient representation capabilities.Conversely, regions with a small loss reduction during thewarmup phase have limited common knowledge and re-quire more task-specific knowledge to compensate. The low-rank expert module needs to be sufficiently differentiatedfrom the backbone to allow the task-specific knowledge todevelop a distinct representation separate from the commonknowledge. The smaller the rank of the low-rank expertmodule, the more differentiated it is from the backbone; as the rank increases and reaches that of the backbone, theybecome equivalent. Therefore, the low-rank expert modulesassociated with these regions are assigned smaller rankvalues to ensure their differentiation from the backbone.To adapt to the varying difficulties across different re-gions, we devise LoRKD*, a variant of our method. LoRKD*adaptively adjusts the ranks of the low-rank modulesthrough an automated mechanism. Specifically, we quantifythe changes in the loss function of data from differentregions during the warmup phase and adjust the ranksof the corresponding low-rank expert modules accordingly.Assume that the loss reduction of each region during thewarmup phase is L1, ..., LT , and the base rank is r. Therank of each low-rank module can then be calculated as:",
  "Efficient Knowledge Separation Convolution": "Task-Specific Gradient Separation Bottleneck. To achieveknowledge decomposition, we propose explicit gradientseparation as our solution. This approach ensures that eachexpert module computes gradients exclusively for its desig-nated task, thus acquiring task-specific knowledge. Concur-rently, the shared backbone aggregates gradients from alltasks, thereby acquiring generic knowledge shared acrossall tasks. However, when a mini-batch of data containsT tasks, the convolution operation must be performed T timesgt = (W0 + BtAt)ht, where t {1, ..., T}. The T times for-ward propagation significantly increases the training time,especially when decomposing a large number of tasks. Toaddress this issue, we propose the Efficient KnowledgeSeparation Convolution (EKS Convolution).In order to elucidate our improvements in convolution,we first review the standard convolution operation. Foreach convolution, the input features can be representedas h RBCinHW D, where B, H, W, D represent thesample number of mini-batch size, the height, width, anddepth of the feature maps, respectively. If the kernel size ofthe convolution is k and the stride is 1, each output featureunit oijl RBCout in output features g RBCoutHW D",
  "o=0h(i+m)(j+n)(l+o) mno,": "where i {1, ..., H} , j {1, ..., W} , l {1, ..., D} ,and h(i+m)(j+n)(l+o) RBCin represents the units of theinput feature map h, while mno RCinCout represents theconvolution weights.For each EKS Convolution, in addition to the inputfeature map h, the task label M RBT , which is a one-hotvector corresponding to the mini-batch, is also inputted as areference for subsequent parameter aggregation. The outputfeatures are then computed as follows:g = g1 gt gTgt = (W0 + BtAt)ht = (W0 + BtAt)Mth,(3)",
  "SUBMITTED TO IEEE TPAMI6": "where denotes the concatenation operation, ht representsthe set of Bt features in h that correspond to the t-th task,and Mt is an index matrix that indicates which features inh belong to the t-th task. To avoid redundant convolutionaloperations, we propose parameter aggregation, wherein theparameters for the current iteration are aggregated into W according to M. This ensures that the number of forwardpropagation is always equal to 1, and the operation g =Wh is equivalent to the Eqn. (3). Specifically, the operationof the Eqn. (3) can be transformed as follows:g = (W0 + B1A1)h1 (W0 + BTAT)hT",
  "BA = B1A1 ... BtAt ... BTAT": "denotes the Hadamard product, andBA MRBT CoutCinkkk represents the configuration of low-rank expert module for each input feature and i correspondsto the second dimension of ( BAM). The weight of sharedconvolution W0 is applied to all tasks. In this way, weobtain the aggregated weight W RBCoutCinkkk thatis equivalent to Eqn. (3) but requires only single forward.Another challenge associated with it is that W hassix dimensions, unlike traditional 3D convolutions whichtypically have five dimensions. To ensure compatibility withexisting deep learning libraries, we adopted the conceptof group convolution (GConv) . Specifically, we setthe group number to B and {1, ..., B}. Then, wereshape h to h R1BCinHW D and reshape W toW RBCoutCinkkk. Consequently, each output featureunit oijl in g can be computed byoijl = o1ijl oijl oBijl",
  "o=0h(i+m)(j+n)(l+o) mno,(5)": "where h(i+m)(j+n)(l+o) and mno represent the reshapedversions. Eqn. (5) is a standard form of group convolution,which can be easily implemented in existing deep learninglibraries such as PyTorch and TensorFlow . With theabove transformations, EKS Convolution improves uponthe traditional convolution operation by enabling gradientseparation to be achieved in a single forward pass, regard-less of the number of tasks. Besides, it eliminates the compu-tational overhead of duplicating input for each convolution,thereby significantly improving training efficiency.",
  "Training Objective": "For objective, we design distinct loss functions specific tomedical foundation models for segmentation and classifica-tion tasks. In general, the loss function of LoRKD comprisestwo main parts: Ltask and Ltransfer. Ltask provides super-vision from the label information of the corresponding task,while Ltransfer transfers knowledge from the foundationmodel to decomposed models, which can be expressed as:",
  "where LKL represents the Kullback-Leibler divergence": "3.5.2Training Objective for ClassificationFor medical foundation models towards the classificationtask, given a mini-batch of training data {(xi, yi, yti)}Bi=1, xirepresents the i-th input image in the current mini-batch, yirepresents the class label across all tasks and yti representsthe class label within its corresponding task t. We denotethe feature extracted from the foundation model as f bi =F(xi; F ), and the features extracted from the lightweightdecompostion model as f di= F(xi; Fs; Et). Then, theLtransfer for sample xi can be written as LKL(f bi , f di ).Moreover, we can also leverage class label information{yti} to enhance task-level supervision. Specifically, duringtraining, we integrate T classification heads {h1, ..., hT } intothe lightweight decompostion model. These classificationheads can individually predict {Y1, ..., YT } classes whereYt represents the number of classes for the t-th task, Y isthe total number of all classes and Ti=1 Yi = Y . The logitsextracted from the decomposition model can be denoted asgdi = ht(f di ) and the prediction can be calculated by:",
  ": Return Y d": "backbone of the model. The introduction of this warmupphase offers two key benefits. Firstly, the low-rank structureneeds to be attached to a well-trained backbone. Training thebackbone first, before integrating the low-rank expert mod-ules, ensures that general and task-specific knowledge areeffectively separated. Secondly, training during the warmupphase provides priors about the difficulty of learning indifferent regions, providing guidance on how to set the rankof low-rank expert modules in subsequent phases 3.3.1.After the warmup phase, the low-rank experts are trainedtogether with the shared backbone.To show the computational merit, we compare our effi-cient knowledge separation convolution with FLoRA , arecent parameter-efficient fine-tuning method that utilizesmultiple low-rank adapters like us. FLoRA allows eachexample in a minibatch to have its unique low-rank adaptersand demonstrates lower computational costs compared tothe vanilla manner. Their comparision w.r.t. computationalcomplexities is presented in the following table:",
  "EKS conv (ours) Y = X(W0 + Ti=1(BA M)i) Tc2(rd2) + c2(bld2)": "Following the notation in , we omit the cost of element-wise multiplications () and omit the dimensions asW Rdk, A Rrk, B Rdr. Here, c2 represents thecomputational coefficient of matrix multiplication, b is thebatch size, l is the sequence length, and T is the number oftasks. For EKS conv to be more efficient than FLoRA, thefollowing condition must be satisfied:",
  "DET10 1Xray103543": "key difference is that while FLoRA reduces costs by replac-ing expensive batched matmuls (bmm) with element-wisemultiplications () and broadcasting, our method furtherreduces computational costs by performing early parameterfusion before the forward pass of DNNs. In summary, ourapproach surpasses the efficiency of FLoRA through earlyparameter fusion. Additionally, FLoRA uses broadcastingto improve efficiency, which cannot be well generalized toconvolution operations, while LoRKD is not subject to this.",
  "EXPERIMENTS": "In this section, we present the experimental results ofknowledge decomposition using LoRKD. We evaluate itsperformance on representative medical foundation modelsfor both segmentation and classification tasks, detailingthe experimental setup 4.1. Extensive experiments on pre-training and downstream datasets validate the generaliza-tion and transfer capabilities of the decomposed models4.2. 4.3 provides a detailed cost analysis to verify theefficiency of knowledge decomposition. Additionally, weinclude ablation studies, knowledge disentanglement, andvisualizations of the results in 4.4.",
  "Dataset and Foundation Model": "To evaluate the decomposition performance on segmen-tation tasks, we choose a recent state-of-the-art founda-tion model, Segment Anything in radiology scans by Textprompts (SAT). The SAT models come in two sizes: SAT-Nano and SAT-Pro. They are trained on the SAT-DS dataset,which is the largest and most comprehensive collectionof public 3D medical image segmentation datasets .Furthermore, to determine the extent to which the decom-posed expert models can fully replace foundation modelsin specific domains, we evaluate the transferability of theseexpert models on five downstream segmentation datasets.For the classification task, we choose three medicalmulti-task datasets of varying scales that are popular formedical image diagnosis pre-training: Radimagenet ,MedMnist , and Med-MT. We decompose the founda-tional models pre-trained on these datasets into 11, 10,and 8 lightweight expert models, respectively. In addition,we evaluated the transferability of these expert modelson seven downstream datasets. Detailed information aboutthese datasets can be found in .",
  "Evaluation Metrics": "We use two metrics: Dice Similarity Coefficient (DSC) andNormalized Surface Dice (NSD) to evaluate the performanceof segmentation models. Region-wise results are reportedfor eight regions of the human body: Brain, Head and Neck,Thorax, Abdomen, Pelvis, Spine, Upper Limb, and LowerLimb. Specifically, we merge results from all segmentationclasses within the same region to indicate the general perfor-mance in that region. The average of all region-wise resultsrepresents the overall performance.For the classification task, we also use the accuracy ofeach region and the average of all region-wise accuracyto evaluate the classification performance. The division ofregions for each dataset varies according to the data type.",
  "Baselines": "For the segmentation task, we compare our decomposedmodel with the original foundation model and nnUNet ,which represent the state-of-the-art universal models andspecialist models, respectively. For nnUNet, we train 49separate models, each specialized on a different sub-dataset,and report their aggregated results. This makes nnU-Net astrong baseline, as it is an ensemble of specialist models,each optimized individually on specific sub-datasets.To ensure a more comprehensive comparison, we im-plemented various baseline methods on less resource-demanding classification tasks. The competitive baselinesare as follows: (1) Baseline refers to training from scratch ondownstream tasks. (2) Single-Task Learning (STL) refers totraining multiple single-task networks independently, sim-ilar to nnUNet in segmentation experiments. (3) Multi-Task Learning (MTL) refers to training a single model topredict all tasks. (4) STL-KD and (5) MTL-KD correspondto the KD version of STL and MTL, respectively, whichutilize knowledge distillation to transfer knowledge fromfoundation models. (6) MoCo-MTL and (7) Aligned-MTL are the advanced MTL algorithms. (8) KF represents the advanced knowledge decomposition method,which is the closest to our goal and serves as our primarycomparison object in classification. 4.1.4Implementation DetailsFor both decomposition training and downstream fine-tuning in segmentation experiments, we use AdamW op-timizer with a learning rate of 1e-4 and CosineAnnealingLRas the scheduler. The default values for the hyperparametersare set as follows: =0.1, r=8. The vision backbone of allmodels is based on the 3D U-Net structure of varyingsizes. During decomposition, we directly inherit the textencoder from the foundation model and keep it frozen.For the decomposition training in the classification task,we use the SGD optimizer with a learning rate of 0.05and CosineAnnealingLR as the scheduler for training 100epochs. For the downstream fine-tuning, we use AdamWoptimizer with a learning rate of 5e-5 and train the modelfor 240 epochs. The default values for the hyperparametersare set as follows: =1, r=8. The pre-trained model struc-ture is ResNet50 , and the structure of the lightweightdecomposition model is ShuffleNetV2 .",
  "Performance in Segmentation": "4.2.1.1Decomposition Performance in SegmentationThe region-wise evaluation results are shown in .Each column corresponds to a specific region. Parmasrepresents the total number of parameters during training.We use -nano and -pro to distinguish between twosizes of models and use * to distinguish between LoRKD-balance and LoRKD-imbalance.Decomposed model vs. Foundation model. In general, thedecomposed model can achieve stronger specialization withlower costs (The cost comparison can be found in ).For SAT-Pro, our decomposed model has only 23% of itsparameters and 17% of its computational overhead, yet itsurpasses the foundation model with approximately a 2%performance improvement. Similarly, for SAT-Nano, ourdecomposed model has 52% of its parameters and 40% of itscomputational overhead, and it also outperforms SAT-Nanoon both metrics. Notably, in four regions, LoRKD providesconsiderable performance gains, up to 8%. In the otherregions, LoRKD can also maintain performance compara-ble to the foundation model with fewer parameters. Thisdemonstrates that our method not only achieves lossless",
  "LoRKD*-Pro87.9077.6689.0194.6597.2789.30": "decomposition but also surpasses the original model byalleviating the conflict between heterogeneous tasks.LoRKD* vs. LoRKD. As shown in , LoRKD* consis-tently outperforms LoRKD in most cases, demonstrating theeffectiveness of our imbalanced rank design. This indicatesthat the loss reduction in the warmup phase can accuratelyreflect the learning difficulty of each region, thereby guidingthe reasonable allocation of parameters for each region. Indetail, LoRKD*-Pro exhibits higher DSC scores than LoRKD-Pro in 7 out of the 8 regionsexcept the Lower Limb.Similarly, LoRKD*-Nano outperforms LoRKD-Nano in mostregions, except for the Pelvis. The regions where LoRKD*-Nano and LoRKD*-Pro perform worse differ because theirbackbone models have different sizes, leading to varyinglearning capabilities in each region. Consequently, the au-tomatically computed rank values of each low-rank expertmodule differ between LoRKD*-Nano and LoRKD*-Pro.Decomposed model vs. Ensemble of SOTA Specialistmodel. It is worth noting that our LoRKD*-Pro can sur-pass nnUNet in overall performance (Avg), filling theperformance gap between universal models and specialistmodels. This is particularly challenging since nnUNet rep-resents an ensemble of 49 state-of-the-art models trainedindependently on each sub-dataset. Specifically, in the fiveregions of Head & Neck, Upper Limb, Lower Limb, Pelvis,and Thorax, LoRKD*-Pro consistently outperforms nnUNet.This indicates that the tasks in these regions benefit fromuniversal training, and all tasks within these regions can beaddressed by a single expert model. However, in the threeregions of the Abdomen, Brain, and Spine, LoRKD*-Proremains inferior to the nnUNet ensemble. This suggests thatthese regions are suitable for fine-grained specialist models,as universal models still struggle to adequately solve thetasks in these regions. 4.2.1.2Transfer Performance in SegmentationFor the decomposed lightweight expert model to fullyreplace the foundation model in a specific domain, it isessential that the expert models not only perform wellon the same distribution of data (pre-training dataset) butalso demonstrate their generalization ability on downstream",
  "tasks with similar distributions. Hence, we evaluate theperformance of the decomposed model and baselines onseveral representative downstream datasets": "presents the performance comparison betweenthe decomposed expert models and the baselines onfive downstream segmentation datasets. For the specialistnnUNet, we directly train five models on each downstreamdataset. As for the decomposed model, we fine-tune theexpert model corresponding to the downstream dataset,such as using the brain expert for the MSD Hippocampusdataset. As for the foundation model, we fine-tune the pre-trained model on the downstream dataset. Foundation model vs. Specialist model. It can be observedthat the overall performance of nnUNet on downstreamdatasets exceeds that of the foundation model. nnUNetdemonstrates superior average performance, surpassingSAT in 4 out of 5 datasets, with the only exception beingthe CHAOS CT dataset. This indicates that despite the uni-versal pre-training knowledge of foundation models, theirability to transfer to downstream data is insufficient to re-place specialist models. Downstream datasets typically haveonly a small amount of data, which makes them difficultto support the fine-tuning of the foundation model withnumerous parameters, according to the Scaling Law . Decomposed model vs. Baselines. Generally, our decom-posed models yield favorable results and significantly sur-pass the original foundation models. Compared to SAT-Nano, LoRKD*-Nano demonstrates a 5.8% performanceimprovement on NSD and a 3.1% improvement on DSC.For SAT-Pro, LoRKD*-Pro achieves a 2.6% performanceimprovement on NSD and a 1.6% improvement on DSC.Notably, the performance of LoRKD*-Nano and LoRKD-Nano is comparable to or better than the larger model SAT-Pro, indicating that compact expert models are more suit-able for downstream datasets than the foundation model.Compared to nnUNet, LoRKD*-Pro and LoRKD-Pro achievecomparable performance, outperforming nnUNet in threeout of five datasets. We also observe that LoRKD-Pro con-sistently outperforms LoRKD-Nano, and LoRKD* is slightlybetter than LoRKD. This demonstrates that the performanceon the pre-training dataset is positively correlated with the",
  "Performance in Classification": "4.2.2.1Decomposition Performance in ClassificationThe performance comparison of different methods on threepre-training classification datasets is presented in .Each column corresponds to a specific task. Only KF andour method focus on the knowledge decomposition ofpre-trained models. Considering the generalization require-ments of foundation models, it is typical for these modelsto employ a unified classification head during trainingrather than configuring a specific classification head foreach task . This practice accounts for the relatively poorperformance of the foundation model depicted in .The foundation model vs. STL. The performance of thefoundation model surpasses that of STL on the Med-MT dataset but is significantly inferior to STL on bothRadimagenet and MedMnist, especially MedMnist. Thisobservation suggests that as the scale and diversity ofthe pre-training dataset increase, the specialization of thepre-trained model gradually diminishes due to conflictsbetween different domain knowledge. In contrast, train-ing models independently for each task (STL) can preventinterference between different tasks, resulting in superiorperformance on Radimagenet and MedMnist compared tofoundation models. However, STL cannot learn common knowledge across tasks, often necessitating more data toensure generalization. Additionally, training T individualmodels is not only time-consuming but also leads to a linearincrease in the number of parameters.MTL-based methods vs. STL-based methods. It can alsobe observed that MTL outperforms STL on Radimagenetand Med-MT, while underperforming STL on MedMnist.This discrepancy may be attributed to the degree of corre-lation between tasks within the pre-training dataset, withMedMnist having the most diverse modalities (refer tosupplementary materials). Unlike standard MTL, advancedMTL methods such as MoCo-MTL and Aligned-MTL donot yield improvements and may even exhibit worse per-formance. This suggests that balancing multiple optimiza-tion objectives to obtain a better shared encoder is notan effective solution when there are significant differencesamong tasks. The knowledge distillation variants of STLand MTL (STL-KD and MTL-KD) do not show significantperformance improvement, which suggests that the generalfeatures extracted by foundation models offer limited bene-fits for specific tasks and indirectly reflect the importance ofspecialized features. It aligns with the design philosophy ofour LoRKD.LoRKD vs. KF and other methods. Compared to the knowl-edge decomposition method KF, our approach demonstratessignificant performance improvements while introducingfewer parameters. Specifically, even with 11, 10, or 8 ex-perts, our method employs less than half the number of",
  "SUBMITTED TO IEEE TPAMI11": "TABLE 5: The transfer performance of the decomposed expert models on seven downstream classification datasets. Comp.Ratio denotes the compression ratio, defined as the ratio of the deployed model parameters to the parameters of thefoundation model. - indicates the absence of data corresponding to the downstream tasks in the pre-training dataset.",
  "LoRKD1.25M5.32%83.2577.6676.9478.3398.3375.1887.8482.50": "parameters used by KF. This outcome validates the ef-fectiveness of our low-rank expert modules and the effi-cient knowledge separation convolution. Furthermore, ourmethod achieves the best average performance compared toother non-knowledge decomposition baselines, underscor-ing the potential of knowledge decomposition in extractingtask-specific knowledge. 4.2.2.2Transfer Performance in ClassificationThe performance comparison of the expert models decom-posed from three pre-training datasets on seven down-stream classification datasets is shown in . For KFand our method, we fine-tune the corresponding expertmodels on downstream datasets, such as using the lungexpert model for the COVID dataset. In the absence of acorresponding expert model, we fine-tune on the sharedbackbone, similar to (denoted with ). As for othernon-knowledge decomposition methods, we use the mod-els trained on the pre-training dataset for fine-tuning todemonstrate the advantages of knowledge decompositionin terms of transferability. Please refer to the supplementarymaterials for further details.The performance of fine-tuning foundation models isobserved to be inferior to the Baseline, reinforcing thatfoundation models cannot replace task-specific models dueto their lack of specialization. Compared to the Baseline, both STL-based and MTL-based methods show minimal im-provement, indicating that focusing solely on task-specificor common knowledge does not enhance transferability.Conversely, our expert models incorporate both commonknowledge and task-specific knowledge, which exhibitstrong transferability and even significantly outperform KF.Another advantage over KF is that our method supportsparameter fusion and does not require the simultaneousdeployment of two networks (CKN and the correspondingTSN need to be deployed simultaneously in KF).Furthermore, an interesting phenomenon was observed.In comparison to MTL-KD, our method exhibits signifi-cantly better performance on downstream datasets. Thisdemonstrates the advantage of knowledge decompositionin transferability, which can not be directly reflected throughthe decomposition performance. As the scale of the pre-training dataset increases, the transferability of our de-composed expert models also improves, indicating thatincreasing the scale of pre-training datasets benefits thetransferability of the decomposed model.",
  ": The comparison of MIG scores on different methods": "ducted a comprehensive analysis of the models resourcerequirements. We measured the model parameters andcomputational overhead (FLOPs) during both training andinference stages.Lower Costs on Segmentation. As shown in (a), ourmethod significantly reduces the resource consumption ofthe foundation model, indicating that LoRKD can effectivelylower deployment costs while maintaining high compu-tational efficiency. For SAT-Pro, the decomposed modelscan achieve compression ratios of 22.96% in parametersand 17.09% in computation. While for SAT-Nano, the de-composed model achieved compression ratios of 52.42%in parameters and 39.83% in computation. This is highlyvaluable for deploying the models in real-world scenariosthat are resource-constrained in under-developed area.Lower Costs on Classification. For the classification task,we compare the costs among different methods on Radima-genet, as shown in (b). Similar to the segmentationtask results, our decomposed models significantly reducethe number of parameters and FLOPs compared to thefoundation model. It is worth mentioning that if parameterfusion is used, our costs will be the same as baseline, achiev-ing a compression ratio of 5.3% in parameters and 3.6% incomputation. As r increases, our costs remain minimal anddo not increase significantly. In comparison to KF, even atr=16, our method still incurs significantly lower costs.",
  "Knowledge Disentanglement": "Enhanced disentanglement. To verify whether our methodcan indeed achieve knowledge decoupling between differ-ent tasks, we measure the mutual information gap (MIG)scores across different methods. MIG is a widely usedmetric for assessing disentanglement. The results are illus-trated in , where higher MIG scores indicate a higherlevel of disentanglement. It can be observed that our methodexhibits a higher level of disentanglement compared to theprevious KF and other baselines. This improvement canlikely be attributed to the explicit gradient separation in-corporated in our method, which effectively minimizes theinterference between gradients from different tasks, therebyenhancing the specialization of the expert modules.Additionally, we find that MTL exhibits a lower degreeof disentanglement compared to STL. This suggests that theshared encoder architecture commonly used in MTL inad-vertently leads to the entanglement of gradients from thesedifferent tasks. As a result, this gradient entanglement man-ifests as knowledge entanglement, potentially diminishingthe models overall effectiveness in handling individualtasks. Furthermore, STL-KD exhibits lower disentanglement",
  "Ablation Study": "The impact of Rank r. The rank r of low-rank expertssignificantly affects their representation ability and the num-ber of parameters. Therefore, we conducted an ablationexperiment to investigate the impact of varying the rankof low-rank experts. The results of the segmentation taskand classification task are presented in and respectively. For the segmentation task, whether decom-posing SAT-Pro or SAT-Nano, increasing r from 4 to 8leads to a significant performance improvement on the pre-training dataset. However, increasing r from 8 to 16 doesnot yield further enhancement; in fact, the performancetends to plateau or even slightly degrade. The results of theclassification task further corroborate this conclusion, whereperformance generally improves from r = 4 to r = 8 butshows diminishing returns or even slight decreases whenr is increased to 16. This suggests that selecting a larger ris not necessarily better. An appropriate rank value enablesthe low-rank expert module to learn distinct representationsfrom the backbone while maintaining a manageable numberof parameters. Therefore, we selected 8 as the base rankvalue. Moreover, we again observed that the improvementin the upstream dataset is positively correlated with theimprovement in transferability. The impact of . shows the ablation experimentabout the impact of the trade-off parameter between theLtask and Ltransfer. We observe that increasing from 0.05to 1 does not lead to significant performance fluctuations,but further increasing to 10 results in a noticeable perfor-mance drop. This indicates that maintaining an appropriate value is critical for optimizing decomposition perfor-mance, as an excessively large value can negatively impactthe training process. Overall, a value of 0.1 achieves anappropriate balance between the two loss functions, consis-tently yielding the best results.",
  "Visualization": "Stronger Specialization. In this subsection, we visualize theexperimental results and analyze the specialization broughtby knowledge decomposition. presents segmen-tation results, comparing the ground truth with imagessegmented by our LoRKD model and the foundation model(SAT-Pro). Different colors represent distinct segmentationtargets: the left side is the segmentation of head of femur,red and yellow denote the right and left femur, respectively;the right side is the segmentation of thoracic cavity, blueand green correspond to different slices respectively. Thefoundation model exhibits several noticeable segmentationflaws, including clearly missing parts of the target regionsand over-segmenting certain areas. In contrast, our LoRKDdemonstrates stronger specialization, producing segmenta-tion results that closely align with the ground truth.Taking the DET10 dataset as an example, we evaluatethe differences in the activated regions between the decom-posed expert model and the foundation model during theprediction process from the perspective of Grad-CAM .Grad-CAM highlights the regions of an input image mostrelevant to a neural networks decision, offering insightsinto how the model interprets the image. As illustrated in, the visualization results reveal notable differencesbetween different models. The foundation model tends tofocus on broader, less specific regions of the image. Thisbroad focus is indicative of the models ability to capturegeneral features across a wide range of tasks, yet it lacksthe precision required for more specialized applications. TheKF models focus is more refined than the foundation modelbut remains less precise than our decomposed expert model.In contrast, our decomposed expert model exhibits a more refined focus, concentrating on smaller, more precise regionsthat are highly relevant to the specific task at hand. This pre-cise localization indicates a higher degree of specialization.These findings underscore the effectiveness of our approachin improving specialization and efficiency, particularly inscenarios where precise region identification is crucial.",
  "CONCLUSION": "In this paper, we propose a new perspective called knowl-edge decomposition, aimed at reducing the deploymentcosts and enhancing specialization for medical foundationmodels. We develop low-rank expert modules and efficientgradient separation convolution to decompose the founda-tion model into multiple lightweight expert models. Ourmethod includes two variants: LoRKD-balance and LoRKD-imbalance. The former assigns a low-rank expert moduleof the same rank to each task, while the latter adaptivelyadjusts the rank of each module based on task complexity.The decomposition performance on upstream tasks and thetransfer performance on downstream tasks fully demon-strate that LoRKD can effectively alleviate the conflict ofheterogeneous data, achieving cost reduction and perfor-mance improvement simultaneously. We hope this researchoffers valuable insights for advancing the development anddeployment of medical foundation models.",
  "SUBMITTED TO IEEE TPAMI15": "S. Nouranian, M. Ramezani, I. Spadinger, W. J. Morris, S. E.Salcudean, and P. Abolmaesumi, Learning-based multi-labelsegmentationoftransrectalultrasoundimagesforprostatebrachytherapy, IEEE transactions on medical imaging, vol. 35, no. 3,pp. 921932, 2015. 1 K. Yan, X. Wang, L. Lu, and R. M. Summers, Deeplesion: au-tomated mining of large-scale lesion annotations and universallesion detection with deep learning, Journal of medical imaging,vol. 5, no. 3, pp. 036501036501, 2018. 1 G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi,M. Ghafoorian, J. A. Van Der Laak, B. Van Ginneken, and C. I.Sanchez, A survey on deep learning in medical image analysis,Medical image analysis, vol. 42, pp. 6088, 2017. 1 S. K. Zhou, H. Greenspan, C. Davatzikos, J. S. Duncan, B. Van Gin-neken, A. Madabhushi, J. L. Prince, D. Rueckert, and R. M. Sum-mers, A review of deep learning in medical imaging: Imagingtraits, technology trends, case studies with progress highlights,and future promises, Proceedings of the IEEE, vol. 109, no. 5,pp. 820838, 2021. 1",
  "X. Zhao, Y. Wu, G. Song, Z. Li, Y. Zhang, and Y. Fan, A deeplearning model integrating fcnns and crfs for brain tumor seg-mentation, Medical image analysis, vol. 43, pp. 98111, 2018. 1": "A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., Segmentanything, in Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pp. 40154026, 2023. 1, 3 Z. Zhao, Y. Zhang, C. Wu, X. Zhang, Y. Zhang, Y. Wang, andW. Xie, One model to rule them all: Towards universal seg-mentation for medical images with text prompts, arXiv preprintarXiv:2312.17183, 2023. 1, 3, 7 D. M. Nguyen, H. Nguyen, N. T. Diep, T. N. Pham, T. Cao,B. T. Nguyen, P. Swoboda, N. Ho, S. Albarqouni, P. Xie, et al.,Lvm-med: Learning large-scale self-supervised vision models formedical imaging via second-order graph matching, arXiv preprintarXiv:2306.11925, 2023. 1 X. Mei, Z. Liu, P. M. Robson, B. Marinelli, M. Huang, A. Doshi,A. Jacobi, C. Cao, K. E. Link, T. Yang, et al., Radimagenet: an openradiologic deep learning research dataset for effective transferlearning, Radiology: Artificial Intelligence, vol. 4, no. 5, p. e210315,2022. 1, 3, 7, 10",
  "Y. Huang, X. Yang, L. Liu, H. Zhou, A. Chang, X. Zhou, R. Chen,J. Yu, J. Chen, C. Chen, et al., Segment anything model for medicalimages?, Medical Image Analysis, vol. 92, p. 103061, 2024. 1, 3": "C. Wu, J. Lei, Q. Zheng, W. Zhao, W. Lin, X. Zhang, X. Zhou,Z. Zhao, Y. Zhang, Y. Wang, et al., Can gpt-4v (ision) serve med-ical applications? case studies on gpt-4v for multimodal medicaldiagnosis, arXiv preprint arXiv:2310.09909, 2023. 1 R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. vonArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., Onthe opportunities and risks of foundation models, arXiv preprintarXiv:2108.07258, 2021. 1",
  "H. Wang, S. Guo, J. Ye, Z. Deng, J. Cheng, T.-X. Li, J. Chen, Y.-C.Su, Z. Huang, Y. Shen, B. Fu, S. Zhang, J. He, and Y. Qiao, Sam-med3d, 2023. 3": "Y. Ye, Y. Xie, J. Zhang, Z. Chen, and Y. Xia, Uniseg: A prompt-driven universal segmentation model as well as a strong represen-tation learner, in International Conference on Medical Image Comput-ing and Computer-Assisted Intervention, pp. 508518, Springer, 2023.3 J. Liu, Y. Zhang, J.-N. Chen, J. Xiao, Y. Lu, B. A Landman, Y. Yuan,A. Yuille, Y. Tang, and Z. Zhou, Clip-driven universal model fororgan segmentation and tumor detection, in Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 2115221164, 2023. 3 V. I. Butoi, J. J. G. Ortiz, T. Ma, M. R. Sabuncu, J. Guttag, andA. V. Dalca, Universeg: Universal medical image segmentation,in Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 2143821451, 2023. 3",
  "Y. Xie and D. Richmond, Pre-training on grayscale imagenet im-proves medical image classification, in Proceedings of the Europeanconference on computer vision (ECCV) workshops, pp. 00, 2018. 3": "R. Zhang, Y. Zheng, T. W. C. Mak, R. Yu, S. H. Wong, J. Y. Lau, andC. C. Poon, Automatic detection and classification of colorectalpolyps by transferring low-level cnn features from nonmedicaldomain, IEEE journal of biomedical and health informatics, vol. 21,no. 1, pp. 4147, 2016. 3 H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao,D. Mollura, and R. M. Summers, Deep convolutional neuralnetworks for computer-aided detection: Cnn architectures, datasetcharacteristics and transfer learning, IEEE transactions on medicalimaging, vol. 35, no. 5, pp. 12851298, 2016. 3 J. Yang, R. Shi, D. Wei, Z. Liu, L. Zhao, B. Ke, H. Pfister, and B. Ni,Medmnist v2-a large-scale lightweight benchmark for 2d and 3dbiomedical image classification, Scientific Data, vol. 10, no. 1, p. 41,2023. 3, 7",
  "T. Dai, R. Zhang, F. Hong, J. Yao, Y. Zhang, and Y. Wang, Unich-est: Conquer-and-divide pre-training for multi-source chest x-rayclassification, IEEE Transactions on Medical Imaging, 2024. 3, 5": "S. Azizi, B. Mustafa, F. Ryan, Z. Beaver, J. Freyberg, J. Deaton,A. Loh, A. Karthikesalingam, S. Kornblith, T. Chen, et al., Bigself-supervised models advance medical image classification, inProceedings of the IEEE/CVF international conference on computervision, pp. 34783488, 2021. 3 Y. Tang, D. Yang, W. Li, H. R. Roth, B. Landman, D. Xu, V. Nath,and A. Hatamizadeh, Self-supervised pre-training of swin trans-formers for 3d medical image analysis, in Proceedings of theIEEE/CVF conference on computer vision and pattern recognition,pp. 2073020740, 2022. 3",
  "R. Krishnan, P. Rajpurkar, and E. J. Topol, Self-supervised learn-ing in medicine and healthcare, Nature Biomedical Engineering,vol. 6, no. 12, pp. 13461352, 2022. 3": "H.-Y. Zhou, C. Lu, C. Chen, S. Yang, and Y. Yu, A unified vi-sual information preservation framework for self-supervised pre-training in medical image analysis, IEEE Transactions on PatternAnalysis and Machine Intelligence, vol. 45, no. 7, pp. 80208035, 2023.3 Y. Li, H. Wang, and Y. Luo, A comparison of pre-trained vision-and-language models for multimodal representation learningacross medical images and reports, in 2020 IEEE internationalconference on bioinformatics and biomedicine (BIBM), pp. 19992004,IEEE, 2020. 3 J. H. Moon, H. Lee, W. Shin, Y.-H. Kim, and E. Choi, Multi-modalunderstanding and generation for medical images and text viavision-language pre-training, IEEE Journal of Biomedical and HealthInformatics, vol. 26, no. 12, pp. 60706080, 2022. 3 S.-C. Huang, L. Shen, M. P. Lungren, and S. Yeung, Gloria: Amultimodal global-local representation learning framework forlabel-efficient medical image recognition, in Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 39423951, 2021. 3 Z. Chen, Y. Du, J. Hu, Y. Liu, G. Li, X. Wan, and T.-H.Chang, Multi-modal masked autoencoders for medical vision-and-language pre-training, in International Conference on MedicalImage Computing and Computer-Assisted Intervention, pp. 679689,Springer, 2022. 3 A. Taleb, M. Kirchler, R. Monti, and C. Lippert, Contig: Self-supervised multimodal contrastive learning for medical imagingwith genetics, in Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pp. 2090820921, 2022. 3 G. Liang, C. Greenwell, Y. Zhang, X. Xing, X. Wang, R. Kavu-luru, and N. Jacobs, Contrastive cross-modal pre-training: Ageneral strategy for small sample medical imaging, IEEE Journalof Biomedical and Health Informatics, vol. 26, no. 4, pp. 16401649,2021. 3 B. Yuan, Y. He, J. Davis, T. Zhang, T. Dao, B. Chen, P. S. Liang,C. Re, and C. Zhang, Decentralized training of foundation mod-els in heterogeneous environments, Advances in Neural Informa-tion Processing Systems, vol. 35, pp. 2546425477, 2022. 3",
  "H. Kim and A. Mnih, Disentangling by factorising, in Interna-tional Conference on Machine Learning, pp. 26492658, PMLR, 2018.3": "L. Tran, X. Yin, and X. Liu, Disentangled representation learninggan for pose-invariant face recognition, in Proceedings of the IEEEconference on computer vision and pattern recognition, pp. 14151424,2017. 3 X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, andP. Abbeel, Infogan: Interpretable representation learning by in-formation maximizing generative adversarial nets, Advances inneural information processing systems, vol. 29, 2016. 3 Y. Liu, Z. Wang, H. Jin, and I. Wassell, Multi-task adversarial net-work for disentangled feature learning, in Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, pp. 37433751, 2018. 3 M. F. Mathieu, J. J. Zhao, J. Zhao, A. Ramesh, P. Sprechmann, andY. LeCun, Disentangling factors of variation in deep represen-tation using adversarial training, Advances in neural informationprocessing systems, vol. 29, 2016. 3 R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bach-man, A. Trischler, and Y. Bengio, Learning deep representa-tions by mutual information estimation and maximization, arXivpreprint arXiv:1808.06670, 2018. 3",
  "A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classifica-tion with deep convolutional neural networks, Advances in neuralinformation processing systems, vol. 25, 2012. 6": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch:An imperative style, high-performance deep learning library,Advances in neural information processing systems, vol. 32, 2019. 6 M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,S. Ghemawat, G. Irving, M. Isard, et al., {TensorFlow}: a systemfor {Large-Scale} machine learning, in 12th USENIX symposiumon operating systems design and implementation (OSDI 16), pp. 265283, 2016. 6",
  "Y. Wen and S. Chaudhuri, Batched low-rank adaptation of foun-dation models, arXiv preprint arXiv:2312.05677, 2023. 7": "M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider,B. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M.Summers, et al., The medical segmentation decathlon, Naturecommunications, vol. 13, no. 1, p. 4128, 2022. 7 A. E. Kavur, N. S. Gezer, M. Bars, S. Aslan, P.-H. Conze, V. Groza,D. D. Pham, S. Chatterjee, P. Ernst, S.Ozkan, et al., Chaoschallenge-combined (ct-mr) healthy abdominal organ segmenta-tion, Medical Image Analysis, vol. 69, p. 101950, 2021. 7 J. Ma, Y. Wang, X. An, C. Ge, Z. Yu, J. Chen, Q. Zhu, G. Dong,J. He, Z. He, et al., Toward data-efficient learning: A benchmarkfor covid-19 ct lung and infection segmentation, Medical physics,vol. 48, no. 3, pp. 11971210, 2021. 7",
  "J. Liu, J. Lian, and Y. Yu, Chestx-det10: Chest x-ray dataset ondetection of thoracic abnormalities, 2020. 7": "H. D. Fernando, H. Shen, M. Liu, S. Chaudhury, K. Murugesan,and T. Chen, Mitigating gradient bias in multi-objective learning:A provably convergent approach, in The Eleventh InternationalConference on Learning Representations, 2022. 8 O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutionalnetworks for biomedical image segmentation, in Medical imagecomputing and computer-assisted interventionMICCAI 2015: 18thinternational conference, Munich, Germany, October 5-9, 2015, proceed-ings, part III 18, pp. 234241, Springer, 2015. 8",
  "S. Kornblith, M. Norouzi, H. Lee, and G. Hinton, Similarity ofneural network representations revisited, in International confer-ence on machine learning, pp. 35193529, PMLR, 2019. 13": "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, andD. Batra, Grad-cam: Visual explanations from deep networks viagradient-based localization, in Proceedings of the IEEE internationalconference on computer vision, pp. 618626, 2017. 14 Haolin Li received a B.S. degree from Universityof Electronic Science and Technology of China,in 2023. He is currently working toward the PhDdegree from Fudan University, advised by Prof.J. Yao and Prof. Y. Zhang. His research interestsinclude computer vision and AI for Healthcare. Yuhang Zhou received a B.S. degree from Uni-versity of Electronic Science and Technologyof China, in 2019. He is currently working to-ward the PhD degree from Shanghai Jiao TongUniversity, advised by Prof. J. Yao and Prof. Y.Zhang. His research interests include computervision, machine learning and AI for Healthcare. Ziheng Zhao received a B.S. degree fromShanghai Jiao Tong University, in 2021. He iscurrently working toward the PhD degree fromShanghai Jiao Tong University, advised by Prof.W. Xie and Prof. Y. Zhang. His research interestsinclude computer vision and AI for Healthcare. Siyuan Du received a B.S. degree from Uni-versity of Electronic Science and Technologyof China, in 2023. He is currently working to-ward the PhD degree from Fudan University,advised by Prof. J. Yao and Prof. Y. Zhang. Hisresearch interests include computer vision andAI for Healthcare. Jiangchao Yao is an Assistant Professor ofShanghai Jiao Tong University, Shnaghai China.He received the B.S. degree in information engi-neering from South China University of Technol-ogy, Guangzhou, China, in 2013. He got a dualPh.D. degree under the supervision of Ya Zhangin Shanghai Jiao Tong University and Ivor W.Tsang in University of Technology Sydney. Hisresearch interests include deep representationlearning and robust machine learning. Weidi Xie is an Associate Professor of ShanghaiJiao Tong University, Shnaghai China. Prior tothat, He completed D.Phil at Visual GeometryGroup, University of Oxford, advised by Pro-fessor Andrew Zisserman (VGG), and Profes-sor Alison Noble (BioMedIA). His research inter-ests include computer vision, deep learning, andbiomedical image analysis. Ya Zhang (Member, IEEE) received the B.S.degree from Tsinghua University and the Ph.D.degree in information sciences and technologyfrom the Pennsylvania State University. SinceMarch 2010, she has been a professor with Co-operative Medianet Innovation Center, ShanghaiJiao Tong University. Prior to that, she workedwith Lawrence Berkeley National Laboratory,University of Kansas, and Yahoo! Labs. Her re-search interest is mainly on data mining and ma-chine learning, with applications to informationretrieval, web mining, and multimedia analysis. Yanfeng Wang received the B.E. degree in infor-mation engineering from the University of PLA,Beijing, China, and the M.S. and Ph.D. degreesin business management from the Antai Collegeof Economics and Management, Shanghai JiaoTong University, Shanghai, China. He is currentlythe Vice Director of the Cooperative MedianetInnovation Center and also the Vice Dean ofthe School of Electrical and Information Engi-neering, Shanghai Jiao Tong University. His re-search interests mainly include media big dataand emerging commercial applications of information technology."
}