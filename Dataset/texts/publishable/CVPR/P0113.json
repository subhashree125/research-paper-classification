{
  "Abstract": "Recent Large Language Models have been enhancedwith vision capabilities, enabling them to comprehendimages, videos, and interleaved vision-language content.However, the learning methods of these large multimodalmodels typically treat videos as predetermined clips, mak-ing them less effective and efficient at handling streamingvideo inputs. In this paper, we propose a novel Learning-In-Video-Stream (LIVE) framework, which enables temporallyaligned, long-context, and real-time conversation within acontinuous video stream. Our LIVE framework comprisescomprehensive approaches to achieve video streaming dia-logue, encompassing: (1) a training objective designed toperform language modeling for continuous streaming in-puts, (2) a data generation scheme that converts offline tem-poral annotations into a streaming dialogue format, and(3) an optimized inference pipeline to speed up the model",
  "Corresponding Author.This is the Llama-3 upgraded version for CVPR camera-ready": "responses in real-world video streams.With our LIVEframework, we built VideoLLM-online model upon Llama-2/Llama-3 and demonstrate its significant advantages inprocessing streaming videos.For instance, on average,our model can support streaming dialogue in a 5-minutevideo clip at over 10 FPS on an A100 GPU. Moreover, italso showcases state-of-the-art performance on public of-fline video benchmarks, such as recognition, captioning,and forecasting. The code, model, data, and demo havebeen made available at showlab.github.io/videollm-online.",
  ". Our model shows strong temporal alignment capability in streaming video narration. The query at the beginning is Pleasedescribe what I am doing in real time": "multimodal models (LMMs) have unveiled impressive ca-pabilities such as vision-language dialogue , spatial understanding ,processing diverse modalities . Semi-nal exemplars, like OpenAIs GPT-4V or GPT-4o ,are progressively evolving into highly versatile human AIassistants.However, even the most advanced GPT-4o has onlyachieved streaming voice-driven multimodal assistance.2 Therefore, it is time to envision an always-on, contex-tual, J.A.R.V.I.S-like video assistant that supports free-formuser-assistant dialogue within the video stream, which weterm video streaming dialogue. Unlike existing LMMsfor video understanding (i.e., VideoLLMs) that work offline with manually selectedshort-video clips, an online assistant should continuouslyreceive video frames with visual content that is constantlyrefreshed.This paradigm shift presents new challenges.First, the user query may come with temporally aligned re-quirements (e.g., alert me when its time to flip the steak),thus the VideoLLM should scan every incoming frame toavoid event missing, instead of only yielding video-levelresponses. Second, to answer questions regarding summa-rization and planning, the VideoLLM must retain the long-context historical vision and language, which poses huge",
  "Based on their demo videos, the GPT-4o responses to visual scenes canonly occur after an active human voice input": "risk of exceeding maximum context window of LLMs, aswell as introduces considerable burden to causal decodingspeed and GPU memory. Third, the VideoLLM should gen-erate the answer in real-time, keeping pace with the videostream for always-on scenario. These abilities, however, areeven partially overlooked by the most advanced AI assis-tants . One possible path towards such an online VideoLLM,inspired by current interleaved vision-language models , is to employ a multi-turn dialogue format toachieve per-frame chatting within a video stream. This canbe accomplished by facilitating very frequent user interac-tions, utilizing the visual frame as query at each timestampto obtain the answer. We follow this to perform promptengineering for GPT-4V , but the results are dis-appointing: GPT-4V tends to output lengthy content at ev-ery frame, leading to significant delays, making it imprac-tical for real-time streaming video. We also explore train-ing baseline models for per-frame chatting. Unfortunately,this approach evidently diminishes the language modelingcapability, likely due to harmful language modeling on anexcessive number of redundant frames. We propose Learning-In-Video-strEam (LIVE), a com-prehensive framework that encompasses learning, data, andinference methods to develop an online video assistant.Unlike per-frame dialogue approach, LIVE introduces anovel training objective termed Streaming EOS (End-Of- Sequence) prediction that enables the model to learn whento response or remain silent in a video stream. This ob-jective differs from next-token prediction since EOS tokenshere will not appear in the input/output sequence. How-ever, it can work well with the autoregressive loss to trainan online VideoLLM. This design reduces unnecessary con-text, helping the model to manage much longer streamingvideos. Nevertheless, the training still requires data fromuser queries and assistant responses within video streams,which is scarce in popular video datasets usually used fortraining offline video models. To address this issue, LIVEpresents a streaming dialogue generation scheme that con-verts offline annotations into online dialogues to supportfree-form chatting. To enhance inference efficiency, LIVEleverages continuous key-value caching for streaming as-sistance, and parallelizes the fast visual encoding and slowlanguage decoding to prevent bottlenecks, thus moving to-wards real-time application.With LIVE framework, we build a simple VideoLLM-online model upon CLIP vision encoder and Llama-2 /Llama-3 language model. To evaluate the perfor-mance of video streaming dialogue, we utilize the languageperplexity metric and design two new metrics to compre-hensively assess the models capabilities in language mod-eling, temporal responsiveness, and overall streaming flu-ency.Experiments with real-time Ego4D narration demonstrate that our method shows advantages in all thatmetrics, with higher speed and lower memory cost. Further-more, our model achieves state-of-the-art results on numer-ous offline benchmarks, such as short- and long-term ac-tivity recognition and forecasting on the COIN and Ego4DLTA benchmarks .In addition, our model hasgood speed/memory efficiency, e.g., allowing continuous5-minute video streaming dialogue with memory cost lessthan 20 GB and average speed higher than 10 FPS on a sin-gle A100 GPU, paving the way for future real-world usage.",
  ". Related Work": "Visual Dialogue. Before transformers become main-stream in vision, visual dialogue methods tend toemploy a visually enhanced encoder, with a discriminativehead to select candidate answers or a recurrent architectureto generate multi-turn responses. For the encoder, a vari-ety of attention mechanism-based approaches have been proposed to learn the interactions between theimage, the answers, and the dialogue history. There havealso been explorations of encoder-only BERT modelsfor visual dialogue . However, most of them relysolely on a single image/video at the beginning of the con-versation, followed by multi-turn pure language dialogue,which makes them less flexible than the current interleavedvision-language dialogue systems.Large Multimodal Models. The advent of large language models (LLMs) has inspired a series of LLMmultimodal variants, i.e. large multimodal models (LMMs).Early LMMs achieve image dialogueby projecting the image encoding (e.g., from CLIP )to align with LLM embedding space.Then, lots of ef-forts explore more free-form interleavedvision-text chatting, spatial understanding , video comprehension ,etc. However, when it comes to online scenario, there isless exploration on how LLMs can fulfill the temporal align-ment, long-context, and real-time requirements for stream-ing video inputs. Our research bridges this gap, offeringcomprehensive solutions across model training, data, andinference to study the problem.Online Video Understanding. Typical video understand-ing benchmarks, such as action recognition , temporalaction localization , video question answering , andvideo dialogue , typically allow models to access en-tire video frames to make predictions, a setting referredto as offline. However, such setting does not align wellwith many real-time demands (e.g., autonomous driving,AR glasses).Instead, there is a growing focus on on-line video understanding problems like online action de-tection and anticipation , which aim to identify thecurrent/future action at each timestamp without seeing thefuture. Our study pioneers LMMs for online video under-standing. Unlike previous online action detection or anticipation models , which primarily addressone task with a highly customized model, we aim to pro-pose a general solution to achieve free-form dialogue alongthe online video stream, enabling a model to flexibly handlediverse tasks. Streaming video caption belongs to ourconcurrent work, but it only supports captioning rather thanfree-form dialogue, and its streaming caption temporal re-gion is fixed, making it much less flexible and general thanour work.Efficient Token Decoding. Efficient token decoding forLLMs and LMMs is essential for applying them to real-timeonline services. To accelerate that, a diversity of strate-gies has been proposed, such as parallelism on batch di-mension or cached key-value sequence , computa-tion/memory management optimization ,and even some lossy approaches .Our focusedstreaming video scenario has less concerns in large batchprocessing, but expects faster decoding to avoid excessiveframe skipping. We have considered this in training objec-tives, and further propose some inference schemes to accel-erate the decoding efficiency.",
  ". Video Streaming Dialogue": "Problem Formulation. Though huge successes have beenwitnessed in large multimodal models (LMMs), the assis-tance scenario like a smart AR glass helping the user withcooking, is still far from the capabilities of current LMMs,even for the most advanced version, GPT-4V .Forexample, despite carefully prompting GPT-4V similarly toMMVid to perform per-frame dialogue for handlingstreaming video inputsthe redundancy in responses be-tween frames, a limited context window of 1050 frames,and slow speed, collectively render the current GPT-4V un-suitable for online video understanding, as our promptinganalysis demonstrates (see supplementary material).To bridge the gap, we define a problem termed videostreaming dialogue. Given the context sequence beforetime t = t1, denoted as [Ctxt<t1], which may encom-pass previous vision-language content (e.g., historical userqueries, video frames, assistant responses), and an ongo-ing continuous video stream from t1 to t2, denoted as[Framet1tt2], our goal is (1) to determine whether thecurrent time t2 is suitable for language modeling; (2) tocarry out language modeling",
  "max P([Txtt2i+1]|[Ctx<t1], [Ft1tt2], [Txtt2i])": "(1)if t2 is determined, where [Txtti] denotes the ideal lan-guage token in the i-th position at timestamp t. [F] is theabbreviation of [Frame].In the following, we analyze if existing techniques havebeen enough to solve the problem.Interleaved/Per-frame Dialogue are Suboptimal. First,we investigate whether the popular approach of interleavedvision-language chatting can address this problem. Relatedto our formulation above, such a method learns languagemodeling after given frames between timestamps t1 and t2.However, if this approach is adopted during inference, itnecessitates the manual selection of timestamps t1 and t2,which does not align with the concept of video streamingdialogue. Current VideoLLMs represent a simplified version of this approach, engagingin single- or multi-turn pure language dialogue followingvideo clip inputs.If we consider a more free-form format, multi-turn inter-leaved vision-language chatting , we canget a per-frame chatting solution that might be hopeful tosolve video streaming dialogue, which performs per-framelanguage modeling",
  "max P([Txtti+1]|[Ctx<t1], [Framet], [Txtti])": "(2)for every frame from timestamp t1 t t2.Sincet2 is necessary to output answer, so we can simply learnshort text (e.g., this is not the time to answer) betweent1 t < t2. However, this approach imposes a significantburden on processing speed. For every frame, performingthe slow, recurrent, and lengthy next-token prediction witha billion-scale language model makes it extremely hard toachieve real-time video streaming dialogue. Then, this willlead to unavoidable frame skipping, which is unexpectedand detrimental for temporal alignment.Furthermore, ittaxes the limited context window of the language model,presenting challenges for modeling long contexts and man-aging GPU memory efficiently.Streaming EOS Prediction. To solve the problem men-tioned above, we first consider a more efficient per-framechatting method: simply assigning the End-of-Sequence(EOS) token as the content for chatting between t1 t <t2. However, this approach remains suboptimal. The dia-logue prompt template (e.g., [INST], [/INST], space to-kens in Llama ) still consumes a considerable num-ber of tokens per frame, which is less favorable due to thenumerous frames in streaming video. Furthermore, the ex-cessive number of EOS tokens in the sequence can signifi-cantly increase the language models perplexity, as we haveobserved in our experiments.Instead, we propose a novel training objective namedstreaming EOS prediction to address this issue. We stillassume t2 is essential for decoding language; thus, we nor-mally learn language modeling here:",
  "max P(EOS|[Ctx<t], [Framet]), where t1 t < t2": "(4)In this way, we skip a dialogue turn and learn to determinewhen it is appropriate to decode language for streaming in-puts. During inference, if EOS is predicted on a frame, thenwe can directly ask the next frame to input. Meanwhile, theEOS token is not appended to the context to prevent it fromaffecting the language modeling. Therefore, this task is notabout next-token prediction; however, it can work with au-toregressive loss to train a video streaming dialogue model.In the following, we first introduce how can we get the dataof streaming video and timestamped language annotations, then present the details about the model and the trainingprocedure.We also note that the EOS token mentioned here is notlimited to the real EOS token used in language models (e.g.,</s> in Llama). It is permissible to use any token or tointroduce a new token, provided that it is specified in thesystem prompt. We use this term solely for simplicity.",
  ". Data": "Online Annotations to Video Streaming Dialogue. Somevideo datasets, such as Ego4D narrations , are inher-ently collected in a streaming manner, with annotators pro-viding real-time narrations as they watch a 5-minute longvideo clip. However, prior research has predom-inantly focused on learning from short, discrete clips (e.g.only 32 frames), rather than in a continuous streaming con-text. For this dataset, we followed the same instructionsprovided to human annotators as our model trainingprompt. This prompt instructs the model to simulate humanannotators to streamingly generate narrations in a 5-minutevideo (about 600 frames in 2 FPS). For demo purposes (notfor experiments), we also utilize Llama-2-13B-Chat orLlama-3-8B-Instruct to rephrase the narration text, cor-recting grammatical errors and typos, and converting it intoa more understandable version (e.g., changing C does...to You do...).Offline Annotations to Video Streaming Dialogue. De-spite the Ego4D narration data being collected in a stream-ing manner, most prevalent video datasets are used to train offline models and only feature temporalsegment annotations paired with basic language descrip-tions (e.g., activities, narrations). To bridge this gap, wepropose a method for synthesizing dialogue data from thesesources. As shown in , our key idea is use LLMto generate user-assistant dialogues based on video annota-tions, involving the following steps: First, we prepare a question template library containingvarious queries about the past, present, and future tensesof the video, totaling N queries. We randomly sampleone question from the library, denoted as Qi. Then, we obtain the video annotation timeline from theoffline dataset. This usually includes timestamped lan-guage descriptions, which we organize into a languageprompt, e.g., time ta tb: boiling the water; timetc td: cutting the vegetables., denoted as A. We con-sider all the state change critical timestamps as the idealresponse times. For this example, ta, tb, tc, and td are allconsidered response times. Third, we prompt the large language model to generateresponses at every critical timestamp, e.g., ta, tb, tc, td,according to Qi and A. We can repeat this procedure foreach Qi until all queries have been processed. The re-sponses are saved for loading during training.",
  "Video Annotation Timeline": ". The streaming dialogue data generation method inour LIVE framework. We randomly insert templated questionsinto the video timeline and expose the ground-truth video anno-tations (along with their timestamps) to LLMs, prompting them toanswer the queries within a period of time. Finally, during training, we (1) randomly sample a queryand load its responses at critical timestamps, (2) randomlyinsert a query into a video timestamp tr, (3) discard theresponses that occur before tr, and add a response attr. Here different queries can be inserted into one video,which only requires discarding the responses of the pre-vious query after the new query insertion timestamp. In this way, we can generate temporally varied and free-form dialogue data within a video stream. We have pre-pared 50 questions each for past, current, and future events,totaling N = 150 queries. We use Llama-2-13B-Chat or Llama-3-8B-Instruct to generate the responses andinsert a maximum of 3 queries per training sample. Theoffline datasets we used are COIN and Ego4D Goal-Step (for demo usage), which belong to the categoriesof egocentric and instructional video datasets, aligning withour aim to develop online video assistants. Here, we do notconsider online action detection benchmarks (e.g., THU-MOS14 , TVSeries ) because they are closed-setonline classification benchmarks, and their labels are toobrief, which may cause language models to generate hallu-cinatory responses. Please refer to the supplementary mate-rial for the generated dialogue example. : _disappeared _disappeared [EOS] AI :AI[EOS][EOS] AI : _appeared [EOS] : _appearedAI[EOS][EOS] [EOS][EOS]",
  "LoRA": ". The training method in our LIVE framework. We organize the user-assistant dialogue data and video frames in temporalorder as the input sequence. To learn the model when to answer or keep silent in a video stream, we employ not only the standard languagemodeling (LM) loss but also introduce a streaming EOS prediction loss. This additional loss supervises the model when it is necessary togenerate language, enabling it to produce temporally aligned responses and reduces the redundant dialogue history.",
  ". Model Training": "Model Architecture. We illustrate the model architecturein . Similar to LLaVA , it comprises threekey components: an image encoder, an MLP projector, anda language model. For the image encoder, we utilize theCLIP ViT-L encoder (pretrained on DataComp-1B ) to extract video frame embeddings at 2 FPS. Eachvideo frame embedding has a shape of (1 + hp wp) c,where (1 + hp wp) denotes the CLS token and averagepooled spatial tokens.3The extracted frame embeddingsfrom the image encoder are then fed into MLP projector toframe tokens, as in LLaVA-1.5 . Then frame tokensare interleaved with language tokens as input to an LLM,Llama-2-7B-Chat or Llama-3-8B-Instruct . Finally,we incorporate LoRA in every linear layer of the LLMfor efficient tuning.Training Loss.As described in .1, our learn-ing objective is twofold. The first part focuses on auto- 3The experiments described in this paper are conducted without extra spa-tial tokens (i.e., hp = wp = 0), which is the most efficient setup andcan handle half-hour videos within a 4096 context window. Our releasedmodels for demo usage include 1 + hp wp = 1 + 3 3 = 10 tokens,offering better detail in dialogue but supporting shorter maximum videolengths. Despite more tokens used per frame, all models can still run atover 10 FPS for 5-minute Ego4D narration streams. regressive language modeling, aiming to maximize the jointprobability of input text sequences. The second training ob-jective involves streaming EOS prediction, which requiresthe model to remain silent when it is unnecessary to out-put responses. With these two training objectives, we havelanguage modeling (LM) loss and streaming loss terms tominimize, both employing cross-entropy loss:",
  "), (5)": "where lj and fj denote condition indicators. lj is 1 if the j-th token is a language response token, and 0 otherwise. fjis 1 if (1) the j-th token is the last token of a frame4, and (2)lj+1 = 0. In essence, the streaming EOS loss is applied toframes before responding. P [Txtj+1]jdenotes the probabil-ity on j + 1-th text token, output from the language modelhead of the j-th token, and P [EOS]jrepresents that probabil-ity for the EOS token. w is a balance term, set to w = 1 bydefault. As shown in Figuer 4, we visualize the ranges oflanguage loss and streaming loss in an input sequence whenwe only use 1 token for each frame.",
  "Delayedframes": ". Inference pipeline in our LIVE framework. Dur-ing inference, video frames serve as streaming inputs. Our modelmaintains a continuous key-value cache as the input progresses tospeed up the inference. Furthermore, we parallelize the fast videoframe encoder and the slower language model to avoid the bottle-neck in the latter. Video frame tokens can be always encoded andbuffered, no need to wait the language decoding.",
  ". Inference": "Probability Correction. The prevalence of EOS token willbias the model towards EOS token prediction. To addressthis, we introduce a threshold to correct the output prob-ability on frame tokens: EOS will not be considered asthe next token if P [EOS]j< . In practical usage, we findthat setting to 0.50.8 yields much better results than nothreshold here.Continuous Key-Value Cache. As shown in , Dur-ing inference, the video is input as a frame-by-frame stream,with a default FPS 2. Our model takes the current frame asinput and generates tokens on-the-fly. In the whole process,we use the key-value cache trick to accelerate token decod-ing, thus we do not need to manually append the generatedtokens for the next frame. As our training encourages themodel to keep silence, this continuous inference would beefficient, providing the possibility to pace with the videostream speed.Parallelization of encoding and decoding.Our videoframe encoder utilizes CLIP ViT-L (307M), which is signif-icantly smaller than the 7B/8B LLM. This size discrepancyleads to a speed mismatch, potentially resulting in frameskipping when the LLM decodes long sentences. To miti-gate this issue, we parallelize the processes and establish aFIFO queue for video frame tokens. The fast encoder doesnot need to wait the slow LLM, it just always encode thevideo frames and append them to the queue. Once the lan-guage model completes its previous frame decoding, it canfetch the frame tokens in the queue but do not delay thevideo encoding.",
  "We implement VideoLLM-online with our LIVE frame-work. It has two versions: The more efficient one, VideoLLM-online-7B-v1, us-": "ing OpenCLIP-ViT-L-224 as the video frame en-coder, a 2-layer MLP as the connector, and Llama-2-7B-Chat as the language model. Each video frame onlycosts 1 CLS token. The more effective one, VideoLLM-online-8B-v1+,using SigLIP-ViT-L-384 as the video frame encoder, a2-layer MLP as the connector, and Llama-3-8B-Instruct as the language model. Each video frame costs 1 CLS to-ken and 33 tokens by average pooling, i.e., 10 tokens perframe.By default, the experiments in this paper are conductedwith VideoLLM-online-7B-v1 due to our limited computa-tion resources. We also train a VideoLLM-online-8B-v1+model for demo purpose.For model training, all models are trained with with 2FPS sampled videos.5 We train model with LoRA toall linear layers, with a rank of 128, scaling factor of 256.For the sake of simplicity, the training is directly performedwithout vision-language aligning procedure . We alsotried to use LlaVA-1.5 to initialize our connector andLLM, but we found the performance is similar, thus wejust keep the MLP randomly initialized. For video stream-ing dialogue experiments, we train 2 epochs for the model.For the downstreaming offline experiments, we train 56epochs without pre-training for fair comparison with pre-vious methods. By default, we set streaming loss weightw = 1.0 during training.",
  ". Evaluation Setting": "Datasets. We use (1) assistance-related, instructional videodataset COIN and (2) continuous, egocentric videodataset Ego4D in various settings: Ego4D Narration Stream: We also leverage the denseEgo4D timestamp-narration to create a streaming set. Thegoal is to generate narrations timely like Ego4D humanannotators . We follow the division of the training,validation, and test set in EgoVLP . COIN+Ego4D Narration Stream: To further evaluatethe potential of models performance to free-form dia-logue, we construct a simple COIN+Ego4D Stream set,constructed from COIN annotations using our data gen-eration methods, and the above Ego4D Narration Stream.The model should remind the user when an action starts,summarizes the action when it ends, as well as forecaststhe next action. We use the same training/testing splits asCOIN benchmarks. See appendix for details. Ego4D GoalStep+Narration Stream: Due to potentialprivacy risks associated with COIN dataset collected fromYouTube videos, we have opted to use Ego4D Goal-Step for training our released model.While it is",
  ", 84, 97], we evaluate our model on six common bench-marks of the COIN dataset: step recognition, step fore-casting, task summarization, procedure forecasting, pro-cedure forecasting with a goal": "Ego4D long-term action anticipation (LTA) bench-mark: This benchmark requires to predict next Z = 20actions (verbs and nouns) for the given video of previous8 steps. We use the standard Ego4d v2 splits as in previ-ous studies .Evaluation metrics. We use the following metrics to eval-uate the model as an online video assistant: Language Modeling Metrics.We use common lan-guage perplexity to indicate the language modeling ca-pability (LM-PPL) at a given timestamp. A lower LM-PPL signifies better accuracy in answering. However, thismetric is not suitable for comparing different LLMs dueto potential variations in language tokenization. There-fore, we calculate the language generation matching ra-tio (LG-Match) to compare VideoLLM-online-7B-v1 andVideoLLM-online-8B-v1+. Note that LG-Match is cal-culated in an autoregressive order, meaning it representsthe ratio of the position of the first error token to the totalnumber of tokens. Time Difference (TimeDiff). To evaluate the temporalalignment capability of an online assistant, we calculatethe discrepancy between the timestamp of its responseand the expected timestamp for each response. We av-erage TimeDiff each turn as the metric. Fluency. Individual LM-PPL, LG-Match or TimeDiff donot entirely evaluate both language and temporal effec-tiveness in a streaming dialogue. We introduce the Flu-ency metric, which evaluates the proportion of consec-utive successful token prediction within a dialogue turn.As the token also including language tokens, Fluency cancomprehensively reflect the language modeling in an on-line streaming. We would like to note that these metrics are mainly formonitoring model performance in the streaming narrationtask. Firstly, the narration text is relatively simple, mainlycomposed of a subject, verb, and object, thus the LM-PPLand LG-Match can still work for this simple language. Sec-ondly, the streaming narration requires the human annotatorto write the description at the moment when the action statehas changed, which is very suitable for validating the timedifferences. However, these metrics are not so effective forevaluating more complicated, free-form online conversationscenarios, and this is also a common problem in evaluatingfree-form LLM generation. We leave this for future work.Baselines. To our best knowledge, we are the first one totackle producing temporal aligned, free-form language an- swer with streaming video settings. To better understandthe challenges, we build baseline models for video-text in-terleaved dialogue, per-frame dialogue, as we described in.1, with the same model architecture and trainingdetails to VideoLLM-online, differing only in their trainingobjective and multi-turn formulation.",
  ". Ablation Study": "Learning Method.a shows the ablation studieson learning methods in a streaming setting. Both vision-language interleaved and streaming methods exhibit lowperplexity loss, indicating that our proposed objective doesnot hurt language modeling capability. However, learningwith per-frame for streaming will produce significant higherLM-PPL than others, which might be attributed to the toomore single EOS token in answering that affects the origi-nal language modeling.When we turn to online metrics of TimeDiff and Fluency,streaming dialogue method yields much better results thanothers.In our observation, the first interleaved dialoguemethod always outputs language after every frame, and thesecond mutli-turn for streaming dialogue approach tends toanswer EOS token after every frame, which decreases theirperformance for streaming video inputs. Furthermore, per-frame for streaming dialogue method will significanly slowdown the training speed due to its lengthy prompts, whileour method has no negative impact on the efficiency.Streaming Loss. We continue to investigate the most suit-able strategy to learn the streaming objective. As shownin b and c, we find a default setting workssurprisingly well (CE loss, = 1.0), which demonstratesthere is no need to apply more advanced loss (e.g. FocalLoss ) to address the imbalance on EOS token.Inference Efficiency.In d, we test the infer-ence efficiency on Ego4D narration stream validation set (5minute), and report the memory cost and average FPS on asingle A100 GPU. The first interleaved dialogue method,which will output language after every video frame, hashuge memory cost and slow generation speed. The sec-ond one, per-frame dialogue for streaming that formulatesall in a multi-turn dialogue, show better efficiency than thefirst one since it can cost less tokens in redundant frames.However, this approach still lags significantly behind ourstreaming dialogue approach, which does not cost extra to-kens in redundant frames thus maintain smaller key-valuecache. We observe for most Ego4D videos, our model canrun larger than 10 FPS, providing possibility for AI assis-tants working in real-time video stream.",
  "(d) Generation memory/speed.Streaming dialogue method hasmuch better efficiency": ". Ablation experiments on Ego4D Narration Stream. We train VideoLLM-online on Ego4D narration stream training setand evaluate on its validation set. The comparison is based on our designed metrics: the ratio of strictly correct prediction tokens (Fluency),language modeling perplexity (LM-PPL) and time difference (TimeDiff) metrics. #Training Token denotes the average token length duringtraining. TimeDiff refers to difference in second. Default settings are highlighted.",
  "(b) Results on Ego4D LTA benchmark, evaluated on public server.ED@Z=20 denotes editing distance for future 20 actions": ". Experiments on COIN and Ego4D benchmarks. VideoLLM-online is finetuned on their training set, and strictly evaluatedon the test set by generated string comparison with the ground-truth text. It achieves best results among end-to-end models. achieves state-of-the-art performances in step/task summa-rization and next step/procedure forecasting benchmarksof COIN dataset .Furthermore, we also obtain thebest performance among end-to-end models evaluated onEgo4D LTA. Although the results of AntGPT are betterthan us, they used egocentric pre-trained visual feature ,and integrates lots of complex cascading methods to im-prove the forecasting results. Our VideoLLM-online, how-ever, directly outputs language as the results, which per-forms better than the similar end-to-end VideoLLM .",
  ". Performance comparison of VideoLLM-online variants": "Comparison between Model Variants.We compareVideoLLM-online-7B-v1, VideoLLM-online-8B-v1, andVideoLLM-online-8B-v1+ on the Ego4D narration streamtask. 7B and 8B refer to Llama-2-7B and Llama-3-8B, respectively, while v1 and v1+ indicate the usageof either one token per frame or multiple tokens per frame.As shown in , the enhanced language model signif-icantly improves performance across all aspects. Utilizingmore tokens per frame enhances the vision-language capa-bility, albeit with limited benefits to online performance. Visualization. In , we visualize two representa-tive examples, real-time narration and online dialogue. Themost distinctive characteristics of our approach are: (1) thedialogue process goes along with the streaming video in-put, rather than chatting based on the full video. (2) The re-sponse will be muted when it is unnecessary, significantlyimproving the overall speed of video streaming dialogue. Another example is shown in . It can be seenthat our model demonstrates strong alignment between thestreaming visual frames and the output responses. With ourefficient inference strategy, we can envision an J.A.R.V.I.S-like intelligent assistant that can assist users in real time.",
  ". Conclusion": "We propose Learning-In-Video-strEam (LIVE), a novelframework empowering LLMs to handle streaming video,to produce temporal aligned answers, hold long-contextvideo duration, and have high inference efficiency. We useLIVE to train a simple VideoLLM-online model, which notonly achieves superior capability in online/offline vision-language tasks, but also enable fast inference for an onlinevideo streaming setting. We believe enabling such abili-ties will be an important step to move towards always-ononline assistant. In future work, to make our VideoLLM-online be more general and improve its spatial capability inzero-shot prediction for downstream applications, we willexplore suitable pre-training data source, and develop mod-els that can employ more spatial tokens but without manytrade-off on speed and memory cost.",
  "Introducing meta llama 3: The most capable openly avail-able llm to date. 2024. 3, 5, 6, 7": "Huda AlAmri, Vincent Cartillier, Abhishek Das, Jue Wang,Anoop Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks,Chiori Hori, Peter Anderson, Stefan Lee, and Devi Parikh.Audio visual scene-aware dialog. In CVPR, pages 75587567, 2019. 3 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-sch, Katherine Millican, Malcolm Reynolds, Roman Ring,Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,Sina Samangooei, Marianne Monteiro, Jacob L. Menick,Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.Flamingo: a visual language model for few-shot learning.In NeurIPS, 2022. 3 Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalk-wyk, Andrew M. Dai, Anja Hauth, Katie Millican, DavidSilver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou,Julian Schrittwieser, Amelia Glaese, Jilin Chen, EmilyPitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Fi-rat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, MalcolmReynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins,Clemens Meyer, Eliza Rutherford, Erica Moreira, KareemAyoub, Megha Goel, George Tucker, Enrique Piqueras,Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Dani-helka, Becca Roelofs, Anas White, Anders Andreassen,Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lu-cas Gonzalez, Misha Khalman, Jakub Sygnowski, and etal. Gemini: A family of highly capable multimodal mod-els. arXiv:2312.11805, 2023. 2, 3, 4",
  "Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Isspace-time attention all you need for video understanding?In ICML, 2021. 9": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners. In NeurIPS, pages 18771901, 2020.1, 3",
  "Joao Carreira and Andrew Zisserman. Quo vadis, actionrecognition? A new model and the kinetics dataset.InCVPR, pages 47244733, 2017. 3": "Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, YifeiHuang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, TongLu, et al. Videollm: Modeling video sequence with largelanguage models. arXiv preprint arXiv:2305.13292, 2023.9 Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, ZechunLiu,Pengchuan Zhang,Raghuraman Krishnamoorthi,Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.Minigpt-v2: large language model as a unified interfacefor vision-language multi-task learning. arXiv:2310.09478,2023. 3",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: pre-training of deep bidirectional trans-formers for language understanding.In NAACL, pages41714186, 2019. 3": "Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, ZhengGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,Haoran Wei, et al. Dreamllm: Synergistic multimodal com-prehension and creation. arXiv:2309.11499, 2023. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold,Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-age is worth 16x16 words: Transformers for image recog-nition at scale. In ICLR, 2021. 6 Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, JonathanHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-gad, Rahim Entezari, Giannis Daras, Sarah M. Pratt, VivekRamanujan, Yonatan Bitton, Kalyani Marathe, StephenMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,Pang Wei Koh, Olga Saukh, Alexander Ratner, ShuranSong, Hannaneh Hajishirzi, Ali Farhadi, Romain Beau-mont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Car-mon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp:In search of the next generation of multimodal datasets.arXiv:2304.14108, 2023. 6, 7",
  "Rohit Girdhar and Kristen Grauman.Anticipative videotransformer. Arxiv, 2106.02036, 2021. 3": "GPT-4o. Hello gpt-4o, 2024. 2 Kristen Grauman,Andrew Westbury,Eugene Byrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Mar-tin, Tushar Nagarajan, Ilija Radosavovic, Santhosh KumarRamakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray,Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Sid-dhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane,Tien Do, Morrie Doulaty, Akshay Erapalli, ChristophFeichtenhofer, Adriano Fragomeni, Qichen Fu, AbrhamGebreselasie, Cristina Gonzalez, James Hillis, XuhuaHuang, Yifei Huang, Wenqi Jia, Weslie Khoo, JachymKolar, Satwik Kottur, Anurag Kumar, Federico Landini,Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Man-galam, Raghava Modhugu, Jonathan Munro, Tullie Mur-rell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes,Merey Ramazanova, Leda Sari, Kiran Somasundaram, Au-drey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo,Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, YunyiZhu, Pablo Arbelaez, David Crandall, Dima Damen, Gio-vanni Maria Farinella, Christian Fuegen, Bernard Ghanem,Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, KrisKitani, Haizhou Li, Richard A. Newcombe, Aude Oliva,Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi,Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani,Mingfei Yan, and Jitendra Malik. Ego4d: Around the worldin 3, 000 hours of egocentric video. In CVPR, pages 1897318990, 2022. 3, 5, 7, 9, 4 Kristen Grauman, Andrew Westbury, Lorenzo Torresani,Kris Kitani, Jitendra Malik, Triantafyllos Afouras, KumarAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote,Eugene Byrne, Zachary Chavis, Joya Chen, Feng Cheng,Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, MaraEscobar, Cristhian Forigua, Abrham Gebreselasie, SanjayHaresh, Jing Huang, Md Mohaiminul Islam, Suyog DuttJain, Rawal Khirodkar, Devansh Kukreja, Kevin J. Liang,Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Mar-tin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ra-gusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Ar-jun Somayazulu, Yale Song, Shan Su, Zihui Xue, Ed-ward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen,Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, PrinceGupta, Jiabo Hu, Yifei Huang, Yiming Huang, Wes-lie Khoo, and et al.Ego-exo4d: Understanding skilledhuman activity from first- and third-person perspectives.arXiv:2311.18259, 2023. 1",
  "Heikki Hyyro. Explaining and extending the bit-parallelapproximate string matching algorithm of myers. 2001. 4": "Haroon Idrees, Amir R. Zamir, Yu-Gang Jiang, Alex Gor-ban, Ivan Laptev, Rahul Sukthankar, and Mubarak Shah.The THUMOS challenge on action recognition for videosin the wild.Comput. Vis. Image Underst., 155:123,2017. 5 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray, AlecRadford, Jeffrey Wu, and Dario Amodei. Scaling laws forneural language models. arXiv:2001.08361, 2020. 1",
  "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and LiYuan. Video-llava: Learning united visual representationby alignment before projection. arXiv:2311.10122, 2023.2, 3, 4": "Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin,Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, LinLiang, Zicheng Liu, Yumao Lu, Ce Liu, and LijuanWang. MM-VID: advancing video understanding with gpt-4v(ision). arXiv:2310.19773, 2023. 2, 4 Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan,Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao,Rongcheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai,Hongfa Wang, Dima Damen, Bernard Ghanem, Wei Liu,and Mike Zheng Shou. Egocentric video-language pretrain-ing. arXiv:2206.01670, 2022. 5, 7, 9",
  "Esteve Valls Mascaro, Hyemin Ahn, and Dongheui Lee.Intention-conditioned long-term human egocentric actionanticipation. In WACV, pages 60376046, 2023. 9, 4": "Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,MakarandTapaswi,IvanLaptev,andJosefSivic.Howto100m: Learning a text-video embedding by watch-ing hundred million narrated video clips. In ICCV, pages26302640, 2019. 5 Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, TusharNagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Any-mal: An efficient and scalable any-modality augmented lan-guage model. arXiv:2309.16058, 2023. 2 Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang,Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, andPing Luo. Embodiedgpt: Vision-language pre-training viaembodied chain of thought. arXiv:2305.15021, 2023. 2, 3,4",
  "OpenAI.Gpt-4v(ision) system card. 2, 3, 4": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-roll L. Wainwright, Pamela Mishkin, Chong Zhang, Sand-hini Agarwal, Katarina Slama, Alex Ray, John Schulman,Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,Amanda Askell, Peter Welinder, Paul F. Christiano, JanLeike, and Ryan Lowe. Training language models to fol-low instructions with human feedback. In NeurIPS, 2022.1, 3",
  "et al. Llama: Open and efficient foundation language mod-els. arXiv:2302.13971, 2023. 1, 4, 3": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer,Moya Chen, Guillem Cucurull, David Esiobu, Jude Fer-nandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,Viktor Kerkez, Madian Khabsa, Isabel Kloumann, ArtemKorenev, Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, YuningMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, RuanSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,Yuchen Zhang, Angela Fan, Melanie Kambadur, Sha-ran Narang, Aurelien Rodriguez, Robert Stojnic, SergeyEdunov, and Thomas Scialom. Llama 2: Open foundationand fine-tuned chat models. arXiv:2307.09288, 2023. 3, 4,5, 6, 7",
  "Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, andCordelia Schmid. Vidchapters-7m: Video chapters at scale.In NeurIPS, 2023. 5": "Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-toine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, andCordelia Schmid. Vid2seq: Large-scale pretraining of a vi-sual language model for dense video captioning. In CVPR,pages 1071410726, 2023. 2, 3, 4 Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang,Heyang Qi, Olatunji Ruwase, Ammar Ahmad Awan,Samyam Rajbhandari, and Yuxiong He.Deepspeed-visualchat: Multi-round multi-image interleave chat viamulti-modal causal attention. arXiv:2309.14327, 2023. 2,3, 4 Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-FuChang, and Yinfei Yang. Ferret: Refer and ground anythinganywhere at any granularity. arXiv:2310.07704, 2023. 2, 3",
  "Supplementary Material": "This supplementary material includes following sections: Section A provides an analysis of per-frame chatting.More specifically, we prompt GPT-4V for video stream-ing dialogue and compare it with interleaved vision-language dialogue and our method. Section B elaborates on data details, especially onprompts, including examples of Ego4D Narration Stream,COIN Dialogue Stream, training and inference prompts,and evaluation schemes for COIN benchmarks andEgo4D LTA.",
  "A. Analysis to Per-frame Chatting": "As shown in , we prompt GPT-4V to do the real-time narration task. In ideal case, we hope the model justoutput narration like cutting vegetables at the first frame,since these frames are nearly no change. We use two meth-ods of prompting: (1) no prompting restriction: this promptallows the GPT-4V to output language at every frame, with-out consideration on the conciseness. See left part,we can observe that the response of GPT-4V is very lengthy,making it impossible for real-time usage; (2) with strongprompting restriction: the right part of the figure suggeststhat GPT-4V can be prompted to approach the video stream-ing dialogue. However, it is still per-frame dialogue andstill cost tokens and times per frame. Moreover, we find itis not so stable; sometimes there would be obvious halluci-nation that may not be appeared in GPT-4V level, like youare peeling vs. you have stopped at the first and secondframe.",
  "B.1. Data Construction": "COIN Stream Set. This set is derived from COIN anno-tations, adapted using our streaming dialogue generationschemes. Initially, a user query outlines the videos over-all task, prompting the model to track and record the activ-ities shown. The model is then required to watch the videoand provide real-time responses. An example of this pro-cess is provided in Section B.2. Its important to note thatthis dataset for experiment has a relatively fixed structure for stable evaluation, i.e., the user query occurs only at thebeginning, which simplifies the evaluation process. How-ever, the models use for demo, as depicted in ofthe paper, is trained with randomized queries, timestamps,and varying numbers of turns.Ego4D Narration Stream Set. The annotation process forEgo4D Narration inherently follows a streaming dialogueformat. Initially, videos are segmented into clips, each witha maximum duration of five minutes, for the purpose of ac-quiring narrations. Annotators are then tasked with provid-ing a concise summary narration, typically 1-3 sentenceslong, for each clip. Once they have established an overallunderstanding of the clip, they proceed to write detailed,play-by-play descriptions of the actions. Here we only usethe second part, i.e. the streaming narration.The train-ing and inference prompts for Ego4D narration, which areadapted from the original text guidelines provided to anno-tators, are detailed in Section B.2. Note the narration forexperiments are not refined by Llama; we use the originalnarration for stability (but remove special strings like #C,#O) .",
  "B.2. Training and Inference Prompt": "System Prompt. We have a simple system prompt at thebeginning of the dialogue:A user wears AR glasses equipped with an intelligent as-sistant. The AR glasses continuously receive streamingvideo frames from the users viewpoint, enabling the as-sistant to observe and provide real-time assistance in re-sponse to the users queries when necessary. Below istheir dialogue, accompanied by streaming video framesincluded in the users query.In the following, we use [System] to denote it.Frame Placeholder. In our training, each video frame isinitially encoded using frozen CLIP ViT. These encodedframes are then projected into a language token-compatiblespace through a learnable MLP. We use [F] to denote to-kens per frame. In our paper experiments, the number of to-kens per frame is |[F]| = 1 for fast training/validation andparameters searching. For our demo, we use |[F]| = 10,i.e. 1 CLS + 3 3 average pooled spatial token for bettervision understanding ability.Streaming Dialogue Examples. To illustrate the streamingdialogue format more clearly, we provide examples of train-ing prompts from our generated COIN Dialogue Stream setand our curated Ego4D Narration Stream set. In these ex-amples, tokens related to the streaming objective are high-lighted in blue, while tokens associated with the languagemodeling objective are marked in orange.We ignored",
  "some chat template strings (e.g., [INST], [/INST] inLlama ) for better visualization": "COIN Stream Example:[System]User: The video is about to install ceiling fan. Pleaseremind me when the related action starts, summarizeswhen it ends, as well as forecasts the next action.[F][F]...[F][F]Assistant:Nowdoingthestep to close switch.Then try to install fantray.[F][F]...[F][F]Assistant:Justfinishedthe step to close switch. Then try to install fan tray.[F][F]...[F][F]Assistant: Now doing the step toinstall fan tray.Then try to install fans and lights.",
  "COIN Procedure Forecasting with Task Goal:[System][F][F] [F][F]User:What are the next 2 steps to hang wallpa-per?[BenchEval]Assistant: Wipe or polish the wall; crop the wallpaper": "COIN Action Segmentation:[System]User: Please output the corresponding action of eachframe.[BenchEval][F][F] [F][F]Assistant:Showtheblankpaper.[F]Assistant:Showtheblankpa-per.[F] [F][F] [F][F]Assistant:Showthe money to the audience.[F]Assistant:Show themoney to the audience.[F] [F][F] Ego4D LTA:[System][F][F] [F][F]User: What are the next 20 steps?[BenchEval]Assistant: apply flour; attach dough; knead dough; takedough; put dough; remove dough; knead dough; takedough; put dough; move dough; apply flour; kneaddough; take dough; put dough; move table; apply flour;knead table; take dough; put dough; move dough.",
  "B.3. Evaluation Scheme": "We detail our methodology for evaluating performance onexisting benchmarks.COIN Benchmarks. Following the approach in , we report top-1 accuracy for the COIN benchmarks. Aunique challenge arises with Online-VideoLLM, as it pro-duces outputs in natural language rather than class indices.To address this, we employ a simple string matching tech-nique: we compare the models language output with theCOIN taxonomy dictionary to assign class indices, whichare then used to calculate accuracy. Outputs not found inthe taxonomy dictionary are automatically considered in-correct. For computing frame-wise accuracy in COIN ac-tion segmentation mask, we apply a similar method.For procedures involving multiple steps, we need to cal-culate step-wise accuracy.We employ a straightforwardapproach using string comparison to identify verb/noun in-dices. As noted in our training prompts, actions are sep-arated by a semicolon ;.Thus, we split the model-generated content using this delimiter to extract the textscorresponding to the 20 steps.",
  ". Online chatting demo of VideoLLM-online": "Ego4D LTA. The Ego4D LTA benchmark, as outlinedin , utilizes Edit Distance (ED) as its evaluation metric,as described in . ED quantifies the minimum numberof operations needed to transform one string into another.In contrast to previous works (e.g., )that used a classification paradigm and calculated ED basedon predicted verb/noun indices, our Online-VideoLLM sys-tem, which exclusively generates text, presents challenges in metric calculation. Additionally, the method we used forevaluating on COIN Benchmarks is limited to producing re-sults for either a single step or an overall procedure, not formore complex text outputs. To derive verb/noun indices from our models outputs,we use a straightforward method involving string splittingand comparison. As outlined in our training prompts, ac-tions are separated by a semicolon ;. We use this delim-",
  ". Joint training of COIN Dialogue Stream and Ego4D Nar-ration Stream. LIVE consistently performs better than per-framedialogue method": "iter to split the model-generated content into the text foreach of the 20 steps. If the split results in more or fewerthan 20 steps, we adjust by adding none for padding or byclamping the excess steps, respectively. Next, we constructa dictionary that maps action text to their correspondingverb/noun category indices, a task facilitated by the avail-able taxonomy annotations. Finally, this dictionary is usedto convert the generated text into verb/noun category in-dices, which are then employed to calculate the Edit Dis-tance (ED).",
  "C. More Results": "Streaming Dialogue.As shown in , we evalu-ate our model on joint COIN and Ego4D streaming set.COIN Stream is built by our streaming dialogue genera-tion method, while the Ego4D narration stream simulatesEgo4D annotators to write the narration while watching thevideo . From the table, we can see our method hasthe similar language modeling ability (reflected by LM-PPL) with the per-frame video-language dialogue format,but achieves huge advantages in fluency and time differ-ence, which suggests better support for streaming videos.Demo Results with More Tokens. shows ourdemo results, supported by model trained with 1 + 3 3tokens per frame. Though we do not show evaluation per-formance for more spatial tokens in our paper, we observetheir quantitative results are much better than 1 token. Wewill update the results in our github repository.",
  "D. Limitations": "Our primary limitation lies in the inadequacy of high-quality streaming dialogue data, which hinders its gener-alization capability. The dialogues generated in our methodare derived from existing video datasets, which cannot cap-ture the complex and varied requirements of real-worldusers. We observe the method can overfit when training ona small dataset. Our future efforts are scaling the methodon larger datasets or ASR texts in streaming video.Furthermore, we also find that the spatial ability is notstrong due to its less spatial token. In the future, we willseek better trade-off strategy to balance spatial and tempo-ral dimensions in video streaming dialogue."
}