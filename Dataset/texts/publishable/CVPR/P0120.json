{
  "Abstract": "When adopting a deep learning model for embodiedagents, it is required that the model structure be optimizedfor specific tasks and operational conditions. Such opti-mization can be static such as model compression or dy-namic such as adaptive inference. Yet, these techniques havenot been fully investigated for embodied control systemssubject to time constraints, which necessitate sequentialdecision-making for multiple tasks, each with distinct infer-ence latency limitations. In this paper, we present MoDeC,a time constraint-aware embodied control framework usingthe modular model adaptation. We formulate model adap-tation to varying operational conditions on resource andtime restrictions as dynamic routing on a modular network,incorporating these conditions as part of multi-task objec-tives. Our evaluation across several vision-based embod-ied environments demonstrates the robustness of MoDeC,showing that it outperforms other model adaptation meth-ods in both performance and adherence to time constraintsin robotic manipulation and autonomous driving applica-tions.",
  ". Introduction": "In the literature on embodied artificial intelligence (embod-ied AI), where deep learning models have been increasinglyadopted, optimizing the deep learning model structure forspecific tasks and operational conditions becomes crucial.Several studies focus on static model optimization and com-pression . On the other hand, there are a fewstudies that investigate dynamic model structures, which aredesigned to adapt more effectively to conditions that changeover time . These dynamic approaches,often known as adaptive inference, are particularly bene-ficial for embodied environments where surrounding con-ditions evolve and the agent models capabilities to adaptin real-time are essential. Yet, a substantial gap remains inthe exploration of these approaches within the context of",
  "*Equal contribution.Honguk Woo is the corresponding author": "embodied control systems, especially those operating un-der strict time constraints and involving more than a singletask. Specifically, the systems are distinct in their require-ment for sequential decision-making across multiple tasks,each with its own latency limitations. For example, consideran autonomous driving agent. This agent must continuouslymake driving control decisions in response to moving ob-stacles. Each decision must be made rapidly and accurately,as any delay can significantly impact driving safety and ef-ficiency due to the time-sensitive nature of driving tasks. To address the challenges faced in such time-constrainedembodied environments, we present MoDeC (MoDel adap-tation for time constrained Embodied Control), a novelmodular network framework with efficient and adaptive in-ference capabilities. In embodied AI, where models are de-ployed on specific hardware with different limitations, theability to rapidly adapt to both operational resource con-ditions and time constraints is crucial. Recognizing this,we employ a constraint-aware modular model architecture,which can transform the procedure of time-sensitive infer-ence into effective module selection within a single mod-ular network that can be deployed on different target de-vices. This approach enables dynamic model adaptation tovaried operational conditions in a sample-efficient way. Wealso use a meta-learning scheme combined with knowledgedistillation for restructuring the module selection in non-iterative time-sensitive computations. Through intensive experiments with several embodiedcontrol scenarios such as robot manipulation tasks in Meta-world , autonomous driving tasks in CARLA , andobject navigation tasks in AI2THOR , we demonstratethat our MoDeC framework is applicable for different timeconstraints and devices, achieving robust adaptation perfor-mance in terms of both constraint satisfaction and modelaccuracy. For instance, MoDeC shows a performance gainof 14.4% in success rates over the most competitive base-line, DS-Net for autonomous driving tasks in CARLA,while it keeps the violation of time constraints to be lessthan 1%. The main contributions of this paper are summa-rized as follows.",
  "work MoDeC, specifically designed for dynamic multi-task model adaptation to time constraints and device re-source specifications": "We devise an efficient joint learning algorithm for op-timizing the combinatorial module selection in a modeland stabilizing the model against the large action spaceand non-stationarity problems. We also employ thedistillation-based inference optimization. We evaluate the framework with several embodied en-vironments and embedded devices, demonstrating its ro-bustness and adaptability in terms of time-sensitive infer-ence performance upon a wide range of tasks and opera-tional conditions.",
  ". Related Work": "Embodied AI. Achieving an embodied agent requireslearning complex and diverse tasks, as well as adapting toa constantly changing, real-world environment. Many re-searchers focused on complex tasks in embodied environ-ments, including object navigation , andembodied question and answering , as wellas model transfer from simulation to deployment environ-ments . Specifically, introduced SplitNet,decoupling visual perception and policy learning. By de-composing the network architecture into a visual encoderand a task decoder, it allows for rapid adaptation to new do-mains and vision tasks. Li et al. also presented a learn-ing framework that can adjust a learned policy to the targetenvironment that differs from the training environment, uti-lizing unlabeled data from the target. While sharing the sim-ilar goal to adapt to different embodied environment con-ditions with the prior works, we focus on dynamic modeladaptation to both time constraints and device limitations inthe context of multi-task policy learning and inference.Real-time model inference. Several works have been in-troduced in the realm of real-time model inference . Specifically, Cai et al. explored thetrade-off between model performance and inference effi-ciency, by selecting certain nodes of the network and someof various filter-size CNN layers. Li et al. introducedDS-Net, where weight ratios within each convolution neuralnetwork determine the network slicing for optimized infer-ence. Unlike these works that enable the model adaptationfor a given static condition, our work considers instance-wise operational conditions that can be given as input to themodel for adaptation.Model adaptation. Various works for quickly adapting amodel to different environment features have been pre-sented; e.g., for task difficulty levels ,unseen tasks , and embodied proper-ties . Wang et al. proposed a framework thatselectively skips CNN layers and channels within layersbased on the task difficulty. They also exploited an early exit",
  ". Overall Architecture": "mechanism for resource-constrained inference. Han et al. introduced a training algorithm and architecture thatcan adjust the latency by pixel-wise masking of CNNs, em-ploying the latency prediction model and dynamic adjust-ment of hyperparameters for different devices. In line withthese dynamic model adaptation works, we employ a modu-lar network architecture and learning algorithm specificallydesigned for embodied control systems with multiple tasks,time constraints, and different device specifications.",
  ". Problem Formulation": "We consider reinforcement learning (RL) for multi-autonomous control tasks in embodied environments. As anindividual task is formulated as a single Markov decisionprocess (MDP) M, a multi-task MDP is equal to a familyof MDPs {Mi = (S, A, Pi, Ri, )}i. S is a set of states,A is a set of actions, P : S A S is a transi-tion probability, R : S A R is a reward function, and is a discount factor. In multi-task RL, task informationis used to reformulate a family of MDPs in a single MDP.Accordingly, given a task index i I, such a reformulatedMDP can be represented as (S I, A, PI, RI, ) where",
  ". Overall Approach": "As illustrated in , we address the problem of multi-task RL with time constraints for embodied control, by em-ploying the dynamic module selection in a multi-task mod-ular network. Our framework MoDeC includes three com-ponents: a modular base network for learning diverse tasks,a module selection network for performing adaptive infer-ence under given time constraints, and a device adapter forconfiguring the module utilization according to the resourceavailability of a specific target device.To achieve an adaptive policy, our approach utilizes amodular base network structure. This structure is flexible,allowing for direct adjustment of the computational load(in FLOPs) through selective module activation. It empow-ers the system to effectively balance the trade-off betweenaccuracy and inference time (delay), thus enabling thetime-sensitive robust model inference. To implement thisstructure, we adopt the soft modularization technique ,which dynamically determines the weight of the path be-tween learning modules for given task information. Weimplement the module selection network to determine themodules of the base network for each inference. This facil-itates instance-wise computational adaptability, by takingtask information and module utilization as input. By jointlearning with the base network in a multi-task environment,the module selection network learns to determine the effec-tive combination of modules for a specific task under theaccuracy and inference time trade-off. Finally, to adapt totime constraints for each device, the device adapter convertsthe constraints into tolerable module utilization. This allowsMoDeC to directly use the constraints for inference. Eachdevice adapter is tailored for its own target device throughfew-shot learning.During the model deployment on a specific target de-vice, the device adapter infers the appropriate module uti-lization that adheres to given constraints. Then, the module utilization is used as input to the module selection networkthat determines the modules to use for each input instance(i.e., each visual state for a multi-task embodied RL agent).Individual instances contain different task information, andeach can be combined with the module utilization so as tomake effective decisions on the module selection.",
  ". Learning A Modular Network": "We describe the joint learning procedure for the modularbase network and the module selection network.Modular base network. To achieve a multi-task RL model,we employ soft modularization , a composite structurewith a modular network and a soft routing network. Themodular network infers actions based on state s, and thesoft routing network infers the weights of paths in the mod-ular network based on both state s and task index . Weadd a module index set to use m as input to the base net-work. The base network uses only the modules specified inm at inference. For batch B = {(s, a, )i}in from replaybuffer Dbase, we obtain a pre-trained base network base byoptimizing multi-task loss LMTRL defined as",
  "T exp().(4)": "Joint learning. To enable an embodied agent to quicklyadapt to time constraints, we jointly optimize the pre-trainedbase network and the module selection network. The mod-ule selection network infers module selection m based onthe state s, task-specific information , and the number ofmodules to use K. The combination of the base network andmodule selection network can adjust the inference time bytaking the number of modules to use as input and partiallyactivating the modules of the base network.In the process of joint learning, two problems arise: (1)the combinatorial optimization problem specific to mod-ule selection, and (2) the non-stationarity in concurrent RLtraining. The former comes from an exponentially largeaction space in module selection, significantly degradingthe performance and learning efficiency. For instance, with16 modules, potential combinations reach approximately105, complicating RL exploration and increasing sampleamounts . Furthermore, in joint learning, the interactionbetween the base network and the module selection networkleads to a non-stationary learning environment .",
  "Few-shot data": ". Learning Procedure of MoDeC. On the left side of the figure, the base network and the iterative module selection networkare jointly leaned through a reward function Rims. The iterative module selection network then distills into a single-step decision moduleselection network, as shown in the middle side. Finally, as depicted on the right side, the device adapter utilizes few-shot samples toassociate the inference time with the number of modules (module utilization), effectively transforming the constraint representation into aspecific number of modules to use for different devices.",
  "end loop": "number of modules to use, ensuring that it does not vio-late the constraint. By using the adapter, time constraintsare directly grounded as values within the network, enablingMoDeC to perform the constraint-aware inference.To train the device adapter da, we use a pre-trained basenetwork base and distilled module selection model ms witha loss function specifically designed to accommodate theconstraints of the current device. For a given device, wecollect a model inference dataset Dda and sample batch de-noted as B = {(K, c)i}i<n, where K is the number of mod-ules to use and c is the inference time of MoDeC when us-ing only K modules. The device adapter is optimized byLDA.",
  "i=1ims(s, , K, m0:i1).(5)": "To train ims, we directly evaluate each selection, leveraginga reward function based on the similarity in actions inferredby the base network base.Given an action inferred through utilizing the en-tire modules, the reward function Rims is defined fromthe difference in distance between base(s, , mfull) andbase(s, , m0:i) subsequent to the previous module selec-tion step:",
  "Rims(base, s, , m0:i) = Dist(i 1) Dist(i)(6)": "where Dist(i) = ||base(s, , mfull)base(s, , m0:i)||. Thisreward function not only accelerates the learning of the it-erative module selection network but also minimizes the re-gret bounds of the actions generated by the base network.When representing R for given task as an L-Lipschitzfunction, we can obtain the upper bound of the differenceof rewards in a multi-task environment.",
  "L Dist(i)(7)": "Thus, by minimizing Dist(K), the difference in rewards inEq. (6) is also minimized. This allows the actions inferredusing a subset of modules to closely approximate the opti-mal reward.To mitigate performance drops caused by the non-stationary problems, we avoid dramatic changes in actionsbetween the pre-trained base network and the fine-tunedbase network by using a regularization loss. Let B=",
  ". Distillation-based Optimization": "We describe two schemes tailored for device-specific adap-tation, the knowledge distillation for the module selectionnetwork and the few-shot learning for the device adapter. Toenhance the efficiency of the module selection network, wereconstruct it with single-step inference through knowledgedistillation. While the iterative module selection networkshows superior performance in the large action space, itsinference often incurs excessive delays and computationalloads compared to the base network. The module selectionnetwork, denoted as ms, takes a B = {(s, , K)i}i<n fromreplay buffer Dms as input in a single step. To train msbased on ims, we use LMTRL in Eq. (3), with the knowledgedistillation loss LKD defined as",
  ". Environments": "Meta-world. We use the MT10 benchmark (i.e., 10 differ-ent control tasks) in Meta-world , where each task isgiven a specific manipulation objective such as opening adoor or closing a window. We compare the performance ofrobot manipulation tasks under time constraints.CARLA. To demonstrate mission-critical scenarios wherethe inference time is of critical importance, we use theautonomous driving simulator CARLA . Models aretrained for autonomous driving tasks with vision-basedstates at a multi-task configuration with 12 different maps.AI2THOR. We use AI2THOR , where an agent navi-gates the map with egocentric vision states, placing variousobjects to complete a rearrangement task. The simulationenvironments are represented in .In our evaluation, we test several embedded devices,each with distinct resources and computational capabilities.The devices include Nvidia Jetson Nano (Nano), Nvidia Jet-son Xavier NX 8GB (Xavier), and Nvidia Jetson AGX Orin32GB (Orin), with the Nano being the least powerful, fol-lowed by the Xavier and the Orin being the most powerful.By testing on these devices with varying levels of capabil-ities, we can better understand how our framework adaptsto different resource limitations. This is crucial in embod-ied AI, where deployment environments can greatly vary",
  ". Comparisons": "We use several dynamic model adaptation methods as base-lines. Dynamic Routing Network (DRNet) is a networkcomprising serially connected cells, each correspondingto a directed acyclic graph of nodes. It optimizes by learn-ing to select paths between the nodes through a loss func-tion that balances the inference time and performance.Unlike MoDeC which adapts a single model to differ-ent conditions, we use individual networks specificallylearned for each constraint condition. We consider DRNetas a baseline for dynamic (adaptive) inference models. Dynamic Deep Neural Networks (D2NN) is a mod-ular neural network that exploits the accuracy-efficiencytrade-off. It exploits RL in module selection, using the re-wards calculated according to performance and inferencetime. Unlike MoDeC, we use individual networks specif-ically learned for each constraint condition. We considerD2NN as an RL baseline tailored for trade-off conditions. Dynamic Slimmable Network (DS-Net) is a dynamicnetwork model, in which an internal gater network deter-mines the weight ratio for convolution neural layers toslice the network for inference. To align with our prob-lem formulation, we modify the gater network (taking theratio as input) in a way of conducting instance-wise dy-namic inference, similar to our approach. In our compari-son, DS-Net serves as a baseline for adaptive models thathandle multiple constraints within a single policy.",
  "ms98.0%8.2%98.1%79.1%": "erarchical policy to dynamically adjust the module us-age. The low-level policy consists of two models, eachwith a small and large scale, and the high-level policy de-termines which policy to use. To adapt to various time-constrained conditions, we include a wider range of low-level policies, each with varying inference time. We useRL-AA as a baseline for resource-adaptive RL methods.",
  "Meta-world Single-task. For 5 individual tasks in MT10, shows the performance under various time con-": "straints (in the column of Constraint), achieved by ourMoDeC and the baselines (DRNet, D2NN, DS-Net, RL-AA). Specifically, we evaluate the average success ratio andcomputation load (in FLOPs) within the constraint viola-tion rate of 1% for 3 different devices (in the column ofConstraint). As shown, MoDeC achieves superior per-formance for most configurations. Compared to D2NN,the most competitive baseline, MoDeC achieves a 25.1%gain. While sharing a common base network structure withD2NN, MoDeC shows better performance, as it employsthe iterative module selection and distillation. More impor-tantly, D2NN needs to be retrained for each configuration(i.e., each constraint and device setting). MoDeC achievesthis performance superiority across different configurations,using only a single model without retraining, demonstrat-ing its adaptation capabilities to different time and resourceconstraints. Meta-world Multi-task. compares the perfor-mance of the MT10 multi-task. MoDeC demonstrates con-sistently its performance superiority, achieving an averageperformance gain of 5.7% over D2NN, the most compet-itive baseline. This specifies the adaptation capabilities ofMoDeC, achieved not only through module selection but also through multi-task learning for the base network.CARLA. shows the performance for autonomousdriving tasks across 12 different maps in CARLA, wheredelayed inference often degrades the performance and posesrisks; we implement such a strategy that upon a constraintviolation (i.e., inference delay), the action at the previoustimestep is reused. MoDeC shows 14.4% higher perfor-mance than DS-Net, which is the most competitive compar-ison in this experiment. Due to the direct impacts of con-straint violations in CARLA, the adaptive inference is morebeneficial, compared to the Meta-World tasks. This leads tobetter performance by the methods capable of constraint-aware inference, such as ours and DS-Net. DS-Net showsa significant performance drop in Nano, which is a smallmemory device; DS-Net requires a large computation loadper single layer, unlike ours.AI2THOR.comparestheperformanceforAI2THORs complex navigation tasks, where Seen refersto initial object positions encountered during training andUnseen refers to those not during training. Both MoDeCand DS-Net perform well in the seen configurations, butMoDeC demonstrates significantly better performance inthe unseen configurations, showing a performance gap rang-ing from 15.4% to 70.8%. In MoDeC, the soft modular-ization combined with module selection facilitates effectivemodule combinations for different tasks and constraints,rendering robust performance in unseen configurations.",
  ". Effect of Distillation": "Distillation. clarifies the effects of distillation,where MoDeC-I denotes a MoDeC variant without distil-lation, which adopts only the iterative module selection net-work; MoDeC-O denotes another variant, which directlylearns the single-step model selection (without distillation).As shown, there is a significant difference in inference timebetween MoDeC and MoDeC-I as the module utilizationincreases in (b), while MoDeC (with distillation) achieveshigher performance in (a). This is because the iterative mod-ule selection network is learned to infer as closely as possi-ble to the original action under a limited module utilization,excluding environment rewards. When using both environ-ment rewards and action distance, we observe a decline in the performance of the iterative module selective network.Due to changes in the environment reward, the actions ofthe iterative module selection network are not properly eval-uated. The performance decline in MoDeC-O stems fromlearning the module selection, which requires extensive ex-ploration, yet is difficult in a single step without distillation.",
  ". Effect of Base Network Architecture": "Base network architecture. illustrates the effectsof the modular base network architecture, where its numberof layers can be configured differently. In our implemen-tation, the base network has 4 layers with 4 modules perlayer, each represented as 4 4 in the figure. We comparethis with other variants, 2 8 and 8 2, in Meta-World.MoDeC shows the best performance by 4 4, which is ahyperparameter in the base network architecture.",
  ". Conclusions": "In this work, we presented MoDeC, which allows embodiedagents to effectively adapt to time constraints on differenttarget devices. The modular multi-task learning in MoDeCenables adaptive inference to a wide range of operationalconditions including device resources, time constraints, andtask specifications of embodied agents multi-task settings,by dynamically adjusting the inference within a singlemodel to satisfy the operational requirement. Through ex-periments with manipulation, autonomous driving, and ob-ject navigation scenarios of embodied agents, we verifiedthat MoDeC is capable of handling those control tasksthrough rapid model adaptation to various operational con-ditions that can change over time. Our future work is totackle the challenge of learning complex constraints frominstructions, including safety and resource limitations.",
  "Acknowledgements": "We would like to thank anonymous reviewers for their valu-able comments and suggestions. This work was supportedby Institute of Information & communications Technol-ogy Planning & Evaluation (IITP) grant funded by the Ko-rea government (MSIT) (No. 2022-0-01045, 2022-0-00043,2021-0-00875, 2020-0-01821, 2019-0-00421) and by theNational Research Foundation of Korea (NRF) grant fundedby the MSIT (No. RS-2023-00213118). Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and VenkateshSaligrama. Adaptive neural networks for efficient inference.In Proceedings of the 34th International Conference on Ma-chine Learning (ICML), pages 527536. PMLR, 2017. 1,2",
  "Shaofeng Cai, Yao Shu, and Wei Wang. Dynamic routingnetworks. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, pages 35883597,2021. 1, 2, 6": "Tommaso Campari, Paolo Eccher, Luciano Serafini, andLamberto Ballan. Exploiting scene-specific features for ob-ject goal navigation. In Proceedings of the 16th EuropeanConference on Computer Vision (ECCV), pages 406421.Springer, 2020. 2 Chin-Jui Chang, Yu-Wei Chu, Chao-Hsien Ting, Hao-KangLiu, Zhang-Wei Hong, and Chun-Yi Lee.Reducing thedeployment-time inference control costs of deep reinforce-ment learning agents via an asymmetric architecture.InProceedings of the 38th IEEE International Conference onRobotics and Automation (ICRA), pages 47624768. IEEE,2021. 2, 6 Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Ab-hinav Gupta, and Russ R Salakhutdinov. Object goal naviga-tion using goal-oriented semantic exploration. In Proceed-ings of the 34th Conference on Neural Information Process-ing Systems (NeurIPS), pages 42474258, 2020. 2 Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee,Devi Parikh, and Dhruv Batra. Embodied question answer-ing. In Proceedings of the 29th IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 110,2018. 2 Alexey Dosovitskiy, German Ros, Felipe Codevilla, AntonioLopez, and Vladlen Koltun. Carla: An open urban drivingsimulator. In Proceedings of the 1st Conference on RobotLearning (CoRL), pages 116. PMLR, 2017. 1, 6 Heming Du, Xin Yu, and Liang Zheng. Learning object re-lation graph and tentative policy for visual navigation. InProceedings of the 16th European Conference on ComputerVision (ECCV), pages 1934. Springer, 2020. 2 Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach,Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell.Speaker-follower models for vision-and-language naviga-tion. In Proceedings of the 32nd Conference on Neural In-formation Processing Systems (NeurIPS), 2018. 2 Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, GovindThattai, and Gaurav S Sukhatme.Dialfred: Dialogue-enabled agents for embodied instruction following.IEEERobotics and Automation Letters, 7(4):1004910056, 2022.2 Daniel Gordon, Abhishek Kadian, Devi Parikh, Judy Hoff-man, and Dhruv Batra.Splitnet: Sim2sim and task2tasktransfer for embodied visual navigation. In Proceedings ofthe 18th IEEE/CVF International Conference on ComputerVision (ICCV), pages 10221031, 2019. 2",
  "Weizhe Hua, Yuan Zhou, Christopher M De Sa, Zhiru Zhang,and G Edward Suh.Channel gating neural networks.InProceedings of the 33rd Conference on Neural InformationProcessing Systems (NeurIPS), 2019. 2": "Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, LaurensVan Der Maaten, and Kilian Q Weinberger.Multi-scaledense networks for resource efficient image classification.arXiv preprint arXiv:1703.09844, 2017. 2 Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3denvironment for visual ai. arXiv preprint arXiv:1712.05474,2017. 1, 6 Klemen Kotar and Roozbeh Mottaghi.Interactron: Em-bodied adaptive object detection.In Proceedings of the33rd IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1486014869, 2022. 2 Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang,Zhihui Li, and Xiaojun Chang.Dynamic slimmable net-work.In Proceedings of the 23nd IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages86078617, 2021. 1, 2, 6 Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu,Yueting Zhuang, and William Yang Wang. Unsupervised re-inforcement learning of transferable meta-skills for embod-ied navigation. In Proceedings of the 31st IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 1212312132, 2020. 2 Jialu Li, Hao Tan, and Mohit Bansal. Envedit: Environmentediting for vision-and-language navigation. In Proceedingsof the 33rd IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1540715417, 2022. 2 Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He,Weizhu Chen, and Tuo Zhao.Less is more: Task-awarelayer-wise distillation for language model compression. InProceedings of the 40th International Conference on Ma-chine Learning (ICML), pages 2085220867. PMLR, 2023.1 Chong Liu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang,Zongyuan Ge, and Yi-Dong Shen. Vision-language naviga-tion with random environmental mixup. In Proceedings ofthe 19th IEEE/CVF International Conference on ComputerVision (ICCV), pages 16441654, 2021. 2 Lanlan Liu and Jia Deng. Dynamic deep neural networks:Optimizing accuracy-efficiency trade-offs by selective exe-cution. In Proceedings of the 34nd AAAI Conference on Ar-tificial Intelligence, 2018. 1, 2, 6 Haonan Luo, Guosheng Lin, Fumin Shen, Xingguo Huang,Yazhou Yao, and Hengtao Shen. Robust-eqa: robust learningfor embodied question answering with noisy labels. IEEETransactions on Neural Networks and Learning Systems,2023. 2 ArjunMajumdar,KarmeshYadav,SergioArnaud,Yecheng Jason Ma, Claire Chen, Sneha Silwal, AryanJain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Ma-lik, et al.Where are we in the search for an artificialvisual cortex for embodied intelligence?arXiv preprintarXiv:2303.18240, 2023. 2",
  "Alex Nichol, Joshua Achiam, and John Schulman.Onfirst-ordermeta-learningalgorithms.arXivpreprintarXiv:1803.02999, 2018. 5": "Shayegan Omidshafiei, Jason Pazis, Christopher Amato,Jonathan P How, and John Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observ-ability. In Proceedings of the 34th International Conferenceon Machine Learning (ICML), pages 26812690. PMLR,2017. 3 Dripta S Raychaudhuri, Yumin Suh, Samuel Schulter, XiangYu, Masoud Faraki, Amit K Roy-Chowdhury, and Manmo-han Chandraker. Controllable dynamic multi-task architec-tures.In Proceedings of the 33rd IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages1095510964, 2022. 2",
  "Sinan Tan, Mengmeng Ge, Di Guo, Huaping Liu, andFuchun Sun. Knowledge-based embodied question answer-ing. IEEE Transactions on Pattern Analysis and MachineIntelligence, 2023. 2": "Surat Teerapittayanon, Bradley McDanel, and Hsiang-TsungKung.Branchynet: Fast inference via early exiting fromdeep neural networks.In Proceedings of the 23rd Inter-national Conference on Pattern Recognition (ICPR), pages24642469. IEEE, 2016. 2 Ayzaan Wahid, Austin Stone, Kevin Chen, Brian Ichter, andAlexander Toshev. Learning object-conditioned explorationusing distributed soft actor critic. In Proceedings of the 5thConference on Robot Learning (CoRL), pages 16841695.PMLR, 2021. 2 Longguang Wang, Xiaoyu Dong, Yingqian Wang, Li Liu,Wei An, and Yulan Guo. Learnable lookup table for neuralnetwork quantization. In Proceedings of the 33rd IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 1242312433, 2022. 1 Yue Wang, Jianghao Shen, Ting-Kuei Hu, Pengfei Xu, TanNguyen, Richard Baraniuk, Zhangyang Wang, and YingyanLin. Dual dynamic inference: Enabling more efficient, adap-tive, and controllable deep inference. IEEE Journal of Se-lected Topics in Signal Processing, 14(4):623633, 2020. 2",
  "Wenhan Xia, Hongxu Yin, Xiaoliang Dai, and Niraj K Jha.Fully dynamic inference with deep neural networks. IEEETransactions on Emerging Topics in Computing, 10(2):962972, 2021. 2": "Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang.Multi-task reinforcement learning with soft modularization.In Proceedings of the 34th conference on neural informationprocessing systems (NeurIPS), pages 47674777, 2020. 1, 3 Felix Yu, Zhiwei Deng, Karthik Narasimhan, and Olga Rus-sakovsky.Take the scenic route: Improving generaliza-tion in vision-and-language navigation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition Workshops, pages 920921, 2020. 2 Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal,Tamara L Berg, and Dhruv Batra.Multi-target embodiedquestion answering. In Proceedings of the 30th IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 63096318, 2019. 2 Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and metareinforcement learning. In Proceedings of the 4th conferenceon robot learning (CoRL), pages 10941100. PMLR, 2020.1, 6"
}