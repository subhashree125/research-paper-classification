{
  "CoStrategist R&D Group, Microsoft Mixed Reality 2University of Texas at Austin": ". Illustration of a zero-shot GPT-4V agent answering 3D Visual Question Answering (VQA) questions. We present a preliminaryinvestigation of how recently introduced open-vocabulary LLMs perform against older well-established closed-vocabulary benchmarks,namely 3D-VQA and ScanQA . To stimulate future research and alleviate the extensive computational cost, we will be releasing allprompts, scene captions, GPT responses and results across all ScanNet scenes in our project repo: Code available upon acceptance.",
  "Abstract": "As interest in reformulating the 3D Visual QuestionAnswering (VQA) problem in the context of foundationmodels grows, it is imperative to assess how these newparadigms influence existing closed-vocabulary datasets.In this case study, we evaluate the zero-shot performanceof foundational models (GPT-4 Vision and GPT-4) on well-established 3D VQA benchmarks, namely 3D-VQA andScanQA. We provide an investigation to contextualizethe performance of GPT-based agents relative to traditionalmodeling approaches. We find that GPT-based agents with-out any finetuning perform on par with the closed vocab-ulary approaches. Our findings corroborate recent resultsthat blind models establish a surprisingly strong base-line in closed-vocabulary settings.We demonstrate thatagents benefit significantly from scene-specific vocabularyvia in-context textual grounding. By presenting a prelim-inary comparison with previous baselines, we hope to in-",
  ". Introduction": "Advancements in 3D scene understanding have rendered AIagents essential for applications spanning from augmentedreality to robotics . Yet, to make these technologiesaccessible to non-experts, it is critical for AI to interpret3D scenes through natural language , where AI agentsincorporate natural language understanding and commonsense in sync with human perception and cognition .The fusion of vision and language has become a staple inmultimodal 3D scene comprehension, with Visual QuestionAnswering (VQA) tasks thoroughly examined across vari-ous benchmarks . However, the rise of foundationmodels has posed new challenges, prompting a shift fromclosed- to open-vocabularies in VQA benchmarks and to-wards metrics that facilitate the assessment of open-ended",
  "arXiv:2405.18831v1 [cs.CV] 29 May 2024": "responses. Punctuating this trend is the recently unveiledOpenEQA benchmark .In this paper, we present an exploratory analysis of howGPT-based agents perform against the well established 3D-VQA benchmarks, specifically 3D-VQA and ScanQA. Ourstudy reveals insights into the zero-shot capabilities of GPT-4V, providing a performance floor that helps contextual-ize agent performance relative to traditional, GPT-free base-lines that are trained using extensive datasets. We demon-strate that zero-shot, finetuning-free GPTs with almost off-the-shelf prompting, are remarkably competitive, achiev-ing scores within 10% of meticulously crafted DNN-basedbaselines. Moreover, our findings corroborate the surprisingoutcomes recently observed for open-vocabulary datasetswithin closed-set benchmarks: text-only or blind LLMagents responding to queries without visual data achievesurprisingly robust performance relying on language priorsas common sense . Lastly, we discover that GPT-V, de-spite its free-form and open-ended nature, significantly ben-efits from the inclusion of scene-specific vocabulary duringits captioning tasks.To efficiently investigate agents we implement a mas-sively parallel task scheduler that facilitates con-current VQA prompt execution across multiple GPT-4V &GPT-4 Turbo endpoints. With the extensive computationaleffort already invested, we are releasing all prompts, scenecaptions, GPT responses and results over all ScanNetscenes in our project repo.1 We hope that this open resourcestimulates research amidst the emerging discussion of re-thinking 3D VQA for the era of foundation models.",
  ". Related Work": "Existing closed-vocabulary VQA. Visual Question An-swering (VQA) has been a key task across numerous AI andcomputer vision benchmarks thanks to its inherently multi-modal nature, which involves the perception of both textualand visual input modalities. The importance of VQA is re-flected in its application across a diverse range of domains,from embodied AI, situated reasoning, object localization,recognition, to activity detection, temporal window local-ization, and future forecasting . Giventhe multimodality, VQA is particularly vital for 3D sceneunderstanding and analysis, with several well-establishedbenchmarks building upon the ScanNet dataset such as 3D-VQA , ScanQA , and ReferIt3D . However, these3D VQA benchmarks were established before the advent oftransformer-based open-vocabulary models, leading to theirtextual components being more closed-vocabulary.open-vocabulary VQA. In recent months, there is anemerging interest around developing new benchmarks thatare better suited to open-vocabulary models, such as MM-",
  "Code available upon acceptance": "Bench and RoboVQA . Notably, Metas releaseof the OpenEQA benchmark concurrent to our work punctuates this shift and provides a basis for researchingmultimodal embodied/3D agents. However, initial resultsare presented with respect to the newly introduced bench-mark, lacking comparative analyses on previously estab-lished benchmarks. Our investigation aims to bridge thisgap, providing insight into the performance of state-of-the-art agents when applied to 3D-VQA and ScanQA tasks. To our knowledge, our work is the first to conductsuch analysis.",
  ". VQA Agents": "Our agents workflow is illustrated in . Given a sceneRGB-D sensor stream, we uniformly sample mesh frames.These frames provides different views of a 3D scene meshand we control the sampling by using frame sample rate pa-rameter F which means every Fth frame is sampled fromthe stream. We then employ GPT-V as a powerful caption-ing model to produce per-frame captions that capture thedetailed characteristics of different items within the scene.These captions are then provided as the language scene-description to GPT-4 Turbo, which is tasked with respond-ing to the benchmark questions.While our methodology aligns closely with state-of-the-art 3D agent pipelines as outlined in , we make two de-liberate implementation choices to tailor our approach tothe benchmarks under consideration. First, for a more rep-resentative comparison with 3D-VQA baselines, we employmesh images as inputs to our captioning model, rather thanreal 2D images. Second, we utilize GPT-V to directlycaption the scene frames. An alternative would be utilizingdedicated captioning models, such as LLaVA-1.5 orConceptGraphs , to create scene-graph representationsthat incorporate 3D object semantics. However, OpenEQA shows that 3D descriptions do not significantly impactagent performance, hence we postulate that the format of se-mantic representation can be more flexibly determined andgenerated directly via GPT-V prompting.Given our overall goal to implement a fully GPT-driven approach, we instruct GPT-V to craft Scene-GraphCaptions-like (SGC) representations directly. Through in-context instructions and prompting, GPT-V is guided toencode the various aspects of the scenedetailing ob-jects and their attributes such as color, size, and relativeposition - into a coherent textual scene-graph description.We explore two distinct captioning schemes. In a purely",
  "Prompting GPT-V for Vocabulary-agnostic SGC": "Prompt:This is a view of 3D mesh from anindoor scene.List out all the objectsyou can see in the image.For eachobject provide color, shape, locationand neighbouring objects following theobject-description format [..] Furthermore, we investigate a vocabulary grounded ap-proach, where GPT-V is given additional context. For this,we extract the scene-specific list of objects from the bench-mark ground truths and we instruct GPT-V to concentrateon these specified items during its description process:",
  ". Benchmarks": "We conduct our analysis on two well-established 3D VQAbenchmarks, namely ScanQA and 3D-VQA . Bothbenchmarks are based on the ScanNet dataset , whichcomprises scenes captured in various indoor environments,including bedrooms and offices. To facilitate a represen-tative comparison with previously published baselines, wereport metrics across all 71 scenes from the ScanNet val-idation set. The agents are evaluated on all the 53,395 and4,675 questions from 3D-VQA and ScanQA, respectively.",
  ". Metrics": "To accurately compare with ScanQA and 3D-VQA base-lines, we evaluate the agents answers using all the metricsreported in the respective papers: for ScanQA, we computeexact matching scores (EM@1, EM@10), BLEU scores(BLEU-1, BLEU-2, BLEU-3, BLEU-4), ROUGE-L, ME-TEOR, CIDEr, and SPICE scores. Since our agent providesonly a single answer, the metrics EM@1 and EM@10 aresame in our evaluation. For 3D-VQA, we report both theoverall accuracy and accuracy per question type (i.e., count-ing, location, query-attribute, Yes/No).Furthermore, consistent with observations from ,we note that the open-vocabulary nature of VQA taskspresents challenges when evaluated using exact matching-based metrics or BLEU/ROUGE scores, which comparesimilarity versus a set of possible closed-vocabulary an-swers.Following this insight, we confirm the relevanceof using an LLM to evaluate the correctness of agent re-sponses, which has been shown to more closely correlatewith human-like answers . For comprehensive analy-sis, we directly employ the recently introduced LLM-basedscore (termed as LLM-Match) directly using its promptingwith GPT-4 Turbo:",
  "LLM-Score Prompting - As proposed in": "You are an AI assistant who will helpme to evaluate the response given thequestion, the correct answer, and extraanswers that are also correct.To marka response, you should output a singleinteger between 1 and 5 (including 1,5).5 means that the response perfectlymatches the answer or any of the extraanswers.1 means that the response iscompletely different from the answer andall of the extra answers [..]",
  ". Experimental Setup": "We utilize the ScanNet toolkit to derive cameraposes, based on which we extract captures from the high-resolution mesh per scene. We use frame sample rate of Fwhich means every Fth frame is used to answer the ques-tion. The default value of F is 50 unless specified and weexplore ablations of this parameter in . In order tocaption all frames across all scenes efficiently, we have de-veloped a massively parallel scheduler that assigns GPT-Vcaptioning tasks concurrently across 50 GPT-V Azure Ope-nAI endpoints. We then aggregate these per-frame captionsto form a comprehensive scene description, which serves asinput for GPT-4 to answer benchmark questions. We set thetemperature for both GPT-4 Turbo and GPT-V to 0.2.To manage the question-answering process acrossbenchmarks efficiently and handle the sheer volume ofqueries, we also deploy the scheduler to simultaneouslylaunch GPT-4 queries across 100 GPT-4 Azure OpenAIendpoints. This approach not only ensures efficient abla-tions, but also helps us circumvent the Requests-Per-Minute(RPM) limits set by GPT APIs . In addition to this, weask Q questions in a single API call to reduce the time toanswer the entire dataset. We carry out ablations on the thisbatch size Q in . Thanks to this level of paralleliza-tion, we are able to complete a run of the 5,000-questionScanQA dataset in just one hour.",
  ". ScanQA results": "presents the comparative analysis of agent perfor-mances on ScanQA. The best vocabulary-grounded GPTagent, with Q = 1, performs slightly better than the bestScanQA baseline for the ROUGE-L score, a remarkablycompetitive result for a finetuning-free, zero-shot methodutilizing almost off-the-shelf prompting. Although the ex-act matching metrics (EM@ and BLEU-) fall below the top-performing ScanQA baseline, the ROUGE-L and METEORscores are comparable or even superior. This discrepancy isexpected, as the baselines are finely tuned through exten-sive training on specific datasets, which biases them towardthe format of the final answer. In contrast, our GPT-basedagents are deployed directly without such tailored training.These observations partially support the discussion regard-ing the limitations of conventional similarity scores . Itis interesting to note, however, that there is a discernibledegree of correlation between ROUGE-L and LLM-Matchbased on the LLM scores for each agent.Blind GPT. Adding to the conversation that OpenEQAinitiated recently that language provides an easierprior about the world , we confirm that bling agentsexhibit unexpectedly strong performance on ScanQA aswell. As we discuss in detail later, we observe that GPTcan employ common sense to achieve apt guesses: for in-stance, GPT is likely to respond correctly to questions suchas what is the material of the kitchen counter.Vocabulary-grounding helps with GPT-V captioning.We note a significant drop in performance when GPT-Vis tasked with open-ended, ungrounded scene descriptions.Qualitatively inspecting the vocabulary-agnostic captionsacross several scenes, we observe a degradation in the qual-ity of scene descriptions in two ways: first, without spe-cific prompts directing attention to certain items, GPT-V faces difficulty in consistently grounding objects within theframes, especially smaller objects that are positioned onlarger pieces of furniture, such as decorations.Second,GPT tends to use more commonly preferred synonyms (e.g.,couch instead of divan), leading to a mismatch withthe underlying object label. An intriguing direction forfuture work, which we are currently exploring, is to em-ploy oracle 3D objects from ScanNet scenes (e.g., replac-ing Detic 3D bounding-boxes with gold detectionsfrom the ReferIt3D ScanNet-based dataset ) to groundthe Sparse Voxel Maps, LLaVA, or GPT-V captions. Shouldthis improve GPT-4 performance further, it would suggestthat properly grounding the scene description substantiallyinfluences the agents overall results.",
  ". 3D-VQA results": "presents the comparative analysis of agent perfor-mances on 3D-VQA. The results indicate a more substan-tial gap than observed in ScanQA, with the best-performingagent trailing the top baseline by over 10% in overall accu-racy. This widened disparity arises partly because the modelstruggles significantly with counting questions. Given GPT-Vs existing limitations in enumerating objects within evenwithin simple COCO-style images, this challenge is exac-erbated when extended to counting across the entire mesh.Interestingly, given OpenEQA showed that 3D camerapose information does not markedly improve GPTs per-formance in captioning, we speculate that accurately dedu-plicating object counts for comprehensive questions (e.g.,How many chairs are in the room?) is a very challengingtask for the agent. Indeed, a closer examination of the re-sponses reveals that GPT tends to default to majority-basedestimations - a behavior reflected by the fact that agentCounting scores are close to the noise-floor of the Major-ity Q (constant liar); in other words, GPT-4 tends to guesslow-count responses (usually 1 or 2) which happen to bemore probable in the benchmark.Similarly, the blind GPT agent exhibits a relativelyweaker baseline than in ScanQA, hinting that zero-shotperformance might be significantly influenced by the ques-tion distribution within a given task. An insightful implica-tion of this for the 3D VQA community might be: shouldOpenEQA had included a greater proportion of countingquestions, the effectiveness of common sense predictionsmight be lower.Last, having observed first-hand the computational over-head or running tens of thousands of VQA queries againstexpensive LLMs, we inspect how the number of VQA tasksimpacts performance. To this end, we created a minival3D-VQA set, by uniformly sampling from the original 53.4kquestion pool to a more manageable 4.7k, mirroring thesize of ScanQA. We find that performance trends remainedconsistent, falling within a 2% variance range . This suggests that merely expanding the number of questions,without a significant shift in the question distribution, maynot offer substantially more insights with regards to agentassessment, corroborating our groups parallel findings onevaluating GPT Copilots across other domains .",
  ". Ablations": "In this section we provide a systematic analysis of thevarious experimental configurations impacting the per-formance of our best peforming agent - SocraticVocab-grounded SGC, specifically tailored for theScanQA benchmark. This decision allows for a more fo-cused examination, given the substantial size of the 3DVQAbenchmark, which encompasses over 53,395 questions.Initially, we study the consistency of our GPT-basedagents across different experimental runs. Then we explorethe performance disparities between utilizing original RGBframes and rendered views from 3D meshes. This compar-ison bridges the gap in the existing literature, which pre-dominantly focuses either on direct RGB data or re-constructed 3D models , thereby enhancing our under-standing of how different visual inputs affect agent perfor-mance.Subsequently, we analyze the influence of the numberof frames used per question.This aspect highlights thetrade-off between the computational demands (in terms ofthe number of calls to GPT-V) and the resultant accuracy.Reducing the number of frames per scene can decrease thenumber of GPTV calls, optimizing resource usage.Lastly, we conduct ablations on the number of questionshandled per API call. This study is aimed at discerning thebalance between operational efficiency (in terms of tokenutilization) and model accuracy.By processing multiplequestions in a single API request, the total number of APIcalls necessary can be reduced, thereby conserving tokensand enhancing overall efficiency.",
  ". Output Consistency Across Multiple Runs": "In this section, we analyze the consistency of performancemetrics across multiple runs under identical experimentalconditions. This investigation aims to determine the de-gree of variation in the answers provided by our GPT-basedagents when posed the same question repeatedly. For thisstudy, we maintained a constant batch size of Q = 20 andframe sample rate of F = 50. We also keep the same tem-perature setting across different runs, as outlined in . Temperature, as a parameter in GPT models, controlsthe randomness of the output by adjusting the probabilitydistribution over the generated tokens; a lower temperaturegenerally results in more deterministic and focused outputs. presents the empirical results from these tests.Interestingly, the variance in performance metrics acrossmultiple runs was minimal, indicating high consistency in",
  "Human 91.89": ". Performance assessment on 3D-VQA-val with previously published baselines and ablations. For each agent, we compute allmetrics as in the original paper, as well as the recently introduced LLM-based scoring metric. the answers generated by the agent. This is further quan-tified by the reported mean and standard deviation of themetrics, where the standard deviation is notably low. Suchconsistency show the deterministic nature of the outputs,likely due to the lower temperature settings employed inour experiments. Additionally, the LLM-Match scores re-mained quite stable across different trials, emphasizing therobustness of the metric as well.",
  ". Mesh vs. RGB Frames for VQA": "In this ablation study, we investigate the use of originalRGB frames versus rendered views of 3D meshes for thevisual question answering task, a departure from the preva-lent approach of utilizing 3D models in previous works onthese benchmarks. Quantifying the potential performanceimprovement achieved by using the original 3D RGB cap-tures for foundational models can be useful in understand-ing the information lost during scene reconstruction andthe utilization of 3D CNNs to answer questions. Leverag-ing original frames has the advantage of producing richercaptions with more detailed information about objects inthe scene, particularly smaller ones. Unlike many previ-ous approaches that rely on deep learning networks, partic-ularly 3D CNNs which limit the input to only 3D models ofscenes, our method allows for a direct comparison betweenusing original RGB frames and mesh frames just by usingthe desired image to generate GPT-V captions and then us-ing these captions for answering questions by agents. presents the comparison between RGB and meshframes. For this experiment, we maintain a default batchsize (Q) of 20 and frames per scene (F) of 50. We ob-serve a substantial increase in all scores, particularly no- table improvements in ROUGE-L, METEOR, and CIDEr,showing an improvement of 13.15%, 12.6% and 18.39%respectively. Additionally, all BLEU scores exhibit signif-icant increases as well. This observation supports the intu-itive notion that original RGB frames contain more informa-tion compared to their rendered 3D counterparts, resultingin richer captions and better answers. RGB frames captureintricate details, colors, textures, and lighting conditions di-rectly from the scene, providing a comprehensive represen-tation of spatial and semantic information. This richnessenables GPT-V to understand scene dynamics more accu-rately and generate more descriptive captions. Additionally,RGB frames preserve subtle nuances such as shadows, re-flections, and object occlusions, contributing to the overallrealism of the scene. Leveraging this additional informationleads to improved performance in visual question answeringtasks.",
  ". Number of Frames per scene": "In this ablation study, we evaluate the impact of varyingthe number of frames used per scene on the performanceof our visual question answering (VQA) system. For theVQA task, we uniformly sample every F-th frame from theRGB-D stream, with each frame being a 3D render of themesh corresponding to an original RGB capture. Descrip-tive scene captions are generated for each selected frameusing GPTV, and these captions are subsequently combinedto provide contextual information for our GPT-based agentsto answer the questions. Ideally, using a higher numberof frames (a lower value of F) should result in more de-tailed and dense captions as certain objects may be miss-ing in some frames, and having a greater number of frames",
  ". Performance assessment on ScanQA-val for different image sources - Original RGB captures vs Mesh frames": "compensates for these absences by providing additional de-tails. However, this approach incurs higher costs due totwo main factors: 1) the increased number of GPTV callsrequired to generate captions for more frames, and 2) thelarger combined caption size that needs to be processed bythe GPT agents, resulting in higher token consumption. Theuse of more tokens not only increases costs but also slowsdown the inference process, thereby extending the overallresponse time.The results are presented in 5, where different values ofF were tested. A clear trend is observed: as the value of Fincreases, leading to fewer frames being used, the accuracymetrics decrease significantly. This decrement is smoothand marked, with ROUGE-L scores dropping from 31.19 to18.14, METEOR from 13.02 to 7.8, and CIDEr from 54.61to 32.23, respectively. The highest scores are obtained whenF = 50, indicating that sampling every 50th frame providesan optimal balance of detail and computational efficiency.We stop at F = 50, as further increasing F becomes ex-pensive, both in terms of monetary cost and time. Futurestudies could explore the potential of achieving the best pos-sible scores by utilizing all available frames, thus providinga comprehensive understanding of the trade-offs betweencost, speed, and accuracy in the context of frame samplingfor VQA tasks.",
  ". Batch size": "In this ablation, we examine the impact of varying the batchsize Q, which represents the number of questions our agentaddresses within a single API call. This investigation helpsunderstand the trade-off between the consumption of tokensand the performance of the agent. By posing more ques-tions per API call, the total number of API calls requireddecreases, which in turn reduces the total token usage. Thisefficiency gain stems from the significant size of the con-text provided to the agent, which consists of concatenated descriptions from all sampled frames. However, increasingthe batch size also complicates the task at hand. Answeringmultiple questions simultaneously can be more challengingthan responding to a single query, as the agent must main-tain accuracy across diverse question contexts. encapsulates the results of this ablation study. Itreveals a discernible trend where an increase in batch sizecorrelates with a slight dip in the performance metrics ofthe agent. Specifically, as the batch size escalates from 1to 20, the ROUGE-L score decreases from 33.43 to 30.18.Similarly, the METEOR and CIDEr scores diminish from14.23 to 12.68 and from 58.32 to 53.41, respectively. Thistrend, with a few anomalies, corroborates the intuitive ex-pectation that batching questions would slightly compro-mise performance. Nonetheless, the magnitude of the per-formance decrement is not very significant, suggesting thatwhile there is a cost to batching in terms of accuracy, thereduction in token usage and API calls might justify thisapproach in scenarios where computational efficiency is pri-oritized.",
  ". Conclusion & Future Work": "In this study, we presented an initial investigation of howemerging GPT-based agents perform on existing 3D VQAbenchmarks, namely 3D-VQA and ScanQA. Our primaryfinding indicates that GPT agents, even when operatingwithout visual data (blind), serve as competitive base-lines, suggesting the effectiveness of common senseguessing. Furthermore, the integration of scene-specific vo-cabulary into GPT-V enhances its captioning performance.We hope that our investigation stimulates further re-search on 3D VQA benchmarks. To this end, and especiallygiven the considerable computational cost of such analysis,we will be releasing our entire workflow. Hopefully, weaid the ongoing discussion regarding the need to reformu-",
  ". Ablation of batch size (Q) on ScanQA-val (71 scenes). We see slight performance degradation as number of questions in a singleAPI call increase": "late older closed-form benchmarks in the era of founda-tion models. With that, we would like to highlight someparticularly exciting and promising areas for future workthat we have identified through our research and that we areactively investigating.What are the ScanQA and 3D-VQA upper bounds forhuman agents and multi-frame GPT? With the ability ofLLM-based metrics to evaluate open-vocabulary answers,it becomes compelling to revisit human-in-the-loop evalu-ation for ScanQA and 3D-VQA benchmarks areas notextensively covered in the original papers, where in fact thereported human performance was about focus was align-ing model replies with closed-vocabulary human answers.Moreover, we note that the current GPT-V API supportsmulti-frame input, a feature we plan to incorporate into ourongoing analysis as in .To what extent does elaborate prompting improveagent performance? Early methodologies, including oursand the OpenEQA agents, utilize straightforwardChain-of-Thought prompts that essentially guide the modelto Think step-by-step.Investigating the effect of ad-vanced prompting strategies, like few-shot ReAct and Tree-of-Thought , will capture whether these augmentor even inadvertently hurt overall agent effectiveness.Can we overcome Force-A-Guess failures more effec-tively? As captured by OpenEQA and confirmed in ouranalysis, Socratic LLMs often abstain from answering. In-stead of defaulting to best-guess responses, we drawing in-spiration from self-consistency prompting : our initialinvestigation shows that repeatedly posing the same ques-tion achieves a reduction of these failures by up to threefold after four to five repeated queries! We are currently pursu-ing this further towards reducing Force-A-Guess failures.Could in-context visual grounding improve agentperformance? Our version of vocabulary grounding couldbe viewed as in-context textual grounding, which in turnprompts the question of whether similar visual groundingstrategies could be employed directly at the GPT-V level.For example, recent work shows that grounding with coor-dinates or markers was beneficial in zero-shot GPT-V per-formance for segmentation tasks .Thus, we are in-trigued to explore analogous techniques for 3D mesh. Infact, we can draw inspiration from the original ScanQApaper and its TopDownImage ablation: would present-ing the overall top-down floorplan scene as a visualprompt to GPT-V aid in addressing deduplication/countingissues?",
  "ence, Glasgow, UK, August 2328, 2020, Proceedings, PartI 16, pages 422440. Springer, 2020. 1, 2, 5": "Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and MotoakiKawanabe. Scanqa: 3d question answering for spatial sceneunderstanding. In Conference on Computer Vision and Pat-tern Recognition (CVPR), 2022. 1, 2, 3, 4, 5 Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-ber, Thomas Funkhouser, and Matthias Niener. Scannet:Richly-annotated 3d reconstructions of indoor scenes.InConference on Computer Vision and Pattern Recognition(CVPR), 2017. 3 Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee,Devi Parikh, and Dhruv Batra. Embodied question answer-ing.In Proceedings of the IEEE conference on computervision and pattern recognition, pages 110, 2018. 2",
  "Michael Fore, Simranjit Singh, and Dimitrios Stamoulis.Geckopt: Llm system efficiency via intent-based tool selec-tion, 2024. 8": "Qiao Gu,Alihusein Kuwajerwala,Sacha Morin,Kr-ishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal,Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa,et al. Conceptgraphs: Open-vocabulary 3d scene graphs forperception and planning. arXiv preprint arXiv:2309.16650,2023. 2 Zhijian Hou, Lei Ji, Difei Gao, Wanjun Zhong, Kun Yan,Chao Li, Wing-Kwong Chan, Chong-Wah Ngo, Nan Duan,and Mike Zheng Shou. Groundnlq@ ego4d natural languagequeries challenge 2023. arXiv preprint arXiv:2306.15255,2023. 2",
  "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.Improved baselines with visual instruction tuning.arXivpreprint arXiv:2310.03744, 2023. 2": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, SongyangZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,Ziwei Liu, et al. Mmbench: Is your multi-modal model anall-around player? arXiv preprint arXiv:2307.06281, 2023.2 Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, PranavPutta, Sriram Yenamandra, Mikael Henaff, Sneha Sil-wal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud,Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma,Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk,Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, ChrisPaxton, Sasha Sax, and Aravind Rajeswaran. Openeqa: Em-bodied question answering in the era of foundation models.In Conference on Computer Vision and Pattern Recognition(CVPR), 2024. 1, 2, 3, 4, 5, 6, 7, 8 Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, De-bidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan,Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi,et al.Robovqa: Multimodal long-horizon reasoning forrobotics. arXiv preprint arXiv:2311.00899, 2023. 1, 2",
  "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdChi, Sharan Narang, Aakanksha Chowdhery, and DennyZhou. Self-consistency improves chain of thought reason-ing in language models, 2023. 8": "Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Ab-hishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, DeviParikh, and Dhruv Batra. Embodied question answering inphotorealistic environments with point cloud perception. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 66596668, 2019. 2 Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,and Chuang Gan. Star: A benchmark for situated reasoningin real-world videos. In Thirty-fifth conference on neural in-formation processing systems datasets and benchmarks track(Round 2), 2021. 2"
}