{
  "Abstract": "Unsupervised domain adaptation (UDA) for semanticsegmentation aims to transfer the pixel-wise knowledgefrom the labeled source domain to the unlabeled target do-main.However, current UDA methods typically assumea shared label space between source and target, limitingtheir applicability in real-world scenarios where novel cat-egories may emerge in the target domain. In this paper, weintroduce Open-Set Domain Adaptation for Semantic Seg-mentation (OSDA-SS) for the first time, where the targetdomain includes unknown classes. We identify two majorproblems in the OSDA-SS scenario as follows: 1) the exist-ing UDA methods struggle to predict the exact boundary ofthe unknown classes, and 2) they fail to accurately predictthe shape of the unknown classes. To address these issues,we propose Boundary and Unknown Shape-Aware open-set domain adaptation, coined BUS. Our BUS can accu-rately discern the boundaries between known and unknownclasses in a contrastive manner using a novel dilation-erosion-based contrastive loss.In addition, we proposeOpenReMix, a new domain mixing augmentation methodthat guides our model to effectively learn domain and size-invariant features for improving the shape detection of theknown and unknown classes.Through extensive experi-ments, we demonstrate that our proposed BUS effectivelydetects unknown classes in the challenging OSDA-SS sce-nario compared to the previous methods by a large margin.The code is available at",
  "(e) BUS (Ours)": ". Visualization of prediction maps in the OSDA-SS sce-nario. The pixels detected by the white color mean the unknownclasses. The naive UDA method (b) is completely unaware of theunknown classes. Even after applying simple techniques to helpthe UDA model recognize the unknown, it still struggles to accu-rately predict the shape of the unknown, as shown in (c) and (d). ality.Over the past decade, there has been notable ad-vancement in supervised semantic segmentation driven bydeep neural networks . However, supervised seman-tic segmentation requires pixel-level annotations, whichare labor-intensive and costly to collect. To mitigate thechallenges, unsupervised domain adaptation (UDA) hasemerged. Many studies leverage the already-labeledsource data to achieve high performance on the unlabeledtarget data. Notably, synthetic datasets such as GTA5 and SYNTHIA which are automatically generated bygame engines present valuable resources for UDA research.UDA methods typically presume that source and targetdomains share the same label space. Such an assumptionis not reasonable in real-world applications. In the targetdata, novel categories not presented in the source dataset(target-private categories) may emerge, leading to an Open-",
  "arXiv:2405.19899v1 [cs.CV] 30 May 2024": "Set Domain Adaptation (OSDA) setting. The conventionalUDA method may significantly fail under the OSDA set-ting, e.g., a model erroneously label a person walking onthe road as the road itself as shown in (b). Thedesired model should reject any target-private classes asunknown rather than misclassifying it as a known class.While OSDA has been widely explored in image classi-fication , its application to semantic segmentationremains unexplored to the best of our knowledge. In thiswork, we tackle the interesting and challenging problem ofOpen-Set Domain Adaptation for Semantic Segmentation(OSDA-SS). Here, we deal with the labeled source data andthe unlabeled target data containing classes not found in thesource domain. In the OSDA-SS setting, the goal is to ac-curately predict pixel-wise category labels in the target do-main and correctly distinguish the classes not seen duringtraining as unknown. One can design reasonable baselines by extending well-established UDA methods.One approach could be aconfidence-threshold baseline.We train a model by us-ing the UDA algorithm without considering target-privateclasses. During inference, the model identifies pixels withconfidence scores below a predefined threshold as un-known. We show the predicted segmentation map from theconfidence-threshold baseline in (c). Another base-line could be a head-expansion baseline. We expand theclassification head from C to (C + 1) dimensions, whereC represents the number of known classes. During training,when generating pseudo labels, we assign pixels with confi-dence scores lower than a specific threshold to the (C + 1)-th head and train with the pseudo labels. We show the pre-dicted segmentation map from the head-expansion baselinein (d). These baselines sometimes reject target-private classes as unknown, but they often fail to do so, re-sulting in poor performance on the target dataset. In this work, we build a model upon the head-expansionbaseline. We find two failure modes of the baseline and pro-pose a novel Boundary and Unknown Shape-Aware (BUS)OSDA-SS method. First, the previous models are often lessconfident or even fail near the boundaries of objects .We find that the problem is even more severe fortarget-private classes due to lack of supervision.To ad-dress this issue, we propose a new Dilation-Erosion-basedCONtrastive (DECON) loss that manifests the boundariesthrough morphological operations, specifically dilation anderosion. Given a target image, we generate a target privatemask using pseudo-labeling with the expanded head. Sub-sequently, we generate a boundary mask by subtracting theoriginal private mask from the dilated private mask, indi-cating the region of known classes near the boundaries. Wegenerate an erosion mask by applying erosion to the pri-vate mask, indicating more confident regions of the privateclasses. We then train the model in a contrastive manner using the features from the erosion mask and the boundarymask as positive and negative samples, respectively. WithDECON loss, our model clearly discerns the common andprivate classes near the boundaries.Second, the baseline model faces challenges in accu-rately predicting the shape of unknown. If the model con-sistently predicts the same object regardless of variations insize, it indicates that the model relies more on shape infor-mation than size information to recognize the object. In-spired by this motivation, we propose a new data mixingaugmentation, OpenReMix. This method involves 1) resiz-ing a random thing class from the source image and mixingit with the target image during training to consistently pre-dict the same object even when its size varies. In addition,since there are no unknown classes in the source image, 2)we cut the parts predicted as unknown from a target imageand paste them into a source image for supplemental learn-ing of the last (C +1)-th head, aiding in the rejection of un-known during source training. This delicate mixing strategynotably enhances the detection capability of unknown, witha specific emphasis on capturing the shape information. Byaddressing the failure modes, the proposed BUS achievessignificant performance gains on public benchmarks: GTA5 Cityscapes and SYNTHIA Cityscapes.We summarize our major contributions as follows: To the best of our knowledge, we introduce a new task,Open-Set Domain Adaptation for Semantic Segmentation(OSDA-SS) for the first time. To tackle this challengingtask, we propose a novel Boundary and Unknown Shape-Aware OSDA-SS method, coined BUS.",
  ". Semantic Segmentation": "Semantic segmentation, which is a task to predict pixel-wise labels from the input images, has witnessed signifi-cant advances over the last decade. Key developments in-clude fully convolution networks (FCNs) , dilated convo-lution , global pooling , pyramid pooling ,and attention mechanism . Despite their success,these methods typically depend on a large amount of la-",
  ". Unsupervised Domain Adaptation for SemanticSegmentation": "Recently, there has been a lot of work on unsupervised do-main adaptation (UDA) for semantic segmentation. UDAmethods for semantic segmentation generally fall into twocategories: adversarial learning-based and self-training ap-proaches. Adversarial learning-based methods utilize an adversarial domain classifier to learn domain-invariant representations, aiming to deceive the domainclassifier.Self-training methods createpseudo labels for each pixel in the target domain image us-ing confidence thresholding. Several self-training methodsiteratively re-train the models, which result in enhanced per-formance on the target domain. Despite the great success,most previous works assume a closed set setting, where thesource and target domains share the same label space. Inthis work, we relax this unrealistic assumption and tacklethe problem of open-set domain adaptation for semanticsegmentation (OSDA-SS). To the best of our knowledge,there is no prior work to tackle this problem.",
  ". Open-Set Domain Adaptation": "Open-set domain adaptation (OSDA) extends UDA to han-dle novel categories in the target domain that are not presentin the source domain. The primary goal of OSDA is toeffectively distinguish the unknown categories from theknown classes while reducing the domain gap between thesource and target domains. Several OSDA methods havebeen proposed for the classification task .However, in semantic segmentation task, which requiresa higher degree of spatial information compared to classi-fication, directly applying classification methods strugglesto effectively differentiate unknown categories. The mostsimilar work to our method also deals with the novelclasses that do not exist in the source domain. However,it accesses pre-defined private category definitions. To ad-dress this challenge, we propose a novel OSDA-SS task todiscriminate unknown categories without needing to knowany information about pre-defined class definitions.",
  ". Problem Formulation": "In this section, we formulate a novel OSDA-SS task forthe first time. In OSDA-SS, a network is trained with thesource images Xs= {x1s, x2s, ..., xiss } and the correspond-ing labels Ys= {y1s, y2s, ..., yiss } to ensure effective perfor-mance in the target domain Xt= {x1t, x2t, ..., xitt } withoutlabels. xiss R3HW and yiss RCHW are the is-thsource domain image and the pixel-wise label. H and Ware the height and width of the image, respectively, and Cdenotes the number of categories in the source domain. Inthe target domain, we only have the image xitt R3HW without the corresponding labels. The source and target do-mains share C categories, and the target domain has addi-tional unknown classes, i.e., the target images contain un-known objects. In this setting, the goal of OSDA-SS is totrain a segmentation model f using both the labeled sourcedata (Xs, Ys) and the unlabeled target data Xt, and even-tually the learned model f should predict both known andunknown classes well on the target domain.",
  ". Baseline": "Inspired by the UDA methods based on self-training , we build a OSDA-SS baseline by extending thenumber of classifier heads from C to (C + 1), where the(C + 1)-th head corresponds to unknown classes. The seg-mentation network f is trained with the labeled source datausing the following categorical cross-entropy loss Lsseg:",
  "c=1y(j,c)slog f(xs)(j,c),(1)": "where j {1, 2, ..., H W} denotes the pixel index andc {1, 2, ..., C + 1} denotes the class index. To alleviatethe domain gap between the source and the target domains,the baseline utilizes a teacher network g to generate the tar-get pseudo-labels. The pseudo-label y(j)tp for the j-th pixelconsidering unknown is acquired as follows:",
  "C + 1,otherwise,(2)": "where c {1, 2, ..., C} denotes a class belonging to knownclasses and p is a threshold. Using the above equation, weassign the less confident pixels as the unknown class whenthe maximum softmax probability is lower than p. Sincewe cannot completely trust the pseudo-labels above, we es-timate the confidence of the pseudo-label by utilizing the . Overview of our proposed Boundary and Unknown Shape-Aware (BUS) method. We generate the mixed source image xms andthe mixed target image xmt from OpenReMix. The model is trained using the mixed source label and the mixed target pseudo-labels withsupervised loss and adaptation loss, respectively. Especially, the expanded head is trained with the parts that predicted as unknown inpseudo-labels. Pseudo-labels are generated by thresholding the softmax probability and passing through the refinement network. DECONloss utilizes the dilation and erosion operations to distinguish the known and unknown classes near the boundaries.",
  ". Dilation-Erosion-based Contrastive Loss": "Semantic segmentation models often struggle to confidentlypredict object boundaries , especially for target-private classes, where the absence of label informationmakes boundary prediction even more challenging. Since the models predict the boundaries with low confidence esti-mates, the quality of the generated pseudo-labels may not beaccurate. If the model can confidently identify the bound-aries of unknown classes, accurate predictions of unknownclasses become feasible.To discern the boundaries effectively, we leverage twomorphological operations, which are dilation and erosion.First, we utilize the pseudo-labels of the target image to cre-ate a target private mask as follows:",
  ",if y(j)tp = C + 10,otherwise,(6)": "where j denotes the pixel index. Next, we apply the di-lation function hd() and the erosion function he() to therandomly cropped target private mask, generating dilationand erosion masks. In the dilation mask, we subtract theoriginal target private mask to identify the regions associ-ated with the common classes near the boundaries. On theother hand, the erosion mask emphasizes the regions thatdefinitively belong to the private class. We generate thesemasks by the following equations:",
  ". OpenReMix": "Resizing Object.We identify that the head-expansionbaseline model fails to accurately predict the shape of theprivate classes. We hypothesize that if a model consistentlypredicts the same object regardless of size variations, themodel can accurately predict the shape of the object as well.To this end, we extend the domain mixing method Class-mix , which selects half of the classes from the sourceand appends them to the target image to learn domain-invariant features. On top of the Classmix, we introducean additional step where we select one more thing classfrom the source image, resize it, and paste it to the ran-dom location of a target image with resizing object maskMr. The mixed target image contains the same objects asthe source image, but the sizes of the objects are different.Therefore, the model learns not only domain-invariant rep-resentations but also size-invariant representations from themixed target images and the source images. This extensionenhances the robustness of the model to size variations, con-tributing to the accurate prediction of the shape of unknownclasses leading to superior open-set domain adaptation per-formance. Attaching Private.As described in .2, to ad-dress the target private classes, we expand the segmenta-tion head. The expanded head is trained with the targetpseudo-labels which contain the private labels. However,since there are no private classes in the source image, wecannot utilize the source data to update the additional headof the model. To overcome this inefficiency in training, wecopy the parts of target private classes and paste them into asource image. Given a target image, we create a target pri-vate mask Mu as Eq. (6). With the target private mask, wecopy the private regions in the target image to a source im-age, resulting in a private class-mixed source image. Simi-larly, by combining the labels of the source and the pseudo-labels of the target, we generate mixed source labels. Thisaugmentation offers a significantly larger dataset for train-ing to reject private classes, leading to improved open-set",
  ". Experimental Setup": "Datasets.We evaluated our framework over two chal-lenging synthetic-to-real scenarios in autonomous driving,i.e., GTA5 Cityscapes and SYNTHIA Cityscapes.GTA5 is a synthesized dataset, which consists of24,966 images with a resolution of 1914 1052. SYN-THIA is also a synthesized dataset, which contains9,400 images with resolution 1280 760. Cityscapes is a real-image dataset with 2,975 training samples and 500validation samples with resolution 2048 1024. It shares19 classes with GTA and 16 classes with SYNTHIA. Scenario Construction.Using these datasets, we estab-lished new scenarios tailored for the OSDA-SS task. First,to create new classes emerging in the target domain thatare not present in the source domain, we selected certainsource classes to be removed. In autonomous driving sce-narios, the classes that are likely to emerge in the target do-main are expected to be thing classes. Stuff classes rep-resenting the background area typically do not emerge asnew classes. Therefore, we selected specific classes fromthe thing categories to be excluded. The following listdenotes the classes designated as unknown in GTA5 andSYNTHIA. GTA5: pole, traffic sign, person, rider, truck,and train. SYNTHIA: pole, traffic sign, person, rider,truck, train, and terrain.Notably, SYNTHIA includes the terrain, which inher-ently lacks labels from the outset. Second, in order to avoidtraining the excluded classes, pixels corresponding to thoseclasses were designated as ignore and were not includedin the loss function during training. Finally, during the eval-uation of the target domain, the above classes were treatedas single unknown class.",
  "GTA5 Cityscapes": "OSBP 4.923.9342.82.556.04 14.29 68.5826.50 44.21 41.78 0.947.203.4220.554.497.34UAN 65.97 23.41 76.41 37.26 18.50 20.13 80.5730.37 82.47 77.35 27.80 16.620.0038.003.596.56UniOT 17.675.1444.86 55.45 2.31 52.61 40.013.3779.43 52.87 52.317.180.0020.205.367.49 ASN 82.342.2175.30 8.013.529.99 71.9615.61 70.97 77.16 22.5920.80.0635.4310.8416.60Pixmatch 79.272.0672.36 6.962.94 11.07 76.2923.23 77.72 79.77 44.72 18.020.0138.039.4615.15DAF 94.26 48.69 83.47 38.67 32.83 41.71 87.7939.15 93.59 85.29 47.04 28.36 46.8661.2614.6323.36HRDA 95.14 62.58 82.92 47.44 43.57 53.18 88.2644.42 92.92 90.23 57.43 14.71 56.8363.8212.1320.39MIC 93.26 58.96 79.30 21.62 31.41 39.32 85.4831.94 91.64 88.16 44.77 47.64 42.7758.1711.8719.71",
  "SYNTHIA Cityscapes": "OSBP 6.719.4949.83 0.700.00.76 26.03 36.91 20.04 4.762.908.7013.204.907.14UAN 33.24 19.03 71.49 4.020.05 14.34 75.78 81.06 53.88 19.348.1421.8431.304.537.91UniOT 0.0016.79 18.52 1.056.4916.8 14.52 57.46.482.593.733.8812.355.497.06 ASN 72.70 41.29 73.59 7.380.081.17 71.35 82.22 67.35 23.300.9420.5638.494.628.25Pixmatch 74.168.1576.21 0.010.05.64 44.15 63.76 44.66 17.270.130.3826.306.8711.00DAF 70.10 39.65 83.09 22.75 4.66 41.19 81.56 91.79 84.36 51.13 43.78 46.2051.499.0715.57HRDA 85.62 41.74 83.29 36.35 0.86 35.17 83.98 90.90 84.74 50.42 46.78 58.3354.6812.6820.82MIC 88.31 70.71 85.00 26.23 6.60 35.27 84.80 91.41 81.47 53.62 55.39 58.2057.4610.0217.23",
  "BUS (Ours) 86.85 43.49 89.35 46.12 4.39 54.29 87.90 92.49 91.46 61.23 58.11 59.8164.6233.3744.01": ".Performance on two different benchmarks. Our proposed BUS achieved the state-of-the-art performance with remarkableimprovement in H-Score +39.45% against DAFormer in GTA Cityscapes and +23.19% against HRDA in SYNTHIA Cityscapes. which averaged the IoU of each class. Since we treatedevery unknown classes as single unknown class, simply av-eraging would diminish the impact of private classes sig-nificantly. Therefore, inspired by , we utilized the har-monic mean of the mean IoU score for known classes (com-mon) and the IoU score for one unknown class (private) asour evaluation metric, known as the H-Score. Implementation Details.We adopted DAFormer network with the MiT-B5 encoder pre-trained onimageNet-1K . We followed the multi-resolution self-training strategy and training parameters of MIC . Thenetwork was trained with AdamW . The learning rateswere set to 6e-5 for the backbone and 6e-4 for the decoderhead, with a weight decay of 0.01 and linear learning ratewarm-up over 1.5k steps. EMA factor was =0.999. Weutilized the Rare Class Sampling , ImageNet FeatureDistance , DACS data augmentation, and MaskedImage Consistency module . We trained on a batch oftwo 512 512 random crops for 40k iterations. We usedMobileSAM for the refinement model. The refinementprocess is described in the supplemental material.",
  "Baselines.We compared our approach with two scenar-ios.The first scenario comprised the Open-Set DomainAdaptation (OSDA) method like OSBP and Univer-": "sal Domain Adaptation (UniDA) methods like UAN and UniOT , which were capable of rejecting un-known classes but were primarily designed for classifica-tion tasks. The second scenario was Unsupervised DomainAdaptation (UDA) methods for semantic segmentation inclosed-set setting, which included AdaptSegNet (ASN) ,Pixmatch , DAFormer (DAF) , HRDA , andMIC . In the UDA method, we assigned the unknownlabel for regions with low confidence scores during infer-ence.For OSDA and UniDA methods, we replaced theclassification network with the DeepLabv2 segmenta-tion network, which uses ResNet-101 as the backbone,and adopted the image-level methods to the pixels.",
  ". Comparison with the State-of-the-Art": "showed the experimental results of GTA5 Cityscapes and SYNTHIA Cityscapes, respectively. Theclassification methods struggled to accurately discriminatethe private classes in semantic segmentation tasks, whichdemanded a higher degree of spatial information. The UDAmethods also faced challenges in effectively distinguish-ing private classes when simply leveraging a confidence-based approach. In contrast, our proposed approach signif-icantly outperformed the other comparison methods in H-Score. Especially, compared to the best baseline, our pro-posed BUS achieved a performance improvement of about",
  ". Qualitative comparison of our method with MIC, confidence-based MIC (Config. A), and head-expansion (Config. B) on theGTA5 Cityscapes. GT represents the ground truth": ". Qualitative comparison of our method with head-expansion (Config. B), DECON loss (Config. C), and OpenReMix (Config. D)on the SYNTHIA Cityscapes. GT represents the ground truth.+39.45% compared to DAF in GTA Cityscapes andabout +23.19% compared to HRDA in SYNTHIA Cityscapes.This experiment demonstrated the effective-ness of our method in discriminating private classes whilemaintaining the performance of common classes. A moredetailed examination revealed that we achieved a signifi-cant improvement in the private class IoU score to approx-imately +40.79% compared to the DAF , and also anincrease in the common class mIoU score of about +8.65%compared to the HRDA . This showed that our pro-posed method not only improved the performance of theprivate class but also contributed to a slight improvementin the common classes. This is because DECON loss en-couraged features of the private class near the boundary toconverge while distancing themselves from features of the common class. This reduced confusion between the com-mon and private classes, improving predictions of the com-mon class. Moreover, since OpenReMix was designed tolearn size-invariant features regardless of the common andprivate classes, it enhanced the accuracy of predicting theshape of both common and private classes. We also com-pared with BUDA . Since BUDA had access to pre-defined private category definitions and direct comparisonwas not practical, we offered a comparative analysis in sup-plementary material.",
  ". Ablation study of the components in our BUS framework.Configuration A, B, C, and D represent confidence-based MIC,head-expansion, DECON loss, and OpenReMix, respectively": "with MIC, confidence-based MIC (Config.A), and thehead-expansion approach (Config.B) in the GTA Cityscapes (see ). Furthermore, we compared ourmethod with the head-expansion approach (Config. B), theincorporation of a new DECON loss (Config. C), and theutilization of the new OpenReMix (Config. D) in the SYN-THIA Cityscapes (see ). In , we ob-served that the UDA method MIC, which was designedfor UDA without considering unknown classes, struggledto detect the private classes in OSDA-SS. Even baselineslike confidence-based MIC (Config. A) and head-expansion(Config. B) faced challenges in identifying private classes.Although head-expansion showed promise, it still had lim-itations in classifying specific pixels in private classes. Incontrast, our method excelled, particularly in discerning ob-ject size. In , our proposed DECON loss and Open-ReMix yielded outstanding performance.",
  ". Ablation Study": "Ablation Study.We conducted an ablation study for theproposed components of the BUS framework on GTA5 Cityscapes.In , row A and B representedthe confidence-threshold and head-expansion baselines, re-spectively.The confidence-threshold baseline (Config.A) recorded inferior performance compared to the head-expansion baseline (Config. B). It revealed that leverag-ing the expanded head was effective in detecting unknownclasses, achieving H-Scores from 19.71% to 43.79%. Whenwe combined DECON loss with the head-expansion, weachieved a +13.78% improvement in the H-Score (see rowC). We also confirmed the effectiveness of our proposedOpenReMix. We gained a +14.55% improvement in theH-Score (see row D). Lastly, using both DECON and Open-ReMix on head-expansion significantly improved H-Scoreof +19.02%. showed a clear improvement inpredicting the unknown compared to the MIC with head-expansion approach (Config. B), and we observed synergyin overcoming individual drawbacks when compared to DE-CON loss (Config. C) and OpenReMix (Config. D).",
  ". Conclusion": "To tackle this challenging OSDA-SS task, we proposed anovel method named BUS. Our approach includes DECONloss, a new dilation-erosion-based contrastive loss designedto rectify less confident and erroneous predictions near classboundaries. In addition, we proposed OpenReMix guidingthe model to acquire size-invariant features and efficientlytrain the expanded head by mixing unknown objects fromthe target into the source. Through extensive experiments,we demonstrated the efficacy of our proposed method onpublic benchmark datasets, surpassing existing approachesby a significant margin. We anticipate that our work will bewidely applied in research or the industry field, providinga strong baseline to detect unexpected and unseen objectsin mission-critical scenarios. As a limitation, our method isprimarily based on pseudo-labeling. Therefore, if the modelis poorly calibrated, it might not assign pixels belonging tothe private classes as unknown. In this case, BUS mightshow a performance drop. This work was supported by MSIT (Ministry of Scienceand ICT), Korea, under the ITRC (Information TechnologyResearch Center) support program (IITP-2024-RS-2023-00258649) supervised by the IITP (Institute for Information& Communications Technology Planning & Evaluation),and in part by the IITP grant funded by the Korea Govern-ment (MSIT) (Artificial Intelligence Innovation Hub) underGrant 2021-0-02068, and by the IITP grant funded by theKorea government (MSIT) (No.RS-2022-00155911, Artifi-cial Intelligence Convergence Innovation Human ResourcesDevelopment (Kyung Hee University)).",
  "A. Implementation Details": "In this section, we provide further implementation details ofthe proposed method. For DECON loss, we crop the targetprivate map to a size of 6464. Then, we apply the dila-tion and erosion function. The results of the dilation anderosion functions vary depending on the kernel size and it-erations. In this study, we utilize 33 kernel size and 1iteration. For OpenReMix, we select one thing class fromthe source image, resize it, and paste it to the random loca-tion of the target image. Here, we resize the selected classby a ratio of 0.5 with bilinear downsampling. We representan example of OpenReMix in . The resized thingclass is marked with a yellow mask. In the attaching pri-vate process, the parts predicted as unknown from the tar-get image are attached to the source image. That parts areindicated with a red mask. Additionally, we utilize Mobile-SAM as a refinement network, which is a lightweightversion of the Segment Anything Model (SAM) for im-age segmentation. MobileSAM is a highly generalized im-age segmentation model that can provide reasonable masksfor objects in an image even in zero-shot scenarios, but itcannot provide labels. Leveraging these label-less but pre-cise masks, we refine the pseudo-labels. For each generatedmask, the pixel count for each class is calculated, and the re-gion of the mask is replaced entirely with the most frequentclass. We apply the last 3k iterations every 10k iterations,resulting in a total of 12k iterations out of 40k iterations.And, we also apply the attaching private process in Open-ReMix only when pseudo-label refinement is applied.",
  "B.1. Crop Size in DECON Loss": "We randomly crop the target private mask and apply thedilation and erosion operation for DECON loss. shows the experimental results on the effect of the crop size.In terms of the H-Score, we confirm the robust performanceacross different crop sizes. And it shows the best perfor-mance when cropped to a size of 6464. Additionally, weobserve that the performance significantly decrease in thecase of 128128. This is because, when too much targetprivate information is included in the mask, the anchor can-not reflect the specific characteristics of a particular targetprivate class.",
  ". Sensitivity of resizing scale in OpenReMix": ", we increase the size from 3 3 to 7 7. Weconfirm that as the kernel size increases, the performancedecreases for both scenarios. As the kernel size increases, itconsiders features further away from the boundary. There-fore, it hinders the model from focusing on the boundaryregions where it is difficult to distinguish between knownand unknown classes.",
  "B.3. Resizing Scale in OpenReMix": "We provide the results on various resizing factors for Open-ReMix in . For each iteration, we randomly selectthe scale factor from a uniform distribution within a speci-fied range. From this result, we confirm that the proposedOpenReMix is robust to scale factors, and a simply fixedscale factor of 0.5 is enough to learn size-invariant featuresfor our model.",
  "B.4. Threshold in Pseudo-Label Generation": "We study the influence of different thresholds p for assign-ment of unknown classes during pseudo label generation. shows the results under the various values of pin GTA5 Cityscapes scenario. We observe that for anyvalue other than p = 0.5, the performance degrades signifi-cantly. Therefore, our method is sensitive to p, so selectingan appropriate threshold is important.",
  "C. Comparison with Other Baselines": "Our proposed methods can be applied to existing self-training-based UDA methods. Therefore, we present the re-sults applying the head expansion baseline to existing UDAmethods, as well as the results incorporating the two com-ponents we propose, which are DECON loss and Open-ReMix. In , we confirm the increase of H-Scorefor DAF by +8.29% and for HRDA by +9.19%.Particularly, in the case of MIC , there was a substantialincrease of +19.02%. We confirm that the better the perfor-",
  ". Experiments on randomly selected private categories. Weconducted three experiments and presented the average deviation": "mance of UDA, the better the performance when applyingour proposed methods. This is because DECON loss andattaching private process are based on the quality of pseudolabels. Therefore, the models that generate more accuratepseudo-labels have an advantage. We also compare with the most similar work BUDA to our method.In BUDA, models have access to pri-vate category definitions, a crucial assumption not sharedby OSDA-SS. In our OSDA-SS setting, there is no provi-sion for such private category definitions. In OSDA-SS,one should devise a method that rejects novel classes with-out needing to know any information about their definition.In BUDA, one should devise a method that predicts novelclasses explicitly at the expense of predefined class defini-tions. Given the fundamental differences between OSDA-SS and BUDA, direct comparison is not practical. Nonethe-less, we offer a comparative analysis in . To demon-strate the applicability of our proposed methods to variousdatasets, we conduct experiments on a new dataset calledIDD (India Driving Dataset). Please note that BUDA hasthe privilege to access novel class definitions while BUS donot.",
  "D. More Experiments about Private Classes": "In the main paper, we experimented with a total of six pri-vate classes in the GTA Cityscapes scenario and includedresults for scenarios where the number of private classesdecreases. In , we further present the comparisonresults when the number of private classes increase. For8 private classes, we include (M.bike, Bike), and for10 unknown classes, we additionally add (Light, Bus).Despite an increase of the number of private classes, ourmethod still outperform the other baselines.As we mentioned in the scenario construction section inmain paper, we selected private classes from the thing cate-gories. While it is rare for stuff classes to emerge in the realworld, we conduct experiments on cases where stuff classesare also treated as private classes. We randomly select 6privates out of 19 classes regardless of thing and stuff cat-egories in GTA5 Cityscapes scenario. demon-strates that BUS still outperforms the previous baseline invarious settings with a significant margin.",
  "E. What if using (C + N) heads?": "In our OSDA-SS task, the number of private classes N isunknown since target private labels are absent. In that sense,setting N = 1 for the private class is a reasonable op-tion. Despite this, we experiment using (C +N) heads withrandom pseudo-labeling. Understandably, demon-strates that our BUS shows the best performance when N isset to 1.",
  "Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the kitti vision benchmarksuite. In 2012 IEEE conference on computer vision and pat-tern recognition, 2012. 1": "Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli,Xin Lu, and Ming-Hsuan Yang. Deep image harmonization.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2017. 1 Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fullyconvolutional networks for semantic segmentation. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition (CVPR), 2015. 1, 2",
  "segmentation with deep convolutional nets, atrous convolu-tion, and fully connected crfs. IEEE transactions on patternanalysis and machine intelligence, 2017. 2, 6": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,Jose M Alvarez, and Ping Luo.Segformer: Simple andefficient design for semantic segmentation with transform-ers.Advances in Neural Information Processing Systems(NeurIPS), 2021. 1, 6 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki-hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker.Learning to adapt structured output space for semantic seg-mentation. In Proceedings of the IEEE conference on com-puter vision and pattern recognition (CVPR), 2018. 1, 3, 6 Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, MatthieuCord, and Patrick Perez. Advent: Adversarial entropy min-imization for domain adaptation in semantic segmentation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2019. Luke Melas-Kyriazi and Arjun K Manrai. Pixmatch: Unsu-pervised domain adaptation via pixelwise consistency train-ing. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), 2021. 3, 5,6 Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Daformer:Improving network architectures and training strategies fordomain-adaptive semantic segmentation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2022. 3, 6, 7, 2 Lukas Hoyer, Dengxin Dai, and Luc Van Gool.Hrda:Context-aware high-resolution domain-adaptive semanticsegmentation. In In Proceedings of the European Confer-ence on Computer Vision (ECCV), 2022. 6, 7, 2 Lukas Hoyer,Dengxin Dai,Haoran Wang,and LucVan Gool.Mic: Masked image consistency for context-enhanced domain adaptation.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2023. 1, 3, 5, 6, 2",
  "Stephan R Richter, Vibhav Vineet, Stefan Roth, and VladlenKoltun.Playing for data: Ground truth from computergames. In In Proceedings of the European Conference onComputer Vision (ECCV), 2016. 1, 5": "German Ros, Laura Sellart, Joanna Materzynska, DavidVazquez, and Antonio M Lopez.The synthia dataset: Alarge collection of synthetic images for semantic segmenta-tion of urban scenes. In Proceedings of the IEEE conferenceon computer vision and pattern recognition (CVPR), 2016.1, 5 Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, andTatsuya Harada. Open set domain adaptation by backpropa-gation. In Proceedings of the European conference on com-puter vision (ECCV), 2018. 2, 3, 6 Silvia Bucci, Mohammad Reza Loghmani, and Tatiana Tom-masi. On the effectiveness of image rotation for open setdomain adaptation. In In Proceedings of the European Con-ference on Computer Vision (ECCV), 2020.",
  "domain adversarial learning for open-set domain adapta-tion. Advances in Neural Information Processing Systems(NeurIPS), 2022. 3, 6": "Kaichao You, Mingsheng Long, Zhangjie Cao, JianminWang, and Michael I Jordan. Universal domain adaptation.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition (CVPR), 2019. 2, 6 Yahao Liu, Jinhong Deng, Xinchen Gao, Wen Li, and LixinDuan. Bapa-net: Boundary adaptation and prototype align-ment for cross-domain semantic segmentation. In Proceed-ings of the IEEE/CVF international conference on computervision (ICCV), 2021. 2, 4",
  "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, XiaogangWang, and Jiaya Jia. Pyramid scene parsing network. InProceedings of the IEEE conference on computer vision andpattern recognition (CVPR), 2017": "Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and KuiyuanYang. Denseaspp for semantic segmentation in street scenes.In Proceedings of the IEEE conference on computer visionand pattern recognition (CVPR), 2018. 2 Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, ZhiweiFang, and Hanqing Lu.Dual attention network for scenesegmentation. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition (CVPR), 2019. 2 Zilong Huang, Xinggang Wang, Lichao Huang, ChangHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-crossattention for semantic segmentation.In Proceedings ofthe IEEE/CVF international conference on computer vision(ICCV), 2019. Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi,Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In Proceed-ings of the European conference on computer vision (ECCV),2018. Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xi-ang Bai. Asymmetric non-local neural networks for seman-tic segmentation. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision (ICCV), 2019. 2",
  "ference on computer vision and pattern recognition (CVPR),2018. 3": "Myeongjin Kim and Hyeran Byun. Learning texture invari-ant representation for domain adaptation of semantic seg-mentation. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition (CVPR), 2020. Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, andIn So Kweon. Unsupervised intra-domain adaptation for se-mantic segmentation through self-supervision. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), 2020. Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, and Manmo-han Chandraker. Domain adaptation for structured outputvia discriminative patch representations. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), 2019. Cheng Chen, Qi Dou, Hao Chen, Jing Qin, and Pheng-AnnHeng. Synergistic image and feature adaptation: Towardscross-modality domain adaptation for medical image seg-mentation. In Proceedings of the AAAI conference on ar-tificial intelligence (AAAI), 2019. Liang Du, Jingang Tan, Hongye Yang, Jianfeng Feng, Xi-angyang Xue, Qibao Zheng, Xiaoqing Ye, and XiaolinZhang. Ssf-dan: Separated semantic feature based domainadaptation network for semantic segmentation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision (ICCV), 2019. 3 Yang Zou, Zhiding Yu, B.V.K. Vijaya Kumar, and JinsongWang. Unsupervised domain adaptation for semantic seg-mentation via class-balanced self-training. In Proceedingsof the European Conference on Computer Vision (ECCV),2018. 3",
  "Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, andJinsong Wang.Confidence regularized self-training.InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), 2019": "Qin Wang, Dengxin Dai, Lukas Hoyer, Luc Van Gool, andOlga Fink.Domain adaptive semantic segmentation withself-supervised depth estimation.In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), 2021. Qing Lian, Fengmao Lv, Lixin Duan, and Boqing Gong.Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A non-adversarial approach.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), 2019. Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectionallearning for domain adaptation of semantic segmentation. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), 2019.",
  "IEEE/CVF International Conference on Computer Vision(ICCV), 2021": "Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang,and Fang Wen. Prototypical pseudo label denoising and tar-get structure learning for domain adaptive semantic segmen-tation. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition (CVPR), 2021. Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, andLennart Svensson.Dacs: Domain adaptation via cross-domain mixed sampling. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vision(WACV), 2021. 3, 4, 6 Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang,and Qiang Yang.Separate to adapt:Open set domainadaptation via progressive separation.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition (CVPR), 2019. 3",
  "Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada.Between-class learning for image classification. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2018. 3": "Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, andLennart Svensson. Classmix: Segmentation-based data aug-mentation for semi-supervised learning. In Proceedings ofthe IEEE/CVF Winter Conference on Applications of Com-puter Vision (WACV), 2021. 3, 5, 1 Sangdoo Yun, Dongyoon Han, Seong Joon Oh, SanghyukChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-larization strategy to train strong classifiers with localizablefeatures. In Proceedings of the IEEE/CVF international con-ference on computer vision (ICCV), 2019.",
  "Geoff French, Avital Oliver, and Tim Salimans.Milkingcowmask for semi-supervised image classification.arXivpreprint arXiv:2003.12022, 2020": "Li Gao, Jing Zhang, Lefei Zhang, and Dacheng Tao. Dsp:Dual soft-paste for unsupervised domain adaptive semanticsegmentation. In Proceedings of the 29th ACM InternationalConference on Multimedia, 2021. 3 David Berthelot, Nicholas Carlini, Ian Goodfellow, NicolasPapernot, Avital Oliver, and Colin A Raffel. Mixmatch: Aholistic approach to semi-supervised learning. Advances inneural information processing systems (NeurIPS), 2019.",
  "Yuan Wu, Diana Inkpen, and Ahmed El-Roby. Dual mixupregularized learning for adversarial domain adaptation. In InProceedings of the European Conference on Computer Vi-sion (ECCV), 2020": "Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, ChengjieWang, Qi Tian, and Wenjun Zhang.Adversarial domainadaptation with domain mixup.In In Proceedings of theAAAI Conference on Artificial Intelligence, 2020. 3 Lin Chen, Zhixiang Wei, Xin Jin, Huaian Chen, Miao Zheng,Kai Chen, and Yi Jin. Deliberated domain bridging for do-main adaptive semantic segmentation. Advances in NeuralInformation Processing Systems (NeurIPS), 2022. 3 Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, andGyeong-Moon Park. Online class incremental learning onstochastic blurry task boundary via mask and visual prompttuning. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision (ICCV), 2023."
}