{
  "Abstract": "Recent works utilize CLIP to perform the challenging unsupervised semantic segmentation task where onlyimages without annotations are available. However, we observe that when adopting CLIP to such a pixel-levelunderstanding task, unexpected bias (including class-preference bias and space-preference bias) occurs. Previousworks dont explicitly model the bias, which largely constrains the segmentation performance. In this paper, wepropose to explicitly model and rectify the bias existing in CLIP to facilitate the unsupervised semantic segmenta-tion task. Specifically, we design a learnable Reference prompt to encode class-preference bias and a projectionof the positional embedding in the vision transformer to encode space-preference bias respectively. To avoid inter-ference, two kinds of biases are firstly independently encoded into different features, i.e., the Reference featureand the positional feature. Via a matrix multiplication between the Reference feature and the positional feature,a bias logit map is generated to explicitly represent two kinds of biases. Then we rectify the logits of CLIP via asimple element-wise subtraction. To make the rectified results smoother and more contextual, we design a maskdecoder which takes the feature of CLIP and the rectified logits as input and outputs a rectified segmentation maskwith the help of Gumbel-Softmax operation. A contrastive loss based on the masked visual features and the textfeatures of different classes is imposed, which makes the bias modeling and rectification process meaningful andeffective. Extensive experiments on various benchmarks including PASCAL VOC, PASCAL Context, ADE20K,Cityscapes, and COCO Stuff demonstrate that our method performs favorably against previous state-of-the-arts.The implementation is available at:",
  "Introduction": "Semantic segmentation aims to attach a semantic labelto each pixel of an image. Since the rise of deep learn-ing , semantic segmentation has been widelyadopted in real-world applications, e.g., autonomousdriving, medical image segmentation, etc. Conven-tional approaches for semantic segmentationhave achieved remarkable performance. However, thesuperior performance of those methods relies heavily on large amounts of fully annotated masks. Collect-ing such high-quality pixel-level annotations can beboth time-consuming and expensive, e.g., some anno-tations for specialized tasks require massive expertknowledge, some are even inaccessible due to privacyreasons, etc. Therefore, it is necessary to explore unsu-pervised semantic segmentation where only imageswithout annotations are available.Unsupervised semantic segmentation (USS) hasbeen studied for years. Many non-language-guided",
  "arXiv:2408.06747v2 [cs.CV] 8 Jan 2025": "USS methods have been proposed, e.g., clustering-based methods , contrastive-learning-basedmethods and boundary-based methods ,etc. Despite promising progress achieved, there stillexhibits a large performance gap between USS andthe supervised segmentation methods. Besides, thesemethods typically obtain class-agnostic masks andhave to depend on additional processing (e.g., Hungar-ian matching) to assign semantic labels to the masks,rendering them less practical in real scenarios.Recently, large-scale visual-language pre-trainedmodels, e.g., CLIP , demonstrate superior zero-shot classification performance by comparing thedegree of alignment between the image feature andtext features of different categories. A few CLIP-based USS approaches emerge and showremarkable performance improvement compared withthe non-language-guided USS methods. These mod-els require no access to any types of annotations, andcan directly assign a label to each pixel, benefitingfrom the aligned vision and text embedding space ofCLIP. However, good alignment between image-levelvisual feature and textual feature doesnt necessar-ily mean good alignment between pixel-level visualfeature and textual feature. Thus, for CLIP, unex-pected bias may inevitably appear. Previous worksdont explicitly model such bias, which may largelyconstrain their segmentation performance.WhendirectlyadoptingCLIPinUSSlikeMaskCLIP , we observe two kinds of biases exist-ing in CLIP. From one aspect, as shown in (a),CLIP exhibits space-preference bias. CLIP performsapparently better for segmenting central objects thanthe ones distributed near the image boundary. It canbe reflected by the fact that mIoU decreases as thedistance between the object centroid and the imagecentroid increases. From the other aspect, as shown in (b), there exists class-preference bias betweensemantically relevant categories in CLIP. For example,according to visualizations (right), when the groundtruth is a sheep (dark yellow), CLIP tends to incor-rectly classify it as a cow (green). We also showsuch a trend between randomly selected classes byconfusion matrix (left). Elements on the diagonalline represent the right classification, while others arefalse. We observe that besides ground truth, CLIP usu-ally prefers to assign an incorrect but semanticallyrelevant label to a pixel in quite a few cases, exhibitinga wide range of class-preference bias of CLIP. In this paper, we propose to explicitly model andrectify the bias of CLIP to facilitate the USS. Specifi-cally, we design two kinds of text inputs for each class,which are named Query and Reference respec-tively. The Query is manually designed and fixedwhile the Reference contains learnable prompts. Wepass the Query and Reference through the text encoderof CLIP to obtain the Query feature and Referencefeature. We adopt the Query feature as the segmenta-tion head to generate the Query logits for each pixelof the image, which represents the original segmen-tation ability of vanilla CLIP. In contrast, we utilizethe Reference feature to encode the class-preferencebias. In addition, we project the positional embed-ding of CLIPs vision transformer to generate posi-tional feature, which encodes the space-preferencebias. The encoding processes of two biases are inde-pendent to avoid interference. Via a matrix multiplica-tion between the Reference feature and the positionalfeature, a bias logit map is generated to explicitlyrepresent two kinds of biases. Then, we remove thebias from the original CLIP via a logit-subtractionmechanism, i.e., subtracting the bias logits from theQuery logits. To make the rectified results smootherand more contextual, we concatenate the rectified logitmap to the feature extracted by CLIPs visual encoderand pass them through a designed decoder. Then weapply the Gumbel-Softmax operation to the output ofthe decoder to generate a rectified mask. To makethe bias modeling and rectification process meaning-ful and effective, the contrastive loss based on maskedvisual features (i.e., applying rectified masks to thevisual feature of CLIP and pooling) and text featuresof different categories is imposed.We conduct extensive experiments on five stan-dard semantic segmentation benchmarks, includ-ing PASCAL VOC , PASCAL Context ,ADE20K , Cityscapes and COCO Stuff .Experiment results demonstrate that ReCLIP++ per-forms favorably against previous state-of-the-arts.Notably, on PASCAL VOC, our method outperformsMaskCLIP+ by 15.4% and CLIP-S4 by13.4% mIoU. Extensive ablation studies verify theeffectiveness of each design in our framework.This paper is an extension of our conferencepaper . To differentiate, we denote the methodproposed in our conference version as ReCLIP andthat in this paper as ReCLIP++. Compared to ourconference version, we make further contributions,which are summarized as follows: 1) We optimize",
  "Image Ground Truth MaskCLIP Ours": "(a) Space-preference bias. (Left): The relationship between distance (x-axis) and mIoU (y-axis) is drawn on PASCAL VOC .The distance means the spatial distance between the object centroid and the image centroid and mIoU is computed based on predictions andground truth. The curve shows that MaskCLIP (pink) is apparently better at segmentation for central objects than boundary ones, butour method ReCLIP++ (blue) effectively mitigates this bias. More details about how we draw this figure have been shown in our appendix.(Right): Visualizations also show our improvement in space-preference bias qualitatively. (b) Class-preference bias. (Left): We randomlyselect 6 classes from PASCAL Context and draw the confusion matrix of MaskCLIP and our model. It shows that besides the groundtruth, MaskCLIP also prefers to assign an incorrect but semantically relevant label to a pixel in quite a few cases, while our results show anapparent improvement. (Right): The visualizations are consistent with what we observed in the confusion matrix. For example, for a sheep(dark yellow), MaskCLIP tends to classify it as a cow (green) incorrectly. the design of the Bias Extraction Module. Specifi-cally, we independently encode class-preference biasand space-preference bias into class-level Referencefeature and patch-level positional feature respectively.Then the bias logit map, which explicitly representsboth biases, is obtained by combining two featureswith matrix multiplication. 2) We additionally intro-duce a mask decoder, which takes the rectified logitmap and the visual feature of CLIP as input andoutputs smoother and more contextual rectified pre-dictions. 3) We design a new strategy to generate amore accurate multi-label hypothesis for each image,which provides better supervision for the bias recti-fication process. 4) Benefiting from our new design,the distillation stage in ReCLIP is no longer necessary.We remove the distillation stage to simplify trainingand achieve even better segmentation performance. 5)We evaluate our methods on two more datasets includ-ing Cityscapes and COCO Stuff . On all thedatasets, the segmentation performance of ReCLIP++outperforms ReCLIP remarkably.In a nutshell, our contributions are summarized asfollows:",
  "Related Work": "Pre-trained vision-language models. Pre-trainedvision-language models (VLMs) have devel-oped rapidly with the help of large-scale image-textpairs available on the Internet. Recently, CLIP ,ALIGN and Slip have made great progress on learning visual and textual representations jointlyby using contrastive learning. With the image-levelalignment with text, pre-trained VLMs have a strongability for zero-shot classification task and can betransferred to various downstream tasks, such asobject detection and semantic segmenta-tion .Unsupervised semantic segmentation. While con-ventional approaches of semantic segmentation rely on pixel-level annotations and weakly-supervisedmethods still ask for image-level labels, unsu-pervised semantic segmentation (USS) methods explore to train a segmentation modelwithout any annotations. Models like adoptgenerative model to separate foreground withbackground or generate corresponding masks. Seg-Sort , HSG and ACSeg use clusteringstrategy, while IIC uses mutual information maxi-mization to perform unsupervised learning. MaskCon-trast introduces contrastive learning into USS.Others like DSM and LNE exploit fea-tures extracted from self-supervised models and spec-tral graph theory to facilitate segmentation. However,the methods mentioned above either fail to seg-ment images with multi-category objects or show alarge performance gap with the supervised methods.Besides, they can only obtain class-agnostic masksand have to depend on additional strategies, such asHungarian-matching algorithm , to match the cor-responding category with the segment. Recently, pre-trained vision-language models are adopted in USS.MaskCLIP modifies the image encoder of CLIPto generate patch-level features and directly performssegmentation with text features as classifiers. CLIP-py performs contrastive learning between visualfeatures from self-supervised ViT and text fea-tures from CLIP. ReCo performs image retrievalwith CLIP and extracts class-wise embedding as clas-sifier with co-segmentation. CLIP-S4 learns pixelembeddings with pixel-segment contrastive learningand aligns such embeddings with CLIP in terms ofembedding and semantic consistency. These methodscan directly assign a label to each pixel, which fallsinto the category of language-guided unsupervisedsemantic segmentation. However, directly applyingCLIP in USS may result in unexpected bias. No pre-vious method considers such bias. In this paper, wepropose to explicitly model and rectify the bias ofCLIP for unsupervised semantic segmentation.Language-guided semantic segmentation. Recently,many works explore semantic segmentation guided bylanguageunderdifferentsettings.Zero-shotworks split classes into seen and unseen sets.During the training period, only masks of seen classesare provided. For inference, models are tested on bothseen and unseen classes, but the test data is still inthe same distribution as the training data. Trainableopen-vocabulary works are trained inone scenario with extra annotations including class-agnostic masks or image captions but are used forpredicting segmentation masks of novel classes inother scenarios. Recent training-free open-vocabularyworks adapt CLIP for semantic segmenta-tion by modifying its architecture. These methods candirectly perform inference on downstream datasetswithout any training. From the technical view, ourmethod also falls into the category of language-guidedsemantic segmentation. However, we consider theunsupervised setting. In this setting, we have accessto images without any annotations during training.The training and inference images are sampled fromthe same distributions and the same set of categories.Such a setting is different from the typical zero-shotor trainable open-vocabulary setting.",
  "Method": "Background In this work, we aim to rectify the biasof CLIP for unsupervised semantic segmentation. InUSS, we only have access to images without any typesof annotations to train the segmentation model. Fortraining and inference, the same set of categories areconsidered and the data distributions are assumed tobe the same.Overview We aim to rectify the bias of CLIP includ-ing the class-preference bias and the space-preferencebias, to facilitate unsupervised semantic segmenta-tion. From a high level, class-preference bias reflectsthe shift of CLIP predictions towards specific classes,while space-preference bias reflects the shift of CLIPpredictions towards specific spatial locations. Bothbiases will be finally reflected in the bias logit map.A reasonable way to rectify the bias is to subtract thebias logit map from the query logit map predicted byoriginal CLIP.The general framework of our method is illustratedin . We first forward the image I R3HW through the image encoder of CLIP to obtain thepatch-level image feature Z. For each class, we man-ually design the text input which is named Query Q,and fix Q throughout the training. We pass Q through the text encoder of CLIP to obtain query text fea-ture Wq for each class, which serves as the weightof the query segmentation head. With Wq, we canobtain a query logit map Mq, which represents thesegmentation ability of the original CLIP (Sec. 3.1).We extract the bias existing in CLIP in a learnableway. Specifically, we design a kind of learnable textinput named Reference R for each class. Passing Rthrough the text encoder of CLIP, we obtain Referencetext feature Wr, which is expected to encode the class-preference bias. Meanwhile, the positional embeddingis projected into the positional feature Wp to encodethe space-preference bias. Then the bias logit map Mbis extracted via a matrix multiplication between Wrand Wp (Sec. 3.2).The Query logit map and the bias logit map arethen passed through a Rectified Mask Generationmodule to generate rectified semantic masks. Firstly,a rectified logit map M is generated via a simple sub-traction operation between Mq and Mb. Then, a maskdecoder takes M and visual feature Z as input andoutputs smoother and more contextual semantic maskswith the help of Gumbel-Softmax operation (Sec. 3.3).To enable a more meaningful and effective biasrectification, we generate a multi-label hypothesis foreach image and impose contrastive loss based onmasked visual features and Query text features of dif-ferent classes (Sec. 3.4). CLIP is kept frozen duringthe training process.",
  "Baseline: Directly Segment with CLIP": "Following MaskCLIP , we adapt pre-trainedCLIP (ViT-B/16) to the semantic segmentationtask. We remove the query and key embedding layersof last attention but reformulate the value embed-ding layer and the last linear projection layer into tworespective 1 1 convolutional layers. Therefore, theimage encoder can not only generate the patch-levelvisual feature for dense prediction but also keep thevisual-language association in CLIP by freezing itspre-trained weights. We forward image I through theimage encoder and obtain patch-level feature Z RnD (n is the number of patches).Each text Qi in Query Q = {Q1, Q2, , QC} (Cis the number of classes) is an ensembling of severalmanually designed templates, e.g., a good/large/badphoto of a [CLS], where [CLS] denotes a specificclass name. Passing Qi through the text encoder, weobtain different text embeddings for different man-ually designed templates. We average all those text embeddings as the embedding of Qi. Then the embed-dings of all the Qi in Q make up the query textfeature Wq RCD. We treat text feature Wq as theweight of the segmentation head to perform 1 1 con-volution. By sending feature Z to the segmentationhead, we get a Query logit map Mq RnC. Thenthe segmentation mask can be predicted by arg maxoperation on Mq across the class channel C.",
  "Bias Extraction": "In the Bias Extraction Module, we aim to encode theclass-preference bias and the space-preference bias ofCLIP independently. Then we combine them into abias logit map to explicitly model both biases.Class-preference Bias Encoding. In order toencode the class-preference bias brought by pre-trained CLIP with respect to a specific segmentationtask, we design learnable Reference text Ri in R ={R1, R2, , RC} as an additional text input for eachclass. Inspired by CoOp , Ri consists of a learn-able prompt which is shared across all the classes forefficiency, and a class name [CLS], which can beformed as",
  "Ri = [v1][v2]...[vl]...[vL][CLS],(1)": "where each [vl](l {1, ..., L}) is a vector with dimen-sion D and serves as a word embedding. The L is aconstant representing the number of word embeddingsto learn. Totally, there are 77 word embeddings for atextual input of CLIP. Among these, two word embed-dings are used for representing class names and twofor indicating the start and end of a textual input. Thus,we set L to 73.Passing Reference R through the text encoder, weobtain reference text feature Wr RCD, which isexpected to encode the class-preference bias. As Ref-erence is only class-aware, the Reference feature isless likely to encode any space-related information.Space-preference Bias Encoding. In ViT ,positional embedding (PE) is important for encodingspatial information into features. Thus, we assume thespace-preference bias should depend on the positionalembedding (PE) and we choose to learn a projec-tion of PE to encode the space-preference bias. Theprojection network of PE is designed as a 1-layer1 1 convolutional network. Then we project PEp by the designed convolutional network to obtainpositional feature Wp RnD. During the train-ing process, the projection network is optimized to Method overview of ReCLIP++. We propose a new framework for language-guided unsupervised semantic segmentation. We aim torectify the class-preference and space-preference bias of CLIP via specifically designed Bias Extraction module, Rectified Mask Generationmodule, and Contrastive Loss module. encode the space-preference bias with respect to eachpatch of the image. Note that the encoding processof space-preference bias is independent of that ofclass-preference bias because the encoding of space-preference bias doesnt rely on any class information.Bias Logits Generation. With Reference featureWr encoding the class-preference bias and positionalfeature Wp encoding the space-preference bias, wecombine the encoding results of two biases to gener-ate the final bias logit map Mb RnC, via a matrixmultiplication as",
  "Mb = Wp W Tr .(2)": "Finally, Mb represents the final bias that we aim toremove from the predicted query logit map Mq of theoriginal CLIP.Bias Extraction: ReCLIP++ vs. ReCLIP. Notethat the way of bias extraction in ReCLIP++ is differ-ent from that in ReCLIP . In ReCLIP, to extractthe class-preference bias, we compute the similar-ity between Reference features of different classesand CLIPs visual feature map Z to generate aclass-preference bias logit map. However, as CLIPsimage encoder takes PE as input to encode spatial information into feature Z, the class-preference biaslogit map may inevitably encode some space-relatedinformation. Consequently, the implicit space-relatedinformation in extracted class-preference bias mayinterfere with the extraction of space-preference bias,resulting in less effective bias extraction and rectifica-tion.Differently, in ReCLIP++, we first encode class-preference bias and space-preference bias into theReference feature and positional feature respectively.The reference feature only depends on class-relatedinformation (i.e., learnable Reference text), while thepositional feature only depends on space-related infor-mation (i.e., PE). Thus, in this way, we may indepen-dently encode two kinds of biases and largely avoidthe interference. As a result, with ReCLIP++, we real-ize more effective bias rectification and achieve bettersegmentation results (see Sec. 4 for more details).",
  "M = Mq Mb.(3)": "From a high level, this operation can be interpretedas subtracting the bias from the predictions withQuery text feature Wq serving as the weight of thesegmentation head.Though the rectified logit map M can be directlyused to generate semantic masks, the local contextinformation is not considered, which may result innon-smooth mask predictions. Thus, different fromour conference version , we additionally introducea mask decoder Fdec to transform M into a smootherand more contextual logit map. In our implementation,Fdec consists of a convolutional layer with a 55 ker-nel and a batch normalization layer. We concatenatethe rectified logit map M to the visual feature Z ofCLIP and pass them through the mask decoder Fdecto generate the rectified output Mo RnC, i.e.,",
  "Mo = Fdec(M, Z).(4)": "To generate the semantic mask M RnC foreach category, a natural way is to employ arg maxoperation to Mo across the class channel C and gen-erate the one-hot label for each spatial location. Then,each channel of M corresponds to the mask for a spe-cific category. However, as the arg max operation isnot differentiable, we are not able to optimize M end-to-end through gradient back-propagation during thetraining process. Thus we utilize a Gumbel-Softmaxtrick to generate the semantic mask for eachcategory with 1 as the temperature, i.e.,",
  "In this section, we introduce a contrastive loss tosupervise the rectification process": "Image-level multi-label hypothesis generation.In order to compute the contrastive loss, we need toknow what classes exist in a specific image. However,this information is not available under the USS setting.In this section, we propose to generate an image-levelmulti-label hypothesis which means the set of classesthat potentially exist in an image, to facilitate thefollowing contrastive loss calculation.In ReCLIP , we directly calculate the sim-ilarity scores between the class-level text featuresof manually designed prompts, e.g., a photo of a[CLS], and the image-level visual feature extractedby CLIP. We select the set of classes whose scoresare higher than a threshold as the hypothesis. AsCLIP is pre-trained to align image-level visual fea-tures with text features, it may focus on the mostsalient objects in an image and fail to recognize everyexisting object, rendering the multi-label hypothesisgenerated for ReCLIP less accurate.In ReCLIP++, we design a new strategy to gen-erate a more accurate multi-label hypothesis for animage. Given an image, we first split it into overlappedcrops. Specifically, we slide a window across an imagewith a certain stride. In our implementation, we setthe window width and height to r times the width andheight of the input image and set the horizontal andvertical stride to half of the window width and heightrespectively. For each crop, we forward it through thevisual encoder of CLIP to obtain its global visual fea-ture and compute the similarity scores between theglobal visual feature and the Query feature Wq. Wetreat the class with the highest score as the detectedclass for this crop. Then for a given image, among allof its crops, we calculate the frequency f(k) of the k-th class, k {1, 2, ..., C}, being detected. We choosethe classes with frequency higher than the threshold tas the multi-label hypothesis H for an image, i.e.,",
  "H = {k|k {1, 2, ..., C} and f(k) > t}.(6)": "Bias Rectification via Contrastive Loss. We thendesign a contrastive loss to supervise the rectificationprocess via aligning masked features with text featuresof corresponding classes. Specifically, we apply M tothe feature Z and perform a global average poolingto get the class-level masked features Zg RCD,which encode the features of regions belonging to dif-ferent classes. We compute the similarities betweenZg and text features of all the categories. As themasked features Zg represent the objects of interestwithout context, we utilize Wq generated by text input",
  "kHlogexp{Sk,k/}Cj=1 exp{Sk,j/}(7)": "where Sk,j denotes the cosine similarity betweenvisual feature of class k H and the text featureof j-th (j {1, 2, , C} category and the is aconstant.A better modeling of bias yields more accurateestimations of object masks. Then the masked featuresof objects are more aligned with the correspondingtext features. As a result, the contrastive loss (Eq. (7)will therefore be lower. In contrast, a worse modelingof bias results in higher contrastive loss. Thus, mini-mizing Eq. (7) will drive the model to update towardsmaking more accurate mask predictions, i.e., recti-fying the bias of CLIP when adapting CLIP to thedownstream USS task.Note that in ReCLIP, an additional distillationstage is employed, where the knowledge of rectifiedCLIP is distilled into a specifically designed segmen-tation network like DeepLab-v2 . However, inReCLIP++, we dont need such a distillation stage.Empirically, we observe that such a distillation pro-cess has two opposing effects. On the one hand, itmay benefit the smoothness of the predicated mask,which has a positive effect on the segmentation per-formance. On the other hand, instead of mitigating thebias, it may enlarge the bias to an extent, which has anegative effect on the segmentation performance. Ben-efitting from Fdec, the rectified mask of ReCLIP++ ismuch smoother and contextual than that in ReCLIP,rendering the negative effect of distillation outweighthe positive effect of distillation. Thus, we choosenot to employ the distillation in ReCLIP++ to makethe training more simplified and achieve even bet-ter segmentation performance (see Sec. 4.4 for moredetails).",
  "Inference": "For the inference with ReCLIP++, we obtain the querylogit map Mq by the query segmentation head and thebias logit map Mb which models both biases by theBias Extraction Module. We then obtain the rectifiedoutput Mo by the Rectified Mask Generation Modulewith Eq. (4). With a simple arg max operation to Moacross the class channel C, we generate the rectifiedmasks as final predictions.",
  "Setup": "Datasets. We conduct experiments on five stan-dard benchmarks for semantic segmentation, includ-ing PASCAL VOC 2012 , PASCAL Con-text , ADE20K , Cityscapes and COCOStuff . PASCAL VOC 2012 (1,464/1,449 train/-validation) contains 20 object classes, while PAS-CAL Context (4,998/5,105 train/validation) is anextension of PASCAL VOC 2010 and we con-sider 59 most common classes in our experiments.ADE20K (20,210/2,000 train/validation) is a segmen-tation dataset with various scenes and 150 most com-mon categories are considered. Cityscapes (2,975/500train/validation) consists of various urban sceneimages of 19 categories from 50 different cities.COCO Stuff (118,287/5,000 train/validation) has 171low-level thing and stuff categories excluding back-ground class. Following the previous evaluation pro-tocol of unsupervised semantic segmentation meth-ods , we use 27 mid-level categories fortraining and inference.Implementation details. For the image encoder ofCLIP , we adopt ViT-B/16 as our visual back-bone. For the text encoder of CLIP, we adopt Trans-former . During the whole training period, wekeep both of the encoders frozen. We use conven-tional data augmentations including random croppingand random flipping. We use an SGD optimizerwith a learning rate of 0.01 and a weight decay of0.0005. We adopt the poly strategy with the power of0.9 as the learning rate schedule. We set r = 1/6 forall the datasets. We set t = 7% for PASCAL VOCand Cityscapes, and t = 0 for the rest datasets. Inour experiment, we report the mean intersection overunion (mIoU) as the evaluation metric. More detailsabout training can be found in our appendix.",
  "Comparison with previous SOTA": "Baselines. We mainly compare our method withthree types of semantic segmentation methods toverify the superiority of our method: (1) Train-able language-guided methods for OVSS task (TL-OVSS), including GroupViT , CoCu , andTCL ; (2) CLIP-based methods for training-free OVSS task (TF-OVSS), including CLIP (vanillaCLIPvisualencoder),MaskCLIP(CLIP visual encoder with modified last-attention block), SCLIP , and ClearCLIP ; (3) CLIP-based methods for USS task (C-USS), includingMaskCLIP+ , CLIPpy , ReCo , CLIP-S4 and ReCLIP . We directly cite the cor-responding results from the original papers, exceptthat means the results are obtained by runningthe officially released source code and means theresults are cited from TCL . All the numbersreported are presented as percentages. Among these,TL-OVSS methods rely on weak annotations likeimage-caption pairs to train the model, while TF-OVSS methods can directly perform open-vocabularysegmentation without any training. C-USS methodsshare the same language-guided unsupervised settingwith ReCLIP++.Comparison. The comparisons with previous state-of-the-art methods on five benchmarks are demon-strated in . From , we have the followingobservations: (1) C-USS methods consider the samesetting as our method. We observe that ReCLIP++outperforms previous state-of-the-art C-USS methodsobviously, achieving new state-of-the-arts on all fivebenchmarks. Since ReCo employs a contextelimination (CE) trick which introduces prior knowl-edge to assist the training, we also report the resultsof ReCo by removing this trick (ReCo w/o. CE in) for a fair comparison. Notably, ReCLIP++outperforms MaskCLIP+ by 15.4%, 5.0%, 4.2%,1.3% and 4.3% respectively on the five datasets andoutperforms CLIP-S4 by 13.4% and 2.5% on PAS-CAL VOC and PASCAL Context respectively. Allthese results verify the effectiveness of our ReCLIP++in rectifying the bias of CLIP to assist unsupervisedsemantic segmentation (2) ReCLIP++ outperformsprevious state-of-the-art TF-OVSS methods, implyingthat the bias of CLIP in complex visual understand-ing tasks cannot be fully rectified by simply modifyingits architecture without any training. (3) TL-OVSSmethods consider a different setting and are trainedwith large-scale image-caption pairs. Strictly speak-ing, our method cannot be directly compared withthose works. However, the superior performance ofReCLIP++ compared to those typical TL-OVSS meth-ods still demonstrates the effectiveness of ReCLIP++.",
  "In , we verify the effectiveness of each tech-nical improvement of ReCLIP++ beyond the rectifi-cation stage of ReCLIP (rec.) (our conferenceversion): 1) we additionally introduce a decoder that": "takes the rectified logit map and visual feature ofCLIP as input and outputs smoother and more con-textual masks (denoted as Decoder); 2) we designa new strategy to generate a more accurate labelhypothesis for each image (denoted as Label Hypoth-esis); 3) we optimize the design of the bias encodingscheme to independently encode the class-preferenceand space-preference bias (denoted as Independent).In , we observe that when sequentially addingeach technical component, the segmentation perfor-mance is consistently improved, which verifies theeffectiveness of each technical contribution.Finally, as shown in , with all thesetechnical improvements, ReCLIP++ remarkably out-performs ReCLIP. Notably, we observe that evenwithout a distillation, ReCLIP++ still outperformsReCLIP on all benchmarks, which exhibits a strongerbias rectification capability of ReCLIP++ comparedwith ReCLIP. For example, on PASCAL VOC andCityscapes, ReCLIP++ outperforms ReCLIP by 9.6%and 6.6% respectively.",
  "Ablation study": "Should Query be learnable? For text inputs Q andR, we only make Reference R learnable while keepingQuery Q fixed. In this ablation, we study whether theprompt of Query should also be learnable. As shownin , we conduct experiments with learnableand fixed Query respectively. The numbers show thatfixing Query obviously outperforms making Querylearnable. The reason is when we make Query learn-able, it may also implicitly capture bias, confusing thefollowing bias rectification operations (e.g., bias logitsubtraction). Therefore, in our framework, we chooseto fix Q to make the bias rectification more effective.Effect of element-wise subtraction on bias rectifica-tion. In order to validate the effect of our element-wisesubtraction, we conduct experiments in . Wecompare our subtraction mechanism with an alterna-tive solution: instead of subtracting the bias logit mapMb from the Query logit map Mq, we add Mb to Mq.Comparisons shown in the table verify the effective-ness of our subtraction way. As we aim to remove thebias from the original CLIP, we speculate that the sub-traction operation may work as a strong prior whichregularizes the training to facilitate meaningful andeffective bias modeling.Sensitivity to t and r in image-level multi-labelhypothesis generation. When generating an image-level multi-label hypothesis as discussed in Sec. 3.4, Comparison with trainable language-guided methods for OVSS (TL-OVSS), CLIP-based methods for training-free OVSS(TF-OVSS), and CLIP-based methods for USS (C-USS) on five various benchmarks. means the results are obtained by running theofficially released source code and means the results are cited from TCL .",
  "Frozen CLIP (Ours)85.436.126.5": "adopted in ReCLIP++ is insensitive to the choices of tand r and is superior to that used in ReCLIP.Effect of Fine-tuning CLIP. We conduct experimentswith fine-tuning the image encoder of CLIP in ourframework. The results on PASCAL VOC, PASCALContext, and Cityscapes are shown in . Wefind a consistent performance decrease compared withfreezing CLIP. This is because fine-tuning CLIP onsmall-scale downstream datasets may harm the image-text alignment in pre-trained CLIP.Effect of different bias rectification. We conductexperiments to evaluate how the rectification of eachbias affects the segmentation performance in .Technically, when testing the rectification effect of theclass-preference bias, we perform average pooling onthe positional feature Wp along the spatial dimensionand expand the pooling result to the original shape ofWp. Similarly, when testing the rectification effect ofthe space-preference bias, we perform average pooling",
  "Distillation80.034.515.8": "we observe that instead of mitigating the bias, fur-ther distillation after ReCLIP++ may enlarge the bias,which has a negative effect on the final segmentationperformance. This phenomenon explains why the dis-tillation stage is no longer necessary in ReCLIP++.In ReCLIP++, thanks to Fdec, the rectified mask ofReCLIP++ is much smoother and contextual than thatof ReCLIP, rendering the negative effect of distillationoutweighs the positive effect of distillation and leadingto a decrease of segmentation performance.Qualitative Results. We visualize our segmentationresults in . It can be observed that there existapparent space-preference bias and class-preferencebias in the segmentation results of the originalCLIP (MaskCLIP). With ReCLIP, both biases canbe partially removed. Moreover, ReCLIP++ exhibitsstronger bias rectification capability compared withReCLIP, yielding the best segmentation results.",
  "Conclusion": "In this paper, we propose a new framework forlanguage-guided unsupervised semantic segmenta-tion. We observe the bias, including space-preferencebias and class-preference bias, exists in CLIP whendirectly applying CLIP to segmentation task. We pro-pose using Reference text feature to encode class-preference bias and projecting positional embeddingto encode space-preference bias independently, andthen manage to combine them into a bias logit map",
  "horse": "Qualitative Results: We visualize segmentation results on both PASCAL VOC (left) and PASCAL Context (right). From the visu-alization, we observe that ReCLIP++ outperforms both MaskCLIP and ReCLIP obviously by rectifying both class-preference bias andspace-preference bias. by matrix multiplication. By a simple element-wiselogit subtraction mechanism, we rectify the bias ofCLIP. Then a contrastive loss is imposed to make thebias rectification meaningful and effective. Extensiveexperiments demonstrate that our method achievessuperior segmentation performance compared withprevious state-of-the-arts. We hope our work mayinspire future research to investigate how to betteradapt CLIP to complex visual understanding tasks. G. Kang, Y. Wei, Y. Yang, Y. Zhuang, andA. Hauptmann, Pixel-level cycle association:A new perspective for domain adaptive seman-tic segmentation, Advances in neural informa-tion processing systems, vol. 33, pp. 35693580,2020. G. Kang, L. Jiang, Y. Wei, Y. Yang, andA. Hauptmann, Contrastive adaptation net-work for single-and multi-source domain adap-tation, IEEE transactions on pattern analysisand machine intelligence, vol. 44, no. 4, pp.17931804, 2020.",
  "transformer for universal image segmentation,in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2022,pp. 12901299": "X. Ji, J. F. Henriques, and A. Vedaldi, Invariantinformation clustering for unsupervised imageclassification and segmentation, in Proceedingsof the IEEE/CVF International Conference onComputer Vision, 2019, pp. 98659874. J. H. Cho, U. Mall, K. Bala, and B. Hariha-ran, Picie: Unsupervised semantic segmenta-tion using invariance and equivariance in cluster-ing, in Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recogni-tion, 2021, pp. 16 79416 804. T.-W. Ke, J.-J. Hwang, Y. Guo, X. Wang, andS. X. Yu, Unsupervised hierarchical seman-tic segmentation with multiview cosegmentationand clustering transformers, in Proceedings ofthe IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2022, pp. 25712581. L. Melas-Kyriazi, C. Rupprecht, I. Laina, andA. Vedaldi, Deep spectral methods: A surpris-ingly strong baseline for unsupervised seman-tic segmentation and localization, in Proceed-ings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2022, pp. 83648375. Y. Ouali, C. Hudelot, and M. Tami, Autore-gressive unsupervised image segmentation, inComputer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020,Proceedings, Part VII 16.Springer, 2020, pp.142158. K. Li, Z. Wang, Z. Cheng, R. Yu, Y. Zhao,G. Song, C. Liu, L. Yuan, and J. Chen,Acseg: Adaptive conceptualization for unsuper-vised semantic segmentation, in Proceedings ofthe IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2023, pp. 71627172. W. Van Gansbeke, S. Vandenhende, S. Geor-goulis, and L. Van Gool, Unsupervised seman-tic segmentation by contrasting object mask pro-posals, in Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, 2021,",
  "pp. 10 05210 062": "K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick,Momentum contrast for unsupervised visualrepresentation learning, in Proceedings of theIEEE/CVF conference on computer vision andpattern recognition, 2020, pp. 97299738. J.-J. Hwang, S. X. Yu, J. Shi, M. D. Collins, T.-J. Yang, X. Zhang, and L.-C. Chen, Segsort:Segmentation by discriminative sorting of seg-ments, in Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, 2019,pp. 73347344. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh,G. Goh, S. Agarwal, G. Sastry, A. Askell,P. Mishkin, J. Clark et al., Learning transferablevisual models from natural language supervi-sion, in International conference on machinelearning.PMLR, 2021, pp. 87488763. C. Zhou, C. C. Loy, and B. Dai, Extract freedense labels from clip, in Computer VisionECCV 2022: 17th European Conference, TelAviv, Israel, October 2327, 2022, Proceedings,Part XXVIII.Springer, 2022, pp. 696712.",
  "G. Shin, W. Xie, and S. Albanie, Reco:Retrieve and co-segment for zero-shot transfer,Advances in Neural Information Processing Sys-tems, vol. 35, pp. 33 75433 767, 2022": "K. Ranasinghe, B. McKinzie, S. Ravi, Y. Yang,A. Toshev, and J. Shlens, Perceptual groupingin contrastive vision-language models, in Pro-ceedings of the IEEE/CVF International Confer-ence on Computer Vision, 2023, pp. 55715584. W. He, S. Jamonnak, L. Gou, and L. Ren, Clip-s4: Language-guided self-supervised semanticsegmentation, in Proceedings of the IEEE/CVFConference on Computer Vision and PatternRecognition, 2023, pp. 11 20711 216.",
  "K. Desai and J. Johnson, Virtex: Learningvisual representations from textual annotations,in Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, 2021,pp. 11 16211 173": "G. Li, N. Duan, Y. Fang, M. Gong, and D. Jiang,Unicoder-vl: A universal encoder for vision andlanguage by cross-modal pre-training, in Pro-ceedings of the AAAI conference on artificialintelligence, vol. 34, no. 07, 2020, pp. 11 33611 344. J. Li, R. Selvaraju, A. Gotmare, S. Joty,C. Xiong, and S. C. H. Hoi, Align beforefuse: Vision and language representation learn-ing with momentum distillation, Advances inneural information processing systems, vol. 34,pp. 96949705, 2021. L. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, andJ. Liu, Hero: Hierarchical encoder for video+language omni-representation pre-training, inProceedings of the 2020 Conference on Empir-ical Methods in Natural Language Processing(EMNLP), 2020, pp. 20462065. C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh,H. Pham, Q. Le, Y.-H. Sung, Z. Li, andT. Duerig, Scaling up visual and vision-language representation learning with noisy textsupervision, in International Conference onMachine Learning.PMLR, 2021, pp. 49044916.",
  "Z. Wang, Y. Li, X. Chen, S.-N. Lim, A. Torralba,H. Zhao, and S. Wang, Detecting everything inthe open world: Towards universal object detec-tion, arXiv preprint arXiv:2303.11749, 2023": "J. Cha, J. Mun, and B. Roh, Learning to gener-ate text-grounded mask for open-world semanticsegmentation from only image-text pairs, inProceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2023,pp. 11 16511 174. M. Xu, Z. Zhang, F. Wei, H. Hu, and X. Bai,Side adapter network for open-vocabularysemantic segmentation, in Proceedings of theIEEE/CVF Conference on Computer Vision andPattern Recognition, 2023, pp. 29452954. J. Ahn and S. Kwak, Learning pixel-levelsemantic affinity with image-level supervisionfor weakly supervised semantic segmentation,in Proceedings of the IEEE conference on com-puter vision and pattern recognition, 2018, pp.49814990. Y. Du, Z. Fu, Q. Liu, and Y. Wang, Weaklysupervised semantic segmentation by pixel-to-prototype contrast, in Proceedings of theIEEE/CVF Conference on Computer Vision andPattern Recognition, 2022, pp. 43204329. P. O. Pinheiro and R. Collobert, From image-level to pixel-level labeling with convolutionalnetworks, in Proceedings of the IEEE confer-ence on computer vision and pattern recognition,2015, pp. 17131721. J. Xie, J. Xiang, J. Chen, X. Hou, X. Zhao, andL. Shen, C2am: contrastive learning of class-agnostic activation map for weakly supervisedobject localization and semantic segmentation,in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2022,pp. 989998. W. Van Gansbeke, S. Vandenhende, S. Geor-goulis, M. Proesmans, and L. Van Gool, Scan:Learning to classify images without labels, inComputer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020,Proceedings, Part X.Springer, 2020, pp. 268285.",
  "M. Bucher, T.-H. Vu, M. Cord, and P. Perez,Zero-shot semantic segmentation, Advances inNeural Information Processing Systems, vol. 32,2019": "Y. Xian, S. Choudhury, Y. He, B. Schiele, andZ. Akata, Semantic projection network for zero-and few-label semantic segmentation, in Pro-ceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, 2019, pp.82568265. G. Pastore, F. Cermelli, Y. Xian, M. Mancini,Z. Akata, and B. Caputo, A closer look at self-training for zero-label semantic segmentation,in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2021,pp. 26932702. B. Li, K. Q. Weinberger, S. J. Belongie,V. Koltun, and R. Ranftl, Language-drivensemantic segmentation, in The Tenth Interna-tional Conference on Learning Representations,ICLR 2022, Virtual Event, April 25-29, 2022,2022. J. Xu, S. De Mello, S. Liu, W. Byeon, T. Breuel,J. Kautz, and X. Wang, Groupvit: Semanticsegmentation emerges from text supervision, inProceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2022,",
  "pp. 18 13418 144": "G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin, Scal-ing open-vocabulary image segmentation withimage-level labels, in Computer VisionECCV2022: 17th European Conference, Tel Aviv,Israel, October 2327, 2022, Proceedings, PartXXXVI.Springer, 2022, pp. 540557. J. Xu, J. Hou, Y. Zhang, R. Feng, Y. Wang,Y. Qiao, and W. Xie, Learning open-vocabularysemantic segmentation models from naturallanguage supervision, in Proceedings of theIEEE/CVF Conference on Computer Vision andPattern Recognition, 2023, pp. 29352944.",
  "P. Ren, C. Li, H. Xu, Y. Zhu, G. Wang, J. Liu,X. Chang, and X. Liang, Viewco: Discov-ering text-supervised segmentation masks viamulti-view semantic consistency, arXiv preprintarXiv:2302.10307, 2023": "F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao,H. Zhang, P. Zhang, P. Vajda, and D. Mar-culescu, Open-vocabulary semantic segmenta-tion with mask-adapted clip, in Proceedings ofthe IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2023, pp. 70617070. Y. Xing, J. Kang, A. Xiao, J. Nie, L. Shao,and S. Lu, Rewrite caption semantics: Bridgingsemantic gaps for language-supervised semanticsegmentation, Advances in Neural InformationProcessing Systems, vol. 36, 2024.",
  "M. Dehghani, M. Minderer, G. Heigold, andS. Gelly, An image is worth 16x16 words:Transformers for image recognition at scale, inInternational Conference on Learning Represen-tations, 2021": "E. Jang, S. Gu, and B. Poole, Categoricalreparameterization with gumbel-softmax, in 5thInternational Conference on Learning Represen-tations, ICLR 2017, Toulon, France, April 24-26,2017, Conference Track Proceedings, 2017. L.-C.Chen,G.Papandreou,I.Kokkinos,K. Murphy, and A. L. Yuille, Deeplab: Seman-tic image segmentation with deep convolutionalnets, atrous convolution, and fully connectedcrfs, IEEE transactions on pattern analysis andmachine intelligence, vol. 40, no. 4, pp. 834848,2018."
}