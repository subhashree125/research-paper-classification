{
  "Abstract": "In the current era of generative AI breakthroughs, gener-ating panoramic scenes from a single input image remainsa key challenge. Most existing methods use diffusion-basediterative or simultaneous multi-view inpainting. However,the lack of global scene layout priors leads to subpar out-puts with duplicated objects (e.g., multiple beds in a bed-room) or requires time-consuming human text inputs foreach view. We propose L-MAGIC, a novel method leverag-ing large language models for guidance while diffusing mul-tiple coherent views of 360 panoramic scenes. L-MAGICharnesses pre-trained diffusion and language models with-out fine-tuning, ensuring zero-shot performance. The outputquality is further enhanced by super-resolution and multi-view fusion techniques. Extensive experiments demonstratethat the resulting panoramic scenes feature better scene lay-outs and perspective view rendering quality compared to re-lated works, with >70% preference in human evaluations.Combined with conditional diffusion models, L-MAGIC canaccept various input modalities, including but not limitedto text, depth maps, sketches, and colored scripts. Apply-ing depth estimation further enables 3D point cloud gen-eration and dynamic scene exploration with fluid cameramotion. Code is available at",
  "Diffusion models have achieved state-of-the-art perfor-mance in image generation. However, generating a 360": "panoramic scene from a single perspective image remainsa challenge, which is an important problem in many com-puter vision applications, such as architecture design, moviescene creation, and virtual reality (VR).Training a model to directly generate panoramic im-ages is challenging due to the lack of diverse large-scaledatasets. Hence, most existing works separate panoramicscenes into multiple perspective views, and inpaint them us-",
  ". Teaser. L-MAGIC is a novel method to generate a 360": "panoramic scene from a single input image. L-MAGIC utilizeslarge language models to control perspective diffusion models togenerate multiple views with coherent 360 layout. L-MAGIC isalso compatible with images synthesized by conditional genera-tive models, making it capable of creating panoramic scenes fromvarious input modalities. A set of perspective images rather thana single panoramic image also allows our method to leverage off-the-shelf monocular depth estimation models to enable immersiveexperiences, e.g., scene fly-through or 3D point cloud generation. ing pre-trained diffusion models. To ensure generalization,the diffusion model is either frozen without any architec-ture change or combined with extra modules trained onsmall datasets for integrating multi-view information .A common approach to encode the scene informa-tion during multi-view inpainting is to provide a text-conditioned diffusion model with the description of the in-put image, which is generated by a user or an image cap-tioning model . Though effective for extending localscene content, such approaches suffer from incoherencein the overall scene layout. Specifically, using the sametext description for diffusing different views along the 360 panorama leads to artifacts and unnecessarily repeated ob-jects. Current inpainting methods have no mechanism toleverage global scene information in individual views.In this work, we show that state-of-the-art (vision) lan-guage models, without fine-tuning, can be used to controlmulti-view diffusion and effectively address the above prob-lem. We propose L-MAGIC (), a novel framework",
  "arXiv:2406.01843v1 [cs.CV] 3 Jun 2024": "leveraging large language models to enable the automaticgeneration of diverse yet coherent 360 views from a giveninput image.L-MAGIC relies on iterative warping-and-inpainting. Pre-trained (vision) language models are usedto: (1) generate layout descriptions of different views thatare used in text-conditioned inpainting, (2) automaticallydetermine whether salient objects should be repeated or notfor a specific scene, and (3) monitor the inpainting outputsto avoid challenging cases where diffusion models violatethe text guidance. A key contribution is the prompt de-sign for language and diffusion models to make L-MAGICfully automatic. In addition, smooth multi-view fusion andsuper-resolution techniques are used to ensure high resolu-tion and quality when producing the final panorama.Experiments show that L-MAGIC generates 360 panoramic scenes with higher quality and more coherentlayouts compared to state-of-the-art methods.Not rely-ing on model fine-tuning makes L-MAGIC effective on in-the-wild images, and extendable, using conditional diffu-sion models , to other types of inputs such as text,depth maps, sketch drawings and color scripts/segmentationmasks. Applying depth estimation further enables the cre-ation of 3D point clouds and immersive scene fly-throughexperiences with both camera rotation and translation.",
  ". Related Work": "Diffusion models. Diffusion models learn to generate databy inverting the diffusion process, i.e., removing the noisein the data (see for a detailed survey). By separating thedata generation process into multiple noise removal steps(reverse process) , diffusion models learn image syn-thesis much more effectively than GANs . Recently, la-tent diffusion models have been proposed to improvethe training speed and image synthesis quality by perform-ing the diffusion in latent space. By training the model onlarge-scale image-caption pairs , they achieve remark-able quality and robustness in text-conditioned image syn-thesis. Further fine-tuning of latent diffusion models usinglarge mask strategies produces robust text-conditionedinpainting models, which are used in this work. Panoramic scene generation.Various approaches havebeen proposed for panoramic scene generation. Some ofthem treat the panorama as a single equirectangularimage, and generate it in a single forward pass. However,such approaches struggle to close the loop at both ends ofthe generated equirectangular image, even with purposelydesigned spherical positional embeddings . Meanwhile,the lack of large scale training data makes it impracticalto train a generalizable model for image-to-panorama, andlimits the robustness of the model, resulting in inconsistentoutputs with the input descriptions.Some recent methods create panoramas by gen- erating multiple perspective views using robust pre-traineddiffusion models trained on large-scale perspective data.Text2room generates 10 rotated views of a panoramausing stable diffusion v2 inpainting without layout controland focuses on indoor scenes and mesh generation. MVD-iffusion ensures multi-view local texture consistencyby extra multi-view attention modules fine-tuned on smalldatasets. Though applicable to both text-to-panorama andimage-to-panorama, these methods struggle to generate di-verse 360 views. Specifically, there is no mechanism toencode the global scene layout into the generation or in-painting of different views. Hence, conditioning the methodonly on the input image or text results in salient objects(e.g., beds in a bedroom) being generated repeatedly acrossviews. In this work, we guide the multi-view diffusion pro-cess with large language models to automatically generatepanoramic scenes with coherent and diverse 360 layouts. Language models. Recent language model advancementshave enabled many important applications (see fordetails). Trained on large-scale data with humans in theloop, ChatGPT has demonstrated super-human perfor-mance on various language-based tasks. In this work, weutilize ChatGPT to automatically generate coherent multi-view scene descriptions for the consumption of a pre-traineddiffusion model that generates multiple perspective viewsof a panoramic scene. Leveraging multi-modal data, visionlanguage models have further enhanced language mod-els to understand visual inputs. In this work, we utilize pre-trained VQA models to automatically generate a scenedescription for the input image, and to avoid unnecessarilyrepeated objects across the generated panoramic scene.",
  "The goal of this work is to generate a coherent 360": "panoramic scene given a single (perspective) image. Notethat this setting is very general since the input image couldbe either captured in the real world or synthesized. For ex-ample using conditional diffusion models , one cansynthesize the input image using inputs such as text descrip-tions, sketches, depth images, and so on.As shown in , L-MAGIC generates the panoramicscene with iterative warping-and-inpainting. The warpingstep generates an incomplete perspective view and the maskof the missing region (Sec. 3.1). The inpainting step com-pletes the masked region with assistance from languagemodels (Sec. 3.2). The final panorama is created by fusingthe generated views with some post-processing to enhancethe quality and resolution (Sec. 3.3).",
  "A living room": ". L-MAGIC pipeline. The input is an image I either captured in the real-world or synthesized, e.g., by conditional diffusionmodels. Multiple novel views to compose a 360 panoramic scene are generated by iterative warping and inpainting. Pre-trained diffusionmodels assisted by pre-trained language models are used to generate views with both high-quality local textures and coherent 360 layouts.Further quality enhancement techniques ensure smooth blending of multiple views into high-resolution panoramic scenes. L-MAGIC cangenerate panorama images, immersive videos, and 3D point clouds from various types of inputs, such as images, text, and sketch drawings. der the next incomplete view to inpaint based on the relativecamera pose. To project an image to the unit sphere, we firstconstruct a mesh by defining the vertices V on each imagepixel and creating edges E between adjacent pixels. Then,we project the vertices to a unit sphere by",
  "||K1v||,(1)": "where K is the intrinsic matrix, v V is the homoge-neous coordinate of a pixel, and vsp is the projectedlocation.To warp a completed view A to a novel viewB, where R is the rotation matrix from A to B, we ro-tate each projected vertex vsp of A by vrot = Rvsp, andthen perform rasterization-based rendering (I, M) =rasterize(Vrot, E, K) where Vrot is the set of rotated verticesand K is the intrinsic matrix of image B. The output I is awarped image and M is a binary mask indicating whetherinpainting is required for a pixel (obtained by checking foreach pixel whether ray-casting hits a valid mesh face).To ensure the local inpainting consistency of eachperspective view, we use a large field of view (FoV) andadjust the rotation angles so that both known and unknownregions are reasonably large after warping. In practice, aFoV of 100 degrees with roughly 40 degrees of rotationbetween adjacent views works well.To further reducethe iterative error accumulation, we expand the scenealternatively from both sides of the input image, ratherthan expanding in a single direction. To ensure a smooth360 loop closure, we tune the rotation angles so thatthe final view has a large incomplete region at the center,resulting in a sequence of views with rotation angles of{0, 41, 41, 82, 82, 123, 200.5(for loop closure)}.",
  "The inpainting step completes a warped view with a con-sistent local style and a coherent 360 scene layout. We": "utilize the Stable Diffusion v2 inpainting model , whichcan effectively extrapolate the large missing region of eachwarped view while maintaining local style consistency.However, naive inpainting without any prior will generatesevere artifacts. One common prior explored before is a user-provided text description of the scene or inputimage. Yet, using the same description in different viewsmay generate duplicate objects such as multiple beds in abedroom (see ), since perspective inpainting methodshave no mechanism to split the layout into different views.Please refer to Sec. 4.3 for a detailed ablation study.To address these problems, we use a vision languagemodel Lv() (BLIP-2 ) and a language model L()(ChatGPT4 ) to guide the inpainting process. In the fol-lowing, we describe our method (Alg. 1) in detail. The exactprompts used in Alg. 1 to interact with language and diffu-sion models are provided in Appendix A.Before warping and inpainting, we first prompt Lv() togenerate the description dI for the input image I (line 2).We ask two questions so that dI contains both coarse andfine levels of detail. Next, we ask L() to imagine the globalscene layout d360 (line 3) based on dI, where each line ofd360 corresponds to the description of a specific view. Toavoid duplicate objects, we request a compact description ofindividual views without mentioning objects in other views.d360 mostly contains objects of individual views. Us-ing such descriptions as the inpainting prompt can lead toinconsistent style at distant views. Hence, we ask L() toremove objects from dI and obtain the final scene-level de-scription dscene, e.g., a bedroom with a wooden bed be-comes a bedroom (line 4). dscene is later used togetherwith d360 to ensure a consistent multi-view style.Though dscene ensures the multi-view style consistency,the training data bias of diffusion models may still result inobjects commonly associated with a particular scene beinggenerated, even if not explicitly mentioned in d360. For ex-",
  ": return merge(C)": "ample, a bed is often generated with the word bedroomin the prompt, resulting in duplicate beds in multiple views.To avoid this problem, we further let L() automatically de-termine whether there are some objects in the scene thatrequire repetition avoidance (line 5).After each warping step, we use the outputs fromlines 2 to 5 to automatically generate the prompt for text-conditioned inpainting (line 12).Specifically, for thewarped view with 0 rotation (i = 1), we use dscene as theprompt (di) for text-conditioned inpainting (line 7). Forother views (line 13), if there is no object in drepeat, i.e., norepetition avoidance required, we perform inpainting withthe prompt a peripheral view of <dscene> where we see<the corresponding description in d360>. If any objectexists in drepeat, we use the positive prompt of a peripheralview of <dscene> where we only see <the correspondingdescription in d360>, and the negative prompt of any typeof <the object in drepeat> (one sentence for each object indrepeat). The positive prompt template prevents Stable Dif-fusion from generating common objects of an environment(e.g., the bed in a bedroom). The negative prompt templateavoids duplication of objects mentioned in drepeat.Bias exists in the training data of diffusion models an image with the caption of a bedroom mostly containsa bed. Therefore, repeated objects can still be generated even with constraints from the prompt. To further alleviatethis problem, we use Lv() to check whether each inpaintedimage Ii contains objects mentioned in drepeat (line 15). Ifthe answer is yes, we re-run inpainting until the answerbecomes no or the maximum number of trials c is reached.",
  ". Quality and Resolution Enhancement": "Several techniques are also proposed to enhance the qualityand resolution of the final panorama.As shown in Appendix. B, adjacent pixels at the centerof an image have a larger angular distance than the ones atthe side of an image. When warping a completed view toa novel view, the original central region becomes the sideregion, making the rendered image blurry due to interpola-tion. Meanwhile, the resolution of the Stable Diffusion out-put is 512512, the panorama created by these images has alow resolution. To address both problems, we apply super-resolution to the output Ii of each inpainting step, in-creasing the resolution of Ii to 2048 2048. Then, we warpthe high-resolution image to a low-resolution novel view sothat no (strong) interpolation is required. After performingall warping and inpainting steps, we simply fuse the super-resolution images to generate a high-resolution panorama.During warping and panorama generation, multiple per-spective images might have overlaps at the same region. Toavoid sharp boundaries when merging them, we perform aweighted average, i.e., given multiple warped pixels at thesame location with colors ci, the final merged pixel coloris cmerge =",
  "i wici": "i wi , where the weight wi is computed asthe distance to the nearest image boundary at the originalview i. This strategy effectively down-weights the pixelsnear the warping boundaries, ensuring a smooth transitionduring multi-view fusion.To create the final panorama (line 22), we first projecteach view to the unit sphere same as in Sec. 3.1. Then, weperform the equirectangular projection to warp multi-ple projected views to the same equirectangular plane, andmerge them into a single equirectangular image.",
  ". Discussion": "L-MAGIC is fully automatic no human interaction is re-quired to link language models and diffusion models. Thisis realized by 1) careful prompt engineering, which enableslanguage models to output texts that can be automaticallyconverted into the inpainting prompt, and 2) handling theedge cases where language models or diffusion models donot generate outputs that satisfy the requirements in the in-put prompt. For example, ChatGPT sometimes still outputsthe layout description d360 with an erroneous format, mak-ing automatic prompt extraction fail catastrophically. Weautomatically detect such cases, and re-run line 3 to ensurethe algorithm flow, see Appendix A for more details. L-MAGIC requires no model fine-tuning, which ensuresthe zero-shot performance and makes it capable of accept-ing other types of inputs leveraging conditional generativemodels (see Sec. 4 for results). This advantage also allowsindividual modules to be replaced by future methods to en-hance the performance, e.g., change BLIP-2+ChatGPT toGPT-4V , or use other inpainting models.",
  ". MVDiffusion image-to-panorama model": "To enable text conditioning in these methods, we use BLIP-2 to obtain the description of the input image. For text-to-panorama, we compare against:1. Text2light : GAN-based text to panorama model.2. Stable Diffusion v2 : we use the prompt 360 de-gree panorama of <input prompt> to generate thepanoramic image in a single diffusion process.",
  ". LDM3D panorama model: we only use the outputRGB image and the prompt 360 degree panorama of<input prompt> to generate the panoramic image in asingle diffusion process": "4. Text2room panorama generation module .5. MVDiffusion text-to-panorama model .Implementation Details. L-MAGIC is implemented withPyTorch and the official release of BLIP-2 , StableDiffusion and the ChatGPT4 API . It takes 2-5 min-utes to generate a 360 panorama depending on the numberof repetitions in line 17 of Alg. 1. We take the official codeand model for all other methods. In text-to-panorama, L-MAGIC uses Stable Diffusion v2 conditioned on the giventext prompt to generate the input image.Data.To evaluate the in-the-wild performance, we col-lect data that does not overlap with the training data of anymethod for both tasks. For image-to-panorama, we use 20indoor and 20 outdoor images from tanks-and-temples and RealEstate10K .For text-to-panorama, we use ChatGPT to generate 20 random scene descriptions (10 in-door and 10 outdoor, see Appendix C).Metrics.To evaluate different methods with respect toquality and multi-view diversity, we compute the InceptionScore (IS) for the perspective views of the panorama.Since existing quantitative metrics do not capture all as-pects of human perception of quality , we follow existingworks for a complementary human evaluation.To this end, we set up a voting web page that shows side-by-side panoramic scenes generated using the same input, onewith our method and one with a baseline. We ask 15 anony-mous voters to choose which panorama has higher qualityand scene structure (see Appendix D for an example votingpage). To minimize voting bias, we randomly shuffle the or-der of the side-by-side panoramas and hide the generationmethod names. We use the votes to compute a preferencescore from 0 to 1 for our method compared to the baselines.This score is simply the percentage of votes for our methodwith respect to quality and structure.",
  ". Main Results": "As shown in , our method performs better for bothimage-to-panorama and text-to-panorama, even comparedto task-specific methods. To further understand the perfor-mance of different methods, we provide in and 5 thequalitative results for both tasks. Stable Diffusion v2 treatsa panorama as a single image. It cannot close the 360 loop since equirectangular projection splits the loop-closure area to two sides of an image (moved to the middleof the rendered panorama for better visualization). In themeantime, due to the lack of large-scale panorama trainingdata, it still generates unnecessarily repeated objects suchas multiple beds in a bedroom. Text2room and MVDiffu-sion separate a panorama into multiple perspective views.Inpainting them using the same prompt results in unreason-ably repeated objects in multiple views. Due to the limitedpanorama training data, Text2light cannot fully understandzero-shot scene descriptions generated by ChatGPT, result-ing in scenes not consistent with the input prompt. Similarto Stable Diffusion v2, treating panorama as a single imagemakes it fail on loop closure. LDM3D is fine-tuned on topof a perspective latent diffusion model. Though better thanText2light, it still cannot close the loop and sometimes failsto generate scenes that are consistent with the details of theprompt (e.g., generating a non-modern living room whenasked for a modern one). Our method works robustly onvarious inputs, generating panoramic scenes with high per-spective rendering quality and reasonable 360 scene lay-outs (see supplementary videos for more details).",
  "(a) Human evaluations.(b) Algorithmic evaluations": ". Quantitative results for image-to-panorama and text-to-panorama. (a) Human evaluations. Each baseline has two barsrepresenting respectively the quality of rendered perspective views and the 360 layout. The value of the bar means the frequency whereour method is preferred in the voting. Above 50% (dashed line) means our method is more preferred than the corresponding baseline. (b)Algorithmic evaluation by computing the Inception Score (IS). L-MAGIC consistently outperforms previous methods on both metrics.",
  "Diffusion v2Text2roomMVDiffusionOurs": ". Image-to-panorama visualizations. Stable Diffusion v2 cannot close the 360 loop (sharp boundaries at the middle). Text2roomand MVDiffusion lack mechanisms to avoid duplicate objects. L-MAGIC outputs have high local view quality and coherent scene layouts. the scene prior, 2) the inpainting method and 3) quality en-hancement techniques. For each aspect, we remove a com-ponent or replace it with other methods, and perform thesame evaluation as in the main experiments. We use thesame data used in the image-to-panorama main experimentfor analysis. The results are reported in . Please referto Appendix E for the visualization comparisons.For the scene prior, we remove the prior from chatGPT(no GPT) and all text guidance (no prompt) respectively.Without the global layout prior from chatGPT, the structureof the outputs becomes worse. Without any prompt guid-ance, both the scene layout and the perspective view ren- dering quality degrade severely.For the inpainting method, we replace the Stable Diffu-sion v2 model with 3 state-of-the-art text-conditioned in-painting methods, namely, Blended Latent Diffusion (BLDinpaint) , Deep Floyd (DF inpaint) and Stable Diffu-sion XL (SDXL inpaint) . Interestingly, though someof the methods are published later than Stabld Dif-fusion v2, their capacity to perform large mask inpaintingis limited on in-the-wild images, resulting in worse perfor-mance in terms of both scene layouts and rendering qual-ity. Note that the inception scores for some methods (e.g.,BLD inpaint) are higher than ours despite a much worse",
  "Input Prompt: Modern living room with a sofa and a TV.Input Prompt: Underwater coral reef scene": ". Text-to-panorama visualizations. Text2light, Stable Diffusion v2 and LDM3D cannot close the 360 loop (sharp boundariesat the middle). Text2room and MVDiffusion generate panoramas with duplicate objects. L-MAGIC effectively addresses these problems,resulting in high-quality panorama with reasonable scene layouts. performance from human evaluation. This is caused by theadversarial samples generated by these methods (see Ap-pendix F for examples), where the local patches of the ad-versarial samples are not consistent with the input image,yet leading to a high diversity in the inception score. Simi-lar issues have also been discovered in other problems ,which shows the importance of human evaluation.For the quality enhancement techniques, we remove re-spectively the super-resolution (no SR) and smoothing (nosmooth) techniques mentioned in Sec. 3.3.Though thescene layout does not degrade much, the perspective viewrendering quality is lower due to the blur or artifacts.",
  "Anything-to-panorama.Conditional diffusion mod-els can now generate an image from diverse types": "of inputs. The strong zero-shot performance of L-MAGICmakes it possible to generate panoramic scenes from po-tentially any inputs compatible with conditional diffusionmodels. As shown in , we can use to generate asingle image from 1) a depth map, 2) a sketch drawing or3) a colored script or segmentation image. Then, this gen-erated image can be used in L-MAGIC to produce realisticpanoramic scenes. This flexibility makes L-MAGIC bene-ficial to a wide range of design applications. 3D scene generation. Applying state-of-the-art depth esti-mation models, we can further generate 3D scenes from theoutput of L-MAGIC. Specifically, we compute the depthmap for multiple perspective views, then we merge thedepth maps into the equirectangular image plane by align-ing all views to the initial view. Then we convert the corre-sponding panoramic depth map to a 3D point cloud. We useMetric3D and DPT-hybrid to estimate the depthfor indoor and outdoor scenes respectively. The alignment",
  "QualityLayout": ". Analysis. We follow the same evaluation protocol asin the main experiment (). From human evaluations, we seethat removing the language assistance results in severe degration ineither the scene layout (No GPT) or both the quality and the layout(No Prompt). Replacing Stable Diffusion v2 with other inpaintingmethods results in performance drop on both the quality and thelayout. The rendering quality decreased with quality enhancementtechniques removed. The Inception Score cannot accurately re-flect the performance on adversarial examples (see Appendix F),resulting in inconsistency with human evaluation results. . Panorama generated from other input modalities.L-MAGIC can effectively create panoramas from various inputmodalities, such as an input depth map (top), a sketch drawing(middle) and a colored script or a segmentation mask (bottom).The dotted bounding box indicates the region of the initial per-spective view, which is generated by conditional diffusion models. is done by optimizing the scale and shift of each depth mapto enforce the depth from multiple views to be the sameat the same pixel. shows sampled results. Despitesome artifacts caused by the limitation of monocular depthmodels, L-MAGIC can generate diverse indoor and outdoorpoint clouds from various types of inputs. Immersive video. One can also render immersive videosfrom our panorama.Specifically, we first generate apanorama using our pipeline, and then generate a series ofcamera poses for individual video frames. Then we warpthe panorama to each frame view according to the cam-era poses. To further enable scene fly-through with cameratranslations, we apply depth-based warping when transla-tion is involved in a frame, and inpaint the missing regionintroduced after translation. See Appendix G for implemen-",
  ". Conclusion": "We have proposed L-MAGIC, a novel method that can gen-erate 360 panoramic scenes from a single input image. L-MAGIC leverages large (vision) language models to guidediffusion models to smoothly extend the local scene con-tent with a coherent 360 layout. We have also proposedtechniques to enhance the quality and resolution of thegenerated panorama. Extensive experiments demonstratethe effectiveness of L-MAGIC, outperforming state-of-the-art methods for image-to-panorama and text-to-panoramaacross metrics. Combined with state-of-the-art computervision techniques such as conditional diffusion models anddepth estimation models, our method can consume varioustypes of inputs (text, sketch drawings, depth maps etc.) andgenerate outputs beyond a single panoramic image (videoswith camera translations, 3D point clouds, etc.). See Ap-pendix H for discussions about limitations and future works.",
  "Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and VladlenKoltun. Tanks and temples: Benchmarking large-scale scenereconstruction. ACM Transactions on Graphics (ToG), 36(4):113, 2017. 5": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models.arXivpreprint arXiv:2301.12597, 2023. 1, 2, 3, 5 Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. Repaint: Inpaintingusing denoising diffusion probabilistic models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1146111471, 2022. 5 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-perative style, high-performance deep learning library. Ad-vances in neural information processing systems, 32, 2019.5 DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann, Tim Dockhorn, Jonas Muller, Joe Penna, andRobin Rombach.Sdxl: improving latent diffusion mod-els for high-resolution image synthesis.arXiv preprintarXiv:2307.01952, 2023. 6",
  "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, VickiCheung, Alec Radford, and Xi Chen. Improved techniquesfor training gans. Advances in neural information processingsystems, 29, 2016. 5": "Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon,Ross Wightman,Mehdi Cherti,TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, et al. Laion-5b: An open large-scale dataset for trainingnext generation image-text models. Advances in Neural In-formation Processing Systems, 35:2527825294, 2022. 2 Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, AlexRedden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-YenTseng, Fabio Nonato, Matthias Muller, et al. Ldm3d: Latentdiffusion model for 3d. arXiv preprint arXiv:2305.10853,2023. 2, 5",
  "Peter Sturm. Multi-view geometry for general camera mod-els. In 2005 IEEE Computer Society Conference on Com-puter Vision and Pattern Recognition (CVPR05), pages206212. IEEE, 2005. 3": "Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,Naejin Kong, Harshith Goka, Kiwoong Park, and VictorLempitsky.Resolution-robust large mask inpainting withfourier convolutions. In Proceedings of the IEEE/CVF winterconference on applications of computer vision, pages 21492159, 2022. 2 Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, andYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion.arXiv preprint arXiv:2307.01097, 2023. 1, 2, 3, 5 Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey ofmethods and applications. ACM Computing Surveys, 2022.2 Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:Towards zero-shot metric 3d prediction from a single image.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 90439053, 2023. 7",
  "A. L-MAGIC Prompts": "In Sec. 3.2 we have briefly described how to use languagemodels in L-MAGIC. Here, we provide more details on ourprompt design when applying language models.For line 2 of Alg. 1, we ask the following two questionsto the BLIP-2 model (Lv()):Q1BLIP Question: What is this place (describe with fewerthan 5 words)? Answer: Q2BLIP Question:Describe the foreground and back-ground in detail and separately? Answer:These two questions make the model output scene-levelcoarse and fine descriptions without focusing on centralizedobjects, which is beneficial for inferring the global scenelayout at line 3. The final dI is the answers of both ques-tions.To obtain scene layout descriptions d360 of individualviews, we ask the following question to ChatGPT (L()) atline 3:Q1GPT Given a scene with <answer of Q1BLIP>, wherein font of us we see <answer of Q2BLIP>. Gener-ate 6 rotated views to describe what else you see inthis place, where the camera of each view rotates60 degrees to the right (you dont need to describethe original view, i.e., the first view of the 6 viewsyou need to describe is the view with 60 degree ro-tation angle). Dont involve redundant details, justdescribe the content of each view. Also dont re-peat the same object in different views. Dont referto previously generated views. Generate concise (<10 words) and diverse contents for each view. Eachsentence starts with: View xxx(view number, from1-6): We see...As mentioned in Sec. 3.4, ChatGPT sometimes cannot fullyfollow the format request in Q1GPT, which makes automaticprompt generation fail. To avoid this catastrophic failure,we check whether the output of Q1GPT has the requirednumber of lines (6), and whether each line starts from ViewXXX (line number): We see. We re-run line 3 if any ofthe condition is violated. This ensures that ChatGPT under-stands our question and satisfies all our format requests.To remove object-level information at line 4, we ask: Q2GPT Modify the sentence: <answer of Q1BLIP> so thatwe remove all the objects from the description (e.g.,a bedroom with a bed would become a bedroom.Do not change the sentence if the description is onlyan object). Just output the modified sentence.To adaptively judge whether we should avoid repeatedobjects, we ask the following two questions at line 5Q3GPT Given a scene with <answer of Q1BLIP>, wherein font of us we see <answer of Q2BLIP>. Whatwould be the two major foreground objects that wesee? Use two lines to describe them where eachline is in the format of We see: xxx (one object, . Angular distance change w.r.t. the pixel location. Wecan see that the angular distance is larger for centered pixels(small |x cx|, where |x cx| represents the horizontal distancefrom pixel x to the image center cx) for different focal length val-ues fx. This phenomenon causes the blurry warping mentioned inSec. 3.3.",
  "As mentioned in Sec. 3.3, adjacent pixels at the center ofan image have a large angular distance than the ones at the": "side of an image, which causes the blurry warped image.The cause of this issue lies in the construction process of animage. Specifically, let x be the horizontal coordinates of apixel at the image plane, and let fx and cx be respectivelythe focal length and the principal location (camera center atthe image plane) of the camera on the horizontal direction.Then, the horizontal angular distance between the camerarays of x1 and x1 + 1 is",
  "fx)|(2)": "shows the change of value w.r.t. |x cx|, wherelarge |x cx| means x is at the side of an image, and small|xcx| means x is at the center of an image. We can see thatthe angular distance is larger for centered pixels regard-less of the focal length fx. Hence, within the same angle,there are more pixels on the side of an image than at the cen-ter of an image. This means that when warping the centerregion of an image to another view, we require interpolationsince more pixels are created in the corresponding warpedregion. This phenomenon causes the blurry warping men-tioned in Sec. 3.3, see for an example.",
  "C. Text Inputs for Text-to-panorama": "In order to evaluate the performance of different algorithmson in-the-wild inputs, we ask ChatGPT to generate 20 ran-dom scene descriptions (10 indoor and 10 outdoor) in themain experiment of text-to-panorama (Sec. 4.2). The re-sulting text prompts are:1. Autumn maple forest path.2. Tropical beach at sunset.3. Snowy mountain peak view.4. Tuscan vineyard in summer.5. Desert under starlit sky.6. Sakura blossom park, Kyoto.7. Rustic Provencal lavender fields.8. Underwater coral reef scene.9. Ancient Mayan jungle ruins.10. Manhattan skyline at night.11. Victorian-era library.12. Rustic Italian kitchen.13. Minimalist Scandinavian bedroom.14. Moorish-styled bathroom.15. Vintage record store interior.16. Luxurious Hollywood dressing room.17. Industrial loft-style office.18. Art Deco hotel lobby.19. Japanese Zen meditation room.20. Modern living room with a sofa and a TV.",
  "(b) Text-to-panorama": ". The voting web page for human evaluations. (a)The web page for image-to-panorama. The outer black region ofthe input image is the missing region when expanding the field ofview. (b) The web page for text-to-panorama. In each voting, weshow the panoramas and the rendered perspective videos for twomethods. For each criterion, we not only allow to vote for one ofthe results but also allow to vote for both results when there is noobvious winner. for both image-to-panorama and text-to-panorama. In eachvoting, we show for each method a panorama and the per-spective video rendered from the panorama so that the usercan use the panorama to clearly see the 360 degree layoutand loop closure, and use the perspective video to see therendering quality. Besides voting for one of the two results,we also allow to vote for both results when there is no obvi-ous winner for a certain criterion.",
  "No smoothL-MAGIC": ". Visualization of ablation results. Consistent with the quantitative result in Sec. 4.3, removing individual components ofL-MAGIC hurt the performance. A zoom-in comparison is recommended for No SR and No smooth. In No SR, the panorama is less sharpeven though the image resolution is the same with L-MAGIC. In No smooth, there were two unnatural black lines (zoom in to the boundingboxes) caused by the non-smooth fusion, which we do not observe in the full L-MAGIC method.",
  "F. Bias in Quantitative Metrics": "As mentioned in Sec. 4.3, the Inception Score (IS) some-times cannot fully reflect the preference from human eval-uations. shows adversarial examples where thepanorama has poor quality and multi-view coherence yethas a higher inception score compared to the result withbetter human evaluation preference. This shows the impor-tance of human evaluations in the experiment.",
  "G. Video generation": "When generating video frames with pure camera rotation,we follow the strategy of Sec. 3.1, project the panorama to aunit sphere, and render each frame according to the rotationmatrix and camera intrinsics of the frame.To generate immersive videos with camera translations,we apply depth-based warping, and inpaint, using StableDiffusion v2, the small missing regions caused by occlu-sion. For depth-based warping, we first apply pre-traineddepth estimation models on perspective views of thegenerated panorama, and warp them to the corresponding frame of the video. Naive mesh-based warping followingSec. 3.1 may generate mesh faces between different ob-jects, which is not ideal. Hence, we rely on point-basedwarping. To avoid grid-shaped sparsely distributed missingpixels ( left) and ensure the sharpness of the warpedimage, we apply a super-resolution-based approach similarto the strategy in Sec. 3.3. Specifically, we enlarge the reso-lution of the depth map from 512 512 to 2048 2048, andthen warp the high-resolution depth map to each frame witha resolution of 512 512 ( right). To achieve super-resolution on the depth map, we firstperform super-resolution on the RGB image, increasing itsresolution to 2048 2048. Since state-of-the-art depth es-timation models are not effective on high-resolution im-ages, instead of directly estimating the depth of the high-resolution image, we separate it into 13 13 patches of res-olution 512 512 with overlappings between neighbouringpatches, perform depth estimation on individual patches andalign the depth map of each patch with the one of the low-resolution image to ensure a smooth depth transition overpatches and a reasonable object geometry.",
  "Ours (IS = 1.84)SDXL (IS = 2.95)": ". Example of the biased Inception Score. We show samples with high inception scores from the experiment in (Left).Even though our method (right) generates scenes with a much better quality and multi-view consistency, the inception score can still belower. . Effect of super-resolution on depth-based warping.Left: Naive point-based warping generates grid-shaped missingpixels distributed uniformly on the whole image. Right: Super-resolution effectively resolves this issue, making most parts of thewarped image sharp and complete."
}