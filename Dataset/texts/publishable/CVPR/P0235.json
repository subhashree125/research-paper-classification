{
  "Abstract": "Style transfer aims to render an image with the artis-tic features of a style image, while maintaining the origi-nal structure. Various methods have been put forward forthis task, but some challenges still exist. For instance, itis difficult for CNN-based methods to handle global infor-mation and long-range dependencies between input images,for which transformer-based methods have been proposed.Although transformers can better model the relationship be-tween content and style images, they require high-cost hard-ware and time-consuming inference. To address these is-sues, we design a novel transformer model that includesonly the encoder, thus significantly reducing the computa-tional cost.In addition, we also find that existing styletransfer methods may lead to images under-stylied or miss-ing content. In order to achieve better stylization, we de-sign a content feature extractor and a style feature extrac-tor, based on which pure content and style images can befed to the transformer. Finally, we propose a novel networktermed Puff-Net, i.e., pure content and style feature fusionnetwork. Through qualitative and quantitative experiments,we demonstrate the advantages of our model compared tostate-of-the-art ones in the literature. The code is availableat",
  ". Introduction": "As personalized expression gains popularity, people in-creasingly seek to transform images into new artistic styles.Imagine turning a plain landscape photo into an oil paintingor a snapshot into an Impressionist-inspired image. Thistechnology, known as Style Transfer in computer vision,offers possibilities for artistic expression. It captures andblends the essence of artistic styles into different images,creating pieces that merge original content with artisticstyles.Early style transfer methods primarily relied on opti-",
  "*Corresponding authors": ". Comparison of different models based on loss and ca-pacity, with the loss being a combination of 60% content loss and40% style loss. Our model shows a favorable balance betweencapacity and loss. Details can be found in the Method and Experi-ments sections. mization algorithms which minimize the differences be-tween the input image and the reference image.How-ever, the high computational complexity of these meth-ods greatly limited their practical applications. With tech-nological advancements, image style transfer techniquesbased on direct inference have made significant progress.The method introduced by Gatys et al. , which em-ploys convolutional neural networks (CNN), extracts fea-tures of content and style from different layers of a pre-trained CNN model. This approach has significantly re-duced computational complexity and spurred a wave of re-lated research, including developments like AdaIN ,Avatar , SANet , and MAST .Despite theachievements of these CNN-based inference methods forimage style transfer, they still face limitations. They de-pend on convolution operations to capture image features,and their performance is limited when the network layersare insufficient to capture global information. On the otherhand, as the number of layers increases, the content details",
  "arXiv:2405.19775v1 [cs.CV] 30 May 2024": ". Some results of our Puff-Net. Our method achievesa better balance between maintaining stylized effects and reduc-ing computational costs. The main body and background of thecontent image can be stylized more reasonably based on the styleimage. of the synthesized image may be lost, which in turn affectsthe overall quality of the stylized image. Therefore, effec-tively transferring style while maintaining content integrityremains a challenge in the field of image style transfer.Vision transformer (ViT) offers a novel approach forutilizing transformer models in visual tasks. By di-viding input images into a series of small patches and re-arranging them to form embedding vectors, this methodhas been shown to surpass the performance of traditionalCNNs in several visual tasks. For example, a transformer-based model has achieved significant breakthroughs instyle transfer tasks. The success of transformers in han-dling image data can be attributed to their attention mech-anism, which captures the global context of images. Addi-tionally, this mechanism helps the model better understandthe relationship between content and style in images, en-hancing stylization efforts. However, the model capacityof transformer is large, with high hardware requirementsand slow training speed. In order to tackle these difficul-ties, we design a transformer that includes only the encoder.We modify the encoder structure of the transformer so thatwe can obtain stylized output sequences of image patchesthrough the encoder alone.The modified transformer isof lower complexity and has a significantly improved in-ference speed.By analyzing the generated results, we found that thegenerated images may have significant differences from thecontent images. The feature distribution of the generatedimages is not visually reasonable where the content featuresof some style images also appear. Our objective is to elim-inate the style attributes from the content images, preserv-ing only their content structure. Simultaneously, we hopethat style images can focus less on content details and al-low their style features to participate in the stylization pro-cess. Therefore, we preprocess the content images and styleimages before feeding them to the transformer for styliza- tion. Accordingly, we develop two distinct feature extrac-tors: one to isolate content features and the other to isolatestyle features from the input images.In summary, we introduce a novel framework for effi-cient style transfer, namely pure content and style featurefusion network (Puff-Net), which incorporates two featureextractors and a transformer equipped solely with an en-coder. Our major contributions are summarized as follows: We enhance the structure of the encoder in the vanillatransformer so that style transfer can be performed effi-ciently through only the encoder, reducing computationaloverhead.",
  "We design two feature extractors that preprocess the inputto obtain pure content images and pure style images, andconsequently, achieving superior stylized results": "Even with a notable reduction in model capacity, ourmodel continues to deliver competitive performance overexisting counterparts. illustrates the comparison of our model withstate-of-the-art models in terms of model capacity and over-all loss, and provides visual stylized results of ourPuff-Net. It is evident that the proposed Puff-Net achieves abalance between style transfer effectiveness and model effi-ciency.",
  ". Style Transfer": "Image style transfer has made significant progress. Gatyset al. discover that when feeding an input image intoa pre-trained CNN (VGG19), one can capture the contentand style information of the image and integrate both infor-mation by using an optimization-based method. Then, rel-evant ensuing studies started emerging. AdaIN alignsthe mean and variance of content image features with themean and variance of style images to implement style trans-fer. SANet integrates local style patterns efficiently andflexibly based on the semantic spatial distribution of con-tent images. MAST enhances feature representationsof content images and style images through position-wiseself-attention, calculates their similarity, and rearranges thedistribution of these representations. ArtFlow proposesa model consisting of reversible neural flows and an un-biased feature transfer module, which can prevent contentleak during universal style transfer. AdaAttN designs anovel attention and normalization module and inserts it intothe traditional encoder-decoder pipeline. IEST utilizesexternal information and employs contrastive learning forstyle transfer. StyleFormer incorporates transformercomponents into the traditional CNN workflow. StyTr2 proposes a model which achieves the style trasnsfer onlythrough the vanilla transformer. CAP-VSTNet adopts areversible framework to protect content images to avoid ar-",
  "Output": ". Schematic illustration of the Puff-Net architecture. The network begins by extracting content and style features from the inputimages. These features are then divided into patches and encoded into patch sequences through a linear projection. After feeding thefeatures into the transformer for stylization, we can finally obtain the result image through the decoder. Additionally, the model leveragesa reconstruction loss function during training to enhance its ability to reconstruct content and style features. tifacts, and achieves style transfer through an unbiased lin-ear transform module. However, it places more emphasis onretaining content, which leads to the image under-stylized.CLIPstyler achieves style transfer through text injec-tion. We aim to provide a practical solution for style trans-fer.Puff-Net balances the output quality and efficiency,while CNN-based methods prioritize speed. StyTr2 focuseson higher-quality outputs at the cost of efficiency, and CAP-VSTNet retains more original content details compromisingquality. More recently, diffusion-based models prioritizecreativity over efficiency.2.2. Transformer The vanilla transformer is designed to tackle tasks inthe field of natural language processing (NLP). The uniqueself-attention mechanism can effectively model the relation-ships between tokens. In order to apply the transformer tothe field of computer vision (CV), lots of related researchhas been carried out. The proposal of the ViT madea groundbreaking contribution to the application of trans-formers in CV. It segments images into patches and arrangesthem into embeddings, which are fed to ViT for processing.Since the advent of ViT, variants of transformers have beenproposed to deal with multiple visual tasks. For example, DETR and YOLOS for object detection, SegFormer and SETR for semantic segmentation, and CTrans and Swin-Transformer for image classification.Transformers are also proven very effective in the area ofmulti-modal fusion, e.g., ViLT . StyTr2 adopts onlythe vanilla transformer for style transfer for the first time,and the improvement is very significant. Compared withthe CNN, transformers can capture long-range dependen-cies of input images by using attention mechanisms. In thispaper, we also leverage the strong global modeling capa-bility of transformers for style transfer. However, differ-ent from prior models, we utilize the transformer encoderto associate the disentangled content feature and disentan-gled style feature, resulting in better stylizing results with asmaller model scale.",
  ". Method": "In this section, we will introduce the workflow of the pro-posed Puff-Net. We set the dimensions of the input andoutput to be H W 3. To make use of the transformerencoder, we treat style transfer as a sequential patch gen-eration task. We split both content and style images intopatches and use a linear projection layer to project input",
  ". Efficient Transformer Encoder": "StyTr2 employs the transformer to implement the task ofstyle transfer. However, their proposed model necessitates alarge number of computational resources. In addition to theencoder, a transformer decoder is adopted to translate theencoded content sequence according to the encoded stylesequence. The output sequential feature embedding o canonly be obtained through a complete transformer. This isvery cumbersome, and thus we hope to obtain o directlythrough the encoder alone. With this objective in mind, wemodify the transformer encoder as follows. We append alearnable sequence feature embedding o whose shape isthe same as c, and we process it in the encoder based onthe purified content and style images. The FFN layer ofthe transformer consumes much of the computation, but itsrole in capturing context features is not significant. To thisend, we make it process and transmit the information ofo. Through the encoder, we can obtain the required se-quence feature embedding o. After applying the decoder,we can obtain the result image. The overview of the Effi-cient Transformer Encoder (ETE) is shown in .Considering that the output image should be close tothe content image, we initialize the learnable o basedon c.We intend to connect c and s and feed themto the encoder.Each layer of the encoder consists of amulti-head self-attention module (MSA) and a feed-forwardnetwork (FFN). However, its computational complexity isO((2L)2 C + 2L C2). Therefore, we redesign thetransformer model. c is encoded into a query (Q) and s isencoded into a key (K) and a value (V). The computationalcomplexity is O(L2C+LC2). We can also better buildthe connection between the content and style images in thisway. Moreover, when we use the attention mechanism, thepositional encoding should be included in the input. Here,we use Content-Aware Positional Encoding (CAPE) in ,which takes image semantics into account when implement-ing positional encoding. We only calculate CAPE for thecontent image as follows:",
  ". Feature Extraction": "Since we aim to transfer the style of the content image, wehope that the underlying model can change the color andother characteristics. Meanwhile, we ought to avoid dam-aging content images as much as possible. Therefore, wetry to increase the proportion of the content loss when de-signing the loss function. Doing so may result in a smalldifference between the results and content images, yet stillincluding style features such as color. In order to ensurethat the content is not missing while the style of the resultimage is closer to that of the style image, we preprocess thecontent and style images to extract their distinct features.To extract different kinds of features from the content andstyle images, we assume the two images contain informa-tion from two modalities, from which we can capture theirunique features. Therefore, we handle the two types of im-ages separately to obtain pure content and style images. To-wards this end, we employ different feature extractors forcontent images and style images, respectively.To process content images with minimal loss of detail,we select the INN module as the backbone for our con-tent extractor, aiming for utmost content preservation. It canbetter preserve the content by making its input and outputfeatures mutually generated. Therefore, we adopt the INNblock with affine coupling layers. In each invertiblelayer, the transformation is written as follows:",
  "Yk+1 = Concat(Yk+1[1 : c], Yk+1[c + 1 : C])": "(4)where is the Hadamard product, Yk (k=1,2, ) is theoutput of the k-th layer, [1:c] represents the 1st to the c-thchannels, and i (i=1,2,3) are the arbitrary mapping func-tions. To balance the feature extraction ability and computa-tional complexity, we employ the bottleneck residual block(BRB) in MobileNetV2 . For style images, our focus is on capturing the generalstyle, rather than the local details. It requires the extractor tograsp the global information and long-distance dependencyfeatures well. Meanwhile, considering the computationalcomplexity of the model, we choose the LT block asthe basic unit of the style extractor. It flattens the bottle-neck of transformer blocks by flattening the feed-forwardnetwork, which saves substantial computation. Please referto the supplementary material for the network details of thetwo extractors.3.3. Loss Function The generated image requires a fusion of content and style.Therefore, we need a content loss function and a style lossfunction, respectively. Following , we obtain featuremaps through a pretrained VGG model and use them to con-struct the content perceptual loss Lc and the style perceptualloss Ls as follows:",
  "(5)": "where Io represents the output of the model, Ic is the con-tent image and Is is the style image, i() denotes the fea-tures extracted from the i-th layer in a pretrained VGG19,and Nl is the number of layers. () and () denote themean and variance of the extracted features, respectively.For the feature extraction module, we can train the twoextractors with these loss functions. We adopt the contentperceptual loss for the input and output of the content ex-tractor (Lcc: the content perceptual loss w.r.t. the contentimage; Lsc: the content perceptual loss w.r.t the style im-age), and the style perceptual loss for the input and outputof the style extractor (Lcs: the style perceptual loss w.r.t. thecontent image; Lss: the style perceptual loss w.r.t. the styleimage). Lfe is used to calculate the total loss of featureextractors. In order to enhance the learning ability of theextractor, we implement two extractors on both the contentand style images. We reconstruct the result image throughthe content and style features extracted from the same im-age, and the result image should be consistent with the orig-inal image. Here we employ two identity losses to in-crease the severity of the penalty as follows:",
  ". Implementation Details": "We adopt MS-COCO as the content dataset andWikiArt as the style dataset. In the training stage, allthe images are randomly cropped into a fixed resolution of256 256, while any image resolution is supported at thetest time. We choose the Adam optimizer with a learn-ing rate of 0.0005 and use the warm-up adjustment strategy. The batch size is set to 1 and we train our network with100,000 iterations. Our model is trained on the NVIDIATesla A40 for about half a day.During the training stage, we found that the style extrac-tor trained for 12,000 iterations produced the best imagestyle. Some style features may disappear after more iter-ations. We believe this is because the difference betweenthe result image and the content image accounts for a largerproportion of the total loss, as we tend to preserve contentdetails as much as possible. In order to reduce the total loss,the extracted style features will decrease after more roundsof training. Without stylization, the content perceptual losswill be very low, and so will the total loss. Therefore, wefreeze the parameters of the style extractor after 12,000 it-erations of training, while the other parts continue to par-ticipate in the training. Maybe we can also use two-stagetraining scheme .",
  ". Comparison with State-of-the-Art Methods": "Transformer networks have proven their powerful perfor-mances in numerous computer vision fields. So far, state-of-the-art models, such as StyTr2 , have utilized the at-tention mechanism. CNN-based models, despite their fastinference, can result in missing details due to their limita-tions of kernel weight sharing. We have chosen the main-stream style transfer models CAP-VSTNet , StyTr2 ,StyleFormer , and IEST for comparison. We con-duct both qualitative and quantitative comparisons.",
  ". Average inference time (in seconds) of the comparisonmethods at two output resolutions": "The StyleFormer sometimes exaggerates details, resultingin some unreasonable stylization. The local details of someresults generated by the StyTr2 are not obvious, leading tothe content missing. By contrast, our model can effectivelyutilize the content and style features of input images andexploit their relationships. It can extract the main contentlines of the original image and adopt attention mechanismto stylize these main features, which can maintain the globalstructure of the content image and make the stylized imagelook very coordinated. But we also observe that when theinput content images and style images are more complex,sometimes there may be unreasonable stylization.",
  "Quantitative comparison": "In , we compare the inference time of these modelsat two output resolutions using one NVIDIA Tesla P100.As can be seen from the table, our models inference speedis at the forefront of these mainstream models.To quantitatively analyze the effect of generating styl-ized images, we randomly select 20 content images and 20style images, and then use the mainstream models to gener-ate 200 stylized images. We calculate the content differenceand the style difference using (5). shows that ourmodels comprehensive performance is at the forefront. Interms of content difference, we have a small gap comparedto the StyTr2. Our method also achieves the second-loweststyle loss. Although the style difference is slightly greater,we do not pay much attention to the local detail differencesbetween the result image and style image. Whats more, wecan see that CAP-VSTNet has the lowest content loss, butits degree of stylization is lacking. Through the quantitativeanalysis, one can see that our proposed model still retains agood performance despite significantly reduced model ca-",
  "Feature Extraction": "Attention mechanisms are proven to be effective in the fieldof style transfer. We intend to investigate whether the fea-ture extractors work. plots the results generated bythese extractors. By using a content feature extractor, thestructure, lines, and other content features of an input im-age are extracted, which meet our expectations. Some back-ground and less important contents are blurred. Through astyle feature extractor, the color, texture, and other aspectsof the input image are extracted. Although the feature distri-bution of some images has changed, we do not pay attentionto the details of the style image.In order to further investigate the efficacy of the featureextractors, we conduct ablation experiments. We removethe content feature extractor and style feature extractor fromthe model and do not extract the pure features of the con-tent and style images.We directly project their patchesinto sequential feature embeddings and feed them into theencoder-based transformer. In order to offset the impact ofdifferent network depths, we add the encoder layers from 3to 6. shows the results generated by pure encoderwithout those extractors.As can be observed from , some generated im-ages have lost their original content structure, resulting invisual distortions (second row, first group). There are alsosome content features such as lines in the style image ap-",
  "Content-Aware Positional Encoding": "Content-Aware Positional Encoding (CAPE) is a learnableposition encoding method based on image semantic infor-mation proposed by . Since our model extracts featuresfrom the content picture image, the content image will losesome semantic information. We have already shown someextracted feature maps in . Through the content ex-tractor, we can extract the structure, lines, and other featuresof the input image, but we discard style features such as col-ors, which destroys some of the semantic information of theimage. Therefore, in order to verify whether CAPE can playa better role in our model, we carry out a ablation study. Inthe ablation experiment, we replace CAPE with traditionalsinusoidal positional encoding and trained the model. Wepresent the results of this ablation experiment in .As can be seen from the experimental results in ,the results using CAPE are better than those using tradi-tional sinusoidal positional encoding. The results withoutusing CAPE may have unreasonable stylization, and someoriginally similar areas may have significant differences af-ter stylization. We believe that although extracting featuresmay cause losses to the semantic information of the image,we still need positional encoding to exploit the remaininginformation for stylization. Some features such as the back-ground need to be similarly stylized, and different detailfeatures can be stylized differently. Therefore, we still em-ploy CAPE as the positional encoding method.",
  ". Ablation experiments for CAPE. From the first to thelast column: style images, content images, result images usingsinusoidal positional encoding, and result images using CAPE": "transformer encoder, appending a learnable sequence fea-ture embedding o to the input. Its shape is the same ascontent sequential feature embedding c. During the train-ing stage, as we know the resultant image should be closerto the content image, we use c to initialize it. In order tofurther investigate its role in the model, we experiment withother initialization methods.We first initialize it using style sequence feature embed-ding s. It can be observed that the resultant image is verysimilar to the style image, which does not meet expecta-tions. We believe that our model realize stylization basedon o, using the attention mechanism to calculate each partsstylization approach. Since we use s to initialize o, it isin the stylized state from the beginning, and the subsequentstylization effect will not be significant. We also use ran-dom initialization and zero initialization, and find that thegenerated stylized images are blank. We believe that ourmodel cannot find a suitable way to stylize images withoutthe content. The qualitative results using different outputfeature embedding initialization methods are presented insupplementary material due to space constraint.In summary, o is the basis for style transfer in ourmodel, and the calculation results of the attention mecha-nism determine the way of stylization for each patch. There-fore, we choose to initialize it with c, which is more in linewith the goal of style transfer.",
  ". User Study": "In order to better evaluate the performance of our model, weconduct a user study. The comparison resources come from. We invited 45 college students and 10 middle-aged people to conduct this survey. We have set three typesof questions for the purpose of style transfer task. The firstquestion is which model can better maintain the originalimages content structure. The second question is whichmodels result is closer to the target style image. The thirdquestion is which models result after stylization looks the",
  ". Results of User Study. The above three figures corre-spond to questions one, two, and three, respectively. A-Puff-Net.B-CAP-VSTNet. C-StyTr2. D-StyleFormer. E-IEST": "most harmonious. We will provide two examples for eachtype of question. The results of the survey are shown in. In terms of the example we provided, from theresults, we can see that our models ability to maintain theoriginal image content structure is similar to that of CAP-VSTNet, and its ability to achieve stylization is optimal,followed closely by StyTr2 and IEST. As for the ability toachieve reasonable stylization, our model is also outstand-ing. In order to further demonstrate the performance of themodel and reduce randomness, we hope that more peoplecan use our model to produce the expected results.",
  ". Conclusion": "In this paper, we proposed a novel style transfer modeldubbed the Puff-Net. The proposed model consists of twofeature extractors and a transformer that only contains theencoders. We first obtained pure content images and purestyle images through the two feature extractors. Then wefed them into an efficient encoder-based transformer forstylization, in which a sequence of learnable tokens wereadded to interact with pure content and style tokens. Ourmodel solves the problem of huge capacity in existingtransformer-based models. We also verified its good per-formance through extensive experiments and demonstratedthe potential application of style transfer in practice.",
  "Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European confer-ence on computer vision, pages 213229. Springer, 2020. 3": "Haibo Chen, Zhizhong Wang, Huiming Zhang, Zhiwen Zuo,Ailin Li, Wei Xing, Dongming Lu, et al.Artistic styletransfer with internal-external learning and contrastive learn-ing. Advances in Neural Information Processing Systems,34:2656126573, 2021. 2, 5 Yingying Deng, Fan Tang, Weiming Dong, Wen Sun, FeiyueHuang, and Changsheng Xu.Arbitrary style transfer viamulti-adaptation network. In Proceedings of the 28th ACMinternational conference on multimedia, pages 27192727,2020. 1, 2",
  "Ho et al. Denoising diffusion probabilistic models. NIPS,33:68406851, 2020. 3": "Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang,Jiyang Qi, Rui Wu, Jianwei Niu, and Wenyu Liu.Youonly look at one sequence: Rethinking transformer in visionthrough object detection. Advances in Neural InformationProcessing Systems, 34:2618326197, 2021. 3 Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-age style transfer using convolutional neural networks. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 24142423, 2016. 1, 2 Xun Huang and Serge Belongie. Arbitrary style transfer inreal-time with adaptive instance normalization. In Proceed-ings of the IEEE international conference on computer vi-sion, pages 15011510, 2017. 1, 2, 5 Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region su-pervision. In International Conference on Machine Learn-ing, pages 55835594. PMLR, 2021. 3",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization.arXiv preprint arXiv:1412.6980,2014. 5": "Gihyun Kwon and Jong Chul Ye. Clipstyler: Image styletransfer with a single text condition.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1806218071, 2022. 3 Jack Lanchantin, Tianlu Wang, Vicente Ordonez, and YanjunQi. General multi-label image classification with transform-ers. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1647816488,2021. 3",
  "Hui Li, Xiao-Jun Wu, and Josef Kittler. Rfn-nest: An end-to-end residual fusion network for infrared and visible images.Information Fusion, 73:7286, 2021. 5": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 5 Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, MeilingWang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding.Adaattn: Revisit attention mechanism in arbitrary neuralstyle transfer. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 66496658, 2021. 2 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 3 Dae Young Park and Kwang Hee Lee.Arbitrary styletransfer with style-attentional networks. In proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 58805888, 2019. 1, 2, 5",
  "Fred Phillips and Brandy Mackintosh. Wiki art gallery, inc.:A case for critical thinking. Issues in Accounting Education,26(3):593608, 2011. 5": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-moginov, and Liang-Chieh Chen.Mobilenetv2: Invertedresiduals and linear bottlenecks.In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 45104520, 2018. 4, 1 Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang. Avatar-net: Multi-scale zero-shot style transfer by feature decora-tion. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 82428250, 2018. 1 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 2, 3 Linfeng Wen, Chengying Gao, and Changqing Zou. Cap-vstnet: Content affinity preserved versatile style transfer. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1830018309, 2023. 2,5 Xiaolei Wu, Zhihao Hu, Lu Sheng, and Dong Xu. Style-former:Real-time arbitrary style transfer via parametricstyle composition. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1461814627,2021. 2, 5",
  "former architecture. In International Conference on MachineLearning, pages 1052410533. PMLR, 2020. 5": "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, TaoXiang, Philip HS Torr, et al. Rethinking semantic segmen-tation from a sequence-to-sequence perspective with trans-formers.In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 68816890,2021. 3, 4 Man Zhou, Jie Huang, Yanchi Fang, Xueyang Fu, and Aip-ing Liu. Pan-sharpening with customized transformer andinvertible neural network. In Proceedings of the AAAI Con-ference on Artificial Intelligence, pages 35533561, 2022. 4",
  "shows the result images generated by initializ-ing o with style images, zero values, and random values. Itis easy to see that when using different methods to initialize": "o, our model will produce different qualitative results. Us-ing style images for initialization will make result imagescloser to style images, and using zero or random values forinitialization will make it difficult for us to obtain visuallyplausible results. Therefore, we believe o is the basis forstyle transfer in our proposed model, and the calculation re-sults of the attention mechanism determine the stylizationway of each patch.",
  ". Limitation": "Though visually better transfer results have been yeilded,our model still has the drawback of content leak likemost existing algorithms. After multiple rounds of styletransfer for a set of images, some details of the content im-age will still be lost. We believe that stylization will disruptthe content features we obtain through the content extractor,such as lines, resulting in fewer and fewer extracted contentfeatures. We demonstrate this phenomenon in ."
}