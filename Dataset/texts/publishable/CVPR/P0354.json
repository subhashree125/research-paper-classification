{
  "Abstract": "Large-scaleText-to-Image(T2I)diffusionmodelsdemonstratesignificantgenerationcapabilitiesbasedon textual prompts.Based on the T2I diffusion models,text-guided image editing research aims to empowerusers to manipulate generated images by altering the textprompts. However, existing image editing techniques areprone to editing over unintentional regions that are beyondthe intended target area, primarily due to inaccuracies incross-attention maps. To address this problem, we proposeLocalization-aware Inversion (LocInv),which exploitssegmentation maps or bounding boxes as extra localizationpriors to refine the cross-attention maps in the denoisingphases of the diffusion process.Through the dynamicupdating of tokens corresponding to noun words in thetextual input, we are compelling the cross-attention mapsto closely align with the correct noun and adjective wordsin the text prompt. Based on this technique, we achievefine-grained image editing over particular objects whilepreventing undesired changes to other regions. Our methodLocInv, based on the publicly available Stable Diffusion, isextensively evaluated on a subset of the COCO dataset, andconsistently obtains superior results both quantitativelyand qualitatively.",
  "*Equal contributionsCorresponding Author": "cessitating significant computational resources. However,despite their impressive capabilities, they do not directlysupport real image editing, and they typically lack the ca-pability to precisely control specific regions in the image.Recent research on text-guided image editing allowsusers to manipulate an image using only text prompts . In this paper, we focus on text-guided editing,where we aim to change the visual appearance of a specificsource object in the image. Several of the existing meth-ods use DDIM inversion to attain theinitial latent code of an image and then apply their proposedediting techniques along the denoising phase. Nonetheless,present text-guided editing methods are susceptible to inad-vertent alterations of image regions. This arises from theheavy reliance of existing editing techniques on the preci-sion of cross-attention maps. DPL observes the phe-nomenon that the cross-attention maps from DDIM andNTI do not only correlate with the corresponding ob-jects. This phenomenon is attributed to cross-attention leak-age, which is the main factor impeding these image edit-ing methods to work for complex multi-object images. Toaddress this, DPL enhances the cross-attention by incorpo-rating additional attention losses. However, DPL relies ona relatively weak connection between the noun and its as-sociated object. This connection occasionally tends to beweak and results in unsatisfactory performance. Further-more, given recent advancements in text-based segmenta-tion and detection foundation models , it isnow straightforward to automatically obtain strong localiza-tion priors into general applications.In this paper, we include localization priors to offer en-hanced mitigation against cross-attention leakage. With theintroduction of localization priors, our approach, namedLocalization-aware Inversion (LocInv), involves updatingthe token representations associated with objects at each",
  "enhancement": ". Compared with the naive DDIM inversion, our method LocInv aims at enhancing the cross-attention maps by applying localizationpriors (segmentation maps or detection bounding boxes provided by the datasets or foundation models) to guide the inversion processes.Furthermore, to force strong bindings between adjective and noun words, we constrain the cross-attention similarity between them. timestep, a technique akin to dynamic prompt learning .In both segmentation and detection scenarios, we opti-mize two lossesnamely, the similarity loss and overlap-ping lossto ensure that the cross-attention maps alignclosely with the provided priors. Moreover, to accommo-date situations in which adjectives describe their associ-ated noun words, we incorporate an additional similarityloss to reinforce the binding between them.In the ex-periments, we quantitatively evaluate the quality of cross-attention maps on a dataset COCO-edit collected from MS-COCO . We further combine LocInv with P2P tocompare with other image editing methods. LocInv showssuperior evaluation metrics and improved user evaluation.Furthermore, we qualitatively show prompt-editing resultsfor Word-Swap and Attribute-Edit.",
  ". Related work": "Inversion based editing is mainly relying on the DDIMinversion , which shows potential in editing tasks bydeterministically calculating and encoding context informa-tion into a latent space and then reconstructing the origi-nal image using this latent representation. However, DDIMis found lacking for text-guided diffusion models whenclassifier-free guidance (CFG) is applied, which is nec-essary for meaningful editing. Leveraging optimization onnull-text embedding, Null-Text Inversion (NTI) furtherimproved the image reconstruction quality when CFG is ap-plied and retained the rich text-guided editing capabilities ofthe Stable Diffusion model . Negative-prompt inversion(NPI) and ProxNPI reduces the computation costfor the inversion step while generating similarly competi-tive reconstruction results as Null-text inversion. Direct In-version further enhances the inversion technique by ad-justing the editing direction in each timestep to offer essen-tial content preservation and edit fidelity. IterInv gen- eralizes the inversion to the DeepFloyd-IF T2I model .Text-guided editing methods of recentresearches in this topic adopt the large pre-trained text-to-image(T2I) models for controllable image editing. Among them, Imagic and P2P attempt structure-preserving editing via Sta-ble Diffusion (SD) models. However, Imagic requiresfine-tuning the entire model for each image.P2P has no need to fine-tune the model and retrains the imagestructure by assigning cross-attention maps from the orig-inal image to the edited one in the corresponding text to-ken. InstructPix2Pix is an extension of P2P by allowinghuman-like instructions for image editing. NTI furthermakes the P2P capable of handling real images. Recently,pix2pix-zero propose noise regularization and cross-attention guidance to retrain the structure of a given image.DiffEdit automatically generates a mask highlightingregions of the input image by contrasting predictions con-ditioned on different text prompts. PnP demonstratedthat the image structure can be preserved by manipulatingspatial features and self-attention maps in the T2I models.There are also text-guided inpainting methods [15, 33, 38, 45] to achieve the editing purposes given user-specificmasks. For example, Blended diffusion adapts from apre-trained unconditional diffusion model and encouragesthe output to align with the text prompt using the CLIPscore. Blended latent diffusion (BLD) further extend tothe LDM . Nonetheless, inpainting methods primarilyconcentrate on filling arbitrary objects in specified regionswhile ensuring visual coherence with the surrounding areas.These methods do not inherently preserve semantic similar-ity between the source and target objects, as is required forimage translation effects.Text-based segmentation and detection models aim atsegmenting or detecting arbitrary classes with the help oflanguage generalization property after pretraining. One of",
  ". Model": ". Illustration of our proposed method LocInv. The image I comes with its localization prior denoted as S (segmentation mapsor detection boxes). For each time stamp t, the noun (and optionally adjective) words in the text prompt are transformed into dynamictokens, as introduced in Sec 3.2. In each denoising step zt1 zt, we update the dynamic token set Vt with our proposed overlappingloss, similarity loss and adjective binding loss, in order to ensure high-quality cross-attention maps. the most representative prompt-based segmentation modelis SAM .Given an image and visual prompt (box,points, text, or mask), SAM encodes image and promptembeddings using an image and prompt encoder, respec-tively which are then combined in a lightweight mask de-coder that predicts segmentation masks. Similar works in-clude CLIPSeg , OpenSeg , GroupViT , etc.For prompt-based object detector, GroundingDINO stands out as the state-of-the-art method by grounding theDINO detector with language pre-training for open-set generalization.Except that, MaskCLIP , X-decoder , UniDetector also offer prompt-based de-tectors. Leveraging these foundational models, we can ac-quire localization information as a valuable semantic priorto enhance image inversion. This, in turn, contributes to anoverall improvement in image editing performance.",
  ". Preliminary": "Latent Diffusion Models.We use Stable Diffusion v1.4which is a Latent Diffusion Model (LDM) . The modelis composed of two main components: an autoencoder anda diffusion model. The encoder E from the autoencodercomponent of the LDMs maps an image I into a latentcode z0 = E(I) and the decoder reverses the latent codeback to the original image as D(E(I)) I. The diffu-sion model can be conditioned on class labels, segmentationmasks or textual input. Let (y) be the conditioning mech-",
  "Lldm = Ez0E(x),y,N (0,1) (zt, t, (y))22(1)": "The neural backbone is typically a conditional UNet which predicts the added noise.More specifically, text-guided diffusion models aim to generate an image from arandom noise zT and a conditional input prompt P. To dis-tinguish from the general conditional notation in LDMs, weitemize the textual condition as C = (P).DDIM inversion.Inversion aims to find an initial noisezT reconstructing the input latent code z0 upon sampling.Since we aim at accurate reconstruction of a given image forimage editing, we employ the deterministic DDIM sampler:",
  ". Dynamic Prompt Learning": "Text-based image editing takes an image I described byan initial prompt P, and aims to modify it according toan altered prompt P in which the user indicates desiredchanges. The initial prompt is used to compute the cross-attention maps. As discussed in Sec. 1, cross-attention leak-age is a challenge for existing text-based editing meth-ods, when facing complex scenarios. DPL introducesthree losses to enhance the alignment between attentionmaps and nouns, which rely on the inherent connection be-tween image and prompt and is not always reliable in real-world scenarios. In this section, we present our method,denoted as LocInv, which leverages localization priors de-rived from existing segmentation maps (Segment-Prior) ordetection boxes (Detection-Prior). This information can bereadily acquired with the assistance of recent advancementsin foundation models and has the potential to dra-matically strengthen the quality of the cross-attention maps.To simplify, we denote the segmentation map and detectionboxes uniformly as S.The cross-attention maps in the Diffusion Model UNetare obtained from (zt, t, C), which is the first componentin Eq. 3.They are computed from the deep features ofthe noisy image (zt) which are projected to a query ma-trix Qt = lQ((zt)), and the textual embedding which isprojected to a key matrix K = lK(C). Then the atten-tion map is computed as At = softmax(Qt KT / d),where d is the latent dimension, and the cell [At]ij de-fines the weight of the j-th token on the pixel i. We op-timize the word embeddings v corresponding to the initialprompt P in such a way that the resulting cross-attention Atdoes not suffer from the above-mentioned cross-attentionleakage.The initial prompt P contains K noun wordsand their corresponding learnable tokens at each timestampVt = {v1t , ..., vkt ..., vKt }. Similar to DPL, LocInv updateseach specified word in Vt for each step t. The final sentenceembedding Ct now varies for each timestamp t and is com-puted by applying the text encoder on the text embeddings.",
  "arg minVtL = sim Lsim + ovl Lovl(7)": "Gradual Optimization for Token Updates.So far, weintroduced the losses to learn new dynamic tokens at eachtimestamp. However, the cross-attention leakage graduallyaccumulated in the denoising phase. Hence, we enforceall losses to reach a pre-defined threshold at each times-tamp t to avoid overfitting the cross-attention maps .We express the gradual threshold by an exponential func-tion.For the losses proposed above, the correspondingthresholds at time t are defined as THt = exp(t/).And for each loss we have a group of hyperparameters as(sim, sim), (ovl, ovl), (adj, adj). We verify the ef-fectiveness of this mechanism in our ablation experiments.Null-Text embeddings. The above described token updat-ing ensures that the cross-attention maps are highly relatedto the noun words in the text prompt and minimize cross-attention leakage. To reconstruct the original image, we useNTI in addition to learn a set of null embeddings t foreach timestamp t. Then we have a set of learnable word em-beddings Vt and null text t which can accurately localizethe objects and also reconstruct the original image.",
  ". Adjective binding": "Existing text-guided image editing methods have focusedon translating a source object to a target one. However, of-ten users would like to change the appearance of objects.Typically, in text-guided image editing, this would be doneby changing the objects attributes described by adjectives.However, existing methods fail when editing attributes ofthe source objects (as shown in ). We ascribe this case",
  "Return zT , {Vt}T1 , {t}T1": "to the disagreement in cross-attention between the adjectiveand its corresponding noun word (as evidenced in ).To empower the T2I model with attribute editing capa-bility, we propose to bind the adjective words with theircorresponding nouns. To achieve this, we use the Spacyparser to detect the object noun and adjective words,as so called the adjective-noun pairs (vit, ait). Given thesepairs, the adjective binding loss is defined as the similaritybetween the attention maps of the adjective and noun words.",
  ". Experiments": "We demonstrate LocInv in various experiments based on theopen-source Stable Diffusion following previous meth-ods . All experiments are done on R6000 GPUs.Datasets.For the quantitative ablation study of hyperpa-rameters and partially for the qualitative editing compari-son, we select 315 images as a subset COCO-edit out ofMS-COCO dataset . We compose this subset from var-ious searching prompts (including concepts as airplane, ap-ple, banana, bear, bench, etc.) and store the groundtruth seg-",
  ". User study compared with methods freezing the StableDiffusion . We request the respondents to evaluate methods inboth editing quality and background preservations": "mentation/detection images for experimental usage. Over-all, there are 7 search prompts with a single object (noun)in the sentence and 6 with multiple objects. More detailedinformation is shown in the supplementary material. Compared methods. We organize two groups of methodsfor qualitative and quantitative comparison. The first groupof methods, which are freezing the Stable Diffusion models,include NTI , DPL , pix2pix-zero , PnP ,DiffEdit and MasaCtrl . The second group of meth-ods is finetuning the large pretrained T2I model as specificmodels for image editing, such as SD-inpaint , Instruct-Pix2Pix and Imagic , or taking masks as locationsfor inpainting, including SD-inpaint and BLD . Evaluation metrics. To quantitatively assess our methodsperformance, we employ well-established metrics, includ-ing LPIPS , SSIM , PSNR, CLIP-Score andDINO-Sim , to evaluate the edited full image. Addi-tionally, to illustrate the quality of background preservation,we follow DirectInversion to compute LPIPS, SSIM,PSNR, and MSE metrics for regions outside the mask.",
  ". Ablation study": "For the ablation study, we experiment on the COCO-editdataset.To quantitatively assess the localization perfor-mance of LocInv, we vary the threshold from 0.0 to 1.0to obtain the segmentation mask from the cross-attentionmaps. We then calculated the Intersection over Union (IoU)metric using the segmentation ground truth for comparison.Our method can operate with both segmentation maps anddetection bounding boxes as localization priors. Here, weconsider the hyperparameters for both these cases. In , we conduct ablation studies over the similarityloss and the overlapping loss. From -(c)(g), we ob-serve that only applying one of these losses does not ensurea satisfactory performance. Empirically, we find the op-timal hyperparameters for the segmentation and detectionprior as sim = 1.0, sim = 50.0, sim = 0.7, ovl =1.0, ovl = 10.0, ovl = 0.7 and sim = 0.1, sim =25.0, sim = 0.5, ovl = 1.0, ovl = 25.0, ovl = 0.3,respectively. For the adjective binding loss, since there areno sufficient image-text pairs for the ablation study, we em-pirically set the hyperparameters to be adj = 2.0, adj =50.0, adj = 0.1. All results in this paper are generatedwith this hyperparameter setting. (a) Segment-Prior: ablate over the similarity loss(b) Segment-Prior: ablate over the overlapping loss(c) Segment-Prior: ablate over the trade-off parameters(d) Segment-Prior: comparison with NTI and DPL (e) Detection-Prior: ablate over the similarity loss(f) Detection-Prior: ablate over the overlapping loss(g) Detection-Prior: ablate over the trade-off parameters(h) Detection-Prior: comparison with NTI and DPL . Ablation study over hyperparameters given the Segment-Prior (first row) or Detection-Prior (second row). For the first andsecond columns, we ablate hyperparameters for the similarity loss and overlapping loss, respectively. Then we illustrate how the trade-offparameters influence in the third column. Lastly, we show the IoU curves of LocInv together with NTI and DPL as baseline comparisons.",
  "BLD (Seg) 0.34230.557117.41530.04550.00400.18950.747825.621421.6880": ". Comparison with various text-based image editing methods based on the evaluation metrics over the COCO-edit dataset. Weevaluate on single-object and multi-object images editing tasks separately. The comparison methods are organized into two groups as westated in Sec. 4 The Seg and Det in the bracket represent the Segment-prior and Detection-Prior, respectively.",
  ". Image editing evaluation": "For image editing, we combine LocInv with the P2P image editing method. In this paper, we mainly focus onlocal editing tasks including Word-Swap and Attribute-Edit.Word-Swap. As shown in , we compare LocInv withvarious methods by swapping one object from the origi-nal image given the segmentation maps as localization pri-ors. Our method, LocInv, more successfully translates thesource object into the target object while maintaining se-mantic similarities. In , we designed one editingtask for each group of images collected in the COCO-Edit dataset (details in the supplementary material).Inboth single-object and multi-object editing tasks, LocInvachieves better full image evaluation and only performsworse than the inpainting-based methods in termsof background preservation (since these methods do notchange background pixels). It is worth noting that LocInvdoes not require fine-tuning the base model, resulting in bet-ter time complexity and no forgetting in the T2I models. In, we question twenty participants to evaluate the im-age editing performance from two aspects: the editing qual-ity and the background preservation. In both cases, LocInvstands out of these six methods of freezing the SD models.Details on the user study are shown in the supplementary.Attribute-Edit.Furthermore, LocInv excels in anotherediting aspect that other methods tend to overlook, whichis attribute editing. This capability is illustrated in .",
  ". Conclusion": "In this paper, we presented Localization-aware Inversion(LocInv) to solve the cross-attention leakage problem in im-age editing using text-to-image diffusion models. We pro-pose to update the dynamic tokens for each noun word inthe prompt with the segmentation or detection as the prior.The resulting cross-attention maps suffer less from cross-attention leakage.Consequently, these greatly improvedcross-attention maps result in considerably better results fortext-guided image editing. The experimental results, con-firm that LocInv obtains superior results, especially on com-plex multi-object scenes. Finally, we show that our methodcan also bind the adjective words to their correspondingnouns, leading to accurate cross-attention maps for the ad-jectives, and allowing for attribute editing which has notbeen well explored before for text-guided image editing.Acknowledgments We acknowledge projects TED2021-132513B-I00 and PID2022-143257NB-I00, financed byMCIN/AEI/10.13039/501100011033 and FSE+ by the Eu-ropean Union NextGenerationEU/PRTR, and by ERDF AWay of Making Europa, and the Generalitat de CatalunyaCERCA Program. Chuanming Tang acknowledges the Chi-nese Scholarship Council (CSC) No.202204910331. Omri Avrahami, Dani Lischinski, and Ohad Fried. Blendeddiffusion for text-driven editing of natural images. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1820818218, 2022. 2",
  "Omri Avrahami, Ohad Fried, and Dani Lischinski. Blendedlatent diffusion. ACM Transactions on Graphics (TOG), 42(4):111, 2023. 2, 5, 6, 8": "Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-ten, and Tali Dekel. Text2live: Text-driven layered imageand video editing.In European Conference on ComputerVision, pages 707723. Springer, 2022. 2 Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-structpix2pix: Learning to follow image editing instructions.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, 2023. 2, 5, 6, 12 Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan,Xiaohu Qie, and Yinqiang Zheng.Masactrl: Tuning-freemutual self-attention control for consistent image synthesisand editing. Proceedings of the International Conference onComputer Vision, 2023. 5, 6, 13 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 96509660, 2021. 3 Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. In-ternational Conference on Machine Learning, 2023. 2",
  "Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffu-siondet: Diffusion model for object detection. arXiv preprintarXiv:2211.09788, 2022. 12": "Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, YoungjuneGwon, and Sungroh Yoon. Ilvr: Conditioning method for de-noising diffusion probabilistic models. In Proceedings of theInternational Conference on Computer Vision, pages 1434714356. IEEE, 2021. 2 Guillaume Couairon, Jakob Verbeek, Holger Schwenk, andMatthieu Cord. Diffedit: Diffusion-based semantic imageediting with mask guidance. In The Eleventh InternationalConference on Learning Representations, 2023. 2, 5, 6, 13",
  "Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta de-noising score. arXiv preprint arXiv:2304.07090, 2023. 1": "Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imageediting with cross attention control. International Confer-ence on Learning Representations, 2023. 2, 4, 8 Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,and Yejin Choi. Clipscore: A reference-free evaluation met-ric for image captioning. In Proceedings of the 2021 Confer-ence on Empirical Methods in Natural Language Processing,pages 75147528, 2021. 5",
  "Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, andQiang Xu. Direct inversion: Boosting diffusion-based edit-ing with 3 lines of code. arXiv preprint arXiv:2310.01506,2023. 2, 5": "Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, HuiwenChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models. Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition, 2023. 2, 5, 6 Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-fusionclip: Text-guided diffusion models for robust imagemanipulation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages24262435, 2022. 2 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment anything. Proceedings of the In-ternational Conference on Computer Vision, 2023. 1, 3, 4",
  "Senmao Li, Joost van de Weijer, Taihang Hu, Fahad ShahbazKhan, Qibin Hou, Yaxing Wang, and Jian Yang. Styledif-fusion: Prompt-embedding inversion for text-based editing,2023. 1, 2": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InEuropean Conference on Computer Vision, pages 740755.Springer, 2014. 2, 5 Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, and Lei Zhang. Grounding dino: Marrying dino withgrounded pre-training for open-set object detection. In arXivpreprint arXiv:2303.05499, 2023. 1, 3",
  "Timo Luddecke and Alexander Ecker.Image segmenta-tion using text and image prompts.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 70867096, 2022. 3": "Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. Repaint: Inpaintingusing denoising diffusion probabilistic models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1146111471, 2022. 2 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guidedimage synthesis and editing with stochastic differential equa-tions. In International Conference on Learning Representa-tions, 2022. 2",
  "Daiki Miyake, Akihiro Iohara, Yu Saito, and ToshiyukiTanaka. Negative-prompt inversion: Fast image inversionfor editing with text-guided diffusion models. arXiv preprintarXiv:2305.16807, 2023. 2": "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, andDaniel Cohen-Or. Null-text inversion for editing real imagesusing guided diffusion models.Proceedings of the IEEEConference on Computer Vision and Pattern Recognition,2023. 1, 2, 3, 4, 5, 6, 12, 13, 14, 15 Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,Pranav Shyam,Pamela Mishkin,Bob Mcgrew,IlyaSutskever, and Mark Chen. GLIDE: Towards photorealis-tic image generation and editing with text-guided diffusionmodels. In Proceedings of the 39th International Conferenceon Machine Learning, pages 1678416804. PMLR, 2022. 2 Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, YijunLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-imagetranslation. Proceedings of the ACM SIGGRAPH Conferenceon Computer Graphics, 2023. 1, 2, 5, 6, 13 Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing object-level shapevariations with text-to-image diffusion models. Proceedingsof the International Conference on Computer Vision, 2023.2 Koutilya Pnvr, Bharat Singh, Pallabi Ghosh, Behjat Sid-diquie, and David Jacobs. Ld-znet: A latent diffusion ap-proach for text-based image segmentation. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 41574168, 2023. 12 DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann, Tim Dockhorn, Jonas Muller, Joe Penna, andRobin Rombach.Sdxl: improving latent diffusion mod-els for high-resolution image synthesis.arXiv preprintarXiv:2307.01952, 2023. 1 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.Zero-shot text-to-image generation. In International Confer-ence on Machine Learning, pages 88218831. PMLR, 2021.2",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,and Mark Chen. Hierarchical text-conditional image gen-eration with clip latents. arXiv preprint arXiv:2204.06125,2022. 2": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1068410695, 2022. 2, 3, 5, 6,7, 8, 12 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III18, pages 234241. Springer, 2015. 3 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour,Burcu Karagol Ayan,S Sara Mahdavi,Rapha Gontijo Lopes, et al. Photorealistic text-to-image dif-fusion models with deep language understanding. Advancesin Neural Information Processing Systems, 2022. 1, 2",
  "Chuanming Tang, Kai Wang, and Joost van de Weijer. Iter-inv: Iterative inversion for pixel-level t2i models. Neurips2023 workshop on Diffusion Models, 2023. 2": "Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and TaliDekel. Splicing vit features for semantic appearance transfer.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1074810757, 2022.5 Narek Tumanyan, Michal Geyer, Shai Bagon, and TaliDekel.Plug-and-play diffusion features for text-drivenimage-to-image translation. Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, 2023.1, 2, 5, 6, 13 Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, andJoost van de Weijer. Dynamic prompt learning: Address-ing cross-attention leakage for text-based image editing. Ad-vances in Neural Information Processing Systems, 2023. 1,2, 4, 5, 6, 12, 14, 15",
  "Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka.Mdp: A generalized framework for text-guided image edit-ing by manipulating the diffusion path, 2023. 1": "Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Mul-tiscale structural similarity for image quality assessment. InThe Thrity-Seventh Asilomar Conference on Signals, Systems& Computers, 2003, pages 13981402. Ieee, 2003. 5 Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Tor-ralba, Hengshuang Zhao, and Shengjin Wang. Detecting ev-erything in the open world: Towards universal object detec-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1143311443,2023. 3",
  "Tao Wu, Kai Wang, Chuanming Tang, and Jianlin Zhang.Diffusion-based network for unsupervised landmark detec-tion. Knowledge-Based Systems, page 111627, 2024. 12": "Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:Semantic segmentation emerges from text supervision. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1813418144, 2022. 1,3 Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-long Wang, and Shalini De Mello. Open-vocabulary panop-tic segmentation with text-to-image diffusion models.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 29552966, 2023. 12",
  "Shiwen Zhang, Shuai Xiao, and Weilin Huang.Forgedit:Text guided image editing via learning and forgetting. arXivpreprint arXiv:2309.10556, 2023. 1": "Yufan Zhou,Bingchen Liu,Yizhe Zhu,Xiao Yang,Changyou Chen, and Jinhui Xu. Shifted diffusion for text-to-image generation. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1015710166, 2023. 1 Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, LuYuan, et al. Generalized decoding for pixel, image, and lan-guage. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1511615127,2023. 3",
  "A. Limitations": "One limitation of our work is related to the size of cross-attention maps.Smaller maps, particularly those sized1616, contain more semantic information compared tolarger maps. While this rich information is beneficial, it re-stricts our ability to achieve precise and fine-grained struc-ture control. We intend to work on pixel-level text-to-imagemodels to address this limitation. Another limitation is thatthe frozen Stable Diffusion (SD) model itself lacksstraightforward editing capabilities, which may impact thequality of editing results. To address this limitation, we planto explore ideas from InstructPix2Pix to develop a bet-ter text-guided image editing method in the future. Addi-tionally, the SD model faces challenges in reconstructingthe original image with intricate details due to the compres-sion mechanism in the first-stage autoencoder model. Edit-ing high-frequency information remains a significant andongoing challenge that requires further research and devel-opment. Overcoming these limitations and advancing ourknowledge in these areas will contribute to the improvementand refinement of image editing techniques. Furthermore,it is noteworthy that diffusion models have shown promiseacross various applications, such as object detection ,image segmentation , and landmark detection .Therefore, a potential future research is to extend diffusionmodels to various practical applications, including visualtracking, continual learning, font generation, and beyond.",
  "B. Broader Impacts": "The application of text-to-image models in image editinghas a wide range of potential uses in various downstreamapplications. Our model aims to automate and streamlinethis process, saving time and resources. However, its im-portant to acknowledge the limitations discussed in this pa-per. Our model can serve as an intermediate solution, ac-celerating the creative process and providing insights forfuture improvements. We must also be aware of potentialrisks, such as the spread of misinformation, the potentialfor misuse, and the introduction of biases. Ethical consider-ations and broader impacts should be carefully examined toresponsibly harness the capabilities of these models.",
  "D. User Study": "In , we provide the full table of our user stud-ies. Each of the twenty participants was randomly assignedtwenty image editing tasks from our COCO-Edit dataset.Ten tasks involved single-object images, and the other teninvolved multi-object images. Participants were asked toanonymously evaluate the best approach out of six methodsbased on image editing quality and background preserva-tion. The results show that LocInv was preferred by the par-ticipants, primarily satisfying their subjective preferencesfor both single-object and multi-object images. For refer-ence, the user study interface is depicted in .",
  "E. Cross-Attention Maps": "In addition to the qualitative and quantitative image edit-ing comparisons presented in the main paper, we have ex-tended our evaluation by including a comparison with fullsentences cross-attention maps. These comparisons are il-lustrated in and . This comprehensive analysisfurther demonstrates the superior adaptability and quality ofcross-attention maps produced by LocInv, in comparison toprevious approaches, including NTI , DDIM , andDPL , even across various input prompt lengths and realimage scenarios."
}