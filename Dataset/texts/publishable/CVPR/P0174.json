{
  "Abstract": "Mitigating biases in generative AI and, particularly intext-to-image models, is of high importance given theirgrowing implications in society. The biased datasets usedfor training pose challenges in ensuring the responsible de-velopment of these models, and mitigation through hardprompting or embedding alteration, are the most commonpresent solutions. Our work introduces a novel approachto achieve diverse and inclusive synthetic images by learn-ing a direction in the latent space and solely modifying theinitial Gaussian noise provided for the diffusion process.Maintaining a neutral prompt and untouched embeddings,this approach successfully adapts to diverse debiasing sce-narios, such as geographical biases. Moreover, our workproves it is possible to linearly combine these learned la-tent directions to introduce new mitigations, and if desired,integrate it with text embedding adjustments. Furthermore,text-to-image models lack transparency for assessing biasin outputs, unless visually inspected. Thus, we provide atool to empower developers to select their desired conceptsto mitigate. The project page with code is available online1.",
  ". Introduction": "Text-to-image models have enabled the possibility of gen-erating personalized images with the content described bywords, transforming industries, and even, our thoughts.Given these models impact on our lives, it is key to guar-antee they are developed responsibly, battling stereotypes, lack of diversity, and inherited biases, but remain-ing truthful . Moreover, if this biased generated data isused for training future models, these biases will persist orbe amplified in the new models themselves.From gender to race, to poor geographic representa-tion, biases can be found everywhere in generative mod-els. Leonardo N. and Dina B. have found a concern-ing pattern in Stable Diffusion v1.5 where low-paying jobsare dominated by women and darker-skinned individuals.",
  ".Debiasing diverse concepts.Generations observedupon the application of our approach in several scenarios": "While most of the work focuses on social and racial bi-ases , Basu et al. conduct a user study validatinggeographical biases in models like DALLE and Sta-ble Diffusion , finding underrepresented 25 out of 27countries. Cultural biases are also found when using ho-moglyphs in text-to-image synthesis . Efforts to under-stand biases in vision-language models lead to the develop-ment of automated tools , the use of gender estima-tion , face and skin tone detection , and the evalu-ation of geographical representativeness using CLIP-basedsimilarity and k-nearest neighbor models . Mitigation ef-forts include prompt interventions , the development ofmore inclusive datasets featuring images reflecting diversegeographic and socioeconomic contexts , and alter-ations in prompt embedding strategies . Chuang et al. propose a method to mitigate bias by maximizing thesimilarity between biased and non-biased prompts. They",
  "arXiv:2406.06352v1 [cs.CV] 10 Jun 2024": "construct a projection matrix to eliminate biased directionsfrom text embeddings before inputting them into the model.Using a similar approach, but employing tokens of availableimage datasets, ITI-Gen learns a set of prompt embed-dings to append to the initial prompt.We first present a tool to enhance developers visibil-ity, given that we believe understanding the relationship be-tween concepts, and the reason for certain attributes appear-ing in generations, is key to mitigating them. Secondly, wepropose a straightforward novel approach for bias mitiga-tion, linearly separating the latents, noisy information ten-sors of two different prompts, learning the transformationfor debiasing in the latent space of the diffusion model.We apply this transformation, named latent direction,at a specific weight, linearly combining it with the initialGaussian noise. The results in a series of diverse exper-iments prove it successfully works to debias, without theneed for prompt alteration. Our approach remains simple,while effective and adaptable to varied scenarios. It allowsthe synergy of different latent directions, and it is flexibleto be used in combination with an approach modifying theprompt embeddings if desired. Through our experiments,we focus on demonstrating the impact of our method for themaximum debiasing transition. However, fair distributions2",
  ". A Tool for Bias Understanding": "Our tool for bias understanding targets two key points: com-prehending the connections between embeddings and gen-erations, and detecting the social characteristics and objectspresented in the image. Theoretically, the closer the rela-tion between attribute and concept in the semantic space,the more prone these attributes are to appear in the gener-ated images. We explain the semantic relationship betweenattributes and concepts by computing the cosine similarityof their embeddings, and comprehending the innate biaseswithin the employed text and vision encoders. In addition,we reveal the visual components of the generated images,using CLIP as a zero-shot classifier for gender and race,and Kosmos-2 as a Multimodal Large Language Model(MLLM) for perceiving object descriptions from the visualoutput seen in the images. With this, we present the fre-quency of objects and social characteristics in the genera-tions, validate if the embedding associations correspond tothe visualized content, and provide an understanding of theresults without seeing the images. For instance, in-forms us that our generations are debiased, with men in suitsin front of their houses. However, it presents the innate bi-ases of CLIPs text and vision encoders in Stable Diffusion",
  ". Our Proposed Method for Bias Mitigation": "Training: Finding the Latent Direction Our approach() proposes a fundamentally different transformationof the diffusion processs input, learning the latent directiondZ, from the Gaussian latents at denoised steps, to conditionthe initial noisy information tensor fed into the diffusionprocess zT N(0, I). Given a pre-trained latent diffusionmodel (LDM) and a neutral prompt P1 (e.g., a photo ofa man, in color, realistic, 8k), we aim to obtain debiasedgenerations in the absence of prompt modifications or em-beddings alterations. We propose a target prompt P2 (e.g.,a photo of a black man, in color, realistic, 8k) and sam-ple N number of images, for both P1 and P2, to constructthe training dataset. The diffusion process for each of theprompts starts from an initial noisy latent zT , which is de-noised over k steps, finally reaching the ultimate latent z,fed into the decoder D to generate the synthetic image x.While generating the N images, we save each imageslatents at chosen denoising steps L = (L0, , Lk), repre-senting (zT , , zT i for i {0, 1, 2, . . . , k} ), building adataset of noisy information tensors. Note, L0 correspondsto the initial Gaussian latent zT , while Lk is z. Once allchosen latents are saved for both prompts, we select one de-noising step i to obtain dZ with those specific latents. Forinstance, we could decide to train with L10, the latents savedfor the N images at step 10 (zT 10).The model. We use a support vector machine (SVM) . Summary of our training (left) and inference (right) approach. We use P1 and P2 to generate N images x. With their latents,chosen at step i, we train a SV M to learn dZ. We debias the neutral prompt P1, applying dZ to the random initial latent zT N(, 2)at a specific weight, shifting the generations towards debiased samples with the attributes learned through the latent direction. to linearly separate the latents across our labeled dataset ofN samples for each prompt. The classifier uses a linearkernel and provides the dZ, the so-called latent direction,we utilize for debiasing.Inference: Applying the Latent Direction. To obtain de-biased generations, the LDM, in our case Stable Diffusion, uses for inference only P1, the neutral prompt. Thisprompt is fed into CLIPs text encoder E forming the firstinput. As the second one, instead of using an initial Gaus-sian random information tensor for denoising, we transformthis latent by applying the learned latent direction dZ, fol-lowing equation 1.",
  "zT = zT + dZ(1)": "Where zT N(, 2) and is the weight parameter atwhich the latent direction is applied. The higher the , thehigher the strength of the debiasing impact.The optimal configuration.Optimal debiasing resultsare found when selecting the most favorable latent L =(L0, , Lk) and weight configuration . Thus, we pro-pose two approaches to automatically find it without havingto visually explore all possibilities. The first one is to usethe clean-fid library to compute the similarity betweenthe distribution of a small subset of generated images witha particular configuration, and the distribution of known de-biased images, and the second one is to leverage CLIP as a zero-shot classifier, selecting the configuration with ahigh classification of the desired debiased class.",
  ". Experimental Results": "We validate our work using Stable Diffusion XL , with50 denoising steps, applying different latent directions dZ ina series of experiments to understand its impact. Success-ful results are obtained in diverse mitigations. We presentfour different debiasing scenarios following the settings ofprevious papers , addressing social group bi-ases, cultural and geographical biases, and the Waterbird benchmark for evaluating spurious correlations. summarizes the experimental results obtained.Quantitative Metrics. We leverage the Statistical ParityDifference (SPD) to evaluate our debiasing method inthe generated image datasets. We use CLIP for attributeprediction and measure the absolute difference in the pro-portions of desired attributes between the original biaseddataset, generated with the plain Stable Diffusion model,and the debiased dataset. A value close to zero indicatesminimal debiasing impact, while a value of one signifiessuccessful debiasing with the desired attribute present in allgenerations.Gender debiasing in professions. We learn dZ by definingN = 50, P1 =a photo of a man, in color, realistic, 8k andP2 =a photo of a woman, in color, realistic, 8k, selectingL25 and = 10. We apply the woman latent direction tothe neutral prompts a photo of a [profession], in color, re-alistic, 8k and observe a positive shift from 0% to 52%, in100 generations for the case of doctor. Other professionsknown to be extremely biased, such as firefighter, engineer,or librarian have shown a slightly improved impact, withshifts of 8%, 3%, and 2%, respectively. We believe majordebiasing can be achieved with these professions upon find-ing the optimal dZ.Skin tone debiasing.We explore the transition of skintones in generated images.For it, we create four train-ing datasets, where N = 50, with P1 =a photo of a[man/woman], in color, realistic, 8k and P2 =a photo ofa black [man/woman], in color, realistic, 8k.. With the pro-posed automated method for configuration selection we set-tle on training with the latents at step 10 (L10), at a weight = 15. The results shift 100 generations using the neutralprompt P1 =a photo of a man, in color, realistic, 8k tocontain a 95% of black men images from the original 8%.Similarly, with P1 =a photo of a woman, in color, real-istic, 8k and the application of the dark-skin dZ at L25, = 14 yields a 79% of black women from an initial 1%.Waterbird debiasing. In this experiment we evaluate the",
  "(SD XL, ours)0.870.780.520.080.09-0.330.28(SD 2.1, PD )0.910.900.140.060.010.290.790.47(SD 2.1, ours + PD )0.941.000.290.040.220.681.000.96": ". Quantitative results across 100 generations. SPD in the presence3of the desired attributes: dark skin tone, female genderfor the case of doctor, firefighter and male gender for cleaner, land environments for waterbirds, Indian wedding attributes and wealthierlooking houses avoiding thatched roofs and mud huts. We learn the latent directions with SD 2.1 for the results seen in the last row. impact of the combination of methods, manipulating boththe prompts embeddings and the initial Gaussian noise,using Stable Diffusion 2.1. We aim to generate waterbirdsin land environments with the prompt P1 = A picture of awaterbird. We replicate their setup and apply our learnedlatent direction (P1 = A picture of a waterbird, P2 = Apicture of a landbird, = 10, L10). The results across100 generations yield exceptional results with 78% of gen-erations displaying waterbirds in terrestrial habitats, 2% inaquatic landscapes, and 20% showing bird portraits.Geographical representativeness. It is hard to obtain bal-anced geographical representations of a picture of a wed-ding, in color, realistic, 8k, given this neutral prompt isnormally biased towards representations of Western wed-dings. In an attempt to shift the distribution towards Indianweddings, we define P1 =a picture of a wedding, in color,realistic, 8k and P2 =a picture of a wedding in India, incolor, realistic, 8k, learning dZ using L30 and applying itwith = 35 we see an increment of 33% in CLIPs classi-fication. Inspired by Bianchi et al. we debias 38% of thethatched roofs observed when using P1=A wealthy Africanman and his house, utilizing P2=A wealthy man and hishouse and applying the learned wealthy man direction dZat L10 and = 15.Comparison with PD. In Tab. 1, we present the quantita-tive results of our study. The comparison with the PromptDebiasing (PD) method is challenging, due to the uti-lization of distinct models and the diverse biases in them,e.g., for P1 =A wealthy African man and his house SDXL presents generations of mansions with thatched roofs,whereas SD 2.1 shows mud huts. We choose to use SD XLgiven the enhanced quality of the model facilitates the learn-ing of dZ and minimizes the inconveniences of elaboratedhard prompting to obtain quality images with SD 2.1. Theoutcomes evaluated through our experiments demonstratethe potential of latent directions to obtain competitive debi-ased generations despite maintaining neutral embeddings.",
  ". Comparison of results with dZ trained at differentlatents L and applied at different weights . Generations of thesame woman in its transition to dark skin": "asing than the choice of training latent L. Moreover, higherlatent directions require lower weights to achieve the de-biased results, given more structured noise is found at thehigher debiasing steps. However, as we move in dZ thereis a limit to how far we go with , given an extremely highweight leads to distorted generations, out of the distribu-tion. Lastly, it is possible to linearly combine latent direc-tions following zT = zT + i=1 i dZi. For instance,by applying the woman [L25 10] and dark-skin [L10 10]latent directions to the Gaussian noise of the neutral promptP1 =a photo of a doctor, in color, realistic, 8k we achievegenerations of dark-skinned female doctors ().",
  ". Conclusion": "After proposing a tool for uncovering and quantifying thepresent bias in text-to-image models, a novel method is pro-posed for mitigation. By learning and applying latent direc-tions dZ we demonstrate it is possible to alter the diversecomplex biased relations, such as those in cultural events,while maintaining unaltered neutral prompt embeddings.Future work encourages the exploration of more advancedclassifiers to find the optimal dZ. 3Presence of classes through CLIPs classification: [A picture of ablack [man/woman], A picture of a white [man/woman]], [A pictureof a woman, A picture of a man], [A picture of a Western wedding,A picture of an Indian wedding]. A user study is used to evaluate thecomplex generations (Wedding, African man) given classification with de-fined classes in these cases does not match reality.",
  "Haiwen Feng, Timo Bolkart, Joachim Tesch, Michael J.Black, and Victoria Abrevaya.Towards racially unbiasedskin tone estimation via scene disambiguation, 2022. 1": "Felix Friedrich, Manuel Brack, Lukas Struppek, DominikHintersdorf, Patrick Schramowski, Sasha Luccioni, andKristian Kersting. Fair diffusion: Instructing text-to-imagegeneration models on fairness, 2023. 3 William Gaviria Rojas, Sudnya Diamos, Keertan Kini, DavidKanter, Vijay Janapa Reddi, and Cody Coleman. The dollarstreet dataset: Images representing the geographic and so-cioeconomic diversity of the world. In Advances in NeuralInformation Processing Systems, pages 1297912990. Cur-ran Associates, Inc., 2022. 1 Niharika Jain,Alberto Olmo,Sailik Sengupta,LydiaManikonda, and Subbarao Kambhampati.Imperfect ima-ganation: Implications of gans exacerbating biases on facialdata augmentation and snapchat selfie lenses, 2021. 3",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, ShaohanHuang, Shuming Ma, and Furu Wei. Kosmos-2: Groundingmultimodal large language models to the world, 2023. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision, 2021. 2, 3 Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B.Adcock, Laurens van der Maaten, Deepti Ghadiyaram, andOlga Russakovsky. Geode: a geographically diverse evalua-tion dataset for object recognition, 2023. 1"
}