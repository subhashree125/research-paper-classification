{
  "Abstract": "Understanding and anticipating human movement hasbecome more critical and challenging in diverse applica-tions such as autonomous driving and surveillance. Thecomplex interactions brought by different relations betweenagents are a crucial reason that poses challenges to thistask. Researchers have put much effort into designing asystem using rule-based or data-based models to extractand validate the patterns between pedestrian trajectoriesand these interactions, which has not been adequately ad-dressed yet. Inspired by how humans perceive social inter-actions with different level of relations to themself, this workproposes the GrouP ConCeption (short for GPCC) modelcomposed of the Group method, which categorizes nearbyagents into either group members or non-group membersbased on a long-term distance kernel function, and the Con-ception module, which perceives both visual and acousticinformation surrounding the target agent. Evaluated acrossmultiple datasets, the GPCC model demonstrates signifi-cant improvements in trajectory prediction accuracy, val-idating its effectiveness in modeling both social and indi-vidual dynamics. The qualitative analysis also indicatesthat the GPCC framework successfully leverages group-ing and perception cues human-like intuitively to validatethe proposed models explainability in pedestrian trajectoryforecasting. Codes are available at",
  "(a)": ". We divide factors that influence our decision to move andbehave into three parts, as shown in (b), (c), and (d), accordingto scene (a). (b) Self intention denotes the agents self-will ofdestination. (c) Grouping relation denotes other group membersimpact on the target agent. (d) Social perception denotes how thetarget agent perceives surrounding agents with visual and acousticinformation. The main contributions in this work originate frommodeling these three factors and how they cooperate to improvethe trajectory prediction performance. edge, such as grouping relations and interaction dynamics,on pedestrian movement. In light of this limitation, thispaper introduces the GPCC model (GrouP ConCeption),which incorporates three essential factors to improve trajec-tory prediction accuracy: self-intention, grouping relations,and social perception().Self-Intention indicates that each pedestrian has an in-trinsic goal or intended destination, which determines theirbasic movement. The incorporation of this self-driven in-tent ensures that the model respects the primary individualmotivation inherent in a pedestrians trajectory, thereby pro-",
  "arXiv:2412.02395v1 [cs.CV] 3 Dec 2024": "viding a base prediction that is aligned with the personsobjectives. Grouping Relations are widely observed in real-world settings, i.e., pedestrians often move in groups (e.g.,with family or friends). Members within these groups ex-ert influence over one anothers movements. Recognizingand modeling these related group dynamics allows themodel to predict the consistent movement patterns often ob-served in group scenarios more accurately. Revealing thisrelation presents a significant improvement over models thattreat all neighboring agents with the same paradigm in ef-fectiveness and explainability. Social Perception representsthat pedestrians continuously adjust their paths based on thepresence and behavior of other unrelated pedestrians, i.e.,agents not belonging to the same group. This factor incor-porates visual and acoustic cues within and out of the pedes-trians field of view (short for FOV ) , enabling the model tocapture the target agents responses to other agents move-ments, such as avoiding collisions or making path adjust-ments.By integrating these three factors, the GPCC model of-fers a self-aware, group-aware, and socially-aware trajec-tory prediction approach, capable of accounting for both in-dividual motivations and the different levels of the interac-tive context of each pedestrians surrounding agents and al-lows the model to better predict trajectories by consideringboth related and unrelated agents in the environment.In summary, we contribute (1) The Group method thatdivides neighboring agents into the grouping members andones out of this group based on long-term distance kernelfunction; (2) The Conception module that perceives visualand acoustic information from different regions around thetarget agent based on vision range of human eyes; (3) Thevalidation of the effectiveness and explainability of the pro-posed GPCC model by evaluating it on different datasetsand analyzing how its components function to co-contributeto improve the models trajectory prediction performance.",
  ". Related Works": "Pedestrian Trajectory Prediction. Pedestrian trajectoryprediction has evolved considerably, beginning with tradi-tional rule-based models and advancingto complex deep learning methods that aim at capturingthe high-dimensional features of social interactions with therise of deep learning. These data-based methods introduced recurrent neural networks and generative ad-versarial networks to better capture the serial dependenciesin trajectory prediction, which improved trajectory accuracyby learning latent social behaviors from data. Graph neuralnetworks (GNNs) have also been adopted to model socialrelationships in trajectory prediction, whichallow for more expressive and context-aware social interac-tion modeling, as they capture the dynamic dependencies",
  "between individuals in the scene": "Holistic Social Interaction Modeling. In most previousworks, they adopt a unified paradigm to rep-resent social interactions across all pedestrians in the sceneusing certain properties of deep learning models. These ap-proaches typically apply a single, generalized representa-tion for all agents, thereby treating social interactions as ho-mogenous across the entire scene. Although this method isoften equipped with considerable prediction performance,it often overlooks critical cues in social relationships, suchas group dynamics or specific relational contexts that differamong individuals. This limitation can lead to less accuratepredictions in complex social environments where group re-lationships play a significant role. Multi-Level Social Interaction Modeling.To addresssome of the information loss inherent in holistic repre-sentations, multi-level approaches attempt to model so-cial interactions at different levels of granularity.Thesemethods incorporate more specific relationalinformation, such as distinguishing between various typesof social connections, which partially alleviates the issueof missing social relationship details. However, the major-ity of these approaches still rely heavily on black-box mod-els driven by data, offering limited interpretability, makingit challenging to understand the models internal decision-making processes.The proposed GPCC model adopts the multi-level in-teraction modeling paradigm with an explainable feature-extracting process, improving the interpretability of themost current data-based methods and acquiring consider-able trajectory prediction accuracy.",
  ". Problem Formulation": "Pedestrian trajectory prediction task can be described asforecasting trajectories in the future time range Tfuturebased on what we observed in the past time range Tpast.Let current time be the origin of the time axis. This workmainly concerns predicting the 2D coordinates of the tar-get agent i when t = {T, 2T, 3T, . . . , nfT} based on allN 1 neighboring agents and agent is past positions whent = {(np 1)T, (np 2)T, . . . , T, 0}, where T isthe interval of sampling continuous trajectories into dis-crete sequences, nfT = Tfuture and (np 1)T = Tpast.Formally, it aims at designing a network N that satisfiesYi = N (Xi, traj\\ {Xi} , C), where Xi indicates the tar-get agent is historical coordinate sequence, traj is a setthat contains all N agents historical coordinate sequences,and C represents other information that can be used as inputof the network N to predict agent is future sequence Yi. . Schema of the modeling process of the proposed GPCCmodel and the computation pipeline of the Group method and theConception module. The Group Networks divide the neighborsinto two sets based on long-term distance kernel function K. Agroup encoder encodes one sequence set within the Group method,and the Conception module perceives the other to gain encodedConception representation. The concatenation of these two fea-tures and the encoded self-trajectory sequence is the backbonetrajectory prediction models input feature to forecast the targetagents future trajectories.",
  "It aligns with our instinct that pedestrians walking in agroup which contains other related agents interact withother agents differently compared with those walking aloneamong other strangers": "Long-Term Distance Kernel Function. To better illustratethis grouping relation, we introduce the long-term distancekernel function denoted as K() to distinguish group mem-bers of the target agent from all neighboring agents by cal-culating the overall distance sum between them during allobserved or past time Tpast.Formally, the long-term distance kernel function K()applying to the target agent i and a neighboring agent j canbe described as",
  ". Conception Module Modeling": "The Conception module is introduced for target agents toperceive unrelated neighbors around them.As shown in , pedestrians perceive other agentswith broader information within the range of FOV. Theymight intuitively observe whether the other pedestriansmotions and behavior will influence their decision to choosea routine. For agents out of the FOV, pedestrians might notbe able to perceive such broad information, but their move-ments could still be noticed when approaching the targetpedestrian. To illustrate this difference, we use two model-ing strategies for them.",
  "imove = atan2pt0i pt1i.(5)": "For all agent j N(iFOV), we mainly consider their dis-tance, relative moving direction, and velocity to the targetagent i. The FOV angle iFOV is equally divided into twopartitions representing the left and right side of the targetagent, and the division line is defined as the moving di-rection of him.We calculate the average of these threevisual information acquired in each partition of FOV re-gions. The computation process of left and right regions(ileft and iright) are symmetrical, and we represent the leftregion here as an example. We define a set N(ileft) com-posed of unrelated agents in the left region and a functionn() to calculate number of agents in a set.",
  "pt0j pt0i2/nN(irear).(9)": "The three partitions, i.e.left, right, and rear, act as posi-tion encoding of neighboring agents, allowing the moduleto perceive their position information from an intuitive per-spective rather than simply pooling features around the tar-get agent. Notably, all average calculations proceed in eachpartition, and all factors are concatenated in the order ofright(short for r in Eq. (10)), left and rear as below:",
  ". Fusion Strategy and Overall Modeling": "In our work, agent is observed trajectory Xi is also firstembedded into high-dimensional feature denoted as f iself Rnpd by emdedding layer f(), i.e., f iself = f (Xi).As shown in , the GrouP ConCeption model (shortfor GPCC) we propose takes the fused vector f i contain-ing Conception feature f icon (representation of social inter-actions with unrelated neighboring agents), self-centeredfeature f iself (representation of ego trajectory) and Groupfeature f igroup (representation of group members trajecto-ries) gained in Eq. (3) as input to learn to predict future",
  "f i = tanhWfuse Concatf icon, f iself, f igroup.(11)": "Wfuse are the trainable weights in Eq. (11) and trainablebias is omitted in equation for brevity.We take Transformer and multi-style trajectory gen-eration module in MSN as backbone trajectory predic-tion model. As depicted in , the Transformer encoderlearns the high-dimensional information of f i by computingself-attention over np steps (number of observation frames).The decoder takes the encoders output features as keys andqueries and the linear fitting trajectory Yli of the target agentas values. The Transformer outputs a feature for the tra-jectory generation module to generate trajectories. Thepipeline isYi = Bpredictionf i, Yli.(12)",
  ". Experimental Settings": "Datasets. To evaluate the trajectory prediction performanceof the proposed GPCC model, we utilize four datasets: (1)ETH-UCY , (2) Stanford Drone Dataset (shortfor SDD) , (3) nuScenes2 , and (4) NBA SportVU(short for NBA3) .(1) ETH-UCY includes several videos captured inpedestrian walking scenes.We follow the leave-one-out strategy to train and validate models with{np = 8, nf = 12} and a sampling interval of T = 0.4s.(2) SDD comprises 60 video recordings obtained froman aerial perspective of the campus. The agents, which fallinto different categories (e.g., pedestrian, bicyclist, skate-boarder, cart, car, and bus), have been labeled in pixels.The 60% of videos were designated for training, the 20%for validation, and the 20% for testing, with the same set-tings {np, nf, T} = {8, 12, 0.4s} as .",
  ". (15)": "Implementation Details. GPCC model and its variationsare trained on one NVIDIA GeForce RTX 3090. The FOVangle of the Conception module is set to be 180(discussionof choosing the FOV angle for the Conception module inSec. 4.5). Following , trajectories are preprocessed bymoving to (0, 0). We use the Adam optimizer with a learn-ing rate 0.0002 to train our models, and the batch size is1000 for 200 epochs.",
  ". Comparisons": "We compare the proposed GPCC model with several state-of-the-art methods as shown in Tabs. 1 and 2. The intro-ductions of grouping relations are also included in some ofthese methods.(1)ETH-UCY. ETH-UCY contains pedestrian trajecto-ries only, and we can observe that the proposed GPCCmodel has competitive performance compared with otherstate-of-the-art methods. As shown in Tab. 1, the GPCCmodel has competitive performance compared with otherstate-of-the-art methods. Notably, it outperforms the out-standing UPDD by 9.4% FDE. Compared with othergrouping-related methods, the GPCC model alsopresents considerable performance.Overall, the perfor-mance of GPCC has been validated on ETH-UCY.(2)SDD. SDD contains diverse scenes where more typesof agents move and behave compared with ETH-UCY. Asshown in Tab. 2, the proposed GPCC model reaches thebest prediction accuracy in gaining better FDE by 2.1%lower than the second best method SocialCircle and thesecond best ADE by only 0.4% worse than the best-ADEmethod. The results on SDD further verify the GPCCmodels capability of handling more complex scenarios.",
  ". Ablation Study": "We conduct ablation experiments4 to further validate theeffectiveness of different modules in our GPCC model asshown in Tab. 3.Comparing v3 (disabling both Group method and Con-ception module) and v0 (original GPCC model) from Tab. 3,we can see that disabling Group and Conception couldlead to a significant prediction performance drop of up to",
  "See results of SDD, NBA, and nuScenes in Supplementary Material": "58.6% larger FDE. Interestingly, it reaches the most perfor-mance drop of all variations at v2, which disables the Groupmethod only and utilizes the Conception module. Thoughtof this phenomenon is that when applying the Conceptionmodule of the GPCC model without the Group methodwould lead the model to focus on the unrelated agentsonly, which could cause more significant errors comparedto the strategy of ignoring both related and unrelatedagents.Therefore, these results5 demonstrate the quantitativeimprovements in prediction performance brought by theGroup method and the Conception module and illustratehow their functions vary with different datasets and vari-ations.",
  ". Time Efficiency": "The parameter amount of the overall GPCC model is2057950. The average inference time is 277 millisecondswith batchsize 1000 and 26 milliseconds with batchsize 100by running model on one Apple Mac Studio (M2 Max) onthe ETH-UCY dataset (49ms with batchsize 1000 and 24mswith batchsize 100 on the SDD dataset.). Comparisons withother methods are demonstrated in Supplementary Material.",
  "The previous Secs. 4.3 and 4.4 present the quantitative ef-ficiency of the proposed GPCC model. In this section, wepresent the qualitative results of GPCC, which numericalmetrics might not reveal": "Analysis of Group Method and kernel Function K. Wefirst analyze whether the Group method in our GPCC modelworks and visualize how the long-term distance kernel func-tion enables the model to learn group relations betweenpedestrians through a family group scene in ETH-UCYzara1.In the groundtruth(real world) scenario shown in (a1) - (f1), Mother, Father, and Child walk as a distinctgroup, which means they are related.Observing thezara1 clip from 28s to 43s can validate this grouping re-lation, which aligns with our manual-labeled relations be-tween Mother, Father, and Child through orange concentricrings (short for the family group). In the observation(GPCCmodel) scenario on the right side of frame axix shown in (a2) - (f2), group relations derived from the Groupmethod of GPCC model are marked with pink concentricrings. We found some intriguing phenomena by comparingthese two scenarios.More specifically, the GPCC model does not consider thefamily group as a group, and it divides Child, Father, anda pedestrian in a long red dress into a group of three mem-bers instead ( (a1) and (a2)). However, the GPCC",
  "GPCC (Ours)6.39/10.17": ".Comparisons to other state-of-the-art methods onSDD under {np, nf, T} = {8, 12, 0.4s}. Notably, metrics areADE/FDE (best-of-20) in pixels rather than meters comparedto ETH-UCY, and lower values indicate better prediction perfor-mance. model does a correct division of this family group in (b1) and (b2), (c1) and (c2), (d1) and (d2). When Motherseems to be walking slower( (e1) and (f1)) than be-fore, the model excludes her from the family group andonly treats Father and Child as a group of two.These splits conducted by the model align with humanjudgment to some extent because what the model receivesare merely 2D coordinates during the past np steps with-out semantic information of social relations between eachagent. Accordingly, the Group method could proceed withgroup divisions based on outputs of the kernel function K.From the visualized results of this in , we could ob-serve that the judgment made in this manner reflects the po-sition relation between the target agent i and another other",
  "Neighbor": ". Illustration of how the Group method and the long-termdistance kernel function K work by comparing groundtruth withthe GPCC models observation. Figure legends on the left indicatea group containing three members ( Mother, Father, and Child) andunrelated neighbors in ETH-UCY ZARA1 scenes. We observethe Child as the target agent here. Indexes on the time axis are theframe number of the corresponding scene. agent j in the observation time.Since the prediction isforecasted with the information presented in the observationsteps rather than in a much more comprehensive time range,this phenomenon of varying grouping splits with differentobservation time windows is in line with human instincts,which validates the effectiveness and the explainability ofthe Group method within the proposed GPCC model.",
  ". Visualization of attention value varies with differentscenes. The model pays more attention to wider regions, whosecolor tends to be yellow accordingly": "visualize these attention fan charts in different scenes in. The target agent pays more attention on his left sidewhen an agent approaches in the left partition( (b)),and the same goes in the right partition in (c). Whenthere is no agent in the front partition (combination of rightand left partition) that could attract the target agents eyesenough, he pays more attention to his rear partition, wherethere might be relatively more agents to raise his interest.Results in Supplementary Material validate the strategyof choosing the FOV angle in the Conception module andfurther support this analysis through visualizing the atten-tion value fan charts. Moreover, represents the effec-tiveness of the Conception module from an intuitive view. Analysis of GPCC modeling. In Eq. (11), we mentionedthat the fused feature f i to be sent to Transformer is com-posed of f iself, f igroup and f icon. After analyzing the effec-tiveness and the explainability of the Group method and theConception module, we mainly focus on the overall model-ing of the proposed GPCC model in this part. To sum up, theGroup method enables the GPCC model to reveal groupingrelations between agents, and the Conception module en-ables the model to perceive social interactions with thosewho do not belong to a group with the target agent. Theycomplement each other in contributing to the trajectory pre-",
  "(g)": ". Visualized results of contribution ratio r(f iself), r(f igroup)and r(f icon) in concentric fan chart form. The fan which has enor-mous angle value means more contribution and r value of the Con-ception feature, the Group feature and the Self feature are markedwith the corresponding color. diction performance of the GPCC model since the model isdesigned to consider relations within group members with-out neglecting the social interactions brought by the un-related ones. In Sec. 4.3, we conducted ablation experi-ments to verify the contributions of the Group method andthe Conception module.To further illustrate how thesetwo strategies (Group and Conception) cooperate and in-teract with each other, we determine the contributions ra-tio function r() of the three features (f iself, f igroup and f icon)by calculating their energy flowing in the linear layer inEq. (11). The pipeline includes:",
  "f m() 2.(16)": "f denotes a set containing all three features, which satis-fies r(f iself) + r(f igroup) + r(f icon) = 1.In , we visualize the contribution of each feature ondifferent datasets with concentric fan charts to be analyzed.The order of the concentric fan is designed to be alignedwith the relative positional relations in reality that r(f iself) ispositioned at inner side and then comes the r(f igroup) whilethe r(f icon) surrounds them. It should be noted that the ratiosare presented relevant only to the angle of each fan chart,and the figure legends in demonstrate an example of",
  "b2)": ". Visualization of the attention value change and the contribution change after placing different kinds of manual neighbors inETH-UCY zara1 and SDD little0 scenes. Fan charts settings are the same as Figs. 4 and 5. the equal contribution ratio of the three features.The contribution ratio (short for r value) of the Concep-tion feature is relatively larger in (a) since the targetagent almost stands still and does not belong to any groupwith another agent in the scene. This might lead the modelto make predictions based on what he perceives throughthe Conception module more when both the ego and thegroup features are limited. As for the family group examplein , r(f igroup) is more significant compared to otherexamples and r(f icon) also presents considerable contribu-tions brought by the Conception feature, which aligns withthe observation that there are relatively more neighboringagents around the Child(see Figs. 3 and 5 for comparisons).Agent walking with a group member in the scene with noone else demonstrates a minor r value of the Conceptionfeature( (c)). A similar phenomenon can be observedin the SDD dataset ( (e) and (f)), and the r value inthe NBA dataset demonstrates an exceptionally high contri-bution of conception feature since the social interactions inNBA games are more abundant than those of other pedes-trian scenes.We also conduct intervention experiments tofurther validate the cooperational property of the Groupmethod and the Conception module as shown in .In this work, we intervene in the prediction scenes fromtwo different aspects.Firstly, we add a manual unre-lated neighbor to the target agent in the original zara1scene ( (a1)). We can observe from the model pre-dictions that the target agent tends to move downwards af-ter placing this manual neighbor who moves right towardshim ( (a2)). The attention value of the Conceptionmodule changes after proceeding with this intervention,and the distinct rise of the value in the right partition isaligned with the position of the manual neighbor. Accord-ingly, the r(f icon) becomes more prominent as more thantwo times the original one. Furthermore, we add a manual related Group member to the target agent of an SDD littl07 scene ( (b1)). The attention value of the Conceptionmodule remains the same since the module could only per-ceive interactions with the unrelated agents. Comparing (b1) and (b2), we could also observe a contributionratio change between features that r(f igroup) increases withr(f iself) dropping while r(f icon) stays almost the same as theoriginal scene.The visualization and intervention experiments abovedemonstrate how the Group method and the Conceptionmodule mutually contribute to the GPCC models predic-tion performance and its effectiveness and explainability.",
  ". Conclusion": "Inspired by how humans perceive other agents in the scene,this work proposes the GPCC model composed of theGroup method, which could reveal grouping relations be-tween pedestrians and the Conception module using visualinformation strategy and acoustic information strategy toperceive social interactions with agents out of the group.Quantitative and qualitative analysis validate the effective-ness and the explainability of the proposed GPCC modeland its components.There are still limitations for further improvements. Theintroduced Group method merely uses the observed trajec-tory coordinates to calculate the long-term distance kernelfunction between agents, and the group-split accuracy re-lies on the sequence length of historical trajectories. How-ever, humans can tell grouping relations within a short time.Considering that little research is being done to study thegrouping relations between agents and validate their effec-tiveness, such limitations might still need to be addressed toacquire better prediction performance.",
  "A subset of the SDD dataset": "Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-cial lstm: Human trajectory prediction in crowded spaces. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 961971, 2016. 4, 11 Inhwan Bae, Jin-Hwi Park, and Hae-Gon Jeon.Learningpedestrian group representations for multi-modal trajectoryprediction.In European Conference on Computer Vision,pages 270289. Springer, 2022. 2, 5, 6",
  "Catarina Barata, Jacinto C. Nascimento, Joao M. Lemos, andJorge S. Marques. Sparse motion fields for trajectory predic-tion. Pattern Recognition, 110:107631, 2021. 2": "Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-ancarlo Baldan, and Oscar Beijbom.nuscenes: A multi-modal dataset for autonomous driving. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1162111631, 2020. 4 Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, XiaZhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai,Jie Tong, et al. Spectral temporal graph neural network formultivariate time-series forecasting. Advances in Neural In-formation Processing Systems, 33:1776617778, 2020. 2 Defu Cao,Jiachen Li,Hengbo Ma,and MasayoshiTomizuka. Spectral temporal graph neural network for tra-jectory prediction.In 2021 IEEE International Confer-ence on Robotics and Automation (ICRA), pages 18391845.IEEE, 2021. 6",
  "Cunyan Li, Hua Yang, and Jun Sun. Intention-interactiongraph based hierarchical reasoning networks for human tra-jectory prediction. IEEE Transactions on Multimedia, 2022.2": "Shijie Li, Yanying Zhou, Jinhui Yi, and Juergen Gall.Spatial-temporal consistency network for low-latency trajec-tory forecasting. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision (ICCV), pages 19401949, 2021. 12 Junwei Liang, Lu Jiang, and Alexander Hauptmann. Simaug:Learning robust representations from simulation for trajec-tory prediction. In Proceedings of the European conferenceon computer vision (ECCV), 2020. 4",
  "Yao Liu, Zesheng Ye, Rui Wang, Binghao Li, Quan Z Sheng,and Lina Yao. Uncertainty-aware pedestrian trajectory pre-diction via distributional diffusion. Knowledge-Based Sys-tems, page 111862, 2024. 6": "Matthias Luber, Johannes A Stork, Gian Diego Tipaldi, andKai O Arras. People tracking with human motion predictionsfrom social forces. In 2010 IEEE international conference onrobotics and automation, pages 464469. IEEE, 2010. 2 Takahiro Maeda and Norimichi Ukita. Fast inference andupdate of probabilistic density estimation on trajectory pre-diction. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 97959805, 2023. 6 Karttikeya Mangalam, Yang An, Harshayu Girase, and Jiten-dra Malik. From goals, waypoints & paths to long term hu-man trajectory forecasting. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1523315242, 2021. 6, 11",
  "Zhao Pei, Xiaoning Qi, Yanning Zhang, Miao Ma, and Yee-Hong Yang. Human trajectory prediction in crowded sceneusing social-affinity long short-term memory. Pattern Recog-nition, 93:273282, 2019. 2": "Stefano Pellegrini, Andreas Ess, Konrad Schindler, and LucVan Gool.Youll never walk alone: Modeling social be-havior for multi-target tracking. In 2009 IEEE 12th Inter-national Conference on Computer Vision, pages 261268.IEEE, 2009. 2, 4 Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi,and Silvio Savarese. Learning social etiquette: Human tra-jectory understanding in crowded scenes. In European con-ference on computer vision, pages 549565. Springer, 2016.4",
  "Saeed Saadatnejad, Yi Zhou Ju, and Alexandre Alahi.Pedestrian 3d bounding box prediction.arXiv preprintarXiv:2206.14195, 2022. 11": "Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, andMarco Pavone. Trajectron++: Dynamically-feasible trajec-tory forecasting with heterogeneous data. In Proceedings ofthe European conference on computer vision (ECCV), pages683700. Springer, 2020. 11 Liushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou,Wei Tang, Nanning Zheng, and Gang Hua.Representingmultimodal behaviors with mean location for pedestrian tra-jectory prediction. IEEE Transactions on Pattern Analysisand Machine Intelligence, 2023. 6 Yuchao Su, Jie Du, Yuanman Li, Xia Li, Rongqin Liang,Zhongyun Hua, and Jiantao Zhou.Trajectory forecastingbased on prior-aware directed graph convolutional neuralnetwork. IEEE Transactions on Intelligent TransportationSystems, pages 113, 2022. 2 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. In Advances in neuralinformation processing systems, pages 59986008, 2017. 4 Anirudh Vemula, Katharina Muelling, and Jean Oh. Model-ing cooperative navigation in dense human crowds. In 2017IEEE International Conference on Robotics and Automation(ICRA), pages 16851692. IEEE, 2017. 2",
  "Conghao Wong, Beihao Xia, Qinmu Peng, and Xinge You.Another vertical view: A hierarchical network for hetero-geneous trajectory prediction via spectrums. arXiv preprintarXiv:2304.05106, 2023. 11": "Conghao Wong, Beihao Xia, Qinmu Peng, Wei Yuan, andXinge You. Msn: multi-style network for trajectory predic-tion. IEEE Transactions on Intelligent Transportation Sys-tems, 24:9751 9766, 2023. 2, 4, 6 Conghao Wong, Beihao Xia, Ziqian Zou, Yulong Wang, andXinge You. Socialcircle: Learning the angle-based social in-teraction representation for pedestrian trajectory prediction.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1900519015, 2024.2, 5, 6, 11, 12 Conghao Wong, Beihao Xia, Ziqian Zou, and Xinge You.Socialcircle+: Learning the angle-based conditioned interac-tion representation for pedestrian trajectory prediction. arXivpreprint arXiv:2409.14984, 2024. 8, 12 Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Si-heng Chen.Groupnet: Multiscale hypergraph neural net-works for trajectory prediction with relational reasoning. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 64986507,2022. 2, 5, 6, 11 Chenxin Xu, Weibo Mao, Wenjun Zhang, and Siheng Chen.Remember intentions: Retrospective-memory-based trajec-tory prediction. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages64886497, 2022. 11 Chenxin Xu, Robby T Tan, Yuhong Tan, Siheng Chen,Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eqmo-tion: Equivariant multi-agent motion prediction with invari-ant interaction reasoning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 14101420, 2023. 6 Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M. Kitani.Agentformer: Agent-aware transformers for socio-temporalmulti-agent forecasting. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pages98139823, 2021. 11",
  "A. Additional Quantitative Analysis": "We only report the GPCC models performance with part ofthe quantitative results due to page limitations. As the qual-itative analysis shown in the main manuscript, the proposedGPCC model presents the capability to handle different pre-diction scenes. This section further validates the modelseffectiveness by presenting additional quantitative analysisof the datasets qualitatively analyzed in the manuscript.",
  "A.1. Experimental Configurations": "NBA comprises trajectories of both players and basketballcaptured by SportVU tracking systems during NBA games.Following the methodology proposed by Xu et al.,we set the parameters to be {np, nf, T} = {5, 10, 0.4s},randomly selecting approximately 50000 samples (ego tra-jectories), with 65% allocated for training, 25% for testing,and 10% for validation.nuScenes contains 1000 driving scenes collected inthe urban area of Boston and Singapore.Each scene is20 seconds long and is annotated at a rate of 2 framesper second.In the manuscript, we only use the two-dimensional trajectories of vehicles to evaluate our GPCCmodel. We follow the methodology proposed by of{np = 4, nf = 12, T = 0.5s}, and training strategy pro-posed by of using 550 scenes to train, 150 scenes tovalidate, and the other 150 scenes to test.",
  "Table S2.Comparisons on nuScenes under {np, nf, T}={4, 12, 0.5s}. Metrics are ADE/FDE in meters under best-of-k (k = 5, 10) and lower values indicate better prediction perfor-mance": "idate the GPCC models capability of modeling differentsocial interactions.We only consider trajectories of vehicles only onnuScenes. Interactions between vehicles could differ en-tirely from those between pedestrians, and there might notbe such group relations between them. This property ofnuScenes are discussed in Sec. 4.5. However, the GPCCmodel still gained a considerable prediction performance,as shown in Tab. S2.Although results in Tab. S4 are relatively minor com-pared to those in Tab. 1, we could still see a performancedrop of larger ADE and FDE at v1,v2 and v3. We couldobserve that agents in SDD tend to move and behave inde-pendently, and there seems to be less interaction betweenagents from the relatively high eye-bird view compared toscenes in ETH-UCY, and this might be the reason for thesmaller contributions of the Group method and Conceptionmodule of the proposed GPCC model when evaluating onthe SDD dataset.In Tab. S3, we could observe that the performance dropsthe most when disabling both the Group method and Con-ception module on the NBA dataset. However, the predic-tion accuracy when disabling the Conception module only(v1) reaches the same level as the original GPCC model(discussed in Sec. 4.5). The results demonstrate an op-posite performance change on the nuScenes dataset. Theprediction performance drops when disabling the Concep-tion module (v1) or both of them (v3) and keeps the samelevel as the original GPCC model when disabling the Groupmethod only (v2). The reason for this phenomenon mightbe common sense that vehicles moving on the road are notin distinct groups. It might lead the model to learn infor-mation through group relations between cars if we use the",
  "B. Additional Analysis of Time Efficiency": "Pedestrian trajectory prediction task requires low-latencyprediction performance to be integrated into correspondingapplications, e.g., autonomous driving. We also use AppleMac Studio (M2 Max) to evaluate the time efficiency of dif-ferent methods, as shown in Sec. 4.4. Considering the prac-tical amount of pedestrians moving in a multi-agent scene,we evaluate the average inference time of 100 target agents(batchsize 100) using different methods.The proposed GPCC model demonstrates considerableinference speed compared with other methods using Trans-former as part of backbone prediction model (32mswith batchsize 100), (96ms with batchsize 100). Withthe inference time already satisfying handling 99 more",
  "C. Additional Analysis of FOV Partitions": "As represented in Tabs. S5 and S6, we first conduct varia-tion experiments on pedestrian dataset ETH-UCY and gamedataset NBA to observe how the performance of the pro-posed GPCC model vary with FOV angle FOV.In Tab. S5, the best performance comes at the originalmodel(v0) whose FOV is set to be 180. This result is inline with the previous study that a humans single-eyeFOV angle is around 150 and the combined FOV from botheyes reaches about 200, which is why we chose to set theFOV = 180 at the first place. Although we can observethat the performance of the GPCC model drops little fromthe overall results on the whole ETH-UCY dataset, the pre-diction performance on specific subsets such as univ, zara1,and zara2 drops relatively more than the other subsets. Thismight be aroused that there are more social interactions inthese subsets and the change of FOV modifies how theConception module perceives social interactions with otheragents.Things are getting more interesting in the NBA datasetshown in Tab. S6.It can be observed that the predic-tion performance is becoming better from FOV = 90 toFOV = 360 (v5, v6, v7 and v8). NBA players mightneed to spread their attention to a broader FOV to gainmore information on the court and make decisions of move-",
  "Table S6. FOV angle FOV analysis on NBA dataset. VariationID is continuous from Tab. S3": "ments based on this information. Furthermore, when theFOV = 0, the Conception module perceives interactionsall the same by only considering the distance factor of otheragents, which might lead to surprisingly minor improve-ments in the prediction performance.We further visualize the attention value of the Concep-tion module at different FOV angle settings with differentconcentric fan charts as shown in Fig. S2. By comparingcharts when FOV = 90 and FOV = 135 (Fig. S2 (b)and (c)), we can observe a change of relative attention valuein right and left partitions. When using a wider FOV angle,agents divided into rear partitions at a narrower FOV anglecan be included into left or right partitions so that they canbe paid more attention.",
  "In the main manuscript, we introduce the Group methodand its core component, long-term distance kernel function": "K(). When calculating the long-term distance kernel func-tion, we mention the threshold to determine whether theagent belongs to the same group as the target agent with-out detailedly introducing how to choose the threshold dmdue to page limitations. This section demonstrates how wedetermine the kernel functions threshold dm.As shown in the manuscript, we also use the familygroup here as an example, considering Child as the tar-get agent. We also use this example because this familyexample includes diverse scenes from multiple neighbor-ing agents to no one else around. Members of the group(Mother, Father, and Child) behave differently accordingto their own will. Overall, Child seems to be talking withFather all the way from the Zara store to the other side ofthe road. Mother walks behind Child at a relatively largerdistance compared to Father. In Fig. S3 (c1), we can ob-serve that Father turns around to Mother to say something(at frame No.790), which further validates their groupingrelations.The long-term distance value of each agent is shownin the bar charts on the right.In Fig. S3 (c1),(d1),(e1),and (f1), the long-term distance of Father-Child or Mother-Child (marked in deep green) is distinctively lower thanother agents (marked in deep blue). However, in Fig. S3(a1), we can observe that the long-term distance of Motherranks third among all neighboring agents of the target agentChild while Father is still at the top of the list. When plentyof agents exist in the scene, the long-term distance can benear each other when the time window used to calculate thedistance sum is relatively short. Although an unrelatedneighbor is classified as a group member, the trajectoriesseem reasonable in this condition. Further, we humans also",
  "Group feature and the Self feature in the form of bar charts. Eachbar corresponds to in the main manuscript": "tric fan chart form. By comparing the angle of each concen-tric fan, we can observe which feature contributes the mostand which plays little role in predicting future trajectories.Here, we further visualize this contribution ratio in a barchart form, which can present a more accurate difference incontribution ratio. Fig. S4 (g) represents the largest contri-bution ratio in conception. This aligns with the situation thatFig. S4 (g) stands for the NBA dataset, where abundant in-teractions can be observed. Combining the fan charts in themanuscript and the bar charts here (Fig. S4), we can betterunderstand how pedestrians decisions to make movementsoriginate from these three features."
}