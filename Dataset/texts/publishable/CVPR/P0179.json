{
  "Abstract": "To compete with existing mobile architectures, Mobile-ViG introduces Sparse Vision Graph Attention (SVGA), afast token-mixing operator based on the principles of GNNs.However, MobileViG scales poorly with model size, fallingat most 1% behind models with similar latency. This paperintroduces Mobile Graph Convolution (MGC), a new visiongraph neural network (ViG) module that solves this scalingproblem. Our proposed mobile vision architecture, Mobile-ViGv2, uses MGC to demonstrate the effectiveness of ourapproach. MGC improves on SVGA by increasing graphsparsity and introducing conditional positional encodingsto the graph operation. Our smallest model, MobileViGv2-Ti, achieves a 77.7% top-1 accuracy on ImageNet-1K, 2%higher than MobileViG-Ti, with 0.9 ms inference latency onthe iPhone 13 Mini NPU. Our largest model, MobileViGv2-B, achieves an 83.4% top-1 accuracy, 0.8% higher thanMobileViG-B, with 2.7 ms inference latency. Besides im-age classification, we show that MobileViGv2 generalizeswell to other tasks. For object detection and instance seg-mentation on MS COCO 2017, MobileViGv2-M outper-forms MobileViG-M by 1.2 AP box and 0.7 AP mask, andMobileViGv2-B outperforms MobileViG-B by 1.0 AP box",
  "Code:": "1.001.251.501.752.002.252.502.75 Latency (ms) ImageNet-1k Top-1 (%) Latency vs. Accuracy on ImageNet-1k MobileViG MobileViGv2 (Ours) . Latency versus top-1 % accuracy on ImageNet-1K ofMobileViG and MobileViGv2. From this graph, we can seethat MobileViGv2 improves on MobileViG, shifting the accuracy-latency curve up for similar points of inference latency. models must provide quick, personalized responses and,more importantly, keep users data private and off the cloud.To achieve this, models must be small in size, fast, lowpower, and still maintain high performance on the targettask.Early efforts at targeting vision applications on mobiledevices used convolutional neural networks (CNNs), suchas with the MobileNet and EfficientNetfamily of architectures.While these models performwell, the introduction of vision transformers (ViTs) brought in new hybrid CNN-ViT mobile architectures that significantly outperformed theirCNN counterparts.The success of CNN-ViT-based mo-bile architectures over CNN-based ones is mainly due tothe global receptive field of the self-attention operation,which accounts for more complex relationships across to-kens. However, this success comes at a cost. The self-attention module is much slower than a convolution layer,as it scales quadratically with the number of input tokens.",
  "Max": ". The new MobileViGv2 architecture. The full architecture is shown on the left. The stem is composed of two stride twoconvolutions that downsample the input image by 4. Each downsampling block contains a single stride two convolution to downsamplethe input by 2. (a) An inverted residual block using GELU activation. For Stage 1, only inverted residuals are used. The number ofinverted residuals in this stage is controlled by N1. (b) For stages 2-4, a combination of inverted residuals and MGCs are used. Each stagehas Ni inverted residuals followed by Mi MGCs, where i is the stage number. The CPE block is a conditional positional encoding implemented with a 77 depthwise convolution. The MRConv block contains graph construction and the max-relative message passingstep. (c) Computing max-relative features using graph construction as outlined in MGC. Given an input image, this module computes themax-relative score against a fixed set of shifted inputs: shifting right, left, up, and down by k. The outputs of this stage are the max-relativescores, which are concatenated to the input and passed through a 11 convolution to complete message passing.",
  "As such, most CNN-ViT-based mobile architectures onlyuse self-attention in low-resolution stages": "More recently, vision graph neural networks (ViGs) were introduced as an alternative to CNNs and ViTs. ViGsconnect tokens based on a predefined algorithm, such as K-nearest neighbors (KNN), and then mix the tokens with amessage-passing scheme. The first use of ViGs in mobilevision architectures came with MobileViG . Mobile-ViG uses Sparse Vision Graph Attention (SVGA), whichreplaces KNN with a static graph construction method, re- sulting in a fast CNN-ViG architecture. While MobileViGperforms well for small model sizes, it scales poorly as themodel size increases, falling nearly 1% behind CNN-ViT-based models with similar latency . To address this scaling problem, we propose a new ViGmodule called Mobile Graph Convolution (MGC), whichimproves on SVGA by increasing graph sparsity and in-troducing positional encodings to the graph operation. Todemonstrate the effectiveness of MGC, we introduce Mo-bileViGv2. This CNN-ViG-based architecture uses inverted residual blocks for processing using local receptive fieldsand MGC blocks for processing using long-range recep-tive fields. With higher graph sparsity, MobileViGv2 canuse MGC at higher resolution stages without impacting la-tency compared to MobileViG. Unlike the original ViG model, MobileViG does not use positional encodings, animprovement introduced in MGC that leads to a significantperformance boost with a slight increase in parameters. Wesummarize our contributions below:1. We propose Mobile Graph Convolution (MGC). Thisnew mobile ViG module creates sparser graphs thanSparse Vision Graph Attention (SVGA) by fixing thenumber of possible connections per token regardless ofinput size. It uses conditional positional encodings toshare the spatial relationships between tokens duringmessage passing. 2. We propose MobileViGv2, as shown in , a CNN-ViG-based mobile architecture that uses MGC to achievesimilar performance to state-of-the-art CNN-ViT-basedmobile architectures. Notably, MobileViGv2 can beginmixing tokens globally at much higher resolution stagesthan existing CNN-ViT-based architectures due to thehigh speed of MGC. 3. Our results show that a CNN-GNN-based mobile visionarchitecture can compete with state-of-the-art CNN-ViT-based image classification models and outperform themon downstream tasks. These results include latency andtop-1 accuracy on ImageNet-1K , object detection andinstance segmentation on MS COCO 2017 , and se-mantic segmentation on ADE20K .The remainder of this paper is structured as follows. Sec-tion 2 covers recent works in the mobile architecture space. provides background on Sparse Vision Graph At-tention (SVGA), the ViG module used in MobileViG . describes the design of the MGC module andMobileViGv2 architecture. describes the experi-mental setup and results for ImageNet-1K image classifica-tion, COCO object detection, COCO image segmentation,and ADE20K semantic segmentation. Additionally, includes ablation studies further outlining the improve-ments of MGC and MobileViGv2 over SVGA and Mobile-ViG. summarizes our contributions.",
  ". Related Work": "We break up previous works in the mobile vision space intothree categories: CNN-based, CNN-ViT-based, and CNN-GNN-based. The most well known CNN-based methodsare MobileNet and EfficientNet .MobileNet introduced depthwise separable convolutions,which separate the convolution operation into a depthwiseconvolution followed by a pointwise convolution. This ap-proach achieves performance similar to a normal convolu-tion op with significantly lower computational cost. Mo- bileNetv2 built on depthwise separable convolutionswith the new inverted residual block.Inverted residualsmake residual links less memory-intensive on mobile de-vices by adding skip links around points where the channellayer is expanded. Hence, these large layers do not haveto be saved in memory for future additions. Lastly, Mo-bileNetv3 uses neural architecture search and squeezeand excitation blocks to further improve on MobileNetv2.Like MobileNetv3, EfficientNet and EfficientNetv2 use neural architecture search to produce fast, highlyaccurate models.There are many CNN-ViT-based models, but two recentworks have achieved remarkable performance with very lowmobile latency: EfficientFormerV2 and FastViT .EfficientFormerV2 combines inverted residual blocks witha modified transformer operation and SuperNet architecturesearch. The modified transformer block uses talking headsand a depthwise convolution on the value matrix to injectlocal information. FastViT uses a new RepMixer blockalong with transformers to achieve similar results to that ofEfficientFormerV2. The RepMixer block combines a depth-wise convolution and convolution-based feed-forward net-work. Additionally, FastViT makes extensive use of repara-materization.To our knowledge, there is currently only one CNN-GNN-based mobile architecture: MobileViG . Mobile-ViG introduced Sparse Vision Graph Attention (SVGA), avision graph neural network module for statically construct-ing graphs and performing message passing.For smallmodel sizes, MobileViG appears to compete with state-of-the-art CNN-ViT-based models, but as the model sizegrows, MobileViG accuracy fails to scale as well as thatof CNN-ViT-based counterparts. To address this limitation,we introduce a new CNN-GNN-based mobile architecture,MobileViGv2, which fixes this scaling problem.",
  ". Methodology": "In this section, we describe the design of Mobile GraphConvolution (MGC) and MobileViGv2.MobileViGv2leverages the speed of MGC to use graph convolutions inhigher resolution stages of the model architecture, result-ing in significantly higher model accuracy on ImageNet-1Kwithout a significant drop in latency. . Mobile Graph Convolution (MGC) (left) versus SparseVision Graph Attention (SVGA) (right). Each grid is broken upsuch that the effective receptive field is equal to that of Mobile-ViGv2 at Stage 4. (left) The connections made for the green tokenusing MGC (L = 2) are shown in blue. (right) The connectionsmade for the green token using SVGA (K = 2), as used in Mo-bileViG, are shown in blue. The image above was obtained fromthe ImageNet-1K dataset and has been modified for this paper.",
  ". Mobile Graph Convolution": "We propose Mobile Graph Convolution (MGC) as a faster,highly scalable alternative to Sparse Vision Graph Atten-tion (SVGA) . MGC improves on SVGA by increas-ing graph sparsity and introducing conditional positionalencodings to the graph operation.The MGC algorithm alters equations 1 and 2, leaving 3 unchanged. Equation 1 is altered by adding a conditionalpositional encoding (CPE), and in equation 2, the car-dinality of G(xi)xi is reduced by using a different graphconstruction method. The updated equation for MGC is:",
  "where CPE is a depthwise convolution and stands for con-ditional positional encoding": "Unlike the original ViG , MobileViG does notuse positional encodings. As shown in ViT , without po-sitional encodings, the model accuracy of ViTs drops sig-nificantly since the self-attention operation becomes per-mutation invariant.We find this performance drop alsoholds for graph operations in MobileViG (see ). As such, MGC uses conditional positional encodings(CPE) to encode spatial information before messagepassing. MGC uses reparameterizable CPE as introducedin FastViT . Before constructing the graph and messagepassing, a positional encoding is added to the feature mapby taking a depthwise convolution of the feature map it-self. During inference, the residual link can be merged withthe depthwise convolution, saving a fraction of a millisec-ond. This simple change introduces spatial information inthe message-passing step yet adds few parameters and sub-stantially increases performance. For example, when CPE is added to SVGA in MobileViG-B, the top-1 accuracyon ImageNet-1K improves by 0.3%, as seen in . The differences in graph construction are straightfor-ward. The SVGA algorithm uses a hyperparameter, K, todetermine the distance and density of connections for eachtoken. For token xi, every Kth token to the right in itsrow and down in its column is connected to it. Thus, thenumber of connections to xi grows according to O( N+M K),where N and M are the dimensions of the input image.Thus, for a fixed value of K, the cardinality of G(xi) growslinearly with respect to the input resolution. As such, us-ing SVGA for higher resolutions requires more computa-tion during graph construction, making it difficult to scaleto higher input resolutions while maintaining competitiveinference latency. One could fix this by scaling K with in-put resolution, but a simple, more straightforward approachis to fix the number of possible connections per token re-gardless of input size. Following this approach, each input token has only fiveconnections in MGC: one self-connection, two long-rangelinks to the left and right of the token on its row, and twolong-range links up and down from the token on its col-umn. This can be seen in greater detail in c and for an input resolution of 7 7. For larger inputresolutions, the distance of the long-range links can be in-creased to gather information from regions further away foreach token. As implemented in MobileViG, SVGA uses 7connections per token (K = 2), while MGC uses only 5(L = 2), contributing to the speedup over SVGA. We alsofound that lowering the number of connections improvesmodel performance, which can likely be attributed to re-ducing over-smoothing. For example, when swapping inSVGA for MGC in MobileViGv2-B, the resulting model is0.2 milliseconds slower with 0.1% worse top-1 classifica-tion accuracy on ImageNet-1K. . Results of MobileViGv2 and other mobile architectures on ImageNet-1K classification task roughly grouped by NPU latency ofan iPhone13 Mini using the ModelBench application. Type indicates whether the model is CNN-based, CNN-ViT-based, or CNN-GNN-based. Params lists the number of model parameters in millions. GMACs lists the number of MACs in billions. Gray highlightsindicate the contributions of this paper. The Top-1 accuracy results for MobileViGv2 models are averaged over three experiments, andthere is about a 0.1% fluctuation between training seeds. Missing entries could not be profiled on the iPhone 13 Mini (iOS 16). meansthe lower, the better. means the higher, the better.",
  "The entry point into the architecture is the stem. The": "stem takes the input image and downsamples it 4 usingconvolutions with stride equal to two. The output of thestem is fed to Stage 1, which consists of N1 inverted residu-als as described in a. Between each stage is anotherconvolution-based downsampling step. Stages 2, 3, and 4each start with a sequence of Ni inverted residuals, wherei is the stage number. The output of the inverted residualsequence is then fed through Mi MGCs, as shown in Fig- . Results of MobileViGv2 and other mobile architectures on COCO object detection, COCO instance segmentation tasks, andADE20K semantic segmentation. Parameters lists the number of backbone parameters in millions, not including Mask-RCNN or SemanticFPN. AP box and AP mask scores are for object detection and instance segmentation on MS COCO 2017 . mIoU scores are forsemantic segmentation on ADE20K . Shaded regions show the contributions of this paper. A (-) denotes a model that did not reportthese results.",
  "MobileViGv2-B27.743.064.947.139.662.242.744.3": "ure 2b. After Stage 4, an average pooling step followed bya feed-forward network produces the predicted class of theinput image.To achieve different model sizes, the channel width ofeach stage and values of Ni and Mi are changed. There arefour different MobileViGv2 configurations, MobileViGv2-Ti, MobileViGv2-S, MobileViGv2-M, and MobileViGv2-B. Note that all inverted residual blocks use an expansionfactor of 4. Additionally, all FFNs used in MGC, as shownin b, use an expansion factor of 4. While a differ-ent mixture of widths and expansion factors may producebetter results, as could be found using a neural-architecturesearch, our work aims to show the potential of using graphconvolutions in mobile vision architectures, not finding theoptimal model structure. We leave this task of using NASfor future work.",
  ". Image Classification": "MobileViGv2 is implemented using PyTorch 1.12.1 and the Timm library . Each model is trained using 16NVIDIA A100 GPUs with an effective batch size of 2048.The models are trained from scratch for 300 epochs on theImageNet-1K dataset with a standard training and inferenceresolution of 224224. We use the AdamW optimizerand a learning rate of 2e-3 with a cosine annealing sched-ule. Like many CNN-ViT-based mobile architec-tures, we use RegNetY-16GF for knowledge distilla- tion. Our data augmentation pipeline includes RandAug-ment , Mixup , Cutmix , random erasing ,and repeated augment . To measure inference latency,all models are packaged as MLModels using CoreML andprofiled on the same iPhone 13 Mini (iOS 16) using Model-Bench . We use the following ModelBench settings toprofile each model: 50 inference rounds, 50 inferences perround, and a low/high trim of 10. shows ImageNet-1K classification results for MobileViGv2 and similar mo-bile vision architectures. Models are roughly grouped bylatency.For models with an inference latency under 1 ms,MobileViGv2-Ti has the highest accuracy, with the nextclosest model, FastViT-T8 , being a full 1% behind.Additionally, MobileViGv2-Ti is 2% more accurate thanMobileViG-Ti for the same inference latency.Whencompared to EfficientFormerV2-S1 , MobileViGv2-S has 0.8% higher accuracy for a similar inference la-tency.MobileViGv2-S is also 1.7% more accurate thanMobileViG-S for the same inference latency.The wide performance gap shown in indi-cates that MGC and the new model configuration of Mo-bileViGv2 successfully solve the scaling problem experi-enced by MobileViG. This, along with better or compara-ble performance to CNN-ViT-based models such as Effi-cientFormerV2 and FastViT , show the potentialof CNN-GNN-based architectures to compete in the mobilevision space.",
  ". Object Detection and Instance Segmentation": "We show that MobileViGv2 generalizes well to downstreamtasks by using it as a backbone for object detection and in-stance segmentation on the MS COCO 2017 dataset,which contains training and validation sets of 118K and . An ablation study of the effects of conditional positional encodings, higher resolution graphers, and different graph constructionmethods on MobileViG-B and MobileViGv2-B. A checkmark indicates this component was used in the experiment. A (-) indicates thiscomponent was not used. 1-Stage indicates that graph convolutions were only used in Stage 4 of the model, while 3-Stage indicates thatgraph convolutions were used in stages 2, 3, and 4. The * indicates that this model swapped the graph convolution for a fully connectedlayer.",
  "MobileViGv2-B27.72.7--83.4": "5K images. We use pre-trained MobileViGv2 backboneswith Mask-RCNN for training. Each model is trainedon 16 NVIDIA A100 GPUs for 12 epochs with an effectivebatch size of 16. We use AdamW optimizer, an ini-tial learning rate of 2e-4, and a standard image resolution of1333800.As shown in , MobileViGv2-M hits an AP box and AP mask of 42.5 and 38.8, respectively. This is 1.2and 0.7 points higher than MobileViG-M and 3.6 and2.9 points higher than FastViT-SA12 . MobileViGv2-Bachieves an AP box and AP mask of 43.0 and 39.6, respec-tively. This is 1.0 and 0.7 points higher than MobileViG-Band 1.0 and 1.6 points higher than the comparable FastViTbackbone, FastViT-SA24.These results show that MobileViGv2 generalizes wellto downstream tasks. Compared to competitive CNN-ViT-based models like FastViT , MobileViGv2 performssignificantly better on these downstream tasks, even thoughthe performance in image classification is comparable.",
  ". Semantic Segmentation": "We also show that MobileViGv2 generalizes well to seman-tic segmentation on the ADE20K dataset , which con-tains 20K training images and 2K validation images with150 semantic categories. For training, we use 8 NVIDIARTX 6000 Ada generation GPUs, the AdamW optimizer,and a learning rate of 2e-4 with polynomial decay. We useMobileViGv2 as a backbone with Semantic FPN as thesegmentation decoder. The backbone is initialized with pre-trained weights on ImageNet-1K, and the model is trainedfor 40K iterations.As shown in , MobileViGv2-M outperformsFastViT-SA12 by 4.9% mIoUand outperformsEfficientFormer-L1 by 4% mIoU.Addition-ally, MobileViGv2-B outperforms FastViT-SA24 by 3.3%mIoUand outperforms EfficientFormer-L3 by 0.8% mIoU. Again, when compared to competitive CNN-ViT-based models like FastViT and EfficientFormer ,MobileViGv2 performs significantly better on this down-stream task even though the performance in image classifi-cation is comparable.",
  ". Ablation Studies": "We perform ablation studies to show the benefits of MGCover SVGA and to demonstrate that graph convolutions pro-vide benefits over a simple feed-forward network solution.A summary of these results can be found in .Starting with MobileViG-B as a base model, we try usingSVGA-style graph convolutions in stages 2, 3, and 4 of themodel while keeping the number of parameters the same.We adjust the number of blocks in each stage and the chan-nel depth to keep the number of parameters similar. Theresulting model achieves a top-1 accuracy on ImageNet-1Kof 83.2%, 0.6% higher than MobileViG-B. However, thismodel has an inference latency of 2.9 milliseconds, signif-icantly slower than the 2.3 milliseconds of MobileViG-Bwithout catching up to the top-1 performance of FastViT and EfficientFormerV2 .We also try adding CPE to SVGA in MobileViG-B andfind that it improves model performance by 0.3%. We thencombine both CPE and 3-stage SVGA, which results ina top-1 accuracy of 83.3%.Even though SVGA makesmore connections than MGC, it performs slightly worsethan MGC when used with the same model settings. We ex-pect that this occurs due to over-smoothing from the higherconnection count.To get the benefits of using more stages without a signif-icant hit to latency, we use MGC, which uses sparser graphsand CPE, in MobileViGv2-B, to achieve the best perfor-mance of 83.4%. This final configuration has the same top-1performance and mobile latency as FastViT-SA24 .To verify that graph convolutions boost model perfor- mance, we swap each graph convolution with a fully con-nected layer of the same expansion size and use this instages 2, 3, and 4 of the model. This experiment is markedas MobileViG-B* in .We find that this modelachieves an accuracy of only 83.0%, which is 0.2% less thanusing SVGA in three stages and 0.4% less than using MGCin three stages. This shows that graph convolutions are im-proving model performance.",
  ". Conclusion": "In this work, we have proposed Mobile Graph Convolution(MGC) and MobileViGv2, a model architecture that usesMGC and competes with state-of-the-art CNN-ViT-basedmobile architectures. MGC uses a sparser, static graph con-struction method than SVGA, resulting in faster inferencespeeds.Additionally, MGC introduces conditional posi-tional encodings to the graph operation, considerably boost-ing model accuracy with only a slight increase in the num-ber of parameters. With these changes, MGC can be used inmuch higher resolution stages than SVGA without signifi-cantly impacting latency.MobileViGv2 takes advantage of this by using MGC inthe last three processing stages. Earlier global processingand the sharing of spatial information during message pass-ing through CPE solves the scaling problem experienced byMobileViG, thus making MobileViGv2 a genuine competi-tor to state-of-the-art CNN-ViT-based mobile architecturesand, consequently, MGC a competitor to self-attention inthe mobile vision model space. Yinpeng Chen, Xiyang Dai, Dongdong Chen, MengchenLiu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu.Mobile-former: Bridging mobilenet and transformer. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 52705279, 2022. 1",
  "Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-aolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-sitional encodings for vision transformers.arXiv preprintarXiv:2102.10882, 2021. 2, 4": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc VLe.Randaugment:Practical automated data augmen-tation with a reduced search space.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition workshops, pages 702703, 2020. 6 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE Conference on Computer Vision andPattern Recognition, pages 248255, 2009. 3, 4, 6",
  "Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-shick. Mask r-cnn. In Proceedings of the IEEE internationalconference on computer vision, pages 29612969, 2017. 7": "Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, TorstenHoefler, and Daniel Soudry. Augment your batch: Improvinggeneralization through instance repetition. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 81298138, 2020. 6 Andrew Howard, Mark Sandler, Grace Chu, Liang-ChiehChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and HartwigAdam. Searching for mobilenetv3. CoRR, abs/1905.02244,2019. 3 Andrew G Howard, Menglong Zhu, Bo Chen, DmitryKalenichenko, Weijun Wang, Tobias Weyand, Marco An-dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-tional neural networks for mobile vision applications. arXivpreprint arXiv:1704.04861, 2017. 1, 3 Alexander Kirillov, Ross Girshick, Kaiming He, and PiotrDollar. Panoptic feature pyramid networks. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 63996408, 2019. 7",
  "Guohao Li, Matthias Muller, Ali Thabet, and BernardGhanem.Deepgcns: Can gcns go as deep as cnns?InProceedings of the IEEE/CVF international conference oncomputer vision, pages 92679276, 2019. 3": "Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, KamyarSalahi, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Re-thinking vision transformers for mobilenet size and speed.arXiv preprint arXiv:2212.08059, 2022. 1, 2, 3, 5, 6, 7 Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evan-gelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Effi-cientformer: Vision transformers at mobilenet speed. arXivpreprint arXiv:2206.01191, 2022. 1, 5, 6, 7 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 3, 6",
  "Adam Paszke et al.Pytorch: An imperative style, high-performance deep learning library. Advances in neural in-formation processing systems, 32, 2019. 6": "Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,Kaiming He, and Piotr Dollar. Designing network designspaces. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1042810436,2020. 6 Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-moginov, and Liang-Chieh Chen.Mobilenetv2: Invertedresiduals and linear bottlenecks.In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 45104520, 2018. 1, 3, 5"
}