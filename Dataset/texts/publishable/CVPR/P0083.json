{
  "KTH, Sweden2 SLU, Sweden3 IMATI-CNR, Italy": ". We estimate the articulated 3D motion of a horse from video, combining both visual and auditory information. We show that bytraining with both of these modalities, we are able to reconstruct poses that are more accurate and natural, even under self-occlusion. Infigure, we show results for an Image-only network (red) and for two networks that exploit audio: Early-fusion (green) and Model-fusion(blue). Note that both audio-based networks can reconstruct more natural head pose and correctly estimate the front left hoof touching theground.",
  "Abstract": "In the monocular setting, predicting 3D pose and shapeof animals typically relies solely on visual information,which is highly under-constrained. In this work, we exploreusing audio to enhance 3D shape and motion recovery ofhorses from monocular video. We test our approach on twodatasets: an indoor treadmill dataset for 3D evaluation andan outdoor dataset capturing diverse horse movements, thelatter being a contribution to this study. Our results showthat incorporating sound with visual data leads to more ac-curate and robust motion regression. This study is the firstto investigate audios role in 3D animal motion recovery. 1. IntroductionAdvancements in computer vision and machine learninghave greatly propelled 3D markerless motion capture ofhumans and animals. Notably, using parametric models,like the SMPL model for humans and the SMALmodel for quadrupeds, has pushed this research areaforward. These methods infer subject motions solely frommonocular images or videos . However, the integration of multimodal data inthis context remains underexplored.Human perception combines senses like vision, crucialfor understanding object movement, and hearing, whichcomplements vision and enhances our comprehension of the environment. Prior research highlights the synergy be-tween sound and visual data in motion estimation and an-imation . This research exploits the corre-lation of audio and visual data for capturing articulated 3Dmotion from monocular videos, specifically for horses. Horses play a significant role in various human activ-ities.The need for advanced markerless motion capturetechniques to analyze equine behavior and health is grow-ing . Their unique sounds through their hooves and res-piratory actions, provide rich audio cues for motion analy-sis. We are the first to combine visual and audio data for 3Danimal motion reconstruction. Our study necessitates a dataset with both audio andvideo data. With only the Treadmill Dataset available,we introduce the Outdoor Dataset, comprising four horseson an outdoor gravel surface, recorded with a 4K cameraand synchronized audio. This dataset broadens this field byoffering diverse motion and audio for 3D motion modeling. We develop two fusion strategies for accurate shape andpose estimation via the hSMAL model , a horse-variantof SMAL: 1) early fusion that employs audio data both intraining and testing and 2) model fusion that only leveragesaudio information during the training phase. Experimentswith different setups show how audio information facilitates3D horse reconstruction learning. We demonstrate that thenetworks learning with audio data using our fusion strate-",
  ". Related Work": "Model-Based3DPoseEstimationFromImagesMonocularmarkerlessmotioncaptureofarticulatedsubjects like humans and animals relies on prior modelsof body shape and pose.This literature review focuseson the use of SMPL model , relevant to our horsemodel approach, pivotal for estimating human pose fromvisuals , addressing inter-actions with environment or objects , multiplebodies in scenes , and camera distortions .Model-based methods have been applied to specific an-imal species, including birds quadrupeds ,zebras , dogs and horses . Noneof these methods incorporate multimodal data. Pose Estimation and Synthesis From AudioAudio-driven research shows a strong link between sound and mo-tion. Studies have mapped speech to facial movement in3D or animated faces with realistic expressions fromspeech in 2D portraits . Other works predict upper bodymovements from instrument music , convert speechinto gestures , synchronize 3D body gestures and facialexpressions with speech , generate dance movementsfrom music . All these methods demonstrate au-dios role for complex animations. However, animal motionanalysis with audio, like horse behavior detection andprimate action recognition , is less explored, presentinga research opportunity. Multimodality FusionRecent studies explore combiningaudio and visual data for integrated representations in multi-ple applications, like speech separation , egocentric ac-tion recognition , scene understanding , which needboth modalities present in the inference stage. Further re-search addresses missing modalities at inference, with a cy-cle translation training , an alignment of independentlatent spaces for multimodality data , an optimization ofjoint representation that separates shared and specific modalfactors , a utilization of Bayesian networks and meta-learning . Variational AutoEncoders (VAEs) are keytechniques with different variants in mul-timodal learning.The use of multimodal data in 3D pose estimation re-mains unexplored. Yang et al.s study stands out as a rareexample, focusing on human 3D pose estimation with met-ric scale . Instead, we focus on modeling the 3D meshof the animals. Drawing inspiration from research on emo-tion detection through multimodal data integration ,we follow a similar path that combines video and audio forestimating the 3D motions of animals. . Videoaudio fusion frameworks. (a) Early-fusion, (b)Model-fusion. Both networks use the same architecture for featureextraction and predicting C1:T , , Global1:Tusing video features.The key difference lies in estimating Joints1:T,: (a) combines videoand audio features before to estimate Joints1:Tand (b) processesthrough the shared separately to obtain IJoints1:Tand AJoints1:T.All parameters are then mapped to 3D meshes v1:T for 2D pro-jection and loss calculation. Inputs are in orange and learnablemodules in green.",
  ". Method": "In this section, we first introduce the hSMAL model utilized in this work. Then, we propose a backbone modulefor video data processing, followed by two fusion strategiesto integrate auxiliary audio data into the learning and infer-ence process. Finally, we present how we construct the lossfunction for learning. The hSMAL ModelWe reconstruct 3D horses frommonocular video and audio inputs by estimating the hS-MAL model parameters , a horse-adapted version ofSMAL that defines the shape and pose of horses. Themodel uses a mapping : (, ) v, where are PCA coef-ficients for the models shape space, = (Global, Joints) in-dicates the global orientation and joint rotations and v rep-resents the 3D mesh vertices. Fusion StrategiesOur two fusion strategies (early fusionand model fusion) are both based on our proposed back-bone module denoted as Image-only network. Image-onlynetwork adapts the CLIFF architecture for video se-quences, estimating 3D pose and shape by considering thesubjects location in the full image frame. This approachis particularly effective for our outdoor data, where horsesmove at varying distances from the camera.",
  "with the center (cxt,cyt) and size bt of the original bounding box and the fo-cal length ffull for the original camera full, calculatedas ffull =": "w2 + h2 with the original images width w andheight h, assuming a 50 diagonal Field-of-View .To predict parameters, we employ an iterative error feed-back (IEF) loop, similar to , called the Regression Block. Here, predicts shape and camera parameters, while focuses on pose estimation.Block processes visual features to estimate modelshape and camera crop1:T= (fcrop, crop1:T ), with weak per-spective projection parameters C1:T including scale s1:Tand translation (px1:T , py1:T ). The full camera translationcrop1:T=croptTt=1 for the cropped image is calculated as",
  "rst": "with fcrop = 5000 is the predefinedfocal length of the camera crop and r = 224 is the bound-ing box size. We convert the cropped camera crop1:Tto theoriginal camera full1:T= (ffull, full1:T ), for reprojecting 3Dpoints to the full image, with the translation calculated asfullt=pxt +2cxtbtst , pyt +2cytbtst ,2ffull",
  "btst": ".For pose estimation, the global rotation Global1:Tis es-timated directly from visual features via a fully-connectedlayer, eliminating manual global rotation initialization. Thepose parameters Joints1:Tare estimated by Block using vi-sual features as input.Regarding the fusion strategies, the main differences arethe given information as the inputs to for 3D pose estima-tion. Audio features are derived by converting audio A1:Tinto a log-mel spectrogram with Librosa , then using aResNet backbone for feature extraction.In early fusion strategy (.a), the encoded visualand audio features are concatenated and processed throughtwo FC layers before entering to predict the pose pa-rameters Joints1:T, forming the Early-fusion network. Thismethod requires video and audio data during inference.In model fusion strategy (.b), visual and audio fea-tures are fed separately into , producing two sets of poseparameters: IJoints1:Tfrom visuals and AJoints1:Tfrom audio,defining the Model-fusion network. Visual data is the pri-mary modality, with audio as an auxiliary to enhance poseestimation accuracy and we only evaluate the poses from vi-sual data. During inference, the model can operate with justthe primary visual input, allowing for the absence of audio.The predicted parameters (, Global1:T, Joints1:T) generate thehSMAL model v1:T. Then, the 3D vertices are projectedto the original image frame to derive 2D keypoints Kfull1:T ,using the original camera full1:T . Differing from CLIFF, weutilize Pytorch3D to render silhouettes Scrop1:T in thebounding box frame, employing the 3D model mesh andthe cropped image camera crop1:T , avoiding original framerendering for computational efficiency. Training LossesAll regression networks are trained end-to-end with the loss defined as L=LKP + LSIL +LSMOOT H + LHSMAL. LKP and LSIL represent the 2Dphotometric loss for keypoints and silhouettes, penalizingthe difference between the predicted and groundtruth val-ues. LSMOOT H enhance the temporal smoothness of thepredicted parameters across frames, while LHSMAL ap-plies the shape and pose prior of the hSMAL model .Check Supplementary Material for more details.",
  ". Experiments": "DatasetsThe Treadmill Dataset, acquired from the Uni-versity of Zurich , includes video, audio, and 3D motioncapture recordings of seven horses trotting on a treadmill.2D groundtruth keypoints are created with the mocap data,supplement with DeepLabCut , and groundtruth sil-houettes are from OSVOS . We further introduce theOutdoor Dataset for evaluating our network in natural set-tings. Captured with a GoPro10 at 4K and synchronizedaudio, it records four horses (white, black, brown, red) per-forming walks, trots, and canters in both directions underhuman guidance. Groundtruth keypoints and silhouettes areobtained with ViTPose+ and Detectron2 respec-tively. More details are in Supplementary Material.Experiment SetupWe conduct two experiments on bothdatasets. The first evaluates the impact of appearance varia-tions by splitting test subjects into those with colors similarto training data (Test Data 1) and those with significantlydifferent colors (Test Data 2), challenging the network without-of-distribution data. This evaluates the audio inputs thatcan complement visual information when the latter is lessreliable or informative. The second experiment assesses therobustness of our audio-enhanced models to visual interrup-tions, using synthetically occluded video frames. Examplesof the synthetic occlusions are in Supplementary Material.Experiments on Treadmill DatasetWe demonstrate re-sults on the Treadmill Dataset, where we have quantitative3D evaluation. The model is trained on three randomly se-lected dark-colored horses, using 75% of the recording fortraining and 25% for validation. Test Data 1 comprises theremaining three dark-colored horses and Test Data 2 con-tains the white horse. The results are reported as the meanper 3D joint position error after rigid alignment with Pro-crustes analysis (P-MPJPE) , in mm, given the accurate3D mocap data. We report the mean and standard devia-tion for all networks on the Treadmill Datasets in Tab. 1.The results using the optimization method are included,serving as an upper bound as the model uses the additionalground-truth mocap data.In Test Data 1, performances across all networks arecomparable. The Image-only network shows similar per-formance to the Early-fusion network and the Model-fusionnetwork.This suggests that for Test Data 1 there isenough information in the visual modality to correctly es-timate the horse motion, as the appearance of the trainingand test horses is similar. We perform a non-parametricWilcoxon significant test to compare the P-MPJPE errors ofthe Image-only network against both the Early-fusion net-work and the Model-fusion networks. We obtain p-valuesof 5.2e-19 and 1.2e-46, respectively. This shows that thedifferences in errors between the methods are statisticallysignificant. Test Data 2, the network faces a more challeng-ing task, as the color of the test horse has not been seen . Example results of different networks in the Treadmill Dataset with Test Data 1 (a) and Test Data 2 (b). (i) The model is shownin different views. (ii) Model overlapped with the original images. Refer to the main text for more details. during training. The performance of the Image-only net-work drops in Test Data 2, highlighting the difficulty posedby the large appearance difference between training and testdata. Both the Early- and Model-fusion networks performbetter than the Image-only, with Model-fusion performingthe best, which shows that audio is an effective source toenhance the robustness of appearance variation even thoughthe visual information varies.The question remains whether a data augmentationstrategy can provide comparable robustness to appearancechanges. We introduce color jittering augmentation duringtraining, adding variations in contrast, brightness, satura-tion, and hue. The performance of the Early-fusion networkand Model-fusion network are slightly better than the per-formance achieved with data augmentation in Test Data 1and the Model-fusion network performs better in Test Data2. Data augmentation reduces the train/test domain gap, buttraining with multimodal data gives better results, indicat-ing that audio is an effective way to improve robustness toappearance variation.In the synthetic occluder experiments, the simulated oc-cluder covers a large area of body parts of the horse. Tab. 1shows that such extreme occlusions impact all networksperformance. However, the Model-fusion network outper-forms all others, which confirms its robustness, while theEarly-fusion network is more sensitive to noisy visual cues.",
  "Image-only8313135801274114636Early-fusion8214116621597617681Model-fusion8213112551263713843": "shows visual examples from different networks.(i) shows that the models tail bends to the rightgiven that the network is only supervised by 2D infor-mation, and the horses have a braided tail, which isnot represented by the hSMAL model.In Test Data 1(a), all networks that use visual features for poseestimation, like Image-only, Early-fusion, and Model-fusion network, perform similarly.In Test Data 2(b), the Image-only network often predicts rigid legs.",
  ".Samples of full body visi-ble for the Outdoor Dataset. Image-onlyNetwork (in LightRed), Early-fusionNetwork (in in LightGreen), Model-fusion Network (in LightBlue)": "The Early-fusionnetworkman-ages to estimateplausibleposesfor the first threeframes,whilethe Model-fusionnetworkconsis-tentlypredictscorrectposes.Thisindicatestherobustnessofnetworksthatincorporateaudioinaccu-rately estimatinghorsemotions,especiallyinsituations wherevisual features alone are insufficient or when self-occlusionoccurs, as the right legs are often not fully visible.Experiments on Outdoor DatasetWe demonstrate theresults on the Outdoor Dataset. The Outdoor Dataset posesgreater challenges due to varied lighting and horses freemovements. We consider two strategies: Per-horse basistraining, using 80% of videos for training and 20% for test-ing per horse; Inter-horse training, dividing horses into Test",
  ". Sample outputs on the Outdoor Dataset. Image-only Network (in LightRed), Early-fusion Network (in LightGreen), Model-fusion Network (in LightBlue)": "Data 1 and Test Data 2 based on appearance similarity.Sample outputs for per-horse training demonstrate thatall networks perform similarly when horses are fully visible(), but differ under occlusion. Self-occlusion cases(a) show the Early- and Model-fusion networks out-performing the Image-only Network, especially in front legand head pose estimation, indicating that the training withaudio integration enhances the pose estimation. For human-induced occlusions (e.g., head occlusion b and leg oc-clusion 5c), the Image-only network struggles with the pre-diction of head poses (b) and left hind leg poses (c)), in contrast to the fusion networks which leverage au-dio cues to improve natural neck and hind leg pose estima-tions, respectively. The enhanced head and leg poses pre-dicted by the fusion networks can be due to the specificmovement pattern of horses. The networks learn the cor-relation between sound and leg movements, and since legmotion directly influences head position, the integration ofaudio allows for more natural predictions of both head andleg poses. This results in more accurate pose estimations,even in the presence of occlusions. More examples are inSupplementary Material.Under the Per-horse basis training approach, a 2D evalu-",
  "Image-only0.70 / 0.590.95 / 0.760.57 / 0.510.78 / 0.59Early-fusion0.69 / 0.580.95 / 0.760.57 / 0.480.78 / 0.56Model-fusion 0.70 / 0.600.95 / 0.770.61 / 0.530.81 / 0.64": "ation using PCK and IOU metrics is less informative, due tothe predominance of strong image cues. Furthermore, in thecase of Inter-horse training, we present a 2D analysis em-ploying IOU on full frame and PCK based on the pseudo-ground truth, covering both the original dataset and datawith synthetic occluder, as detailed in Tab. 2. The resultson the original dataset show similar performance across net-works for Test Data 1. The Model-fusion network excels inTest Data 2, outperforming the Early-fusion network. Theresults of introduced color jittering during training showthat data augmentation reduces the domain gap, notably en-hancing Model-fusions performance on Test Data 2. Thesefindings highlight the Model-fusion network better handlesthe noisy visual cues from corrupted test data.",
  ". Conclusions": "In this study, we investigate using both audio and monocu-lar video for 3D horse reconstruction. We adopt the hSMALmodel to represent the 3D articulated horse, and introducetwo strategies for audio-video fusion: Early fusion, whereaudio and video features are concatenated in the first stagesof the network, and Model fusion which leverages audio in-formation only during the training phase. Our fusion mod-els achieve more accurate reconstructions and natural poses,even with appearance shifts or visual ambiguities, which in-dicates the advantage of combining audio and video data forenhanced 3D pose estimation. Current limitations and future work.We capture audioprimarily of ground contact with a fixed camera. Futureefforts will include attaching a microphone to the horse tocapture breathing and other body sounds that are indepen-dent of the type of ground. We also aim to apply this methodto other species like dogs, which also produce breathingsounds while moving.",
  ". Dataset": "In of the main paper, we describe two datasets forour experiments. We here provide more detailed informa-tion about two datasets.Treadmill DatasetThe Horse Treadmill Dataset, ac-quired from the University of Zurich , includes record-ings of ten horse subjects trotting on a treadmill. Due tocamera calibration problems, we exclude three subjects, fo-cusing on the remaining seven: one white and six dark-colored (brown or black) horses, yielding a total of 702.24seconds of video at 25 fps. This dataset is unique in offeringsynchronized video, audio, and 3D motion capture data.We map 3D motion capture to images for 34 ground-truth keypoints, add four tail keypoints with DeepLab-Cut , and generate segmentations via OSVOS .Audio is denoised using Aukit . With horses centered inframes, we assume the bounding box is the full image, andresize each frame to 224 224.Outdoor DatasetTo complement the controlled condi-tions of the Treadmill Dataset, we create the OutdoorDataset to assess our networks performance in a more nat-ural setting. Captured with a GoPro10 camera at 4K res-olution and synchronized audio, this dataset includes fourhorses of diverse colors (white, black, brown, red) andsizes.They perform walk, trot, and canter motions inboth clockwise and anti-clockwise directions, under humanguidance via a line attached to their head collars, amountingto 1604.54 seconds of video recordings at 30 fps.Here we use Detectron2 to obtain horse silhouettesand their bounding boxes GSIL. ViTPose+ provides17 2D pseudo-ground-truth key points, with a confidencethreshold of 0.5, from which we derive a keypoint-basedbounding box GKP . A few frames where the detector failsare manually labeled. The final bounding box G combinesboth keypoint and silhouette data for enhanced accuracy.",
  "(1)": "where MSIL and MKP are the pixel area of GSIL andGKP , respectively. is a preset threshold, that we set at2.78. Then, the bounding box images are resized to 224 224 pixels. The silhouette loss is ignored for frames wherethe final bounding box is not calculated with GSIL.For each 2D keypoint Ki, we define a corresponding 3Dpoint i. In the case of the Treadmill Dataset, where 2Dkeypoints K are projected from the mocap data, we manu-ally select a point on the horse model surface and express it with barycentric coordinates of the neighboring vertices.When the 2D keypoints K are obtained from DeepLabCutor ViTPose+, we define 3D keypoints as the interpolationof a set of model vertices, such that the keypoints can alsorepresent skeleton joints.",
  "(3)": "where t are the body keypoints on the model, projectedon the full image as Kfulltwith perspective projection .Kfulltare the ground-truth 2D keypoints with confidencescores , and is the Geman-McClure robustifier .To constrain the model shape, we use segmentation forsupervision. A silhouette loss LSIL is defined as the smoothL1 loss between the projected model silhouette Scrop1:Tandthe ground-truth silhouette Scrop1:T :",
  "t=3t 2t1 + t22 ,(5)": "where N is the length of the input data. representsdifferent predictions, namely the predicted pose parame-ters and the global rotation in rotation matrix represen-tation, (R(Joints1:T) and R(Global1:T)), or the full translationof the original cameras full1:T , with corresponding weightsJointsSMOOT H, GlobalSMOOT H, fullSMOOT H.The prior loss LHSMAL is the weighted sum of the shapeand the pose priors of the hSMAL model, defined in ,with corresponding weights P rior and P rior. Training DetailWe use a ResNet-50 backbone networkto extract visual and audio features. The Temporal Encoder,adopted from , consists of a residual block with twogroup norm layers with 32 groups and two 1D convolu-tional layers, with a filter size of 2080. For the input to thetemporal encoder, we concatenate the image features fromResNet with bounding box information per frame, wherethe bounding box has been padded to the length of 32. Fol-lowing the residual block, the data is processed through afully-connected layer to get the final visual input featureswith a dimension of 2048. Our analysis operates on videosegments spanning T = 5 frames.We train the networks with a learning rate of 5 105",
  "JointsSMOOT H=10": "We assume that both modalities in the Model-fusion Net-work contribute equally to the pose estimation, setting theequal weight to the pose estimated from audio and videochannels.For inference, we choose the model with the lowest losson the validation set. We set a sliding window to selectoverlapping clips for each test video and consider the resultfrom the middle frame in each clip.For network parameters, the Image-only, Early-fusion,Model-fusionnetworks have 98 million, 134 million, and121 million training parameters, respectively. In the Tread-mill dataset, these networks are trained using three 2080TIswith a batch size of 9, and take 13 hours,17 hours, 24 hours,respectively. For testing on a single 2080TI with a batchsize of 1, they take 27 minutes, 27 minutes, 54 minutes,respectively.",
  ". Synthetic Occlusions": "In of the main paper, we describe one of the ex-periments with adding artificial visual noise to the originaldata. The purpose of this is to demonstrate the robustnessof our models in the presence of visual noise by adding asynthetic occluder to the images.In the Treadmill Dataset, the synthetic occluder is part ofthe image from the training dataset and covers most areas ofthe horse. In the Outdoor Dataset, the synthetic occluder isa human. Some examples of the synthetic occlusions areshown in and .",
  ". More Qualitative Results on the OutdoorDataset": "In addition to the quantitative experiments in Per-horse ba-sis training in of the main paper, we here providemore qualitative results.When the horse is fully visible, the three networks pro-duce very similar pose estimates (in ). Two examplesare shown in the case where the horse is occluded by a hu-man in . The Image-only network produces rigid frontlegs (left) and unnatural head poses (right) that point to theright side, while the Early- and Model-fusion networks pre-dict more natural front legs and neck poses. This demon-strates that networks trained with both audio and visual in-formation have the potential to be more robust to occlusion.",
  "Aukitaudiotoolkit.https : / / github . com /KuangDD/aukit, 2021. 1": "Marc Badger, Yufu Wang, Adarsh Modh, Ammon Perkes,Nikos Kolotouros, Bernd G Pfrommer, Marc F Schmidt, andKostas Daniilidis. 3d bird reconstruction: a dataset, model,and shape recovery from a single view. In ECCV. Springer,2020. 2 Max Bain,Arsha Nagrani,Daniel Schofield,SophieBerdugo, Joana Bessa, Jake Owen, Kimberley J. Hockings,Tetsuro Matsuzawa, Misato Hayashi, Dora Biro, Susana Car-valho, and Andrew Zisserman. Automated audiovisual be-havior recognition in wild primates. Science Advances, 7(46):eabi4883, 2021. 2",
  "Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis,Matt McVicar, Eric Battenberg, and Oriol Nieto.librosa:Audio and music signal analysis in python. In SciPy, 2015.3": "Tanmay Nath*, Alexander Mathis*, An Chi Chen, AmirPatel, Matthias Bethge, and Mackenzie W Mathis. Usingdeeplabcut for 3d markerless pose estimation across speciesand behaviors. Nature Protocols, 2019. 3, 1 Leon Nunes, Yiannis Ampatzidis, Lucas Costa, and MarceloWallau.Horse foraging behavior detection using soundrecognition techniques and artificial intelligence. Comput-ers and Electronics in Agriculture, 183:106080, 2021. 2"
}