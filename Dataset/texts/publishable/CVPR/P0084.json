{
  "This paper describes our champion solution to the": "LOVEU Challenge @ CVPR24, Track 1 (Long VideoVQA). Processing long sequences of visual tokens is com-putationally expensive and memory-intensive, making longvideo question-answering a challenging task.The keyis to compress visual tokens effectively, reducing memoryfootprint and decoding latency, while preserving the es-sential information for accurate question-answering. Weadopt a hierarchical memory mechanism named STARMemory, proposed in Flash-VStream , that is capa-ble of processing long videos with limited GPU mem-ory (VRAM). We further utilize the video and audio dataof MovieChat-1K training set to fine-tune the pretrainedweight released by Flash-VStream, achieving 1st place inthe challenge.Code is available at project homepage",
  ". Introduction": "Recently, natural language is becoming a general inter-face for various tasks and modalities Most existing large video-language models face challengeswhen performing long video question-answering upon userqueries . The main reason is that: visual tokensbetween consecutive frames are heavy and redundant with-out effective compression, making it impossible to save allvisual features in limited GPU Memory (VRAM), as wellas significantly increasing the decoding latency of languagemodel. Some previous works solve this by feature com-pression or pruning , and achieve real-time per-formance . To better address this issue, we adopta hierarchical memory mechanism named STAR Memory,which is proposed recently in Flash-VStream . By fur-ther fine-tuning the pretrained weight released by Flash-",
  ". Method": "As shown in, the Flash-VStream frameworkthat we adopted consists of three main components: (1) astreaming visual encoder that continuously processes videoframes, (2) a Spatial-Temporal-Abstract-Retrieved mem-ory mechanism (STAR memory), including memory writ-ing and reading with the help of a feature buffer. (3) a LLMdecoder capable of providing responses to questions raisedby users.",
  ". Streaming visual encoder": "We use the pre-trained CLIP ViT-L as visual encoder.Only patch tokens are used during training and inference.Specifically, given a frame stream {V t}t=1, the encodermaps the t-th frame V t RHW 3 to feature map et RP P D, where P P is the number of ViT patch tokensand D is the hidden dimension of ViT.",
  ". Spatial-Temporal-Abstract-Retrieved memory": "In order to handle information of different levels of granu-larity, the STAR memory consists of 4 components: spatialmemory Mspa RNspaP 2spaD, temporal memory Mtem RNtemP 2temD, abstract memory Mabs RNabsP 2absD andretrieved memory Mret RNretP 2spaD. A feature bufferMbuff RNbuffP 2spaD is used to store the feature of latestNbuff frames. Therefore, the overall memory size is limitedto MAXSIZE = (Nspa+Nret)P 2spa+NtemP 2tem+NabsP 2abs tokens.Spatial memory. Spatial memory houses the most re-cent and detailed spatial information for short-term use, im-",
  "Abstract MemoryRetrieved Memory": ". The overview of Flash-VStream framework that we adopted for real-time online video stream understanding. Flash-VStream is executed by two processes, namely frame handle and question handler. The frame handler is responsible for encodingframes and writing to memory, which contains a visual encoder, a STAR memory and a feature buffer. The question handler is responsiblefor reading from memory and answering questions anytime, which contains a projector and a Large Language Model.",
  "Wsoftmax": ". STAR memory writing mechanism. (a) Update spatial memory by a FIFO queue. (b) Update temporal memory by WeightedK-means Clustering. (c) Update abstract memory by Semantic Attention. (d) Update retrieved memory by key frame feature retrival. Herefeature map eT has multiple sizes. S, T, A and R represent tokens of spatial, temporal, abstract and retrieved memory, respectively. plemented as a FIFO (First-In-First-Out) queue, as illus-trated in and Equation (2). This architecture en-ables continuous updating with the newest frames, facilitat-ing immediate access to fine-grained spatial data. Temporal memory. Temporal memory integrates dy-namic information over time, crucial for long-term reten-tion. When its size surpasses Ntem, the gwkmeans (WeightedK-means Clustering) algorithm is applied, as shown inEquation (3). This strategy condenses the memory contentinto Ntem clusters which can be seen as the representationof key events in videos. Then the centroids of these clustersare used as the new memory for efficiently storing temporalcontexts. Abstract memory.Abstract memory supports high-level semantic concept interpretation through fSA, the Se-mantic Attention model.It follows Equation (4) to syn-thesize the insights gained from both spatial and tempo-ral memories into abstracted, actionable knowledge. fSAkeeps adjusting Mabs, the synopsis of whole video bynewest features. Refer to for details. Retrieved memory. Retrieved memory focuses on re-calling precise spatial details by identifying and retrievingthe most substantial frame features. As shown in ,it first selects the top-K (where K equals Nret) largest clus-ters from the Ntem clusters obtained in temporal memoryMtem. Then the nearest frame features in feature buffer to centroids of these K clusters are retrieved to supplement thetemporal memory with more detailed spatial information.This process is illustrated in Equation (5).In brief, a new feature et is written to STAR memory asfollows:",
  ". Real-time LLM decoder": "The LLM decoder works as part of a real-time questionanswering server.When triggered by a question Qt attime t, the LLM decoder first calculates the text embed-ding Ittext = fembed(Qt) and maps the STAR memory M t =M tspa+M ttem+M tabs+M tret to embedding space with the pro-jector Itvision = fproj(M t). Then it starts to generate answerAt = fLLM(Ittext, Itvision).decode() in real time.",
  ". Implementation details": "In this study, we utilize pre-trained CLIP ViT-L/14-336px as streaming visual encoder.FollowingLLaVA , we choose a 2-layer-MLP as visual projectorand pre-trained Vicuna-7B as LLM decoder. We adoptthe open-source ASR model Whisper-large-v3 to pre-transcribe the audio stream into text.Considering the balance between performance and re-source consumption, we set Pspa = 8, Ptem = 4, Pabs = 1,Nbuff = 300, Nspa = 1, Ntem = Nabs = 25 and Nret = 3.The MAXSIZE of STAR memory is set to 681 tokens inorder to keep computational efficiency.We train Flash-VStream for 3 stages: modality align-ment, instruction tuning, and domain-specific fine-tuning.The training data of first 2 stages are keep samewith LLaMA-VID , including LLaVA-filtered-558K",
  "FVS + stage-3 & speech96.04.6059.62.99": "image-caption pairs and LLaMA-VID-filtered-232K video-caption pairs for stage 1, LLaVA-filtered-665K image QA pairs and Video-ChatGPT-filtered-98K video QA pairs for stage 2. The training data of stage 3is the training branch of MovieChat-1K . Speech datatranscribed from the ASR model are only used in the thirdstage. Speech data are feed into the LLM decoder as a partof the question text, thus providing additional informationfor video question answering.For each stage, the model is trained for 1 epoch on 8A100 80G GPUs. During training, the parameters of visualencoder are frozen and the parameters of LLM are frozenonly for the first stage. All training and inference exper-iments was conducted under BF16 precision to save timeand resources.During inference on the test branch of MovieChat-1K,we use different strategy for global and breakpoint ques-tions. For global questions, we use the STAR memory tostore the whole video information and answer the questions.For breakpoint questions, we only use a part of the video,which ranges from 15s before the breakpoint to 15s after thebreakpoint, to answer the questions.",
  ". Experiments": "To better showcase how different designs affect the perfor-mance of our method, we split out 20% of the MovieChat-1K training set as the validation set and evaluate our modelon it with GPT-3.5 as the evaluator. As shown in the upperpart of , it is necessary to fine-tune the pretrainedweight released by Flash-VStream on the training branch ofMovieChat-1K, as the performance of the model is signifi-cantly improved after stage-3 fine-tuning. The lower part of shows the performance of our model on the test setof MovieChat-1K with Gemini-Pro as the evaluator, eval-uated by the organizers. The performance of our model issignificantly improved after adding the speech data as anadditional input to the LLM decoder. It also surpass the",
  ". Conclusion": "In conclusion, we adopted Flash-VStream , a recentlyproposed long video-QA model. By incorporating a hier-archical memory, the model can effectively compress vi-sual tokens throughout the whole video. We further fine-tuned the pretrained weight released by Flash-VStreamon the training branch of MovieChat-1K. The transcribedspeech data is also leveraged as an additional input to theLLM decoder to further improve its performance on theMovieChat-1K dataset, achieving 1st place in the challenge.We hope our method could inspire further research and ad-vancements in the field of long video stream understand-ing. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. NeurIPS, pages35,2371623736, 2022. 1 Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.Xing.Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023. 3 Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu,Bo Dai, and Yansong Tang. Motionlcm: Real-time control-lable motion generation via latent consistency model. arXivpreprint arXiv:2404.19759, 2024. 1 Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao,and Li Yuan. Chat-univi: Unified visual representation em-powers large language models with image and video under-standing. arXiv preprint arXiv:2311.08046, 2023. 1",
  "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,Christine McLeavey, and Ilya Sutskever.Robust speechrecognition via large-scale weak supervision, 2022. 3": "Enxin Song, Wenhao Chai, Guanhong Wang, YuchengZhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, YanLu, Jenq-Neng Hwang, et al. Moviechat: From dense to-ken to sparse memory for long video understanding. arXivpreprint arXiv:2307.16449, 2023. 1, 3, 4 Teng Wang, Wenhao Jiang, Zhichao Lu, Feng Zheng, RanCheng, Chengguo Yin, and Ping Luo. Vlmixer: Unpairedvision-language pre-training via cross-modal cutmix. In In-ternational Conference on Machine Learning, pages 2268022690. PMLR, 2022. 1"
}