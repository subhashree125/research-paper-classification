{
  "(c) A Forward Moving Vehicle Stops and Reverses": ". Illustration of Instantaneous Motion Perception. We visualize motion of three objects from Waymo dataset , each withthree consecutive frames. Objects in fast and subtle motions are marked as red and blue, respectively, with arrow length indicating motionmagnitude. While standard motion detection handles general large motions such as (a), we focus on instantaneous perception of subtlemotions that may indicate changes in driving behavior, for example (b) parking car starts to move, and (c) forward moving car stops andreverses. The visualized subtle motions (b)(c) are output from our framework. We also provide the video visualization in supplementary.",
  "Abstract": "The perception of 3D motion of surrounding traffic par-ticipants is crucial for driving safety. While existing worksprimarily focus on general large motions, we contend thatthe instantaneous detection and quantification of subtle mo-tions is equally important as they indicate the nuances indriving behavior that may be safety critical, such as behav-iors near a stop sign of parking positions. We delve intothis under-explored task, examining its unique challengesand developing our solution, accompanied by a carefullydesigned benchmark. Specifically, due to the lack of cor-respondences between consecutive frames of sparse Lidarpoint clouds, static objects might appear to be moving the so-called swimming effect. This intertwines with the trueobject motion, thereby posing ambiguity in accurate esti-mation, especially for subtle motions. To address this, wepropose to leverage local occupancy completion of objectpoint clouds to densify the shape cue, and mitigate the im-pact of swimming artifacts. The occupancy completion islearned in an end-to-end fashion together with the detec-tion of moving objects and the estimation of their motion,instantaneously as soon as objects start to move. Extensiveexperiments demonstrate superior performance comparedto standard 3D motion estimation approaches, particularlyhighlighting our methods specialized treatment of subtlemotions.",
  ". Introduction": "Human drivers pay special attention to surrounding movingobjects to understand and predict their driving behavior, andreact accordingly to avoid collisions. Similarly, intelligentautonomous systems must also navigate safely through traf-fic scenes, where preventing collisions with moving objectsis considerably more complex than with static backgroundscenes. This gives rise to several lines of computer visionresearch centered around motion, ranging from low-leveltasks like 3D scene flow , to middle-level motion seg-mentation or detection , and high-level perception on3D object tracking .However, these methods are designed to handle general3D motion without considering the extent and context of themotion. In this paper, we would like to focus on an impor-tant subset of motion small subtle motions. Such motionsare of significance as they often indicate changes in drivingintention or behavior; for instance, as illustrated in ,parking vehicles start to move and cut into the driving lane,or vehicles in the driving lane start to reverse back for re-verse parking. As a more general note, the instant captureof all nuanced changes happening in the scene is essentialfor situation awareness, especially in corner-scenario cases.This however remains under-explored in computer vision,thus motivates our research in this paper, which aims to de-tect the presence of subtle motions as well as estimate theirmotion flow instantaneously.While prominent motions from fast-moving objects aremore feasible to detect and quantify due to a strong signal-to-noise ratio, recovering subtle motions with high accuracypresents its challenges. Specifically, the Lidar sensor cap-tures only a sparse point set of the surrounding scene el-ements, and the pattern of points varies depending on therelative position between the Lidar and the scene. Conse-quently, there are typically no point correspondences acrossframes with a moving Lidar sensor, even for static scene el-ements. This further implies that static objects may appearto be moving, known as the swimming artifact . Itintertwines with and hence obfuscates the true object mo-tion, especially under small motions; as such, we empiri-cally observe that a model trained for general motions doesnot perform as well with subtle motions.To address this, our framework proposes to learn shapecompletion before performing motion detection and estima-tion. Taking sequential frames of Lidar point clouds withina short period as input, our method voxelizes the pointclouds as occupancy grids, and leverages the accumulatedLidar points from nearby frames to generate a denser occu-pancy grid, which is then applied as supervision for occu-pancy completion. This effectively densifies and enhancesthe surface cue to mitigate the impact of the swimming ar-tifact. To prevent the noises brought about by imperfectcompletion from affecting the downstream motion estima- tion task, we refrain from performing full object shape com-pletion but rather do so locally only for the visible surfacepart, where the major motion signals lie. Our frameworktakes the point clouds of each object individually as inputto our network, which is trained exclusively on the regimeof small motion. We name our method SMore, indicatingsubtle motion regressor. Since there is no standard train-ing dataset and evaluation benchmark specific to subtle mo-tions. We contribute one by extracting small motions fromthe large-scale Waymo dataset , leveraging its existingannotations. We demonstrate the efficacy of the proposedmethod with the newly proposed benchmark.",
  ". Related Work": "3D Scene Flow. 3D scene flow aims to estimate the motionfield of each observed 3D point. It is an important tool foranalyzing scene dynamics and has been extensively studiedin computer vision . Whilethe scene flow of background points as the dominant rigidmotion may be reliably estimated , accurately estimatingthe motion flow for dynamic foreground objects remains achallenge. This leads to object-aware scene flow works that leverage rigidity prior of objects. However,the nearest neighbor-based approach for motion estimationlike scene flow suffers from inherent ambiguity brought bythe swimming effect of Lidar point clouds, which ismore severe with smaller motion magnitude. In this work,we develop insights towards addressing this issue.Moving Object Detection. Moving object detection is anessential capability for autonomous vehicles and other ar-eas, which results in many prior works, such as . However, they detect motions at acoarse level for general large motions. SemanticKITTI ,a commonly used dataset in this field, labels moving ob-ject in a coarse sequence level instead of in an instanta-neous manner.In contrast, our approach offers specialtreatment for detecting small motions instantaneously. An-other possible way for moving object detection is through3D detection and track-ing .However, we found empirically that suchmethods stumble in identifying small motions due to im-perfect object localization. Lastly, we also note a concur-rent work, M-detector , that instantly detects point-levelmoving events based on occlusion principles.",
  "Training-time + Inference-timeTraining-time only": ". Overview of SMore. Given a sequential LiDAR point cloud, we first identify objects of interest by filtering out backgroundand objects with large motion. We then voxelize the point cloud for each object, extract features through an encoder-decoder network, andperform occupancy completion. The output is passed to a motion detector and a motion flow predictor for final detection and estimation. Occupancy Prediction.Occupancy is an effective 3Dscene representation that has wide applications in au-tonomous driving. Argo et al. and Reza et al. applythe occupancy flow field for perception and motion forecast-ing. performs 4D occupancy forecasting supervisedby point cloud forecasting. ALSO utilizes occupancycompletion as a tool for self-supervised feature learningfor Lidar point clouds. Another line of work learns occupancy prediction from monocular cameras. Inthis work, we present the first known attempt to use occu-pancy completion to facilitate the estimation of small mo-tions from LiDAR point clouds.Subtle Motions in General.The field of computer vi-sion has shown a long history of interest in small mo-tions in a broader context.Differential structure-from-motion aims to recover instantaneous camera mo-tion from optical flow. Several works utilize accidental camera motion to perform 3D reconstruc-tion. Another line of research targets magnifyinginvisible subtle motions in videos. Our work focuses onperceiving the subtle motions of surrounding vehicles, a ca-pability critical to the safety of autonomous systems.",
  ". Integration of SMore with 3D tracking systems": "per on vehicles while leaving the human category for futurework. Unlike existing works, we concentrate on small mo-tion for instantaneous detection as objects begin to move.As a preprocessing step, our framework filters out fast-moving objects, thereby targeting static and slow-movingones. illustrates a practical use case of such setting ina 3D tracking system. Further, we make a practical assump-tion that the ego-vehicles motion can be reliably estimatedby ICP, possibly aided by GPS/INS, as validated by recentstudies . This allows for the exclusion of ego-motionfrom the observed object motion, resulting in a noisy obser-vation of the true object motion w.r.t. the world coordinate.Our method is object-centric, processing point clouds fromfive consecutive frames (Ft, t=1, ..., T) to classify objectsas static or moving. For moving objects, we estimate themotion flow from F1 to FT for each point in F1, settingT=5 as per .Swimming Effect. The detection and estimation of smallmotions present its challenges, primarily arising from thesparse nature of Lidar point clouds. Remarkably, the spa-tial distribution of captured points closely depends on the",
  ". Illustration of Swimming Effect on ground (a) andstatic object (b), and (c) our ground truth occupancy completionfor (b). Bule and red points indicate points from two frames": "relative position between the Lidar sensor and surroundingscene elements. Hence, as the Lidar sensor moves alongwith the ego-vehicle, there are typically no exact point cor-respondences across frames, and even static scene elementsmay appear to be moving. This effect manifests itself onboth background scene and foreground objects, as illus-trated in (a)(b). In particular, the ground points vi-sually appear to be swimming across frames, hence termedas swimming effect . This effect poses challenges, es-pecially to characterizing subtle motions as one would needto distinguish the true object motion from this effect. Wenote that the sparse nature of Lidar points distinguishes theproblem from optical flow , where dense correspon-dences exist and small motions simplify the flow estimationthrough brightness consistency assumption.",
  ". Our Framework": "illustrates the overview of SMore. Given five con-secutive Lidar frames, we filter out fast-moving objectsand for each remaining ones, our network estimates thesmall motion or the lack thereof. We start by voxelizingthe input point clouds, followed by feature extraction usingan encoder-decoder network, and then perform occupancycompletion, the output of which is passed to the motion seg-mentation head and the instantaneous flow estimation head.",
  "Occupancy Completion": "Input Voxelization. First, following state-of-the-art 3D de-tection frameworks , we voxelize the point cloudXt RN3 at each frame as a binary grid of size[Wx, Wy, Wz], with the voxels containing Lidar pointsfilled with 1, and 0 elsewhere. This grid may be viewedas an incomplete occupancy grid, in that it indicates partof the visible object surface captured by Lidar at a singletimestamp. We stack consecutive frames to form a spatial-temporal grid of size [T, Wx, Wy, Wz].Local Occupancy Completion. Recall that the sparse na-ture of Lidar point clouds poses challenges to accurate smallmotion estimation, due to the actual subtle motion inter-twined with the swimming effect. To mitigate this issue,our framework first learns occupancy completion that effec- tively densifies the object surface, to offer stronger cues forsubsequent networks to reason correspondence, and hencethe motion between frames.Before proceeding, one should be mindful of the poten-tial trade-off brought by this step the estimated occupancycompletion may well be imperfect, introducing extra noiseto the system. This may be harmful to the final motionsegmentation and estimation if the noises reach a certainlevel, hence defeating the purpose of occupancy comple-tion. Therefore, while the standard shape completion prob-lem (e.g. ) is tasked to recover the entire object shapefrom a single-frame input, it is an overly complicated andunnecessary task in our case, besides the infeasibility of get-ting the ground truth in real driving scenes. Instead, sincethe T Lidar frames collectively observe only a local part ofthe object within an instantaneous timeframe, we target lo-cal occupancy completion at these observed regions, whilerefraining from hallucinating areas invisible to all T Lidarframes.This way, we enhance the critical signal essen-tial for motion characterization while minimizing the extranoises from imperfect completions.Supervision for Occupancy. We densify the local occu-pancy grid by leveraging nearby frames, as shown in .Specifically, for each frame Ft, t [1, ..., T] in the inputwindow, we warp all Lidar points from the rest T 1 framesto Ft, using the ground truth object motion (recall that ego-motion has been factored out), and then mark the corre-sponding voxel as occupied, i.e. 1. In addition, we mark thepoints along the line-of-sight as empty i.e. 0. All other vox-els are deemed as unknown. We apply a fast voxel traversalalgorithm to implement this step, similarly as in ,with an example illustrated in (b)(c). Note we onlyuse the Lidar frames inside the input window to generatethe target occupancy grid, to simplify the task. By learningoccupancy, the network is explicitly enforced to learn thenotion of dense shapes in an end-to-end manner, therebyfacilitating the task of motion detection and estimation.",
  "Network Architecture and Losses": "Network Architecture.We apply an encoder-decoderfor occupancy grid prediction, which is passed to anotherencoder-decoder for motion detector and motion flow pre-dictor. The motion detector classifies input objects as staticor moving, while the flow estimator regresses a motion vec-tor for each occupied voxel in the grid. We then extractthe motion flow for each raw input point as the predictedflow in the voxel that point resides in. Note that we do notenforce rigidity constraints on the flow field, maintainingthe methods generality, though we do evaluate the settingwith rigidity prior as well. We utilize the encoder-decoderstructure as in , consisting of simple convolutional lay-ers with skip connection; we follow to treat the height and temporal dimension as the channel dimension, whichallows to use 2D convolutional layers for efficiency; seesupplementary for details. Our network processes each ob-ject separately, but remains efficient and runs at 27 fps for ascene consisting of 30 objects of interest.Overall Losses. The overall loss function of our model is aweighted combination of five terms: L = occLocc + motLmot + epeLepe + relLrel + angLang.(1)Specifically, we apply a binary cross-entropy (BCE) lossLocc for the occupancy grid prediction, a BCE loss Lmoton static/moving object classification, a L1 loss Lepe anda scale-aware Lrel loss on motion flow prediction for mov-ing objects. Additionally, since the motion direction carriesimportant information about driving intention such as re-versing or left/right turning, we add an angular loss Lang forthe motion flow. We denote the set of occupied and emptyvoxels as o and e, respectively.Occupancy Loss is written as Locc = Ev{o,e}Ov log(Ov) + (1 Ov) log(1 Ov),(2)where Ov and Ov indicate the predicted and ground truthoccupancy at voxel v.Flow Prediction Losses. For each voxel v, we define theground truth flow (denoted as fv) as the average of theground truth flow associated with the points falling into thatvoxel. The relative flow loss Lrel is written as",
  ". Evaluation of SMore": "Evaluation Benchmark. In the absence of existing bench-mark dedicated to subtle motions, we curate one suchdataset from the Waymo open dataset , where each se-quence provides Lidar frames at 10Hz for about 20s. Wecollect the point clouds for each object from every five con-secutive frames (0.5s) denoted as Fi, i=[1, ..., 5]. To gener-ate ground truth motion status, we follow to derive the",
  "FastNSF 0.11890.55920.6180ICP 0.05540.44990.7456Point-to-Plane ICP 0.22630.43790.7856Generalized ICP 0.11170.41700.7693CenterPoint 0.09270.56220.7270SMore0.04370.31890.8323": "spatial transformation from the 3D boxes annotations, andhence compute the scene flow fi from F1 to F5 for everypoint xi in F1. To concentrate on small motions, we deemthe data sample valid only when the minimal flow magni-tude fmin= minxiF1 ||fi|| is less than 0.2m. Further, welabel the object as static if fmin<fthre. We set fthre=0.05mbut also evaluate under other settings shortly. This way, wecollect about 140k and 9k samples for training and test sets,respectively. More details are in the supplementary. Evaluation Metrics. We apply the standard F1 score tomeasure the accuracy of static/moving object classification.We apply end-point error (EPE) and the angular error tomeasure the object motion flow error. Baselines. In the absence of existing detection methodsdedicated to small motions, we mainly compare with: (i)the classical Iterative Closest Point (ICP) , which re-mains competitive for motion flow task; (ii) the point-to-plane ICP and the generalized ICP imple-mented in Open3D ; (iii) the leading scene flow methodFastNSF ; (iv) the detection and tracking based methodCenterPoint , where we use ground-truth tracking byassociating detected objects with the ground truth, and themotion flows are derived from boxes transformation. For allmethods we use their output motion flows to detect movingobjects, according to the aforementioned criterion. Comparison.Tab. 1 shows quantitative evaluation re-sults, indicating the significantly superior performance ofour model compared to the baselines.We note that theobject-tracking method CenterPoint gives decent accuracybut lags behind SMore, likely because their imperfect ob-ject localization causes ambiguity in distinguishing smallmotions from static ones. In , we provide a visual-ization comparison with ICP and FastNSF. The input com-prises two sets of point clouds: the first frame (in red) andthe last frame (in green). We visualize the flow accuracythrough alignment we shift the red points with the flowand the resultant points (marked as blue) should ideallyalign well with the green points if the flows are correct.Our model demonstrates superior alignment accuracy, es-pecially at the subtle level of local registration, attributableto its advanced motion estimation capabilities. More exam-ples are provided in the supplementary material.",
  "(a) Input(b) Ground Truth(c) SMore (ours)(d) FastNSF(e) ICP": ". Qualitative Comparison. We exhibit point cloud registration results for two point cloud sets: the first frame (in red) and the lastframe (in green). The results are shown using (b) ground truth motion, and estimated motions by (c) SMore(ours), (d) FastNSF, and (e)ICP. The blue points indicate resultant positions after adding flow to the red points, which should ideally align with the green points.",
  ". Ablation study on Occupancy Completion": "To investigate the impact of occupancy completion, we re-move this module from SMore, with structure shown in(a) in relation to SMore in (c). Further, we alsoevaluate the setting with the occupancy completion as just an auxiliary trained in parallel with the motion detector andflow predictor, as shown in (b). We report the accu-racy in (e)(f), which indicates the significant impactof occupancy completion towards good performance. Weattribute this to its role in effectively densifying object sur-faces. In (d) we provide visualization of point cloudregistration to evaluate the estimated motion, further sup-porting the efficacy of occupancy completion. The qualita-tive results of the occupancy completion itself are demon-strated in . False Positive/Negative. This analysis explores how in-corporating occupancy impacts the reduction of false posi-tives and negatives. In (a), we display three sequentialframes that highlight how the model, without occupancy, er-roneously detects a stationary vehicle as moving, marked bya red box. Conversely, when employing dense occupancyestimation, the model correctly identifies the stationary na-ture of the vehicle, reducing the false positives. Similarly, in(b), we show that without occupancy, the model fails",
  "(false negative)": ". Visualization of false positive/negative samples in the absence of occupancy completion. Each row shows three consecutiveframes, one per column. The motion of objects is marked with blue for detected subtle motions and red for GT large motions, with the arrowlength representing the motions magnitude. In (a), a false positive occurs where a stationary vehicle is mistakenly marked as moving. In(b), a moving vehicle is incorrectly identified as static, a false negative. Both of them are rectified with the occupancy completion; note theground truth is same as the correct detection here hence is not visualized. . Ablation Study of Losses. This study observes a correlation between decreasing flow thresholds fthre and the degradation ofmodel performance. Notably, our model achieves the best average performance over five thresholds in terms of all metrics.",
  "fthre0.050.040.030.020.01Avg0.050.040.030.020.01Avg0.050.040.030.020.01Avg": "w/o Locc0.04920.04430.04400.04890.04390.04610.34890.34550.38280.42190.44050.38790.82560.82020.80250.75750.76030.7932w/o Langle0.04580.04410.04520.04440.04540.04500.34690.36540.40520.42950.48600.40660.83440.83380.81110.77040.78410.8068w/o Lrel0.04460.04350.04390.04370.04300.04370.31790.33950.37800.41560.42910.37600.83780.83070.79370.79070.78470.8075SMore0.04370.04250.04360.04280.04210.04290.31890.33920.38340.40530.43240.37580.83230.83200.80030.79970.78570.8100 to detect a moving vehicle as static. This is rectified usingthe model with occupancy. This comparison underscoresthe effectiveness of using occupancy data in enhancing mo-tion detection accuracy in our model. An interesting obser-vation we found is that the occurrence frequency of falsepositives is much higher than false negatives. We attributethis to the swimming effect, where static objects appear tobe moving, especially under subtle motion conditions.",
  "Small + large motion0.09790.55500.1414Small motion only0.04370.31890.8323": "due to the large signal-to-noise ratio. This perfectness sup-ports our focus on the small motion regime for enhancinga practical system. We also note that being a detection-tracking method, CenterPoint yields more precise flow esti-mation, as its accuracy largely depends on 3D box localiza-tion instead of motion.Benefit of using small-motion-specific dataset.Recallthat we have filtered out the large-motion data during thedataset curation, i.e. they are not in training data. Our ex-perience is that excluding large motions in training data im-proves the model performance on the small motion regime.We report the accuracy in Tab. 4 to quantify this effect. Thebenefit may be explained by the unique swimming artifact,which mandates a small-motion-specific dataset.",
  ". Evaluation in terms of Latency": "With a focus on instantaneous detection, a time-sensitivetask, it is helpful to also evaluate with a time-related met-ric. Our original task is to detect objects moving more thanfthre=0.05m in a 0.5s latency. Here, we increase fthre to tar-get at larger motion, which effectively allows proportionallyincreased latency if assuming constant velocity, hence de-creasing the requirement on the latency. We report in Tab. 5the detection accuracy (F1) across different latencies, indi-cating the consistently superior performance from SMore.",
  ". Important Design Choices": "Grid Size. We study the impact of occupancy grid sizeand find it important in our design. We compare the per-formance of two grid sizes: a balanced 100 100 100grid and an alternative 500 500 4 grid, where the lattersignificantly reduces the resolution along z-axis. The re-sults in (a), reveal a notable performance degradation(see the two dashed lines are consistently lower than thesolid lines) when the z-axis resolution is reduced, despitethe increased axial resolution to 500 500. This may alsoresult in detection ambiguity which stems from the modelsreduced capacity to discern subtle vertical variations, andlead to less reliable vehicle localization and motion detec-tion.",
  "ICP 0.77580.81290.82290.83080.8346CenterPoint 0.72700.76580.78000.78990.7935SMore0.83230.90030.92240.95680.9582": "Point-/Object-level Instance Flow Estimation. We studythe behavior of the point-level and object-level trainingstrategies for motion flow estimation. The point-level ap-proach predicts a separate flow for each point, whereasthe object-level strategy regresses a single rigid transfor-mation for the entire object to calculate motion flow. Theresults detailed in (b) in terms of EPE and Angle Er-ror across various flow thresholds, reveal that point-levelinstance flow estimation (indicated by two solid lines) con-sistently outperforms the object-level approach (indicatedby two dashed lines). Notably, point-level estimation main-tains stable performance even at very low thresholds. Incontrast, object-level flow prediction exhibits significantfluctuations in performance across different thresholds. Loss Components. We ablate each loss component andreport the results under various flow thresholds in Tab. 2.Notably, we observe that as the flow threshold decreases,there is a corresponding degradation in model performance.This trend aligns with our expectation, as lower thresholdsare designed to detect subtler motions. Subtler motions of-ten come with more severe swimming effects and thus leadto less accurate predictions. We also observe that each losscomponent plays a critical role in tuning the model for thesesubtleties, making them essential for maintaining perfor-mance across varying motion dynamics.",
  ". Conclusion": "This paper defines the problem of perceiving subtle motionfor vehicles, presenting practical significance. To mitigateswimming artifacts causing ambiguity in subtle motion per-ception, we leverage occupancy completion as an effectivestrategy to facilitate motion learning. Despite the overallgood performance, our method faces challenges under ex-tremely sparse or high-occluded objects. Also, we currentlyonly handle vehicles but not pedestrians or cyclists. Wehope our work and its limitations can inspire more researchinto this important yet under-explored problem.",
  "John Amanatides, Andrew Woo, et al. A fast voxel traver-sal algorithm for ray tracing. In Eurographics, pages 310.Citeseer, 1987. 4": "Aseem Behl, Despoina Paschalidou, Simon Donne, and An-dreas Geiger.Pointflownet: Learning representations forrigid motion estimation from point clouds. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, 2019. 2 Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall.Se-mantickitti: A dataset for semantic scene understanding oflidar sequences. In ICCV, 2019. 2",
  "Paul J Besl and Neil D McKay. Method for registration of3-d shapes. In Sensor fusion IV: control paradigms and datastructures, pages 586606. Spie, 1992. 5, 8": "Alexandre Boulch, Corentin Sautier, Bjorn Michele, GillesPuy, and Renaud Marlet.Also: Automotive lidar self-supervision by occupancy estimation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023. 3 Qi Chang, Zhennan Yan, Mu Zhou, Di Liu, Khalid Sawalha,Meng Ye, Qilong Zhangli, Mikael Kanski, Subhi AlAref,Leon Axel, et al. Deeprecon: Joint 2d cardiac segmentationand 3d volume reconstruction via a structure-specific gener-ative method. In International Conference on Medical ImageComputing and Computer-Assisted Intervention, pages 567577. Springer, 2022. 2",
  "Nathaniel Chodosh, Deva Ramanan, and Simon Lucey. Re-evaluating lidar scene flow for autonomous driving. arXivpreprint arXiv:2304.02150, 2023. 2, 3, 5": "Ilya Chugunov, Yuxuan Zhang, Zhihao Xia, Xuaner Zhang,Jiawen Chen, and Felix Heide. The implicit values of a goodhand shake: Handheld multi-frame neural depth refinement.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 28522862, 2022. 3 Fangqiang Ding, Andras Palffy, Dariu M Gavrila, andChris Xiaoxuan Lu.Hidden gems: 4d radar scene flowlearning using cross-modal supervision. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023. 2 Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, PhilipHausser, Caner Hazirbas, Vladimir Golkov, Patrick VanDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:Learning optical flow with convolutional networks. In Pro-ceedings of the IEEE international conference on computervision, 2015. 4 Brandon Y Feng,Hadi Alzayer,Michael Rubinstein,William T Freeman, and Jia-Bin Huang. 3d motion mag-nification: Visualizing subtle motions from time-varying ra-diance fields. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 98379846, 2023. 3 Artem Filatov, Andrey Rykov, and Viacheslav Murashkin.Any motion detector: Learning class-agnostic scene dynam-ics from a sequence of lidar point clouds. In 2020 IEEE in-ternational conference on robotics and automation (ICRA),pages 94989504. IEEE, 2020. 2 Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, ShaotingZhang, and Dimitris N Metaxas. A data-scalable transformerfor medical image segmentation: architecture, model effi-ciency, and benchmark. arXiv preprint arXiv:2203.00131,2022. 2 Yunhe Gao, Zhuowei Li, Di Liu, Mu Zhou, Shaoting Zhang,and Dimitris N Meta. Training like a medical resident: uni-versal medical image segmentation via context prior learn-ing. arXiv preprint arXiv:2306.02416, 2023. 2 Hyowon Ha, Sunghoon Im, Jaesik Park, Hae-Gon Jeon, andIn So Kweon. High-quality depth from uncalibrated smallmotion clip.In Proceedings of the IEEE conference oncomputer vision and pattern Recognition, pages 54135421,2016. 3 Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, KunpengSong, Mengwei Ren, Ruijiang Gao, Anastasis Stathopou-los, Xiaoxiao He, Yuxiao Chen, et al. Proxedit: Improvingtuning-free real image editing with proximal guidance. InProceedings of the IEEE/CVF Winter Conference on Appli-cations of Computer Vision, pages 42914301, 2024. 2 Xiaoxiao He, Chaowei Tan, Bo Liu, Liping Si, Weiwu Yao,Liang Zhao, Di Liu, Qilong Zhangli, Qi Chang, Kang Li,et al. Dealing with heterogeneous 3d mr knee images: A fed-erated few-shot learning method with dual knowledge dis-tillation.In 2023 IEEE 20th International Symposium onBiomedical Imaging (ISBI), pages 15. IEEE, 2023. 2",
  "Shengyu Huang, Zan Gojcic, Jiahui Huang, Andreas Wieser,and Konrad Schindler. Dynamic 3d scene analysis by pointcloud accumulation. In European Conference on ComputerVision, 2022. 2, 3, 5": "Tarasha Khurana, Peiyun Hu, David Held, and Deva Ra-manan. Point cloud forecasting as a proxy for 4d occupancyforecasting. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2023. 2, 3, 4 Kuan-Hui Lee, Matthew Kliemann, Adrien Gaidon, Jie Li,Chao Fang, Sudeep Pillai, and Wolfram Burgard. Pillarflow:End-to-end birds-eye-view flow estimation for autonomousdriving.In 2020 IEEE/RSJ International Conference onIntelligent Robots and Systems (IROS), pages 20072013.IEEE, 2020. 2",
  "Ce Liu, Antonio Torralba, William T Freeman, Fredo Du-rand, and Edward H Adelson. Motion magnification. ACMtransactions on graphics (TOG), 24(3):519526, 2005. 3": "Di Liu, Jiang Liu, Yihao Liu, Ran Tao, Jerry L Prince, andAaron Carass. Label super resolution for 3d magnetic reso-nance images using deformable u-net. In Medical Imaging2021: Image Processing, pages 606611. SPIE, 2021. 2 Di Liu, Zhennan Yan, Qi Chang, Leon Axel, and Dimitris NMetaxas. Refined deep layer aggregation for multi-disease,multi-view & multi-center cardiac mr segmentation. In Inter-national Workshop on Statistical Atlases and ComputationalModels of the Heart, pages 315322. Springer, 2021. 2 Di Liu, Yunhe Gao, Qilong Zhangli, Ligong Han, Xiaox-iao He, Zhaoyang Xia, Song Wen, Qi Chang, Zhennan Yan,Mu Zhou, et al. Transfusion: multi-view divergent fusionfor medical image segmentation with transformers. In In-ternational Conference on Medical Image Computing andComputer-Assisted Intervention, pages 485495. Springer,2022. 2 Di Liu, Xiang Yu, Meng Ye, Qilong Zhangli, Zhuowei Li,Zhixing Zhang, and Dimitris N Metaxas. Deformer: Inte-grating transformers with deformable models for 3d shapeabstraction from a single image.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1423614246, 2023. 3",
  "Di Liu, Long Zhao, Qilong Zhangli, Yunhe Gao, Ting Liu,and Dimitris N Metaxas. Deep deformable models: Learning3d shape abstractions with part consistency. arXiv preprintarXiv:2309.01035, 2023": "Di Liu, Qilong Zhangli, Yunhe Gao, and Dimitris Metaxas.Lepard: Learning explicit part discovery for 3d articulatedshape reconstruction. Advances in Neural Information Pro-cessing Systems, 36, 2024. 3 Xingyu Liu,Charles R Qi,and Leonidas J Guibas.Flownet3d: Learning scene flow in 3d point clouds. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 529537, 2019. 2 Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,Huizi Mao, Daniela L Rus, and Song Han.Bevfusion:Multi-task multi-sensor fusion with unified birds-eye viewrepresentation. In 2023 IEEE International Conference onRobotics and Automation (ICRA), 2023. 2, 4",
  "Yi Ma, Jana Kosecka, and Shankar Sastry. Linear differen-tial algorithm for motion recovery: A geometric approach.International Journal of Computer Vision, 36:7189, 2000.3": "Reza Mahjourian, Jinkyu Kim, Yuning Chai, MingxingTan, Ben Sapp, and Dragomir Anguelov. Occupancy flowfields for motion forecasting in autonomous driving. IEEERobotics and Automation Letters, 7(2):56395646, 2022. 3 Carlos Martn-Isla, Vctor M Campello, Cristian Izquierdo,Kaisar Kushibar, Carla Sendra-Balcells, Polyxeni Gkontra,Alireza Sojoudi, Mitchell J Fulton, Tewodros WeldebirhanArega, Kumaradevan Punithakumar, et al.Deep learn-ing segmentation of the right ventricle in cardiac mri: The",
  "m&ms challenge. IEEE Journal of Biomedical and HealthInformatics, 2023. 2": "Benedikt Mersch, Xieyuanli Chen, Ignacio Vizzo, LucasNunes, Jens Behley, and Cyrill Stachniss. Receding movingobject segmentation in 3d lidar data using sparse 4d convo-lutions. IEEE Robotics and Automation Letters, 7(3):75037510, 2022. 2 Benedikt Mersch, Tiziano Guadagnino, Xieyuanli Chen, Ig-nacio Vizzo, Jens Behley, and Cyrill Stachniss. Building vol-umetric beliefs for dynamic environments exploiting map-based moving object segmentation. IEEE Robotics and Au-tomation Letters, 2023. 2",
  "Zhixiang Min, Bingbing Zhuang, Samuel Schulter, BuyuLiu, Enrique Dunn, and Manmohan Chandraker. Neurocs:Neural nocs supervision for monocular 3d object localiza-tion. In CVPR, 2023. 2": "Himangi Mittal, Brian Okorn, and David Held. Just go withthe flow: Self-supervised scene flow estimation. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1117711185, 2020. 2 Jeong Joon Park, Peter Florence, Julian Straub, RichardNewcombe, and Steven Lovegrove. Deepsdf: Learning con-tinuous signed distance functions for shape representation.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 165174, 2019. 4",
  "Aleksandr Segal, Dirk Haehnel, and Sebastian Thrun.Generalized-icp. In Robotics: science and systems, 2009.5": "Jiadai Sun, Yuchao Dai, Xianjing Zhang, Jintao Xu, Rui Ai,Weihao Gu, and Xieyuanli Chen. Efficient spatial-temporalinformation fusion for lidar-based 3d moving object segmen-tation. In 2022 IEEE/RSJ International Conference on In-telligent Robots and Systems (IROS), pages 1145611463.IEEE, 2022. 2 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, AurelienChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Benjamin Caine, et al. Scalability in perceptionfor autonomous driving: Waymo open dataset. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, 2020. 1, 2, 5 Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, SileiWu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin,et al. Scene as occupancy. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 84068415, 2023. 3",
  "surrounding semantic occupancy perception. arXiv preprintarXiv:2303.03991, 2023. 3": "Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, JieZhou, and Jiwen Lu. Surroundocc: Multi-camera 3d occu-pancy prediction for autonomous driving.In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 2172921740, 2023. 3 Song Wen, Hao Wang, Di Liu, Qilong Zhangli, and DimitrisMetaxas. Second-order graph odes for multi-agent trajectoryforecasting. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, pages 51015110,2024. 2"
}