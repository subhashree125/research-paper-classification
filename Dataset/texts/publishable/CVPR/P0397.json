{
  "Abstract": "Text-conditioned image-to-video generation (TI2V) aimsto synthesize a realistic video starting from a given image(e.g., a womans photo) and a text description (e.g., awoman is drinking water.). Existing TI2V frameworks of-ten require costly training on video-text datasets and spe-cific model designs for text and image conditioning.Inthis paper, we propose TI2V-Zero, a zero-shot, tuning-free method that empowers a pretrained text-to-video (T2V)diffusion model to be conditioned on a provided image,enabling TI2V generation without any optimization, fine-tuning, or introducing external modules.Our approachleverages a pretrained T2V diffusion foundation model asthe generative prior. To guide video generation with theadditional image input, we propose a repeat-and-slidestrategy that modulates the reverse denoising process, al-lowing the frozen diffusion model to synthesize a videoframe-by-frame starting from the provided image. To ensuretemporal continuity, we employ a DDPM inversion strat-egy to initialize Gaussian noise for each newly synthesizedframe and a resampling technique to help preserve visualdetails.We conduct comprehensive experiments on bothdomain-specific and open-domain datasets, where TI2V-Zero consistently outperforms a recent open-domain TI2Vmodel. Furthermore, we show that TI2V-Zero can seam-lessly extend to other tasks such as video infilling and pre-diction when provided with more images. Its autoregressivedesign also supports long video generation.",
  "A serene mountain cabin covered in a fresh blanket of snow": ". Examples of generated video frames using our proposedTI2V-Zero. The given first image x0 is highlighted with the redbox, and the text condition y is shown under each row of the video.The remaining columns show the 6th, 11th, and 16th frames of thegenerated output videos. Each generated video has 16 frames witha resolution of 256 256. thesize M new frames to yield a realistic video, x =x0, x1, . . . , xM, starting from the given frame x0 and sat-isfying the text description y.Current TI2V generationmethods typically rely on computationally-heavy training on video-text datasets and specific archi-tecture designs to enable text and image conditioning.Some are constrained to specific domains due tothe lack of training with large-scale open-domain datasets.Other approaches, such as , utilize pretrained foun-dation models to reduce training costs, but they still need totrain additional modules using video data.In this paper, we propose TI2V-Zero, which achieveszero-shot TI2V generation using only an open-domain pre-trained text-to-video (T2V) latent diffusion model .Here zero-shot means that when using the diffusion",
  "arXiv:2404.16306v1 [cs.CV] 25 Apr 2024": "model (DM) that was trained only for text conditioning,our framework enables image conditioning without any op-timization, fine-tuning, or introduction of additional mod-ules. Specifically, we guide the generation process by incor-porating the provided image x0 into the output latent codeat each reverse denoising step. To ensure that the tempo-ral attention layers of the pretrained DM focus on informa-tion from the given image, we propose a repeat-and-slidestrategy to synthesize the video in a frame-by-frame man-ner, rather than directly generating the entire video volume.Notably, TI2V-Zero is not trained for the specific domain ofthe provided image, thus allowing the model to generalizeto any image during inference. Additionally, its autoregres-sive generation makes the synthesis of long videos possible.While the standard denoising sampling process start-ing with randomly initialized Gaussian noise can producematching semantics, it often results in temporally inconsis-tent videos. Therefore, we introduce an inversion strategybased on the DDPM forward process, to provide a moresuitable initial noise for generating each new frame. Wealso apply a resampling technique in the video DM tohelp preserve the generated visual details. Our approach en-sures that the network maintains temporal consistency, gen-erating visually convincing videos conditioned on the givenstarting image (see ).We conduct extensive experiments on MUG , UCF-101 , and a new open-domain dataset. In these experi-ments, TI2V-Zero consistently performs well, outperform-ing a state-of-the-art model that was based on a videodiffusion foundation model and was specifically trainedto enable open-domain TI2V generation.",
  ". Conditional Image-to-Video Generation": "Conditional video generation aims to synthesize videosguided by user-provided signals. It can be classified accord-ing to which type(s) of conditions are given, such as text-to-video (T2V) generation , video-to-video (V2V) generation , and image-to-video (I2V) generation . Here wediscuss previous text-conditioned image-to-video (TI2V)generation methods .Hu et al. introduced MAGE, a TI2V generator that integrates amotion anchor structure to store appearance-motion-alignedrepresentations through three-dimensional axial transform-ers.Yin et al. proposed DragNUWA, a diffusion-based model capable of generating videos controlled bytext, image, and trajectory information with three modulesincluding a trajectory sampler, a multi-scale fusion, andan adaptive training strategy. However, these TI2V frame-works require computationally expensive training on video-text datasets and a particular model design to support text- and-image-conditioned training. In contrast, our proposedTI2V-Zero leverages a pretrained T2V diffusion model toachieve zero-shot TI2V generation without additional opti-mization or fine-tuning, making it suitable for a wide rangeof applications.",
  ". Adaptation of Diffusion Foundation Models": "Due to the recent successful application of diffusion models(DM) to both image and video genera-tion, visual diffusion foundation models have gained promi-nence. These include text-to-image (T2I) models such asImagen and Stable Diffusion , as well as text-to-video (T2V) models such as ModelScopeT2V andVideoCrafter1 . These models are trained with large-scale open-domain datasets, often including LAION-400M and WebVid-10M . They have shown immense po-tential for adapting their acquired knowledge base to ad-dress a wide range of downstream tasks, thereby reducing oreliminating the need for extensive labeled data. For exam-ple, previous works have explored the application of largeT2I models to personalized image generation , im-age editing , image segmentation ,video editing , and video generation .In contrast to T2I models, there are fewer works on theadaptation of large-scale T2V models. Xing et al. pro-posed DynamicCrafter for open-domain TI2V generationby adapting a T2V foundation model . To control thegenerative process, they first employed a learnable imageencoding network to project the given image into a text-aligned image embedding space. Subsequently, they uti-lized dual cross-attention layers to fuse text and image in-formation and also concatenated the image with the initialnoise to provide the video DM with more precise image de-tails. In contrast, in this paper we explore how to inject theprovided image to guide the DM sampling process basedsolely on the pretrained T2V model itself, with no addi-tional training for the new TI2V task.",
  ". Methodology": "Given one starting image x0 and text y, let x=x0, x1, . . . , xM represent a real video correspondingto text y.The objective of text-conditioned image-to-video (TI2V) generation is to synthesize a video x =x0, x1, . . . , xM, such that the conditional distribution ofx given x0 and y is identical to the conditional distributionof x given x0 and y, i.e., p(x|x0, y) = p(x|x0, y). Ourproposed TI2V-Zero can be built on a pretrained T2V diffu-sion model with a 3D-UNet-based denoising network. Herewe choose ModelScopeT2V as backbone due to itspromising open-domain T2V generation ability. Below, wefirst introduce preliminaries about diffusion models, thenintroduce the architecture of the pretrained T2V model, andfinally present the details of our TI2V-Zero.",
  "Resample": ". Illustration of the process of applying TI2V-Zero to generate the new frame xi+1, given the starting image x0 and text y. TI2V-Zero is built upon a frozen pretrained T2V diffusion model, including frame encoder E, frame decoder D, and the denoising U-Net . Atthe beginning of generation (i = 0), we encode x0 as z0 and repeat it K times to form the queue s0. We then apply DDPM-based inversionto s0 to produce the initial Gaussian noise zT . Subsequently, in each reverse denoising step using U-Net , we keep replacing the first Kframes of zt with the noisy latent code st derived from s0. Resampling is also applied within each step to improve motion coherence. Wefinally decode the final frame of the clean latent code z0 as the new synthesized frame xi+1. To compute the new s0 for the next iterationof generation (i > 0), we perform a sliding operation by dequeuing s00 and enqueuing zK0 within s0.",
  ". Preliminaries: Diffusion Models": "Diffusion Models (DM) are probabilistic mod-els designed to learn a data distribution. Here we introducethe fundamental concepts of Denoising Diffusion Proba-bilistic Models (DDPM). Given a sample from the datadistribution z0 q(z0), the forward diffusion process ofa DM produces a Markov chain z1, . . . , zT by iterativelyadding Gaussian noise to z0 according to a variance sched-ule 1, . . . , T , that is:",
  "q(zt|zt1) = N(zt;": "1 tzt1, tI) ,(1)where variances t are constant. When the t are small,the posterior q(zt1|zt) can be well approximated by a di-agonal Gaussian . Furthermore, if the length of thechain, denoted by T, is sufficiently large, zT can be wellapproximated by a standard Gaussian distribution N(0, I).These suggest that the true posterior q(zt1|zt) can be esti-mated by p(zt1|zt) defined as: p(zt1|zt) = N(zt1; (zt), 2t I) ,(2)where variances t are also constants. The reverse denois-ing process in the DM (also termed sampling) then gener-ates samples z0 p(z0) by starting with Gaussian noisezT N(0, I) and gradually reducing noise in a Markovchain zT 1, zT 2, . . . , z0 using a learned p(zt1|zt). Tolearn p(zt1|zt), Gaussian noise is first added to z0 togenerate samples zt. Utilizing the independence propertyof the noise added at each forward step in Eq. (1), we cancalculate the total noise variance as t = ti=0(1i) andtransform z0 to zt in a single step:",
  "L = EtU(1,T ),z0q(z0),N (0,I)|| (zt, t)||2,(4)": "where diffusion step t is uniformly sampled from{1, . . . , T}.Then (zt) in Eq. (2) can be derivedfrom (zt, t) to model p(zt1|zt) .The denois-ing model is implemented using a time-conditioned U-Net with residual blocks and self-attention lay-ers . Diffusion step t is specified to by the sinu-soidal position embedding .Conditional generationthat samples z0 p(z0|y) can be achieved by learninga y-conditioned model (zt, t, y) with classifier-free guidance .During training, the condition y in(zt, t, y) is replaced by a null label with a fixed proba-bility. When sampling, the output is generated as follows:(zt, t, y) = (zt, t, ) + g ((zt, t, y) (zt, t, )) , (5)",
  ". Architecture of Pretrained T2V Model": "TI2V-Zero can be built upon a pretrained T2V diffusionmodel with a 3D-UNet-based denoising network. Here wechoose ModelScopeT2V as the pretrained model (de-noted M). We now describe this T2V model in detail.Structure Overview. Given a text prompt y, the T2Vmodel M synthesizes a video x = x0, x1, . . . , xK with apre-defined video of length (K +1) using a latent video dif-fusion model. Similar to Latent Diffusion Models (LDM), M incorporates a frame auto-encoder for theconversion of data between pixel space X and latent spaceZ through its encoder E and decoder D. Given the realvideo x = x0, x1, . . . , xK, M first utilizes the frame en-coder E to encode the video x as z = z0, z1, . . . , zK.Here the sizes of pixel frame x and latent frame z areHx Wx 3 and Hz Wz Cz, respectively. To beconsistent with the notation used for the DM, we denote the",
  ": end for22: return x": "clean video latent z = z0 = z00, z10, . . . , zK0 . M thenlearns a DM on the latent space Z through a 3D denois-ing U-Net . Let zt = z0t , z1t , . . . , zKt represent thelatent sequence that results from adding noise over t stepsto the original latent sequence z0. When training, the for-ward diffusion process of a DM transforms the initial latentsequence z0 into zT by iteratively adding Gaussian noise for T steps. During inference, denoising U-Net predictsthe added noise at each step, enabling the generation of theclean latent sequence z0 = z00, z10, . . . , zK0 starting fromrandomly sampled Gaussian noise zT N(0, I).Text Conditioning Mechanism. M employs a cross-attention mechanism to incorporate text informationinto the generative process as guidance. Specifically, Muses a pretrained CLIP model to encode the prompt yas the text embedding e. The embedding e is later used asthe key and value in the multi-head attention layer withinthe spatial attention blocks, thus enabling the integration oftext features with the intermediate U-Net features in .Denoising U-Net. The denoising U-Net includes fourkey building blocks: the initial block, the downsamplingblock, the spatio-temporal block, and the upsampling block.The initial block transfers the input into the embedding",
  "A person is riding horse": ". Illustration of the motivation behind our framework.We explore the application of a replacing-based baseline approach(rows 24, labeled Replacing) and our TI2V-Zero (rows 56, la-beled TI2V-Zero) in various video generation tasks. The givenreal frames for each task are highlighted by red boxes and the textinput is shown under the block. The replacing-based approachis only effective at predicting a single frame when all the otherframes in the video are provided, while TI2V-Zero generates tem-porally coherent videos for both the TI2V and video infilling tasks. space, while the downsampling and upsampling blocks areresponsible for spatially downsampling and upsampling thefeature maps. The spatio-temporal block is designed to cap-ture spatial and temporal dependencies in the latent space,which comprises 2D spatial convolution, 1D temporal con-volution, 2D spatial attention, and 1D temporal attention.",
  ". Our Framework": "Leveraging the pretrained T2V foundation model M, wefirst propose a straightforward replacing-based baseline foradapting M to TI2V generation. We then analyze the possi-ble reasons why it fails and introduce our TI2V-Zero frame-work, which includes a repeat-and-slide strategy, DDPM-based inversion, and resampling. and Algorithm 1demonstrate the inference process of TI2V-Zero.Replacing-based Baseline.We assume that the pre-trained model M is designed to generate the video with afixed length of (K + 1). So we first consider synthesiz-ing videos with that same length (K + 1), i.e., M = K.Since the DM process operates within the latent space Z,we use the encoder E to map the given starting frame x0into the latent representation z0. Additionally, we denotez0 = z00 to specify that the latent is clean and corresponds to diffusion step 0 of the DM. Note that each reverse de-noising step in Eq. (2) from zt to zt1 depends solely onzt = z0t , z1t , . . . , zKt . To ensure that the first frame of thefinal synthesized clean video latent z0 = z00, z10, . . . , zK0 at step 0 matches the provided image latent, i.e., z00 = z00,we can modify the first generated latent z0t of zt at each re-verse step, as long as the signal-to-noise ratio of each framelatent in zt remains consistent. Using Eq. (3), we can add tsteps of noise to the provided image latent z00, allowing us tosample z0t through a single-step calculation. By replacingthe first generated latent z0t with the noisy image latent z0t ateach reverse denoising step, we might expect that the videogeneration process can be guided by z00 with the followingexpressions defined for each reverse step:",
  "zt1 N((zt, y), 2t I) .(6c)": "Specifically, in each reverse step from zt to zt1, as shownin Eq. (6a), we first compute the noisy latent z0t by addingGaussian noise to the given image latent z00 over t steps.Then, we replace the first latent z0t of zt with z0t in Eq. (6b)to incorporate the provided image into the generation pro-cess. Finally, in Eq. (6c), we pass zt through the denoisingnetwork to generate zt1, where the text y is integrated byclassifier-free guidance (Eq. (5)). After T iterations, the fi-nal clean latent z0 at diffusion step 0 can be mapped backinto the image space X using the decoder D.Using this replacing-based baseline, we might expectthat the temporal attention layers in can utilize the con-text provided by the first frame latent z0t to generate the sub-sequent frame latents in a manner that harmonizes with z0t .However, as shown in , row 2, this replacing-based ap-proach fails to produce a video that is temporally consistentwith the first image. The generated frames are consistentwith each other, but not with the provided first frame.To analyze possible reasons for failure, we apply thisbaseline to a simpler video infilling task, where every otherframe is provided and the model needs to predict the inter-spersed frames. In this case, the baseline replaces the gener-ated frame latents at positions corresponding to real frameswith noisy provided-frame latents in each reverse step. Theresulting video, in , row 3, looks like a combinationof two independent videos: the generated (even) frames areconsistent with each other but not with the provided (odd)frames. We speculate that this may result from the intrinsicdissimilarity between frame latents derived from the givenreal images and those sampled from . Thus, the tempo-ral attention values between frame latents sampled in thesame way (both from the given images or both from ) willbe higher, while the attention values between frame latentssampled in different ways (one from the given image and theother from ) will be lower. Therefore, the temporal atten-tion layers of M tend to utilize the information from latents",
  "A woman with the expression of slight sadness on her face": ". Qualitative ablation study comparing different samplingstrategies for our TI2V-Zero on MUG. The first image x0 is high-lighted with the red box and text y is shown under the block. The1st, 6th, 11th, and 16th frames of the videos are shown in each col-umn. The terms Inversion, DDIM, and Resample denote theapplication of DDPM inversion, the steps using DDIM sampling,and the iteration number using resampling, respectively. produced by to synthesize new frames at each reversestep, ignoring the provided frames. We further simplify thetask to single-frame prediction, where the model only needsto predict a single frame when all the other frames in thevideo are given. In this setting, all the frame latents exceptfor the final frame are replaced by noisy provided-frame la-tents in each reverse step. Thus, temporal attention layerscan only use information from the real frames. In this case,, row 4, shows that the baseline can now generate afinal frame that is consistent with the previous frames.Repeat-and-Slide Strategy. Inspired by the observa-tion in , to guarantee that the temporal attention lay-ers of M depend solely on the given image, we make twomajor changes to the proposed replacing-based baseline:(1) instead of using M to directly synthesize the entire(K + 1)-frame video, we switch to a frame-by-frame gen-eration approach, i.e., we generate only one new frame la-tent in each complete DM sampling process; (2) for eachsampling process generating the new frame latent, we en-sure that only one frame latent is produced from , whilethe other K frame latents are derived from the given realimage and previously synthesized frames, thereby forcingtemporal attention layers to only use the information fromthese frame latents. Specifically, we construct a queue of K frame latents, denoted as s0 = s00, s10, , sK10. Wealso define st = s0t, s1t, , sK1t, which is obtained byadding t steps of Gaussian noise to the clean s0. Similar toour replacing-based baseline in the single-frame predictiontask, in each reverse step from zt to zt1, we replace thefirst K frame latents in zt by st. Consequently, the tem-poral attention layers have to utilize information from s0to synthesize the new frames latent, zK0 . Considering thatonly one starting image latent z0 is provided, we propose arepeat-and-slide strategy to construct s0. At the begin-ning of video generation, we repeat z0 for K frames toform s0, and gradually perform a sliding operation withinthe queue s0 by dequeuing the first frame latent s00 and en-queuing the newly generated latent zK0 after each completeDM sampling process. Note that though the initial s0 is cre-ated by repeating z0, the noise added to get st is differentfor each frames latent in st, thus ensuring diversity. Thefollowing expressions define one reverse step in the DMsampling process:",
  "zt1 N((zt, y), 2t I) .(7c)": "Specifically, in each reverse denoising step from zt to zt1,we first add t steps of Gaussian noise to the queue s0 to yieldst in Eq. (7a). Subsequently, we replace the previous Kframes of zt with st in Eq. (7b) and input zt to the denoisingnetwork to produce the less noisy latent zt1 (Eq. (7c)).With the repeat-and-slide strategy, model M is taskedwith predicting only one new frame, while the preceding Kframes are incorporated into the reverse process to ensurethat the temporal attention layers depend solely on informa-tion derived from the provided image.DDPM-based Inversion.Though the DM samplingprocess starting with randomly sampled Gaussian noiseproduces matching semantics, the generated video is oftentemporally inconsistent (, row 2). To provide initialnoise that can produce more temporally consistent results,we introduce an inversion strategy based on the DDPM forward process when generating the new frame latent.Specifically, at the beginning of each DM sampling processto synthesize the new frame latent zK0 , instead of startingwith the zT randomly sampled from N(0, I), we add T fullsteps of Gaussian noise to s0 to obtain sT using Eq. (3).Note that z has K + 1 frames, while s has K frames. Wethen use sT to initialize the first K frames of zT . We copythe last frame sK1Tof sT to initialize the final frame zKT ,as the (K 1)th frame is the closest to the Kth frame.Resampling. Similar to , we further apply a re-sampling technique, which was initially designed for theimage inpainting task, to the video DM to enhance motioncoherence. Particularly, after performing a one-step denois-ing operation in the reversed process, we add one-step noiseagain to revert the latent. This procedure is repeated mul-",
  ". Datasets and Metrics": "We conduct comprehensive experiments on three datasets.More details about datasets, such as selected subjects andtext prompts, can be found in our Supplementary Materials.MUG facial expression dataset contains 1,009videos of 52 subjects performing 7 different expres-sions.We include this dataset to evaluate the perfor-mance of models in scenarios with small motion and asimple, unchanged background.To simplify the exper-iments, we randomly select 5 male and 5 female sub-jects, and 4 expressions.We use the text prompt tem-plates like A woman with the expression ofslight {label} on her face. to change the ex-pression class label to be text input. Since the expressionsshown in the videos of MUG are often not obvious, we addslight in the text input to avoid large motion.UCF101 action recognition dataset contains 13,320videos from 101 human action classes.We include thisdataset to measure performance under complicated motionand complex, changing backgrounds. To simplify the exper-iments, we select 10 action classes and the first 10 subjectswithin each class. We use text prompt templates such asA person is performing {label}.to changethe class label to text input.In addition to the above two datasets, we create anOPEN dataset to assess the models performance in open-domain TI2V generation. We first utilize ChatGPT togenerate 10 text prompts. Subsequently, we employ Stable",
  "(Ours)": ". Qualitative comparison among different methods on multiple datasets for TI2V generation. Columns in each block display the1st, 6th, 11th, and 16th frames of the output videos, respectively. There are 16 frames with a resolution of 256 256 for each video. Thegiven image x0 is highlighted with the red box and the text prompt y is shown under each block.",
  ". Quantitative comparison among different methods on multiple datasets for TI2V generation": "Diffusion 1.5 to synthesize 100 images from each textprompt, generating a total of 1,000 starting images and 10text prompts for evaluating TI2V models.Data Preprocessing. We resize all the videos/images to256 256 resolution. For UCF101, since most of the videoframes are not square, we crop the central part of the frames.To obtain ground truth videos for computing metrics, weuniformly sample 16 frames from each video in the datasetsto generate the video clips with a fixed length.Metrics. Following prior work , we assessthe visual quality, temporal coherence, and sample diversityof generated videos using Frechet Video Distance (FVD).Similar to Frechet Inception Distance (FID) ,which is used for image quality evaluation, FVD utilizes avideo classification network I3D pretrained on Kinetics-400 dataset to extract feature representation of real andsynthesized videos. Then it calculates the Frechet distancebetween the distributions of the real and synthesized videofeatures. To measure how well a generated video alignswith the text prompt y (condition accuracy) and the givenimage x0 (subject relevance), following , we design twovariants of FVD, namely text-conditioned FVD (tFVD) andsubject-conditioned FVD (sFVD). tFVD and sFVD com-pare the distance between real and synthesized video fea-ture distributions under the same text y or the same subjectimage x0, respectively. We first compute tFVD and sFVDfor each condition y and image x0, then report their mean and variance as final results. In our experiments, we gen-erate 1,000 videos for all the models to estimate the fea-ture distributions. We compute both tFVD and sFVD onthe MUG dataset, but for UCF101, we only consider tFVDsince it doesnt contain videos of different actions for thesame subject. For the OPEN dataset, we only present quali-tative results due to the lack of ground truth videos. Unlessotherwise specified, all the generated videos are 16 frames(i.e., M = 15) with resolution 256 256.",
  ". Implementation Details": "Model Implementation. We take the ModelScopeT2V1.4.2 as basis and implement our modifications. Fortext-conditioned generation, we employ classifier-free guid-ance with g = 9.0 in Eq. (5). Determined by our prelimi-nary experiments, we choose 10-step DDIM and 4-step re-sampling as the default setting for MUG and OPEN, and50-step DDIM and 2-step resampling for UCF101.Implementation of SOTA Model.We compare ourTI2V-Zero with a state-of-the-art (SOTA) model Dynami-Crafter, a recent open-domain TI2V framework . Dy-namiCrafter is based on a large-scale pretrained T2V foun-dation model VideoCrafter1 . It introduces a learnableprojection network to enable image-conditioned generationand then fine-tunes the entire framework. We implementDynamiCrafter using their provided code with their defaultsettings. For a fair comparison, all the generated videos are",
  "!10": ". Example of long video generation using our TI2V-Zeroon the OPEN dataset. The given image x0 is highlighted with a redbox, and the text prompt y is shown under the set of frames. Thereare a total of 128 video frames (M = 127), and the synthesizedresults for every 14 frames are presented.",
  ". Result Analysis": "Ablation Study. We conduct ablation study of differentsampling strategies on MUG. As shown in Tab. 1 and ,compared with generating using randomly sampled Gaus-sian noise, initializing the input noise with DDPM inversionis important for generating temporally continuous videos,improving all of the metrics dramatically. For MUG, in-creasing the DDIM sampling steps from 10 to 50 does notenhance the video quality but requires more inference time.Thus, we choose 10-step DDIM as the default setting onMUG. As shown in and Tab. 1, adding resamplinghelps preserve identity details (e.g., hairstyle and facial ap-pearance), resulting in lower FVD scores. Increasing re-sampling steps from 2 to 4 further improves FVD scores.Effect of Real/Synthesized Starting Frames. We alsoexplore the effect of video generation starting with realor synthesized frames on UCF101.We initially use thefirst frame of the real videos to generate videos with ourTI2V-Zero, termed TI2V-Zero-Real. Additionally, we uti-lize the backbone model ModelScopeT2V to generatesynthetic videos using the text inputs of UCF101. We thenemploy TI2V-Zero to create videos from the first frame ofthe generated fake videos, denoted as TI2V-Zero-Fake. Asshown in Tab. 2, [TI2V-Zero-Fake vs. ModelScopeT2V]can achieve better FVD scores than [TI2V-Zero-Real vs.Real Videos]. The reason may be that frames generatedby ModelScopeT2V can be considered as in-distributiondata since TI2V-Zero is built upon it. We also comparethe output video distribution of TI2V-Zero-Fake and Mod-elScopeT2V with real videos in Tab. 2. Though startingfrom the same synthesized frames, TI2V-Zero-Fake cangenerate more realistic videos than the backbone model.Comparison with SOTA Model. We compare our pro-posed TI2V-Zero with DynamiCrafter in Tab. 3 and.From , one can find that DynamiCrafter struggles to preserve details from the given image, and themotion of its generated videos is also less diverse. Notethat DynamiCrafter requires additional fine-tuning to enableTI2V generation. In contrast, without using any fine-tuningor introducing external modules, our proposed TI2V-Zerocan precisely start with the given image and output morevisually-pleasing results, thus achieving much better FVDscores on both MUG and UCF101 datasets in Tab. 3. Thecomparison between our TI2V-Zero models with and with-out using resampling in and Tab. 3 also demonstratesthe effectiveness of using resampling, which can help main-tain identity and background details. Extension to Other Applications. TI2V-Zero can alsobe extended to other tasks as long as we can construct s0with K images at the beginning. These images can be ob-tained either from ground truth videos or by applying therepeating operation. Then we can slide s0 when generatingthe subsequent frames. We have applied TI2V-Zero in videoinfilling (see the last row in ), video prediction (seeSupplementary Materials), and long video generation (see). As shown in , when generating a 128-framevideo on the OPEN dataset, our method can preserve themountain shape in the background, even at the 71st frame(frame x70). The generated video examples and additionalexperimental results are in our Supplementary Materials.",
  ". Conclusion": "In this paper, we propose a zero-shot text-conditionedimage-to-video framework, TI2V-Zero, to generate videosby modulating the sampling process of a pretrained videodiffusion model without any optimization or fine-tuning.Comprehensive experiments show that TI2V-Zero canachieve promising performance on multiple datasets. While showing impressive potential, our proposed TI2V-Zero still has some limitations. First, as TI2V-Zero relies ona pretrained T2V diffusion model, the generation quality ofTI2V-Zero is constrained by the capabilities and limitationsof the pretrained T2V model. We plan to extend our methodto more powerful video diffusion foundation models in thefuture. Second, our method sometimes generates videosthat are blurry or contain flickering artifacts. One possiblesolution is to apply post-processing methods such as blindvideo deflickering or image/video deblurring toenhance the quality of final output videos or the newly syn-thesized frame in each generation. Finally, compared withGAN and standard video diffusion models, our approach isconsiderably slower because it requires running the entirediffusion process for each frame generation. We will in-vestigate some faster sampling methods to reducegeneration time. Niki Aifanti, Christos Papachristou, and Anastasios De-lopoulos. The mug facial expression database. In 11th In-ternational Workshop on Image Analysis for Multimedia In-teractive Services WIAMIS 10, pages 14. IEEE, 2010. 2,6",
  "DmitryBaranchuk,IvanRubachev,AndreyVoynov,Valentin Khrulkov, and Artem Babenko. Label-efficient se-mantic segmentation with diffusion models. arXiv preprintarXiv:2112.03126, 2021. 2": "Andreas Blattmann, Timo Milbich, Michael Dorkenwald,and Bjorn Ommer. Understanding object dynamics for in-teractive image-to-video synthesis.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 51715181, 2021. 2 Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.Align your latents: High-resolution video synthesis with la-tent diffusion models. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages2256322575, 2023. 2 Joao Carreira and Andrew Zisserman.Quo vadis, actionrecognition? a new model and the kinetics dataset. In pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition, pages 62996308, 2017. 7",
  "Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei AEfros.Everybody dance now.In Proceedings of theIEEE/CVF international conference on computer vision,pages 59335942, 2019. 2": "Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,Qifeng Chen, Xintao Wang, et al.Videocrafter1: Opendiffusion models for high-quality video generation. arXivpreprint arXiv:2310.19512, 2023. 2 Ozgun C icek, Ahmed Abdulkadir, Soeren S Lienkamp,Thomas Brox, and Olaf Ronneberger.3d u-net: learningdense volumetric segmentation from sparse annotation. InInternational conference on medical image computing andcomputer-assisted intervention, pages 424432. Springer,2016. 4 Michael Dorkenwald, Timo Milbich, Andreas Blattmann,Robin Rombach, Konstantinos G Derpanis, and Bjorn Om-mer. Stochastic image-to-video synthesis using cinns. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 37423753, 2021. 2 Patrick Esser, Robin Rombach, and Bjorn Ommer. Tamingtransformers for high-resolution image synthesis.In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1287312883, 2021. 3 Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, and Sean Bell.Tell mewhat happened: Unifying text-guided video completion viamultimodal masked video generation.In Proceedings of",
  "the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1068110692, 2023. 1, 2": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.An image is worth one word: Personalizing text-to-image generation using textual inversion.arXiv preprintarXiv:2208.01618, 2022. 2 Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, YuQiao, Dahua Lin, and Bo Dai. Animatediff: Animate yourpersonalized text-to-image diffusion models without specifictuning. arXiv preprint arXiv:2307.04725, 2023. 1, 2 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3",
  "Yitong Li, Martin Min, Dinghan Shen, David Carlson, andLawrence Carin. Video generation from text. In Proceedingsof the AAAI conference on artificial intelligence, 2018. 2": "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, ChongxuanLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-sion probabilistic model sampling in around 10 steps. arXivpreprint arXiv:2206.00927, 2022. 8 Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. Repaint: Inpaintingusing denoising diffusion probabilistic models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1146111471, 2022. 2, 6 Aniruddha Mahapatra and Kuldeep Kulkarni. Controllableanimation of fluid elements in still images. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 36673676, 2022. 2 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guidedimage synthesis and editing with stochastic differential equa-tions. arXiv preprint arXiv:2108.01073, 2021. 2",
  "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,and Daniel Cohen-Or. Null-text inversion for editing realimages using guided diffusion models.arXiv preprintarXiv:2211.09794, 2022": "Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit,Ye Wang, Toshiaki Koike-Akino, Vishal M Patel, and Tim KMarks. Steered diffusion: A generalized framework for plug-and-play conditional image synthesis. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2085020860, 2023. 2 Haomiao Ni, Yihao Liu, Sharon X Huang, and Yuan Xue.Cross-identity video motion retargeting with joint transfor-mation and synthesis. In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages412422, 2023. 2 Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, andMartin Renqiang Min. Conditional image-to-video gener-ation with latent flow diffusion models. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1844418455, 2023. 1, 2, 7 Haomiao Ni, Jiachen Liu, Yuan Xue, and Sharon X Huang.3d-aware talking-head video motion transfer. In Proceed-ings of the IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 49544964, 2024. 2 Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. Glide: Towards photorealistic image generationand editing with text-guided diffusion models. arXiv preprintarXiv:2112.10741, 2021. 3",
  "OpenAI. Openai: Introducing chatgpt. URL 2022. 6": "Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-wongsa, and Supasorn Suwajanakorn.Diffusion autoen-coders: Toward a meaningful and decodable representation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1061910629, 2022.2 Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-ing attentions for zero-shot text-based video editing. arXivpreprint arXiv:2303.09535, 2023. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 4 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1068410695, 2022. 2, 3, 4, 7 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In International Conference on Medical image com-puting and computer-assisted intervention, pages 234241.Springer, 2015. 3 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration. arXiv preprint arXiv:2208.12242, 2022. 2 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour,Burcu Karagol Ayan,S Sara Mahdavi,Rapha Gontijo Lopes, et al.Photorealistic text-to-imagediffusion models with deep language understanding. arXivpreprint arXiv:2205.11487, 2022. 2",
  "Open dataset of clip-filtered 400 million image-text pairs.arXiv preprint arXiv:2111.02114, 2021. 2": "Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,Oran Gafni, et al. Make-a-video: Text-to-video generationwithout text-video data. arXiv preprint arXiv:2209.14792,2022. 2 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,and Surya Ganguli.Deep unsupervised learning usingnonequilibrium thermodynamics. In International Confer-ence on Machine Learning, pages 22562265. PMLR, 2015.2, 3",
  "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.Ucf101: A dataset of 101 human actions classes from videosin the wild. arXiv preprint arXiv:1212.0402, 2012. 2, 6": "Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-wards accurate generative models of video: A new metric &challenges. arXiv preprint arXiv:1812.01717, 2018. 7 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 3 Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-dermans, Hernan Moraldo, Han Zhang, Mohammad TaghiSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.Phenaki: Variable length video generation from open domaintextual description. arXiv preprint arXiv:2210.02399, 2022.1",
  "Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan WeixianLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu": "Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuningof image diffusion models for text-to-video generation. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 76237633, 2023. 2 Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-tao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:Animating open-domain images with video diffusion priors.arXiv preprint arXiv:2310.12190, 2023. 1, 2, 7, 8 Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-long Wang, and Shalini De Mello. Open-vocabulary panop-tic segmentation with text-to-image diffusion models.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 29552966, 2023. 2 Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, JianpingShi, and Dahua Lin. Pose guided human video generation.In Proceedings of the European Conference on Computer Vi-sion (ECCV), pages 201216, 2018. 2 Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, HouqiangLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grainedcontrol in video generation by integrating text, image, andtrajectory. arXiv preprint arXiv:2308.08089, 2023. 1, 2",
  "A. Dataset Details": "We conduct extensive experiments on three diverse datasets,including facial expression dataset MUG, action recogni-tion dataset UCF101, and our self-created dataset OPEN.Here we present comprehensive details about these datasets.For the MUG dataset, we randomly select 5 male and 5female subjects from the available 52 individuals, and 4 ex-pressions from the provided 7 expression classes. Detailedinformation about selected subjects and corresponding ex-pression labels are presented in Tab. 1. To convert expres-sion class labels to text prompts for input, we use the follow-ing templates:A woman with the expressionof slight {label} on her face.for femalesubjects,and A man with the expression ofslight {label} on his face. for male subjects.Considering that the average original video length on MUGis about 72 frames, we uniformly sample 16 frames frommost of the videos to create the real videos. For videos withmore than 80 frames, we sample the videos every 5 framesuntil we obtain 16 frames to form the real videos.For the UCF101 dataset, we initially randomly selectsome action classes from the provided 101 classes. Sub-sequently, we identify and choose 10 action classes whereboth ModelScopeT2V and VideoCrafter1 are able to syn-thesize promising videos. shows the details ofselected action class labels and their corresponding textprompts. For each action class, we simply choose the first10 subjects. Given that the average original video length onthe UCF101 dataset is approximately 200 frames, we sam-ple the videos every 10 frames until 16 frames are obtainedto form the real videos. For videos containing less than 160frames, we uniformly sample 16 frames.For the OPEN dataset, we first employ ChatGPT 3.51",
  ". Details of selected action class labels and correspondingtext prompts on the UCF101 dataset": "to generate 10 text prompts by inputting the query Couldyou randomly generate 10 text prompts for testing text-to-video models?. We list these 10 text prompts in Tab. 3.Then we use Stable Diffusion 1.5 with the model IDdreamlike-photoreal-2.02 to generate 100 imagesfor each of 10 text prompts, resulting in a total of 1,000images as starting frames.",
  "A person is applying eye makeup": ". Examples of generated video frames in video predictiontask conditioning on different numbers of given images. The 1st,6th, 11th, and 16th frames of each output video are shown in eachcolumn. Each generated video has 16 frames with a resolution of256 256. 1 image, 4 images, 8 images indicate the useof the first 1, 4, and 8 real video frames in the ground truth videoto predict the next 15, 12, and 8 frames, respectively.",
  "rior performance": "Extension to Video Prediction Task.We have pre-sented the results of video infilling and long video gener-ation in the main paper. In and our supplementaryvideos, we show the application of our proposed TI2V-Zeroto the video prediction task. Specifically, we conduct exper-iments using the first 1, 4, and 8 real video frames from theground truth videos to generate 16-frame videos, i.e., syn-thesize the subsequent 15, 12, and 8 frames, respectively.As illustrated in , when only 1 image is provided, thewoman in the generated video applies the powder brush tothe eye differently from the real video. With 4 images, thewoman in the synthesized video applies the brush to the",
  "C. Discussion with Concurrent Work": "A concurrent work to ours, AnimateZero , also adopts asimilar repeating operation. However, we are different inseveral aspects. In our framework, when computing tem-poral attention outputs, the sources of keys are derived ei-ther from the given image or previously synthesized images,whereas AnimateZero relies on keys from the given imageor noise. Moreover, AnimateZero shares keys and valuesfrom spatial self-attention of the first frame across the otherframes; this may make it hard to generate large motions andnovel scenes, as the content is constrained to the informa- tion available in the first frame. In contrast, our frameworkdemonstrates the ability to generate promising videos con-taining intricate motions with input images of various stylesacross a wide variety of scenes.",
  "motivation.mp4 shows the video clips generated bythe replacing-based baseline approach and our proposedTI2V-Zero for different video tasks (corresponding to in our main paper)": "intricate.mp4 shows the video clips generated with in-tricate text and image inputs, including two (16-frame)videos and one long (64-frame) video. Each first-frameimage in the video clips was generated by Stable Diffu-sion 1.5. Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-uniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-gren Zhou. Videocomposer: Compositional video synthesiswith motion controllability. arXiv preprint arXiv:2306.02018,2023. 1, 2"
}