{
  "Abstract": "In this paper, we address the challenging problem oflong-term 3D human motion generation. Specifically, weaim to generate a long sequence of smoothly connectedactions from a stream of multiple sentences (i.e., para-graph). Previous long-term motion generating approacheswere mostly based on recurrent methods, using previouslygenerated motion chunks as input for the next step. How-ever, this approach has two drawbacks: 1) it relies on se-quential datasets, which are expensive; 2) these methodsyield unrealistic gaps between motions generated at eachstep. To address these issues, we introduce simple yet ef-fective T2LM, a continuous long-term generation frame-work that can be trained without sequential data. T2LMcomprises two components: a 1D-convolutional VQVAE,trained to compress motion to sequences of latent vectors,and a Transformer-based Text Encoder that predicts a la-tent sequence given an input text. At inference, a sequenceof sentences is translated into a continuous stream of la-tent vectors.This is then decoded into a motion by the VQVAE decoder; the use of 1D convolutions with a localtemporal receptive field avoids temporal inconsistencies be-tween training and generated sequences. This simple con-straint on the VQ-VAE allows it to be trained with shortsequences only and produces smoother transitions. T2LMoutperforms prior long-term generation models while over-coming the constraint of requiring sequential data; it is alsocompetitive with SOTA single-action generation models.",
  ". Introduction": "Human motion generation plays a vital role in numerous ap-plications of computer vision and robotics . Recent trends focus on controlling generatedhuman motions with input prompts such as discrete actionlabels , or free-form text . However, controllable synthesis of long-termhuman motion is less studied and remains challeng-ing, mainly due to the scarcity of long-term training data. Inthis work, we propose a model to produce long-term humanmotion from a given stream of textual descriptions of arbi-",
  "T2LM (Ours)": ". Comparison to previous methods. T2LM can be trainedwithout sequential datasets such as BABEL. Previous models withdiscontinuous decoding generate unrealistic gaps between the con-secutive actions. In contrast, our approach employs a continuousdecoding scheme for smoother transitions between actions. trary length without requiring sequential data for training.Real-life human motion is continuous and can be viewedas a temporal composition of actions, with transition in be-tween. Although the text-conditional generation of shortactions has been thoroughly addressed by previous work, modeling smooth and realistic transitions re-mains a core challenge for generating long-term motionsusable in practical applications .While a body of work on long-term mo-tion generation has been introduced, we identify two lim-itations of these methods summarized in .First,existing methods such as MultiAct , TEACH , orST2M rely on sequential data for training. Comparedto single-action datasets , which contain annotationsfor short actions, a sequential dataset contains frame-level annotations for each individual action and transitionwithin long-term motion. While this provides valuable datato capture how transitions connect consecutive actions, ac-quiring such dense frame-level annotation at scale is ex-pensive, and determining the segment between actions isnot trivial. In addition, capturing transitions for all possi-ble pairs of actions at scale is impossible. This dependencylimits the applicability of existing methods to new domains.Second, existing methods empirically struggle to createsmooth and realistic transitions. We hypothesize this is dueto discontinuities in the generation process when chainingactions together. The majority of works recur-rently generates the long-term motions at two granulari-ties: actions of each step are conditioned on the output ofthe previous step, and those actions are concatenated intolong-term motion. Concurrently, DoubleTake uses theMDM to generate actions independently and blendsthem into a long-term motion with a diffusion model. Thisapproach also operates at two granularities, generating indi-vidual actions and merging them. It results in abrupt speedchanges and discontinuities between consecutive actions. Inthis work, we hypothesize that a framework that insteadstays at a single granularity can alleviate these issues andgenerate smoother transitions.As illustrated in , we propose a conceptually sim- ple yet effective framework T2LM. Our method a) can gen-erate a motion continuously across the input sentences andb) does not require long-term action sequences for training,thus overcoming the limitations of existing work. At traintime, we first train VQVAE to map an input motion into adiscrete latent space. The mapped latent sequence is usedas a target for a Text Encoder, a text-and-length conditionallatent prediction model. Both are trained with single ac-tions and accompanying texts. At inference time, a streamof input sentences and desired motion lengths is encodedinto latent vectors. Finally, we continuously reconstruct thedesired long-term motion with a 1D convolutional decoder.Our model has two key properties: First, it produces se-quences of latent vectors, unlike approaches that encode theentire sequence into a single latent vector like Actor .Second, we learn a prior over small chunks of motion, eachencoded independently from the others, using a VQVAE en-coder built from 1D convolutional layers with a local recep-tive field. This assumption, which departs from methodstaking all past motion into account like PoseGPT , isthe simplest way to avoid any discrepancies between shorttraining sequences and long sequences at inference time.These two key properties offer several advantages forlong-term generations. First, it is possible to process a se-quence of infinite length on the fly, as the cost of forward-ing the model is linear in the size of the local receptivefield . This is in contrast with methods that employ avanilla transformer architecture with a complexity that isquadratic in the sequence length. Thus, our model can pro-cess a continuous stream rather than a sequence of chunksthat have to be later post-processed . Secondly, usinga sequence of latents with local receptive field allows toconvey fine-grained semantics at the right temporal loca-tion. Empirically, we show that these simple changes leadto higher-quality actions compared to existing methods thatgenerate variable-length actions with a single latent vector.Our experiments show that T2LM outperforms theSOTA on long-term generation method when evaluated withFID scores and R-precision.We present two novel metrics aimed at evaluating thequantitative excellence of long-term motion more effec-tively: a) during transitions and b) along the sequence uti-lizing a sliding window approach.Our contributions are the following:",
  ". Related works": "Human motion synthesis.Human motion synthesis isnaturally formulated as a generative modeling problem. Inparticular, prior works have relied on Generative Adversar-ial Networks (GANs) , Variational Auto-encoders(VAEs) , Normalizing flows, diffusionmodels , or the VQ-VAE framework . Motion can be predicted from scratch or givenobserved frames, from the past only , oralso with future targets . Other forms of condition-ing can be used, such as speech , music ,action labels , or text . Inthe presence of text inputs, human motion generation canalso be cast into a machine-translation problem ;a joint cross-modal latent space can also be used .In this work, we consider motion generation conditioned ontext sentences from a generative modeling perspective. Action and text conditioned human motion generation.Early action conditional motion models relied on Condi-tional GANs and conditional VAEs . Moreflexible variants have been proposed using the VQ-VAEframework; in particular, PoseGPT allows conditioningon past observations relying on a GPT-like model to sam-ple motions. Human motion can be generated conditionallyon text. Earlier works include the Text2Action model ,based on an RNN conditioned on a short text.Motion- CLIP aligns text and motion by leveraging the pow-erful CLIP model as the text encoder and empiricallyshows that this enables out-of-distribution motion genera-tion. TEMOS extends the VAE-based approach AC-TOR to obtain a text-conditional model using an addi-tional text encoder. T2M proposed a large-scale datasetcalled HumanML3D, which is better suited to the task oftext-conditional long motion generation. TM2T jointlyconsiders text-to-motion and motion-to-text predictions andshows performance gains from jointly training both tasks.Recently, T2M-GPT have achieved competitive per-formance using the VQ-VAE framework, where motion isencoded into discrete indices, which are then predicted us-ing a GPT-like model. Diffusion-based models have alsoemerged as a powerful class of models to generate motionconditionally on text . Related to our works, Multi-Act , ST2M and TEACH utilize a recurrentgeneration framework with past-conditional VAE to gen-erate multiple actions sequentially. These require sequen-tial training data , an inherent limitation of the recurrentparadigm. DoubleTake, a part of PriorMDM that uti-lizes MDM as a generative prior, individually generatesthe actions and connects them with a diffusion model.",
  "= /": ". Text Encoder architecture. We present the architecture of Text Encoder. A first test encoder injects information about the textand length embeddings into a sequence of tokens, and a second autoregressive model predicts the latent sequence. to map a given text to a sequence in the discrete latentspace learned by the VQ-VAE (Sec. 3.2). Third, we dis-cuss in Sec. 3.3 our procedure to generate long-term motionsequences corresponding to input text streams. We also in-clude a desired length for each action in the stream. At traintime, this is extracted from the data, while at inference, thiscan be either treated as an input or sampled from a prior.",
  ". Learning a discrete latent representation": "Motivation. Human motion is typically represented as atemporal sequence of 3D points human meshes or skele-tons or a sequence of model parameters that produce such3D representations . Plausible human motion usu-ally represents a very small portion of these representationspaces, as evidenced by the fact that sequences of randomsamples do not produce any realistic motion. This has moti-vated methods that compress human motion into a discretelatent space and has shown to be beneficial for reconstruc-tion and manipulation . In contrast to previous ap-proaches , where a single latent represents theentire action available at each step, we design our approachso that each latent represents a fixed length of human mo-tion. This enables continuous decoding of the semanticsfrom textual descriptions without creating a duration mis-match between train and test sequences. We employ a 1Dconvolutional VQVAE to learn such a latent representation.Model. As depicted in , our VQVAE consists of an Encoder Econv, a Decoder Dconv, and a quantization mod-ule Q using a codebook V .The model is inspired by. The Encoder and Decoder, composed of 1Dconvolution layers, use two stride-2 convolutions and two 2upscaling layers each, setting the upscaling and downscal-ing rate l to 4. The input motion X RT d is encodedby the encoder in Z = Econv(X) RTzdV , which is thenquantized in Z RTzdV . Note that l denotes the temporaldown-scaling factor of the mapping, Tz := T/l denotesthe length of the downscaled motion in the latent space.Also, d and dV denote the dimensions of the single-framehuman pose representation and the quantized latent space,respectively. Finally, Z is reconstructed as X RT d bythe decoder.Quantization and optimization.Our quantization Qaligns with a discrete codebook V = {v1, ..., vC}, whereC represents the number of codes in the codebook andvi RdV . Specifically, each element zi of the latent vec-tor sequence Z = Econv(X) = {z1, ..., zTz} is quantizedinto the closest codebook entry vsi with the correspondingcodebook index si {1, ..., C}. Thus, our VQVAE can berepresented by the following equation:",
  "LVQ =Lrecon(X, X) + ||sg [Econv(X)] Z||22+ ||sg[ Z] Econv(X)||22.(3)": "The term ||sg[ Z] Econv(X)||22, is referred to as a commit-ment loss, has shown to be necessary to stable training .The reconstruction loss Lrecon consists of L1-loss on the pa-rameter, reconstructed joint, and velocity.Product quantization. To enhance the flexibility of thediscrete representations learned by the encoder Econv, weemploy a product quantization.Each element zi withinZ = Econv(X) is divided into K chunks (z1i , ..., zKi ), witheach chunk discretized separately using K different code-books.The size of the learned discrete latent space in-creases exponentially with K, resulting in a total of CT K combinations, where C is the size of each codebook. Al-though the increase in T and K provides a positive gainin both reconstruction quality and diversity, it introduces atrade-off that makes mapping text to latent space more chal-lenging. The utility of using product quantization is empir-ically validated in our experiments.",
  ". Mapping a text onto discrete latent space": "Motivation.We propose a Transformer-based Text En-coder that predicts a sequence of indices in discrete latentspace given an input text and desired motion length T.At train time, the target sequences are obtained using thetrained VQVAE by encoding ground truth target motions.One difficulty is that the input text is of variable dimension,a-priori independent of the length of the corresponding mo-tion. To address this, we embed the conditioning signals anduse a first Transformer block to inject that information intoa sequence of Tz positional embeddings, as illustrated in. Note that T and Tz denote the desired length in mo-tion space and downscaled length in motion latent space, re-spectively. This yields a sequence of Tz vectors, which areall functions of the input text and length. A second Trans-former block, this time causal, then uses this informationto perform autoregressive next index prediction, ultimatelyobtaining the predicted index sequence.Model. As depicted in , our approach involves twoTransformers, H1 and H2. To form the input for H1, wefirst encode the text through CLIP and a linear layerinto etext RdH, and embed the desired length T throughthe embedding layer Ilen into elen RdH, respectively.Note that dH denotes the input dimension of the Trans-former layers.We concatenate etext and elen, along thetime dimension, following with positional embedding vec-tors PE1 RTzdH representing the temporal dimension in",
  "motion latent space. This is used as input to H1; we discardthe first two outputs along the time dimension and obtainthe text-length embedding": "{eitext-len}Tzi=0 RTzdH = H1(etext, elen, PE1)[2 : Tz + 2].(4)The second Transformer block is used for autoregres-sive next index prediction.Given the previous indices,{si}t1i=0 = (s0 := s, s1, ..., st1), and {eitext-len}t1i=0, weestimate the distribution p(st|{eitext-len}t1i=0, {si}t1i=0). Eachindex {si}t1i=0 is embedded through the embedding layerIidx into {eiidx}t1i=0, concatenated with {eitext-len}t1i=0.Theconcatenated input is added with positional embeddingPE2 Rt2dH and passed to the Transformer layer H2.The output corresponding to et1idx is then processed througha linear layer to estimate the likelihood,",
  "p(st|{eitext-len}t1i=0, {eiidx}t1i=0).(5)": "During training, we utilize a causal mask, followingPoseGPT , to handle this process in a single forwardpass. At test time, we repeat the autoregressive samplingTz times to obtain the final indices {si}Tzi=1.Optimization goal. This part of the model is trained toestimate the likelihood conditioned on the text and lengthinput by minimizing the negative log-likelihood of the targetindices under the output distribution.",
  ". Generation of long-term motion with T2LM": "gives an overview of how T2LM works at testtime. Note that we use different notation in Sec. 3.3 fromSecs. 3.1 and 3.2.Given a stream of sequential inputs{(wi, Ti)}Li=1 of arbitrary length L, with wi and Ti corre-sponding to the i-th (i {1, ..., L}) textual action descrip-tion and desired motion length, respectively. We generate acorresponding realistic and smooth long-term motion, rep-resented as a sequence of poses, Xlong R(Li=1 Ti)d.Each pair of element (wi, Ti) is first individually passedto the Transformer Text Encoder to obtain a sequence{si1, ..., siTi/l} of discrete indices, where l denotes the tem-poral down-scaling factor of the mapping. Then, the ex-tracted discrete indices {{sij}Ti/lj=1}Li=1 are dereferenced us-ing the codebook V and concatenated into a continuous se-quence of latent vectors. This gives us the final input to thedecoder:",
  ". Implementation details": "For VQVAE, we used a codebook of 512 dimensions, C =256 vectors in each K = 2 book for product quantiza-tion.We implement our framework with PyTorch .Our Text Encoder is a Transformer with three layers, 2048inner dimensions, and 16 multi-head attentions. We useAdamW as an optimizer with a learning rate of 2e-4and 3e-4, respectively, for training the VQVAE and TextEncoder. VQVAE and Text Encoder are trained for 1000and 700 epochs, respectively, with the StepLR learning ratescheduler of step size 350 and a decrease rate of 0.5. Thesize of the mini-batch is set to 128. We applied a linear in-terpolation augmentation during VQVAE training and ran-dom corruption augmentation for the Text Encoder.Training our model takes about a day on a single Nvidia2080Ti GPU.",
  ". Dataset": "Weconductedexperimentsontwodatasets:Hu-manML3D and BABEL . Our experiments focusedmainly on the HumanML3D dataset to show the perfor-mance of our proposed T2LM without sequential trainingdatasets, emphasizing its effectiveness in long-term genera-tion. Regarding the BABEL dataset, we also compared ourapproach with existing long-term generation methods thatrely on sequential data. Both datasets were evaluated usingwidely used evaluation protocols .HumanML3D.TheHumanML3Ddatasetcomprises",
  ". Comparison to SOTA: Long-term motion on BABELtest set. We compare the long-term generation performance withprevious state-of-the-art methods": "14,616 motions, each associated with 3-4 textual descrip-tions. These motions, sampled at 20 FPS, originated fromthe AMASS and HumanAct12 motion datasets, with man-ual additions of text descriptions. During training, we usedmotions with lengths ranging from a minimum of 40 framesto a maximum of 196 frames.BABEL We utilized the text version of the BABELdataset . This dataset includes 10,881 sequential mo-tions, each annotated with textual labels for action seg-ments. We used motions processed similarly to TEACH ,with lengths ranging from a minimum of 44 frames to amaximum of 250 frames.",
  ". Evaluation metrics": "Sliding-scope and Transition-scope. Existing evaluationmetrics for motion generation rely heavily on extracting fea-tures from the entire motion, making them dependent onmotion length and inadequate for quantitatively assessingthe quality of generated long-term motions. We proposetwo new evaluation criteria to address this limitation: FIDand diversity within a Sliding-scope and Transition-scope.We use a fixed window of 80 frames for both scopesto extract subsets of long-term motions. We then measureFID and Diversity by comparing these subsets with sets ex-tracted identically from the ground truth motion set.InSliding-scope (SS-FID and SS-Div), we slide the windowwith a stride of 40 frames from the beginning to the endof the generated long-term motion to extract samples. Inthe Transition-scope (TS-FID and TS-Div), we extract sam-ples centered around transitions in the generated long-termmotion. The Sliding-scope provides an overall measure ofhow realistically the generated long-term motion represents",
  "Long-term(w.o. seq. data)DoubleTake --0.590.609.505.61T2LM(Ours)0.4450.6310.7310.45710.0473.311": ". Comparison to SOTA: Single-action on HumanML3D test set. We compare the generation performance of a single action toprevious state-of-the-art methods. Note that our main comparison target are only the long-term generation methods. the entire sequence. At the same time, the Transition-scopeevaluates how smoothly and seamlessly the long-term mo-tion portrays transitions between actions. We use the pre-trained feature extractor from to encode the represen-tation of motion and text. We evaluate the quality of gener-ated short-term action with R-precision, FID, MultiModaldistance, and Diversity. Furthermore, we propose SS-FIDand TS-FID to assess the quality of generated long-termmotion quantitatively. R-Precision. For each motion, werank the Euclidean distance to 32 text descriptions of 1 pos-itive and 31 negatives. We report the Top-1, Top-2, andTop-3 accuracy. FID. We report the Frechet Inception Dis-tance between the set of ground truth motions and generatedmotions. MM-Distance. We report the average Euclideandistances between the features of each text and motion. Di-versity. We report the average Euclidean distances of thepairs in a set of 300 generated motions.",
  ". Ablation study": "This section presents an ablation study on an alternative de-sign idea using a transition latent vector and alternative con-figurations of the codebook in VQVAE. Quantitatively, it isconducted using five metrics: FIDV Q, R-Prec., FID, Diver-sity, and TS-FID. Note that FIDV Q represents the FID scoreof the reconstructed motion by the VQVAE. Please refer tothe supplementary material for other ablation studies.Transition latent vector.We considered two ways ofchaining a stream of latents from different texts at inferencetime. The first consists of simply concatenating the fea-tures; the second uses an additional token in the VQ-VAEcodebook to denote transitions. For this second option, weadd the learnable transition vectors in between latents ofeach text: V (siTi/l) and V (si+11) at inference time as de- picted in and Sec. 3.3. To train these transition latentvectors, we randomly substitute part of the quantized latentvectors Z into the transition latent vectors while training theVQVAE. While using a transition latent is a very reasonableidea used in methods such as MultiAct and Double-Take , empirically, we found that a technique based onconcatenation works best while being more straightforward.Tab. 2 presents the results. The leftmost column indi-cates the size of transition vectors; the length of the addi-tional transition is 2 l if we use two transition vectors,where l denotes the scaling rate of the VQVAE. Interest-ingly, the most straightforward approach of using concate-nation (i.e., first idea) performs best in our case. Specifi-cally, a decrease in performance was observed as the sizeof transition latents increased in four metrics. The decreasein FID and Diversity, reflecting single-action quality, sig-nals a reduction in the representation power of the latentspace during transition latent training. This is evidencedby the decrease in reconstruction metrics for the VQVAEmeasured by FIDV Q. We conclude that using additional la-tents to represent transitions is not beneficial when sequen-tial datasets are not employed, as evidenced by the degrada-tion of TS-FID, which indicates transition quality.Codebook configuration. In Tab. 3, we present quantita-tive measures for various codebook configurations used inthe VQVAE. Commonly, an increase in the complexity ofthe codebook results in better performance of VQVAE re-construction. However, this comes at the expense of morecomplicated predictions for the latent sequence predictionmodel. Indeed, it does not lead to monotonously improvingfinal generations, which is clearly visible when using fourcodebooks. Given these results, we chose the setting with 2codebooks, 256 vectors each, and 512 dimensions.",
  ". Comparison to state-of-the-art": "In this section, we compare the quality of motions gen-erated with our T2LM to previous methods on the Hu-manML3D and BABEL datasets. Regarding theexperiment on BABEL, we trained our model with indi-vidual actions and text annotations without using transi-tions.Our main comparison target on BABEL and Hu-manML3D is DoubleTake , the only long-term gen-eration method trained without sequential data.Further-more, we also compare with TEACH and MultiAct on BABEL dataset.1 Our straightforward approach out-performs previous long-term generation methods in bothsingle-action and long-term generation despite not requir-ing any sequential data for training.Long-term generation.Tabs. 4 and 5 shows that ourT2LM outperforms the main competing method, Double-Take , in every criteria on both HumanML3D and BABEL .Regarding the Sliding-scope evalua-tion, our model demonstrates better overall quality of gener-ated long-term motion compared to DoubleTake. Addition-ally, in the Transition-scope evaluation, our model producesmore realistic transitions than those generated by Double-Take. When evaluating long-term generation on the BA-BEL dataset, our model outperforms MultiAct on SS-FID,SS-Div. and TS-FID metric. Our method also shows thebetter performance compared to TEACH on the SS-FIDmetric, indicating better overall quality.However, oursshowed inferior performance in the Transition-scope eval-uation. This can be attributed to the usage of transitions 1ST2M is excluded from the comparison, since they do not use the135-dimension representation as TEACH, DoubleTake and Ours. Instead,ST2M used 263-dimension representation.As a result, their quantita-tive evaluation lies on different dimension from TEACH, DoubleTake andOurs. (Quantitative scores of GT motions in and are different.)",
  "from BABEL in TEACH during training time, while wetrain with individual actions only": "Single-action generation. Tabs. 6 and 7 show that T2LMoutperforms previous long-term generation methods by alarge margin on both HumanML3D and BABEL .Specifically, our T2LM scored 14.1% higher Top-3 R-precision compared to DoubleTake on HumanML3D.Moreover, we gained 16.2%, 15.9% and 12.9% Top-3 R-precision over MultiAct , DoubleTake andTEACH , respectively, on BABEL. Our superior perfor-mance is credited to the localized representative regions ofeach latent vector, combined with our Text Encoder, effec-tively conveying semantics from the text to the appropriatetemporal dimensions.",
  ". Conclusion": "In this work, we proposed a conceptually simple yet ef-fective long-term human motion generation framework bycomposing VQVAE and Transformer-based Text Encoder.Our approach achieved state-of-the-art performance com-pared to previous long-term generation methods on both ac-tions and transitions. We also performed a detailed analysison various model designs. Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, andSonghwai Oh. Text2Action: Generative adversarial synthe-sis from language to action. In International Conference onRobotics and Automation (ICRA), 2018. 3",
  "Wei Mao, Miaomiao Liu, and Mathieu Salzmann. Weakly-supervised action transition learning for stochastic humanmotion prediction. CVPR, 2022. 2": "Adam Paszke, Sam Gross, Soumith Chintala, GregoryChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-ban Desmaison, Luca Antiga, and Adam Lerer. Automaticdifferentiation in pytorch. NeurIPS Workshop on Autodiff,2017. 6 Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, andMichael J. Black. Expressive body capture: 3D hands, face,and body from a single image. In CVPR, 2019. 4",
  "Mathis Petrovich, Michael J. Black, and Gul Varol. TEMOS:Generating diverse human motions from textual descriptions.In ECCV, 2022. 1, 2, 3, 4": "Matthias Plappert, Christian Mandery, and Tamim Asfour.Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neu-ral networks. Robotics Auton. Syst., 2018. 3 Matthias Plappert, Christian Mandery, and Tamim Asfour.Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neu-ral networks. Robotics and Autonomous Systems, 2018. 1",
  "Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and JanKautz.Physdiff: Physics-guided human motion diffusionmodel. 2022. 3": "Andrei Zanfir,Eduard Gabriel Bazavan,Hongyi Xu,William T. Freeman, Rahul Sukthankar, and Cristian Smin-chisescu. Weakly supervised 3D human pose and shape re-construction with normalizing flows. In ECCV, 2020. 3 Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, ShaoliHuang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and XiShen. T2m-gpt: Generating human motion from textual de-scriptions with discrete representations. In CVPR, 2023. 1,3, 4, 6 Mingyuan Zhang, Zhongang Cai, Liang Pan, FangzhouHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-fuse: Text-driven human motion generation with diffusionmodel. arXiv preprint arXiv:2208.15001, 2022. 1"
}