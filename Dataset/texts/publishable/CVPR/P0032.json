{
  "Ziqi Cai1,2Kaiwen Jiang3Shu-Yu Chen1Yu-Kun Lai4Hongbo Fu5,6Boxin Shi8,9Lin Gao*1,7": "1Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences2Beijing Jiaotong University3University of California San Diego4Cardiff University5City University of Hong Kong6The Hong Kong University of Science and Technology7University of Chinese Academy of Sciences8National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University9 National Engineering Research Center of Visual Technology, School of Computer Science, Peking University",
  "Abstract": "Synthesizing realistic videos of talking faces under cus-tom lighting conditions and viewing angles benefits variousdownstream applications like video conferencing. However,most existing relighting methods are either time-consumingor unable to adjust the viewpoints. In this paper, we presentthe first real-time 3D-aware method for relighting in-the-wild videos of talking faces based on Neural RadianceFields (NeRF). Given an input portrait video, our methodcan synthesize talking faces under both novel views andnovel lighting conditions with a photo-realistic and disen-tangled 3D representation. Specifically, we infer an albedotri-plane, as well as a shading tri-plane based on a de-sired lighting condition for each video frame with fast dual-encoders. We also leverage a temporal consistency networkto ensure smooth transitions and reduce flickering artifacts.Our method runs at 32.98 fps on consumer-level hardwareand achieves state-of-the-art results in terms of reconstruc-tion quality, lighting error, lighting instability, temporalconsistency and inference speed. We demonstrate the effec-tiveness and interactivity of our method on various portraitvideos with diverse lighting and viewing conditions.",
  ". Introduction": "Portrait videos are widely used in various scenarios, such asvideo conferencing, video editing, entertainment, virtual re-ality, etc. However, many portrait videos are captured underunsatisfactory conditions, such as environments that are ei-ther too dark or too bright, or with virtual backgrounds thatdo not match the lighting of the foreground. These factorsdegrade the visual quality and realism of videos and affectthe user experience.Of particular significance is the context of augmented re-",
  "GeometryLightingLighting": ". Given a portrait video shown in the leftmost column, ourmethod reconstructs a 3D relightable face for each video frame.Users can then adjust their viewpoints and lighting conditions in-teractively. The second column displays relighted video frameswith a head pose yaw of 0.3, while the third column presents facesrelighted under an alternative lighting condition with a frontal headpose. The rightmost column provides the predicted albedo and ge-ometry of the reconstructed face. Please see the supplementaryvideo for the full results. ality (AR) and virtual reality (VR) applications, where usersoften seek to create 3D faces that can be dynamically re-lighted to fit the environment. This dynamic relighting ca-pability becomes possible only when the underlying methodis inherently 3D-aware and operates in real time.However, 3D-aware portrait video relighting is a chal-lenging task, since it involves modeling the complex inter-actions between the light, geometry, and appearance of hu-man faces, as well as ensuring the temporal coherence andnaturalness of synthesized videos. It is even more challeng-",
  "arXiv:2410.18355v1 [cs.CV] 24 Oct 2024": "ing when real-time performance is required. Existing meth-ods for face relighting suffer from some limitations that pre-vent them from being widely adopted in practice. First,most of them (e.g., ) can only relight the facesfrom the input viewpoints, thus restricting the users free-dom to change the camera angle or perspective. This alsolimits the creative possibilities and applications for AR/VRscenarios. Second, many methods (e.g., ) are de-signed for monocular image inputs and thus produce flick-ering or unnatural results when directly applied to videos,making them inferior for practical usage, where smooth andrealistic transitions are expected. Third, some methods aretime-consuming in terms of both training and inference. Forexample, ReliTalk takes 3 days of training for a 2-minute video clip. Once trained, it takes 0.2 seconds to re-light a video frame. Although DPR achieves real-timeperformance, it suffers from low-quality results. It is stillchallenging to balance quality and efficiency with existingsolutions.In this paper, we present a novel real-time 3D-aware por-trait video relighting method that jointly solves the aboveproblems by generating realistic and consistent relightingresults for faces from novel viewpoints in real time, en-abling users to create realistic and natural personas forAR/VR applications, as shown in . In summary,our technical contributions are: We contribute to the ongoing field of 3D-aware portraitvideo relighting by introducing a novel approach thatachieves real-time performance while producing realisticand consistent results. We propose to use dual feed-forward encoders to capturethe albedo and shading information within a portrait. Theshading encoder is conditioned on the albedo encoder toensure spatial alignment of albedo and shading, resultingin realistic reconstruction and accurate relighting.",
  ". 3D-aware Portrait Generation": "3D-aware portrait generation is the task of generating re-alistic and diverse images of human faces. Previous workon this task relied on 3D face priors to model the geome-try and appearance of faces, such as 3D morphable mod-els or neural face models . However, thesemethods require expensive 3D scanning or manual annota-tion and often produce low-resolution or unnatural results. With the advancement of generative models , it is nowpossible to learn a 3D representation of faces from a collec-tion of 2D images without any explicit 3D supervision. Inparticular, recent approaches combine neural radiance fields(NeRF) and generative models such as generative ad-versarial networks (GANs) and diffusion models to generate high-resolution and multi-view consistent faceimages . In this paper, weadopt the tri-plane representation from EG3D as our 3Drepresentation for portrait synthesis and relighting.Thischoice is motivated by the insights presented in , whichshows that the tri-plane 3D representation facilitates the dis-entanglement of albedo and shading. This disentanglement,afforded by the tri-plane structure, enables a 3D-aware ap-proach for relighting portraits in a photorealistic manner.",
  ". Portrait Relighting": "Portrait relighting requires changing the illumination of aportrait image or video while preserving the identity andappearance of the subject. Previous works (e.g., ) usedOne-Light-at-A-Time (OLAT) capturing systems to obtaindetailed portrait geometry and reflectance, which enabledrealistic relighting results. However, OLAT data is expen-sive and difficult to acquire, thus limiting the applicabilityof these methods. To overcome this limitation, some recentworks (e.g., ) used synthetic data for trainingand showed good generalization to real data.Another line of research explored 3D-aware portraitrelighting, which leveraged the recent advances in un-conditional 3D-aware portrait generation by combin-ing GANs and NeRFs .Concurrently, Jiang etal. and Ranjan et al. modeled the lighting ef-fects in generative models, either implicitly or explicitly,and achieved impressive quality of image relighting. How-ever, these methods are unsuitable for video relighting sincethey require inverting each frame separately, which is time-consuming and does not ensure temporal consistency, lead-ing to flickering artifacts.This paper proposes a novel method for real-time 3D-aware video relighting, which builds on and distillsits knowledge into a feedforward network with a temporalenhancement module. Our method can produce realistic,high-quality portrait relighting videos with various light-ing effects and novel views. In contrast to our approach,none of the existing portrait relighting techniques can han-dle consistent and real-time novel view synthesis for a videosequence.",
  "iST": ". The pipeline of our method. Given a portrait video shown on the left side, we embed each video frame into an albedo tri-planeand a shading tri-plane using Dual-Encoders. For example, for frame Fi, we predict the albedo tri-plane T iA. Next, we use the estimatedlighting condition L and the albedo tri-plane T iA to predict the shading tri-plane T iS that models the illumination effects on the face. Thenwe feed T iS and T iA along with the tri-planes predicted from previous n frames into two transformer models CA and CS to enhance thetemporal consistency. The two transformers use cross-attention to cooperate for information sharing and alignment between the albedo andshading branches. We add the predicted residual to T iA and T iS as T iA, T iS for better temporal consistency. Finally, we use T iA and T iS tocondition the volumetric rendering process, producing depth, albedo, shading, color, and super-resolved images. hybrid approaches.Optimization-based methods minimize reconstructionerrors for high-quality results but are slow, as seen in and , since these methods require end-to-end optimiza-tion across numerous video frames. Learning-based meth-ods (e.g., ), using an encoder, are faster but at the costof lower-quality reconstruction quality. With recent trendsof predicting richer information from input images, Yuanet al. , Bhattarai et al. , and Trevithick et al. propose to predict a tri-plane from input images, striking agood balance between quality and efficiency. Hybrid meth-ods (e.g., ) combine optimization and learn-ing, enhancing both quality and efficiency. Nevertheless,their practical utility is hindered because they still requireminutes to hours to process a video clip, preventing real-time applications.Among these methods, only pure learning-based meth-ods have the potential for real-time applications. Based onthe idea of LP3D , we propose a novel learning-basedmethod for video inversion, which predicts tri-plane repre-sentations from input images instead of latent codes. Tri-plane representations contain richer information than latentcodes and can better capture the geometry and appearancevariations of the input images. Unlike previous learning-based methods, such as , that are designedfor single-image inversion and thus neglect the temporal in-formation in videos, we introduce a temporal consistencynetwork to enforce smooth transitions between consecutiveframes. Our method can achieve high-quality and consistentvideo inversion in real time with relighting capabilities.",
  ". Methodology": "In this section, we give the preliminaries of the pre-trainedgenerator in Sec. 3.1. Then, we describe how we achievereal-time video inversion and enable lighting control by us-ing two tri-planes in Sec. 3.2. Next, we introduce how to en-hance temporal consistency for video inputs in Sec. 3.3. Fi-nally, we introduce our training objectives in Sec. 3.4. Theoverall pipeline is illustrated in .",
  ". Preliminaries": "Our work distills knowledge from a pre-trained 3D-awaregenerator G trained based on the GAN framework , toenable real-time synthesis and lighting control of multiviewconsistent video frames. Given a latent code w in an albedolatent space, an albedo tri-plane is first predicted througha generator and then fed into a convolutional network to predict a shading tri-plane, which is additionally condi-tioned on the second-order spherical harmonic (SH) coeffi-cients L . Both albedo tri-plane and shading tri-planeare used to condition the neural rendering process given aviewing angle. In this way, a realistic facial image I and itscorresponding albedo A can be generated, while allowingthe disentangled control of camera and lighting conditions.",
  ". Tri-plane Dual-encoders": "We present dual-encoders () that can infer an albedotri-plane and a shading tri-plane from a single RGB im-age. These two tri-planes are later rendered into a high-resolution (512 512) RGB image I and an albedo im-age A through a rendering process identical to . Our network extends the LP3D model , which encodes animage into a tri-plane representation for neural rendering.However, unlike LP3D, our network can produce two dis-entangled tri-planes, allowing for dynamic adjustments oflighting conditions from a single image. Our network con-sists of two branches: one is Albedo Encoder EA for infer-ring an albedo tri-plane that captures the shape and textureof the scene, and the other is Shading Encoder ES for infer-ring a shading tri-plane that models the fine-grained illumi-nation effects.Albedo Encoder.Inspired by LP3D , we use an en-coder based on Vision Transformer (ViT) in the albedobranch for albedo prediction. The input to our method isa single RGB image F with an overlaid coordinate map,forming a 5-channel image. We use a DeepLabV3 net-work pretrained on ImageNet to extract low-frequencyfeatures from the input image, which capture global con-text and semantic information.We then feed these fea-tures into a ViT-based encoder that further enhancesthe global features by self-attention mechanisms to get fi-nal low-frequency feature flow. We also use a convolutionalneural network (CNN) to extract high-frequency fea-tures fhigh from the input image F, which capture the finedetails and edges. We feed fhigh into another ViT-based en-coder , along with the low-frequency features flow topredict the final albedo tri-plane TA.Shading Encoder.To predict the shading tri-plane TS, weuse a CNN with additional StyleGAN blocks, condi-tioned on the albedo tri-plane TA and the lighting conditionL. We represent the lighting condition L as second-orderSH coefficients mapped using an off-the-shelf mapping net-work . This design ensures that the shading tri-plane TSis spatially aligned with the albedo tri-plane TA for realisticreconstruction and relighting.We employ a three-stage training strategy for our en-coder. In the initial stage, we adhere to the procedure out-lined in to train the albedo encoder, focusing on re-constructing the provided portrait without considering thedisentanglement between albedo and shading. In the sec-ond stage, we independently train the albedo branch andthe shading branch. In the third stage, we integrate the twobranches and train them jointly. This strategic approach en-hances convergence and performance compared to trainingboth branches simultaneously from the outset.",
  ". Temporal Consistency Network": "We aim to invert a video sequence into a sequence of tri-planes, which are low-dimensional representations of the3D scene structure, texture, and illumination.However,simply inverting each video frame independently leads totemporal inconsistency and causes flickering artifacts in therendered images.To address this problem, we propose a temporal consis- tency network (), which exploits the rich tempo-ral information in the video sequence to enhance the tem-poral consistency of the tri-plane features. The network iscomposed of two transformers, denoted as CA and CS, ac-companied by an additional convolutional neural network(CNN). Our design is inspired by , yet distinctively em-ploys features at the tri-plane level. Both transformers takein corresponding predicted tri-planes for n frames, and pre-dict residual tri-planes for each frame i to be added to theoriginal tri-planes as T iA, T iS. The residual tri-planes cap-ture the temporal variations and dynamics of the subject andhelp to eliminate the flickering effects. Moreover, this net-work uses cross-attention between the albedo branch andthe shading branch, which allows them to interact with eachother for better temporal consistency.We use synthetic data to train such a temporal consis-tency network. Similar to training the tri-plane encoder, wegenerate synthetic data with augmentation techniques tai-lored for temporal consistency. This involves interpolatingbetween two randomly selected camera views to simulaterealistic video sequences.Additionally, random noise isadded to both tri-planes to emulate flickering effects. Thisprocess for generating synthetic data provides us with aground truth for de-flickering, devoid of errors stemmingfrom inaccurate camera and lighting estimations. We empir-ically find that such a temporal consistency network trainedon dynamic viewing angles and artificial noises make ourmethod robust towards more diverse temporal dynamics inthe real-world case, such as dynamic expressions.",
  "(1)": "where Llpips denotes a perceptual loss ,Ar, A, andTg are the rendered albedo images in the raw and super-resolution domains, and the predicted albedo tri-plane, re-spectively. A, Ar, and Tg are the corresponding groundtruth. The parameter g decreases from 1 to 0.01 after theinitial 8 million iterations.Shading Loss.This loss measures the disparity betweenthe predicted and ground-truth shading features. It is de-fined as",
  "Lshading = || S S||1 + s|| TS TS||1,(2)where S and TS are the predicted shading maps and theshading tri-plane, respectively, and S and TS are the corre-": "sponding ground truth. The parameter s decreases from 1to 0.01 after the initial 8 million iterations.RGB Loss.This loss assesses the dissimilarity betweenthe predicted and ground-truth composed images in the raw,super-resolution, and feature domains. In addition to a per-ceptual loss , an identity loss is employed to retainthe appearance and identity of facial images. The RGB lossis defined as",
  "(3)": "where I, Ir, and If are the predicted RGB images in theraw, super-resolution, and feature domains, respectively,and I, Ir, and If are the corresponding ground truth. Theparameter f decreases from 1 to 0 after the initial 8 millioniterations.Adversarial Loss.This loss enforces the indistinguisha-bility of the predicted RGB images from the source RGBimages in both the raw and super-resolution domains. Adual discriminator D from is utilized to discriminatebetween the predicted and real images. The adversarial lossis defined as",
  "L = albedoLalbedo + shadingLshading+ rgbLrgb + advLadv,(5)": "where albedo, shading, rgb and adv are the weights for eachloss term. Initially, we set albedo = shading = rgb = 1and adv = 0. After the first 16M iterations, we activatethe adversarial loss by setting adv = 1 and keep the otherweights unchanged.For training our temporal consistency network, besidesa reconstruction loss, we use an additional temporal losssimilar to to ensure consistency in both short-termand long-term contexts. Specifically, this loss is defined asfollows:Temporal Consistency Loss.Without loss of generality,we assume current frame index is i for discussion.Theshort-term temporal loss is computed by calculating the op-tical flow fs between consecutive input frames Fi and Fi1.Subsequently, the previous outputs are warped to align withthe current frame. Formally, the short-term temporal loss isdefined as:",
  "are the corresponding frames warped using fs from the": "previous time step.The mask M is is defined as M is =exp(||Ii Ii1||1), which mitigates errors introduced dur-ing the warping process.For the long-term temporal loss, the same procedure isapplied, but with the temporal index i 1 replaced by 1.In other words, this process ensures temporal consistencybetween the first frame and the current frame. Similarly,the long-term temporal loss is defined as",
  "{I,Ir, A, Ar, S}Llpips(i 1),(7)": "where Ii1, Ii1r, Ai1, Ai1r, and Si1 are the corre-sponding frames warped using fl from the first time step.The mask M il is defined as M il = exp(||Ii I1||1).Our final loss function for training the temporal consis-tency network isLtemporal = shortLshort+longLlong+lpipsLlpips(Ii, Ii). (8)where Ii denotes the ground-truth image, Llpips promotesthe reconstruction and short = 1, long = 1, lpips = 1.",
  ". Implementation Details": "Datasets.We evaluate our method on the portrait videosfrom INSTA , which consist of 31,079 frames in total.Following , we crop the images and videos to focus onthe faces. We estimate the camera pose for each frame usingthe technique from . We also extract the lighting condi-tions with DPR .Training Details.As to the tri-plane dual-encoders, wefirst freeze the generator and train only our encoder. Af-ter the first 16M iterations, we unfreeze the albedo de-coder, shading decoder, and super-resolution module andtrain them jointly with the dual-encoders. As to the tem-poral consistency network, we sample camera poses fromnormal and uniform distributions for each person. We usetwo views for each person.For the first view, we sam-ple the focal length, camera radius, principal point, cam-era pitch, camera yaw, and camera roll from N(18.837, 1),N(2.7, 0.1), N(256, 14), U(26, 26), U(49, 49), andN(0, 2), respectively.For the second view, we samplethe camera pitch and camera yaw from U(26, 26) andU(36, 36), respectively and fix the other parameters to18.837 (focal length), 2.7 (camera radius), 256 (principalpoint), and 0 (camera roll).We train our network using the Adam optimizerwith a learning rate of 0.0001, except for the Transformerparameters, which have a learning rate of 0.00005. It takesabout 30 days to train our network on 8 NVIDIA Tesla",
  "Ours": ".Comparison of video relighting quality in the inputview. We compare our method with three methods: SMFR ,DPR , and ReliTalk . We show the input video frames inthe first row and the relighted results under different lighting con-ditions in the remaining rows. Our method produces more realisticand consistent results than other methods, especially under chal-lenging conditions like the side lighting.",
  ". Quantitative Evaluation": "To evaluate the performance of our method, we compare itwith other methods capable of 3D-aware portrait relighting.However, none of existing techniques can achieve this goalin a single step, so we have to combine different methodsto construct the baselines. Specifically, we use the follow-ing methods. B-DPR uses PTI to invert each frameof an input video as a latent code of EG3D , allowingfor rendering novel views and relighting using DPR .B-SMFR uses the same inversion and rendering method asB-DPR, but uses SMFR to relight the rendered framesfrom novel views.B-E4E uses an off-the-shelf encoderfrom a state-of-the-art NeRF-based face image relightingmethod to invert each frame of the input video and re-light it from novel views, which achieves real-time perfor-mance at the cost of quality. B-PTI uses the same encoderas B-E4E, but we apply the PTI to fine-tune a singlegenerator for each input video. This improves the recon-struction quality but takes more training time than B-E4E.We evaluate the performance of different methods regardingreconstruction quality, novel view relighting quality, iden-tity perseverance, and time cost.Novel View Relighting Quality.To evaluate the relight-ing quality under novel views, we relight first 500 framesfrom each video from . We render each video fromthree novel views and pair them with five distinct lightingconditions, resulting in a total of 75,000 frames for a com-prehensive comparison. Following , we adopt an off-the-shelf estimator to calculate the lighting accuracyand instability. We use MagFace , different from theone we use in training, to measure identity preservation be-tween different views. To assess temporal consistency, weuse an optical flow estimator to calculate warping error(WE). This involves warping the preceding frame to alignwith the current frame and measuring MSE loss. We alsocompute the LPIPS between adjacent frames for an addi-tional evaluation of temporal consistency. We list the time",
  "InputLightOursLumos TR NVPR SIPR-W DPR SMFR": ". Comparison of relighting quality on the input view. We compare our method with six methods: Lumos , TR , NVPR ,SIPR-W , DPR and SMFR . We show the input image in the first column, the sphere renderings from the environment map inthe second column, and the relighted results in the remaining columns. Our method produces more realistic and consistent results than theother methods. . Quantitative evaluation using lighting error (LE), lightinginstability (LI), Identity Perservance (ID), Warping Error (WE),LPIPS between consecutive frames and avarage time cost (Time)on the INSTA video dataset. We highlight the best score inboldface and underline the second best.",
  "B-PTI0.82200.26300.47280.00490.108030Ours0.77100.25330.53960.00030.01590.03": "each method takes to relight a face. summarizesthe quantitative evaluation results using the lighting error,lighting instability calculated based on the lighting transfertask introduced in , identity preservation (ID), and pro-cessing time (Time) on the INSTA video dataset. Ourmethod outperforms the baselines, demonstrating the sec-ond lowest lighting error and instability, the highest identitypreservation, the lowest warping error and LPIPS, and thelowest time cost.Reconstruction Quality.To assess the quality of recon-struction, we use four quantitative metrics: LPIPS ,DISTS , Pose Error (Pose), and Identity Preservation(ID). We obtained and used the same test data as LP3D .Input View Relighting Quality.We compare our methodwith four state-of-the-art portrait relighting methods: SIPR-W , TR , NVPR , and Lumos . We followthe same protocol as Lumos to obtain the results for compar-ison. As shown in , our method achieves the lowestFrechet Inception Distance, suggesting more realistic out-comes, and the highest Identity Preservation. For a visualcomparison, please refer to , where our approachyields the most realistic and natural results.For the video input, we evaluate the relighting accuracyand instability while performing the video relighting on the . Quantitative evaluation using LPIPS, DISTS, Pose Accu-racy (Pose), and Identity Consistency (ID) on 500 FFHQ images.Evaluated only using the face region. Evaluated only using theforeground on 2562 images. We highlight the best score in bold-face and underline the second best.",
  "FID87.3965.2355.3055.1851.1645.08ID0.64420.72420.61930.73740.62850.7711": "input view. Following , we adopt an off-the-shelf es-timator , which is different from the one we useduring the inference time, to calculate the lighting accuracyand the lighting instability. As shown in . Comparedto DPR, SMFR and ReliTalk, our method achieves the low-est lighting instability and the second lowest lighting error.",
  "w/o TCN0.77070.25260.00060.0304Ours0.77100.25330.00030.0159": "ods, our method preserves the lighting conditions of the ref-erence images the most faithfully.Input View Relighting Quality. presents thevideo relighting results in the input view by our methodin comparison with three existing methods. Our approachdemonstrates superior accuracy in reproducing lighting ef-fects, especially compared to existing non-3D-aware meth-ods. This is particularly evident under challenging lightingconditions, such as side lighting, where our method outper-forms others in maintaining image quality.",
  ". Ablation Study": "We perform an ablation study to evaluate the necessity ofeach key component in our method.Temporal Consistency Network.We remove the tempo-ral consistency network and then compute lighting error andlighting instability based on the lighting transfer task intro-duced in . We also evaluate the temporal consistencybased on the warp loss and LPIPS loss between consecu-tive frames, which serve as a reliable approximation of hu-man perception regarding temporal consistency, capturingnuances like flickering effects. As shown in , theabsence of the temporal consistency network results in anincrease in warping error and LPIPS, signaling a decline intemporal consistency. Tri-plane Dual-Encoders Design.We remove the dual-encoders (DE) and use an existing latent code encoder from instead.While this alternative design does achievereal-time 3D-aware relighting, it comes at the cost of a sub-stantial reduction in reconstruction quality, as visually de-picted in .",
  "InputReconstruction Condition 1Condition 2": ". Albation study comparing our model with and with-out the tri-plane encoders. The model without tri-plane encodersreplaces our tri-plane encoders with an existing latent space en-coder. This replacement results in images that bear much less re-semblance to the input person, indicating a lower level of identitypreservation. relight the video under novel lighting conditions for a givenfacial video. Our method combines the benefits of a re-lightable generative model, i.e., disentanglement and con-trollability, to capture the intrinsic geometry and appearanceof the face in a video and generate realistic and consistentvideos under novel lighting conditions. We evaluated ourmethod on portrait videos and showed its superiority overexisting methods in terms of lighting accuracy and lightingstability. Our work opens up new possibilities for 3D-awareportrait video relighting and synthesis. Limitations.One of the limitations of our method is thatit fails to model glares on the eyeglasses, as shown in therightmost column of . Future enhancements couldbenefit from incorporating advanced reflection and refrac-tion modeling techniques. Furthermore, our method doesnot separate the motion information from the identity in-formation, thus limiting its ability to perform video-drivenanimation. This challenge might be addressed through theintegration of the latest advancements in talking head gen-eration techniques. Future Work.We are interested in extending our methodto handle more complex scenes, such as multiple faces, oc-clusions, and full-body relighting. We also intend to exploremore applications of our method, such as face editing andanimation. AcknowledgementThis work was supported by National Natural ScienceFoundation of China (No. 62322210, No. 62102403, No.62136001 and No. 62088102), Beijing Municipal NaturalScience Foundation for Distinguished Young Scholars (No.JQ21013), and Beijing Municipal Science and TechnologyCommission (No. Z231100005923031). We thank Yu Lifrom the High Performance Computing Center at BeijingJiaotong University for his support and guidance in paral-lel computing optimization. We also thank Yu-Ying Yeh forgenerously sharing data for comparison. Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y.Ogras, and Linjie Luo. PanoHead: Geometry-aware 3D full-head synthesis in 360deg. In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 2095020959,2023. 2 Yunpeng Bai, Yanbo Fan, Xuan Wang, Yong Zhang, Jingx-iang Sun, Chun Yuan, and Ying Shan. High-fidelity facialavatar reconstruction from monocular video with generativepriors. In IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 45414551, 2023. 3",
  "V. Blanz and T. Vetter. A morphable model for the synthesisof 3D faces. In Proceedings of ACM SIGGRAPH, pages 187194, 1999. 2": "Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas JGuibas, Jonathan Tremblay, Sameh Khamis, et al.Effi-cient geometry-aware 3D generative adversarial networks.In IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1612316133, 2022. 2, 5, 6 SreenithyChandran,YannickHold-Geoffroy,KalyanSunkavalli, Zhixin Shu, and Suren Jayasuriya. Temporallyconsistent relighting for portrait videos. In IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages719728, 2022. 5",
  "Liang-Chieh Chen, George Papandreou, Florian Schroff, andHartwig Adam. Rethinking atrous convolution for seman-tic image segmentation. arXiv preprint arXiv:1706.05587,2017. 4": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. ImageNet: A large-scale hierarchical im-age database. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 248255, 2009. 4 Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kot-sia, and Stefanos Zafeiriou. ArcFace: Additive angular mar-gin loss for deep face recognition. IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 44(10):59625979,2022. 5",
  "Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.GRAM: Generative radiance manifolds for 3D-aware imagegeneration. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2022. 2": "Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli.Image quality assessment: Unifying structure and texturesimilarity. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 44(5):25672581, 2022. 7 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An imageis worth 16x16 words: Transformers for image recognitionat scale. International Conference on Learning Representa-tions, 2021. 4 Fan Fei, Yean Cheng, Yongjie Zhu, Qian Zheng, Si Li, GangPan, and Boxin Shi. SPLiT: Single portrait lighting estima-tion via a tetrad of face intrinsics. IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 46(02):10791092,2024. 2",
  "Yao Feng, Haiwen Feng, Michael J. Black, and TimoBolkart.Learning an animatable detailed 3D face modelfrom in-the-wild images. ACM Transactions on Graphics,40(8), 2021. 2, 6, 7": "Anna Fruhstuck, Nikolaos Sarafianos, Yuanlu Xu, PeterWonka, and Tony Tung. VIVE3D: Viewpoint-independentvideo editing using 3D-aware GANs. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, 2023. 3 Guy Gafni, Justus Thies, Michael Zollhofer, and MatthiasNiener.Dynamic neural radiance fields for monocular4D facial avatar reconstruction. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 86498658, 2021. 7 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-nications of the ACM, 63(11):139144, 2020. 2",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-sion probabilistic models. Advances in Neural InformationProcessing Systems, 33:68406851, 2020. 2": "Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, YiyingTong, and Xiaoming Liu.Towards high fidelity face re-lighting with realistic shadows. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, 2021. 2, 6, 7,8 Kaiwen Jiang, Shu-Yu Chen, Hongbo Fu, and Lin Gao.NeRFFaceLighting: Implicit and disentangled face lightingrepresentation leveraging generative prior in neural radiancefields. ACM Transactions on Graphics, 42(3), 2023. 2, 3, 4,5, 6, 7, 8 Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 44014410, 2019. 7 Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,Jaakko Lehtinen, and Timo Aila. Analyzing and improvingthe image quality of StyleGAN. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 81108119, 2020. 3, 4",
  "Michael Niemeyer and Andreas Geiger.GIRAFFE: Rep-resenting scenes as compositional generative neural featurefields. In IEEE/CVF Conference on Computer Vision andPattern Recognition, 2021": "Xingang Pan, Xudong Xu, Chen Change Loy, ChristianTheobalt, and Bo Dai. A shading-guided generative implicitmodel for shape-accurate 3D-aware image synthesis. In Ad-vances in Neural Information Processing Systems, 2021. 2 Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-tian Haene, Sofien Bouaziz, Christoph Rhemann, Paul De-bevec, and Sean Fanello. Total Relighting: Learning to re-light portraits for background replacement. ACM Transac-tions on Graphics, 40(4):121, 2021. 2, 7 Foivos Paraperas Papantoniou, Alexandros Lattas, StylianosMoschoglou, and Stefanos Zafeiriou.Relightify:Re-lightable 3D faces from a single image via diffusion models.In International Conference on Computer Vision, 2023. 2 Haonan Qiu, Zhaoxi Chen, Yuming Jiang, Hang Zhou, Xi-angyu Fan, Lei Yang, Wayne Wu, and Ziwei Liu. ReliTalk:Relightable talking portrait generation from a single video.In International Journal of Computer Vision, 2024. 2, 6, 8 Ravi Ramamoorthi and Pat Hanrahan. An efficient repre-sentation for irradiance environment maps. In Proceedingsof the 28th Annual Conference on Computer Graphics andInteractive Techniques, page 497500, 2001. 3",
  "Katja Schwarz, Yiyi Liao, Michael Niemeyer, and AndreasGeiger. GRAF: Generative radiance fields for 3D-aware im-age synthesis. In Advances in Neural Information ProcessingSystems, 2020. 2": "Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, JueWang, and Yebin Liu. IDE-3D: Interactive disentangled edit-ing for high-resolution 3D-aware portrait synthesis. ACMTransactions on Graphics, 41(6):110, 2022. Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, YongZhang, Hongwen Zhang, and Yebin Liu. Next3D: Genera-tive neural texture rasterization for 3D-aware head avatars.In IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023. 2",
  "Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, andDaniel Cohen-Or. Designing an encoder for StyleGAN im-age manipulation. ACM Transactions on Graphics, 40(4),2021. 3": "Alex Trevithick, Matthew Chan, Michael Stengel, Eric R.Chan, Chao Liu, Zhiding Yu, Sameh Khamis, ManmohanChandraker, Ravi Ramamoorthi, and Koki Nagano. Real-time radiance fields for single-image portrait view synthesis.In ACM Transactions on Graphics, 2023. 3, 4, 7 Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, JianminBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, FangWen, Qifeng Chen, et al. RODIN: A generative model forsculpting 3D digital avatars using diffusion. In IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 45634573, 2023. 2",
  "Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong JaeLee.Giraffe hd: A high-resolution 3d-aware generativemodel. In IEEE/CVF Conference on Computer Vision andPattern Recognition, 2022. 2": "Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz,Ming-Yu Liu, and Ting-Chun Wang.Learning to relightportrait images via a virtual light stage and synthetic-to-realadaptation. ACM Transactions on Graphics, 2022. 2, 7 Fei Yin, Yong Zhang, Xuan Wang, Tengfei Wang, Xiaoyu Li,Yuan Gong, Yanbo Fan, Xiaodong Cun, Oztireli Cengiz, andYujiu Yang. 3D GAN inversion with facial symmetry prior.In IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023. 3 Yu Yin, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang,Xin Tong, and Yun Fu. NeRFInvertor: High fidelity NeRF-GAN inversion for single-shot real image animation.InIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 85398548, 2023. 3 Ziyang Yuan, Yiming Zhu, Yu Li, Hongyu Liu, and ChunYuan.Make encoder great again in 3D GAN inversionthrough geometry and occlusion-aware encoding. In Inter-national Conference on Computer Vision, 2023. 3 Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, andLan Xu. Neural video portrait relighting in real-time via con-sistency modeling. In International Conference on ComputerVision, pages 802812, 2021. 2, 7 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,and Oliver Wang. The unreasonable effectiveness of deepfeatures as a perceptual metric. In IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2018. 4, 5, 7"
}