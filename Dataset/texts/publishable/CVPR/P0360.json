{
  "Abstract": "The emergence of foundation models, such as the Seg-ment Anything Model (SAM), has sparked interest inParameter-Efficient Fine-Tuning (PEFT) methods that tai-lor these large models to application domains outside theirtraining data. However, different PEFT techniques modifythe representation of a model differently, making it a non-trivial task to select the most appropriate method for thedomain of interest. We propose a new framework, Mixture-of-PEFTs methods (MoPEFT), that is inspired by tradi-tional Mixture-of-Experts (MoE) methodologies and is uti-lized for fine-tuning SAM. Our MoPEFT framework incor-porates three different PEFT techniques as submodules anddynamically learns to activate the ones that are best suitedfor a given data-task setup. We test our method on the Seg-ment Anything Model and show that MoPEFT consistentlyoutperforms other fine-tuning methods on the MESS bench-mark.",
  ". Introduction": "The machine learning research community has witnessedan explosion in the development of foundation models inrecent years, such as CLIP , GPT-4 , and PaLM .More recently, the Segment Anything Model (SAM) , apromptable model pretrained on over 1 billion masks and 11million images, emerged as a foundation model for imagesegmentation. It has demonstrated performance compara-ble to state-of-the-art approaches in multiple applicationsrelated to segmentation tasks. Moreover, SAMs zero-shotand few-shot capabilities have garnered significant atten-tion across multiple domains . However, prior works have shown that despite noteworthy proficiency insegmenting real-world objects in natural images, SAM hasdifficulty with objects outside its training domain.Following the pretraining-fine-tuning paradigm , itis desirable to fine-tune SAM in order to enhance its perfor-mance in the application domain of interest. However, fine-tuning foundation models can be costly due to their large number of parameters.This motivates the developmentof fine-tuning methods with the goal of achieving compa-rable performance to full fine-tuning while employing asfew trainable parameters as possible. Interest in Parameter-Efficient Fine-Tuning methods (PEFT) has increased signif-icantly since the advent of foundation models .Recent studies have shown that some PEFT meth-ods are more effective at fine-tuning with the objective of re-ducing overfitting on the target domain- especially in data-sparse environments. However, we find that combining dif-ferent PEFT methods often yields better results without asubstantial loss in efficiency. This is because different tech-niques operate on different parts of the transformer archi-tecture, making it possible to use more than one techniqueat a time.In light of this, we propose a new framework, calledMixture-of-PEFTs (MoPEFT), that incorporates differentPEFT methods as submodules and learns to dynamicallyactivate the fine-tuning method(s) that best suit the dataor task of interest. Inspired by the Mixture-of-Experts ap-proach , MoPEFT switches between differentPEFT methods using a gating mechanism that learns to fa-vor the method that positively contributes to a given task.In addition, since the number of parameters introduced byeach PEFT is very small, e.g. compared to the entire SAMarchitecture, combining multiple PEFT methods has littleeffect on the efficiency of our framework. In this paper, weinclude the three most commonly used PEFT techniques-LoRA , Prefix Tuning , and Adapters . Our ex-periments shed light on the effectiveness of these methodsacross multiple domains, and their effectiveness when com-bined together in our MoPEFT framework.Our contributions can be outlined as follows: (i) Weconduct a comprehensive survey of the widely-used PEFTmethods and benchmark their performance across multipledomains; (ii) We introduce our MoPEFT framework, whichincorporates multiple PEFT methods as submodules andlearns to dynamically activate or deactivate the appropri-ate submodule based on the given task; and (iii) We showthat our MoPEFT framework achieves better performance",
  ". Parameter Efficient Fine-Tuning Methods": "Low Rank Adaptation (LoRA). Low Rank Adaptation(LoRA) exploits the low rank structure inherent in deeplearning models to align them to specific tasks. It worksby introducing trainable low-rank matrices and combinesthem with the original matrices in the multi-head self-attention (MHSA) blocks. The pre-trained weight matrixW0 Rdk is updated as W0 + W, where W Rdk is a low-rank matrix decomposed as W = BA. Here,B Rdr, A Rrk and the rank r << min(d, k). Dur-ing fine-tuning, the pre-trained weights remain frozen, andW serves as the trainable parameter. The decompositionof W = BA as a product of two low-rank matrices effec-",
  "tively reduces the memory and computational cost of fine-tuning": "Prefix Tuning.Prefix Tuning prepends a numberof tunable, task-specific vectors to the input of the multi-head self-attention in each Transformer block, which orig-inal tokens can attend to as if they were virtual tokens.This method was originally developed for natural languageprocessing and was eventually extended to vision applica-tions as Deep Visual Prompt Tuning (VPT-Deep) . Weuse VPT-Deep for all our experiments and call it PrefixTuning to maintain uniformity with literature in the field.We denote the original sequence length L0, the number oftunable vectors (i.e., prefix length) as L, and the Trans-former layer input as hin RDhiddenL0. First, three lin-ear projections, WQ, WK, WV RDhiddenDhidden trans-form hin into Query (Q), Key (K), and Value (V ) ma-trices.The two prefix matrices PK RDhiddenL andPV RDhiddenL are pre-pended to K and V . The pre-fix matrix P is reparametrized by a feedforward network tostabilize the optimization procedure. Adapters. Adapters align the model to the target taskby adding a trainable MLP after the feedforward layer ineach Transformer block. The MLP consists of a down+upprojection that condenses and recovers the size of the origi-nal hidden token space. Mathematically, we can denote theAdapter operation as",
  ". Task Formulation": "Given a very large model M, which cannot be efficientlyfine-tuned due to computational costs, assume we have aselection of PEFT methods FT [ft1, ft2...ftn], each ofwhich have negligible trainable parameters compared to M(ie. n FT << |M|). Our goal is to design a frameworkthat incorporates [ft1, ft2...ftn] as individual, independentsubmodules and learns to dynamically activate different ftibased on different data-task scenarios. This would ensurethat a singular framework would be capable of achievingoptimal results in terms of both accuracy and efficiencywithout permuting through all data-task combinations forevery datapoint.",
  "Intuition.During the analysis of individual PEFT meth-ods, we observed that different methods often involve dif-ferent parts of the Vision Transformer model in the image": "encoder of SAM. For instance, Adapters add an MLP afterthe feedforward layer in each Transformer block, while Pre-fix Tuning prepends tunable tensors before the multi-headself-attention layers. This unique property makes it possi-ble to essentially combine multiple PEFT techniques in theproposed framework without interfering with each other.Keeping the above in mind, we propose a unifiedMoPEFT framework which takes a hybrid approach by in-corporating multiple PEFT methods as submodules.Ata high level, MoPEFT shows better performance than itsindividual components due to two main reasons. Firstly,MoPEFT learns to dynamically access individual submod-ules based on the given task. This means that for a givendata-task sample, a particular PEFT method may be allot-ted different weights or turned off entirely to ensure op-timal performance in all cases.Secondly, we find thatour MoPEFT framework generally outperforms the best-performing individual PEFT technique in multiple domains,suggesting that there may be benefits due to compoundingeffects that lead to better model effectiveness, as multiplePEFT techniques are used together. We show how we in-corporate these different techniques under one frameworkin the next section. Gating Mechanism. To achieve fine-grained control overthe activation of the individual PEFT techniques that makeup our MoPEFT framework, we take inspiration from cur-rent Mixture-of-Experts (MoE) methods . Similarto the Sparsely-Gated-MoE method , we add a gatingmechanism that dynamically links different PEFT methodsto the relevant layers in the image encoder of SAM. As de-picted in , we add three trainable gates, one for eachPEFT technique. Intuitively, if a particular PEFT techniqueis useful for a given data-task setup, then the output of thecorresponding gate would be set to high. This would ensurethat the specific PEFT plays a more important role duringthe execution.For LoRA, our gate is not added directly in the form ofthe traditional MLP architecture as seen in MoE literature.Instead, we make use of the inherent scaling factor, al-ready present in the LoRA architecture as a pseudo-gatingmechanism. A higher assigns more weight to the LoRAactivations, while a lower makes the effect of LoRA neg-ligible. Thus, we already have a gating mechanism in place.To integrate this with our broader framework, we make thescaling factor learnable by using a feedforward network in-stead of specifying the constant manually.For Prefix Tuning, we design a gating function GP (0, 1) that is applied to the Prefix vectors PK and PV keep-ing the representations of the original Key and Value tokensK and V intact. GP is estimated using another feedforwardnetwork which takes in the input provided to the specificViT layer.For Adapters, we make use of the residual connection between the Adapter MLP and the feedforward network ofthe ViT Transformer block. This connection is responsiblefor summing up the input to the Adapter MLP. Our AdapterGating Function GA (0, 1) estimates the importance ofthe Adapter MLP using a feedforward network with sig-moid activation. The Adapter MLP is essentially bypassedif GA = 0.",
  ". Experiments": "Datasets. We employ the Multi-domain Evaluation of Se-mantic Segmentation (MESS) benchmark , which mea-sures the mIOU score of models performing semantic seg-mentation tasks on 22 datasets spread across five majordomains- General, Earth Monitoring, Medical Imaging, En-gineering, and Agriculture and Biology. For brevity, wepresent results on only the first three domains. The datasetsare not distributed evenly across all domains (for instance,General has six datasets while Engineering has three) butwe examine at performance on individual dataset as op-posed to collective domains. Implementation Details. We use the Segment AnythingModel for all our fine-tuning and experiments. Thetraditional implementation of SAM consists of an imageencoder (we use ViT-B for our experiments), a PromptEncoder and a Mask Decoder. However, to better equipSAM for end-to-end semantic segmentation, we freeze thePrompt Encoder, always providing constant prompt tokensto the Mask Decoder when fine-tuning. Additionally, weapply full fine-tuning to the Mask Decoder, since it is anextremely lightweight module.For consistency, we include public implementations forall PEFT methods in our framework. We use a batch size of4 and the Adam optimizer with a learning rate of 1x104 asa default with a weight decay of 1x104. All PEFT meth-ods are implemented in the same codebase to ensure a faircomparison. We largely follow the default PEFT-specifichyperparameters and keep them unchanged across domainsfor uniformity. Unless otherwise specified, we set the LoRArank r = 8 prefix length L = 20, and the adapter bottlenecksize Dmid = 64 for our experiments. Comparison with State-of-the-art. shows theperformance of our MoPEFT framework against the threemost commonly used PEFT methods, i.e., LoRA , Pre-fix Tuning (VPT Deep) , and Adapters . We comparethese methods against a vanilla SAM framework (Baseline),fully fine-tuning the SAM decoder on the target dataset (de-coderFT), and simple Visual Prompt Tuning (VPT) ,which is similar to Prefix Tuning except that the tunabletensors are added to only the first Transformer block as op-posed to all of them. We measure the Mean Intersection-over-Union (mIOU) to compare performance across allmethod and datasets.",
  ". Comparison of our MoPEFTs framework with fine-tuned SAM variants across multiple domains. Scores shown are mIOU scores": "Analysis of Gating Mechanism. The results in this sec-tion provide a better understanding of what the MoE learnsduring fine-tuning. To gain a better understanding of ourgating mechanism, we conduct an analysis by tracking thefrequency of the selection of each PEFT technique acrossdifferent datasets during inference. We present our detailedresults in . Most notable in our results is the fact that differentdatasets give more preference to different PEFT techniques.For instance, the graph depicting iSAID (an EarthMonitoring dataset in the MESS benchmark), tends toselect LoRA more often than the other two PEFT methods.Similarly, Kvasir-Instrument (a Medical Imaging datasetin the MESS benchmark ) tends to select Adapters moreoften, instead of LoRA or Prefix Tuning. This observationsupports our initial claim that our gating mechanism learnsto dynamically select appropriate PEFT techniques basedon the provided data-task setup. This reinforces the signif-icance of the MoPEFT framework in tailoring its selectionto the unique characteristics of diverse datasets enhancingits effectiveness across different domains.",
  ". Conclusion": "In this paper, introduce a new framework, MoPEFT, that isinspired by Mixture-of-Experts and dynamically learns toactivate a particular PEFT technique based on a given data-task setup. We also present a comprehensive study of thetop three PEFT techniques and compare their effectivenesswith our proposed framework for fine-tuning the SegmentAnything Model (SAM). Our results show that MoPEFTusually outperforms all traditional fine-tuning techniques onmultiple datasets across different domains.",
  "Acknowledgements": "This research was partly supported by the Air Force Officeof Scientific Research (AFOSR) under SBIR grant FA9550-22-P-0009 with Intelligent Fusion Technology, AFOSRgrant FA9550-20-1-0039, and the Empire State Develop-ments Division of Science, Technology and Innovationthrough the University of Rochester Center of Excellencein Data Science. The authors would like to thank RIT Re-search Computing for making computing resources avail-able for experimentation. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXiv preprint arXiv:2303.08774,2023. Benedikt Blumenstiel, Johannes Jakubik, Hilde Kuhne, andMichael Vossing. What a mess: Multi-domain evaluation ofzero-shot semantic segmentation. Advances in Neural Infor-mation Processing Systems, 36, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, SebastianGehrmann, et al.Palm: Scaling language modeling withpathways. Journal of Machine Learning Research, 24(240):1113, 2023. Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding,Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. Onthe effectiveness of adapter-based tuning for pretrained lan-guage model adaptation. arXiv preprint arXiv:2106.03164,2021.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.Lora: Low-rank adaptation of large language models. arXivpreprint arXiv:2106.09685, 2021": "Isprs ISPRS. 2d semantic labeling contest, 2014. Debesh Jha, Sharib Ali, Krister Emanuelsen, Steven AHicks, Vajira Thambawita, Enrique Garcia-Ceja, Michael ARiegler, Thomas de Lange, Peter T Schmidt, Havard D Jo-hansen, et al. Kvasir-instrument: Diagnostic and therapeu-tic tool segmentation dataset in gastrointestinal endoscopy.In MultiMedia Modeling: 27th International Conference,MMM 2021, Prague, Czech Republic, June 2224, 2021,Proceedings, Part II 27, pages 218229. Springer, 2021. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-sual prompt tuning. In European Conference on ComputerVision, pages 709727. Springer, 2022.",
  "Ye Lyu, George Vosselman, Gui-Song Xia, Alper Yilmaz,and Michael Ying Yang. Uavid: A semantic segmentationdataset for uav imagery. ISPRS journal of photogrammetryand remote sensing, 165:108119, 2020": "Amirreza Mahbod, Gerald Schaefer, Benjamin Bancher,Christine Low, Georg Dorffner, Rupert Ecker, and IsabellaEllinger. Cryonuseg: A dataset for nuclei instance segmenta-tion of cryosectioned h&e-stained histological images. Com-puters in biology and medicine, 132:104349, 2021. Gonzalo Mateo-Garcia, Joshua Veitch-Michaelis, LewisSmith, Silviu Vlad Oprea, Guy Schumann, Yarin Gal,Atlm Gunes Baydin, and Dietmar Backes. Towards globalflood mapping onboard low cost satellites with machinelearning. Scientific reports, 11(1):7249, 2021. Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, JichenYang, Nicholas Konz, and Yixin Zhang. Segment anythingmodel for medical image analysis: an experimental study.Medical Image Analysis, 89:102918, 2023. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, andPeter Kontschieder. The mapillary vistas dataset for semanticunderstanding of street scenes. In Proceedings of the IEEEinternational conference on computer vision, pages 49904999, 2017. Hien D Nguyen and Faicel Chamroukhi. Practical and theo-retical aspects of mixture-of-experts modeling: An overview.Wiley Interdisciplinary Reviews: Data Mining and Knowl-edge Discovery, 8(4):e1246, 2018. Lucas Prado Osco, Qiusheng Wu, Eduardo Lopes de Lemos,Wesley Nunes Goncalves, Ana Paula Marques Ramos,Jonathan Li, and Jose Marcato Junior. The segment anythingmodel (sam) for remote sensing applications: From zero toone shot. International Journal of Applied Earth Observa-tion and Geoinformation, 124:103540, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-perative style, high-performance deep learning library. Ad-vances in neural information processing systems, 32, 2019. Jonas Pfeiffer, Andreas Ruckle, Clifton Poth, Aishwarya Ka-math, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, andIryna Gurevych.Adapterhub: A framework for adaptingtransformers. arXiv preprint arXiv:2007.07779, 2020. Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan,Rao Muhammad Anwer, Salman Khan, Timothy Baldwin,and Hisham Cholakkal. Bimedix: Bilingual medical mixtureof experts llm. arXiv preprint arXiv:2402.13253, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021.",
  "son Murphy.Floodnet: A high resolution aerial imagerydataset for post flood scene understanding.IEEE Access,9:8964489654, 2021": "Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Guidedcurriculum model adaptation and uncertainty-aware evalua-tion for semantic nighttime image segmentation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 73747383, 2019. Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Guidedcurriculum model adaptation and uncertainty-aware evalua-tion for semantic nighttime image segmentation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 73747383, 2019. Constantin Seibold, Simon Rei, Saquib Sarfraz, Matthias AFink, Victoria Mayer, Jan Sellner, Moon Sung Kim, Klaus HMaier-Hein, Jens Kleesiek, and Rainer Stiefelhagen.De-tailed annotations of chest x-rays via ct projection for reportunderstanding. arXiv preprint arXiv:2210.03416, 2022.",
  "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, AndyDavis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-geously large neural networks: The sparsely-gated mixture-of-experts layer, 2017": "Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, JingGao, Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix:Mixture-of-adapter for parameter-efficient tuning of largelanguage models.arXiv preprint arXiv:2205.12410, 1(2):4, 2022. Syed Waqas Zamir, Aditya Arora, Akshita Gupta, SalmanKhan, Guolei Sun, Fahad Shahbaz Khan, Fan Zhu, LingShao, Gui-Song Xia, and Xiang Bai. isaid: A large-scaledataset for instance segmentation in aerial images. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops, pages 2837, 2019. Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven CHHoi, and Qianru Sun. A large-scale benchmark for food im-age segmentation. In Proceedings of the 29th ACM interna-tional conference on multimedia, pages 506515, 2021."
}