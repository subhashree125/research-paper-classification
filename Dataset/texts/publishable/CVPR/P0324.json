{
  "Abstract": "Mitigating hallucinations in large vision-language mod-els (LVLMs) remains an open problem. Recent benchmarksdo not address hallucinations in open-ended free-form re-sponses, which we term Type I hallucinations. Instead,they focus on hallucinations responding to very specificquestion formatstypically a multiple-choice response re-garding a particular object or attributewhich we termType II hallucinations. Additionally, such benchmarksoften require external API calls to models which are sub-ject to change. In practice, we observe that a reduction inType II hallucinations does not lead to a reduction in TypeI hallucinations but rather that the two forms of halluci-nations are often anti-correlated. To address this, we pro-pose THRONE, a novel object-based automatic frameworkfor quantitatively evaluating Type I hallucinations in LVLMfree-form outputs. We use public language models (LMs) toidentify hallucinations in LVLM responses and compute in-formative metrics. By evaluating a large selection of recentLVLMs using public datasets, we show that an improvementin existing metrics do not lead to a reduction in Type I hal-lucinations, and that established benchmarks for measuringType I hallucinations are incomplete. Finally, we provide asimple and effective data augmentation method to reduceType I and Type II hallucinations as a strong baseline.",
  "Introduction": "This paper proposes a benchmark to evaluate hallucinationsby large vision-language models (LVLMs) when generat-ing free-form responses, specifically detailed descriptions,based on a given image.The rapid advancement in large language models(LLMs) has pushed the development of large vision-language models (LVLMs). LVLMs take input text and images and generatetext responses to enable multi-modal perception and com-",
  "*Work conducted during an internship at AmazonCorresponding author": "prehension.LVLMs are largely built on LLMs and therefore inheritboth their advantages and their disadvantages. LLMs havebeen shown to produce hallucinations , generatedtext responses that are coherent and plausible but factuallyincorrect. LVLMs echo this behavior with generated textcontradicting with the visual or text input . Halluci-nations prevent the use of LVLMs in safety-critical situa-tions and therefore evaluating and mitigating hallucinationsin LVLMs is crucial for their deployment in such settings.Determining the presence and cause of hallucinations inLVLMs remains an open question .We divide LVLM hallucinations into two types. TypeI hallucinations occur in response to open-ended questionswith a very large set of possible responses e.g. What ishappening in this image?. Type II hallucinations are in-correct responses to a factual question regarding a spe-cific concept about the image with a fixed set of optionssuch as yes/noe.g. Is there a traffic light in thisimage? illustrates the difference between these twotypes of hallucination.Reducing hallucinations in bothcases is required for useful, multi-purpose LVLMs. How-ever, later in we observe that the same LVLM cangive contradicting answers when being evaluated for Type Ivs. Type II hallucinations. This implies that measuring andreducing one type does not necessarily reduce the other.Existing works to evaluate LVLMs often avoid directquantification of hallucinations and instead develop com-prehensive benchmarks that judge various other desirableabilities such as: optical character recognition, fine-grainedrecognition and attribute detection . The ex-tent of hallucinations in these benchmarks is obfuscated,since it is only one of many factors influencing other met-rics. It requires human effort to inspect individual predic-tions. There are two major established works which specif-ically develop a benchmark for evaluating hallucinationsin vision-language models: POPE and CHAIR ,which we discuss in detail in Sec. 2. However, we observethey both have shortcomings in effectively evaluating hallu-cinations:",
  ".POPE: Questions with specific concepts prompt anLVLM directly to evaluate Type II hallucinations .Hand-crafted rules parse LVLM responses to give yes/no labels": "POPE is a recent work addressing Type II halluci-nations with respect to object classes. However, we findType I and Type II hallucinations are disconnected, andthat POPE gives an incomplete picture on LVLM hallucina-tions. Moreover, POPE systematically under-samples neg-ative object categories leading to a large underestimation ofType II hallucinations (see Sec. 5.4).CHAIR does address Type I hallucinationsestablishing object category hallucination in short imagecaptions using simple text matching. However, CHAIR isnot suited to current LVLMs because the simple text match-ing it employs cannot comprehend abstract or hypotheticalconcepts present in todays LVLMs (see ). Further,hand-crafted rules for each set of classes are required forusable text matching, and trivial model answers can attain aperfect CHAIR score.To address the issues of current LVLM hallucina-tion benchmarks, we propose THRONE (Text-from-imageHallucination Recognition with Object-probes for open-eNded Evaluation). THRONE leverages language models(LMs) to evaluate Type I hallucinations in free-form, open-ended image descriptions with respect to a pre-defined ob-ject vocabulary of interest. By utilizing LMs, THRONE isable to accurately judge whether an object mentioned in anLVLM response is implied to exist in the image or is ab-stractly mentioned with no implication about its existence(see ).Moreover, in THRONE, we provide easy access to ourbenchmark, by leveraging open-source LMs that can run oncommon GPUs, instead of relying on closed-source com-mercial models that are subject to arbitrary change, asdone in other works . Through combining multi-ple open-source LMs, we mitigate any single-model biasesin judging hallucinations when calculating Type-I halluci-nation scores with THRONE.We make four contributions: first, we establish an ac-curate and accessible benchmark to quantitatively evalu-ate object hallucinations in free-form responses, leverag-ing LMs to judge the existence of Type I hallucinationsquantitatively showing half the judgement errors of CHAIR; second, we evaluate a number of current LVLMs onTHRONE and demonstrate that observed progress in re-ducing Type II hallucinations do not translate to a corre-sponding reduction in Type I hallucinations; third, we showa recent method, POPE, is inaccurately capturing the ex-tent of Type II hallucination discovery due to its samplingstrategy; we mitigate this issue in our implementation ofTHRONE while presenting results for a complete versionof POPE; and fourth, we provide a simple augmentation ofvisual instruction tuning data which significantly improvesperformance on Type I hallucinations while maintaining orimproving Type II hallucination performance.",
  "Related Work": "Hallucination Benchmarks for LVLMs.In responseto the development of LVLMs, few evaluation bench-marks focusing on hallucinations have been introduced.CHAIR , one of the first works to assess hallucinations,is designed for short image captions. CHAIR uses a fixedset of object classes (extended with their synonyms) as aset of text strings to find predicted object classes in im-age captions via exact text matching. Subsequently, eachclass matched in the caption is compared to the COCO captions and bounding box annotations to establish ob-ject hallucinations. CHAIR was developed prior to currentinstruction-tuned LVLMs which produce long free-form re-sponses (10 longer than image captions) with a diversevocabulary, limiting its applicability to modern LVLMs. Asshown in , exact text matching in CHAIR is prone toincorrectly matching the vocabiliary classes with abstractconcepts in the free-form response and the and synonymsof the class list must be manually selected to prevent confu-sion during evaluation. For example, in CHAIR the wordchair will match all responses for the phrase toiletseat, because the exact text matching classifies seat asthe COCO class chair. Finally, CHAIR metrics com-pare the overall number of predicted objects to the overallnumber of predicted objects judged to be hallucinationsignoring the recall of ground-truth objects and the distri-bution of object classes. This means that a single correct",
  "prediction across the entire evaluation dataset along with ageneric response otherwise e.g. A natural scene, achievesa perfect score ( 0 hallucinated objects": "1 predicted objects= 0.0). See the Supple-mentary Material for a full overview of CHAIR. In con-trast, THRONE uses pre-trained LMs that go beyond directsynonym matching, to automatically judge the existence ofconcepts and hallucinations in free-form responses. In ad-dition, our method considers both recall and precision toyield a holistic benchmark and does not require any man-ual curation of synonyms.THRONE and CHAIR bothevaluate Type I hallucinationshallucinations in responseto concept-neutral prompts e.g. Describe this image indetail.POPE is a recently proposed benchmark to evaluateobject hallucinations in LVLMsspecifically Type II hal-lucinations, in which an LVLM is directly queried with ayes-no question regarding the existence of a particular ob-ject of the form: Is there {a/an} {object class name}in the image?. The LVLM response is parsed using sim-ple rules to determine whether a Type II hallucination hasoccurred. Precision and recall metrics are compiled usingthe parsed LVLM responses and the ground-truth annota-tion data. Despite focusing on measuring hallucinations,POPE only queries an LVLM with 3 positive and 3 sam-pled negative questions per COCO image i.e. the evaluationis artificially balanced. This means many potential halluci-nations with respect to the COCO categories are not cap-tured by their method. In Sec. 5.4, we show POPE dra-matically underestimates Type II hallucinations and presentthe results of a complete version. Our method, THRONE,evaluates the prevalence of Type I hallucinations, which weobserve are disjoint to Type II hallucinations.Comprehensive Benchmarks for LVLMs have recentlygrown in number. MMBench and MM-Vet as-sess various aspects of LVLM performance such as: colorperception, celebrity recognition, and numerical calcula-tion. However many of these works integrate evolving APIswhich are modified often (or even discontinued) and are in-herently stochastic. Over time this greatly reduces the con-sistency of these benchmarks. Exceptions are MME and SEED-Bench , but the impact of Type II hallucina-tions on final metrics is conflated with a number of otheraspects of model performance. Our method, THRONE, di-rectly addresses Type I hallucinations, only making use ofopen-source language models and datasets.Large Vision Language Models (LVLMs) have rapidlydeveloped by harnessing advancements in large languagemodels (LLM) and by directly integrating pre-trained LLMs into their architectures. In contrast to earliervision-language models such as CLIP , LVLMs aregenerally comprised of a pretrained LLM and image en-coder, aligned with a connector module of varying com-plexity. Some works are highlighted here. Frozen is",
  "near it": ". Type I vs. Type II Hallucinations: (Top) LVLMsprompted with concept-neutral instructions produce Type I hal-lucinations. (Bottom) Instructions specifying a concept produceType II hallucinations. Examples from LLaVA-v1.5 . an early work fine-tuning the vision encoder to dynamicallyprefix the prompt to a frozen LLM. Flamingo combinesvisual and language features using cross-attention layers inan otherwise frozen LLM. BLIP-2 uses a frozen im-age encoder to learn a Querying-Transformer (Q-Former)on image-text pairs that is used as the connector. This ar-chitecture is adapted in for dialogue via training withvisual instruction tuning.LLaVA uses COCO annotations and GPT-4 to generate visual instructiontuning data in a plain-text pipeline. Combining this gener-ated data with standard VQA datasets (VQAv2 etc.) furtherboosts performance . Different works modify the train-ing approach by using efficient adaptation multipletraining stages or introduce the use of discrete tokensfor localization . We note however, that most of theseworks evaluate performance on traditional vision-languagedatasets like VQAv2 , which do not consider the extentof hallucinations, a known problem with LLMs .",
  "THRONE": "Recap of existing methods. POPE and other bench-marks (MME , MMBench ) directly query LVLMswith a restricted desired answer space, e.g. yes-no (MME,POPE) and A-B-C-D multiple choice (MMBench), asshown in .These benchmarks only consider suchshort answer formats, whereas a key quality of LVLMs isin their ability to generate free-form coherent text. More-over, POPE, which addresses Type II hallucinations, under-samples negative classes meaning hallucinations are dra-matically underestimated (see Sec. 5.4 and ). In con-trast, we skip class subsampling and enumerate all classesfor every image, ensuring a full evaluation of Type I hallu-cinations of the ground-truth classes.CHAIR also evaluates Type I hallucinations, butwas developed when typical vision-language models couldonly generate short and simple captions similar to those inCOCO Captions . Moreover, it lacks accurate compre-hension of responses (see the right side of ) and ig-",
  "detail": "The image showcases a fruit stand at a grocery store, featuring a variety of fruits on display. There are several bunches of bananas, with some placed in the foreground and others in the background. The bananas are arranged in different sections, creating an appealing presentation for customers. In addition to the bananas, there are also apples and oranges on display. The apples are located towards the left side of the image, while the oranges are placed in the middle and right side of the stand. The fruits are well-organized and presented in an attractive manner, making it an inviting sight for shoppers.",
  "- Type II Hallucination - Hypothetical Content (not a Hallucination)": ". A Comparison of POPE, CHAIR and THRONE: Directly querying LVLMs for object existence (person, banana etc.) usingconcept-specific instructions, as in POPE (bottom left), does not produce the same hallucinations as using concept-neutral instructions(right). We highlight the Type I hallucinations in orange. CHAIR relies on exact text matching to a fixed set of objects and synonyms,thus incorrectly labels customers and shoppers as hallucinations, highlighted in red. THRONE is designed for the rich vocabularyand the free-form generations of modern LVLMs by harnessing LMs to establish object existence. By using an LM to pass judgement, ourevaluation correctly captures customers and shoppers as hypothetical content in the free-form generation. nores the recall of ground-truth objects. In Sec. 5.5, we de-scribe quantitative evaluations, using a human oracle, whichdemonstrate THRONE halves the rate of hallucination mis-judgement in CHAIR. See Sec. 2 and the SupplementaryMaterial for more details on CHAIR and its shortcomings. shows an overview of the three aforementioned meth-ods: POPE, CHAIR and our method, THRONE.",
  "Evaluating Hallucinations with THRONE": "To address these limitations, we propose a framework,THRONE, shown in , to evaluate the prevalence ofType I hallucinations in LVLM responses conditioned onan image and a neutral text prompt.For each image in a labeled dataset, I, addressing a set ofclasses, C, the LVLM is queried with the same instruction: Describe this image in detail., regardless of imagecontent. The LVLM response, which is expected to be longfree-form text containing an image description, is generatedand stored. Next, a publicly available, open-source, exter-nal language model (LM) performs abstractive question an-swering (AQA) using the LVLM response as context anda question of the form: Please answer yes or no.Isthere {a/an} {object class name} in this image? orsimilar, for every class in C (right side of ). By se-lecting an appropriate LM and using a simple prompt tem-plate (see Sec. 4 for specific details), we ensure the AQAresponse is either yes or noour method does not requireany additional parsing. This is in contrast to other workswhich require added parsing or interpretation by a closed-source model.After performing AQA on each response generated bythe LVLM for every class in C, we obtain an array of pre-",
  "Y {0, 1}|I||C|(2)": "Using these two arrays, we calculate four metrics: (1)Overall Precision, PALL; (2) Overall Recall, RALL; (3)Class-wise Precision, PCLS; and (4) Class-wise Recall,RCLS. Overall metrics are calculated in a class-agnosticmanner.Class-wise metrics are calculated in a class-conscious manner by computing precision and recall foreach category separately and then averaging. This followscommon practice in object detection and instance segmen-tation .False positives in LVLMs reduce precision and are dom-inated by hallucinationsprecision indicates the extent ofType I hallucinations in LVLM responses. The recall met-rics inform the level of class coverage by an LVLM whenproducing image descriptions. The class-wise metrics givea general measure of performance as the overall metricsare skewed towards the most common categories. A com-mon way to combine precision, P, and recall, R, metrics isthrough the generalized F score, F:",
  "Prior work such as POPE , use the common balancedF-score (or F 1-score) which equally weights precision and": "recall. However, given that THRONE is concerned withmeasuring hallucinations and is therefore particularly inter-ested in precision, we choose = 0.5 or the F 0.5-score,thus weighing precision twice as important as recall. TheF 0.5-score is commonly used in pandemic misinformationfilters , recommender systems , active stock selec-tion and other areas where false positives are costlierthan false negatives. From this we can calculate the overalland class-wise F 0.5-scores, F 0.5ALL and F 0.5CLS, respectively.To mitigate against class imbalance issues, we use F 0.5CLSas the principle metric of comparison between LVLMs inTHRONE.",
  "Ensuring Robustness via Ensembling": "Any LM used for AQA in THRONE may misjudge TypeII hallucinationsno LM is perfect. (top) showstwo different variants from the same model family (FLAN-T5 ), yielding opposite responses when prompted withthe same response and question. Moreover, an LM mayyield different answers to semantically identical questions,despite conditioning on the same responseshown in (bottom). To ensure THRONE is robust to spurious perfor-mance by any one LM, we ensemble various LMs and se-mantically equivalent question formats. The use of N dis-tinct LMs and M distinct question formats yields NM an-swers for each (LVLM response, class) combination. Thisset of NM answers is combined based on equal voting byeach (LM, question) pair to elect the predicted answer.Continuing the notation from Eq. (1), stacking predictionsfrom each (LM, question) pair yields a 3D array of pre-dicted labels: Y {0, 1}|IOURS||COURS|NM. To combinethe answers from each (LM, question) pair via voting, werequire agreement between at least k answers. Where suf-ficient agreement does not exist we introduce an ignorelabel. Mathematically, the elements in Y (from Eq. (1)) arecalculated as:",
  ",NMk=1 Yi,j,k (NM k)1,NMk=1 Yi,j,k k1,otherwise": "where an ignore label exists in Y, it is removed from thecalculations of PALL, RALL, PCLS, and RCLS, as we cannotbe confident in the AQA process for that particular (LVLMresponse, class) combination. The choice of k reflects thedesired level of confidence. We make use of a unanimousvoting mechanism (k = NM)use the prediction only ifall (LM, question) pairs agree, otherwise ignore. Humanevaluation of our benchmark and choice of voting mech-anism is found in the Supplementary Material. Once wehave applied this voting mechanism, we can calculate themetrics described at the end of Sec. 3.1. The image features a dining table with a delicious slice of chocolate cake on a white plate. The cake is accompanied by a glass of beer, which is placed on the table as well. There are two bowls on the table, one near the cake and the other further away. A fork is also visible on the table, ready to be used for enjoying the cake. The scene creates a cozy and inviting atmosphere for a delightful dessert and beverage experience.",
  "Dataset": "Any proper evaluation of object hallucinations (a type offalse positive error) requires knowing, with certainty, whichclasses are absent in an image. In our benchmark, we useCOCO for a number of reasons: (1) its annotationsof 80 categories are exhaustive at an image level (image-level recall99% )if there are many book instancesin a COCO image, at least one is annotated with boundingboxes; (2) many LVLMs are partly trained on COCO dataand so should be familiar with the set of categories; (3) itsimages generally contain complex scenes suitable for gener-ating long free-form descriptions unlike image recognitiondatasets like ImageNet .We utilize the validation set of COCO 2017, which con-tains |I| = 5000 images and |C| = 80 categories. Us-ing the single LVLM text prompt Describe this imagein detail., we generate 5000 responses.As we queryeach LVLM response for each category in C, a singleLM performs AQA |I| |C| = 400k times across theLVLM responsesone instance of AQA per (image re-sponse, class) pair. In Sec. 5.3, we also present results usingObjects365 which is rarely used in LVLM training.",
  "Language Models": "To assess Type I hallucinations in an LVLM response us-ing THRONE, we require a language model (LM) whichcan answer questions on the existence of object categoriesbased on the LVLM response. MMBench makes useof a ChatGPT model to identify multiple choice answer se-lections and still reports mistakes. In our experience, someLMs give rather incoherent judgements when used to as-sess hallucinations when the prompt is changed (see sup-plemental material). Therefore for THRONE, we chooseFLAN-T5 models . We make this choice becauseFLAN-T5 model family: (1) have undergone instructiontuning with thousands of tasks ; (2) are open-source and",
  "LLaVA-v1.5 50968.161.064.466.669.956.462.566.8LLaVA-Mistral 52486.871.878.383.684.464.270.877.5": ".THRONE Results with COCO for a selection ofinstruction-tuned LVLMs. We select F 0.5CLS as the principal met-ric for evaluation in our benchmark to balance across classes andto prioritize precision (which reflects the extent of hallucination)over recall. Best and second-best performance are denoted by blueand red, respectively. *Our implementation using official code toenable fair comparison. L corresponds to the median responselength (measured in # of characters). therefore accessible to the community; (3) can fit locally ona single GPU for the models we consider; (4) follow usersinstruction to only respond yes or no; and (5) are optimizedfor use in the free and public Text-Generation-InferenceAPI for acceleration. As described in Sec. 3.2, we uti-lize N LMs to ensure our method is robust. Specifically, weuse N = 3 variants of FLAN-T5, namely: FLAN-T5-Base(250M parameters), FLAN-T5-Large (780M parameters),and FLAN-T5-XL (3B parameters).",
  "Is there a/an {class name} in this image? Does the text imply a/an {class name} is in theimage?": "Does the text explicitly mention a/an{class name} is in the image?As outlined in Sec. 3.2, we use a unanimous votingmechanism to combine the answers from each (LM, ques-tion) pair and so k = 3 3 = 9.5Evaluation Results In this section, we: (1) outline our LVLM selection andreasoning; (2) present THRONE on COCO for evaluatingType I hallucinations; (3) extend THRONE to Objects365(containing a larger vocabulary); (4) analyze and extendPOPE to enable improved evaluation of Type II halluci-nations; and (5) highlight results from our ablation studiesfound in the Supplemental Material.",
  "Models": "For fair comparison between existing LVLMs, each pub-licly available model we evaluate uses an LLM with 7Bparameters. The LVLMs generally have different sized im-age encoderse.g. LLaVA uses a CLIP ViT-L/14 with an input resolution of 336 336, while Instruct-BLIP uses a ViT-g/14 trained with EVA andan input resolution of 224 224. Note that image encodersize and resolution is not something we can easily controlin a pre-trained model. Each model we consider containsinstruction tuning in the final training phaseinstructiontuned models provide free-form descriptions; THRONEfocuses on models that can generate free-form descriptions.E.g. we leave out BLIP-2 (response median length31 characters) in favor of InstructBLIP (median responselength 525).",
  "THRONE Results on COCO": "Results are shown in Tab. 1. The principal metric that weuse to judge model performance is the classwise F 0.5-score(highlighted gray). We also report all the metrics outlinedin Sec. 3.1, (P, R, F 1, F 0.5) for overall (left) and class-wise averaging (right), utilizing the unanimous voting pre-sented in Sec. 3.2. See the Supplementary Material for re-sults and an analysis of different voting mechanisms. Theseresults demonstrate that improvements on other bench-marks (POPE, MME, MMBench etc.)may be orthogo-nal and potentially at odds with improved performance onTHRONE. Using the results of the 11 LVLMs that we eval-uate, THRONE and POPE, which measure Type I and TypeII hallucinations, respectively, have a Spearmans rank cor-relation coefficient of just 0.2, and THRONE vs POPE-C has just 0.4the relationship between performance onPOPE and THRONE on the same dataset is far from mono-tonic. For class-wise precisionPCLS, the best performingmodels hallucinate 20% of the objects. We show in theSupplementary Material that the vast majority of false pos-itive objects in the free-form image descriptions evaluatedare direct hallucinations rather than misclassifications of vi-sually similar objects (e.g. mistaking a squash racket for atennis racket). These results demonstrate that much workstill remains to adequately suppress Type I hallucinations inLVLMs.",
  "THRONE Results on Objects365": "Many LVLMs train on COCO directly or indirectly, thusto demonstrate generality we apply THRONE to the Ob-jects365 dataset . Like COCO, Objects365 aims to beexhaustive in its image-level class labeling (it aims to la-bel at least one instance for each class present), but it has alarger object vocabulary and is not used as training data forthe LVLMs that we evaluate. To gather a manageable sub-set of the Objects365 validation set (80k images), we use thenatural sampling algorithm from , resulting in 5110 im-",
  "LLaVA-Mistral 58.339.146.953.157.835.944.351.5": ". THRONE Evaluation with Objects365. Evaluationresults for a selection of instruction-tuned LVLMs, we use a subsetof Objects365 for the THRONE evaluation. Best and second-best performance are denoted by blue and red, respectively. *Ourimplementation using official code to enable fair comparison. 0.660.680.700.720.740.760.780.80 Classwise F0.5-Score (COCO Classes) 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 Classwise F0.5-Score (Objects-365 Classes) Spearman's rank correlation: 0.912 . Comparing THRONE on COCO and Objects365.We observe that despite variations in the data distribution,THRONE metrics, which measure Type I hallucinations gener-alize and have a strong Spearmans rank correlation of r = 0.900.One model (red circle) designated as an outlier and ignored whencalculating ranking correlation. ages (the COCO validation set has 5k images). We presentthe results for THRONE on Objects365 in Tab. 2. shows the strong correlation in THRONE performance be-tween evaluating on COCO and Objects365. This demon-strates that measuring Type I hallucinations in an LVLMusing THRONE with a relatively small dataset like COCO,is indeed indicative of the intrinsic level of Type I halluci-nation in a given LVLM.",
  "Completing POPE for Type II Hallucinations": "Our experiments have used THRONE to evaluate theprevalence of Type I hallucinations in LVLM responses onCOCO. POPE evaluates Type II hallucinations on COCO,but we find POPE is largely incomplete. First, POPE onlyevaluates on 500 COCO validation set images.Second,for each image only a subset of classes (at most 12) areevaluatedeach image is only queried with 15% of possi-ble questions. Finally, POPE artificially balances evalua-tion questions between positives and negatives, despite ob-ject class existence in images being inherently imbalanced. . Instability of POPE to complete evaluation of TypeII Hallucinations. Extending POPE to an exhaustive analysis onall COCO images and classes (POPE-C) leads to a dramatic re-duction in performance across all 11 models. POPE is sensitive tothe sampling mechanism, and by undersampling negative classes,POPE severely underestimates Type II hallucinations in LVLMs. The above reasons make the evaluation of Type II halluci-nations using POPE insufficient (see Sec. 2 for more detailson POPE). To correct this, we complete POPE by using allimages to exhaustively query each LVLM for every classin the COCO vocabulary, as done in THRONE. We namethis POPE-Complete (POPE-C). shows the extremedifference in evaluation between POPE and our exhaustiveversion, POPE-Creporting F 1-score (POPE does not uti-lize F 0.5-score). As POPE only evaluates at most 9 nega-tive classes, only a small subset of potential hallucinationsof COCO classes are evaluated, thereby heavily underesti-mating the extent of Type II hallucination. For each LVLManalyzed, we observe a largein many cases an extremereduction in precision and therefore in F1-score.Our evaluation contains three pairs of LVLMs in whichone is the follow-up work to the other, where each follow-up work generally trains on more data for more tasks witha more advanced language model.Comparing the righthand side of to Tab. 1, we observe that these follow-up works generally show an improvement in POPE (andPOPE-C), but surprisingly indicate a small reduction in per-formance on THRONE with COCO. This observation sug-gests that progress in reducing Type II hallucinations can beorthogonal to reducing Type I hallucinations.",
  "Ablations": "In the Supplementary Material we present three key ablationexperiments and give an executive summary of results here.First, after subsampling COCO images and LVLM re-sponses, we replace the LMs in THRONE with humanjudgement as an oracle for Type I hallucination occur-rence. When comparing THRONE and CHAIR with hu-man judgements, we estimate using THRONE improvesthe precision of judging Type I hallucinations to 96% versus91% when using CHAIRthis reduces the false discoveryrate by more than 50%. Note that we find most estimatederrors in THRONE arise from the particular class defini-tions in COCO, e.g. the COCO class tv includes computermonitors, which the oracle judgement is aware of.Second, we apply the same class (and image) sampling strategy as in POPE to THRONE and show this samplingoverestimates F 0.5CLS by an average of 12.3 points comparedto the complete use of classes in THRONE (Tab. 1).Finally, we vary the choice of k i.e. the voting mecha-nism used to combine answers from multiple (LM, ques-tion) pairs. We use the unanimous voting mechanism (k =9) in THRONE to minimize the false discovery rate andfind the valid alternatives of simple majority (k = 5) orall-but-one (k = 8) voting mechanisms have strong corre-lations and rank correlations across all compute metrics, inTHRONE, of > 0.99 and > 0.94, respectively. 6Improved BaselinesMuch needs to be done to study Type I hallucinations.As a first step to their mitigation, we demonstrate a base-line method to augment the visual instruction tuning datafor LLaVA models , yielding improvements onTHRONE while maintaining similar performance regard-ing Type II hallucinations on POPE.6.1Visual Instruction Tuning Data Augmentation Similar to chain-of-thought learning , during instruc-tion tuning, we augment all visual instruction tuning sam-ples constructed by LLaVA by prepending the task ofenumerating a list of objects (present and absent) and indi-cating approximate locations, if applicable. Other than this,the LLaVA data and training pipeline remains unchanged.To generate the new data for this object enumeration task,we use the same COCO bounding box annotations used togenerate the vision instruction tuning data in LLaVA. Thesimple text-only format (no special tokens) we use is:",
  "Instruction: <image> Give a list of objects and locationsin the image.Response:{class_name_1} [{location_1}/absent]...{class_name_N} [{location_N}/absent]": "where location i represents the location of the bounding box center point on a 3 3 grid.We give additional details on the construction of the object enumeration task here.E.1Object Enumeration Implementation Details The LLaVA visual instruction tuning data contains 157712 samples applied to 81479 images from the COCO training set(some images correspond to multiple samples). We ensure the absolute character length of our object enumerate task for a single sample is not exceedingly longwe do not want the visual instruction tuning data to be pushed outside the contextlength of the LLaVA model. This is done by limiting the number of instances per class in a sample to 3.For each sample we construct an object enumeration task using bounding box data as follows: first, sort bounding boxannotations for a given image by box area in descending order; second, loop over the sorted annotations adding the instance(class name i, location i) to the object enumeration task if there are less than 3 instances of class name in the task; third,sample 6 negative classes and append them to the object enumeration task using absent as the location string.The sampling of negative classes is detailed next.E.2Negative Sampling Implementation Details",
  "COCO + VG86.177.084.189.8 83.7 86.7 64.5 86.1 73.7": ". Improved Baseline via Object Enumeration: Addingour object enumeration task to LLaVA training and inference leadsto large improvements on THRONE particularly in terms of class-wise precision, PCLS, over standard LLaVA models, demonstrat-ing a reduction in Type I hallucinations, as well as small reductionsin Type II hallucinations judged by POPE and POPE-C. COCO images where available. On THRONE, we observea large increase in classwise precision and therefore F 0.5CLS,particularly for LLaVA-v1.5, demonstrating the ability ofour method to reduce Type I hallucinations. Moreover, onPOPE and POPE-C, using our object enumeration yieldssmall improvements in precision, indicating reduced TypeII hallucinations as well. In the Supplementary Material, weablate the sampling of negatives during object enumerationtraining and the effect of removing the object enumerationtask during inference.",
  "Conclusion": "We establish a novel benchmark, THRONE, for evaluat-ing hallucinations generated by LVLMs in free-form im-age descriptions i.e. Type I hallucinations.Our bench-mark utilizes multiple LMs and prompt formats with asimple voting mechanism to yield an accurate evaluationof Type I hallucinations in LVLM responses. We ensurethat THRONE is broadly accessible by utilizing open-source LMs capable of running on a single commercialGPU. Using THRONE, we benchmark 11 publicly avail-able LVLMs on two datasets, COCO and Objects365, anddemonstrate that limited progress has been made in address-ing Type I hallucinations. Moreover, we show how the es-tablished benchmark, POPE, underestimates Type II hal-lucinations, which occur in response to specific questionse.g. yes-no questions. We present results for a completedversion (POPE-C) to enable a comparison of Type I halluci-nations through THRONE and Type II hallucinations usingPOPE-C. Finally, we propose a simple data augmentationfor LVLM training that can result in a large reduction inType I hallucinations whilst maintaining or improving TypeII hallucination performance.Limitations and Ethical Considerations are discussed inthe Supplementary Material. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. Advances inNeural Information Processing Systems, 35:2371623736,2022. 1, 3 Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, MargaretMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.Vqa: Visual question answering. In Proceedings of the Inter-national Conference on Computer Vision, pages 24252433,2015. 3 Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, SinanTan, Peng Wang, Junyang Lin, Chang Zhou, and JingrenZhou. Qwen-vl: A versatile vision-language model for un-derstanding, localization, text reading, and beyond. arXivpreprint arXiv:2308.12966, 2023. 3 Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao,Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori,and Ludwig Schmidt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use,2023. 2 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners. Advances in Neural In-formation Processing Systems, 33:18771901, 2020. 3 Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, ZechunLiu,PengchuanZhang,RaghuramanKrishnamoorthi,Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.Minigpt-v2: large language model as a unified interfacefor vision-language multi-task learning.arXiv preprintarXiv:2310.09478, 2023. 1, 6, 7 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, PascaleFung, and Steven Hoi.Instructblip:Towards general-purpose vision-language models with instruction tuning. InAdvances in Neural Information Processing Systems, 2023.1, 3, 6, 7 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, 2009. 5 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An imageis worth 16x16 words: Transformers for image recognitionat scale.Proceedings of the International Conference onLearning Representations, 2021. 6",
  "Hugging Face.Text generation inference. 2023. Accessed: November 10, 2023. 6": "Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Sri-vastava, Li Deng, Piotr Dollar, Jianfeng Gao, Xiaodong He,Margaret Mitchell, John C Platt, et al. From captions to vi-sual concepts and back. In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, 2015.3 Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,Xinggang Wang, Tiejun Huang, Xinlong Wang, and YueCao. Eva: Exploring the limits of masked visual representa-tion learning at scale. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, 2023. 6 Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang,Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: Acomprehensive evaluation benchmark for multimodal largelanguage models. arXiv preprint arXiv:2306.13394, 2023.1, 3 Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, ShijieGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-angyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2:Parameter-efficient visual instruction model. arXiv preprintarXiv:2304.15010, 2023. 6, 7 Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.LoRA: Low-rank adaptation of large language models. InProceedings of the International Conference on LearningRepresentations, 2022. 3 Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Ag-garwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Sub-hojit Som, Xia Song, and Furu Wei.Language is not allyou need: Aligning perception with language models. arXivpreprint arXiv:2302.14045, 2023. 1 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In Proceedings ofthe International Conference on Machine Learning, pages49044916, 2021. 3 Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,Chris Bamford, Devendra Singh Chaplot, Diego de lasCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-ple, Lucile Saulnier, et al.Mistral 7b.arXiv preprintarXiv:2310.06825, 2023. 6, 7 Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-tidis, Li-Jia Li, David A Shamma, et al.Visual genome:Connecting language and vision using crowdsourced denseimage annotations. International Journal of Computer Vi-sion, 123:3273, 2017. 8 Kibok Lee, Hao Yang, Satyaki Chakraborty, ZhaoweiCai, Gurumurthy Swaminathan, Avinash Ravichandran, andOnkar Dabeer. Rethinking few-shot object detection on amulti-domain benchmark. In Proceedings of the EuropeanConference on Computer Vision, 2022. 6",
  "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,Jingkang Yang, and Ziwei Liu.Otter:A multi-modalmodel with in-context instruction tuning.arXiv preprintarXiv:2305.03726, 2023. 6, 7": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for unifiedvision-language understanding and generation. In Proceed-ings of the International Conference on Machine Learning,pages 1288812900. PMLR, 2022. 1 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.BLIP-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models. In Pro-ceedings of the International Conference on Machine Learn-ing, 2023. 1, 3, 6 Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne XinZhao, and Ji-Rong Wen. Evaluating object hallucination inlarge vision-language models. In Proceedings of the Confer-ence on Empirical Methods in Natural Language, 2023. 1,2, 3, 4, 13 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InProceedings of the European Conference on Computer Vi-sion, 2014. 2, 3, 4, 5",
  "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, YuanhanZhang, Sheng Shen, and Yong Jae Lee.Llava-next: Im-proved reasoning, ocr, and world knowledge, 2024. 6, 7": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, SongyangZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is yourmulti-modal model an all-around player?arXiv preprintarXiv:2307.06281, 2023. 1, 2, 3, 5 ShayneLongpre,LeHou,TuVu,AlbertWebson,Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, BarretZoph, Jason Wei, and Adam Roberts. The flan collection:Designing data and methods for effective instruction tuning.In Proceedings of the International Conference on MachineLearning, 2023. 5 Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadi-winoto, Raymond Hendy Susanto, and Christopher Bryant.The CoNLL-2014 shared task on grammatical error correc-tion. In Proceedings of the Eighteenth Conference on Com-putational Natural Language Learning: Shared Task, 2014.5",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, ShaohanHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-ing multimodal large language models to the world. arXivpreprint arXiv:2306.14824, 2023. 1, 3": "Tina D Purnat, Paolo Vacca, Christine Czerniak, SarahBall, Stefano Burzo, Tim Zecchin, Amy Wright, SupriyaBezbaruah, Faizza Tanggol, `Eve Dube, Fabienne Labbe,Maude Dionne, Jaya Lamichhane, Avichal Mahajan, SylvieBriand, and Tim Nguyen. Infodemic signal detection dur-ing the covid-19 pandemic: Development of a methodologyfor identifying potential information voids in online conver-sations. JMIR Infodemiology, 1(1):e30971, 2021. 5 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In Proceedings of the International Conference onMachine Learning, pages 87488763. PMLR, 2021. 3, 6 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J. Liu. Exploring the limits of transfer learning with aunified text-to-text transformer. Journal of Machine Learn-ing Research, 2020. 3, 5 Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, TrevorDarrell, and Kate Saenko.Object hallucination in imagecaptioning. In Proceedings of the Conference on EmpiricalMethods in Natural Language, pages 40354045, 2018. 1,2, 3, 14",
  "Giuliano Rossi, Jakub Kolodziej, and Gurvinder Brar. A rec-ommender system for active stock selection. ComputationalManagement Science, 17, 2020. 5": "Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, GangYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: Alarge-scale, high-quality dataset for object detection. In Pro-ceedings of the International Conference on Computer Vi-sion, 2019. 5, 6 Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothee Lacroix, BaptisteRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.Llama:Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. 3 Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, S. M. AliEslami, Oriol Vinyals, and Felix Hill. Multimodal few-shotlearning with frozen language models. In Advances in NeuralInformation Processing Systems, pages 200212, 2021. 1, 3 Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi,Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, JiZhang, Jihua Zhu, Jitao Sang, and Haoyu Tang. Evaluationand analysis of hallucination in large vision-language mod-els. arXiv preprint arXiv:2308.15126, 2023. 1 Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large lan-guage models. Advances in Neural Information ProcessingSystems, 2022. 8",
  "language models. arXiv preprint arXiv:2309.06794, 2023.1, 3": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Yaya Shi, et al.mplug-owl: Modularization empowerslarge language models with multimodality. arXiv preprintarXiv:2304.14178, 2023. 1, 3, 6, 7 Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.Mm-vet: Evaluating large multimodal models for integratedcapabilities. arXiv preprint arXiv:2308.02490, 2023. 3",
  "Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-cas Beyer. Scaling vision transformers. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion, 2022. 6": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yu-long Chen, et al.Sirens song in the ai ocean: A surveyon hallucination in large language models. arXiv preprintarXiv:2309.01219, 2023. 1, 3 Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, XiaoleiWang, Yupeng Hou, Yingqian Min, Beichen Zhang, JunjieZhang, Zican Dong, et al. A survey of large language mod-els. arXiv preprint arXiv:2303.18223, 2023. 1",
  ". Balanced Sampling (POPE) vs. Exhaustive Sampling (THRONE): Applying POPE sampling to THRONE leads to anunderestimation of the prevalence of Type I hallucinations regardless of LVLM": "In the main paper, we demonstrated how the sampling method used in POPE leads to an underestimation of TypeII hallucinations and outline a complete version of POPE (POPE-C), which shows the true extent of Type II hallucinationsin LVLMs. In this section, we perform the oppositewe apply POPE type sampling to THRONE and compare the resultsto the complete sampling method used in THRONE as outlined in the main paper. Tab. 5 shows the results of applyingPOPE style sampling to THRONE, once again, applying POPE style sampling leads to a large underestimation of Type Ihallucinations. POPE style sampling applied to THRONE leads to a mean underestimation of F 0.5CLS by 10.9 points comparedto complete sampling, which is the default in THRONE.",
  "CCHAIR Overview": "We present a detailed description of the CHAIR evaluation presented in below. The method of CHAIR, like THRONE,attempts to capture the extent of hallucinations in free-form generated text pertaining an image, however, focuses on moretraditional image captioners. Similarly to THRONE, CHAIR does not use concept-focused prompts (i.e. instead is focusedon Type-I) and is intended to be used only in captioning tasks. CHAIR defines a manual pipeline on-top of the annotatedMSCOCO image dataset to produce a list of ground truth objects present in the scene.Given a set of ground truth objects in an image and a model-generated image caption, CHAIR extracts the objects presentin the scene via a traditional hard-rule extraction method and then attempts to map each of the predicted objects into one of the80 class categories of MSCOCO. The mapping of the extracted objects from the caption into the class set of MSCOCO uses apre-defined synonym dictionary for each object category. Once the objects of the predicted caption are extracted and mappedto one of the MSCOCO categories, CHAIR evaluates for false-positive predictions (i.e. hallucinations). In particular theauthors introduce two variants of CHAIR, the first variant CHAIRi quantifies the extent of hallucinated instances as,",
  "|{pred. object}|": "In this setting, hallucinations would be objects extracted from the model-generated captions that after being mapped, are stillnot present in the ground-truth object list for the corresponding instance. We note that CHAIRi can be viewed as measuringthe false discovery rate (FDR) that is 1 P where P is the precision. Using Precision as the main metric, howeveris limiting as it does not take into account the False Negative Rate (FNR) which is 1 R where R is the recall. Thisimplies that by lacking recall measurements, CHAIRi may assign high scores to short and incomplete captions which arenot comprehensive in detailing the image. This is in stark contrast with the new generation of LVLM models powered byLLMs which are designed to be more exhaustive and detailed and makes the use of CHAIRi problematic when evaluatingwith LVLMs. We note that CHAIRi is the main metric used when people report CHAIR scores, and typically reportednumbers correspond to the mean CHAIRi score across the MSCOCO validation set of images.The second variation of CHAIR is the CHAIRs which simply measure the number of sentences (predictions) that includeat least 1 hallucination as compared with all sentences considered,",
  "|{all sentences}|.(4)": "Note that CHAIRs does not measure the extent of hallucination within a sentence, just the existence of at least one halluci-nation. This is problematic as it does not capture the extent of hallucination in the sentence especially for long-form text anddoes not elucidate if a caption contains many or a single hallucination. Producing ground truth in CHAIRTo create a list of ground truth objects from MSCOCO annotations, the authors ofCHAIR harness two annotation types to produce the most exhaustive list of ground truth objects. First the authors directly useall of the instance segmentation labels for each image, which they aggregate into a unique list of objects existing in the image.Next the authors use the 5 human-labelled captions of each image in MSCOCO and use the same extraction and mappingpipeline applied to the predictions to produce an additional set of objects that are present in the captions. Both objects listsare combined and the authors note that captioning ground truth and instance segmentation ground truth objects are oftencomplementary as they follow different styles. Therefore combining objects from both types of object lists is beneficial forthe most exhaustive final ground truth list.",
  "D.1Evaluation Method": "We include a self-contained file (THRONE qual eval results.html) in the Supplementary Material, which shows the qual-itative evaluation and comparison of THRONE and CHAIR. For each LVLM evaluated, we sample 10 COCO images atrandom in which THRONE and CHAIR disagree and 5 COCO images in which THRONE and CHAIR agree. Therefore,we qualitatively evaluate 165 responses. These results are summarized in Tab. 6",
  "D.2Discussion": "We find that the plurality of errors made in THRONE relate to a mismatch between the LM definition of a certain class andthe definiton in COCO. The most clear example is in the tv COCO class. In COCO, this class includes computer monitors,whereas for an LM, the implication of the existence computer monitors in an LVLM response does not lead to a yesresponse when asked Is there a tv in this image? or similar. When doing an manual evaluation our human oracle isaware of the particular COCO class definitions and answers accordingly. Using a handcrafted rule for tv and other similarCOCO classes, we would expect the error rate of THRONE to reduce significantly, but in THRONE we deliberately avoidthe use of handcrafted rules.As mentioned in the main paper, the errors in CHAIR are more fundamental and result due to simple text matching ofsynonyms not being able to discriminate between abstract concepts alluded to in a response and direct objects implied to existin the image based on the response.Tab. 7 shows results for human analysis of false positives. We analyze 90 responses, the 15 samples for mPLUG-Owl,MiniGPT-v2, MiniGPT-4, LLaVA-7b-v1.5, LLaVA-7b-v1.3 and InstructBLIPthe final 90 responses in the self-containedfile: THRONE qual eval results.html in the Supplementary Material.",
  "To sample negatives in the object enumeration task we first build a co-occurence matrix from the bounding box annotations.The pseudocode for building this matrix is as follows:": "from pycocotools.coco import COCOimport numpy as nptrain_dset = COCO(instances_path)num_cats = len(train_dset.getCatIds())co_occur = np.array((num_cats, num_cats))cat_id2cont_id = {x: i for i, x in sorted(enumerate(train_dset.getCatIds()))}for iid in train_dset.getImgIds():anns = train_dset.loadAnns(train_dset.getAnnIds(imgIds=iid))pres_cats = [coco_cid2cont_cid[x[category_id]] for x in anns]pres_cats = np.unique(pres_cats)for r in pres_cats:for c in pres_cats:if r != c:co_occur[r, c] += 1 After building this co-occurence matrix negative classes are sampled in a manner which is aware of the classes present ina given image. The pseudocode is as follows (using some variables from the above pseudocode):",
  "E.3Object Enumeration Data Details": "In of the main paper, we present results on THRONE, POPE and POPE-C when training with our object enumerationtask using COCO or COCO and VisualGenome as object enumeration data. Approximately 33000 of the 81479 COCOimages in the LLaVA visual instruction tuning data are contained in the VisualGenome dataset. When using COCO andVisualGenome data, we construct the object enumeration task for an image from VisualGenome data when possible andCOCO otherwisewe do not combine COCO and VisualGenome annotations for any image.",
  "E.4Inference Details": "In of the main paper, we present results on THRONE, POPE and POPE-C when training with our object enumerationtask and performing the object enumeration task at inference. In the next section we show the effect of not performing objectenumeration during inference on THRONE and POPE, instead directly addressing the relevant task.",
  "COCO + VG86.177.084.189.883.786.7": ". Effect of Negatives and Inference: Including negatives in our object enumeration task improves performance on THRONE andPOPE in terms of precision and F-score. Performing the object enumeration task at inference time improves performance on THRONEand POPE, but hampers inference time as the object enumeration task can generate long sequences.FLimitations In this paper we present THRONE which is a step towards measuring and mitigating hallucinations in LVLMs, nonetheless,our work has few key limitations which we list below.1. THRONE is concerned with only measuring hallucinations in LVLM predictions in the form of a false existence of anobject in a closed set of classes. As observed in LLMs, hallucinations are much more multifaceted and include not justobjects outside a pre-defined vocabulary, but also many abstract concepts such as wrong reasoning relating to a visualscene as well as wrong attributes of a particular objects or person. These additional hallucinations are not possible to bemeasured with THRONE without modifications. 2. The presented method of THRONE only focused on Type-I hallucinations which does not paint a complete pictureof the hallucinating behavior of an LVLM. Indeed we present POPE-C in to extend hallucination measurementsin both Type-I and Type-II hallucinations. We present POPE-C as an extension of POPE since we observe that POPEseverely undercounts hallucinations in Type-II form. 3. Due to lack of general and exhaustive ground truth object label lists for a given image, our method relies on curateddatasets such as MCOCO or Object365 that have detailed annotations that are complete on an image level, which areneeded for our evaluation. 4. Our method focuses only on the hallucination bias of LVLMs but does not include measurements of other types of bias ofLVLM generations (e.g. related to concepts of fairness in generation) which we leave for future work.",
  "GEthical Considerations": "We present THRONE which is a general evaluation pipeline for measuring hallucinations (specifically Type-I halluci-nations) in Large-Vision-Language Models (LVLMs). Overall we believe that our contribution is ethically positive as itmeasures and shows that existing public LVLMs are not yet ready to be deployed in mission critical applications, as weobserve that they still suffer from hallucinating objects to a large extent. In addition we believe our presented evaluationframework also provides for a north-star in measuring evaluation and can aid the field and practitioners alike in measuringand making progress towards reducing evaluations in LVLMs as well as electing to use one LVLM over another. We note thatmeasuring societal bias in LVLMs is highly important pre-requisite before their deployment, however this is not investigatedin the current work."
}