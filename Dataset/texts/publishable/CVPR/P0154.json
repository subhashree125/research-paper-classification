{
  "Abstract": "While the generation of document layouts has beenextensively explored, comprehensive document genera-tionencompassing both layout and contentpresents amore complex challenge. This paper delves into this ad-vanced domain, proposing a novel approach called Doc-Synthv2 through the development of a simple yet effectiveautoregressive structured model. Our model, distinct in itsintegration of both layout and textual cues, marks a step be-yond existing layout-generation approaches. By focusing onthe relationship between the structural elements and the tex-tual content within documents, we aim to generate cohesiveand contextually relevant documents without any relianceon visual components. Through experimental studies on ourcurated benchmark for the new task, we demonstrate theability of our model combining layout and textual informa-tion in enhancing the generation quality and relevance ofdocuments, opening new pathways for research in documentcreation and automated design. Our findings emphasize theeffectiveness of autoregressive models in handling complexdocument generation tasks.",
  ". Introduction": "Recent advancements in generative models have made significant impacts on language, image, and mul-timodal content generation. There is an increasing focus onvector graphic document generation within thisrealm, where these models support users in creating, modify-ing, publishing, and designing both business and artistic doc-uments. Documents differ from standard natural images asthey contain structured layers of text and media content. Thefield of document generation presents unique challenges inseamlessly integrating visual elements such as style, layout,and multimedia with textual content, posing new problemsfor the vision community.",
  "*Work done during internship at Adobe Research": "Document layout generation has played a crucial role in numerous applications, rang-ing from automated report creation to dynamic webpagedesign, significantly impacting how information is perceivedand interacted with by users. With large language models(LLMs) becoming more and more capable of com-positional reasoning of visual concepts , it opens furtheravenues for exploiting autoregressive approaches in the au-tomatic end-to-end generation of both document contentand layout structure. Moreover, synthetic document genera-tion has gained attention in recent times owing to lackof multi-domain large-scaled layout annotated datasets nec-essary for document pre-training . However, end-to-endpixel-based approaches suffer from low-resolutiongenerated outputs where the textual content can be hardlyextracted. In this work, we introduce DocSynthv2 to seam-lessly generate layout structure with integrated text, essentialto convey specific information and context, completing thecommunication objective of the document.This work contributes to document generation researchin three different folds: 1) We curate a large-scale extendedbenchmark called PubGenNet tailor-made for documentgeneration and completion task. 2) We introduce a simpleand flexible autoregressive approach for generating high-resolution document outputs, capable of handling sequencesof arbitrary lengths. 3) We outline future challenges andopportunities in evaluating document generation, setting thestage for advancements in this evolving field.",
  ". Related Work": "Document Layout Generation Recently, there has been asurge in research on layout generation. Foundational workslike LayoutGAN and LayoutVAE have been influ-ential in synthesizing layouts by modeling geometric rela-tions of different 2D elements and then rendering them inthe image space. Document layout generation has receivedextensive interest in recent years owing to its integration intasks such as content generation and graphic web de-signs . While attempts to generate document layouts",
  ". Overall Architecture of DocSynthv2": "given user-conditioned prompts (eg. input reference image,keywords, and category of the document), proposed anapproach to construct hierarchies of document layouts andlater sample and generate them using a recursive VAE. Theaforementioned method was further extended using graph au-toencoder networks with optional design constraints forfurther improvement. Gupta et. al. proposed LayoutTransformer with self-attention which is the mostrelevant to this work. They used a next element predictionobjective (i.e. layout completion) using a transformer archi-tecture in an autoregressive manner to produce layout tokens,including class labels and bounding boxes of document ob-jects. tried to combine these generative transformers with VAEs to learn better layout refinement and prediction.Synthetic Document Generation The Computer Visioncommunity has also captured emerging interest to generatesynthetic realistic scene images with plausible layouts from auser provided reference layout , emphasizing par-ticularly on high-resolution image outputs. The DocSynthframework introduced the first image-to-image transla-tion pipeline for creating synthetic document image datasetsfor augmenting real data during training for document layoutanalysis tasks . In this work, we move a step forwardtowards generating synthetic data with content preservation.",
  ". Overview": "Document Representation The document layout of a pagecan comprise multiple sets of elements, where each elementcan be described by its category c, left and top coordinatex and y, as well as width w and height h. The continu-ous attributes x, y, w and h are often quantized, whichhas proven to be useful for graphic layout generation ap-proaches . Following the FlexDM approach ,we represent document D as a vector consisting of a tuple oflayout components (D1, D2, . . . , DS), where S is the num-ber of elements in D. Each element Di =dki | k E can represent either element type , position, style attributes,or raw text content where k represents the indices of the at-tributes. Contrary to FlexDM , we do not use any embed-dings in the input sequence but rather use only the elementslayout information or its content attributes. We concatenatethe layout information along with the text attribute tokens forevery element as shown in Equation 1. Here, N representsthe total number of elements, while sos and eos arespecial tokens which denote the start and end of a sequence.Also, a special [NULL] token appears when dki is inevitablymissing (e.g., font type for a non-text element), or paddingvariable-length sequences when training a mini-batch. D = {sosc1x1y1w1h1t1 . . . cNxNyNwNhNtNeos}(1)Representing with Discrete Variables Following Layout-Transformer , we applied an 8-bit uniform quantizationon every document element (image region or text) and mod- elled them using Categorical distribution. We note that whileconverting coordinates into discrete values leads to someloss of precision, this approach enables the modeling of mul-tiple kinds of distributions, which is crucial for documentlayouts. Every document object (text or non-text) is pro-jected to the same dimension such that we can concatenateevery element (cN, xN, yN, wN, hN, tN) in a single linearsequence of their element values. The overall structure ofa page can then be represented by a sequence of m latentvectors where m is decided by the total number of tokensencoded in the input sequence S. For conciseness, we usej, j {1, . . . , m} to represent any document element inthe above sequence. We model this joint distribution as aproduct over a series of conditional distributions using thechain rule as shown in Equation 2.",
  ". Model Architecture": "DocSynthv2 is a document generation transformer pre-trained on document datasets containing multiple elementswith a combined set of layout and text attributes. The modellearns neural representations of document data, capturingboth physical and logical relationships of the document el-ements with the previously predicted element. Our overallarchitecture of DocSynthv2 is shown in .Training: Given an initial set of T visible tokens as in-put containing attributes representing: 1) Layout Category(eg. Table, Table Cell, Paragraph, Title, Caption etc.) 2)Position 3) Font Style 4) Text Content, the model tries topredict the next element with an autoregressive GPT-2 Trans-former decoder . Each of these GPT blocks consists ofa masked multi-head attention (MHA) and a feedforwardnetwork (FFN) as shown. The output at the final layer corre-sponds to the next parameter.Inference: During inference, both the position and texttokens are synthesized auto-regressively for the fixed cate-gory token (i.e. a reference layout you would like to gener-ate). During both training and inference, the ground-truthsequences have been used to train the model more efficientlyas done in .Losses: Since the model has both continuous and discretesets of parameters as already discussed, we use a variationalloss to minimize KL-Divergence between the softmax pre-dictions for all discrete parameters as in .",
  "Our evaluation of DocSynthv2 primarily utilizes two vectorgraphic document datasets, Crello and DocGenNet, our": "curated version of PubLayNet streamlined for the taskof document generation.Crello: Originating from an online design platform, thisdataset encompasses a broad range of design templates, in-cluding but not limited to social media posts, banner ads,blog headers, and printed materials. We use a similar experi-mental setting as used in FlexDM . The released datasetby the authors was partitioned into 18,738 training instances,2,313 for validation, and 2,271 for testing. Detailed defini-tions of each attribute can be found in the original paper .PubGenNet: For experimental validation, we generateda new benchmark called \"PubGenNet,\" a large-scaled ex-tended dataset curated to advance the field of document gen-eration. This dataset was assembled by extracting a diversearray of samples from the original PubLayNet dataset ,which itself is derived from an extensive collection of sci-entific publications available in PubMed Central. To ensurea comprehensive set of text attributes (eg. font type) alongwith raw textual content, we utilized a PDF extraction proce-dure, using the PyMuPDF library enabling us to align thisextracted data with the original COCO annotations. In sum-mary, the overall curation process involved extraction andprocessing of layout and text data from a set of documentsrepresented in the PubLayNet format. After obtaining thedocument-specific attributes, the processed data was com-piled into a structured dataset suitable for training and eval-uating document generation models. The resulting trainingand validation instances are similar to the dataset statisticsin PubLayNet with 335,703 document samples for trainingand 11,245 instances for validation.",
  ". Tasks": "The primary motivations for our model are to address thekey aspects of document design and generation. We haveselected the evaluation tasks based on: (1) Creating a newdocument or completing a partially finished one, focusing onmaintaining coherence, appearance, and relevance to the in-tended content. (2) Test the models ability in layout design,specifically its understanding of spacing, alignment, and theinterplay between text and other elements.Document Completion: This task requires the model toanalyze the current layout elements and content within thedocument (eg. text, title, tables, figures etc.) and logicallypredict what elements should follow to maintain the coher-ence and plausible structure of a document.Single and Multiple Text Box Placement: This task interms of next element prediction requires the model to iden-tify optimal locations and sizes for text boxes within a doc-ument, based on the existing layout and design principles.It assesses the models capability to seamlessly incorporatenew text elements, ensuring they align with the documentsstructure and visual appeal.",
  ". Quantitative Evaluation": "summarizes the performance comparison of Doc-Synthv2 over the existing SOTA transformer decoder-onlymodels. Our full model (with text attributes) gives us boostin performance over the layout-only model, demonstratingthat utilizng the raw text can help guide models for lay-out generation when avaialble. Although our model is alightweight decoder-only architecture, it can perform on parwith LayoutFormer++ which is an encoder-decoder-based transformer. Our results with high Alignment andOverlap scores also suggest that layout generation and com-pletion models gain substantial improvement when trainedon sequences integrating textual content.In , we summarize the performance of Single andMultiple Text Box Placement in the Crello dataset. Theresults show that the model does worse for text placementin the Single Text box condition, likely due to the weakermultimodal features compared to . However, it performson par for IoU and outperforms for BDE in the Multiplecondition, which may be due to the raw text in our model.",
  ". Qualitative Evaluation": "shows example of our applied for text synthesis anddocument completion on the Crello and PubGenNet datasets.In the Crello Text prediction example, it can be seen that thetext is aligned with the layout showing a plausible flyer titlefor the heading section followed by an address and date inthe sub text fields. For the Document Completion Task, wehave the model generate the text within in an existing Table",
  ". Future Scope and Challenges": "In conclusion, DocSynthv2 demonstrates that integratingtext with layout sequences into an autoregressive frame-work enriches the data representation and provides additionalcontext, leading to improved stability and performance ingenerating coherent and contextually appropriate documentcontent and motivates future work. First, the integration oflayout and text needs to advance beyond current capabilitiesto address the diversity of document styles and industry-specific standards. We believe future work may benefit fromvisual-language models that can understand multimodalcontent or code generation models that can learn com-plex structure from a wide array of document formats andcontent types. We also believe, the evaluation of documentgeneration systems remains a critical challenge. There isa pressing need for evaluation frameworks which can ef-fectively measure the usefulness of generated documents interms of both their visual layout and textual content. Theseframeworks must encompass metrics that evaluate coher-ence, relevance, readability, and visual appeal, reflecting themulti-functional nature of documents.",
  "Acknowledgement": "The resources and support from the Adobe Document Intel-ligence Lab (DIL) team were instrumental in the successfulcompletion of this project. Special thanks to Ani Nenkova,Joe Barrow, Varun Manjunatha and Chris Tensmeyer, whoseguidance and expertise were invaluable throughout the in-ternship. Additionally, Sanket Biswas expresses his gratitudeto Nora Graichen for her constant assistance and perceptivecriticism, particularly during the last phases of submission. Diego Martin Arroyo, Janis Postels, and Federico Tombari.Variational transformer networks for layout generation. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1364213652, 2021. 1,2 Sanket Biswas, Pau Riba, Josep Llads, and Umapada Pal.Docsynth: a layout guided approach for controllable docu-ment image synthesis. In Document Analysis and RecognitionICDAR 2021: 16th International Conference, Lausanne,Switzerland, September 510, 2021, Proceedings, Part III,pages 555568. Springer, 2021. 1, 2 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,Pranav Shyam, Girish Sastry, Amanda Askell, et al. Languagemodels are few-shot learners. Advances in neural informationprocessing systems, 33:18771901, 2020. 1 Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hib-schman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ran-jitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th annualACM symposium on user interface software and technology,pages 845854, 2017. 1 Patrick Esser, Robin Rombach, and Bjorn Ommer. Tamingtransformers for high-resolution image synthesis. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1287312883, 2021. 1 Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Ar-jun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, andWilliam Yang Wang. Layoutgpt: Compositional visual plan-ning and generation with large language models. Advances inNeural Information Processing Systems, 36, 2024. 1 Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry SDavis, Vijay Mahadevan, and Abhinav Shrivastava. Layout-transformer: Layout generation and completion with self-attention. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 10041014, 2021. 1,2, 3, 4 Sen He, Wentong Liao, Michael Ying Yang, Yongxin Yang,Yi-Zhe Song, Bodo Rosenhahn, and Tao Xiang. Context-aware layout to image generation with enhanced object ap-pearance. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 1504915058,2021. 2",
  "decoupled diffusion model. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 19421951, 2023. 1": "Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, MayuOtani, and Kota Yamaguchi. Layoutdm: Discrete diffusionmodel for controllable layout generation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1016710176, 2023. 1 Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani,and Kota Yamaguchi. Towards flexible multi-modal docu-ment models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1428714296, 2023. 1, 2, 3, 4 Zhaoyun Jiang, Jiaqi Guo, Shizhao Sun, Huayu Deng,Zhongkai Wu, Vuksan Mijovic, Zijiang James Yang, Jian-Guang Lou, and Dongmei Zhang. Layoutformer++: Con-ditional graphic layout generation via constraint serializa-tion and decoding space restriction.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1840318412, 2023. 4",
  "Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image gener-ation from scene graphs. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages12191228, 2018. 2": "Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal,and Greg Mori. Layoutvae: Stochastic scene layout gen-eration from a label set. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 98959904, 2019. 1 Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeonNam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sang-doo Yun, Dongyoon Han, and Seunghyun Park. Ocr-freedocument understanding transformer. In European Confer-ence on Computer Vision, pages 498517. Springer, 2022.1 Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, YuanHao, Haifeng Gong, and Irfan Essa. Blt: Bidirectional layouttransformer for controllable layout generation. In EuropeanConference on Computer Vision, pages 474490. Springer,2022. 1",
  "Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang,and Tingfa Xu. Layoutgan: Generating graphic layouts withwireframe discriminators. arXiv preprint arXiv:1901.06767,2019. 1": "Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, and HadarAverbuch-Elor. Read: Recursive autoencoders for documentlayout generation. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition Workshops,pages 544545, 2020. 1, 2 Akshay Gadi Patil, Manyi Li, Matthew Fisher, Manolis Savva,and Hao Zhang. Layoutgmn: Neural graph matching forstructural layout similarity. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1104811057, 2021. 2 Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed SNassar, and Peter Staar. Doclaynet: a large human-annotateddataset for document-layout segmentation. In Proceedings ofthe 28th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining, pages 37433751, 2022. 2",
  "Zecheng Tang, Chenfei Wu, Juntao Li, and Nan Duan. Layout-nuwa: Revealing the hidden layout expertise of large languagemodels. arXiv preprint arXiv:2309.09506, 2023. 4": "Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, YangLiu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mo-hit Bansal. Unifying vision, text, and layout for universaldocument processing. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1925419264, 2023. 4 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-tinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roz-ire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:Open and efficient foundation language models.arXivpreprint arXiv:2302.13971, 2023. 1 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 2",
  "Kota Yamaguchi. Canvasvae: Learning to generate vectorgraphic documents. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 54815489,2021. 1, 3": "Moonbin Yim, Yoonsik Kim, Han-Cheol Cho, and SungraePark. Synthtiger: Synthetic text image generator towardsbetter text recognition models. In International conference ondocument analysis and recognition, pages 109124. Springer,2021. 1 Junyi Zhang, Jiaqi Guo, Shizhao Sun, Jian-Guang Lou, andDongmei Zhang. Layoutdiffusion: Improving graphic layoutgeneration by discrete diffusion probabilistic models.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 72267236, 2023. 1"
}