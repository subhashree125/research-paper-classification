{
  "Wei SunHaoning WuYixuan GaoYuqin CaoZicheng ZhangXiele Wu": "Radu Timofte *Fei PengHuiyuan FuAnlong MingChuanming WangHuadong MaShuai HeZifei DouShu ChenHuacong ZhangHaiyi XieChengwei WangBaoying ChenJishen ZengJianquan YangWeigang WangXi FangXiaoxin LvJun YanTianwu ZhiYabin ZhangYaohui LiYang LiJingwen XuJianzhao LiuYiting LiaoJunlin LiZihao YuFengbin GuanYiting LuXin LiHossein MotamedniaS. Farhad Hosseini-BenvidiAhmad Mahmoudi-AznavehAzadeh MansouriGanzorig GankhuyagKihwan YoonYifang XuHaotian FanFangyuan KongShiling ZhaoWeifeng DongHaibing YinLi ZhuZhiling WangBingchen HuangAvinab SahaSandeep MishraShashank GuptaRajesh SureddiOindrila SahaLuigi CelonaSimone BiancoPaolo NapoletanoRaimondo SchettiniJunfeng YangJing FuWei ZhangWenzhi CaoLimei LiuHan PengWeijun YuanZhan LiYihang ChengYifan DengHaohui LiBowen QuYao LiShuqing LuoShunzhou WangWei GaoZihao LuMarcos V. CondeRadu TimofteXinrui WangZhibo ChenRuling LiaoYan YeQiulin WangBing LiZhaokun ZhouMiao GengRui ChenXin TaoXiaoyu LiangShangkun SunXingyuan MaJiaze LiMengduo YangHaoran XuJie ZhouShiding ZhuBohan YuPengfei ChenXinrui XuJiabin ShenZhichao DuanErfan AsadiJiahe LiuQi YanYouran QuXiaohui ZengLele WangRenjie Liao",
  "Abstract": "This paper reports on the NTIRE 2024 Quality Assess-ment of AI-Generated Content Challenge, which will beheld in conjunction with the New Trends in Image Restora-tion and Enhancement Workshop (NTIRE) at CVPR 2024.This challenge is to address a major challenge in the field ofimage and video processing, namely, Image Quality Assess-ment (IQA) and Video Quality Assessment (VQA) for AI-Generated Content (AIGC). The challenge is divided intothe image track and the video track. The image track usesthe AIGIQA-20K, which contains 20,000 AI-Generated Im-ages (AIGIs) generated by 15 popular generative models.The image track has a total of 318 registered participants.A total of 1,646 submissions are received in the developmentphase, and 221 submissions are received in the test phase.Finally, 16 participating teams submitted their models and",
  "*The organizers of the NTIRE 2024 Quality Assessment of AI-Generated Content Challenge.The NTIRE 2024 website:": "fact sheets.The video track uses the T2VQA-DB, which contains10,000 AI-Generated Videos (AIGVs) generated by 9 popu-lar Text-to-Video (T2V) models. A total of 196 participantshave registered in the video track. A total of 991 submis-sions are received in the development phase, and 185 sub-missions are received in the test phase. Finally, 12 partici-pating teams submitted their models and fact sheets. Somemethods have achieved better results than baseline meth-ods, and the winning methods in both tracks have demon-strated superior prediction performance on AIGC.",
  "arXiv:2404.16687v2 [cs.CV] 7 May 2024": "models. Therefore, it is significant to propose efficient Im-age Quality Assessment (IQA) and Vmage Quality Assess-ment (VQA) methods to accurately predict the quality ofgenerated images and videosThis NTIRE 2024 Quality Assessment of AI-GeneratedContent Challenge aims to promote the development of theI/VQA methods for generated images and videos to guidethe improvement and enhancement of the performance ofgenerative models, thereby improving the quality of expe-rience of AIGC. The challenge is divided into the imagetrack and the video track. In the image track, we use theAIGIQA-20K , which contains 20,000 AIGIs generatedby 15 Text-to-Image (T2I) models. 21 subjects are invitedto produce accurate Mean Opinion Scores (MOSs). Thevideo track uses the T2VQA-DB , in which 9 Text-to-Vido (T2V) models are used to generate 10,000 videos. TheMOSs are obtained from 27 subjects.This is the first time that a quality assessment of AIGCchallenge has been held at the NTIRE workshop. The chal-lenge has a total of 514 registered participants, 318 in theimage track and 196 in the video track. A total of 2,637submissions were received in the development phase, while406 prediction results were submitted during the final test-ing phase. Finally, 16 valid participating teams in the im-age track and 12 valid participating teams in the video tracksubmitted their final models and fact sheets.They haveprovided detailed introductions to their I/VQA methods forAIGIs and AIGVs. We provide the detailed results of thechallenge in and . We hope that thischallenge can promote the development of I/VQA methodsfor image and video generation.This challenge is one of the NTIRE 2024 Workshop 1 se-ries of challenges on: dense and non-homogeneous dehaz-ing, night photography rendering, blind compressed imageenhancement, shadow removal, efficient super-resolution,image super-resolution (4), light field image super-resolution, stereo image super-resolution, HR depth fromimages of specular and transparent surfaces, bracketing im-age restoration and enhancement, portrait quality assess-ment, Restore Any Image Model (RAIM) in the wild, raWimage super-resolution, short-form UGC Video quality as-sessment, low light enhancement, and raw burst alignmentand ISP challenge.",
  ". AIGI dataset": "Several AIGI datasets have been proposed in recent years.Benefiting from the successful Stable Diffusion , Dif-fusionDB is the first large-scale text-to-image promptdataset, containing 14 million images generated by StableDiffusion using prompts and hyperparameters specified by real users.HPS collects 98,807 generated imagesfrom the Stable Foundation Discord channel, along with25,205 human choices.ImageReward proposes adataset containing 137k prompt-image pairs sampled fromDiffusionDB. Each pair has 3 MOSs from overall rating,image-text alignment, and fidelity. Pick-A-Pic con-tains over 500,000 examples and 35,000 distinct prompts,Each example contains a prompt, two generated images,and a label for which image is preferred. AGIQA-1K ,AGIQA-3K , and AIGCIQA2023 contain 1,080,2,982, and 2,400 images respectively. AGIN collects6,049 images and conducts a large-scale subjective study tocollect human opinions on the overall naturalness. In thischallenge, we use the AIGIQA-20K , including 20,000images generated by 15 popular T2I models, along with theMOSs collected from 21 subjects.",
  ". AIGV dataset": "Compared with AIGI datasets, the number of proposedAIGV datasets is small.Chivileva et al. proposes adataset with 1,005 videos generated by 5 T2V models. 24users are involved in the subjective study. EvalCrafter builds a dataset using 500 prompts and 5 T2V models, re-sulting in 2,500 videos in total. However, only 3 users areinvolved in the subjective study. Similarly, FETV uses619 prompts, 4 T2V models, and 3 users for annotation aswell. VBench has a larger scale with in total of 1,7kprompts and 4 T2V models. In the video track, we use theT2VQA-DB . The dataset has 10,000 videos generatedby 9 different T2V models. 27 subjects are invited to collectthe MOSs.",
  ". IQA model": "Traditional IQA models focus on distortions like noises,blurriness, semantic contents, etc.DBCNN han-dles both synthetic and authentic distortions by training twoCNN networks. StairIQA proposes a staircase struc-ture to hierarchically integrate the information from low-level to high-level.LIQE proposes a general andautomated multitask learning scheme to exploit auxiliaryknowledge from IQA, scene classification, and distortiontype identification. In the meantime, several IQA modelsdesigned for AIGIs have been proposed. HPS andPickScore are CLIP-based models to imitate hu-man preference on generated images. ImageReward uses a BLIP-based architecture to predict the imagequality.In recent years, researchers have been paying attention tousing the ability of Large Multi-modality Models (LMMs)to solve IQA tasks.Q-bench first investigates theperformance of LMMs in evaluating visual quality. further introduce the training procedure to utilizeLMMs for IQA tasks. Q-Refine is a quality-awarded refiner to guide the refining process in T2I models. The de-velopment of IQA models not only provides more accuratepredictions on AIGIs quality but also benefits the develop-ment of image generation models.",
  ". VQA model": "The traditional VQA models are usually designed for user-generated videos or a certain attribute of videos. . For example, SimpleVQA trainsan end-to-end spatial feature extraction network to directlylearn quality-aware spatial features from video frames, andextracts motion features to measure temporally related dis-tortions at the same time to predict video quality. FAST-VQA proposes the fragments sampling strategiesand the Fragment Attention Network (FANet) to accommo-date fragments as inputs. DOVER evaluates the qual-ity of videos from the technical and aesthetic perspectivesrespectively. Q-Align can also address the VQA taskby relying on the ability of multi-modal large models.There are several works targeting the VQA tasks ofAIGVs. VBench and EvalCrafter build bench-marks for AIGVs by designing multi-dimensional metrics.MaxVQA and FETV propose separate metricsfor the assessment of video-text alignment and video fi-delity, while T2VQA handles the features from the twodimensions as a whole. We believe the development of theVQA model for AIGV will certainly benefit the generationof high-quality videos.",
  ". Overview": "The challenge has two tracks, i.e. image track and videotrack. The task is to predict the perceptual quality of a gen-erated image/video based on a set of prior examples of im-ages/videos and their perceptual quality labels. The chal-lenge uses the AIGIQA-20K and the T2VQA-DB dataset and splits them into the training, validation, and test-ing sets. As the final result, the participants in the challengeare asked to submit predicted scores for the given testingset.",
  "In the image track, we use the AIGIQA-20K fortraining, validating, and testing.The dataset contains": "20,000 images generated by 15 T2I models, which are:DALLE 2 , DALLE 3 , Dreamlike , IF ,LCM Pixart , LCM SD1.5 , LCM SDXL ,Midjourney v5.2 , Pixart , Playground v2 ,SD1.4 , SD1.5 , SDXL , SDXL Turbo , andSSD1B . Concretely, 20,000 prompts are selected fromDiffusionDB . For Dreamlike, Pixart , Playground v2,SD1.4, SD1.5, SDXL, and SSD1B ,each model generates 2,000 images for their strong gener-alize ability. LCM Pixart, LCM SD1.5, LCM SDXL, andSDXL Turbo are assigned 1,000 images each, andDALLE2, DALLE3, IF, Midjourney v5.2 are 500images each.In the video track, we use the T2VQA-DB .The dataset contains 10,000 generated videos from:Text2Video-Zero,AnimateDiff,Tune-a-video ,VidRD ,VideoFusion ,Mod-elScope , LVDM , Show-1 , and LaVie .1,000 prompts are selected from WebVid-10M , alarge-scale text-video dataset. Each model generates onevideo for each prompt.For Tune-a-video , twodifferent pre-trained weights are used. The video resolutionis unified to 512 512, and the video length is 4s.21 subjects are invited to rate the generated imagesin AIGIQA-20K , and 27 subjects for the videos inT2VQA-DB . After normalizing and averaging the sub-jective opinion scores, the mean opinion score (MOS) ofeach image/video can be obtained. Furthermore, we ran-domly split the AIGIQA-20K into a training set, a valida-tion set, and a testing set according to the ratio of 7 : 1 : 2.The same split is conducted to the T2VQA-DB. The num-bers of generated images in the training set, validation set,and testing set are 14, 000, 2, 000, and 4, 000, respectively.For the video track, the numbers are 7, 000, 1, 000, and2, 000.",
  ". Challenge phases": "Both tracks consist of two phases: the developing phase andthe testing phase. In the developing phase, the participantscan access the generated images/videos of the training setand the corresponding prompts and MOSs. Participants canbe familiar with dataset structure and develop their meth-ods. We also release the generated images and videos of thevalidation set with the corresponding prompts but withoutcorresponding MOSs. Participants can utilize their meth-ods to predict the quality scores of the validation set and upload the results to the server. The participants can receiveimmediate feedback and analyze the effectiveness of theirmethods on the validation set. The validation leaderboardis available. In the testing phase, the participants can ac-cess the images and videos of the testing set with the corre-sponding prompts but without corresponding MOSs. Partic-ipants need to upload the final predicted scores of the testingset before the challenge deadline. Each participating teamneeds to submit a source code/executable and a fact sheet,which is a detailed description file of the proposed methodand the corresponding team information. The final results",
  ". Result analysis": "The main results of 28 teams methods and the baselinemethods are shown in and . It can be seenthat in both tracks, the results of the baseline methods arenot ideal in the testing set of the two datasets, while mostof the submitted methods have achieved better results. Itmeans that these methods are closer to human visual percep-tion when used to evaluate the generated images and videos.In the image track, 9 teams achieve a main score higherthan 0.9, and 4 teams are higher than 0.91. In the videotrack, 9 teams achieve a main score higher than 0.8, 5 teamshigher than 0.82, and the championship team is higher than0.83. In the meantime, the top-ranked teams only have a",
  ". Scatter plots of the predicted scores vs. MOSs in the video track. The curves are obtained by a four-order polynomial nonlinearfitting": "small difference in the main score. and show scatter plots of predicted scores versus MOSs for the28 teams methods on the testing set. The curves are ob-tained by a four-order polynomial nonlinear fitting. We canobserve that the predicted scores obtained by the top teammethods have higher correlations with the MOSs. They cannot only meet the need to predict quality scores for gen-erated images/videos but also contribute to improving theperformance of image/video generation methods.",
  ".1pengfei": "Team pengfei wins the championship in the imagetrack.Their method enhances LIQE by consider-ing the correlation between prompts and generated imagesin the AIGC task, as shown in the .To rep-resent this correlation, they design corresponding textualtemplates such as a how image matching the prompt,where how corresponds to five different adverbs: badly,poorly, fairly, well, and perfectly. The textual tem-plates are fed into the text encoder of the CLIP model to obtain text features. Subsequently, the images are input into the image encoder of CLIP to obtain image features.CLIPs capability to compute the correlation between twofeatures allows them to derive five levels of correlation be-tween images and prompts. These levels of correlation arethen weighted and summed using a softmax function to de-rive the final score. Additionally, considering the variationin image sizes, they not only feed image chunks into the im-age encoder but also include a resized version of the originalimage to learn global semantics. Furthermore, in AIGI quality assessment, differences inimage ratings compared to traditional image assessment rat-ings are observed. Traditional image assessment involvesintuitive sorting based on factors like blurriness, distortion,and contamination. However, AIGI quality assessment isheavily influenced by prompts, making it difficult to effec-tively rank images between different prompts. Therefore,the ordinal relationship between two different image-textpairs is complex and challenging to directly learn. Tradi-tional ranking loss is ineffective in this scenario. Instead,using L1 loss to directly fit score values and indirectly learnthe order yields better SRCC scores.",
  ". Overview diagram of the proposed method of team pengfei": "may lead to insufficient learning of the data distribution.However, considering the model generating the images maycause overfitting due to the uneven distribution of modelcategories. Therefore, two models are trained to integratethe results. One model undergoes normal training, while theother model has the name of the generating model added tothe prompt to learn different distributions. The final infer-ence results of the two models are multiplied and assem-bled.",
  "MediaSecurity SYSU&Alibaba": "Team MediaSecurity SYSU&Alibabas solution ensembleconsists of four types of models: single-modal model witha single frame, single-modal model with multiple frames,multi-modal model with a single frame, and multi-modalmodel with multiple frames. In the single-modal model with a single frame, they uti-lize the Swin-L pre-trained on ImageNet 22K topredict the quality of a single frame. In the single-modalmodel with multiple frames, they add NeXtVLAD tothe Swin-L model, which is initialized from the single-modal model with a single frame. In the multi-modal modelwith a single frame, they utilize multiple combinationsof image encoder and text encoder, including ConvNeXt-xlarge and Bert-base (max length= 64) fused by Vi-sualBert , Swin large and Bert-base (max length= 64)fused by VisualBert, ConvNeXt-large and Bert-base (maxlength= 64) fused by concatenation, ConvNeXt-large andDeBERTaV3-base (max length= 64) fused by concate-nation, ConvNeXt-xlarge and Bert-base (max length= 32)fused by concatenation, and Swin large and Bert-base (maxlength= 64) fused by concatenation. Finally, in the multi-modal model with multiple frames, they use two combina-tions, in terms of Swin large and Bert-base (max length=64) fused by VisualBert, and ConvNeXt-large and Bert-base(max length= 64) fused by concatenation, both initializedby weights from the multi-modal model with a single frame.The final score is obtained by the ensemble of all the pre-dictions.",
  ". Overview of team geniuswwg proposed method": "concatenate features (finetuned in all training set)- weight=50/3200, eva02 large patch14 448.mim m38m ft in22k + bert-base-uncased (max length=75)+ decoder6, concatenate features (trained in fold 2,num fold=10)- weight=87.5/3200, eva02 large patch14 448.mim m38m ft in22k + bert-base-uncased (max length=75) + decoder6, concatenate features (finetuned in all trainingset)- weight=50/3200, eva02 large patch14 448.mim m38m ft in22k + roberta-base (max length=75) , con-catenate features (trained in fold 2, num fold=10)- weight=87.5/3200, eva02 large patch14 448.mim m38m ft in22k + roberta-base (max length=75),concatenate features (finetuned in all training set)",
  ".3geniuswwg": "Team geniuswwg wins third place in the image track.They propose an innovative approach to assess the qualityof AIGC by treating it as a regression task under specifiedprompt conditions. It employs a dual-source CLIP textencoder to interpret prompts, combining their features for anuanced understanding. Image features are extracted usingmodels like ConvNeXt , pre-trained on ImageNet ,and adapted for interaction with the text features and vi-sion features. A feature mixer module then blends the textand video features, using dot product and concatenation tomodel their correlation and conditional relationship. Thefinal quality score is predicted by a two-layer Multi-LayerPerception (MLP). To enhance generalization, the system applies light data augmentations like flips and brightnessadjustments. The ensemble method further refines the as-sessment by blending predictions from three diverse mod-els, normalized for consistency, and averaged to produce arobust quality evaluation. shows the overview oftheir method.Concretely, they use the frozen CLIP text encoder to en-code the prompt, whose pre-trained weights are broughtfrom two different open-source CLIP implements (i.e.Open-CLIP and EVA-CLIP ) and pre-trained ondifferent datasets (e.g. DFN-5B , LAION-2B ,DataComp-1B and WebLI). They build a hybridprompt encoder by simply concatenating the output featuresfrom these two different CLIP text encoders. After obtain-ing the text features for prompts, they use a trainable denselayer as a prompt adapter, to align features to make betterinteraction with image features.They simply use ConvNeXt-Small (or ViT and any other backbones with ImageNet pre-trainedweights) as the vision backbones.Similarly, they use atrainable dense layer as a vision adapter.After the adapted prompt feature and vision featureshave been obtained, they use a module named feature mixerto make these two features interact with each other. Theypropose to use the features of prompts as a condition. Theyintroduce two types of lightweight feature mixers: dot prod-uct and concatenation. The dot product can more effectivelymodel the correlation between the generated images and theprompt. The use of concatenation is more akin to treatingthe prompt as a conditional factor. They employ these twotypes of feature mixers with different experts and ultimatelyutilize them for model blending. After obtaining the fusedfeatures, they employ a two-layer MLP as the predictionhead to regress the final quality score.Besides, they use random horizontal flip, slight randomresized crop, and slight brightness contrast transformationfor augmentation. These augmentations are relatively minorand generally do not affect the subjective quality assessmentof the image, but can improve the models generalizationability.They blend 3 different models with different vision back-bones: ConvNeXt , ViT-Transformer , and EVA02-Transformer .Firstly, we normalize the predictedscores of each model on the testing set, so that differentmodels have the same mean and variance of prediction. Fi-nally, they blend all the models by averaging.",
  ". Overview of team Yag proposed method": "normalizing and rescaling the local features via a featurepooling operation, they obtain global frame-level featuresthrough the amalgamation of local representations acrossa multi-scale fusion module, which comprises a series oftransformer layers. Subsequently, they concatenate the lo-cal and global features along the channel dimension and in-put them into a linear layer to ascertain the quality score,which is inspired by TReS . This methodology har-nesses spatial and temporal information from both globaland local perspectives, thereby enhancing the perceptual ca-pability of video quality assessment. For the image-text similarity prediction component, theyemploy PickScore to predict the similarity between im-ages and text. They input the results from both the qualityprediction and similarity prediction into a fully connectedlayer to derive the final quality score. The overview of theproposed method is shown in . Besides the provided training data,they also useCLIVE , LIVE , KonIQ-10K , KADID-10K , AGIQA-1k , AGIQA-3K , AIG-CIQA2023 and PKU-I2IQA as additional data.The training images are paired-cropped into 256 256patches for the image quality prediction component and224 224 patches for the image-text similarity predictioncomponent.They train the model using the Adam opti-mizer, setting the initial learning rate to 2e5 for the Swin",
  "QA-FTE": "Team QA-FTE proposes a vision-language fused videoquality evaluator, which is designed for AIGC. Consider-ing the quality of AI-generated images is affected by theconsistency of vision and language, they use CLIP asthe backbone model. Specifically, they first use the CLIPencoder to extract the vision feature of the image and thelanguage feature of the prompt. Then, a bilinear pooling isused to obtain an interactive feature, which represents theconsistency between vision information and language in-formation. Finally, the above features are fused to predictAI-generated image quality scores.",
  "HUTB-IQALab": "Team HUTB-IQALab proposes a novel mixture-of-experts boosted visual perception and semantic-aware qual-ity assessment for AI-generated images. Firstly, they designthe visual perception network to establish perceptual rulesto obtain visual perception features. Secondly, they enhancethe diversity of degradation-specific knowledge through thesemantic-aware network, generating semantic-aware fea-tures. Thirdly, instead of fusing on predicted image scores,",
  ". Overview of team HUTB-IQALab proposed method": "they propose to conduct cross-attention on visual percep-tion and semantic-aware features, so that they can obtaincomprehensive features and the inherent correlation be-tween these features. Finally, they propose a mixture-of-experts model, involving multiple experts working collab-oratively. Each expert is responsible for a specific set offeatures and outputs a corresponding prediction score. Themixture of multiple experts will ultimately yield a holistic,perceptually-aware score. shows the overview ofthe proposed method.",
  "IQ Analyzers": "IQ Analyzers propose a methodology that adopts a Mixture-of-Experts approach, integrating a broad spectrum of fea-ture types.This includes low-level quality features ob-tained from Re-IQA , text-to-image alignment featuresvia BLIP2 and ImageBind , image aesthetics repre-sentations from VILA , the naturalness attributes of im-ages as judged by DINOv2 , and traits from the text-to-image human preference model, ImageReward . Thefeatures are combined and utilized to train an ElasticNetmodel, which maps the aggregated representation to theMOSs.",
  "Team PKUMMCAL believes that compared to the task ofNatural Scene Image Quality Assessment (NSIQA), whichfocuses only on the perceptual quality of images, the qual-": "ity evaluation of AI-generated images needs to consider thetext-image consistency additionally. Therefore, they bringtextual information into the model using a text encoder pre-trained in CLIP . In terms of training methods, inspiredby the works on natural image quality evaluation based onmulti-task learning, they introduce additional tasks, hopingthat the model can learn auxiliary knowledge from them.Unlike natural images, the distortion types of AI-generatedimages are difficult to distinguish directly, so they choose topredict the generative model used for creating AI-generatedimages as their auxiliary task. At the same time, to utilizethe consistency of text and image information, they inte-grate the fine-tuning method from CLIPIQA into theirmodel. They specifically designed three models, which areResNet50-based , DINOv2-based , and ConvNeXt-Based . Besides, they integrate the HyperNet part ofHyperIQA into the ResNet50-based model to achievesemantic adaptive evaluation for different images. The three models all share a similar framework, whichis a dual-stream architecture to simultaneously process theimage and its associated textual prompt.To fusion thevisual and textual information, they propose an attention-based module. The fused feature and the visual feature areboth fed to the quality prediction head, while the visual fea-ture is also fed to the generation model classification head.Meanwhile, there is something different in ResNet50-basedmodel architecture. They integrate several effective mod-ules proposed for the NSIQA task, as illustrated.",
  "BDVQAGroup": "Team BDVQAGroup chooses two methods to assess thequality of AIGIs. One is Q-Align , which is basedon large multi-modality models (LMMs).Q-Align con-verts MOSs into rating levels and uses classification to teachLMMs with text-defined rating levels instead of scores.During inference, it extracts the close-set probabilities ofrating levels and performs a weighted average to obtain theLMM-predicted score. Another is based on MSTRIQ ,a Swin-Transformer based method. They use several dataaugmentation methods to increase the training dataset andenhance the robustness of MSTRIQ, which are: 1) Expandthe image along its longer side to form a square. 2) Ran-domly rotate the image at a degree of 90. 3) Resize the im-age to 448 448. 4) Randomly resize and crop the imageat a ratio of 0.7.They use a Q-Align model pre-trained on KonIQ ,SPAQ , KADID , AVA , and LSVQ , andfinetune this model through three strategies. The first modelis based on the Q-Align Image Quality Scorer, which is fine-tuned for 4 epochs on the AGIQA-1K images and thenfinetuned for another 2 epochs on the provided training im-ages. The second model is based on the Q-Align ImageAesthetic Scorer, which is also finetuned for 2 epochs onthe provided training images. The third model is based on the Q-Align Image Quality Scorer, which is finetuned for 2epochs on the provided training images. The fourth modelis an MSTRIQ model pre-trained on TID2013 , KonIQ-10k and PIPAL , and finetune this model for 150epochs on the provided training images.The following methods are implemented to increasemodel performance: 1) Expand the image along its longerside to form a square. 2) Randomly crop the image 18 times.The crop size is 384 384 for all the 18 patches. 3) Theyuse four models to predict and weight the four prediction re-sults according to the following weights to obtain the finaloutput. That is:",
  "JNU 620": "Team JNU 620 designs a method based on StairIQA ,which includes two parts, a staircase network, and an imagequality regressor. To make the model pay more attention tothe important features, they added channel attention at theend of the staircase network. Moreover, to make full useof the prompts, they leverage the Sentence2Vec technologyto convert prompts into sentence vectors. Then, the sen- tence vectors are transferred into the multi-layer perceptron,which strengthens the representation of the features. Af-ter extracting prompt-aware features by the multi-layer per-ceptron and quality-aware features by the staircase network,they add these features and map them to the quality scoreswith a regression model. shows the overview ofthe proposed method.They use seven models as the backbone of the proposedmodel, including ShuffleNet , MobileNetV2 ,MobileNetV3 ,ResNet50 ,Res2Net50 ,ResNeXt50 , and ResNeSt . The weights of thebackbone are initialized by training on ImageNet , andother weights are randomly initialized. The proposed modelis only trained on the provided image training set. In thetraining stage, images are resized to 680 680 and ran-domly cropped with resolutions of 640 640. The Adammethod is employed for optimization with 1 = 0.9 and2 = 0.999. The initial learning rate was set to 3 103.MSE loss is used as the loss function for training. In thetesting phase, the seven models are used for the ensemble.By performing the model ensemble, the results produced bymultiple models are averaged for better results.",
  "MT-AIGCQA": "Similar to VBench , team MT-AIGCQA integrates a va-riety of basic models by fine-tuning or directly testing on theprovided dataset to obtain image quality scores in differentdimensions. The basic models include improved versionsof BLIP , CLIP , etc. The inspection dimensionsinclude image-text consistency, image quality, etc. Finally,they fuse the scores of the basic models to obtain the finalMOS.Specifically, for the image quality dimension, they usethree models to obtain deep differential information, basedon ResNet , NAS , and Swin Transformer re-spectively. Since there is no separate image quality score inthe training set, the MOS is expressed as the target imagequality score. For the image-text consistency dimension,they obtaine the image-text consistency score by finetun-ing BLIP on the training set. Since there is no sepa-rate image-text consistency score in the training set, theyregard image-text pairs with MOSs exceeding 2.5 (rang-ing from 0 to 5) as matches, and MOSs lower than 2.5 asmismatches. For the overall image quality dimension, theyadded an MLP layer to BLIP for the regression of MOS.In the testing phase, they use the five models trained inthe previous stage to obtain the corresponding basic scoresrespectively, and then use a series of pre-trained models toobtain the corresponding scores of image-text pairs, includ-ing CLIP , Q-Instruct and ImageReward .Following , they obtain the corresponding BLIP scoreand CLIP score. Finally, for images generated by different",
  "IVL": "Team IVL exploits BLIP-2 to encode prompt and im-age, respectively. BLIP-2 consists of a vision encoder, alanguage model, and a Querying Transformer (Q-Former).The input image is first resized to the resolution of 224224pixels and then fed to the model that outputs a feature mapof 32768 features. Spatial features are finally averaged toobtain the 768-dimensional feature vector. The text promptis first tokenized and then fed to the model which outputs afeature map with shape 12 768. Following , they se-lect the first token as representative of the whole text input.The two feature vectors are l2-normalized and concatenatedinto a 1536-dimensional feature vector. A Support VectorRegression (SVR) machine with a Radial Basis Function(RBF) kernel is used to map the features into the final qual-ity score.",
  "CVLab": "Team CVLab proposed model is based on a pre-trainedtext encoder (CLIP ) and an image encoder (Con-vNeXt ). They use the image encoder to extract fea-tures from the images, and then pass those features with thedropout function (ratio 0.3) to a full-connection layer with1000 input and 512 output. At the same time, they use thetext encoder and the tokenized prompts, to extract the textfeatures. Next, they concatenate the image features and textfeatures and provide a vector with 1024 dimensions. Theyuse this concatenated image-text feature to predict the finalMOS with another fully connected layer.",
  ".14z6": "Team z6 introduces a network that combines image and textfeatures for assessing the quality of AI-generated images.The network comprises three main components. The initialcomponent is the image feature encoder, inspired by the ar-chitecture of the staircase network . For the text featureencoder, they employ a basic transformer network. Finally,the feature fusion component utilizes a concatenation func-tion along with a straightforward 1 1 convolution opera-tion.",
  "IVP-Lab": "The proposed method of team IVP-Lab represents a hy-brid model that incorporates both textual and visual datato evaluate the quality of the generated video. The men-tioned model is employed to process the video and its re-lated text, mapping the video and textual data into featurevectors. Multiple mapping layers are employed to align thetext and video-based feature vectors.In evaluating the quality of AI-generated videos, severalfactors should be considered. These include the videossnatural appearance considering both spatial and temporalinformation and preservation of structural information es-pecially in the spatial domain. Another important factor isthe conceptual relevance of the videos content.In the proposed method, two quality-based feature vec-tors are computed: one assesses the similarity of the videoto the text, while the other evaluates the video quality inde-pendently. These two feature vectors are then subjected toan inner product, resulting in a final vector for quality as-sessment. The resultant quality-based feature vector is fedto the fully connected network to estimate the quality of theAI-generated Videos.",
  "IMCL-DAMO": "Team IMCL-DAMO is the final winner of the videotrack. They propose a versatile video quality evaluator forAI-generated content, which can learn technical quality,aesthetic quality, and text-video alignment from differentpriors, as shown in . Specifically, the input videois pre-processed to handle the disentangled information ex-traction from the three perspectives (i.e., technical quality,aesthetic, text-video alignment): they utilize the fragmentsextracted from original videos for technical quality assess-ment and resize the videos for aesthetic assessment and text-video alignment. Then, these separate inputs pass throughmultiple branches (technical branch, aesthetic branch, andalignment branch) to obtain the related score for differentperspectives. To fuse the scores from different prior, wesimply add them. Finally, they use PLCC loss and rank lossfor score regression of each branch.During training, they train the technical branch andaesthetic branch by loading the pre-trained weight fromLSVQ . Then the alignment branch is trained with40% unfixed parameters, loading the pre-trained weightfrom ImageReward . Note that these datasets are notinvolved in training with the provided video dataset. Fi-nally, they finetune the technical branch, aesthetic branch,and alignment branch with 85% unfixed parameters for latefusion. During testing, they test their network using videosprovided on the official website. A self-ensemble strategyis used during testing, and it brings performance gains of",
  ". The overview of team Kwai-kaa proposed LMM and CLIP branches": "about 0.008 on PLCC.In the training phase, the input frames for the aestheticand text-video alignment branches are resized to 224224.And fragments are sampled for the technical branch likein DOVER . They use the Swin Transformer asthe technical branch backbone, the ConvNeXt as theaesthetic branch, and BLIP as the alignment branch.The training process takes 12 hours on 4 V100 GPUs. Dur-ing testing, it takes 4 seconds for each video including theensemble strategy.",
  "Kwai-kaa": "Team Kwai-kaa wins second place in the video track. Theypropose to tackle the challenge by leveraging LMMs. Theyfollow a similar design to Q-Align , which is based onmPLUG-Owl2 , with the exception of the conversationformats. Specifically, they reformulate the conversation forAI-generated video assessment as follows:#User: <video> How would you judge the quality ofthis AI-generated video given the input prompt <prompt>?#Assistant: The quality of the video is <level>.In the context of conversation, <video> denotes the in-put video, <prompt> denotes the prompt used to generatethe input video, and <level> denotes the predicted scoreby LMMs.However, the exclusive utilization of the Q-Align achieves limited performance due to the quantification of MOS into 5 discrete text-defined levels.This quantiza-tion strategy restricts the models ability to learn more pre-cise quality scores. To complement the Q-Align architec-ture, they introduce an additional CLIP-based archi-tecture, leveraging it as a robust feature extractor for pre-dicting precise quality scores with MSE constraints. Theoriginal CLIP was tailored for images and cannot capturethe temporal consistency and interconnectedness betweenvideo frames, which significantly influences video quality.To address this, they incorporate attention layers betweenframes to capture temporal relationships. As the promptserves as a global abstract of the video, they believe thatassessing alignment is sufficient within the LMM branchand, therefore, does not utilize text information within theCLIP branch. By employing diverse architectures and train-ing strategies across different branches, they aim to enhancethe variety of information and contribute to improved re-sults. The final score is obtained by averaging the results ofeach branch. The overview of LMM and CLIP branches isshown in .In particular, the LMM branches are finetuned withthe pre-trained One-Align weights, and the CLIPbranches are finetuned with the pre-trained clip-vit-large-patch14 weights. They employed two finetuning strategiesfor the LMM branch: Vision Encoder & Vision Abstrac-tor (VEVA) finetuning and full model finetuning. The pre-trained model of the LMM branch is fine-tuned with aninitial learning rate of 2 105, gradually decreasing to0 using a cosine scheduler. Each strategy is trained for 2 epochs. The precise labels are divided into five text-definedlevels: <excellent>, <good>, <fair>, <poor>, and<bad>. During training, they utilize 8 Tesla V100 GPUs,with a batch size of 24 for VEVA fine-tuning and 8 for fullfine-tuning. The fine-tuning process for the LMM branchtakes approximately 1 hour each to complete. The CLIPbackbone is trained for 20 epochs, with a learning rate setto 1 106. The training process is carried out on 8 TeslaA100 GPUs, with a batch size of 8. It takes approximately8 hours to complete the training process. The provided gen-erated videos consist of 4-second sequences at 4 frames persecond and all frames are provided as input to the models.For videos with 15 frames, they pad the last frame to gen-erate a complete set of 16 frames. All frames are resizedto 448 448 and training processes are finished with theAdamW optimizer.They get the MOS values of LMM branches via theweighted average of the LMM-predicted probabilities foreach rating level, which can be denoted as:",
  "5j=1 elj,(3)": "where wi is the logit weight for text level i and liis the corresponding logit output.they set the value ofw to [1, 0.75, 0.5, 0.25, 0] for text-label <excellent>,<good>, <fair>, <poor>, and <bad>. They rescalethe predicted scores of the CLIP branches to by di-viding them with a constant factor of 100. To get the finalscore, they combined the scores from different branches us-ing a weighted approach. Each video takes approximately 1second to process in a single LMM branch in the inference,while the CLIP branch requires approximately 1.46 secondsper video.",
  "SQL": "Team SQL wins third place in the video track. Theypropose to evaluate the video quality of AIGVs from fivedimensions: aesthetic scores, technical scores, video-textconsistency, fluency, and temporal consistency, as shownin .They refer to aesthetic and technical as-pects as visual harmony and refer to fluency and tempo-ral consistency as temporal dynamics. Additionally, theyapply model assembling and domain distribution estima-tion to optimize the model performance. They referred toDOVER for the aesthetic and technical evaluation ofthe videos. To measure video-text consistency, they applyexplicit prompt injection, implicit text guidance, DIFT feature, and caption similarity. They inject the correspond-ing prompts of the videos into the video features usingcross-attention.They also utilized BVQIs implicit textmethod and jointly optimized the evaluation network",
  ". Five dimensions for SQL team proposed method": "using both implicit text and explicit prompts.They fur-ther use the diffusion feature to measure the per-frame text-content consistency. Building upon this, they utilize thecross-modal text-video multi-modal large language model,Video-LLaVA to generate additional captions for eachvideo segment. They then calculate the similarity betweenthe generated captions and the given prompts to further op-timize the network. As for fluency, they incorporate an op-tical flow estimation module to measure the flicker degreebetween keyframes in the videos. After estimating the opti-cal flow between keyframes using the RAFT algorithm,they assess video stability by computing the flow magnitudeand warp loss. Regarding temporal consistency, they intro-duced pre-trained video understanding backbones such asUniformer-V2 , UnmaskedTeacher , and MVD to extract robust spatiotemporal features. In regard to model assembly, they incorporate addi-tional five models into their main model: FAST-VQA ,Faster-VQA , ZoomVQA , XCLIP , and Im-ageReward . Its important to note that the integra-tion of these models serves as a finishing touch, and theyhavent thoroughly adjusted the weights of the integration.The enhancement they bring is usually found in three deci-mal places. Considering the domain distribution differences in theresults generated by different models, in supervised learn-ing, they predict not only the score but also which text-to-video model generated the video. This additional predictionaids the model in better understanding video features. Ex-periments have shown that this significantly enhances theperformance of their model.",
  ".5finnbingo": "Team finnbingo and team pengfei in the image track are thesame teams. They use the same architecture as introduced in.1.1. To adapt their methodology for video qualityassessment, they treat a video as a collection of N frames,from which they selectively extract n frames at regular in-tervals to represent the video. They then calculate the aver-age score of these n frames to determine the overall qualityscore of the video.",
  "PromptSync": "The method proposed by team PromptSync is structuredinto three main components. At the video level, buildingupon the FAST-VQA framework, they introduce ad-ditional feature constraints. They use the CLIP imageencoder to obtain image features for all video frames andapply a cross-attention mechanism to detect semantic dis-continuities between frames. Furthermore, they project thevideo features derived from FAST-VQA into the text fea-ture space obtained from the CLIP text encoder, calculatingthe semantic consistency between the video and the prompt.The final AIGC evaluation score is aggregated from theFAST-VQA video features, video-prompt consistency fea-tures, and frame sequence consistency features.At the segment level, they replace the backbone modelwith Swin Transformer , concatenate text features fromthe CLIP text encoder, and utilize image features extractedby Swin Transformer and video features from the slow-fast model. The entire network is pre-trained on theLSVQ dataset and then finetuned on the competitionstraining set.Last, they conduct a frame-level evaluation of videoquality. By leveraging the CLIP text encoder, they extracttext features and concatenate them with image features ob-tained from the convent model to perform scoring. By cap-turing video features from different perspectives, these threecomponents collectively contribute to their comprehensiveAI-generated video quality assessment score.",
  "IPPL-VQA": "The architecture proposed by IPPL-VQA is composed oftext branches and image branches. The input of the textbranch is the text description of the image, and text featuresare extracted by the frozen text encoder of the pre-trainedCLIP-B-32 model . There are two ways of sampling theimage part (MaxVQA method ): 1. Sampling distincttexture details through cropping and splicing fragments; 2.Scaled sampling containing global information. The im-ages of these two sampled branches undergo a frozen CLIPimage encoder and two different temporal fusion models re-spectively. The features of both branches are concatenatedand reduced to the width of the textual features with an MLPlayer. The inner product of the final text and image featuresare calculated to get a matching score.",
  "Oblivion": "Team Oblivion uses the Video Swin Transformer asthe visual backbone, and then they use the CLIP textencoder as the text feature extractor, using text featuresto enhance visual understanding to evaluate the quality ofthe video. They use the Swin-tiny network pre-trained onthe Kinetics-400 dataset to initialize the Video SwinTransformer backbone, and the ResNet-50 networkpre-trained on the Kinetics-400 dataset to initialize the textencoder in the CLIP model.",
  "UBC DSL Team": "UBC DSL Team aims to build a video quality assessmentmodel leveraging multi-faceted video representations, tak-ing the visual quality, text prompt, and motion coherenceinto account. Specifically, they use the off-the-shelf pre-trained video encoder VideoMAE and text encoderCLIP to extract vision and language features. Addi-tionally, they use the Inflated 3D Convnet (I3D) as an-other video feature extractor, following prior work on gen-erated video quality assessment.To address temporal inconsistencies in AI-generateddata, such as unnatural movements or blurring, they empha-size the importance of motion coherence in video qualityevaluation. Leveraging the pre-trained motion tracking net-work PIPs++ , they extract motion features by track-ing key points trajectories in videos. They calculate thevelocity and acceleration of these points, underpinning the notion that realistic motions should exhibit consistent ac-celeration. This approach yields dense motion features, en-riching their VQA models ability to detect and interprettemporal anomalies effectively.After extracting video and text representations using var-ious encoders, they use a vanilla transformer consisting ofan encoder layer only to mix these representations in the to-ken space. They freeze all pre-trained encoders to preventoverfitting. They add an additional global token to improvenetwork capacity and use the global token to read out thefinal prediction score.",
  "Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisser-man. Frozen in time: A joint video and image encoder forend-to-end retrieval. In IEEE International Conference onComputer Vision, 2021. 3": "Joao Carreira and Andrew Zisserman. Quo vadis, actionrecognition? a new model and the kinetics dataset. In Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition, pages 62996308, 2017. 17 Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, PingLuo, Huchuan Lu, and Zhenguo Li. Pixart-: Fast train-ing of diffusion transformer for photorealistic text-to-imagesynthesis. 2310.00426, 2023. 3",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova.Bert:Pre-training of deep bidirectionaltransformers for language understanding.arXiv preprintarXiv:1810.04805, 2018. 7, 8, 16": "Yunlong Dong, Xiaohong Liu, Yixuan Gao, Xunchu Zhou,Tao Tan, and Guangtao Zhai.Light-vqa:A multi-dimensional quality assessment model for low-light videoenhancement.In Proceedings of the 31st ACM Interna-tional Conference on Multimedia, pages 10881097, 2023.3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold,Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-age is worth 16x16 words: Transformers for image recog-nition at scale. ICLR, 2021. 8, 13",
  "Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang,Xinlong Wang, and Yue Cao. Eva-02: A visual represen-tation for neon genesis. arXiv preprint arXiv:2303.11331,2023. 7, 8": "Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and ZhouWang. Perceptual quality assessment of smartphone pho-tography.In IEEE Conference on Computer Vision andPattern Recognition, pages 36773686, 2020. 11 Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, andKaiming He. Slowfast networks for video recognition. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 62026211, 2019. 16 Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, JonathanHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-acomp: In search of the next generation of multimodaldatasets. Advances in Neural Information Processing Sys-tems, 36, 2024. 8 Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-YuZhang, Ming-Hsuan Yang, and Philip Torr.Res2net: Anew multi-scale backbone architecture. IEEE transactionson pattern analysis and machine intelligence, 43(2):652662, 2019. 12 Yixuan Gao, Yuqin Cao, Tengchuan Kou, Wei Sun, Yun-long Dong, Xiaohong Liu, Xiongkuo Min, and GuangtaoZhai. VDPVE: VQA dataset for perceptual video enhance-ment. arXiv preprint arXiv:2303.09290, 2023. 3",
  "Deepti Ghadiyaram and Alan C Bovik.Massive on-line crowdsourced study of subjective and objective pic-ture quality.IEEE Transactions on Image Processing,25(1):372387, 2015. 9": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, MannatSingh, Kalyan Vasudev Alwala, Armand Joulin, and IshanMisra. Imagebind: One embedding space to bind them all.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1518015190, 2023.10 S Alireza Golestaneh, Saba Dadsetan, and Kris M Kitani.No-reference image quality assessment via transformers,relative ranking, and self-consistency. In Proceedings of theIEEE/CVF winter conference on applications of computervision, pages 12201230, 2022. 9 Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,George Toderici, Susanna Ricco, Rahul Sukthankar, et al.Ava: A video dataset of spatio-temporally localized atomicvisual actions. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 60476056,",
  ". 11": "Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, XingZhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-GangJiang, and Hang Xu.Reuse and diffuse:Iterativedenoising for text-to-video generation.arXiv preprintarXiv:2309.03549, 2023. 3 Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen.Giqa: Generated image quality assessment. In ComputerVisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part XI 16, pages369385. Springer, 2020. 13 Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, YuQiao, Dahua Lin, and Bo Dai. Animatediff: Animate yourpersonalized text-to-image diffusion models without spe-cific tuning. arXiv preprint arXiv:2307.04725, 2023. 3",
  "DavidHolz.Midjourney.https : / / www .midjourney.com, 2023. 3": "Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and DietmarSaupe. Koniq-10k: An ecologically valid database for deeplearning of blind image quality assessment. IEEE Transac-tions on Image Processing, 29:40414056, 2020. 9, 11 Andrew Howard, Mark Sandler, Grace Chu, Liang-ChiehChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 13141324, 2019. 12 Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger. Densely connected convolutional net-works. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 47004708, 2017. 12 Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, andXihui Liu.T2i-compbench:A comprehensive bench-mark for open-world compositional text-to-image genera-tion. Advances in Neural Information Processing Systems,36, 2024. 12 Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, ChenyangSi, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, QingyangJin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen,Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench:Comprehensive benchmark suite for video generative mod-els. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, 2024. 2, 3, 12",
  "Gu Jinjin, Cai Haoming, Chen Haoyu, Ye Xiaoxing,": "Jimmy S Ren, and Dong Chao. Pipal: a large-scale imagequality assessment dataset for perceptual image restoration.In Proceedings of the Computer VisionECCV 2020: 16thEuropean Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part XI 16, pages 633651. Springer, 2020.11 Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Vi-ola, Tim Green, Trevor Back, Paul Natsev, et al.Thekinetics human action video dataset.arXiv preprintarXiv:1705.06950, 2017. 13, 17 Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Mi-lanfar, and Feng Yang.Vila: Learning image aestheticsfrom user comments with vision-language pretraining. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1004110051, 2023.10 Levon Khachatryan, Andranik Movsisyan, Vahram Tade-vosyan,Roberto Henschel,Zhangyang Wang,ShantNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 1595415964, 2023. 3 Yuval Kirstain, Adam Polyak, Uriel Singer, ShahbulandMatiana, Joe Penna, and Omer Levy. Pick-a-pic: An opendataset of user preferences for text-to-image generation.Advances in Neural Information Processing Systems, 36,2024. 2, 9",
  "Hyunsuk Ko, Dae Yeol Lee, Seunghyun Cho, and Alan CBovik. Quality prediction on deep generative images. IEEETransactions on Image Processing, 29:59645979, 2020.13": "Tengchuan Kou, Xiaohong Liu, Wei Sun, Jun Jia, XiongkuoMin, Guangtao Zhai, and Ning Liu. Stablevqa: A deep no-reference quality assessment model for video stability. InProceedings of the 31st ACM International Conference onMultimedia, pages 10661076, 2023. 3 Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, ChunyiLi, Haoning Wu, Xiongkuo Min, Guangtao Zhai, and NingLiu. Subjective-aligned dateset and metric for text-to-videoquality assessment.arXiv preprint arXiv:2403.11956,2024. 2, 3 Chunyi Li, Tengchuan Kou, Yixuan Gao, Yuqin Cao, WeiSun, Zicheng Zhang, Yingjie Zhou, Zhichao Zhang, Haon-ing Wu, Weixia Zhang, Xiaohong Liu, Xiongkuo Min,and Guangtao Zhai.Aigiqa-20k: A large database forai-generated image quality assessment. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition Workshops, 2024. 2, 3 Chunyi Li, Haoning Wu, Zicheng Zhang, Hongkun Hao,Kaiwei Zhang, Lei Bai, Xiaohong Liu, Xiongkuo Min,Weisi Lin, and Guangtao Zhai.Q-refine: A perceptualquality refiner for ai-generated image.arXiv preprintarXiv:2401.01117, 2024. 2 Chunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun,Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, and WeisiLin. Agiqa-3k: An open database for ai-generated imagequality assessment. IEEE Transactions on Circuits and Sys-tems for Video Technology, 2023. 2, 5, 9, 13 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2: Bootstrapping language-image pre-training withfrozen image encoders and large language models. In In-ternational conference on machine learning, pages 1973019742. PMLR, 2023. 10, 12 Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for uni-fied vision-language understanding and generation. In In-ternational conference on machine learning, pages 1288812900. PMLR, 2022. 2, 12, 14",
  "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, andLi Yuan.Video-llava: Learning united visual represen-tation by alignment before projection.arXiv preprintarXiv:2311.10122, 2023. 15": "Hanhe Lin, Vlad Hosu, and Dietmar Saupe. Kadid-10k:A large-scale artificially distorted iqa database.In 2019Eleventh International Conference on Quality of Multime-dia Experience (QoMEX), pages 13. IEEE, 2019. 9, 11 Rongcheng Lin, Jing Xiao, and Jianping Fan. Nextvlad:An efficient neural network to aggregate frame-level fea-tures for large-scale video classification. In Proceedingsof the European Conference on Computer Vision (ECCV)Workshops, pages 00, 2018. 16 Xiaohong Liu, Radu Timofte, Yunlong Dong, Zhiliang Ma,Haotian Fan, Chunzheng Zhu, Xiongkuo Min, GuangtaoZhai, Ziheng Jia, Mirko Agarla, et al. Ntire 2023 qualityassessment of video enhancement challenge. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 15511569, 2023. 3 Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang,Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Ray-mond Chan, and Ying Shan.Evalcrafter: Benchmark-ing and evaluating large video generation models. arXivpreprint arXiv:2310.11440, 2023. 2, 3 Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, ShichengLi, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: A bench-mark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Process-ing Systems, 36, 2024. 2, 3 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov.Roberta:A ro-bustly optimized bert pretraining approach. arXiv preprintarXiv:1907.11692, 2019. 7, 8",
  "sampling for image and video quality assessment. arXivpreprint arXiv:2401.02614, 2024. 8": "Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,et al. Swin transformer v2: Scaling up capacity and resolu-tion. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1200912019,2022. 8, 9 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 1001210022, 2021. 12, 14, 16 Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-enhofer, Trevor Darrell, and Saining Xie. A convnet forthe 2020s. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1197611986, 2022. 7, 8, 10, 12, 14, 16 Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,Stephen Lin, and Han Hu. Video swin transformer. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 32023211, 2022. 17 Yiting Lu, Xin Li, Bingchen Li, Zihao Yu, Fengbin Guan,Xinrui Wang, Ruling Liao, Yan Ye, and Zhibo Chen. Aigc-vqa: A holistic perception metric for aigc video qualityassessment. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition Workshops,2024. 13",
  "Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrickvon Platen, Apolinario Passos, Longbo Huang, Jian Li, andHang Zhao. Lcm-lora: A universal stable-diffusion accel-eration module, 2023. 3": "Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, andTieniu Tan.Videofusion: Decomposed diffusion mod-els for high-quality video generation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1020910218, 2023. 3 Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang,Gaofeng Meng, Jianlong Fu, Shiming Xiang, and HaibinLing.Expanding language-image pretrained models forgeneral video recognition.In European Conference onComputer Vision, pages 118. Springer, 2022. 15 Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.Dinov2: Learning robust visual features without supervi-sion. arXiv preprint arXiv:2304.07193, 2023. 10 Fei Peng, Huiyuan Fu, Anlong Ming, Chuanming Wang,Huadong Ma, Shuai He, Zifei Dou, and Shu Chen. Aigc im-age quality assessment via image-prompt correspondence.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, 2024. 6",
  "tives. Signal processing: Image communication, 30:5777,2015. 11": "Bowen Qu, Xiaoyu Liang, Shangkun Sun, and Wei Gao.Exploring aigc video quality: A focus on visual harmony,video-text consistency and domain distribution gap. In Pro-ceedings of the IEEE CVF Conference on Computer Visionand Pattern Recognition Workshops, 2024. 15 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International conference on machine learning,pages 87488763. PMLR, 2021. 2, 6, 8, 9, 10, 12, 14, 16,17 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International conference on machine learning,pages 87488763. PMLR, 2021. 13",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, CaseyChu, and Mark Chen. Hierarchical text-conditional imagegeneration with clip latents. 2204.06125, 2022. 3": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution imagesynthesis with latent diffusion models. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 2 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution imagesynthesis with latent diffusion models. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 3",
  "Robin Rombach, Andreas Blattmann, and Bjorn Ommer.Text-guided synthesis of artistic images with retrieval-augmented diffusion models. 2207.13038, 2022. 3": "Avinab Saha, Sandeep Mishra, and Alan C Bovik. Re-iqa:Unsupervised learning for image quality assessment in thewild. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 58465855,2023. 10 Mark Sandler, Andrew Howard, Menglong Zhu, AndreyZhmoginov, and Liang-Chieh Chen. Mobilenetv2: Invertedresiduals and linear bottlenecks. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition,pages 45104520, 2018. 12",
  "Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and YueCao.Eva-clip: Improved training techniques for clip atscale. arXiv preprint arXiv:2303.15389, 2023. 8": "Wei Sun, Xiongkuo Min, Wei Lu, and Guangtao Zhai. Adeep learning based no-reference quality assessment modelfor ugc videos. In Proceedings of the 30th ACM Interna-tional Conference on Multimedia, pages 856865, 2022. 3,4, 5 Wei Sun, Xiongkuo Min, Danyang Tu, Siwei Ma, andGuangtao Zhai. Blind quality assessment for in-the-wildimages via hierarchical feature fusion and iterative mixeddatabase training. IEEE Journal of Selected Topics in Sig-nal Processing, 2023. 2, 4, 11, 12",
  "Luming Tang, Menglin Jia, Qianqian Wang, Cheng PerngPhoo, and Bharath Hariharan. Emergent correspondencefrom image diffusion. Advances in Neural Information Pro-cessing Systems, 36:13631389, 2023. 15": "Zachary Teed and Jia Deng.Raft: Recurrent all-pairsfield transforms for optical flow.In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, Au-gust 2328, 2020, Proceedings, Part II 16, pages 402419.Springer, 2020. 15 Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.Videomae: Masked autoencoders are data-efficient learnersfor self-supervised video pre-training. Advances in neuralinformation processing systems, 35:1007810093, 2022. 17 Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-ploring clip for assessing the look and feel of images. InProceedings of the AAAI Conference on Artificial Intelli-gence, volume 37, pages 25552563, 2023. 10 Jiarui Wang, Huiyu Duan, Jing Liu, Shi Chen, XiongkuoMin, and Guangtao Zhai. Aigciqa2023: A large-scale im-age quality assessment database for ai generated images:from the perspectives of quality, authenticity and corre-spondence. In CAAI International Conference on ArtificialIntelligence, pages 4657. Springer, 2023. 2, 9 Jing Wang, Haotian Fan, Xiaoxia Hou, Yitian Xu, Tao Li,Xuechao Lu, and Lean Fu. Mstriq: No reference imagequality assessment based on swin transformer with multi-stage fusion. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 12691278, 2022. 11",
  "Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,Xiang Wang, and Shiwei Zhang. Modelscope text-to-videotechnical report. arXiv preprint arXiv:2308.06571, 2023. 3": "Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen,Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang.Masked video distillation: Rethinking masked feature mod-eling for self-supervised video representation learning. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 63126322, 2023. 15 Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, JiashuoYu, Peiqing Yang, et al. Lavie: High-quality video genera-tion with cascaded latent diffusion models. arXiv preprintarXiv:2309.15103, 2023. 3 ZijieJWang,EvanMontoya,DavidMunechika,Haoyang Yang,Benjamin Hoover,and Duen HorngChau. Diffusiondb: A large-scale prompt gallery datasetfor text-to-image generative models.arXiv preprintarXiv:2210.14896, 2022. 2, 3 Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, XinleiChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-vnext v2: Co-designing and scaling convnets with maskedautoencoders. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1613316142, 2023. 7 Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao,Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin.Fast-vqa: Efficient end-to-end video quality assessmentwith fragment sampling. In Proceedings of the ComputerVisionECCV 2022: 17th European Conference, Tel Aviv,Israel, October 2327, 2022, Proceedings, Part VI, pages538554. Springer, 2022. 3, 4, 5, 15, 16 Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou,Wenxiu Sun, Qiong Yan, Jinwei Gu, and Weisi Lin. Neigh-bourhood representative sampling for efficient end-to-endvideo quality assessment. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2023. 15 Haoning Wu, Liang Liao, Jingwen Hou, Chaofeng Chen,Erli Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, andWeisi Lin.Exploring opinion-unaware video quality as-sessment with semantic affinity criterion. arXiv preprintarXiv:2302.13269, 2023. 15 Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-wen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and WeisiLin. Towards explainable in-the-wild video quality assess-ment: a database and a language-prompted approach. InProceedings of the 31st ACM International Conference onMultimedia, pages 10451054, 2023. 3, 16 Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-wen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan,and Weisi Lin.Exploring video quality assessment onuser generated contents from aesthetic and technical per-spectives. In International Conference on Computer Vision(ICCV), 2023. 3, 4, 5, 14, 15 Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, QiongYan, Guangtao Zhai, et al.Q-bench: A benchmark forgeneral-purpose foundation models on low-level vision.arXiv preprint arXiv:2309.14181, 2023. 2 Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, JingwenHou, Guangtao Zhai, et al. Q-instruct: Improving low-levelvisual abilities for multi-modality foundation models. arXivpreprint arXiv:2311.06783, 2023. 2, 12 Haoning Wu, Zicheng Zhang, Weixia Zhang, ChaofengChen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang,Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmmsfor visual scoring via discrete text-defined levels.arXivpreprint arXiv:2312.17090, 2023. 3, 11, 14 Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang,Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang,Wenxiu Sun, Qiong Yan, et al. Towards open-ended vi-sual quality comparison. arXiv preprint arXiv:2402.16641,",
  ". 2": "Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan WeixianLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, XiaohuQie, and Mike Zheng Shou. Tune-a-video: One-shot tuningof image diffusion models for text-to-video generation. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 76237633, 2023. 3 Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hong-sheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings ofthe IEEE/CVF International Conference on Computer Vi-sion, pages 20962105, 2023. 2 Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, andKaiming He. Aggregated residual transformations for deepneural networks. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 14921500,2017. 12 Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, QinkaiLi, Ming Ding, Jie Tang, and Yuxiao Dong.Imagere-ward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Pro-cessing Systems, 36, 2024. 2, 10, 12, 13, 15 Junfeng Yang, Jing Fu, Wei Zhang, Wenzhi Cao, Limei Liu,and Han Peng.Moe-agiqa: Mixture-of-experts boostedvisual perception-driven and semantic-aware quality as-sessment for ai-generated images. In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition Workshops, 2024. 9 Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, HaoweiLiu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.mplug-owl2: Revolutionizing multi-modal large languagemodel with modality collaboration.arXiv preprintarXiv:2311.04257, 2023. 14 Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram,and Alan Bovik. Patch-vq:patching upthe video qualityproblem. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1401914029, 2021. 11, 13, 16 Zihao Yu, Fengbin Guan, Yiting Lu, Xin Li, and ZhiboChen.Sf-iqa: Quality and similarity integration for aigenerated image quality assessment.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition Workshops, 2024. 8 Jiquan Yuan, Xinyan Cao, Changjin Li, Fanyi Yang, JinlongLin, and Xixin Cao. Pku-i2iqa: An image-to-image qualityassessment database for ai generated images. arXiv preprintarXiv:2311.15556, 2023. 9, 13 David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, andMike Zheng Shou. Show-1: Marrying pixel and latent dif-fusion models for text-to-video generation. arXiv preprintarXiv:2309.15818, 2023. 3 Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller,R Manmatha, et al. Resnest: Split-attention networks. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 27362746, 2022. 12",
  "ear convolutional neural network. IEEE Transactions onCircuits and Systems for Video Technology, 30(1):3647,2020. 2, 4, 5": "Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang,and Kede Ma. Blind image quality assessment via vision-language correspondence: A multitask learning perspec-tive. In IEEE Conference on Computer Vision and PatternRecognition, pages 1407114081, 2023. 2, 4, 5, 6 Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.Shufflenet: An extremely efficient convolutional neural net-work for mobile devices. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages68486856, 2018. 12 Zicheng Zhang, Chunyi Li, Wei Sun, Xiaohong Liu,Xiongkuo Min, and Guangtao Zhai.A perceptual qual-ity assessment exploration for aigc images. In 2023 IEEEInternational Conference on Multimedia and Expo Work-shops (ICMEW), pages 440445. IEEE, 2023. 2, 9, 11 Zicheng Zhang, Wei Sun, Yingjie Zhou, Haoning Wu,Chunyi Li, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai,and Weisi Lin. Advancing zero-shot digital human qual-ity assessment through text-prompted evaluation.arXivpreprint arXiv:2307.02808, 2023. 3 Zicheng Zhang, Haoning Wu, Zhongpeng Ji, Chunyi Li,Erli Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min,Fengyu Sun, Shangling Jui, et al. Q-boost: On visual qual-ity assessment ability of low-level multi-modality founda-tion models. arXiv preprint arXiv:2312.15300, 2023. 2 Zicheng Zhang, Yingjie Zhou, Chunyi Li, Kang Fu, WeiSun, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai.A reduced-reference quality assessment metric for texturedmesh digital humans. In ICASSP 2024-2024 IEEE Inter-national Conference on Acoustics, Speech and Signal Pro-cessing (ICASSP), pages 29652969. IEEE, 2024. 3 Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen.Quality-aware pre-trained models for blind image qualityassessment. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2230222313, 2023. 15 Yang Zheng, Adam W Harley, Bokui Shen, Gordon Wet-zstein, and Leonidas J Guibas. Pointodyssey: A large-scalesynthetic dataset for long-term point tracking. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 1985519865, 2023. 17"
}