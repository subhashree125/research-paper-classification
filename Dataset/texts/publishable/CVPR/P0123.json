{
  "Abstract": "Channel pruning approaches for convolutional neuralnetworks (ConvNets) deactivate the channels, statically ordynamically, and require special implementation. In addi-tion, channel squeezing in representative ConvNets is carriedout via 1 1 convolutions which dominates a large portionof computations and network parameters. Given these chal-lenges, we propose an effective multi-purpose module fordynamic channel sampling, namely Pick-or-Mix (PiX), whichdoes not require special implementation. PiX divides a setof channels into subsets and then picks from them, wherethe picking decision is dynamically made per each pixelbased on the input activations. We plug PiX into prominentConvNet architectures and verify its multi-purpose utilities.After replacing 1 1 channel squeezing layers in ResNetwith PiX, the network becomes 25% faster without losingaccuracy. We show that PiX allows ConvNets to learn bet-ter data representation than widely adopted approaches toenhance networks representation power (e.g., SE, CBAM,AFF, SKNet, and DWP). We also show that PiX achievesstate-of-the-art performance on network downscaling anddynamic channel pruning applications.Code:",
  ". Introduction": "Convolutional neural networks (ConvNets) havebeen successfully applied to many machine vision tasks. With the introduction of larger models, a generaltrend is to make them faster via channel pruning. Prior worksin channel pruning focus on making networklighter to accelerate the inference speed. However, someapproaches require specialized convolution implementationsand pre-trained models , or they are constrained by thebaseline accuracy . Moreover, whether static or dynamic,these channel pruning methods remove or deactivate thenetwork channels, thus hindering the network from handlingdifficult inputs .It is a fundamental property of ConvNets that for a givenspatial location or pixel in the ConvNets feature map, any",
  ",,,": ". Conceptual overview of PiX in the context of channelreduction for ConvNets. Top: Traditional dense 1 1 convolution.Although not all channels are important, dense convolutions processall the channels equally. Bottom: PiX avoids dense convolutionand samples the channels dynamically from the input by producingsampling probabilities with far fewer FLOPs. PiX is multipurposewithout requiring specialized implementations. one channel may have stronger activation, thus of consider-able importance, while for another pixel, the same channelmight be less important. Therefore, it is crucial to allowthe network to prioritize channels differently per each pixelinstead of dropping a whole channel applied by pruningapproaches. This inspires us to pick neuron-specific outputfrom the channels instead of shutting down an entire channel.In addition, we observe that standard ConvNet designsstill have room for improvement, i.e., 1 1 convolutionlayers (or called channel squeezing layers) dominate in bothnumber and computations without contributing to the re-ceptive field due to their pixel-wise operation nature. Forinstance, ResNet-50 consists of 16 such layers out of50, accounting for 25% (1.05B/4.12B) of overall FLOPs.In this context, we introduce a novel module, namelyPick-or-Mix (PiX) that addresses the computational domi-nance of channel-squeezing layers by dynamically samplingchannels, PiX transforms a feature map X RCHW into another one Y RC/HW (). Essentially,our method picks or mixes C/ channels from the inputC channels with a sampling factor . It divides a set ofchannels into subsets and then outputs one channel fromeach subset via our Pick-or-Mix strategy. PiX samples the",
  "arXiv:2406.10935v1 [cs.CV] 16 Jun 2024": "channel based on the pixel-level runtime decisions made bythe preceding layers; thus, decisions of PiX are dynamicand input-dependent. In addition, Pick-or-Mix does not in-volve extensive pixel-wise convolution, making the networkmore efficient. The simple design allows us to plug PiXinto representative ConvNets. We plug PiX into representa-tive ConvNets for the purpose of faster channel squeezing,network downscaling, and dynamic channel pruning.Our experiments show that PiX can reduce the computa-tional cost of the vanilla channel squeezing layer (i.e., 1 1convolution layer) while maintaining or achieving even bet-ter performance, e.g., ResNet becomes 25% faster withoutbells and whistles (Sec 3.5.1, ). PiX can customizeConvNets in a controlled manner while being faster andmore accurate than the baseline counterpart with similar pa-rameters (Sec. 3.5.2, ), e.g., PiX outperforms recentRepVGG without a complicated training phase whilehaving simple network design. We also observe similar ac-curacy but at reduced parameters (). PiX performsbetter by 3% relative to various recent dynamic channelpruning approaches on ResNet18 with 2FLOPs saving. (Sec. 3.5.3, ).We compare the accuracy and FLOPs of PiX with otherstate-of-the-art approaches. We also conduct transfer learn-ing on PiX-enhanced network on CIFAR-10, CIFAR-100for classification, and CityScapes for semantic segmentation.We observe better performance relative to the baselines.",
  ". Related Work": "Convolutional Neural Networks. The earlier ConvNets are accuracy-oriented but still dominant in the in-dustry , thanks to their high representation power, ar-chitectural simplicity, and customizability. EfficientNet emerged with network architecture search, but due to itsnature of AutoML, it is deep and branched compared to tra-ditional ConvNets . Even after half a decade, ResNetcontinues to improve , indicating its architectural sig-nificance, while VGG-like architecture continues as it isdesign-friendly with low-powered computing devices due toits shallow, easily scalable, and low latency design .This is also visible from ResNet design space explo-ration that provides a competitive alternative to theadvanced ConvNets while being simpler. SENet ,CBAM , and ResNest , Attentional Feature Fusion further depict the importance of older architectures bydeveloping novel units to improve the accuracy of ResNetby adding parameters and marginal computational overhead.More recently, RepVGG improves the inference of yearsold VGG model. In this paper, we tackle the overhead of1 1 layers in standard ConvNets and expand its applicationto state-of-the-art transformers.",
  "Accelerated Inference. ConvNet acceleration begins with": "static pruning or network compression . Thesemethods are model agnostic, but they require theadditional overhead of pre-training and fine-tuning, thusincreasing the training time .Furthermore, by using more efficient convolutions suchas depthwise separable convolution , MobileNets address this issue at the network architecture level.In contrast, PiX, without any significant architectural modifi-cations, enables faster inference by providing an alternativeto channel squeezing 1 1 convolutions.",
  ". Pick-or-Mix (PiX)": "Modern ConvNets are essentially a stack of con-volution layers, but the design of channel squeezing 1 1convolution still has room for improvement. The main chal-lenge is exploiting the cross-channel information appropri-ately and developing a suitable mixing strategy to ensureaccurate model learning.In this section, we introduce Pick-or-Mix (PiX) in detail. Overview Consider a tensor X = {X, X, ..., X[C]},where X[i] RHW denotes ith channel of X.Weaim to produce Y = {Y , Y , ..., Y [C/]}, such thatO(Fpix) O(Fs), where Fpix is the PiX enhanced net-work and Fs is the original network. Here, R is thechannel sampling factor which controls the dimensional-ity of the output Y . The proposed dynamic channel sam-pling approach (PiX) progressively infers intermediate 1Ddescriptors z RC, p RC/ from input feature mapX RCHW for channel sampling by using learnableparameter = {, }. It then applies per-pixel dynamicchannel sampling operator for fusing a subset of channelsand produces an output feature map Y RC/HW ofreduced dimensionality that is controllable by the samplingfactor R1.The PiX module is illustrated in and can besectioned into three stages: (1) global context aggregation,which provides a channel-wise global spatial context in theform of z (Sec. 3.1) (2) cross-channel information blendingthat transforms z into p, referred to as PiX sampling proba-bility (Sec. 3.2), and (3) channel sampling stage that utilizesp and X to produce Y . (Sec. 3.3)",
  ". Sampling Probability": "Now the output of the previous step z = gca(X) (Eq. 1) ispassed through sampling probability predictor , servingtwo purposes. First, since each element of z consists of spa-tial information of only a single channel of X, the descriptorz lacks cross-channel information. mitigates this issue byblending the cross-channel information in the elements ofz. Second, the fusion factor , i.e., C to C/, reducesthe input number of channels. We define (z) = z + ,where, RC/C and RC/ are the weights andthe biases, initialized with Xavier and zero respectively.After (z), we obtain channel subset-wise sampling prob-ability p RC/0with sigmoid function.",
  "p[i] Max([i]),p[i] p[i] Avg([i]),p[i] > ,(2)": "where is Pick (selecting the maximum) or Mix (averagingresponses) channel function function, and p[i] is the pre-calculated sampling probability for a i-th subset (Sec. 3.2). is hyperparameter, set to 0.5 based on our ablations. In Eq. 2,the selection of a fusion operator is performed dynamicallyvia the sampling probability p produced via the input, thusmaking PiX input adaptive.To generalize this idea over the whole input feature mapX, the functor F for this strategy can be given as:",
  "F(X; p) =(), (), ..., ([C/1]),(3)": "as depicted in .It is important to note that channel sampling applies dif-ferently for each spatial location in PiX. For example, when > 1 and p[i] , with the help of Max, the selectedchannel index in a subset varies for each spatial location(or simply pixel) depending on channel values of that pixel.Moreover, each [i] subset applies a different operator, i.e.,some subgroup applies Max(), and the other applies Avg().This subset-wise operation selection introduces 2C/ com-binations, giving numerous ways to fuse the input channels.Since fusion is done on a pixel basis, one pixel may prior-itize any channel over another, demonstrating the capabilityof PiX.This degree of freedom to fuse channels dynamically ina spatially varying manner introduces a high level of non-linearity into the network, which helps to achieve PiX acompetitive accuracy on various tasks with a simplified net-work structure. When = 1, since Max(v) = Avg(v), PiXwill act as global channel-wise attention as in SENet . From the perspective of computation cost, note that just refers to pre-computed p[i] for selecting lightweightoperation (Max or Avg), and does not involve expensivepixel-wise 1 1 convolution. Therefore, PiX can effectivelysave computation costs.Our motivation to selectively utilize Max and Avg liesin the fundamentals of ConvNets where max and avg.pooling are essentially summarization operations. The dy-namic decision based on p[i] enables the ConvNets to learnrich representations and allows sub-sampling of the features.We also support our motivation empirically by employingthe Min operator instead of Max or Avg. We observe a per-formance degradation by roughly 2% (see the supplement).",
  ". Computational Complexity": "In PiX, the computation reduction primarily occurs due tocollapsing the input tensor X RCHW into z RC. Ina naive channel squeezing operation, a 1 1 convolution isapplied densely over X RCHW , having C H WFLOPs. In contrast, in PiX, X is first collapsed into z RC,and then the sampling probability predictor is applied over z,resulting in only C (C/) FLOPs. This is how PiX savescomputations drastically.Note that the only learnable parameter in PiX is and",
  ". PiX Embodiment as a Multi-Purpose Module": "The ability of PiX to perform channel sampling naturallytranslates to the underlying operations of different tasks,such as channel squeezing (Sec. 3.5.1), network scaling(Sec. 3.5.2), and dynamic channel pruning (Sec. 3.5.3).We describe below in detail how PiX achieves these ob-jectives despite keeping its structure the same. We alsodiscuss the benefit of using PiX for these tasks. Note thatit is the functionality of PiX that it can act as a networkdownscaler by controlling the channels. However, it is not adirect method of model compression.",
  "Channel Squeezing": "Prior works have conducted channel squeezing operationsmostly with 1 1 layers in ResNet-like designs . PiXmaintains a similar level of accuracy to such approaches byutilizing channel sampling probability (Sec. 3.2) in conjunc-tion with the pixel-wise dynamic channel sampling (Sec. 3.3).More importantly, PiX is free from expensive dense 11 con-volution. Instead, by operating on a vector z, PiX effectivelysaves FLOPs and squeezes the channel faster.To demonstrate our claims, we replace channel squeezing11 layers in the representative ResNet family (ResNet-50, -101, and -152) with PiX and evaluate the accuracy,FLOPs, and training and inference time. PiX speeds up the training and inference, which are empirically verified in and (see the supplement for the details).Alternatively, channel squeezing can be done via depth-wise pooling in a non-parametric way . However, iteliminates all the squeeze convolution layers, resulting in anaccuracy drop, as shown in E4 in .",
  "Network Downscaling": "We can control ConvNets parameters and computationalcomplexity by adjusting the number of input or output chan-nels. When conducting parameter reduction, it is callednetwork downscaling. PiX can achieve this goal via its chan-nel reduction capability. In our approach, the input featuremap for each layer is squeezed by the PiX module withsampling factor > 1 and then sent to the next layers.PiX module can be inserted into the existing layers, al-lowing it to downscale ConvNets by changing . We useResNet-18, ResNet-50, VGG-16, and MobileNet for the ef-fectiveness of this application. Notably, PiX-downscalednetwork variant consistently outperforms the downscaledbaseline. PiX-downscaled networks have the same parame-ters but lower FLOPs and higher accuracy ().",
  "Dynamic Channel Pruning": "When we plug PiX into a model, it uses to determinethe number of output channels. Thus, once is set, thenumber of channels obtained from PiX is deterministic orstatic. However, as PiX selects channels on the fly, meaningthat which channels will be sent to the next layer is notpredetermined, it leads to a dynamic reduction behavior.For this reason, we call PiX as static-dynamic channelpruner. This contrasts with the dynamic channel pruning ap-proach, which keeps all the channels in the network intact butdecides which ones to compute to save computations. Thismandates the need for specialized convolution implementa-tion to take advantage. On the other hand, the static-dynamicbehavior of PiX is free of such necessity, which is of practi-cal significance. The static behavior reduces the networksmemory footprint and bandwidth while outperforming dy-namic channel pruning approaches.Please refer to the supplement for the procedure to em-body PiX as a dynamic channel pruner. shows a com-parison with dynamic pruning approaches. We use ResNet-18 and VGG-16 for evaluation.",
  "PiX": "predict channel saliency. CBAM extends SENet, per-forming both max and avg. pooling during global contextextraction then passes them through a shared MLP. FBS uses global attention to predict channel saliency. FBS picksTop-K channels using the predicted channel saliency, andthe suppressed channels are inhibited in the computations ofthe subsequent layer. PiX inherits the idea of using globalcontext to generate sampling probability p. (Sec. 3.2) Channel Pruning. PiX differs from existing channel prun-ing approaches in both structure and functionality. PiXis not natively a channel pruner; it is the ability of PiX tosample channels on the fly, which can be utilized as a chan-nel pruner. Therefore, PiX does not require an architecturalchange to behave as a channel pruner. On the other hand,FBS , for instance, is a channel pruner, and the design isnot intended for other purposes, e.g., as a channel squeezer.For reference, we report the accuracy drop when FBS ismodified to work as a channel squeezer in Sec. 4.5.A functional comparison of PiX with prior work is shownin . We recommend referring to the supplement forvisual differences between PiX and SENet, CBAM, and FBS.In the supplement, we also provide details on the memoryand FLOPs requirements of PiX, SE, CBAM, and FBS. Notethat PiX has the lowest FLOPs and memory consumption.",
  "Group Convolution. Apart from the above modules, interms of operation, the channel space partition should not": "be confused with group convolution (GC) . In GC,the input channels are divided into groups, and convolutionis performed over each group, whereas we perform our PiXdynamic channel sampling operation onto each pixel. More-over, the kernel size in GC is a hyperparameter, which doesnot exist in PiX. Also, GC requires the input number of chan-nels to be exactly divisible by the number of groups, whichis not the case with PiX. Please see the supplement for thevisual differences between GC and PiX.",
  ". Experiments": "We evaluate PiX by plugging it into various prominent Con-vNets and Transformers , and we compareagainst recent approaches . We follow thetradition of training the models on ImageNet with 1.28Mtraining and 50K validation images over 1,000 categoriesfor image classification task. For transfer learning, we useCIFAR-10 and CIFAR-100 datasets for image classificationand CityScapes for the downstream task of semantic seg-mentation. We use for FLOP calculations, which alignswith our theoretical calculations.Please see the supplement for training details, code snip-pets, ablations, and our theoretical FLOP calculations.",
  "Channel Squeezing (Sec. 3.5.1) aims to reduce FLOPs whilemaintaining accuracy and parameters ()": "E0 - E2: PiX reduces FLOPs by 23% in ResNet familywhile having better accuracy. PiX achieves computation-ally efficient squeezing, as visible by the 23% reduction inFLOPs in all of the PiX variants. Interestingly, ResNet-101+ PiX surpasses the baseline ResNet-152 with a significantFLOP difference of 47%. We argue that our conjecture onreusing the parameters of PiX works to maintain the non- . PiX as a network downscaler. Increasing in the networks where our PiX is applied decreases the number of parameters, workingas a network downscaler. For a fair comparison with the baseline networks, we match the size of the ResNet, VGG, and MobileNet family toour downscaled networks. Baseline networks + PiX consistently shows better accuracy and reduced FLOPs with similar network parameters.",
  "NVIDIA GPUs Cores Computing powerResNet-50ResNet-50 +PiXResNet-101ResNet-101 +PiXResNet-152ResNet-152 +PiX": "A401075237.00 TFLOPs142 FPS166 FPS (17% )90 FPS100 FPS (11% )66 FPS71 FPS (8% )RTX-2080Ti435213.45 TFLOPs125 FPS166 FPS (32% )71 FPS83 FPS (17% )58 FPS66 FPS (14% )GTX-1080Ti358411.45 TFLOPs111 FPS142 FPS (28% )76 FPS83 FPS (10% )58 FPS66 FPS (14% )Jetson NX3841.00 TFLOPs20 FPS25 FPS (25% )13 FPS16 FPS (23% )10 FPS12 FPS (20% ) linearity of the network is verified. Also, the empirical resultshows that PiX learns useful data representations (Sec. 3.5.1).Despite the reduction in FLOPs, PiX exhibited slight accu-racy improvements. E3: PiX with a higher squeezing factor. We analyze PiXfor a higher squeezing factor, i.e., = 8, and observe thatPiX performs better than the baseline while having almost25% fewer FLOPs. Interestingly, the accuracy gap betweenResNet@ = 4 and = 8 is 2.64%, while this gap reducesto 2.30% for PiX at a notable 56% reduction in the FLOPs.These empirical results demonstrate the robustness of PiXtowards parameter reduction and its ability to learn to samplechannels efficiently. E4: PiX enabled squeeze-excitation (SE) networks are more accurate.It is noticeable that PiX performsbetter than SE, especially in FLOPs, indicating that PiX im-proves the computational performance of SE-like modules.It is because PiX reduces the computations of the channelsqueezing layer from the network equipped with SE-likemodules. Hence, the network can take advantage of globalattention weighting from SE-like modules and computation-ally efficient channel squeezing operation via PiX.",
  ". Inference Latency": "Since FLOPs are not an accurate measure of the actual speed, we conduct a latency analysis on four different typesof GPUs (). The first three are entry-level desktopGPUs, while the last one is a low-powered (10W) embeddedcomputing device that is far less powerful. The table showsthat PiX brings a maximum of 32% speedup, which demon-strates the practicality of PiX for real-time applications.",
  ". PiX as Network Downscaler": "Along with channel squeezing, PiX also offers simplified net-work downscaling (Sec. 3.5.2). By increasing , we achievea similar effect to that of network downscaling, outperform-ing the downscaled networks by other approaches. We usedwidth scaling (increasing the number of channels in eachconv layer) for the baseline.The empirical result in shows that our proposedPiX is seamlessly applicable for network downscaling re-gardless of network architectures (ResNet-18, ResNet-50, . PiX + ViT. We replace the vanilla channel squeezinglayer with PiX in the feed-forward network (FFN) of recent Ef-ficientViT . We observe that the utility of PiX also transfersto the Transformer models, as evidenced by the reduced runtime.Note: EfficientViT uses a squeezing factor of two in its FFN.",
  ". PiX into Vision Transformers (ViT)": "Although our approach is designed for ConvNets, we goeven further and apply PiX into ViT models to investigatethe feasibility. We apply PiX to the feed-forward network(FFN) of the ViTs, which is essentially a stack of channelexpansion 1 1 layer followed by a channel squeezing 1 1layers. We experiment with the latest EfficientViTs . Wechoose the EfficientViT-M5 variant.Since FFN layers form only a small portion of Transform-ers, the parameter and FLOPs roughly remain the same, asshown in . However, the wall time of the PiX variantis smaller, reducing the training time from 36 hours to 24hours and reducing the downscaled models training timefrom 32 hours to 24 hours. Despite similar FLOPs, the func-tioning of PiX requires less memory access, which reducesthe memory access cost (MAC) and hence latency .We believe that with further improvement in the context ofViTs, the classification performance of PiX can be improved,which we leave as future work.",
  ". PiX as Dynamic Channel Pruner": "The ability of PiX to pick channels dynamically is similarto dynamic pruning (Sec. 3.5.3). The difference is that PiXselects the channels dynamically while existing approachesturn off a few channels. We compare PiX with dynamicpruning approaches. PiX vs. dynamic pruning approaches.Referring to , the PiX baseline (i.e., ResNet-18+ PiX @ = 1, Top-1 Acc. 73.15%) and the downscaled(ResNet-18 + PiX @ = 3, Top-1 Acc. 70.60% in ),shows compelling performance than the state-of-the-art dy-namic pruning approaches .Note that PiX does not require fine-tuning to obtain betterperformance, unlike other approaches, such as , leadingto a simpler pipeline of PiX.Following , we report Top-5 error with thebenefit of FLOP reduction using VGG-16 as a baseline. Ta- . PiX as a dynamic channel pruner. We compare our ap-proach with representative dynamic or static channel pruning meth-ods using ResNet-18 and VGG-16. Vanilla ConvNet + PiX showscompatible accuracy and FLOPs saving gain.",
  "ble 6 shows that PiX offers a competitive performance thanother approaches": "Existing dynamic channel pruning approach is not multi-purpose. To highlight the key advantage of PiX that it doesnot need to change its structure to serve different purposes,we customize FBS for channel squeezing, although FBSis not intended to perform. FBS was chosen because of itsstrong resemblance with disabling channels via global at-tention. FBS picks top-k channels in its original operationand has the same input-output dimensions, i.e., RCHW .However. for this experiment, we configure FBS to output RC/kHW , where k = .We then replace all the channel squeezing layers with thismodified FBS module and train the model. We observe thatFBS faces convergence issues. We identify the underlyingcause is due to the drop-out of intermediate channels fromthe input X when selecting top-k channels. Also, the chan-nels appearing in the output (Y ) that lost position identityor channel index causes convergence issues. When Y isoperated upon via subsequent convolutions, the approach isnot intended to learn the relation between the channels, asthe position or index of a given channel in X keeps changingin Y . This indicates that FBS-like pruning methods can notcomplement PiX, but vice-versa is possible, as demonstratedearlier, highlighting the utility of PiX.",
  "the newly proposed layer. We observe that PiX performsbetter than SE and CBAM, even on MobileNet , whilethe proposed PiX has a simpler structure and multi-purposeutility": "E3: PiX vs. AFF and SKNet . Attentional FeatureFusion (AFF) fuses two feature maps adaptively, and SKNetimproves accuracy by adaptively weighting the output oftwo convolutions with different kernel sizes. These modelsare trained for longer epochs. Therefore, we also train PiXat the same setting . We observe that PiX outperformsthese two methods while being architecturally simple. E4: PiX + VGG vs. RepVGG . RepVGG is a recentapproach that speeds up VGG via structural reparame-terization (Sec. 2) during inference time only. We see thatVGG-16 + PiX offers a competitive performance to RepVGGwhile being simpler at both train and test time. E5: PiX vs. DWP . Depth-wise pooling (DWP) is acomparable approach for channel squeezing. Hence, wetrained ResNet-50 endowed with DWP.As mentioned in Sec. 3.5.1, eliminating sampling proba-bility predictor from the network removes all the squeezinglayers, leading to parameter and accuracy loss. DWP is anexample of this case, which eliminates all the 1 1 squeez-ing layers, facing a loss of accuracy (1.30%), compared toPiX used for channel squeezing.Due to the parameter differences in ResNet-50 + PiX andResNet-50 + DWP, we compare the latter with a downscaledvariant of ResNet-50 + PiX. As a result, PiX surpasses DWP,verifying our hypothesis that in channel squeezing mode,PiX preserves the non-linearity that allows for maintainingaccuracy.",
  ". Transfer Learning": "E0: PiX transfers better on image classification task. Toanalyze the generalization of PiX across datasets and tasks,we perform transfer learning from ImageNet to CIFAR-10and CIFAR-100. Each of the datasets consists of 50K train-ing and 10K test images. For training, we finetune the mod-els pretrained over ImageNet. The training strategy for bothdatasets remains identical to that of ImageNet except for200 epochs. From , it can be seen that PiX performsbetter at lower FLOP requirements. E1: PiX transfers better on semantic segmentation task.We evaluate PiX for a challenging task of semantic segmen-tation. We use a prominent approach and replace thebackbone with ResNet-101+PiX. Consequently, PiX outper-forms the baseline both in terms of FLOPs and accuracy by0.7% units mIoU.",
  ". Conclusion": "In this work, we introduce Pick-or-Mix (PiX) for dynamicchannel sampling. It works by exploiting global spatial con-text by blending cross-channel information and then pickingor mixing channels on per-pixel basis. The picked channelscan be different for each pixel depending upon the operatorselection. This capability allows PiX to maintain accuracyeven by cutting down FLOPs. PiX can work as a computa-tionally efficient channel squeezer, can downscale a givenmodel, or function as a dynamic channel pruner. We showthat PiX is easy to plug into the existing ConvNets or evenViT, without altering its structure, and we show that PiXoutperforms state-of-the-art approaches. Limitations. Currently, our approach is designed for dis-crete squeezing factors . Future extensions of the proposedapproach include developing a more generalized fusion ap-proach that can sample channels at non-integer . Acknowledgment. This study was supported by the I-HubFoundation for Cobotics (IHFC), Technology InnovationHub of Indian Institute of Technology, Delhi (IIT Delhi) un-der the project grant IITM/IHFC/IITDELHI/LB/370. DanuelKim and Jaesik Park were supported by IITP grant fundedby the Korea government (MSIT) (No.2021-0-01343, AIGraduate School Program: Seoul National University, 5%)and NRF grant No.2023R1A1C200781211 (95%). Jianda Chen, Shangyu Chen, and Sinno Jialin Pan. Storageefficient and dynamic flexible runtime channel pruning viadeep reinforcement learning. Advances in neural informationprocessing systems, 33:1474714758, 2020. 2, 7 Marius Cordts, Mohamed Omran, Sebastian Ramos, TimoRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,Stefan Roth, and Bernt Schiele. The cityscapes dataset for se-mantic urban scene understanding. In Proc. of the IEEE Con-ference on Computer Vision and Pattern Recognition (CVPR),2016. 5 Yimian Dai, Fabian Gieseke, Stefan Oehmcke, Yiquan Wu,and Kobus Barnard. Attentional feature fusion. In Proceed-ings of the IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 35603569, 2021. 2, 5, 7, 8 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiFei-Fei. Imagenet: A large-scale hierarchical image database.In 2009 IEEE conference on computer vision and patternrecognition, pages 248255. Ieee, 2009. 5 Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,Guiguang Ding, and Jian Sun. Repvgg: Making vgg-styleconvnets great again. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1373313742, 2021. 2, 5, 6, 7, 8 Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan.More is less: A more complicated network with less infer-ence complexity. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 58405848,2017. 7",
  "Xitong Gao, Yiren Zhao, ukasz Dudziak, Robert Mullins,and Cheng-zhong Xu. Dynamic channel pruning: Featureboosting and suppression. arXiv preprint arXiv:1810.05331,2018. 1, 2, 5, 7, 12, 13, 14": "Xavier Glorot and Yoshua Bengio. Understanding the dif-ficulty of training deep feedforward neural networks.InProceedings of the thirteenth international conference onartificial intelligence and statistics, pages 249256. JMLRWorkshop and Conference Proceedings, 2010. 3 Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, ChunjingXu, and Chang Xu. Ghostnet: More features from cheapoperations. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 15801589,2020. 1 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 1, 2, 3, 4, 5, 8, 11, 15, 16",
  "conference on computer vision (ECCV), pages 784800, 2018.2, 7": "Andrew G Howard, Menglong Zhu, Bo Chen, DmitryKalenichenko, Weijun Wang, Tobias Weyand, Marco An-dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-tional neural networks for mobile vision applications. arXivpreprint arXiv:1704.04861, 2017. 2, 5, 8 Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitationnetworks. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 71327141, 2018. 2, 3,4, 5, 6, 7, 8, 12, 13, 14, 15",
  "Weizhe Hua, Yuan Zhou, Christopher M De Sa, Zhiru Zhang,and G Edward Suh. Channel gating neural networks. Ad-vances in Neural Information Processing Systems, 32, 2019.1, 7": "Abid Hussain and Wang Hesheng. Depth-wise pooling: Aparameter-less solution for channel reduction of feature-mapin convolutional neural network. In 2019 IEEE InternationalConference on Real-time Computing and Robotics (RCAR),pages 299304. IEEE, 2019. 4, 8 Sergey Ioffe and Christian Szegedy. Batch normalization:Accelerating deep network training by reducing internal co-variate shift. In International conference on machine learning,pages 448456. PMLR, 2015. 11, 14 Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Chris Choy, An-ima Anandkumar, Minsu Cho, and Jaesik Park. Perfcep-tion: Perception using radiance fields. In Thirty-sixth Confer-ence on Neural Information Processing Systems Datasets andBenchmarks Track, 2022. 1",
  "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradientdescent with warm restarts. arXiv preprint arXiv:1608.03983,2016. 15": "Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filterlevel pruning method for deep neural network compression.In Proceedings of the IEEE international conference on com-puter vision, pages 50585066, 2017. 7 Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.Shufflenet v2: Practical guidelines for efficient cnn architec-ture design. In Proceedings of the European conference oncomputer vision (ECCV), pages 116131, 2018. 5, 12",
  "Jun-Hyung Park, Yeachan Kim, Junho Kim, Joon-YoungChoi, and SangKeun Lee. Dynamic structure pruning forcompressing cnns. arXiv preprint arXiv:2303.09736, 2023.2, 7": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: Animperative style, high-performance deep learning library. Ad-vances in neural information processing systems, 32, 2019.15 Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-ing He, and Piotr Dollr. Designing network design spaces. InProceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1042810436, 2020. 2 Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-cnn: Towards real-time object detection with regionproposal networks. Advances in neural information process-ing systems, 28:9199, 2015. 1 Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-moginov, and Liang-Chieh Chen. Mobilenetv2: Invertedresiduals and linear bottlenecks. In Proceedings of the IEEEconference on computer vision and pattern recognition, pages45104520, 2018. 2 Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE internationalconference on computer vision, pages 618626, 2017. 15",
  "Mingxing Tan and Quoc Le. Efficientnet: Rethinking modelscaling for convolutional neural networks. In InternationalConference on Machine Learning, pages 61056114. PMLR,2019. 2": "Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, ChaoXu, Dacheng Tao, and Chang Xu.Manifold regularizeddynamic network pruning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 50185028, 2021. 1, 2, 7 Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In SoKweon. Cbam: Convolutional block attention module. InProceedings of the European conference on computer vision(ECCV), pages 319, 2018. 2, 5, 7, 8, 12, 13, 14 Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, HaibinLin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Man-matha, et al. Resnest: Split-attention networks. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 27362746, 2022. 2 Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.Shufflenet: An extremely efficient convolutional neural net-work for mobile devices. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages68486856, 2018. 2, 5, 12 Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, XiaogangWang, and Jiaya Jia. Pyramid scene parsing network. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 28812890, 2017. 8 Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu,Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu.Discrimination-aware channel pruning for deep neural net-works. Advances in neural information processing systems,31, 2018. 7",
  "The BatchNorm operation is performed per spatial locationand can be given as X = (X )": "+ . It can be implementedin three FLOPs, i.e., first for computing X , second for /,and last as FMA with . In general, is stored as 2, therefore,it requires to compute square-root of 2 to obtain . Overall, ittakes four FLOPs to implement a BatchNorm operation per spatiallocation. Thus, the total number of FLOPs for a BatchNorm layercan be given as:",
  "(c)(d)": ". Embedding the proposed PiX into various standard net-works for various purposes. (a) Channel Squeezing Mode: wereplace 1 1 channel squeezing layers in ResNet with PiX,where the remaining 1 1 conv layers in the original ResNet areuntouched as it is intended for expanding channel dimensions. (b& c) Network Downscaling Mode: We insert PiX modules intoResNet and VGG . We make the output channel dimensionsmaller than the input channel dimension by adjusting samplingfactor in PiX. In other words, depending on , The input andoutput channel dimensions of 1 1 and 3 3 conv layers changeaccordingly. As a result, as gets larger, the channel dimensionof the original network reduces. (c & d) Dynamic Channel Prun-ing: These configurations are used for comparing PiX with otherdynamic channel pruning approaches.",
  ". Global pooling": "Apart from the above layers, in the PiX module, a global poolingoperation is also performed. There are several ways to implement aglobal pooling operation. However, the most common is by usingmatrix multiplication routines and Fused-Multiply-Add (FMA)instructions. The whole channel of a feature map can be consideredas a vector of size H W which can be reduced to a scalar bytaking its dot product with a vector whose all elements are equalto one. Hence, the total number of FLOPs for the global poolingoperation can be given by:",
  ". Computation Reduction by PiX in ChannelsSqueezing i.e. > 1": "In the baseline method, the squeeze layer operates upon X RCHW which requires C/ C H W FLOPs. Whereas inPiX, the global context aggregation requires C H W FLOPs,cross-channel information blending requires C/ C FLOPs. andchannel fusion requires C/ ( 1) H W FLOPs.As an example, consider an input tensor X R1255 to asqueeze layer kernels of size 1 1. With = 4, the number ofsubsets becomes 12/ = 3. From the equations discussed, the totalnumber of FLOPs for a squeeze layer equals 1,275.",
  "SECBAMFBSPiX": ". Flops and Memory performance of PiX in contrast to SE CBAM , and FBS per-instance of a module. In the memoryplot, SE and PiX has almost same overhead but PiX lesser than SE in terms of Bytes ( 1,000), and same is with CBAM and FBS. For thisreason plots are overlapping in the memory plot. The actual values are also highlighted in .",
  ". Effect of Pick-or-Mix on Memory in ChannelSqueezing": "Despite the computational benefits, PiX does not introduce anymemory overhead. The total memory required by the baselinesqueeze operation with = 4 can be given by: #M = C/4 H W. On the other hand, the memory required for PiX isgiven by: #M = C + C/4 + C/4 H W. We can see thatthere is a negligible increment in the memory footprint, i.e., from0.75 C H W to 0.75 C H W + 1.25C. For FP32precision, the raw memory footprint will be 4 M.",
  ". Ablation Study": "We empirically validate Pick-or-Mix design practices using themost pertinent ablations possible. ResNet-50 is adopted as thebaseline for this purpose, and channel squeezing mode. To beginwith, we first analyze the effect of changing the activation functionin the cross-channel information blending stage and then examinethe effect of placing a BatchNorm prior to the sigmoidal activa-",
  "tion. Further, we verify the behavior of proposed channel fusionstrategies and also the effect of varying fusion threshold": "E0: Fusion Activation.The channel fusion stage utilizesthe sampling probability p. Given that the value of p lies in theinterval , we wish to examine the behavior of PiX if this rangeis achieved via a different activation function. For this purpose,we select TanH function which natively squeezes the input into arange . Therefore, we rewrite the mathematical expressionto 0.5 (1 + TanH) in order to place the output of TanH into thedesired range of . We replace the sigmoidal activation withthe above expression and retrain the network. From , it canbe seen that sigmoidal activation outperforms the TanH activationfor the case of PiX.",
  "the squeeze layer in the baseline method is also followed by aBatchNorm layer. We observe that BatchNorm negatively impactsperformance": "E2: Effect of Fusion Threshold ().The hyperparameter is evaluated against three values {0.0, 0.5, 1.0}. In accordancewith Eq. 2 of the main manuscript, = 0 corresponds to Maxoperator, = 1.0 corresponds to Avg operator regardless of thevalue of p. Whereas = 0.5 offers equal opportunity to the Maxand Avg fusion operators which are adaptively taken care of by thevalue of p. We present an ablation over the aforementioned threevalues of .From , we observe that = 0.5 results in best perfor-mance, which is the case when the network has the flexibility tochoose from both reduction operators adaptively. Hence, in theexperiments, we use = 0.5 for threshold-based fusion. E4: Effect of Operator Type.We also experiment for opera-tor Min other than Max and Avg. We found out that Min performsseverely worse. This justifies our choice of operators and is in linewith the performance achieved by using the pooling operation whenthey are used spatially.",
  ". Role of Fusion Probability": "We analyze the sampling probabilities across all classes in theImageNet validation set for ResNet-50 + PiX @ = 2 for the lastblock of each stage ().It can be seen that the importance of probability is significantsince distribution for the fusion operator selection is variable, i.e.,while training, the network does not bias towards only one typeof fusion operator, indicating that both of the fusion operators arecrucial. In the deeper layers (stage-5), the variance starts increasing,indicating deeper layers are class-specific and need different activa-tion distributions. This is in line with . Moreover, we noticethat, unlike , none of the layers in the stage-5 show saturation.This is also an indication that PiX naturally pushes a convolutionlayer to learn more complex representation.",
  ". GradCAM Visualization": "The performance of PiX, especially in the channel squeezing mode,inspires us to analyze how PiX attends the spatial regions relativeto the baseline. It explains qualitatively the improved performanceof PiX despite the reduction in FLOPs. We use GradCAM forthis purpose. shows the analysis for ResNet and VGG. Noticeably,PiX shows improvement in the attended regions of a target classrelative to the baseline (R-I2, V-I4). Also, in images with multipleinstances, PiX focuses on each instance strongly (R-I4, V-I2), indi-cating that PiX enhances networks generalization by learning toemphasize class-specific parts.",
  ". Training Specifications": "The training procedure is kept standard to ensure reproducibility.We use a batch size of 256, which is split across 8 GPUs. Weuse a RandomResized crop of 224224 pixels, along with ahorizontal flip. We use SGD with Nesterov momentum of 0.9,base_lr=0.1 with CosineAnnealing rate scheduler and aweight decay of 0.0001. Unless otherwise stated, all models aretrained from scratch for 120 epochs following ."
}