{
  "Abstract": "Split Learning (SL) is a distributed learning frameworkrenowned for its privacy-preserving features and minimalcomputational requirements.Previous research consis-tently highlights the potential privacy breaches in SL sys-tems by server adversaries reconstructing training data.However, these studies often rely on strong assumptions orcompromise system utility to enhance attack performance.This paper introduces a new semi-honest Data Reconstruc-tion Attack on SL, named Feature-Oriented Reconstruc-tion Attack (FORA). In contrast to prior works, FORA re-lies on limited prior knowledge, specifically that the serverutilizes auxiliary samples from the public without know-ing any clients private information.This allows FORAto conduct the attack stealthily and achieve robust perfor-mance. The key vulnerability exploited by FORA is the reve-lation of the model representation preference in the smasheddata output by victim client. FORA constructs a substi-tute client through feature-level transfer learning, aiming toclosely mimic the victim clients representation preference.Leveraging this substitute client, the server trains the at-tack model to effectively reconstruct private data. Extensiveexperiments showcase FORAs superior performance com-pared to state-of-the-art methods. Furthermore, the papersystematically evaluates the proposed methods applicabil-ity across diverse settings and advanced defense strategies.",
  "*Corresponding author.Our code is available at": "and expressive power. Split Learning (SL) emerged as a distributed collaborative frameworkthat enables clients to cooperate with a server to performlearning task. In SL, the complete DNN model is dividedinto two parts, which are deployed on the client and serverrespectively. For a normal training process in SL, the clientperforms the computational process locally and communi-cates with the server solely based on intermediate features(referred to as smashed data) and their corresponding gradi-ents. In this case, the server does not have access to any pri-vate information (raw data, parameters, architecture) aboutthe client. Therefore, SL is considered effective in protect-ing the privacy of clients. However, recent works have shownthat there are still privacy risks associated with SL. It ispossible for the server to steal private information aboutthe client according to auxiliary knowledge.One par-ticular concern is the Data Reconstruction Attack (DRA), where a server attempts to recover the trainingdata of a client in SL systems. Depending on whether theserver affects the normal process of SL, we can categorizeadversaries into malicious and semi-honest attackers. Ma-licious servers such as FSHA can manipulate the SLtraining process to conduct more effective attack. However,the latest findings show that FSHAs mischief iseasily detected by the client, leading to the termination ofSL training protocol For semi-honest attackers, e.g. PCAT and UnSplit , their superior camouflage makes themless likely to be detected. But current semi-honest attack-ers often rely overly on assumptions that favor their perfor-mances. For example, UnSplit requires knowledge of theclients architecture and is only applicable to simple net-works or datasets. As for PCAT, it unduly depends on theavailability of partial private data to assist in training thepseudo-client. These assumptions contradict the basic prin-",
  "arXiv:2405.04115v3 [cs.CR] 20 Dec 2024": "ciple of SL, which is to ensure that the clients knowledgeremains hidden from the server. In summary, we find previ-ous attacks lack consideration of the intrinsic security of SLand the plausibility of their attack hypothesis, which limitsthe effectiveness and threat of their approach in real-worldSL systems scenarios.In this work, we introduce a novel DRA toward morerealistic and more challenging scenarios, where the servercannot access private data or structures and parameters ofthe client model. Our scheme stems from new insights intopotential privacy breaches in SL. We discover a fundamen-tal phenomenon that the client model has its own represen-tation preference, which can be reflected through the outputsmashed data. More importantly, this unique informationcan indicate the feature extraction behavior of the client.Based on this new insight, we propose a semi-honest pri-vacy threat, namely Feature-Oriented Reconstruction At-tack (FORA). A server adversary could establish a substi-tute client by narrowing the reference distance with the realclient, which allows the substitute model to mimic the be-havior of the target model at a finer granularity.To ef-ficiently measure the preference distance of different rep-resentations, we introduce domain Discriminator network and Multi-Kernel Maximum Mean Discrepancy(MK-MMD) . These techniques are widely usedin domain adaptation , enabling us to project variousrepresentation preferences into a shared space for compar-ison. With a well-trained substitute client, the server cansuccessfully recover the private data by constructing an in-verse network.We conduct our evaluation on two benchmark datasetsand corresponding networks against different model parti-tioning strategies. The experimental results indicate thatthe proposed method significantly outperforms baseline at-tacks. Taking the reconstructed images of CelebA at layer2 as an example, UnSplit, PCAT and FORA achieve effectsof 8.70, 12.05, and 17.11 on the PSNR . This demon-strates that FORA has significantly outperformed by 1.97xand 1.42x compared to the other two attacks.AlthoughFSHA can achieve attack performance similar to ours, itsmalicious attack process can be promptly halted throughmonitoring mechanisms , resulting in poor reconstruc-tions. Furthermore, we investigate the potential influenceson FORA, including different public knowledge conditionsand existing defense strategies, to validate the robustness ofFORA.The main contributions of this paper can be summarizedas follows: We propose a novel attack, named Feature-Oriented Re-construction Attack (FORA). As far as we know, FORAis the first work enabling a semi-honest server to performpowerful DRA in more realistic and challenging SL sys-tems. In such scenarios, the server has no prior knowl-",
  "edge of the client model or access to raw data": "We have uncovered an inherent vulnerability in SL, wherethe server can exploit rich information in the smasheddata to steal client representation preference, therebybuilding a substitute client for better reconstruction. We conduct comprehensive experiments with various ad-versarial knowledge against different benchmark datasetsand models.The results demonstrate that FORA canachieve state-of-the-art attack performance comparedwith baselines and exhibits notable robustness across dif-ferent settings.",
  ". Background and Related Work": "Split Learning (SL). SL is an emerg-ing distributed learning paradigm for resource-limited sce-narios, which can split the neural network model into bothclient-side and server-side. As shown in , the clientperforms forward propagation and transmits the smasheddata to the server, which then uses the computed lossfor backward propagation and sends the gradients of thesmashed data back to the client. Both the client and serverwill update their weights after receiving the gradients. Itis generally believed that SL provides a secure and efficienttraining protocol by allowing the client to retain a portion ofthe model and training data locally while offloading most ofthe computing overhead to the server . How-ever, recent studies have highlighted vul-nerabilities in SL, where the server can exploit the latter partof the model to carry out privacy attacks.Data Reconstruction Attack (DRA) on SL. DRA [22, 32, 41, 56] is one of the most powerful privacy attacks thataim to steal the input data by the models intermediate fea-tures. In SL, the server can utilize the smashed data outputby the client to reconstruct the training data . Onenotable attack is known as FSHA , where a maliciousattacker utilizes the elaborated loss to alter the feature spaceof the victim client for reconstructing private data. In Un-Split , the semi-honest server attempts to reconstruct thetraining data and clients parameters simultaneously by uti-lizing the smashed data. Specifically, UnSplit optimizes pa-rameters and inputs sequentially by minimizing the outputsbetween the clone client and the target client. To the best ofour knowledge, PCAT represents the most advanced at-tack under the semi-honest assumption. PCAT leverages the",
  "(d) Model 3": ". Input image and behavior visualization by Grad-CAM. All the models are trained in CelebA with the task of smil-ing classification. The figure displays the original images and therepresentation preferences of three models trained under the samehyperparameter settings but with different random seeds. knowledge embedded in various stages of the server modelsto steal private data by constructing a pseudo-client. Unlikeprevious work, SFA focuses on reconstructing samplesduring the inference stage rather than the training samples.Although existing works claim that their attacks posesignificant privacy threats to SL, they disregard the plau-sibility of their threat model.For FSHA, the server re-constructs the raw data while at the cost of destroying theclients utility. While FSHA assumes that the client is en-tirely free of any awareness of being maliciously disrupted,recent research indicates that such a malicious servercan be easily detected by the client, leading to a halt in theSL. UnSplit needs the knowledge of the clients structureand is not suitable for complex networks and datasets due tothe infinite searching space of input data and model param-eters. As for PCAT, it requires the adversary to have accessto a portion of the private dataset. This is an unreasonableassumption that violates the original intention of SL sinceone of the distinctive characteristics of SL is the ability totrain models without sharing the raw data . As a result,how to explore DRA under more realistic assumptions inSL remains an open question.Domain Adaptation. Domain adaptation [12, 15, 18, 34, 46, 47, 52] is a technique that seeks to enhance the gen-eralization of a model by transferring knowledge acquiredfrom a source domain to a distinct yet related target domain.The core idea of domain adaptation is to map data from dif-ferent domains into the same space for comparison. Here,we apply two popular methods: the domain Discriminatornetwork and the Multi-Kernel Maximum MeanDiscrepancy (MK-MMD) function to comparethe feature spaces of different models.",
  ". Threat Model": "Without loss of generality, given a two-party SL protocol,the SL model F is partitioned to a server model Fs and aclient model Fc. The server aims to stealthily recover theprivate training data of the client through the smashed dataZ output by Fc. We assume that the server adversary is a semi-honest en-tity, ensuring that the training process is indistinguishablefrom ordinary training during attack. Furthermore, we positthat the server adversary must adhere to the foundationalprinciple of the SL she lacks any means of accessingclient-sensitive information. Specifically, the server doesnot require knowledge of the structure or hyperparametersof Fc and is devoid of access to the clients private trainingdataset Dpriv. The sole piece of public knowledge availableto the server pertains to the auxiliary dataset Daux, sourcedfrom the same domain as the private samples. Its importantto note that the distribution of Daux typically differs fromthat of Dpriv. Compared to the threat model of previousworks, this assumption is more reasonable and realistic.",
  ". Motivation": "Current DRAs rely overly on constructing inverse networksfrom input-output pairs obtained by querying the targetmodel. However, this approach is impractical for SL be-cause the server only has access to the clients outputs andis not qualified to query. A potential solution is to build asubstitute client to mimic the target client, thus enabling thetraining of the inverse network. However, the variability ofthe substitute clients behavior affects the generalization ofthe inverse network to the target client, leading to the failureof the reconstruction, especially without the knowledge ofthe client model structure and private data distribution.As illustrated in , we employ Grad-CAM tovisualize the attention of intermediate features generatedby different clients. From (a)-(d), it can be noticedthat even for models trained under the same setup, therestill exists evident differences between their image process-ing attention. This phenomenon suggests that the smasheddata output by the client reflects its distinctive feature ex-traction behavior, which we define as representation pref-erences. Our general assumption is that narrowing the gapbetween the substitute client and the target client in termsof intermediate features can make the representation pref-erences of the two models more similar, which ensures thatthe inverse network trained by the substitute client perfectlymaps the target smashed data back to the private raw data.",
  ". Feature-Oriented Reconstruction Attack": "Inspired by the differences in model representation pref-erences, we propose a novel data reconstruction attackagainst SL, called Feature-Oriented Reconstruction Attack(FORA). In order to mount FORA, the adversary needs tocontrive a way to obtain the representation preferences ofthe Fc. To address this problem, we utilize domain adapta-tion techniques to project different preferencerepresentations into the same space. Specifically, the adver-sary conducts feature-level transfer learning by exploitingthe Zc collected in each training iteration and then obtains a",
  "b) Attack Model Training": ". Attack pipeline of Feature-Oriented Reconstruction Attack (FORA) against SL. (a) shows the substitute model training phase.The attacker constructs a substitute model Fc using LDISC and LMKMMD to mimic the behavior of the client model Fc. (b) meanstraining an inverse network f 1cusing public data Xaux. (c) represents the final attack phase using the attack model to reconstruct trainingdata from snapshot Zsnap of target smashed data. substitute model that mimics well the feature extraction be-havior of the Fc. Through this approach, the adversary cansmoothly construct an attack model (inverse mapping net-work) to recover the private samples. The detailed pipelineof FORA is shown in . It consists of three phases:substitute model construction, attack model training, andprivate data reconstruction.Substitute Model Construction.Before SL trainingcommences, the server initializes a substitute client, de-noted by Fc. The Fc will be trained locally at the serverin parallel with the victims Fc, and such process will takeplace throughout the entire SL collaboration. In each train-ing iteration, the client will send smashed data of the cur-rent batch to the server for completing the subsequent com-putations. Concurrently, the server will use the collectedsmashed data to perform training on the Fc. For this pur-pose, the server introduces the Discriminator module andthe MK-MMD module to extract the representation prefer-ences. We define its training objective as:",
  "minFcLDISC + LMKMMD,(1)": "where LDISC is the Discriminator module constrainingZaux =Fc(Xaux) and Zpriv = Fc(Xpriv) to be indis-tinguishable, while LMKMMD is the MK-MMD modulemaking Zaux as close as possible to Zpriv in shared space.The Discriminator D is also a network thatneeds to be trained synchronously and is tasked with ef-ficiently distinguishing the generated features between Fcand Fc, maximizing probabilities of the former and mini-mizing probabilities of the latter . Therefore, the pa-rameters of D will be updated to minimize the followingloss function:",
  "LDISC = log (1 D( Fc(Xaux))).(3)": "The MK-MMD module is designed to align twosets of generated features into a shared space using kernelfunctions and compute their difference, where a smaller dif-ference signifies closer representation preferences. Then,for the substitute client, the objective extends beyond max-imizing the probabilities output by the D, it also seeks tominimize the MK-MMD loss function, namely:",
  "(5)": "where k is a single kernel function, denotes a set of ker-nel functions that project different smashed data into Repro-ducing Kernel Hilbert Space H, is the weight coefficientcorresponding to the single kernel function.Attack Model Training. At the end of the training ofSL, the server can obtain a substitute client with a featureextraction behavior extremely similar to that of the victimclient. Moreover, its feature space is known to the adver-sary, who can recover the original input from the smasheddata by applying an inverse network (denoted as f 1c). Fol-lowing previous DRAs , we adopt the f 1cconsist- ing of a set of Transposed Convolution layers and Tanh ac-tivations as our attack model. The server can leverage theauxiliary dataset to train the attack model by minimizingthe mean square error between f 1c( Fc(Xaux)) and Xauxas follows:",
  "Lf 1c= f 1c( Fc(Xaux)) Xaux22.(6)": "Private Data Reconstruction. The server keeps a snap-shot Zsnap = Fc(Xpriv) of all smashed data output by thetarget client under the final training iteration for reconstruc-tion. Since the substitute client is able to mimic the tar-get clients representation preferences well, the server cansubtly use f 1cto perform the attack by mapping the tar-get smashed data directly into the private raw data space,namely:",
  ". Experimental Setup": "Datasets. In our experiments, we rely on CIFAR-10 and CelebA to validate the attacks, due to their domi-nance in the research on SL . They will be usedas private data for the clients target training tasks. Ac-cording to Sec. 3.1, we assume that the server adversaryhas access to a set of auxiliary samples that are distinctfrom the clients private data. Therefore, we choose CINIC-10 and FFHQ as the adversarys auxiliary dataset,respectively. We exclude images in CINIC-10 that over-lapped with CIFAR-10, and randomly select 5,000 samplesand 10,000 samples from the preprocessed CINIC-10 andFFHQ as the final auxiliary data. Appendix A.1 providesthe detailed information for different datasets.Models. We consider two popular types of neural net-work architectures, including MobileNet and ResNet-18 , as target models for the classification tasks ofCIFRA-10 and CelebA, respectively. We set various splitpoints for different target models to show our attack perfor-mance. Since the server is entirely unaware of the clientsmodel structure from Sec. 3.1, we use VGG blocks (consisting of a sequence of Convolutional, BatchNorm,ReLU, and MaxPool layers) to construct substitute mod-els. In addition, the adversarys substitute models adap-tively depend on the size of the intermediate features outputby the client. All the architecture information and splittingschemes used in this paper are reported in Appendix A.2.Metrics. In addition to analyzing the qualitative resultsof attack performances visually, we chose three quantita-tive metrics to evaluate the quality of the reconstructed im-ages: Structural Similarity (SSIM) , Peak Signal-to-Noise Ratio (PSNR) , and Learned Perceptual Image",
  "(b) Reconstruction Results": ".Attack performance comparison of FSHA andFORA on CIFAR-10 with layer 2. (a) shows the detection scoreof two attacks detected by GS. (b) represents the reconstructionresults of two attacks, and FSHA-GS is the reconstructed imageswhen detected by GS. Patch Similarity (LPIPS) . We also use Cosine Simi-larity and Mean Square Error to measure the similarity be-tween the substitute client and the target client in featurespace.Attack Baselines.We mainly compare our approachwith three representative existing methods, which are FSHA, UnSplit , and PCAT . For the malicious attackFSHA, we use sophisticated detection mechanism to jointlyevaluate the attacks effectiveness. For the semi-honest at-tack UnSplit, we make it consistent with our experimentalsettings to ensure fairness. PCAT requires an understandingof the learning task while relying on a subset of the pri-vate training data to build the pseudo-client, and in order tocomply with this assumption, we set the proportion of theCIFAR-10 private dataset to be 5% (the maximal thresh-old suggested by the original paper), and for more complexCelebA dataset, we extend the proportion to be 10%.",
  ". Comparison with Malicious Attack": "Since FSHA severely undermines the utility of the targetclient, recent work has proposed the Gradients Scrutinizer(GS) to defend against such hijacking attacks by de-tecting the gradients returned from the server to the client.The GS will perform a similarity computation on the gradi-ents, and if the calculated value is lower than a set thresh-old, it will be considered as a potential attack, resulting inthe training of SL being immediately suspended. More de-tails about GS can be found in Appendix C.1. We can ob-serve from that the reconstruction results of FORAare almost the same as those of FSHA in the unprotectedSL system. Although FSHA performs well in capturing finegraphical details, it also leads to noticeable color shifts insome reconstruction results. Moreover, since FSHA dras-tically tampers with the updated gradient returned to theclient model, it is easily detected by GS, leading to the fail-ure of reconstruction.",
  ". Comparison with Semi-Honest Attacks": "Reconstruction Performance. We show in detail the re-construction results for UnSplit, PCAT, and our proposedFORA on all split points for both datasets. As depicted inTab. 1, compared to other attacks, the images reconstructedby FORA exhibit a significant improvement visually. Dueto the vast search space and inefficient optimization ap-proach, UnSplit almost fails to recover training data in bothdatasets, even at layer 1. Although PCAT can reconstructtraining samples in the shallow settings of the CIFAR-10dataset, such as layer 1 and layer 2, the reconstruction qual-ity is still lower than that of FORA. For the more complexCelebA dataset, PCAT struggles to produce quality recon-structions. Tab. 2 and Tab. 3 provides the quantitative resultsof the attacks. Except for the anomaly at the layer 4 splitpoint of CIFAR-10, where FORA slightly underperformsPCAT in terms of SSIM and PSNR metric, FORA is supe-rior to both methods in all other settings, especially in termsof the LPIPS metric, which is considered to be more alignedwith human perception. Notably, even though PCAT hasaccess to a subset of the private data, while FORA only ob-tains samples with different distributions, FORA substan-tially surpasses PCAT for reconstruction. This further em-phasizes the robust privacy threat our approach poses to SL.More reconstructed images are presented in Appendix B.1.Feature Similarity. As shown in Tab. 4, we measure thefeature distance between the proxy clients built by UnSplit,PCAT, and FORA and the target client at layer 2. The re-sults show that the substitute clients trained by our method",
  "Mean Square Error1.0410.5280.27450.7731.3530.753Cosine Similarity0.2000.5920.8100.3330.4800.778": "exhibit more similar representation preferences to the targetclient. The basic optimization approach of UnSplit makes itdifficult to regularize the feature space of the proxy client.As for PCAT, it simply makes the smashed data generatedby the pseudo model more favorable to the server model butfails to mimic the behavior of the client model. In contrast,FORA can impose stronger constraints in the feature space,which directly contributes to successful reconstruction.",
  ". Effect of Auxiliary Dataset": "Next, we analyze the effect of several important factors re-garding the auxiliary dataset on attack performance. Wefirst explore the impact of the fitting level of substitute mod-els by varying the size of the auxiliary data. Then, we dis-cuss the impact of the presence of a more significant distri-bution shift, i.e., the absence of some categories, betweenthe auxiliary and target samples. Finally, we relax the ma-jor assumption about the adversary, namely that the serverhas access to the similarly distributed auxiliary dataset. Weset the split point at layer 2 for ablation, and the full exper-imental results are provided in Appendix B.2.Auxiliary Set Size. As shown in , when we re-duce the size of the auxiliary dataset to half of the previous",
  ". Effects of varying auxiliary data size on FORA per-formed on CIFAR-10 and CelebA at layer 2": "one, the attack performance of FORA remains almost un-changed. When we further reduce the number of auxiliarysamples to 20%, the quality of the reconstructed images de-creases slightly but still preserves the full outline and mostof the details. In that case, the percentage of the publicauxiliary dataset is very small compared to the huge privatetraining set (50,000 for CIFAR-10 and 162770 for CelebA),only 2% and 1.2%, respectively. This implies that even witha rather limited auxiliary dataset, FORA is still able to ef-fectively reconstruct the clients training samples.",
  "Non-living0.73218.430.395": "Absence of Categories. It is likely that the adversaryspublic auxiliary data misses some semantic classes of theprivate data distribution. To model this situation, we cre-ate two special auxiliary datasets for CIFAR-10, one con-taining Living items (birds, cats, etc.), and the other con-taining Non-living items (airplanes, cars, etc.), both with5,000 randomly sampled samples from CINIC-10. As pre-sented in Tab. 5, even if a class is absent from the auxiliarydataset, FORA can still reconstruct samples of that class.In fact, FORA focuses on stealing the mapping relationshipbetween client inputs and smashed data and therefore doesnot require class alignment. We observe that the absence ofthe Non-living category leads to a moderate degradationin the reconstruction results. We believe that the reason be-hind this phenomenon is that the greater variation of classeswithin the Non-living category helps to increase the gen-eralization level of the substitute client, which in turn facil-itates improved attack performance.Distribution Shift.Here we further analyze the im-pact of the auxiliary dataset distribution on FORA. In con-trast to our default experimental setup, we selected 5000 . Effects of auxiliary dataset distribution shift on FORAperformed on CIFAR-10 and CelebA at layer 2. Different rep-resents auxiliary data sampled from CINIC-10, and FFHQ respec-tively, and Same means auxiliary dataset come from their origi-nal test set.",
  "SSIM0.8300.8320.4760.777PSNR22.1922.7817.1121.55LPIPS0.2520.2070.3810.264": "and 10000 images from the original testing sets of CIFAR-10 and CelebA, respectively, as the auxiliary datasets withthe same distribution. As shown in Tab. 6, a more simi-lar distribution can facilitate substitute clients stealing therepresentation preference, resulting in better reconstructionperformance. We observe that the attack results on the fa-cial dataset are more vulnerable to the data distribution shiftcompared to the object dataset. One possible reason is thattasks related to facial datasets are more sensitive to varia-tions in sampling methods and alignment conditions acrossdifferent datasets. For object datasets, due to substantialdistribution variation between different categories of them-selves, e.g. ranging from animals to vehicles, which con-tributes to their robustness in handling distribution shifts.",
  ". Effect of Substitute Client Structure": "After validating the impact of the auxiliary dataset, here weare interested in the impact of substitute client architectureson FORA. We chose three different model structures as at-tack variants: the VGG block , the ResNet block ,and the DenseNet block . As can be seen in , theSSIM and LPIPS quantization results for the reconstructedimages remain similar.This indicates that the extractedrepresentation preferences on the basis of MK-MDD andDiscriminator are close to that of the target client, despitethe fact that the substitute clients use different architectures.Additional results are shown in Appendix B.3. CIFAR-10CelebA",
  ". Counter Defense Techniques": "There have been a number of defenses aimed at perturb-ing the smashed data claiming that they can reduce the riskof privacy leakage in SL to a certain extent.We selectthree well-known defense techniques, i.e., distance corre-lation minimization , differential privacy ,and noise obfuscation , to evaluate the effectiveness ofFORA. Tab. 7 shows the limited impact of these defenseson FORA. See Appendix C.1 for more details on defensetechniques. See Appendix C.2 for more defense results anddiscussions about possible adaptive defenses.Distance Correlation Minimization (DCOR). DCORcan uncorrelate irrelevant and sensitive features from thesmashed data associated with the target client, which re-sults in a lack of detailed expression of the input data in therepresentation preferences learned by the substitute client,especially in colors. However, FORA retains the ability toreconstruct the structural details of the private image.Differential Privacy (DP). DP protects training data pri-vacy by adding carefully crafted Laplace noise to the gradi-ents. However, the effectiveness of DP against FORA isvery limited under all privacy budgets. When the test accu-racy of the model is reduced by nearly 10% (the function-ality is severely damaged), the SSIM of the reconstructedsamples still reaches about 75% of the original. This trade-off between classification accuracy and defense strengthmakes DP not feasible for practical applications of SL.Noise Obfuscation (NO). NO is a direct defense to de-stroy the mapping relationship between smashed and inputdata. We observe that on the one hand, the noise of a smallscale enhances the generalization level of the SL model tomaintain or even improve the classification accuracy, on theother hand raising the noise scale helps to introduce devia-tions to the features extracted from the target client, makingit more difficult to learn the representations and reconstructthe data for FORA.",
  ". Discussion and Conclusion": "In this section, we first discuss the potential improvementand scalability of FORA, then we summarize this work. Wealso show limitation and future work in Appendix D.Improvement using Generative Adversarial Net-works.Li et al. propose a novel StyleGAN-basedreconstruction attack against split inference, and their re-search focus is orthogonal to our contribution. Therefore,the reconstruction task in FORA can be further optimizedusing pre-trained StyleGAN .As shown in ,the well-trained substitute client in FORA combined withStyleGAN optimization can provide additional improve-ments in reconstruction performance.Attack on Label-Protected SL. Another popular setupfor SL requires the client to keep the labels locally , but",
  ". Reconstructed CelebA images of FORA and FORA-G,FOAR-G represents FORA combined with StyleGAN": "this case does not have any influence on the implementationand performance of FORA. Since FORA is only related tothe smashed data output from the target client, it does notdepend on the server model as well as the training task.Conclusion.In this work, we propose a novel datareconstruction attack against SL, named Feature-OrientedReconstruction Attack (FORA). Unlike all previous attackschemes, FORA enables a semi-honest server to secretlyreconstruct the clients private training data with very lit-tle prior knowledge. Thanks to our new perspective of ex-tracting representation preferences from smashed data, theserver can contemporaneously train a substitute client thatapproximates the target clients behavior to conduct the at-tack. Our extensive experiments in various settings demon-strate the state-of-the-art performance of FORA. Due to itsstealth and effectiveness, it poses a real privacy threat to SL.We hope our work can inspire future efforts to explore it inmore practical SL, and we are eager to draw attention tomore robust defense techniques. Martin Abadi, Andy Chu, Ian Goodfellow, H BrendanMcMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deeplearning with differential privacy. In Proceedings of the 2016ACM SIGSAC conference on computer and communicationssecurity, pages 308318, 2016. 8, 4 Sharif Abuadbba, Kyuyeon Kim, Minki Kim, ChandraThapa, Seyit A Camtepe, Yansong Gao, Hyoungshick Kim,and Surya Nepal. Can we use split learning on 1d cnn mod-els for privacy preserving training?In Proceedings of the15th ACM Asia Conference on Computer and Communica-tions Security, pages 305318, 2020. 1, 2",
  "Luke N Darlow, Elliot J Crowley, Antreas Antoniou, andAmos J Storkey. Cinic-10 is not imagenet or cifar-10. arXivpreprint arXiv:1810.03505, 2018. 5, 1": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 5 Cynthia Dwork.Differential privacy.In Automata, Lan-guages and Programming: 33rd International Colloquium,ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings,Part II 33, pages 112. Springer, 2006. 4",
  "Cynthia Dwork, Aaron Roth, et al. The algorithmic foun-dations of differential privacy. Foundations and Trends inTheoretical Computer Science, 9(34):211407, 2014. 4": "Ege Erdogan, Alptekin Kupcu, and A Ercument Cicek. Split-guard: Detecting and mitigating training-hijacking attacks insplit learning. In Proceedings of the 21st Workshop on Pri-vacy in the Electronic Society, pages 125137, 2022. 1, 3 Ege Erdogan, Alptekin Kupcu, and A Ercument C icek. Un-split: Data-oblivious model inversion, model stealing, andlabel inference attacks against split learning. In Proceedingsof the 21st Workshop on Privacy in the Electronic Society,pages 115124, 2022. 1, 2, 5 Chong Fu, Xuhong Zhang, Shouling Ji, Jinyin Chen,Jingzheng Wu, Shanqing Guo, Jun Zhou, Alex X Liu, andTing Wang. Label inference attacks against vertical feder-ated learning. In 31st USENIX Security Symposium (USENIXSecurity 22), pages 13971414, 2022. 2 Jiayun Fu, Xiaojing Ma, Bin B. Zhu, Pingyi Hu, RuixinZhao, Yaru Jia, Peng Xu, Hai Jin, , and Dongmei Zhang. Fo-cusing on pinocchios nose: A gradients scrutinizer to thwartsplit-learning hijacking attacks using intrinsic attributes. In30th Annual Network and Distributed System Security Sym-posium, NDSS 2023, San Diego, California, USA, February27-March 3, 2023. The Internet Society, 2023. 1, 2, 3, 5, 4",
  "Yaroslav Ganin and Victor Lempitsky. Unsupervised domainadaptation by backpropagation. In International conferenceon machine learning, pages 11801189. PMLR, 2015. 2, 3,4": "Xinben Gao and Lan Zhang. PCAT: Functionality and datastealing from split learning by Pseudo-Client attack. In 32ndUSENIX Security Symposium (USENIX Security 23), pages52715288, Anaheim, CA, 2023. USENIX Association. 1,2, 5, 4 Yansong Gao, Minki Kim, Sharif Abuadbba, Yeonjae Kim,Chandra Thapa, Kyuyeon Kim, Seyit A Camtepe, Hyoung-shick Kim, and Surya Nepal. End-to-end evaluation of fed-erated learning and split learning for internet of things. arXivpreprint arXiv:2003.13376, 2020. 1 Muhammad Ghifary, W Bastiaan Kleijn, and MengjieZhang. Domain adaptive neural networks for object recog-nition. In PRICAI 2014: Trends in Artificial Intelligence:13th Pacific Rim International Conference on Artificial Intel-ligence, Gold Coast, QLD, Australia, December 1-5, 2014.Proceedings 13, pages 898904. Springer, 2014. 3 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. In Advances inNeural Information Processing Systems. Curran Associates,Inc., 2014. 4 Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. In Proceedingsof the 27th International Conference on Neural InformationProcessing Systems-Volume 2, pages 26722680, 2014. 2 Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivara-man Balakrishnan, Massimiliano Pontil, Kenji Fukumizu,and Bharath K Sriperumbudur. Optimal kernel choice forlarge-scale two-sample tests. Advances in neural informa-tion processing systems, 25, 2012. 2, 3, 4",
  "Otkrist Gupta and Ramesh Raskar. Distributed learning ofdeep neural network over multiple agents. Journal of Net-work and Computer Applications, 116:18, 2018. 1, 2": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 5 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 7 Zecheng He, Tianwei Zhang, and Ruby B Lee. Model inver-sion attacks against collaborative inference. In Proceedingsof the 35th Annual Computer Security Applications Confer-ence, pages 148162, 2019. 1, 2, 4",
  "Alain Hore and Djemel Ziou. Image quality metrics: Psnrvs. ssim. In 2010 20th international conference on patternrecognition, pages 23662369. IEEE, 2010. 2, 5": "Andrew G Howard, Menglong Zhu, Bo Chen, DmitryKalenichenko, Weijun Wang, Tobias Weyand, Marco An-dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-tional neural networks for mobile vision applications. arXivpreprint arXiv:1704.04861, 2017. 5 Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger.Densely connected convolutional net-works. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 47004708, 2017. 7",
  "Sanjay Kariyappa and Moinuddin K Qureshi. Exploit: Ex-tracting private labels in split learning.In 2023 IEEEConference on Secure and Trustworthy Machine Learning(SaTML), pages 165175. IEEE, 2023. 2": "Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 44014410, 2019. 5, 1 Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,Jaakko Lehtinen, and Timo Aila.Analyzing and improv-ing the image quality of stylegan.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 81108119, 2020. 8",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiplelayers of features from tiny images. 2009. 5, 1": "Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu,Daniel Hsu, and Suman Jana.Certified robustness to ad-versarial examples with differential privacy. In 2019 IEEESymposium on Security and Privacy (SP), pages 656672.IEEE, 2019. 4 Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He,Deliang Fan, and Chaitali Chakrabarti. Ressfl: A resistancetransfer framework for defending model inversion attack insplit federated learning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1019410202, 2022. 4 Ziang Li, Mengda Yang, Yaxin Liu, Juan Wang, HongxinHu, Wenzhe Yi, and Xiaoyang Xu. Gan you see me? en-hanced data reconstruction attacks against split inference.Advances in Neural Information Processing Systems, 36,2024. 2, 8",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.Deep learning face attributes in the wild. In Proceedings ofthe IEEE international conference on computer vision, pages37303738, 2015. 5, 1": "Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-dan. Learning transferable features with deep adaptation net-works.In International conference on machine learning,pages 97105. PMLR, 2015. 2, 3, 4 Sida Luo, Fangchao Yu, Lina Wang, Bo Zeng, Zhi Pang,and Kai Zhao. Feature sniffer: A stealthy inference attacksframework on split learning. In International Conference onArtificial Neural Networks, pages 6677. Springer, 2023. 3 Dario Pasquini, Giuseppe Ateniese, and Massimo Bernaschi.Unleashing the tiger: Inference attacks on split learning.In Proceedings of the 2021 ACM SIGSAC Conference onComputer and Communications Security, pages 21132129,2021. 1, 2, 4, 5 Maarten G Poirot, Praneeth Vepakomma, Ken Chang,Jayashree Kalpathy-Cramer, Rajiv Gupta, and RameshRaskar.Split learning for collaborative deep learning inhealthcare. arXiv preprint arXiv:1912.12115, 2019. 1, 2 Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.Grad-cam:Visual explanations from deep networks viagradient-based localization. In Proceedings of the IEEE in-ternational conference on computer vision, pages 618626,2017. 3",
  "Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, andTrevor Darrell.Deep domain confusion: Maximizing fordomain invariance. arXiv preprint arXiv:1412.3474, 2014. 3": "Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.Adversarial discriminative domain adaptation. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 71677176, 2017. 3 Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, andRamesh Raskar. Split learning for health: Distributed deeplearning without sharing raw patient data.arXiv preprintarXiv:1812.00564, 2018. 1, 2, 3, 8, 6",
  "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-moncelli. Image quality assessment: from error visibility tostructural similarity. IEEE transactions on image processing,13(4):600612, 2004. 5": "Zhibo Wang, He Wang, Shuaifan Jin, Wenwen Zhang, JiahuiHu, Yan Wang, Peng Sun, Wei Yuan, Kaixin Liu, and KuiRen. Privacy-preserving adversarial facial features. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 82128221, 2023. 4 Taihong Xiao, Yi-Hsuan Tsai, Kihyuk Sohn, ManmohanChandraker, and Ming-Hsuan Yang. Adversarial learning ofprivacy-preserving and task-oriented representations. In Pro-ceedings of the AAAI Conference on Artificial Intelligence,pages 1243412441, 2020. 4 Mengda Yang, Ziang Li, Juan Wang, Hongxin Hu, Ao Ren,Xiaoyang Xu, and Wenzhe Yi. Measuring data reconstruc-tion defenses in collaborative inference systems. Advancesin Neural Information Processing Systems, 35:1285512867,2022. 2 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 586595, 2018. 5",
  "A.1. Datasets": "We elaborate on the four datasets used in our main experiments and Tab. 8 shows the details of datasets partitioning.CIFAR-10 . CIFAR-10 is a classification benchmark dataset comprising 60,000 33232 images categorized into10 classes. It features 50,000 training images and 10,000 testing images, evenly distributed among the classes.CINIC-10 . CINIC-10 extends CIFAR-10 by adding downsampled ImageNet samples in the same classes with CIFAR-10. Both datasets share the same classes, but CINIC-10 consists of 270,000 33232 images. In comparison to CIFAR-10,CINIC-10 presents a more complex and diverse distribution.CelebA . CelebA is a dataset related to facial attribute classification. It includes 202,599 facial images from 10,177different celebrities, and each image is associated with 40 different attribute labels. In our experiment, we resize the imagesin the CelebA to 36464.FFHQ . FFHQ was originally designed as a benchmark for Generative Adversarial Networks which contains 70,000facial images. The dataset exhibits rich diversity and noticeable variations in terms of age, ethnicity, and image backgrounds.As well as CelebA, we resize the images in FFHQ to 36464.",
  "B.2. Effect of Auxiliary Dataset": "Then we present more detailed results of the impact of the auxiliary dataset on FORA. presents complete results forthe absence of categories on FORA. displays additional results regarding the impact of the size of auxiliary datasetson FORA. shows the overall results for the impact of auxiliary datasets distributions on FORA.",
  "C.1. Defense Details": "Gradients Scrutinizer. Gradients Scrutinizer (GS) is a defense method against the malicious attacker FSHA . Innormal SL, gradients returned by servers exhibit greater similarity within the same label compared to gradients from differentlabels. However, in FSHA the client is trained as the encoder of an autoencoder to reconstruct training data without usingtarget labels. As a result, gradients received by the client will not show notable distinctions between the same and differentclasses in FSHA. Based on this intrinsic difference, GS first computes the cosine similarity of gradients with the same labeland those of different labels in each batch according to the received gradients. Subsequently, GS will calculate decisionscores from three aspects: set gap, fitting error, and overlapping ratio, to distinguish hijacking servers from honest servers. Ifthe detection score is above a set threshold, it is considered a normal SL. If it falls below the threshold, it is identified as ahijacking attack, and the training is stopped immediately.The detection mechanism of GS depends on the models classification ability. Therefore, in the early stages of GS, somebatches should be skipped to avoid the detection being misguided by the model that has not been well-fitted. Following thismechanism, we start GS at the 450th iteration in our experimental setup.Distance Correlation Minimization. Distance Correlation is a defense method widely used in SL to measureand mitigate the correlation between smashed data and the raw input, thereby preventing server adversaries from reconstruct-ing the original input data. The loss function for this approach is as follows:",
  "L = DCOR(x, Fc(x)) + (1 ) TASK(y, Fs(Fc(x)))(8)": "where DCOR represents the distance correlation metric, and TASK denotes the classification loss between the true label yand the models prediction. By jointly minimizing the above loss, a better tradeoff can be achieved between preserving inputdata privacy and maintaining the utility of the model.Differential Privacy. Differential Privacy was initially introduced to provide privacy guarantees for algorithms on ag-gregate databases , and it was later applied to deep learning through DP-SGD . Differential privacy has foundwidespread usage in various scenarios , not exclusively in SL. Following the approach described in PCAT , weemploy DP-SGD in the client model. Specifically, the client receives gradients from the server, clips each gradient using athreshold value C, and adds random noise to it. The client then utilizes these protected gradients to update its model, therebysafeguarding the privacy of the subsequent smashed data transmitted to the server. Different combinations of the clippingthreshold C and noise scale yield varying privacy budgets and levels of accuracy reduction.Noise Obfuscation. Titcombe et al. proposed an approach where additive Laplacian noise was directly added tosmashed data before transmitting it to the server to defend against input reconstruction. This randomness introduces a higherlevel of complexity for adversaries, making it more challenging for them to infer the mapping between the smashed data andthe private input.",
  "C.2. More Defense Results and Possible Adaptive Defenses": "Attack Results of CelebA. The limited effectiveness of these defenses on CelebA is illustrated in Tab. 11. In comparisonto CIFAR-10, CelebA shows a more robust performance in terms of test accuracy. This is because CelebA is employed fora simpler binary classification task (smile classification), making the model more easily convergent even in the presence ofdefense methods.Results with Smaller on CIFAR-10. presents the results with smaller on CIFAR-10. We observe aninteresting phenomenon: as the applied noise increased, there is a nonlinear relationship with the defense results. Thepossible reason is that the noise can only act on partial gradients (client), limiting its effectiveness.Possible Adaptive Defenses. We discuss two potential adaptive defenses. One is that the client adopts an adversarial lossto enhance robustness against DRA . Though adversarial learning proves effective against certain known attacks,client should carefully consider the additional training overhead and utility degradation it introduces. Another promisingapproach is to craft noise against FORA to increase the inconsistency between client and substitute client in feature space,which would make attack more difficult .",
  "D. Limitation and Future Work": "In this section, we discuss the limitations of our proposed method FORA and some possible enhancements for future work.Previous work and FORA lack sufficient experiments on larger models and datasets e.g. vision transformer , so we en-courage future work to pay more attention on larger models. Additionally, although FORA only requires auxiliary data ofthe same type to launch an attack, exploring how to reconstruct client inputs in more challenging scenarios, such as whenattackers do not know the data type, is also unsolved. We hope our work can contribute to better exploring vulnerabilities ofSL and raising awareness of privacy issues within the community.",
  "E. Label-Protected SL": "In label-protected SL shown in , besides the client model and server model, there is also a portion of model calledthe top model retained on the client. In this scenario, labels are treated as private information and kept locally on the client.Differing from label-share SL, the server models results are forwarded to the top model for further forward propagation. Thetop model then calculates the loss using labels and received results, transferring relevant gradients to the server for parameterupdates."
}