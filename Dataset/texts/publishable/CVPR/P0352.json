{
  "Abstract": "Accurately predicting whether an image is private beforesharing it online is difficult due to the vast variety of con-tent and the subjective nature of privacy itself. In this pa-per, we evaluate privacy models that use objects extractedfrom an image to determine why the image is predicted asprivate. To explain the decision of these models, we usefeature-attribution to identify and quantify which objects(and which of their features) are more relevant to privacyclassification with respect to a reference input (i.e., no ob-jects localised in an image) predicted as public. We showthat the presence of the person category and its cardinal-ity is the main factor for the privacy decision. Therefore,these models mostly fail to identify private images depict-ing documents with sensitive data, vehicle ownership, andinternet activity, or public images with people (e.g., an out-door concert or people walking in a public space next to afamous landmark). As baselines for future benchmarks, wealso devise two strategies that are based on the person pres-ence and cardinality and achieve comparable classificationperformance of the privacy models.",
  ". Introduction": "People take photos in a large variety of situations (e.g.,at a party, of themselves, of a landmark, or of friends, fam-ily, animals, or food) and share them on social media plat-forms, often lacking awareness of privacy risks associatedwith their sharing . Images may contain a set of ob-jects that reveal private information about a person or be as-sociated with a specific location or event that the person isattending. Therefore, an automatic warning prior to sharingcould help users protect their privacy .Privacy classification methods are trained on datasets an-",
  "Alessio Xompero and Myriam Bontonou equally contributed. MyriamBontonou is also affiliated with Inserm, France, and Jean-Michel Arbonawith Univ Lyon and LBMC Lyon, France": "notated by one or multiple annotators with a binary label(public or private) . As the notion of privacyvaries among people and also depends on the context, theannotation in these datasets is potentially ambiguous. Mostof the existing works design methods that aimed at improv-ing the classification performance on these datasets. We cat-egorise existing methods for image privacy as single-stageand two-stage. Single-stage methods directly train or fine-tune a deep neural network (DNN) from the images .Two-stage methods uses DNNs (e.g., convolutional neu-ral networks or CNNs) to extract concepts (i.e., objects,scenes) from the images followed by a privacy classifier inthe second-stage, such as a Multi-Layer Perceptron (MLP)or a graph neural network (GNN) . Two-stage methods can be further split into end-to-end train-ing or hybrid. End-to-end training based methods fine-tunethe DNNs to initialise the concept features for the privacyclassifier . Hybrid methods extract concepts fromthe images with a pre-trained detector or multi-label imageclassifier .In this paper, we explain the decisions made by arange of privacy classifiers that use as input the cardinal-ity and confidence features of objects identified in an im-age (see ). Among many existing explainability meth-ods , we select integrated gradients that is computationally efficient and attributes the decisionof the privacy models to the identified objects and their fea-tures with respect to a reference input. This reference inputconsists of features with zero values to represent the caseof no objects localised in an image and hence classified aspublic. Based on the findings from the explainability anal-ysis, we define two simple strategies using people presenceas main driving factor to determine whether an image is pri-vate. These explainable-by-design strategies achieve com-parable performance to the more complex privacy-decisionmodels1. As baselines in future comparisons, these strate-gies will also enable the design of explainable and more",
  "Privacy model": ". Two-stage privacy method: a pre-trained object detector identifies concepts (e.g., objects, scene type) within an image and aprivacy model is trained to classify an image as private or public, considering the cardinality and confidence level of the extracted objects(numbers below each object). The input image is from the PrivacyAlert dataset , with obfuscation added on the face of the person.",
  ". Problem formulation": "Let I be an image and f() a privacy model trainedon a dataset D = {(I, y)n}Nn=1 to predict a class y {0, 1}, where 0 denotes public and 1 private, containsthe model parameters, and N is the number of images inthe training dataset.We consider the privacy model tomap the outputs of other models to the predicted class y:y = f(d(I)), where contains pre-trained parameters.For example, d() can be a pre-trained object detector thatlocalises a set of objects with their confidence in the im-age I.We refer to the pre-defined categories outputtedby these pre-trained models as concepts.Therefore, letX = {xc|c = 0, . . . , C 1} be the set of C concepts withtheir F-dimensional feature vectors xc =xc0, . . . , xcF 1 that are provided as input to the privacy model.Our objective is to explain why the trained model f()predicts the label y = 1 for a given image I (observable ex-planation ). Specifically, we want to determine whichconcepts contribute to the prediction of the private labelfor the input image. To this end, we use post-hoc explain-ability to assign a score to each feature of each concept,(xcj) {1, 1}.Following previous works , we consider ob-jects as concepts and we use a pre-trained object detector tolocalise a pre-defined set of objects (i.e., C = 80 forthe COCO dataset ). We define two features (F = 2)for each object: cardinality, xc0 N and confidence, xc1 . For cardinality, we count the number of instanceslocalised in an image and belonging to each object. If noinstances are localised for an image, then the cardinality isset to 0. For confidence, we retain the value of the mostconfident object instance if multiple instances of the sameobject are localised in an image. The privacy model couldbe an MLP or a GNN . For MLP, the in-put is the concatenation of all object features, resulting in avector of dimensionality CF: x = [. . . , xc, . . .], xc X.For GNN, the input is a C F matrix of the object fea-tures, where each row corresponds to a node of a graph. For",
  ". Explaining image privacy predictions": "In this section, we describe the dataset and models usedfor image privacy (see the Supplementary Material docu-ment for additional details), and discuss their classificationperformance. We explain the models decision and analysethe explainability results.Dataset. We use PrivacyAlert as a recent image pri-vacy dataset D for our evaluations and analyses.Priva-cyAlert has 6,800 images2 split into a training set of 3,136images (788 private images and 2,348 public images), a val-idation set of 1,864 images (466 private images and 1,398public images), and a testing set of 1,800 images (450 pri-vate images and 1,350 public images), as originally de-scribed by the authors . The dataset has a high classimbalance towards the public images (ratio of about 3:1).Methods. We consider MLP , two graph-based mod-els, GIP and GPA , and a graph-agnostic model(GA-MLP) . The MLP aims at reproducing Tonge etal.s method that uses Support Vector Machine as a pri-vacy classifier and, as input, a binary feature vector of thetop-k most confident classes recognised by a pre-trainedCNN for multi-label object recognition. In our case, wereplace the multi-label classifier with the object detectorand the object presence with the cardinality and confidencefeatures of the identified objects. The MLP consists of 3hidden layers, each with a 16-dimensionality hidden statusand followed by batch normalisation. GIP and GPA mod-elled graphs to relate the objects with two privacy classesor the objects with each other, respectively, using a GraphReasoning Model as GNN. These two models belongto the two-stage end-to-end training category. They fine-tune the CNNs in the first stage to initialise the node fea-tures and the CNN thus contribute to the privacy decisionof the models. For a fair comparison, we adapt GIP andGPA to the two-stage hybrid approach by decoupling theGNN from the CNNs. We use only the GNN with the graph",
  "Concept confidence": ". Comparison of the explainability scores across training images correctly classified as private by the graph-agnostic (GA-MLP)and MLP models on the training set of PrivacyAlert . We show only the top 5 objects based on the largest mean absolute explainabilityscores. Note that colours of the data points represent the value of the object feature. Also note the different limits of the colour bars. plainabilty scores for a sample of images predicted as pri-vate by GA-MLP. Images are selected from the training setof PrivacyAlert based on the objects and their cardinalityidentified in the images, contrasting correct and incorrectpredictions. When images are correctly predicted as pri-vate (top row), high confidence in detecting a person signif-icantly influences the decision of the model. On the con-trary, the localisation of multiple individuals in an imagetends to favour the public class. Public images are oftenmisclassified due to the detection of person (second row ofimages). compares the predictions and explainabilityof GA-MLP and MLP across all images correctly identifiedas private in the training set3. As for the previous analysis,person is the most relevant concept for private predictions.Unlike MLP, GA-MLP favours the public class when threeor more people are detected.",
  ". Person-centric classification": "Based on the outcomes of the previous analysis, we de-vise two person-centric decision strategies that act directlyon the objects extracted by the vision models and the cor-responding features. The first, simple strategy classifies animage as private if at least one person is detected, xc0 1,where xc0 is the cardinality feature and the object c corre-sponds to person. The second, simple strategy includes anadditional constraint that limits the number of people lo-calised in an image, i.e., xc0 > 0 xc0 2, where is thelogical AND operator.We report the performance of these two strategies on thetesting set of PrivacyAlert in . The second strat-",
  "The confidence to predict the reference input as private is 0.2 for MLPand 0.1 for GA-MLP": ". Classification performance on the testing set of Priva-cyAlert . All the models are using the same object detectorto extract object features from the images. Note the failure ofGPA and GIP adapted to the hybrid approach and us-ing only GNN (recall of 100% for public class, and precision andrecall of 0% for private class). Their original performance wasdriven by the dependence on CNNs .",
  "Strategy-194.7655.0540.3490.8967.5572.97Strategy-289.6773.5548.5574.6769.1174.11": "KEY P: precision; R: recall, BA: Balanced accuracy; MLP: multi-layerperceptron; GA: graph-agnostic baseline; GPA: Graph Privacy Advisor ;: GPA adapted to the hybrid approach; : adapted GPA with corrected im-plementation of adjacency matrix; : GIP adapted to the hybrid ap-proach, using cardinality and confidence as object features and privacy nodeswith zero-initialised features. egy achieves performance comparable to GA-MLP and out-performs MLP, especially in terms of recall on the privateclass and balanced accuracy. The first strategy has lowerbalanced accuracy than the second strategy but achieves arecall of 90.89% in the private class denoting that most ofthe private images contain people. Nonetheless, this firststrategy has many false positives (a precision of 40.43% inthe private class), indicating that images with people are notnecessarily private. The more restrictive condition of thesecond strategy better balances the issues of the first strat-egy, but the recall for private images is limited to 74.67%.",
  ". Conclusion": "In this paper, we used post-hoc explainability to identifyand quantify objects contributing to the decision of imageprivacy classification models, which are trained on conceptsextracted from an image by a pre-trained detector. The ex-plainability analysis showed that privacy models, such asMLP and GA-MLP, are biased towards the presence of theobject person. Based on this finding, we devised two sim-ple person-centric strategies that achieve comparable over-all classification performance to that of the state-of-the-artmodels consodered in the comparison.Future work will extend the explainability analysis toother publicly available datasets, such as VISPR ,IPD and DIPA , and other models with differ-ent concepts and features . We will also in-clude and compare the results of other explainability meth-ods . We provide details of the methods considered for ourevaluation and explainability analysis. Specifically, we pro-vide the rationale behind the chosen methods, a review ofthe main aspects of each method. We also provide detailsof the parameter settings, and training details, and imple-mentation details in common to all privacy models for a faircomparison under the settings designed in the main paper.",
  "A.1. MLP": "Tonge et al.s method uses a convolutional neuralnetwork, pre-trained on ImageNet , for multi-label objectrecognition. The feature vector with the confidence of the1,000 objects is converted into binary values by assigning 1to the top-k most confident classes and 0 to the all the otherclasses. The binarised feature vector is used as input to aclassifier trained to predict the privacy of an image. Sup-port Vector Machine was used as classifier and k was set to10 in the study . Baranouskaya and Cavallaro de-fined different input features, such as person presence, per-son cardinality, outdoor scene, and sensitive features (e.g.,violence), and evaluated both a logistic regression and anMLP as privacy models.Following the ideas and results of these two studies, wedevise a baseline that aims to reproduce the method but us-ing the objects and their features as defined in the mainpaper (see ). Specifically, we replace the multi-label object recognition with the object detector, and thebinary feature vector with the cardinality and confidence",
  "A.2. Graph-based methods": "GIP and GPA belong to the two-stage end-to-end training-based category. Both methods model a graphof objects and two additional nodes representing the publicand private classes (privacy nodes). The 80 COCO cate-gories are used as objects.GIP relates objects and privacy nodes with a weighted,undirected, bipartite graph using the frequency of each ob-ject with respect to all images labelled as either private orpublic in a given dataset . A convolutional neural net-work (VGG-16) is fine-tuned to extracts deep features fromthe regions of interest localised in an image and associatedwith the corresponding object node in the graph. The pri-vacy nodes are initialised with the deep features extractedfrom the whole image by another fine-tuned convolutionalneural network (ResNet-101).When objects are not lo-calised in an image, their features are initialised to 0. Allfeatures are also complemented with a 1-hot encoding vec-tor to distinguish the privacy nodes, the object nodes, andthe object nodes with zero-initialised features.GPA relates objects with each other by finding at leastone co-occurrence of the objects in the dataset, resulting inan unweighted and undirected graph. GPA uses cardinalityas object features and initialises the features of the privacynodes with the logits from a trainable fully connected layerthat maps the outputs (logits) of a ResNet-50 pre-trainedfor scene recognition to the two privacy classes. The sceneclassifier is also fine-tuned during the training of GPA. BothGPA and GIP use a Graph Reasoning Model to propa-gate and refine the node features according to the modelledgraph structures, and then use a fully connected layer forthe final classification. The Graph Reasoning Model con-sists of three layers of Gated Graph Neural Network and a modified Graph Attention Network .",
  "A.3. From end-to-end to a hybrid approach": "To adapt the two methods to a two-stage hybrid ap-proach, we decoupled the graph component (Graph Reason-ing Model and fully connected layer) from the CNNs, andwe initialise the nodes with the cardinality and confidencefeatures obtained from the pre-trained object detector. Thismeans that there is no longer the end-to-end training of thewhole pipeline and fine-tuning of the CNNs.Under our setting, we cannot initialise the privacy nodesof GIP with the high-dimensionality (4,096) feature vectorsextracted by ResNet-101 and hence we initialise the fea-tures of the two nodes to 0. We refer to this model as GIP",
  "in of the main paper. Note that GIP was trained andevaluated only on the Image Privacy dataset , whereas": "we train a new GIP model trained only on PrivacyAlert.Similarly, we removed the dependency of the scene clas-sifier and the trainable fully connected layer for GPA. Be-cause of the presence of the privacy nodes, we also discardthe background category that was included to account forimages with no detected objects. We therefore train a modelas close as possible to the original implementation4 wherethe features of the object nodes are the cardinality and thebinary flag5. However, we replace the features of the pri-vacy nodes with pseudo-randomly generated values in theinterval according to the range of the logits esti-mated by the fine-tuned CNN to simulate a non-optimisedand non-zero initialisation of the features. We refer to thismodel as GPA in of the main paper. Note that wealso evaluated a variant with zero-initialisation of the fea-tures of the privacy nodes and we obtained the same results.As we noticed a misplacement of the adjacency matrixin the original implementation, we also corrected this errorand train a second model. For this second model, we useboth cardinality and confidence features, without the binaryflag, for a fair comparison with the other models. We referto this model as GPA in of the main paper. We alsotried with either of the two features, as well as using the pro-jection to a higher dimensionality as done for GA-MLP, butall of these models degenerate to predicting a single class.",
  "B. Parameters setting and training details": "Object detector. We use YOLOv3 , pre-trained on the80 categories of COCO , as object detector. When lo-calising the objects, we allow a maximum of 50 objects foreach image while retaining the most confident ones afterre-ranking. We also use a minimum threshold of 0.6 anda non-maximum suppression threshold at 0.4. Accordingto the detector settings, we resize images to a resolutionof 416416 pixels. Note that these settings are differentfrom GIP and GPA, which limit the maximum number ofregions of interest only to 12. Moreover, GIP used MaskR-CNN as object detector with a threshold of 0.7 on theobject confidence and the weighted edges of their modelledgraph included images from the testing set (data leakage).On the contrary, GPA used YOLOv3 with a thresholdof 0.8 on the object confidence. Our choice to decrease thethreshold is to allow the localisation of more objects in animage, increasing the detected categories and the cardinal-ity for more discriminative features. However, the lowerthreshold can also result in more false positives and affect-ing the input features of the privacy model that should bedesigned to handle noisy data.Training. For reproducibility of models and experiments,we set the seed to an arbitrary value of 789. Note that we",
  "our experiments, we found that the flag does not provide any con-tribution to the model": "do not analyse variations in the performance due to multi-ple and different seeds, which is beyond the scope of thispaper. As training strategy, we follow the recipe of Bench-marking Graph Neural Networks . We use Adam as op-timizer with an initial learning rate of 0.001 and with-out weight decay. We schedule the learning rate to halveif the balanced accuracy of the validation set does not im-prove for at least 10 epochs (patience). We use early stop-ping to interrupt the training of the models if the learningrate decreases to a value lower than 0.00001 or the train-ing time lasts longer than 12 hours. In case none of thetwo conditions is satisfied, we also set the maximum num-ber of epochs to 1,000. Note that we save the model at theepoch with the highest balanced accuracy in the validationsplit and We use this model for the evaluation on the testingsplit. Moreover, we set the batch size to 100.",
  "C. Implementation": "We implement all models using PyTorch 1.13.1. We usethe PyTorch Geometric library for GIP, GPA, and GA-MLP.We trained all models on a Linux-based machine with aNVIDIA GeForce GTX 1080 Ti (12 GB RAM). To en-sure the fairness of the benchmark, all methods share thesame training and testing software (i.e., only the model isreplaced)."
}