{
  "Abstract": "This paper reviews the NTIRE 2024 RAW Image Super-Resolution Challenge, highlighting the proposed solutionsand results. New methods for RAW Super-Resolution couldbe essential in modern Image Signal Processing (ISP)pipelines, however, this problem is not as explored as inthe RGB domain. Th goal of this challenge is to upscaleRAW Bayer images by 2x, considering unknown degrada-tions such as noise and blur. In the challenge, a total of230 participants registered, and 45 submitted results duringthee challenge period. The performance of the top-5 sub-missions is reviewed and provided here as a gauge for thecurrent state-of-the-art in RAW Image Super-Resolution.",
  ". Introduction": "RAW Image Super-Resolution represents an active researchdirection, aiming at upscaling hardware-specific RAW im-age representations, while dealing with hardware charac-teristic properties, often depending on the technical imple-mentation of the camera product. The lack of standardiza-tion in camera Image Processing Signal (ISP) implementa-tion induces plenty variety into the camera market segment,with plenty corrections made through image processing al-gorithms , overcoming the hardware lim-itations of various devices.The RAW image represents the discretized and quan-tized representation of the image signal. Of course, boththe aforementioned operation depend on the sensor nature,with different sensor types, different spatial resolutions or Marcos V. Conde ( corresponding author, project lead), Florin-Alexandru Vasluianu, amd Radu Timofte are the challenge organizers,while the other authors participated in the challenge and survey. University of Wurzburg, CAIDAS & IFI, Computer Vision Lab.NTIRE 2024 webpage: color resolutions (the number of bits used for quantization).Naturally, portable camera devices, that are usually sub-ject to extreme limitations in terms of size, power supply,and used optics are constricted in terms of achieved im-age quality, usually employing low color resolutions andspatial resolutions. However, these specific hardware im-plementations are currently leading the market in terms ofnew acquired devices, with cameras becoming ubiquitousand universal accessible. Therefore, the image informationspace is also characterized by a high availability of imagescorresponding to these devices, with complex ISP systems mapping the RAW image representation toa perceptually meaningful RGB counterpart.RAW image upscaling remains also relevant in the pro-fessional photography field. Given the strong developmentof Internet-based image transfer services, the drive for highresolution images is now higher than ever before. However,building a professional photography or videography setup isa difficult challenge, since the market is divided by variousapplication-specific parameters, with the acquisition costsremaining the main limiting factor for a new investment.Thus, it is important to develop algorithms which are robustto the various hardware limitations affecting the low-costamateur photography systems, matching the characteristicsof the high resolution sensors used in highly professionalapplications.RAW information is extremely important, since theRAW data directly correlates with scene radiance, with thediscretization and quantization being the only non linear op-erations affecting the naturally continuous radiance signalmeasure in a photography. This property of the RAW datais extremely beneficial in analyzing typical image degra-dations as acquisition noise or image blur.Futher awayfrom the sensor representation, ISP represents a sequenceof highly non-linear operations, happening with irrevocableloss of information . This further complicates the im-age restoration task, with complex algorithms needing every",
  ". Samples of the NTIRE 2024 RAW Image Super-Resolution Challenge testing set": "achievable bit of signal based variance . Consider-ing all the aforementioned factors, RAW image processingposes significant advantages over the standard sRGB repre-sentation, with superior performance in a multitude of low-level imagery applications like image denoising ,deblurring , exposure adjustment , and image super-resolution .Due to the lack of standardization at the hardware imple-mentation level, RAW images are characterized by vendoror even product specific properties, which are explainableonly with private, with the photography products shieldedby implementation patents or trade secrets. Coupled to theabundance of the standard sRGB representation, most ofthe existing high complexity image restoration algorithms are specifically designed for compressed oruncompressed RGB image or video.As a sub-task of Image Restoration, cutting-edge Sin-gle Image Super-Resolution (SISR) algorithms follow the same data specific, even if they are relying ondeep convolutional networks or Transformer archi-tectures. One of the largest drawbacks characterizing thesealgorithms is represented by them being limited by the qual-ity of the data used for optimization. Various image restora-tion applications are characterized by extreme difficultiesin acquiring real domain paired data , driving theneed for realistic and relevant data synthesis . Ac-curately modeling application specific degradations in thesRGB representation proves extremely difficult, given the fact that the highly nonlinear ISP characteristic shifts alsothe physical characteristics of the degradation appearance.This represents the main factor limiting the performance ofthese algorithms in real applications deployment, with thegap observable at the data level being difficult to overcomewith model-free algorithms. Therefore, studying RAW data becomes a crucial step indriving the general image restoration performance improve-ment trend. Consequently, RAW Image Super-Resolutionapplications can benefit from the increased variance sig-nal, with algorithms robust to fine architectural propertiesof the involved devices. Developing highly effective algo-rithms like the ones well-established in the sRGB domain can prove a step ahead in the development ofhigh performance imagery applications, with cost effectivedevices. Thus, in this work we are presenting the solutions sub-mitted for the NTIRE 2024 RAW Image Super Resolu-tion Challenge. We are providing information regarding thechallenge setup, with the task description and the challengedata properties characterizing the challenge dataset splits.We are also listing information regarding the challenge par-ticipants, with their teams and affiliations.",
  "Interpolation 35.95 / 0.9536.038 / 0.95236.926 / 0.956": ". We provide PSNR/SSIM results on the validation set (40 images), the complete testing set (200 images), and the testing set atfull-resolution (12MP) RAW images . All the fidelity metrics are calculated in the RAW domain. NA indicates the results are notavailable for the method. We highlight two baseline methods. We also report the number of parameters of each method, if the method wastrained end-to-end (Yes/No), and the image resolution used for training the models. Related Computer Vision ChallengesOur challengeis one of the NTIRE 2024 Workshopassociated chal-lenges on:dense and non-homogeneous dehazing ,night photography rendering , blind compressed im-age enhancement , shadow removal , efficient su-per resolution , image super resolution (4) ,light field image super-resolution , stereo image super-resolution , HR depth from images of specular andtransparent surfaces , bracketing image restoration andenhancement , portrait quality assessment , qualityassessment for AI-generated content , restore any im-age model (RAIM) in the wild , RAW image super-resolution , short-form UGC video quality assess-ment , low light enhancement .",
  ". Dataset": "The challenge dataset is based on BSRAW . Followingprevious work , we use images from the AdobeMIT5K dataset , which includes images from multpleCanon and Nikon DSLR cameras.The DSLR images are manually filtered to ensure di-versity and natural properties (i.e. remove extremely darkor overexposed images), we also remove the blurry images(i.e. we only consider all-in-focus images).The pre-processing is as follows: (i) we normalize allRAW images depending on their black level and bit-depth.(ii) we convert (pack) the images into the well-knownRGGB Bayer pattern (4-channels), which allows to applythe transformations and degradations without damaging theoriginal color pattern information .Training: We provide the participants 1064 102410244clean high-resolution (HR) RAW images. The LR degradedimages can be generated on-line during training using thedegradation pipeline proposed in BSRAW .Such degradation pipeline considers different noise pro-files, multiple blur kernels (PSFs) and a simple downsam-",
  ". Results": "We use three testing splits: (i) Validation, 40 1024px imagesusing during the model development phase. (ii) Test 1MP,200 images of 1024px resolution. (iii) The same 200 testimages at full-resolution 12MP. The participants processthe corresponding LR RAW images (e.g. 512 512 4),and submit their results. Thus, the participants do not haveaccess to the ground-truth images.We provide samples of the testing set in .In Tab. 1 we provide the challenge benchmark. Besidesfidelity metrics such as PSNR and SSIM, we also providerelevant implementation details of each method. The meth-ods can greatly improve the RAW images quality and res-olution, even in the case of full-resolution 12MP images asoutput. We provide detailed visual comparisons in , and . All the proposed methods are able to in-crease the resolution and details of the RAW images whilereducing blurriness and noise. Moreover, there are not de-tectable color artifacts.We can conclude that (synthetic) RAW image super-resolution can be solved similarity to RAW denoising.However, more realistic downsampling remains an openchallenge. AcknowledgementsThis work was partially supportedby the Humboldt Foundation. We thank the NTIRE 2024sponsors: Meta Reality Labs, OPPO, KuaiShou, Huaweiand University of Wurzburg (Computer Vision Lab).",
  "Samsung Research China - Beijing (SRC-B)2 Samsung MX(Mobile eXperience) Business": "Team Samsung MX,SRC-B is introducing a two-stagenetwork for RAW Image Super Resolution. The solutionis using the divide-and-conquer strategy, with the first stagetasked with recovering the image structure from the low res-olution degraded RAW image, and the second stage aimingat recovering the maximum amount of details, offering a re-fined reconstruction. Moreover, the team is further extend-ing existing methods for synthetic data generation, study-ing further into hardware specific RAW image degradations,proposing new definitions for the relevant device-specificnoise profiles and new blur kernels aligning with typicalreal-world scenarios. They propose a randomized degra-dation model, simulating different interactions between theobserved simulated defects.Finally, Team Samsung MX,SRC-B is proposing a novelFocal Pixel Loss, which was proven through the perfor-mance improvements during the model fine-tuning stage.As shown in , the network structure mainly in-cludes two stages.The first stage mainly draws onRestormer , whose main role is to restore the maincontent of raw images. The second stage mainly uses aNAFNet based design, whose main role is to restoremore details on the basis of the first stage of recovery.The training procedure accounts for the dual stage de-sign of the proposed method. The first stage of the trainingprocedure follows the optimization of the parameters cor-responding to the first stage of the model. In the secondstage of the training procedure, the optimized parametersare frozen, with the parameters of the second stage startingbeing refined, for optimum specialization. The final estima-tion is then performed using both sets of optimized param-eters (Restormer and NAFNet models).one of the main details of the solution is the proposal of anovel training objective, based on the characteristics of theinput data. The proposition of the Focal Loss (see Eq. (1))is a solution for the observed imbalance in terms of highlyaffected pixels ratio, given the non-unifor effect of the sig-nal degradation function . Therefore, the Focal Pixel Loss(FPL) is introducing exponential penalties to those pixelscharacterized by a large signal shift.",
  "In Eq. (1), D(., .) is a standard L-norm distance betweenthe restored image I and the reference image I, and is anadjustable factor, controlling the strength of the penalty": "Implementation detailsThe model is trained solely onthe data provided by the challenge organizers. It only con-tains more than 1000 RAW images from various DSLRcamera sensors. The dataset was augmented using standardimage augmentation techniques and simulations of the im-age degradation pipeline. The training procedure is a dualstage operation, optimizing the model stages sequentially.The optimization technique used is the AdamW opti-mizer (1 = 0.5, 2 = 0.999, weight decay 0.0001) withthe cosine annealing strategy, where the learning rate grad-ually decreases from the initial learning rate 5 105 to1 107 for 5 105 iterations. The model goes through apre-train optimization phase, base don the L1 loss. Duringthe fune-tune phase, the objective is set to the Focal PixelLoss, with the initial learning rate being set to 5106. Thetraining batch size is set to 4 and patch size is 384. Hori-zontal/vertical flipping and rotation are used for data aug-mentation. All experiments are conducted on A100 GPUs.",
  "Xiaomi Inc. 2Georgia Institute of Technology": "The solution proposed by Team XiaomiMMAI is adual branch network based on HAT , adopting re-parameterization during training, using the additionalparameters to fully exploit the potential of the method.They are introducing, a task-by-task and step-by-step train-ing method for RAW Image Super-Resolution to simulta-neously address three tasks: denoising, deblurring, and 2Super Resolution.To address the limitation given by the low number ofsamples offered for training, Team XiaomiMMAI convertsthe RAW images into an RGB images and performs com-bined data enhancement on the set of produced RGB im-age, using random rotations, flips, color changes, brightnesschanges, random blur, etc.. Then, the set of enhanced RGBimages are translated back to the RGGB RAW domain, withthe processed data used to train the proposed solution.The model proposed by Team XiaomiMMAI is in-spired by HAT , with the architecture being optimizedfor the RAW Image Super Resolution task.The opti-mized dual-branch network structure (DB-HAT) is shown in. The introduced Step-by-step and task-by-task train-ing method for RAWISR further enhances the performancelevel achieved by their solution.Step-by-step: To accelerate training and achieve goodperformance, Team XiaomiMMAI adopted a strategy whereeach sub-task, including the final joint optimization, istrained based on a pyramid image representation.Ini-tially, the model is trained on small scale images (6464),gradually increasing the resolution of the image patches to128128 and 256256.Task-by-task: Team XiaomiMMAI divided RAWISRinto three sub-tasks: denoising, deblurring, and 2x SR. Ini-tially, they start by training for RAW image denoising, fol-lowed by the connected tasks of deblurring and 2 SuperResolution. Finally, a joint optimization procedure is ap-plied for entire network to produce the final estimator.In the process of training denosing and deblurring, TeamXiaomiMMAI used the RepConv re-parameterization tech-nique on the final stage of the proposed DB-HAT, to im-prove the visual image quality of the task. The reparameter-izable convolution block (RepConv) is shown in Fig 4. Implementation detailsThe dataset used for three sub-task training consists of 1000+ RAWs, and the data aug-mentation methods can refer to the previous section for de-tails. In the final stage of joint optimization of the entire net-work, Team XiaomiMMAI used the provided 1000+ datasetinstead of the augmented dataset. The learning rate is ini-tialized at 4 104 and decays according to the CosineAn-nealing strategy during the training of three sub-tasks. Thenetwork undergoes training for a total of 2x105 iterations,with the L2 loss function being minimized as the trainignobjective of the Adam optimizer.",
  "Contact:": "The solution proposed by Team NUDT RSR addressesthree major components of the proposed challenge, extend-ing on the degradation pipeline, model design, and modelsupervision, achieving a significant performance level interms of restoration fidelity.For the image signal degradation pipeline, the considereddegradations include diverse blur kernels, exposure defects,image downsampling, finally coupled with a noise modelcharacteristic to real-world raw image data. . The overall network architecture proposed by Team NUDT RSR. The proposed FFM FFT block is a two-branched dolution, withthe Fourier branch extracting the amplitude and phase, guiding the global-local feature mixing performed in the spatial branch, throughSAFM and CCM blocks. Following , the proposed solution is benefittingfrom the amplitude and phase components, added to theSAFMN backbone. the model is fusing frequency do-main and spatial domain information, for global-local levelfeature mixing.Moreover, the knowledge distillation is deployed to thedescribed solution, using a NAFNet teacher, with mul-tiple complexity level feature supervision. Finally, we ap-ply a progressive training strategy, gradually increasing thepatch size at each stage to accommodate larger test inputs.Degradation pipeline: Inspired by and , inorder to enable the model to learn real degradation infor-mation, the Bayer pattern RAW images are cropped, fol-lowing by degrading the RAW signal in a sequence of op-erations composed of multiple blurring operations, expo-sure compensations, downsampling, and hardware specificadded noise.The first step is represented by a blurring operator, basedon randomly generated Gaussian blur, generalized Gaus-sian blur, with a plateau-shaped distribution, and their an-isotropic version. The augmented PSF kernels provided bystarter kit are also considered. All kernel sizes are rangingfrom 7 7 to 25 25.Then, the pipeline continues with linear adjustment forimage exposure. As discussed, to simulate the artifactscaused by underexposure and overexposure, the pipelineimplements exposure adjustment by linearly scaling the im-age. The adjustment factor is tuned to the [-0.25, 0.25] in-terval, applied in the unit interval normalized images.Next, the image suffers further downsampling, consid-ering different downsampling kernels, including bicubic in-terpolation, bilinear interpolation, and an average-pooling operator. To build multi-scale training pairs, the input im-age is either upsampled or downsampled to a random sizefirst, and rescaled back to half of the original size as for 2super-resolution tasks.On the downscaled image, heteroscedastic Gaussiannoise is then applied, followed by the practical shot-read noise for different exposure levels. An image withhigher exposure factor in step 2 is more likely to get noisedby heteroscedastic noise, and the shot-read noise for low-light images.The last step of the degradation pipeline is a secondblurring operator.To expand the degradation space likethe high-order degradation model , a random operationbased on a set of second blurring kernels (same kernels con-sidered in the first step), characterized by smaller standarddeviations, is applied in the final stage. NetworkAccording to , the Fourier spectrogram ofan image has similar amplitude to its downsampled ones,while the phase is related to the noise observed in the ac-quired image signal. Although it is designed for low lightimage enhancement tasks, Team NUDT RS started withthe observation that blind RAW Image super-resolution canalso benefit from refining these two components of an lowresolution image.Thus, the Team NUDT RS proposed model is repre-sented in , where the input image is encoded by a3 3 convolution layer for shallow feature extraction, andthe FFM FFT blocks are used for deep feature extraction.Following , the main blocks are divided into spatialbranches and Fourier branches. In each block, the inputis simultaneously sent to both branches, then the processed",
  ". Main branch of the framework proposed by Team McMaster. (a) Residual Swin Fourier Transformer Block (RSFTB)(b) Swin Transformer Layer (STL)": "as a form of degradation in the forward process. Diffu-sion models have been proposed for various imag-ing tasks, including image super-resolution, demonstratingtheir capability for end-to-end training to transform pureGaussian noise into meaningful data representations. Theyinvestigate the applicability of gradual magnitude Gaus-sian noise, as utilized in diffusion models, for address-ing the degradation process inherent in Raw Image Super-Resolution tasks.Team McMaster adopted the additivenoise model from , exposing the input RAW imagesto noise using the forward diffusion definition described in. In the forward diffusion process, the input data under-goes a degradation process the iterative addition of Gaus-sian noise across 1000 discrete steps. In the backward de-noising process of DiT, the model is optimized for the for-ward diffusion process, maximizing the data likelihood viathe variational lower bound. The architectural configuration of the Team McMasterproposed model is depicted in .In the proposedmethod, a SwinFSR-based design performs RAW do-main image feature extraction, combined with feature up-sampling, matching the size of high-resolution images via acomplex convolution operator. SwinFSR builds on the suc-cess of SwinIR , with an additional data modality givenby the Frequency Domain Knowledge, through the FFT im-age representation . This proves to be a superior strategy,combining spatial and spectral features as a way to balancethe local information of the spatial domain, and the globalinformation accessed through the spectral representation. Itintroduces a novel cross-attention module for efficient in-formation exchange between the two modalities and adaptsto rectangular input patches for flexibility.",
  "For the proposed model, only the feature extractionbranch of SwinFSR is deployed": "Implementation detailsThe solution was optimizedsolely on the NTIRE 2024 official challenge data , usingthe proposed Development Phase submission set for valida-tion. This dataset contains 1064 4-channel DSLR-specificRGGB RAW images for training, and an additional 40 rawimages set for validation. The images were pre-processed,applying white-black level correction, then being normal-ized to the unit interval. Since the degraded low-resolutionraw images are characterized by low quality, with a con-siderable level of details being lost, a data augmentationtechnique is applied, to improve the training procedure interms of stability, convergence, and the achieved perfor-mance level. The strategy combines simple horizontal orvertical flips with channel shifts and mixup augmentations.The training objective is based on the L1 loss.",
  "SAFMN0.22959.0641.20SAFMN FFT0.27267.2941.81": ". A comparison between the NAFNet, vanilla SAFMN andthe method proposed by Team NUDT RS. The large version ofNAFNet uses encoding blocks, decodingblocks for each stage, and 12 middle blocks, the width is set to64. While the smaller version use width 32, with both con-figuration for encoding and decoding blocks and 1 middle block.The vanilla SAFMN has dim=36, ffn scale=2 configuration, and 8main blocks, which are the same to teh poposed SAFMN FFT. Allflops are calculated with input size 1 4 512 512. features of the branches are fused. After a residual connec-tion, a final pixel-reshuffle operator is used to upscale thefeqature set to the resolution of the reference image .In the spatial branch, the cross domain communica-tion is performed through efficient Feature Mixing Module(FMM) blocks of SAFMN . A FMM block consists of aSAFM block and a CCM block. SAFM splits the channelsinto different parts, fuses their features at different scale lev-els and obtain attention map after GELU activation. Then,the original input multiplies the attention map. The CCMblock consists of a 3 3 convolution layer and a 1 1 con-volution layer, which works as a channel mixer to capturelocal context information.In the Fourier branch, an image is transformed to fre-quency map by Fast Fourier Transform (FFT) operator toget amplitude and phase component, which are later pro-cessed by two 1 1 convolution layers with GELU activa-tions respectively. Next, these refined components are com-bined to a new frequency map, and the inverse FFT its usedto transfer back to the spatial domain. Features from differ-ent branches are concatenated and fused by another 1 1convolution. The proposed model uses 8 such blocks withwidth 36. To avoid expensive computations on high dimen-sions, the SFT layers in the final reconstruction stage areremoved . This results in an efficient estimator, withthe total parameter count being 272.068 K for the proposedsolution. The team presents an ablation study in Tab. 2. Training strategyA two-stage optimizing strategy is ap-plied. All the training is based on the development datasetand there are no external datasets used.Firstly, a large version of NAFNet model is trainedas a teacher network, with increased complexity degrada-tions on patch size 128. Then this NAFNET model is usedto apply knowledge distillation, as part of the optimizationtechnique used for the proposed solution. To reduce the per-formed computations, the knowledge does not rely on dis- tances computed between multi-level feature sets, but ratheron statistics defining the feature see .Secondly, to accommodate the high resolution test im-ages, the student model is progressively finetuned on in-creased resolution patches.The training patch size in-creases from 128, 256, 352 to 448.The optimization objective is based on the L! dis-tance simultaneously applied in the spatial and Fourier do-mains . The total loss is defined in Eq. (5).",
  "Ltotal = Lp + Lf + Lkd(5)": "I1, I2 denotes the restored image prediction and the cor-responding reference image.F() is the FFT operator,Ni() is the feature extracting operator determined by thenetwork, where i D is a set of the intermediate layersand G() is defined to the square of the channel-wise meanof intermediate features. The c represents Charbonnierloss. and are used to control the weights of differentcomponents. The finetuning stage only uses the Lp and Lfloss terms. Implementation detailsThe experiments are based onthe Pytorch framework for implementation. The NAFNetteacher model uses a width 64, with 2, 2, 4, 8 encoderblocks and 2, 2, 2, 2 decoder blocks at each stage. Themiddle block num is set to 12.The teacher module istrained for 600000 iterations. The learning rate is set to2e-4, and halves for every 100000 steps after 300000 steps.The training procedure applied for the teacher network usesonly the Lp loss. For knowledge distillation, the procedurestarts with an initial learning rate warm-up stage, lasting for30000 iterations. Then, a cosine annealing stage is applied,decreasing the learning rate to 1e-6.The FFT L1 loss is balanced with a weight = 0.05,and the distillation loss with = 0.1. For the progressivefinetuning, each training stage for the proposed model lastsfor 200,000 iterations with learning rate 5e-5 that is reducedto 1e-6 with a cosine scheduler. Alongside the augmenta-tion involved in the degradation pipeline, some traditionaldata augmentation methods, such as random cropping, flip-ping, and rotation are deployes to further diversify the train-ing dataset. The testing phase deploys the geometric self-ensemble strategy , which averages 8 outputs corre-sponding to 8 augmented versions of the input image. Allthe experiments are conducted on a NVIDIA GeForce RTX2080Ti GPU, using the AdamW optimizer with 1 = 0.99,2 = 0.9.",
  "Abdelrahman Abdelhamed, Stephen Lin, and Michael SBrown.A high-quality denoising dataset for smartphonecameras. In CVPR, pages 16921700, 2018. 2": "Eirikur Agustsson and Radu Timofte. Ntire 2017 challengeon single image super-resolution: Dataset and study. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition workshops, pages 126135, 2017. 2 Cosmin Ancuti,Codruta O Ancuti,Florin-AlexandruVasluianu, Radu Timofte, et al. NTIRE 2024 dense and non-homogeneous dehazing challenge report. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) Workshops, 2024. 3 Nikola Banic, Egor Ershov, Artyom Panshin, Oleg Karasev,Sergey Korchagin, Shepelev Lev, Alexandr Startsev, DaniilVladimirov, Ekaterina Zaychenkova, Dmitrii R Iarchuk,Maria Efimova, Radu Timofte, Arseniy Terekhin, et al.NTIRE 2024 challenge on night photography rendering. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR) Workshops, 2024. 3 Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen,Dillon Sharlet, and Jonathan T Barron. Unprocessing imagesfor learned raw denoising. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition,pages 1103611045, 2019. 1, 2, 5",
  "NTIRE 2024 challenge on image super-resolution (4):Methods and results. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition (CVPR)Workshops, 2024. 3": "Marcos V Conde, Ui-Jin Choi, Maxime Burchi, and RaduTimofte. Swin2SR: Swinv2 transformer for compressed im-age super-resolution and restoration. In Proceedings of theEuropean Conference on Computer Vision (ECCV) Work-shops, 2022. 2 Marcos V Conde, Steven McDonagh, Matteo Maggioni,Ales Leonardis, and Eduardo Perez-Pellitero. Model-basedimage signal processors via learnable dictionaries. In Pro-ceedings of the AAAI Conference on Artificial Intelligence,pages 481489, 2022. 1 Marcos V Conde, Florin Vasluianu, Javier Vazquez-Corral,and Radu Timofte.Perceptual image enhancement forsmartphone real-time applications.In Proceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision, pages 18481858, 2023. 1, 2 Marcos V Conde, Florin Vasluianu, and Radu Timofte.Bsraw: Improving blind raw image super-resolution. In Pro-ceedings of the IEEE/CVF Winter Conference on Applica-tions of Computer Vision, pages 85008510, 2024. 1, 2, 3, 6,8, 10, 11, 12 Marcos V Conde, Florin Vasluianu, and Radu Timofte. DeepRAW image super-resolution. a NTIRE 2024 challenge sur-vey. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition Workshops, 2024. 3, 6,7 Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,Guiguang Ding, and Jian Sun. Repvgg: Making vgg-styleconvnets great again. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages1373313742, 2021. 5",
  "Qinquan Gao, Yan Zhao, Gen Li, and Tong Tong. Imagesuper-resolution using knowledge distillation. In ComputerVision ACCV 2018, pages 527541, Cham, 2019. SpringerInternational Publishing. 9": "Samuel W. Hasinoff, Dillon Sharlet, Ryan Geiss, AndrewAdams, Jonathan T. Barron, Florian Kainz, Jiawen Chen, andMarc Levoy. Burst photography for high dynamic range andlow-light imaging on mobile cameras. ACM Transactions onGraphics (Proc. SIGGRAPH Asia), 35(6), 2016. 2 Felix Heide, Markus Steinberger, Yun-Ta Tsai, MushfiqurRouf, Dawid Pajak, Dikpal Reddy, Orazio Gallo, Jing Liu,Wolfgang Heidrich, Karen Egiazarian, et al. Flexisp: A flex-ible camera image processing framework. ACM Transactionson Graphics (ToG), 33(6):113, 2014. 1 Andrey Ignatov, Luc Van Gool, and Radu Timofte. Replac-ing mobile camera isp with a single deep learning model.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, pages 536537,2020. 1 Andrey Ignatov, Cheng-Ming Chiang, Hsien-Kai Kuo, Anas-tasia Sycheva, and Radu Timofte. Learned smartphone isp onmobile npus with deep learning, mobile ai 2021 challenge:Report. In CVPR Workshops, pages 25032514, 2021. 1 Siyuan Jiang, Senyan Xu, and Xingfu Wang. Rbsformer: En-hanced transformer network for raw image super-resolution.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, 2024. 3, 6, 10,11, 12",
  "Chongyi Li, Chun-Le Guo, Man Zhou, Zhexin Liang,Shangchen Zhou, Ruicheng Feng, and Chen Change Loy.Embedding fourier for ultra-high-definition low-light imageenhancement, 2023. 8, 9": "Siyuan Li, Iago Breno Araujo, Wenqi Ren, ZhangyangWang, Eric K Tokuda, Roberto Hirata Junior, Roberto Cesar-Junior, Jiawan Zhang, Xiaojie Guo, and Xiaochun Cao. Sin-gle image deraining: A comprehensive benchmark analysis.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 38383847, 2019. 2 Xin Li, Kun Yuan, Yajing Pei, Yiting Lu, Ming Sun, ChaoZhou, Zhibo Chen, Radu Timofte, et al. NTIRE 2024 chal-lenge on short-form UGC video quality assessment: Meth-ods and results. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR) Work-shops, 2024. 3 Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, LucVan Gool, and Radu Timofte. Swinir: Image restoration us-ing swin transformer. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 18331844,2021. 2, 7 Jie Liang, Qiaosi Yi, Shuaizheng Liu, Lingchen Sun,Rongyuan Wu, Xindong Zhang, Hui Zeng, Radu Timo-fte, Lei Zhang, et al.NTIRE 2024 restore any imagemodel (RAIM) in the wild challenge.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) Workshops, 2024. 3 Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, andKyoung Mu Lee. Enhanced deep residual networks for singleimage super-resolution. In 2017 IEEE Conference on Com-puter Vision and Pattern Recognition Workshops (CVPRW),pages 11321140, 2017. 9 Jiaming Liu, Chi-Hao Wu, Yuzhi Wang, Qin Xu, YuqianZhou, Haibin Huang, Chuan Wang, Shaofan Cai, Yifan Ding,Haoqiang Fan, et al.Learning raw image denoising withbayer pattern unification and bayer preserving augmentation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, pages 00, 2019.3 Xiaohong Liu, Xiongkuo Min, Guangtao Zhai, Chunyi Li,Tengchuan Kou, Wei Sun, Haoning Wu, Yixuan Gao, YuqinCao, Zicheng Zhang, Xiele Wu, Radu Timofte, et al. NTIRE2024 quality assessment of AI-generated content challenge.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR) Workshops, 2024. 3",
  "Ilya Loshchilov and Frank Hutter.Decoupled weight de-cay regularization. In International Conference on LearningRepresentations, 2019. 4": "Ben Mildenhall, Jonathan T Barron, Jiawen Chen, DillonSharlet, Ren Ng, and Robert Carroll. Burst denoising withkernel prediction networks. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages25022510, 2018. 2 Guocheng Qian, Yuanhao Wang, Chao Dong, Jimmy S Ren,Wolfgang Heidrich, Bernard Ghanem, and Jinjin Gu. Re-thinking the pipeline of demosaicing, denoising and super-resolution. arXiv preprint arXiv:1905.02538, 2019. 2 Bin Ren, Yawei Li, Nancy Mehta, Radu Timofte, et al. Theninth NTIRE 2024 efficient super-resolution challenge re-port. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR) Workshops,2024. 3",
  "Eli Schwartz, Raja Giryes, and Alex M Bronstein. Deepisp:Toward learning an end-to-end image processing pipeline.IEEE Transactions on Image Processing, 28(2):912923,2018. 1": "Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,Andrew P Aitken, Rob Bishop, Daniel Rueckert, and ZehanWang. Real-time single image and video super-resolutionusing an efficient sub-pixel convolutional neural network. InIEEE Conference on Computer Vision and Pattern Recogni-tion, pages 18741883, 2016. 2 Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and ZehanWang. Real-time single image and video super-resolutionusing an efficient sub-pixel convolutional neural network.In 2016 IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 18741883, 2016. 9 Long Sun, Jinshan Pan, and Jinhui Tang. Shufflemixer: Anefficient convnet for image super-resolution.In Advancesin Neural Information Processing Systems, pages 1731417326. Curran Associates, Inc., 2022. 9 Long Sun, Jiangxin Dong, Jinhui Tang, and Jinshan Pan.Spatially-adaptive feature modulation for efficient imagesuper-resolution. In 2023 IEEE/CVF International Confer-ence on Computer Vision (ICCV), pages 1314413153, 2023.8, 9",
  "Timofte, et al.NTIRE 2024 challenge on stereo imagesuper-resolution: Methods and results.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) Workshops, 2024. 3": "Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.Real-esrgan: Training real-world blind super-resolution withpure synthetic data. In 2021 IEEE/CVF International Con-ference on Computer Vision Workshops (ICCVW), pages19051914, 2021. 8 Yingqian Wang, Zhengyu Liang, Qianyu Chen, LongguangWang, Jungang Yang, Radu Timofte, Yulan Guo, et al.NTIRE 2024 challenge on light field image super-resolution:Methods and results. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition (CVPR)Workshops, 2024. 3 Xiangyu Xu, Yongrui Ma, and Wenxiu Sun. Towards realscene super-resolution with raw images. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 17231731, 2019. 1, 2, 3",
  "Xiangyu Xu, Yongrui Ma, Wenxiu Sun, and Ming-HsuanYang. Exploiting raw images for real-scene super-resolution.IEEE transactions on pattern analysis and machine intelli-gence, 44(4):19051921, 2020. 3": "Ren Yang, Radu Timofte, et al. NTIRE 2024 challenge onblind enhancement of compressed image: Methods and re-sults. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR) Workshops,2024. 3 Huanjing Yue, Zhiming Zhang, and Jingyu Yang.Real-rawvsr: Real-world raw video super-resolution with a bench-mark dataset. In Computer VisionECCV 2022: 17th Eu-ropean Conference, Tel Aviv, Israel, October 2327, 2022,Proceedings, Part VI, pages 608624. Springer, 2022. 2 Pierluigi Zama Ramirez, Fabio Tosi, Luigi Di Stefano, RaduTimofte, Alex Costanzino, Matteo Poggi, et al. NTIRE 2024challenge on HR depth from images of specular and trans-parent surfaces. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR)Workshops, 2024. 3 Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.Restormer: Efficient transformer for high-resolution imagerestoration. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 57285739,2022. 2, 4, 6"
}