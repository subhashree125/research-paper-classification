{
  "NVIDIA": "A happy little girl is standing in a green field, wearinga plaid shirt and holding onto a string of pink balloons.The balloons are floating in the air, creating a playfuland joyful atmosphere. The girl appears to be enjoyingthemoment,withasmileonherface.Inthe background, there is a vast expanse of green grass,stretching out as far as the eye can see. The sky aboveis a bright blue, with a few white clouds scatteredabout. The overall mood ofthe scene is one of",
  "The image shows a young girl in a field with her armsjoyfully raised. She is holding a bunch ofpink": "balloons in her right hand. She's wearing a plaid shirtand jeans, and her hair is styled in what appears to bea ponytail. The girl is smiling widely, and her posesuggests she is in mid-celebration or play. The field isfilled with wildflowers, and trees can be seen in thebackground under a blue sky with some clouds. Thesetting appears to be a sunny day, perfect for outdooractivities. The image captures a moment of carefreechildhood happiness. A 3D model of a three-level, intricately designedwooden tower with a contrasting blue roof and door,standing on a wooden base. The tower, brown incolor, resembles a fusion of a house, a tower, and acastle. At the very top of the tower, there is acrescent moon design. The overall design adds atouch of fantasy to the scene.",
  "Abstract": "Existing automatic captioning methods for visual con-tent face challenges such as lack of detail, content hallu-cination, and poor instruction following. In this work, wepropose VisualFactChecker (VFC), a flexible training-freepipeline that generates high-fidelity and detailed captionsfor both 2D images and 3D objects. VFC consists of threesteps: 1) proposal, where image-to-text captioning modelspropose multiple initial captions; 2) verification, where alarge language model (LLM) utilizes tools such as objectdetection and VQA models to fact-check proposed captions;3) captioning, where an LLM generates the final caption bysummarizing caption proposals and the fact check verifica- tion results. In this step, VFC can flexibly generate cap-tions in various styles following complex instructions. Weconduct comprehensive captioning evaluations using fourmetrics: 1) CLIP-Score for image-text similarity; 2) CLIP-Image-Score for measuring the image-image similarity be-tween the original and the reconstructed image generatedby a text-to-image model using the caption. 3) human studyon Amazon Mechanical Turk; 4) GPT-4V for fine-grainedevaluation. Evaluation results show that VFC outperformsstate-of-the-art open-sourced captioning methods for 2Dimages on the COCO dataset and 3D assets on the Obja-verse dataset. Our study demonstrates that by combiningopen-source models into a pipeline, we can attain caption-ing capability comparable to proprietary models such asGPT-4V, despite being over 10 smaller in model size.",
  ". Introduction": "Image captioning is a pivotal challenge in computer visionand natural language processing. Its central goal is to en-capsulate visual data within a textual description, which re-quires a nuanced understanding of both modalities. Therecent advent of multimodal large language models (MM-LLMs), such as GPT-4V , and text-to-image genera-tion models, such as DALLE-3 , has marked significantprogress in this field. These proprietary models could lever-age expansive human-labeled data and enormous comput-ing resources to learn to generate detailed and contextuallyappropriate image descriptions. On the other hand, exist-ing open-sourced captioning methods in the community stillface significant challenges. Methods such as BLIP-2 and OFA often yield overly succinct captions that ne-glect essential visual information. Conversely, systems likeMini-GPT4 , InstructBLIP , and LLaVA cansuffer from hallucination, producing long descriptions thatdo not align with the actual content of the images.In light of this, we propose VisualFactChecker (VFC),a flexible training-free pipeline designed to produce accu-rate and comprehensive captions for both 2D images and3D objects. shows examples of captions generatedby VFC and their comparisons with captions generated byGPT-4V and Cap3D . Captions generated by VFCare faithful textural representations of the visual contents.This can also be verified by reconstructing images and 3dobjects from captions using state-of-the-art text-to-imageand text-to-3d models, as shown in .VFC focuses on tackling hallucinations and insufficientdetails in generated captions and is structured around threecore components: Proposer, serving as the systems eye,creating detailed caption proposals as preliminary captionsby using image-to-text captioning models; Large Lan-guage Model, acting as the brain, calling and summariz-ing information from other components, and leveraging itsadvanced generalization capabilities to steer the captioningprocess following specified captioning instructions; Detec-tor and VQA models, functioning as tools utilized bythe LLM for fact-checking caption proposals, ensuring thefidelity of the final generated caption. VFC is versatile andeffectively handles captioning for both 2D images and 3Dobjects through a unified pipeline. shows an overviewof the pipeline. The details of each component and their in-terplay are explained in Sec. 3.To comprehensively evaluate the generated captions,other than leveraging the commonly used CLIP-Score thatprimarily gauges the image-caption similarity, we proposea new metric: the CLIP-Image-Score. This metric assessesthe similarity between the input image and a reconstructedimage created by a text-to-image model from the caption,offering a complementary measure. Furthermore, we con-ducted a human study on Amazon Mechanical Turk for cap- tion evaluation. Finally, we also performed a fine-grainedevaluation by asking GPT-4V to compare and judge cap-tions with detailed reasoning. The combination of CLIP-Score, CLIP-Image-Score, GPT-4V, and human study pro-vides a more robust evaluation of captions.We summarize our main contributions as follows: 1) Wepropose VisualFactChecker (VFC), a training-free pipelineto generate high-fidelity detailed 2D and 3D captions, effec-tively mitigating the challenge of hallucination in long cap-tions. (2) CLIP-Image-Score: A novel caption evaluationmetric that measures the similarity between the input imageand a reconstructed image from the caption. (3) Our evalua-tion shows that VisualFactChecker achieves state-of-the-artresults in 2D and 3D captioning tasks compared with open-sourced models. (4) Our work shows that using an LLM tochain open-source models can achieve captioning capabilityon par with proprietary models such as GPT-4V.",
  ". Image Captioning": "Image captioning has made significant progress with the ad-vent of deep learning. Pioneering works primar-ily focus on integrating deep neural networks for enhancedimage understanding and language generation.Recent strides have been made with the introduction ofMultimodal-Large Language Models (MM-LLMs), whichare trained on extensive vision and language data. The gen-eral approach involves leveraging a pre-trained large lan-guage model (LLM) and a vision encoder with a projectorto align with the LLMs embeddings, thus enhancing vi-sual understanding. Several models have emerged as sig-nificant contributors in this domain.BLIP , BLIP-2 , OFA , Flamingo , Kosmos-2 , MiniGPT-4 , InstructBLIP , LLaVA have demon-strated impressive performance in single-view image cap-tioning tasks. However, they exhibit varying limitations.For instance, BLIP-2 and OFA often generate overly con-cise captions, while others, like InstructBLIP, can producedetailed captions that often include inaccurate or hallucina-tory content. Our method aims to address these limitationsby combining different models into a pipeline via an LLM,striking a better balance between accuracy and detailednessin generated captions while mitigating hallucinations.",
  "(Ours)Cap3D": ". We use DALLE-3 as a text-to-image model to reconstruct 2D images using generated captions from different captioningmethods (BLIP-2, LLaVA-1.5 and ours). Similarly, we use MVDream as a text-to-3D model to reconstruct 3D objects using different3D captions (generated by Cap3D and ours). From the results, we can see that the reconstructed images or 3D objects using BLIP-2or Cap3D captions are less similar than the input ones, suggesting their captions may not contain sufficient information or incorrectlydescribe the visual contents; the reconstructed images using LLaVA-1.5 captions contain objects or scenes that are not present in theoriginal images (top: people in the background, bottom: pedestrians and cars on the street), suggesting there might be hallucinations inLLaVA-1.5 captions. Images or 3D objects reconstructed using our captions are more similar to the inputs. Vision-blind LLMs are prominent in multimodal appli-cations, often utilizing language-only prefixes generated bypre-trained tools.Clipcap demonstrates this by us-ing a continuous embedding as a prompt for a GPT-stylelanguage model, achieving notable performance in single-viewpoint image captioning.Similarly, Promptcap and PNP-VQA leverage natural language prompts withGPT models to excel in visual question answering.Recent methods have employed LLMs to generate im-age captions by summarizing initial captions or keywordsfrom Vision-Language models. For instance, Socratic mod-els use a CLIP-based model to extract key tags fromimages, followed by GPT-3 with specialized prompts tocreate stylized captions. ChatCaptioner builds uponthis by integrating ChatGPT and BLIP-2 in a conver-sational approach for question-answering about the image,and summarizing them into a caption. Visual Clues uses similar tags to generate a paragraph-caption. IC3 and LLM-Fusion use LLMs to summarize captions fromexisting models augmented with temperature-based sam-pling. Cap3D extends this concept to 3D object.Our method differentiates itself in two critical ways:First, we focus on reducing hallucinations in captions byemploying visual grounding tools, such as object detection,to fact-check captions for enhanced accuracy. Second, ourpipeline can be used for captioning both 2D images and3D objects. Unlike previous methods that rely on a singlecaptioning model, we integrate multiple captioning sourcesfrom different models, ensuring a more comprehensive cov-erage of visual content to generate captions.",
  ". Hallucination in MM-LLM": "There are two popular topics on the hallucination of MM-LLMs. (1) Hallucination evaluation: Detection approachessuch as Gunjal et al. train classification models to iden-tify hallucination. They focus on distinguishing betweenaccurate and hallucinated content. Ground truth comparisonmethods compare model outputs with ground truthdata to detect hallucinations. These techniques assess thealignment of generated captions with actual image content.(2) Mitigation Strategies : Data optimization methodssuch as Liu et al. address hallucination by creating neg-ative instances in training datasets to reduce model over-confidence. Iterative generation methods such as Wang etal. adopt an iterative process for caption generation,where brief answers are generated in succession and amal-gamated, aiming to improve accuracy and relevance.Our VisualFactChecker is a training-free pipeline mit-igating hallucination in image captioning.Our methodutilizes visual grounding tools for improved accuracy,thereby actively reducing the hallucination and offeringhigh-fidelity captions for both 2D images and 3D objects.",
  "Multi-view Summary": ". Pipeline of the VisualFactChecker for captioning 2D images (top) and 3D objects (bottom). The process begins with the inputbeing captioned by two multimodal captioning models (Captioner-1 and Captioner-2) to generate preliminary captions. These captions arethen verified using a Large Language Model (LLM) to call object detection (Detector) and VQA models for fact-checking the captions.Finally, the LLM incorporates all the results and summarizes the final caption by following instructions.",
  ". 2D Image Captioning": "The caption generation takes three steps: 1) proposal, 2)verification, and 3) captioning. Each step is detailed below.Proposal: The Proposal step serves as the cornerstone ofthe captioning process that generates initial captions. This isachieved through the utilization of advanced image-to-textmodels, specifically LLaVA and Kosmos2. These mod-els are trained on expansive datasets, enabling them to com-prehend and interpret visual content effectively. By analyz-ing the input image, they suggest various preliminary cap-tions, each reflecting different facets and interpretations ofthe image ( top). The rationale behind using multipleimage-to-text multimodal LLMs lies in the complexity ofadequately capturing an images essence in a single attempt.Since an image can be accurately described in numerousways, different models bring unique perspectives, therebyencompassing a broader range of information present in theimage. Although the initial captions proposed may not pos-sess perfect fidelity, the primary objective at this stage isto generate captions that are as comprehensive as possible. displays the specific prompts we used for each step,with more details in Appendix A. Verification and Captioning: The goal of the verificationstep is to scrutinize and rectify any inaccuracies or hallu-cinations in the captions during the proposal step. It em-ploys a combination of a Large Language Model (LLM)and grounding tools, including an open-vocabulary objectdetection model and/or a visual question answering (VQA)model. Here the LLM can be GPT-4 or Llama2. As shownin (top), the process involves the following steps:Step 1: LLM first summarizes the initial detailed descrip-tions from different MM-LLMs into a single, detailed cap-tion. While this caption is comprehensive, it may not al-ways be accurate. Step 2: The LLM then analyzes this syn-thesized caption, identifying all objects that could be veri-fied by object detection and summarizing an object check-list. In 2D image captioning, the focus is on eliminatinghallucinations, particularly descriptions of non-existent ob-jects in the image. Identifying these objects is crucial forthe subsequent verification process. Step 3: Taking the ob-ject checklist as input, an open-vocabulary object detectionmodel examines candidate objects in the checklist and de-termines their presence in the image. This step is pivotal invalidating the existence of objects mentioned in the caption,thus supporting the fidelity of the caption.",
  "After verification, we go to the last captioning step:Based on the object detection results, the LLM revises thesummarized single detailed caption. Each object described": "in the caption is cross-checked; if detected, it remains un-changed, while undetected objects are considered potentialhallucinations and are removed from the caption. This stepresults in a final caption that is both detailed and reliable.The underlying assumption is that the detection model,serving as an object grounding expert, provides more re-liable results than a general image descriptor.In the verification and captioning steps, the LLM plays apivotal role as a brain. It starts by parsing the initial cap-tion and identifying key objects for detailed examination.The LLM then meticulously assesses whether each objectmentioned actually appears in the image based on detec-tion results. Following this thorough analysis, it refines andrevises the initial captions, transforming them into final ver-sions that are both coherent and richly detailed. The LLMis instrumental in guaranteeing linguistic fluency, ensuringthat the captions not only accurately represent the image butalso maintain the necessary level of detail for high-fidelitycaptioning. Moreover, the LLM can follow complex in-structions to write the captions in a specified style, such asa caption that only mentions the foreground objects with-out mentioning the background. displays the specificprompts used for each step.",
  ". 3D Object Captioning": "The 3D object captioning process follows a similar struc-tural pipeline to that of 2D images, with a few key distinc-tions in certain steps, as depicted in (bottom). In 3Dcaptioning, an object may present multiple views, each of-fering unique information. The comprehensive caption for a3D object is derived by integrating the perspectives from allthese views. For each view, VisualFactChecker is employedto create a detailed, high-fidelity description. Subsequently,the LLM (GPT-4 or Llama-2) is used to amalgamate the in-formation from all views, producing a unified caption forthe 3D object. In particular, for each views captioning,we have the same three-step approach akin to 2D imagecaptioning. In the proposal step, LLaVA-1.5 and Instruct-BLIP are utilized for generating initial detailed descriptions.We opt out of using Kosmos2 for single 3D objects due toits less effective performance in providing detailed descrip-tions, possibly linked to its reliance on an implicit detectionmodel. Additionally, a slightly modified prompt is used (see bottom), which incorporates 3D-specific considera-tions. In the verification and captioning step, we primar-ily address hallucinations related to the attributes of 3D ob-jects, such as shape and color. To mitigate these inaccura-cies, rather than enumerating potential objects, we employthe LLM to generate five critical questions that could influ-ence a text-to-3D generation model in reconstructing the 3Dmodel. Following this, we utilize VQA models (specificallyLLaVA-1.5) to respond to these questions based on the in-put 3D object view image. Subsequently, the LLM amends",
  "! !!": ". The CLIP-Image-Score pipeline evaluates caption accu-racy by encoding an original image X into a feature representationIX using a CLIP image encoder. A captioning model generates acaption that is then input into a text-to-image model to reconstructan image X, which is encoded to IX. The score is computed byassessing the cosine similarity between IX and IX, providing ameasure of the captions fidelity and hallucination detection. the initial caption in accordance with the answers providedby the VQA model. We operate under the assumption thatanswering targeted questions results in fewer hallucinationscompared to generating a general description. Once the cap-tion for each individual view is complete, the LLM synthe-sizes these multiple perspectives into a singular, compre-hensive caption for the entire 3D object. The prompts usedfor the LLM at each stage are detailed in Appendix A.",
  ". CLIP-Image-Score": "Accurate evaluation of caption correctness and detailed-ness is paramount in determining the performance of animage captioning model.Traditional metrics like theCLIP-Score have served as a standard for measuringthe alignment between generated captions and their corre-sponding images. However, our CLIP-score may lack thesensitivity needed to detect the specific issue of hallucina-tion within captions.We present the CLIP-Image-Score, an alternative met-ric specifically developed to reflect the subtleties of cap-tion quality. This metric is different from CLIP-Score byintroducing an additional reconstruction step. Specifically,the CLIP-Image-score evaluates the similarity between theoriginal image and a reconstructed version of the imagegenerated by a fixed text-to-image model using the captionas a prompt. By comparing the raw image to its recon-structed image, the metric is able to detect discrepancies in-dicative of hallucination, thus providing a different perspec-tive of the caption quality assessment. The underlying prin-ciple of the CLIP-Image-Score is the recognition that mul-tiple correct captions may exist for a single image. How-ever, its only when a caption is both detail and correctthat the reconstructed image closely resembles the original.Moreover, any hallucinations present in the caption becomeevident in the reconstructed image. presents exam-ples of such reconstructions. For instance, consider the re-sults from LLaVA-1.5 shown in the third column. The cap- tion generated for the first image falsely mentions severalother people in the background. This error is clearly re-flected in the image reconstructed by the text-to-image gen-erator. In essence, comparing the two images indirectly en-sures alignment between the image and its caption, therebyproviding a complementary method to assess the quality ofthe caption than directly comparing the image and caption.The CLIP-Image-Score evaluation process is depicted inthe following steps: Caption Generation: An original image X is input intoa captioning model, which generates a caption.",
  "IX IX(1)": "Most notably, CLIP-Image-Score offers a sensitive mea-sure for detecting hallucinations. In scenarios where thegenerated caption includes elements that are not in the orig-inal image, the reconstructed image will also likely con-tain these discrepancies. By comparing the original and re-constructed images, the CLIP-Image-Score can effectivelyhighlight these differences, offering a clearer insight intothe fidelity and accuracy of the generated caption.Furthermore, CLIP-Image-Score turns a cross-modalitycomparison into a more intuitive comparison in the sameimage modality (as shown in ). CLIP-Image-Scorerepresents a new complementary perspective for image cap-tioning evaluation. By leveraging the capabilities of text-to-image models and focusing on the congruence betweenthe original and reconstructed images, it provides an ac-curate assessment of caption quality, particularly in identi-fying and measuring hallucinations, thereby enhancing theoverall reliability of caption generation systems.",
  ". Overall: CLIP-Score and CLIP-Image-Score": "2D image captioning. Dataset: Our evaluation utilized5,000 COCO test images from the Karpathy split. Baselinemethods: We benchmarked against state-of-the-art caption-ing models, including BLIP-2 , InstructBLIP , andLLaVA-1.5 . The evaluation focused on each modelsability to produce accurate, detailed, and coherent captionsthat effectively encapsulate the essence of the images. Eval-uation Metric: We employed two metrics: CLIP-Score and CLIP-Image-Score (Sec. 4). The CLIP-Score, aprevalent metric in image caption quality assessment, in-volves processing the raw image through the CLIP imageencoder and the caption through the CLIP text encoder. Theresultant embeddings are then compared for cosine similar-ity, with a higher score indicating greater semantic resem-blance between the image and the caption. For our analysis,we first calculated the CLIP-Score for each image-captionpair, then averaged these scores across all 50,000 text/imagepairs, scaling the result by a factor of 100. displaysthe comparative performance of various image captioningmethods on the 5,000 COCO test set images. The resultsdemonstrate that our VisualFactChecker surpasses all base-line methods in performance.",
  ". Image captioning comparison with different metrics on5000 COCO test set in Karpathy split, we use raw image and cap-tion as input pairs for evaluation": "As outlined in Sec. 4, the CLIP-Image-Score providesa complementary view to assess the quality of image cap-tions. This metric is derived by comparing the cosine sim-ilarity between the CLIP embeddings of two images: theoriginal image and a reconstructed image, which is gen-erated using the provided caption through a text-to-imagegeneration model. A higher CLIP-Image-Score signifies amore accurate and effective image caption. For this process,Stable Diffusion XL (SDXL) is utilized as the desig-nated text-to-image model to reconstruct images based on",
  "VisualFactChecker (Ours)34.0180.32": ". 3D object captioning comparison with different metricson 1000 objects in Objaverse. For CLIP-Score, we use the aver-age score of two views for evaluation. For CLIP-Image-Score, weuse an off-the-shelf text-to-3D model, MVDream, to generate 3Dmodels from 3D captions. We compare two views of the raw ob-ject and the same views of generated 3D object for evaluation. the generated captions. presents the CLIP-Image-Scores obtained for the 5000 images in the COCO test set,where our method outperforms all baseline methods.3D object captioning. Dataset: 1,000 3D objects sam-pled from Objaverse dataset . Baseline methods: Weuse state-of-the-art 3D object captioning model Cap3D as the baseline.Cap3D uses 8 view images to generatethe final object caption, our VisualFactChecker uses only2 views to generate the object caption. Evaluation Met-ric: CLIP-Score and CLIP-Image-Score on multiple viewsrendered from 3D objects. To evaluate the similarity of a3D object and the generated caption, we evaluate the sim-ilarity of the caption with the multi-view images used togenerate the caption. Specifically, we evaluate the similar-ity of the generated caption with the two views that wereused to generate the caption and use the average score torepresent the CLIP-Score. Table. 3 shows the performanceof 3D object captioning methods on 1,000 3D objects fromObjaverse dataset. VisualFactChecker outperforms Cap3D.We also use CLIP-Image-Score to evaluate the 3D cap-tion quality. CLIP-Image-Score needs reconstructed imagesto compare with the raw images. We treat the two views thatwere used to generate the 3D object caption as the raw im-age. To obtain the reconstructed image, we use an off-the-shelf text-to-3D generation model, MVDream, to generatea 3D object given the generated 3D object caption. We thenrender the same two views of images based on the generated3D object, and we calculate the CLIP-Image-Score betweenthe raw image and the rendered image. Table. 3 shows theCLIP-Image-Score on 1000 objects in Objaverse dataset.",
  ". Per Image Evaluation: Wining Rate": "CLIP-Score and CLIP-Image-Score indicate an overallperformance comparison, which shows an average scoreamong all 5000 images. The average score may be dom-inated by a small group of images that have extremely highor low scores. To zoom in and show a more detailed com-parison, we try to answer the following question: Given animage, what is the probability that one method performsbetter than another method on caption generation? To an-swer this question, we need to go over each image and cal-culate the winning rate for a pair of methods.Specifically, for each image, we compare the CLIP-",
  ". 2D image captioning comparison with pair-wise winningrate. VisualFactChecker (VFC) outperforms all baseline methodson both CLIP-Score (top) and CLIP-Image-Score (bottom)": "Score of our VisualFactChecker caption against the captionsgenerated from different baselines respectively, and calcu-late the wining probability of our method and the baselines. shows the results, for example, we can see that in thepair-wise comparison, our VisualFactChecker performs bet-ter (higher CLIP-Score) than LLaVA-1.5 on 64.4% of 5000images (3220 images).Calculating the winning rate over all images provides amore detailed analysis that zooms in on the comparison ofeach image, which shows a complementary view than over-all average CLIP-Score.",
  ". Fine-grained Evaluation: Human and GPT-4V": "The CLIP-Score and CLIP-Image-Score offer a generalcomparison of overall performance. A pairwise per-imagewinning rate provides a more specific analysis, evaluatingperformance on individual images. However, the researchhighlighted in related studies indicates that the CLIP-Score may not be ideally suited for image-to-image com-parison tasks. Furthermore, relying on a single score failsto provide a nuanced comparison across criteria, such as ac-curacy and level of detail. We use Human evaluation andGPT-4V to provide a more fine-grained evaluation.Human evaluation using Amazon Mechanical Turk(AMT). We employed a pairwise comparison strategy.From the COCO dataset, we randomly selected 100 imagesout of 5000. For each image, our caption was comparedagainst 5 baseline captions respectively. To reduce variance,each comparison was done by 3 different AMT workers andwe used their majority voting as the final selection. This re-sulted in a total of 1500 comparisons collected on AMT.AMT UI is shown in the appendix. The workers were pre-sented with two competing captions one from a baselinemethod and one from our VisualFactChecker, in random-ized order. They were instructed to select the better captiondescribing the image based on 3 aspects: correctness, de-",
  ". Amazon Mechanical Turk human evaluation results": "GPT-4V evaluation. Our study applied GPT-4V for eval-uating captions in a manner akin to the caption evaluationprocess used in DALLE-3. We use the same randomly se-lected 100 images from COCO as in Human evaluation.For each image, we considered the captions generated by 5baseline methods alongside the caption produced by our Vi-sualFactChecker. We then presented GPT-4V with the rawimage, our reference caption, and the four baseline captions.Our designed prompt instructed GPT-4V to compare eachbaseline caption against our reference caption, focusing ontwo primary aspects: correctness and detail. GPT-4V wastasked with providing a pairwise, detailed comparison foreach pair, including justifications for its assessments. Basedon these comparative insights, GPT-4V classified each base-line method caption as either better or worse than ourVisualFactChecker. shows the comprehensive results.More details about the GPT-4V evaluation prompt and ex-amples are shown in Appendix B.",
  ". Ablation Study": "In our ablation study, we explore the impact of various com-ponents on performance. For 2D captioning tasks, we as-sess the efficacy of initial captioning models, LLaVA-1.5and Kosmos-2, using the CLIP-Score metric for the cap-tions they generate on the same 5000 COCO test images.Additionally, we ablate our methods performance in theabsence of the verification (fact checker) step, which aimsto mitigate hallucinations through detection grounding. Ta-ble 4 shows the detailed results. Likewise, in the context of3D object captioning, we evaluate the individual contribu-",
  ". Ablation study on captioning 2D images (5000 COCOtest dataset) and 3D objects (1000 Objaverse)": "tions of initial captioners, namely LLaVA-1.5 and Instruct-BLIP on the same 1000 Objaverse 3D objects. We furtherinvestigate the performance of our methodology without thefact checker, which in this case operates by leveraging aVQA model to reduce hallucinations. shows thedetailed results. These results highlight the significance offact checker in our approach.",
  ". Conclusion": "We propose the VisualFactChecker (VFC), a training-freepipeline to generate high-fidelity and detailed captions. Byutilizing an LLM to chain multimodal models and objectdetection and VQA models, VFC reduces hallucination inlong captions. We conducted a comprehensive caption eval-uation using different metrics, including 1) image-text simi-larity using CLIP-Score, 2) image-reconstructed image sim-ilarity using our proposed CLIP-Image-Score, 3) humanstudy, and 4) fine-grained evaluation using GPT-4V. Com-pared with open-sourced captioning models, our methodachieves state-of-the-art in both 2D and 3D captioning. Ourwork shows combining open-sourced models into a pipelinecan significantly close the captioning performance gap withproprietary models like GPT-4V. In the future, we plan toimprove our pipeline further by including more componentsfor fact-checking and making it more automatic in decidingwhich components to use.Acknowledgments We would like to thank Siddharth Gu-rurani for helping with our human evaluation using Ama-zon Mechanical Turk; Haochen Wang for his help in pre-processing 3D data. We also thank Qinsheng Zhang, Yo-gesh Balaji, and Yen-Chen Lin for their helpful discussion. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. In NeurIPS,2022. 2 Peter Anderson, Xiaodong He, Chris Buehler, DamienTeney, Mark Johnson, Stephen Gould, and Lei Zhang.Bottom-up and top-down attention for image captioning andvisual question answering. In CVPR, 2018. 2 James Betker, Gabriel Goh, Li Jing, Tim Brooks, JianfengWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,and Yunxin Jiao. Improving image generation with bettercaptions. Technical report, OpenAI, 2023. 2, 3, 11",
  "DavidMChan,AustinMyers,SudheendraVijaya-narasimhan, David A Ross, and John Canny.Ic3: Im-age captioning by committee consensus.arXiv preprintarXiv:2302.01328, 2023. 3": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, SebastianGehrmann, et al.Palm: Scaling language modeling withpathways. arXiv preprint arXiv:2204.02311, 2022. 2 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, PascaleFung, and Steven Hoi.Instructblip:Towards general-purpose vision-language models with instruction tuning.arXiv preprint arXiv:2305.06500, 2023. 2, 6 Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,Oscar Michel, Eli VanderBilt, Ludwig Schmidt, KianaEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:A universe of annotated 3d objects. In CVPR, 2023. 7 Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,and Trevor Darrell. Long-term recurrent convolutional net-works for visual recognition and description. In CVPR, 2015.2",
  "Ron Mokady, Amir Hertz, and Amit H Bermano.Clip-cap:Clip prefix for image captioning.arXiv preprintarXiv:2111.09734, 2021. 3": "OpenAI. Gpt-4 technical report, 2023. 2 Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, ShaohanHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-ing multimodal large language models to the world. arXivpreprint arXiv:2306.14824, 2023. 2 DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann, Tim Dockhorn, Jonas Muller, Joe Penna, andRobin Rombach.Sdxl: Improving latent diffusion mod-els for high-resolution image synthesis.arXiv preprintarXiv:2307.01952, 2023. 6",
  "Jin, Taylor Bos, Leslie Baker, Yu Du, et al.Lamda:Language models for dialog applications.arXiv preprintarXiv:2201.08239, 2022. 2": "Anthony Meng Huat Tiong, Junnan Li, Boyang Li, SilvioSavarese, and Steven CH Hoi. Plug-and-play vqa: Zero-shotvqa by conjoining large pretrained models with zero training.arXiv preprint arXiv:2210.08773, 2022. 3 Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothee Lacroix, BaptisteRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.Llama:Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. 2 Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong,Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang,et al.Vigc: Visual instruction generation and correction.arXiv preprint arXiv:2308.12714, 2023. 3 Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi,Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, JiZhang, Jihua Zhu, et al.Evaluation and analysis of hal-lucination in large vision-language models. arXiv preprintarXiv:2308.15126, 2023. 3 Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, andHongxia Yang.Ofa: Unifying architectures, tasks, andmodalities through a simple sequence-to-sequence learningframework. In ICML, 2022. 2",
  "Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, NguyenBach, Ce Liu, and Michael Zeng. Visual clues: Bridgingvision and language foundations for image paragraph cap-tioning. In NeurIPS, 2022. 3": "Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-manski, Adrian Wong, Stefan Welker, Federico Tombari,Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. So-cratic models: Composing zero-shot multimodal reasoningwith language. arXiv preprint arXiv:2204.00598, 2022. 3 Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen,Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks,blip-2 answers: Automatic questioning towards enriched vi-sual descriptions. arXiv preprint arXiv:2303.06594, 2023.3",
  "Proposal (LLaVA-1.5 / Kosmos-2):Describe this image in detail": "Verification step-1 (GPT-4 / Llama-2):This is a hard problem. Carefully summarize in ONE detailed captionbased on the following two captions by different (possibly incorrect)people describing the same scene. Be sure to describe everything, andavoid hallucination. Verification step-2 (GPT-4 / Llama-2):I want to use an object detector to check the correctness of an imagecaption obtained by an image caption model. Can you help to parse thecaption below and list all objects that could be detected with an objectdetection model in the image? Please only list the object name andignore the description. Please use singular for all listed objects.Caption: {}.Please concatenate them together with . as separation.",
  "I will put the detection results to you in the following format:[object: detected object name, number: number of detected object(N)]. Please follow the following steps:": "Instructions:Parse the object in the caption, (Note: only parse and modify the object(not color, action, size, shape, or other descriptions))1. If the object was detected by the detection model, keep everythingincluding all descriptions. For instance, if the original caption is: ablack and white panda toy, if the toy was detected, keep all contenteven though the panda and black and white are not detected. Keepall descriptions about color, shape, actions .etc.2. If the subject object was not detected, remove only the object. DoNOT remove color, shape, action, text and other descriptions.3. Only decrease the object number if the detected object number issmaller than the caption number. This is a hard problem.Please minimize modifications of thecaption, and list all changes made along with the reasoning.BEGIN Detection results: {}END Detection resultsBEGIN Raw caption: {}END Raw caption",
  "Please give the output in the following format:Modification:Updated caption:": "We find Llama-2 may encounter difficulties in the last2D captioning step due to the complexity of the prompt.A workaround is to use a script to compare the detectionresults with the object list from step-2 and identify objectsto be removed. Then, employ Llama-2 solely for removingthese objects and summarizing the description.",
  "Proposal (InstructBLIP):Describe the 3D object in detail, step by step": "Verification step-1 (GPT-4 / Llama-2):This is a hard problem. Carefully summarize in ONE detailed captionbased on the following two captions by different (possibly incorrect)people describing the same 3D object. The detailed caption will beused for a text to 3D model to generate this 3D object. Be sure todescribe everything, and avoid hallucination. Verification step-2 (GPT-4 / Llama-2):I have a description of a 3D object, the detailed caption will beused for a text to 3d model to generate the same 3D object. Somepart of the description may have some hallucination, so I want touse a VQA model to double check some key description, Pleaseask at most 5 most important and concrete questions that I needto double check to improve the fidelity of the description.Pleasefocus on the factors that influence the final text to 3D model generation.",
  "Verification step-3 (LLaVA-1.5):N/A (LLaVA-1.5 takes questions above and raw view image as inputand give answers)": "Single view Captioning (GPT-4 / Llama-2):I have a description of a 3D object, the detailed caption will be usedfor a text to 3d model to generate the same 3D object. Some part ofthe description may have some hallucination, so I use a VQA model todouble check some key description.Here is the original description that may contain hallucination: {}Here are the questions and answers from a VQA model: {}",
  "GPT-4V EVALUATION PROMPT (2D IMAGES)": "You are responsible for judging the quality of image captions generatedby computer programs.You will be presented with an image andcaptions describing the image.One of the captions is a referencecaption and you are responsible for comparing other captions againstthe reference caption, one by one.Please evaluate captions in thefollowing two aspects:1.Correctness.A good caption should correctly describe imagecontent, including the major objects and the surrounding scene.2.Detailedness.A good caption should contain rich details aboutthe image content, for example, spatial location of objects, clothingand actions of people, color, counting of objects, texts in the image, etc.",
  "Given the image and a reference caption, you need to make de-cisions as to whether or not the given captions are better or worse thanthe reference caption in describing the overall image": "This is a difficult task.You should first evaluate the referencecaption for the given image. And then evaluate each caption in thesame way and compare against the reference caption. You should thinkout loud about your eventual conclusion for each caption, enumeratingreasons why each of the captions is better or worse compared with thereference caption. After thinking out loud, you should output eitherBetter or Worse for each caption depending on whether you thinkthe caption is a better description of the image than the referencecaption.Reference Caption: {}Caption 1: {}Caption 2: {}Caption 3: {}Caption 4: {}",
  "GPT-4V EVALUATION RESPONSE (2D IMAGES)": "To evaluate each caption, I will consider the aspects of correctness and detailed-ness against the reference caption.Reference Caption: The reference caption accurately describes that a person issnowboarding and performing a jump while kite flying, mentioning the colors ofthe outfit and the snowboard. It also provides context for the backdrop and givesa sense of the snowboarders abilities.Caption 1: This caption introduces the term kite surfing, which could be mis-leading because that term is typically associated with water rather than snow. Itlacks the detail of the outfit and the specifics of the snowboarding action.Caption 2: This caption provides a summarization close to the reference but omitsthe colors of the outfit and the snowboard. It does, however, accurately affirm theskill and control of the snowboarder and the backdrop of the scene.Caption 3: This caption mentions objects that are not present in the image, suchas other people in the scene. It retains the mention of the green outfit and em-phasizes the height of the jump and the kites position but suggests parasailing orkiteboarding, which might not be accurate terms for this activity.Caption 4: The description of the snowboarders action and outfit matches theimage well, and it adds the color of the sky. It also introduces additional contextby mentioning other spectators in the scene, but it does not mention the kite flyingaspect, which is essential for the activity depicted.Judgment:Caption 1: WorseCaption 2: WorseCaption 3: WorseCaption 4: Worse",
  "GPT-4V EVALUATION PROMPT (3D OBJECTS)": "You are responsible for judging the quality of captions generated bycomputer programs designed for describing 3d models. You will bepresented with two rendered images of the same 3d model and twocaptions describing the 3d model.Please evaluate captions in thefollowing aspects: 1.Correctness.A good caption should correctly describe the3d object, including what the object is and the major properties of theobject.2. Detailedness. A good caption should contain rich details about the3d object, for example, part, color, material, function, etc.",
  "Given rendered images of the 3d model and two captions, youneed to make decisions on which caption is better in describing theoverall 3d object": "This is a difficult task.You should first evaluate two captionsbased on rendered views of the 3d object.You should think outloud about your eventual conclusion for each caption, enumeratingreasons why each of the captions is better or worse compared withthe other. After thinking out loud, you should output either Caption1 or Caption 2 depending on which caption you think is a betterdescription of the 3d model.",
  "Caption 1 is very basic, only stating that the object is a 3D model of aracing car. This is correct but lacks detail": "Caption 2, on the other hand, gives a much more comprehensive descrip-tion. It addresses the color scheme (white and red sports car, with a black andred stripe), the decorative elements (checkered pattern on the hood), specificparts of the car (spoiler on the back, curved roof, black and sporty tires),and even the background which helps to highlight the object (positioned on aneutral background). This caption goes beyond simply identifying the object asa racing car; it describes its aesthetic and functional elements, which aligns withboth correctness and detailedness. While Caption 1 is correct, it is very brief and does not provide any de-tail. Caption 2 is also correct but provides a rich and detailed description thatwould give someone who cannot see the image a clear understanding of the 3Dmodels appearance and character.",
  "Which image caption is better?": "Choose the caption that better describes the image. A good caption should be correct, detailed, and well-written: 1. Correct: A good caption should correctly describe image content, including the major objects and the surrounding scene. 2. Detailed: A good caption contains rich details about the image content, for example, the spatial location of objects, clothing,actions of people, color, counting of objects, texts in the image, etc. 3. Well-written: A good caption should be clear, coherent, and grammatically correct. CaptionA: In the image, there is a wedding cake sitting on a dining table. The cakeis three-tiered and decorated with a plaid pattern. A man and a woman can be seen sitting at the table, possibly enjoying the wedding celebration. There are severalwine glasses placed around the table, indicating that wine might be served duringthe event. Additionally, there are two bottles of wine visible in the scene, one on the left side of the table and the other on the right side. CaptionB: The image prominently features a white wedding cake adorned with blue and green tartan ribbons, elegantly displayed on a dining table. The table istastefully set with various items including wine glasses, a cup, and a knife, suggesting a wedding reception setting. In the background, a couple, presumablythe bride and groom, are engaged in conversation. The bride is distinguished by atiara, while the groom is dressed in a suit. Additional elements like a dining room window in the background contribute to the overall ambiance of the scene. CaptionACaptionB",
  "DALLE-3 text-to-imageInput imageCaptions": "VisualFactChecker (Ours): The image depicts a young boy, donned in a yellow jacket, engrossedin reading a book while sitting on a bench in a park. The park, situated in a residential area,features a playground with various equipment such as a swing set and a slide in the background,suggesting a lively and engaging environment for children. Several cars parked nearby indicatethe proximity of the park to the residential area. The boy seems to be enjoying his time, possiblytaking a break from playing.",
  "BLIP-2: a street sign on a pole": "InstructBLIP: The image features a street pole with multiple street signs attached to it. One ofthe signs reads \\\"Woodward Ave\\\", while the other sign reads \\\"Woodward St\\\". The pole issituated in front of a tall building, which can be seen in the background. There are several treesscattered throughout the scene, including one on the left side of the image and another on theright side. In addition to the trees, there are several cars parked around the area, adding to theurban atmosphere. LLaVA-1.5: The image features a street corner with a wooden pole holding up two street signs.The signs are green and white, indicating the names of the intersecting streets. The pole islocated in front of a large building, which serves as a backdrop for the scene.\\n\\nThere areseveral people in the area, with one person standing close to the pole and the street signs, andtwo others further away. Additionally, there is a car parked on the street, slightly to the right ofthe pole.",
  "Kosmos-2: The image features a street sign on a pole, with a green street sign reading\\\"Madsworth\\\" and a black street sign post reading \\\"\\\"Madsworth.\\\"": "VisualFactChecker (Ours): The image showcases a large banana tree with a bunch of unripe,green bananas hanging from its top and a large purple flower growing on it, both creating avisually appealing display. The tree is surrounded by green leaves and is enclosed by a fence,providing a sense of protection. The tree is situated in a yard filled with various objects. Oneother banana tree is visible in the background.",
  "BLIP-2: a banana tree with a large purple flower": "InstructBLIP: The image depicts a lush green banana tree with several bananas hanging from itsbranches. One of the bananas is located closer to the top of the tree, while others are scatteredthroughout the branches. There is also a purple flower growing on the tree, adding a touch ofcolor to the scene. In the background, there is a power line visible, indicating that the bananatree is situated in an urban or suburban area. Overall, the image captures a vibrant and livelysetting, showcasing the natural beauty of the banana tree and its surroundings. LLaVA-1.5: The image features a large banana tree with a bunch of bananas hanging from itstop. The bananas are still green, indicating that they are unripe. The tree is surrounded by afence, providing a sense of enclosure to the scene. The bananas are spread out across the tree,with some hanging closer to the top and others further down. The tree appears to be healthyand thriving, providing a natural and lush backdrop to the image. Kosmos-2: The image features a large banana tree with a large purple flower growing on it. Theflower is situated at the top of the tree, and it is surrounded by green leaves. The tree is situatedin a yard, and there are several other banana trees visible in the background. The yard is filledwith various objects, including a car, a truck, and a bicycle. The car is located near the left sideof the image, while the truck is located towards the right side. The bicycle is located furtherback in the scene, closer to the right edge of the yard. VisualFactChecker (Ours): The image depicts a young boy, donned in a hat and a green sweater,standing in front of a bus parked on the side of a road in a desert-like environment. The boyappears to be looking at the camera with an ambiguous expression that could be interpreted aseither a smile or a somewhat sad look. Behind the boy, another person, possibly a bus driver ora passenger, is standing, although their features are not clearly discernible. The bus seems toserve as a mode of transportation for the people in the area.",
  "BLIP-2: a young boy standing in front of a bus": "InstructBLIP: The image features a young boy standing in front of a bus, wearing a greensweatshirt and a baseball cap. There are several other people in the scene, including a manstanding to the left of the boy, another man standing to the right of the boy, and a third manstanding further away from the boy. In addition to the people, there are two backpacks visible inthe scene. One backpack is located closer to the boy, while the other is positioned further awayfrom him. LLaVA-1.5: The image features a young boy wearing a hat and a green sweater, standing in frontof a bus. He appears to be looking at the camera, possibly posing for a picture. Another person,possibly a man, is standing behind the boy, also looking at the camera. They seem to be the mainsubjects of the photo.\\n\\nIn the background, there is a bus visible, which could be the mode oftransportation for the people in the scene. The boy and the man are the only two individuals inthe image, with the boy being the main focus. Kosmos-2: The image features a young boy wearing a hat and a green sweater standing in frontof a bus. He appears to be looking at the camera, possibly smiling. In the background, there isanother person standing, possibly a bus driver or a passenger. The bus is parked on the side ofthe road, and the boy is standing close to it.",
  "(c)": ". Comparison of different 2D image captioning methods (1/3). The left column is the input image from COCO, the middle columnshows the caption generated by different methods (BLIP-2, InstructBLIP, Kosmos-2, LLaVA-1.5, and our proposed VisualFactChecker).In the right column, we use DALLE-3 as a text-to-image model to reconstruct 2D images using different generated captions.",
  "BLIP-2: a person is kite surfing in the snow": "InstructBLIP: The image depicts a person riding a snowboard in the air while being pulled by akite. The snowboarder is high up in the sky, likely enjoying the thrill of parasailing orkiteboarding. The kite can be seen in the background, flying high above the snowboarder'shead. The snowboarder is wearing a green outfit and appears to be fully immersed in theactivity. In addition to the snowboarder and the kite, there are several other people visible in thescene. They are scattered throughout the snow-covered area, some closer to the snowboarderand others further away. LLaVA-1.5: The image captures a snowboarder in mid-air, performing a jump while riding asnowboard. The snowboarder is wearing a green and black outfit, and the snowboard is visiblebeneath them. The scene takes place in a snowy landscape, with a blue sky in thebackground.\\n\\nThere are several other people in the scene, some of them closer to thesnowboarder and others further away. They appear to be watching the snowboarder'simpressive jump, likely enjoying the winter sports action. Kosmos-2: In the image, a person is skillfully riding a snowboard while flying a kite in the air. Thekite is positioned high in the sky, and the snow-covered field provides a perfect backdrop for theactivity. The snowboarder is skillfully navigating the snow, showcasing their skill and control. VisualFactChecker (Ours): The image showcases a wooden dining table adorned with a vase; ared vase holding a bouquet of red and white flowers on the right, and a clear vase filled with avariety of colorful flowers on the far left. The flowers in the clear vase are arranged to be visiblefrom multiple angles, enhancing the visual appeal. The table setting, surrounded by a chair and amirror, creates a pleasant and inviting atmosphere, suggesting a special occasion or gathering.",
  "BLIP-2: a table with a vase of flowers on it": "InstructBLIP: The image depicts a dining room with a wooden table in the center. On the table,there are two glass vases filled with various types of flowers, including pink and purple lilies,white daisies, and red carnations. The vases are placed on either side of the table, creating avisually appealing display of colorful blooms. In addition to the vases, there is a bowl placed onthe table as well. A chair can be seen in the corner of the room, providing seating for someonewho might want to enjoy the beautiful floral arrangement. LLaVA-1.5: The image features a dining table with two vases filled with beautiful flowers. Onevase is placed on the left side of the table, while the other is on the right side. The flowers in thevases are arranged in a way that creates a visually appealing display.\\n\\nIn addition to the vases,there are several books scattered around the table, with some placed near the right edge andothers closer to the left side. A chair can be seen in the background, positioned behind the table.The overall scene creates a pleasant and inviting atmosphere. Kosmos-2: The image features a wooden dining table with a vase containing various flowers, anda smaller vase on the left side. The table is adorned with two vases, one on the right side and theother on the far left. In addition to the vases, there are two bowls placed on the table, one closerto the left vase and the second one further to the right. The dining table is surrounded by a chairand a mirror, adding to the room's ambiance. VisualFactChecker (Ours): The image depicts an urban scene featuring a street sign labeled\\\"Robson Street\\\" located near the corner of the street and a tall building. The sign is positionednear a traffic light on its left side. Adding to the atmosphere is a pole from which a Canadian flagis hanging, infusing a sense of national pride into the area.",
  "BLIP-2: a street sign and a canadian flag on a pole": "InstructBLIP: The image depicts a street corner with a traffic light and a street sign for RobsonStreet, located in Vancouver, Canada. A Canadian flag can be seen hanging from the top of the tallbuilding in the background, adding a patriotic touch to the scene. There are several cars parked onthe side of the street, including a yellow car, a blue car, and a red car. In the foreground, there is apedestrian crossing sign, indicating that pedestrians are allowed to cross the street at this location.Overall, the image captures a bustling urban setting with a mix of vehicles and pedestrians. LLaVA-1.5: The image features a street sign on a pole with a Canadian flag hanging from it. Thestreet sign is located in front of a tall building, which appears to be a skyscraper. The scene is setagainst a blue sky, creating a pleasant atmosphere.\\n\\nIn addition to the Canadian flag, there areseveral other flags visible in the scene, including a few smaller flags hanging from the pole and alarger flag in the background. The presence of these flags suggests that the location might be aplace of international significance or a gathering spot for people from various countries. Kosmos-2: The image features a street sign with the name \\\"Robson Street\\\" and a Canadian flaghanging from a pole. A tall building is visible in the background, adding to the urban atmosphere.The street sign is located near the corner of the street and the tall building, with the Canadianflag hanging from the pole.",
  "BLIP-2: a boy sitting on a bench": "InstructBLIP: The image depicts a young boy sitting on a bench in a park. He is engrossed inreading a book, likely enjoying his leisure time in the outdoors. In the background, there is aplayground with a slide and other play equipment visible. The park appears to be well-maintained and inviting for children to play and have fun. LLaVA-1.5: The image features a young boy sitting on a park bench, reading a book. He ispositioned in the middle of the scene, with the bench extending across the lower part of theimage. The boy appears to be enjoying his time outdoors, engrossed in his reading.\\n\\nThe parksetting is evident by the presence of a swing set in the background, along with a few carsparked nearby. There are also a couple of other people in the scene, one standing close to theboy and another further away. Additionally, there is a backpack placed on the ground near thebench, possibly belonging to the boy or another park visitor. Kosmos-2: The image features a young boy sitting on a bench in a park, reading a book. He issurrounded by various playground equipment, including a slide and a swing. The park is locatedin a residential area, and there are several cars parked nearby. The boy appears to be enjoyinghis time in the park, possibly taking a break from playing or reading. VisualFactChecker (Ours): The image depicts a white donkey, standing on its three legs in agrassy field near a body of water, creating a serene atmosphere. A small brown and white dog ispositioned to the right of the donkey, both animals seemingly engaged in looking at each other orsomething in the distance.",
  "BLIP-2: a dog and a horse are standing next to each other": "InstructBLIP: The image depicts a donkey, a dog, and a horse standing on a grassy field next to abody of water. The donkey is the largest animal in the scene, while the dog and horse are smallerin size. The three animals seem to be interacting with each other, possibly playing or exploringtheir surroundings. The dog is positioned towards the left side of the image, while the horse iscloser to the right side. The donkey is situated in the middle of the grassy field, surrounded by theother two animals. LLaVA-1.5: The image features a dog and a horse standing next to each other in a grassy field.The dog is positioned to the right of the horse, and both animals appear to be looking atsomething in the distance. The dog is smaller in size compared to the horse, and they seem to beenjoying their time together in the field. Kosmos-2: The image features a white donkey and a small brown and white dog standing next toeach other on a grassy field. The donkey is standing on its hind legs, looking at the dog, while thedog is standing next and looking at the donkey. The field is located near a body of water, addingto the serene atmosphere of the scene. VisualFactChecker (Ours): The image features a man, dressed in a white shirt and shorts,wearing a hat and sunglasses, standing on a tennis court with a green fence visible behind him.He is holding a tennis racket in his right hand, swinging it and preparing to hit a ball, possiblytracking its movement. The man appears focused and ready, suggesting he is in the middle of atennis match.",
  "BLIP-2: a man holding a tennis racket and a tennis ball": "InstructBLIP: In the image, a man is standing on a tennis court, holding a tennis racket andpreparing to serve the ball. He is wearing a white shirt and shorts, and appears to be focused onthe game. There are several tennis balls scattered around the court, some closer to the player andothers further away. The background features a fence and trees, adding to the outdoor atmosphereof the scene. LLaVA-1.5: The image features a man standing on a tennis court, holding a tennis racket in hishand. He appears to be in the middle of a game, possibly preparing to serve the ball. The man iswearing a white shirt and white shorts, which are typical attire for playing tennis.\\n\\nThe tenniscourt is surrounded by a fence, and there is a green tarp visible in the background. The man'stennis racket is positioned towards the right side of the image, and he seems focused and readyfor the next play. Kosmos-2: The image features a man in a white shirt and hat, wearing sunglasses, swinging atennis racket and preparing to hit a ball. He is standing on a tennis court, and the green fence isvisible behind him. The man appears to be focused and ready to play a game of tennis.",
  "MVDream text-to-3d": "VisualFactChecker (Ours): The 3D object is a sleek and aerodynamic white and red sports car,with a black and red stripe on the side and a number \"4\" prominently displayed. The car has acheckered pattern on the hood, a spoiler on the back and a curved roof. The tires are black andsporty, complementing the car's overall design. The car is positioned on a neutral background,which accentuates its predominantly white color scheme with red accents and black stripes.",
  "GPT-4V analysisInput 3D objectCaptions": "VisualFactChecker (Ours): A 3D model of a sleek, futuristic motorcycle with a streamlined andaerodynamic design, featuring a predominantly black and red color scheme, positioned on awhite background for a clean, minimalistic appearance. The motorcycle is equipped with twolarge, red-accented wheels and a black seat, adding to its modern and stylish appearance.",
  "Cap3D: A 3D model of a futuristic motorcycle with orange wheels": "VisualFactChecker (Ours): The 3D object is a vintage-style, small wooden tripod with three legsproviding stability. Attached to the top of the tripod is a round, silver, metal lamp with a reflectivesurface, giving it the appearance of a mirror. The wooden legs of the tripod are brown, while thelamp/mirror has a silver appearance. This functional and decorative piece of furniture ordecoration, with its simple yet effective design, is set against a clean, white background.",
  "Cap3D: Wooden tripod floor lamp with a 3D model": "VisualFactChecker (Ours): The 3D object is a visually appealing dessert featuring a cake with apumpkin on top, and a bird perched next to the pumpkin. The cake, which is brown and white, isplaced on a blue plate. The pumpkin, made of chocolate, is orange and protrudes from the top ofthe cake. The bird is brown in color. The bird, pumpkin, and cake are all part of the same 3Dobject, creating a unique and delicious-looking dessert.",
  "Cap3D: A 3D-rendered small cake with a pumpkin on top, on a blue plate and napkin": "VisualFactChecker (Ours): A 3D model of a rust-covered, blue robot with a segmented body,appears to be part of a futuristic or science-fiction setting. The robot is standing upright witharms positioned to the side. The robot has a helmet on its head, which is also blue. It has a pairof intimidating horns on its body, resembling armor or protective gear. The overall appearanceof the robot is old and worn, yet unique.",
  "Cap3D: 3D model of a blue armored robot with horns": "VisualFactChecker (Ours): The 3D object is an antique, metal sewing machine with a uniquedesign, placed on a small, old-fashioned desk with a black top and a metal frame, supported byfour legs. The desk, which also serves as the table for the sewing machine, has a metal frameand a black metal top. The sewing machine, with its black metal body, is centrally positioned onthe desk, serving as the focal point. The sewing machine has a pedal on the side for operation.The entire setup is placed against a white background, highlighting the intricate details of thedesk and sewing machine.",
  "(f)": ". Comparison of different 3D object captioning methods. The left column shows the input 3D objects, the middle column showsthe captions generated from Cap3D and our VisualFactChecker. In the right column, we use MVDream as a text-to-3D model toreconstruct 3D objects using different 3D captions (generated by Cap3D and ours). From the results, we can see that the reconstructed3D objects using Cap3D captions are less similar than the input ones compared with ours, suggesting their captions may not containsufficient information to reconstruct the 3D object or incorrectly describe the visual contents. 3D objects reconstructed using our captionsare more similar to the inputs.",
  "Input imageCaptions": "GPT-4V: The image shows a family moment on the beach, with warmsunlight illuminating the scene. A man is kneeling on the sand,engaging with two young children, a boy and a girl. The man and thechildren are focused on playing with sand toys, including buckets andshovels. They appear to be in the process of building a sandcastle.The girl, wearing a polka-dot coat, is to the man's left, while the boy,wearing a patterned sweater and denim pants, is on his right. It's acandid, joyful moment, capturing the essence of family leisure time atthebeach.Thebackgroundfeaturesthesoftblurofahill,",
  "emphasizing the focus on the family": "VisualFactChecker (Ours): A father and his two children are spendingquality time together on a sandy beach, engaging in fun activities likebuilding sandcastles and playing with toys. The father is kneeling downon the sand, surrounded by his children, who seem to be enjoying theirtime together. There are several sandcastles and toys scattered aroundthe scene, indicating that the family has been having a great timeplaying and building together. The atmosphere is filled with joy andtogetherness, as the family members enjoy each other's company onthe beach.",
  "(b)": "GPT-4V: The image features a young girl with blonde hair styled inbraids, wearing glasses, and holding a chalkboard sign that reads\"BACK TO SCHOOL\" with a smiley face drawn below the text. She'ssmiling, looking towards the camera, and appears to be happy aboutthe return to school. The girl is wearing a striped sleeveless top and ispositioned against a vibrant yellow background, which creates acheerful and energetic atmosphere. The chalkboard and the girl'spose suggest a promotional or celebratory picture for the beginningof a new school year. VisualFactChecker (Ours): A cheerful young girl with glasses isexcitedly holding a chalkboard sign that reads \"Back to School\" in frontof a bright yellow wall. She is smiling and posing for the camera,showcasing her enthusiasm for the upcoming school year. The girl iswearing a striped shirt and a pair of blue jeans, and her glasses have asubtle glint from the light. The chalkboard sign is neatly written in whitechalk, with the words \"Back to School\" and a smiling face standing outclearly against the dark background. The overall atmosphere of thescene is one of excitement and anticipation for the new school year.",
  "GPT-4V: Theimagefeaturesaminiatureredpickuptrucktoy": "positioned in the foreground, with a focus on its details, highlightingthe glossy finish and classic design. In the background, slightly out offocus, is a stack of books and a small blue house model, both creatinga sense of depth and context for the scene. The background appearsto be outdoors, as indicated by the green hues that suggestvegetation. The overall setting seems to be a staged arrangement,perhaps used for educational purposes or as a decorative display.The shallow depth of field used in this photograph emphasizes the toytruck while softly blurring the background elements. VisualFactChecker (Ours): A red toy truck is sitting on a table in front of a blue toy house, giving the impression of a playful and creative setup. The truck appears to be positioned as if it is driving past the house, with its front bumper near the house's front door. To the left of the house, there is a stack of books, possibly suggesting a cozy reading nook or a place for a child to play and learn. Overall, the scene seems to be designed to evoke a sense of whimsy and imagination, perhaps for a child's play area or a collector's display. . Comparison of VisualFactChecker with GPT-4V. Our method can generate high-fidelity detailed captions that closely matchGPT-4Vs quality. To compare with GPT-4V, we use Llama-2 as our LLM instead of GPT-4 when generating captions for the above images.",
  "ending with an overall feeling": "A happy little girl is standing in a green field,wearing a plaid shirt and holding onto a string ofpink balloons. The balloons are floating in the air,creating a playful and joyful atmosphere. The girlappears to be enjoying the moment, with a smileon her face. In the background, there is a vastexpanse of green grass, stretching out as far asthe eye can see. The sky above is a bright blue,with a few white clouds scattered about. Theoverall mood of the scene is one of carefreehappiness and joy, with the pink balloons addingatouchofwhimsyandplayfulnesstothe"
}