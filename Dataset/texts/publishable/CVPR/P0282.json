{
  "Abstract": "Currently, low-light conditions present a significantchallenge for machine cognition. In this paper, rather thanoptimizing models by assuming that human and machinecognition are correlated, we use zero-reference low-lightenhancement to improve the performance of downstreamtask models. We propose to improve the zero-reference low-light enhancement method by leveraging the rich visual-linguistic CLIP prior without any need for paired or un-paired normal-light data, which is laborious and difficult tocollect. We propose a simple but effective strategy to learnprompts that help guide the enhancement method and exper-imentally show that the prompts learned without any needfor normal-light data improve image contrast, reduce over-enhancement, and reduce noise over-amplification. Next,we propose to reuse the CLIP model for semantic guid-ance via zero-shot open vocabulary classification to op-timize low-light enhancement for task-based performancerather than human visual perception. We conduct exten-sive experimental results showing that the proposed methodleads to consistent improvements across various datasetsregarding task-based performance and compare our methodagainst state-of-the-art methods, showing favorable resultsacross various low-light datasets.",
  ". Introduction": "Low-light conditions present a significant challenge forimage quality, adversely impacting the performance ofhigh-level computer vision models. In addition to high-levelnoise caused by photon-limited imaging, low-light imagesoften contain out-of-focus and motion blur, unnatural ap-pearance caused by camera flash and image signal processorfailures such as incorrect white-balancing or tone-mapping,all affecting the image quality negatively.With the in-creased integration of high-level computer vision methodsinto daily life, addressing the robustness of machine cogni-tion in low-light conditions becomes ever more important.",
  "Ours": ". Our proposed method leverages the CLIP modelfor unsupervised image prior via prompt learning and open-vocabulary semantic guidance. Our proposed method improvesthe over-all image hue, reduces over-enhancement and reducesnoise over-amplification. Further, we conduct extensive experi-ments to show that our proposed method significantly improvesmachine cognition as measured by task-based performance ofdown-stream tasks models, without incurring any additional com-putation costs on the light-weight enhancement baseline model. While most works try to improve machine cognition by cor-relating the human visual perception with the image quality,we study machine cognition-oriented low-light image en-hancement in this paper.Many works propose to su-pervise the training using paired low- and normal-lightdata. However, the collection of paired low- and normal-light data is laborious because it usually requires capturingthe same scene with different sensor exposure time. Be-cause of the collection costs of low- and normal-light data,some works propose to learn image en-hancement in an unsupervised fashion. While some meth-ods still require the selection of unpaired low-and normal-light datasets, zero-reference methods such as only require low-light data for training. Moti-",
  "Prompt LearningTraining": ". Our proposed two-stage training process leverages the pre-trained CLIP model that can capture lighting conditions and qualityof images. We propose to use the CLIP model to learn the positive and negative image priors with a simple data augmentation strategywithout any need for paired or unpaired normal-light data via prompt learning, and use them for guiding the image enhancement model.During the training, we use the learned prompts and reuse the CLIP model for semantic guidance to improve the quality of the enhancedimages. Our proposed method uses open-vocabulary classification, so it can be easily extended to any dataset, without limiting objectcategories, with annotated bounding boxes or any type of annotation that can be used to extract patches with an object category, as well asto paired low- and normal-light datasets, increasing the variety of the data in the training. vated by difficulties in collecting paired normal- and low-light data, we focus on zero-reference image enhancementin this work.While zero-reference methods solve the problems associated with collection costs, they donot integrate semantic knowledge during the training pro-cess.Motivated by the zero-shot capabilities of open vocabu-lary image understanding enabled by the CLIP model, we incorporate the rich CLIP prior that can cap-ture the lighting conditions and image quality intothe zero-reference training process.Our contributions are as follows: We propose to learn a low-light image prior via promptlearning, using a simple data augmentation strategy,without any need for paired or unpaired normal-lightdata. The proposed prompts help to guide the imageenhancement model by improving the contrast, reduc-ing loss of information caused by over-enhancementof bright regions, and reducing over-amplification ofnoise. We reuse the CLIP model for semantic guidance viaopen vocabulary image classification. While our strat-egy is simple, it scales favorably to include datasetswith any object categories. In addition to that, pairedlow- and normal-light datasets can also be included in",
  ". Low-Light Enhancement": "Traditional methods. Traditional approaches to low-light enhancement include histogram-based and Retinex-based methods. Histogram-based methods employ a map-ping of intensity values based on histograms at a global or local level.While simple and effi-cient, traditional histogram-based methods typically dis-count structural information, do not address the problem ofdenoising and are prone to producing unnatural artifacts in-cluding color shift. On the other hand, methods based onthe Retinex theory aim to decompose a low-light im-age into reflectance and illumination components and treatthe estimated reflectance or its modification as the final out-put. Seminal works single-scale and multi-scale Retinex propose to estimate image illumination with single-and multi-scale Gaussian filters.Over the years, more complex Retinex-based methods such as havebeen proposed, including joint enhancement and denoisingmethods , to produce more natural-looking results.Because traditional Retinex-based methods rely on hand-crafted features and assumptions about images, they requirecareful parameter tuning, might not generalize well to thevariety of real-life images, and often lack robustness againstimage degradation.Learning-based methods. In recent years, motivatedby impressive results demonstrated by deep learning meth-ods in computer vision, many deep learning low-light im-age enhancement methods have been proposed. Existingmethods can be generally divided into end-to-end meth-ods that directly enhance the input imageand Retinex-based methods that typically decompose the image into reflectance and il-lumination components.Many of these methods requirelarge amounts of paired low- and normal-light images totrain . While synthetic data is easyto generate, models trained on it have difficulties general-izing to real-life images. On the other hand, real data re-quires significant collection and annotation efforts becausethe paired data is collected either by post-processing im-ages by trained experts or varying camera and light-ing settings . Although it is possible to usea beam splitter to capture the same scene with two differ-ent sensors , the majority of the existing datasetsare collected by capturing the same scene multiple timeswith varying exposure settings, and are thus limited to staticscenes. Motivated by the costs and limitations of real paireddatasets, some works propose to use unpairedlow- and normal-light data instead. However, careful selec-tion of normal-light data is still necessary. Consequently,Guo et al. proposed a zero-reference method whichdoes not require any paired or unpaired normal-light data.In Guo et al. formulated image enhancementas a curve estimation problem and proposed a set of zero-reference losses based on assumptions about natural im-ages, such as average image brightness and gray world hy-pothesis.",
  ". Low-Light Image Understanding": "Loh and Chan have experimentally demonstratedthat convolutional neural networks trained on low- andnormal-light data for image recognition do not learn to nor-malize deep image features. That is, Loh and Chan showed that extracted low- and normal-light features belongto distinct feature clusters. Since then, low-light image un-derstanding has further received the interest of researchersin high-level tasks such as face detection , ob-ject detection , pose estimation and ac-tion recognition . Another line of works focuses onimproving machine cognition by introducing low-light en- hancement to the cognition framework and learning the en-hancement under supervision of a high-level task model. In contrast with previous works in im-age enhancement and restoration that use semantic informa-tion for guidance at a loss or feature level ,task-oriented enhancement models opti-mize for machine perception rather than assuming correla-tion between human perception and downstream task per-formances . Moreover, as an enhancedimage can be reused by many different generic downstream-task models, this approach might be attractive in resource-limited scenarios.",
  ". CLIP in Vision": "FollowingtheremarkablesuccessofContrastiveLanguage-Image Pre-Training (CLIP) ,the CLIPvisual-linguistic prior has been leveraged to generalize toopen-vocabulary problems in high-level vision . Recently, Wang et al. showed that CLIP can beused to capture the quality and abstract feel of the images,and Liang et al. showed that the CLIP prior can differ-entiate between various lighting conditions in natural im-ages. Motivated by this, Liang et al. proposed to lever-age the CLIP prior for image enhancement by iterativelylearning prompts to discriminate between normal-light andback-lit images. As we focus on low-light machine cognition, we makeminimal assumptions about correlating the human andmachine visual perception and employ task-based low-light performance to evaluate our proposed enhancementmethod.",
  ". Proposed Method": "We propose a two-stage training process that leveragesthe pre-trained CLIP model for semantic guidance andlearning an image prior from low-light data without any ad-ditional paired or unpaired normal-light data. During the pre-training stage, we propose a simple buteffective strategy to learn the pair of positive and negativeimage prompts to help to guide the enhancement. Next, weuse the learned prompts and additionally reuse the CLIPmodel for semantic guidance to improve the enhancementof low-light images without any need for paired or un-paired low-light data. Because our proposed semantic guid-ance is realized by open-vocabulary CLIP classification, theproposed method can be easily extended to any low-lightdataset containing bounding-box annotations or any type ofannotation that can be used to extract patches with an objectcategory.",
  ". Unsupervised Image Prior via Prompt Learn-ing": "Motivated by the observation that the CLIP image priorcan capture diverse lighting conditions and image qual-ity, we propose to leverage CLIP to learn an image priorfrom low-light data without the need for paired or unpairednormal-light data. During the first stage of training, we usethe pre-trained CLIP model to learn positive and negativeprompts to discriminate between averaged and subsampledlow-light data, respectively.Given a low-light image I RHW C and a pair of randomly initialized positive and negative prompts, Pp RN512 and Pn RN512, where N denotes the promptlength, we first augment the image using random photo-metric augmentation into I to avoid learning prompts thatwould overly constraint the overall image brightness. Next,we generate a pair of positive and negative images.We generate a positive image Ip = avgmm(I) by ap-plying mm average pooling to the augmented image I asa fast and simple proxy for denoising. We generate a nega-tive image In = sub1:m(I) by applying 1 : m subsamplingto the augmented image I, keeping the original noise lev-els. The process is illustrated in and the effect ofaveraging and subsampling is showed in .We use the binary cross-entropy loss to learn the promptpair to capture the image prior while differentiating betweenthe image quality.",
  "Zero-Reference Image Losses": "We follow Guo et al. and adopt a set of zero-referenceloss functions for low-light image enhancement. For clar-ity, we briefly discuss the employed exposure control Lexp,spatial consistency Lspa, color constancy LRGB and illu-mination smoothness LT VA below.Exposure control loss Lexp encourages exposure cor-rection of under- and over-exposed regions by setting anexpected average region intensity E:",
  "c{R,G,B}(|xAc| + |yAc|)2,(7)": "where Ac is the intermediate channel-wise curve param-eter used to enhance the image I, and denotes the gradi-ent operation.Additional zero-reference losses.We have experi-mented with additional zero-reference losses, such as total-variation smoothness loss applied to the enhanced imageI, total-variation smoothness loss applied to -tonemappedenhanced image I, comparing differently sampled en-hanced image I and input image I. While the additionalconstraints we tried led to improved performance for somedatasets, we found that the impact is not consistent acrossmany datasets and sometimes lead to significant decrease inperformance.",
  "We propose to leverage the pre-trained CLIP model whichcan capture lighting conditions, image quality and semanticinformation for image enhancement at the loss level": "Learned CLIP Image Priors.To further constrainthe enhanced image I and improve the quality of the en-hanced images, we use the learned image prompts intro-duced in subsection 3.1. We found that the image priorlearned via prompt learning using our methods reducesover-amplification of the image noise and overexposure ofbright regions.To this end, we first encode the enhanced image I us-ing the CLIP image encoder img and the learned promptpair using the CLIP text encoder txt. Next, we computethe cosine similarity between the embedded enhanced im-age img(I) and the learned prompt pair txt(P), whereP = Pp, Pn is a pair of the positive and negative learnedprompts.",
  "i{p,n} ecos(img(I).txt(Pi) ,(8)": "Finally, we compute the cross-entropy loss, assumingthat the enhanced image should match the positive promptPp.Semantic guidance. We propose to leverage the zero-shot capabilities of the CLIP model to introduce semanticguidance during training in a straightforward way. Thanksto the simplicity, the proposed semantic guidance methodcan be used with any low-light dataset containing bounding-box annotations or any type of annotation that can be usedto extract patches with an object category, which is advan-tageous given the sparsity and high collection costs of low-light datasets. During the training, we use annotation toextract image patches counting objects of a class cls. Be-cause we use CLIP to perform zero-shot classification, thelabel set is not pre-fixed or limited and can be adjusted fora single batch dynamically during the training. For each in-stance, we use a pair of positive and negative class prompts,T = {a photo of a cls,not a photo of a cls}} and ex-perimentally show that this results in improved task-basedperformance of our method.Given an enhanced low-light image I and its class labelcls, we perform the classification based on the cosine simi-larity between the embedded enhanced image img(I) anda pair of antonym prompts txt(T), where T = {a photoof a cls,not a photo of a cls}} is a pair of positive andnegative prompts.",
  ". Quantitative results of the ablation study of our proposed method in terms of task-based performance. Our proposed improvementsleads to consistent improvement over the baseline method": "In contrast with methods that perform semantic guid-ance using pre-trained normal-light high-level models, ourmethod can be applied to datasets without paired normal-light ground-truth data. Moreover, our method leveragesthe zero-shot capabilities of CLIP and is scalable to anydataset with at least bounding box-level annotation viaopen-vocabulary classification. Additionally, the method iseasily extendable to unannotated paired low- and normal-light datasets by extracting high-confidence detections us-ing a normal-light object detector.",
  ". Experiments": "In our work, we focus on improving low-light cognitionby enhancing images before processing with downstreammodels.Similar to related work ,rather than assuming correlation between human percep-tion and downstream task performances, we focus on task-based evaluation. Such evaluation has been used previouslyin .Implementation details1. We use Zero-DCE as ourbaseline model and CLIP for image classification and thelearned prompt. For training the prompt, we learn the pos-itive and negative prompts of length 16 using brightness,contrast and hue augmentation, and down-sample imagecrops by the scale factor of 4 for positive samples and down-sample by nearest-neighbor downsampling by the factorof 4 for negative samples. For training the enhancementmodel, we extract image patches containing objects basedon dataset bounding box annotation and include a portionof the image around the bounding box if the object is verysmall (e.g. in the DARKFACE dataset). For pairedlow- and normal-light datasets without annotated object in-",
  "The experiments were completed by authors at National Taiwan Uni-versity": "stances, we ran an open-vocabulary detector YOLO-World on paired normal-light data using 365 labels in the Ob-jects365 dataset and extract patches based on predic-tions with a confidence score over 0.3. We train our en-hancement network on 224 224 image patches with abatch size 8, for 105K steps, using the Adam optimizerwith a learning rate 0.0001, weight decay 0.0001 and gra-dient norm clipping set to 0.1. We use zero-reference lossterms proposed by Guo et al. and set the the weights ofcolor loss term to 5, spatial consistency term to 1, exposurecontrol term to 10, and TV term 200.",
  ". Ablation Study": "We conduct an extensive ablation study on multiple low-light datasets to show the contribution of each of the pro-posed loss terms.Following recommendations, wetrain the model using a variety of different lighting condi-tion. To this end, we use a collection of data extracted fromthe NOD , ExDark , DarkFace , ExLPose ,LOL and BAID datasets to train all of the vari-ants in Tab. 1. To further increase the diversity of data, weuse paired low- and normal-light datasets by using an open-vocabulary detector on normal-light data. For the LOL and BAID datasets which do not contain any instance annotation, we extract object instances from low-light im-ages based on paired detections on normal-light data witha confidence score over 0.3 using YOLO-World , using365 labels in the Objects365 dataset. In addition to alarge variety of illumination condition, our collection has avariety of different labels categories, and our method lever-ages zero-shot capabilities of the CLIP model by ad-justing the class prompts for each batch on-line. We showthe basics dataset statistics in .Task-based performance. As seen in the ablation studyresults presented in Tab. 1, the proposed open-vocabularyclassification loss leads to consistent improvements over thebaseline method , except for the DarkFace dataset.Similarly, introducing the proposed learned prompt leadsto consistent improvement over both the baseline method and detection with unenhanced images. The two pro-posed improvements together lead to consistent improve-ments over the baseline method except the ExDarkdataset where the differences between the methods inthe study are relatively small and well above detection per-formance using unenhanced images. Possible explanationsfor the observed difference for the ExDark dataset in-clude small proportion of the ExDark data during trainingor relatively higher average brightness levels in the datasetas seen in .Qualitative results. Qualitative results of the ablationstudy are shown in . Semantic guidance improvesthe color distribution of the images, removing the color hueshift visible in images produced by the baseline method.The learned prompt improves the color distribution of theimages and contributes to improving the contrast in theimages. Compared with the baseline method, the learnedprompt reduces the overexposure that leads to informationloss from the original input image. Moreover, the learnedprompt reduces over-amplification of the noise in the im-ages.",
  ". Task-based Comparison with Related Methods": "We show task-based performance comparison with re-lated methods in Tab. 2, all trained on the LOL . Wefirst compare our supervised methods DRBN and SNR-LLNet with semantic guidance . Our method ison-par with DRBN+SKF and outperforms SNR-LLNet in terms of task-based performance across thetested datasets. However, if compared to the computationalcomplexity of the DRBN+SKF our method relieson a much more lightweight architecture . Similarly tothe ablation study, our method shows improvement in termof task-based performance across all tested datasets exceptthe ExDark dataset. Again, ossible explanations forthe observed difference for the ExDark dataset includesmall proportion of the ExDark data during training or rela-tively higher average brightness levels in the dataset as seen",
  ". Conclusion": "We proposed to leverage the rich CLIP prior and CLIPszero-shot capabilities at the training stage to improve zero-reference low-light image enhancement. We proposed tofirst pre-train a pair of prompts that capture enhanced low-light image prior via prompt learning with a simple dataaugmentation strategy without any need for paired or un-paired normal-light data. We experimentally showed thatthe learned prompt helps guiding the enhancement by im-proving the image contrast, reducing over-enhancement andreducing over-amplification of noise. Next, we proposed tofurther reuse the CLIP model during the training processusing a straightforward yet effective and scalable semanticsegmentation via zero-shot open vocabulary classification.We conducted extensive experiments that show consistentimprovements over the baseline method across various low-light datasets in terms of task-based performance.",
  "time open-vocabulary object detection.arXiv preprintarXiv:2401.17270, 2024. 2, 3, 7, 8": "Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, ChaoZhang, Jiewen Yang, Zhe Jin, Andrew Beng Jin Teoh, andJiajun Shen. Abandoning the bayer-filter to see in the dark.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1743117440, 2022.3 Chi-Mao Fan, Tsung-Jung Liu, and Kuan-Hsien Liu. Halfwavelet attention on m-net+ for low-light image enhance-ment. In 2022 IEEE International Conference on Image Pro-cessing (ICIP), pages 38783882. IEEE, 2022. 1, 3 Huiyuan Fu, Wenkai Zheng, Xiangyu Meng, Xin Wang,Chuanming Wang, and Huadong Ma. You do not need addi-tional priors or regularizers in retinex-based low-light imageenhancement. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1812518134, 2023. 3",
  "Rafael C Gonzalez. Digital image processing. Pearson edu-cation india, 2009. 2": "Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy,Junhui Hou, Sam Kwong, and Runmin Cong. Zero-referencedeep curve estimation for low-light image enhancement. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 17801789, 2020. 1, 2,3, 4, 6, 7, 8 Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy,Junhui Hou, Sam Kwong, and Runmin Cong. Zero-referencedeep curve estimation for low-light image enhancement. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), June 2020. 7",
  "Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light im-age enhancement via illumination map estimation.IEEETransactions on image processing, 26(2):982993, 2016. 3": "Jiang Hai, Zhu Xuan, Ren Yang, Yutong Hao, Fengzhu Zou,Fang Lin, and Songchen Han.R2rnet: Low-light imageenhancement via real-low to real-normal network.Jour-nal of Visual Communication and Image Representation,90:103712, 2023. 3 Khurram Azeem Hashmi, Goutham Kallempudi, DidierStricker, and Muhammad Zeshan Afzal. Featenhancer: En-hancing hierarchical features for object detection and beyondunder low-light vision.In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 67256735, 2023. 3, 7",
  "Edwin H Land. The retinex theory of color vision. Scientificamerican, 237(6):108129, 1977. 2": "SohyunLee,JaesungRim,BoseungJeong,GeonuKim,ByungJu Woo,Haechan Lee,and Suha KwakSunghyun Cho. Human pose estimation in extremely low-light conditions. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),2023. 3, 7 Chongyi Li, Chunle Guo, and Chen Change Loy. Learning toenhance low-light image via zero-reference deep curve esti-mation. IEEE Transactions on Pattern Analysis and MachineIntelligence, 44(8):42254238, 2021. 1, 2, 3 Mading Li, Jiaying Liu, Wenhan Yang, and ZongmingGuo.Joint denoising and enhancement for low-light im-ages via retinex model. In International Forum on DigitalTV and Wireless Multimedia Communications, pages 9199.Springer, 2017. 3 Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, andZongming Guo.Structure-revealing low-light image en-hancement via robust retinex model. IEEE Transactions onImage Processing, 27(6):28282841, 2018. 3 Xiaoming Li, Chaofeng Chen, Shangchen Zhou, XianhuiLin, Wangmeng Zuo, and Lei Zhang. Blind face restorationvia deep multi-scale component dictionaries. In Europeanconference on computer vision, pages 399415. Springer,2020. 3 Jinxiu Liang, Jingwen Wang, Yuhui Quan, Tianyi Chen, Ji-aying Liu, Haibin Ling, and Yong Xu. Recurrent exposuregeneration for low-light face detection. IEEE Transactionson Multimedia, 24:16091621, 2021. 3 Zhexin Liang, Chongyi Li, Shangchen Zhou, RuichengFeng, and Chen Change Loy. Iterative prompt learning forunsupervised backlit image enhancement.In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 80948103, 2023. 1, 2, 3",
  "Yuen Peng Loh and Chee Seng Chan. Getting to know low-light images with the exclusively dark dataset.ComputerVision and Image Understanding, 178:3042, 2019. 3, 6, 7,8": "Xiaoqian Lv, Shengping Zhang, Qinglin Liu, Haozhe Xie,Bineng Zhong, and Huiyu Zhou. Backlitnet: A dataset andnetwork for backlit image enhancement. Computer Visionand Image Understanding, 218:103403, 2022. 7 Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongx-uan Luo. Toward fast, flexible, and robust low-light imageenhancement. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 56375646, 2022. 3, 7 Igor Morawski, Yu-An Chen, Yu-Sheng Lin, Shusil Dangi,Kai He, and Winston H Hsu. Genisp: neural isp for low-lightmachine cognition. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages630639, 2022. 3, 7 Igor Morawski, Yu-An Chen, Yu-Sheng Lin, and Win-ston H Hsu.Nod: Taking a closer look at detection un-der extreme low-light conditions with night object detectiondataset. arXiv preprint arXiv:2110.10364, 2021. 3, 6, 7 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 1, 2, 3, 8 Nicolas Robidoux, Luis E Garcia Capel, Dong-eun Seo,Avinash Sharma, Federico Ariza, and Felix Heide. End-to-end high dynamic range camera pipeline optimization. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 62976307, 2021. 3,7 Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, GangYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: Alarge-scale, high-quality dataset for object detection. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 84308439, 2019. 7, 8",
  "Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Nat-uralness preserved enhancement algorithm for non-uniformillumination images. IEEE transactions on image process-ing, 22(9):35383548, 2013. 3": "Wenjing Wang, Wenhan Yang, and Jiaying Liu. Hla-face:Joint high-low adaptation for low light face detection.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1619516204, 2021. 3 Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.Recovering realistic texture in image super-resolution bydeep spatial feature transform. In Proceedings of the IEEEconference on computer vision and pattern recognition,pages 606615, 2018. 3 Yudong Wang, Jichang Guo, Ruining Wang, Wanru He, andChongyi Li. Tienet: task-oriented image enhancement net-work for degraded object detection. Signal, Image and VideoProcessing, pages 18, 2023. 3, 7 Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-Pui Chau, and Alex Kot. Low-light image enhancement withnormalizing flow. In Proceedings of the AAAI conference onartificial intelligence, volume 36, pages 26042612, 2022. 3",
  "Chen Wei, Wenjing Wang, Wenhan Yang, and JiayingLiu. Deep retinex decomposition for low-light enhancement.arXiv preprint arXiv:1808.04560, 2018. 1, 3, 7, 8": "Wenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wen-han Yang, and Jianmin Jiang. Uretinex-net: Retinex-baseddeep unfolding network for low-light image enhancement.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 59015910, 2022. 3 Yuhui Wu, Chen Pan, Guoqing Wang, Yang Yang, Jiwei Wei,Chongyi Li, and Heng Tao Shen. Learning semantic-awareknowledge guidance for low-light image enhancement. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 16621671,June 2023. 1, 3, 7, 8 Ke Xu, Xin Yang, Baocai Yin, and Rynson WH Lau.Learning to restore low-light images via decomposition-and-enhancement. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 22812290, 2020. 1, 3 Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia.Snr-aware low-light image enhancement. In 2022 IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 1769317703, 2022. 1, 3, 7, 8 Xiaogang Xu, Ruixing Wang, and Jiangbo Lu. Low-lightimage enhancement via structure modeling and guidance.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 98939903,June 2023. 1, 3 Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianx-iong Yin, and Simon See. Arid: A new dataset for recogniz-ing action in the dark. In Deep Learning for Human Activ-ity Recognition: Second International Workshop, DL-HAR2020, Held in Conjunction with IJCAI-PRICAI 2020, Ky-oto, Japan, January 8, 2021, Proceedings 2, pages 7084.Springer, 2021. 3",
  "supervised approach for low-light image enhancement. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 30633072, 2020. 1, 3,7, 8": "Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang,and Jiaying Liu. Sparse gradient regularized deep retinexnetwork for robust low-light image enhancement.IEEETransactions on Image Processing, 30:20722086, 2021. 1,3 Wenhan Yang, Ye Yuan, Wenqi Ren, Jiaying Liu, Walter J.Scheirer, Zhangyang Wang, Zhang, and et al. Advancingimage understanding in poor visibility environments: A col-lective benchmark study. IEEE Transactions on Image Pro-cessing, 29:57375752, 2020. 3, 6, 7, 8 Masakazu Yoshimura, Junji Otsuka, Atsushi Irie, andTakeshi Ohashi. Dynamicisp: dynamically controlled im-age signal processor for image recognition. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1286612876, 2023. 3, 7",
  "Yonghua Zhang, Xiaojie Guo, Jiayi Ma, Wei Liu, and JiawanZhang. Beyond brightening low-light images. InternationalJournal of Computer Vision, 129:10131037, 2021. 1, 3": "Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindlingthe darkness: A practical low-light image enhancer. In Pro-ceedings of the 27th ACM international conference on mul-timedia, pages 16321640, 2019. 1, 3 Zhao Zhang, Huan Zheng, Richang Hong, Mingliang Xu,Shuicheng Yan, and Meng Wang. Deep color consistent net-work for low-light image enhancement. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 18991908, 2022. 1, 3 Chuanjun Zheng, Daming Shi, and Wentian Shi. Adaptiveunfolding total variation network for low-light image en-hancement. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 44394448, 2021. 1,3 Shen Zheng and Gaurav Gupta. Semantic-guided zero-shotlearning for low-light image/video enhancement.In Pro-ceedings of the IEEE/CVF Winter conference on applicationsof computer vision, pages 581590, 2022. 1, 2, 3, 7"
}