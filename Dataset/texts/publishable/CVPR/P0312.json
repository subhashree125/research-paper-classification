{
  "Abstract": "Vision graph neural networks (ViG) offer a new avenuefor exploration in computer vision. A major bottleneck inViGs is the inefficient k-nearest neighbor (KNN) operationused for graph construction. To solve this issue, we proposea new method for designing ViGs, Dynamic Axial GraphConstruction (DAGC), which is more efficient than KNNas it limits the number of considered graph connectionsmade within an image. Additionally, we propose a novelCNN-GNN architecture, GreedyViG, which uses DAGC.Extensive experiments show that GreedyViG beats exist-ing ViG, CNN, and ViT architectures in terms of accuracy,GMACs, and parameters on image classification, object de-tection, instance segmentation, and semantic segmentationtasks. Our smallest model, GreedyViG-S, achieves 81.1%top-1 accuracy on ImageNet-1K, 2.9% higher than VisionGNN and 2.2% higher than Vision HyperGraph Neural Net-work (ViHGNN), with less GMACs and a similar numberof parameters. Our largest model, GreedyViG-B obtains83.9% top-1 accuracy, 0.2% higher than Vision GNN, witha 66.6% decrease in parameters and a 69% decrease inGMACs. GreedyViG-B also obtains the same accuracy asViHGNN with a 67.3% decrease in parameters and a 71.3%decrease in GMACs. Our work shows that hybrid CNN-GNN architectures not only provide a new avenue for de-signing efficient models, but that they can also exceed theperformance of current state-of-the-art models1.",
  ". Introduction": "Rapid growth in deep learning has lead to numerous suc-cesses across a diverse set of computer vision tasks includ-ing image classification , object detection , instancesegmentation , and semantic segmentation . Keydrivers behind this growth include convolutional neural net-works (CNNs) , vision transformers (ViTs), and multi-layer perceptron (MLP)-based vision mod-",
  ". Comparison of model size and performance (top-1accuracy on ImageNet-1K). GreedyViG achieves the highest per-formance compared to other state-of-the-art models": "els . In CNNs and MLPs input images are repre-sented as a grid of pixels, however in ViTs images are repre-sented as a sequence of patches. By splitting an input imageinto a sequence of patch embeddings, the image is trans-formed into an input usable by the transformer modules of-ten used in natural language processing . UnlikeCNNs and MLPs, which have a local receptive field, ViTshave global receptive fields allowing them to learn from dis-tant interactions within images . The recently proposed Vision GNN (ViG) representsimages in a more versatile manner through a graph structurerather than as a sequence of patches as in ViTs . ViGconstructs the graph through dividing an image into patchesand then connecting the patches (i.e., nodes) through theK-nearest neighbors (KNN) algorithm . Vision Hyper-Graph Neural Network (ViHGNN) improves upon theoriginal ViG by using the hypergraph structure to removethe constraint of exclusively connecting pairs of nodes. LikeViTs, ViG-based models can process global object interac-tions, but are also computationally expensive. To deal withthe computationally expensive nature of ViG-based mod-els, MobileViG used a structured graph that does not",
  "arXiv:2405.06849v1 [cs.CV] 10 May 2024": "change across input images and removes the need for KNN-based graph construction.While the success of ViG , ViHGNN , and Mo-bileViG show the potential of treating an image as agraph for computer vision tasks, they also show some lim-itations. In general ViG-based models are computationallyexpensive, due to the expensive nature of graph construc-tion. MobileViG alleviates this issue through static graphconstruction, but at the cost of a graph that does not changeacross input images, thus limiting the benefit of using agraph-based model. The limitations of current ViG-basedmodels are as follows:1. Computational Cost of Graph Construction: A fun-damental issue facing ViG-based models is the cost ofKNN-based graph construction. KNN-based graph con-struction requires comparing every single node withinthe ViG-based model to determine the K nearest nodes.This cost makes KNN-based ViG models inefficient. 2. Inability of a Static Graph to Change Across Inputs:The computational cost of KNN-based ViG models leadto static graph construction. The fundamental issue withstatic graph construction is it removes the benefit ofusing a ViG-based model as the graph constructed nolonger changes across input images.In this work, we propose Dynamic Axial Graph Con-struction (DAGC) to address the current limitations of ViG-based models.We also introduce GreedyViG, an effi-cient ViG-based architecture using a hybrid CNN-GNN ap-proach. DAGC is more computationally efficient comparedto KNN-based graph construction while maintaining a dy-namic set of connections that changes across input images.In , we show that our proposed GreedyViG ar-chitecture outperforms competing state-of-the-art (SOTA)models across all model sizes in terms of parameters. Wesummarize our contributions as follows:1. We propose a new method for designing efficient visionGNNs, Dynamic Axial Graph Construction (DAGC).DAGC is more efficient compared to KNN-based ViGsas DAGC limits the graph connections made withinan image to only the most significant ones.Ourmethod is lightweight compared to KNN-based ViGsand more representative than SOTA static graph con-struction based ViGs. 2. We propose a novel efficient CNN-GNN architecture,GreedyViG, which uses DAGC, conditional positionalencoding (CPE) , and max-relative graph convolution. We use convolutional layers and grapher layers inall four stages of the proposed architecture to performlocal and global processing for each resolution. 3. We conduct comprehensive experiments to underscorethe efficacy of the GreedyViG architecture, which beatsexisting ViG architectures, efficient CNN architectures,and efficient ViT architectures in terms of accuracy and/or parameters and GMACs (number of MACs inbillions) on four representative vision tasks: ImageNetimage classification , COCO object detection ,COCO instance segmentation , and ADE20K se-mantic segmentation . Specifically our GreedyViG-B model achieves a top-1 accuracy of 83.9% on the Ima-geNet classification task, 46.3% Average Precision (AP)on the COCO object detection task, and 47.4% mean In-tersection over Union (mIoU) on the ADE20K semanticsegmentation task.The paper is organized as follows. covers re-lated work in the ViG and efficient computer vision archi-tecture space. describes the design methodologybehind DAGC and the GreedyViG architecture.Section 4 describes experimental setup and results for ImageNet-1k image classification, COCO object detection, COCOinstance segmentation, and ADE20K semantic segmenta-tion. covers ablation studies on how different de-sign decisions impact performance on ImageNet-1k. Lastly, summarizes our main contributions.",
  ". Related Work": "The mainstream network architecture for computer visionhas historically been convolutional neural networks (CNN). In the efficient computer vision space,CNN-based architectures have been evenmore dominant due to the computationally expensive na-ture of ViTs . Many works have attempted to addressthe computational costs associated with self-attention lay-ers and recently hybrid architectures that combineCNNs and ViTs to effectively capture local and global in-formation have been proposed .Traditionally graph neural networks (GNNs) have oper-ated on biological, social, or citation networks . GNNs have also been used for tasks in computer vi-sion such as, point cloud classification and segmentation, as well as human action recognition . But,with the introduction of Vision GNN , the adoption ofGNNs as a general purpose vision backbone has grown withworks like . MobileViG introduces a hybridCNN-GNN architecture to design an efficient computer vi-sion backbone to compete with CNN, ViT, and hybrid archi-tectures. MobileViG accomplishes this through introduc-ing a static graph construction method called Sparse VisionGraph Attention (SVGA) to avoid the computationally ex-pensive nature of ViGs. Despite the efficiencies of Mobile-ViG , it does not take full advantage of the global pro-cessing possible with GNNs since it only uses graph con-volution at the lowest resolution stage of its design. Mo-bileViG also loses representation ability because allimages construct the same graph in their proposed staticmethod, decreasing the benefits of using a GNN-based ar-chitecture. Thus, to address these limitations, we introduce",
  "a new CNN-GNN architecture, GreedyViG, that takes ad-vantage of graph convolution at higher resolution stages andconstructs a graph that changes across input images": "a) SVGAb) DAGC .DAGC and SVGA graph construction.a) SVGAgraph construction for the green patch of an 88 image. All redpatches will be connected to the green patch regardless of simi-larity. b) DAGC for the green patch of an 88 image. DAGCdynamically constructs a graph along the axes, through applyinga mask (the blue patches) to only connect similar patches in termsof Euclidean distance. The red patches will not be connected tothe green patch as they are not a part of the mask.",
  ". Methodology": "In this section, we describe the DAGC algorithm and pro-vide details on the GreedyViG architecture design. Moreprecisely, .1 describes the DAGC algorithm. Sec-tion 3.2 explains how we adapt the Grapher module fromViG to create the DAGC block. .3 describeshow we combine the DAGC blocks along with invertedresidual blocks to create the GreedyViG architecture.",
  ". Dynamic Axial Graph Construction": "We propose Dynamic Axial Graph Construction (DAGC) asan efficient alternative dynamic graph construction methodto the computationally expensive KNN graph constructionmethod from Vision GNN . DAGC builds upon SVGA, but instead of statically constructing a graph, DAGCconstructs a graph that changes across input images. DAGCretains the efficiencies of SVGA through the removal of theKNN computation and input reshaping. It also introducesan efficient graph construction method based on the mean() and standard deviation () of the Euclidean distance be-tween patches in the input image.In ViG, the KNN computation is required for every in-put image, since the nearest neighbors of each patch can-not be known ahead of time. This results in a graph withconnections throughout the image. Due to the unstructurednature of KNN, ViG contains two reshaping operations.The first to reshape the input image from a 4D tensor to a3D tensor for graph convolution and the second to reshapethe input from 3D back to 4D for the convolutional layers. SVGA eliminates these two reshaping operations andKNN computation through using a static graph where eachpatch is connected to every Kth patch in its row and columnas seen in a.DAGC leverages the axial construction of SVGA to re-tain its efficiencies, while dynamically constructing a morerepresentative graph. To do this, DAGC first obtains an es-timate of the and of the Euclidean distance betweennodes through using a subset of nodes. The subset of nodesis obtained by splitting the image into quadrants and com-paring the quadrants diagonal to one another as shown in below. Then, the and can be calculated withthose Euclidean distance values. This allows the estimated and to be computed between two images (the originaland the one with its quadrants flipped across the diagonal).This is to decrease the number of comparisons for gettingthe and values. The reason we avoid calculating thetrue and is that computing them directly would requirecalculating the Euclidean distance between each individualnode and all other nodes in the image. We then considerconnections across the row and column as in SVGA to de-crease computation, as MobileViG demonstrated thatnot every patch needs to be considered. If the Euclideandistance between two nodes is less than the difference ofthe estimated and , then we connect the two nodes.",
  ". Euclidean distance calculation between the originalimage and the image with its quadrants flipped along the diagonal": "DAGC also enables a variable amount of connectionsacross different images, unlike KNNs fixed K number ofconnections for all images.This is because in differentimages, different nodes will have a Euclidean distance be-tween them be less than the difference of the estimated and . The intuition behind the use of and is that nodepairs that are within one of are close to one anotherand should share information. These values are then usedto make the connections generated by DAGC as shown inb. In we can see that SVGA connects thefish to parts of the image that are not fish, while DAGC onlyconnects the fish to other parts of the image that are fish.Now that we have the estimated and within the im-age, we roll the input image X, mK to the right or downwhile mK is less than H (height) and W (width) of the im-age as seen in Algorithm 1. The roll operation is used tocompare the patches that are N hops away. In b,",
  ". GreedyViG architecture. (a) Network architecture showing the stages and blocks. (b) The Conv Stem. (c) MBConv Block. (d)Downsample. (e) DAGC Block. (f) Dynamic Grapher. (g) FFN": "node (5,1) in x, y coordinates is compared to nodes (1,1),(3,1), (7,1), (5,3), (5,5), and (5,7) through rolling to thenext node. After rolling, we compute the Euclidean dis-tance between the input X and the rolled version (Xrolled)to determine whether to connect the two points. If the dis-tance is less than , then the mask is assigned a valueof 1, else it is assigned a value of 0. This mask is then mul-tiplied with Xrolled X, to mask out max-relative scoresbetween patches not considered connections. This value isdenoted as Xdown and Xright in Algorithm 1. Next, themax operation is taken and the result is stored in Xfinal.Lastly, after the rolling, masking, and max-relative opera-tions a final Conv2d is performed. Through this methodology, DAGC provides a more rep-resentative graph construction compared to SVGA , asdissimilar patches (i.e., nodes) are not connected. DAGCis also less computationally expensive compared to KNNdue to less comparisons being needed for constructing thegraph (KNN must compute the nearest neighbors for every patch). DAGC also does not require the reshaping neededfor performing graph convolution in KNN-based methods. Thus, DAGC provides representation flexibility likeKNN and decreased computational complexity like SVGA.",
  ". DAGC Block": "The DAGC block consists of a Dynamic Grapher modulefollowed by a feed-forward network (FFN). The DynamicGrapher module differs from the Grapher module used in through the use of an updated max-relative graph con-volution step called DynConv using Algorithm 1 and con-ditional positional encoding (CPE) . DynConv dynami-cally creates a graph that changes across input images, un-like the graph construction used in the graph convolutionof SVGA . Given an input feature X RNN, theupdated Dynamic Grapher is expressed as:",
  "end whilereturn Conv2d(Concat(X, Xfinal))": "layer weights, CPE is a depthwise convolution, and is aGeLU activation. The updated Dynamic Grapher module isvisually depicted in f.Following the updated Dynamic Grapher, we use thefeed-forward network (FFN) module as used in Vision GNN and MobileViG , which can be seen in g.The FFN module is a two layer MLP expressed as:",
  "Z = (XW1)W2 + Y(2)": "where Z RNN, W1 and W2 are fully connected layerweights, and is once again GeLU. We call this combina-tion of the Dynamic Grapher module and FFN the DAGCblock, as shown in e.CPE is introduced into the DAGC block to provide theposition of the node within the image as this is important to performance . The CPE used in DAGC followsthe method of , i.e., a depthwise convolution computesthe encodings, and then the encodings are added to the in-put. The addition of CPE adds spatial information into themessage passing step of DynConv improving performance.",
  ". GreedyViG Architecture": "The GreedyViG architecture shown in a is com-posed of a convolutional stem, and four stages of invertedresidual blocks (MBConv) and DAGC blocks each followedby a downsample reducing the resolution to get to the nextstage. The stem consists of 33 convolutions with the strideequal to 2, each followed by batch normalization (BN) andthe GeLU activation function as seen in b. TheMBConv block is used for local processing at each stage,before the DAGC block performs global processing at eachstage. Each MBConv block consists of pointwise convo-lutions, BN, GeLU, a depth-wise 33 convolution, and aresidual connection as seen in c. The DAGC blockis used at each resolution to better learn global object inter-actions. Between each stage there is a downsample, whichconsists of a 33 convolution with a stride equal to 2 fol-lowed by BN as shown in d to half the input reso-lution and expand the channel dimension. Each stage in theGreedyViG architecture is composed of multiple MBConvand DAGC blocks, where the number of repetitions andchannel width is changed depending on model size. Withinthe DAGC blocks used in all GreedyViG model sizes, thedistance between connections of nodes before masking isset to K = 8, 4, 2, 1 for stages 1 to 4, respectively. Thisallows the graph constructed to still be dense in lower reso-lution stages, as too sparse of a graph can negatively impactaccuracy as seen in our ablation studies. After thefinal DAGC block there is a classification head consistingof Average Pooling and an FFN.",
  ". Image Classification": "We implement the model using PyTorch 1.12.1 andTimm library . We use 16 NVIDIA A100 GPUs to trainour models, with an effective batch size of 2048. The mod-els are trained from scratch for 300 epochs on ImageNet-1K with AdamW optimizer . Learning rate is setto 2e3 with cosine annealing schedule. We use a standardimage resolution, 224 224, for both training and testing. . Classification results on ImageNet-1k for GreedyViG and other state-of-the-art models. Different training seeds result in about0.1% variation in accuracy for GreedyViG over three runs. Bold entries indicate results obtained for GreedyViG proposed in this paper.",
  "Similar to DeiT , we perform knowledge distillation us-ing RegNetY-16GF with 82.9% top-1 accuracy": "As seen in , for a similar number of parame-ters and GMACs, GreedyViG outperforms Pyramid ViG(PViG) , Pyramid ViHGNN (PViHGNN) , and Mo-bileViG significantly.For example, our smallestmodel, GreedyViG-S, achieves 81.1% top-1 accuracy onImageNet-1K with 12.0 M parameters and 1.6 GMACs,which is 2.9% higher top-1 accuracy compared to PViG-Ti and 2.2% higher than PViHGNN-Ti with lessGMACs and a similar number of parameters. Our largest model, GreedyViG-B obtains 83.9% top-1 accuracy withonly 30.9 M parameters and 5.2 GMACs, which is a 0.2%higher top-1 accuracy compared to PViG-B with a66.6% decrease in parameters (61.7 M fewer parameters)and a 69% decrease in GMACs (11.6 fewer GMACs) andthe same top-1 accuracy as PViHGNN-B with a 67.3%decrease in parameters (63.5 M fewer parameters) and a71.3% decrease in GMACs (12.9 fewer GMACs).",
  "GreedyViG-B (Ours)30.946.368.451.342.165.545.447.4": "S beats PoolFormer-s12 with 3.9% higher top-1 accu-racy while having the same number of parameters and 0.4fewer GMACs. GreedyViG-M achieves 82.9% top-1 accu-racy beating ConvNext-T with 0.2% higher top-1 ac-curacy while having 6.7 M fewer parameters and 4.2 fewerGMACs. Additionally, GreedyViG-B achieves 83.9% top-1 accuracy beating the EfficientFormer family ofmodels for a similar number of parameters.",
  ". Object Detection and Instance Segmentation": "We show that GreedyViG generalizes well to downstreamtasks by using it as a backbone in the Mask-RCNN frame-work for object detection and instance segmentationtasks on the MS COCO 2017 dataset . The dataset con-tains training and validations sets of 118K and 5K images,respectively. We implement the backbone using PyTorch1.12.1 and Timm library . The model is initial-ized with ImageNet-1k pretrained weights from 300 epochsof training. We use the AdamW optimizer withan initial learning rate of 2e4 and train the model for 12epochs with a standard resolution (1333 800) followingthe process of prior work .As seen in , with similar model size GreedyViGoutperforms PoolFormer , EfficientFormer , Effi-cientFormerV2 , MobileViG , and PVT interms of either parameters or improved average precision (AP) on object detection and instance segmentation. TheGreedyViG-S model gets 43.2 AP box and 39.8 AP mask onthe object detection and instance segmentation tasks out-performing PoolFormer-s12 by 5.9 AP box and 5.2AP mask. Our GreedyViG-B model achieves 46.3 AP box and 42.1 AP mask outperforming MobileViG-B by4.3 AP box and 3.2 AP mask and FastViT-SA36 by2.5 AP box and 2.7 AP mask. The strong performance ofGreedyViG on object detection and instance segmentationshows the capability of DAGC and GreedyViG to general-ize well to different tasks in computer vision.",
  ". Semantic Segmentation": "We further validate the performance of GreedyViG onsemantic segmentation using the scene parsing dataset,ADE20k .The dataset contains 20K training im-ages and 2K validation images with 150 semantic cate-gories. Following the methodologies of , webuild GreedyViG with Semantic FPN as the segmen-tation decoder. The backbone is initialized with pretrainedweights on ImageNet-1K and the model is trained for 40Kiterations on 8 NVIDIA RTX 6000 Ada generation GPUs.We follow the process of existing works in segmentation,using the AdamW optimizer, set the learning rate as 2 104 with a poly decay by the power of 0.9, and set thetraining resolution to 512 512 . a) b) . Comparison of model size and performance (mIoUon ADE20K). GreedyViG achieves the highest performance on allmodel sizes compared to other state-of-the-art models. a) showsperformance compared to parameters and b) shows performancecompared to GMACs. Asshownin,GreedyViG-Soutper-forms PoolFormer-S12 ,FastViT-SA12 ,andEfficientFormer-L1 by 6.0, 5.2, and 4.3 mIoU,respectively.Additionally,GreedyViG-Boutper-forms PoolFormer-S24 ,FastViT-SA36 ,andEfficientFormer-L3 by 7.1, 4.5, and 3.9 mIoU,respectively. shows GreedyViG significantlyoutperforms FastViT , PVT , Poolformer ,EfficientFormer , and ResNet models with a similarnumber of parameters and GMACs.",
  ". Ablation Studies": "The ablation studies are conducted on ImageNet-1K . reports the ablation study of GreedyViG-B on vary-ing distances of considered graph connections (K) in theDAGC algorithm and how conditional positional encodingaffects performance.Distance between considered nodes for graph con-struction (K). The distance considered between possiblenode connections for graph construction can create a sparsergraph, but can lead to decreased accuracy as the graph be-comes too sparse and does not contain enough connections.We can see this in , which shows that for K = 16, 8,",
  "K = 8, 4, 2, 130.9Yes83.9": "4, 2 in stages 1, 2, 3, and 4 that the top-1 accuracy is 0.4%lower than for when K = 8, 4, 2, 1. We also find that usingK = 9, 6, 3, 1 leads to a 0.2% decrease in top-1 accuracycompared to K = 8, 4, 2, 1 used in GreedyViG.Conditional Positional Encoding (CPE). The encod-ing of positions within GreedyViG also boosts performancefor relatively few parameters. When removing CPE fromGreedyViG-B, we see a drop in accuracy of 0.2% with onlya 0.2 M decrease in parameters, showing that CPE is bene-ficial in the GreedyViG architecture.Further ablation studies on the effects of removing graphconvolutions at higher resolution stages, static versus dy-namic graph construction, and how graph construction im-pacts latency are included in the supplementary material.",
  ". Conclusion": "In this work, we have proposed a new method for designingefficient vision GNNs, Dynamic Axial Graph Construction(DAGC). DAGC is more efficient compared to KNN-basedViGs and more representative compared to SVGA. This isbecause DAGC uses an axial graph construction method tolimit graph connections, and it does not have a fixed numberof graph connections allowing for a variable number of con-nections based on the input image. Compared to past axialgraph construction methods, DAGC limits the graph con-nections made within an image to only the significant con-nections thereby constructing a more representative graph.Additionally, we have proposed a novel CNN-GNN archi-tecture, GreedyViG, which uses DAGC. GreedyViG outper-forms existing ViG, CNN, and ViT models on multiple rep-resentative vision tasks, namely image classification, objectdetection, instance segmentation, and semantic segmenta-tion. GreedyViG shows that ViG-based models can be le-gitimate competitors to ViT-based models through DAGCand by performing local and global processing at each res-olution through a hybrid CNN-GNN architecture.",
  "Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and En-hua Wu. Vision gnn: An image is worth graph of nodes.arXiv preprint arXiv:2206.00272, 2022. 1, 2, 3, 4, 5, 6, 7": "Yan Han, Peihao Wang, Souvik Kundu, Ying Ding, andZhangyang Wang. Vision hgnn: An image is more than agraph of nodes. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1987819888,2023. 1, 2, 5, 6, 7 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 1, 2, 6, 7, 8",
  "Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-shick. Mask r-cnn. In Proceedings of the IEEE internationalconference on computer vision, pages 29612969, 2017. 7": "Andrew G Howard, Menglong Zhu, Bo Chen, DmitryKalenichenko, Weijun Wang, Tobias Weyand, Marco An-dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-tional neural networks for mobile vision applications. arXivpreprint arXiv:1704.04861, 2017. 2 Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger.Densely connected convolutional net-works. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 47004708, 2017. 2",
  "Guohao Li, Matthias Muller, Ali Thabet, and BernardGhanem.Deepgcns: Can gcns go as deep as cnns?InProceedings of the IEEE/CVF international conference oncomputer vision, pages 92679276, 2019. 2": "Jiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xue-feng Xiao, Rui Wang, Min Zheng, and Xin Pan.Next-vit:Next generation vision transformer for efficient de-ployment in realistic industrial scenarios.arXiv preprintarXiv:2207.05501, 2022. 7 Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, KamyarSalahi, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Re-thinking vision transformers for mobilenet size and speed.arXiv preprint arXiv:2212.08059, 2022. 2, 6, 7 Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evan-gelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Effi-cientformer: Vision transformers at mobilenet speed. arXivpreprint arXiv:2206.01191, 2022. 2, 6, 7, 8 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 1, 2, 7 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 1, 6 Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-enhofer, Trevor Darrell, and Saining Xie. A convnet for the2020s. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1197611986,2022. 1, 6, 7",
  "Mingxing Tan and Quoc Le. Efficientnetv2: Smaller modelsand faster training. In International conference on machinelearning, pages 1009610106. PMLR, 2021. 2": "Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.Mlp-mixer: An all-mlp architecture for vision.Advancesin neural information processing systems, 34:2426124272,2021. 1 Hugo Touvron, Matthieu Cord, Matthijs Douze, FranciscoMassa, Alexandre Sablayrolles, and Herve Jegou. Trainingdata-efficient image transformers & distillation through at-tention. In International conference on machine learning,pages 1034710357. PMLR, 2021. 6 Hugo Touvron, Piotr Bojanowski, Mathilde Caron, MatthieuCord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac-ard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al.Resmlp: Feedforward networks for image classification withdata-efficient training. IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 2022. 1 Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, On-cel Tuzel, and Anurag Ranjan. Fastvit: A fast hybrid visiontransformer using structural reparameterization. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, 2023. 2, 7, 8 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 1",
  "Ross Wightman.PyTorch Image Models.https:/ / github . com / rwightman / pytorch - image -models, 2019. 5, 7": "JiaFu Wu, Jian Li, Jiangning Zhang, Boshen Zhang, Ming-min Chi, Yabiao Wang, and Chengjie Wang. Pvg: Progres-sive vision graph for vision recognition.In Proceedingsof the 31st ACM International Conference on Multimedia,page 24772486, New York, NY, USA, 2023. Associationfor Computing Machinery. 2 Yongji Wu, Defu Lian, Yiheng Xu, Le Wu, and EnhongChen. Graph convolutional networks with markov randomfield reasoning for social spammer detection. In Proceed-ings of the AAAI conference on artificial intelligence, pages10541061, 2020. 2 Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-ral graph convolutional networks for skeleton-based actionrecognition. In Proceedings of the AAAI conference on arti-ficial intelligence, 2018. 2 Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformeris actually what you need for vision.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1081910829, 2022. 6, 7, 8 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, AdelaBarriuso, and Antonio Torralba.Scene parsing throughade20k dataset. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 633641,2017. 1, 2, 7 Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang,Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,and Maosong Sun.Graph neural networks: A review ofmethods and applications. AI open, 1:5781, 2020. 2",
  "A. Further Ablation Studies": "The ablation studies are conducted on ImageNet-1K . reports the ablation study of GreedyViG-B (GViG-B) on the effects of graph convolutions at higher resolutionstages and reports the effects of static versus dy-namic graph construction.Graph convolutions at higher resolution stages. In Ta-ble 4 we can see that adding graph convolutions at higherresolution stages improves top-1 accuracy with a relativelysmall increase in parameters. By 1-stage, 2-stage, 3-stage,and 4-stage we mean that the DAGC blocks (graph convolu-tion block) will be used in stage 4, stages 3 and 4, stages 2,3, and 4, or in all stages as shown in . GreedyViG-B increases in top-1 accuracy as we move from 1-stage to4-stage increasing from 83.1% at the 1-stage configurationto 83.5% at the 2-stage configuration. Moving from the2-stage configuration to the 3-stage configuration we see a0.2% increase in accuracy reaching 83.7%. Finally, movingfrom 3-stage to 4-stage we see a 0.2% increase in accuracyreaching 83.9% top-1 accuracy at the 4-stage configuration.Comparing the 1-stage and 4-stage configurations we see a0.8% gain in top-1 accuracy with only an increase of 4.4 Mparameters, showing the benefits of graph convolutions athigher resolution stages. . Ablation study for graph convolutions at higher reso-lution stages on ImageNet-1K benchmark. 1-S, 2-S, 3-S, and 4-S indicate that graph convolutions were used in 1-stage, 2-stages,3-stages, or all 4-stages. A check mark indicates this componentwas used in the experiment. A (-) indicates this component wasnot used.",
  "GViG-B30.9---83.9": "Static versus dynamic graph construction. Comparedto the static graph construction method (SVGA) proposed in, DAGC connects only the similar connections based onEuclidean distance resulting in improved performance. In we can see the direct benefit of using DAGC com-pared to SVGA as it adds no parameters and increases thetop-1 accuracy of GreedyViG-B with 4-stages by 0.4% from83.5% to 83.9%. We can also see the benefit of DAGC andour overall GreedyViG architecture compared to the Mo-bileViG architecture, which uses SVGA, through compar-ing MobileViG-B (MViG-B) and a 1-stage configuration of GreedyViG-B. The 1-stage configuration of GreedyViG-Bshows a 0.5% improvement in top-1 accuracy from 82.6%to 83.1% while reducing parameters by 0.2 M, showing thebenefits of dynamic graph construction. . Ablation study for static versus dynamic graph con-struction on ImageNet-1K benchmark. 1-S indicates that graphconvolutions were only used in Stage 4, while 4-S indicates thatgraph convolutions were used in stages 1, 2, 3, and 4. A checkmark indicates this component was used in the experiment. A (-)indicates this component was not used.",
  "B. Network Configurations": "The detailed network architectures for GreedyViG-S, M,and B are provided in . We report the configurationof the stem, stages, and classification head. In each stagethe number of MBConv and DAGC blocks repeated as wellas their channel dimensions is reported. For GreedyViG-B,stage 4 has 3 repeated MBConv and DAGC blocks insteadof 4 in order to have comparable parameters to other com-peting architectures.",
  ". DAGC: O( W +H": "N). For each node, DAGC only needsto compare nodes that are every N hops away, thus de-creasing the number of comparisons. Also, since DAGCcomputes the and beforehand, it makes connectionsin the first search through of the image rather than need-ing to compare again for K connections. 3. SVGA: O(1). Connects each node along the axes.DAGC is more computationally expensive than SVGA,but more representative. KNN may be more representativethan DAGC, but can cause oversmoothing and is more com-putationally expensive. The measured time taken for graphconstruction is 0.06 ms in DAGC, 0.38 ms in KNN, and 0.04ms in SVGA when measured on an Nvidia RTX A6000; thisshows DAGC is slower than SVGA and faster than KNNin graph construction time. This can also be seen throughour latency results in . GreedyViG-S is faster andmore accurate than PViG-Ti, but is slower and more accu-rate than a smaller MobileViG-S model. GreedyViG-S isslower compared to MobileViG-S because DAGC is slowerthan SVGA, GreedyViG has more parameters, and becauseGreedyViG contains more global processing stages that per-form graph convolution (DAGC blocks) as compared toMobileViG which only does graph convolution at its low-est resolution stage after multiple downsample layers. . Graph construction impact on accuracy and latency.We show GreedyViG-S with KNN and DAGC to compare withPViG-Ti with KNN and DAGC. We also show MobileViG-S withSVGA to show it is less accurate, but faster than GreedyViG-S.",
  "GreedyViG-S (Ours) w/ DAGC12.0 M53.4 ms81.1": "The graph construction and architecture of GreedyViGboth contribute to the performance of GreedyViG models.When using DAGC with the original ViG architecture andKNN with our GreedyViG architecture in , we cansee that DAGC is faster and provides higher accuracy com-pared to KNN in these cases. GreedyViG-B with SVGA canalso be seen , showing with the same configurationDAGC has 83.9% accuracy compared to SVGAs 83.5%."
}