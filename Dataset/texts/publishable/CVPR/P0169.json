{
  "Abstract": "We propose Hydra-MDP, a novel paradigm employingmultiple teachers in a teacher-student model. This approachuses knowledge distillation from both human and rule-basedteachers to train the student model, which features a multi-head decoder to learn diverse trajectory candidates tailoredto various evaluation metrics. With the knowledge of rule-based teachers, Hydra-MDP learns how the environmentinfluences the planning in an end-to-end manner instead ofresorting to non-differentiable post-processing. This methodachieves the 1st place in the Navsim challenge, demonstrat-ing significant improvements in generalization across diversedriving environments and conditions. More details by visiting",
  ". Introduction": "End-to-end autonomous driving, which involves learning aneural planner with raw sensor inputs, is considered a promis-ing direction to achieve full autonomy. Despite the promisingprogress in this field , recent studies haveexposed multiple vulnerabilities and limitations of imitationlearning (IL) methods, particularly the inherent issues inopen-loop evaluation, such as the dysfunctional metrics andimplicit biases . This is critical as it fails to guaranteesafety, efficiency, comfort, and compliance with traffic rules.To address this main limitation, several works have proposedincorporating closed-loop metrics, which more effectivelyevaluate end-to-end autonomous driving by ensuring thatthe machine-learned planner meets essential criteria beyondmerely mimicking human drivers.Therefore, end-to-end planning is ideally a multi-targetand multimodal task, where multi-target planning involvesmeeting various evaluation metrics from either open-loopand closed-loop settings. In this context, multimodal indi-cates the existence of multiple optimal solutions for eachmetric.Existing end-to-end approaches often try to",
  ". Comparison between End-to-end Planning Paradigms": "consider closed-loop evaluation via post-processing, whichis not streamlined and may result in the loss of additionalinformation compared to a fully end-to-end pipeline. Mean-while, rule-based planners struggle with imperfectperception inputs. These imperfect inputs degrade the per-formance of rule-based planning under both closed-loopand open-loop metrics, as they rely on predicted perceptioninstead of ground truth (GT) labels.To address the issues, we propose a novel end-to-endautonomous driving framework called Hydra-MDP (Multi-modal Planning with Multi-target Hydra-distillation). Hydra-MDP is based on a novel teacher-student knowledge distil-lation (KD) architecture. The student model learns diversetrajectory candidates tailored to various evaluation metricsthrough KD from both human and rule-based teachers. Weinstantiate the multi-target Hydra-distillation with a multi-head decoder, thus effectively integrating the knowledgefrom specialized teachers. Hydra-MDP also features an ex-",
  ". The Overall Architecture of Hydra-MDP": "tendable KD architecture, allowing for easy integration ofadditional teachers.The student model uses environmental observations dur-ing training, while the teacher models use ground truth (GT)data. This setup allows the teacher models to generate betterplanning predictions, helping the student model to learn ef-fectively. By training the student model with environmentalobservations, it becomes adept at handling realistic condi-tions where GT perception is not accessible during testing.Our contributions are summarized as follows: 1. We propose a universal framework of end-to-end multi-modal planning via multi-target hydra-distillation, allow-ing the model to learn from both rule-based planners andhuman drivers in a scalable manner.",
  ". Preliminaries": "Let O represent sensor observations, P and P denote groundtruth and predicted perceptions (e.g. 3D object detection,lane detection), T be the expert trajectory, and T be the pre-dicted trajectory. Lim represents the imitation loss. We firstintroduce the two prevailing paradigms and our proposedparadigm () in this section:A. Single-modal Planning + Single-target Learning. Inthis paradigm , the planning network directly re-gresses the planned trajectory from the sensor observations.Ground truth perceptions can be used as auxiliary supervi-sion but does not influence the planning output. Perceptionlosses are not included in the formula for simplicity. Thewhole processing can be formulated as:",
  "T = arg minTif(Ti, P),(3)": "which is a non-differentiable process based on imperfectperception P.C. Multimodal Planning + Multi-target Learning. Wepropose this paradigm to simultaneously predict variouscosts (e.g., collision cost, drivable area compliance cost) viaa neural network f. This is performed in a teacher-studentdistillation manner, where the teacher has access to groundtruth perception P but the student relies only on sensorobservations O. This paradigm can be formulated as:",
  "As shown in , Hydra-MDP consists of two networks: aPerception Network and a Trajectory Decoder": "Perception Network. Our perception network builds uponthe official challenge baseline Transfuser , which con-sists of an image backbone, a LiDAR backbone, and per-ception heads for 3D object detection and BEV segmenta-tion. Multiple transformer layers connect features fromstages of both backbones, extracting meaningful informationfrom different modalities. The final output of the percep-tion network comprises environmental tokens Fenv, whichencode abundant semantic information derived from bothimages and LiDAR point clouds. Trajectory Decoder. Following Vadv2 , we construct afixed planning vocabulary to discretize the continuous ac-tion space. To build the vocabulary, we first sample 700Ktrajectories randomly from the original nuPlan database .Each trajectory Ti(i = 1, ..., k) consists of 40 timestampsof (x, y, heading), corresponding to the desired 10Hz fre-quency and a 4-second future horizon in the challenge. Theplanning vocabulary Vk is formed as K-means clusteringcenters of the 700K trajectories, where k denotes the size ofthe vocabulary. Vk is then embedded as k latent queries withan MLP, sent into layers of transformer encoders , andadded to the ego status E:",
  ". Multi-target Hydra-Distillation": "Though the imitation target provides certain clues for theplanner, it is insufficient for the model to associate the plan-ning decision with the driving environment under the closed-loop setting, leading to failures such as collisions and leaving drivable areas . Therefore, to boost the closed-loop per-formance of our end-to-end planner, we propose Multi-targetHydra-Distillation, a learning strategy that aligns the plannerwith simulation-based metrics in this challenge.The distillation process expands the learning targetthrough two steps: (1) running offline simulations ofthe planning vocabulary Vk for the entire training dataset;(2) introducing supervision from simulation scores for eachtrajectory in Vk during the training process. For a givenscenario, step 1 generates ground truth simulation scores{ Smi |i = 1, ..., k}|M|m=1 for each metric m M and the i-thtrajectory, where M represents the set of closed-loop metricsused in the challenge. For score predictions, latent vectorsVk are processed with a set of Hydra Prediction Heads, yield-ing predicted scores {Smi |i = 1, ..., k}|M|m=1. With a binarycross-entropy loss, we distill rule-based driving knowledgeinto the end-to-end planner:",
  "f(Ti, O) = (w1 log Simi+ w2 log SNCi+ w3 log SDACi+ w4 log (5ST T Ci+ 2SCi + 5SEPi)),(11)": "where {wi}4i=1 represent confidence weighting parametersto mitigate the imperfect fitting of different teachers. Theoptimal combination of weights is obtained via grid search,which typically fall within the following ranges: 0.01 w1 0.1, 0.1 w2, w3 1, 1 w4 10, indicatingthe necessity to prioritize rule-based costs over imitation.Finally, the trajectory with the lowest overall cost is chosen.",
  "PDM-Closed Perception GT94.699.889.986.999.989.1": "Transfuser LiDAR & Camera96.587.973.990.210078.0Vadv2-V4096 *LiDAR & Camera97.188.874.991.410079.7Vadv2-V4096 *-PPLiDAR & Camera97.089.175.091.210079.9Vadv2-V8192 *LiDAR & Camera97.289.176.091.610080.9Hydra-MDP-V4096LiDAR & Camera97.791.577.592.710082.6Hydra-MDP-V8192LiDAR & Camera97.991.777.692.910083.0Hydra-MDP-V8192-PDMLiDAR & Camera97.588.974.892.510080.2Hydra-MDP-V8192-WLiDAR & Camera98.196.177.893.910085.7Hydra-MDP-V8192-W-EPLiDAR & Camera98.396.078.794.610086.5 . Performance on the Navtest Split. The official Navsim implementation of PDM-Closed is potentially prone to errors due toinconsistent braking maneuvers and offset formulation compared with the nuPlan implementation . All end-to-end methods use theofficial Transfuser as the perception network. * Our distance-based imitation loss is adopted for training. PP: Transfuser perception isused for post-processing. PDM: The learning target is the overall PDM score. W: Weighted confidence during inference. EP: The model istrained to fit the continuous EP (Ego Progress) metric.",
  "V2-99": ". The Impact of Scaling Up on the Navtest Split. The official Navsim implementation of PDM-Closed. * ViT-L is initialized fromDepth Anything . ViT-L is EVA pretrained on Objects365 and COCO . V2-99 is initialized from DD3D . relevant annotations and sensor data sampled at 2 Hz. Thedataset primarily focuses on scenarios involving changes inintention, where the ego vehicles historical data cannot beextrapolated into a future plan. The dataset provides anno-tated 2D high-definition maps with semantic categories and3D bounding boxes for objects. The dataset is split into twoparts: Navtrain and Navtest, which respectively contain 1192and 136 scenarios for training/validation and testing.",
  ". Implementation Details": "We train our models on the Navtrain split using 8 NVIDIAA100 GPUs, with a total batch size of 256 across 20 epochs.The learning rate and weight decay are set to 1104 and 0.0following the official baseline. LiDAR points from 4 framesare splatted onto the BEV plane to form a density BEV fea-ture, which is encoded using ResNet34 . For images, thefront-view image is concatenated with the center-croppedfront-left-view and front-right-view images, yielding an in-put resolution of 256 1024 by default. ResNet34 is also",
  ". Main Results": "Our results, presented in Tab. 1, highlight the absolute ad-vantage of Hydra-MDP over the baseline. In our explorationof different planning vocabularies , utilizing a larger vo-cabulary V8192 demonstrates improvements across differentmethods. Furthermore, non-differentiable post-processingyields fewer performance gains than our framework, whileweighted confidence enhances the performance comprehen-sively. To ablate the effect of different learning targets, thecontinuous metric EP (Ego Progress) is not considered inearly experiments and we attempt the distillation of the over-all PDM score. Nonetheless, the irregular distribution of thePDM score incurs performance degradation, which suggeststhe necessity of our multi-target learning paradigm. In thefinal version of Hydra-MDP-V8192-W-EP, the distillation ofEP can improve the corresponding metric.",
  ". Scaling Up and Model Ensembling": "Previous literature suggests larger backbones only leadto minor improvements in planning performance. Neverthe-less, we further demonstrate the scalability of our modelwith larger backbones. Tab. 2 shows three best-performingversions of Hydra-MDP with ViT-L and V2-99 as the image backbone. For the final submission, we use theensembled sub-scores of these three models for inference. Sourav Biswas, Sergio Casas, Quinlan Sykora, Ben Agro,Abbas Sadat, and Raquel Urtasun. Quad: Query-based in-terpretable neural motion planning for autonomous driving.arXiv preprint arXiv:2404.01486, 2024. 2 Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye KitFong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom,and Sammy Omari. nuplan: A closed-loop ml-based plan-ning benchmark for autonomous vehicles. arXiv preprintarXiv:2106.11810, 2021. 3 Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye KitFong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom,and Sammy Omari. nuplan: A closed-loop ml-based plan-ning benchmark for autonomous vehicles. arXiv preprintarXiv:2106.11810, 2021. 3 Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, QingXu, Qian Zhang, Chang Huang, Wenyu Liu, and XinggangWang. Vadv2: End-to-end vectorized autonomous drivingvia probabilistic planning. arXiv preprint arXiv:2402.13243,2024. 1, 2, 3, 4 Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, ZehaoYu, Katrin Renz, and Andreas Geiger. Transfuser: Imita-tion with transformer-based sensor fusion for autonomousdriving. IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022. 3, 4",
  "Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xin-long Wang, and Yue Cao. Eva-02: A visual representation forneon genesis. arXiv preprint arXiv:2303.11331, 2023. 4": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 4 Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, WenhaiWang, et al. Planning-oriented autonomous driving. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1785317862, 2023. 1, 2, 4 Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen,Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, andXinggang Wang. Vad: Vectorized scene representation for ef-ficient autonomous driving. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 83408350, 2023. 1, 2",
  "Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, TongLu, and Jose M Alvarez.Is ego status all you need foropen-loop end-to-end autonomous driving? arXiv preprintarXiv:2312.03031, 2023. 1, 2, 3": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings, PartV 13, pages 740755. Springer, 2014. 4 Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, andAdrien Gaidon. Is pseudo-lidar needed for monocular 3dobject detection? In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 31423152,2021. 4 Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, GangYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: Alarge-scale, high-quality dataset for object detection. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 84308439, 2019. 4"
}