{
  "Abstract": "We focus on the problem of recognising the end state ofan action in an image, which is critical for understandingwhat action is performed and in which manner. We studythis focusing on the task of predicting the coarseness of acut, i.e., deciding whether an object was cut coarselyor finely. No dataset with these annotated end states isavailable, so we propose an augmentation method to syn-thesise training data. We apply this method to cutting ac-tions extracted from an existing action recognition dataset.Our method is object agnostic, i.e., it presupposes the lo-cation of the object but not its identity. Starting from lessthan a hundred images of a whole object, we can generateseveral thousands images simulating visually diverse cutsof different coarseness. We use our synthetic data to traina model based on UNet and test it on real images show-ing coarsely/finely cut objects. Results demonstrate that themodel successfully recognises the end state of the cuttingaction despite the domain gap between training and testing,and that the model generalises well to unseen objects.",
  ". Introduction": "Action recognition is central to understanding the vi-sual world. When we observe people performing a task(e.g., cooking dinner), we are able to decompose the task interms of discrete actions (e.g., boiling water, cooking pasta).AI systems that understand videos face the same problemand in response to this, a large literature on action recog-nition was sprung up . A system recognising actionsneeds to identify the people and objects involved in the ac-tion. Crucially, most actions are characterised by the statechange of an object. An example is the cutting action: in-dependent of which object this action is applied to, it willresult in the object being in smaller parts than before. More-over, the manner expressed by an adverb in whichan action is performed is crucial to understanding it. Forinstance, to cut garlic finely we would perform a different",
  "Coarsely/Finely": ". Summary of our work. We aim to recognise the end stateof an action, e.g., whether an object is cut coarsely or finely. Weassume no labels and propose an object-agnostic image augmen-tation method to synthesise training data. Our model successfullylearns from this synthetic data, as we show by testing on real im-ages and videos, including for unseen objects. operation (e.g., we would mince it with a knife) comparedto what we would do to cut it coarsely (e.g., we might justsplit it with our hands). Recognising object end states as theresult of the way an action is performed is thus important toenable systems to understand more about the action itself.This is a hard endeavour because even for a single actionthere are a large number of objects that the action can beapplied to. Not only do these objects vary visually, the endstate will also look differently depending on the type of ob-ject and the manner of the action. For example, a finely cutcarrot would typically be sliced in small strips, whereas afinely cut clove of garlic would be minced.In this paper, we present a case study to recognise theend states of one action, cutting, characterised by the mainmanner in which it can be performed: coarsely and finely.To focus on the end state recognition problem we do notwork on videos and assume only an image depicting theend of the action is given. This allows us to isolate the task",
  "arXiv:2405.07723v1 [cs.CV] 13 May 2024": "of discerning a coarse cut from a fine cut, without havingto worry about video-related issues such as motion, findingthe object in a video, etc. We propose a method to generatetraining data to address the limited availability of datasetslabelling coarsely/finely cut objects in images. Using dataaugmentation, we generate a large, high-quality dataset ofsynthetic images (VOST-AUG) on which we then train ourmodel for end state recognition. This is illustrated in Fig-ure 1. Our augmentation method starts with an image of awhole object and a mask segmenting the object. The ap-proach is object agnostic in that it does not need to knowwhat object is in the picture, only where it is. We deviseseveral ways to control the simulated coarseness of a cut,which enables us to generate numerous diverse images froma single source. Starting from only 184 images, we gener-ate 90,809 images simulating objects being cut in realisticways with different coarseness. We also propose a modelbased on a UNet Encoder-Decoder architecture to takefull advantage of our data. Our model is able to achieve0.856 Mean Average Precision (MAP) on unseen objects, a4% improvement over the closest baseline.Our training data is synthetically generated, so it is cru-cial to test whether the model we propose is able to gener-alise to real images and videos. Our experiments demon-strate that this is the case, both for an image test set thatwe collected and annotated (COFICUT) and for the existingvideo dataset Adverbs in Recipes . In both cases, ourmodel outperforms an existing adverb recognition model. Furthermore, we show that our approach beats a su-pervised model trained on a portion of COFICUT.To summarise: (i) We focus on the problem of recog-nising the end state of an action, which is critical for bothaction and manner (adverb) recognition. (ii) We propose anobject-agnostic augmentation method to synthesise trainingdata for this task. (iii) We present a model based on UNetand train it on our synthetic data. (iv) To evaluate the ef-fectiveness of the synthetic images, we collect a small testset of real images showing coarsely/finely cut objects. Bothon this test set and on an existing video dataset, our modelachieves good performance, even for unseen objects.",
  ". Related Work": "Adverb Recognition in VideosThe closest line of re-search to action end state recognition is understanding ad-verbs in videos , where models learn torecognise the manner in which actions are performed ina video.In some cases this includes end states, as incut coarsely/finely. The approaches of arefully-supervised, while proposes a method that assignspseudo-labels to the training videos based on the modelpredictions. However, this still assumes adverb labels areavailable, since pseudo-labels are assigned from the set ofclasses in a given dataset. In existing datasets for adverb recognition videos are loosely trimmed and oftennoisy, without a ground truth localising which frames showthe object. This means that learning action end states fromsuch datasets would be difficult. In contrast, we generatetraining images via augmentation without action, adverb, orobject labels. Our model learns to recognise the end stateof an action from augmented images in a granular way, i.e.,without splitting images into adverb categories. Neverthe-less, we show that our model outperforms the adverb recog-nition model presented in , including on videos fromthe Adverbs in Recipes dataset . Object Attributes in ImagesOur task also overlaps withthe problem of predicting attributes in images , with an important distinction: inobject attribute discovery images are typically grouped ina single category (e.g., tomato), and the goal is to organisethe input group of images into distinct states or attributes(e.g., ripe, raw, peeled, etc). We instead start from an un-structured group of objects and aim to recognise a changein a visual attribute: the coarseness resulting from a cut.In other cases object attribute discovery is addressed froma zero-shot compositional learning perspective, which is adistinct problem compared to action end state recognition.Nevertheless, for completeness we also adapt an attributediscovery model when comparing to state-of-the-artwork. We note that the popular MIT-States annotatesadjectives including cut, sliced, peeled, chopped and es-pecially thin/thick. However in this dataset thin/thickdo not necessarily correspond to coarsely/finely cut, i.e.,there are objects such as sauce, cloud, wall, book, etc an-notated with thin/thick. For this reason this dataset is nota suitable resource for our problem. Image AugmentationThe success of deep learning onimage tasks is in good part due to image augmentation tech-niques such as cropping, rotation, colour and perspectivemodifications, etc . Indeed, thanks to these tech-niques we can expand the visual and semantic diversity ofthe training data to prevent models from overfitting andenhance their generalisability. In this work we propose amethod to augment images, however our method is tailoredto synthesise training data from the scratch rather than aug-menting an existing training dataset.",
  ". Probing Existing Methods": "In this section we will try to establish how good current re-trieval systems are at telling if an object was cut coarsely orfinely. We search for food images on Microsoft Bing usingthe query {coarsely, finely} cut o, where o is one of 27 ob-jects such as carrot, garlic, tomato (see Section C for thefull list). We take the top 100 retrieved images, drop dupli-cates and inspect each image to establish how many imageswere incorrectly retrieved, i.e., showing a coarse cut whensearching for a fine cut, and vice-versa. Amongst 1,869",
  ". Trying to generate images of coarsely/finely cut objects with InstructPix2Pix . Text indicates the prompts used": "images, we found that 42.5% were incorrectly retrieved.This high percentage suggests that retrieval models strug-gle to distinguish images in these two categories. We donot have internal access to the retrieval system employed byMicrosoft Bing, i.e., we do not know for sure whether thesearch engine uses vision-based text-image retrieval mod-els. If this is the case, then we can ascribe the relatively poorcoarse/fine retrieval performance to the fact that text-imageretrieval models are typically optimised to distinguish ob-jects into broad classes rather than fine-grained categories.We believe the main reasons for this are the lack of exten-sive fine-grained labels and the fact that models need tolearn beyond the visual appearance of an object, i.e., theyneed to be able to generalise to recognise coarseness acrossvisually distinct objects. Generative models are often used in low data regimes tosynthesise new images with the desired label. We thus ex-perimented with a generative model to synthesise images ofcoarsely/finely cut objects. We tested InstructPix2Pix ,which uses GPT-3 and Stable Diffusion to edit im-ages based on a text prompt. For testing, we used a few im-ages from EPIC Kitchens , asking the model to replace avisible object with a finely/coarsely cut version of the sameobject. shows some examples from this experi-ment (first two columns). The model generates mostly plau-sible images, however it just replaces the prompted objectwith a newly generated version of the same object, ignor-ing the adverb in the prompt. InstructPix2Pix was trainedwith hand-made transformation prompts, which means ourtest instructions are too distinct from the prompts the modelwas trained on. We further probe this with simpler prompts,asking the model to only replace an object with another one.We still see that the model fails (last two columns) despitethe easier prompts. This confirms that the model has notbeen trained to cover our domain of interest sufficiently. Weshow more examples in Section D. From this Section we conclude that current retrievalmethods struggle to differentiate a coarsely cut object froma finely cut one, and that current generative models can-not reliably synthesise images of coarsely/finely cut objects.We therefore propose an image augmentation method thatmakes it possible to generate synthetic images of coarselyand finely cut objects. We show that this synthetic data can",
  ". Augmenting to Simulate Cuts": "Let us assume we are given an image depicting an object inits whole state and a mask segmenting the object. Our goalis to generate several images depicting the object as if itwas cut at different coarseness levels, i.e., from coarsely tofinely. We augment the image to achieve this. Specifically,we first remove the object from the image, which we theninpaint to fill the hole left by the removed object using .Next we break the object to simulate the result of a cut-ting action, and overlay the split parts of the object onto theinpainted image to obtain a picture where the object is cut. illustrates our augmentation method in detail.To break the object, we start sampling n points from themask. The sampled points act as seeding points to segmentobject regions, which are obtained by grouping pixels thatare closest to one of the n seed points. This is how Voronoidiagrams are built, with the important difference that pointsare not random but sampled in a way that simulates dif-ferent human cuts. Specifically, we devise four samplingstrategies: grid: we sample uniformly both horizontally andvertically, which simulates an object being cut in squares orcubes; horizontally/vertically: points are sampled only hor-izontally or vertically, which simulates objects being cut invertical or horizontal strips; diagonally, where points aresampled along the main or secondary diagonal of the mask,which also simulates objects cut in strips but with an angle(see Step 1 in ). Points are evenly spaced initially,however we add random noise to each seeding point to geta more natural looking cut. We next move object regions bya few pixels to break the object (Step 2 in ), se-lecting a reference point and pushing each region along theline connecting the region to the reference point. To gener-ate more natural and diverse images, each region is shiftedby a number of pixels randomly sampled within an inter-val. Finally, the moved object regions are overlaid onto theinpainted image without object (Step 3 in ). shows a few synthetic images illustrating howthe parameters of our augmentation method affect the re-",
  "Step 3: overlay new objectregions onto image w/o object": ". Our augmentation method to transform whole objects into cut objects. Given an image and a mask segmenting the object, we firstremove the object and inpaint the image to fill the resulting hole (image w/o object, bottom left). We then split the object into regions (Step1). For this we sample n seeding points (nine in this example, indicated by circles) and group object pixels into regions based on theirdistance to each point, as in a Voronoi diagram. We devise four sampling strategies which affect the topology of the regions and simulatedifferent cut types. We then break regions given a reference point (Step 2), shown as a red dot, i.e., we push each region away from thereference point along the line connecting the region and the point. Lastly (Step 3), we overlay the new regions onto the image w/o object toobtain the final augmented image. We show four examples with reference point (centre, middle) and each of the four sampling strategies.",
  "Noise added to the seeding points (in pixels)": ". Illustrating how the parameters of our augmentation affect the output image. The number of seeding points controls the coarsenessof the simulated cut, with fewer/more points corresponding to a coarser/finer cut (left). To obtain more diversified and realistic images wepush regions by a random number of pixels sampled within an interval (centre) and add noise to the seeding points (right). sulting image. The most important parameter is the numberof seeding points, which controls the coarseness of the cut(fewer/more points correspond to a coarser/finer cut). Therandom region movement and the seeding point noise en-sure that images are more diverse and natural-looking. In-terestingly, these two parameters also affect the perceivedroughness of a cut, i.e., a greater seeding point noise and agreater region movement will make the cut look rougher ormore haphazard. Semantically, the concepts of roughnessand coarseness overlap, so it would be difficult and some-what arbitrary to label the augmented images as coarse orfine based on one or multiple augmentation parameters. Forthis reason we do not label images into categories. We latershow that the difference between an original and augmentedimage provides a proxy measure to gauge coarseness.",
  ". The VOST-AUG Dataset": "We now provide details about how we use augmentation togenerate a dataset for this work. We are interested in ex-ploring the potential of a small-scale but high-quality setof annotated images, i.e., we would like to see whether itis possible to train a model to recognise a coarse or finecut starting from a small set of good images. With thispremise, the Video Object Segmentation under Transfor- Original images184Objects41Augmented images90,809Objects seen in training30Avg. aug. per image493Objects unseen in training11Training orig. images96Testing orig. images84Training aug. images47,395Testing aug. images43,414",
  ". Summary of the VOST-AUG dataset. Starting from only184 images we generate 90,809 augmentations showing objectscut at different coarseness scales": "mations (VOST) dataset is a good resource, as it fo-cuses on actions that significantly transform an object andoffers high-quality manual object masks for a few videosfrom EPIC Kitchens and Ego4D . Furthermore, ob-jects are typically well visible in these datasets thanks to theegocentric viewpoint. VOST annotates 702 videos compris-ing different actions such as cut, squeeze, paint, etc. Weselect only videos labelled with the verb cut, obtaining184 videos showing 41 different objects. Video segmentsare well trimmed in EPIC Kitchens and Ego4D, thus we as-sume that the first frame in each segment contains the objectin its whole state and select the first frame of each segmentto build our set of images to augment.1",
  "D =": ". Our model to predict the coarseness of a cut. The model adopts a UNet architecture, where the Encoder bottleneck features z areoptimised in two ways. We use an MLP to predict coarseness given z with the L1 loss, using c as target. To learn a stronger z, the UNetdecoder adds an auxiliary segmentation task, where we use the augmented object mask as target. The decoder is used only during training.For inference we employ only the Encoder and the MLP output to predict the coarseness of a test image. As detailed before, there are a few parameters involvedin our augmentation method. For this work we augmentimages taking all combinations of the following parametervalues: number of seeding points: 2, 3, 5, 10, 20, 30, 40,50; seeding points sampling: diagonal (main), diagonal(secondary), grid, horizontal, vertical; region move-ment intervals: , , ; seeding points noise:0, 5, 10, 20, 50. For each combination we sample a randomreference point among the nine illustrated in . Withthese combinations we generated in total 90,809 augmentedimages, starting from only 184 original images. On averagethere are 493 augmented images per original image (someaugmentations are rejected if object regions are pushed out-side the image bounds, which can happen if the object isnear an edge). We split the augmented images in a 70/30ratio for training/testing, where all augmented images froma given source are either in the train or the test split. Wecall the set of augmented images the VOST-AUG dataset. provides a summary of our dataset. We show moreexamples in Section B.",
  ". Model": "As discussed in , the coarseness of a cut is mainlycontrolled by the number of parts a whole object is cut into,however other factors such as the distance between partsand the regularity of their shape also influence the perceivedcoarseness. We thus design the model based on the differ-ence between an augmented image and its original source:visually, an augmented image will change less/more if thecut is coarser/finer, as we show in .To quantify this, let Ma and Mo be the 2D binary maskssegmenting the object in an augmented image Ia and itsoriginal source Io. Let D = |Ma Mo| be the binary ma-trix obtained taking the absolute value of the pixel-wise dif-ference between the two masks. We can measure how much",
  "c(Ma, Mo) =D(Ma Mo) = |Ma Mo|(Ma Mo)(1)": "where the denominator normalises the difference between0 and 1 and ensures that c is independent of the size of theobject. Values of c closer to 0/1 indicate a small/large differ-ence between the augmented and the original image, whichin turn correspond to a coarser/finer cut.With the above definition, we can now introduce ourmodel to learn c from Ia to discern the coarseness of acut. In principle, a model trained with a regression objectivesuch as the L1 loss could be sufficient for this task. How-ever, as we will show in , it is hard for a model tosolve this task without extra guidance due to the subtle dif-ferences between the numerous images augmented from asingle source. We thus propose an Encoder-Decoder modelbased on UNet . UNet was designed for medical imag-ing segmentation, where small-scale details are crucial, thusit is particularly suited for our problem. Our model is de-picted in .The Encoder f receives in input anaugmented image Ia and outputs the bottleneck featuresf(Ia) = z. We optimise z in two ways: firstly, we feedz to an MLP g which outputs a scalar, and use the L1 lossto learn c: LL1 = |(g(z)) c|, where is the sigmoidfunction2. The decoder learns a segmentation mask fromthe bottleneck features z via skip connections with the En-coder. In our case, the output h(z) is a one-channel imagewith the same shape as the input. We optimise h(z) with theDice loss , using the augmented object mask as target:",
  "2z has shape (2048, u, v), we average along dimensions u and v beforefeeding it to the MLP": "where is a small constant for numerical stability. The com-bined loss to train our model is the sum of the two losseswith equal weight, i.e. L = LL1 + LDice. The purposeof the Decoder is to provide an auxiliary segmentation taskthat strengthens the Encoder representation. As is typicalwith Encoder-Decoder architectures, the Encoder must gen-erate a high-quality representation for the Decoder to effec-tively solve the dense segmentation task. In other words,the Decoder and the segmentation task provide the extraguidance to focus on nuanced differences and learn a bet-ter representation for our main task, the coarseness cut es-timation. We also use the original images and masks fortraining. In this case c = 0, whereas the target for LDice isMo instead of Ma. We use the Decoder only during train-ing. For inference we only employ the Encoder f and theMLP g and use the output (g(f(x))) to predict the coarse-ness of the cut in the image x, where values closer to 0/1indicate a coarse/fine cut as per cs definition. Our modelis able to learn the differences that define the coarseness ofa cut by seeing only a single image at the time rather thanboth the original and the augmented image. This is advan-tageous as we do not need a reference image for testing.Importantly, our model is object agnostic, so it does not re-quire object labels and does not need an object mask duringinference, which makes the model more useful in a real-world setting. We will evaluate the model on real-world(i.e., non-augmented) data in .1.",
  ". Experiments": "Implementation DetailsWe employ ResNet50 pre-trained on ImageNet as our backbone for all experimentsand baselines. Models are trained with the ADAM opti-miser for 300 epochs with learning rate 1e 4, weightdecay 5e 5, batch size 64, dropout 0.1 and no batch nor-malisation. The MLP in our model has one hidden layer.Input images are resized to 224 224. All experiments areconducted on a single 12GB NVIDIA GeForce RTX 3060. Evaluation MetricWe report Mean Average Precision(MAP) with macro average (the two classes have equalweight). We report MAP globally as well as for seen/unseenobjects (except on AIR, where we do not have object la-bels). For evaluation on VOST-AUG, we group images aug-mented from the same source and assign them coarse/finelabels based on the median value of c. To clarify, let IA =(Iia, i = 1 . . . N) be the sequence of N images augmentedfrom the same source Io, and let C = (c(M ia, Mo), i =1 . . . N) be the sequence containing the c values obtainedfrom the corresponding augmented masks (see Equation 1).We label each augmented image as follows:",
  "where C denotes the median of C": "BaselinesTo the best of our knowledge no prior work hasfocused on our problem with this setting. The closest lineof work is adverb recognition in video . We com-pare against , who propose two methods to recogniseadverbs termed CLS and REG . We adapt this modelas follows, using the same backbone we use for our model.For CLS, we label training images as coarse/fine as we dofor testing on VOST-AUG (see Equation 3). CLS is then astandard classification baseline where the model is trainedwith binary cross entropy (BCE), i.e., we optimise (g(z))with the BCE loss (see ). We also train the CLSmodel by splitting images based on the number of seedingpoints instead of the c value, i.e., in Equation 3 we replaceC with S = (si, i = 1 . . . N) and c(M ia, Mo) with si, wheresi is the number of seeding points used to generate the i-thaugmented image. Test images in VOST-AUG are still splitas in Equation 3 to compare all models equally.For REG in verb-adverb video-text embeddings areused to build a regression target. This is sensible when verbsvary, i.e., when there are samples annotated with differentverbs for a given adverb. This is not the case in our settingas we only have one verb (cut). We thus adapt REG by usingthe c values as regression target. This is essentially the sameas training our model without the Decoder and the segmen-tation task, so REG serves also as an ablation study for ourfull model. All models are trained on VOST-AUG. As weonly have two classes, we also provide a random baseline toprovide a lower bound. In this case the mean average pre-cision equals the support size of the positive class (testingimages with the fine label).We also adapt CANet-CZSL , a model for composi-tional zero-shot attribute learning. We fine-tune the modelpre-trained on MIT-States , training the model to recog-nise two attributes: coarse and fine.",
  ". Datasets": "COFICUTWe collect a set of food images from Mi-crosoft Bing.We start querying {coarsely, finely} cuto, where o is an object from the list of objects in VOST-AUG, labelling each image with either coarse or finebased on the query. We take the top 100 retrieved images.We drop duplicates and manually review all images dis-carding irrelevant results, adjusting their labels to ensurethat each image is correctly annotated (as shown in Sec-",
  ". Samples from COFICUT, the dataset of coarsely/finely cut food images we collect for evaluation": "tion 3, the retrieved images were often relevant to the op-posite adverb). We remove objects altogether when therewas no visible difference between the coarse and fine im-ages. After reviewing, we retain 1,869 images (1,211 la-belled as finely and 658 labelled as coarsely) showing27 different objects (of which 8 are not seen in training).We name this dataset COFICUT (COarse-FIne CUT FoodImages), which is summarised in . This dataset isused only for evaluation. Despite its small scale, imageshave different viewpoints and style than those seen in train-ing, i.e., in training images are all from a first-person pointof view (PoV), whereas in testing they are mostly from athird-person PoV. Training images are daily-life captures,whereas COFICUT images are a mix of product pictures,still frames from vlogs or recipe pictures, with very dif-ferent lighting and style, as illustrated in . Aboveall, training images are synthetic augmentations, whereastest images are real examples of cut objects.For thesereasons, we believe COFICUT is a challenging bench-mark. COFICUT, VOST-AUG and our code are available atgithub.com/dmoltisanti/coficut-cvprw24. Other DatasetsWe also evaluate models on the test splitof the VOST-AUG dataset and the video dataset Adverbs inRecipes (AIR) . As we do not have real binary labelsfor VOST-AUG, evaluation on VOST-AUG should be seenmore as a sanity check rather than a benchmark for com-parison. AIR annotates 10 adverbs in instructional videos.We select videos labelled with either coarsely or finelyand one of the following verbs: chop, cut, mince, grind,grate, for a total of 992 videos. Like in COFICUT, the PoVin AIR is different from that in VOST-AUG. Furthermore,the nature of the videos (instructional) introduces additionaldiversity, e.g., people explaining their actions are often vis-ible, and videos contain jump cuts and irrelevant content.For this reason, evaluation on AIR is particularly challeng-ing as the models we test are image-based and there is noground truth localising objects temporally. To test a videoin AIR we sample two frames per second and rank the pre-dictions obtained for each frame, aggregating the scores bytaking the average of the top 5% scores.",
  "ModelAllSeenUnseenAllSeenUnseenAll": "Random0.6480.6050.7590.5000.5000.5000.613CLS 0.6920.6840.7230.6250.6310.5890.623CLSs 0.6600.6260.7420.6590.6690.6040.619REG 0.7220.7020.7780.5750.5840.5380.621CANet 0.7100.6860.8110.4920.4870.5350.617Ours0.7770.7410.8560.5610.5640.5560.632 . Results obtained training models on VOST-AUG. The re-ported metric is MAP (Mean Average Precision) with macro aver-aging, where the two classes have equal weight. We report the per-formance of a random baseline which is equal to the support sizeof the positive class (the fine class). All/Seen/Unseen refersto performance evaluated respectively on all images and imagesshowing objects seen/unseen in training.",
  "ModelTraining datasetAllSeenUnseen": "Random-0.648 0.0000.605 0.0000.759 0.000BCECOFICUT0.447 0.0260.447 0.026-CLS VOST-AUG0.693 0.0400.688 0.0450.727 0.091CLSs VOST-AUG0.665 0.0310.633 0.0300.751 0.083REG VOST-AUG0.725 0.0170.706 0.0180.782 0.053CANet VOST-AUG0.713 0.0260.690 0.0300.811 0.061OursVOST-AUG0.779 0.0050.744 0.0250.856 0.039 . Results obtained with 5-fold cross validation on COFI-CUT. The reported metric is mean std MAP (classes have equalweight). Models trained on VOST-AUG were only tested on thefive different folds, while BCE is a classification baseline where amodel with the same backbone as the others is trained using thelabels available on COFICUT.",
  ". Results": "compares the performance of the models trainedon VOST-AUG and tested on COFICUT, VOST-AUG, andAIR. We note that all models surpass the random base-line on all datasets (except CLS and CANet on somemetrics and datasets), which validates our augmentationmethod: models can tell a coarsely cut object from a finelycut one after being trained on synthetic images withoutcoarse/fine labels. Recall from that we generatedVOST-AUGs training set from only 96 original images. We highlight that COFICUT is the most appropriatebenchmark for this task as it collects manually reviewed realimages of cut objects with a significant visual domain gap.Despite such gap, the diversity of our augmented images al-lows us to successfully train a model to recognise coarse-ness in out-of-domain images.In particular, our modelachieves the best results by a large margin. This is thanks tothe auxiliary task introduced with the UNet decoder, whichhelps the backbone to focus on the minute details that dis-tinguish the coarseness of a cut. This is evident compar-ing the regression model (REG) with our model, since REGis essentially an ablation of our model where we discardthe Decoder and the segmentation task. Our model is bet-ter than REG on the realistic datasets, COFICUT and AIR.This validates the idea of adding the extra task to provideauxiliary guidance. The performance gain for unseen ob-jects further highlights the ability of our model (which isobject-agnostic) to generalise well despite the visual gap.On VOST-AUG we note that CLS (training images splitaccording to c) and CLSs (split according to seeding points)achieve the best performance. This is not surprising as themodel is trained to separate images in the same (CLS) orsimilar (CLSs) way as they are split for testing. However, onthe remaining datasets both CLS variants rank lowest. Per-formance on COFICUT unseen objects is even lower thanthe random baseline, which indicates that the model strug-gles to generalise. This suggests that splitting images intotwo classes in our setting is a sub-optimal choice since wehave a continuum of simulated cuts ranging from very thinto very coarse, without a neat separation into two classes.We also note that CANet achieves decent results onCOFICUT, but performs worse than the random baseline onVOST-AUG on the all/seen metrics. As mentioned before,attribute learning is a different task, so it is difficult for themodel to perform well in our distinct setting.On AIR we observe that performance is poor across allmodels and closer to the random baseline. This is due to thefact that AIR is a video dataset, so without a ground truth lo-calising the object it is difficult for any model to effectivelypredict the coarseness of the object shown in the video. Training on COFICUTWe now check whether it wouldbe possible to successfully train a binary classifier on COFI-CUT. We train the same backbone employed for the othermodels with binary cross entropy, using the labels availableon COFICUT. Given its small size, we conduct this experi-ment with 5-fold cross validation, comparing against mod-els trained on VOST-AUG by testing them on each fold aswell. From we see that the model trained on COFI-CUT (BCE in the Table) severely under-performs, withresults well below the random baseline (there is no unseenMAP for BCE since all objects are now seen in training).This was expected as COFICUT contains in total 1,869 im- ages, so the model overfits to the training set. However, thisalso shows that coarseness classification is not a trivial prob-lem and that large training datasets are necessary. Instead ofmanually annotating images, our augmentation method al-lows to automatically generate a high-quality, large trainingdataset that models can successfully learn from. We alsonote that results with our methods are more robust as theyexhibit a lower variation (MAP std is lowest).",
  ". Conclusion": "We addressed the problem of recognising the end state ofan action expressed by the manner in which it is performed.We explore this focusing on the cutting action, proposingan approach to detect whether an object is cut coarsely orfinely. We devise an effective image augmentation methodto simulate an object being cut at different coarseness lev-els and in different ways. Starting from only 96 images,we were able to synthesise 47,395 images to train modelsto successfully recognise whether an object is cut finely orcoarsely, without labels. Despite being trained on syntheticimages, models achieve good performance on real imagesand even on unseen objects. We also proposed a model tobetter leverage the data, boosting performance by over 4%. LimitationsOur augmentation method does not analysethe input scene, and as a result the synthesised image mightsometimes look unrealistic. Also, objects may be cut whilebeing held by hand mid-air, in which case the augmentationmethod produces an image of levitating object pieces, aswe show in Section B. Scene understanding or affordanceapproaches could be employed to alleviate this issue.We also need a good object mask to synthesise good images.Recent segmentation models (e.g. Segment Anything )could help lifting this requirement, though objects wouldstill need to be localised (e.g. providing a 2D point or a textprompt describing the object). Future DirectionsBeing object agnostic, our augmenta-tion method can be adapted to synthesise images where theend state of an object affects its geometry and shape. Forexample, our method could be extended to predict the com-pleteness of a cut, i.e., telling whether an object is fully orpartially cut. Other directions include adapting the augmen-tation method to synthesise videos and use the augmenteddata to instil knowledge in retrieval and generative models.",
  "A. Using Seeding Points as Regression Target": "In this Section we validate the introduction of the changeratio value (c, see Equation 1 in the paper) used to definecoarseness and as a regression target. As discussed in thepaper, the number of seeding points controls the coarsenessof the simulated cut, however other parameters involved inour augmentation method also affect the perceived coarse-ness. To show that using only the number of seeding pointsto measure coarseness is a sub-optimal choice, we train bothour model and REG using the number of seeding pointsas target for the L1 loss instead of c. compares re-sults obtained with the two regression targets. Results ob-tained with seeding points as regression target are worse forboth models on COFICUT and AIR, but better on VOST-AUG for our model. These results suggest that the alter-native regression target limits the ability of the model togeneralise to real images while overfitting to the trainingdomain. We conclude that using the change ratio to gaugecoarseness is thus a better way to train the model.",
  "B. VOST-AUG": "Illustrating c Values illustrates how c (see Equa-tion 1 in the paper) varies for a set of images synthesisedfrom an original image. Note how small/large values visu-ally correspond to a coarser/finer cut. Failure Cases illustrates examples where ouraugmentation method fails to synthesise realistic images.This happens mostly when objects are cut while held mid-air, which causes the split object to appear as though it lev-itates. In some cases objects regions are pushed over handsor other objects, which also simulates a less realistic image.As noted in the paper, these issues could be alleviated usingscene understanding or affordance models. More ExamplesFigures 9 shows more synthesised im-ages from VOST-AUG, together with the correspondingoriginal source (left-most column). To facilitate illustra-tion we crop images around the object. Our method works well in challenging conditions, e.g., when the original im-age shows more than one instance of the object or when theobject is held in hand (third row from the top). The aug-mentation method is able to generate good images regard-less of the object size and shape We note that the majorityof images simulates realistic cuts, though as the number ofseeding points increases (i.e., as the number of split partsincreases), images may tend to look more artificial. Thisis not a concern as the purpose of these images is to traina model, which we are able to do successfully as demon-strated in the paper. Seen and Unseen ObjectsThe objects seen during train-ing in the VOST-AUG train split are: aubergine, beef,bread, broccoli, butter, cake, carrot, chicken, chilli, cloth,courgette, cucumber, dough, garlic, ginger, gourd, guava,lettuce, mango, olive, onion, paper, pea, peach, pepper,potato, pumpkin, salad, tomato, vegetable. The unseenobjects are: asparagus, bacon, celery, corn, ham, herbs,ladyfinger, melon, mozzarella, spinach, spring onion.",
  "C. COFICUT": "The list of objects in COFICUT after reviewing is: aspara-gus, aubergine, bacon, beef, broccoli, butter, carrot, celery,chicken, corn, courgette, cucumber, garlic, ginger, gourd,guava, ham, lettuce, mango, melon, mozzarella, onion, pep-per, potato, pumpkin, spring onion, tomato.Amongstthese, the following were not seen during training: aspara-gus, bacon, celery, corn, ham, melon, mozzarella, springonion. shows more images from COFICUT (onecoarse/fine per object). Note the diversity of the images(point of view, lighting, style), especially compared to thetraining images from VOST-AUG, and how distinct eachobject looks in its coarse and fine states.",
  "D. More Examples from InstructPix2Pix": "shows more examples from our experiments withInstructPix2Pix . As seen in the paper, the model ignoresthe adverb specified in the prompt and fails to replace theindicated object with another one in a realistic way, oftenhallucinating the image. We speculate that the model reliesheavily on colour to ground the queried object to the image.We thus hypothesise that the model struggles to separatethe object when it has a similar colour to its surroundingelements. This is particularly visible in the bottom right ex-ample in , where the bread and the whole sceneshare a similar colour. Note how the model inpaints aspara-gus over the whole image, including the hands and arms ofthe subject, the chopping board and the cupboard.In many cases the model did not modify the input imageat all. We do not illustrate these cases here. We show in (middle row) that results are independent of the"
}