{
  "Abstract": "Operators devoid of multiplication, such as Shift andAdd, have gained prominence for their compatibility withhardware. However, neural networks (NNs) employing theseoperators typically exhibit lower accuracy compared to con-ventional NNs with identical structures. ShiftAddAug usescostly multiplication to augment efficient but less powerfulmultiplication-free operators, improving performance with-out any inference overhead. It puts a ShiftAdd tiny NN intoa large multiplicative model and encourages it to be trainedas a sub-model to obtain additional supervision. In orderto solve the weight discrepancy problem between hybridoperators, a new weight sharing method is proposed. Ad-ditionally, a novel two stage neural architecture search isused to obtain better augmentation effects for smaller butstronger multiplication-free tiny neural networks. The supe-riority of ShiftAddAug is validated through experiments inimage classification and semantic segmentation, consistentlydelivering noteworthy enhancements. Remarkably, it securesup to a 4.95% increase in accuracy on the CIFAR100 com-pared to its directly trained counterparts, even surpassingthe performance of multiplicative NNs.1",
  ". Introduction": "The application of deep neural networks (DNNs) onresource-constrained platforms is still limited due to theirhuge energy requirements and computational costs. To ob-tain a small model deployed on edge devices, the commonlyused techniques are pruning, quantization,and knowledge distillation. However, the NNs designedby the above works are all based on multiplication. The com-mon hardware design practice in digital signal processingtells that multiplication can be replaced by bit-wise shiftsand additions to achieve faster speed and lower en-ergy consumption. Introducing this idea into NNs design,DeepShift and AdderNet proposed ShiftConv opera-tor and AddConv operator respectively.",
  "Accepted by 2024 CVPR Workshop : Efficient Deep Learning forComputer Vision": "This paper takes one step further along the direction ofmultiplication-free neural networks, proposing a methodto augment tiny multiplication-free NNs by hybrid compu-tation, which significantly improves accuracy without anyinference overhead. Considering that multiplication-freeoperators cannot restore all the information from the orig-inal operator, tiny NNs employing ShiftAdd calculationsexhibit pronounced under-fitting. Drawing inspiration fromNetAug, ShiftAddAug chooses to build a larger hybridcomputing NN for training and sets the multiplication-freepart as the target model used in inference and deployment.The stronger multiplicative part as augmentation is expectedto push the target multiplication-free model to a better con-dition.In augmented training, the hybrid operators share weights,but because different operators have varying weight distri-butions, effective weights for multiplication may not suitshift or add operations. This led us to develop a strategy forheterogeneous weight sharing in augmentation.Furthermore, since NetAug limits augmentation to width,ShiftAddAug aims to extend this by exploring depth and op-erator change levels. Thus, we adopt a two-step neural archi-tecture search strategy to find highly efficient, multiplication-free tiny neural networks.ShiftAddAug is evaluated on MCU-level tiny mod-els.Compared to the multiplicative NNs, directlytrained multiplication-free NNs can obtain considerablespeed improvement (2.94 to 3.09) and energy saving(67.75%69.09%) at the cost of reduced accuracy. Shif-tAddAug consistently enhances accuracy(1.08%4.95%)while maintaining hardware efficiency. Our contributionscan be summarized as follows: For the multiplication-free tiny neural network, we pro-pose hybrid computing augmentation that leverages mul-tiplicative operators to enhance the target multiplication-free network. While maintaining the same model structure,yields a more expressive and ultra-efficient network.",
  ". Related Works": "Multiplication-Free NNs. To mitigate the high energyand time costs associated with multiplication, a key strategyinvolves employing hardware-friendly operators instead ofmultiplications. ShiftNet proposes a zero-parameter,zero-flop convolution. DeepShift retains the calculationmethod of original convolution, but replaces the multiplica-tion with bit-shift and bit-reversal. BNNs binarizethe weight or activation to build DNNs consisting of signchanges. AdderNet chooses to replace multiplica-tive convolution with less expensive addition, and designan efficient hardware implementation. ShiftAddNetcombines bit-shift and add, getting up to 196 energy sav-ings on hardware as shown in Tab. 1. ShiftAddVit putsthis idea into the vision transformer and performs hybridcomputing through mixture of experts.",
  "MULT.3.70.93.10.2ADD1.10.40.10.03SHIFT--0.130.024": "Network Augmentation. Research on tiny neural net-works are developing rapidly. Networks and optimizationtechniques designed for MCU have already appeared atpresent. Once-for-all proposes the ProgressiveShrinking and finds that the accuracy of the obtained modelis better than the counterpart trained directly. Inspired by thisresult, NetAug raises a point that tiny neural networksneed more capacity rather than regularization in training.Therefore, they chose a scheme that is the opposite of reg-ularization methods like Dropout: expand the modelwidth and let the large model lead the small model to achievebetter accuracy.Neural Architecture Search.NAS remarkably suc-ceeded in automating the creation of efficient NNarchitectures, enhancing accuracy while incor-porating hardware considerations like latencyand memory usage into the design process.NASalso extends its utility to exploring faster operatorimplementations and integrating network structures foroptimization, bringing designs closer to hardwarerequirements. ShiftAddNAS pioneered a search spacethat includes both multiplicative and multiplication-free op-erators.",
  ". Preliminaries": "Shift. The calculation for the shift operator parallels thatof standard linear or convolution operators using weight W,except that W is rounded to the nearest power of 2. Bit-shiftand bit-reversal techniques are employed to achieve calcula-tion results equivalent to those obtained through traditionalmethods as Equ. 1. Inputs are quantized before computationand dequantized upon obtaining the output.S = sign(W)P = round(log2(|W|))Y = X WqT = X(S 2P )T ,train.Y =",
  ". Hybrid Computing Augment": "ShiftAddAug goes a step further based on NetAug,using strong operators to augment weak operators.Taking an n-channel DepthWise Conv as an example,NetAug expands it by a factor of , resulting in a convolutionweight for n channels. During calculation, only the first nchannels are used for the target model, while the augmentedmodel employs all n channels. After trained as Equ. 3,the important weights within n channels are reorderedinto the first n, and only these n channels are exported fordeployment.As shown in , ShiftAddAug uses [0, n) chan-nels (target part) and [n, n) channels (augmented part)for different calculation methods. The target part will usemultiplication-free convolution (MFConv, ShiftConv orAddConv can be chosen) while multiplicative convolution(MConv, i.e. original Conv) is used as the augmented part.Because the channels of Conv are widened, the input ofeach convolution is also widened and can be conceptually",
  "Linear": "GateCat on channel TrainEval Mult-free OnlyHybrid Compute >>10>>2 0<<1 <<2 >>10>>3Mult.-free kerneltargetShiftConv -0.5 -1.1 +1 -0.30-1.2 +1 +0.2 -0.2 AddConv Mult. kernelaugmented * 0.5 * -0.3 * 1.1 0* 0.2* 2 * -1.20* 0.4 Mult. Conv reorderimportantweights . ShiftAddAug augments weak operators with stronger ones. In this framework, pink indicates the multiplication-free kernelsdesignated for the target model (either Shift or Add), while orange represents the multiplicative kernels of the augmented portion. Duringtraining, a wide weight is maintained, with the initial n channels processed in a multiplication-free manner and the subsequent channelsutilizing multiplicative operations. The weights of both are updated but only the multiplication-free part is exported for deployment.Therefore, the important weights in the wide weight will be reordered into the multiplication-free part. Obtained tiny models have higheraccuracy (up to 4.95%) than their directly trained counterparts. split into the target part Xt and the augmented part Xa, sodoes the output Yt, Ya. In ShiftAddAug, Xt and Yt mainlycarry information of MFConv, while XA and YA are ob-tained by original Conv.Here three types of operators commonly used to build tinyNNs are discussed: Convolution (Conv), DepthWise Con-volution (DWConv), and Fully Connected (FC) layer. Thehybrid computing augmentation for DWConv is the most in-tuitive: split the input into Xt and Xa, then use MFConv andMConv to calculate respectively and connect the obtained Ytand Ya in the channel dimension. For Conv, We use all inputX to get Ya through MConv. But to get Yt, we still need tosplit the input and calculate it separately, and finally add theresults. Since the FC layer is only used as the classificationhead, its output does not require augmentation. We dividethe input and use Linear and ShiftLinear to calculaterespectively, and add the results. If bias is used, it will bepreferentially bounded to multiplication-free operators.",
  ". Weight distribution of different convolution operatorsin MobileNetV2-w0.35. Inconsistent weight distribution leads todiscrepancy, making weight sharing difficult": "Dilemma. The important weights will be reordered tothe target part at each end of the training epoch (1 norm forimportance). This is a process of weight sharing and is thekey to effective augmentation.However, the weight distribution of the multiplication-free operator is inconsistent with the original Conv. It causesthe weight discrepancy, i.e. good weights in original Convmay not be good in MFConv. As shown in , the weightin the original Conv conforms to Gaussian distribution, whileShiftConv has spikes at some special values. The weight inAddConv conforms to the Laplace distribution. The weightin ShiftConv is the one of the original Conv plus a Laplacedistribution with a small variance.ShiftAddNas adds a penalty term to the loss function,guides the weight to conform to the same distribution. Itaffects the network to achieve its maximum performance.The Transformation Kernel they proposed also doesnt workon our approach since the loss diverges as Tab. 8. We arguethat their approach makes training unstable. This dilemmamotivated us to propose our heterogeneous weight sharingstrategy. sharedweight",
  "Memory": ". Weight remapping strategy creates mappings between dif-ferent weight distributions, making weight sharing workable. cpfgfor cumulative probability function (CPF) for Gaussian distributionand cpfl CPF for Laplace distribution Solution: heterogeneous weight sharing. To solve thedilemma above, we propose a new heterogeneous weightsharing strategy for the shift and add operators. This methodis based on original Conv and remap parameters to weightsof different distribution through a mapping function R().In this way, all weights in memory will be shared under theGaussian distribution, but will be remapped to an appropriatestate for calculation.When mapping the Gaussian distribution to the Laplacedistribution, we hope that the cumulative probability of theoriginal value and mapping result is the same. Firstly, cal-culate the cumulative probability of the original weight inGaussian. Then put the result in the percent point function ofLaplacian. The workflow is shown in . The mean andstandard deviation of the Gaussian can be calculated throughthe weights, but for the Laplace, these two values need to bedetermined through prior knowledge.",
  "(5)": "Where Wg is the weight in original Conv that conforms tothe Gaussian distribution, and Wl is the weight obtained bymapping that conforms to the Laplace distribution. FC is afully connected layer, which is previously trained and frozenin augmented training. We need this because the weightsdont fit the distribution perfectly. cpfg() is the cumulativeprobability function of Gaussian, ppfl() is the percentagepoint function of Laplace.",
  ". Neural Architecture Search": "To obtain SOTA multiplication-free model at the tinymodel size, a two-stage NAS is proposed.Based on the idea of augmentation, ShiftAddAug startsfrom a multiplicative SuperNet and cuts a deep SubNet fromit as depth-augmented NN. Then select some layers on theSubNet to form the tiny TargetNet for final use. The Target-Net should meet the pre-set hardware limitation. This setupallows the TargetNet to be a part of the SubNet, facilitatingjoint training with weight sharing as Equ. 3. Layers withinthe SubNet that are not selected for the TargetNet serve asa form of depth augmentation. Moreover, the layers usedfor deep augmentation are initially selected but graduallyphased out from the target network in training progresses.A new block mutation training is also proposed, whichtends to gradually transform multiplication operators intomultiplication-free states during training to make the trainingprocess more stable. The training process starts with allmultiplications, and the layers of the target network becomemultiplication-free one by one from shallow to deep. At theend of training, a completely multiplication-free TargetNetcan be obtained.While ShiftAddNas directly uses hybrid computingto train their SuperNet and directly cut SubNets that meetthe hardware requirements, we start from the multiplicativeSuperNet and split the search process into two steps. Themiddle step is used for augmented training, which is theunique part of ShiftAddAug.Combining the Width Augmentation and Expand Aug-mentation we used in section 3.2, we construct our searchspace for the augmentation part according to Tab. 2.We follow tinyNAS to build SuperNet and cut SubNet.Then use evolutionary search to search for subsequent steps.",
  "input": "... ... output SuperNetSubNetAugmented at Width & DepthGradually mutateTarget Net Evolution search a . Light orange block for MConv; pink block for MFConv; deep orange block for Depth Augmentation. (a).Search process: two-stagesearch. Find a SubNet as depth augmented one and then further cut out a tiny TargetNet on it for development. Start with multiplicationand gradually convert the TargetNet to multiplication-free during training. (b).Methods to augment. Augment Width: use MConv to widenthe MFConv channel; Augment Expand: increase expand ratio of depthwise separable convolution; Augment Depth: Augmented blocksfor depth. Only participate in training and will not be exported; Block Mutation: start with MConv, mutate the block into MFConv duringtraining. Training Details. The NNs are trained with batch size128 using 2 GPUs. The SGD optimizer is used with Nesterovmomentum 0.9 and weight decay 4e-5. By default, theMult. and Shift models are trained for 250 epochs and Addmodels are trained for 300 epochs. The initial learning rateis 0.05 and gradually decreases to 0 following the cosineschedule. Label smoothing is used with a factor of 0.1. ForShiftConv and ShiftLinear, the weight is quantized to 5bitPower-of-2 and the activation is quantized to 16bit. AddConvis calculated under 32bit.Hardware Performance.Since many works haveverified the efficiency of shift and add on proprietaryhardware, we follow their evaluation metrics.Hardware energy and latency are measured based on a sim-ulator of Eyeriss-like hardware accelerator, whichcalculates not only computational but also data movementenergy.",
  ". ShiftAddAug vs. Baseline": "WevalidateourmethodonMobileNetV2,MobileNetV3,MCUNet,ProxylessNAS,MobileNetV2-Tiny. ShiftAddAug provides consistentaccuracy improvements (average 2.82%) for ShiftConvaugmentation over the multiplicative baselines.ForAddConv augmentation, it improves the accuracy comparedwith direct training (average 1.59%). The resulting modelwill be faster (3.0 for Shift) and more energy-efficient (68.58% for Shift and 52.02% for Add) due to the useof hardware-friendly operators. As shown in Tab. 3, thesemultiplication-free operators usually hurt the performanceof the network.Changing all operators to Shift willcause 0.82% accuracy drop on average compared to themultiplication baseline. But after using our method, theaccuracy increased by 3.63% on average under the sameenergy cost. 2 In addition, our method achieves higher results than mul-tiplicative NetAug on some models (MobileNetV3:1.17%,MCUNet:1.44%, ProxylessNAS:1.54%). This means thatour method enables the multiplication-free operator to bestronger than those of the original operator.To verify the generality of our method, we also conductexperiments on more datasets. As shown in Tab. 4, ourmethod can achieve 0.89% to 4.28% accuracy improve-ments on different datasets. Hybrid computing augmentationworks better on smaller models and datasets with less clas-sification. On Flower102, MobileNetV2-w0.35 has 3.83%accuracy improvements with our method, while MCUNethas only 1.47%. This shows that a smaller model capac-ity can achieve a better effect on this dataset. The largerthe model, the smaller the gain brought by augmentation.The same phenomenon also occurs in CIFAR10. For biggerdatasets such as ImageNet, even if it is augmented, the capac-",
  "Loss becomes NaN when we use AddConv on MobileNetV3, bothdirect training and augmented training": ". ShiftAddAug vs. Multiplicative and directly trained Multiplication-free Baseline in terms of accuracy and efficiency on CIFAR100classification tasks. The results obtained by our method are bolded. The accuracy on the left is directly trained and the one on the right isobtained with augmentation. Base for Mult.; Shift for ShiftConv in DeepShift and Add for AddConv in AdderNet.",
  ". ShiftAddAug vs. SOTA Mult.-Free Models": "We compare ShiftAddAug over SOTA multiplication-freemodels, which are designed manually for tiny computingdevices, on CIFAR-10/100 to evaluate its effectiveness. Asshown in , the model structures we use are smaller andhave better energy performance. With ShiftAddAug, the ac-curacy still exceeds existing work. For DeepShift and Adder-Net, our method boosts 0.67% and 1.95% accuracy onCIFAR100 with 84.17% and 91.7% energy saving. Com-pared with the SOTA shift quantization method APoT,we achieve an improved accuracy of 3.8%. With the sameaccuracy on CIFAR10, our model saves 84.2% of the en-ergy compared with Deepshift, and 56.45% of the energycompared with AdderNet.",
  ". ShiftAddAug on Neural Architecture Search": "Based on hybrid computing augmentation, we intro-duce neural architecture search into our method to getstronger tiny neural networks.We conduct our experi-ments on Cifar-10/100 and compare them with the results ofShiftAddNAS which is better than multiplication-basedFBNet. As shown in Tab. 6, the multiplication-freemodel we obtained achieved higher accuracy (3.61%) thanShiftAddNas. For hybrid-computed models, we have to usea smaller input resolution (96 instead of 160) and larger mod-els. While the input resolution of ShiftAddNas is 32, thiswould give us 9 the number of calculations if we have thesame model architecture. Even so, we can still save 37.1%of calculations on average with similar accuracy.",
  ". Ablation Study": "Hybrid Computing Augment. In order to prove thathybrid computing works better, we add an experiment usingonly multiplication-free operators for augmentation. Weexert experiments based on NetAug, and replace all the original operators with Shift operators. The difference fromour method is that the Shift operator is also used in theaugmentation part, while our method uses multiplicationin it. As shown in Tab. 7, it yields an average accuracyimprovement of 1.02%.Then without using the heterogeneous weight sharing(HWS) method, just augmenting tiny NNs with the multi-plicative operator will cause 0.9% accuracy drop on averagedue to the weight tearing problem. However, the situationchanged after we applied for HWS. Compared with usingthe shift operator for augmentation, the accuracy increasedby 2.2%.Heterogeneous Weight Sharing. Since the help of HWSon training results has been discussed above, here we visual-ize the weight distributions of Conv layers in tiny NNs underthree scenarios, (a) w/o weight sharing; (b) heterogeneousweight sharing; (c) weight after remapped, as shown in . We consistently observe that the three operators exhibitdifferent weight distributions without weight sharing. Withour HWS, the saved weights are based on the original Convand conform to Gaussian distribution. After remapping, theweights can show different distribution states in Shift/AddConv calculations.Different methods for solving weight tearing are also com-pared. As shown in Tab. 8, both the ShiftAddNas methodand direct linear remapping will cause loss to diverge. Theadditional penalty has a larger impact on tiny NNs, causingaccuracy to decrease.Additionally, Tab. 9 shows why weight sharing is impor-tant, which also explains why we have to face the problemof heterogeneous weights. Tab. 10 tells that HWS is not aparameterized trick directly improving accuracy, but just acompensation in augmentation.Neural Architecture Search. Our neural architecturesearch approach is dependent on our proposed hybrid com-puting augmentation. And it can help the multiplication-freeoperator to be as strong as the original operator. Block aug-mentation and block mutation help us further improve theperformance of multiplication-free tiny NNs. As shownin Tab. 11, under similar energy consumption and latency,block augmentation improves accuracy by 0.5%, and blockmutation improves by 0.79%. Combining all methods, theaccuracy of the target model is increased by 1.22%.",
  ". Limitation": "Since we use additional multiplication structures for aug-mentation, training consumes more resources. This is con-sistent with NetAug.In terms of memory usage, which depends on the size ofthe augmented NN, ShiftAddAug usually doubles or triplesit compared with direct training. This makes ShiftAddAugnot suitable for on-device training. . ShiftAddAug vs. SOTA NAS method for hybrid operators in term of accuracy and efficiency on Cifar-10/100 classification tasks.ShiftAddAug can further improve the performance of the multiplication-less model.",
  ". Conclusion": "In this paper, ShiftAddAug is proposed for trainingmultiplication-free tiny neural networks, which can improveaccuracy without expanding the model size. Its achieved byputting the target multiplication-free tiny NN into a largermultiplicative NN to get auxiliary supervision. To relocateimportant weights into the target model, a novel hetero-geneous weight sharing strategy is used to approach theweight discrepancy caused by inconsistent weight distribu-tion. Based on the work above, a two stage neural architec-ture search is utilized to design more powerful models. Ex-tensive experiments on image classification and semantic seg-mentation task consistently demonstrate the effectiveness ofShiftAddAug. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.Food-101 mining discriminative components with randomforests. In European Conference on Computer Vision, pages446461, Cham, 2014. Springer International Publishing. 4",
  "Han Cai, Chuang Gan, Ji Lin, and song han. Network aug-mentation for tiny deep learning. In International Conferenceon Learning Representations, 2022. 1, 2": "Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, ChaoXu, Qi Tian, and Chang Xu. Addernet: Do we really needmultiplications in deep learning? In 2020 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 14651474, 2020. 1, 2 Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng,Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang,Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishna-murthy. Tvm: an automated end-to-end optimizing compilerfor deep learning. In Proceedings of the 13th USENIX Con-ference on Operating Systems Design and Implementation,page 579594, USA, 2018. USENIX Association. 2 Weijie Chen, Di Xie, Yuan Zhang, and Shiliang Pu. Allyou need is a few shifts: Designing efficient convolutionalneural networks for image classification. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2019. 2 Yu-Hsin Chen, Tushar Krishna, Joel S. Emer, and VivienneSze. Eyeriss: An energy-efficient reconfigurable acceleratorfor deep convolutional neural networks. IEEE Journal ofSolid-State Circuits, 52(1):127138, 2017. 5 Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Train-ing deep neural networks with weights and activations con-strained to +1 or -1, 2016. 2 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiFei-Fei. Imagenet: A large-scale hierarchical image database.In 2009 IEEE Conference on Computer Vision and PatternRecognition, pages 248255, 2009. 4 Mostafa Elhoushi, Zihao Chen, Farhan Shafiq, Ye Henry Tian,and Joey Yiwei Li. Deepshift: Towards multiplication-lessneural networks. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR)Workshops, pages 23592368, 2021. 1, 2 Mark Everingham, Luc van Gool, Christopher K. I. Williams,John Winn, and Andrew Zisserman. The pascal visual objectclasses (voc) challenge. International Journal of ComputerVision, 88(2):303338, 2010. 4, 6",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling theknowledge in a neural network, 2015. 1": "Andrew Howard, Mark Sandler, Bo Chen, Weijun Wang,Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasude-van, Yukun Zhu, Ruoming Pang, Hartwig Adam, and Quoc Le.Searching for mobilenetv3. In 2019 IEEE/CVF InternationalConference on Computer Vision (ICCV), pages 13141324,2019. 5 Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.3d object representations for fine-grained categorization. In2013 IEEE International Conference on Computer VisionWorkshops, pages 554561, 2013. 4",
  "Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efficient non-uniform discretization forneural networks. In International Conference on LearningRepresentations, 2020. 6": "Ji Lin, Wei-Ming Chen, Yujun Lin, john cohn, Chuang Gan,and Song Han. Mcunet: Tiny deep learning on iot devices. InAdvances in Neural Information Processing Systems, pages1171111722. Curran Associates, Inc., 2020. 2, 4, 5 Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and SongHan. Memory-efficient patch-based inference for tiny deeplearning.In Advances in Neural Information ProcessingSystems, pages 23462358. Curran Associates, Inc., 2021. 2",
  "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Dif-ferentiable architecture search. In International Conferenceon Learning Representations, 2019. 2": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,and Jan Kautz. Pruning convolutional neural networks forresource efficient inference. In International Conference onLearning Representations, 2017. 1 Maria-Elena Nilsback and Andrew Zisserman. Automatedflower classification over a large number of classes. In 2008Sixth Indian Conference on Computer Vision, Graphics &Image Processing, pages 722729, 2008. 4",
  "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,and Ali Farhadi. Xnor-net: Imagenet classification usingbinary convolutional neural networks, 2016. 2": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-moginov, and Liang-Chieh Chen. Mobilenetv2: Invertedresiduals and linear bottlenecks. In 2018 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition(CVPR),pages 45104520, 2018. 5 Huihong Shi, Haoran You, Yang Zhao, Zhongfeng Wang,and Yingyan Lin. Nasa: Neural architecture search and ac-celeration for hardware inspired hybrid networks. In 2022IEEE/ACM International Conference On Computer AidedDesign (ICCAD), pages 19, 2022. 2 Dehua Song, Yunhe Wang, Hanting Chen, Chang Xu, Chun-jing Xu, and Dacheng Tao. Addersr: Towards energy effi-cient image super-resolution. In 2021 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition(CVPR), pages1564315652, 2021. 2 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, IlyaSutskever, and Ruslan Salakhutdinov. Dropout: A simpleway to prevent neural networks from overfitting. J. Mach.Learn. Res., 15(1):19291958, 2014. 2 Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,Mark Sandler, Andrew Howard, and Quoc V. Le. Mnasnet:Platform-aware neural architecture search for mobile. In2019 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 28152823, 2019. 2"
}