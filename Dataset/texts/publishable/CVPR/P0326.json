{
  "Abstract": "The 3D Human Pose Estimation (3D HPE) task uses2D images or videos to predict human joint coordinates in3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of cou-pling accessible texts and naturally feasible knowledge ofhumans, missing out on valuable implicit supervision toguide the 3D HPE task. Moreover, previous efforts oftenstudy this task from the perspective of the whole humanbody, neglecting fine-grained guidance hidden in differentbody parts. To this end, we present a new Fine-GrainedPrompt-Driven Denoiser based on a diffusion model for 3DHPE, named FinePOSE. It consists of three core blocksenhancing the reverse process of the diffusion model: (1)Fine-grained Part-aware Prompt learning (FPP) block con-structs fine-grained part-aware prompts via coupling acces-sible texts and naturally feasible knowledge of body parts",
  "*Corresponding author": "with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block estab-lishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality.(3) Prompt-driven Timestamp Stylization (PTS) block inte-grates learned prompt embedding and temporal informationrelated to the noise level to enable adaptive adjustment ateach denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outper-forms state-of-the-art methods. We further extend FinePOSEto multi-human pose estimation. Achieving 34.3mm averageMPJPE on the EgoHumans dataset demonstrates the poten-tial of FinePOSE to deal with complex multi-human scenar-ios. Code is available at",
  "arXiv:2405.05216v1 [cs.CV] 8 May 2024": "body joints in 3D space. It is vital in various applications,including self-driving , sports analysis ,abnormal detection , and human-computer interaction. Considering the expensive computational costsof directly obtaining 3D human poses from 2D contents, 3DHPE is usually decomposed into two stages: 1) detecting 2Dkeypoints in images or videos , and 2) mapping2D keypoints to 3D human poses . In thiswork, we mainly focus on the second stage, estimating 3Dhuman poses given 2D keypoints.Existing monocular 3D HPE methods [4, 6, 10, 17 19, 27, 28, 35, 36, 43, 44, 47, 48, 52, 54, 59, 61] usuallyhave three challenges as follows: 1) Uncertainty: the depthambiguity inherently exists in the mapping from 2D skele-tons to 3D ones (one-to-many); 2) Complexity: flexiblehuman body structure, complex inter-joint relationships, anda high limb freedom degree lead to self-occlusion or rareand complicated poses; 3) Generalizability: current publiclyavailable 3D HPE datasets have limited action classes, andthus, the models trained on such data are prone to overfittingand difficult to generalize to more diverse action classes.To address these issues, we consider improving the 3DHPE model performance by enhancing the input information.We found that existing methods ignore accessible texts andnaturally feasible knowledge of humans while they promiseto provide the model with more guidance. We explicitlyutilize (1) the action class of human poses, (2) kinematicinformation speed, and (3) the way that different humanbody parts (e.g., person, head, body, arms, and legs) move inhuman activities to build fine-grained part-aware promptsfor the reconstruction task. Specifically, we incorporate afine-grained part-aware prompt learning mechanism into ourframework to drive 3D human pose estimation via vision-language pre-trained models. It is well known that textprompts play a crucial role in various downstream tasksfor vision-language pre-training models (e.g., CLIP ).However, manually designing prompt templates is expensiveand cannot ensure that the final prompt is optimal for the 3DHPE task. Thus, we create a new fine-grained part-awareprompt learning mechanism that adaptively learns modifiersfor different human body parts to precisely describe theirmovements from multiple granularities, including actionclass, speed, the whole person, and fine-grained human bodyparts. This new mechanism, coupled with diffusion models,possesses controllable high-quality generation capability,which is beneficial in addressing the challenges of the 3Dhuman pose estimation task.In this work, we propose a Fine-grained Prompt-drivenDenoiser (FinePOSE) based on diffusion models for 3Dhuman pose estimation, in , which is composed ofa fine-grained part-aware prompt learning (FPP) block,fine-grained prompt-pose communication (FPC) block, andprompt-driven timestamp stylization (PTS) block. Con- cretely, the FPP block encodes three kinds of informationabout the human pose, including action class, coarse- andfine-grained parts of humans like person, head, body, arms,legs, and kinematic information speed, and integratesthem with pose features for serving subsequent processes.Then, the FPC block injects fine-grained part-aware promptembedding into noise 3D poses to establish fine-grainedcommunications between learnable part-aware prompts andposes for enhancing the denoising capability. To handle3D poses with different noise levels, the PTS block intro-duces the timestamp coupled with fine-grained part-awareprompt embedding into the denoising process to enhance itsadaptability and refine the prediction at each noise level.Our contributions can be summarized as follows: We propose a new fine-grained part-aware prompt learningmechanism coupled with diffusion models that possesseshuman body part controllable high-quality generation ca-pability, beneficial to the 3D human pose estimation task. Our FinePOSE encodes multi-granularity informationabout action class, coarse- and fine-grained human parts,and kinematic information, and establishes fine-grainedcommunications between learnable part-aware promptsand poses for enhancing the denoising capability. Extensive experiments illustrate that our FinePOSE ob-tains substantial improvements on Human3.6M and MPI-INF-3DHP datasets and achieves state-of-the-art. Moreexperiments on EgoHumans demonstrate the potential ofFinePOSE to deal with complex multi-human scenarios.",
  ". Related Work": "Diffusion Models. Diffusion models area kind of generative models that sequentially add a se-ries of noise with different levels to the raw data, gradu-ally transforming it from an original data distribution toa noisy distribution, and subsequently reconstructing theoriginal data by denoising. Diffusion models have strongcapabilities in many applications, from 2D image or videogeneration/editing to 3D human pose estima-tion/generation . The3D HPE task, for example, encounters various difficulties,including occlusions, limited training data, and inherent am-biguity in pose representations. Therefore, diffusion modelsability to generate high-fidelity 3D human poses makes themmore suitable for 3D HPE.3D Human Pose Estimation. Considering that extract-ing 2D human skeletons from videos or images requiresexpensive costs, the 3D human pose estimation task isusually divided into two phases: (1) estimating 2D posi-tions of human joints from images or videos ,and (2) mapping 2D positions to the 3D space to esti-mate the 3D positions of human joints[4, 6, 10, 17",
  "Reconstructed 3D poses:": ". The architecture of the proposed FinePOSE. In the diffusion process, Gaussian noise is gradually added to the ground-truth 3Dposes Y0, generating the noisy 3D poses Yt for the timestamp t. In the denoising process, Yt, X and t are fed to fine-grained prompt-drivendenoiser D to reconstruct pure 3D poses Y0. D is composed of a Fine-grained Part-aware Prompt learning (FPP) block, a Fine-grainedPrompt-pose Communication (FPC) block, and a Prompt-driven Timestamp Stylization (PTS) block, where FPP provides more preciseguidance for all human part movements, FPC establishes fine-grained communications between learnable prompts and poses for enhancingthe denoising capability, and PTS integrates learned prompt embedding and current timestamp for refining the prediction at each noise level. fully convolutional network based on dilated temporal con-volutions over 2D keypoints to estimate 3D poses in video.SRNet proposed a split-and-recombine approach, lead-ing to appreciable improvements in predicting rare and un-seen poses. Anatomy decomposed the task into bonedirection prediction and bone length prediction, from whichthe 3D joint locations can be derived entirely. Recently,MixSTE used temporal and spatial transformers alter-nately to obtain better spatio-temporal features. Motion-BERT proposed a pretraining stage to recover the under-lying 3D motion from noisy partial 2D observations. GLA-GCN globally modeled the spatio-temporal structure for3D human pose estimation. D3DP proposed the joint-level aggregation strategy to benefit from all generated poses.Unlike previous methods, our approach proposes a new fine-grained part-aware prompt learning mechanism coupled withdiffusion models that possess controllable, high-quality gen-eration capability of human body parts, which benefits the3D human pose estimation task. Prompt Learning. Prompt learning has been widely usedin the computer vision community . Typically,CoOp utilized a continuous prompt optimization fromdownstream data instead of hand-craft design, the pioneeringwork that brings prompt learning to adapt pre-trained visionlanguage models. CoCoOp extended CoOp by learn-ing image conditional prompts to improve generalization.ProDA learned a prompt distribution over the outputembedding space. VPT introduced variational prompttuning by combining a base learned prompt with a residualvector sampled from an instance-specific underlying distribu-tion. PointCLIPV2 combined CLIP with GPT to be a unified 3D open-world learner. Unlike the above methods, we propose a new fine-grained part-aware promptlearning mechanism, which encodes multi-granularity in-formation about action class, coarse- and fine-grained hu-man parts, and kinematic data, and establishes fine-grainedcommunications between learnable part-aware prompts andposes for enhancing the denoising capability.",
  ". The Proposed Approach: FinePOSE": "Given a 2D keypoints sequence X RNJ2, constructedby N frames with J joints in each, the proposed approach isformulated to predict the 3D pose sequence Y RNJ3.Considering the high-quality generation capability of thetext-controllable denoising process of diffusion models, wedevelop a Fine-grained Prompt-driven Denoiser (FinePOSE)D for 3D human pose estimation. FinePOSE generates accu-rate 3D human poses enhanced by three core blocks: Fine-grained Part-aware Prompt learning (FPP), Fine-grainedPrompt-pose Communication (FPC), and Prompt-drivenTimestamp Stylization (PTS) blocks.",
  ". Diffusion-Based 3D Human Pose Estimation": "Diffusion models are generative models that model the datadistribution in the form of p(Y0) :=p(Y0:T )dY1:Tthrough chained diffusion and reverse (denoising) processes.The diffusion process gradually adds Gaussian noise intothe ground truth 3D pose sequence Y0 to corrupt it into anapproximately Gaussian noise Yt(tT) using a varianceschedule {t}Tt=1, which can be formulated as",
  "where t :=ts=0s and t :=1t. Afterward, the denois-ing process reconstructs the uncontaminated 3D poses by a": "denoiser D. Since the degraded data is well approximatedby a Gaussian distribution after the diffusion process, wecan obtain initial 3D poses YT by sampling noise from aunit Gaussian. Passing YT (t = T) to the denoiser D, weobtain Y0 that is thereafter used to generate the noisy 3Dposes Yt1 as inputs to the denoiser D at timestamp t1 viaDDIM , which can be formulated as",
  ". Fine-grained Prompt-driven Denoiser": "Fine-grained Part-aware Prompt Learning (FPP). To as-sist the reconstruction of pure 3D poses Y0 from contami-nated 3D poses Yt with additional information, FinePOSEguides the denoising process with regular 2D keypoints X,timestamp t, and fine-grained part-aware prompt embeddingP. We design the FPP block to learn P. It encodes threepose-related information in the prompt embedding space,including its action class, coarse- and fine-grained parts ofhumans like person, head, body, arms, legs, and kinematicinformation speed. Afterward, P is integrated with posefeatures for subsequent processes.A learnable prompt embedding P = {p}Kk=1 is withthe shape of K L D, where K denotes the numberof text prompts, L indicates the number of tokens in eachtext prompt, and D is the dimension of token embedding.Since the number of valid tokens is found to be three to fourthrough the text encoder Etx, the first four tokens are taken asrepresentations pk for each text. Moreover, since modifiershelp precisely describe the movements of human body parts,we design a learnable vector rk R(Lk4)D to wrap therepresentations as pk. The above can be formulated as",
  "pk = Concat(rk, pk),(4b)": "where K = 7 and {textk}7k=1 indicate {person, [ActionClass], speed, head, body, arms, legs}. rk is initializedwith Gaussian distribution of = 0 and = 0.02, and{Lk}7k=1 ={7, 12, 10, 10, 10, 14, 14}, which sums to 77 re-garding the text embedding dimension of CLIP . Inshort, the FPP block builds multi-granularity text promptsand learnable modifiers, providing precise guidance for eachhuman body part, as shown in . Fine-grained Prompt-pose Communication (FPC). Afterobtaining fine-grained part-aware prompt embedding P, weestablish fine-grained communications between learned part-aware prompts and poses using the FPC block to improve thedenoising quality. Specifically, when processing the noised3D poses Yt, it injects prompt embedding P, 2D keypointsX, and timestamp t within.First, FPC integrates Yt and guidance information (i.e.,X, t, and P) by a series of concatenation and addition op-erations, as Zt = Concat(Yt, X)+P[L]+F(t). F is thetimestamp embedding network containing a sinusoidal func-tion followed by two Linear layers connected by a GELUnon-linearity. The timestep embedding adaptively adjuststhe quantity of Gaussian noise additions. Since the denoiserD works iteratively, providing detailed information about thecurrent timestamp t is crucial for D to handle 3D poses con-taining different noise levels effectively. Then, Zt is encodedby a spatial transformer, where the multi-head self-attention(MHSA) mechanism helps to focus on the fine-grained rela-tionships between joints within each frame, obtaining Zst.To completely inject prompt embedding P into Zst, weimplement a multi-head cross-attention model, where thequery, key, and value are as Q = WQZst, K = WKP,V = WV P. The value is aggregated with cross-attentionA to generate fine-grained prompt-driven pose features Zspt ,achieving fine-grained prompt-pose communication. Themechanism can be formulated as",
  "Zspt= A V, Zspt= P(Zspt ),(5b)": "where d = D/H and H is the number of attention heads.P indicates the PTS block that bring timestamp t into thegeneration process to obtain timestamp stylized output Zspt .On the other hand, to model inter-frame relationships be-tween poses, Zspt is encoded using a temporal transformervia MHSA to obtain Zspft. Finally, we utilize a spatial-temporal transformer accompanied by permutation opera-tions between spatial and temporal dimensions to extractmore compact fine-grained prompt-driven pose features fromZspft, which are decoded as the predicted 3D poses Y0.Prompt-driven timestamp Stylization (PTS). As men-tioned, providing timestamp embedding to the denoising pro-cess is critical for handling 3D poses with different noise lev-els. Therefore, inspired by Motiondiffuse , we introducethe PTS block that explicitly embeds timestamp t by posi-tional embedding and sums it with the learnable promptembedding P obtained by the FPP block, as v=P[L]+F(t).Given the intermediate output Zspt of the FPC block, the PTSblock calculates Zspt = Zspt w((v))+b((v)), whereb, w, are three different linear projections, and () is theHadamard product.",
  "(-3.5)(-3.7)(-0.2)(-4.9)": ". Quantitative comparison with the state-of-the-art 3D human pose estimation methods on the Human3.6M dataset. N: thenumber of input frames. CPN, HRNet, SH: using CPN , HRNet , and SH as the 2D keypoint detectors to generate the inputs.GT: using the ground truth 2D keypoints as inputs. The best and second-best results are highlighted in bold and underlined formats.",
  ". Training & Inference": "Training. The contaminated 3D poses Yt is sent to a fine-grained prompt-driven denoiser D to reconstruct the 3Dposes Y0 =D(Yt, X, t, P) without noise. The entire frame-work is optimized by minimizing the MSE loss Y0 Y02.Inference.Since the distribution of YT is nearly anisotropic Gaussian distribution, we sample H initial 3Dposes {YhT }Hh=1 from a unit Gaussian. After passing themto the denoiser D, we obtain H feasible 3D pose hypotheses{ Yh0}Hh=1. Each hypothesis Yh0 is used to generate the noisy3D poses Yht1 as inputs to the denoiser D for the next times-tamp t1. Then, we regenerate { Yh0}Hh=1 using { Yht1}Hh=1as inputs to the denoiser D for the next timestamp t2.Analogously, this process iterates M times starting fromthe timestamp T, so each iteration m [1, M] is with thetimestamp t=T(1m M ). Following Joint-Wise Reprojection-Based Multi-Hypothesis Aggregation (JPMA) in , wereproject { Yh0}Hh=1 to the 2D camera plane using known orestimated intrinsic camera parameters and then choose jointswith minimum projection errors with the input X, as",
  ". Extension to 3D Multi-Human Pose Estimation": "We append a post-integration to FinePOSE to apply for themulti-human scenario, avoiding incorporating extra computa-tional cost. Specifically, given a multi-human 2D keypointssequence Xmul RCNJ2, which involves C human characters, FinePOSE first predicts Yc0 for each characterc [1, C]. Considering that some characters may tem-porarily leave the camera field of view, their positions inthose frames are set as zeros to ensure synchronization ofall characters states in Xmul. Next, we integrate { Yc0}Cc=1by stacking over the character dimension, obtaining the finalprediction YC0 RCNJ3.",
  ". Datasets and Metrics": "Human3.6M is a widely used benchmark dataset inhuman pose estimation tasks, which provides a large-scalecollection of accurate 3D joint annotations on diverse hu-man activities. Human3.6M consists of 3.6 million RGBimages, captured from multiple camera views, of 11 profes-sional actors performing 15 activities, e.g., walking, running,and jumping. Following previous efforts , ourFinePOSE is trained on five subjects (S1, S5, S6, S7, S8) andevaluated on two subjects (S9, S11). We calculate the meanper joint position error (i.e., MPJPE) to measure the aver-age Euclidean distance in millimeters between the groundtruth and estimated 3D joint positions for evaluation. Wealso report procrustes MPJPE (i.e., P-MPJPE) that calculatesMPJPE after aligning the estimated poses to the ground truthusing a rigid transformation.MPI-INF-3DHP provides synchronized RGB videosequences with accurate 3D joint annotations for 3D humanpose estimation. It comprises 8 activities conducted by 8actors in the training set, while the test set encompasses 7activities. We calculate MPJPE, the percentage of correctlyestimated keypoints (i.e., PCK) within a 150mm range, andthe area under the curve (i.e., AUC).EgoHumans collects multi-human ego-exo videos cov-ering 7 sports activities. Recently, a subset of 2D to 3D",
  "(-1.6)(-3.3)(-2.9)(-2.4)(-3.2)(-7.2)(-5.6)(-3.6)(-3.7)(-5.3)(-3.7)(-3.1)(-3.6)(-1.7)(-1.4)(-3.5)": ". Quantitative comparison with the state-of-the-art 3D human pose estimation methods on the Human3.6M dataset using 2Dkeypoint detectors to generate the inputs. Dir., Disc., , and WalkT. correspond to 15 action classes. Avg indicates the average MPJPEamong 15 action classes. The best and second-best results are highlighted in bold and underlined formats.",
  "(+0.4)(+0.9)(-1.6)": ". Quantitative comparison with the state-of-the-art 3Dhuman pose estimation methods on the MPI-INF-3DHP datasetusing ground truth 2D keypoints as inputs. N: the number ofinput frames. The best and second-best results are highlighted inbold and underlined formats. keypoints annotations has been released covering tagging,lego-assembling, and fencing. It contains 105 RGB videostaken by ego cameras. Between 1 and 3 human charactersappear in each video, resulting in a total of 238 subsequences.We report the average MPJPE per video.",
  ". Implementation Details": "We take MixSTE as the backbone of the denoiser Dand CLIP as the frozen text encoder Etx. The numbers ofMHSA-MLP-LN building blocks of the spatial, temporal,and spatio-temporal transformer in the FPC block are 1, 1,and 3. The training epoch in all the experiments below is100, and the batch size is 4. We adopt AdamW optimizerwith the momentum parameters of 1 =0.9, 2 =0.999, andthe weight decay of 0.1. The learning rate starts from 6e5",
  "FinePOSE (Ours)31.925.0": ". Ablation study on different configurations of FinePOSEon Human3.6M using 2D keypoint detectors as inputs. Baseline:the method without any textual information via prompt learning.w FPP: the method only contains the FPP block and adds P[L] tothe input. w/o FPP: the method without the FPP block leads to aninfeasible FPC block. w/o FPC: the method without the FPC block.w/o PTS: the method without the PTS block. tion class Photo and decreases average MPJPE by 3.5mm(35.4mm31.9mm).MPI-INF-3DHP. Tab. 3 reports comparisons between ourFinePOSE and SOTA 3D HPE methods on the MPI-INF-3DHP dataset, using ground truth 2D keypoints as inputs.Compared with the SOTA existing method GLA-GCN ,FinePOSE decreases MPJPE by 1.6mm and increases thePCK by 0.4% and AUC by 0.9%. Overall, these experi-mental results demonstrate that our FinePOSE benefits fromfine-grained part-aware prompt learning and pose-promptcommunications, resulting in higher denoising quality andestimation accuracy.",
  ". Comparison with the State-of-the-Arts": "Human3.6M. Tab. 1 reports comparisons between ourFinePOSE with state-of-the-art (SOTA) 3D HPE methods onthe Human3.6M dataset. FinePOSE significantly achievesnew SOTA performance, especially when using detected2D keypoints as inputs. Compared with existing 3D HPEmethods, FinePOSE surpasses the SOTA method D3DP by 3.5mm in MPJPE and 3.7mm in P-MPJPE. When usingground truth 2D keypoints as inputs, FinePOSE also signif-icantly outperforms the SOTA method MotionBERT ,improving MPJPE by 0.2mm. Tab. 2 provides detailed com-parisons between on each action class using 2D keypointdetectors as inputs. For example, our FinePOSE achievesnoticeable improvements (43.7mm36.5mm) for the ac-",
  ". Ablation Study": "We conduct a series of analysis experiments of ourFinePOSE on the Human3.6M dataset to investigate theeffects on the performance of different prompt learning de-signs in the FPP block and different blocks in FinePOSE.Effects of Different Designs in FPP. We design various ver-sions of the FPP block for our FinePOSE, including a) w/oPrompt, b) M-Prompt, c) S-Prompt, d) C-Prompt, and e) AL-Prompt. Specifically, w/o Prompt denotes FinePOSE withoutintroducing textual information and learnable prompts. M-Prompt indicates using the action class to design the promptmanually instead of the FPP block. Taking the action classDirections as an example, the manually designed prompt isa person is pointing directions with hands. There are 15 ac-tion classes available in the Human3.6M dataset correspond-ing to 15 kinds of manually designed prompts. S-Promptindicates utilizing learnable prompts combined with the ac-tion class. C-Prompt indicates employing the action classand coarse-grained information like person and speedto create the prompt. Finally, AL-Prompt means only usinglearnable prompts without any manual design.We first evaluate the effect of manually designed prompts(i.e., M-Prompt) on Human3.6M. As shown in Tab. 4, com-pared to w/o Prompt, M-Prompt achieves a decrease of1.4mm on MPJPE and 1.0mm on P-MPJPE, indicating that",
  ". Quantitative comparison with D3DP on the EgoHu-mans dataset using 2D keypoints as inputs. Tag., Lego, and Fenc.correspond to 3 action classes. Avg indicates the average MPJPEamong 3 action classes": "manually designing prompts is a practical strategy eventhough they cannot guarantee the prompt is optimal dur-ing the denoising process for the 3D HPE task. To evalu-ate the effectiveness of S-Prompt, we compare it with w/oPrompt. As shown in Tab. 4, MPJPE and P-MPJPE arereduced by 1.0mm and 0.2mm, respectively, for S-Prompt,which demonstrates that with the help of learnable prompts,integrating textual information can improve the performanceon 3D HPE task. While compared to M-Prompt, S-Promptresults in performance degradation, indicating that learnableprompts must be meticulously designed. In addition, we alsoinvestigate the impact of manual intervention degrees on 3DHPE performance using two groups of comparative exper-iments. In the first group, we used only learnable promptswithout any textual information and manual intervention,named AL-Prompt, which differs from S-Prompt with theaction class. The second group designed a coarse-grainedprompt involving action class, person, speed, and corre-sponding learnable prompts, denoted as C-Prompt. We seethat both AL-Prompt and C-Prompt outperform S-Promptsince AL-Prompt is without interference from uncompletetextual information and C-Prompt contains some importanttextual information like action class, person, and speed,which provide the action subject and kinematic data. Fi-nally, it is observed that our FinePOSE outperforms variousversions of prompt learning on both MPJPE and P-MPJPE,indicating the effectiveness of the fine-grained part-awareprompt learning mechanism in FinePOSE. Effects of Different Blocks in FinePOSE. In Tab. 5, weprovide different settings of our FinePOSE to evaluate theeffects of different blocks for the 3D HPE performance, in-cluding Baseline, w FPP, w/o FPP, w/o FPC, and w/o PTS.Specifically, Baseline denotes FinePOSE without introduc-ing textual information and learnable prompts, the same asthe configuration of w/o Prompt. w FPP indicates FinePOSEonly contains the FPP block without introducing the FPC andPTS blocks and only adds textual information P[L] to theinput. w/o FPP denotes FinePOSE without the FPP block,leading to the FPC block being infeasible and only utilizingthe PTS block. w/o FPC means FinePOSE without the FPCblock but using the FPP and PTS blocks. w/o PTS refersto FinePOSE without the PTS block but using the FPP andFPC blocks to integrate textual information for fine-grained SittingDown MotionBERTD3DPFinePOSE WalkDogSittingPurchasesDiscussionPhotoPosing . Qualitative comparisons of our FinePOSE with MotionBERT and D3DP on Human3.6M. The gray skeleton is theground-truth 3D pose. The blue skeleton represents the prediction of the human left part, and the orange indicates the human right part. Thered dashed line represents the incorrect regions of the compared methods, and the blue dashed line indicates the counterparts of FinePOSE.",
  "part-aware prompt learning": "Compared w FPP and Baseline, we observe that theformer can achieve 1.9mm and 1.1mm improvements onMPJPE and P-MPJPE. This is because our FinePOSE con-tains the FPP block, which adds the prompt embedding P[L]into the input Zt of denoiser D, significantly improving thedenoising capability. We observe that the results betweenw/o FPP and Baseline are almost equivalent. The baselinehas already brought timestamp t into the denoising process,while the PTS block refines the prediction at each noiselevel by reusing the timestamp to the denoising process afterthe FPP and FPC block. Thus, there is nearly no effect inadding only the PTS block without FPP and FPC blocks tothe denoiser. Making a comparison between w/o FPC andw/o FPP, the former achieves a decrease of 1.4mm on bothMPJPE and P-MPJPE over w/o FPP, indicating that the FPPblock in the denoiser plays a critical role in the fine-grainedpart-aware prompt learning mechanism. Finally, we observethat FinePOSE achieves a decrease of 4.7mm on MPJPEand 4.0mm on P-MPJPE compared to w/o PTS, indicatingthe necessity to integrate learned prompt embeddings andtimestamps in the PTS block.",
  ". Results on 3D Multi-Human Pose Estimation": "In real-world applications, the multi-human scenario is morecommon than the single-human one. However, its complex-ity hinders existing work from handling it. In Sec. 3.4, wepresent a post-integration to extend FinePOSE for the multi-human pose estimation task. We implemented the extensionusing the SOTA method D3DP for a convincing compari-son. The experimental results on EgoHumans are reported inTab. 6, demonstrating that (1) the integration strategy indeedhas potential feasibility and (2) FinePOSE has a dominantperformance even in the complex multi-human scenario.",
  ". Visualization": "shows the visualization results of D3DP , Mo-tionBERT and our FinePOSE on Human3.6M. Thesemethods have performed well for actions in which the body,legs, and other parts of the person in the scene are relativelyclear. For the actions with simple shapes, e.g., Discussionand Photo, the 3D poses predicted by FinePOSE matchbetter with ground-truth 3D poses than those of D3DP andMotionBERT, especially in the left knee, right arm, and righthip of Discussion and in the left knee of Photo. Forthe actions with complex shapes, e.g., Sitting and Sit-tingDown, FinePOSE is more accurate at various joints,especially for arms and legs, while the 3D poses predictedby D3DP and MotionBERT differ significantly from ground-truth 3D poses.",
  ". Conclusion and Discussion": "This work has presented FinePOSE, a new fine-grainedprompt-driven denoiser for 3D human pose estimation.FinePOSE was composed of FPP, FPC, and PTS blocks.FPP learned fine-grained part-aware prompts to provide pre-cise guidance for each human body part. FPC establishedfine-grained communication between learnable part-awareprompts and poses to enhance denoising capability. PTSbrought timestamp information to the denoising process,strengthening the ability to refine the prediction at each noiselevel. Experimental results on two benchmarks demonstratedthat FinePOSE surpasses the state-of-the-art methods. Wehave also extended FinePOSE from single-human scenariosto multi-human ones, exhibiting that our model performswell in complex multi-human scenarios.Limitations. FinePOSE is not designed explicitly for themulti-person scenario. The diffusion model-based 3D HPEmethod is relatively computationally expensive.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. NeurIPS, 33:68406851, 2020.2": "Christian Keilstrup Ingwersen, Christian Mller Mikkelstrup,Janus Nrtoft Jensen, Morten Rieger Hannemose, and An-ders Bjorholm Dahl. Sportspose-a dynamic 3d sports posedataset. In CVPR, pages 52185227, 2023. 2 Catalin Ionescu, Dragos Papava, Vlad Olaru, and CristianSminchisescu. Human3. 6m: Large scale datasets and predic-tive methods for 3d human sensing in natural environments.TPAMI, 36(7):13251339, 2013. 5",
  "Tuomas Oikarinen, Daniel Hannah, and Sohrob Kazerounian.Graphmdn: Leveraging graph structure and deep learning tosolve inverse problems. In IJCNN, pages 19, 2021. 2": "Dario Pavllo, Christoph Feichtenhofer, David Grangier, andMichael Auli. 3d human pose estimation in video with tem-poral convolutions and semi-supervised training. In CVPR,pages 77537762, 2019. 2, 5, 6 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervision.In ICML, pages 87488763, 2021. 2, 3, 4",
  "Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Shanshe Wang,Siwei Ma, and Wen Gao. P-stmo: Pre-trained spatial temporalmany-to-one model for 3d human pose estimation. In ECCV,pages 461478, 2022. 5, 6": "Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang,Kai Han, Shanshe Wang, Siwei Ma, and Wen Gao. Diffusion-based 3d human pose estimation with multi-hypothesis aggre-gation. In ICCV, pages 1476114771, 2023. 5, 6, 8 Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang,Kai Han, Shanshe Wang, Siwei Ma, and Wen Gao. Diffusion-based 3d human pose estimation with multi-hypothesis ag-gregation. In ICCV, pages 1476114771, 2023. 2, 3, 5, 7,8",
  "Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Videoprobabilistic diffusion models in projected latent space. InCVPR, pages 1845618466, 2023. 2": "Andrei Zanfir, Mihai Zanfir, Alex Gorban, Jingwei Ji,Yin Zhou, Dragomir Anguelov, and Cristian Sminchisescu.Hum3dil: Semi-supervised multi-modal 3d humanpose es-timation for autonomous driving. In RL, pages 11141124,2023. 2 Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, QiangXu, and Stephen Lin. Srnet: Improving generalization in 3dhuman pose estimation with a split-and-recombine approach.In ECCV, pages 507523, 2020. 3, 6",
  "Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, andChen Chen. Poseformerv2: Exploring frequency domain forefficient and robust 3d human pose estimation. In CVPR,pages 88778886, 2023. 2, 5, 6": "Hongwei Zheng, Han Li, Bowen Shi, Wenrui Dai, BotaoWang, Yu Sun, Min Guo, and Hongkai Xiong. Actionprompt:Action-guided 3d human pose estimation with text and poseprompting. In ICME, pages 26572662, 2023. 5, 6 Jingxiao Zheng, Xinwei Shi, Alexander Gorban, Junhua Mao,Yang Song, Charles R Qi, Ting Liu, Visesh Chari, AndreCornman, Yin Zhou, et al.Multi-modal 3d human poseestimation with 2d weak supervision in autonomous driving.In CVPR, pages 44784487, 2022. 2"
}