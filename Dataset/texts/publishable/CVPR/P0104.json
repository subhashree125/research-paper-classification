{
  "Abstract": "Large Language Models (LLM) and Vision LanguageModels (VLM) enable robots to ground natural languageprompts into control actions to achieve tasks in an openworld. However, when applied to a long-horizon collab-orative task, this formulation results in excessive prompt-ing for initiating or clarifying robot actions at every stepof the task. We propose Language-driven Intention Track-ing (LIT), leveraging LLMs and VLMs to model the humanusers long-term behavior and to predict the next human in-tention to guide the robot for proactive collaboration. Wedemonstrate smooth coordination between a LIT-based col-laborative robot and the human user in collaborative cook-ing tasks.",
  ". Introduction": "The groundbreaking advances in Large Language Models(LLM) and Vision Language Models (VLM) endow robotswith exceptional cognition capabilities and reasoning skillsto both understand the surrounding open world and follownatural language commands of human users . Morerecent works explore conversations between the human userand the robot to allow the robot to perform multi-step tasksor clarify ambiguity of the human command .When the philosophy of grounding natural languagecommands into robot control policies is applied to human-robot collaboration (HRC), the human user may have tohave a conversation with the robot at each step of thelong-horizon task .This situation rarely happens inhuman-human collaboration, as a human is able to track theprogress on the partners side based on their shared knowl-edge over the task. For examples, a worker rarely has tohave a conversation with a co-worker in a collaborative as-sembly task on which they have collaborated many times,and a sous chef rarely has to have a conversation with thechef when creating a regular dish together.To address this challenge in human-robot collaboration, the robot needs to build an effective understanding of notonly the environment, but also the human user.Thiswork proposes Language-driven Intention Tracking (LIT)to model long-term behavior of the human user, and inte-grates LIT into an LLM-driven collaborative robot frame-work.LIT extends intention tracking by applyingan LLM to model measurement likelihood and transitionprobabilities in the probabilistic graphical model of humanintentions, which is defined by grounding an overall taskprompt (e.g., make a salad) with understanding of the sceneusing LLM and VLM models. Note this is the only promptneeded from the human user in LIT framework. LIT uses aVLM to generate text descriptions of the human users be-havior in the frames as measurements to track the humanusers intention and filter out hallucinations. Intention pre-diction in the near-term allows the collaborative robot toproactively assist the human user. By harnessing the powerof foundation models, we believe the LIT framework canbe generalized to any collaborative tasks. We demonstratethe effectiveness of the LIT framework in a scenario wherethe collaborative robot acts as a sous-chef to assist a humanuser in cooking.",
  ". Language Models in Robotics": "There has been an explosion of work in robotic planningtasks driven by LLM. LLMs have been shown to exhibitthe ability to write multi-step control logic code based onnatural language command . However, such meth-ods typically do not understand whether such a commandis feasible to execute a priori; though this can be cor-rected for with e.g., feasibility planners , such as isdone by Text2Motion . VoxPoser , SayCan ,and LATTE ground robotic affordances into their LLM-driven planning and modification steps following a com-mand provided via natural language.RT-2 extendsthe PALM-E VLM with action representations.Re-cent works explore LLM agent embodiment withinvideo games for accomplishing goals using visual input",
  "arXiv:2406.13787v1 [cs.RO] 19 Jun 2024": ". Language-driven Intention Tracking (LIT) based collaborative robot framework. The open scene understanding module detectsobjects in the scene and generate potential manipulation options, which in our case are top-down grasp poses. The task graph reasoningmodule takes the users prompt on the overall task and the detected objects as input to generate a list of task steps, which we define asintention in this work. As some steps of the overall task can switch order without impact on the outcome, the LLM checks on reversibilityof sequences of task steps, and builds a task graph. The Language-driven Intention Tracking module uses the task graph to build theprobabilistic graphical model for intention transition. The VLM is used to generate text descriptions from frames as measurements. Wecompute time-varying transition probabilities and make prediction steps, and use measurements to compute measurement likelihood andmake update steps to track the human intention. The intention-grounded planning module make an additional prediction step on the currentintention posterior, and manipulate the objects relevant to the predicted next intention to proactively collaborate with the human. and/or high-level textual task description. Specifically Voy-ager leverages Minecraft to build an LLM-driven agentintended to continually explore, and learn new skills andstore them for future use. Instead of caching skills, we pro-pose caching intentions for more effective collaborative au-tonomy.",
  ". Intention Modeling in HRC and NLP": "Understanding human intention is essential for safe andseamless human-robot interaction (HRI) and human-robotcollaboration (HRC). Human intentions have been ex-tensively studied in various contexts, including a pedes-trians desired destination for social navigation ,a drivers lane-changing intention for autonomous driv-ing , and a workers desired tool/part for collabo-rative manufacturing . However, such intention es-timation methods that match the observations to these well-defined intentions are usually carefully crafted and are hardto generalize to novel scenarios. On the other hand, intentclassification is a crucial task in NLP and has been compre-hensively studied over decades . In particular,identification of unseen open intents has become an emerg-ing area in the field of intent classification . Never-theless, intent classification in NLP is typically formulatedas a static recognition task, whereas HRC requires using measurement sequences to perform online tracking of hu-man intentions which can vary across time . We applythe concept of open intent in NLP to the intention trackingmethod for HRC, so our language-driven intention trackingmethod can easily generalize to novel scenarios and tasks.",
  ". Robotic Cooking": "There has long been interest in developing robots that cancook for and/or with people. Many works propose robotic systems which learn to autonomouslyperform cooking and baking tasks from various modali-ties. BakeBot and RoboCook take as input theplain text recipe and target shapes for deformable manipu-lation respectively before determining its own step-by-stepinstructions for the long horizon planning task of bakingcookies and making dumplings from scratch.However,while the latter is able to recover from human meddling ofits planned tasks, it is not able to understand why the hu-man has interrupted the task, and will instead simply resumefrom a prior step to reach the original goal. Other workspropose teaching a robot to cook from human demonstra-tion . However, these works seek only to imitatethe human motion of a certain cooking action, with the lat-ters Hidden Markov Model limited to inferring the corre-sponding recipe based on observations of the human skele- tal motion, before subsequently continuing the recipe itself.More similar to our work, Wang et al. propose a collabora-tive framework for a robotic cooking assistant in MOSAIC, using an LLM as a task-planner and large vision mod-els for determining locations of ingredients. However, thissystem requires the chef to give explicit natural languagecommands to incite action, and only forecasts human mo-tion primarily as a means to provide safety for the human,such as by preventing collisions with the robot. Finally, it isspecific to the cooking application.",
  ". Problem Formulation": "A human user collaborates with a robot to perform a long-horizon multi-step task. The robot can understand the sceneand reason about the task to formulate a directed task graph,where vertices are defined as steps of the task, and edges aredefined as the feasible orders between the task steps. Thereason we use a task graph instead of a task chain is therelationships between task steps may not always be causaland the order may be reversible, such as cut tomatoes andcut cucumbers for a salad making task. We define humanintention Gt as the task step the human intends to work onat time t. The robot uses the measurement history of thehuman behavior X1:t to track the human intention Gt.",
  ". Language Probabilistic Graphical Model": "We propose Language Probabilistic Graphical Model(LPGM) to describe the dynamics of human behavior in, where the value of each node is a natural languagesentence. To calculate a conditional probability for exam-ple P(A = a|B = b, C = c) in an LPGM, we use an LLM,where the prompt has two parts: a conditional part and aquery part. We formulate the conditional part of the promptas We observe {B} is {b}, and {C} is {c}, . We pro-pose three different methods to compose the query part ofthe prompt and calculate the conditional probability accord-ingly.The first method is to directly ask for P(a|b, c). Thequery part of the prompt is formulated as provide the prob-ability of {A} being {a}.. This is similar to Ren et. al , where the LLM is treated as an expert in modeling the math-ematical relationships between A, B and C. However, theoutputs from the LLM may not be trustworthy if corre-sponding materials are not covered much by the corpus usedto train the LLM. We use this as a baseline in our work.The second method is to ask the LLM to generatea value of A, and compare the similarity score such asBERTScore of the generated text with respect to a toquantify P(a|b, c). The query part of the prompt is formu-lated as what do you think {A} would be?. This methodessentially uses the LLM to provide a maximum likelihoodestimate arg maxa P(A = a|B = b, C = c), and uses thedistance between this point estimate and the value to com-pute the conditional probability.The third method addresses the case where A is a dis-crete variable. We ask LLM to generate a list of valuesof A with a large length N, compare the values in the listto all possible values of A, aggregate the number of mostsimilar generated values to each possible value, and form astatistical estimate of the P(a|b, c). The query part of theprompt is formulated as what do you think {A} wouldbe? Provide N different examples.. This is similar to aMonte Carlo method which approximates the distributionof P(A|B = b, C = c) by sampling from the LLM.",
  ". Collaborative Cooking Setup": "The human user wants to make a dish, but all the requiredmaterials and tools are not reachable by the human butare by the robot. The robot needs to act as a sous-chef tosmoothly coordinate with the human by passing essential materials and tools at appropriate times while not makingthe humans cooking table overly occupied with unneces-sary items at the moment. The robot is assumed to onlyreceive the prompt at the beginning on what dish is going tobe made, and will not receive prompts during collaboration.We choose LLaVA with a 13-billion parame-ter Vicuna backbone (derived from Llama 2 ) asthe VLM in the system, due to both its open source natureand competitiveness with commercial-grade models such asGemini . Note that we use the same model as the LLMfor consistent performance by inputting the text promptwith a full-black image. The collaborative robot is a UR5earm equipped with a Robotiq Hand-E Gripper. We use In-tel RealSense RGBD Cameras to provide a top-down viewof the robot table with objects on it, and to provide a frontview of the human users behavior. We use Robot OperatingSystem (ROS) to build the LIT framework.",
  ". Open Scene Understanding Module": "The overhead camera provide a top-down view of the robotworkspace with all object reachable by the robot.Weprompt the VLM to name and describe the objects in theimage frame from the overhead camera, and take thesenames as input to Grounding DINO coupled with Seg-ment Anything to locate and segment all objects on theframe. Object names listed as present in the scene by theVLM that are detected with low confidence are thrown out.We perform Principal Component Analysis on object seg-mentations and compute object orientation and correspond-ing grasp poses. The detected object names and grasp posesare fed into the downstream modules.",
  ". Task Graph Reasoning Module": "Given the available objects produced by the open scene un-derstanding module and the general task prompt from thehuman user, we query the LLM to output a sequence oftask steps in order to achieve the overall task. The LLMis also asked to provide the corresponding objects neededin each task step, which will be used to inform the down-stream planning module. We initialize the task graph withthe sequence of task steps, and query the LLM whether ad-jacent steps can be reversible to add new edges to the taskgraph.",
  ". Language-driven Intention Tracking Module": "We use the task steps from the reasoned task graph as pos-sible values of intention Gt. The task graph is used to ini-tialize the uniform prior among the first steps the human canstart on, and to inform which pairs of (gt, gt+1) are requiredto compute the intention transition P(gt+1|gt, xtTw:t).During collaboration, we use a front-view camera to collectframes of the human user, and feed the frames to VLM togenerate text descriptions of human behavior as measure-",
  ". Intention-grounded Planning Module": "To collaborate with the human proactively, the robot pre-dicts the human intention at the next time step by runningone prediction step in LIT on the current posterior of the in-tentions, and outputting the maximum probability intentionas the prediction. The intention-grounded planning mod-ule performs planning and control to manipulate the objectsrelevant to the predicted next intention. In our collabora-tive cooking scenario, the robot sous-chef would pass theobjects needed for the next cooking step to the human chefin advance.",
  ". Preliminary Study": "We collect a salad cooking demonstration and run language-driven intention tracking to compare how similarity metricsaffect tracking performance as presented in . In ad-dition to BERTScore , we introduce BERT-mean-cosand Word2Vec-mean-cos , which take the mean of wordembeddings from the corresponding pre-trained model togenerate candidate and reference sentence embeddings, andapply cosine similarity to generate similarity score. Dur-ing evaluation, we isolate the effect of similarity metrics bycomputing measurement likelihood with the similarity met-rics while using a fixed intention transition matrix based onthe task graph . shows that BERT-mean-cos em-pirically outperforms BERTScore and Word2Vec-mean-cosfor tracking human intentions.",
  ". Conclusions and Future Work": "We propose Language-driven Intention Tracking (LIT) tomodel long-term behavior of the human user in an openscenario for proactive human-robot collaboration withoutrepetitive prompting. We develop a LIT-based collaborativerobot framework powered by LLMs and VLMs to under-stand the open scene, construct a task graph, track varyinghuman intentions, and ground intention prediction to plan-ning. We demonstrate the framework in a robot sous-chefapplication, where the robot seamlessly assist the humanuser in cooking.In future, we will conduct human subject experimentswith more comprehensive metrics to evaluate performanceefficiency and user satisfaction compared to existing works.We will study the tradeoff between expressiveness andspeed of the foundation models on performance of the LITframework. We will show versatility of this framework bytesting in different daily tasks such as collaborative furni-ture assembly. We will also work on generalization of theframework to multiple users. . Language-driven Intention Tracking with different sim-ilarity metrics. The ground truth order of the human intentions:slice tomatoes; slice cucumbers; put tomatoes and cucumbers in abowl; put salad dressing on tomatoes and cucumbers; stir and mixthe salad with a spoon. Snapshots show the moment when inten-tion transition happens. (a) The human starts cutting a cucumberafter finishing cutting a tomato. (b) The human starts putting veg-etables into a bowl after cutting the cucumber.",
  "Roy, and Daniela Rus. Interpreting and Executing Recipeswith a Cooking Robot, pages 481495.Springer Interna-tional Publishing, Heidelberg, 2013. 2": "Anthony Brohan, Noah Brown, Justice Carbajal, YevgenChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence,Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakr-ishnan, Kehang Han, Karol Hausman, Alex Herzog, Jas-mine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, RyanJulian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu,Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kan-ishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar,Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, AnikaitSingh, Radu Soricut, Huong Tran, Vincent Vanhoucke, QuanVuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, JialinWu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu,and Brianna Zitkovich. Rt-2: Vision-language-action modelstransfer web knowledge to robotic control. In arXiv preprintarXiv:2307.15818, 2023. 1 Arthur Bucker, Luis Figueredo, Sami Haddadin, AshishKapoor, Shuang Ma, Sai Vemprala, and Rogerio Bonatti.Latte: Language trajectory transformer. In 2023 IEEE In-ternational Conference on Robotics and Automation (ICRA),pages 72877294, 2023. 1",
  "Inigo Casanueva, Tadas Temcinas, Daniela Gerz, MatthewHenderson, and Ivan Vulic. Efficient intent detection withdual sentence encoders. arXiv preprint arXiv:2003.04807,2020. 2": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.Xing.Vicuna: An open-source chatbot impressing gpt-4with 90%* chatgpt quality, 2023. 4 Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, CoreyLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,Jonathan Tompson, Quan Vuong, Tianhe Yu, WenlongHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,and Pete Florence. Palm-e: An embodied multimodal lan-guage model. In arXiv preprint arXiv:2303.03378, 2023. 1",
  "Zipeng Fu, Tony Z. Zhao, and Chelsea Finn.Mobilealoha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. In arXiv, 2024. 2": "Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,Jiajun Wu, and Li Fei-Fei.Voxposer:Composable 3dvalue maps for robotic manipulation with language models.In Conference on Robot Learning, pages 540562. PMLR,2023. 1 Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, JackyLiang, Pete Florence, Andy Zeng, Jonathan Tompson, IgorMordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jack-son, Noah Brown, Linda Luu, Sergey Levine, Karol Haus-man, and brian ichter. Inner monologue: Embodied reason-",
  "ing through planning with language models. In Proceedingsof The 6th Conference on Robot Learning, pages 17691782.PMLR, 2023. 1": "Zhe Huang, Aamir Hasan, Kazuki Shin, Ruohua Li, andKatherine Driggs-Campbell.Long-term pedestrian trajec-tory prediction using mutable intention filter and warp lstm.IEEE Robotics and Automation Letters, 6(2):542549, 2020.2 Zhe Huang, Ye-Ji Mun, Xiang Li, Yiqing Xie, NinghanZhong, Weihang Liang, Junyi Geng, Tan Chen, and Kather-ine Driggs-Campbell. Hierarchical intention tracking for ro-bust human-robot collaboration in industrial assembly tasks.In 2023 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 98219828, 2023. 1, 2, 4 Alex Irpan, Alexander Herzog, Alexander Toshkov Toshev,Andy Zeng, Anthony Brohan, Brian Andrew Ichter, By-ron David, Carolina Parada, Chelsea Finn, Clayton Tan,Diego Reyes, Dmitry Kalashnikov, Eric Victor Jang, FeiXia, Jarek Liam Rettinghouse, Jasmine Chiehju Hsu, Jor-nell Lacanlale Quiambao, Julian Ibarz, Kanishka Rao,Karol Hausman, Keerthana Gopalakrishnan, Kuang-HueiLee,Kyle Alan Jeffrey,Linda Luu,Mengyuan Yan,Michael Soogil Ahn, Nicolas Sievers, Nikhil J Joshi, NoahBrown, Omar Eduardo Escareno Cortes, Peng Xu, Peter Pas-tor Sampedro, Pierre Sermanet, Rosario Jauregui Ruano,Ryan Christopher Julian, Sally Augusta Jesmonth, SergeyLevine, Steve Xu, Ted Xiao, Vincent Olivier Vanhoucke, YaoLu, Yevgen Chebotar, and Yuheng Kuang. Do as i can, notas i say: Grounding language in robotic affordances. In Con-ference on Robot Learning (CoRL) 2022, 2022. 1 Bernard J Jansen, Danielle L Booth, and Amanda Spink. De-termining the informational, navigational, and transactionalintent of web queries. Information Processing & Manage-ment, 44(3):12511266, 2008. 2 Kapil D Katyal, Gregory D Hager, and Chien-Ming Huang.Intent-aware pedestrian prediction for adaptive crowd navi-gation. In 2020 IEEE International Conference on Roboticsand Automation (ICRA), pages 32773283. IEEE, 2020. 2 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment anything. In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), pages 40154026, 2023. 4 Puneet Kumar, Mathias Perrollaz, Stephanie Lefevre, andChristian Laugier. Learning-based approach for online lanechange intention prediction. In 2013 IEEE Intelligent Vehi-cles Symposium (IV), pages 797802. IEEE, 2013. 2 Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, KarolHausman, Brian Ichter, Pete Florence, and Andy Zeng. Codeas policies: Language model programs for embodied control.In 2023 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 94939500. IEEE, 2023. 1",
  "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, YuanhanZhang, Sheng Shen, and Yong Jae Lee.Llava-next: Im-proved reasoning, ocr, and world knowledge, 2024. 4": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, and Lei Zhang. Grounding dino: Marrying dino withgrounded pre-training for open-set object detection, 2023. 4 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,and Jeff Dean.Distributed representations of words andphrases and their compositionality. Advances in neural in-formation processing systems, 26, 2013. 4 Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-jes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. Deeplearningbased text classification: a comprehensive review.ACM computing surveys (CSUR), 54(3):140, 2021. 2 Davide Nicolis, Andrea Maria Zanchettin, and Paolo Rocco.Human intention estimation based on neural networks for en-hanced collaboration with robots. In 2018 IEEE/RSJ Interna-tional Conference on Intelligent Robots and Systems (IROS),pages 13261333. IEEE, 2018. 2 Claudia Perez-DArpino and Julie A Shah. Fast target pre-diction of human reaching motion for cooperative human-robot manipulation tasks using time series classification. In2015 IEEE international conference on robotics and au-tomation (ICRA), pages 61756182. IEEE, 2015. 2 Allen Z Ren, Anushri Dixit, Alexandra Bodrova, SumeetSingh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama,Fei Xia, Jake Varley, et al. Robots that ask for help: Uncer-tainty alignment for large language model planners. In Con-ference on Robot Learning, pages 661682. PMLR, 2023. 1,3 Haochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li, and Ji-ajun Wu. Robocook: Long-horizon elasto-plastic object ma-nipulation with diverse tools. In Proceedings of The 7th Con-ference on Robot Learning, pages 642660. PMLR, 2023. 2 Grzegorz Sochacki, Arsen Abdulali, Narges Khadem Hos-seini, and Fumiya Iida. Recognition of human chefs inten-tions for incremental learning of cookbook by robotic saladchef. IEEE Access, 11:5700657020, 2023. 2",
  "Gemini Team, Rohan Anil, and Sebastian Borgeaud et al.Gemini: A family of highly capable multimodal models,2024. 4": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,Moya Chen, Guillem Cucurull, David Esiobu, Jude Fer-nandes, Jeremy Fu, Wenyin Fu, Brian Fuller, CynthiaGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-tor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-renev, Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, RuanSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,Yuchen Zhang, Angela Fan, Melanie Kambadur, SharanNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. 4",
  "Hung Tran, Vuong Le, and Truyen Tran. Goal-driven long-term trajectory prediction. In Proceedings of the IEEE/CVFwinter conference on applications of computer vision, pages796805, 2021. 2": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-mar. Voyager: An open-ended embodied agent with largelanguage models. arXiv preprint arXiv: Arxiv-2305.16291,2023. 1, 2 Huaxiaoyue Wang, Kushal Kedia, Juntao Ren, Rahma Ab-dullah, Atiksh Bhardwaj, Angela Chao, Kelly Y Chen,Nathaniel Chin, Prithwish Dan, Xinyi Fan, et al. Mosaic: Amodular system for assistive and interactive cooking. arXivpreprint arXiv:2402.18796, 2024. 1, 2, 3 Yang Xing, Chen Lv, Huaji Wang, Hong Wang, YunfengAi, Dongpu Cao, Efstathios Velenis, and Fei-Yue Wang.Driver lane change intention inference for intelligent vehi-cles: Framework, survey, and challenges. IEEE Transactionson Vehicular Technology, 68(5):43774390, 2019. 2 Danfei Xu, Ajay Mandlekar, Roberto Martn-Martn, YukeZhu, Silvio Savarese, and Li Fei-Fei. Deep affordance fore-sight: Planning through what can be done in the future. In2021 IEEE International Conference on Robotics and Au-tomation (ICRA), page 62066213. IEEE Press, 2021. 1 Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang,Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang,Kaiyang Zhou, and Ziwei Liu. Octopus: Embodied vision-language programmer from environmental feedback, 2023.1"
}