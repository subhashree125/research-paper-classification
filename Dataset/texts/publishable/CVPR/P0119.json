{
  "Abstract": "Weakly supervised semantic segmentation has witnessedgreat achievements with image-level labels. Several recentapproaches use the CLIP model to generate pseudo labelsfor training an individual segmentation model, while thereis no attempt to apply the CLIP model as the backbone to di-rectly segment objects with image-level labels. In this paper,we propose WeCLIP, a CLIP-based single-stage pipeline,for weakly supervised semantic segmentation. Specifically,the frozen CLIP model is applied as the backbone for se-mantic feature extraction, and a new decoder is designedto interpret extracted semantic features for final predic-tion. Meanwhile, we utilize the above frozen backbone togenerate pseudo labels for training the decoder. Such la-bels cannot be optimized during training.We then pro-pose a refinement module (RFM) to rectify them dynami-cally. Our architecture enforces the proposed decoder andRFM to benefit from each other to boost the final perfor-mance. Extensive experiments show that our approach sig-nificantly outperforms other approaches with less trainingcost. Additionally, our WeCLIP also obtains promising re-sults for fully supervised settings. The code is available at",
  ". Introduction": "Weakly supervised semantic segmentation (WSSS) aims to learn a pixel-level segmentation model fromweak supervision so as to reduce the manual annotation ef-forts. The common weak supervision signals contain scrib-ble , bounding-box , point and image-level la-bels . Among these supervisions, image-levelannotation is the most popular one, as such annotations canbe easily obtained through web-crawling.There are two training solutions for WSSS with image-level labels: multi-stage training and single-stage training.For existing single-stage approaches, their backbones rely",
  "*Corresponding author": "dogdog dog ImageNet CLIP CLIP ImageNet seg. cls. cls. seg. cls. seg. TrainedFrozen Cls. Classification process Seg. Segmentation process (a)(b) (c) text text P.S. P.S. P.S. P.S.Pseudo label . Comparisons between our approach and other single-stage or CLIP-based approaches. (a) Previous single-stage ap-proach, which uses a trainable ImageNet pre-trained back-bone with trainable classification and segmentation process. (b)Previous CLIP-based approach, which is a multi-stage approachthat uses the Frozen CLIP model to produce pseudo labels andtrains an individual ImageNet pre-trained segmentation model. (c)Our approach. Our approach is a single-stage approach that usesa frozen CLIP model as the backbone with a trainable segmenta-tion process, significantly reducing the training cost. on pre-training on ImageNet and fine-tuning duringtraining, as in (a). Such single-stage training focuses on using one model to directly segment objects withweak signals as supervision. The primary consideration ofprevious single-stage architectures is to online refine theClass Activation Map (CAM) or to improve the seg-mentation branch . Due to the complicated architec-ture, single-stage approaches perform normally worse thanmulti-stage approaches.On the other hand, multi-stage training attempts to utilizeseveral individual models to form a training pipeline , where offline pixel-level pseudo labels are firstlygenerated from weak labels using CAM and then a seg-mentation model is trained with such pseudo labels. SinceCAM can only highlight discriminate regions, many previ-ous approaches focus on improving the quality of CAM for better pseudo labels. Besides, some recentmulti-stage approaches attempt to introduce",
  "arXiv:2406.11189v1 [cs.CV] 17 Jun 2024": "Contrastive Language-Image Pre-training (CLIP) forWSSS. Trained on 400 million image-text pairs, CLIP es-tablishes a strong relationship between the image and text,demonstrating great ability to locate objects . Based on this, existing approaches use CLIP toimprove CAM, providing surprisingly high-quality pseudolabels. They follow the pipeline in (b). However,these methods only use the CLIP model to improve CAMfor better pseudo labels. The potential of the CLIP model tobe directly used as the backbone to extract strong semanticfeatures for segmentation prediction is not explored.In this paper, we propose a CLIP-based single-stagepipeline for weakly supervised semantic segmentation(WeCLIP) in which the CLIP model can be directly appliedfor segmentation prediction, as demonstrated in (c).Specifically, we adopt the frozen CLIP model as the back-bone, followed by a newly designed light frozen CLIP fea-ture decoder, where the CLIP backbone does not need anytraining or fine-tuning. Our decoder can successfully inter-pret the frozen CLIP features to conduct the segmentationtask with a small number of learnable parameters.We utilize the frozen CLIP backbone to generate CAMsfor providing pixel-level pseudo labels to train our de-coder.However, the frozen backbone can only providestatic CAM, which means pseudo labels cannot be im-proved during training. The same errors in pseudo labelslead to uncorrectable optimization in the wrong directions.Thus, we propose a Frozen CLIP CAM Refinement module(RFM) to rectify the static CAM dynamically. Particularly,our RFM utilizes the dynamic features from our decoderand the prior features from the frozen CLIP backbone toestablish high-quality pair-wise feature relationships to re-vise the initial CAM, leading to higher-quality pseudo la-bels. With such a design, our proposed two modules benefitfrom each other: refined pseudo labels provide more accu-rate supervision to train the decoder, and the trained decoderbuilds more reliable feature relationships for RFM to gen-erate accurate pseudo labels.Extensive experiments show that our approach achievesnew state-of-the-art performances on both the PASCALVOC 2012 and MS COCO datasets and significantly out-performs other approaches by a large margin. Further, ourapproach also achieves satisfactory performance for fullysupervised semantic segmentation. More importantly, sinceWeCLIP has a frozen backbone, it only requires a smallquantity of training cost, i.e., 6.2GB GPU memory and lessthan 6M learnable parameters, much less than other weaklyor fully supervised approaches.Our contributions are summarized as: We find that the CLIP backbone can be directly used forweakly supervised semantic segmentation without fine-turning. With our designed decoder, the frozen CLIP fea-ture is directly interpreted as semantic information to seg-",
  "ment objects, building a strong single-stage solution": "To overcome the drawback that the frozen backbone onlyprovides static pseudo labels, we design a Frozen CLIPCAM Refinement module (RFM) to dynamically renewthe initial CAM to provide better pseudo labels to trainour model. With less training cost, our approach significantly outper-forms previous approaches, reaching a new state-of-the-art performance for weakly supervised semantic segmen-tation (mIoU: 77.2% on VOC 2012 test set, 47.1% onCOCO val set). Moreover, our approach also shows greatpotential for fully supervised semantic segmentation.",
  ". Weakly Supervised Semantic Segmentation": "Weakly supervised semantic segmentation with image-levelsupervision attracts more attention than otherweak supervisions due to less human effort. Thereare two main solutions: multi-stage approaches and single-stage approaches .The key to the multi-stage solution is to generate high-quality pseudo labels. For example, RIB designed amargin loss in the classification network to reduce the infor-mation bottleneck, producing better pixel-level responsesfrom image-level supervision. Du et.al. proposed apixel-to-prototype contrast strategy to impose feature se-mantic consistency to generate higher-quality pseudo la-bels. MCTformer designed multi-class tokens in thetransformer architecture to produce class-specific attentionresponses to generate refined CAM. Some recent multi-stage approaches attempted to introduce CLIP for this task.CLIMS utilized the CLIP model to activate more com-plete object regions and suppress highly related backgroundregions. CLIP-ES proposed to use the softmax func-tion in CLIP to compute the GradCAM . With carefullydesigned text prompts, the GradCAM of CLIP provided re-liable pseudo labels to train the segmentation model.Previoussingle-stagesolutionsadoptedtheIma-geNet pre-train model as the backbone to concurrentlylearn the classification and segmentation tasks, and mostof them focused on improving segmentation by providingmore accurate supervision or constraining its learning. Forexample, RRM proposed to select reliable pixels as su-pervision for the segmentation branch. 1Stage designeda local consistency refinement module to directly generatesemantic masks from image-level labels. AA&AR pro-posed an adaptive affinity loss to enhance semantic prop-agation in the segmentation branch.AFA designedan affinity branch to refine CAMs to generate better onlinepseudo labels. ToCo proposed token contrast learningto mitigate over-smoothing in online CAM generation, thusproviding better supervision for segmentation.",
  "pooling": ". Framework of our WeCLIP. The image is input to the Frozen CLIP image encoder to generate the image features, and classlabels are used to build text prompts and then input to the Frozen CLIP text encoder to generate the text features. The classification scoresare generated based on the distance between the pooled image and text features. Using GradCAM, we can generate the initial CAMMinit. Then, the frozen image features from the last layer of each transformer block are input to our decoder to generate the final semanticsegmentation predictions. Meanwhile, the affinity map Af from our decoder and the multi-head attention maps As from CLIP are input toour RFM to establish refining maps R to refine Minit as Mf. After post-processing, it will be used as the supervision to train our decoder. The CLIP model shows great effectiveness in the multi-stage solution, but using it as a single-stage solution, i.e.,directly learning to segment objects with image-level su-pervision, is not explored.",
  ". Fully Supervised Semantic Segmentation": "Fully supervised semantic segmentation aims to segmentobjects using pixel-level labels as supervision. Most pre-vious approaches are based on Fully Convolutional Net-work (FCN) architecture, such as DeepLab , PSP-Net and UperNet . Recent approaches introducedvision transformer as the backbone to improve per-formance by building global relationships. For example,PVT used a pyramid vision transformer for seman-tic segmentation.Swin designed a window-basedattention mechanism in the vision transformer to effec-tively improve attention computing. They added a Uper-Net head for semantic segmentation. MaskFormer and Mask2Former proposed universal image segmen-tation architecture by combining the transformer decoderand pixel decoder. No matter whether fully or weakly su-pervised semantic segmentation, almost all segmentation models rely on the ImageNet Pre-train models, and allthe model parameters require to train or finetune, which re-quires a large number of computing costs, while we used afrozen CLIP model as the backbone, leading to much lessresource on the computation.",
  ". Overview": "shows the whole framework of our approach, includ-ing four main modules: a frozen CLIP backbone (imageencoder and text encoder) to encode the image and text, aclassification process to produce initial CAM, a decoder togenerate segmentation predictions, a RFM to refine initialCAM to provide pseudo labels for training.The training pipeline is divided into the following steps: 1. First of all, the image is input to the CLIP image en-coder for image features. Besides, the foreground andbackground class labels are used to build text promptsand then input to the CLIP text encoder to generate thecorresponding text features. Note here both image andtext encoders are frozen during training. 2. Then, the classification scores are generated by com-puting distances between image features (after pooling)and text features. Based on classification scores, Grad-CAM is utilized to generate the initial CAM.",
  ". Frozen CLIP Feature Decoder": "We use the frozen CLIP encoder with ViT-B as the back-bone, which is not optimized during training. Therefore,how to design a decoder that interprets CLIP features to se-mantic features becomes a core challenge. We propose alight decoder based on the transformer architecture to con-duct semantic segmentation using CLIP features as input.Specifically, suppose the input image is I R3HW ,H and W represent the height and width of the image, re-spectively. After passing the CLIP image encoder, we gen-erate the initial feature mapsF linitNl=1 from the output ofeach transformer block in the encoder, where l representsthe index of the block. Then, for each feature map F linit,an individual MLP module is used to generate new corre-sponding feature maps F lnew:",
  ". Frozen CLIP CAM Refinement": "To provide supervision for the prediction P in Eq. (3), wegenerate the pixel-level pseudo label from the initial CAMof the frozen backbone. The frozen backbone can only pro-vide static CAM, which means pseudo labels used as su-pervision cannot be improved during training. The sameerrors in pseudo labels lead to uncorrectable optimizationin the wrong directions. Therefore, we design the FrozenCLIP CAM Refinement module (RFM) to dynamically up-date CAM to improve the quality of pseudo labels.We first follow to generate the initial CAM. Forthe given Image I with its class labels, I is input to theCLIP image encoder. The class labels are used to buildtext prompts and input to the CLIP text encoder.Then,the extracted image features (after pooling) and text fea-tures are used to compute the distance and further activatedby the softmax function to get the classification scores. Af-ter that, we use GradCAM to generate the initial CAMMinit R(|CI|+1)hw, where (|CI|+1) indicates all classlabels in the image I including the background. More de-tails can be found in our supplementary material or .To thoroughly utilize the prior knowledge of CLIP, theCLIP model is fixed. Although we find that such a frozenbackbone can provide strong semantic features for the ini-tial CAM with only image-level labels, as illustrated in(a), Minit cannot be optimized as it is generated fromthe frozen backbone, limiting the quality of pseudo labels.Therefore, how to rectify Minit during training becomes akey issue. Our intuition is to use feature relationships torectify the initial CAM. However, we cannot directly use theattention maps from the CLIP image encoder as the featurerelationship, as such attention maps are also fixed. Never-theless, the decoder is constantly being optimized, and weattempt to use its features to establish feature relationshipsto guide the selection of attention values from the CLIP im-age encoder, keeping useful prior CLIP knowledge and re-moving noisy relationships. With more reliable feature re-lationships, the CAM quality can be dynamically enhanced.In detail, we first generate an affinity map based on thefeature map Fu in Eq. (2) from our decoder:",
  ",else,(6)": "where Gl R11, and it is expanded to Gle Rhwhw forfurther computation. We use the average value of all Sl asthe threshold. If the current Sl is less than the threshold, itis more reliable, and we set its filter value as 1. Otherwise,we set the filter value as 0. Based on this rule, we keep high-quality attention maps and remove weak attention maps.We then combine Af and the above operation to buildthe refining map:",
  "M cinit,(8)": "where c is the specific class, M cf is the refined CAM forclass c, Rnor is obtained from R using row and columnnormalization (Sinkhorn normalization ). is a hyper-parameter. This part passes a box mask indicator torestrict the refining region. M cinit is the CAM for class cafter reshaping to Rhw1. Finally, Mf is input to the on-line post-processing module, i.e., pixel adaptive refinementmodule proposed in , to generate final online pseudolabels Mp Rhw.In this way, our RFM uses the updated feature relation-ship in our decoder to assess the feature relationship inthe frozen backbone to select reliable relationships. Then,higher-quality CAM can be generated with the help of morereliable feature relationships for each image. showsthe detailed comparison of generated CAM using differentrefinement methods. Our method generates more accurateresponses than the static refinement method proposed in and the initial CAM.",
  ". Loss Function": "In our RFM, we use the affinity map Af to select the at-tention map and build the final refining map. Therefore,the effectiveness of Af directly determines the quality ofthe online pseudo labels. Considering Af is generated us-ing the feature map Fu in our decoder, and is a learnablemodule, we propose a learning process for Af that uses theconverted online pseudo label from Mp as supervision.Specifically, Mp is first converted to the pixel-wise affin-ity label for each pair of pixels:",
  "L = Lce(P, Mp ) + Lce(Af, A),(10)": "where Lce is the cross-entropy loss, Mp RHW , and is the weighting parameter. P is the prediction in Eq. (3).With Eq. (10), more accurate feature relationships are estab-lished for higher-quality pseudo labels. In turn, with betterpseudo labels, more precise feature relationships are estab-lished. Thus, our decoder and RFM can benefit from eachother to boost the training.",
  ". Datasets": "Following the setting in most previous weakly super-vised semantic segmentation approaches , twodatasets are used to evaluate our approach: PASCAL VOC2012 and MS COCO-2014 . PASCAL VOC 2012is appended with SBD to expand the dataset, and thewhole dataset contains 10,582 training images, 1,446 val-idation images, and 1,456 test images with 20 foreground classes.The MS COCO-2014 dataset includes approxi-mately 82,000 training images and 40,504 validation im-ages with 80 foreground classes.Mean Intersection-over-Union (mIoU) is applied as theevaluation criterion.",
  ". Implementation Details": "We use the frozen CLIP backbone with the ViT-16-base ar-chitecture , N is a fixed number that equals 12. Fortraining on the PASCAL VOC 2012 dataset, the batchsizeis set as 4, and the maximum iteration is set as 30, 000. Fortraining on the MS COCO-2014 dataset, we set batchsize as8, and the maximum iteration as 80, 000.All other settings adopt the same parameters for twodatasets during training: We use AdamW as the op-timizer, the learning rate is 2e3 with weight decay 1e3,and all images are cropped to 320 320 during training. in Eq. (10) is set as 0.1, The dimension of the MLP module(Eq. (1)) in our decoder is set as 256. In of Eq. (3), threetransformer encoder (the multi-head number is 8) layers arecascaded to generate the final feature map, and each layersoutput dimension is 256. N0 in Eq. (6) is set as 6. is setas 2 in Eq. (8) following .Duringinference,weusethemulti-scalewith{0.75, 1.0}. Following previous approaches ,DenseCRF is used as the post-processing method torefine the prediction.",
  ". Comparison with State-of-the-art Methods": "In Tab. 1, we compare our approach with other state-of-the-art approaches on the PASCAL VOC 2012 dataset. It canbe seen that our WeCLIP reaches 76.4% and 77.2% mIoUon val and test sets, both of which significantly outperformother single-stage approaches by a large margin. Specifi-cally, compared to ToCo , the previous state-of-the-artsingle-stage approach, our WeCLIP brings 5.3% and 5.0%mIoU increase on val and test set, respectively. Besides,CLIP-ES is the previous state-of-the-art multi-stage ap-proach, and it is also a CLIP-based solution. Our approachperforms much better than it, with 3.6% and 3.3% mIoUincrease.Tab. 2 shows the comparisons between our approach andprevious state-of-the-art approaches on MS COCO-2014val set. Our approach achieves new state-of-the-art perfor-mance, reaching 47.1% mIoU. Compared to other single-stage approaches, our WeCLIP brings more than 4.8%mIoU increase, which is a significant improvement. Moreimportantly, our WeCLIP also outperforms other multi-stage approaches by a clear margin with fewer trainingsteps. Considering our WeCLIP uses a frozen backbone,it shows great advantages to this task.In Tab. 3, we compare the training cost between our ap-proach and other state-of-the-art approaches on the PAS- . Comparison of state-of-the-art approaches on the PAS-CAL VOC 2012 val and test dataset. mIoU (%) as the evaluationmetric. I: image-level labels; S: saliency maps; L: language. mIoUas the evaluation metric. Without a specific description, results arereported with multi-scales and DenseCRF during inference.",
  "ours-WeCLIP (w/o CRF)ViTI+L74.975.2ours-WeCLIP (w/ CRF)ViTI+L76.477.2": "CAL VOC 2012 dataset. It can be seen that our approachonly needs 6.2G GPU memory, while other approaches re-quire at least 12G GPU memory. ToCo has less train-ing time than us, but its GPU memory is much higher thanour WeCLIP. More importantly, ToCo spent 4 hourswith 20,000 training iterations, while our WeCLIP spent4.5 hours with 30,000 iterations, which also shows the hightraining efficiency of our approach.In , we show some qualitative comparisons be-tween our approach and other approaches on the PASCALVOC 2012 and MS COCO-2014 val set. The visual resultsshow that our WeCLIP generates more accurate object de-tails than ToCo for both the two datasets.",
  "ours-WeCLIP (w/o CRF)ViTI+L46.4ours-WeCLIP (w/ CRF)ViTI+L47.1": "We cannot generate the prediction without it. Besides, in-troducing RFM brings a clear improvement, with a 6.2%mIoU increase. Since RFM is designed to improve the on-line pseudo labels, this increase also evaluates its effective-ness in generating higher quality pseudo labels.Tab. 5 reports the influence of the number of transformerlayers in our decoder, i.e., in Eq. (3). The performanceincreases when the layer number increases to 3. This is be-",
  "mIoU (%)73.274.474.972.670.3": "cause the limited size of the decoder cannot capture enoughfeature information, and it is easy to under-fit the features.With the increase of layer number, the decoder learns betterfeature representation. However, the performance drops ifthe layer number is larger than 3. One possible reason is thatdeeper decoder layers cause the over-fitting problem. Thus,it is reasonable that the performance drops after increasingto 4 or 5 for .In Tab. 6, we evaluate the effectiveness of our refiningmap. When only As is used, it means that all attention mapsare selected to refine the CAM, i.e., the same process pro-posed in , it generates 71.8% mIoU score. Note that",
  ". Performance on Fully-supervised SemanticSegmentation": "We also use our WeCLIP to tackle fully-supervised seman-tic segmentation. For fully-supervised semantic segmenta-tion, it provides accurate pixel-level labels, so we removethe frozen text encoder and our RFM, only keeping thefrozen image encoder and our decoder. Besides, the lossfunction removes the part related to A. The framework canbe found in our supplementary material.",
  "* results are reproduced by": "In Tab. 7, we evaluate our approach on PASCAL VOC2012 set for fully-supervised semantic segmentation. Sinceour approach utilizes a frozen backbone, it has less trainableparameters, but high-level segmentation performance ismaintained, showing its great potential for fully-supervisedsemantic segmentation.To illustrate why the vision feature from frozen CLIP canbe directly used for semantic segmentation, we show somefeature visualization results to compare the difference be-tween the CLIP features and ImageNet features in .We randomly select 200 images from the PASCAL VOC2012 train set. Without any training or finetune, we use ViT-B as the backbone and directly initialize it with frozen pre-train weights. It can be found that features belonging to the a ImageNetb CLIP .Feature visualization with T-SNE to show whyfrozen CLIP can be used for semantic segmentation. Each colorrepresents one specific category. (a) Frozen ImageNet pre-trainedfeature visualization of ViT-B. (b) Frozen CLIP pre-trained featurevisualization of VIT-B. It can be seen that without any retraining,the features belonging to the same class from the frozen CLIP aremore compact compared with that in (a). Best viewed in color. same class, pre-trained by CLIP, are denser and clustered,while features belonging to the same class, pre-trained byImageNet, are more sparse and decentralized. indi-cates that the extracted features from the CLIP model canbetter represent semantic information for different classes,making features belonging to different classes not confused.With such discriminative features, It is more convenient toconduct segmentation tasks.",
  ". Conclusion": "We propose WeCLIP, a single-stage pipeline based on thefrozen CLIP backbone for weakly supervised semantic seg-mentation.To interpret the frozen features for semanticprediction, we design a frozen CLIP feature decoder basedon the transformer architecture. Meanwhile, we proposea frozen CLIP CAM refinement module, which uses thelearnable feature relationship from our decoder to refineCAM, thus clearly improving the quality of pseudo labels.Our approach achieves better performance with less train-ing cost, showing great advantages to tackle this task. Wealso evaluate the effectiveness of our approach to fully-supervised semantic segmentation. Our solution offers adifferent perspective from traditional approaches that thetraining of the backbone is unnecessary. We believe the pro-posed approach can further boost research in this direction.Acknowledge: This work was supported by the Na-tional Key R&D Program of China (No.2022YFE0200300),the National Natural Science Foundation of China (No.62301613 & No. 62301451)), the Taishan Scholar Programof Shandong (No. tsqn202306130), the Suzhou Basic Re-search Program (SYG202316), Shandong Natural ScienceFoundation (No.ZR2023QF046), Qingdao PostdoctoralApplied Research Project (No.QDBSH20230102091),and Independent Innovation Research Project of UPC (No.22CX06060A).",
  "B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.Semantic contours from inverse detectors. In ICCV, pages991998, 2011. 5": "Tianyu Huang, Bowen Dong, Yunhan Yang, XiaoshuiHuang, Rynson WH Lau, Wanli Ouyang, and WangmengZuo.Clip2point: Transfer clip to point cloud classifica-tion with image-depth pre-training. In ICCV, pages 2215722167, 2023. 2 Peng-Tao Jiang, Yuqi Yang, Qibin Hou, and Yunchao Wei.L2g: A simple local-to-global knowledge transfer frame-work for weakly supervised semantic segmentation.InCVPR, pages 1688616896, 2022. 1, 2, 5, 6, 7",
  ". Initial CAM Generation": "We follow to generate the initial CAM. For a givenimage I with class label set CI, the image is input to thefrozen CLIP image encoder to generate the image featuremap as F Rd(hw), after passing global average pooling,the feature vector Fv R1 is generated. Meanwhile, Theclass labels set CI, with the pre-defined background labelset Cbg , are used to build text prompts using the text aclear origami {}, where is the specific class label. Thenthe text prompts are input to the text encoder to generatethe feature map Ft Rd(|CI|+|Cbg|). Using Fv and Ft, thedistance is compute as:",
  ". More Experimental Results": "To show the effectiveness of our approach, we comparethe quality of the pseudo labels with other multi-stage ap-proaches in Tab. 8. Since our WeCLIP is a single-stage solu-tion, we directly use segmentation predictions as the pseudo . Performance comparison about the generated pseudo la-bels between our approach and others on PASCAL VOC 2012train set. Note that we regard WeCLIP as a pseudo label gener-ation method and directly use its predictions as the pseudo labels.",
  "ours-WeCLIP-I+L78.2": "labels for comparison. In other words, by using the pre-diction as the pseudo labels, our approach can be regardedas a pseudo label generation part of the multi-stage solu-tion, which aims to provide high-quality pseudo labels totrain an individual segmentation model. It can be seen thatour approach significantly outperforms other approaches.For example, compared to the CLIP-based solutions suchas CLIMS and CLIP-ES , our approach brings outmore than 3% mIoU increase. shows some qualitativecomparisons, which also illustrates our approach can gener-ate high-quality pseudo labels. Ours are more complete andsmooth. . Ablation study of the input frozen image features fordecoder on PASCAL VOC 2012 val set. 1, 5, 8, 11, 12 indicatesthe value of N0. For example, N0 = 1 means that frozen imagefeatures from 1 to 12 layers (all layers) are selected as input forthe decoder.",
  "mIoU (%)74.974.774.674.574.3": "In Tab. 9, we conduct the ablation study to illustrate theinfluence of different frozen image features, which are se-lected as input for our decoder. When N0 = 1, image fea-tures from all blocks in the frozen image encoder are se-lected, and the best performance is generated. Besides, N0from 1 to 12, the mIoU score is decreased from 74.9% to74.3%, indicating that fewer features are selected, and lowerperformance is generated. The possible reason is that usingall features has a more comprehensive semantic representa-tion.Tab. 10 is the ablation study for the different supervisionsignals of Af. Mp means using the online pseudo labels for",
  "{1.0}74.0{0.5, 1.0}74.2{0.75, 1.0}74.9{0.5, 0.75, 1.0}74.4{0.75, 1.0, 1.25}74.8{0.75, 1.0, 1.5}74.5": "Tab. 12 shows the influence of the multi-scale strategyduring inference. It can be seen that {0.75, 1.0} performsbetter than other settings. Introducing a larger scale, suchas 1.5, does not improve the performance, showing that theFrozen CLIP backbone is not sensitive to the large scale.In , we show more feature visualization results tocompare the difference between the CLIP features and Im-ageNet features. For each pair visualization (each column),we randomly select 200 images from the PASCAL VOC2012 train set. All other settings are the same as our paper. a b . Feature visualization with T-SNE to show why frozen CLIP can be used for semantic segmentation. Each color representsone specific class. (a) Frozen ImageNet pre-trained feature of ViT-B. (b) Frozen CLIP pre-trained vision feature of VIT-B. It can be seenthat without any retraining, the features belonging to the same class from the frozen CLIP are denser and more clustered than the ImageNetpre-trained features. Best viewed in color.",
  "conv": ". Framework for fully-supervised semantic segmentation. Given an image, it passes the frozen CLIP image encoder to extract thefeature map, which is then input to our decoder to generate the final prediction. It can be found that features belonging to the same class,pre-trained by CLIP, are denser and clustered, while fea-tures pre-trained by ImageNet are more sparse and decen-tralized, which explains why the frozen CLIP feature can be directly used for semantic segmentation. indicatesthat the extracted features from the CLIP model can betterrepresent semantic information for different classes, makingfeatures belonging to different classes not confused. With",
  ". Background Text Set": "We follow CLIP-ES to define the background classset. For PASCAL VOC 2012 set, the set is {ground, land,grass, tree, building, wall, sky, lake, water, river, sea, rail-way, railroad, keyboard, helmet, cloud, house, mountain,ocean, road, rock, street, valley, bridge, sign}, For MSCOCO-2014, {sign, keyboard} is removed. Besides, thetext prompt for the background class is a clear origami{background class}."
}