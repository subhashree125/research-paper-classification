{
  "Abstract": "Transductive inference has been widely investigated infew-shot image classification, but completely overlookedin the recent, fast growing literature on adapting vision-langage models like CLIP. This paper addresses the trans-ductive zero-shot and few-shot CLIP classification chal-lenge, in which inference is performed jointly across a mini-batch of unlabeled query samples, rather than treating eachinstance independently. We initially construct informativevision-text probability features, leading to a classificationproblem on the unit simplex set. Inspired by Expectation-Maximization (EM), our optimization-based classificationobjective models the data probability distribution for eachclass using a Dirichlet law. The minimization problem isthen tackled with a novel block Majorization-Minimizationalgorithm, which simultaneously estimates the distribu-tion parameters and class assignments.Extensive nu-merical experiments on 11 datasets underscore the ben-efits and efficacy of our batch inference approach.Onzero-shot tasks with test batches of 75 samples, our ap-proach yields near 20% improvement in ImageNet accu-racy over CLIPs zero-shot performance. Additionally, weoutperform state-of-the-art methods in the few-shot setting.The code is available at:",
  ". Introduction": "The emergence of large-scale vision-language models likeCLIP has marked a significant turning point in repre-sentation learning . By integrating both visualand textual modalities, these models have shown remark-able potential in crafting generic and richly informative con-cepts. Unlike traditional vision models, often constrainedby task specificity, the representations gleaned from vision-language models are versatile, setting the stage for a breadthof downstream vision tasks and expanding the horizons ofwhat is achievable in the domain.Among the vision tasks that can be addressed with vision-language models, zero-shot and few-shot classifica-tion have particularly attracted attention. Notably, CLIP hasdemonstrated strong performance in zero-shot classification. Several subsequent works have leveraged few-shotdata, a few labeled samples in the target downstream task,to further improve CLIPs classification accuracy. Follow-ing on from the research on prompt learning in the NLPcommunity, CoOp and CoCoOp fine-tuned the pre-trained CLIP model using learnable textual tokens.An-other type of approaches, like CLIP-Adapter and TIP-Adapter provided CLIP with a parametric feature trans-formation, which generates adapted features and combinesthem with the original CLIP-encoded features.Despitetheir efficacy on few-shot classification benchmarks, thesemethods predominantly operate within the so-called induc-tive setting, where inference is conducted independently foreach query (i.e., test) sample. In contrast, in the transductive paradigm, one makesjoint predictions for a batch of query samples, taking ad-vantage of the query set statistics. The transductive set-ting for few-shot classification with vision-only models waspioneered in , and have since become prominent re-search subject, triggering an abundant, very recent litera-ture on the subject, e.g., ,to list a few. These transductive few-shot classifiers wereshown to significantly outperform their inductive counter-parts, with benchmarks indicating up to a 10% increasein classification accuracy . In fact, this is in line withwell-established theoretical facts in the classical literatureon transductive learning , which points to transduc-tive prediction as a way to alleviate the scarcity of labeleddata. Importantly, and beyond theoretical justification, thetransductive setting is highly relevant in a breadth of practi-cal computer vision scenarios, in which test data may comein mini-batches. This is the case, for instance, of onlinevideo streams and various types of time-series imaging, ofportable-device photos, or of pixel-level tasks such as seg-mentation.",
  "arXiv:2405.18437v1 [cs.CV] 8 Apr 2024": "vision-language pre-trained CLIP model. We first make thesurprising observation that standard clustering models, inthe zero-shot case, and recent transductive methods, in thefew-shot setting, do not bring improvements comparable tothose observed with vision-only models, scoring even be-low their inductive counterparts; see Tables 1 and 2. Thismight explain why the transductive setting, despite its popu-larity, has not been explored so far for vision-language mod-els. Potential questions that may fill this gap are (i) How tobuild informative text-image features for transductive infer-ence, leveraging the textual knowledge in vision-languagemodels? and (ii) Are the statistical assumptions underly-ing standard clustering and transductive inference methodsappropriate for text-image features? In light of these chal-lenges, this paper brings the following contributions:1. We propose a methodology to compute text-vision prob-ability feature vectors, setting the stage for transductivefew-shot classification specifically tailored for CLIP. 2. We reformulate the transductive zero-shot and few-shotclassification challenge as an optimization problem onthe unit simplex set by modeling the data with Dirichletprobability distributions. Crucially, the non-trivial de-ployment of the Dirichlet distributions brings substantialimprovements in comparison to the common statisticalmodels underlying standard clustering and transductivefew-shot methods (e.g. Gaussian). 3. We propose a novel block Majorization-Minimizationalgorithm that addresses our problem efficiently and ef-fectively, removing the need for cumbersome inner iter-ations in estimating the Dirichlet parameters. 4. We report comprehensive evaluations, comparisons andablations over 11 datasets, which point to the benefitsof our mini-batch inference approach. On zero-shot Im-ageNet tasks with batches of 75 samples, the proposedmethod scores near 20% higher than inductive zero-shotCLIP in classification accuracy. Additionally, we out-perform state-of-the-art methods in the few-shot setting.",
  ". Vision-language models": "Vision-Language models, like CLIP, integrate visual andtextual data to improve accuracy over various vision tasks.CLIP uses a dual-encoder structure, with one deep networkdedicated for image encoding and another one specilaizedfor text. This structure, along with proper projections atits bottleneck, yield image and text embeddings lying inthe same low-dimensional vector space. Trained on a largedataset of 400 million text-image pairs, CLIP maximizes thecosine similarity between text and image embeddings usinga contrastive loss. CLIP is pre-trained to match images withtext descriptions, making it well-suited for zero-shot pre-diction. At inference time, to classify an image x among",
  ". Few-shot classification": "Inductive v.s. transductive settingFew-shot image clas-sification with pre-trained vision models has been the sub-ject of extensive research recently . Within thisarea, the problem is tackled either in the transductive or in-ductive setting. The latter assumes that each instance in thetesting batch is classified independently, omitting the corre-lations or shared information among instances .In contrast, transductive inference is more comprehensive,as it makes joint predictions for the entire mini-batch ofquery samples, leveraging their statistics and shared infor-mation. Recent research has increasingly focused on trans-ductive few-shot learning, including, for instance, methodsbased on constrained clustering , label propagation, optimal transport , information maximiza-tion , prototype rectification , among other re-cent approaches . It has been consistently observedin this body of literature that the gap in accuracy betweentransductive and inductive methods could be considerable. Few-shot CLIPBeyond its zero-shot capabilities, theCLIP model has also been explored for few-shot image clas-sification. In , the authors evaluated linear probe, whichperforms a simple fine-tuning of the visual encoders finallayer using a few-shot support set (i.e., a few labeled sam-ples in the downstream task). This approach has proven tobe relatively ineffective in few-shot scenarios. Since then,a recent body of works have explored CLIPs few-shot gen-eralization. For instance, there is a noticeable emergenceof prompt learning methods in computer vision, focusingon this specific problem . Inspired by inten-sive recent prompt learning research in NLP , thesemethods fine-tune learnable input text tokens using the few-shot support set. A different type of approaches, coinedadapters , fine-tune the encoded features rather thaninput text. For example, CLIP-Adapter incorporatesadditional bottleneck layers to learn new features, whileperforming residual-style blending with the original pre-trained features. In a similar spirit, TIP-Adapter bal-ances two prediction terms, one summarizing adaptively theinformation from the support set and the other preservingthe textual knowledge from CLIP. All of these recent meth-ods belong to the inductive family. To the best of our knowl-edge, our work is the first to explore transduction for CLIPs",
  "\"A photo of a {}\"": ". Given a transductive few-shot task, both visual and textual information are extracted from the images and class-wise prompts.The embeddings are next combined into vision-text probability vectors. Classification is carried out on the simplex set of RK using thelabels of the support set. An empty support set corresponds the the zero-shot scenario, which is akin to a clustering problem.",
  "N is the number of images in each randomly sampledtask, with (xn)1nN denoting the set of images": "K is the total number of distinct classes in the whole dataset, among which a much smaller set of randomly sam-pled classes might appear in each mini-batch task, andmight differ from one batch to another. Hence, apart fromknowing the set of K classes in the whole data, as in stan-dard inductive inference , our transductive set-ting do not assume any additional knowledge about theparticular set of classes that might appear randomly ineach mini-batch. S {1, . . . , N} indicates the indices of samples withinthe support set in the few-shot setting. For all n S, onehas access to the one-hot-encoded labels yn {0, 1}K,such that for all k {1, . . . , K}, yn,k = 1 if xn is aninstance of class k, yn,k = 0 otherwise.",
  ". Computing informative feature vectors": "A seemingly intuitive approach to tackle the transductivechallenge might be to use the visual embeddings obtainedfrom CLIPs visual encoder as the input features for theclassifier. This is analogous to CLIPs linear probe whenit operates inductively. We pinpoint two main difficultiesraised by this approach:1. Overlooking textual information: A significant limi-tation of this method is that it omits the modelstextualknowledge. This is problematic as textual information isone of CLIPs most powerful features. 2. Normalization dilemma:CLIPs pre-training maxi-mizes the scalar product between normalized textualand visual embeddings. Using normalized embeddingscan introduce complexities in data distribution model-ing, which, if misjudged, can impact the method inter-pretability and accuracy.While some works in the classification literature have ex-plored spherical distributions like the Von Mises-Fisher and the Fisher-Bingham , our approachdiffers to address both issues mentioned above.Our strategy consists in defining, for every n{1, . . . , N}, the feature vector for the data sample xn asCLIPs zero-shot probability. Precisely, we define",
  ". Data distribution": "Given feature vectors lying within the unit simplex set ofRK, we advocate modeling the data using Dirichlet distri-butions. The Dirichlet distribution extends the beta distri-bution into higher dimensions, serving as a natural choicefor modeling probability vectors over the simplex. For eachclass k within the set {1, . . . , K}, the data is assumed tofollow a Dirichlet distribution, characterized by positive pa-rameters k = (k,i)1iK (0, +)K, which describesthe distribution shape. An illustration in R3 is given is Ap-pendix A. Mathematically, the density function is given by,for every z = (zi)1iK RK,",
  ". Simplex-based classification criterion": "The proposed method simultaneously determines: (i) thesoft assignment vectors u = (un)1nN within the sim-plex (K)N, where the k-th component un,k of vectorun specifies the probability for the n-th sample belong-ing to class k; (ii) the Dirichlet distribution parameters = (k)1kK where each k is a K-dimensional vectorwith nonnegative components. We achieve this through thefollowing maximum-likelihood estimation",
  "F(k) q(k; k),F(k) = q(k; k).(12)": "The efficiency of the procedure (11) is highly dependent onthe choice of the majorant. In , the author proposed amajorant function of (10) which consists in linearizing theconcave term k ln Ki=1 k,iat k. The re-sulting MM algorithm was used for simplex clustering in. However, minimizing this majorant requires invert-ing the digamma function (i.e., the derivative of the log-Gamma function) with a Newton method, which can jeop-ardize the numerical convergence and slow down the overallalgorithm.In the following lemma, we introduce a novel tight ma-jorant of Fk which yields closed-form updates, thereforeavoiding sub-iterations within the MM algorithm.",
  ". Minimization step w.r.t assignment variable": "Let the iteration number N and = (k)1kK ((0, +)K)K be fixed. Because of the partition complex-ity term in (9), the direct minimization of the partial func-tion with respect to un, for every n Q, is not closed form.Since the partition complexity penalty is concave, we pro-pose to replace it by its linear upper-bound, leading us to",
  ". Global algorithm and class-assignment": "Finally, given the estimation steps on the assignment vari-ables u=(un)nQ and on the Dirichlet parameters(k)1kK derived respectively in Sections 4.2 and 4.1,our complete procedure to tackle the minimization problemin (6) is detailed in Algorithm 2. We name it EM-Dirichletas it shares close links to the EM algorithm, as it will beestablished in Proposition 1 in .In the zero-shot scenario, the tasks at hand can be seen asa form of simplex clustering. There exists a straightforwardmethod to map each cluster to a corresponding class labelin an injective manner. Let (Ck)kK denote the set of non-empty clusters found with a clustering method, for instanceours, with K a subset of k {1, . . . , K} and for all k K,Ck a subset of Q. We proceed in the following way:1. For each k K, calculate the mean of cluster k, mk =(mk,)1K K, as mk =1",
  ". Links with other clustering and transductivefew-shot objectives": "The general log-likelihood model fitting objective in (7),also referred to as probabilistic K-means , is well-established in the clustering literature. Indeed, it is a gen-eralization of the ubiquitous K-means, which correspondsto the particular choice of the Gaussian distribution forthe densities in (7), with covariance matrices fixed to theidentity matrix.This general objective has a strong, in-herent bias towards K-balanced partitions, a theoreticallywell-established fact in the clustering literature . Tomitigate this bias and address realistic, potentially imbal-anced few-shot query sets, the recent transductive few-shotmethod in coupled the MDL term in (9) with the stan-dard K-means objective. This corresponds to the generaldata-fitting function we tackle in (7), but with the likeli-hood densities assumed to be Gaussian. As we will see inour experiments (), the non-trivial deployment of theDirichlet model is crucial, outperforming significantly in CLIPs few-shot setting. Furthermore, we show in thefollowing an interesting result, which connects the generalunbiased clustering problem we propose in (6), to the well-known Expectation-Maximization (EM) algorithm for mix-ture models [3, p.438]. Indeed, optimizing the objective in(6) could be viewed as a generalization of EM, enabling to",
  ". Experiments": "We evaluated our method on 11 publicly accessible im-age classification datasets which were also utilized in CLIP: ImageNet , Caltech101 , OxfordPets ,StanfordCars , Flowers102 , Food101 , FGV-CAircraft , SUN397 , DTD , EuroSAT andUCF101 . To ensure reproducibility, we adhere to thedataset splits provided by CoOp and use the promptsemployed in TIP-Adapter . All experiments are con-ducted using CLIPs pre-trained ResNet50 visual encoder.The temperature in the probabilities (2) is fixed to T = 30.",
  ". Zero-shot": "Tasks generationFor generating query sets in our trans-ductive zero-shot setting, we employ a practical approachthat maintains manageable batch sizes. At each new task(mini-batch), we randomly select the classes that will berepresented in the query set, with the actual number of dis-tinct classes ranging from 3 to 10, also selected at random.It is important to note that the set of classes occurring ineach batch remain undisclosed, and vary randomly fromone batch to another, ensuring that the clustering task is stillperformed over all K potential classes present in the wholedataset. Subsequently, we randomly select |Q| = 75 imagesin to the chosen classes to constitute the query set. Duringtransductive inference, the query set of each task is treatedindependently of the other randomly sampled tasks. Comparative methodsWe conduct a comparative eval-uation of our clustering methodology, EM-Dirichlet, andits variant utilizing hard assignments, denoted as Hard EM-Dirichlet, against a range of clustering objective functionsand algorithms: Hard and soft K-means [32, p.286], EM forGaussian mixtures with identity covariance (EM-Gaussian(cov. Id)) and with diagonal covariance (EM-Gaussian (cov.diag)) [3, p.438], and Hard KL K-Means . Furthermore,our comparison provides a full ablation study of the termsin general objective function (6):1. The log-likelihood model fitting term (7), which variesacross Gaussian (employed in Hard K-means, Soft K-means, EM-Gaussian), and Dirichlet (in our method).",
  ". The entropic barrier (8) featured in both Soft K-meansand the EM-based approaches": "3. The MDL partition-complexity term (9), incorporatedexclusively in the EM methods.Initialization is uniform across different clustering tech-niques, utilizing CLIPs predictions from Equation (2). Inall EM-based methods, the regularization parameter is setaccording to =5K |Q|, to maintain consistency acrosscomparisons. ResultsWe assess the clustering methods on zero-shottasks, using both the visual embeddings and the combinedtext-vision feature vectors. We also include the zero-shotclassification results from CLIP. In , we report av-erage accuracy over 1,000 tasks using the graph cluster-to-classes assignment described in .3. con-veys several crucial messages: Clustering visual embeddings alone does not suffice tosurpass inductive CLIPs zero-shot performance. Incor-porating textual information via probability features en-hances the performance, even for methods initially de-signed for Gaussian distributions.",
  ". Few-shot": "Task generationWe follow the realistic transductive few-shot evaluation protocol proposed recently in . Specifi-cally, the query sets are constructed with a fixed number ofeffective classes keff = 5, from which |Q| samples are ran-domly selected. This approach aligns with established few-shot protocols in the literature . These classesremain undisclosed during inference, ensuring the task isa K-way classification. The support set is created by uni-formly selecting s images from each of the K classes. Theensuing results are derived performing few-shot tasks with1, 2, 4, 8, and 16 shots. During inference on the test set, thesize of the query set is set to |Q| = 75, while for validation,the size is reduced to|Q| = 35 due to data limitations.",
  "Hyper-parametersParameter in EM-Dirichlet is set tothe fixed value = keff": "K |Q|. Methods with tunable hyper-parameters are fine-tuned using the validation split providedwith each dataset. In line with , validation is performedon five s-shot tasks across all datasets and for every shotnumber. These tasks, crafted as previously detailed, usesupport and query instances drawn from the validation set.The hyper-parameters are then optimized through a gridsearch to maximize accuracy on the validation set. ResultsWe evaluate the accuracy of our proposed trans-ductive methods, EM-Dirichlet and Hard EM-Dirichlet,against several recent transductive few-shot methods, in-cluding BD-CSPN , Laplacian Shot , -TIM ,and PADDLE . Additionally, we benchmark againsttwo inductive few-shot methods designed for CLIP: TIP-Adapter and CoOp . The results, averaged across1, 000 tasks with 4 shots, are presented in and forthe other number of shots in Appendix G.Our method surpasses competing approaches on the ma-jority of datasets, with a more pronounced advantage ob-served on challenging datasets that have a large number of",
  "Hard EM-Dirichlet87.950.860.591.790.589.875.324.272.680.278.372.96.97101": ". Evaluation of our approach against two benchmarks 1) inductive methods specifically designed for few-shot classification usingCLIP, and 2) transductive few-shot methods applied to probability feature vector classification. The analysis encompasses 1,000 distinct 4shots tasks. We also report average execution time for a single task, computed over 1,000 tasks, on the ImageNet dataset. classes, such as SUN397 and ImageNet. The accuracy gapbetween our method and the inductive ones shows the bene-fits of transductive inference. On the other hand, the inferiorperfomance of other transductive methods can be attributedto their lack of adaptability to simplex classification.Interestingly, our results indicate that on some datasetssuch as Food101, our method perform better in the zero-shot than in the few-shot setting. This is consistent withRadford et al. , suggesting that few labeled examplescan negatively impact classification, possibly due to outliersor ambiguous examples in the support set.Lastly, we observe that inductive methods outperformours on the EuroSAT dataset. This might be due to the in-clusion of text information in the vision-text features. While",
  "David M. Blei, Andrew Y. Ng, and Michael I. Jordan. LatentDirichlet allocation. Journal of Machine Learning Research,3(Jan):9931022, 2003. 5": "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.Food-101mining discriminative components with randomforests. In European Conference on Computer Vision, pages446461, Zurich, Switzerland, 2014. Springer. 6 Malik Boudiaf, Imtiaz Ziko, Jerome Rony, Jose Dolz, PabloPiantanida, and Ismail Ben Ayed. Information maximizationfor few-shot learning. Advances in Neural Information Pro-cessing Systems, 33:24452457, 2020. 1, 2 Malik Boudiaf, Etienne Bennequin, Myriam Tami, AntoineToubhans, Pablo Piantanida, Celine Hudelot, and Ismail BenAyed. Open-set likelihood maximization for few-shot learn-ing. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2023. 2 Y. Boykov, H. N. Isack, C. Olsson, and I. Ben Ayed. Volu-metric bias in segmentation and reconstruction: Secrets andsolutions. In IEEE/CVF International Conference on Com-puter Vision (ICCV), 2015. 4, 6",
  "Onur C Hamsici and Aleix M Martinez.Spherical-homoscedastic distributions: The equivalency of sphericaland normal distributions in classification. Journal of Ma-chine Learning Research, 8(7), 2007. 3": "Md Abul Hasnat, Julien Bohne, Jonathan Milgram, StephaneGentric, and Liming Chen.Von Mises-Fisher mixturemodel-based deep learning: Application to face verification.arXiv preprint arXiv:1706.04264, 2017. 3 Patrick Helber, Benjamin Bischke, Andreas Dengel, andDamian Borth. EuroSAT: A novel dataset and deep learn-ing benchmark for land use and land cover classification.IEEE Journal of Selected Topics in Applied Earth Observa-tions and Remote Sensing, 12(7):22172226, 2019. 6 Z hong, D Friedman, and D Chen. Factual probing is [mask]:Learning vs. learning to recall. In Conference of the NorthAmerican Chapter of the Association for Computational Lin-guistics (NAACL), 2021. 2 Shell Xu Hu, Da Li, Jan Stuhmer, Minyoung Kim, and Tim-othy M Hospedales. Pushing the limits of simple pipelinesfor few-shot learning: External data and fine-tuning makea difference. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 90689077, 2022. 2, 7",
  "M. Kearns, Y. Mansour, and A. Ng. An information-theoreticanalysis of hard and soft assignment methods for clustering.In Conference on Uncertainty in Artificial Intelligence (UAI),1997. 6": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.3d object representations for fine-grained categorization. InProceedings of the IEEE international conference on com-puter vision workshops, pages 554561, 2013. 6 Michalis Lazarou, Tania Stathaki, and Yannis Avrithis. It-erative label cleaning for transductive and semi-supervisedfew-shot learning. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 87518760,2021. 2 Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, WanliOuyang, Jing Shao, Fengwei Yu, and Junjie Yan.Su-pervision exists everywhere: A data efficient contrastivelanguage-image pre-training paradigm.In InternationalConference on Learning Representations, 2022. 1 Jinlu Liu, Liang Song, and Yongqiang Qin. Prototype rec-tification for few-shot learning. In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part I 16, pages 741756. Springer,2020. 1, 2, 7, 8 Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, EunhoYang, Sung Ju Hwang, and Yi Yang.Learning to propa-gate labels: Transductive propagation network for few-shotlearning. In International Conference on Learning Repre-sentations, 2019. 1, 2, 7",
  "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypicalnetworks for few-shot learning. Advances in Neural Infor-mation Processing Systems, 30, 2017. 7": "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.UCF101:A dataset of 101 human actions classes fromvideos in the wild. Technical report, Center for Researchin Computer Vision, University of Central Florida, 2012.arXiv:1212.0402. 6 Ran Tao, Hao Chen, and Marios Savvides. Boosting trans-ductive few-shot fine-tuning with margin-based uncertaintyweighting and probability regularization. In IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 1575215761, 2023. 1, 2 Long Tian, Jingyi Feng, Xiaoqiang Chai, Wenchao Chen,Liming Wang, Xiyang Liu, and Bo Chen.Prototypes-oriented transductive few-shot learning with conditionaltransport. In IEEE International Conference on ComputerVision (ICCV), pages 1631716326, 2023. 1, 2 Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenen-baum, and Phillip Isola. Rethinking few-shot image classi-fication: a good embedding is all you need?In EuropeanConference on Computer Vision (ECCV), 2020. 2 Daniel J. Trosten, Rwiddhi Chakraborty, Sigurd Lkse,KristofferKnutsenWickstrm,RobertJenssen,andMichael C. Kampffmeyer.Hubs and hyperspheres: Re-ducing hubness and improving transductive few-shot learn-ing with hyperspherical embeddings. In IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), pages75277536, 2023. 1, 2",
  "(25)": "with respect to and . The process involves two steps:expectation and maximization, and the algorithm itera-tively generates sequences {()}N K and, for everyk {1, . . . , K}, {()k }N (0, +)K.During the expectation step, for a given iteration num-ber N, we compute the expected responsibilities. Foreach query sample n Q, we define u()n= (u()n,k)1kKby",
  "Ki=1 ()i pzn | ()i.(26)": "This quantity corresponds to the probability of the datapoint n belonging to class k based on the current estimatesof () and ()k .In the maximization step, we derive an upper boundfor the log-likelihood at the current iterate using the re-sponsibilities calculated in the expectation step, along withJensens inequality. This majorization reads",
  "G. Additional results in the few-shot setting": "In addition to the results in the 4-shot case presented in, we provide the results for other number of shots. displays the accuracy as a function of the numberof shots. This analysis includes our methods EM-Dirichletand Hard EM-Dirichlet, other transductive methods (BDC-SPN, Laplacian Shot, -TIM, PADDLE), and the inductiveTip-Adapter method. We did not evaluate CoOp because ofthe prohibitive time required to run the method, as under-lined in . We observe that our method significantlyoutperforms its closest competitor, TIP, on the challengingSUN397 and ImageNet datasets, as well as on the averageof the 11 datasets. This gap gets even wider when the num-ber of shots increases. Complete results for all datasets aregiven in .",
  "H. Ablation study on each term of the objective": "We provide an ablation study on our objective function,which minimizes L + + under simplex constraints,where L is the log-likelihood, a barrier term, and apartition complexity term promoting fewer clusters. Notethat, when removing barrier term , our update step for theassignment variables (Eq. (15) without the barrier term)amounts to solving a linear programming problem, result-ing in integer solutions (i.e., hard assignments), akin to whatwe coined Hard EM-Dirichlet. demonstrates the effect of each term. The parti-tion complexity term significantly enhances performance.In contrast, the barrier term , in isolation, does not im-prove performance. However, when combined with , it",
  "I. Using the similarity scores as feature vectors": "One might consider directly using the visual-textual embed-dings as input features (specifically, the cosine similarities)without applying a softmax function. It could be hypothe-sized that methods targeting a Gaussian distribution mightperform more effectively with these raw features than withprobability features. However, as indicated in , thisis not the case. Employing a Gaussian distribution withinthe joint visual-textual embedding space actually leads todecreased accuracy when compared to our method that uti-lizes probability features."
}