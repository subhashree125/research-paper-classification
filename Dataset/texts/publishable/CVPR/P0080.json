{
  "Abstract": "Event cameras capture the world at high time resolution andwith minimal bandwidth requirements. However, event streams,which only encode changes in brightness, do not contain suffi-cient scene information to support a wide variety of downstreamtasks. In this work, we design generalized event cameras thatinherently preserve scene intensity in a bandwidth-efficient man-ner. We generalize event cameras in terms of when an event isgenerated and what information is transmitted. To implementour designs, we turn to single-photon sensors that provide digi-tal access to individual photon detections; this modality givesus the flexibility to realize a rich space of generalized eventcameras. Our single-photon event cameras are capable of high-speed, high-fidelity imaging at low readout rates. Consequently,these event cameras can support plug-and-play downstreaminference, without capturing new event datasets or designingspecialized event-vision models. As a practical implication, ourdesigns, which involve lightweight and near-sensor-compatiblecomputations, provide a way to use single-photon sensors with-out exorbitant bandwidth costs.",
  ". Introduction": "Event cameras sense the world at high speeds, pro-viding visual information with minimal bandwidth and power.They achieve this by transmitting only changes in scene bright-ness, when significant events occur. However, there is a cost.Raw event data, a sparse stream of binary values, does nothold sufficient information to be used directly with mainstreamvision algorithms. Therefore, while event cameras have beensuccessful at certain tasks (e.g., object tracking , obstacleavoidance , and high-speed odometry ),they are not widely deployed as general-purpose vision sen-sors, and often need to be supplemented with conventionalcameras . These limitations are holding back this",
  "otherwise powerful technology": "Is it possible to realize the promise of event cameras, i.e.,high temporal resolution at low bandwidth, while preservingrich scene intensity information? To realize these seemingly con-flicting goals, we propose a novel family of generalized eventcameras. We conceptualize a space of event cameras alongtwo key axes ( (top)): (a) when to transmit information,formalized as a change detection procedure ; and (b) whatinformation to transmit, characterized by an integrator thatencodes incident flux. Existing event cameras represent oneoperating point in this (,) space. Our key observation is thatby exploring this space and considering new (,) combina-tions, we can design event cameras that preserve scene intensity.We propose more general integrators, e.g., that represent fluxaccording to motion levels, that span spatial patches, or thatemploy temporal coding ( (middle)). We also introduce ro-bust change detectors that better distinguish motion from noise,by considering increased spatiotemporal contexts and modelingnoise in the sensor measurements. Despite their conceptual appeal, physically implementinggeneralized event cameras is a challenge. This is because the req-uisite computations must be performed at the sensor to achievethe desired bandwidth reductions. For example, existing eventcameras perform simple integration and thresholding operationsvia analog in-pixel circuitry. However, more general (,)combinations are not always amenable to analog implementa-tions; even feasible designs might require years of hardwareiteration and production scaling. To build physical realizationsof generalized event cameras, we leverage an emerging sen-sor technology: single-photon avalanche diodes (SPADs) thatprovide digital access to photon detections at extremely highframe rates (100 kHz). This allows us to compose arbitrarysoftware-level signal transformations, such as those required bygeneralized event cameras. Further, we are not locked to a partic-ular event camera design and can realize multiple configurationswith the same sensor.",
  "kM4fwB87nD+LbjPA=</latexit>x": ". Generalized event cameras. (top) Event cameras generate outputs in response to abrupt changes in scene intensity. We describe this asa combination of a low-pass integrator and a threshold-based change detector. (middle) We generalize the space of event cameras by designingintegrators that capture rich intensity information, and more reliable change detectors that utilize larger spatiotemporal contexts and noise-awarethresholding (Secs. 4.1 to 4.3). Unlike existing events, our generalized event streams inherently preserve scene intensity, e.g., this ping-pongball slingshotted against a brick wall backdrop. (bottom) Generalized event cameras enable high-fidelity bandwidth-efficient imaging: providing3025 FPS reconstructions with a readout equivalent to a 30 FPS camera. Consequently, generalized events facilitate plug-and-play inference on amultitude of tasks in challenging scenarios (insets depict the extent of motion over 30 ms). Implications: extreme, bandwidth-efficient vision. Gener-alized event cameras support high-speed, high-quality imagereconstruction, but at low bandwidths quintessential of currentevent cameras. For example, (middle, bottom) showsreconstructions at 3025 FPS that have an effective readout of a30 FPS frame-based camera. Further, our methods have stronglow-light performance due to the SPADs single-photon sensi-tivity. As we show in (bottom), preserving scene intensityfacilitates plug-and-play inference in challenging scenarios, withstate-of-the-art vision algorithms. Critically, this does not re-quire retraining vision models or curating dedicated datasets,which is a significant challenge for unconventional imagers.This plug-and-play capability is vital to realizing universal eventvision that retains the benefits of current event cameras. Scope. We consider full-stack event perception: we conceptu-alize a novel space of event cameras, provide relevant single-photon algorithms, analyze their imaging capabilities and rate-distortion trade-offs, and show on-chip feasibility. We demon-strate imaging capabilities in Secs. 5.1 and 5.2 using the SwissS- PAD2 array , and show viable implementations of our al-gorithms for UltraPhase , a recent single-photon computeplatform. All of these are critical to unlocking the promise ofevent cameras. However, our objective is not to develop anintegrated system that incorporates all these components; thispaper merely takes the first steps toward that goal.",
  ". Related Work": "Event camera designs. Perhaps most widespread is the DVSevent camera , where each pixel generates an event in re-sponse to measured changes in (log) intensity. The DAVIS eventcamera couples DVS pixels with conventional CMOS pix-els, providing access to image frames. However, the frameslack the dynamic range of DVS events. A recent design, Celex-V , provides log-intensity frames using an external trigger.ATIS , a less prevalent design, features asynchronous inten-sity events, but its sophisticated circuitry reduces pixel fill factor.The above designs are based on analog processing; we insteaddesign event cameras on digital photon detections. Intensity imaging with event cameras. Several approacheshave been explored to obtain images from events, includingPoisson solvers , manifold regularization , assumingknowledge of camera motion or optical flow , andlearning-based methods . However, becauseevents often lack sufficient scene information, they are oftensupplemented by conventional frames , eitherfrom sensors such as DAVIS or using a multi-camera setup.Fusing events and frames presents challenges due to poten-tial spatiotemporal misalignment and discrepancies in imagingmodalities. Even when these challenges are overcome, we showthat fusion methods produce lower fidelity than our proposedgeneralized event cameras. Passive single-photon imaging. In the past few years, SPADshave found compelling passive imaging applications; this in-cludes high-dynamic range imaging , motiondeblurring , high-speed tracking , and ul-tra wide-band videography . A particularly relevant methodis proposed by Seets et al. , which uses flux changepointestimation to perform burst photography on single-photon se-quences. This approach uses flux changepoints to estimatemotion, then integrates along spatiotemporal motion trajecto-ries to circumvent the noise-blur tradeoff. This spatiotemporalintegration allows for high-quality reconstructions under chal-lenging lighting and motion conditions. In contrast, our paperemphasizes changepoint estimation as a means to compresssingle-photon data. Further, since we aim to run our proposedtechniques near sensor, where there are limited memory andcompute capabilities, we focus on online changepoint estimationthat processes photon detections in a single pass.The fine granularity of passive single-photon acquisitionmakes it possible to emulate a diverse set of imaging modali-ties , including event cameras, via post-capture processing.In this work, we go beyond emulating existing event camerasand design alternate event cameras that preserve high-fidelityintensity information.",
  ". What is an Event Camera?": "The defining characteristic of event cameras is that they transmitinformation selectively, in response to changes in scene content.This selectivity allows event cameras to encode scene informa-tion at high time resolutions required to capture scene dynamics,without proportionately high bandwidth. This is in contrast toframe-based cameras, where readout occurs at fixed intervals.We characterize event cameras in terms of two axes: what thecamera transmits and when it transmits. As a concrete example,consider existing event cameras. They trigger events (when totransmit) based on a fixed threshold:",
  "(a) 1-bit events(b) Flux levels(c) Adaptive exposures": ". Altering what to transmit. (a) We sum the events gen-erated by a jack-in-the-box toy as it springs up. This sum gives alossy encoding of brightness changes in dynamic regions. (b) Trans-mitting levels instead of changes helps recover details in static regions.(c) Adaptive exposures, which accumulate flux between consecutiveevents, provide substantial noise reduction.",
  "(x,t,sign((x,t) ref(x)))(2)": "that encodes the polarity of the change (what to transmit).Event polarities, although adequate for some applications,do not retain sufficient information to support a general setof computer vision tasks. A stream of event polarities is anextremely lossy representation. Notably, it only defines sceneintensity up to an initial unknown reference value, and it doesnot encode any information in regions not producing events, i.e.,regions with little or no motion.Our key observation is that existing event cameras representjust one operating point in a broader space of generalized eventcameras, which is defined by two axes: what to transmit andwhen to transmit. By considering alternate points in this space,we can design event cameras that preserve high-fidelity sceneintensity. This enables plug-and-play inference with a host ofalgorithms developed by the mainstream vision community.We begin with a conceptual exploration of this generalizedspace, before describing its physical implementation. Generalizing what to transmit. As a first step, we canmodify the event camera such that it transmits n-bit valuesinstead of one-bit change polarities. When an event is triggered,we send the current value of (x,t); if a pixel triggers no events,we transmit during the final readout. As we show in (b),this simple change allows us to recover scene intensity, even instatic regions. It is important to note that, while the transmittedquantity differs from conventional events, we retain the definingfeature of an event camera: selective transmission based onscene dynamics (where we transmit according to Eq. (1)). Thus,the readout remains decoupled from the time resolution.If (x,t) were a perfect estimate of scene intensity, then thechanges thus far would suffice. However, is fundamentallynoisy: to capture high-speed changes, must encompass ashorter duration, which leads to higher noise. This is a manifes-tation of the classical noise-blur tradeoff.To address this problem, we introduce the abstraction of anintegrator (or ), that defines how we accumulate incident fluxand, in turn, what we transmit. Ideally, we want the integrator to",
  "Event cameraIntegrator ()Change detector ()Event packetsMin. latencyIntensity info.?Low-light perf": "Existing (DVS )logarithmiccomparatorbinary106 to 105spoorSec. 4.1adaptive exposureBayesian change detector scalar105sgoodSec. 4.2adaptive exposurevariance-aware differencespatches104sgoodSec. 4.3coded exposureBinomial confidence intervalvector103sgood . Summary of generalized event cameras. Our designs integrate photon detections () and detect scene-content changes () in distinctways. We compare our designs to existing DVS event cameras based on their event streams, latencies, and intensity-preserving nature. Whileproviding a direct power comparison to DVS is difficult, we compare the power characteristics among our designs in Sec. 5.4. adapt to scene dynamics, i.e., accumulate over longer durationswhen there is less motion, and vice versa. We observe that eventgeneration, which is based on scene dynamics, can be used toformulate an adaptive integrator. Specifically, we propose anintegrator cuml that computes the cumulative flux since the lastevent:",
  "T0(x,s)ds,(3)": "where T0 is the time of the last event. When an event is triggeredat time T1, we communicate the value2 of cuml(x,T1), whichwe interpret as the intensity throughout [T0,T1]. This approachyields a piece-wise constant time series, with segments delimitedby events. We note that a similar idea, of virtual exposuresbeginning and ending with change points, was also explored inSeets et al. as part of a motion-adaptive deblurring pipeline.Adaptive exposures significantly reduce noise while preservingdynamic scene content, as we show in (c). Generalizing when to transmit. The success of the adap-tive integrator crucially depends on the reliability of events; forexample, triggering false events in static regions causes unnec-essary noise. We refer to the event-generation procedure as thechange detector, denoted by . Current event cameras detectchanges by applying a fixed threshold to measured intensitydifferences (Eq. (1)). This method has two key limitations: itonly considers the value of at pixel location x and time t andis not attuned to the stochasticity in .We design more robust change detectors that (1) leverageenhanced spatiotemporal contexts, and (2) incorporate noiseawareness, either explicitly by tuning thresholds, or implicitlyby modulating the detectors behavior. Specifically, we improvereliability by using temporal forecasters (Sec. 4.1), by leverag-ing correlated changes in patches (Sec. 4.2), or by exploitingintegrator statistics (Sec. 4.3). Realizing generalized event cameras. The critical detail re-maining is how we implement our proposed designs in prac-tice. We need direct access to flux estimates at a high timeresolution. Conventional high-speed cameras can provide suchaccess, however, they incur substantial per-frame read noise(2040e ) that grows with frame rate .We turn to an emerging class of single-photon sensors, single-photon avalanche diodes (SPADs ), that has witnessed dra-",
  "We can either transmit values of cuml or differences (changes) to cuml;we treat this as an implementation detail here": "matic improvements in device practicality and key sensor charac-teristics (e.g., array sizes and fill factors) in recent years .SPADs can operate at extremely high speeds (100 kHz) with-out incurring per-frame read noise. Each (x,t) measured by aSPAD is limited only by the fundamental stochasticity of pho-ton arrivals (shot noise). This allows SPADs to provide hightiming resolution without a substantial noise penalty. In the nextsection, we describe the image formation model of SPADs andprovide single-photon implementations of our designs.",
  "P ((x,t) = 1) = 1 eN(x,t),(4)": "where N(x,t) is the average number of photo-electrons duringan exposure, including any spurious detections. The inherentlydigital SPAD response allows us to compute software-leveltransformations on the signal (x,t), including operations thatmay be challenging to realize via analog processing. Thesetransformations can be readily reconfigured, which permits aspectrum of event camera designs, not just one particular choice.However, there is one consideration: our designs should belightweight and computable on chip. As we show in Sec. 5.4,this is vital to implementing generalized event cameras withoutthe practical costs associated with reading off raw SPAD outputs.We now describe a set of SPAD-based event cameras (sum-marized in Tab. 1), beginning with the adaptive exposure methodfrom the previous section.Adaptive-exposure event camera. We obtain a SPAD imple-mentation of the adaptive exposure described in Eq. (3) byreplacing the integral with a sum over photons:",
  "Changes detectedChanges detectedIntegrator values": ". Bayesian- vs. EMA-based change detection. (left) A fixed-threshold change detector (used in adaptive-EMA) makes it difficultto segment low-contrast changes. (center) The Bayesian formulationattunes to the stochasticity in incident flux and can detect fine-grainedchanges such as the corners of the hole saw bit; (right) as a result, theintegrator captures the rotational dynamics.",
  ". Bayesian Change Detector": "A fixed-threshold change detector such as Eq. (1) does notaccount for the SPADs image formation model; it uses thesame threshold irrespective of the underlying variance in photondetections. As a result, such a detector may fail to detect changesin low-contrast regions without producing a large number offalse-positive detections (see (left)). In this section, we consider a Bayesian change detector,BOCPD , that is tailored to the Bernoulli statistics of photondetections. BOCPD uses a series of forecasters to estimatethe likelihood of an abrupt change. At each time step, a newforecaster t is initialized as a recurrence of previous forecasters,and existing forecasters are updated:",
  "s=1lss,s lss s < t,(6)": "where is the sensitivity of the change detector, withlarger resulting in more frequent detections. ls is the predictivelikelihood of each forecaster, which we compute by trackingtwo values per forecaster, s and s, that correspond to theparameters of a Beta prior. For a new forecaster, these valuesare initialized to 1 each, reflecting a uniform prior. Existing(s,s), s < t, are updated as",
  "s s + (x,t),s s + 1 (x,t).(7)": "ls is given by s/(s + s) if (x,t) = 1, and s/(s + s)otherwise. An event is triggered if the highest-value forecasterdoes not correspond to T0, the timestamp of the last event;mathematically, if argmaxt t = T0. To make BOCPD viable in memory-constrained scenarios,we apply extreme pruning by retaining only the three highest-value forecasters . We also incorporate restarts, deletingprevious forecasters when a change is detected . Compared to an EMA-based change detector, the Bayesianapproach more reliably triggers events in response to scenechanges while better filtering out stochastic variations causedby photon noisewhich we show in .",
  ". Spatiotemporal Chunk Events": "Sec. 4.1 leverages an expanded temporal context for changedetection; however, it treats each pixel independently and doesnot exploit spatial information. In this section, we devise anevent camera with enhanced spatial context that operates onsmall patches, e.g., of 4 4 pixels. It is difficult to deriveefficient Bayesian change detectors for multivariate time series;thus, we adopt a model-free approach that does not explicitlyparameterize the patch distribution. To afford computationalbreathing room for more expensive patch-wise operations, weemploy temporal chunking. That is, we average (x,t) over asmall number of binary frames (e.g., 32 binary frames) instead ofoperating on individual binary frames; generally, this averagingdoes not induce perceptible blur.Let vector chunk(y,t) represent the chunk-wise average ofphoton detections at patch location y. Let vector patch(y,t)be an integrator representing the cumulative mean since thelast event, but excluding chunk. We want to estimate whetherchunk belongs to the same distribution as patch. We do so witha lightweight approach, that computes the distance betweenchunk and patch in the linear feature space of matrix P. As weshow in , linear features allow us to capture spatial struc-ture within a patch. Geometrically, P induces a hyperellipsoidaldecision boundary, in contrast to the spherical boundary of theL2 norm.This method generates an event whenever",
  "P(chunk(y,t) patch(y,t))2 ,(8)": "where is the threshold. When there is no event, we extendthe cumulative mean to include the current chunk. Before com-puting linear features, we normalize chunk and patch element-wise according to the estimated variance in chunk patch; weannotate the normalized versions with a tilde. We estimate thevariance based on the fact that, in a static patch, the elements ofchunk and patch are independent binomial random variables.We train the matrix P on simulated SPAD data, generatedfrom interpolated high-speed video. We apply backpropagationthrough time to minimize the MSE error of the transmittedpatch values. To address the non-differentiability arising fromthe threshold, we employ surrogate gradients. Please see thesupplementary material for complete details of this method.",
  ". Coded-Exposure Events": "In this section, we design a generalized event camera by apply-ing change detection to coded exposures , whichcapture temporal variations by multiplexing photon detectionsover an integration window. This is interesting in two aspects.First, we are designing event streams based on an modality nottypically associated with event cameras. Second, we show thathigh-speed information can be obtained even when the changedetector operates at a coarser time granularity. Coded-exposureevents provide somewhat lower fidelity than our designs inSecs. 4.1 and 4.2, but are more compute- and power-efficient,",
  "s=tTcode(x,s)Cj(x,s).(9)": "The codes Cj are chosen to be random, mutually orthogonalbinary masks, each containing Tcode/max(2,J) ones .We exploit the statistics of coded exposures to derive achange detector. Observe that in static regions, jcoded(x,t)are independent and identically distributed (iid) binomial ran-dom variables. Thus, we can expect them to lie within a bino-mial confidence interval of one another. If not, we assume thepixel is dynamic and generate an event. We trigger an event ifjcoded / conf(n,p) for any j. Here, conf refers to a binomialconfidence interval (e.g., Wilsons score), n = Tcode/J draws,and p = s (x,s)/Tcode is the empirical success probability.If a pixel is static, we store the sum of the J coded exposures,which is a long exposure, denoted by long. If the pixel remainsstatic across more than one temporal chunk, we extend long toinclude the entire duration. Whereas, if the pixel is dynamic,we transmit {jcoded}, as well as any previous static intensityencoded in long. Downstream, we can apply coded-exposurerestoration techniques to recover intensity framesfrom the coded measurements.",
  ". Experimental Results": "We demonstrate the capabilities of generalized event camerasusing a SwissSPAD2 array with resolution 512 256,which we use to capture one-bit frames at 96.8 kHz. We showthe feasibility of our designs on UltraPhase , a recent single-photon computational platform (Sec. 5.4). Refinement model. For each of our event cameras, we traina refinement model that mitigates artifacts arising from theasynchronous nature of events. This model takes a periodicframe-based sampling of integrator values and outputs a videoreconstruction. The sampling rate is configurable; in practice,",
  "ms": ". High-speed videography of a stress ball hurled at a coffeemug. (top row) This indoor scene is challenging for existing imagingsystems, including: high-speed cameras (SNR-related artifacts), eventcameras (poor restoration quality), and even hybrid event + frametechniques (reconstruction artifacts). (bottom rows) In contrast, our gen-eralized event cameras capture the stress balls extensive deformationswith high fidelity and an efficient readout. we set it 1664 lower than the SPAD rate. We use a densely-connected residual architecture , trained on data generatedby simulating photon detections on temporally interpolated high-speed videos from the XVFI dataset . See the supple-ment for training details.",
  ". Extreme Bandwidth-Efficient Videography": "High-speed videography. In , we capture the dynamics ofa deformable ball (a stress ball) using a SPAD, a high-speedcamera (Photron Infinicam) operated at 500 FPS, and a commer-cial event camera (Prophesee EVK4). The high-speed camerasuffers from low SNR due to read noise, which manifests asprominent artifacts after on-camera compression. Meanwhile,conventional events captured by Prophesee, when processed byintensity-from-events methods such as E2VID+ fail torecover intensities reliably, especially in static regions. We alsoevaluate EDI , a hybrid event-frame method. We consideran idealized variant that operates on SPAD events (obtainedvia EMA thresholding), which gives perfect event-frame align-ment and a precisely known event-generation model. We refinethe outputs of EDI using the same model as for our methods.We refer to this idealized, refined version of EDI as EDI++.While EDI++ recovers more detail than other baselines, thereare considerable artifacts in its outputs.Our method achieves high-quality reconstructions at3025 FPS (96800/32) that faithfully capture non-rigid defor-mations, with only 431 bits per second per pixel (bps/pixel)readout, which is a 227 compression (96800/431) of the rawSPAD capture. Viewed differently, for a 1 MPixel array, wewould obtain a bitrate of 431 Mbps, implying that we can read",
  "EDI++(315 bps/pixel)": ". Event imaging in urban nighttime (7 lux, sensor side). (left to right) Low-light conditions necessitate long exposures in frame-basedcameras, resulting in unwanted motion blur. The Prophesee EVK4 suffers from severe degradation in low light, causing E2VID+ to fail. RunningEDI++ on perfectly aligned SPAD-frames and -events improves overall restoration quality but still gives failures on fast-moving objects. Ourgeneralized events recover significantly more detail in low light, as seen in the inset of the motorcyclist.",
  "ms long exposureConventional events (Prophesee EVK4, 331 bps/pixel)": ". Plug-and-play inference on a tennis scene. (top left) Conventional events encode temporal-gradient polarities; this lossy representationlimits performance on downstream tasks. (bottom left) Generalized events encode rich scene-intensity information, with a readout comparableexisting event cameras. They facilitate high-quality plug-and-play inference, without requiring dedicated algorithms. (right) Generalized eventcameras give image quality comparable to burst photography techniques that have a much higher readout rate. off these 3025 FPS reconstructions over USB 2.0 (which sup-ports transfer speeds of up to 480 Mbps).Event imaging in low light. compares the low-light per-formance of frame-based, event-based, and a generalized eventcamera on an urban night-time scene at 7 lux (lux measuredat the sensor). For frame-based cameras, a short exposure thatpreserves motion may be too noisy, while a long exposure canbe severely blurred. The Prophesees performance deterioratesin low light, resulting in blurred temporal gradients. EDI++,benefiting from the idealized SPAD-based implementation, canimage this scene, but finer details like the motorcyclist are lost.Our generalized event cameras, on the other hand, providereconstructions with minimal noise, blur, or artifactswhileretaining the bandwidth efficiency of event-based systems. Thecompression here is 307 with respect to raw SPAD outputs.",
  ". Plug-and-Play Inference": "Generalized event cameras preserve scene intensity, which en-ables plug-and-play event-based vision. We consider a tennissequence (of 8196 binary frames) containing a range of ob-ject speeds. We evaluate a range of tasks: pose estimation(HRNet ), corner detection , optical flow (RAFT ), object detection (DETR ), and segmentation (SAM ).We compare against event-based methods applied to Propheseeevents; we use Arc* for corner detection and E-RAFT for optical flow. For the remaining tasks, which do not haveequivalent event methods, we run HRNet, DETR, and SAM onE2VID+ reconstructions.As (top left) shows, traditional events are bandwidth ef-ficient (331 bps/pixel), but do not provide sufficient informationfor successful inference. Generalized events (bottom left) have amodestly higher readout (520 bps/pixel), but support accurateinference without requiring dedicated algorithms. To providecontext for these rates, we compare them against frame-basedmethods (right). A long exposure (120 bps/pixel) blurs out theracket. Burst methods recover a sharp image from a stackof short exposures, but with a large readout of 15100 bps/pixel.",
  "Burst denoising": ". Rate-distortion evaluation. Our techniques feature a tunableparameter that controls the output event rate. Generalized events offera 48 dB improvement in PSNR over EDI++ (at the same readout),and can compress raw photon data by 80. high-speed videos captured by a Phantom Flex4k at 1000 FPS;see the supplement for thumbnails and links. We upsamplethese videos to the SPADs frame rate and then simulate 4096binary frames using the image formation model described inEq. (4). When computing readout for our methods, we assumethat events encode 10-bit values and account for the header bitsof each event packet.As baselines, we consider EDI++, a long exposure, com-pressive sensing with 8-bucket masks, and burst denoising using 32 short exposures. As shows, generalized eventcameras provide a pronounced 48 dB PSNR improvement overbaseline methods. Further, our methods can compress the rawSPAD response by around 80 before a noticeable drop-off inPSNR is observed.Among our methods, the spatiotemporal chunk approach ofSec. 4.2 gives the best PSNR, followed by the Bayesian method(Sec. 4.1) and coded-exposure events (Sec. 4.3). That said, allmethods are fairly similar in terms of rate-distortion (e.g., allthree give comparable results for the scenes in Secs. 5.1 and 5.2).The methods are better distinguished by their practical charac-teristics. The Bayesian method gives single-photon temporalresolution; however, as we show in Sec. 5.4, it is the most ex-pensive to compute on-chip. The chunk-based method occupiesa middle ground in terms of latency and cost. Coded-exposureevents have the highest latencyevents are generated only every 256512 binary framesbut the lowest on-chip cost. Thisprovides an end user the flexibility to choose from the space ofgeneralized event cameras based on the latency requirementsand the compute constraints of the target application.",
  ". On-Chip Feasibility and Validation": "A critical limitation of single-photon sensors is the exorbitantbandwidth and power costs involved in reading off raw photondetections. However, the lightweight nature of our event cameradesigns allows us to sidestep this limitation by performing com-putations on-chip. We demonstrate that our methods are feasibleon UltraPhase , a SPAD compute platform. UltraPhase con-sists of 36 compute cores, each of which is associated with44 pixels.We implement our methods for UltraPhase using customassembly code. Some methods require minor modifications",
  "Rigid dynamics(200x compression)Phone screen(11x compression)Camera motion (130x compression)": ". Limitations and failure modes. (left) Our reconstructions(yellow inset) on dynamic scenes with rigid objects can be inferiorto burst photography (green inset). (center) Modulated light sources,such as this phone screen, can trigger a deluge of events (change pointsshown in the inset). (right) Rapid camera motion can result in an eventrate divergent from scene dynamics. due to instruction-set limitations; see the supplement for details.We process 2500 SPAD frames from the tennis sequence usedin Sec. 5.2, cropped to the UltraPhase array size of 12 24pixels. We determine the number of cycles required to executethe assembly code and estimate the chips power consumptionand readout bandwidth.All our proposed methods run comfortably within the chipscompute budget of 4202 cycles per binary frame and its memorylimit of 4 Kibit per core. As seen in , compared to rawphoton-detection readout, our techniques reduce both bandwidthand power costs by over two orders of magnitude. The coded-exposure method is particularly efficient; on most binary frames,it only requires multiplying a binary code with incident photondetections. Our proof-of-concept evaluation may pave the wayfor future near-sensor implementations of generalized eventcameras, which with advances in chip-to-chip communication,could involve a dedicated photon processing unit, similar to acamera image signal processor (ISP).",
  ". Limitations and Discussion": "Generalized events push the frontiers of event-based imaging;however, some scenarios lead to sub-optimal performance. Asseen in (left), if the scene dynamics is entirely comprisedof rigid motion, burst photography gives better image qual-ity, albeit with much higher readout. (middle) Similar to currentevent cameras, modulated light sources trigger unwanted eventsthat reduce bandwidth savings. However, it may be possible toignore some of these events, perhaps by modeling the lightingvariations .Ego-motion events. Camera motion can trigger events in static regions, although our methods still yield substantial compression(130 over SPAD outputs, right). We analyze the impactof ego-motion on bandwidth savings further in the supplement.However, single-photon cameras can emulate sensor motion byintegrating flux along alternate spatiotemporal trajectories .We can imagine a generalized event camera that is ego-motioncompensated, by computing events along a suitable trajectory.Photon-stream compression. SPADs generate a torrent ofdatae.g., 12.5 GBps for a MPixel array at 100 kHzthatcan easily overwhelm data interfaces. Generalized event cam-eras reduce readout by around two orders of magnitude, bydecoupling readout from the SPADs frame rate and instead bas-ing it on scene dynamics. This could pave the way for practical,high-resolution single-photon sensors.",
  "Souptik Barua, Yoshitaka Miyatani, and Ashok Veeraraghavan.Direct face detection and video reconstruction from event cameras.In WACV, pages 19. IEEE, 2016. 3": "Raphael Berner, Christian Brandli, Minhao Yang, S-C Liu, andTobi Delbruck. A 240x180 120dB 10mW 12us-latency sparseoutput vision sensor for mobile applications. In Proceedings ofthe International Image Sensors Workshop, number CONF, pages4144, 2013. 1, 2 Anthony Bisulco, Fernando Cladera Ojeda, Volkan Isler, andDaniel D Lee. Fast motion understanding with spatiotemporalneural networks and dynamic vision sensors. In 2021 IEEEInternational Conference on Robotics and Automation (ICRA),pages 1409814104. IEEE, 2021. 1",
  "shutter spatiotemporal vision sensor. IEEE Journal of Solid-StateCircuits, 49(10):23332341, 2014. 2, 3": "Christian Brandli, Lorenz Muller, and Tobi Delbruck. Real-time, high-speed video decompression using a frame-and event-based DAVIS sensor. In 2014 IEEE International Symposium onCircuits and Systems (ISCAS), pages 686689. IEEE, 2014. 3 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Computer Vision ECCV 2020, pages 213229, Cham, 2020. Springer InternationalPublishing. 7",
  "Chris Harris, Mike Stephens, et al. A combined corner and edgedetector. In Alvey vision conference, pages 105244. Citeseer,1988. 7": "Samuel W. Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams,Jonathan T. Barron, Florian Kainz, Jiawen Chen, and Marc Levoy.Burst photography for high dynamic range and low-light imagingon mobile cameras. ACM TOG, 35(6):112, 2016. 7, 8, 27 Botao He, Haojia Li, Siyuan Wu, Dong Wang, Zhiwei Zhang,Qianli Dong, Chao Xu, and Fei Gao. Fast-dynamic-vision: Detec-tion and tracking dynamic objects with event and depth sensing.In 2021 IEEE/RSJ International Conference on Intelligent Robotsand Systems (IROS), pages 30713078. IEEE, 2021. 1",
  "frames to realistic DVS events. In CVPRW, pages 13121321,2021. 27": "Jing Huang, Menghan Guo, and Shoushun Chen. A dynamicvision sensor with direct logarithmic output and full-frame picture-on-demand. In 2017 IEEE International Symposium on Circuitsand Systems (ISCAS), pages 14, 2017. 2 Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, andShuchang Zhou. Real-time intermediate flow estimation for videoframe interpolation. In Proceedings of the European Conferenceon Computer Vision (ECCV), 2022. 6, 19, 25, 35",
  "Diederik P Kingma and Jimmy Ba. Adam: A method for stochas-tic optimization. arXiv preprint arXiv:1412.6980, 2014. 25": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead,Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick.Segment anything, 2023. 7 Martin Laurenzis, Trevor Seets, Emmanuel Bacher, Atul Ingle,and Andreas Velten. Comparison of super-resolution and noise re-duction for passive single-photon imaging. Journal of ElectronicImaging, 31(3):033042033042, 2022. 3 Martin Laurenzis, Emmanuel Bacher, Trevor Seets, Atul Ingle,Andreas Velten, and Frank Christnacher. Single photon fluximaging with sub-pixel resolution by motion compensation. InAdvanced Photon Counting Techniques XVII, pages 7785. SPIE,2023. 3 Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon See,Xiaogang Wang, Hongwei Qin, and Hongsheng Li. A simplebaseline for video restoration with grouped spatial-temporal shift.In CVPR, pages 98229832, 2023. 25",
  "Mark Sheinin, Yoav Y. Schechner, and Kiriakos N. Kutulakos.Computational imaging on the electric grid. In CVPR, 2017. 8": "Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. XVFI: extremevideo frame interpolation. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pages1448914498, 2021. 6, 19, 25, 35 Timo Stoffregen, Cedric Scheerlinck, Davide Scaramuzza, TomDrummond, Nick Barnes, Lindsay Kleeman, and Robert Mahony.Reducing the sim-to-real gap for event cameras. In ECCV, pages534549. Springer, 2020. 6",
  "Binyi su, Lei Yu, and Wen Yang. Event-based high frame-ratevideo reconstruction with a novel cycle-event network. In ICIP,pages 8690, 2020. 3": "Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. InProceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), 2019. 7 Varun Sundar, Andrei Ardelean, Tristan Swedish, Claudio Brus-chini, Edoardo Charbon, and Mohit Gupta. Sodacam: Software-defined cameras via single-photon imaging. In ICCV, pages81658176, 2023. 3, 6, 9, 21, 27, 30, 37",
  "Guan Yu. Variance stabilizing transformations of poisson, bino-mial and negative binomial distributions. Statistics & ProbabilityLetters, 79(14):16211629, 2009. 27": "Xin Yuan, Yang Liu, Jinli Suo, Fredo Durand, and QionghaiDai. Plug-and-play algorithms for video snapshot compressiveimaging. IEEE Transactions on Pattern Analysis and MachineIntelligence, 44(10):70937111, 2022. 6, 21 Zelin Zhang, Anthony J Yezzi, and Guillermo Gallego. Formulat-ing event-based image reconstruction as a linear inverse problemwith deep regularization using optical flow. IEEE TPAMI, 45(07):83728389, 2023. 3",
  "Restore": ". Algorithmic overview of generalized event cameras. Our algorithms take as input the SPADs response, (x,t), and output a streamof integrator values, i.e., a stream of tuples (x,t,(x,t)), where represents the integrators value. After sensor readout, we perform backtracking,which takes the value of the integrator as the flux estimate between the current and previous event timestamps at location x. We then sample theseflux estimates at any time t, providing a stack of backtracked frames. Notice the rich scene information present in these backtracked frames. Toalleviate artifacts (shown in the insets here) arising from the pixel-wise independent emission of events, we perform video restoration to recoverhigh-quality outputs. shows an overview of recovering scene intensity in a bandwidth-efficient manner using generalized event cameras. Thisprocess begins from the SPADs output response, (x,t) at pixel location x and (discrete) time t, and culminates in a high-fidelity,high-temporal resolution video reconstruction. In what follows, we go over the salient steps before laying down specific algorithms(in Appendices A.2 to A.5) and detailing any algorithm-specific modifications to these steps. Event generation.Each of our algorithms (Secs. 4 and 4.1 to 4.3) takes as input the high-speed binary frames produced by theSPAD, (x,t), and processes it in an online mannerwithout any buffering of photon detections. The output of our algorithms is anasynchronous (each pixel or patch can emit events independently) spatiotemporal stream of event packets. Event packets.For our methods, we assume events are sparsely encoded using a coordinate list (COO) format. In other words,we represent each event as a tuple (x,t,), where x is the spatial location, t is the time, and is the event payloadwhich istypically the integrators value (), except in the case of coded-exposure events where we transmit a set of integrator values. Forthe adaptive-EMA and Bayesian methods, = cuml, the adaptive integrator. For the spatiotemporal chunk method, = patch,the patch-wise cumulative mean (a vector). For the coded method, = (long,{jcoded}), the long exposure and most recent codedexposures (note that we can exclude long if this is the change detectors first timestep or if we produced an event on the timestepimmediately preceding this one). Backtracking.When an event is emitted at time t = T1, the integrators value, , is taken to be the flux estimate (or representation)between T1 and the previous event emission time (T0)if this is the first event emission, we assume T0 = 0. Thus, from thespatiotemporal event stream, we can construct a piece-wise constant representation of the incident flux, by traversing the event streambackward in time; we term this process as backtracking. Sampling.After backtracking, we obtain a spatiotemporal cube of intensity estimates. We can now sample this cube discretely toobtain one or more frame-based samples. The main motivation for sampling the intensity cube, rather than processing it in its entirety,is that existing video restoration models are not capable of inference on very long video sequences (most models infer on 3264video frames at a time). In this work, we consider a very simple sampling strategy: temporally uniform sampling. There can be,however, more sophisticated ways to sample, potentially based on the rate of events across time we do not explore these moresophisticated variants in this paper. Restoration.Having obtained a frame-based sampling (video representation) of our backtracked cube, we can now process it usingvideo restoration techniques. The purpose of video restoration here is to remove artifacts arising from the independence of eventsbetween pixels (or spatial patches). For instance, neighboring pixels may fire events at different times, which tends to produce jaggedartifacts around motion boundaries. We show these asynchronous artifacts (and their removal) in the insets of .",
  "b9aT9WK9Wx/T1pSVzOyiP7A+fwCl/ZxE</latexit>cuml(x, t = 41.3 ms)": ". Intermediate outputs and recovered intensity from adaptive-Bayesian. (left to right) The change points detected by restarted-BOCPDare more informative that those detected by an EMA-based change detector. While there appears to be more change points here than in ,the changes are transmitted more parsimoniously over timeas a result, the overall readout is lower (495 bps/pixel for adaptive-Bayesian versus658 bps/pixel for adaptive-EMA). Using an improved change detector (BOCPD) also results in backtracked images that preserve more detail, andconsequently a final reconstructed image that has significantly less blur (the ping-pong ball is better recovered here). In Algorithm 2, we describe our generalized event camera (from Sec. 4.1), which uses (a variant of) restarted Bayesian onlinechange detection (R-BOCPD ) as the per-pixel change detector. As mentioned in Sec. 4.1, our main modification is the use ofextreme pruning : instead of storing one forecaster for each timestep (or per binary frame), we only retain the top-K forecasters.We found that the performance of the change detector is not significantly impacted even with substantial pruning where we retain justthe top-3 forecasters; see . We now describe a few variants of Algorithm 2. Restarts and pseudo-distribution array.We can also consider the base variant of BOCPD , which does not involve restarts oran array of pseudo-distribution values. In this case, we can remove the arrays in line 7 of Algorithm 2, and initialize a new forecasteras = k klk. Removing restarts allows us to skip lines 3538 in Algorithm 2, and replace line 31 by argmaxk k = T0, whereT0 is the timestamp of the previous event at the same pixel location x. We use this simplification for our on-chip implementation forUltraPhase . Direct extensions to spatial patches.While BOCPD is a per-pixel temporal change detector, there are simple ways to exploit thecorrelated changes observed in a patchalthough these modifications should be seen as simple extensions and not a more principledapproach, such as what we describe in Sec. 4.2. To reduce detection delay, we can fire events whenever a changepoint is detected inany pixel in a patch. While this may be preemptive for pixels where change has not yet been detected, being preemptive may not bedetrimental, as a change in one pixel indicates that changes may soon be observed at other pixels in a patch. We show intermediate and final outputs of adaptive-Bayesian on the slingshot sequence in . Notice that the dynamics ofthe slingshots elastic band and the propelled ping-pong ball are much better preserved in (as compared to ), whileentailing a reduced sensor readout as well.",
  "Previous": "change-point Restarted BOCPD (3 forecasters) . Impact of restarts and pruning on change detection. Using a (simple 1D) synthetic example, we show the impact of the restart strategydescribed by Alami et al. and extreme pruning (retaining just top-3 forecasters by value). (first two rows) We consider a piece-wise stationarytime series from which we draw Bernoulli samples. (third and fourth rows) Incorporating restarts (and the pseudo distribution) helps reduce thedetection delay. (last two rows) Meanwhile, we do not see a substantial impact on the detectors performance when pruning the number of forecasters(for either BOCPD or restarted BOCPD). Plotted here are the values of the previous change-point timestep, as Bernoulli observations come in.",
  "BacktrackedRestored508 bps/pixel": ". Intermediate outputs and recovered intensity from the spatiotemporal chunk method. (left to right) We first accumulate photonmeasurements into temporal chunks of, e.g., 32 binary frames. We then apply change detection on 4 4 spatial patches. Applying a restorationmodel to the backtracked outputs removes patch-boundary artifacts (e.g., patches that recently triggered an event may appear noisier than theirneighbors).",
  "P(chunk(y,t) patch(y,t)) .(10)": "Let p p be the patch size, with q = p2 the number of pixels in a patch. We use a patch size of 4 4 in all our experiments (exceptfor , where we use 88 for illustrative purposes). chunk(y,t) Rq is the normalized mean over a temporal chunk of m binaryframes; we use m = 32 throughout the paper. patch(y,t) Rq is the normalized cumulative mean since the last event, excludingthe current temporal chunk. P Rrq is a feature matrix. We use r = 16 in our main experiments; for UltraPhase experiments, wereduce this to r = 4 due to on-chip memory constraints (see Appendix E.7).We normalize via",
  "n + 1,(14)": "where m is the temporal chunk size, n is the number of temporal chunks comprising patch, and the square root is pointwise. c2 is anempirical estimate of the variance in chunk patch under a static assumption, in which case chunk and patch are binomial randomvariables with the same probability. We use ghost sampling when computing p to prevent numerical instability near p = 0 and p = 1;specifically, we add 8 ghost Bernoulli measurements, 4 of which are successes. Training matrix P.We use the procedure outlined in Algorithm 4 to generate outputs that can be used to train the matrix Pvia backpropagation through time (BPTT). Algorithm 4 does not involve any branched control flow, and gives outputs identical toAlgorithm 3. The event-generation decision is represented by a binary value k. On the forward pass, k is computed using a Heavisidefunction. On the backward pass, we approximate the gradients of the Heaviside using a surrogate gradient approach. Specifically, wereplace the Heaviside gradients with those of a logistic sigmoid.Note that Algorithm 4 also includes an autodiff-compatible backtracking step, where we fill in the values of B in time-reverseorder. Unlike an indexing operation, which is not differentiable with respect to its indices, this implementation is differentiable withrespect to the values of k, which delimit the piecewise-constant segments of B. We train using interpolated videos from the XVFI dataset . We interpolate from the native 1 kHz frame rate to 16 kHz usingRIFE , then linearly interpolate over time by another factor of 8 (for a total binary frame rate of 128 kHz). We use the interpolatedframes as the ground truth during training. To generate SPAD frames, we treat the ground truth values (in the range ) as theBernoulli photon-detection probability.We initialize P with uniformly-distributed random values in the range [0.0125,0.0125]. We train for 20 epochs, using vanillaSGD with a learning rate of 0.06, reducing the learning rate by a factor of 5 after 10 epochs. Each epoch consists of 200 batches, witheach batch containing 64 patch time series. Throughout training, we randomly vary the contrast threshold via uniform sampling in therange [1/1.3,1/0.7].",
  "NaA=</latexit>j = 1": ". Intermediate outputs and recovered intensity from (two-bucket) coded-exposure events. (left) We show the changes detected usingthe confidence-interval test between two coded measurements that were computed across a temporal chunk of 1000 binary frames. (middle) Thecoded-exposure measurements, jcoded, differ predominantly in dynamic regions, while being statistically similar in static regionsthis fact formsthe basis of the change detector that we design for coded-exposure events. (right) Reconstructions obtained using our video restoration model on astack of pre-processed (pseudo-inverse step) and backtracked coded exposures.",
  "Cj(n)Ck(n) = 0 j = k, 1 n N.(15)": "The motivation behind such a choice is to ensure that the pseudo-inverse step (Moore-Penrose inverse), which is applied to thecoded measurements as a processing step, can be computed efficiently. With these constraints, the pseudo-inverse step involvesmultiplication (of the coded measurements) by a diagonal matrix, which can be carried out efficiently.One way to construct such a mask sequence is by choosing j Uniform(1,J) at each sequence location n 1,2,...,N, andthen setting Cj(n) = 1 and Cj(n) = 0 for all other j = j. In other words, we pick a random bucket at each subframe index n.For instance, when J = 2, this amounts to picking to mask sequences, C1,C2 {0,1}N, such that",
  "C1(n) = 1 C2(n) 1 n N.(16)": "This choice corresponds to the coded two-bucket camera . Thus, these random binary masks can be seen as a generalization ofcomputing a single coded measurement to computing J coded measurements .Finally, we remark that we do not consider the case of J = 1 here, since we implement coded exposures computationally onsingle-photon sensorshence, the complementary coded exposure (or the two-bucket measurement) is always readily available. Inother words, there is no drastic compute or memory overhead of a (computational) two-bucket coded exposure over a single codedexposure. This would not be the case if an optical setup (e.g., using digital micromirror devices, DMDs) were used to implementcoded exposureswhere using a two-bucket measurement would likely entail a beam splitter and a second DMD. Pseudo-inverse step.After backtracking, we can sample a J bucket coded-exposure measurement at any time t. Of course, if thepixel is static, we only get 1 static measurement at the pixel locationso we repeat static measurements J times. Before applyingour video restoration module, we perform a pseudo-inverse step that is derived from the linear forward model of J-bucket codedexposures and is similar to the pseudo-inverse pre-processing step adopted for single-bucket compressive captures . Thispre-processing step involves computing",
  "We show intermediate and final outputs of (two-bucket) coded-exposure events on the slingshot sequence in": "Algorithm 5 Coded-Exposure Event Camera. We typically choose J to be 2, 4 or 8 and N to (correspondingly) be 8, 16 or 32.Further, we set in the range 1.83.5, with smaller values resulting in more sensitive (frequent) change detection. Require: SPAD response, (x,t)Temporal extent of integrator, TcodeNumber of buckets, JSubframes per code, NWilsons score significance, Pixel locations, XTotal bit-planes, TAssume that Tcode 0 (mod N), T 0 (mod Tcode)",
  "B.1. More Sophisticated Integrators": "In this work, we propose two integrators: adaptive exposures and coded exposures. There remains an extensive, unexplored space ofalternate integrators. Both adaptive and coded exposures are linear projections of a pixels photon detections over time. We couldconsider more general linear mappings, such as continuous-valued temporal codes. Further, we are not restricted to projections overtime; it may also be advantageous to consider spatial projections, e.g., frequency-domain transforms. With these alternate projections,we might be able to reduce bitrates while maintaining reconstruction quality.We have so far assumed that our objective is intensity reconstruction. However, there may be situations where we know that thefinal goal is a specific inference task. In such a scenario, we could design integrators that only encode information relevant to the taskat hand; this may be significantly more bandwidth-efficient than transmitting a generic intensity encoding. This integrator couldtake the form of a learned module, e.g., a neural network, that operates near-sensor and computes a compressed, task-specific scenerepresentation. We could train this module end-to-end with the downstream layers that perform final inference. The resulting systemwould involve a neural network that spans multiple compute devices, with an event-based communication layer in the middle. Thissetup somewhat resembles spiking neural networks and other event-based networks, although the goal with such methods is generallyreduced computation costs (arising from sparse layer inputs) and not reduced bandwidth along a data-transfer interface.",
  "B.2. Entropy Coding and Quantization": "Generalized event cameras encode intensity levels, in the range , representing the photon-detection rate. In our experiments, weapply uniform quantization to the transmitted values; this keeps our comparisons straightforward and fair.However, in a practical deployment, it may be more bandwidth-efficient to encode changes rather than values and apply entropycoding to the changes. When transmitting changes, the first event encodes a value in , and subsequent events encode a differencein [1,+1]. This approach is functionally equivalent to transmitting levels or values, assuming the event camera correctly tracksquantization effects (to avoid drift). For natural scenes, the distribution of changes is non-uniform. The shape of this distributiondepends in part on the change-detection algorithm; in general, we would expect changes near zero to be unlikely, as these would nottrigger an event. We can apply entropy coding (e.g., Huffman coding) to exploit the nonuniformity in the distribution and achieve someadditional compression. Entropy coding could give substantial bandwidth savings, albeit at the cost of some increased near-sensorcomputation.In addition to entropy coding, we could apply non-uniform quantization, either to values or changes. For example, we couldnon-uniformly quantize values in based on perceptual considerations (e.g., human sensitivity to intensity differences). Withchanges, we could exclude portions of the range [1,+1] based on the characteristics of the change detector; for example, with afixed contrast threshold , there is no need to represent changes in (,).",
  "B.3. Spatial Compression": "One advantage of patch-wise events (e.g., as described in Sec. 4.2) is that they permit some spatial compression. For example, wecan apply JPEG block compression to the payload of an 8 8 patch-wise event. Such a change would bring our techniques morein line with existing video-compression algorithms, which compress in both space and time. The compression ratio we observewould depend on the sensor resolution; with higher resolution, we would expect image patches to be more uniform, and thus moreeasily compressible. Likewise, patches with more noise (e.g., due to a short adaptive integration window) would be more difficult tocompress. Under ideal conditions, patch-wise compression might give us an additional 5 reduction in bitrate. We leave this idea asa topic for future work.",
  "B.4. Alternate Sparse Formats": "As described in Appendix A.1, we assume a COO event format (x,t,) when evaluating the bandwidth requirements of our methods.We could improve the sparse format to reduce bandwidth costs further. One such improvement would be to use an implicit timeencoding. Specifically, on each binary frame, we would transmit a header (t,n) indicating the current timestamp and the number ofevents on this frame. We would then send n event packets (x,). If n 1, this approach would virtually eliminate the overheadassociated with transmitting values of t. Note, however, that this approach assumes we communicate events in time order; i.e., ift2 > t1, all events at time t1 are sent before any events at time t2.To reduce the overhead associated with x, we could employ sparse matrix formats such as compressed sparse row (CSR)or compressed sparse column (CSC). These formats compress one of the two spatial coordinates (the row and column indices,respectively). The savings relative to COO would depend on the density of events on each frame, with denser events leading to morecompression with CSR or CSC. For pixel-wise methods (i.e., adaptive-EMA, Bayesian, and coded), another potential optimization is to adaptively group eventsinto patch-wise packets. Assume we are using the COO format (x,t,). If a spatial patch contains many events, it may be moreefficient to transmit them in a single packet, as this amortizes the overheads of x and t. To implement this optimization, we wouldadd an indicator bit b to each event packet. If b = 0, then the packet should be treated as a pixel-wise event; if b = 1, it is a patch-wiseevent, meaning encodes integrator information for an entire patch. Within a patch-wise event we would use dense format for ,marking the pixels that triggered an event with a one-bit mask. Each patch would adaptively determine whether to encode its eventspixel-wise or patch-wise. This decision would be based on the number of events, with the optimal threshold depending on the numberof bits required for x, t, and , as well as the patch size.",
  "B.5. Latency of the Adaptive Integrator": "One limitation of the adaptive integrator cumul(x,T1) is that it creates some latency in the intensity estimates. The estimate for theduration [T0,T1]which the adaptive integrator computes as the mean of (x,t) over [T0,T1]is not known until T1. Thus, there isa latency of T1 t to obtain an estimate for t [T0,T1]. The expected delay depends on the frequency of events, with lower delayin more dynamic regions. The latency of cumul(x,T1) is not an issue for offline reconstruction and inference (which we assumethroughout this paper). However, it may be of concern for certain real-time applications.As a potential solution, we could introduce a second type of event, which we call an eager event, in contrast to the change eventswe have considered thus far. Assume again an event is generated at time T1. In addition to transmitting cumul(x,T1) at that moment,we could send an eager event encoding a noisy estimate of (x,T1). We could then send periodic eager events encoding refinementsto the value of the adaptive integrator. Assuming constant flux after T1, the adaptive integrator converges to the true flux value astime passes; transmitting refinements would allow downstream components to leverage this improved estimate in the absence of asubsequent change event. We could consider various schedules for sending refinementse.g., at exponentially increasing intervals, orat fixed intervals until some maximum time has passed. Note, however, that to keep the event cameras bandwidth coupled with(proportional to) the scene dynamics, there would need to be an upper bound to the number of eager refinement events, i.e., with eachchange event triggering at most N refinements.The solution described above would involve some additional bandwidth costs. We can thus imagine a generalized event camerathat operates in two modes: online mode and offline mode, with eager events only being used in the online mode, where lowlatency is necessary.",
  "C.1. Model Architecture": "While any video restoration model could be used, we choose the densely-connected residual network proposed in Wang et al. (EfficientSCI) for our video restoration architecturewhich was successful at restoring backtracked outputs of all of our generalizedevent cameras (adpative-EMA, adaptive-Bayesian, spatiotemporal chunk, and coded-exposure events). We also experimented withthe spatial-temporal shift-based model of Li et al. (ShiftNet): while this architecture was successful at restoring three of ourfour proposed events, it did not succeed at restoring backtracked coded exposures. We attribute this to the fact that EfficientSCI wasdesigned with video compressive sensing in mind, whereas ShiftNet is targeted for more general video-restoration tasks.The memory cost of EfficientSCI scales with the number of input frames. Thus, we are constrained by the device memory in thenumber of frames we can reconstruct. For example, with 24 GB of GPU memory and a resolution of 512 256, we can reconstructabout 96 video frameswhich can cover a temporal extent of 30009000 binary frames (3090 ms). Increasing the temporal extentrequires sampling the backtracked cube at an increased temporal stride, which can lead to blurring and lower-quality results. Onesolution to this problem might be to employ a recurrent architecture with a fixed-size memory. However, forward-mode recurrentinference must be causal; i.e., the predicted frame at time t may only consider backtracked samples from times before t. An efficientrecurrent model may enable reconstructing videos at frame-rates faster than what we show in this work (3000 FPS)indeed, ourmethods that operate the granularity of individual binary frames (e.g., Sec. 4.1) can, in principle, provide reconstructions at theframe-rate of SPAD photon-detections, i.e., 96.8 kHz.",
  "C.2. Dataset": "We use 4403 high-speed videos from the training split of the XVFI dataset to train our restoration models. We temporallyinterpolate these 1000 FPS to 16000 FPS using RIFE and treat each video frame, normalized between 0...1 as the photon-detectionprobability. We then draw 6 binary frames per video frame (as a per-pixel Bernoulli random variable, based on the photon-detectionprobabilities)thereby giving us binary-valued responses at 96 kHz. We remark that our dataset generation approach here (unlike inSec. 5.3 and Appendix E.6) is not physically accurate, since we directly treat the videos values as photon-detection probabilitiesinstead of average photon-arrival rates. We adopt this approach for its simplicity and note that the difference (between detectionprobabilities and Poisson rates) manifests as a tone-mapping operation by the SPADs response function (f(x) = 1 ex). We didnot see any generalization issues when applying our models trained this synthetic dataset to real SPAD datain both ambient andlow-light scenarios.",
  "C.3. Training Parameters": "All restoration models were trained until convergence (4060 epochs) using the Adam optimizer and the mean squared error(MSE) loss objective. We used an initial learning rate of 105, which was decayed as per a cosine-annealed scheduler to aminimum value of 108. When training the spatiotemporal chunk method, we randomly choose on each training iteration from auniform distribution covering the range [0.76,1.43]. We also clip the gradient norm to 1.0 to resolve instability during training.",
  "array": ". Camera setup for experimental acquisition. (left) Our setup comprises of the SwissSPAD2 array , Prophesee EVK4 event cameraand the Photron Infinicam high-speed camera. (right) An example setup, which we used for capturing the tennis scenes shown in Figs. 1 and 7. Weblur out the player for anonymity. shows the cameras we used for demonstrating the capabilities of generalized events: our algorithms process the outputs ofthe SwissSPAD2 array. We compare the performance of generalized events against the imaging capabilities of a commercial eventcamera (Prophesee EVK4) and a high-speed camera that is capable of real-time streaming (Photron Infinicam).",
  "D.1. Imager Specifications": "SwissSPAD2 Array.The sensor has a resolution of 512 512 pixels: however, we operate the device in its half-array mode,meaning that only one subarray of resolution 512 256 pixels is used. Each pixel has a pixel pitch of 16.4 m and a fill factor of lessthan 10%. The fill factor can be improved by the inclusion of microlens arrays which this prototype lacks.The SPAD arrays features a certain number (5%) of hot pixels: pixels whose dark count rate (DCR) is abnormally high, and asa result, almost always return 1s. We calibrate a mask of hot pixels by using sensor responses that were taken in the dark. We find thathot pixels do not impact the generation of generalized eventsthus, we inpaint hot pixels (either using the Telea algorithm ornearest-neighbor replacement) using the calibrated mask after backtracking is performed, i.e., post-sensor readout.",
  "Prophesee EVK4.This commercially available event camera has a sensor resolution of 1280 720 pixels. Each pixel has a pixelpitch of 4.86 m and a fill factor of > 77%": "Photron Infinicam.This high-speed camera provides a USB-C data interface, which is enabled by performing compression on thefly. The camera SDK does not provide access to parameters that control this online compressionas a result, we find that whenimaging scenes where the camera is per-frame SNR limited, such as the indoor scene of , read-noise induced artifacts translateto severe compression artifacts. The camera has a resolution of 1246 1024 pixels.",
  "D.2. Scene Acquisition Details": "We list the (focal lengths of C-mount) lenses used to capture each of the scenes shown in our main paper, along with the acquisitiontime of each scene (in terms of the number of binary frames captured by the SPAD at 96.8 kHz). Slingshot sequence in : 25 mm lens, 4000 binary frames. Tennis sequence in : 50 mm lens, 3000 binary frames. Vertical wheel in : 12 mm lens, 3000 binary frames. Nighttime traffic sequence in : 25 mm lens, 3000 binary frames. Dartboard sequence in : 16 mm lens, 3000 binary frames. Jack-in-the-box sequence in : 16 mm lens, 4000 binary frames. Rotating hole-saw bit sequence in : 35 mm lens, 6000 binary frames. Casino roulette sequence in : 25 mm lens, 4000 binary frames. Stress ball sequence in : 25 mm lens, 4096 binary frames. Nighttime traffic sequence in : 25 mm lens, 8000 binary frames. Tennis sequence in : 100 mm lens, 8192 binary frames.",
  "E.1. Baseline Details": "EDI++ construction.EDI is a hybrid event-plus-frame technique that can produce a series of sharp images from an eventstream and a long exposure spanning the same time duration. There are two practical difficulties in implementing this method: (1)precise spatial- and temporal-alignment is needed between the frames and events, (2) differences in imaging modalities betweenconventional CMOS cameras and DVS event sensors. There is also a third (DVS specific) difficulty, which Pan et al. overcomeby solving an optimization problem (either across one pair of event streams and frames or across multiple such inputs): the contrastthreshold in conventional event cameras is not necessarily known and can have inherent randomness . In contrast, whenimplementing EDI using SPAD-events and frames, we have none of these difficulties: frames and events are perfectly aligned (byconstruction), the same imaging modality (SPAD photon detection) is used to obtain both events and frames, and finally, the forward(or event generation) model is precisely controlled (no optimization problem has to be solved). Thus, SPAD-based EDI can be thoughtof as an ideal version of EDI. We further refine EDI outputs using a trained restoration model (with the same architecture used forvideo restoration of our generalized events), and call this idealized, refined version of EDI EDI++. 8-bucket compressive sensing.We use the multi-bucket video compressive sensing method described in Sundar et al. , with 8buckets and spanning 32 subframeswhere the sum of 64 binary frames comprises a single subframe. For restoring these coded8-bucket capture, we use EfficientSCI , while retaining the same densely-connected residual architecture that constitutes the videorestoration model for the proposed generalized events. Burst denoising baseline.We perform burst denoising on a stack of 32 short exposureswhere the sum of 64 binary framesconstitutes one short exposureusing the align-and-merge technique of Hasinoff et al. . After the merging step, we additionallyperform BM4D denoising (with = 0.02, after binomial variance stabilization ). Further, we find that BM4D applieddirectly to the stack of short exposures (with binomial variance stabilization) results in poor performance (28 dB PSNR on therate-distortion plot of ).",
  "bps/pixel": ". Ego-motion results on the nighttime driving sequence. Column and row descriptions are identical to . The SPAD was operatedat a lower speed in this sequence (16.6 kHz instead of 96.8 kHz)we report compression factors with respect to photon-detection readout at16.6 kHz. While this sequence also has lesser texture than , the low-light conditions here make it more challenging. When ego-motion isexaggerated by 4 (or higher, last two rows), we observe that details of the bushes are blurred out. However, the pedestrian crossing sign, which ishas better contrast, is still recovered.",
  "Frame-based camerasEvent camerasGeneralized event cameras": ". Extended low-light results. In addition to the results shown in , we show outputs obtained using our other event cameras, viz.adaptive-EMA from Sec. 4, spatiotemporal chunk from Sec. 4.2, and coded-exposure events from Sec. 4.3. We provide an additional result on an indoor sequence shot in the dark (see ), which features lower (and controllable)light-levels1, 2 and 5 lux measured using a light meter on the sensor side, as opposed to 7 lux in . We note that the low-lightperformance shown here could be improved upon with the inclusion of microlens arrays in the SPAD prototype, which could increaseits fill factor (and in turn photon detection efficiency) from 10% to > 40%.",
  "lux2 lux5 lux": ". Additional low-light results. We throw darts in the dark, at light levels of 5, 2 and 1 lux (measured on the sensor side, top to bottomrows). All three sequences span a duration of 41 ms (4000 binary frames). Our restoration models are not trained on low-light sequences, despitethis, we see reasonable low-light performance (e.g., at 5 lux). At 1 lux and 2 lux, we see (somewhat graceful) degradation in our reconstructions.Lower the light level, harder it is to reliably distinguish scene motion from noisewhich results in lower compression rates (and image quality) atlower lux values. Please zoom in to see details.",
  "E.4. Scenes with Camera Motion": "In this subsection, we provide a qualitative analysis of the impact of ego-motion on the event-generation rate and the output-imagequality. We consider three scenes with camera motion: the building sequence that features a significant amount of spatial structureand image detail (downtown buildings); the Ramanujan bust sequence, which is an indoor scene with a moderate amount of texture(from the bust and metal plaque); and a nighttime driving sequence where the SPAD was placed on the cars dashboard.We show the change points and reconstructions obtained using one of our proposed generalized events (adaptive-Bayesian, Sec. 4.1)for these sequences in Figs. 22 to 24. To bring out the effect of camera motion, we speed up the SPADs output response (by skippingbinary frames) by factors of 2, 4 and 8. Thus, a 4 sped up sequence sees 4 as much camera motion. For additional context,we also show the extent of motion using a long exposure and the response of a (SPAD-based) event camera across the same duration.Across all sequences, we observe that even with an exaggerated amount of camera motion (e.g., 4 and 8 sped-up sequences),we still see a significant amount of compression (reported with respect to raw photon readout) and a modest sensor readout (reportedin bps/pixel).",
  "E.5. Plug-and-Play Event Inference": "Expanded results. shows an expanded set of plug-and-play inference results. This figure includes results for a 42 ms longexposure (4096 binary frames, second row) and a burst reconstruction method , run on consecutive 0.3 ms short exposures (32binary frames). The long exposure fails to capture fast-moving objects; there is significant blur in the arm, racket, and ball, causinginference to fail in these regions. The burst reconstruction gives good-quality inference in both static and dynamic regions; however, itrequires a high readout bandwidth (about 30 higher than our method). Experiment details.We manually trim the Prophesee outputs to a temporal extent corresponding to the SPAD capture (consistingof 8192 binary frames). We run a Prophesee-provided E2VID model on these events to reconstruct a video. For our method and theburst reconstruction in , we run pose detection, corner detection, object detection, and segmentation on the reconstructed framecorresponding to the 2224th binary frame. For the long exposure results in , we run these methods on the mean over binaryframes 04095. See below for optical flow extents. Below we provide additional details for some of the experiments in Figs. 7 and 25. HRNet pose: We use the HRNet-W48 version of the model. Harris corners: We run a standard Harris corner detector with = 4. RAFT flow: For our method and the burst reconstruction in , we run RAFT between the reconstructions corresponding tothe 2224th and 2864th binary frames. For the long exposure results in , we consider the interval between the 04095 and40968191 exposures. We use the RAFT-Large version of the model. DETR detection: In most cases, we use a confidence threshold of 90%. We lower the threshold to 80% for the E2VIDreconstruction given the lack of high-confidence predictions. We use the ResNet-50 version of the model. Arc* corners: We run the algorithm on the entire event sequence. We temporally trim the predicted corners to a 0.8 ms windowaround the target instant. In the figure, we show events within an 8 ms window to provide visual context.",
  "E.6. Rate-Distortion Evaluation": "Dataset details.We source 15 videos from YouTube that were captured by a Phantom Flex 4K camera at 1000 FPS. showsthumbnails depicting the scene content in each video, as well as a long exposure over 42 ms that shows the extent of motion. Wedownload these videos at a resolution of 854 480 pixels and further downsize (by 1.6) and vertically crop them to the SPADsresolution of 512 256 pixels.We did not utilize the XVFI dataset , which we used for training our video restoration models, for evaluating rate-distortiontradeoffs to prevent the possibility of data leakage. Further, we find that these YouTube-sourced videos have a more extreme range ofmotion than XVFI videos.",
  "N(x,t) = I(x,t) + d,(19)": "where we I(x,t) represents a video frame, d is number of spurious detections (7.74 104 counts per binary frame, using valuesreported in Ulku et al. ). We choose such that the average value of I(x,t) over each pixel location x and time t is 1wefind that our ambient captures with the SwissSPAD2 have an average photon per pixel per binary frame (PPP) of 1.",
  "From each frame at 16000 FPS, we draw 6 binary frames, thereby simulating photon detections at 96000 Hz": "Computing metrics.We evaluate perceptual distortion using PSNR (computed from the average mean squared error across theentire video ), SSIM (computed per-frame and averaged) and MS-SSIM (computed per-frame and averaged) metrics. Bothmetrics are converted with respect to linear values. Specifically, each of our generalized event cameras and baselines (EDI++, burstdenoising, coded 8-bucket) recovers the time-varying estimate of 1 eN(x,t), i.e., the probability of photon detection. Let us denotethis by p(x,t). We can obtain linear estimates by computing",
  "d.(21)": "Baseline parameter sweeps.We implement EDI++ with SPAD events, with an exponential decay of 0.95, and sweep the thresholdbetween 0.30.54. The other baselines (burst denoising, long exposure, compressive sensing) are frame-based, and do not feature atunable parameter that controls their readout rate. Generalized-event parameter sweeps.For adaptive-EMA, we set the exponential decay (of the EMA) to 0.95, and sweep thethreshold between 0.30.54. For adaptive-Bayesian (Sec. 4.1), we retain the top-3 forecasters and vary the sensitivity of BOCPDuniformly (on a logarithmic scale) between 107 and 102.2. For the spatiotemporal chunk method (Sec. 4.2), we use a patch size of4 4 pixels, average 32 binary frames per temporal chunk, and vary the threshold () of the change detector uniformly between 0.6to 1.28. For coded-exposure events, we use a chunk size of 1024 binary frames, with 4 coded buckets (measurements per chunk)that multiplex 16 subframes eachthus, each subframe consists of 1024/16 = 64 binary frames. We vary the confidence level ofWilsons score ( parameter) uniformly between 1.2 to 6.8. Extended results.In addition to the PSNR-based rate-distortion plot shown in , we include evaluations based on SSIMand MS-SSIM metrics in (top). Across metrics, we see that generalized event cameras provide a pronounced differencein performance over the considered baselines (shown in shades of gray). (bottom) We include another evaluation on 4096 binaryframes (instead of 2048). We observe that the readout rate, measured as bps/pixel, is lower across this extended durationsince thefixed readout costs of static (and less dynamic) regions is amortized over a longer duration, unlike frame-based cameras that involvefixed readout for all regions of an image. Finally, we remark that the performance gap between EDI++ and our techniques is furtherwidened across this longer duration.",
  "binary frames": ". Extended rate-distortion evaluation. (top) Rate-distortion evaluations based on PSNR, SSIM and MS-SSIM metrics across 2048 binaryframes. (bottom) When considering a longer temporal extent, i.e., 4096 binary frames, we see that the readout rate at which a significant PSNR (orother metrics) drop-off is noticed becomes smallerin other words, we obtain more compression of raw photon detections.",
  "E.7. UltraPhase Experiments": "System description.UltraPhase consists of 3 6 cores, each of which processes data from a 4 4 patch of SPAD pixels. The chipoperates at a frequency of 0.42 GHz, implying a maximum of 4341 instructions per binary frame at 96.8 kHz. We additionally referreaders to Ardelean for a detailed description of the chip architecture. Implementing event cameras on UltraPhase.We implement our event camera designs in UltraPhase assembly code. Textfiles containing this code are included with this supplement (please see assembly_adaptive_ema.txt, assembly_adaptive_bocpd.txt, assembly_spatiotemporal_chunk.txt and assembly_coded_exposure.txt).At this point, UltraPhase does not have native hardware for computing divisionsso we modify our methods to work avoiddivision operations. For the Bayesian method, we skip restarts and avoid division when computing the likelihood by multiplyingarguments with their least common multiplethis is possible because the min and argmax operations carried out in BOCPD areinvariant to the scale (of forecaster values). For the spatiotemporal chunk method, we skip the normalization step and use a 4 16,8-bit quantized feature matrix P, in contrast to the 16 16 matrix we use in the rest of our experiments. For the coded-exposuremethod, we consider (2-bucket, 8-subframe) masks; additionally, we replace Wilsons score by a fixed confidence interval (essentiallyamounting to a fixed threshold operation). In we show comparisons between the modified and original methods; we see that,for most scenes, the above modifications do not significantly reduce the quality of the results. Clock cycle measurements.Due to circumstances beyond our control, we were unable to run our methods on a physical testbedsystem. We evaluate the runtime characteristics of our methods by assembling them for UltraPhase and measuring the numberof compute cycles required to execute them. Given the deterministic nature of the digital hardware, the compute and memoryrequirements we measure in this evaluation are identical to those we would measure in physical hardware.We confirm that all methods operate within the memory budget of the chip. In Tab. 2, we show the measured clock cycles (theaverage per binary frame) for each method. In the case of branching, we assume the more computationally expensive branch is taken.Therefore, all compute values are an upper bound. Readout estimation.The chip readout depends on the dynamics of the scene. To estimate the readout, we run our methods on a12 24 crop from the tennis sequence in , over 2500 binary frames. We show frames from this crop in . We scale themeasured readout values to units of kilobytes per second; see Tab. 2 for results. Power estimation.We estimate two components of power consumption: compute power and chip readout power. We base ouranalysis on . We assume the chip consumes 3.5 picojoules per clock cycle spent executing an instruction, and that the chipexpends 54 nanowatts per kilobit of readout. Tab. 2 shows our estimated compute and readout power.",
  "UltraPhase Original": ". The effect of modifications for UltraPhase. Some minor modifications are required to make our methods compatible with UltraPhase.As we observe in the first three columns, these modifications do not usually have any noticeable impact on the quality of the results; the modifiedmethods (bottom) give results that closely match the original methods (top). However, we do observe differences in low light, as seen in the rightmostcolumn, which shows a scene captured at 0.3 lux. In low light, a lack of noise-aware thresholding (e.g., the Wilsons bound for the coded method orthe normalization step in the spatiotemporal chunk method) leads to less reliable change detections.",
  "Coded17733.11.18 1061.44 1051.56 105": ". UltraPhase results. We measure the number of compute cycles (per binary frame) required to implement each of our methods, and estimatethe readout bandwidth. Based on these values, we estimate the power required for on-chip computation and readout. All of our methods fit withinthe chips computational budget of 4341 instructions per binary frame and give two orders of magnitude reduction in bandwidth compared to readingout raw photon detections. Due to these bandwidth reductions, our methods are also much more power efficient than raw photon readout."
}