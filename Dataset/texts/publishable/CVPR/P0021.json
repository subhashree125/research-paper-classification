{
  "Abstract": "Inspired by the success of generative image models, recentwork on learned image compression increasingly focuseson better probabilistic models of the natural image distri-bution, leading to excellent image quality. This, however,comes at the expense of a computational complexity that isseveral orders of magnitude higher than todays commercialcodecs, and thus prohibitive for most practical applications.With this paper, we demonstrate that by focusing on model-ing visual perception rather than the data distribution, wecan achieve a very good trade-off between visual qualityand bit rate similar to generative compression modelssuch as HiFiC, while requiring less than 1% of the multiplyaccumulate operations (MACs) for decompression. We dothis by optimizing C3, an overfitted image codec, for Wasser-stein Distortion (WD), and evaluating the image reconstruc-tions with a human rater study. The study also reveals thatWD outperforms other perceptual quality metrics such asLPIPS, DISTS, and MS-SSIM, both as an optimization ob-jective and as a predictor of human ratings, achieving over94% Pearson correlation with Elo scores.",
  ". Introduction": "Methods of lossy data compression are characterized by along tradition of exploiting both: correlations in the statistics of the compressed data, forexample by linearly or nonlinearly predicting the nextsample in audio codecs, or by applying decorrelating trans-formations such as the DCT ; and information bottlenecks of human perception, for exampleby representing chrominance in images with lower resolu-tion than luminance, or by modeling frequency maskingin the auditory system.Owing to the conceptual connection between decoding ofan image and sampling one from a conditional distribution,research on learned image compression has recently beengreatly influenced by generative methods such as GANs or diffusion models , which excel at probabilistic model-ing of real-world data, as evidenced by the impressive qualityof samples drawn from such models. The generated samplesare visually so convincing that measures of sample quality(i.e., realism, formalized by divergences between the distri-bution of the source data and the distribution of generateddata) have even been called perception .Unfortunately, a model which is extremely good at sam-pling real-world data has to have a certain computationalcomplexity, because the world (and hence the space of pos-sible images) is very complex. This is evidenced by thecomplexity of generative models such as Imagen, Stable Dif-fusion, or DALL-E. When it comes to image compression,this would make it seem that we ultimately need highly com-plex methods to achieve high image quality. In other words:out of the three qualities, good (image quality), fast (decod-ing speed), and cheap (low bit rate), we must pick two, as inthe old proverb. Luckily, while good probabilistic modelscan lead to remarkable perceived quality, the inverse doesnot hold: good image quality does not require precisely mod-eling the distribution of natural images, as we demonstratein this paper.Here, we focus on modeling human perception (specifi-cally, texture perception in the periphery of the human visualsystem ) instead of the data distribution. We take anexisting low-complexity neural codec, C3 , and replaceits distortion loss by either of two variants of WassersteinDistortion (WD) : one which assumes the observer togaze at each image location with equal probability, and onewhich uses EML-net , a saliency model, to predict imagelocations that are more likely to be scrutinized. To aid texturereproduction, we supply the codec with common random-ness (CR), i.e., randomness available to both the encoder andthe decoder, implemented using a pseudo-random numbergenerator with identical seeds.These changes lead to significantly better image quality atthe same bit rate compared to baseline C3 , illustrated in, but retain its low decoding complexity. The encodingcomplexity increases with the complexity of the loss func-tion, which we mitigate by introducing a novel, approximate",
  "C3/WD8 without CR at 0.1": ". Crop of image 28 from the CLIC2020 professional dataset (best viewed on screen). Top left: Original image. Top right: C3optimized for MSE, compressed to 0.172 bits/pixel. Bottom left: C3 optimized for WD with = 8, compressed to 0.167 bits/pixel. Bottomright: C3 optimized for WD with = 8, without common randomness, compressed to 0.180 bits/pixel. While optimization for MSE leadsto flattened texture, as seen in the reproduction of the grass and the walls of the building, texture is vastly improved in the WD-optimizedversions. Not providing common randomness to the decoder (bottom right) means the codec must reproduce all textures using deterministicstructure such as straight lines (as in, e.g., the texture of the roof note the lines not being consistent with the original, while providing abetter approximation of the texture than the MSE version). This is not adequate for random textures such as the grass in front of the building,where providing common randomness significantly helps to maintain the visual quality of texture.",
  "header": ". Decoding an image with COOL-CHIC and C3 . A. A latent element znij ( ) is autoregressively decoded by applying theentropy network g to the spatial context c(zn; (i, j)) (), yielding parameters , of the Laplacian probability model used for entropydecoding the latent element. B. The decoded latent spatial arrays at multiple resolutions are first bilinearly upsampled to the target resolutionand then transformed into pixel space using the synthesis network f. We supply common randomness at multiple resolutions by drawingi.i.d. elements from a pseudo-random number generator with fixed seed, which is novel to the present work. The common randomnessarrays are upsampled and concatenated with the upsampled latents. Figure adapted with permission from Kim et al. . implementation of WD. We perform a human rating studycomparing our method to several baselines, including gen-erative and commercial codecs. It reveals that our methodachieves a trade-off between image quality and bit rate sim-ilar to generative methods, but with much lower decodingcomplexity. In addition, we find that WD predicts the humanratings remarkably well.",
  ". C3 codec": "Our experiments are based on the COOL-CHIC family ofmethods and more specifically on C3 ,a neural codec which rivals modern classical codecs likeVVC in terms of ratedistortion performance, while main-taining a very low computational cost. These codecs are builtfrom three main components: a set of latent arrays, a syn-thesis network and an entropy network, each of which areoptimized per image (see ).The latents consist of a set of multi-resolution arrays,starting with the resolution of the image, followed by pro-gressively smaller resolutions (by a factor of two in eachdimension). To synthesize an image, these latents are bilin-early upsampled and concatenated to create a single multi-channel array at the resolution of the image. This array ispassed through a synthesis network f, a small convolutionalneural network, to output RGB values at every pixel location.The entropy network g is used to model the conditionaldistribution of each latent element given its spatial context.The bit sequence is made up of the bits encoding the quan-tized and entropy coded latents (with probabilities estimatedby g) as well as the bits required to store the quantized parameters of both f and g.The latent arrays and the parameters of the synthesis andentropy networks are jointly optimized for a ratedistortionobjective, with distortion given by the MSE between theoriginal and reconstructed image pixels, and rate approxi-mated by the cross entropy of the latent arrays under theentropy model. Since the latent arrays are quantized, thisoptimization is done in a quantization-aware manner. Forfurther details on the architecture and optimization, we referthe reader to Ladune et al. and Kim et al. .In this paper, we use the identical architecture and opti-mization procedure as C3 , except for two changes: 1.We supply common randomness arrays to the decoder in theform of pseudo-random i.i.d. standard Gaussian noise, whichremains fixed during optimization and decoding. Since theseed is fixed, no additional information needs to be encoded.2. We replace MSE by the Wasserstein Distortion betweenthe original and its reconstruction.",
  ". Wasserstein Distortion": "Wasserstein Distortion (WD) is a distance metricbetween pairs of images which, similar to the popularLPIPS and DISTS metrics, can utilize feature em-beddings. The feature space is interpreted as a perceptualspace, in which distances are more predictive of subjectivehuman notions of distance between images than distancesbetween pixel intensities. While WD itself is agnostic to thefeature space, we chose to rely on the well-known VGG em-beddings , derived from training a convolutional neuralnetwork on the ImageNet task, and also used by LPIPS.",
  "A. Extract VGG features": ". Computation of Wasserstein Distortion (WD) between two images, and . A. We extract spatial feature maps fi fromselected layers of a VGG network. B. For each feature i, we estimate local first and second raw moments at multiple scales bysuccessively applying a linear downsampling operation D: i = Dfi and i = Df 2i ; the local standard deviations are derived bytaking i =",
  "i. The total WD for the image pair is obtained by adding the contributions from all features": "In contrast to LPIPS and DISTS, WD generalizes thenotion of pointwise distances in a feature space by takinginto account foveation and peripheral vision. WD is param-eterized by a spatial -map of the same resolution as theimages. Assume feature maps fi also have that resolution.Then the local WD for feature i at a location (x, y) is the2-Wasserstein divergence between the local empirical dis-tributions of fi for both images, aggregated using a poolingkernel of size (x, y) and centered at fi(x, y). For featuremaps that have different resolutions than the images, thepooling kernel sizes need to be adjusted accordingly. TheWD values are then spatially averaged and summed over allfeatures i. As goes to zero, WD converges to computingpointwise distances, as in LPIPS. This models human visionin the fovea (the center of gaze), which is most accuratein distinguishing deviations between the images. Larger values model peripheral vision, where humans are capable ofdistinguishing deviations in texture characteristics, but fail atpicking up on subtler deviations. (For a complete descriptionof WD, refer to Qiu et al. .)The larger , the more permissive is WD to texture resam-pling, i.e. replacing a visual texture with one that has similarstatistics. For image compression, we exploit this by choos-ing smaller in salient image regions that are likely to bedirectly looked at, and larger in other regions. Making thedistance metric more permissive in such regions allows thecodec to allocate fewer bits in representing the local imagecontent, while maintaining visual quality.Even after approximating empirical distributions as inde-pendent Gaussians, computing WD can be computationallycostly, since it generally requires aggregation of local statis-tics with a different pooling kernel at each spatial location.We propose an approximate, but more efficient way of com- puting WD by discretizing to powers of two (). Forevery feature map fi extracted from VGG (panel A), weestimate spatial maps of the local first and second raw mo-ments by building a multi-scale cascade akin to a Gaussianpyramid using a 3 3 convolutional downsampling filterD (panel B). This yields a full spatial map of the local meani and standard deviation i of the ith feature, for poolingregions of size 2/ri, where is the scale index, and ri isthe relative resolution of the ith feature map with respect tothe images (e.g., 0.5 if fi is downsampled by a factor of 2).Local WD maps di between the two images are com-puted elementwise from two sets of i and i computedon each image. This gives us dense maps of precomputed lo-cal WD distances, but only for specific pooling region sizescorresponding to -values of 2/ri. We can approximatethe WD for a desired -value by interpolating WD valuesfrom these precomputed maps (panel C). First, the -map isadapted for each feature i and scale by computing",
  "wi = max1 log2 i , 0(2)": "which are 1 wherever is equal to 2/ri, and linearly fadeto 0 as approaches either 2+1/ri or 21/ri. Each diis multiplied with wi, averaged across space, and summedacross , to yield one WD value di for each feature i. Al-though the precomputed maps for each scale have differingresolutions, the weights for each spatial location in fi add",
  "C3/MSE CDC VVC": ". Human rating study results and decoder complexity. Left: Evaluation of image compression methods in terms of visual fidelityvs. bit rate. Error bars indicate 99th percentile. Squares indicate C3 using CR, circles indicate C3 without CR, and diamonds mark othercompression methods. Right: Computational complexity of the decoder of the same methods for the middle bit rate. HiFiC slightlyoutperforms C3 in terms of visual quality vs. bit rate. However, it requires more than two orders of magnitude more computations at thedecoder. Note that neural methods tend to have similar complexity across different bit rates and objectives. This is also true for C3; only theaddition of CR increases the complexity slightly. The complexity of VVC can vary across bit rates, and cant strictly be shown in the sameplot, since it doesnt use floating point operations; we plot an estimated equivalent on typical hardware . up to one, effectively interpolating local WD maps fromprecomputed WD maps di. Finally, WD values di areaggregated across different features i by summation.To determine an appropriate -map, we examined twoalternatives: First, we may simply chose constant acrossthe image, which could be interpreted as a belief that everyimage location is equally likely to be scrutinized by a humanobserver. In this case the value needs to be chosen conser-vatively, small enough that a desired level of exact detail inthe reconstruction is preserved at all locations. Second, wemay derive from an actual gaze map obtained with an eyetracking device or a prediction thereof. In our experiments,we obtained predictions from EML-net in the form ofa saliency map s valued between 0 and 1 (). We con-verted s to a spatial density p using the ad-hoc elementwiseformula:",
  "Available at": "that CLIC uses). For CDC and HiFiC, we were not able toexactly match these target rates, since it would have requiredre-training. Still, we included all three available target ratesfor HiFiC, as well as the two lower target rates for CDC in thestudy. This gave us a total of 29 method/rate combinations,and hence 29 distinct reconstructions for each of the 41images in the dataset.Our evaluation protocol closely follows the CLIC ap-proach. For each individual rating, three images are availableto the rater: On one side of the screen, a random 512 432pixel crop of the original. On the other, the correspondingcrop of two reconstructed images, between which the ratercan flip by pressing a key. The rater is asked to select thereconstruction that looks more similar to the original. For10% of the ratings, one of the reconstructions is replaced bythe original, which is used to assess rater reliability (goldenquestions). While the original is selected at random, the pairof compared methods/rates is dynamically selected to max-imize expected information gain considering past ratings.2 The resulting Elo score for each method/rate minimizes thecross-entropy between observed and predicted ratings.The main result of our study is shown in . Meth-ods optimized for MSE (C3/MSE, MLIC+, VVC) and thediffusion model CDC all achieve similar trade-offs betweenbit rate and visual quality, while their complexity variessubstantially (with VVC being least complex). HiFiC andC3 optimized for WD achieve much better and comparabletrade-offs, but our method requires two orders of magnitudefewer MACs at decoding time.We provide further comparisons to versions of C3 opti-mized for other perceptual metrics in , including WDwithout saliency, illustrating that optimizing for either vari-ant of WD significantly outperforms other metrics in termsof human preference. Prior to the study, we determinedthat supplying C3 with CR made a visual difference withWD (), while the C3 decoder largely ignored the ex-tra latent when optimized for MSE or MS-SSIM, which iswhy we only provided CR for WD and LPIPS in the study. reveals that both switching the objective from MSE toWD, and providing CR to a WD-optimized codec lead to asmaller fraction of the bit rate being allocated to the highest-resolution array, which encodes detail, allowing other arraysto encode more information. This is in line with the intuitionthat both steps enable texture resampling: switching the ob-jective allows the encoder to find instantiations of texturethat are visually equivalent, but take fewer bits to encodewith the C3 architecture (for example, the roof in ),and providing CR enables certain stochastic textures to bereproduced by a form of noise shaping (the grass in ). also reveals that basing WD or MSE on saliency",
  "C3/WDsC3/WD8C3/LPIPSC3/MS-SSIMC3/wMSEC3/MSE": ". Human rating study results for alternative perceptualobjectives, including WD without saliency. Error bars indicate 99thpercentile. As loss functions, both MS-SSIM and LPIPS lead toinstabilities. To achieve reasonable results, we clipped the gradientsof the exponentiation operations in MS-SSIM (C3/MS-SSIM), andused a loss equally weighting LPIPS and MSE (C3/LPIPS). We alsoassessed the effect of weighting MSE using saliency (C3/wMSE).",
  ".30array 7array 6array 5array 4array 3array 2array 1networks": ". Dataset-average bit allocation into individual latent arraysof varying resolution from array 1 (highest resolution) to array 7(lowest resolution) for three target bit rates (0.075, 0.15, and 0.3bits/pixel), and three variants of C3: optimized for MSE , andoptimized for WD with = 8, supplied with CR or without. predictions gives both a moderate boost in terms of Eloscores, but the visual improvement with MSE is not sufficientto make it competitive with better metrics. We find that it cansubstantially improve the quality of semantically importantfeatures such as text (). Additional visual examples areprovided in the supplement.After observing the strong performance of WD as an",
  "bits/pixelC3/WDs at 0.191 bits/pixel": ". Crop of image 32 from the CLIC2020 professional dataset (best viewed on screen). Top left: original image. Bottom left: Saliencymap s as derived from EML-net (0: black, 1: white). Top center: C3 optimized for MSE, compressed to 0.191 bits/pixel. Bottom center:C3 optimized for MSE, weighted with the density map p, compressed to 0.202 bits/pixel. Top right: C3 optimized for WD with = 8,compressed to 0.194 bits/pixel. Bottom right: C3 optimized for WD with derived from the saliency map, compressed to 0.191 bits/pixel.Optimizing for MSE leads to flattened texture on the shirt and skin (center column), while WD retains an accurate perception of texture(right column). However, assuming a flat sigma map in WD is equivalent to assuming all image locations will be perceived via peripheralvision, meaning that the text on the camera is subject to texture resampling, making for instance the text on the lens (ZENITAR-M)indecipherable (top right). This is clearly undesirable, as text is a semantically relevant feature. According to the predicted saliency, thecamera will be the subject of visual scrutiny. Modulating according to the saliency map accounts for this. The end result is an image withboth improved texture reproduction as well as legible text (bottom right). Note that simply weighting MSE using predicted saliency does notimprove texture reproduction (bottom center).",
  "WDs (max = 16)0.9420.913": ". Perceptual metric predictivity of human ratings. Left column: Percent-age of 16 659 binary ratings predicted correctly by each metric. Raters comparedrandomly selected image crops of 512 432 pixels. needs to chosen based ondisplay resolution, and the crops are displayed on screen with 4 times lowerresolution than the full images (which roughly have QXGA format). Hence, wechose 4 times smaller values for WD to make crop predictions. Middle andright column: Correlation of metrics with Elo scores. We computed averagedmetric values between all full reconstructions and their originals, yielding onemetric value for each metric and each of 29 methods/rate combinations. Wethen assessed how well each metric predicted the 29 Elo scores by comput-ing the Pearson correlation coefficient (PCC) and Spearmans rank correlationcoefficient (SRCC) between Elo scores and metric averages. optimization objective, we investigated how well it does asan image quality assessment (IQA) method. Tab. 1 showsthat WD well outperforms existing metrics both in terms ofpredicting individual ratings on crops, as well as predictingthe Elo rankings of methods/rates on full images. To put theresult of nearly 73% into perspective, note that agreementamong raters on individual binary answers can vary around80%. Remarkably, with a PCC of 94%, WD is an excellentlinear predictor of Elo scores, which we did not anticipate.",
  ". Discussion": "Image and video compression applications enforce draco-nian limitations on computational complexity, so much sothat codecs using generative methods are generally infeasibleto deploy on todays (and even tomorrows) mobile devices,for reasons of both latency and energy efficiency. Neuraloverfitted codecs were introduced in COIN and NeRV for images and video, respectively, introducing a newclass of neural image compression methods that mirror theasymmetric computational complexity trade-off between theencoder (high complexity) and decoder (low complexity) ofhybrid video codecs such as VVC . Initial follow up workfocused on reducing the encoding time or extendingthe capabilities of these codecs to various data modalities. Better architectures and more refined quan-tization and domain specific optimizations have improvedcompression performance both for images andvideo . However, many of these improve-ments come at the cost of increased computational complex-ity for decoding, weakening one of the key advantages ofoverfitted codecs in the first place. COOL-CHIC andfollow-up work was introduced later, maintainingthe low decoding cost of COIN and improving performanceby not only learning a decoder network, but also latent arraysand an entropy model per image. C3 further improvedperformance in terms of MSE vs. bit rate.In the present work, we introduce a neural image compres-sion method which is nearly the same as C3, but optimizedfor Wasserstein Distortion. We evaluate it using a human rater study, which reveals that the method truly is good,cheap, and fast! To our knowledge, this is the first method toachieve a comparable qualityrate trade-off at this decodingcomplexity. It provides a compelling demonstration that gen-erative methods are not necessary to achieve highly efficientimage compression. With that said, encoding complexityremains a significant limitation of overfitted compression even more so due to the complexity of computing WDinstead of MSE during optimization. A naive implemen-tation computing WD for arbitrary pooling sizes wouldbe computationally prohibitive. Our implementation, whilenot specifically optimized for speed, still leads to a 6-foldincrease in wall time required for each optimization step.With regards to the strong performance of WD both asa loss function and IQA method in our study, we can askwhy it performs better than LPIPS, even though we also usea feature space derived from VGG. There are three maindifferences between WD and LPIPS: 1. Our implementationof WD uses raw VGG features rather than fitting linear fine-tuning layers to human ratings. Instead, we include thesame VGG features computed on two downsampled versionsof the image. This adds the same visual features at largerscales, making the overall set of features more robust toscale effects. 2. To achieve stable results, we had to addMSE to the LPIPS loss at equal weights, which was notnecessary with WD. The reason may be that we included theimage pixels as a 0th layer. Note that this is not equivalent,since 3. WD always compares local statistics (i.e. meanand standard deviation) aggregated according to the -maprather than pixel or feature values directly, which is more inline with models of peripheral vision.Its worth noting that our choice of feature space and-map is entirely ad-hoc. More work is needed to identifywhich types of visual features are most useful, and whichmay be redundant (in the spirit of Portilla and Simoncelli). Better saliency models, perhaps targeted specificallyat image compression, could be identified. Lastly, the per-formance of WD for any and all of these choices shouldbe validated with more extensive human rating studies, forwhich we can only provide a starting point here.",
  "Sen Jia and Neil DB Bruce. EML-net: An expandable multi-layer network for saliency prediction. Image and vision com-puting, 95:103887, 2020. 1, 5": "Wei Jiang, Jiayu Yang, Yongqi Zhai, Peirong Ning, FengGao, and Ronggang Wang. Mlic: Multi-reference entropymodel for learned image compression. In Proceedings ofthe 31st ACM International Conference on Multimedia, page76187627, New York, NY, USA, 2023. Association for Com-puting Machinery. 5, 1 Hyunjik Kim, Matthias Bauer, Lucas Theis, Jonathan RichardSchwarz, and Emilien Dupont. C3: High-performance andlow-complexity neural compression from a single image orvideo. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 93479358, 2024. 1, 2, 3, 5, 6, 7, 8",
  "Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, andDavid Bull. Nvrc: Neural video representation compression.arXiv preprint arXiv:2409.07414, 2024. 8": "Theo Ladune, Pierrick Philippe, Felix Henry, Gordon Clare,and Thomas Leguay. Cool-chic: Coordinate-based low com-plexity hierarchical image codec.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1351513522, 2023. 3, 8 Valero Laparra, Johannes Balle, Alexander Berardino, andEero P. Simoncelli. Perceptual image quality assessmentusing a normalized Laplacian pyramid. In Human Vision andElectronic Imaging 2016, 2016. 8",
  "Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. arXivpreprint arXiv:1409.1556, 2014. 3": "Yannick Strumpler, Janis Postels, Ren Yang, Luc Van Gool,and Federico Tombari. Implicit neural representations forimage compression. In European Conference on ComputerVision, pages 7491. Springer, 2022. 8 Towaki Takikawa, Alex Evans, Jonathan Tremblay, ThomasMuller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.Variable bitrate neural fields.In ACM SIGGRAPH 2022Conference Proceedings, pages 19, 2022. 8",
  "Please see the next pages for further selected examples": ". Horizontal crop of a image 6 from the CLIC2020 professional dataset (best viewed on screen). Left: C3 optimized for MSE,compressed to 0.321 bits/pixel. Right: C3 optimized for WD with = 8, compressed to 0.270 bits/pixel. While optimization for MSE leadsto flattened texture and staircasing artifacts, as seen in the reproduction of the vegetation, texture is vastly improved in the WD-optimizedversion, while using 15% fewer bits. . Horizontal crop of image 11 from the CLIC2020 professional dataset (best viewed on screen). Left: C3 optimized for MSE,compressed to 0.133 bits/pixel. Center: C3 optimized for WD with = 8, compressed to 0.134 bits/pixel. Right: C3 optimized for WDwith derived from a saliency map, compressed to 0.130 bits/pixel. Again, optimization for MSE leads to flattened texture, as seen inthe reproduction of the street and chimneys, while the signage is reproduced well due to its high contrast (difference between dark andlight), which MSE is sensitive to. While texture is vastly improved in the WD-optimized version, the flat version struggles to reproducethe signage well, since it is interpreted as texture. On the right, the saliency model predicts the signage to be scrutinized, and hence thereproduction improves such that the text is legible."
}