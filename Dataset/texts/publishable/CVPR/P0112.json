{
  "Abstract": "We study the visual semantic embedding problem forimage-text matching.Most existing work utilizes a tai-lored cross-attention mechanism to perform local alignmentacross the two image and text modalities. This is compu-tationally expensive, even though it is more powerful thanthe unimodal dual-encoder approach. This work introducesa dual-encoder image-text matching model, leveraging ascene graph to represent captions with nodes for objectsand attributes interconnected by relational edges. Utiliz-ing a graph attention network, our model efficiently en-codes object-attribute and object-object semantic relations,resulting in a robust and fast-performing system. Repre-senting caption as a scene graph offers the ability to uti-lize the strong relational inductive bias of graph neuralnetworks to learn object-attribute and object-object rela-tions effectively.To train the model, we propose lossesthat align the image and caption both at the holistic level(image-caption) and the local level (image-object entity),which we show is key to the success of the model. Ourmodel is termed Composition model for Object Relationsand Attributes, CORA. Experimental results on two promi-nent image-text retrieval benchmarks, Flickr30K and MS-COCO, demonstrate that CORA outperforms existing state-of-the-art computationally expensive cross-attention meth-ods regarding recall score while achieving fast computa-tion speed of the dual encoder. Our code is available at",
  ". Introduction": "Image-text matching is a fundamental computer visionproblem that aims to measure the semantic correspondencebetween an image and a text. Such correspondence can beused for image retrieval given a text description, or text re-trieval provided an image query, both of which are impor-tant in various computer vision applications (e.g., weaklysupervised problems ). The problem is inherentlychallenging due to the ambiguous nature of the image andtext modalities . For example, an image can depict acomplicated situation that a multitude of different captions leap through A man in white is leaping through the air escaping the arena containing an angry black bull leap man airarena bull escape contain",
  "Similarity": "score in whiteblackangry . Illustration of CORA. CORA has a dual-encoder ar-chitecture, consisting of one encoder that embeds the input imageand one encoder that embeds the text caption scene graph into ajoint embedding space. (Best viewed in color and zoomed in.) can describe, whereas a single caption is too abstract andcan semantically apply to multiple images. Various studieshave been proposed and can be categorized into two maindirections: (1) the unimodal dual encoder and (2) the cross-attention approach.Inthedual-encoderframework,twomodality-independent encoders embed the image and text captionseparately into a joint embedding space. In this space, asimilarity function such as a dot product can measure theimage-text similarity. This strategy is also referred to asthe global alignment approach, as the goal is to holisticallyrepresent an image (or text) as a single embedding. Due totheir simplicity and low computational cost (e.g., retrievingan image given a text query can be done via a vector-matrixmultiplication with the cached embeddings), such methodsare more widely adopted for real-world retrieval databases.The second approach, cross-attention network, consti-tutes the majority of recent work.Instead of embed-ding each modality separately, cross-modality attention isadopted to locally align fine-grained visual cues of an im-age (image regions) with textual cues of a caption (wordtokens), from which the overall correspondence score is ag-gregated. While this approach outperforms dual encoder interms of power, it presents a substantial computational chal-lenge. Upon receiving a text (or image) query, every imagevs. text query pair must be processed through the cross-attention model to determine their similarity scores. This re-quirement renders the method impractical for retrieval sys-tems managing large databases due to its extensive compu-tational demands. This work focuses on the dual-encoder",
  "arXiv:2406.11820v1 [cs.CV] 17 Jun 2024": "approach and shows that our dual-encoder proposal evenoutperforms the SOTA cross-attention networks.Existing approaches use a text sequence model (e.g.,GRU , LSTM ) to encode the text caption. A textusually contains an extensive range of semantic informa-tion, such as object categories, attributes of objects, and re-lations between objects. Attributes describe appearance ofobjects , while relations describe how ob-jects interact with one another . Forcing a text sequencemodel to learn to parse a caption into different levels of se-mantics is challenging, especially in the low data regime.For example, by design, a sequence model that simply pro-cesses a caption from left to right (GRU, LSTM) may find itchallenging to determine which attributes belong to an ob-ject and which objects participate in a relation. Numerousworks have shown that Transformer-based text sequencemodels (BERT ) can produce good structural parsing ofa sentence , however, these models must be trained onlarge amounts of data.Nevertheless, it has been shownin that even the CLIP text encoder in Stable Diffu-sion still exhibits incorrect object-attribute binding(i.e., pair an attribute with the wrong object in the sentence)despite having been trained on large datasets. Therefore, itbecomes desirable to have a text embedding model that cancapture the semantic relations between concepts accurately.In this work, instead of a sequence model, we proposerepresenting a caption as a scene graph of object and at-tribute nodes connected by relation edges. An example of ascene graph is illustrated in , where we show that se-mantic structures such as object-attribute and object-objectpairings are already organized. To this end, we propose ourComposition model for Object Relations and Attributes,CORA, a dual-encoder model for image-text matching. Onthe image side, we re-use GPO which is a SOTA pool-ing operator for image-text matching to embed the imageas a vector. On the text side, we propose to use a graph at-tention network with strong relational inductive biasto produce a holistic scene graph embedding for the cap-tion. Scene graph-based approaches have been previouslyexplored in for image-text matching, butthey all employ expensive cross-attention. In addition tothe margin-based triplet ranking loss adopted by priorwork, we propose a contrastive loss to guide CORA in mak-ing alignment at both the holistic image-caption level andthe local image-object entity level. The proposed loss helpsmake training more stable and result in better downstreamretrieval accuracy, as well as additionally acquires CORAwith the image-object entity retrieval capability.Our model is evaluated on two image-text retrievalbenchmarks, Flickr30K and MS-COCO, where it outper-forms SOTA dual-encoder and expensive cross-attentionmethods. Our paper makes the following contributions: We propose CORA, a dual encoder for image-text match-",
  ". Related Work": "Dual-encoder.This approach is dominant in earlierworks in image-text matching. Theimage and text captions are independently embedded ina joint metric space where matching image-caption pairsare located close to each other.Existing work in thisparadigm often improves the joint embedding space by in-troducing new losses , proposing new architecturefor each modality encoder , or learning betterpooling methods . For example, VSE++ pro-poses a triplet loss with hard negative mining which hasbeen adopted by all following image-text matching work.VSRN , DSRAN , SAEM implement graphconvolution and self-attention to improve the encoder archi-tecture. GPO achieves competitive results by designinga new pooling operator that can learn from data. Recently,MV-VSE and SDE propose using multiple em-beddings per sample data, and HREM presents a dual-encoder model that can be trained with a cross-modalitymatching loss for enhancing the embedding quality. Cross-attention. In contrast to embedding the image andtext independently, this approach considers the fine-grainedlocal correspondence between image features and text to-kens before computing the similarity. SCAN is the firstrepresentative work that introduces this idea of using cross-attention between the two modalities to find their align-ments. CAAN later improves the idea by employing anadditional intra-modal interaction step after the cross-modalinteraction. SGARF proposes to learn jointly from boththe global and local alignment to highlight important imageregions. Recently, NAAF encourages the dissimilar-ity degrees between mismatched pairs of image region andword to boost the similarity matching, and CHAN pro-poses a new cross-modal alignment method that can neglectthe redundant misalignments. Graph-based image-text matching.Among both dual-encoder and cross-attention methods, some have utilizedscene graphs as part of their pipeline for more accurateimage-text alignment . Frameworks basedon this approach leverage the capacity of Graph Convolu- tional Networks (GCN) to capture the spatial and semanticrelationships between visual regions and textual tokens. Forexample, SGM , GCN+DIST , GraDual utilizeoff-the-shelf visual scene graph generator to extractscene graph from images, then perform cross-modal align-ment between the visual and textual graph. GSMN , onthe other hand, uses a fully connected graph for the visualregions but additionally uses the regions polar coordinatesto encode their spatial relationships.In our work, we build upon the scene graph representa-tion of the caption to develop the text encoder for our dual-encoder model. Our model focuses on explicitly learning tocompose objects with their attributes and all objects in thescene through their relationships to produce a single em-bedding vector for the text rich in semantic information. Tothe best of our knowledge, there has yet to be any previ-ous dual-encoder work on explicitly capturing the object,attribute, and relation semantics through scene graphs forimage-text matching. Our method is different from previ-ous graph-based approaches in that we do not use externalvisual scene graph generator, which is prone to wrong pre-diction, and we carefully design a 2-step graph encodingapproach trained with a contrastive loss to align at both theglobal image-text and local image-object level. Our net-work outperforms SOTA methods without the heavy cross-attention module.",
  ". Method": "This section describes our Composition model for ObjectRelations and Attributes.We first describe the overallframework in Sec. 3.1, then present in Sec. 3.2 how we per-form visual embedding on the input image, how we parsethe text caption into a scene graph and extract text featuresfor each node in the graph. In Sec. 3.3, we describe how wecan embed this scene graph into the joint embedding spacewith the image using the graph attention network. Finally,training objectives are detailed in Sec. 3.4.",
  ". Overall Framework": "We begin by describing the overall framework of CORA,which is illustrated in . The model consists of twoencoders: a visual encoder f V that takes in an input imagex and produces the image embedding vector v = f V(x) RD; and a text encoder f T that takes in the text caption yand produces its embedding t = f T (y) RD in the jointD-dimensional embedding space. Instead of embedding thetext caption directly, we first parse it into a scene graph us-ing a parser SG, then apply a graph attention network f G to embed this scene graph. Our text embedding formulationtherefore can be rewritten as t = f G(SG(y)).The similarity score between the image and the text cap-tion is defined as the cosine similarity between their embed-",
  ". Feature Extraction": "Visual feature extractor.Given an input image x, wefollow convention from prior work to use the pre-trainedbottom-up detection model BUTD . With this model,the top-36 most confident salient regions in x are detected,along with their visual features {xk R2048}NVk=1, NV =36. The detection model used here is a Faster R-CNN withResNet-101 backbone , pre-trained on Visual Genome.We also transform the region features with an FClayer so that they have the same dimensions as the jointembedding space: xk RD. Furthermore, we also ap-ply multi-head self-attention to contextualize the region fea-tures against one another. Then, in order to perform featureaggregation on this set to obtain a holistic representation forthe input image v = f V(x) RD, we implement f V usingGPO which is a SOTA pooling operator for image-textmatching. Essentially, GPO learns to generate the best pool-ing coefficient for every visual region, which is better thannaively applying mean pooling over the visual feature set. Scene graph parser. Formally, we implement a textualscene graph parser that can construct a graph G = (V, E)given a text caption y, where V = O A denotes the set ofobject nodes O and attribute nodes A, and E = EOA EOOrepresents the set of object-attribute edges EOA and object-object relation edges EOO. Example of a scene graph isillustrated in . We implement a scene graph parserbased on , using the syntactical dependency parserfrom the spaCy library . We develop rules to extractobject nouns (e.g., construction worker), adjective and verbattributes (e.g., salmon-colored, sitting), verb relations (e.g.,person-jump over-fence, dog-wear-costume), and preposi-tion relations (e.g., flag-above-building).Existing scenegraph parsers are developed upon inferior languagetoolkits, thus often misdetect concepts (e.g., those consist-ing of multiple word tokens are not detected). The imple-mentation of our parser is made publicly available. Semantic concept encoder. We denote the set of objectnodes O = {oi}, attribute nodes A = {ai}, and object-object relation edges EOO = {rij}. These concepts are stillin text format that need to be encoded into vector repre-sentation. As these concepts often consist of multiple wordtokens (e.g., pair of shoes, jump over), we use a text se- Five people walking up metal stairs surrounded by large-sized building covered in graffiti paint Faster R-CNN Bottom-upAttention",
  "Scenegraphparser": "Object-Attribute GATObject-Object GAT people walkingfive stairs metal building large-sized graffitipaint peoplestairs walk up building surrounded by graffitipaint covered in Joint embedding space Input image Input text Scene graph GPO GPO large-sized graffitipaint GRU /BERT GRU /BERT walkingGRU /BERTwalking large-sized graffitipaint init attribute node init object node surrounded by GRU /BERT init relation edge",
  "Featurepooling": "a) Overall frameworkb) Semantic concept encoder Image region features Multi-Head Self-Attention . Overview of CORA. a) CORA consists of (1) an image encoder that detects and extracts the salient regions features from theinput image, contextualizes them through a multi-head self-attention, then aggregates them into a single image embedding through theGPO pooling operator, (2) a text encoder that first parses the input text into a scene graph where all semantic information is readilyorganized, then two graph attention networks Object-Attribute GAT and Object-Object GAT are used to encode this graph into the samejoint space with the image. The red arrow denotes the edge of the active role, while the yellow arrow is for the passive role in the relation(refer to Sec. 3.3.2). b) The semantic concept encoder that uses GRU or BERT to encode each semantic concept in the graph correspondingto the object, attribute nodes and relation edges. quence model as a phrase encoder to encode all semanticconcepts. To demonstrate the generalizability of our methodacross different language features, we implement this se-mantic concept encoder using Bi-GRU and BERT .For Bi-GRU, given an L-word semantic concept, we usethe GloVe word embedding of each word to obtain asequence of L 300-dimensional vectors. Next, we employa Bi-GRU and take the final hidden states as the represen-tation for the concept c R300. For BERT, we use theaverage of the output hidden states of all tokens at the lastlayer to represent the concept c R768. For both typesof features, we then use an FC layer to transform the con-cept embedding to have the same dimension D as the jointembedding space. These concept embeddings are used toinitialize the node features for {oi} and {ai} and the edgefeatures for {rij} in the scene graph.",
  ". Scene Graph Embedding": "After obtaining the graph structure from the parser and theinitialized features for all nodes and edges in the graph, wecontinue to elaborate on our scene graph embedding methodas follows. The core idea of our method is that the scene se-mantics should be composed at two levels in a bottom-upmanner, where we use a separate graph attention network(GAT) for each level. At the bottom level, a GATmodels the relations between an object and its associatedattributes. At the top level, another GAT is used to modelthe relations between solely the objects, compose them to-gether and produce the final scene embedding.",
  "GAT Preliminaries. GAT is among the most popular graph": "neural network methods, with SOTA results in graph rep-resentation learning.We follow the implementation ofGATv2 , which is an improved version of the origi-nal GAT . We provide a brief description of GATv2here. Given a directed graph G = (V, E), containing nodesV = {1, ..., N} and E V V where (j, i) E denotesan edge from node j to i. For each node i, we also haveits initial representation denoted as hi Rd. In a messagepassing step, to update features for node i, we first computethe importance value of neighbor node j w.r.t. i as following",
  "Object-Attribute GAT": "At the bottom level, we care about how the semantic repre-sentation of an object is modified by its connected attributesin the graph. These attributes are modifiers that alter thevisual appearance of the object. Because an attribute of one object should in no way alter the appearance of anotherobject, in this step, we apply GAT only on the subgraphGOA = (V, EOA) consists of only edges between the objectand attribute nodes.We denote {hi}|V |i=1, hi RD as the initial representa-tions for all nodes in the graph. These representations areinitialized from the aforementioned semantic concept em-bedding step. We train a graph attention network, which wename GATObj-Att to perform message passing in graph GOA.The updated representation of all nodes is therefore",
  "Object-Object Relation GAT": "At the top level, after acquiring the entity embeddings{ei}|O|i=1 for all object nodes, we continue to apply an-other GAT, which we name GATObj-Obj on the subgraphGOO = (O, EOO) consisting of only object nodes and edgesbetween them. Because these object nodes are connectedwith object-object relation edges {rij}, our first step be-fore applying GAT is to contextualize the entity embeddingswith their corresponding edges.Edge features. Consider a directed relation edge rij. In thisrelation, node i plays the subject (active) role while node jplays the object (passive) role. For example, in the relationman-hold-cup, man is the subject while cup is the object.To obtain the edge features for this relation, we concatenateits semantic encoding rij with the embedding of the entitythat plays the passive role ej as follows: rij = [rijej].While existing work often concatenates rij with boththe subject and object entity, in our work, we find that itis empirically better to characterize a relation with only thepassive object entity. This is intuitively reasonable since themeaning of a relation such as hold-cup, use-computer doesnot depend on what kind of subject is involved.Edge-contextualized entity features.Consider objectnode i, we define Active(i) = {j|rij EOO} consistingall nodes that node i has a subject (active) relation with.Vice-versa, we define Passive(i) = {j|rji EOO} whichis all nodes that node i has an object (passive) relation. Wecontextualize the embedding of entity i with its edges as",
  ". Training Objectives": "Let B = {(vi, ti, {eik}|Oi|k=1)}Ni=1 be the training batch ofoutput image embedding vi of the i-th image, output textembedding ti of the i-th text caption from GATObj-Obj, andset of output entity embeddings {eik}|Oi|k=1 of the i-th textcaption from GATObj-Att. It is reminded that these entities{eik} are embeddings of the object nodes in the scene graphof ti. We train our model CORA with the following losses.For brevity, we denote s(v, t) = vTt/(vt) to be thecosine similarity between v and t.",
  "+ maxj [ + s(vj, ti) s(vi, ti)]+.(9)": "Essentially, for every matching image-caption vi and tiin the training batch, this loss looks for the negative cap-tion tj that is closest to vi, and the negative image vj that isclosest to ti in the embedding space. tj and vj are the hard-est negatives in the training batch and help provide a strongdiscriminative learning signal to the model. Contrastive loss. As observed by previous work , thehardest triplet loss above results in unstable learning duringearly training epochs. We find that applying a contrastiveloss that encourages the model to align the output repre-sentations of all matching image, text, and object entity to-gether results in more stable training and better final results.Because the entity embeddings {eik}|Oi|k=1 are also involvedin the equation here, our model CORA is also trained toperform image retrieval given an object entity (e.g., imagesearching for straw hat). The loss is formulated as follows",
  "vNu exp (s(v, u)),(11)": "where u {ti} {eik}|Oi|k=1 is the semantic embedding ofeither the text or an object entity corresponding to imagei, Ni is the negative set of semantic concepts that do notcorrespond to image i, and similarly Nu is the negative setof images that do not contain semantic concept u.Specificity loss. The contrastive loss above aligns the em-beddings of image, text, and entity together in the jointspace. In addition, we would like to impose some structurein this space such that the similarity between an image viand text ti should be larger than between vi and all entities{eik}. The reason is that a caption always depicts more se-mantic information than an entity alone, hence ti should bemore specific w.r.t. vi and exhibits a larger similarity score.The loss takes the form of a hinge-based triplet loss",
  ". Dataset and Evaluation Metrics": "Datasets. We perform experiments on two standard bench-marks, Flickr30K and MS-COCO , on the image-to-text retrieval (I2T) and text-to-image retrieval (T2I)tasks. In both datasets, every image is annotated with fivetext descriptions. As in prior work , we follow the splitsconvention on both datasets. Flickr30K contains 31K im-ages, of which 29K images are for training, 1K for vali-dation, and 1K for testing. MS-COCO provides 123,287images and is split into 113,287 images for training, 5000images for validation, and 5000 images for testing.Metrics. We report the commonly used Recall@K (R@K),where K {1, 5, 10}. This metric computes the percent-age of queries where the correct match appears in the top-Kretrievals. To summarize performance, we report RSUMwhich is the sum of R@K at all values of K {1, 5, 10} onI2T and T2I tasks. For MS-COCO, by convention, the re-sults are reported in two settings: 5K setting, and 1K settingwhere the results are averaged over five 1K data folds.",
  "Ours83.495.998.664.188.193.1523.3": "or dual-encoder approaches, and are divided into groups de-pending on the textual backbone used (Bi-GRU vs. BERT).Following previous work , we also report theensemble results which are obtained by averaging the simi-larities from two checkpoints trained with different seeds. Comparisons with state-of-the-art methods. When us-ing Bi-GRU as the semantic concept encoder, our methodCORA outperforms all state-of-the-art methods by an im-pressive margin.CORA achieves +5.4 RSUM abso-lute improvement over HREM on Flickr30K, and +13.7RSUM over NAAF on MS-COCO 5K. Note that NAAF isamong the SOTA cross-attention methods (CHAN, GraD-ual, CODER, SGARF) which are more computationally ex-pensive but having more learning capacity advantage overdual encoders, however CORA is still able to surpass them.The non-ensemble version of CORA also outperforms allnon-ensemble methods while even exceeding the ensembleones (SDE, GraDual).When using BERT for encoding semantic concepts,CORA achieves second best RSUM score on Flickr30K andMS-COCO and is only inferior to the recent SOTA HREM.HREM is also a dual encoder, but is trained with a cross-modality mechanism (which is later discarded at inference)to enhance each modality embedding for matching. Thesame idea of HREM can be applied to CORA to boost theperformance even further, but is out of the scope of ourwork. Switching from Bi-GRU to using BERT, our method . Our method yields competitive results on the MS-COCO dataset. Our performance is competitive in all test schema withprevious works, especially on the simple Bi-GRU architecture. denotes methods that use ensembling of multiple models. Bold andunderline highlight the best and second-best performance.",
  "Ours82.897.399.067.392.496.9535.664.387.593.645.474.784.6450.1": "enjoys a smaller RSUM improvement compared to otherwork (+5.5 RSUM for CORA vs. +15.9 for HREM and+10.5 for CHAN on MS-COCO 5K). This is due to BERTbeing more suitable for encoding long text while CORAis using BERT to encode short phrases (e.g., constructionworker, sitting). A text encoder that is more suitable forencoding short phrases is therefore more desirable and weleave this as future work.Similarly, when using BERT,CORA surpasses the performance of SOTA cross-attentionmethods CODER and CHAN. This shows the ability to gen-eralize across different feature extractors of CORA. Comparisonswithscenegraph-basedapproaches.CORA outperforms all scene-graph based methods, whichincludes SGM , GCN+DIST , GSMN , andGraDual . These methods all employ an additional off-the-shelf visual scene graph generator (except GSMN)to produce a scene graph for an input image, and use cross-attention to exchange information between the textual andthe visual graph, but still achieve inferior results to CORA.This further shows that CORA is very effective at encod-ing scene graphs. These methods all embed a whole scenegraph holistically (unlike CORA which separates it into twoobject-attribute and object-object steps), are trained with aholistic loss to align image and text (unlike CORA that hasloss terms to additionally align image and local object en-tity), and use visual scene graph generator which issusceptible to making wrong predictions and has been re-",
  ". Ablation Studies": "We perform a series of ablation studies to explore the impactof our graph attention network design and how the lossesaffect the final performance. All experiments in this sectionuse Bi-GRU for the semantic encoder and are performed onthe Flickr30K dataset. The results are reported in Tab. 3. Number of layers in GAT. The experiments show thathaving 1 layer for GATObj-Att and 2 layers for GATObj-Objachieves the best accuracy. For the object-attribute graph,1 layer is sufficient to propagate the attribute informationto their corresponding object node. For the object-objectrelation graph, using only 1 layer is not enough to aggre-gate information from the whole graph, while increasing to3 layers starts to give diminishing returns. Graph structure. We study whether our 2-step scene graphencoding step is beneficial to the final performance. Werefer to Joint as the model that uses a single GAT on thewhole graph at once, FC as the variant that uses fully con-nected graph instead of the structure parsed from the scenegraph parser, and Obj-Att & Obj-Obj as our proposed 2-stepscene graph encoding model. Note that having a separateobject-attribute encoding step allows our model to produceindividual entity embeddings (see Sec. 3.3.1) that are laterused in the contrastive loss to align an image with each of .Ablation studies for the number of layers in GAT,the graph structure whether encoding scene graph jointly or in 2separate steps is beneficial, and the impact of losses. Bold andunderline highlight the best and second-best performance.",
  ". Qualitative Results & Text-to-Entity Retrieval": "illustrates how CORA can perform image-to-text andimage-to-object entity retrieval.More qualitative resultscan be found in the supplementary.In addition, we also experiment with using the image-entity score for re-ranking the image-text matching. We em-ploy the idea that if an object entity in the text is not closelymatched with the image, then the image-text matching scoreshould be lower. Formally, we utilize the following formula",
  "CORA64.387.593.645.474.784.6450.1CORA + reranking64.287.693.845.574.884.7450.6": "Image-to-text retrieval1. A woman dressed in black with a tattoo on her right arm is taking a picture with her camera .2. A woman with long hair in black clothing is taking a photograph .3. A person with tattoos is looking at a photo on a digital camera , or cellphone .4. A tattooed woman taking a picture with a digital camera .5. Somebody took a photo of a girl with long black hair taking a photo . Image-to-entity retrieval:digital camera, camera lens, woman wearing black, gun range, mobile phone, photographer,black blouse, black backpack, black purse, black leather pumps, black leather bag,dark haired woman",
  ". Conclusion": "Limitation. Despite achieving new SOTA results, CORAstill faces some limitations. CORA is strongly dependenton the scene graph quality from the parser. If the parserfails to extract a scene graph from the input text, CORAalso fails to encode the text. This happens seldomly in MS-COCO, where there are captions that are just exclamatorysentences uttered by the annotator, e.g., I am so happy tosee this view, There are so many things to see here. Onthe other hand, text sequence model is still able to capturethe nuances of these text descriptions.In this paper, we propose a dual-encoder model CORAfor image-text matching that is based on scene graph.CORA achieves new SOTA results, outperforms all SOTAcomputationally expensive cross-attention methods.Weshow a promising future direction for image-text matchingthat, by representing a caption as a scene graph of objectand attribute nodes connected by relation edges, we can uti-lize the strong relational inductive bias of graph neural net-work to compose objects, relations, and their attributes intoa scene graph embedding that is effective for image-text re-trieval.Acknowledgements.Thisprojectwaspartiallyfunded by NSF CAREER Award (#2238769) to AS. Peter Anderson, Xiaodong He, Chris Buehler, DamienTeney, Mark Johnson, Stephen Gould, and Lei Zhang.Bottom-up and top-down attention for image captioning andvisual question answering. In CVPR, 2018. 3, 1",
  "Matthew Honnibal, Ines Montani, Sofie Van Landeghem,and Adriane Boyd. spaCy: Industrial-strength Natural Lan-guage Processing in Python. 2020. 3": "Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Lin-jun Shou, Daxin Jiang, and Ming Zhou. Unicoder: A uni-versal language encoder by pre-training with multiple cross-lingual tasks. arXiv preprint arXiv:1909.00964, 2019. 2 Chuong Huynh, Yuqian Zhou, Zhe Lin, Connelly Barnes,Eli Shechtman, Sohrab Amirghodsi, and Abhinav Shrivas-tava. Simpson: Simplifying photo cleanup with single-clickdistracting object segmentation network. In CVPR, 2023. 1",
  "Quynh Phung, Songwei Ge, and Jia-Bin Huang. Groundedtext-to-image synthesis with attention refocusing. In CVPR,2024. 2": "Bryan A Plummer, Liwei Wang, Chris M Cervantes,Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-nik. Flickr30k entities: Collecting region-to-phrase corre-spondences for richer image-to-sentence models. In ICCV,2015. 6 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2, 1",
  "Sijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan,and Xilin Chen.Cross-modal scene graph matching forrelationship-aware image-text retrieval. In WACV, 2020. 2,3, 6, 7": "Keyu Wen, Xiaodong Gu, and Qingrong Cheng. Learningdual semantic relations with graph attention for image-textmatching.IEEE transactions on circuits and systems forvideo technology, 31(7):28662879, 2020. 2 Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, LeiLi, Weiwei Sun, and Wei-Ying Ma. Unified visual-semanticembeddings: Bridging vision and language with structuredmeaning representations. In CVPR, 2019. 3",
  ". Visual features": "We use the pre-extracted Faster R-CNN 2048-dimensionalregion features from BUTD , which is the standardconvention from prior work in the image-text matching lit-erature . To transform them to have the same di-mensions D as the joint embedding space, we implementa 2-layer MLP with residual connection. The region fea-tures are then pooled using the GPO pooling operatorinto RD.",
  ". Textual features": "Bi-GRU. The dimension of the word embedding is set to300 for both experiments where we initialize the word em-bedding from GloVe or from scratch (refer to Sec. 8 for ex-periment of CORA without using GloVe). The GRU has 1layer and its hidden dimension is also 300. BERT.Similartopriorwork,weusethebert-base-uncasedarchitectureandpre-trainedweights for the BERT semantic concept encoder.Asmentioned in the main paper, using BERT to encode shortphrases (e.g., construction worker, sitting) does not takeadvantage of the full capability of BERT. BERT has neverseen short text during its pre-training stage , and with itsability to capture long-range dependencies, BERT is moresuitable for encoding long sentences. As a result, directfine-tuning BERT for CORA leads to slightly lower results.In our work, instead of fine-tuning the whole BERTmodel (with 110M params), we employ the prefix tuningtechnique P-Tuning v2 in order to repurpose the pre-trained BERT model into encoding short phrases. With thistechnique, at every BERT encoding layer, a sequence oflearnable N token embeddings RN768 is added as prefixinto the textual prompt. Intuitively, these tokens providelearnable context that assist BERT into learning the taskat hand, which is encoding short phrases. The number oftrainable params with P-Tuning v2 is only 2N L 768(where L = 12 is the number of BERT encoding layers, andN = 24 is the number of prefix tokens). In our experiment,we find fine-tuning the last BERT layer along with P-Tuninggives slightly better results. In overall, the number of train-able params of our BERT component is only 7M, which ismuch smaller than 110M params of the whole BERT model.For both types of features (Bi-GRU and BERT), we im-plement an FC layer to transform the semantic encoded out-put into RD before using them to initialize the node andedge features of the GATs. . Inference time comparison. We compare the text-to-image retrieval inference time between our method CORA againsttwo SOTA cross-attention methods SGRAF and NAAF (lower is better). The inference time is calculated with differentnumber of images in the database. CORA with its dual-encoderarchitecture is much faster and scalable than cross-attention ap-proaches.",
  ". Training and hyperparameter details": "We use the AdamW optimizer to train our model for50 epochs. The learning rate is initialized at 5e-4, then de-cayed to 5e-5 after 15 epochs. The learning rate for the pre-trained components (i.e., GloVe and BERT) is scaled by 0.1w.r.t. the base learning rate. We set the batch size to 128when training on Flickr30K, and 256 when training on MS-COCO. The margin in the triplet loss is set to 0.4, whilethe cosine similarity in the contrastive loss is scaled by atemperature of 0.01 similar to CLIP . Following ,we perform size augmentation to randomly drop 35% re-gion features. For data augmentation on the text, we per-form subsampling on the scene graph by randomly dropping10% of the nodes and edges and randomly masking 10% ofthe word tokens. We set CON = 0.25 and SPEC = 3.0.",
  ". Inference Time": "We illustrate in the inference time comparison be-tween our method CORA against SOTA cross-attentionmethods SGRAF and NAAF with different num-ber of images in the database (ranging from a very small toa very large number of images).To conduct this experiment, for all methods, we first for-ward all images through the image encoder of each respec-tive method in order to cache all image embeddings. Then,for CORA, when a text query arrives, it takes 0.04s to parseit into a scene graph, 0.014s to compute its scene graph em- .Our framework achieves the best results on theFlickr30K dataset when initializing the word embeddings fomscratch for the Bi-GRU semantic encoder. Without the CA -cross-attention, our method still has competitive results to otherbaselines. denotes methods that use ensembling of multiplemodels, and we highlight the highest and second-highest RSUM.",
  "Ours81.795.598.162.086.691.8515.7": "bedding, then 0.01s to perform the vector-matrix multipli-cation with the image embeddings to find nearest neighborresults, which in total accounts to around 0.06s per queryfor all number of images from 10 to 105. On the other hand,for cross-attention approaches SGRAF and NAAF, when atext query arrives, these methods have to pair the text querywith every image embedding in the database, then forwardeach pair through the cross-attention module in order to cal-culate their similarity. shows that the inference timefor SGRAF and NAAF scale up linearly w.r.t. the num-ber of images in the database (e.g., SGRAF takes 46s with104 images, and 470s with 105 images), which is due tothe iterative pairing of the input text with each image. Ourmodel CORA enjoys the benefit of being fast and scalableof the dual-encoder architecture, while still achieving betterretrieval results than SOTA cross-attention approaches (e.g.,SGRAF and NAAF).",
  ". More Ablation Studies": "Initialize from GloVe vs. from scratch. When using Bi-GRU, we follow all recent studies to initialize the word embeddings using GloVe . Tofairly compare against other methods prior to these work,we also report our results when using Bi-GRU with wordembeddings initialized from scratch in Tabs. 5 and 6 for theFlickr30K and MS-COCO dataset respectively. The resultsshow that even when initializing the word embeddings fromscratch, our method CORA still outperforms all previouswork with and without cross-attention. BERT P-Tuning v2.We compare between direct fine-tuning the whole BERT model against using P-Tuningv2 to encode short phrases of semantic concepts. Theresults are displayed in Tab. 7. Note that this model is ab-lated without having multi-head self-attention in the visualencoder.",
  ". More Analysis": "With larger visual backbone.We select ResNeXT-101 pretrained on the Instagram dataset as thelarger visual extractor than the Faster R-CNN model used inour main experiments. This visual backbone is also reportedin VSE and SDE . The results of this experimenton the Flickr30K test set are displayed in Tab. 8, where itshows we obtain a large increase over region features andothers. Simulate parsing errors. As discussed in the conclusions,our CORA model is strongly dependent on the scene graphquality from the parser. To study this dependence, we sim-ulate errors by performing the followings onto the parsedgraphs: drop word tokens from nodes and edges, move at-tribute node to wrong object node, and move edge to wrongobject pair. We randomly perform these onto 10%, 20%,30% F30K captions and achieve 513.2, 512.2, 509.8 RSUM(original performance is 515.8).We observe that mov-ing the edge affects performance more than moving the at-tribute. Why consider CORA. CORA is a promising graph methodthat can supplement what CLIP (& other text encoders) maystruggle against, i.e., sentences with many semantics thatare mixed among objects (discussed in Sec. 1). To show ex-ample, we select 100 sentences in Flickr30K with the high-est number of attributes and relations, then evaluate imageretrieval on them. We obtain results respectively for CLIP,HREM, CORA as 239.3, 240.0, 241.5 RSUM. This showsthe large model CLIP is even slightly inferior to CORA onsentences that are rich in semantics. Compare with pretrained image-text models. We scaleup CORA on larger data and compare with prior SOTAimage-text models in Tab. 9. We pretrain CORA on 1Mimage-text pairs in Conceptual Captions. All of the mod-els in the table are finally fine-tuned on Flickr30K. Com-pared with CORA-BERT (refer to Tab. 1), CORA pre-trained gets a +6.8 score.Despite smaller data and notusing cross-attention, CORA is better than ViLBERT ,UNITER , and can potentially reach Unicoder withmore data. However, it is inferior to CLIP zero-shot .This shows the promising ability to scale up CORA further,e.g. by using ViT instead of regions, CLIP-text instead ofBERT, and more data.",
  ". Qualitative Results": "Image-to-text and image-to-entity retrieval.We illus-trate some examples of successful and failed results whenperforming image-to-text retrieval using our CORA modelwith Faster R-CNN + BERT trained on the MS-COCOdataset in and . Because CORA also has theability to retrieve object entities, we also include image-to- . Our framework achieves the best results on the MS-COCO dataset when initializing the word embeddings fom scratch forthe Bi-GRU semantic encoder. Without the CA - cross-attention, our method still has competitive results to other baselines. denotesmethods that use ensembling of multiple models, and we highlight the highest and second-highest RSUM.",
  "CORA-BERT - Data: CC1M530.1CLIP zero-shot - ICML21 - Data: CLIP 400M540.6": "entity retrieval results in the figures. The image-to-entityretrieval results also help display some of the biases of themodel. One interesting application of image-to-entiy re-trieval is for auto image tagging.Among the examples in , the wrong matching textsand entities are understandable because they are still verysemantically aligned with the input image. We explain each case below:1. In the top image, all retrieved captions are correct.Among the retrieved entities, there are a few incorrectresults which show that the model has not learned veryaccurately the visual appearance of receipt, hairbrush,calendar. Images of toddler holding a hairbrush is com-mon in the training set, which must have made the modelsteered towards aligning hairbrush with something thata toddler is holding. 2. In the middle image, most matching captions are cor-rectly retrieved except one that is incorrect due to objectcounting. Counting the correct number of objects is in-deed a challenge for image-text matching model. Themodel also mistakenly recognizes the kite as a plane. 3. In the bottom image, the 1st caption is incorrect, but themodel still ranks it at the top due to multiple semanticinformation in the text are still correct w.r.t. the image(e.g., young boy, living room, cat). All other captions arecorrectly retrieved. The image-to-entity retrievals showthe concepts that the model does not grasp well.We continue to explain the failure cases in as fol-lowing:1. In the top image, all of the retrieved captions are incor-rect matchings as determined by the ground truth data.However, we notice that the 2nd, 3rd captions still cor-rectly describe the image to a certain extent. This is aweakness of the benchmark. 2. In the middle image, the model must have wrongly as-sociated skin with bikini, hence why it retrieves captionswith bikini at rank 4 and 5. In the entity retrieval results,interestingly, we notice the model returns dental proce-dure and dental work. We figure that the model musthave aligned the action of mouth opening with dental,hence why these two entities are retrieved in this case.",
  ". In the bottom image, this is again an example of wherethe retrieved captions correctly describe the image, butbecause the ground truth data specify otherwise, they are": "Image-to-text retrieval1. A baby standing next to a refrigerator reaching for a magnet .2. A baby standing by a refrigerator with an object in hand .3. A baby grabs a magnet from the refrigerator .4. A young child curiously examines a refrigerator magnet .5. A baby girl playing with magnets on a refrigerator . Image-to-entity retrieval:magnet, refrigerator, boy giving, note, receipt, toddler boy, father's perspective, card, hairbrush,young boy reading, young boy pointing, fridge, curious toddler, baby reaching, calendar Image-to-text retrieval1. Five people watch a kite as it flies over a sandy hill .2. A few people standing on top of a hill flying a kite .3. Some people are flying a kite on a brown hill .4. Three people flying a kite in the air during the day .5. some people standing on a hill with a kite flying above Image-to-entity retrieval:couple of people, people standing, sand dune, sandy hill, couple of kids, sand hill, people down below, children,sand flats, kite flying, sandy plain, sandy desert area, plane flying Image-to-text retrieval1. A young boy walking through a living room towards a cat .2. A man with a backpack on with a cat hanging out of it .3. A man with a backpack and a cat peeking out from it .4. The man is carrying the backpack with a kitten in it .5. A man wearing a back pack with a cat inside of it . Image-to-entity retrieval:sweater vest, tabby cat, cat climbing, tiger cat, arm, cat looking upward, tiger suit, long-sleeved shirt, striped cat,striped shirt, cat looking down, man standing, poised cat, house cat, cat hanging, living room, shoulders . Successful image-to-text and image-to-entity retrieval on MS-COCO. In image-to-text retrieval, green denotes matchingtext according to the ground truth of MS-COCO, while red denotes incorrect matching. In image-to-entity retrieval, green and red denotecorrect and incorrect matching, respectively, as judged subjectively by us. considered incorrect by the benchmark.Text-to-image retrieval results. We illustrate some text-to-image retrieval results in . In both examples, ourmodel is able to retrieve the correct image at rank 1. Theimages from rank 2 to rank 5 all exhibit visual traits thatmatch partially with the input text. Image-to-text retrieval1. A train stop with a band including a tuba and drum .2. A brick wall has colorful graffiti on it .3. A circular piece of architecture is in this city .4. A man advertises with a giant sign tied onto his bicycle .5. A man stands with his arms out inside a large green piece of equipment . Image-to-entity retrieval:large circular object, giant sign, bicycle, large bike basket, colorful graffiti, ship wheel, caution tape, large metalsculpture, huge house, hispanic writing, skyscrapers, utility lift, man Image-to-text retrieval1. A guy performing a song shirtless with a tattoo of a safety pen on his body with a second tattoo on his body .2. A tattooed man pours a beer into the mouth of a thin blond man .3. A tattooed man pouring beer out of a bottle into a young man 's mouth .4. A lady in a bikini is pouring a drink into a red cup for a man who is wearing trunks and sunglasses .5. A woman in a bikini is pouring a drink for a man . Image-to-entity retrieval:wife beater, muscular man, black weights, black bars, metal bar, two mechanics working, tattoo, shirtless guy,cowboy, dental procedure, barechested men, dental work, tank top, fitness machine, two shirtless men Image-to-text retrieval1. Man falling off of a bucking bull , at a rodeo , in front of spectators .2. People at a rodeo are watching a cowboy getting thrown from the bull .3. A man in a rodeo is riding a bull while others watch .4. A cowboy is riding a bull on a rodeo and is having trouble staying upright .5. A man is riding a bull in a rodeo . Image-to-entity retrieval:bull crashing, mad bull, bull, bull kicking, bull rider jumping, black bull, fiery obstacle, rodeo, spectators watching,cowboy, dog racing, man falling, people cheering, spectators nearby, horse bucking . Failure cases of image-to-text and image-to-entity retrieval on Flickr30K. In image-to-text retrieval, green denotes matchingtext according to the ground truth of Flickr30K, while red denotes incorrect matching. In image-to-entity retrieval, green and red denotecorrect and incorrect matching, respectively, as judged subjectively by us. A large white dog sits on a bench with people next to a path . A fire hydrant on a cobbled stone sidewalk with a red bus in the distance . . Text-to-image retrieval on MS-COCO. For every text, we show the top-5 retrieved images on MS-COCO. The image with thegreen tick mark is the correct matching according to ground truth in the dataset."
}