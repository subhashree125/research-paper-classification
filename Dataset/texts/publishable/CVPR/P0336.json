{
  "Abstract": "Scene text images contain not only style information(font, background) but also content information (charac-ter, texture).Different scene text tasks need differentinformation, but previous representation learning meth-ods use tightly coupled features for all tasks, resultingin sub-optimal performance. We propose a DisentangledRepresentation Learning framework (DARLING) aimed atdisentangling these two types of features for improvedadaptability in better addressing various downstream tasks(choose what you really need). Specifically, we synthesizea dataset of image pairs with identical style but differentcontent. Based on the dataset, we decouple the two typesof features by the supervision design. Clearly, we directlysplit the visual representation into style and content fea-tures, the content features are supervised by a text recogni-tion loss, while an alignment loss aligns the style features inthe image pairs. Then, style features are employed in recon-structing the counterpart image via an image decoder witha prompt that indicates the counterparts content. Such anoperation effectively decouples the features based on theirdistinctive properties. To the best of our knowledge, this isthe first time in the field of scene text that disentangles theinherent properties of the text images. Our method achievesstate-of-the-art performance in Scene Text Recognition, Re-moval, and Editing.",
  ". Introduction": "As an important information carrier, language is widelyfound in natural scenes. Scene text is a significant topicin scene understanding and perception. There have been anumber of researches on scene text including Scene TextRecognition (STR), Scene Text Editing (STE), Scene TextRemoval (STRM), etc. These researches are widely usedin humancomputer interaction , cross-modal under-standing , automatic pilots, etc.",
  "Recognition": ".(a) The pipeline of previous representation learningmethods that use a tightly coupled feature for all tasks. D meansdecoder and R/E represents the recognizer or eraser. (b) Our de-coupled representation learning framework for multi-tasking. In contemporary research, numerous studies try toleverage representation learning to enhance feature qual-ity, thereby improving performance in downstream tasks.Within the domain of scene text, approaches employingmask image modeling (MIM) and feature contrastive learn-ing (CL) have garnered notable success . As de-picted in (a), these works first use a decoder to pre-train the backbone, facilitating the completion of the pre-tasks. Then, the pre-trained backbone is used for fine-tuningwith a task-specific decoder. Although achieve impressiveperformance, it is apparent that these pipelines face chal-lenges. Using the same representation for different down-stream tasks including discriminative and generative is sub-optimal, limiting the generalization of the methods. To address the above issue, we investigate distinctiveproperties of scene text images that set them apart fromgeneral scene images. Specifically, cropped scene text im-ages contain a focused region with high information den-sity alongside a diverse background. We categorize these",
  "arXiv:2405.04377v1 [cs.CV] 7 May 2024": "distinctive attributes into style and content features. Stylefeatures encompass the background and text style elements(such as font, color, tilt, etc.), while content features encom-pass content and texture details. In STR, the essential infor-mation is content features. Style information is consideredas noise that hinders accurate recognition. Nevertheless, inSTE, the process can be divided into two stages: text re-moval and text rendering. The text removal stage resemblesan image inpainting task that reconstructs the backgroundpixels in the original text area. This process requires contentfeatures for stroke localization and style features for back-ground reconstruction. The text rendering stage relies oncontent features to generate text strokes and style featuresto define the font. Therefore, different downstream tasksneed different information ( (b)), and features irrele-vant to a particular task may hinder task completion. Thedecomposition of these two types of features can be appliedto various scene text tasks.In this paper, we explore the representation learning froma novel perspective by considering the above unique prop-erties of scene text. To decouple two types of features, weintroduce a method as illustrated in . To begin with,we synthesized a dataset containing pairs of images to facil-itate the design of subsequent decoupled pre-training meth-ods. The image pairs contain identical backgrounds andstyles but differ in content. These pairs are simultaneouslyinputted into the network. Features of these image pairs areextracted using a ViT-based backbone along with a decou-pling block. We directly divide the output tokens into twoparts and subsequently achieve decoupling through differ-ent losses. Multi-task decoder utilizes these decoupled fea-tures to perform various tasks. To achieve the intended dis-entanglement of features and effectively train the model, wepropose a training paradigm. This paradigm involves exclu-sively using the content features for recognition while align-ing the style features of the image pairs. Additionally, stylefeatures are used to reconstruct the counterpart image witha text prompt. After pre-training, the multi-task decoder caneffectively handle both generative and discriminative tasks,while also serving as a great starting for fine-tuning. Com-pared with previous methods, our method can accomplishboth generative and discriminative tasks without the needfor additional modules in the pre-training stage.To summarize, our contributions are as follows: We propose to decouple the features for scene text tasks,leveraging the distinctive properties of scene text images.This endeavor may encourage the research community toreconsider the distinctiveness of textual images.",
  ". Scene Text Tasks": "We mainly review three widely researched scene text tasks:Scene Text Recognition, Editing, and Removal.Scene Text Recognition (STR) has been a significantresearch term in computer vision. There are three kinds ofmethods: CTC-based, attention-based, and segmentation-based.(1) CTC-based methods use a Con-nectionist Temporal Classification (CTC) decoder todirectly transform the image features into text sequences.Such an operation has a fast inference speed but a limitedperformance. (2) Attention-based methods use a learnable query and a cross-attention oper-ation to decode the recognition result. Such methods havehigh recognition accuracy but usually not as fast as CTC-based methods. (3) Segmentation-based methods treat the scene text images as a combination of characters.Recently, some works use pre-processing algo-rithms to obtain the segmentation map which can furtherdirect the model to get better performance. Our method usesthe attention-based decoder because of its high performanceand elegant consistency with the transformer structure.Scene Text Editing (STE) aims to replace text in ascene image with new text while maintaining the originalbackground and styles. Due to the great development ofGenerating Adversarial Networks (GAN) , GAN-basedscene text editing methods attract increasing research in-terest. SRNet first proposes to divide the editing taskinto three sub-process: background inpainting, text con-version and fusion, which inspires many subsequent works.Nowadays, with the advances in diffusionmodels, some works use the diffusion process toachieve excellent editing results. However, due to the com-plexity of the diffusion model, these methods are less effi-cient. From a unified perspective, we abandon the use ofdiffusion and GAN, in favor of an efficient attention struc-ture that achieves high-quality generation through decou-pled features.Scene Text Removal (STRM) can be seen as the firststep in STE. Recently a variety of approaches tried to accomplish text removal on a wholescene image. However, as texts are sparse in the scene im-ages, text erasing directly on the scene image tends to affectnon-text areas and seems uncontrollable. For uniformity,our approach uses cropped images.",
  "Image Pair": ". The pipeline and training paradigm of our DARLING. The Decoupling Block divides features from the backbone into style andcontent features. The multi-task decoder processes these features to perform both discriminative and generative tasks. [p] is the paddingsymbol. Image pairs with the same style but different content are input. The style features are aligned and recognition loss supervises thecontent features to eliminate the style from content features. masked image modeling (MIM) to glean implicit knowl-edge from substantial unlabeled data. DiG proposesthe combined use of MIM and contrastive learning to bol-ster backbone representation for accurate text recognition. introduces a high-quality dataset for unsupervised pre-training, leading to significant improvements in STR. Inthe field of STRM, recent efforts leverage syn-thetic datasets for pre-training, enhancing performance andyielding more robust features. However, these methods fo-cus on using a tightly coupled feature to accomplish down-stream tasks, which limits the generalization of the method.SimAN introduces a Similarity-Aware Normalizationmodule, implicitly decomposing features and achieving no-table performance across some tasks. Despite this, the de-composition using instance normalization is not thorough.We first propose to address multiple tasks by feature disen-tanglement and introduce a disentangled training paradigm.",
  ". Method": "Our DARLING is a pre-training method for scene text tasksincluding STR, STE, and STRM. In this section, we detailthe pipeline of the proposed method in Sec. 3.1. Then, wepropose the disentanglement training paradigm and multi-task decoder in Sec. 3.2 and Sec. 3.3. Finally, we describethe training objective in Sec. 3.4.",
  "ponents denoted as FD = [FS, FC], where FS RL2 D": "indicates the style features and FC RL2 D represents thecontent features. The decoupling block comprises multi-ple self-attention layers designed to capture long-range in-formation essential for feature separation. Subsequently,the two kinds of features are inputted into the multi-taskdecoder with a text prompt indicating the desired text forrendering. This decoder is capable of handling both dis-criminative tasks like recognition and generative tasks suchas editing and removal. Meanwhile, multi-task supervisionaids in constraining features, fostering feature disentangle-ment, and acquiring diverse features.",
  ". Disentangled Training Paradigm": "To achieve the expected disentanglement of the two featuretypes, we introduce a pre-training paradigm. The concept isdepicted in . First, we employ a synthesis engine tocreate pairs of images that share identical backgrounds andfonts but differ in content. Subsequently, pairs of images aresimultaneously fed into our proposed network. Due to thesimilarity in background and font, which we categorize asstyle features, we employ an alignment loss LA to align thestyle features of image pairs. The formulation is as follows:",
  "Block Gradient": ". The structure of Multi-task Decoder. It comprises theGenerative Branch (GEB) and the Discriminative Branch (DIB),each dedicated to specific tasks. Gated Injection strategy is pro-posed to convey fine-grained details from DIB to GEB. coder, the text recognition task only uses the content fea-ture.Therefore, the supervision of recognition loss willeliminate the style information from content features, whichfurther ensures the decoupling.Discussion. Through the utilization of the proposed dis-entangled pre-training, the model is constrained to align thecommon style features shared between a pair of images anduse it for text editing. Meanwhile, since scene text recog-nition is inherently a fine-grained task that solely requirescontent features while treating style features as noise, weemploy the recognition loss to guide FC toward contentfeatures. As a result of the pre-training and the set of lossfunctions, the intended decoupling of the two feature typesis achieved.Additionally, in contrast to previous worksthat solely pre-train the backbone, our method includes pre-training of the multi-task decoder, which proves beneficialfor subsequent fine-tuning processes.",
  ". Multi-task Decoder": "The Multi-task Decoder (MTD) is designed to handle bothgenerative tasks and discriminative tasks. As depicted in, due to the distinct feature requirements of thesetasks, the MTD structure comprises two branches: gener-ative and discriminative. Additionally, we propose a gatedinjection strategy to integrate the information from the dis-criminative branch into the generative branch.The discriminative branch (DIB) aims to accomplishSTR task. This task demands fine-grained details but suf-fers from interference with style features.The input ofthe discriminative branch is FC and the branch consists ofN self-attention layers to further extract the fine-graineddetails.Subsequently, the output features FNC are em-ployed for text prediction by an STR head, utilizing a cross-attention operation and a linear projection. The predictionprocess is formalized in Eq. (2), we have projection weightsWK, WV RDD and WR RDC. R RT C rep-",
  "R = Q(FNC WK)(FNC WV )WR.(2)": "The generative branch (GEB) is designed to fulfill gen-erative tasks such as STE and STRM. This branch is fol-lowed by two heads: the background head and the text ren-dering head, responsible for obtaining the background re-construction result and the edited text image, respectively.Similar to DIB, GEB shares the same structure but receivesdifferent input. Considering the necessity for both contentfeatures in generating or removing text areas and style fea-tures in background reconstruction, we configure GEBs in-put as a three-part concatenation, as illustrated in Eq. (3).CT serves as the text prompt indicating the target text weintend to render on the original image.",
  "input = concat(CT , FS, FC).(3)": "In response to the absence of fine-grained details in FC,which are subsequently enhanced by DIB, we introducea Gated Injection strategy to provide well-managed fine-grained information to GEB. To be specific, different layersof features in DIB represent information at different lev-els of detail, all under the supervision of recognition loss.We introduce an adaptive fusion mechanism that operatesbetween each layer of DIB and GEB. As formalized inEq. (4), represents dot production and T stands for theself-attention operation. The superscript indicates the layernumber. WG RD1 transforms the features into a gat-ing weight G, which is further used to control the fusion ofFC and FC, where FC denotes the content features in eachlayer of GEB.",
  "(4)": "Finally, the background head and the text rendering headleverage a cross-attention operation to generate the residualof the background and the text expected, similar to Eq. (3).Overall, in our MTD, the decoupled learning approachguides the model in acquiring more discriminative features,while diverse task-guided training enhances feature diver-sity. Clearly, The recognition loss facilitates the decouplingof features, and the gated injection strategy combines di-verse fine-grained details supervised by recognition loss tobetter accomplish generative tasks.",
  ". Datasets": "For pre-training, we generate a dataset using the publiclyavailable synthesis engine 1. We use the backgroundimages provided by SynthText and the lexicon pro-vided by MJSynth and SynthText . The datasetcontains 4M image pairs for training (TSE-4M) and 10K forevaluation (TSE-10k). Some image samples are shown in. This dataset features more complex text styles andbackgrounds, a broader array of fonts, and includes low-quality images affected by blurring, noise, etc. The eval-uation set serves as a more comprehensive benchmark forassessing the performance of scene text editing methods.The dataset will be made publicly available.For the evaluation of STE, we utilize the synthetic(Tamper-Syn2k) and real (Tamper-Scene) datasets intro-duced by MOSTEL along with our STE-10k.ForSTR, we conduct fine-tuning on Union-L for fair com-parison.Then, the performance is evaluated on 7 com-monly used benchmarks including Union-benchmark,IIIT 5K-Words (IIIT5K) , ICDAR2013 (IC13) ,ICDAR2015 (IC15) , Street View Text (SVT) ,Street View Text-Perspective (SVTP) , and CUTE80(CUTE) .Comprehensive details regarding thesedatasets can be found in prior works . As for STRM,we fine-tune and evaluate the removal part of our model onSCUT-EnsText .",
  ". Implementation Details": "In our implementation, The layer number of the decouplingblock is set to 3 and the layer number of the multi-task de-coder is set to 6. Following most previous methods, theimage size is fixed at 128 32. We conduct the experi-ments on 4 NVIDIA 4090 GPUs with a batch size of 384.For scene text recognition, the vocabulary size C is set to98, including 0 - 9, a - z, A-Z, special characters, [PAD] forpadding symbol and [EOS] for ending symbol. A and Care both set to 0.5.The network is pre-trained end-to-end using Adam optimizer with an initial learning rate set at 1e-4. The learn-ing rate is then decayed to 1e-5 after seven epochs. The . Some sample images from our generated datasets: TSE-4M and TSE-10k. The datasets comprise more diverse imageswith a variety of fonts and backgrounds, including low-quality im-ages. TSE-10k can facilitate a more comprehensive evaluation ofthe models performance. pre-training phase encompasses 200K iterations. For fine-tuning in the STR task, an additional 400K iterations areexecuted, maintaining the same learning rate schedule as inpre-training. For STRM, we fine-tuned 100K iterations.",
  "Scene Text Recognition": "For the task of scene text recognition, we evaluated ourapproach on six widely used benchmarks (common bench-marks) as well as the more diverse Union14M-Benchmark.The results are shown in Tab. 1, the Baseline isthe result of our model trained on STE-4M and Union-L without disentangled pre-train. Compared with it, ourpre-trained model obtains a remarkable performance en-hancement across all datasets. Compared with other meth-ods, our approach surpasses the 0.5% average accuracy ofthe state-of-the-art model while employing fewer parame-ters. This outcome substantiates that our approach attainsa high-quality representation adept at effectively handlingdiverse text images in real-world scenes. The Union14M-Benchmark encompasses a variety of challenging images,such as curved, multi-oriented, artistic, and salient text.Our significant performance improvement is evident acrossthese datasets.Notably, our approach outperforms theSOTA performance by 4.4%, 2.6%, and 0.5% on curved,salient, and multi-oriented datasets, respectively. Our ap-proach demonstrates a slightly diminished performance onmulti-word images which contain several words within asingle image. This limitation can be mitigated by employ-ing a robust text detector.Compared with other pre-training methods, our approachstill exhibits high performance. Note that we just pre-train . Comparison with state-of-the-art STR methods on Common benchmarks and Union-14M Benchmarks. stands for reproducingby ourselves. means the method has a pre-training stage. All methods are trained or fine-tuned on Union-L .",
  "Baseline78.362.878.718.7DARLING (Ours)82.570.881.218.7": "our model on 4M synthetic image pairs we proposed. Incontrast, other pre-training methods rely on approximately10M real images for pre-training. This dataset is consider-ably more difficult to acquire than our synthetic data.Furthermore, we conduct experiments on more challeng-ing datasets in Tab. 2. These datasets encompass sceneswith occlusions (WOST, HOST ) and art characters(WortArt ). A more discriminative representation is re-quired in these scenarios, and our approach achieves thebest performance (1.9%, 3.5%, 1.9% performance gain)compared to previous methods. Despite being a language-independent framework, our approach yields high-qualityfeatures that offer dependable information in scenarios in-volving occlusion and art words.",
  "Scene Text Editing": "Our method uses a disentangled training paradigm, allow-ing for the direct acquisition of a scene text editing networkwithout the need for fine-tuning. We assess its performanceon the datasets introduced by MOSTEL and our STE-10k. To comprehensively evaluate the edited images of our method on synthetic datasets, we adopt the following com-monly used metrics: 1) MSE, the L2 distances; 2) PSNR,the ratio of peak signal to noise; 3) SSIM, the mean struc-tural similarity; 4) FID, the distances between features ex-tracted by Inception V3. In experiments with real-scenedatasets, the absence of ground truth poses a challenge. Weemploy a metric named SeqAcc, which measures recogni-tion accuracy using a widely utilized OCR engine2. Further-more, we found in our experiments that the above metricsbeing good does not mean that the edited images are of highquality. In order to better assess the authenticity of editedimages, we propose a new metric called ClassAcc. Specifi-cally, we employ a simple convolutional network trained onthe original and edited images to distinguish the authentic-ity of the images. The accuracy of this networks classifica-tion acts as the metric for edit quality, where lower accuracyrepresents more authentic generated images, thus indicatingsuperior editing quality. For fair comparison, we resize theimages to 128 32. The results are shown in Tab. 3.Its noteworthy that other methods often necessitate moresupervision and pre-training. For instance, SRNet demandssupervision for text masks, text skeleton images, and stan-dard text images, along with the use of a discriminator foradversarial training. MOSTEL, on the other hand, requiressupervision for text masks and pre-training for both textremoval and text recognition models. In contrast, our ap-proach solely relies on image pairs and does not require adiscriminator or additional pre-training.Compared with other approaches, our method achievessignificant performance on both synthetic and real datasets.On synthetic datasets, we have the best performance exceptFID score. This is due to the constraints in the synthesizerssynthesis quality, causing the ground truth image to not al-ways be the most realistic and unique. Due to the limits ofsynthetic images, real-world scenarios can better reflect the . Comparison with state-of-the-art STE methods on synthetic and real datasets. stands for the methods we reproduce. ClassAcc isthe metric we proposed to evaluate the realism of the generated images. The SSIM, SeqAcc, and ClassAcc are presented in percent (%).",
  ". Comparison of generating details in real scenes. Detailslike artifacts, textures, and sharp edges lead to the fake appearance": "generative capability. Our model attains 10.94% SeqAccimprovement on the real dataset (Tamper-Scene), showcas-ing the readability of our outputs. As shown in (a),the generated images exhibit more accurate text pixels. Ad-ditionally, our method effectively preserves the style of thesource image, as demonstrated in (b). Furthermore,we argue that achieving readability and correct stylizationin generated images is insufficient for measuring generationauthenticity. As shown in (c), in certain instances, thegenerated images are easily recognizable as fake. Hence,we introduce the ClassAcc metric, and our method sur-passes the state-of-the-art by 3%. We further magnify thelocal details of the generated images in . In these in-stances, details like artifacts, textures, and sharp edges leadto fake appearance, but our results remain high quality.",
  "Our model incorporates a background head in the multi-task decoder, enabling it to perform the scene text removal": "task. We feed the image into the model with the text promptCT set as [B][E][P][P]... to complete the STRM task,where [B], [E], and [P] represent the beginning, end,and padding symbols, respectively. The evaluation was con-ducted on the widely used real scene dataset SCUT-EnsText.The results are detailed in Tab. 4.In comparisonto our baseline without disentangled pre-training, our pre-training approach demonstrates a noteworthy improvement.This underscores the advantageous impact of disentangledpre-training on representation learning.When comparedto state-of-the-art methods, our approach achieves the bestperformance across all metrics. Although our method oferasing a cropped image is naturally better than the previousmethod of erasing over the entire image, we can still see thesuperior erasing power of our method from the table com-parison. Moreover, some qualitative examples are shownin , our method can obtain a more effective removaloutcome while keeping the unrelated areas unaffected.",
  ". Ablation Study": "The disentanglement of our model: We propose a dis-entangled representation learning framework for scene texttasks. Leveraging the disentangled training paradigm, wealign style features across style-consistent image pairs. Therecognition loss then guides content features to eliminatestyle information, achieving effective decoupling. The su-perior performance across various tasks substantiates theeffectiveness of our disentangled representation learningparadigm. We further visualize the features in a scatter-plot with the help of PCA algorithm to validate the decou-pling capability of our model. Some simple images are de-picted in . Along the axis of the style feature, differentpositions roughly correspond to different backgrounds and",
  ". Some qualitative examples in STRM task": "fonts, and along the axis of the content features, differentpositions roughly correspond to different contents. This vi-sualization affirms that the representation of two types offeatures differs significantly. When handling diverse tasks,we selectively employ different features to achieve notableperformance improvements.Effectiveness of Gated Injection: The proposed gatedinjection can provide well-managed fine-grained details ofdifferent levels to assist in the generation of text pixels. Tosubstantiate this, we conducted an ablation study (see Tab.5) and presented examples in . It is evident that the in-clusion of the gated injection strategy significantly enhancestext editing performance, resulting in a clearer and more re-alistic generation of text details.",
  ". Limitation": "Compared to other self-supervised works using real-worlddata, our method, utilizing synthetic data, offers certainadvantages. Firstly, synthetic data is more easily accessi-ble. Secondly, unlabeled real data cannot be employed topre-train the decoder, a distinctive feature of our method.However, our approach has its limitations. Synthetic dataexhibits a domain gap with real data, which may impactperformance. Additionally, a substantial amount of unla-beled real data already exists. The question arises of how",
  ". Conclusion": "we explore the distinctions between scene text and generalscene images, proposing to decouple the two distinctive fea-tures (style and content) within scene text images. Employ-ing our disentangled representation learning framework, themodel acquires more discriminative features.When ad-dressing various downstream tasks, distinct features are uti-lized. Specifically, for STR, only content features are em-ployed to eliminate style interference. For generative taskssuch as STE and STRM, style features coupled with well-managed content features are utilized to generate more re-alistic images. Our approach achieves state-of-the-art per-formance in STR, STE, and STRM. We believe this workoffers valuable insights into differentiating scene text im-ages from general images, fostering inspiration for futureresearch in the field of scene text.Acknowledgments: This work is supported by the Na-tional Key Research and Development Program of China(2022YFB3104700), the National Nature Science Founda-tion of China (U23B2028, 62121002, 62102384).",
  "Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, QifengChen, and Furu Wei. Textdiffuser-2: Unleashing the powerof language models for text rendering.arXiv preprintarXiv:2311.16465, 2023. 1, 2": "Xiangcheng Du, Zhao Zhou, Yingbin Zheng, Xingjiao Wu,Tianlong Ma, and Cheng Jin. Progressive scene text erasingwith self-supervision. Computer Vision and Image Under-standing, 233:103712, 2023. 2, 3, 8 Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tian-lun Zheng, Chenxia Li, Yuning Du, and Yu-Gang Jiang. Svtr:Scene text recognition with a single visual model.arXivpreprint arXiv:2205.00159, 2022. 2, 6 Shancheng Fang, Hongtao Xie, Yuxin Wang, ZhendongMao, and Yongdong Zhang.Read like humans:Au-tonomous, bidirectional and iterative language modeling forscene text recognition.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 70987107, 2021. 2, 5, 6 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-nications of the ACM, 63(11):139144, 2020. 2 Alex Graves, Santiago Fernandez, Faustino Gomez, andJurgen Schmidhuber. Connectionist temporal classification:labelling unsegmented sequence data with recurrent neuralnetworks. In Proceedings of the 23rd international confer-ence on Machine learning, pages 369376, 2006. 2 Tongkun Guan, Chaochen Gu, Jingzheng Tu, Xue Yang,Qi Feng, Yudi Zhao, and Wei Shen.Self-supervised im-plicit glyph attention for text recognition. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1528515294, 2023. 2 Tongkun Guan, Wei Shen, Xue Yang, Qi Feng, ZekunJiang, and Xiaokang Yang.Self-supervised character-to-character distillation for text recognition.In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1947319484, 2023. 2, 6 Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman.Synthetic data for text localisation in natural images. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 23152324, 2016. 5",
  "Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-drew Zisserman. Reading text in the wild with convolutionalneural networks. International journal of computer vision,116(1):120, 2016. 5": "Qing Jiang, Jiapeng Wang, Dezhi Peng, Chongyu Liu, andLianwen Jin. Revisiting scene text recognition: A data per-spective.In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 2054320554, 2023.1, 3, 5, 6 Dimosthenis Karatzas,Faisal Shafait,Seiichi Uchida,Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi RoblesMestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-mazan, and Lluis Pere De Las Heras. Icdar 2013 robust read-ing competition. In 2013 12th international conference ondocument analysis and recognition, pages 14841493. IEEE,2013. 5 Dimosthenis Karatzas, Lluis Gomez-Bigorda, AnguelosNicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-mura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-drasekhar, Shijian Lu, et al. Icdar 2015 competition on robustreading. In 2015 13th international conference on documentanalysis and recognition (ICDAR), pages 11561160. IEEE,2015. 5",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization.arXiv preprint arXiv:1412.6980,2014. 5": "Junyeop Lee, Sungrae Park, Jeonghun Baek, Seong Joon Oh,Seonghyeon Kim, and Hwalsuk Lee. On recognizing textsof arbitrary shapes with 2d self-attention. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition Workshops, pages 546547, 2020. 6 Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, LeiZhang, Yun Zheng, Deli Zhao, and Yongdong Zhang. Mo-mentdiff: Generative video moment retrieval from random toreal. In Advances in neural information processing systems,2023. 1 Pandeng Li, Chen-Wei Xie, Liming Zhao, Hongtao Xie,Jiannan Ge, Yun Zheng, Deli Zhao, and Yongdong Zhang.Progressive spatio-temporal prototype matching for text-video retrieval. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 41004110,2023. 1 Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jia-jun Liang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Scenetext recognition from two-dimensional perspective. In Pro-ceedings of the AAAI conference on artificial intelligence,pages 87148721, 2019. 2",
  "Chongyu Liu, Yuliang Liu, Lianwen Jin, Shuaitao Zhang,Canjie Luo, and Yongpan Wang. Erasenet: End-to-end textremoval in the wild. IEEE Transactions on Image Process-ing, 29:87608775, 2020. 2, 5, 7, 8": "Chongyu Liu, Lianwen Jin, Yuliang Liu, Canjie Luo, Bang-dong Chen, Fengjun Guo, and Kai Ding. Dont forget me:accurate background recovery for text removal via modelinglocal-global context. In European Conference on ComputerVision, pages 409426. Springer, 2022. 2, 8 Zhihang Liu, Jun Li, Hongtao Xie, Pandeng Li, Jiannan Ge,Sun-Ao Liu, and Guoqing Jin. Towards balanced alignment:Modal-enhanced semantic modeling for video moment re-trieval. arXiv preprint arXiv:2312.12155, 2023. 1 Canjie Luo, Lianwen Jin, and Jingdong Chen.Siman:exploring self-supervised representation learning of scenetext via similarity-aware normalization. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 10391048, 2022. 3",
  "Guangtao Lyu, Kun Liu, Anna Zhu, Seiichi Uchida, andBrian Kenji Iwana. Fetnet: Feature erasing and transferringnetwork for scene text removal. Pattern Recognition, 140:109531, 2023. 2, 8": "Pengyuan Lyu, Chengquan Zhang, Shanshan Liu, MeinaQiao, Yangliu Xu, Liang Wu, Kun Yao, Junyu Han, Er-rui Ding, and Jingdong Wang. Maskocr: text recognitionwith masked encoder-decoder pretraining.arXiv preprintarXiv:2206.00311, 2022. 2 Weihong Ma, Hesuo Zhang, Lianwen Jin, Sihang Wu, Jia-peng Wang, and Yongpan Wang. Joint layout analysis, char-acter detection and recognition for historical document digi-tization. In 2020 17th International Conference on Frontiersin Handwriting Recognition (ICFHR), pages 3136. IEEE,2020. 2",
  "Anand Mishra, Karteek Alahari, and CV Jawahar. Scene textrecognition using higher order language priors. In BMVC-British machine vision conference. BMVA, 2012. 5": "Dezhi Peng, Lianwen Jin, Weihong Ma, Canyu Xie, HesuoZhang, Shenggao Zhu, and Jing Li. Recognition of handwrit-ten chinese text by segmentation: A segment-annotation-freeapproach. IEEE Transactions on Multimedia, 2022. 2 Dezhi Peng, Chongyu Liu, Yuliang Liu, and Lianwen Jin.Viteraser: Harnessing the power of vision transformers forscene text removal with segmim pretraining. arXiv preprintarXiv:2306.12106, 2023. 2, 3, 8 Trung Quy Phan, Palaiahnakote Shivakumara, ShangxuanTian, and Chew Lim Tan. Recognizing text with perspectivedistortion in natural scenes. In Proceedings of the IEEE In-ternational Conference on Computer Vision, pages 569576,2013. 5 Yadong Qu, Qingfeng Tan, Hongtao Xie, Jianjun Xu, YuxinWang, and Yongdong Zhang. Exploring stroke-level mod-ifications for scene text editing.In Proceedings of theAAAI Conference on Artificial Intelligence, pages 21192127, 2023. 2, 5, 6, 7 Anhar Risnumawan, Palaiahankote Shivakumara, Chee SengChan, and Chew Lim Tan. A robust arbitrary text detectionsystem for natural scene images. Expert Systems with Appli-cations, 41(18):80278048, 2014. 5",
  "on Computer Vision and Pattern Recognition, pages 1322813237, 2020. 2": "Fenfen Sheng, Zhineng Chen, and Bo Xu.Nrtr: A no-recurrence sequence-to-sequence model for scene text recog-nition. In 2019 International conference on document anal-ysis and recognition (ICDAR), pages 781786. IEEE, 2019.2, 6 Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-endtrainable neural network for image-based sequence recog-nition and its application to scene text recognition. IEEEtransactions on pattern analysis and machine intelligence,39(11):22982304, 2016. 2, 6 Baoguang Shi, Mingkun Yang, Xinggang Wang, PengyuanLyu, Cong Yao, and Xiang Bai. Aster: An attentional scenetext recognizer with flexible rectification. IEEE transactionson pattern analysis and machine intelligence, 41(9):20352048, 2018. 2, 6",
  "Peng Wang, Cheng Da, and Cong Yao. Multi-granularityprediction for scene text recognition. In European Confer-ence on Computer Vision, pages 339355. Springer, 2022. 2,5, 6": "Yuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang,Shenggao Zhu, and Yongdong Zhang.From two to one:A new scene text recognizer with visual language model-ing network. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1419414203, 2021.6 Yuxin Wang, Hongtao Xie, Zixiao Wang, Yadong Qu, andYongdong Zhang. What is the real need for scene text re-moval? exploring the background integrity and erasure ex-haustivity properties. IEEE Transactions on Image Process-ing, 2023. 2, 3, 8 Zixiao Wang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Bo-qiang Zhang, and Yongdong Zhang. Symmetrical linguisticfeature distillation with clip for scene text recognition. InProceedings of the 31st ACM International Conference onMultimedia, pages 509518, 2023. 2 Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jing-tuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild.In Proceedings of the 27th ACM international conference onmultimedia, pages 15001508, 2019. 2, 7 Xudong Xie, Ling Fu, Zhifei Zhang, Zhaowen Wang, andXiang Bai. Toward understanding wordart: Corner-guidedtransformer for scene text recognition. In European Confer-ence on Computer Vision, pages 303321. Springer, 2022.6 Mingkun Yang, Minghui Liao, Pu Lu, Jing Wang, Sheng-gao Zhu, Hualin Luo, Qi Tian, and Xiang Bai. Reading andwriting: Discriminative and generative modeling for self-supervised text recognition. In Proceedings of the 30th ACMInternational Conference on Multimedia, pages 42144223,2022. 1, 3",
  "Qiangpeng Yang, Jun Huang, and Wei Lin.Swaptext:Image based texts transfer in scenes.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1470014709, 2020. 2, 7": "Moonbin Yim, Yoonsik Kim, Han-Cheol Cho, and SungraePark.Synthtiger: Synthetic text image generator towardsbetter text recognition models.In International Confer-ence on Document Analysis and Recognition, pages 109124. Springer, 2021. 5 Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han,Jingtuo Liu, and Errui Ding. Towards accurate scene textrecognition with semantic reasoning networks. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1211312122, 2020. 6 Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun,and Wayne Zhang.Robustscanner: Dynamically enhanc-ing positional clues for robust text recognition. In EuropeanConference on Computer Vision, pages 135151. Springer,2020. 6 Boqiang Zhang, Hongtao Xie, Yuxin Wang, Jianjun Xu, andYongdong Zhang. Linguistic more: Taking a further steptoward efficient and accurate scene text recognition. arXivpreprint arXiv:2305.05140, 2023. 2 Shuaitao Zhang, Yuliang Liu, Lianwen Jin, Yaoxiong Huang,and Songxuan Lai. Ensnet: Ensconce text in the wild. In Pro-ceedings of the AAAI conference on artificial intelligence,pages 801808, 2019. 2, 8 Tianlun Zheng, Zhineng Chen, Shancheng Fang, HongtaoXie, and Yu-Gang Jiang. Cdistnet: Perceiving multi-domaincharacter distance for robust text recognition. InternationalJournal of Computer Vision, pages 119, 2023. 2"
}