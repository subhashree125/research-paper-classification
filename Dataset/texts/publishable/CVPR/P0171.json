{
  "Abstract": "Adapters provide an efficient and lightweight mechanismfor adapting trained transformer models to a variety of dif-ferent tasks. However, they have often been found to beoutperformed by other adaptation mechanisms, includinglow-rank adaptation. In this paper, we provide an in-depthstudy of adapters, their internal structure, as well as vari-ous implementation choices. We uncover pitfalls for usingadapters and suggest a concrete, improved adapter architec-ture, called Adapter+, that not only outperforms previousadapter implementations but surpasses a number of other,more complex adaptation mechanisms in several challengingsettings. Despite this, our suggested adapter is highly robustand, unlike previous work, requires little to no manual inter-vention when addressing a novel scenario. Adapter+ reachesstate-of-the-art average accuracy on the VTAB benchmark,even without a per-task hyperparameter optimization.",
  ". Introduction": "Transfer learning from an off-the-shelf model, pre-trainedon a large dataset like ImageNet to a downstream taskby fully fine-tuning the models parameters is a commonparadigm. A typical CNN architecture, like a ResNet ,has several tens of millions of parameters. However, sincethe introduction of transformers into the realm of com-puter vision , model sizes have grownexponentially from around a hundred million parametersfor a vision transformer (ViT) to more than a billionparameters . This leads to huge storage requirementswhen fine-tuning on multiple downstream tasks because acomplete set of the models parameters needs to be savedper task. Additionally, large models require correspondinglylarge datasets [e.g., 56] to be trained to their full potential,yet tend to overfit easily if the target dataset in transfer learn-ing is too small. One solution is linear probing , whereonly the linear classifier is trained, but this usually yieldsinferior results compared to full fine-tuning.As a consequence, there is a growing interest in parameter-efficient tuning methods. The main idea is to freeze the",
  "FacT-TK LoRA VPT": ". Parameter-accuracy characteristics of adaptationmethods on the VTAB test sets. We report original resultsand re-evaluations () after a complete training schedule withsuitable data normalization. Our Adapter+ has clearly the bestparameter-accuracy trade-off. The vertical, dashed line shows thepossible minimal number of tunable parameters when only theclassifiers are trained, i.e., using linear probing (61% accuracy). parameters of the pre-trained model and add a compara-tively small amount of parameters to the model, which arethen tuned together with the classifier to adapt the modelto the downstream task at hand. Representative methodswith different underlying concepts include VPT , whichprepends the sequence of image tokens in the attention withtrainable tokens to learn a prompt tuning, LoRA , wherethe attention weights are updated with learnable low-rankdecomposition matrices, and Adapters , which are smallbottleneck modules that are added to every transformer layerof the network. Adapters were first proposed for CNNs byRebuffi et al. and various formulations existfor the now common ViT architecture.Recent work on parameter-efficient transfer learning [e.g., 20, 21, 31, 32, 39, 67] presents adapters as a baseline methodfor the adaptation to downstream tasks in computer vision.However, we identified various common issues in their imple-mentations, which we find to have a negative influence on the",
  "To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2024": "2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, includingreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, orreuse of any copyrighted component of this work in other works.",
  "Structured": ". Average accuracy for VTAB subgroups on the test sets. For methods marked with , we report results of our re-evaluation aftera complete training schedule with suitable data normalization to ensure a fair comparison. Adapter+ is evaluated with rank r[1..32]. adaptation performance. For further details, refer to the sup-plemental material. Additionally, while adapters have beenwell studied in natural language processing (NLP), there isno study that broadly examines the different adapter config-urations for vision transformers. As a result, adapters haveseemed to underperform in comparison to recent parameter-efficient adaptation methods, e.g., reported accuracies ofadapters on VTAB of 73.9% in and 60.8% in .In this work, we therefore revisit the idea of adaptersand investigate how they can perform at their best in con-nection with ViTs. Our contribution hereby is threefold:(1) We show the first in-depth and systematic study on theeffects of the adapter position in the transformer and of theadapters inner structure with ViTs, as well as evaluate differ-ent variants of parameter initialization. (2) We further pro-pose a learnable, channel-wise scaling as extension to plainadapters, which proves to be beneficial for computer visiontasks. (3) Finally, we present Adapter+, an adapter configu-ration with an excellent parameter-accuracy trade-off com-pared to other work, as shown in . Adapter+ reachesa state-of-the-art average accuracy of 77.6% on VTAB without any hyperparameter optimization per task and 3.7percentage points (pp) over previous adapter baselines. Wealso reach a state-of-the-art accuracy of 90.7% on FGVC with the lowest number of parameters compared toother methods. Finally, Adapter+ shows the best robustnessin terms of accuracy across the VTAB subgroups, see .",
  ". Related work": "One possibility to adapt a pre-trained network to a novel task,apart from full fine-tuning, is to only selectively tune some ofthe parameters, e.g., only training the classifier . Cai et al. proposed to tune only the biases of an otherwise frozennetwork to adapt it to a downstream task. BitFit thenshowed the efficacy of this method for NLP transformers.Modular adaptation.The concept of adding small, train-able modules with only a few parameters to an otherwisefrozen network was first proposed for adapting CNNs byRebuffi et al. and called adapters. Other approachesreplaced all convolutions in the network with depth-wise separable convolutions and only tuned their spatial parts, learned binary masks to prune a pre-trained networkper target task , or created a student network by aug-menting the original network with adapter-like modules andskip connections, which then mimicked a teacher networkby disabling parts of its pre-trained and added modules .Following the rise of transformers in NLP ,Houlsby et al. proposed adapter modules in the formof bottlenecks for transformer layers. Pfeiffer et al. conducted an architecture search on NLP tasks to find amore parameter-efficient configuration of adapter modulesthat only acts on the transformers feed-forward network(FFN), thus saving roughly half of the parameters over . Prompt tuning.Inspired by changing the output of a net-work for NLP with hand-crafted textual prompts, whichmodifies the attention over the original input tokens, Lesteret al. proposed prompt tuning: A set of learnable to-kens is added to the input sequence and trained with back-propagation to prompt a frozen language model to performdownstream tasks. Li and Liang extended on prompttuning by adding learnable tokens at every transformer layerof the model, which they termed prefix tuning. Jia et al. applied prompt tuning to vision transformers, then calledvisual prompt tuning (VPT), by preprending the sequence ofimage patch embeddings with such trainable tokens (VPT-Shallow). They also showed a variant resembling prefixtuning with stronger adaptation capabilities that adds tokensat every layer of the network (VPT-Deep). Low-rank approaches.Also focusing on the attention partof the transformer layers, Hu et al. proposed low-rankadaptation (LoRA) where the attention weights are updatedwith low-rank decomposition matrices. The matrices canbe merged with the attention weights for inference. Thestructure of LoRA is very similar to an adapter, which can beseen as a superset of LoRA acting on the transformers FFN.He et al. proposed a formalism to unify LoRA, adapters,and prefix tuning . It allowed them to combine thebeneficial aspects of all three methods into a scaled paralleladapter (Scaled PA) for NLP tasks. AdaptFormer thenapplied the concept of Scaled PA to vision transformers. Other related work.Newer approaches for vision trans-formers proposed different techniques to further enhance theparameter-accuracy trade-off in adaptation. NOAH per-forms an architecture search for a combination of adapters,LoRA, and VPT for each task. SSF scales and shifts thefeatures in the network after every operation, i.e., attention,FFN, layer normalization, with task-specific, trainable mod-ules. Jie and Deng aggregate the weights of a ViT into asingle 3D tensor. Task-specific weight updates of this tensorare learned as a matrix decomposed into parameter-efficientfactors, hence they termed their method factor-tuning (FacT).SPT measures the importance of the weights of a pre-trained network for a downstream task. Based on a desiredparameter budget, the most important parameters are chosenfor tuning and adapters or LoRA are used for weight matricesthat contain enough parameters of importance. Consolidator adapts weights in multiple orderings of channel-wisegroups. The updates for all groups are merged for efficientstorage and inference.Despite these new developments, we show that the sim-ple concept of adapters exhibits an even better parameter-accuracy trade-off in combination with vision transformers if done right and with the addition of a channel-wise scaling.",
  ". Vision transformer basics": "In this work, we concentrate on the parameter-efficientadaptation of vision transformers (ViT) . The ViT isclosely modeled after the transformer model for natural lan-guage processing (NLP) proposed by Vaswani et al. . Alearned linear projection embeds non-overlapping and flat-tened patches of the input image into a sequence of n tokensx Rnd, where d is called the hidden dimension of thetransformer. A positional encoding is added to the embed-dings and the sequence is prepended with a trainable [CLS]token. The sequence length and the dimension of the tokensstay fixed throughout the architecture. The sequence is sentthrough consecutive transformer layers that each consist of amulti-head self-attention and a feed-forward network (FFN).For the self-attention, the tokens are projected to queries,keys, and values (Q, K, and V ) and the output of each ofthe M attention heads is calculated as",
  ". Adapters and their inner structure": "Adapters are small modules that are added to the trans-former layers. They allow to tailor a network to a new taskor domain, where instead of tuning the parameters of thewhole network, only the adapter parameters and the classi-fier are trained. Adapters take the form of bottlenecks withan inner dimension of r d. We call r the rank of theadapter. In detail, a down-projection to dimension r withweights Wdown Rdr and biases bdown Rr is followedby a non-linear activation function (), typically a GELU as used throughout the ViT, and an up-projection withweights Wup Rrd and biases bup Rd back to the hid-den dimension d of the transformer layer. This yields a baseadapter module",
  "Adapter(x) = s AdapterbaseLN(x).(5)": "For layer-wise scaling, the factor s is taken to be a scalar,i.e. s R, and can be either fixed as a hyperparameter orlearned during training. Layer-wise scaling was proposedby He et al. and Hu et al. but deemed not effectivecompared to a fixed scaling for tasks in NLP. Here, weadditionally propose to use a channel-wise, learned scalingwhere s Rd. We investigate its capabilities in Sec. 4.3. Inmost cases, the adapter is used with a skip connection, hencethe complete feature transformation becomes",
  ". Adapter positions": "Although the architecture of bottleneck adapters for trans-formers is rather simple, there are various ways to plug theminto the transformer layer. Previous work has not yet in-vestigated what the optimum position is for the use with aViT . Here, we evaluate four possible adapter positions,shown in Figs. 3b to 3e. We postulate that it is easier for anadapter to learn to modify features previously transformed FF down FF up",
  "(e) Intermediate": ". Illustrations of (a) the inner structure of an adapter with feed-forward layers (FF), activation layer (Act), and optional layernormalization (LN) and scaling, (b)(d) different possible adapter positions to connect the adapter to the FFN section of the transformerlayer. Modules with trainable parameters are shown in red and frozen modules in blue. by a frozen module in the network rather than to anticipatewhat changes to the features are needed in adapting for afrozen module that follows the adapter. Putting it differently,we argue that the adapter should follow a frozen module.Pre-Adapter.The first adapter position we analyze ap-plies the adapter to the output x of the attention sectionof the transformer layer before it is passed into the FFN,but with the skip connection of the attention already added(b). The feature transformation of the FFN sectionwith the adapter attached, therefore, becomes",
  "x FFNAdapter(x) + x+Adapter(x) + x.(7)": "Note that the two occurrences of Adapter(x) in Eq. (7) referto the same instantiation. In this configuration, the adapterhas the full information from the feature transformation hap-pening in the attention but needs to estimate the transforma-tion that will be happening in the FFN that follows. As aresult, especially the last FFN before the linear classifier willbe hard to adapt. To the best of our knowledge, this adapterposition has not been considered in the literature.Post-Adapter.In this case, the adapter is positioned at thevery end of the transformer layer on the output of the FFNwith its skip connection added as",
  "x AdapterFFN(x) + x+FFN(x) + x,(8)": "where the FFNs refer to the same intantiation (c). Thatway, the adapter has access to the feature transformationhappening in the FFN and the unmodified features via theskip connection. This position has been proposed by Pfeifferet al. as the result of an architecture search, but only foradapting transformers for NLP tasks and not for a ViT.Parallel-Adapter.Next, we consider a parallel setting asproposed by , where the adapter is located parallel to theFFN and both share a skip connection (d):",
  "x FFN(x) + Adapter(x) + x .(9)": "Therefore, both adapter and FFN work on the output of theattention section of the transformer layer and the adapterneeds to learn the necessary residual transformation to theone produced by the frozen FFN.Intermediate-Adapter.Finally, we consider the originaladapter position as proposed by Houlsby et al. . Theadapter is plugged behind the FFN but before the skip con-nection of the FFN is added (e). The adapter addition-ally possesses its own skip connection:",
  ". Initialization of adapter parameters": "Since training a deep learning model is a non-convex opti-mization problem, the initialization of parameters is impor-tant. In this work, we evaluate three different variants ofparameter initializations for adapters proposed in the litera-ture. All of them have the goal to initialize the adapters in away that minimizes the initial influence of the adapters at thestart of their training. This is a sensible goal since adaptersextend an already pre-trained frozen network.Houlsby initialization.Houlsby et al. propose to drawthe weights of the projection matrices from a zero-centeredGaussian distribution with a standard deviation of = 0.01,truncated at 2, and use zero for their biases.BERT initialization.For the BERT model , the initial-ization works similar to but the Gaussian distributionhas a standard deviation of = 0.02 and is not truncated.This form of initialization is used by Pfeiffer et al. .LoRA initialization.LoRA initializes the weights andbiases of the down-projection with a uniform Kaiming He ini-tialization ; the weights and biases of the up-projection",
  "Most widely used are the mean = (0.485, 0.456, 0.406)T": "and standard deviation = (0.229, 0.224, 0.225)T of theImageNet dataset , commonly referred to as ImageNetnormalization. Another option is using 0.5 for every elementof and , which is commonly referred to as Inceptionnormalization because it is used for the Inception family ofCNN architectures, starting with Inception-v3 . The Im-ageNet normalization aims to center the input data around 0with a standard deviation of 1. The Inception normalization,on the other hand, transforms the input values such they arestrictly in range .Because we try to adapt to a target domain on a very lowparameter budget, it is important to use the data normaliza-tion the network saw during its pre-training. Otherwise, theparameter-efficient transfer method of choice needs to firstcompensate for the shift in input data statistics and losesparts of its capacity to adapt to the target domain.",
  ". Datasets": "In order to carry out a detailed study of the utility of adaptersin the context of ViT models, we experiment with two stan-dard benchmarks for task adaptation.VTAB. The Visual Task Adaptation Benchmark (VTAB) consists of 19 tasks, which are further grouped intothree categories: Natural, Specialized, and Structured. TheNatural group contains natural images captured using stan-dard photographic equipment. The Specialized group is builtfrom datasets of images captured with specialized equip-ment, from remote sensing and medical domains. Lastly,the Structured group is for evaluating the understanding ofthe scene structure. Here, the majority of the datasets arecompiled from synthetic images with scenes that are easyto assess for humans but have a large domain gap to naturalimage datasets. Each task of VTAB consists of 800 train-ing and 200 validation images. The test sets have the samenumber of images as the test sets in the original datasets.FGVC. Following Jia et al. , we compile five datasetsfor fine-grained visual classification (FGVC): CUB-200-2011 , NABirds , Oxford Flowers , StanfordDogs , and Stanford Cars . Because VTAB bench-marks task adaptation in a low-data regime in terms of the . Adapter position. We report the average accuracy in %( std. dev.) on the VTAB val sets for different adapter positions.Adapterbase with Houlsby initialization and rank r=8 is used in allexperiments.",
  ". Experimental settings": "For all our experiments, we use a ViT-B/16 network thatwas pre-trained on ImageNet-21k . We follow its pre-training settings, in particular, regarding input data normal-ization. We train all models with an AdamW optimizerwith a learning rate of 103, a weight decay of 104, and abatch size of 64, following . For full fine-tuning, weuse a learning rate of 104, which we found leads to betterresults. We use a cosine learning rate schedule with a linearwarm-up over the first 10 epochs and train for 100 epochs intotal. We use stochastic depth with linearly increasing droprates as a function of network depth from 0 to 0.1 for thefrozen network and with a drop rate of 0.1 for the adaptersduring training. Apart from data normalization (cf. Sec. 3.4),we resize input images to 224224 px for VTAB and use arandomly resize crop to 224224 px and horizontal flippingfor FGVC. For the ablations and to determine hyperparam-eters, we evaluate on the validation splits. We include thevalidation sets in the training data for producing final results.",
  ". Exploring adapter configurations": "Adapter position.We first evaluate the four possible posi-tions to connect an adapter to the FFN section of the trans-former layer, as described in Sec. 3.3. In our ablation, we useAdapterbase (cf. Eq. (4)) with rank r=8 and use the Houlsbyinitialization. In this experiment, the adapters neither have alayer normalization nor use scaling.The results on the VTAB validation set for all four adapterpositions are presented in Tab. 1. The Post-Adapter yieldsthe best result with 76.0% average accuracy over all VTABsubgroups. It confirms our hypothesis that the adapter shouldfollow the frozen FFN module because it can then post-hocmodify the features flowing through the network. The par-allel configuration comes in second with 75.6% averageaccuracy, receiving the same input as the FFN but having to . Inner adapter structure. We evaluate the different com-ponents of the adapter structure, e.g., normalization layer (Norm),layer-wise and channel-wise learnable scaling on the VTAB valsets. The difference to Adapterbase (first row) is shown in base.",
  "channelHoulsby75.8 0.30.2channelHoulsby76.5 0.2+0.5": "learn a residual modification to the FFN instead of a subse-quent one. Pre-Adapter and Intermediate-Adapter are subparcompared to the other positions. They either do not haveaccess to the feature transformation happening afterwards inthe FFN or to the features of the skip connection containingthe output of the attention.Inner structure.Next, we investigate the impact of the in-ner structure of adapters including their initialization. Tab. 2shows our findings with average accuracies calculated overthe three VTAB subgroups. Removing the biases from thelinear layers leads to a decrease in accuracy of 0.4 percent-age points (pp). We find that the Houlsby initialization of theadapter parameters is best while BERT and LoRA initializa-tions reduce the accuracy by 0.2 pp and 0.5 pp. Adding layernormalization (LN) to the adapter is slightly detrimental forall settings, both with scaling and without, while addition-ally adding 2d parameters per layer. We find that a learnedscaling is in general beneficial for image-classification tasks.Adding layer-wise scaling leads to a gain of 0.2 pp. Theinclusion of a learned, channel-wise scaling, as proposedhere, gives the strongest improvement of 0.5 pp, reaching anaccuracy of 76.5% on the VTAB validation set while onlyadding half of the parameters compared to LN.What makes a great adapter?From our systematic explo-ration of possible adapter configurations, we conclude thatadapter modules in the Post-Adapter position with a learn-able, channel-wise scaling and Houlsby initialization workbest for computer vision tasks. We call our proposed adapterconfiguration Adapter+. The addition of layer normaliza-tion, as suggested by Pfeiffer et al. , is not necessary andeven leads to detrimental effects in our setting.Configurations from previous work.Different configu-rations of adapters have been established in previous work.We compare their configurations to our systematic approachwith rank r=8 on the VTAB validation sets. Using our ownimplementations already leads to better results than reportedin literature but enables us to compare on equal footing.Houlsby et al. use an Intermediate-Adapter with their",
  "Configuration# Param (M)NaturalSpecialized StructuredAverage": "Houlsby , r =80.3982.9 0.2 85.5 0.3 58.9 0.875.8 0.3Houlsby , r =40.2482.9 0.4 84.9 0.3 58.3 0.675.4 0.3Pfeiffer 0.2182.9 0.3 86.1 0.9 58.4 0.775.8 0.4AdaptFormer 0.1983.0 0.4 85.0 0.2 57.4 0.575.2 0.2Adapter+0.2083.0 0.2 86.8 0.6 59.7 0.476.5 0.2 proposed initialization both at the FFN section as well at theattention part of the transformer layer. Additionally, theyadapt the LN parameters of the backbone. We, therefore,compare their setting additionally with r = 4 to compareon roughly the same parameter budget. Pfeiffer et al. suggest a Post-Adapter like us but with a BERT initializationand they employ a layer normalization inside the adapter.AdaptFormer has the same configuration as a scaledparallel adapter (Scaled PA) , which was proposed forNLP tasks, the only difference being the layer-wise scaling s.Scaled PA uses a fixed scaling of s = 4 for the adapterswhereas AdaptFormer suggests to use s = 0.1 for visiontasks. Optimizing s for VTAB may lead to better results.Our results are presented in Tab. 3. We see a clear advantageof our Adapter+ configuration, gaining at least 0.7 pp overall previous adapter realizations considered despite havingthe second lowest number of trainable parameters.",
  ". Main results": "VTAB.We evaluate Adapter+ on the VTAB test sets andcompare to other methods in Tab. 4. We provide results forfull fine-tuning and tuning only the linear classifier whilefreezing the rest of the backbone as a baseline of classi-cal fine-tuning methods. As competing parameter-efficienttuning methods, we include LoRA , VPT , NOAH, SSF , FacT , Consolidator , and SPT .Wherever possible, we re-evaluate the other methods witha suitable data normalization for the pre-trained backboneand after the full training schedule to enable a fair compar-ison. For LoRA, we use our own implementation becausethe original work does not cover VTAB. For VPT, we adoptthe number of tokens per task from their hyperparameteroptimization but find that we do not need to tune learningrate and weight decay per task. Additionally, deviating fromthe original implementation, we optimize with AdamW instead of SGD and change to an appropriate data nor-malization. We present the original results from onVTAB together with our re-evaluation. Our improved imple-mentation of VPT increases the average accuracy by 4.4 ppfrom 72.0% to 76.4%. SSF, FacT, and SPT released code toevaluate on VTAB. For FacT and SPT, we change the datanormalization to match the backbone; SSF already uses thecorrect one. We re-run the provided code and present the . Detailed results on the VTAB test sets. We report original results and re-evaluations () in % after a complete training schedulewith suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. : Early-stoppingbased on the test set, : unsuitable data normalization, : per-task hyperparameter optimization. 1Average across the average accuracies ofthe VTAB groups, following previous work. 2No complete code release for Consolidator, hence training and evaluation details are unknown.",
  "SPT-Adapter 0.4374.9 93.2 71.6 99.2 91.1 87.9 57.282.287.0 95.4 86.5 72.485.381.1 63.2 50.3 80.2 84.4 51.4 31.5 42.260.576.0": "Adapter+, r =10.0785.4 92.4 73.1 99.1 91.3 83.1 58.183.287.2 96.6 85.3 72.685.580.7 60.6 50.9 79.9 83.3 55.6 27.1 43.060.176.3Adapter+, r =20.0985.4 93.0 72.7 99.2 90.6 85.3 58.083.587.9 96.8 85.5 71.485.483.2 61.0 51.6 80.1 86.1 56.3 30.7 46.561.976.9Adapter+, r =40.1384.8 93.8 72.7 99.2 90.6 86.5 57.483.687.5 96.9 85.9 71.585.483.4 61.6 53.6 81.4 87.3 55.3 34.4 48.163.177.4Adapter+, r =80.2084.6 94.2 72.3 99.3 90.7 87.6 56.783.687.7 97.0 86.7 72.385.983.2 60.9 53.8 80.3 88.1 55.6 35.7 47.763.177.6Adapter+, r =160.3583.7 94.2 71.5 99.3 90.6 88.2 55.883.387.5 97.0 87.4 72.986.282.9 60.9 53.7 80.8 88.4 55.2 37.3 46.963.377.6 Adapter+, r [1..4] 0.1185.4 93.8 72.7 99.1 90.6 86.5 58.183.787.5 96.8 85.9 71.485.483.4 61.0 53.6 81.4 87.3 55.3 34.4 48.163.177.4Adapter+, r [1..8] 0.1685.4 93.8 72.7 99.1 90.7 87.6 58.183.987.7 96.8 86.7 72.385.983.4 60.9 53.8 80.3 88.1 55.3 35.7 47.763.177.7",
  "Adapter+, r [1..32] 0.2785.4 93.8 72.7 99.1 90.7 88.2 58.184.087.5 96.8 87.8 73.986.583.4 60.9 53.8 80.3 87.2 55.3 37.9 47.763.377.9": "results after a full training schedule. For completeness, wealso report the results from the original publications. How-ever, we found that the code releases of use earlystopping based on the best result on the test set. We arguethat tuning hyperparameters such as the number of trainingepochs on the test set goes against established practices inmachine learning; rather the validation set should be usedfor early stopping. Yet, due to the limited size of the trainingand validation sets in VTAB, it is not feasible to report testresults without also training on the validation data. Hence,we chose to complete a full training schedule of 100 epochsinstead of using early stopping. Training SSF for the fullschedule leads to a decrease in average accuracy of 1.1 ppover the original publication and re-evaluating SPT leadsto a decrease of up to 0.5 pp, even with a corrected datanormalization. FacT on the other hand benefits from ourre-revaluation, since the accuracy decrease from training acomplete schedule is offset by improvements from applyingthe appropriate data normalization. There was no completecode release with configurations to train Consolidator onVTAB at the time of writing, hence we report results as-is.Adapter+ shows the best parameter-accuracy trade-offamong all methods evaluated. This can also be clearly seen in. Additionally, Adapter+ sets a new state of the art with an average accuracy of up to 77.6% over all VTAB subgroupseven without any per-task hyperparameter optimization. Ifwe determine the optimal rank r per task on the validation set,we can further improve the accuracy to 77.9%. Optimizingthe rank leads to a better parameter-accuracy trade-off thanusing a fixed rank across all tasks.In , we compare the average accuracy on the sub-groups of VTAB. Wherever possible, we present the resultsof re-evaluating methods after the last training epoch andmatching the data normalization to the backbone. The aver-age accuracies of Adapter+ with r [1..32] are consistentlyhigher than those of the competing methods. Note that theaccuracies of other methods except SPT differ drasticallyacross the different VTAB subgroups. Adapter+, on the otherhand, shows a high degree of robustness to the domain shiftsbetween groups.FGVC. Next, we present our results on the FGVC bench-mark in Tab. 5. From the contenders, only SSF hasreleased code and hyperparameter configurations for train-ing on FGVC at the time of writing. As we know fromthe code releases for VTAB, the reported numbers show theaccuracy for early stopping based on the test set. There-fore, we expect a similar evaluation for FGVC. While wedo not endorse early stopping based on the test set, we ad- . Detailed results on the FGVC test sets. We reportoriginal results and re-evaluations () in % after a complete trainingschedule with suitable data normalization. Grayed out numbers arenot included in the ranking for best and second best results.",
  "Adapter+ (best epoch)0.3490.4 85.0 99.7 92.6 89.191.4": "ditionally provide numbers for that setting in Tab. 5 for thesake of comparability. Even when training for a completeschedule, Adapter+ shows the best average accuracy with90.7% over all five datasets in FGVC, 0.4 pp over the secondbest method under similar evaluation. When early stoppingwith the test set, Adapter+ reaches 91.4% average accuracy,0.7 pp over the second best method and 2.4 pp better thanfull fine-tuning. This demonstrates that Adapter+ also yieldsstate-of-the-art results for task adaptation when training datais abundant while having the best parameter efficiency.",
  ". Ablations": "Data normalization.We showcase the effect of using anunsuitable data normalization for the chosen ViT in Tab. 6.The gap between ImageNet and Inception normalization (seeSec. 3.5) is largest for VPT , with a 3.4 pp difference inaverage accuracy, which explains around two-thirds of thegain for our re-evaluation as shown in . We suspect thatVPT has less of an ability to scale and shift the data becausethe learnable tokens only act on the attention mechanism.LoRA , FacT , and adapters all employ linear layersthat can directly scale and shift the features of the frozenbackbone and thus compensate better for improper data nor-malization. It is worth mentioning that our Adapter+ is themost robust to improper normalization out of the methodsevaluated, with a gap of only 2.6 pp average accuracy.Training regularization.We investigate the importance oftraining regularization methods like stochastic depth anddropout for training adapters on a frozen ViT backboneand evaluate on the VTAB validation sets. We use linearlyincreasing drop rates as a function of network depth from0 to 0.1 for the frozen layers of the ViT model, and a drop rate",
  "Stochastic Depth76.075.475.3None74.574.373.7": "of 0.1 when using dropout or stochastic depth for the adaptermodules. The results in Tab. 7 show a clear benefit for usingstochastic regularization for the frozen layers as well as theadapters during training. Using dropout in the adapters isonly slightly better than no regularization for adapters, witha gain of only 0.1 pp. With an increase in accuracy of 0.7 pp,stochastic depth is the preferred regularization method foradapters. However, our results show that the more importantpart is the stochastic depth regularization for the frozenmodules of the ViT backbone. Disabling it in training leadsto a loss of 1.5 pp accuracy compared to a training wherestochastic depth is used throughout the model.",
  ". Conclusion": "Applied at the right position and with an optimal inner struc-ture, the simple concept of adapters produces state-of-the-artresults for task adaptation. To understand how adapters canstrike back, we conducted the first systematic and in-depthstudy on how to best construct adapters and integrate themwith vision transformers. This allowed us to determine theoptimal connection point for the adapter in the transformerlayer. Further, we proposed to use a learnable, channel-wisescaling and showed its benefit for computer vision tasks. Ourinsights led us to the creation of Adapter+ that yields thehighest accuracy and the best parameter-accuracy trade-offon VTAB (77.6%, 0.2M) without any per-task hyperparame-ter optimization and on FGVC (90.7%, 0.34M), showing itssuperiority over more complicated methods.",
  "Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and QuocLe. RandAugment: Practical automated data augmentationwith a reduced search space. In NeurIPS*2020": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, PiotrPadlewski, Jonathan Heek, Justin Gilmer, Andreas PeterSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen,Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, MatthiasMinderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerdvan Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahen-dran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bast-ings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birod-kar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink,Alexander Kolesnikov, Filip Pavetic, Dustin Tran, ThomasKipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J.Harmsen, and Neil Houlsby. Scaling vision transformers to22 billion parameters. In ICML, pages 74807512, 2023.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: Pre-training of deep bidirectional trans-formers for language understanding. In NAACL-HLT, pages41714186, 2019": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deepconvolutional activation feature for generic visual recognition.In ICML, pages 647655, 2014. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In ICLR, 2021.",
  "Dan Hendrycks and Kevin Gimpel. Gaussian error linearunits (GELUs). arXiv:1606.08415 [cs.LG], 2023": "Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber,Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Be-longie. Building a bird recognition app and large scale datasetwith citizen scientists: The fine print in fine-grained datasetcollection. In CVPR, pages 595604, 2015. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, BrunaMorrone, Quentin de Laroussilhe, Andrea Gesmundo, MonaAttariyan, and Sylvain Gelly. Parameter-efficient transferlearning for NLP. In ICML, pages 27902799, 2019.",
  "Frank Rosenblatt. The perceptron: A probabilistic model forinformation storage and organization in the brain. Psychol.Rev., 65(6):386408, 1958": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, Alexander C. Berg, andLi Fei-Fei. ImageNet large scale visual recognition challenge.Int. J. Comput. Vision, 115(13):211252, 2015. Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,Aarush Katta, Clayton Mullis, Mitchell Wortsman, PatrickSchramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generationimage-text models. In NeurIPS*2022. Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, RossWightman, Jakob Uszkoreit, and Lucas Beyer. How to trainyour ViT? Data, augmentation, and regularization in visiontransformers. Trans. Mach. Learn. Res., 2022.",
  "A. Why did adapters underperform for ViTs?": "First, we want to shed more light on why adapters do not rankwell in the literature for parameter-efficient transfer learningfor vision tasks. By comparison of numbers reported foradapters on VTAB in the publications referenced in Tab. 4of the main paper, we found that they essentially stem fromonly two sources.The first source is VPT , where results for an adapterwith a reduction factor of 256, amongst other configurations,are reported. For a ViT-B/16 with a hidden dimension ofd=768, this is equal to an adapter with rank r=3. Despiteciting Pfeiffer et al. , who suggest a Post-Adapter po-sition, the actual implementation in the code base* equalsan Intermediate-Adapter that performs worse on VTAB (seeSec. 3.3 of the main paper). The initialization used for theadapter parameters most resembles a LoRA initialization butsets the adapter parameters to zero everywhere. Therefore,there is no randomization in the initialization of the adapterparameters, and different seeds only affect the initializationof the classifier. Additionally, the intermediate features inthe adapter bottlenecks then become all zero, leading to iden-tical gradients in the up-projections at the start of training,which hinders optimization. As a result, the adapter baselineused by VPT only reaches 60.0% average accuracy on theVTAB test sets. This is a gap of 17.6 percentage points (pp)compared to our Adapter+ with rank r=8 (77.6% averageaccuracy). Even when considering the loss of around 2 ppto 3 pp caused by an unsuitable data normalization in theVPT implementation, this is still a very significant gap. Thenumbers for an adapter with rank r =3 from VPT are alsoreported in as a baseline.The second source for adapter baseline results is theNOAH pre-print . There, an adapter with rank r = 8is used. Its implementation performs the following featuretransformation:",
  "*": "Therefore, the adapter becomes harder to train , leadingto an average accuracy of 73.9% on the VTAB test sets or3.7 pp behind our Adapter+. For the NOAH adapter results,we see a proliferation to the publications of FacT andSPT . The adapter implementation from NOAH is alsoused in the code released for Consolidator but theirresults are produced with rank r=16, giving a slightly betteraverage accuracy of 74.3%, or 3.3 pp less than Adapter+.In summary, the examined baseline implementations dif-fer from the configurations proposed by Houlsby et al. and Pfeiffer et al. and introduce issues that lead to theirunderperformance. In our paper, we show that adapters arecapable of reaching 77.6% average accuracy for rank r=8and 77.9% for our optimized version of Adapter+, upliftingadapters from an easy-to-beat baseline to a state-of-the-arttransfer method.",
  "C. More experimental settings": "For all experiments conducted with our implementation,we average the results over three seeds. This includes the(re-)evaluations of LoRA and VPT. We built our implemen-tation on PyTorch , PyTorch Lightning, and timm. Werun experiments with bfloat16 mixed precision on a NVIDIARTX A6000 GPU.For our experiments in the main paper, we report resultsfor a fixed adapter rank r as well as ranks optimized per task.For the per-task optimization of Adapter+, we use a hyper-parameter sweep over the set of ranks r{1, 2, 4, 8, 16, 32}.We evaluate on the validation sets of VTAB and FGVC andchoose the per-task ranks from the specified range(s) to steerthe number of average parameters. The ranks we used toproduce the results on the VTAB and FGVC test sets (seeTabs. 4 and 5 in the main paper) are shown in detail in Tab. 10and Tab. 11, respectively.",
  "D. Calculation of no. of trainable parameters": "Suppose we have a ViT with a hidden dimension d, N trans-former layers, and adapters with rank r. The total num-ber of learnable parameters for Adapterbase modules (cf.Eq. (4) of the main paper) attached to the FFN of everytransformer layer then amounts to N(2dr + r + d). Includ-ing layer normalization in the adapter modules amounts toN2d additional parameters. The addition of learned, layer-wise scaling amounts to N extra parameters and choosinglearned, channel-wise scaling instead adds Nd extra parame-ters. Adapter+ (see Sec. 4.3 of the main paper) thus amountsto N(2dr + 2d + r) total parameters. Additionally, for atask with c classes, we add a classifier with dc + c learnableparameters.",
  "r[1..32]0.34221132": "differences in their training procedures. Here, we examinethree different pre-trainings as examples: (1) Original: TheViT-B/16 weights used in the main paper, pre-trained with su-pervision on ImageNet-21k following the training proce-dure of the original ViT publication ,|| (2) ImageNet-1k:the same ViT weights further fine-tuned on ImageNet-1k,** and (3) AugReg: weights from a pre-training withstronger data augmentation in the form of Mixup andRandAugment following . In Tab. 12, we summarize our results for Adapter+ withrank r=8 evaluated on the VTAB validation sets. We noticethat additional fine-tuning on ImageNet-1k gives a slightedge (83.4% average accuracy over 83.0% for second best) inadaption for tasks that contain natural images. However, thefine-tuning is detrimental for the Specialized and Structuredgroup. Not fine-tuning on ImageNet-1k is beneficial for theStructured group with a large increase of 3.7 pp. The Aug-Reg training setting improves the transfer to the Specializedgroup but is worse than the other settings for natural images.Overall, the original supervised training on ImageNet-21kgeneralizes best across all tasks in VTAB with an averageaccuracy of 76.5%, 0.3 pp better than AugReg training and1.2 pp better than ImageNet-1k fine-tuning.",
  "F. Generality of the conclusions": "Using DINO as an example of a ViT trained with self-supervision, we show in Tab. 13 that the orders of best-to-worst adapter position is consistent with that of a supervisedbackbone in terms of average accuracy, albeit with a higherstandard deviation. The ranking also stays the same for thecomparison of Adapter+ with adapter configurations fromprevious work as presented in Tab. 14. This shows thatour conclusions generalize beyond backbones with super-vised pre-training to backbones based on self-supervisedpre-training."
}