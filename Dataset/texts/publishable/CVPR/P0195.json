{
  "Abstract": "The robust association of the same objects across videoframes in complex scenes is crucial for many applications,especially Multiple Object Tracking (MOT). Current meth-ods predominantly rely on labeled domain-specific videodatasets, which limits the cross-domain generalization oflearned similarity embeddings. We propose MASA, a novelmethod for robust instance association learning, capable ofmatching any objects within videos across diverse domainswithout tracking labels. Leveraging the rich object seg-mentation from the Segment Anything Model (SAM), MASAlearns instance-level correspondence through exhaustivedata transformations. We treat the SAM outputs as dense ob-ject region proposals and learn to match those regions froma vast image collection. We further design a universal MASAadapter which can work in tandem with foundational seg-mentation or detection models and enable them to track anydetected objects. Those combinations present strong zero-shot tracking ability in complex domains. Extensive testson multiple challenging MOT and MOTS benchmarks indi-cate that the proposed method, using only unlabeled staticimages, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain videosequences, in zero-shot association. Our code is availableat github.com/siyuanliii/masa.",
  ". Introduction": "Multiple Object Tracking (MOT) is one of the fundamentalproblems in computer vision. It plays a pivotal role in numer-ous robotics systems such as autonomous driving. Trackingrequires both detecting the objects of interest in videos andassociating them across frames. While recent advancementsin vision foundation models havedemonstrated an exceptional ability to detect, segment, andperceive depth for any objects, associating those objectsin videos remains challenging. Recent successful multipleobject tracking approaches have emphasized the",
  "BootstrapSamplingSegment Everything": ". Given an unlabeled image from any domain, we ap-ply strong augmentations, () and (), to the image, generatingtwo different views with automatically established pixel corre-spondences. Then, we leverage the rich object-level informationencoded by the foundation segmentation model SAM to transferthe pixel-level to dense instance-level correspondence. Such cor-respondences enable us to utilize a diverse collection of unlabeledimages to train a universal tracking adapter atop any segmentationor detection foundation models e.g. SAM. This adapter empowersthe foundational models to track any objects they have detected,and shows strong zero-shot tracking ability in complex domains.",
  "arXiv:2406.04221v1 [cs.CV] 6 Jun 2024": "objects from a specific domain with a small number of fixedcategories or a limited number of labeled frames.Training on those datasets limits the generalizability oftracking models to different domains and novel concepts.Although recent studies have made successfulattempts to address the model generalization issue for objectdetection and segmentation, the path to learning a universalassociation model for tracking any objects is still unclear.Our goal is to develop a method capable of matchingany objects or regions. We aim to integrate this generaliz-able tracking capability with any detection and segmentationmethods to help them track any object they have detected.A primary challenge is acquiring matching supervision forgeneral objects across diverse domains, without incurringsubstantial labelling costs.To this end, we propose the Matching Anything bySegmenting Anything (MASA) pipeline to learn object-levelassociations from unlabeled images of any domain. presents an overview of our MASA pipeline. We leveragethe rich object appearance and shape information encoded bythe foundation segmentation SAM, combined with extensivedata transformation, to establish strong instance correspon-dence.Applying different geometric transformations to the sameimage gives automatic pixel-level correspondence in twoviews from the same image. SAMs segmentation abilityallows for the automatic grouping of pixels from the sameinstance, facilitating the conversion of pixel-level to instance-level correspondence. This process creates a self-supervisionsignal for learning discriminative object representation, uti-lizing dense similarity learning between view pairs. Ourtraining strategy enables us to use a rich collection of rawimages from diverse domains, demonstrating that such auto-matic self-training on diverse raw images provides excellentzero-shot multiple object tracking performance, even sur-passing models reliant on in-domain video annotations forassociation learning.Beyond the self-training pipeline, we further build auniversal tracking adapter MASA adapter, to empowerany existing open-world segmentation and detection founda-tion models such as SAM , Detic and Grounding-DINO for tracking any objects they have detected. Topreserve their original segmentation and detection ability, wefreeze their original backbone and add the MASA adapteron the top.Moreover, we propose a multi-task training pipeline thatjointly performs the distillation of SAMs detection knowl-edge and instance similarity learning. This approach allowsus to learn the objects location, shape and appearance priorof SAM, and simulate real detection proposals during con-trastive similarity learning. This pipeline further improvesthe generalization capabilities of our tracking features. Ad-ditionally, our learned detection head speeds up the original SAM dense uniform point proposals for segmenting every-thing by over tenfold, crucial for tracking applications.We evaluate MASA on multiple challenging benchmarks,including TAO MOT , Open-vocabulary MOT ,MOT and MOTS on BDD100K , and UVO . Exten-sive experiments indicate that compared with state-of-the-artobject tracking approaches trained on thoroughly in-domainlabeled videos, our method achieves on-par or even betterassociation performance, using a single model with the samemodel parameters and testing in zero-shot association set-tings.",
  ". Learning Instance-level Association": "Learning robust instance-level correspondence is crucial toobject tracking. Existing approaches can be divided intoself-supervised and supervised strategies. Specifically, as a representativeself-supervised method, UniTrack attempts to directlyuse off-the-shelf self-supervised representations forassociation. Despite competitive results on some bench-marks , these methods cannot fully exploit instance-level training data, limiting their performance in challengingscenarios. In contrast, supervised methods train discrimi-native instance embeddings on frame pairs, by contrastivelearning. Although achieving superior performance on chal-lenging benchmarks , these methods relyon tremendous in-domain labeled video data. Several meth-ods learn tracking signals from staticimages but still require substantial fine-grained instance an-notations in specific domains or post-hoc test-time adapta-tion , limiting their ability for cross-domain generaliza-tion. To tackle these problems, we exploit the exhaustiveobject shape, and appearance information encoded by SAMto learn universal instance matching, purely from unlabeledimages. Our learned representation shows exceptional zero-shot association ability across diverse domains.",
  ". Segment and Track Anything Models": "Deva , TAM and SAM-Track integrateSAM with video object segmentation (VOS) approaches(such as XMem and DeAOT ) to enable an interac-tive pipeline for tracking any object, where SAM is mainlyused for mask initialization/correction and XMem/DeAOThandle the tracking and prediction. SAM-PT combinesSAM with point-tracking methods such as toperform tracking. However, all those approaches face limita-tions, such as poor mask propagation quality due to domaingaps and the inability to handle multiple diverse objects orrapid objects entry and exit, common in scenarios like au-tonomous driving. Our work focuses on a different direction.Instead of building an interactive tracking pipeline or using",
  ". Preliminaries: SAM": "SAM is composed of three modules: (a) Image encoder:A heavy ViT-based backbone for feature extraction. (b)Prompt encoder: Modeling the positional information fromthe interactive points, box, or mask prompts. (c) Mask de-coder: A transformer-based decoder takes both the extractedimage embedding with the concatenated output and prompttokens for final mask prediction. To generate all potentialmask proposals, SAM adopts densely sampled regular gridsas point anchors and generates mask predictions for eachpoint prompt. The complete pipeline includes patch crop-ping with greedy box-based NMS, three-step filtering, andheavy post-processing on masks. For more details on SAMseverything mode, we refer readers to .",
  ". Matching Anything by Segmenting Anything": "Our method consists of two key components. First, based onSAM, we develop a new pipeline: MASA (.2.1).With this pipeline, we construct exhaustive supervision fordense instance-level correspondence from a rich collectionof unlabeled images. It enables us to learn strong discrimi-native instance representations to track any objects, withoutrequiring any video annotations. Second, we introduce auniversal MASA adapter (.2.2) to effectively trans-form the features from a frozen detection or segmentationbackbone for learning generalizable instance appearance rep-resentations. As a byproduct, the distillation branch of theMASA adapter can also significantly improve the efficiencyof segmenting everything. Besides, we also construct a uni-fied model to jointly detect / segment and track anything(.2.3). Our complete training pipeline is shown in.",
  ". MASA Pipeline": "To learn instance-level correspondence, previous works heavily relied on manually labeled in-domainvideo data. However, current video datasets con-tain only a limited range of fixed categories. This limiteddiversity in datasets leads to learning appearance embed-dings that are tailored to specific domains, posing challengesin their universal generalization.UniTrack demonstrates that universal appearancefeatures can be learned through contrastive self-supervisedlearning techniques from raw images or videos.These representations, harnessing the diversity of a largevolume of unlabeled images, can generalize across differenttracking domains. However, they often depend on clean, object-centered images, such as those in ImageNet , orvideos like DAVIS17 , and focus on frame-level simi-larities. This focus causes them to fail in fully leveraginginstance information, leading to difficulties in learning dis-criminative instance representations in complex domainswith multiple instances, as demonstrated in .To address these issues, we propose the MASA trainingpipeline. Our core idea is to increase diversity from two per-spectives: training image diversity and instance diversity. Asshown in , we first construct a rich collection of rawimages from diverse domains to prevent learning domain-specific features. These images also contain a rich numberof instances in complex environments to enhance instance di-versity. Given an image I, we simulate appearance changesin videos by adopting two different augmentations on thesame image. By applying strong data augmentations (I)and (I), we construct two different views V1 and V2 of I,thereby automatically obtaining pixel-level correspondence.If the image is clean and contains only one instance, suchas those in ImageNet, frame-level similarity can be applied asin . However, with multiple instances, we need tofurther mine the instance information contained in such rawimages. The foundational segmentation model SAM offers us this capability. SAM automatically groups pixelsbelonging to the same instances and also provides the shapeand boundary information of detected instances, valuable forlearning discriminative features.Since we construct the dataset by selecting images withmultiple instances, SAMs exhaustive segmentation of theentire images automatically yields a dense and diverse col-lection of instance proposals Q. With pixel-level correspon-dences established, applying the same () and () to Qtransfers pixel-level correspondence to dense instance-levelcorrespondence. This self-supervision signal enables us touse the contrastive learning formula from tolearn a discriminative contrastive embedding space:",
  ",": "Here, q+ and q denote the positive and negative samplesto q, respectively. Positive samples are the same instanceproposals being applied different () and (). Negativesamples are from different instances. Furthermore, sim()denotes the cosine similarity and is a temperature parame-ter, set to 0.07 in our experiments.This contrastive learning formula pushes object embed-dings belonging to the same instance closer while distancingembeddings from different instances. As demonstrated by ex-isting works , negative samples are crucial for learn-ing discriminative representations. Under the contrastivelearning paradigm, the dense proposals generated by SAM",
  "Head": ". MASA training pipeline. Given an unlabeled image from any domain, SAM automatically generates exhaustive instance masks forit. Then we apply strong augmentations, () and (), to the original image and exhaustive instance segmentation, obtaining two differentviews as the inputs of our model. We train our MASA adapter by joint distillation of SAMs detection knowledge and instance similaritylearning. Better view in color with zoom-in.",
  ". MASA Adapter": "We introduce the MASA adapter, designed to extend theopen-world segmentation and detection models (such asSAM , Detic , and Grounding-DINO ) to trackany detected objects. The MASA adapter operates in con-junction with frozen backbone features from these foun-dational models, ensuring their original detection and seg-mentation capabilities are preserved. However, as not allpre-trained features are inherently discriminative for track-ing, we first transform these frozen backbone features intonew features more suitable for tracking.Given the diversity in shapes and sizes of objects, we con-struct a multi-scale feature pyramid. For hierarchical back-bones like the Swin Transformer in Detic and Ground-ing DINO, we directly employ FPN . For SAM, whichutilizes a plain ViT backbone, we use Transpose Con-volution and MaxPooling to upsample and downsample thesingle-scale features of stride 16 to produce hierarchicalfeatures with scale ratios of 1",
  "k=1wk F j(p + pk + pjk) mjk,(1)": "where L represents the feature level, K is the number ofsampling locations for a convolutional kernel, wk and pkare the weight and predefined offset for the k-th location,respectively, and pjk and mjk are the learnable offset andmodulation factor for the k-th location at the j-th feature level. For SAM-based models, we additionally use task-aware attention and scale-aware attention from Dyhead ,since the detection performance is important for accurateauto mask generation as in (b). After acquiring thetransformed feature map, we extract instance-level featuresby applying RoI-Align to the visual features F, followedby processing with a lightweight track head comprising 4convolutional layers and 1 fully connected layer to generateinstance embeddings.Additionally, we introduce an object prior distillationbranch as an auxiliary task during training. This branch em-ploys a standard RCNN detection head to learn bound-ing boxes that tightly encompass SAMs mask predictions foreach instance. It effectively learns exhaustive object locationand shape knowledge from SAM and distils this informationinto the transformed feature representations. This design notonly strengthens the features of the MASA adapter, resultingin improved association performance but also acceleratesSAMs everything mode by directly providing the predictedbox prompts.The MASA adapter is optimized using a combination ofdetection and contrastive losses as defined in .2.1:L = Ldet +LC. The detection loss is identical to that in .",
  ". Inference": "shows the test pipeline with our unified models.Detect and Track Anything When we integrate the MASAadapter with object detectors, we remove the MASA de-tection head that was learned during training. The MASAadapter then solely serves as a tracker. The detectors predictthe bounding boxes, and then they are utilized to promptthe MASA adapter, which retrieves corresponding trackingfeatures for instance matching. We use a simple bi-softmaxnearest neighbor search for accurate instance matching, asillustrated in Section J.4 of the Appendix.",
  "Ours-Detic46.365.844.128.9": "Segment and Track Anything With SAM, we keep the de-tection head. We use it to predict all potential objects withina scene, forwarding box predictions as prompts to both theSAM mask decoder and the MASA adapter for segmentingand tracking everything. The predicted box prompts omit theneed for the heavy post-processing illustrated in the originalSAMs everything mode, therefore, significantly speedingup the auto mask generation of SAM.Testing with Given Observations When detections are ob-tained from sources other than the one the MASA adapter isbuilt upon, our MASA adapter serves as a tracking featureprovider. We directly utilize the provided bounding boxes asprompts to extract tracking features from our MASA adapterthrough the ROI-Align operation.",
  ". Experimental Setup": "TAO MOT TAO dataset is designed to track a diverserange of objects, encompassing over 800 categories, mak-ing it the most diverse MOT dataset with the largest classcollection to date. It contains 500, 988, and 1,419 videos an-notated at 1 FPS in the train, validation, and test sets, respec-tively. We report performances on the validation set. TAOcomprises several benchmarks, each highlighting differentcharacteristics and requirements. The TAO TETA bench-mark emphasizes association by rewarding trackers thatproduce clean trajectories with no overlaps. Conversely, theTAO Track mAP benchmark values particularly theclassification of trajectories, and does not heavily penalizeoverlapping trajectories. The open-vocabulary MOT bench-mark requires trackers to avoid training with annotationsfrom novel classes, focusing on the generalization ability totrack novel categories.BDD100K MOT requires trackers to track commonobjects in autonomous driving scenarios. The dataset isannotated at 5 FPS with 200 videos in the validation set.BDD100K MOTS Different from BDD100K MOT,BDD100K MOTS requires trackers to track and seg-ment objects simultaneously, evaluating tracking perfor-mance on masks. There are 154 videos for training, 32videos for validation, and 37 videos for testing.UVO is a challenging benchmark for open-world in-stance segmentation in videos. Compared with previousvideo-level object segmentation datasets , it annotatesmuch more diverse instances. UVO has two evaluationtracks, an image track, and a video track. We evaluate allmethods on the UVOv0.5 validation set.Evaluation Metrics As analyzed in previous works ,traditional tracking metrics like mMOTA , and trackmAP can be misleading, particularly in long-tail scenar-ios, due to their high sensitivity to classification. To addressthis issue, introduced TETA, a new tracking metric thatdecomposes into three separate components: AssocA, LocA,and ClsA, reflecting the accuracy of association, localization,and classification, respectively. In standard MOT bench-marks, to ensure a fair comparison of trackers associationabilities, we adopt the same detection observations used byleading state-of-the-art trackers. Therefore, our focus is pri-marily on association-related metrics like AssocA, mIDF1,and IDF1. Additionally, when evaluating our unified mod-els, we consider the full spectrum of metrics to capture theircomprehensive capabilities. Particularly for open-world seg-mentation on UVO, our emphasis is on AR100 and TrackAR100 metrics in the image and video levels. This is dueto the fact that SAM often segments every part of an ob-ject, whereas UVO lacks such detailed annotations, makingtraditional AP evaluations less accurate.Training Data SA-1B consists of 11M diverse, high-resolution images, containing diverse scenarios with multiple object interactions in complex environments. We sub-samplethe SA-1B raw images to construct a training set of 500Kimages, SA-1B-500K.Implementation Details For our models, we utilize theofficial weights of SAM , Detic, and Grounding-DINO,ensuring that all components of these models remain frozenduring the training phase. Specifically, we employ SAMwith both ViT-Base and ViT-Huge backbones, and Detic andGrounding-DINO are used with the SwinB backbone. Wetrain the models with bootstrapping sampling for 200,000images per epoch, with a batch size of 128. We use SGDwith an initial learning rate of 0.04, coupled with a steppolicy for learning rate decay. Momentum and weight decayparameters are set to 0.9 and 1e-4. Our training spans 12epochs, with the learning rate being reduced at the 8th and11th epochs. For data augmentation, we use random affine,MixUp , and Large-scale Jittering , in addition tostandard practices like flipping, color jittering, and randomcropping. More details are provided in Section J of theAppendix.",
  ". State-of-the-Art Comparison": "We evaluate our methods in two ways. Firstly, to accu-rately assess the association ability of our method, we alwaysprovide the same detection observations as current state-of-the-art methods in standard MOT benchmarks. Secondly,to evaluate the integrated abilities of our unified models,we follow this protocol: for SAM-based models, we eval-uate on the open-world video segmentation dataset UVO.For the detectors-based models, we evaluated on the open-vocabulary MOT benchmark . We also report the scoreson TAO TETA and TAO TrackmAP benchmarks. Note thatwe perform zero-shot association tests for all our variants,and use the same weights across all benchmarks.TAO TETA We use the same observations as TETer-SwinT .As shown in , our method withGrounding-DINOs backbone performs the best, in the zero-shot setting, without training on any in-domain labeledvideos, on both AssocA and TETA. We also test our uni-fied Detic model which jointly outputs the detection andtracking results. It outperforms all other methods signifi-cantly and achieves the new state-of-the-art. It demonstratesour method can couple well with current detection founda-tion models and transfer their strong detection ability intotracking.Open-vocabulary MOT Similar to the open-vocabularyobject detection task , open-vocabulary MOT stipu-lates that methods should only use the frequent and commonclasses annotations from LVIS for training, treating therare classes as novel. We evaluated our unified detect andtrack anything model Detic, which was trained exclusivelywith base class annotations. shows our unified Deticmodel outperforms existing models on all metrics across both base and novel splits, and it achieves this significantlead despite our tracker being trained solely with out-of-domain, unlabeled images.TAO Track mAP We use the same observations as GTR .As shown in , our method with SAM-B performs thebest (Track mAP50 of 23.9) given the same detections. Mostof our models outperform the current state-of-the-art GTR,which is an offline method that utilizes future informationfor association. In contrast, our methods conduct trackingin an online fashion and test in a zero-shot setting. Ourunified Detic model again, achieves the new state-of-the-artby outperforming GTR by a large margin.BDD100K MOTS We use the same observations as thestate-of-the-art method, UNINEXT-H and perform zero-shot association test on BDD100K MOTS benchmark. Asshown in , our method achieves the best associationperformance (mIDF1 of 49.7 and AssocA of 54.5) amongall approaches. This demonstrates the superiority of theinstance embeddings learned by our method.BDD100K MOT As shown in , given the same ob-servations as ByteTrack , our method achieves the bestIDF1 of 71.7 and AssocA 52.9. Compared with state-of-the-art ByteTrack , our method also achieves better associ-ation performance, being about 1.4% higher on both IDF1and AssocA, without using any BDD images for training.ByteTrack additionally selects low-confidence boxes andadds them to the tracklets, resulting in a better mMOTAscore which prioritizes detection performance .UVO VIS We perform zero-shot tests for our unified seg-ment and track anything model based on SAM. We directlyuse the box prompts from the MASA detection head forfaster segmenting everything. As shown in a, ourmethod achieves the best performance on both image andvideo tracks, outperforming its counterparts by a large mar-gin. Besides, we also compare our method with SAMsdefault auto mask segmentation. As shown in b, asthe inference time increases, AR100 of our method growsmuch faster than SAM due to the distillate detection branch.The upper bound AR100 of our method with ViT-Base back-bone even surpasses SAM by 10%. Besides, when achievingthe same AR100, our method is about 10 faster than SAM.This stems from the fact that our method learns a strong ob-ject prior to capturing potential objects with a small numberof sparse proposals. However, to segment everything, SAMhas to sample about 1k points evenly, which is inflexibleand inefficient, while also relying on hand-crafted complexpost-processing methods.Compare with VOS Methods We evaluated the VOS-basedmethod Deva , which integrates XMem for track-ing multiple objects and SAM-PT , which uses point-tracking. To ensure a fair comparison, we provide the sameobservations on BDD MOTS, TAO TETA and UVO bench-marks. For UVO, we use SAMs auto-mask generation to",
  "Time": ". Qualitative results of unified proposal generation and association. The same colour indicates the same instance. We notice thatalthough we can learn strong associations using MASA, it is still very difficult to generate consistent proposals across frames. For example,we can see the missing segmentation of the building on the left in the second row. This indicates further research efforts are needed onconsistent proposal generation in videos.",
  "generate masks first, then we resolve the overlapping masksfollowing the heuristic in Deva and use Deva to generateper-frame observations": "shows that our method outperforms Deva acrossall benchmarks.Notably, on the autonomous drivingBDD100K benchmark, where objects frequently enter andexit the scene, VOS-based methods like Deva are prone toa significant increase in false positives. This is reflected inthe TETA scores, where such errors are heavily penalized.Additionally, Deva struggles with overlapping predictions, acommon issue with current detection models. We provide amore in-depth analysis in Section H of the Appendix. Compare with Self-supervised Methods We further com-pare our approach with self-supervised methods aimed atlearning universal appearance features from raw images orvideos. To ensure a fair comparison, we train all methodsusing a mix of BDD and COCO raw images. Specifically,for VFS, we utilize raw videos from BDD. We employ aResNet-50 model for VFS and MoCov2 , and a ViT-B model for DINO , following the association trackingstrategy outlined in UniTrack . Additionally, we ensurethat detection observations are identical across all models. demonstrates that our methods significantly outper-form other self-supervised approaches. This advantage stemsfrom the fact that traditional self-supervised learning primar-ily focuses on frame-level similarities, which limits theireffectiveness in leveraging instance information and causesstruggles when training with images containing multiple ob-jects. Further analysis of this is provided in Section G of theAppendix.",
  ". Ablation Study and Analysis": "To reduce the training costs, we bootstrap fewer raw images(40K) for training for the ablation experiments. Unless spec-ified we train the model with an image collection containing70k raw images from and 110k images from train-ing set respectively. We employ the Ours-SAM-B model andtest on BDD MOT and TAO TETA benchmarks.Effect of Training Strategies and Model Architectures illustrates that directly using the off-the-shelf SAMfeatures (row 1) for association yields poor results. The pri-mary reason is that SAMs original features are optimizedfor segmentation, not for instance-level discrimination. How-ever, integrating our MASA training approach and adding alightweight track head significantly enhances performance,yielding improvements of 15.6% in AssocA and 14.4% inmIDF1 on BDD MOT. This underscores the efficacy of ourtraining strategy. Incorporating a dynamic feature fusionblock further enhances performance by 1.6%. Additionally,joint training with the object prior distillation branch leads toan increase of 1.8% in AssocA and 1.6% in mIDF1, showingthe effect of these architectural designs.Effect of Proposal Diversity We evaluate different proposalgeneration mechanisms in association learning. We use onlyraw images from the training set of the BDD detection taskfor training. By substituting SAM in our MASA pipeline",
  ". Qualitative results of our unified models using Ours-Grounding-DINO (top) and Ours-SAM-H (bottom). We use SAM-H to generate masks given the detected boxes": "with Mask2former-SwinL , pre-trained on COCO. Asshown in a, we found that the model trained withSAMs proposals significantly enhanced both in-domain per-formance on BDD and zero-shot tracking on TAO. Thisunderscores the importance of SAMs dense, diverse objectproposals for superior contrastive similarity learning.Effect of Proposal Quantity Investigating the impact ofSAMs proposal quantity on learning, we experimented withdifferent upper bounds of 64, 128, and 256 proposals perbatch. b shows consistent improvements in AssocAon BDD and TAO with increasing proposal numbers, indicat-ing that a rich collection of instances fosters more discrimi-native tracking features.Effect of Data Augmentations As shown in c, thecombination of random affine, Mixup and LSJ gives the best performance. Method 1 represents basic dataaugmentation including flipping, resizing, color jitter andrandom cropping. If there is no strong augmentation (method1), its mIDF1 on BDD MOT drops by 6.7%, being muchworse than that with method 5. These results illustrate thenecessity of strong augmentations in training only on staticimages.Qualitative Results In , we present the qualita-tive results of our unified methods, Grounding-DINO andSAM-H. Our methods accurately detect, segment, and trackmultiple objects and even their parts across diverse domains.This includes animated movie scenes featuring many similar-looking characters and driving scenes within complex envi-ronments.",
  ". Conclusion": "We present MASA, a novel method that exploits the exten-sive instance-level shape and appearance information fromSAM to learn generalizable instance associations from un-labeled images. MASA demonstrates exceptional zero-shotassociation performance across various benchmarks, elimi-nating the need for expensive domain-specific labels. More-over, our universal MASA adapter can be added to any ex-isting detection and segmentation models, enabling them toefficiently track any objects across diverse domains.",
  "Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, andBen Upcroft. Simple online and realtime tracking. In ICIP,2016. 5": "Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-modal dataset for autonomous driving. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1162111631, 2020. 3 Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khi-rodkar, and Kris Kitani. Observation-centric sort: Rethink-ing sort for robust multi-object tracking. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 96869696, 2023. 5",
  "Fei Du, Bo Xu, Jiasheng Tang, Yuqi Zhang, Fan Wang, andHao Li. 1st place solution to eccv-tao-2020: Detect and repre-sent any object for tracking. arXiv preprint arXiv:2101.08040,2021. 5": "Yang Fu, Sifei Liu, Umar Iqbal, Shalini De Mello, HumphreyShi, and Jan Kautz. Learning to track instances without videoannotations. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 86808689,2021. 2 Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simplecopy-paste is a strong data augmentation method for instancesegmentation. In CVPR, 2021. 6, 8",
  "Tsung-Yi Lin, Piotr Dollr, Ross Girshick, Kaiming He,Bharath Hariharan, and Serge Belongie. Feature pyramidnetworks for object detection. In CVPR, 2017. 4": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, et al. Grounding dino: Marrying dino with groundedpre-training for open-set object detection. arXiv preprintarXiv:2303.05499, 2023. 1, 2, 4, 19 Yang Liu, Idil Esen Zulfikar, Jonathon Luiten, Achal Dave,Deva Ramanan, Bastian Leibe, Aljoa Oep, and Laura Leal-Taix. Opening up open world tracking. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1904519055, 2022. 2 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 4, 19",
  "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster RCNN: Towards real-time object detection with re-gion proposal networks. In NeurIPS, 2015. 4": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, and Michael Bernstein. ImageNet Large scalevisual recognition challenge. IJCV, 2015. 3 Mattia Segu, Bernt Schiele, and Fisher Yu. Darth: Holistictest-time adaptation for multiple object tracking. In Proceed-ings of the IEEE/CVF International Conference on ComputerVision, pages 97179727, 2023. 2 Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,Bharath Hariharan, Aleksander Holynski, and Noah Snavely.Tracking everything everywhere all at once. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision,pages 1979519806, 2023. 2",
  "Zongxin Yang and Yi Yang. Decoupling features in hierarchi-cal propagation for video object segmentation. In NeurIPS,2022. 2, 16": "Mingqiao Ye, Lei Ke, Siyuan Li, Yu-Wing Tai, Chi-KeungTang, Martin Danelljan, and Fisher Yu. Cascade-detr: Delvinginto high-quality universal object detection. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision,pages 67046714, 2023. 1 Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, YingyingChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-rell. BDD100K: A diverse driving dataset for heterogeneousmultitask learning. In CVPR, 2020. 2, 3, 6, 8, 13",
  "Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and PhilippKrhenbhl. Global tracking transformers. In CVPR, 2022.2, 5, 7": "Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-formable convnets v2: More deformable, better results. InProceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 93089316, 2019. 19 In this supplementary material, we provide additional ab-lation studies and qualitative results of our fast proposalgeneration and of our association. We also elaborate onour experimental setup, method details, and training andinference hyper-parameters.The supplementary material is structured as follows: Section A: Effectiveness on other backbones. Section B: Zero-shot evaluation on YoutubeVIS. Section C: Visualization of instance embeddings. Section D: Domain gap and adaptation. Section E: Impact of additional photometric augmentation. Section F: More detailed comparison of proposal diversity. Section G: Compare with self-supervised methods. Section H: Comparison with VOS-based methods. Section I: More qualitative results. Section J: More implementation details. Section K: Limitations.",
  "A. Effectiveness on Other Backbones": "In our main paper, we introduced four method variants, eachbuilding upon foundational detection and segmentation mod-els: SAM-ViT-B, SAM-ViT-H, Grounding-DINO, and Detic.Notably, the latter two variants leverage the Swin-B back-bone. Our MASA training pipeline and adapter have showngreat adaptability to a range of variables, including variationsin backbone structures, pre-training methods (such as detec-tion or segmentation), and the diverse datasets employed intraining these foundational models.A critical observation from our study is the reliance ofthese variants on large, complex backbones and their pre-training on extensive datasets. This reliance poses an im-portant question about scalability and efficiency: Can ourmethod sustain its effectiveness when applied to smaller,more streamlined backbones, like the ResNet-50, especiallywith standard ImageNet pre-training? To explore this, wedevised a new variant, \"Ours-R50,\" which integrates theResNet-50 backbone pre-trained on the ImageNet classifi-cation task (IN-Sup R50). This new variant maintains theMASA adapter architecture from our main research, adher-ing to the identical training protocol established by our initialfour variants.We have assessed the performance of Ours-R50 acrossvarious benchmarks, including BDD MOTS, BDD MOT,and TAO TETA. The quantitative results, detailed in Ta-bles 10, 11, and 12, demonstrate the efficacy of Ours-R50.These findings are significant as they suggest that our ap-proach can be effectively adapted to smaller backbones, of-fering the potential for more efficient and scalable solutions",
  "Ours-R5054.871.354.051.744.2": "in detection and segmentation tasks.BDD MOTS: For BDD MOTS (), Ours-R50,equipped with the ResNet-50 backbone and our MASA train-ing approach, not only outperforms the UNINEXT-H modelwith a +0.2 mIDF1 and +0.4 AssocA but also shows min-imal performance drop compared to the strongest variant,Ours-SAM-H (-1 mIDF1 and -0.9 AssocA). This highlightsour methods ability to yield competitive instance embed-dings, even without the advanced features provided by larger,specialized models.BDD MOT: In the BDD MOT benchmark (), Ours-R50 surpasses ByteTrack in terms of IDF1 (+0.9) and As-socA (+0.2) scores. Its performance is on par with our othervariants, showing only a slight decrease compared to Ours-Detic (-1 mIDF1 and -1.2 AssocA). These results reaffirmthe adaptability of our method across various backbone ar-chitectures and pre-training environments.TAO TETA: Evaluating on TAO TETA (), Ours-R50, with its standard ResNet-50 backbone, continues toperform robustly. It closely matches the fully supervisedTETer model, with only a slight decrease in AssocA (-1).This performance, consistent with our other variants, furthervalidates the generalizability of our MASA approach acrossdifferent backbones and pre-training methodologies.",
  "VisTR 36.2 36.9MaskProp 40.0 42.9IFC 42.8 46.8SeqFormer 47.4 51.8IDOL 49.5 52.9MFVIS 46.6 49.7VITA 49.8 54.5UNINEXT-R50 53.0 59.1Ours-SAM-B51.8 58.1": "Our method uses the same object detection observations asthe state-of-the-art VIS method UNINEXT-R50 . Asshown in , our method achieves comparable perfor-mance with SOTA UNINEXT trained with the in-domainYoutubeVIS data, while outperforming all other approachessignificantly. This outcome underlines the robust zero-shotassociation capabilities of our method, highlighting its effec-tiveness in scenarios without domain-specific training.",
  "C. Visualization of Instance Embeddings": "In , we use t-SNE to visualize instance embeddingslearned in different ways. We compare self-supervised ap-proaches such as MoCo-v2 , VFS , and DINO ,alongside two base models: SAM ViT-B , originallypre-trained on SA-1B for segmentation tasks, and IN-SupR50 , initially pre-trained on ImageNet for image clas-sification. Additionally, we present embeddings from fullysupervised in-domain video models and the same basemodels enhanced with our MASA adapters. In these visu-alizations, instances that share the same ground-truth IDare represented in the same colors. We use the BDD100Ksequence as the data source.Our observations indicate that the embeddings from theoriginal SAM, IN-Sup R50, as well as the self-supervisedmethods like MoCo, VFS, and DINO, do not consistentlyseparate different instances within certain complex scenarios,as highlighted by the instances marked in green, orange, andyellow. In contrast, by applying our MASA adapter to theoriginal SAM ViT-B and IN-Sup R50 features, the resulting adapted embeddings exhibit a successful delineation of dis-tinct instances. This performance is comparable to that offully supervised methods that have been trained on labeledin-domain videos. Significantly, our method achieves theseresults without any labeled in-domain video data, demon-strating its considerable potential for robust instance-levelcorrespondence learning.",
  "D. Domain Gap and Adaptation": "Except for previously mentioned applications, MASA canalso serve as a useful domain adaption method for instanceassociation. To be specific, due to the domain gaps such asobject categories, scenarios, and lighting conditions, trackerstrained on data of domain A may suffer from performancedrop when evaluating on domain B. For example, comparedwith BDD , TAO covers much more diverse scenar-ios and object categories. Thus, we choose BDD100K as the source domain and TAO as the target domain.Then we train two separate models with the same architec-ture as TETer using labeled data of BDD and LVIS+TAO respectively. These two models are repre-sented by the blue and green bars in . Please notethat when evaluating their associating ability on TAO, theyuse the same object detection observations. As shown in, directly applying embeddings trained on BDD toTAO (blue bar) leads to poor AssocA, which is 6.5% lowerthan the model trained on in-domain TAO (green bar). Toalleviate this performance gap, we fine-tune the track headof the original TETer model represented by the blue barwith the MASA training pipeline, while freezing all otherparameters (orange bar). Specifically, we only fine-tune themodel using unlabeled images of LVIS and TAO, while notusing any original TAO annotation. As shown in ,compared with the blue bar, the orange bar achieves an im-provement of 3.9% on AssocA, reducing the domain gap by60%. This demonstrates that MASA can effectively improvethe association performance in out-of-domain scenarios, onlyrequiring unlabeled images from the target domain.",
  "E. Impact of Photometric Augmentation": "In .3 of our main paper, we focused on variousgeometric augmentations, including random affine transfor-mations, and large-scale jittering. We also use MixUp toenhance the instance diversity and simulate the occlusioneffect. This section delves into the impact of additional pho-tometric augmentation. We specifically examine the effectsof motion blur, Gaussian noise, snow, fog, and brightnessadjustments. Photometric augmentations are characterizedby their ability to modify pixel values in an image. These al-terations often mimic changes in environmental factors suchas lighting and weather, impacting how scenes are capturedby cameras. Unlike geometric augmentations that change",
  "(e) SAM ViT-B(f) DINO ViT-B(g) Video Fully Supervised R50(h) SAM ViT-B + MASA": ". t-SNE visualization demonstrates the distinctiveness of instance embeddings across various methods on a selected BDD100ksequence. The embeddings generated by our method (indicated by MASA-enhanced models) exhibit greater inter-instance separation andtighter intra-instance clustering than other self-supervised methods (MoCo, VFS, DINO) and the original supervised methods (IN-Sup,SAM). This enhanced discrimination highlights the effectiveness of our adapted features for downstream tasks. TETer-BDD to TAOTETer-BDD+ MASA adaptationTETer-TAO",
  ". Domain adaptation for TETer with MASA": "the spatial arrangement of pixels through rotation, scaling, orcropping, photometric augmentations do not alter the struc-tural integrity of objects within an image. illustratesthese augmentations visually.We maintained the same training regimen as our abla-tion study in the main paper, and using SAM-ViT-B as thefoundational model for our experiments. presentsthe results, indicating that the inclusion of photometric aug-mentation yields only modest improvements. We observed amarginal increase of +0.1 mIDF1 and +0.2 AssocA on theBDD dataset and +0.1 AssocA on the TAO dataset. Conse-quently, these augmentations are not included as a defaultin our methodology to achieve a better balance betweenperformance improvement and the potential increase in com-putational complexity.",
  "G. Compare with Self-Supervised Methods": "The task of extracting meaningful information from purelyunlabeled images is notably challenging. UniTrack hasshowcased the potential of self-supervised trained represen-tations, such as MoCo and VFS , in generalizing tovarious tracking tasks across different domains. However, asdepicted in , current self-supervised methods pre-dominantly employ contrastive training with clean, object-centered images or videos. In particular, VFS trains on theKinetics dataset, while MoCo and DINO utilize ImageNet. However, these approaches primarily focus on frame-level similarities and fail to leverage instance informationeffectively. Consequently, they struggle to learn accurateinstance representations in complex domains with multipleinstances appearing together, demonstrating a notable weak-ness in extracting robust and generalized representations. Visualization of Object-Centered Training Data We visu-alize the training data of VFS , the Kinetics dataset, andcompare it with the driving videos from BDD100K in Fig-ure 11. Kinetics, being an action recognition dataset, ensuresthe presence of instances throughout its videos by focusingon contained actions. Centred entities in Kinetics videosusually remain consistent over time, making VFSs samplingstrategy suitable for Kinetics. In contrast, BDD100K drivingvideos present a more dynamic and unpredictable environ-ment. These videos frequently feature objects that enter andexit the frame, leading to a significant variation in the pres-ence of instances across different frames. This characteristicof BDD100K poses a challenge as two frames sampled fromthe same video may not share the same instances, highlight-ing a fundamental difference in the nature of training databetween the two datasets.Training with Different Data Sources For a fair compari-",
  "Train on BDD & COCOVFS29.235.019.130.730.1MoCov242.746.730.75145.3DINO23.116.812.920.222.2Ours-SAM-B51.954.935.853.749.1": "son, when comparing our method with other self-supervisedcounterparts in of the main paper, we train all meth-ods using the same raw training images (BDD and COCO),which are not object-centered and usually contain multipleinstances in complex environments. In this section, we alsopresent the tracking performance of those self-supervisedmethods using their original object-centered training data.As shown in the table below, the AssocA of MoCo trainedon images from BDD and COCO remains relatively stablecompared to its original version trained on ImageNet, withonly a slight drop on the BDD MOT dataset. However, forVFS, training on images with multiple instances leads to asignificant performance drop of 15.9 AssocA on BDD MOTand 12.7 AssocA on TAO, respectively. The reason is asfollows: VFS considers frames from the same video as pos-itive samples and frames from different videos as negativesamples. This strategy is reasonable for Kinetics but not forBDD, as demonstrated in . Specifically, centredentities in Kinetics videos usually do not change over time,but in BDD videos, objects frequently move in and out offrames. Two frames from the same BDD video may notcontain the same instances at all. Lastly, in DINOs trainingprocess, it forces representations of two augmented viewsfrom the same image to be similar without explicitly usingnegative samples. However, for images in BDD and COCO,two augmented views may contain many different instances,considering the complex scenes of these two datasets. Thistraining strategy may cause the learned embeddings to beless discriminative.Our approach, which leverages instance-level knowledgefrom the pre-trained SAM, moves beyond frame-level simi-larity to embrace a more nuanced instance-level similarity.The strong results obtained underscore the effectiveness ofour proposed methods in learning robust representations fortracking purposes.",
  "H. Comparison with VOS-based Methods": "The recent segmentation foundation model, SAM, hasdemonstrated exceptional ability in segmenting any object.However, simultaneously tracking all instances generated bySAM in videos remains a challenging task. Current meth- ods typically employ SAM as a mask generator for the firstframe of a video, then apply off-the-shelf video object seg-mentation (VOS) methods to propagate the initialized maskto subsequent frames . One notable method,Deva , utilizes XMem for mask propagation to trackmultiple instances simultaneously. However, these methodsencounter several key disadvantages.Inadequate Mask Propagation Quality: Trained on rela-tively small-scale video segmentation datasets, these meth-ods experience substantial domain gaps when tasked withtracking any object in any domain, resulting in inadequatemask propagation quality. Our main paper illustrates thatour method significantly outperforms Deva in zero-shottesting across various multiple object tracking benchmarks,especially in driving scenes, which are out-of-domain forboth Deva and our method. We further provide a qualitativecomparison in . Additional video comparisons canbe found in the provided video file. Testing Deva in thedriving domain, which differs significantly from its trainingdata, results in poor mask quality and accumulating errorsover time. Moreover, there is no effective mechanism tohandle the rapid entry and exit of objects in a scene, a com-mon occurrence in real-world applications like autonomousdriving. In contrast, our method exhibits stable performancein such scenarios.Difficulty in Managing Multiple Granularities of Pix-els: Furthermore, these methods are primarily developed forvideo object segmentation (VOS) tasks, which typically in-volve videos and annotations of single, rather than multiple,diverse objects. As a result, most VOS-based approaches aredesigned to track only one instance at a time. While recentadvancements like those in allow for the simultane-ous tracking of multiple instances, they often work on thepremise that each pixel is part of a single instance. Thisoverlooks complexities in pixel granularity, where a pixelmay be part of multiple instances depending on the level ofgranularitya common situation in the outputs of SAM, asdepicted in . This issue is further illustrated usingthe UVO dataset, which contains only coarse object-levelannotations, often omitting finer details of object parts.We apply SAM to generate mask predictions for eachframe in the UVO dataset for both methods. To track ob-jects segmented by SAM, a VOS-based method like Devahas to resolve overlaps by assigning each pixel to a uniqueinstance. For example, if a group of pixels belongs to a partof an object, it must decide whether to track the part or thewhole object. Assigning pixels to a part implies that thecorresponding object is partially excluded, as shown withthe cars in . Conversely, assigning pixels to theobject results in the removal of the part mask. We present thequantitative results of these scenarios on the UVO datasetin . Tracking parts leads to an incomplete represen-tation of object masks on UVO, thus affecting performance",
  "(#)(#)": ". Comparison of self-supervised representation learning methods for object association. (a) Traditional methods, such asSimCLR , MoCo , focus on learning representations by leveraging frame-level similarity. They utilize augmented views of entireimages to extract meaningful features. These methods often struggle with complex scenarios involving multiple objects. The reliance onframe-level similarity can be limiting in environments where object-centric learning is crucial. (b) Methods like VFS take a differentroute by extracting positive pairs from different frames within the same video. This approach aims to capture temporal consistency andobject dynamics. Similar to traditional methods, it also requires clean, object-centred video data. The complexity increases significantly inmulti-object environments, where distinguishing between different objects becomes challenging. (c) Our method innovatively combines dataaugmentation with SAMs mask generation technique. This synergy allows for learning dense instance-level correspondences fromunlabeled images. By focusing on dense correspondences at the instance level, it can effectively disentangle and learn from intricate objectinteractions and dynamics in complex environments.",
  "KineticsBDD100K": ". Comparison between Kinetics and BDD100K videos. Kinetics, as an action recognition dataset, ensures that actions are containedwithin selected videos, thus guaranteeing the presence of instances throughout the video. Centred entities in Kinetics videos usually do notchange over time. This makes VFSs sampling strategy reasonable for Kinetics. However, in BDD videos, objects get into and out of theframes frequently. Two frames sampled from the same video may not contain the same instances at all.",
  "I.1. Fast Proposal Generation": "In , we compare the segmentation quality of ourfast proposal generation with SAMs original everythingmode on raw images from COCO validation set. By default,we output 300 bounding boxes per image, and use a bound-ing box NMS with 0.5 threshold as the only post-processingduring inference. The results show that our fast proposalgeneration can achieve similar segmentation quality to theeverything mode of SAM, despite using much less time.",
  "Original ImageOriginal SAMs maskOur method can track areas with overlapping": ". Challenges of VOS-Based Methods with Multi-Granular Pixel Overlaps. This figure illustrates the complexity encountered whendealing with overlapping masks in SAMs output, where a single pixel may be associated with multiple instances at different granularities.Traditional VOS methods, operating under the assumption that each pixel belongs to only one instance, often resort to heuristics to resolvethese overlaps, as depicted in the second row. In contrast, our method effectively handles such overlapping masks, showcasing its adaptabilityin complex scenarios. . Performance Comparison on the UVO Dataset for track-ing objects and their parts. This table presents a detailed analysis oftracking performance using the UVO dataset. VOS-based methodslike Deva have to resolve overlaps by assigning each pixel to aunique instance. Tracking parts leads to an incomplete representa-tion of object masks on UVO, thus affecting performance negatively.In contrast, our method, capable of handling multiple granularities,tracks both entire objects and their parts without compromisingperformance on the UVO dataset.",
  "I.3. Joint Segment and Track Everything": "We provide qualitative results on our joint segmentation andtracking models. Since we learn proposal generation andassociation in a joint way, it makes our model capable ofsegmenting and associating anything in videos. shows the qualitative association performance using our self-generated proposals. We notice that although we can learnstrong associations using MASA, it is very difficult to gen-erate consistent proposals across frames. For example, we can see the missing segmentation for the building on theleft in the second row. Those inconsistent detections willlead to severe flickering effects when visualising the resultson videos. This indicates we still need further efforts onconsistent proposal generation for robust detecting objectsin videos.",
  "J.1. Architecture Detail": "MASA Adapter The MASA Adapter comprises two mainparts. The first part involves the construction of a featurepyramid and dynamic feature fusion. The second part is theFasterRCNN-based detection head for the object prior to dis-tillation and the track head for producing tracking features.The construction process for the feature pyramid varies de-pending on the backbone used. These variations are detailedin the respective sections for each model. The dynamic fea-ture fusion employs standard deformable convolution, asoutlined in , to aggregate information across spatial lo-cations and feature levels. Additionally, task-aware attentionand scale-aware attention from are utilized for SAM-based models. In total, three fusion blocks are establishedfor the feature fusion process.The FasterRCNN-based detection head includes a regionproposal network and a class-agnostic box regression head.The track head comprises four convolutional layers and onefully connected layer, used to generate instance embeddings.Ours-Detic We utilize the pre-trained Detic modelwith Swin-B as the backbone. The pre-trained modeladheres to the open-vocabulary object detection setup de-scribed in , where rare classes from LVIS are excludedfrom training. We freeze the Detic Swin-B backbone andemploy the standard FPN for constructing the feature pyra-mid. Specifically, we extract features from the 4th, 22nd,and 24th blocks of the Swin-B backbone. Subsequently, weintegrate the dynamic feature fusion atop the feature pyramidto learn tracking features through detection distillation andinstance contrastive learning.Ours-Grounding-DINOWeemploythepre-trainedGrounding-DINO model with Swin-B as the back-bone. The Swin-B backbone is frozen, and we use the stan-dard FPN to construct the feature pyramid. Apart from thediffering pre-training and window sizes for the Swin back-bone, all learnable components are identical to Ours-Detic.Ours-SAM-B This model is based on SAM, with all originalSAM components frozen. To obtain multi-level hierarchicalfeatures from the plain ViT backbone of SAM, we extractfeature maps from the outputs of the 3rd, 6th, 9th, and 12th",
  "J.2. More Training Details": "For SAM-based models, we turn off MixUp augmentation inthe last two epochs. After that, we finetune the track headsof the SAM-based models while freezing the other parts withall augmentations for 6 epochs.For training our model with any raw image collection,the following pipeline is utilized. Initially, the everythingmode of SAM is employed to generate training data on rawimages offline, using the SAM-ViTH model to ensure higherquality. We adhere to the default SAM settings, which in-volve using 32 sampling points along each side of an image.Additionally, an Intersection over Union (IoU) predictionthreshold of 0.88 is applied to filter out low-quality predic-tions. Subsequently, small disconnected regions and holes inmasks are removed. Bounding box Non-Maximum Suppres-sion (NMS) is also used to eliminate overlapping predictionswith a threshold of 0.7. In our ablation studies, this pipelineis applied to generate data on raw COCO and BDD100Kimages.",
  ": end for": "for track , capturing the features of the tracked object. Thes1(, r) employs an exponential function to compute thedot product of these embeddings, reflecting the degree ofsimilarity between the object candidate and the track. Thissimilarity score is normalized twice: firstly, across all objectcandidates r in the set P for a given track , and secondly,across all tracks in the set T for a given object candidater. This dual normalization ensures a balanced and compre-hensive assessment of similarity, facilitating accurate objectassociation in dynamic video sequences. s2(, r) computesthe cosine similarity. The final s(, r) score is the averagebetween s1(, r) and s2(, r).",
  "K. Limitations": "One key limitation of our approach is handling temporalinconsistencies in detection or segmentation results acrossvideo frames. This issue, common in open-world object de-tection and segmentation models like SAM, is evident whenan object detected in one frame is missed in the next, caus-ing flickering effects in video visualization, as seen in ourdemonstrations. While our MASA adapter excels in learningassociations, it cannot rectify foundational models detectionor segmentation errors. The challenge of generating consis-tent proposals across frames highlights an important areafor future research to enhance the robustness and stability ofobject detection in dynamic video environments.Another limitation is the lack of a long-term memory sys-tem, which is crucial for handling occlusions. We only learna universal instance appearance model, which can be used di-rectly by different detectors. However, the tracking strategyand memory management are still done by the bi-softmaxmatching and a queue to store each instances appearanceembeddings. This simple strategy is prone to failure in cases",
  "tt + 1t + 2t + 3t + 4": ". Open-Vocabulary Tracking. We condition our Grounding-DINO tracker on text prompts unseen during training and successfullytrack the corresponding objects in the videos. We use SAM to generate the mask from given the detected boxes. The mask color depicts theobjects identity. We choose random internet videos to test our algorithm on diverse real-world scenarios. Best viewed digitally."
}