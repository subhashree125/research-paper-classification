{
  "Abstract": "Multimodal sentiment analysis (MSA) aims to under-stand human sentiment through multimodal data.MostMSA efforts are based on the assumption of modality com-pleteness. However, in real-world applications, some prac-tical factors cause uncertain modality missingness, whichdrastically degrades the models performance. To this end,we propose a Correlation-decoupled Knowledge Distilla-tion (CorrKD) framework for the MSA task under uncer-tain missing modalities. Specifically, we present a sample-level contrastive distillation mechanism that transfers com-prehensive knowledge containing cross-sample correlationsto reconstruct missing semantics. Moreover, a category-guided prototype distillation mechanism is introduced tocapture cross-category correlations using category proto-types to align feature distributions and generate favorablejoint representations.Eventually, we design a response-disentangled consistency distillation strategy to optimizethe sentiment decision boundaries of the student networkthrough response disentanglement and mutual informa-tion maximization.Comprehensive experiments on threedatasets indicate that our framework can achieve favorableimprovements compared with several baselines.",
  "Positive": ". Traditional model outputs correct prediction when in-putting the sample with complete modalities, but incorrectly pre-dicts the sample with missing modalities. We define two missingmodality cases: (i) intra-modality missingness (i.e., the pink areas)and (ii) inter-modality missingness (i.e., the yellow area). understands and recognizes human emotions through mul-tiple modalities, including language, audio, and visual .Previous studies have shown that combining complemen-tary information among different modalities facilitates thegeneration of more valuable joint multimodal representa-tions . Under the deep learning paradigm , numerous studies assuming the avail-ability of all modalities during both training and inferencestages . Nevertheless, thisassumption often fails to align with real-world scenarios,where factors such as background noise, sensor constraints,and privacy concerns may lead to uncertain modality miss-ingness issues. Modality missingness can significantly im-pair the effectiveness of well-trained models based on com-plete modalities. For instance, as shown in , theentire visual modality is missing, and some frame-level fea-",
  "tures in the language and audio modalities are missing, lead-ing to an incorrect sentiment prediction.In recent years, many works [20, 21, 23, 24, 32, 45,": "46, 66] attempt to address the problem of missing modal-ities in MSA. As a typical example, MCTN guaran-tees the models robustness to the missing modality caseby learning a joint representation through cyclic transla-tion from the source modality to the target modality. How-ever, these methods suffer from the following limitations:(i) inadequate interactions based on individual samples lackthe mining of holistically structured semantics. (ii) Fail-ure to model cross-category correlations leads to loss ofsentiment-relevant information and confusing distributionsamong categories. (iii) Coarse supervision ignores the se-mantic and distributional alignment.To address the above issues, we present a Correlation-decoupled Knowledge Distillation (CorrKD) framework forthe MSA task under uncertain missing modalities. Thereare three core contributions in CorrKD based on the tai-lored components. Specifically, (i) the proposed sample-level contrastive distillation mechanism captures the holis-tic cross-sample correlations and transfers valuable super-vision signals via sample-level contrastive learning.(ii)Meanwhile, we design a category-guided prototype distilla-tion mechanism that leverages category prototypes to trans-fer intra- and inter-category feature variations, thus deliver-ing sentiment-relevant information and learning robust jointmultimodal representations.(iii) Furthermore, we intro-duce a response-disentangled consistency distillation strat-egy to optimize sentiment decision boundaries and encour-age distribution alignment by decoupling heterogeneous re-sponses and maximizing mutual information between ho-mogeneous sub-responses.Based on these components,CorrKD significantly improves MSA performance underuncertain missing-modality and complete-modality testingconditions on three multimodal benchmarks.",
  ". Multimodal Sentiment Analysis": "MSA aims to understand and analyze human sentimentutilizing multiple modalities.Mainstream MSA studies focus on designing complexfusion paradigms and interaction mechanisms to enhancethe performance of sentiment recognition.For instance,CubeMLP utilizes three independent multi-layer per-ceptron units for feature-mixing on three axes. However,these approaches based on complete modalities cannot bedeployed in real-world applications. Mainstream solutionsfor the missing modality problem can be summarized intwo categories: (i) generative methodsand (ii) joint learning methods . Recon-struction methods generate missing features and seman- tics in modalities based on available modalities. For ex-ample, TFR-Net leverages the feature reconstructionmodule to guide the extractor to reconstruct missing seman-tics. MVAE solves the modality missing problem bythe semi-supervised multi-view deep generative framework.Joint learning efforts refer to learning joint multimodal rep-resentations utilizing correlations among modalities. For in-stance, MMIN generates robust joint multimodal rep-resentations via cross-modality imagination.TATE presents a tag encoding module to guide the network to fo-cus on missing modalities. However, the aforementionedapproaches fail to account for the correlations among sam-ples and categories, leading to inadequate compensation forthe missing semantics in modalities. In contrast, we designeffective learning paradigms to adequately capture potentialinter-sample and inter-category correlations.",
  ". Knowledge Distillation": "Knowledge distillation utilizes additional supervisory in-formation from the pre-trained teachers network to assistin the training of the students network . Knowledgedistillation methods can be roughly categorized into twotypes, distillation from intermediate features and responses .Many studies [13, 18, 33, 40, 47] employ knowledge distillation for MSA taskswith missing modalities.The core concept of these ef-forts is to transfer dark knowledge from teacher networkstrained by complete modalities to student networks trainedby missing modalities. The teacher model typically pro-duces more valuable feature presentations than the studentmodel. For instance, utilizes the complete-modalityteacher network to implement supervision on the unimodalstudent network at both feature and response levels. Despitepromising outcomes, they are subject to several significantlimitations: (i) Knowledge transfer is limited to individualsamples, overlooking the exploitation of clear correlationsamong samples and among categories. (ii) Supervision onstudent networks is coarse-grained and inadequate, withoutconsidering the potential alignment of feature distributions.To this end, we propose a correlation-decoupled knowledgedistillation framework that facilitates the learning of robustjoint representations by refining and transferring the cross-sample, cross-category, and cross-target correlations.",
  ". Problem Formulation": "Given a multimodal video segment with three modalitiesas S = [XL, XA, XV ], where XL RTLdL, XA RTAdA, and XV RTV dV denote language, audio,and visual modalities, respectively. Tm() is the sequencelength and dm() is the embedding dimension, where m {L, A, V }. Meanwhile, the incomplete modality is denoted Teacher representations Student representations",
  "Student Network": ". The structure of our CorrKD, which consists of three core components: Sample-level Contrastive Distillation (SCD) mechanism,Category-guided Prototype Distillation (CPD) mechanism, and Response-disentangled Consistency Distillation (RCD) strategy. as Xm. We define two missing modality cases to simu-late the most natural and holistic challenges in real-worldscenarios: (i) intra-modality missingness, which indicatessome frame-level features in the modality sequences aremissing.(ii) inter-modality missingness, which denotessome modalities are entirely missing. Our goal is to rec-ognize the utterance-level sentiments by utilizing the multi-modal data with missing modalities.",
  ". Overall Framework": "illustrates the main workflow of CorrKD. Theteacher network and the student network adopt a consistentstructure but have different parameters. During the train-ing phase, our CorrKD procedure is as follows: (i) we trainthe teacher network with complete-modality samples andthen freeze its parameters. (ii) Given a video segment sam-ple S, we generate a missing-modality sample S with theModality Random Missing (MRM) strategy. MRM simulta-neously performs intra-modality missing and inter-modalitymissing, and the raw features of the missing portions are re-placed with zero vectors. S and S are fed into the initializedstudent network and the trained teacher network, respec-tively. (iii) We input the samples S and S into the modalityrepresentation fusion module to obtain the joint multimodalrepresentations Ht and Hs. (iv) The sample-level con-trastive distillation mechanism and the category-guided pro-totype distillation mechanism are utilized to learn the fea-ture consistency of Ht and Hs. (v) These representationsare fed into the task-specific fully-connected layers and thesoftmax function to obtain the network responses Rt andRs. (vi) The response-disentangled consistency distillation strategy is applied to maintain consistency in the responsedistribution, and then Rs is used to perform classification.In the inference phase, testing samples are only fed into thestudent network for downstream tasks. Subsequent sectionsprovide details of the proposed components.",
  ". Modality Representation Fusion": "We introduce the extraction and fusion processes of modal-ity representations using the student network as an ex-ample.The incomplete modality Xsm RTmdm withm {L, A, V } is fed into the student network. Firstly,Xsm passes through a 1D temporal convolutional layer withkernel size 3 3 and adds the positional embedding to obtain the preliminary representations, denoted as F sm =W33( Xsm) + PE(Tm, d) RTmd. Each F sm is fed intoa Transformer encoder Fs(), capturing the modal-ity dynamics of each sequence through the self-attentionmechanism to yield representations Esm, denoted as Esm =Fs(F sm). The representations Esm are concatenated to ob-tain Zs, expressed as Zs = [EsL, EsA, EsV ] RTm3d.Subsequently, Zs is fed into the Global Average Pooling(GAP) to further enhance and refine the features, yieldingthe joint multimodal representation Hs R3d. Similarly,the joint multimodal representation generated by the teachernetwork is represented as Ht R3d.",
  ". Sample-level Contrastive Distillation": "Most previous studies of MSA tasks with missing modal-ities are sub-optimal, exploiting only one-sided information within a single sample and neglectingto consider comprehensive knowledge across samples. To this end, we propose a Sample-level Contrastive Distilla-tion (SCD) mechanism that enriches holistic knowledgeencoding by implementing contrastive learning betweensample-level representations of student and teacher net-works. This paradigm prompts models to sufficiently cap-ture intra-sample dynamics and inter-sample correlations togenerate and transfer valuable supervision signals, thus pre-cisely recovering the missing semantics. The rationale ofSCD is to take contrastive learning within all mini-batches,constraining the representations in two networks originatingfrom the same sample to be similar, and the representationsoriginating from different samples to be distinct.Specifically, given a mini-batch with N samples B ={S0, S1, , SN}, we obtain their sets of joint multimodalrepresentations in teacher and student networks, denoted as{Hw1 , Hw2 , , HwN} with w {t, s}. For the same inputsample, we narrow the distance between the joint represen-tations of the teacher and student networks and enlarge thedistance between the representations for different samples.The contrastive distillation loss is formulated as follows:",
  "j=1,j=iD(Hsi , Hti )2+max{0, D(Hsi , Htj)}2,": "(1)where D(Hs, Ht) = Hs Ht2 , 2 represents 2norm function, and is the predefined distance boundary.When negative pairs are distant enough (i.e., greater thanboundary ), the loss is set to 0, allowing the model to focuson other pairs. Since the sample-level representation con-tains holistic emotion-related semantics, such a contrastiveobjective facilitates the student network to learn more valu-able knowledge from the teacher network.",
  ". Category-guided Prototype Distillation": "MSA data usually suffers from the dilemmas of high intra-category diversity and high inter-category similarity. Pre-vious approaches based on knowledge distilla-tion to address the modality missing problem simply con-strain the feature consistency of the teacher and studentnetworks. The rough manner lacks consideration of cross-category correlation and feature variations, leading to am-biguous feature distributions. To this end, we propose aCategory-guided Prototype Distillation (CPD) mechanism,with the core insight of refining and transferring knowledgeof intra- and inter-category feature variations via categoryprototypes, which is widely utilized in the field of few-shotlearning . The category prototype represents the em-bedding center of every sentiment category, denoted as:",
  "Hi2 ck2,(3)": "where Mk(i) denotes the similarity between the sampleSi and the prototype ck. If the sample Si is of categoryk, Mk(i) represents intra-category feature variation. Oth-erwise, it represents inter-category feature variation. Theteacher and student networks compute similarity matricesM t and M s, respectively. We minimize the squared Eu-clidean distance between the two similarity matrices tomaintain the consistency of two multimodal representa-tions. The prototype distillation loss is formulated as:",
  ". Response-disentangled Consistency Distillation": "Most knowledge distillation studies fo-cus on extracting knowledge from intermediate features ofnetworks.Although the models response (i.e., the pre-dicted probability of the models output) presents a higherlevel of semantics than the intermediate features, response-based methods achieve significantly worse performancethan feature-based methods .Inspired by , themodels response consists of two parts: (i) Target Cate-gory Response (TCR), which represents the prediction ofthe target category and describes the difficulty of identifyingeach training sample. (ii) Non-Target Category Response(NTCR), which denotes the prediction of the non-target cat-egory and reflects the decision boundaries of the remainingcategories to some extent. The effects of TCR and NTCRin traditional knowledge distillation loss are coupled, i.e.,high-confidence TCR leads to low-impact NTCR, thus in-hibiting effective knowledge transfer.Consequently, wedisentangle the heterogeneous responses and constrain theconsistency between the homogeneous responses. From theperspective of information theory, knowledge consistencybetween responses can be characterized as maintaining highmutual information between teacher and student networks. This schema captures beneficial semantics and encour-ages distributional alignment.Specifically, the joint multimodal representation Hw with w {t, s} of teacher and student networks passthrough fully-connected layers and softmax function to ob-tain response Rw. Based on the target indexes, we decou-ple the response Rw to obtain TCR RwT and NTCR RwNT .Define Q Q and U U as two random variables. For-mulaically, the marginal probability density functors of Qand U are denoted as P(Q) and P(U). P(Q, U) is re-garded as the joint probability density functor. The mutual",
  "dQdU. (5)": "The mutual information I(Q, U) can be written as theKullback-Leibler divergence between the joint probabilitydistribution PQU and the product of the marginal distribu-tions PQPU, denoted as I(Q, U) = DKL (PQUPQPU) .For efficient and stable computation, the Jensen-Shannondivergence is employed in our case to estimate the mu-tual information, which is denoted as follows:",
  ". Datasets and Evaluation Metrics": "We conduct extensive experiments on three MSA datasetswith word-aligned data, including MOSI , MOSEI ,and IEMOCAP . MOSI is a realistic dataset that com-prises 2,199 short monologue video clips. There are 1,284,229, and 686 video clips in train, valid, and test data, re-spectively. MOSEI is a dataset consisting of 22,856 videoclips, which has 16,326, 1,871, and 4,659 samples in train,valid, and test data. Each sample of MOSI and MOSEI islabeled by human annotators with a sentiment score of -3(strongly negative) to +3 (strongly positive). On the MOSIand MOSEI datasets, we utilize weighted F1 score com-puted for positive/negative classification results as evalua-tion metrics. IEMOCAP dataset consists of 4,453 samplesof video clips. Its predetermined data partition has 2,717,798, and 938 samples in train, valid, and test data. As rec-ommended by , four emotions (i.e., happy, sad, angry,and neutral) are selected for emotion recognition. For eval-uation, we report the F1 score for each category.",
  ". Implementation Details": "Feature Extraction.The Glove embedding is usedto convert the video transcripts to obtain a 300-dimensionalvector for the language modality.For the audio modal-ity, we employ the COVAREP toolkit to extract 74-dimensional acoustic features, including 12 Mel-frequencycepstral coefficients (MFCCs), voiced/unvoiced segment-ing features, and glottal source parameters. For the visualmodality, we utilize the Facet to indicate 35 facial ac-tion units, recording facial movement to express emotions.Experimental Setup.All models are built on the Py-torch toolbox with NVIDIA Tesla V100 GPUs. TheAdam optimizer is employed for network optimiza-tion.For MOSI, MOSEI, and IEMOCAP, the detailedhyper-parameter settings are as follows:the learningrates are {4e 3, 2e 3, 4e 3}, the batch sizes are{64, 32, 64}, the epoch numbers are {50, 20, 30}, the at-tention heads are {10, 8, 10}, and the distance boundaries are {1.2, 1.0, 1.4}. The embedding dimension is 40 onall three datasets. The hyper-parameters are determined via",
  "MOSEI": "Self-MM 71.5343.5737.6175.9174.6249.5258.7983.69CubeMLP 67.5239.5432.5871.6970.0648.5454.9983.17DMD 70.2646.1839.8474.7872.4552.7059.3784.78MCTN 75.5062.7259.4676.6477.1364.8469.3881.75TransM 77.9863.6858.6780.4678.6162.2470.2781.48SMIL 76.5765.9660.5777.6876.2466.8770.6580.74GCNet 80.5266.5461.8381.9681.1569.2173.5482.35CorrKD80.7666.0962.3081.7481.2871.9274.0282.16 the validation set. The raw features at the modality miss-ing positions are replaced by zero vectors. To ensure anequitable comparison, we re-implement the state-of-the-art(SOTA) methods using the publicly available codebases andcombine them with our experimental paradigms. All experi-mental results are averaged over multiple experiments usingfive different random seeds.",
  ". Comparison with State-of-the-art Methods": "We compare CorrKD with seven representative and repro-ducible SOTA methods, including complete-modality meth-ods: Self-MM , CubeMLP , and DMD , andmissing-modality methods: 1) joint learning methods (i.e.,MCTN and TransM ), and 2) generative methods(i.e., SMIL and GCNet ). Extensive experimentsare implemented to thoroughly evaluate the robustness andeffectiveness of CorrKD in the cases of intra-modality andinter-modality missingness.Robustness to Intra-modality Missingness. We randomlydrop frame-level features in modality sequences with ratiop {0.1, 0.2, , 1.0} to simulate testing conditions ofintra-modality missingness. Figures 3 and 4 show the per-formance curves of models with various p values, whichintuitively reflect the models robustness.We have thefollowing important observations.(i) As the ratio p in-creases, the performance of all models decreases.Thisphenomenon demonstrates that intra-modality missingnessleads to a considerable loss of sentiment semantics andfragile joint multimodal representations. (ii) Compared tothe complete-modality methods (i.e., Self-MM, CubeMLP, and DMD), our CorrKD achieves significant performanceadvantages in the missing-modality testing conditions andcompetitive performance in the complete-modality testingconditions. The reason is that complete-modality methodsare based on the assumption of data completeness, whereascustomized training paradigms for missing modalities per-form better at capturing and reconstructing valuable sen-timent semantics from incomplete multimodal data. (iii)Compared to the missing-modality methods, our CorrKDexhibits the strongest robustness. Benefiting from the de-coupling and modeling of inter-sample, inter-category, andinter-response correlations by the proposed correlation de-coupling schema, the student network acquires informativeknowledge to reconstruct valuable missing semantics andproduces robust multimodal representations. Robustness to Inter-modality Missingness. In and2, we drop some entire modalities in the samples to simu-late testing conditions of inter-modality missingness. Thenotation {l} indicates that only the language modalityis available, while audio and visual modalities are miss-ing.{l, a, v} represents the complete-modality testingcondition where all modalities are available. Avg. indi-cates the average performance across six missing-modalitytesting conditions. We present the following significant in-sights.(i) Inter-modality missingness causes performancedegradation for all models, suggesting that the integrationof complementary information from heterogeneous modali-ties enhances the sentiment semantics within joint represen-tations. (ii) In the testing conditions of the inter-modalitymissingness, our CorrKD has superior performance among",
  "(c) GCNet(a) Self-MM(b) MCTN(d) CorrKD": ". Visualization of representations from different methods with four emotion categories on the IEMOCAP testing set. The defaulttesting conditions contain intra-modality missingness (i.e., missing ratio p = 0.5 ) and inter-modality missingness (i.e., only the languagemodality is available). The red, orange, green, and blue markers represent the happy, angry, neutral, and sad emotions, respectively. case. In the bimodal testing conditions, cases containingthe language modality perform the best, even surpassingthe complete-modality case in individual metrics. This phe-nomenon proves that language modality encompasses therichest knowledge information and dominates the sentimentinference and missing semantic reconstruction.",
  ". Ablation Studies": "To validate the effectiveness and necessity of the proposedmechanisms and strategies in CorrKD, we conduct abla-tion studies under two missing-modality cases on the MOSIdataset, as shown in and . The principalfindings are outlined as follows. (i) When SCD is elim-inated, there is a noticeable degradation in model perfor-mance under both missing cases. This phenomenon sug-gests that mining and transferring comprehensive cross-sample correlations is essential for recovering missing se-mantics in student networks.(ii) The worse results un-der the two missing modality scenarios without CPD in-dicate that capturing cross-category feature variations andcorrelations facilitates deep alignment of feature distribu-tions between both networks to produce robust joint mul-timodal representations. (iii) Moreover, we substitute theKL divergence loss for the proposed RCD. The decliningperformance gains imply that decoupling heterogeneous re-sponses and maximizing mutual information between ho-mogeneous responses motivate the student network to ade-quately reconstruct meaningful sentiment semantics.",
  ". Qualitative Analysis": "To intuitively show the robustness of the proposed frame-work against modality missingness, we randomly choose100 samples from each emotion category on the IEMO-CAP testing set for visualization analysis. The compari-son models include Self-MM (i.e., complete-modalitymethod), MCTN (i.e., joint learning-based missing-modality method), and GCNet (i.e., generative-basedmissing-modality method). (i) As shown in , Self- MM cannot address the modality missing challenge, as therepresentations of different emotion categories are heavilyconfounded, leading to the least favorable outcomes. (ii)Although MCTN and GCNet somewhat alleviate the issueof indistinct emotion semantics, their effectiveness remainslimited since the distribution boundaries of the differentemotion representations are generally ambiguous and cou-pled. (iii) Conversely, our CorrKD ensures that representa-tions of the same emotion category form compact clusters,while representations of different categories are clearly sep-arated. These observations confirm the robustness and supe-riority of our framework, as it sufficiently decouples inter-sample, inter-category and inter-response correlations.",
  ". Conclusions": "In this paper, we present a correlation-decoupled knowl-edge distillation framework (CorrKD) to address diversemissing modality dilemmas in the MSA task.Con-cretely, we propose a sample-level contrast distillationmechanism that utilizes contrastive learning to capture andtransfer cross-sample correlations to precisely reconstructmissing semantics.Additionally, we present a category-guided prototype distillation mechanism that learns cross-category correlations through category prototypes, refiningsentiment-relevant semantics for improved joint representa-tions. Eventually, a response-disentangled consistency dis-tillation is proposed to encourage distribution alignment be-tween teacher and student networks. Extensive experimentsconfirm the effectiveness of our framework.",
  "Acknowledgements": "This work is supported in part by the Shanghai Munici-pal Science and Technology Committee of Shanghai Out-standing Academic Leaders Plan (No. 21XD1430300), andin part by the National Key R&D Program of China(No. 2021ZD0113503). Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil DLawrence, and Zhenwen Dai. Variational information dis-tillation for knowledge transfer.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 91639171, 2019. 4 CarlosBusso,MurtazaBulut,Chi-ChunLee,AbeKazemzadeh, Emily Mower, Samuel Kim, Jeannette NChang, Sungbok Lee, and Shrikanth S Narayanan. Iemo-cap: Interactive emotional dyadic motion capture database.Language Resources and Evaluation, 42:335359, 2008. 5",
  "Jang Hyun Cho and Bharath Hariharan. On the efficacy ofknowledge distillation. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages47944802, 2019. 2": "Gilles Degottex, John Kane, Thomas Drugman, TuomoRaitio, and Stefan Scherer. Covarepa collaborative voiceanalysis repository for speech technologies. In IEEE Inter-national Conference on Acoustics, Speech and Signal Pro-cessing (ICASSP), pages 960964. IEEE, 2014. 5 Changde Du, Changying Du, Hao Wang, Jinpeng Li, Wei-Long Zheng, Bao-Liang Lu, and Huiguang He.Semi-supervised deep generative modelling of incomplete multi-modality emotional data. In Proceedings of the 26th ACMinternational conference on Multimedia (ACM MM), pages108116, 2018. 2 Yangtao Du, Dingkang Yang, Peng Zhai, Mingchen Li, andLihua Zhang. Learning associative representation for facialexpression recognition. In IEEE International Conferenceon Image Processing (ICIP), pages 889893, 2021. 1 Tommaso Furlanello, Zachary Lipton, Michael Tschannen,Laurent Itti, and Anima Anandkumar.Born again neuralnetworks. In International Conference on Machine Learn-ing (ICML), pages 16071616. PMLR, 2018. 2",
  "Minhao Hu, Matthis Maillard, Ya Zhang, Tommaso Ciceri,Giammarco La Barbera, Isabelle Bloch, and Pietro Gori": "Knowledge distillation from multi-modal to mono-modalsegmentation networks. In Medical Image Computing andComputer Assisted InterventionMICCAI 2020: 23rd Inter-national Conference, Lima, Peru, October 48, 2020, Pro-ceedings, Part I 23, pages 772781. Springer, 2020. 2, 4 iMotions. Facial expression analysis. 2017. 5 Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphras-ing complex network: Network compression via factor trans-fer.Advances in Neural Information Processing Systems(NeurIPS), 31, 2018. 2, 4",
  "Mingcheng Li, Dingkang Yang, and Lihua Zhang. Towardsrobust multimodal sentiment analysis under uncertain signalmissing. IEEE Signal Processing Letters, 2023. 2": "Mingcheng Li, Dingkang Yang, Yuxuan Lei, Shunli Wang,Shuaibing Wang, Liuzhen Su, Kun Yang, Yuzheng Wang,Mingyang Sun, and Lihua Zhang. A unified self-distillationframework for multimodal sentiment analysis with uncertainmissing modalities. In Proceedings of the AAAI Conferenceon Artificial Intelligence (AAAI), pages 1007410082, 2024.2 Yong Li, Yuanzhi Wang, and Zhen Cui. Decoupled multi-modal distilling for emotion recognition. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 66316640, 2023. 1, 2, 6, 7 Zheng Lian, Lan Chen, Licai Sun, Bin Liu, and JianhuaTao. Gcnet: graph completion network for incomplete mul-timodal learning in conversation. IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 2023. 2, 6, 7, 8",
  "ference on Artificial Intelligence (AAAI), pages 23022310,2021. 6, 7": "Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, NirLevine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Im-proved knowledge distillation via teacher assistant. In Pro-ceedings of the AAAI Conference on Artificial Intelligence(AAAI), pages 51915198, 2020. 2 Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi.Towards multimodal sentiment analysis: Harvesting opin-ions from the web. In Proceedings of the 13th InternationalConference on Multimodal Interfaces, pages 169176, 2011.1",
  "Adam Paszke, Sam Gross, Soumith Chintala, GregoryChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-ban Desmaison, Luca Antiga, and Adam Lerer. Automaticdifferentiation in pytorch. 2017. 5": "Jeffrey Pennington, Richard Socher, and Christopher D Man-ning. Glove: Global vectors for word representation. In Pro-ceedings of the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 15321543, 2014. 5 Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-PhilippeMorency, and Barnabas Poczos. Found in translation: Learn-ing robust joint representations by cyclic translations be-tween modalities. In Proceedings of the AAAI Conferenceon Artificial Intelligence (AAAI), pages 68926899, 2019. 2,6, 7, 8 Masoomeh Rahimpour, Jeroen Bertels, Ahmed Radwan,Henri Vandermeulen, Stefan Sunaert, Dirk Vandermeulen,Frederik Maes, Karolien Goffin, and Michel Koole. Cross-modal distillation to improve mri-based brain tumor segmen-tation with missing mri sequences. IEEE Transactions onBiomedical Engineering, 69(7):21532164, 2021. 2, 3, 4 Roee Shraga, Haggai Roitman, Guy Feigenblat, and MustafaCannim. Web table retrieval using multimodal deep learning.In Proceedings of the 43rd International ACM SIGIR Confer-ence on Research and Development in Information Retrieval,pages 13991408, 2020. 1",
  "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypicalnetworks for few-shot learning. Advances in Neural Infor-mation Processing Systems (NeurIPS), 30, 2017. 4": "Matthias Springstein, Eric Muller-Budack, and Ralph Ew-erth. Quti! quantifying text-image consistency in multimodaldocuments. In Proceedings of the 44th International ACMSIGIR Conference on Research and Development in Infor-mation Retrieval, pages 25752579, 2021. 1 Hao Sun, Hongyi Wang, Jiaqing Liu, Yen-Wei Chen, andLanfen Lin. Cubemlp: An mlp-based model for multimodalsentiment analysis and depression estimation. In Proceed-ings of the 30th ACM International Conference on Multime-dia (ACM MM), pages 37223729, 2022. 2, 6, 7",
  "YonglongTian,DilipKrishnan,andPhillipIsola.Contrastive representation distillation.arXiv preprintarXiv:1910.10699, 2019. 2, 4": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in NeuralInformation Processing Systems (NeurIPS), 30, 2017. 3 Hu Wang, Congbo Ma, Jianpeng Zhang, Yuan Zhang, JodieAvery, Louise Hull, and Gustavo Carneiro. Learnable cross-modal knowledge distillation for multi-modal learning withmissing modality.In International Conference on Medi-cal Image Computing and Computer-Assisted Intervention,pages 216226. Springer, 2023. 2, 3 Lin Wang and Kuk-Jin Yoon. Knowledge distillation andstudent-teacher learning for visual intelligence: A reviewand new outlooks. IEEE Transactions on Pattern Analysisand Machine Intelligence, 44(6):30483068, 2021. 4 Shunli Wang, Dingkang Yang, Peng Zhai, Chixiao Chen, andLihua Zhang. Tsa-net: Tube self-attention network for ac-tion quality assessment. In Proceedings of the 29th ACMInternational Conference on Multimedia (ACM MM), pages49024910, 2021. 1",
  "Shunli Wang, Dingkang Yang, Peng Zhai, and Lihua Zhang.Cpr-clip: Multimodal pre-training for composite error recog-nition in cpr training. IEEE Signal Processing Letters, 2023.1": "Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, AmirZadeh, and Louis-Philippe Morency. Words can shift: Dy-namically adjusting word representations using nonverbalbehaviors. In Proceedings of the AAAI Conference on Ar-tificial Intelligence (AAAI), pages 72167223, 2019. 5 Yuanzhi Wang, Zhen Cui, and Yong Li.Distribution-consistent modal recovering for incomplete multimodallearning.In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (CVPR), pages 2202522034, 2023. 2 Zilong Wang, Zhaohong Wan, and Xiaojun Wan.Trans-modality: An end2end fusion method with transformer formultimodal sentiment analysis. In Proceedings of The WebConference, pages 25142520, 2020. 2, 6, 7",
  "Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong,Dejing Dou, and Di Hu.Robust cross-modal knowl-edge distillation for unconstrained videos.arXiv preprintarXiv:2304.07775, 2023. 2, 3": "Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snap-shot distillation: Teacher-student optimization in one gener-ation. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 28592868, 2019. 2 Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du,and Lihua Zhang. Disentangled representation learning formultimodal emotion recognition. In Proceedings of the 30thACM International Conference on Multimedia (ACM MM),pages 16421651, 2022. 1",
  "VisionECCV 2022: 17th European Conference, Tel Aviv, Is-rael, October 2327, 2022, Proceedings, Part XXXVII, pages144162. Springer, 2022": "Dingkang Yang, Haopeng Kuang, Shuai Huang, and LihuaZhang. Learning modality-specific and-agnostic representa-tions for asynchronous multimodal language sequences. InProceedings of the 30th ACM International Conference onMultimedia (ACM MM), pages 17081717, 2022. Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, ShunliWang, Mingcheng Li, Siao Liu, Xiao Zhao, Shuai Huang,Zhiyan Dong, Peng Zhai, and Lihua Zhang.Context de-confounded emotion recognition.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1900519015, 2023. 1, 2 Dingkang Yang, Shuai Huang, Zhi Xu, Zhenpeng Li, ShunliWang, Mingcheng Li, Yuzheng Wang, Yang Liu, Kun Yang,Zhaoyu Chen, Yan Wang, Jing Liu, Peixuan Zhang, PengZhai, and Lihua Zhang. Aide: A vision-driven multi-view,multi-modal, multi-tasking dataset for assistive driving per-ception. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision (ICCV), pages 2045920470,2023. 1 Dingkang Yang, Yang Liu, Can Huang, Mingcheng Li, XiaoZhao, Yuzheng Wang, Kun Yang, Yan Wang, Peng Zhai, andLihua Zhang. Target and source modality co-reinforcementfor emotion understanding from asynchronous multimodalsequences. Knowledge-Based Systems, 265:110370, 2023.1, 2 Dingkang Yang, Mingcheng Li, Dongling Xiao, Yang Liu,Kun Yang, Zhaoyu Chen, Yuzheng Wang, Peng Zhai, KeLi, and Lihua Zhang.Towards multimodal sentimentanalysis debiasing via bias purification.arXiv preprintarXiv:2403.05023, 2024. Dingkang Yang, Dongling Xiao, Ke Li, Yuzheng Wang,Zhaoyu Chen, Jinjie Wei, and Lihua Zhang. Towards multi-modal human intention understanding debiasing via subject-deconfounding. arXiv preprint arXiv:2403.05025, 2024.",
  "Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang,Shuaibing Wang, and Lihua Zhang. Robust emotion recog-nition in context debiasing. In CVPR, 2024. 1, 2": "Kun Yang, Dingkang Yang, Jingyu Zhang, Mingcheng Li,Yang Liu, Jing Liu, Hanqi Wang, Peng Sun, and Liang Song.Spatio-temporal domain awareness for multi-agent collab-orative perception.In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages2338323392, 2023. 1 Kun Yang, Dingkang Yang, Jingyu Zhang, Hanqi Wang,Peng Sun, and Liang Song.What2comm:Towardscommunication-efficient collaborative perception via featuredecoupling. In Proceedings of the 31th ACM InternationalConference on Multimedia (ACM MM), page 76867695,2023. 1 Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim.A gift from knowledge distillation: Fast optimization, net-work minimization and transfer learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 41334141, 2017. 2, 4 Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learningmodality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis.In Pro-ceedings of the AAAI Conference on Artificial Intelligence(AAAI), pages 1079010797, 2021. 1, 6, 7, 8 Ziqi Yuan, Wei Li, Hua Xu, and Wenmeng Yu. Transformer-based feature reconstruction network for robust multimodalsentiment analysis. In Proceedings of the 29th ACM Interna-tional Conference on Multimedia (ACM MM), pages 44004407, 2021. 2 Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-PhilippeMorency. Mosi: multimodal corpus of sentiment intensityand subjectivity analysis in online opinion videos.arXivpreprint arXiv:1606.06259, 2016. 5 AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, ErikCambria, and Louis-Philippe Morency.Multimodal lan-guage analysis in the wild: Cmu-mosei dataset and inter-pretable dynamic fusion graph. In Proceedings of the 56thAnnual Meeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 22362246, 2018.5 Jiandian Zeng, Tianyi Liu, and Jiantao Zhou. Tag-assistedmultimodal sentiment analysis under uncertain missingmodalities. In Proceedings of the 45th International ACMSIGIR Conference on Research and Development in Infor-mation Retrieval, pages 15451554, 2022. 2 Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and JiajunLiang. Decoupled knowledge distillation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1195311962, 2022. 4 Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, XiaogangWang, and Jiaya Jia. Pyramid scene parsing network. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 28812890, 2017. 2 Jinming Zhao, Ruichen Li, and Qin Jin.Missing modal-ity imagination network for emotion recognition with un-certain missing modalities. In Proceedings of the 59th An-nual Meeting of the Association for Computational Linguis-tics and the 11th International Joint Conference on NaturalLanguage Processing (Volume 1: Long Papers), pages 26082618, 2021. 2"
}