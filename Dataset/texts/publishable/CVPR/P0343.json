{
  "Abstract": "Building accurate maps is a key building block to en-able reliable localization, planning, and navigation of au-tonomous vehicles. We propose a novel approach for build-ing accurate maps of dynamic environments utilizing a se-quence of LiDAR scans. To this end, we propose encodingthe 4D scene into a novel spatio-temporal implicit neuralmap representation by fitting a time-dependent truncatedsigned distance function to each point. Using our repre-sentation, we extract the static map by filtering the dynamicparts. Our neural representation is based on sparse featuregrids, a globally shared decoder, and time-dependent ba-sis functions, which we jointly optimize in an unsupervisedfashion. To learn this representation from a sequence of Li-DAR scans, we design a simple yet efficient loss functionto supervise the map optimization in a piecewise way. Weevaluate our approach 1 on various scenes containing mov-ing objects in terms of the reconstruction quality of staticmaps and the segmentation of dynamic point clouds. Theexperimental results demonstrate that our method is capa-ble of removing the dynamic part of the input point cloudswhile reconstructing accurate and complete 3D maps, out-performing several state-of-the-art methods.",
  ". Introduction": "Mapping using range sensors, like LiDAR or RGB-D cam-eras, is a fundamental task in computer vision and robotics.Often, we want to obtain accurate maps to support down-stream tasks such as localization, planning, or navigation.For achieving an accurate reconstruction of an outdoor en-vironment, we have to account for dynamics caused bymoving objects, such as vehicles or pedestrians. Further-more, dynamic object removal plays an important role in au-tonomous driving and robotics applications for creating dig-ital twins for realistic simulation and high-definition map-ping, where a static map is augmented with semantic andtask-relevant information.",
  "(d)": ". Given a sequence of point clouds, as shown in (a), weoptimize our 4D neural representation that can be queried at ar-bitrary positions for a specific time. (b) Based on the estimatedtime-dependent TSDF values, we can extract a mesh at a specificpoint in time. Additionally, our 4D neural representation can bealso used for static mapping (c) and dynamic object removal (c). Mapping and state estimation in dynamic environmentsis a classical problem in robotics . Approaches forsimultaneous localization and mapping (SLAM) can applydifferent strategies to deal with dynamics. Common waysare: (1) filtering dynamics from the input as a pre-processing step, which requires a semantic inter-pretation of the scene; (2) modeling the occupancy in themap representation , where dynam-ics can be implicitly removed by retrospectively removingmeasurements in free space; (3) including it in the state es-timation to model which measurementsoriginated from the dynamic and static parts of the environ-ment. Our proposed method falls into the last category andallows us to model dynamics directly in the map represen-tation leading to a spatio-temporal map representation.",
  "arXiv:2405.03388v1 [cs.CV] 6 May 2024": "Recently, implicit neural representations gained increas-ing interest in computer vision for novel view synthesis and 3D shape reconstruction .Due to theircompactness and continuity, several approaches investigate the use of neural representations in large-scale3D LiDAR mapping leading to accurate maps while sig-nificantly reducing memory consumption. However, theseapproaches often do not address the problem of handlingdynamics during mapping. The recent progress on dynamicNeRF and neural deformable object recon-struction indicates that neural representations can bealso used to represent dynamic scenes, which inspires usto tackle the problem of mapping in dynamic environmentsfrom the perspective of 4D reconstruction.In this paper, we propose a novel method to reconstructlarge 4D dynamic scenes by encoding every points time-dependent truncated signed distance function (TSDF) intoan implicit neural scene representation. As illustrated in, we take sequentially recorded LiDAR point cloudscollected in dynamic environments as input and generate aTSDF for each time frame, which can be used to extract amesh using marching cubes . The background TSDF,which is unchanged during the whole sequence, can be ex-tracted from the 4D signal easily. We regard it as a staticmap that can be used to segment dynamic objects from theoriginal point cloud. Compared to the traditional voxel-based mapping method, the continuous neural representa-tion allows for the removal of dynamic objects while pre-serving rich map details. In summary, the main contribu-tions of this paper are: We propose a novel implicit neural representation tojointly reconstruct a dynamic 3D environment and main-tain a static map using sequential LiDAR scans as input.",
  ". Related Work": "Mapping and SLAM in dynamic environments is a classi-cal topic in robotics with a large body of work,which tackles the problem by pre-processing the sensordata , occupancy estimation to filter dy-namics by removing measurements in free space , or state estimation techniques . Below, we focus on closely related approachesusing neural representations but also static map building ap-proaches for scenes containing dynamics. Dynamic NeRF. Dynamic NeRFs aim to solve the prob-lem of novel view synthesis in dynamic environments.Some approaches address this challenge bymodeling the deformation of each point with respect to acanonical frame. However, these methods cannot representnewly appearing objects. This can render them unsuited forcomplicated real-life scenarios. In contrast, NSFF andDynIBaR get rid of the canonical frame by computingthe motion field of the whole scene. While these methodscan deliver satisfactory results, the training time is usuallyin the order of hours or even days.Another type of method leverages the compactness ofthe neural representation to model the 4D spatio-temporalinformation directly. Several works project the4D input into multiple voxelized lower-dimensional fea-ture spaces to avoid large memory consumption, which im-proves the efficiency of the optimization. Song et al. propose a time-dependent sliding window strategy for accu-mulating the voxel features. Instead of only targeting novelview synthesis, several approaches decomposethe scene into dynamic objects and static background ina self-supervised way, which inspired our work.Otherapproaches accomplish neural representation-based reconstruction for larger scenes by adding additionalsupervision such as object masks or optical flow.Neural representations for LiDAR scans. Recently,many approaches aim to enhance scene reconstruction us-ing LiDAR data through neural representations. The earlywork URF leverages LiDAR data as depth supervisionto improve the optimization of a neural radiance field. Withonly LiDAR data as input, Huang et al. achieve novelview synthesis for LiDAR scans with differentiable render-ing. Similar to our work, Shine-mapping and EIN-RUL utilize sparse hierarchical feature voxel structuresto achieve large-scale 3D mapping. Additionally, the data-driven approach NKSR based on learned kernel re-gression demonstrates accurate surface reconstruction withnoisy LiDAR point cloud as input.Although these ap-proaches perform well in improving reconstruction accu-racy and reducing memory consumption, none of them con-sider the problem of dynamic object interference in real-world environments.Static map building and motion detection. In addi-tion to removing moving objects from the voxel map withray tracing, numerous works try to segmentdynamic points from raw LiDAR point clouds. However,these methods require a significant amount of labeled data,which makes it challenging to generalize them to variousscenarios or sensors with different scan patterns. In con-trast, geometry-based, more heuristic approaches have alsoproduced promising results. Kim et al. solve this prob-lem using the visibility of range images, but their resultsare still highly affected by the resolution. Lim et al. pro- posed Erasor , which leverages ground fitting as priorto achieve better segmentation for dynamic points. Morerecent approaches extend it to instance level to im-prove results. However, these methods rely on an accurateground fitting method, which is mainly designed for au-tonomous driving scenarios, which cannot be guaranteed incomplex unstructured real environments.In contrast to the approaches discussed above, we fol-low recent developments in neural reconstruction and pro-pose a novel scene representation that allows us to capturethe spatio-temporal progression of a scene. We representthe time-varying SDF of a scene in an unsupervised fash-ion, which we exploit to remove dynamic objects and re-construct accurate meshes of the static scene.",
  ". Our Approach": "The input of our approach is given by a sequence of pointclouds, S1:N= (S1, . . . , SN), and their correspondingglobal poses Tt R44, t [1, N], estimated via scanmatching, LiDAR odometry, or SLAM methods . Each scans point cloud St = {s1t, . . . , sMtt} is a set ofpoints, sit R3, collected at time t. Given such a sequenceof scans S1:N, our approach aims to reconstruct a 4D TSDFof the traversed scene and maintain a static 3D map at thesame time.In the next sections, we first introduce our spatio-temporal representation and then explain how to optimizeit to represent the dynamic and static parts of a point cloudsequence S1:N.",
  ". Map Representation": "The key component of our approach is an implicit neuralscene representation that allows us to represent a 4D TSDFof the scene, as well as facilitates the extraction of a staticmap representation. Our proposed spatio-temporal scenerepresentation is optimized for the given point cloud se-quence S1:N such that we can retrieve for an arbitrary pointp R3 and time t [1, N] the corresponding time-varyingsigned distances value at that location.Temporal representation. We utilize an TSDF to rep-resent the scene, i.e., a function that provides the signeddistance to the nearest surface for any given point p R3.The sign of the distance is positive when the point is in freespace or in front of the measured surface and is negativewhen the point is inside the occupied space or behind themeasured surface.In a dynamic 3D scene, measuring the signed distance ofany coordinate at each moment produces a time-dependentfunction that captures the signed distance changes overtime, see for an illustration. Additionally, if a co-ordinate is static throughout the period, the signed dis-tance should remain constant. The key idea of our spatio-temporal scene representation is to fit the time-varying SDF . Principle of our 4D TSDF representation: The left figureshows a moving object and a query point p. The one on the rightdepicts the corresponding signed distance at p over time. At t0,ps signed distance is a positive truncated value. When the movingobject reaches p at time t1, p is inside the object and its signeddistance is negative accordingly. At t2, the moving object movedpast p, the signed distance of p gets positive again. at each point with several basis functions. Inspired by Liet al. s representation of moving point trajectories, weexploit K globally shared basis functions k : R R. Us-ing these basis functions k(t), we model the time-varyingTSDF F(p, t) that maps a location p R3 at time t to asigned distance as follows:",
  "N (2t + 1)(k 1).(2)": "The first basis function for k = 1 is time-independentas 1(t) = 1. During the training process, we fix 1(t) anddetermine the other basis functions by backpropagation. Weconsider 1(t)s corresponding weight w1p as the static SDFvalue of the point p. Hence, F(p, t) consists of its staticbackground value, i.e., w1p1(t) = w1p, and the weightedsum of dynamic basis functions 2(t), . . . , K(t).As the basis functions 1(t), . . . , K(t) are shared be-tween all points in the scene, we need to optimize thelocation-dependent weights that are implicitly representedin our spatial representation.Spatial representation. To achieve accurate scene re-construction while maintaining memory efficiency, we em-ploy a multi-resolution sparse voxel grid to store spatial ge-ometric information.First, we accumulate the input point clouds, S1, . . . , SNbased on their poses T1, . . . , TN computed from LiDARodometry and generate a hierarchy of voxel grids aroundpoints to ensure complete coverage in 3D. We use a spatialhash table for fast retrieval of the resulting voxels that areonly initialized if points fall into a voxel. . Overview of querying a TSDF value in our 4D map representation. For querying a point p at ti and ti+1, we first retrieve eachcorners feature in F l of the voxel that p is located in and obtain the fused feature fp by trilinear interpolation. Then, we feed fp into thedecoder Dmlp and take the output as the weights of different basis functions 1(t), . . . , K(t). Finally, we calculate the weighted sum ofbasis functions values at ti and ti+1 to get their respective SDF results. For simplicity, we only illustrate one level of hashed feature grids. Similar to Instant-NGP , we save a feature vectorf RD at each corner vertex of the voxel grid in each res-olution level, where we denote as Fl the level-wise cornerfeatures. We compute the feature vector fp RD for givenquery point p R3 inside the hierarchical grid as follows:",
  ". Objective Function": "We take samples along the rays from the input scans Stto collect training data. Each scan frame St correspondsto a moment t in time, so we gather four-dimensional datapoints (q, t) via sampling along the ray from the scan originot R3 to a point sit St. We can represent the sampledpoints qis along the ray as qis = ot + (sit ot). By settinga truncation threshold , we split the ray into two regions,at the surface and in the free-space:",
  "where = (sit ot)1. Thus, T isurf represents the re-gion close to the endpoint sit St, and T ifree is the region": "in the free space. We uniformly sample Ms and Mf pointsfrom T isurf and T ifree separately. We obtain two sets Dsurf andDfree of samples by sampling over all scans. Unlike priorwork that use differentiable rendering to calculatethe depth by integration along the ray, we design differentlosses for Dsurf and Dfree to supervise the 4D TSDF directly.Near Surface Loss. Since the output of our 4D map isthe signed distance value d = F(p, t) at an arbitrary posi-tion p R3 in time t [1, N], we expect that the predictedvalue d does not change over time for static points. How-ever, this cannot be guaranteed if we use the projective dis-tance dsurf to the surface along the ray direction directly asthe target value, since the projective distance would changeover time due to the change of view direction by the movingsensor, even in a static scene. Thus, for the sampled datain Dsurf, i.e., the sampled points near the surface, we canonly obtain reliable information about the sign of the TSDFvalue of these points, which should be positive if the pointis before the endpoint and negative if the point is behind.In addition, for a sampled point in front of the endpoint, itsprojective signed distance dsurf should be the upper boundof its actual signed distance value. And for sampled pointsbehind the endpoint, dsurf should be the lower bound.We design a piecewise loss Lsurf to supervise the sampledpoints near the surface:",
  "| d |if d dsurf < 0| d dsurf |if d dsurf > d2surf0otherwise,(7)": "where d = F(q, t) is the predicted value from our mapfor a sample point q Dsurf and dsurf is its correspondingprojective signed distance for that sampled point in the cor-responding scan St. This loss punishes only a predictionwhen the sign is wrong or its absolute value is larger thanthe absolute value of dsurf. For a query point exactly on thesurface, i.e., dsurf = 0, Lsurf is simply the L1 loss. To calculate an accurate signed distance value and main-tain the consistency of constraints for static points from dif-ferent observations, we use the natural property of signeddistance function to constraint the length of the gradientvector for samples inside Dsurf, which is called Eikonal reg-ularization :",
  "where xF(p, t) is the component of the gradient F (p,t)": "pon the x axis, and x = (, 0, 0) is the added perturbation.We apply the same operation on y and z axes to calculatethe numerical gradient. Furthermore, in order to get fasterconvergence at the beginning and ultimately recover the richgeometric details, we first set a large and gradually reduceit during the training process.Free Space Loss. As we tackle the problem of map-ping in dynamic environments, we cannot simply accumu-late point clouds and then calculate accurate supervision ofsigned distance value via nearest neighbor search. There-fore, we use a L1 loss Lfree to constrain the signed distanceprediction d of the free space points, i.e., p Dfree:",
  "Lfree( d) = | d |,(10)": "where is the truncation threshold we used in Sec. 3.2.Thanks to our spatio-temporal representation, a singlequery point can get both, static and dynamic TSDF values.Thus, for some regions that are determined to be free space,we can directly add constraints to their static TSDF values.We divide the free space points Dfree into dense andsparse subset Ddense and Dsparse based on a threshold rdensefor the distance from the free space point sampled at timet to the scan origin ot. For each point p Ddense, we findthe nearest neighbor np in the corresponding scan St, i.e.,np = arg minqStp q2. Let Dcertain = {p Ddense |||p np|| > } be the points that we consider in the cer-tain free space. Then, we supervise p Dcertain by its staticsigned distance value directly:",
  "(p,t)DcertainLcertain(p),(12)": "where d = F(p, t) is the predicted signed distance at thesample position p at time t and dsurf is the projective signeddistance of sample p. With the above loss function and datasampling strategy, we train our map offline until conver-gence. In , we show TSDF slices obtained using ouroptimized 4D map at different times.One application of our 4D map representation is dy-namic object segmentation. For a point p in the input scansS1:N, its static signed distance value w1p can be obtainedby a simple query. If p belongs to the static background, itshould have w1p = 0. Therefore, we simply set a thresholddstatic and regard a point as dynamic if w1p > dstatic.",
  ". Implementation Details": "As hyperparameters of our approach, we use the valueslisted in Tab. 1 in all LiDAR experiments. Additional pa-rameters are determined by the characteristics of the sensorand the dimensions of the scene. For instance, in the recon-struction of autonomous driving scenes, like KITTI, we setthe highest resolution for the feature voxels to 0.3 m. Thetruncation distance is set to = 0.5 m, and the dense areasplit threshold rdense = 15 m. Regarding training time, ittakes 12 minutes to train 140 frames from the KITTI datasetusing a single Nvidia Quadro RTX 5000.",
  ". Experiments": "In this section, we show the effectiveness of our proposedapproach with respect to two aspects: (1) Static mappingquality: The static TSDF built by our method allows usto extract a surface mesh using marching cubes . Wecompare this extracted mesh with the ground truth mesh toevaluate the reconstruction. (2) Dynamic object segmenta-tion: As mentioned above, our method can segment out thedynamic objects in the input scans. We use point-wise dy-namic object segmentation accuracy to evaluate the results.",
  ". Static Mapping Quality": "Datasets. We select two datasets collected in dynamic en-vironments for quantitative evaluation. One is the syntheticdataset ToyCar3 from Co-Fusion , which provides ac-curate depth images and accurate masks of dynamic objectsrendered using Blender, but also depth images with addednoise. For this experiment, we select 150 frames from thewhole sequence, mask out all dynamic objects in the accu-rate depth images, and accumulate background static pointsas the ground-truth static map. The original noisy depth im-ages are used as the input for all methods.Furthermore, we use the Newer College dataset asthe real-world dataset, which is collected using a 64-beamLiDAR. Compared with synthetic datasets, it contains moreuncertainty from measurements and pose estimates. We se-lect 1,300 frames from the courtyard part for testing and thisdata includes a few pedestrians as dynamic objects. Thisdataset offers point clouds obtained by a high-precision ter-restrial laser scanner that can be directly utilized as groundtruth to evaluate the mapping quality.Metric and Baselines. We report the reconstruction ac-curacy, completeness, the Chamfer distance, and the F1-score. Further details on the computation of the metrics canbe found in the supplement.We compare our method with several different types ofstate-of-the-art methods: (i) the traditional TSDF-fusionmethod, VDBfusion , which uses space carving toeliminate the effects of dynamic objects, (ii) the data-driven-based method, neural kernel surface reconstruc-tion (NKSR) , and (iii) the neural representation based3D mapping approach, SHINE-mapping .For NKSR , we use the default parameters providedby Huang et al. with their official implementation. To en-sure a fair comparison with SHINE-mapping, we adopt anequal number of free space samples (15 samples), aligningwith our method for consistency.For the ToyCar3 dataset, we set VDB-Fusions resolu-tion to 1 cm. To have all methods with a similar memoryconsumption, we set the resolution of SHINE-mappingsleaf feature voxel to 2 cm, and our methods highest resolu-tion accordingly to 2 cm. For the Newer College dataset, weset the resolution to 10 cm, 30 cm, and 30 cm respectively.",
  "Ours0.4380.4680.45298.35": "Results. The quantitative results for synthetic datasetToyCar3 and real-world dataset Newer College are pre-sented in Tab. 2 and Tab. 3, respectively. We also showthe extracted meshes from all methods in and .Our method outperforms the baselines in terms of Com-pleteness and Chamfer distance for both datasets (cf. and ). Regarding the accuracy, SHINE-mapping andVDB-Fusion can filter part of high-frequency noise by fu-sion of multiple frames, resulting in better performance onnoisy Newer College dataset. In comparison, our methodconsiders every scan as accurate to store 4D information,which makes it more sensitive to measurement noise. Onthe ToyCar3 dataset, both our method and VDB-Fusion suc-cessfully eliminate all moving objects. However, on theNewer College dataset, VDB-Fusion incorrectly eliminatesthe static tree and parts of the ground, resulting in poor com-pleteness shown in Tab. 3. SHINE-mapping eliminates dy-namic pedestrians on the Newer College dataset but retains aportion of the dynamic point cloud on the ToyCar3 dataset,which has a larger proportion of dynamic objects, leadingto poorer accuracy in Tab. 2. NKSR performs the worst ac-curacy because it is unable to eliminate dynamic objects,which means its not suitable to apply NKSR in dynamicreal-world scenes directly.",
  "(e) SHINE-mapping": ". A comparison of the static mapping results of different methods on the Newer College dataset. Several pedestrians are movingthrough the scene during the data collection. Our method can reconstruct the static scene completely and eliminate the moving pedestrians.Although VDB-Fusion manages to eliminate the pedestrians, it incorrectly removes the tree highlighted in the orange box. .Quantitative results of the reconstruction quality onNewer College. We report the distance error metrics, namely com-pletion, accuracy and Chamfer-L1 in cm. Additionally, we showthe F-score in % with a 20 cm error threshold.",
  "Ours5.856.496.1797.50": "cludes four sequences in total: sequence 00 (frame 4,390 4,530 ) and sequence 05 (frame 2,350 2,670) from theKITTI dataset , which are captured by a 64-beam Li-DAR, one sequence from the Argoverse2 dataset con-sisting of 575 frames captured by two 32-beam LiDARs,and a semi-indoor sequence captured by a sparser 16-beamLiDAR. All sequences come with corresponding pose filesand point-wise dynamic or static labels as the ground truth.It is worth noting that the poses for KITTI 00 and 05 wereobtained from SuMa and the pose files for the Semi-indoor sequence come from NDT-SLAM .Metric and Baselines. The KTH-Dynamic-Benchmarkevaluates the performance of the method by measuring theclassification accuracy of dynamic points (DA%), staticpoints (SA%) and also their associated accuracy (AA%)where AA =",
  "DA SA. The benchmark provides various": "baselines such as the state-of-the-art LiDAR dynamic objectremoval methods Erasor and Removert , as wellas the traditional 3D mapping method, Octomap ,and its modified versions, Octomap with ground fitting andoutlier filtering. As SHINE-mapping demonstrates the abil-ity to remove dynamic objects in our static mapping exper-iments, we also report its result in this benchmark. Addi-tionally, we report the performance of the state-of-the-artonline moving object segmentation methods, 4DMOS and its extension MapMOS . As these two methods uti-lize KITTI sequences 00 and 05 for training, we only showthe results of the remaining two sequences. For the parame-ter setting, we set our methods leaf resolution to 0.3 m, andthe threshold for segmentation as dstatic = 0.16 m. We setthe leaf resolution for Octomap to 0.1 m. Results. The quantitative results of the dynamic objectsegmentation are shown in Tab. 4. And we depict the ac-cumulated static points generated by different methods in. We can see that our method achieves the best associ-ated accuracy (AA) in three autonomous driving sequences(KITTI 00, KITTI 05, Argoverse2) and vastly outperformsbaselines. The supervised learning-based methods 4DMOSand MapMOS do not obtain good dynamic accuracy (DA)due to limited generalizability. Erasor and Octomap tendto over-segment dynamic objects, resulting in poor staticaccuracy (SA). Removert and SHINE-mapping are too con-servative and cannot detect all dynamic objects. Benefiting . Quantitative results of the dynamic object removal quality on the KTH-Dynamic-Benchmark. We report the static accuracy SA,dynamic static DA and the associated accuracy AA. Octomap* refers to the modified Octomap implementation by Zhang et al. .",
  "(j) Octomap*": ". Comparison of dynamic object removal results produced by our proposed method and three baseline methods on the Argoverse2data sequence of the KTH-benchmark. We show the birds eye view on the first row and the zoomed view from the blue frustum shown in(a) on the second row. For the ground truth results in (a), the dynamic objects are shown in red. We only show the static points of groundtruth for clearer comparison in zoomed view (f). We highlight the over-segmented parking car and sign by Erasor and the undetectedmoving vehicle by Removert. from the continuity and large capacity of the 4D neural rep-resentation, we strike a better balance between preservingstatic background points and removing dynamic objects.It is worth mentioning again that our method does notrely on any pre-processing or post-processing algorithmsuch as ground fitting, outlier filtering, and clustering, butalso does not require labels for training.",
  ". Conclusion": "In this paper, we propose a 4D implicit neural map repre-sentation for dynamic scenes that allows us to represent theTSDF of static and dynamic parts of a scene. For this pur-pose, we use a hierarchical voxel-based feature representa-tion that is then decoded into weights for basis functions torepresent a time-varying TSDF that can be queried at arbi-trary locations. For learning the representation from a se- quence of LiDAR scans, we design an effective data sam-pling strategy and loss functions. Equipped with our pro-posed representation, we experimentally show that we areable to tackle the challenging problems of static mappingand dynamic object segmentation. More specifically, ourexperiments show that our method has the ability to accu-rately reconstruct 3D maps of the static parts of a scene andcan completely remove moving objects at the same time. Limitations. While our method achieves compelling re-sults, we have to acknowledge that we currently rely on esti-mated poses by a separate SLAM approach, but also cannotapply our approach in an online fashion. However, we seethis as an avenue for future research into joint incrementalmapping and pose estimation.",
  "Peter Biber and Tom Duckett. Dynamic Maps for Long-TermOperation of Mobile Service Robots. In Proc. of Robotics:Science and Systems (RSS), 2005. 1, 2": "Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif,Davide Scaramuzza, Jose Neira, Ian Reid, and John J.Leonard. Past, Present, and Future of Simultaneous Local-ization And Mapping: Towards the Robust-Perception Age.IEEE Trans. on Robotics (TRO), 32(6):13091332, 2016. 1,2 Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang, andJuyong Zhang.Neural surface reconstruction of dynamicscenes with monocular rgb-d camera. In Proc. of the Conf.on Neural Information Processing Systems (NeurIPS), 2022.2",
  "Ang Cao and Justin Johnson. HexPlane: A Fast Representa-tion for Dynamic Scenes. In Proc. of the IEEE/CVF Conf. onComputer Vision and Pattern Recognition (CVPR), 2023. 2": "Xieyuanli Chen, Shijie Li, Benedikt Mersch, Louis Wies-mann, Juergen Gall, Jens Behley, and Cyrill Stachniss. Mov-ing Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data. IEEE Roboticsand Automation Letters (RA-L), 6(4):65296536, 2021. 2 Xieyuanli Chen, Benedikt Mersch, Lucas Nunes, RodrigoMarcuzzi, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss.Automatic Labeling to Generate Training Data for OnlineLiDAR-Based Moving Object Segmentation. IEEE Roboticsand Automation Letters (RA-L), 7(3):61076114, 2022. 3 Xu Chen, Tianjian Jiang, Jie Song, Max Rietmann, AndreasGeiger, Michael J. Black, and Otmar Hilliges. Fast-snarf: Afast deformer for articulated neural fields. IEEE Trans. onPattern Analysis and Machine Intelligence (TPAMI), 45(10):1179611809, 2023. 2 Pierre Dellenbach,Jean-Emmanuel Deschaud,BastienJacquet, and Francois Goulette. CT-ICP Real-Time ElasticLiDAR Odometry with Loop Closure. In Proc. of the IEEEIntl. Conf. on Robotics & Automation (ICRA), 2022. 3",
  "Jean-Emmanuel Deschaud.IMLS-SLAM: scan-to-modelmatching based on 3D data.In Proc. of the IEEEIntl. Conf. on Robotics & Automation (ICRA), 2018. 3": "Sara Fridovich-Keil, Giacomo Meanti, Frederik R. Warburg,Benjamin Recht, and Angjoo Kanazawa. K-Planes: ExplicitRadiance Fields in Space, Time, and Appearance. In Proc. ofthe IEEE/CVF Conf. on Computer Vision and Pattern Recog-nition (CVPR), 2023. 2 Andreas Geiger, Peter Lenz, and Raquel Urtasun. Are weready for Autonomous Driving? The KITTI Vision Bench-mark Suite. In Proc. of the IEEE Conf. on Computer Visionand Pattern Recognition (CVPR), 2012. 5, 7",
  "Dirk Hahnel, Dirk Schulz, and Wolfram Burgard.Mo-bile robot mapping in populated environments. In Proc. ofthe IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems(IROS), 2002. 1, 2": "Armin Hornung, Kai M. Wurm, Maren Bennewitz, CyrillStachniss, and Wolfram Burgard. OctoMap: An EfficientProbabilistic 3D Mapping Framework Based on Octrees. Au-tonomous Robots, 34(3):189206, 2013. 1, 2, 7, 8 Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, SanjaFidler, and Francis Williams. Neural Kernel Surface Recon-struction. In Proc. of the IEEE/CVF Conf. on Computer Vi-sion and Pattern Recognition (CVPR), 2023. 2, 6, 7",
  "Shengyu Huang, Zan Gojcic, Jiahui Huang, Andreas Wieser,and Konrad Schindler. Dynamic 3D Scene Analysis by PointCloud Accumulation. In Proc. of the Europ. Conf. on Com-puter Vision (ECCV), 2022. 2": "Shengyu Huang, Zan Gojcic, Zian Wang, Francis Williams,Yoni Kasten, Sanja Fidler, Konrad Schindler, and Or Litany.Neural LiDAR Fields for Novel View Synthesis. In Proc. ofthe IEEE/CVF Intl. Conf. on Computer Vision (ICCV), 2023.2, 4 Giseop Kim and Ayoung Kim. Remove, Then Revert: StaticPoint Cloud Map Construction Using Multiresolution RangeImages. In Proc. of the IEEE/RSJ Intl. Conf. on IntelligentRobots and Systems (IROS), 2020. 2, 7, 8 Xin Kong, Shikun Liu, Marwan Taher, and Andrew J. Davi-son. vMAP: Vectorised Object Mapping for Neural FieldSLAM. In Proc. of the IEEE/CVF Conf. on Computer Visionand Pattern Recognition (CVPR), 2023. 2 Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi,Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi,Frank Dellaert, and Thomas Funkhouser. Panoptic neuralfields: A semantic object-aware neural scene representation.In Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-tern Recognition (CVPR), 2022. 2 Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.Neural scene flow fields for space-time view synthesis of dy-namic scenes. In Proc. of the IEEE/CVF Conf. on ComputerVision and Pattern Recognition (CVPR), 2021. 2 Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H Tay-lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.Neuralangelo: High-fidelity neural surface reconstruction. InProc. of the IEEE/CVF Conf. on Computer Vision and Pat-tern Recognition (CVPR), 2023. 5 Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,and Noah Snavely. DynIBaR: Neural Dynamic Image-BasedRendering. In Proc. of the IEEE/CVF Conf. on ComputerVision and Pattern Recognition (CVPR), 2023. 2, 3 Hyungtae Lim, Sungwon Hwang, and Hyun Myung. ERA-SOR: Egocentric Ratio of Pseudo Occupancy-Based Dy-namic Object Removal for Static 3D Point Cloud Map Build-ing. IEEE Robotics and Automation Letters (RA-L), 6(2):22722279, 2021. 3, 7, 8 Hyungtae Lim, Lucas Nunes, Benedikt Mersch, XieyuanliChen, Jens Behley, and Cyrill Stachniss.ERASOR2:Instance-Aware Robust 3D Mapping of the Static World inDynamic Scenes. In Proc. of Robotics: Science and Systems(RSS), 2023. 3 William E. Lorensen and Harvey E. Cline. Marching Cubes:a High Resolution 3D Surface Construction Algorithm. InProc. of the Intl. Conf. on Computer Graphics and Interac-tive Techniques (SIGGRAPH), 1987. 2, 6 John McCormac, Ankur Handa, Aandrew J. Davison, andStefan Leutenegger. SemanticFusion: Dense 3D SemanticMapping with Convolutional Neural Networks. In Proc. ofthe IEEE Intl. Conf. on Robotics & Automation (ICRA),2017. 1, 2 Benedikt Mersch, Xieyuanli Chen, Ignacio Vizzo, LucasNunes, Jens Behley, and Cyrill Stachniss. Receding Mov-ing Object Segmentation in 3D LiDAR Data Using Sparse4D Convolutions.IEEE Robotics and Automation Letters(RA-L), 7(3):75037510, 2022. 2, 7, 8 Benedikt Mersch, Tiziano Guadagnino, Xieyuanli Chen,Tiziano, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss.Building Volumetric Beliefs for Dynamic Environments Ex-ploiting Map-Based Moving Object Segmentation.IEEERobotics and Automation Letters (RA-L), 8(8):51805187,2023. 2, 7, 8 Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-bastian Nowozin, and Andreas Geiger. Occupancy networks:Learning 3d reconstruction in function space. In Proc. of theIEEE/CVF Conf. on Computer Vision and Pattern Recogni-tion (CVPR), 2019. 2 Daniel Meyer-Delius, Maximilitan Beinhofer, and WolframBurgard.Occupancy Grid Models for Robot Mapping inChanging Environments. In Proc. of the Conf. on Advance-ments of Artificial Intelligence (AAAI), 2012. 1, 2 Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:Representing Scenes as Neural Radiance Fields for ViewSynthesis. In Proc. of the Europ. Conf. on Computer Vision(ECCV), 2020. 2",
  "ception. In Proc. of Robotics: Science and Systems (RSS),2022. 5": "Emanuele Palazzolo, Jens Behley, Philipp Lottes, PhilippeGiguere, and Cyrill Stachniss. ReFusion: 3D Reconstructionin Dynamic Environments for RGB-D Cameras ExploitingResiduals. In Proc. of the IEEE/RSJ Intl. Conf. on IntelligentRobots and Systems (IROS), 2019. 2 Jeong Joon Park, Peter Florence, Julian Straub, RichardNewcombe, and Steven Lovegrove.DeepSDF: LearningContinuous Signed Distance Functions for Shape Represen-tation. In Proc. of the IEEE/CVF Conf. on Computer Visionand Pattern Recognition (CVPR), 2019. 2 Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, SofienBouaziz, Dan B Goldman, Steven M. Seitz, and RicardoMartin-Brualla.Nerfies:Deformable Neural RadianceFields. In Proc. of the IEEE/CVF Intl. Conf. on ComputerVision (ICCV), 2021. 2 Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz.Hypernerf:A higher-dimensional representation for topologically varying neuralradiance fields.ACM Trans. on Graphics (TOG), 40(6),2021. Albert Pumarola, Enric Corona, Gerard Pons-Moll, andFrancesc Moreno-Noguer. D-nerf: Neural radiance fields fordynamic scenes. In Proc. of the IEEE/CVF Conf. on Com-puter Vision and Pattern Recognition (CVPR), 2021. 2",
  "Sameera Ramasinghe, Violetta Shevchenko, Gil Avraham,and Anton Van Den Hengel.Blirf: Band limited radi-ance fields for dynamic scene modeling.arXiv preprintarXiv:2302.13543, 2023. 2": "Milad Ramezani, Yiduo Wang, Marco Camurri, DavidWisth, Matias Mattamala, and Maurice Fallon. The NewerCollege Dataset: Handheld LiDAR, Inertial and Vision withGround Truth. In Proc. of the IEEE/RSJ Intl. Conf. on Intel-ligent Robots and Systems (IROS), 2020. 6 Konstantinos Rematas,Andrew Liu,Pratul P. Srini-vasan, Jonathan T. Barron, Andrea Tagliasacchi, ThomasFunkhouser, and Vittorio Ferrari. Urban radiance fields. InProc. of the IEEE/CVF Conf. on Computer Vision and Pat-tern Recognition (CVPR), 2022. 2, 4",
  "Martin Runz and Lourdes Agapito. Co-Fusion: Real-TimeSegmentation, Tracking and Fusion of Multiple Objects. InProc. of the IEEE Intl. Conf. on Robotics & Automation(ICRA), 2017. 1, 2, 6": "Martin Runz, Maud Buffier, and Lourdes Agapito. MaskFu-sion: Real-Time Recognition, Tracking and Reconstructionof Multiple Moving Objects. In Proc. of the Intl. Sympo-sium on Mixed and Augmented Reality (ISMAR), 2018. 1,2 Jari Saarinen, Henrik Andreasson, and Achim Lilienthal. In-dependent Markov Chain Occupancy Grid Maps for Rep-resentation of Dynamic Environments.In Proc. of theIEEE/RSJ Intl. Conf. on Intelligent Robots and Systems(IROS), 2012. 1, 2",
  "Environments Using Normal Distributions Transform Occu-pancy Maps. In Proc. of the IEEE/RSJ Intl. Conf. on Intelli-gent Robots and Systems (IROS), 2013. 1, 2, 7": "Renato F. Salas-Moreno, Richard A. Newcombe, HaukeStrasdat, Paul H. Kelly, and Andrew J. Davison. SLAM++:Simultaneous Localisation and Mapping at the Level of Ob-jects. In Proc. of the IEEE/CVF Conf. on Computer Visionand Pattern Recognition (CVPR), 2013. 1, 2 Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,Hongwen Zhang, and Yebin Liu. Tensor4d : Efficient neural4d decomposition for high-fidelity dynamic reconstructionand rendering. In Proc. of the IEEE/CVF Conf. on ComputerVision and Pattern Recognition (CVPR), 2023. 2 Chonghyuk Song, Gengshan Yang, Kangle Deng, Jun-YanZhu, and Deva Ramanan. Total-recon: Deformable scenereconstruction for embodied view synthesis. In Proc. of theIEEE/CVF Intl. Conf. on Computer Vision (ICCV), 2023. 2 Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, LeleChen, Junsong Yuan, Yi Xu, and Andreas Geiger. NeRF-Player: A Streamable Dynamic Scene Representation withDecomposed Neural Radiance Fields.IEEE Transactionson Visualization and Computer Graphics, 29(5):27322742,2023. 2",
  "Ignacio Vizzo, Tiziano Guadagnino, Jens Behley, and CyrillStachniss. VDBFusion: Flexible and Efficient TSDF Inte-gration of Range Sensor Data. Sensors, 22(3):1296, 2022. 6,7": "Ignacio Vizzo, Tiziano Guadagnino, Benedikt Mersch, LouisWiesmann, Jens Behley, and Cyrill Stachniss. KISS-ICP:In Defense of Point-to-Point ICP Simple, Accurate, andRobust Registration If Done the Right Way. IEEE Roboticsand Automation Letters (RA-L), 8(2):10291036, 2023. 3 Aishan Walcott-Bryant, Michael Kaess, Hordur Johannsson,and John J. Leonard. Dynamic Pose Graph SLAM: Long-Term Mapping in Low Dynamic Environments. In Proc. ofthe IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems(IROS), 2012. 1, 2",
  "Chaoyang Wang, Ben Eckart, Simon Lucey, and OrazioGallo. Neural trajectory fields for dynamic novel view syn-thesis. arXiv preprint arXiv:2105.05994, 2021. 3": "Chung-Yi Weng,Brian Curless,Pratul P. Srinivasan,Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-manNeRF: Free-Viewpoint Rendering of Moving PeopleFrom Monocular Video. In Proc. of the IEEE/CVF Conf. onComputer Vision and Pattern Recognition (CVPR), 2022. 2 Thomas Whelan, Stefan Leutenegger, Renato F. Salas-Moreno, Ben Glocker, and Andrew J. Davison. ElasticFu-sion: Dense SLAM Without A Pose Graph.In Proc. ofRobotics: Science and Systems (RSS), 2015. 1, 2 Louis Wiesmann, Tiziano Guadagnino, Ignacio Vizzo, NickyZimmerman, Yue Pan, Haofei Kuang, Jens Behley, andCyrill Stachniss. LocNDF: Neural Distance Field Mappingfor Robot Localization. IEEE Robotics and Automation Let-ters (RA-L), 8(8):49995006, 2023. 2 Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,Deva Ramanan, Peter Carr, and James Hays.Argoverse2: Next Generation Datasets for Self-driving Perception andForecasting.In Proc. of the Conf. on Neural InformationProcessing Systems (NeurIPS), 2021. 7",
  "Denis F. Wolf and Guarav S. Sukhatme. Mobile Robot Si-multaneous Localization and Mapping in Dynamic Environ-ments. Autonomous Robots, 19, 2005. 1, 2": "Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-rester Cole, and Cengiz Oztireli. D2NeRF: Self-SupervisedDecoupling of Dynamic and Static Objects from a Monocu-lar Video. In Proc. of the Conf. on Neural Information Pro-cessing Systems (NeurIPS), 2022. 2 Kai M. Wurm, Armin Hornung, Maren Bennewitz, CyrillStachniss, and Wolfram Burgard. OctoMap: A Probabilistic,Flexible, and Compact 3D Map Representation for RoboticSystems. In Workshop on Best Practice in 3D Perceptionand Modeling for Mobile Manipulation, IEEE Int. Conf. onRobotics & Automation (ICRA), 2010. 7",
  "Dongyu Yan, Xiaoyang Lyu, Jieqi Shi, and Yi Lin. EfficientImplicit Neural Reconstruction Using LiDAR. In Proc. of theIEEE Intl. Conf. on Robotics & Automation (ICRA), 2023. 2": "Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, and StevenLovegrove. Star: Self-supervised tracking and reconstruc-tion of rigid objects in motion with neural rendering.InProc. of the IEEE/CVF Conf. on Computer Vision and Pat-tern Recognition (CVPR), 2021. 2 Qingwen Zhang, Daniel Duberg, Ruoyu Geng, Mingkai Jia,Lujia Wang, and Patric Jensfelt. A dynamic points removalbenchmark in point cloud maps. In IEEE 26th InternationalConference on Intelligent Transportation Systems (ITSC),pages 608614, 2023. 6, 8 Xingguang Zhong, Yue Pan, Jens Behley, and Cyrill Stach-niss.SHINE-Mapping: Large-Scale 3D Mapping UsingSparse Hierarchical Implicit Neural Representations.InProc. of the IEEE Intl. Conf. on Robotics & Automation(ICRA), 2023. 2, 6, 7, 8"
}