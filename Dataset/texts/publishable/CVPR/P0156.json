{
  "Abstract": "Complex video object segmentation serves as a funda-mental task for a wide range of downstream applicationssuch as video editing and automatic data annotation. Herewe present the 2nd place solution in the MOSE track ofPVUW 2024. To mitigate problems caused by tiny objects,similar objects and fast movements in MOSE. We use in-stance segmentation to generate extra pretraining data fromthe valid and test set of MOSE. The segmented instances arecombined with objects extracted from COCO to augmentthe training data and enhance semantic representation ofthe baseline model. Besides, motion blur is added duringtraining to increase robustness against image blur inducedby motion. Finally, we apply test time augmentation (TTA)and memory strategy to the inference stage. Our methodranked 2nd in the MOSE track of PVUW 2024, with a J of0.8007, a F of 0.8683 and a J &F of 0.8345.",
  ". Introduction": "Pixel-level Scene Understanding is one of the fundamen-tal problems in computer vision, which aims at recogniz-ing object classes, masks and semantics of each pixel in thegiven image. The pixel-level Video Understanding in theWild Challenge (PVUW) shop challenge advances the seg-mentation task from images to videos, aiming at enablingchallenging and practical realistic applications. The PVUW2024 workshop challenge includes two new tracks, Com-plex Video Object Segmentation Track based on MOSEand Motion Expression guided Video Segmentation trackbased on MeViS. The Complex Video Object Segmen-tation Track focuses on semi-supervised video object seg-mentation (VOS) under complex environments. As an im-portant branch of the VOS task, semi-supervised VOS aimsat tracking and segmenting agnostic objects given only thefirst-frame annotations, which has been widely applied inautonomous driving, video editing, automatic dataannotation, and universal video segmentation. Recent memory-based approaches have become themain stream data driven VOS methods. Memory-based ap-proaches store past segmented frames in a memory bank,when a new query frame comes, it will read from the mem-ory bank through cross attention, which is more robust todrifting and occlusions. Due to the advantages over otherVOS methods, the memory-based paradigm has beenpaid much attention by the research community. As oneof the most successful early attempts, Space-Time Mem-ory network (STM) stores the past frames with objectmasks into the memory and performs pixel-level matchingbetween the encoded key of query frame and memory.",
  ". Overview of our method": "Space-Time Correspondence Network (STCN) is de-veloped from STM, it encodes the key features from frameswithout masks and replace dot product with L2 similarityin the affinity for memory reading. STCN achieves betterefficiency and effectiveness than STM. XMem intro-duces three indendent memory banks: a sensory memory,a working memory and a long-term memory. The three-level memories are inspired by the AtkinsonShiffrin mem-ory model of Human. Xmem performs especially well onlong-video datasets because of the short-term to long-termmemories. To solve the mismatch problem in pixel-levelmatching, Cutie proposes the object memory and objecttransformer for bidirectional information interaction. Theobject memory and object transformer improve robustnessin challenging scenes with heavy occlusions and similarity.Due to the state-of-the-art performance of Cutie, we chooseCutie as our baseline model.However, the challenging nature of MOSE still",
  ". Architecture of Cutie": "poses several obstacles to overcome.Apart from higherdisappearance-reappearance rates and heavier occlusionsthan previous VOS datasets such as DAVIS andYouTubeVOS, MOSE has many tiny and similar ob-jects, which may confuse VOS models.Besides, somevideos in MOSE contain objects with fast movements,which are hard to track in consecutive frames.Tomitigatetheaboveissues,wecombineMask2Former and motion blur as data augmenta-tion to enhance semantic representation during the trainingprocess. We exploit images from the valid and test set ofMOSE, and use Mask2Former to segment instance masksand generate pretraining data for Cutie. We also introduceCOCO to further enrich the pretraining data, with thepurpose of enhancing semantic learning in the early stageof training and improve the segmentation accuracy of tinyand similar objects. At inference time, we employ test timeaugmentation (TTA) and memory strategy to optimize theresults. Our solution reaches a J of 0.8007, a F of 0.8683and a J &F of 0.8345, which achieved the 2nd place in theMOSE track of the PVUW challenge in CVPR 2024.",
  ". Baseline model": "To ensure good performance under challenges such as fre-quent disappearance-reappearance, heavy occlusions, smalland similar objects, we introduce Cutie as the strong base-line model, as shown in .Cutie stores a high-resolution pixel memory F and a high-level object memoryS. The pixel memory is encoded from the memory framesand corresponding segmented masks. The object memorycompresses object-level features from the memory frames.When a new query frame comes, it bidirectionally inter-acts with the object memory in a couple of object trans-former blocks. Specifically, given the feature map of thequery frame, the pixel readout R0 is extracted by readingfrom the pixel memory with a sensory memory, thenthe pixel readout interacts with the object memory and a setof learnable object queries through bottom-up foreground-background masked cross attention.Next, the obtainedhigh-level object query representation communicates backwith the pixel readout through top-down cross attention.The output pixel readout Rl and object queries Xl are sentto the next object transformer block. The final pixel readoutwill be combined with multi-scale features passed from skipconnections for computing the output mask in the decoder.Cutie enriches pixel features with object-level semantics ina bidirectional fashion, hence is more robust to distractions . Examples of generated pretraining data and motion blur. Left: binary mask generated from the valid set and test set of MOSE.Middle: binary mask generated from COCO, the masks of different classes are merged into one mask. Right: example of motion blur inthe horizontal direction.",
  ". Data augmentation": "Like most state-of-the-art VOS methods, Cutie also adoptsa two-stage training paradigm. The first stage pretraininguses short video sequences generated from static images.Then main training is performed using VOS datasets in thesecond stage. However, the original Cutie fails to performwell when similar objects move in close proximity or suffersfrom serious motion blur.To solve the above problems, we conduct data augmen-tation to enhance the training of Cutie. First, we employthe universal image segmentation model Mask2Former tosegment instance targets from the valid set and test set ofMOSE. As shown in the left column of , the seg-mented small objects represent typical object appearancesin MOSE, which is helpful for learning the semantics ofdiverse objects in advance. Meanwhile, as shown in themiddle column of , we convert the instance annota-tions of COCO into independent binary masks. Here weselect object classes such as human, animal and vehiclethat frequently occur in MOSE to reduce discrepancy be-tween two data distributions.The acquired data is usedas extra pretraining data to enable more robust semanticsand improve discrimination ability against diverse objectsof MOSE. Second, with the observation that motion blur is asignificant challenge, we add motion blur with random ker-nel sizes and angles to both the pretraining and main train-ing stages. An example of motion blur is shown in the rightcolumn of . The proposed data augmentation aims attraining towards better robustness and generalization.",
  ". Inference time operations": "TTA.We use two kinds of TTA: flipping and multi-scaledata enhancement.We only conduct horizontal flippingsince experiments show flipping in other directions is detri-mental to performance. In addition, we inference results onthe test set under three maximum shorter side resolutions:600p, 720p and 800p. The multi-scale results are then aver-aged to get the final result.Memory strategy. We find in experiments that larger mem-ory banks and shorter memory intervals lead to better per-formance.Therefore, we adjust the maximum memoryframes Tmax to 18 and the memory interval to 1.",
  ". Implementation details": "Data.ECSSD,DUTS,FSS-1000,HRSOD and BIG are used as image segmen-tatio datasets for pretraining. Besides, we generate 66823image-mask pairs from the valid and test set of MOSEusing Mask2Former and 89490 image-mask pairs fromCOCO, and add them into the data for pretraining.Formain training, we mix the training sets of DAVIS-2017,YouTubeVOS-2019,BURST,OVISandMOSE.Training.The parameter is updated using AdamWwith a learning rate of 0.0001, a batch size of 16, and aweight decay of 0.001. Pretraining is carried out for 80Kiterations with a crop size of 384384 and no learning ratedecay. Main training is carried out for 175K iterations, with",
  ". Ablation study": "We conduct an ablation study to verify the effectiveness ofdifferent components in our method. Specifically, we takethe original Cutie as the baseline, then we incorporate thedata augmentation, TTA and memory strategy into the base-line and design two ablation variants. From the quantitativeresults in Tab. 2, data augmentation through instance seg-mentation and motion blur improves the J &F for about0.0184. Test time augmentation and memory strategy bringthe most significant improvement, with about 0.03 increase",
  ". Conclusion": "In this paper, we propose a method for complex video ob-ject segmentation. Specifically, we take Cutie as the base-line model, and conduct data augmentation to enhance fea-ture learning through Mask2Former and motion blur. TTAand memory strategy are employed in the inference stageto improve the segmentation results. Our method achievedthe 2nd place on the MOSE track of the PVUW Challenge2024 with 0.8345 J &F.",
  "the IEEE/CVF International Conference on Computer Vi-sion, pages 2022420234, 2023. 1, 3": "Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, andChen Change Loy.Mevis: A large-scale benchmark forvideo segmentation with motion expressions. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 26942703, 2023. 1 Ziyu Zhang, Sanja Fidler, and Raquel Urtasun.Instance-level segmentation for autonomous driving with deepdensely connected mrfs. In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, pages669677, 2016. 1 Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modularinteractive video object segmentation: Interaction-to-mask,propagation and difference-aware fusion. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 55595568, 2021. 1 Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khu-rana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst:A benchmark for unifying object recognition, segmentationand tracking in video.In Proceedings of the IEEE/CVFwinter conference on applications of computer vision, pages16741683, 2023. 1, 3 Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexan-der Schwing, and Joon-Young Lee.Tracking anythingwith decoupled video segmentation. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 13161326, 2023. 1 Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset,Laura Leal-Taixe, Daniel Cremers, and Luc Van Gool. One-shot video object segmentation.In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 221230, 2017. 1 Tianfei Zhou, Wenguan Wang, Yazhou Yao, and JianbingShen. Target-aware adaptive tracking for unsupervised videoobject segmentation. In The 2020 DAVIS Challenge on VideoObject Segmentation-CVPR Workshops, volume 3, 2020. Federico Perazzi, Anna Khoreva, Rodrigo Benenson, BerntSchiele, and Alexander Sorkine-Hornung. Learning videoobject segmentation from static images. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 26632672, 2017. Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusion-seg: Learning to combine motion and appearance for fullyautomatic segmentation of generic objects in videos. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 36643673, 2017. 1 Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon JooKim. Video object segmentation using space-time memorynetworks.In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 92269235, 2019. 1 Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethink-ing space-time networks with improved memory coveragefor efficient video object segmentation. Advances in NeuralInformation Processing Systems, 34:1178111794, 2021. 1",
  "Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, YuchenLiang, Jianchao Yang, and Thomas Huang.Youtube-vos:A large-scale video object segmentation benchmark. arXivpreprint arXiv:1809.03327, 2018. 2, 3": "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-der Kirillov, and Rohit Girdhar.Masked-attention masktransformer for universal image segmentation. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 12901299, 2022. 2 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 2",
  "Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia.Hierar-chical image saliency detection on extended cssd.IEEEtransactions on pattern analysis and machine intelligence,38(4):717729, 2015. 3": "Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,Dong Wang, Baocai Yin, and Xiang Ruan. Learning to de-tect salient objects with image-level supervision. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 136145, 2017. 3 Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, andChi-Keung Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages28692878, 2020. 3 Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, andHuchuan Lu. Towards high-resolution salient object detec-tion. In Proceedings of the IEEE/CVF international confer-ence on computer vision, pages 72347243, 2019. 3 Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-KeungTang.Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 88908899, 2020. 3 Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, XiaoyuLiu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HSTorr, and Song Bai. Occluded video instance segmentation:A benchmark.International Journal of Computer Vision,130(8):20222039, 2022. 3"
}