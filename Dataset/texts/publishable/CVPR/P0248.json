{
  "Abstract": "Personalized Federated Learning (PFL) is proposed tofind the greatest personalized models for each client. Toavoid the central failure and communication bottleneck inthe server-based FL, we concentrate on the DecentralizedPersonalized Federated Learning (DPFL) that performsdistributed model training in a Peer-to-Peer (P2P) man-ner. Most personalized works in DPFL are based on undi-rected and symmetric topologies, however, the data, com-putation and communication resources heterogeneity resultin large variances in the personalized models, which leadthe undirected aggregation to suboptimal personalized per-formance and unguaranteed convergence. To address theseissues, we propose a directed collaboration DPFL frame-work by incorporating stochastic gradient push and partialmodel personalized, called Decentralized Federated PartialGradient Push (DFedPGP). It personalizes the linear clas-sifier in the modern deep model to customize the local solu-tion and learns a consensus representation in a fully de-centralized manner.Clients only share gradients with asubset of neighbors based on the directed and asymmetrictopologies, which guarantees flexible choices for resourceefficiency and better convergence. Theoretically, we showthat the proposed DFedPGP achieves a superior conver-gence rate of O( 1 T ) in the general non-convex setting,and prove the tighter connectivity among clients will speedup the convergence. The proposed method achieves state-of-the-art (SOTA) accuracy in both data and computationheterogeneity scenarios, demonstrating the efficiency of thedirected collaboration and partial gradient push.",
  ")": ". An overview of the DFedPGP with a directed graph.We take Client 1 as an example. It pushes the shared parame-ters ptj,1, ut+1/21and bias information ptj,1, t1 to its out-neighbors(Client 2, 3); pulls the shared parameters pt1,j, ut+1/2jand bias in-formation pt1,j, tj from its in-neighbors (Client 3, 4). cal Federated Learning (FL) . The existing PFL algo-rithm can be categorized into two branches in terms of theexistence of the centralized server (i.e., Centralized Person-alized Federated Learning (CPFL) and De-centralized Personalized Federated Learning (DPFL) ). The challenges of centralized communication bot-tleneck or central failure may incur low communication ef-ficiency or system crash in the federated processing. Thus,we focus on the DPFL, which allows edge clients to com-municate with each other in a peer-to-peer manner, aimingto reduce the communication column of the busiest servernode and embrace peer-to-peer communication for fasterconvergence. In decentralized FL, clients usually followan undirected and symmetric communication topology toreach a consensus model , which means if oneclient receives neighbors models, it sends its model back.",
  "arXiv:2405.17876v1 [cs.LG] 28 May 2024": "In order to satisfy the unique needs of individual clients,most existing works in PFL carefully designed the relation-ships between the global model and personalized models tofit the local data distribution via different techniques, suchas parameter decoupling , knowledge distillation, multi-task learning , model interpola-tion and clustering . These techniques canalso be adopted to improve the personalized performance inDPFL . However, the heterogeneity among clientsexists not only in local data distribution but also in the com-munication power and computation resources . Thepower level of the wireless channel among clients may bedifferent and time-varying in communication networks, andsome clients may get offline occasionally without sendingmessages to their neighbors. These result in long-term waitsor incidents of deadlock for their neighbors and alsolead to poor convergence for the whole system. Besides,there is no reason to expect that the exchanged models aretrained at the same convergence level due to the heteroge-neous computation resources. Clients may receive exces-sive poor-performing models which can not help their train-ing and degrade the personalized performance. To tackle the challenges above, we propose a DPFLframework with a directed communication topology, termedDFedPGP, which incorporates the partial model personal-ization and stochastic gradient push to boost the personal-ized performance of the heterogeneous clients. Both par-tial model personalization and stochastic gradient push con-tribute to speeding up the convergence and reducing thecommunication resources to reach an ideal performance.Instead of exchanging the full model with their undirectedneighbors, we decouple the model as a mixture of a sharedfeature representation part and a private linear classifier partand only push the shared partial gradients to the directedout-neighbors (as depicted in ). Specifically, theproposed method consists of three steps: (1) pull the sharedpartial gradient and the bias weights from in-neighbors;(2) local update the personalized linear classifier and theshared feature representation alternately with the de-biasedparameters; (3) push the updated shared gradients and thebias information to out-neighbors. In-neighbors and out-neighbors are the in-coming and out-coming links for eachclient here. Partial gradient push makes the personalizedinformation well stored in the private linear classifier, re-ducing communication costs as well as protecting clientsprivacy. Moreover, directed contact allows clients to choosetheir neighbors flexibly, meaning that the shared part modelhas a larger feature search space among clients, which guar-antees better performance in a computation-constrained andcommunication-constrained scenario.",
  "T ) in the general": "non-convex setting.Empirically, we conduct extensiveexperiments on the CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets in non-IID settings with different datapartitions. Experimental results confirm that the proposedalgorithm can achieve competitive performance relative toother SOTA baselines (see ) in PFL.In summary, our main contributions are four-fold: We introduce the directed Push-sum optimization to PFL,which allows clients to choose their neighbors flexiblyand guarantees a larger feature search space in a com-munication, and computation heterogeneity scenario.",
  ". Related Work": "Personalized Federated Learning (PFL). The PFL aimsto produce the greatest personalized models for each clientby model decoupling , knowledge distillation , multi-task learning , model interpolation and clustering . More details can be referredto in . In this paper, we mainly focus on the modeldecoupling methods, which divide the model into a globalshared part and a personalized part, also called partial per-sonalization. Existing partial personalized works in CFLachieve better performance than full model personalizationwith fewer shared parameters.FedPer , FedRep and FedBABU all use one global feature representationwith many local classifiers but with differences in the rela-tionship between the shared representation and the privatelinear parts. Fed-RoD simultaneously trains a globalfull model and many private classifiers with both class-balanced loss and empirical loss. Theoretically, FedSim andFedAlt provide the convergence analyses of both algo-rithms in the general non-convex setting, while FedAvg-Pand Scaffold-P improve the existing results in . Decentralized Federated Learning (DFL). Due to thecomputation and communication resources heterogeneityamong clients, DFL has been an encouraging field in recentyears , where clients only connect with theirneighbors through peer-to-peer communication. We discussthe PFL methods in DFL considering multi-step local iter-ations.Specifically, DFedAvgM applies multiple localiterations with SGD and the quantization method to reducethe communication cost. Dis-PFL customizes the per- sonalized model and pruned mask for each client to speedup the personalized convergence. KD-PDFL leveragesthe knowledge distillation technique to empower each de-vice to discern statistical distances between local models.ARDM presents lower bounds on the communicationand local computation costs for this personalized FL formu-lation in a peer-to-peer manner. Push-sum over Directed Graphs.Push-sum optimizeris proposed to solve the asymmetric optimization prob-lems over (time-varying) directed graphs. The first Push-sum study in discusses gossip-type problems in di-rected graphs. PS-DDA extends this method to a de-centralized scenario and proves the convergence in a con-vex set. More optimization analysis can be referred to in. As an effective optimizer, Push-sum andits variants have been applied to various machine learning(ML) tasks . For example, SGP com-bines Push-sum with stochastic gradient updates and alsoproposes the Overlap SGP, allowing overlaps of communi-cation and computation to hide communication overhead.Quantized Push-sum quantizes the Push-sum basedalgorithm over directed graphs to tackle the heavy com-munication load. AsyNG proposes an asynchronousDFL system with directed communication by incorporatingneighbor selection and gradient push to boost the perfor-mance on non-IID local data and heterogeneous edge nodes. Nowadays, almost all PFL works suffer from the risk ofdeadlock from unstable communication channels and sub-optimal convergence from the different convergence-levelaggregations. Therefore, we try to propose a frameworkof partial gradient push based on a directed communica-tion graph for DPFL. It differs from the existing directedDFL methods in the exchange model part like OSGP,where clients focus on the whole parameters exchange forthe only consensus model. Also, we adopt multi-step localsteps and multiple alternate optimizations for better conver-gence, which leads to an unbiased gradient estimation andthe dependent stochastic variance between the shared partsand the personal parts. Therefore, the algorithm design andthe theoretical analysis are both unique and non-trivial.",
  "where F : Rd R is the global object function; wi Rd": "represents the parameters of the machine learning model inclient i; Fi is the loss function associated with the data sam-ple randomly drawn from the distribution Di in client i.To relieve the communication burden and improve per-sonalized performance, we consider the partial model per-sonalized version in DPFL. Specifically, the model param-eters are partitioned into two parts: the shared parame-ters u Rd0 and the personal parameters vi Rdi fori = 1, . . . , m.The full model on client i is denotedas wi = (ui, vi).To simplify presentation, we denoteV = (v1, . . . , vm) Rd1+...+dm, and then our goal is tosolve this problem:",
  "where u denotes the consensus model averaged with ui,i.e., u =1mmi=1 ui and we use u and v to representstochastic gradients with respect to ui and vi, respectively": "Directed Graph Network. In the decentralized networktopology, the communication between clients can be mod-eled as a directed connected graph G(t) = (N, V(t), E(t)),where N = {1, 2, . . . , m} represents the set of clients,V(t) N N represents the set of communication chan-nels and (i, j) E(t) represents a directed link from clienti to client j. Considering the time-varying directed graph,the link (i, j) E(t) (where i = j) does not imply the link(j, i) E(t). To further describe the directed communi-cation, we define N ini= {j|(j, i) E(t), j N} as thein-neighbor set and N outi= {j|(i, j) E(t), j N} asthe out-neighbor set, which are the sets with in-coming andout-coming links into node i separately.Most works in DPFL assume the communication isbased on a time-varying undirected graph, which satisfiesN ini= N outiand the link (i, j) E(t) (where i = j) mustbe equal to the link (j, i) E(t). But in reality, the undi-rected communication graph requires high attention in theimplementation to avoid deadlocks. Directed communica-tion graph networks mitigate this issue by flexibly selectingneighbors within clients and exhibiting higher robustness interms of network communication quality.",
  "In this section, DFedPGP (see Algorithm 1) is proposed tosolve the problem (2) in a fully decentralized manner": "Partial Model Personalization. Drawing from previousresearch on CNNs, layers that serve specific engineeringpurposes: lower convolution layers (close to the input) areresponsible for feature extraction, and the upper linear lay-ers (close to the output) focus on complex pattern recogni-tion . The feature extraction layers, mapping data fromhigh-dimensional feature space to an easily distinguishedlow space, are similar between clients but prone to over-fitting. The linear classification layers, which determine thedata category from the output of the previous feature ex-traction layers, are very different from data heterogeneityclients . Therefore, we set the feature extraction lay-ers as the shared parts and the linear classification layers asthe personalized parts as , and we leveragethe alternating update approach for model training in Line5-12, which aims to increase the compatibility between thepersonalized and the shared parts. Push-sum Based DFedPGP. The Push-sum method tosolve the decentralized optimization problem performs onelocal stochastic gradient descent update with one iterationof push-pull transmission at each client. It maintains fourvariables locally at each round t: the biased shared modelparts parameters uti, the private model parts parameters vti,the Push-sum bias weight ti, and the de-biased sharedmodel parts parameters zti = uti/ti. To save the overallcommunication, we introduce an idea from local SGD toperform a few epochs of local training before weights trans-mission. So at each round, every client performs a few localSGD steps in Lines 512 followed by one step of push-pulltransmission in Lines 1417. Notably, the local gradient iscalculated at the de-biased parameters zti in line 6 and theyare then used to be updated in Line 10. The push-pull trans-mission includes the biased shared model parameters uti andthe Push-sum bias weight ti. Directed Communication Graph. We set the mixing ma-trix P t to describe the communication topology at eachround t. DFedPGP can be adapted to various communi-cation topologies such as time-varying, asymmetric, andsparse networks. We used the time-varying, asymmetricnetwork here to encounter the limited communication band-width.Clients only need to know the outgoing mixingweights at each communication round and can indepen-dently choose the mixing weights from the other clients inthe network. In this work, we introduce a simple yet effec-tive random client selection method that satisfies our theory() and the limited communication bandwidth in theexperiments ().",
  ". Assumption": "Assumption 1 (B-bounded Connectivity ). The time-varying graph (i.e., the communication topology) is B-bounded strongly connected to ensure the convergence ofmodel training . There exists a window size B 1 1such that the graph union l+B1k=lG(k)(l = 0, 1, 2, ) isstrongly connected. Note that if B = 1, each instance ofgraph G(k) is strongly connected at global iteration k.",
  ". Challenge and Proof": "Challenges of Convergence Analysis. (1) Compared to theclassical Push-sum based method like SGP, ut,ki ut,0iafter multiple local iterations and alternately update is notan unbiased estimate of Fi(uti). Multiple local iterationanalyses are non-trivial; (2) In contrast to the symmetrictopology, DFedPGP communicates with its clients basedon an asymmetric topology, resulting in j pti,j = 1. Asa consequence, each client needs to maintain a Push-sumweight ti to de-bias the model parameters; (3) To real-ize better-personalized performance, we need to analyze theconvergence in a partial model personalized way, where theshared part u is updated with gradient pushing and pullingwhile the personalized part v is updated with local SGD sep-arately. Now, we present the rigorous convergence analysisof DFedPGP as follows.",
  "(5)": "Remark 1. Corollary 1 provides explicit insights into howvarious key factors affect the convergence of DFedPGP.Specifically, the convergence analysis illustrates that thelarge values of the gradient variance 2u, 2v, 2g and gra-dient bounded B lead to slower convergence. It also showsthat more local update steps Ku accelerate the convergence,quantitatively justifying the benefit of exploiting more localupdates in the algorithm. Also, the smoothness of local lossfunctions such as Lu, Lv, and Lvu, have a significant influ-ence on the convergence bound.",
  "Remark 2. As the definition in , the q in Corollary 1 canbe explicitly expressed as q = (1 aB)1": "B+1 , where isthe diameter of the communication network, B is the samedefined in Assumption 1, and a < 1 is a constant. Note thatthe bound will be tighter as q decreases, which means thenetwork connectivity improves and clients exchange param-eters with more neighbors in the communication progress.Moreover, the connectivity constant C decreases as the con-nectivity of the communication network improves, whichleads to the same conclusion as q. More details about thecommunication constant can be found in Lemma 3.",
  ". Experiment Setup": "Dataset and Data Partition. We evaluate the performanceon CIFAR-10, CIFAR-100 , and Tiny-ImageNet datasets in the Dirichlet distribution and Pathological distri-bution, where CIFAR-10 and CIFAR-100 are two real-lifeimage classification datasets with total 10 and 100 classes.Experiments on the Tiny-ImageNet dataset are placed inAppendix C.3 due to the limited space. We partition thetraining and testing data according to the same Dirichletdistribution Dir() such as = 0.1 and = 0.3 for eachclient. The smaller the is, the more heterogeneous thesetting is. Meanwhile, for each client, we sample 2 and 5classes from a total of 10 classes on CIFAR-10, and 5 and 10classes from a total of 100 classes on CIFAR-100, respec-tively . The number of sampling classes is representedas c in and the fewer classes each client owns, themore heterogeneous the setting is. Baselines and Backbone. We compare the proposed meth-ods with the SOTA baselines PFL. For instance, Local isthe simplest method where each client only conducts train-ing on their own data without communicating with other clients. Federated learning methods include FedAvg ,FedPer , FedRep , FedBABU and Ditto .For DFL methods, we take DFedAvgM , Dis-PFL and OSGP as our baselines. All methods are evaluatedon ResNet-18 and replace the batch normalization withthe group normalization followed by to avoidunstable performance. For the partial PFL methods, we setthe lower linear classifier layers as the personal part respon-sible for complex pattern recognition, and the rest upperrepresentation layer as the shared layers focusing on featureextraction. Note that we compare the personal test accuracyfor all methods since our goal is to solve PFL. Implementation Details. We keep the same experimentsetting for all baselines and perform 500 communicationrounds with 100 clients. The client sampling radio is 0.1in CFL, while each client communicates with 10 neighborsin PFL accordingly. The batch size is 128. For DFedPGP,we train the shared part for 5 epochs per round as the sameas other baselines, and train 1 epoch for the personal partto align the shared part and save the computation consump-tion. We set SGD as the base optimizer for all methodswith a learning rate u = 0.1 to update the model parame-ters and the learning rate decreasing by 0.99 exponentially.All methods are set with a decay rate of 0.005 and a localmomentum of 0.9. We report the mean performance with3 different random seeds and more details of the baselinemethods can be found in Appendix C.1.",
  ". Performance Evaluation": "Comparison with the Baselines. As shown in and, the proposed DFedPGP outperforms other base-line methods with the best stability and better performancein both different datasets and different data heterogeneityscenarios. Specifically, on the CIFAR-10 dataset, DFedPGPachieves 86.50% on the Directlet-0.3 setups, 1.11% aheadof the best-comparing method FedRep.On the CIFAR- Commincation round T 0.60 0.65 0.70 0.75 0.80 0.85 0.90",
  ". Test accuracy on CIFAR-10 (first line) and CIFAR-100 (second line) with heterogenous data partitions. With limited pages, weonly show the training progress of the typical methods": "100 dataset, DFedPGP achieves at least 2.60% and 1.11%improvement from the other baselines on the Directlet-0.3and Pathological-10 settings.The communication basedon the directed graph allows clients to choose their in-neighbors and out-neighbors flexibly, guaranteeing that theycan choose useful information for their local training. Comparison on Heterogeneous Setting. We discuss twodata heterogeneities, Dirichlet distribution and Pathologi-cal distribution in , and prove the effectiveness androbustness of the DFedPGP. In PFL tasks, since the localtraining cant cater for all classes inside clients, the accu-racy decreases with the heterogeneity decreasing.1Onthe CIFAR-10 dataset, when the heterogeneity decreasesfrom 0.1 to 0.3 in Directlet distribution, FedRep dropsfrom 88.78% to 84.50%, while DFedPGP drops about3.24% to 85.61%, meaning its stronger stability for severalheterogeneous settings. On the Pathological distribution,DFedPGP beat the best-compared baselines over 1.11% onthe CIFAR-100 dataset with only 10 categories per client,which confirm that the proposed methods could achieve bet-ter performance in the strong heterogeneity.",
  "Comparison on the Convergence Speed with Baselines.We show the convergence speed via the learning curves ofthe compared methods in and . DFedPGP": "1Generally, the higher data heterogeneity means a greater differencebetween local data distribution. But in extreme data heterogeneity PFLtasks, the higher heterogeneity means it owns fewer data classes locally,which makes the classification task easier and clients will achieve betterperformance. For example, in the Pat-2 setting, the local binary classifica-tion task is easier than the five classification tasks in the Pat-5 setting, sothe average test performance in Pat-2 is better than that in Pat-5. The samephenomenon can be seen in most PFL works . achieves the fastest convergence speed among the compar-ison methods, which benefits from the direct partial modeltransmission and alternate update. For example, DFedPGPis almost twice as fast as the other methods in Dirichlet-0.3on CIFAR-10 and CIFAR-100 settings. In comparison withthe CFL methods, directly learning the neighbors featurerepresentation in DFL can speed up the convergence ratefor personalized problems. Notably, we target the settingwhere the busiest nodes communication bandwidth is re-stricted for fairness when compared with the CFL methods. Comparison on Computation Resources Heterogeneity.In reality, the federating process often involves hetero-geneous devices, which means the shared parameters aretrained at different convergence levels. We follow to",
  ". Ablation study. (a) Effect of the number of neighboringclients. (b) Effect of the number of participated clients": "divide 100 clients into 5 parts and transmit their parametersafter 1, 2, 3, 4 and 5 local epochs to simulate the differentcomputation capabilities of each device . shows the comparison among PFL methods un-der a computation heterogeneous setting on the CIFAR10dataset with Dirichlet-0.3 distribution. DFedPGP achievesthe best compared with the other baselines, indicating thatthe partial gradient push can alleviate the effect of the dif-ferent convergence level aggregation. Another interestingfinding is that FedRep is the best PFL method encounteringthe computation heterogeneous challenge in CFL, indicat-ing that keeping the classifiers locally and updating the pri-vate and shared parts alternately is an effective way to solvethe computation heterogeneity problem.",
  ". Ablation Study": "Number of Neighboring Clients. We conduct experimentson the convergence performance under different neighborparticipation numbers of {2, 5, 10, 20, 40} on CIFAR-10with Dir-0.3 distribution. As shown in a, the high-est personalized performance is achieved when the partici-pation number is set to 40, which indicates that with moreclients exchanging their information, a quicker convergencespeed will be achieved, aligning with our insight. Notably,the proposed DFedPGP can realize a stable convergenceeven when transmitting information to only 2 neighbors. Number of Participated Clients. As depicted in b, we compare the personalized performance between dif-ferent numbers of client participation of {5, 10, 20, 50,100, 200} on the CIFAR-10 dataset with Dirichlet-0.3 dis-tribution under the same hyper-parameters. Compared withlarger participated clients {50, 100, 200}, the smaller par-ticipated clients {5, 10} can achieve better test accuracy andconvergence as the number of local data increases.",
  "DFedAvgM82.4990.23DFedAvgM-P84.6990.90OSGP83.1490.72DFedPGP85.6191.26": "fect of partial personalization and directed communicationwith different data heterogeneity on the CIFAR-10 dataset.From , DFedPGP achieves the best in both Dirichlet-0.3 and Pathological-2. In the comparison of partial per-sonalization, DFedAvgM-P and DfedPGP outperform theirfull personalization versions DFedAvgM and OSGP by agreat margin separately. In directed communication com-parison, DfedPGP and OSGP outperform their undirectedversions DFedAvgM-P and DFedAvgM, respectively. Fromthe ablation study, both partial personalization and directedcommunication have a great influence on decentralized andpersonalized performance. Randomly choosing clients in-neighbors and out-neighbors in directed graphs means thatthe shared part model has a larger feature search spaceamong clients, compared with the undirected graphs. In-tuitively, this increases the involved clients in one commu-nication round and enhances communication efficiency.",
  ". Conclusion": "In this paper, we propose a novel method DFedPGP forPFL, which simultaneously guarantees robust communica-tion and better personalized performance with convergenceguarantee via partial gradient push in a directed commu-nication graph. The directed collaboration allows clientsto choose their corporate neighbors flexibly, which guaran-tees effective aggregation and learning under data and de-vice heterogeneity scenarios. For theoretical findings, wepresent the personalized convergence rate of O(1/",
  "Michael Blot, David Picard, Matthieu Cord, and NicolasThome. Gossip training for deep learning. arXiv preprintarXiv:1611.09726, 2016. 12": "Zheng Chai, Hannan Fayyaz, Zeshan Fayyaz, Ali Anwar, YiZhou, Nathalie Baracaldo, Heiko Ludwig, and Yue Cheng.Towards taming the resource and data heterogeneity in fed-erated learning. In 2019 USENIX conference on operationalmachine learning (OpML 19), pages 1921, 2019. 2 Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zi-tao Li, Yaliang Li, Bolin Ding, and Jingren Zhou. Fs-real:Towards real-world cross-device federated learning. arXivpreprint arXiv:2303.13363, 2023.",
  "Yiming Chen,Liyuan Cao,Kun Yuan,and ZaiwenWen. Sharper convergence guarantees for federated learn-ing with partial model personalization.arXiv preprintarXiv:2309.17409, 2023. 2": "Liam Collins, Hamed Hassani, Aryan Mokhtari, and SanjayShakkottai. Exploiting shared representations for personal-ized federated learning. In International Conference on Ma-chine Learning, pages 20892099. PMLR, 2021. 2, 4, 6, 13 Rong Dai, Li Shen, Fengxiang He, Xinmei Tian, andDacheng Tao. Dispfl: Towards communication-efficient per-sonalized federated learning via decentralized sparse train-ing.In International Conference on Machine Learning,ICML, pages 45874604. PMLR, 2022. 1, 2, 6, 7, 13",
  "Chaoyang He, Murali Annavaram, and Salman Avestimehr.Group knowledge transfer: Federated learning of large cnnsat the edge. Advances in Neural Information Processing Sys-tems, 33:1406814080, 2020. 2": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 6 Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and PhillipGibbons. The non-iid data quagmire of decentralized ma-chine learning.In International Conference on MachineLearning, pages 43874398. PMLR, 2020. 12 Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang,Jiangchuan Liu, Jian Pei, and Yong Zhang.Personalizedcross-silo federated learning on non-iid data. In Proceed-ings of the AAAI Conference on Artificial Intelligence, pages78657873, 2021. 1, 2, 7",
  "Eunjeong Jeong and Marios Kountouris. Personalized decen-tralized federated learning with knowledge distillation. arXivpreprint arXiv:2302.12156, 2023. 3": "Jiawen Kang, Dongdong Ye, Jiangtian Nie, Jiang Xiao, Xi-anjun Deng, Siming Wang, Zehui Xiong, Rong Yu, and DusitNiyato. Blockchain-based federated learning for industrialmetaverses: Incentive scheme with optimal aoi.In 2022IEEE International Conference on Blockchain (Blockchain),pages 7178. IEEE, 2022. 2 Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,Sashank Reddi, Sebastian Stich, and Ananda TheerthaSuresh. Scaffold: Stochastic controlled averaging for fed-erated learning.In International Conference on MachineLearning, pages 51325143. PMLR, 2020. 5 David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-based computation of aggregate information. In 44th An-nual IEEE Symposium on Foundations of Computer Science,2003. Proceedings., pages 482491. IEEE, 2003. 3, 12",
  "Ya Le and Xuan Yang.Tiny imagenet visual recognitionchallenge. CS 231N, 7(7):3, 2015. 6": "Boyue Li, Shicong Cen, Yuxin Chen, and Yuejie Chi.Communication-efficient distributed optimization in net-works with gradient tracking and variance reduction. Journalof Machine Learning Research, JMLR, pages 180:1180:51,2020. 12 Bo Li, Mikkel N Schmidt, Tommy S Alstrm, and Sebas-tian U Stich. On the effectiveness of partial variance reduc-tion in federated learning with heterogeneous data. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 39643973, 2023. 1, 4",
  "Qinglun Li, Miao Zhang, Nan Yin, Quanjun Yin, and LiShen.Asymmetrically decentralized federated learning.arXiv preprint arXiv:2310.05093, 2023. 3, 5": "Shuangtong Li, Tianyi Zhou, Xinmei Tian, and DachengTao.Learning to collaborate in decentralized learning ofpersonalized models. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages97669775, 2022. 2, 12 Tian Li, Shengyuan Hu, Ahmad Beirami, and VirginiaSmith.Ditto: Fair and robust federated learning throughpersonalization.In International Conference on MachineLearning, pages 63576368. PMLR, 2021. 6, 13 Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, WeiZhang, and Ji Liu. Can decentralized algorithms outperformcentralized algorithms? a case study for decentralized paral-lel stochastic gradient descent. In Advances in Neural Infor-mation Processing Systems, pages 53305340, 2017. 12 Tao Lin, Lingjing Kong, Sebastian U Stich, and MartinJaggi. Ensemble distillation for robust model fusion in fed-erated learning. Advances in Neural Information ProcessingSystems, 33:23512363, 2020. 1, 2",
  "Herbert Robbins and Sutton Monro. A stochastic approx-imation method. Annals of Mathematical Statistics, 22(3):400407, 1951. 6": "AbdurakhmonSadiev,EkaterinaBorodich,AleksandrBeznosikov, Darina Dvinskikh, Saveliy Chezhegov, RachaelTappenden, Martin Takac, and Alexander Gasnikov.De-centralized personalized federated learning: Lower boundsand optimal algorithm for all personalization modes. EUROJournal on Computational Optimization, 10:100041, 2022.3 Felix Sattler, Klaus-Robert Muller, and Wojciech Samek.Clustered federated learning:Model-agnostic distributedmultitask optimization under privacy constraints.IEEEtransactions on neural networks and learning systems, 32(8):37103722, 2020. 2",
  "Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.Towards personalized federated learning. IEEE Transactionson Neural Networks and Learning Systems, 2022. 2": "Konstantinos I Tsianos, Sean Lawlor, and Michael G Rab-bat. Push-sum distributed dual averaging for convex opti-mization. In 2012 ieee 51st ieee conference on decision andcontrol (cdc), pages 54535458. IEEE, 2012. 3 Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, andH Vincent Poor. Tackling the objective inconsistency prob-lem in heterogeneous federated optimization.Advancesin neural information processing systems, 33:76117623,2020. 8",
  "Mang Ye, Xiuwen Fang, Bo Du, Pong C Yuen, and DachengTao. Heterogeneous federated learning: State-of-the-art andresearch challenges. ACM Computing Surveys, 56(3):144,2023. 1": "Hao Yu, Rong Jin, and Sen Yang. On the linear speedupanalysis of communication efficient momentum sgd for dis-tributed non-convex optimization. In International Confer-ence on Machine Learning, pages 71847193. PMLR, 2019.2 Zhengxin Yu, Jia Hu, Geyong Min, Han Xu, and Jed Mills.Proactive content caching for internet-of-vehicles based onpeer-to-peer federated learning. In 2020 IEEE 26th Interna-tional Conference on Parallel and Distributed Systems (IC-PADS), pages 601608. IEEE, 2020. 12",
  "Supplementary Material for Decentralized Directed Collaboration for Personalized Federated Learning": "In this part, we provide supplementary materials including more introduction to the related works, experimental details andresults, and the proof of the main theorem. Appendix A: More details in the related works. Appendix B: More details in the client selection. Appendix C: More details in the experiments. Appendix D: Proof of the theoretical analysis.",
  "A. More Details in the Related Works": "Decentralized/Distributed Training. Decentralized/Distributed Training, which allows edge clients to communicate witheach other in a peer-to-peer manner, is an encouraging field that shares several benefits: (1) guarantees collaborative learningthrough local computation and the exchange of model parameters; (2) is low for feeding the models of adjacent clients, gener-ating a more intelligent private model; (3) avoids central failure in the collaborative system. Thus, Decentralized/DistributedTraining has been applied in many fields: (1) Healthcare , favoring the decentralization of clinical records and collab-orative diagnosis; (2) Mobile Services , decreasing response times and increasing the bandwidth of constraints devices;(3) Vehicles , ensuring high mobility and local storage management.Since the prototype of DFL (fully decentralized federated learning ) was proposed, it has been a promising approachto save communication costs as the compromise of CFL. By combining SGD and gossip, early work achieved decentralizedtraining and convergence in . D-PSGD is the classic decentralized parallel SGD method. FastMix investigatesthe advantage of increasing the frequency of local communications within a network topology, which establishes the optimalcomputational complexity and near-optimal communication complexity. DeEPCA integrates FastMix into a decen-tralized PCA algorithm to accelerate the training process. DeLi-CoCo performs multiple compression gossip steps ineach iteration for fast convergence with arbitrary communication compression. Network-DANE uses multiple gossipsteps and generalizes DANE to decentralized scenarios. QG-DSGDm modifies the momentum term of decentralizedSGD (DSGD) to be adaptive to heterogeneous data, while SkewScout replaces batch norm with layer norm. Meta-L2C dynamically updates the mixing weights based on meta-learning and learns a sparse topology to reduce communicationcosts. The work in provides the topology-aware generalization analysis for DSGD, they explore the impact of variouscommunication topologies on the generalizability.",
  "B. More details in the client selection": "Push sum based directed distributed averaging. The initial Push sum algorithm considers the averaged consensus1/n ni=1 y0i of all clients. Let y0i Rd be a vector at client i and typical gossip iterations forms yt+1i= nj=1 pti,jytj,where P t Rnn is the mixing matrix. Inspired by the Markov chains , the mixing matrices P t are designed to becolumn stochastic (each column must sum to 1). So the gossip iterations converge to a limit yi= inj=1 y0j , where isthe ergodic limit of the chain. When the matrices P t are symmetric, it is straightforward to satisfy i = 1/n by defining P t doubly-stochastic (each row and each column must sum to 1). However, symmetric P t are hard to meet due to the unstablecommunication in reality. The Push sum algorithm adds one additional scalar parameter wti to achieve i = 1/n under thecolumn-stochastic and asymmetric mixing matrices P t. The parameter is initialized to w0i = 1 for all i and updated usingthe same linear iteration, wt+1i= nj=1 pti,jwtj. It recovers the average of the initial vectors by computing the de-biased ratioyi /wi , and the scalar parameters converge to wi= inj=1 w0j.Directed random graph. We transfer the mixing matrices from column stochastic (all columns sum to 1) to row stochastic(all rows sum to 1), meaning that the clients can actively select the information they need rather than passively accept, whichis more beneficial for directed collaboration in the DPFL problem. In the experiments, each client pulls the shared parametersfrom its in-neighbors j N ini,t , and pulls a message from itself as well. Recall that each client i can choose its mixingweights (ith row of P t) independently of the other clients. So in order to provide more flexible collaboration and closer tiesfor clients, we randomly choose the in-neighbors under the communication bandwidth limitation. We use uniform mixingweights for the pulled models here, meaning that clients assign uniform model weights to all neighbors. So assuming that",
  "/(n + 1),j N ini,t ;0,otherwise.(6)": "Undirected random graph. For the undirected DPFL methods (i.e. DFedAvgM and Dis-PFL), we use a time-varying andundirected random graph to represent the inter-client connectivity. Clients randomly choose their in-neighbors to pull theshared models and push a message in return. We adopt these graphs to be consistent with the experimental setup used in. So the mixing matrics in the undirected graph is a symmetric doubly-stochastic (each row and each columnmust sum to 1), which satisfies pti,j = ptj,i in Formula (6). Notably, the model communication bandwidth of in-neighbors inDPFL is strictly limited as the same as the busiest server in CPFL.",
  "C. More details in the experiments": "In this section, we provide more details of our experiments including datasets, baselines, and more extensive experimentalresults to compare the performance of the proposed DFedPGP against other baselines on the Tiny-ImageNet dataset. All ourexperiments are trained and tested on a single Nvidia RTX3090 GPU under the environment of Python 3.8.5, PyTorch 1.11.1,CUDA 11.6, and CUDNN 8.0.",
  "C.1. More Details about Baselines": "Local is the simplest method for personalized learning. It only trains the personalized model on the local data and does notcommunicate with other clients. For the fair competition, we train 5 epochs locally in each round.FedAvg is the most commonly discussed method in FL. It selects partial clients to perform local training on each datasetand then aggregates the trained models to update the global model. Actually, the local model in FedAvg is also the comparablepersonalized model for each client.FedPer proposes a model decoupling approach for PFL, with a consensus representation and many local classifiers, tocombat the ill effects of statistical heterogeneity. We set the linear layer as the personalized layer and the rest model as thebase layer. It follows FedAvgs training paradigm but only passes the base layer to the server and keeps the personalizedlayer locally.FedRep also proposes a personalized model decoupling framework like FedPer, but it fixes one part when updating theother. We follow the official implementation2 to train the head for 10 epochs with the body fixed, and then train the body for5 epochs with the head fixed.FedBABU is also a model decoupling method that achieves good personalization via fine-tuning from a good sharedrepresentation base layer. Different from FedPer and FedRep, FedBABU only updates the base layer with the personalizedlayer fixed and finally fine-tunes the whole model. Following the official implementation3, it fine-tunes 5 times in ourexperiments.Ditto achieves personalization via a trade-off between the global model and local objectives. It totally trains two modelson the local datasets, one for the global model (similarly aggregated as in FedAvg) with its local empirical risk, and onefor the personal model (kept locally) with both empirical risk and the proximal term towards the global model. We set theregularization parameters as 0.75.DFedAvgM is the decentralized FedAvg with momentum, in which clients only connect with their neighbors by anundirected graph. For each client, it first initials the local model with the received models then updates it on the local datasetswith a local stochastic gradient.OSGP is the directed version of DFedAvg, which allows clients to send the local models to their out-neighbors by adirected graph. It is regarded as a representative of a personalized baseline over directed communication.Dis-PFL employs personalized sparse masks to customize sparse local models in the PFL setting. Each client firstinitials the local model with the personalized sparse masks and updates it with empirical risk. Then filter out the parameterweights that have little influence on the gradient through cosine annealing pruning to obtain a new mask. Following theofficial implementation4, the sparsity of the local model is set to 0.5 for all clients.",
  "C.2. Datasets and Data Partition": "CIFAR-10/100 and Tiny-ImageNet are three basic datasets in the computer version study. As shown in , they are allcolorful images with different classes and different resolutions. We use two non-IID partition methods to split the trainingdata in our implementation. One is based on Dirichlet distribution on the label ratios to ensure data heterogeneity amongclients. The Dirichlet distribution defines the local dataset to obey a Dirichlet distribution (see in a), where a smaller means higher heterogeneity. Another assigns each client a limited number of categories, called Pathological distribution.Pathological distribution defines the local dataset to obey a uniform distribution of active categories c (see in b),where fewer categories mean higher heterogeneity. The distribution of the test datasets is the same as in training datasets. Werun 500 communication rounds for CIFAR-10, CIFAR-100, and 300 rounds for Tiny-ImageNet.",
  "C.3. More Experiments Results on Tiny ImageNet": "Comparison with the baselines. In and , we compare DFedPGP with other baselines on the Tiny-ImageNetwith different data distributions. The comparison shows that the proposed method has a competitive performance, especiallyunder higher heterogeneity, e.g. Pathological-10. Specifically in the Pathological-10 setting, DFedPGP achieves 49.16%, atleast 1.81% and 7.08% improvement from the CFL methods and DFL methods. However, in the Dirichlet-0.3 setting, almostall the partial model personalized methods (i.e. FedPer, FedRep, DFedPGP except FedBABU) face a severe performancedegradation compared with the full model personalized methods (i.e. FedAvg, DFedAvgM, OSGP). This may account forthe low classification ability in partial model personalized methods without aggregation with neighbors in the multiple-imageclassification tasks, especially in the long-tail data distribution scenario (i.e. Dirichelet-0.3). The original intention of ourdesign is to build a great personalized model through partial model personalization training and directed collaboration withneighbors. So when the heterogeneity increases, our algorithms have a significant improvement. Commincation round T 0.100 0.125 0.150 0.175 0.200 0.225 0.250",
  "DFedPGP742.41 1081.96 544.83 534.87": "Convergence speed. We show the convergence speed of DFedPGP in and by reporting the number ofrounds required to achieve the target personalized accuracy (acc@) on Tiny-ImageNet. We set the algorithm that takes themost rounds to reach the target accuracy as 1.00, and find that the proposed DFedPGP achieves the fastest convergencespeed on average (3.51 on average) among the SOTA PFL algorithms. Direct communication guarantees flexible choiceof neighbors and closer ties between clients, which speeds up personalized convergence and achieves higher personalizedperformance for each client. Also, the partial model personalization and alternate updating mode will both bring a comparablegain to the convergence speed from the difference between DFedPGP and OSGP. Thus, our methods can efficiently train the"
}