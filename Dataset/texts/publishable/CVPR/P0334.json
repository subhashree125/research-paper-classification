{
  "Touch Pred.VisionTouch SampleVision": ". Tactile-augmented radiance fields. We capture a tactile-augmented radiance field (TaRF) from photos and sparsely sampledtouch probes. To do this, we register the captured visual and tactile signals into a shared 3D space, then train a diffusion model to imputetouch at other locations within the scene. Here, we visualize two touch probes and their (color coded) 3D positions in the scene. We alsoshow two touch signals estimated by the diffusion model. The touch signals were collected using a vision-based touch sensor thatrepresents the touch signals as images. Please see our project page for video results.",
  "Abstract": "We present a scene representation, which we call atactile-augmented radiance field (TaRF), that brings visionand touch into a shared 3D space.This representationcan be used to estimate the visual and tactile signals fora given 3D position within a scene. We capture a scenesTaRF from a collection of photos and sparsely sampledtouch probes. Our approach makes use of two insights: (i)common vision-based touch sensors are built on ordinarycameras and thus can be registered to images using meth-ods from multi-view geometry, and (ii) visually and struc-turally similar regions of a scene share the same tactile fea-tures. We use these insights to register touch signals to acaptured visual scene, and to train a conditional diffusionmodel that, provided with an RGB-D image rendered froma neural radiance field, generates its corresponding tactilesignal. To evaluate our approach, we collect a dataset ofTaRFs. This dataset contains more touch samples than pre-vious real-world datasets, and it provides spatially alignedvisual signals for each captured touch signal. We demon-strate the accuracy of our cross-modal generative modeland the utility of the captured visual-tactile data on sev-eral downstream tasks.Project page:",
  ". Introduction": "As humans, our ability to perceive the world relies cruciallyon cross-modal associations between sight and touch . Tactile sensing provides a detailed understanding ofmaterial properties and microgeometry, such as the intri-cate patterns of bumps on rough surfaces and the complexmotions that soft objects make when they deform.Thistype of understanding, which largely eludes todays com-puter vision models, is a critical component of applica-tions that require reasoning about physical contact, such asrobotic locomotion and manipula-tion , and methods that simulate the behav-ior of materials . In comparison to many other modalities, collecting tac-tile data is an expensive and tedious process, since it re-quires direct physical interaction with the environment. Arecent line of work has addressed this problem by havinghumans or robots probe the environment with touch sensors(see ). Early efforts have been focused on capturingthe properties of only a few objects either in simulation or in lab-controlled settings ,which may not fully convey the diversity of tactile signalsin natural environments. Other works have gone beyond a",
  "TaRF (Ours)19.3kFull sceneHuman": ". Dataset comparison. We present the number of realvisual-tactile pairs and whether such pairs are visually aligned,i.e., whether the visual image includes an occlusion-free view ofthe touched surface. YCB-Slide has real-world touch probes butsynthetic images rendered with CAD models of YCB objects on awhite background . lab setting and have collected touch from real scenes .However, existing datasets lack aligned visual and tactile in-formation, since the touch sensor and the person (or robot)that holds it often occlude large portions of the visual scene(). These datasets also contain only a sparse set oftouch signals for each scene, and it is not clear how the sam-pled touch signals relate to each other in 3D.In this work, we present a simple and low-cost procedureto capture quasi-dense, scene-level, and spatially-alignedvisual and touch data (). We call the resulting scenerepresentation a tactile-augmented radiance field (TaRF).We remove the need for robotic collection by leveraginga 3D scene representation (a NeRF ) to synthesize aview of the surface being touched, which results in spatiallyaligned visual-tactile data (). We collect this data bymounting a touch sensor to a camera with commonly avail-able materials (). To calibrate the pair of sensors, wetake advantage of the fact that popular vision-based touchsensors are built on ordinary cameras. Therelative pose between the vision and tactile sensors can thusbe estimated using traditional methods from multi-view ge-ometry, such as camera resectioning .We use this procedure to collect a large real-worlddataset of aligned visual-tactile data. With this dataset, wetrain a diffusion model to estimate touch at loca-tions not directly probed by a sensor. In contrast to the re-cent work of Zhong et al. , which also estimates touchfrom 3D NeRF geometry, we create scene-scale reconstruc-tions, we do not require robotic proprioception, and we usediffusion models . This enables us to obtain tactile dataat a much larger scale, and with considerably more diver-sity. Unlike previous visual-tactile diffusion work , wecondition the model on spatially aligned visual and depthinformation, enhancing the generated samples quality andtheir usefulness in downstream applications. After training,the diffusion model can be used to predict tactile informa-",
  "TaRF (Ours)": ". Visual-tactile examples. In contrast to the visual-tactiledata captured in previous work, our approach allows us to sampleunobstructed images that are spatially aligned with the touch sig-nal, from arbitrary 3D viewpoints using a NeRF. tion for novel positions in the scene. Analogous to quasi-dense stereo methods , the diffusion model effec-tively propagates sparse touch samples, obtained by prob-ing, to other visually and structurally similar regions of thescene.We evaluate our visual-tactile models ability to accu-rately perform cross-modal translation using a variety ofquality metrics. We also apply it to several downstreamtasks, including localizing a touch within a scene and un-derstanding material properties of the touched area. Ourexperiments suggest: Touch signals can be localized in 3D space by exploitingmulti-view geometry constraints between sight and touch.",
  ". Related Work": "Visual-tactile datasets.Previous work has either usedsimulators or robotic arms fordata generation. Our work is closely related to that of Zhonget al. , which uses a NeRF and captured touch data togenerate a tactile field for several small objects. They usethe proprioception of an expensive robot to spatially alignvision and touch. In contrast, we leverage the properties ofthe tactile sensor and novel view synthesis to use commonlyavailable material (a smartphone and a selfie stick) to alignvision and touch. This enables the collection of a larger,scene-level, and more diverse dataset, on which we train ahigher-capacity diffusion model (rather than a conditionalGAN). Like several previous works , we also collectscene-level data. In contrast to them, we spatially align thesignals by registering them in a unified 3D representation,thereby increasing the prediction power of the visual-tactilegenerative model.Capturing multimodal 3D scenes.Our work is relatedto methods that capture 3D visual reconstructions of spaces using RGB-D data and multimodal datasetsof paired 3D vision and language . Our work isalso related to recent methods that localize objects in NeRFsusing joint embeddings between images and language or by semantic segmentation . In contrast to languagesupervision, touch is tied to a precise position in a scene.3D touch sensing.A variety of works have studiedthe close relationship between geometry and touch, moti-vating our use of geometry in imputing touch.Johnsonet al. proposed vision-based touch sensing, andshowed that highly accurate depth can be estimated fromthe touch sensor using photometric stereo. Other work hasestimated object-scale 3D from touch . By contrast, wecombine sparse estimates of touch with quasi-dense tactilesignals estimated using generative models.Cross-modal prediction of touch from sight.Recentwork has trained generative models that predict touch fromimages.Li et al. used a GAN to predict touch forimages of a robotic arm, while Gao et al. appliedthem to objects collected on a turntable. Yang et al. used latent diffusion to predict touch from videos of hu-mans touching objects. Our goal is different from theseworks: we want to predict touch signals that are spatiallyaligned with a visual signal, to exploit scene-specific in-formation, and to use geometry. Thus, we use a differentarchitecture and conditioning signal, and fit our model toexamples from the same scenes at training and test time.Other work has learned joint embeddings between visionand touch .",
  ". Method": "We collect visual and tactile examples from a scene and reg-ister them together with a 3D visual reconstruction to build aTaRF. Specifically, we capture a NeRF F : (x, r) (c, )that maps a 3D point x = (x, y, z) and viewing directionr to its corresponding RGB color c and density .We associate to the visual representation a touch modelF : vt that generates the tactile signal that one wouldobtain by touching at the center of the image vt. In the fol-lowing, we explain how to estimate F and F and put theminto the same shared 3D space.",
  ". Capturing vision and touch signals": "Obtaining a visual 3D reconstruction.We build the vi-sual NeRF, F, closely following previous work . Ahuman data collector moves through a scene and records avideo, covering as much of the space as possible. We thenestimate camera pose using structure from motion andcreate a NeRF using off-the-shelf packages . Additionaldetails are provided in the supplement.Capturing and registering touch.We simultaneouslycollect tactile and visual signals by mounting a touch sensor",
  "Visual-TactileCorrespondences": ". Capturing setup. (a) We record paired vision and touchsignals using a camera attached to a touch sensor. (b) We estimatethe relative pose between the touch sensor and the camera usingcorrespondences between sight and touch. on a camera (), obtaining synchronized touch signals{ i}Ni=1 and video frames v. We then estimate the pose ofthe video frames using off-the-shelf structure from motionmethods , obtaining poses {pvi }Ni=1. Finally, we use thecalibration of the mount to obtain the poses {pti}Ni=1 of thetactile measurements with respect to the scenes global ref-erence frame. As a collection device, we mount an iPhone14 Pro to one end of a camera rod, and a DIGIT touchsensor to the other end. Note that the devices can be re-placed with any RGB-D camera and vision-based tactilesensor.Capturing setup calibration.To find the relative posebetween the camera and the touch sensor (), we ex-ploit the fact that arbitrary viewpoints can be synthesizedfrom F, and that ubiquitous vision-based touch sensors arebased on perspective cameras. In these sensors, an elas-tomer gel is placed on the lens of a commodity camera,which is illuminated by colored lights. When the gel ispressed into an object, it deforms, and the camera recordsan image of the deformation; this image is used as the tac-tile signal. This design allows us to estimate the pose of thetactile sensor through multi-view constraints from visual-tactile correspondences: pixels in visual images and tactileimages that are of the same physical point.We start the calibration process by synthesizing novelviews from F. The views are generated at the camera loca-tion {pvi }Ni=1, but rotated 90 on the x-axis. This is becausethe camera is approximately orthogonal to the touch sen-sor (see ). Then, we manually annotate correspondingpixels between the touch measurements and the generatedframes (). To simplify and standardize this process,we place a braille board in each scene and probe it with thetouch sensor. This will generate a distinctive touch signalthat is easy to localize .We formulate the problem of estimating the six degreesof freedom relative pose (R, t) between the touch sensorand the generated frames as a resectioning problem .We use the estimated 3D structure from the NeRF F toobtain 3D points {xi}Mi=1 for each of the annotated corre-",
  "i=1(K[R | t], Xi) ui1,(1)": "where projects a 3D point using a given projection matrix,K are the known intrinsics of the tactile sensors camera,and the point Xi is in the coordinate system of the generatedvision frames. We perform the optimization on 6-15 anno-tated correspondences from the braille board. For robust-ness, we compute correspondences from multiple frames.We represent the rotation matrix using quaternions and op-timize using nonlinear least-squares. Once we have (R, t)with respect to the generated frames, we can derive the rel-ative pose between the camera and the touch sensor.",
  ". Imputing the missing touch": "We use a generative model to estimate the touch signal (rep-resented as an image from a vision-based touch sensor) forother locations within the scene. Specifically, we train a dif-fusion model p( | v, d, b), where v and d are images anddepth maps extracted from F (see ). We also pass asinput to the diffusion model a background image capturedby the touch sensor when it is not in contact with anything,denoted as b. Although not essential, we have observed thatthis additional input empirically improves the models per-formance (e.g., the background provides the locationof defects in the gel, which appear as black dots). We trainthe model p on our entire vision-touch dataset (Sec. 4).The training of p is divided into two stages.In thefirst, we pre-train a cross-modal visual-tactile encoder withself-supervised contrastive learning on our dataset.Thisstage, initially proposed by , is equivalent to theself-supervised encoding pre-training that is common forimage generation models . We use a ResNet-50 as the backbone for this contrastive model.In the second stage, we use the contrastive model togenerate the input for a conditional latent diffusion model,which is built upon Stable Diffusion . A frozen pre-trained VQ-GAN is used to obtain the latent represen-tation with a spatial dimension of 64 64. We start train-ing the diffusion model from scratch and pre-train it on thetask of unconditional tactile image generation on the YCB-Slide dataset . After this stage, we train the conditionalgenerative model p on our spatially aligned visual-tactiledataset, further fine-tuning the contrastive model end-to-endwith the generation task.At inference time, given a novel location in the 3D scene,we first render the visual signals v and d from NeRF, andthen estimate the touch signal of the position using thediffusion model.",
  ". Data Collection Procedure": "The data collection procedure is divided into two stages.First, we collect multiple views from the scene, capturingenough frames around the areas we plan to touch. Duringthis stage, we collect approximately 500 frames. Next, wecollect synchronized visual and touch data, maximizing thegeometry and texture being touched. We then estimate thecamera location of the vision frames collected in the previ-ous two stages using off-the-shelf mapping tools . Afterestimating the camera poses for the vision frames, the touchmeasurements poses can be derived by using the mountcalibration matrix. More details about the pose estimationprocedure can be found in the supplement.Finally, we associate each touch sensor with a color im-age by translating the sensor poses upwards by 0.4 metersand querying the NeRF with such poses. The field of viewwe use when querying the NeRF is 50. This provides uswith approximately 1,500 temporally aligned vision-touchimage pairs per scene. Note that this collection procedure isscalable since it does not require specific expertise or equip-ment and generates abundant scene-level samples.",
  ". Dataset Statistics": "We collect our data in 13 ordinary scenes including two of-fices, a workroom, a conference room, a corridor, a table-top, a corridor, a lounge, a room with various clothes andfour outdoor scenes with interesting materials. Typically,we collect 1k to 2k tactile probes in each scene, resulting ina total of 19.3k image pairs in the dataset.Some representative samples from the collected datasetare shown in . Our data includes a large variety ofgeometry (edges, surfaces, corners, etc.) and texture (plas-tic, clothes, snow, wood, etc.) of different materials in thescene. During capturing process, the collector will try to . Representative examples from the captured dataset. Our dataset is obtained from nine everyday scenes, such as offices,classrooms, and kitchens. We show three such scenes in the figure above, together with samples of spatially aligned visual and tactile data.In each scene, 1k to 2k tactile probes were collected, resulting in a total of 19.3k image pairs. The data encompasses diverse geometries(edges, surfaces, corners, etc.) and textures (plastic, clothes, snow, wood, etc.) of various materials. The collector systematically probeddifferent objects, covering areas with distinct geometry and texture using different sensor poses. thoroughly probe various objects and cover the interestingareas with more distinguishable geometry and texture withdifferent sensor poses. To the best of our knowledge, ourdataset is the first dataset that captures full, scene-scale spa-tially aligned vision-touch image pairs. We provide moredetails about the dataset in the supplement.",
  ". Implementation Details": "NeRF. We use the Nerfacto method from Nerfstudio .For each scene, we utilize approximately 2,000 images astraining set, which thoroughly cover the scene from variousview points. We train the network with a base learning rateof 1 102 using Adam optimizer for 200,000 stepson a single NVIDIA RTX 2080 Ti GPU to achieve optimalperformance.Visual-tactile contrastive model.Following priorworks , we leverage contrastive learning methodsto train a ResNet-50 as visual encoder. The visual andtactile encoders share the same architecture but have differ-ent weights. We encode visual and tactile data into latentvectors in the resulting shared representation space.Weset the dimension of the latent vectors to 32. Similar toCLIP , the model is trained on InfoNCE loss obtainedfrom the pairwise dot products of the latent vectors. Wetrain the model for 20 epochs by Adam optimizer witha learning rate of 104 and batch size of 256 on 4 NVIDIARTX 2080 Ti GPUs. Visual-tactile generative model.Our implementation ofthe diffusion model closely follows Stable Diffusion ,with the difference that we use a ResNet-50 to generatethe visual encoding from RGB-D images for conditioning.Specifically, we also add the RGB-D images rendered fromthe tactile sensors poses into the conditioning, which werefer to in Sec. 5.2 as multiscale conditioning. The modelis optimized for 30 epochs by Adam optimizer with abase learning rate of 105. The learning rate is scaled bygpu number batch size. We train the model with batchsize of 48 on 4 NVIDIA A40 GPUs. At inference time,the model conducts 200 steps of denoising process with a7.5 guidance scale. Following prior cross-modal synthe-sis work , we use reranking to improve the predictionquality. We obtain 16 samples from the diffusion model forevery instance and re-rank the samples with our pretrainedcontrastive model. The sample with highest similarity is thefinal prediction.",
  ". Dense Touch Estimation": "Experimental setup.We now evaluate the diffusionmodels ability to generate touch images. To reduce overlapbetween the training and test set, we first split the framesinto sequences temporally (following previous work ).We split them into sequences of 50 touch samples, then di-vide these sequences into train/validation/test with a ratioof 8/1/1. We evaluate the generated samples on FrechetInception Distance (FID), a standard evaluation metricfor cross-modal generation .We also include PeakSignal to Noise Ratio (PSNR) and Structural Similarity(SSIM), though we note that these metrics are highly sen-sitive to spatial position of the generated content, and canbe optimized by models that minimize simple pixelwiselosses . We also include CVTP metric proposed by priorwork , which measures the similarity between visualand tactile embeddings of a contrastive model, analogous to",
  "wallsurfacedeskcarpet": ". Qualitative touch estimation results. Each model is conditioned on the RGB image and depth map rendered from the NeRF(left). The white box indicates the tactile sensors approximate field of view (which is much smaller than the full conditional image).The G.T. column shows the ground truth touch images measured from a DIGIT sensor. L1 and VisGel often generate blurry textures andinaccurate geometry. By contrast, our model better captures the features of the tactile image, e.g., the rocks microgeometry and complextextures and shapes of furniture. The last row shows two failure cases of our model. In both examples, our model generates a touch imagethat is geometrically misaligned with the ground truth. All of the examples shown here are at least 10cm away from any training sample. CLIP score. We compare against two baselines: Vis-Gel, the approach from Li et. , which trains a GAN fortouch generation, and L1, a model with the same architec-ture of VisGel but trained to minimize an L1 loss in pixelspace. Results.As is shown in , our approach performsmuch better on the high-level metrics, with up to 4x lowerFID and 80x higher CVTP. This indicates that our proposeddiffusion model captures the distribution and characteristicsof the real tactile data more effectively. On the low-levelmetrics (PSNR and SSIM), all methods are comparable.In particular, the L1 model slightly outperforms the othermethods since the loss it is trained on is highly correlatedwith low-level, pixel-wise metrics. qualitatively com-pares samples from the different models. Indeed, our gener-ated samples exhibit enhanced details in micro-geometry offabrics and richer textures, including snow, wood and car-peting. However, all methods fail on fine details that arebarely visible in the image, such as the tree bark. Ablation study.We evaluate the importance of the maincomponents of our proposed touch generation approach(). Removing the conditioning on the RGB imageresults in the most prominent performance drop. This isexpected since RGB image uniquely determines the fine-",
  "L124.340.8297.050.01VisGel 23.660.81130.220.03Ours22.840.7228.970.80": ".Quantitative results on touch estimation for novelviews. While comparable on low-level metrics with the baselines,our approach captures the characteristics of the real tactile datamore effectively, resulting in a lower FID score. grained details of a tactile image. Removing depth image orcontrastive pretraining has small effect on CVTP but resultsin a drop on FID. Contrastive re-ranking largely improvesCVTP, indicating the necessity of obtaining multiple sam-ples from the diffusion model. We also find that multiscaleconditioning provide a small benefit on FID and CVTP.",
  ". Downstream Task I: Tactile Localization": "To help understand the quality of the captured TaRFs, weevaluate the performance of the contrastive model (used forconditioning our diffusion model) on the task of tactile lo-calization. Given a tactile signal, our goal is to find thecorresponding regions in a 2D image or in a 3D scene thatare associated with it, i.e., we ask the question: what partof this image/scene feel like this? We perform the following",
  "QueryHeatmapQueryQueryHeatmapHeatmapQueryHeatmap": ". Tactile localization heatmaps. Given a tactile query image, the heatmap shows the image patches with a higher affinity tothis tactile signal, as measured by a contrastive model trained on our dataset. We use a sliding window and compare each extracted patchwith the touch signal. In each case, the center patch is the true position. Our model successfully captures the correlation between the twosignals. This enables it to localize a variety of touch signals, including fine-grained geometry, e.g., a cable or a keyboard, various types ofcorners and edges, and large uniform regions, such as a clothing. This ability enables our diffusion model to effectively propagate sparsetouch samples to other visually and structurally similar regions of the scene.",
  "Model variationPSNR SSIM FID CVTP": "Full22.840.7228.970.80No RGB conditioning22.130.7034.310.76No depth conditioning22.570.7133.160.80No contrastive pretraining22.820.7132.980.79No re-ranking22.920.7229.460.61No multiscale23.190.7230.890.77 . Ablation study. Since the fine-grained details of touchimages can be determined from a RGB image, removing condi-tioning on the latter results in the largest performance drops. Re-ranking has notable impact on CVTP, indicating the necessity ofobtaining multiple samples from the diffusion model.",
  "evaluations on the test set of our dataset. Note that we runno task-specific training": "2D Localization.To determine which part of an imageare associated with a given tactile measurement, we followthe same setup of SSVTP . We first split the image intopatches and compute their embedding. Then, we generatethe tactile embedding of the input touch image. Finally, wecompute the pairwise similarities between the tactile andvisual embeddings, which we plot as a heatmap. As wecan see in , our constrastive encoder can successfullycapture the correlations between the visual and tactile data.For instance, the tactile embeddings of edges are associatedto edges of similar shape in the visual image. Note that themajority of tactile embeddings are highly ambiguous: alledges with a similar geometry feel the same. 3D Localization.In 3D, the association of an image totactile measurements becomes less ambiguous.Indeed,since tactile-visual samples are rotation-dependent, objectswith similar shapes but different orientations will generatedifferent tactile measurements. Lifting the task to 3D stilldoes not remove all ambiguities (for example, each side ofa rectangular table cannot be precisely localized). Nonethe-less, we believe it to be a good fit for a quantitative evalua-tion since its rare for two ambiguous parts of the scene tobe touched with exactly the same orientation.We use the following experimental setup for 3D local-ization. Given a tactile image as a query, we compute itsdistance in embedding space to all visual test images fromthe same scene. Note that all test images are associated witha 3D location. We define as ground-truth correspondencesall test images at a distance of at most r from the 3D lo-cation of the test sample. We vary r to account for localambiguities. As typical in the retrieval literature, we bench-mark the performance with metric mean Average Precision(mAP).We consider three baselines: (1) chance, which ran-domly selects corresponding samples; (2) real, which usesthe contrastive model trained on our dataset; and (3) real+ estimated, which trains the contrastive model on bothdataset samples and a set of synthetic samples generated viathe scenes NeRF and our touch generation model. Specif-ically, we render a new image and corresponding touch byinterpolating the position of two consecutive frames in thetraining dataset. This results in a training dataset for thecontrastive model that is twice as large.",
  "Chance3.556.8210.2518.2621.33Real12.1022.9332.1050.3057.15Real + Est.14.9226.6936.1753.6260.61": ". Quantitative results on 3D tactile localization. Weevaluate using mean Average Precision (mAP) as a metric. Train-ing the contrastive model on our dataset of visually aligned realsamples together with estimated samples from new locations inthe scene results in the highest performance. The results, presented in , demonstrate the perfor-mance benefit of employing both real and synthetic tactilepairs. Combining synthetic tactile images with the originalpairs achieves highest performance on all distance thresh-olds. Overall, this indicates that touch measurements fromnovel views are not only qualitatively accurate, but also ben-eficial for this downstream task.",
  ". Downstream Task II: Material Classification": "We investigate the efficacy of our visual-tactile dataset forunderstanding material properties, focusing on the task ofmaterial classification. We follow the formulation by Yanget al. , which consists of three subtasks: (i) materialclassification, requiring the distinction of materials among20 possible classes; (ii) softness classification, a binaryproblem dividing materials as either hard or soft; and (iii)hardness classification, which requires the classification ofmaterials as either rough or smooth.We follow the same experimental procedure of : wepretrain a contrastive model on a dataset and perform linearprobing on the sub-tasks training set. Our experiments onlyvary the pretraining dataset, leaving all architectural choicesand hyperparameters the same. We compare against fourbaselines. A random classifier (chance); the ObjectFolder2.0 dataset ; the VisGel dataset ; and the Touchand Go dataset . Note that the touch sensor used in thetest data (GelSight) differs from the one used in our dataset(DIGIT). Therefore, we use for pretraining a combinationof our dataset and Touch and Go. To ensure a fair compar-ison, we also compare to the combination of each datasetand Touch and Go.The findings from this evaluation, as shown in ,suggest that our data improves the effectiveness of the con-trastive pretraining objective, even though our data is froma different distribution. Moreover, we find that adding esti-mated touch probes for pretraining results in a higher per-formance on all the three tasks, especially the smoothnessclassification. This indicates that not only does our datasetcovers a wide range of materials but also our diffusionmodel captures the distinguishable and useful patterns ofdifferent materials.",
  "Touch and Go 54.777.379.4+ ObjectFolder 2.0 54.687.384.8+ VisGel 53.186.783.6+ Ours (Real)57.688.481.7+ Ours (Real + Estimated)59.088.786.1": ".Material classification.We show the downstreammaterial recognition accuracy of models pre-trained on differentdatasets. The final rows show the performance when combiningdifferent datasets with Touch and Go . The task-specifictraining and testing datasets for this task are collected with a Gel-Sight sensor. We note that our data comes from a different distri-bution, since it is collected with a DIGIT sensor .",
  ". Conclusion": "In this work, we present the TaRF, a scene representationthat brings vision and touch into a shared 3D space. Thisrepresentation enables the generation of touch probes fornovel scene locations. To build this representation, we col-lect the largest dataset of spatially aligned vision and touchprobes.We study the utility of both the representation andthe dataset in a series of qualitative and quantitative experi-ments and on two downstream tasks: 3D touch localizationand material recognition. Overall, our work makes the firststep towards giving current scene representation techniquesan understanding of not only how things look, but also howthey feel. This capability could be critical in several applica-tions ranging from robotics to the creation of virtual worldsthat look and feel like the real world. Limitations.Since the touch sensor is based on a highlyzoomed-in camera, small (centimeter-scale) errors in SfMor visual-tactile registration can lead to misalignments ofseveral pixels between the views of the NeRF and the touchsamples, which can be seen in our TaRFs. Another limita-tion of the proposed representation is the assumption thatthe scenes coarse-scale structure does not change when itis touched, an assumption that may be violated for someinelastic surfaces. Acknowledgements.We thank Jeongsoo Park, AyushShrivastava, Daniel Geng, Ziyang Chen, Zihao Wei, Zix-uan Pan, Chao Feng, Chris Rockwell, Gaurav Kaul and thereviewers for the valuable discussion and feedback. Thiswork was supported by an NSF CAREER Award #2339071,a Sony Research Award, the DARPA Machine CommonSense program, and ONR MURI award N00014-21-1-2801. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, MohamedElhoseiny, and Leonidas Guibas. Referit3d: Neural listenersfor fine-grained 3d object identification in real-world scenes.In Computer VisionECCV 2020: 16th European Confer-ence, Glasgow, UK, August 2328, 2020, Proceedings, PartI 16, 2020. 3 Peter Anderson, Ayush Shrivastava, Joanne Truong, ArjunMajumdar, Devi Parikh, Dhruv Batra, and Stefan Lee. Sim-to-real transfer for vision-and-language navigation. In Con-ference on Robot Learning, pages 671681. PMLR, 2021.3 Jakub Bednarek, Michal Bednarek, Lorenz Wellhausen,Marco Hutter, and Krzysztof Walas. What am i touching?learning to classify terrain via haptic sensing. In 2019 In-ternational Conference on Robotics and Automation (ICRA),pages 71877193. IEEE, 2019. 1 Katherine L Bouman, Bei Xiao, Peter Battaglia, andWilliam T Freeman. Estimating the material properties offabric from video. In Proceedings of the IEEE internationalconference on computer vision, pages 19841991, 2013. 1",
  "Alexander Burka. Instrumentation, data, and algorithms forvisually understanding haptic surface properties. 2018. 2": "Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wen-zhen Yuan, Justin Lin, Edward H. Adelson, and SergeyLevine. The feeling of success: Does touch sensing helppredict grasp outcomes?Conference on Robot Learning(CoRL), 2017. 1, 2 Roberto Calandra, Andrew Owens, Dinesh Jayaraman,Justin Lin, Wenzhen Yuan, Jitendra Malik, Edward H. Adel-son, and Sergey Levine. More than a feeling: Learning tograsp and regrasp using vision and touch. Robotics and Au-tomation Letters (RA-L), 2018. 1, 2 Roberto Calandra, Andrew Owens, Dinesh Jayaraman,Justin Lin, Wenzhen Yuan, Jitendra Malik, Edward H. Adel-son, and Sergey Levine. More than a feeling: Learning tograsp and regrasp using vision and touch. IEEE Roboticsand Automation Letters, 3:33003307, 2018. 2 Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srini-vasa, Pieter Abbeel, and Aaron M Dollar. The ycb objectand model set: Towards common benchmarks for manipula-tion research. In 2015 international conference on advancedrobotics (ICAR), pages 510517. IEEE, 2015. 2",
  "Dave Zhenyu Chen, Angel X Chang, and Matthias Niener.Scanrefer: 3d object localization in rgb-d scans using naturallanguage. In European conference on computer vision, 2020.3": "Alex Church, John Lloyd, Raia Hadsell, and Nathan F Lep-ora. Deep reinforcement learning for tactile robotics: Learn-ing to type on a braille keyboard. IEEE Robotics and Au-tomation Letters, 5(4):61456152, 2020. 1 Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-ber, Thomas Funkhouser, and Matthias Niener. Scannet:Richly-annotated 3d reconstructions of indoor scenes.InProceedings of the IEEE conference on computer vision andpattern recognition, 2017. 3",
  "Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, andJiajun Wu. Objectfolder: A dataset of objects with implicitvisual, auditory, and tactile representations. In CoRL, 2021.1, 2": "Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jean-nette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. Ob-jectfolder 2.0: A multisensory object dataset for sim2realtransfer.In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 1059810608, 2022. 1, 2, 8 Ruohan Gao, Yiming Dou, Hao Li, Tanmay Agarwal, Jean-nette Bohg, Yunzhu Li, Li Fei-Fei, and Jiajun Wu. The ob-jectfolder benchmark: Multisensory learning with neural andreal objects. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1727617286, 2023. 1, 2, 3",
  "Richard Hartley and Andrew Zisserman. Multiple view ge-ometry in computer vision.Cambridge university press,2003. 2, 3": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 4, 5 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. Gans trained by atwo time-scale update rule converge to a local nash equilib-rium. Advances in neural information processing systems,30, 2017. 5",
  "Carolina Higuera, Byron Boots, and Mustafa Mukadam.Learning to read braille: Bridging the tactile reality gap withdiffusion models. arXiv preprint arXiv:2304.01182, 2023. 3,4": "Mark A Hoepflinger, C David Remy, Marco Hutter, LucianoSpinello, and Roland Siegwart. Haptic terrain classificationfor legged robots. In 2010 IEEE International Conference onRobotics and Automation, pages 28282833. IEEE, 2010. 1 Micah K Johnson and Edward H Adelson.Retrographicsensing for the measurement of surface texture and shape.In 2009 IEEE Conference on Computer Vision and PatternRecognition, pages 10701077. IEEE, 2009. 2, 3",
  "Micah K Johnson, Forrester Cole, Alvin Raj, and Edward HAdelson. Microgeometry capture using an elastomeric sen-sor. ACM Transactions on Graphics (TOG), 2011. 2, 3": "Justin Kerr, Huang Huang, Albert Wilcox, Ryan Hoque,Jeffrey Ichnowski, Roberto Calandra, and Ken Goldberg.Learning self-supervised representations from vision andtouch for active sliding perception of deformable surfaces.arXiv preprint arXiv:2209.13042, 2022. 5 Justin Kerr, Huang Huang, Albert Wilcox, Ryan Hoque, Jef-frey Ichnowski, Roberto Calandra, and Ken Goldberg. Self-supervised visuo-tactile pretraining to locate and follow gar-ment features. In Robotics: Science and Systems, 2023. 1, 2,3, 7 Justin Kerr, Chung Min Kim, Ken Goldberg, AngjooKanazawa, and Matthew Tancik. Lerf: Language embeddedradiance fields. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, 2023. 3",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization. ICLR, 2015. 5": "Hendrik Kolvenbach, Christian Bartschi, Lorenz Well-hausen, Ruben Grandia, and Marco Hutter. Haptic inspec-tion of planetary soils with legged robots. IEEE Roboticsand Automation Letters, 4(2):16261632, 2019. 1 Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang,Benjamin Maloon, Victoria Rose Most, Dave Stroud, Ray-mond Santos, Ahmad Byagowi, Gregg Kammerer, et al.Digit: A novel design for a low-cost compact high-resolutiontactile sensor with application to in-hand manipulation. IEEERobotics and Automation Letters, 2020. 1, 2, 3, 8",
  "Hongyu Li, Snehal Dikhale, Soshi Iba, and Nawid Jamali.Vihope: Visuotactile in-hand object 6d pose estimation withshape completion. IEEE Robotics and Automation Letters, 8(11):69636970, 2023. 1": "Yunzhu Li, Jun-Yan Zhu, Russ Tedrake, and Antonio Tor-ralba. Connecting touch and vision via cross-modal predic-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1060910618,2019. 1, 2, 3, 6, 8 Justin Lin, Roberto Calandra, and Sergey Levine. Learningto identify object instances by touch: Tactile recognition viamultimodal matching. In 2019 International Conference onRobotics and Automation (ICRA), pages 36443650. IEEE,2019. 3",
  "Gabriel B Margolis, Xiang Fu, Yandong Ji, and PulkitAgrawal. Learning physically grounded robot vision withactive sensing motor policies. In 7th Annual Conference onRobot Learning, 2023. 1": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. Communications of the ACM, 65(1):99106, 2021. 2,3 Andrew Owens, Phillip Isola, Josh McDermott, Antonio Tor-ralba, Edward H Adelson, and William T Freeman. Visuallyindicated sounds. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 24052413,2016. 1",
  "Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta,Yi Ma, Roberto Calandra, and Jitendra Malik. General in-hand object rotation with vision and touch. arXiv preprintarXiv:2309.09979, 2023. 1": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision, 2021. 5, 6 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.Zero-shot text-to-image generation. In International Confer-ence on Machine Learning, pages 88218831. PMLR, 2021.5",
  "Linda Smith and Michael Gasser. The development of em-bodied cognition: Six lessons from babies. Artificial life,2005. 1": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,and Surya Ganguli.Deep unsupervised learning usingnonequilibrium thermodynamics.In International confer-ence on machine learning, pages 22562265. PMLR, 2015.2 Sudharshan Suresh, Zilin Si, Stuart Anderson, MichaelKaess, and Mustafa Mukadam. Midastouch: Monte-carloinference over distributions across sliding touch. In Confer-ence on Robot Learning, pages 319331. PMLR, 2023. 1, 2,4 Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,Brent Yi, Terrance Wang, Alexander Kristoffersen, JakeAustin, Kamyar Salahi, Abhik Ahuja, David Mcallister,Justin Kerr, and Angjoo Kanazawa. Nerfstudio: A modu-",
  "Fengyu Yang, Jiacheng Zhang, and Andrew Owens. Gen-erating visual scenes from touch.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2207022080, 2023. 2, 3, 4, 5": "Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park,Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gan-gopadhyay, Andrew Owens, and Alex Wong. Binding touchto everything: Learning unified multimodal tactile represen-tations.In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2024. 3 Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Niener,and Angela Dai. Scannet++: A high-fidelity dataset of 3d in-door scenes. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1222, 2023. 3",
  "Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen,and Xiaolong Wang.Rotating without seeing:To-wards in-hand dexterity through touch.arXiv preprintarXiv:2303.10880, 2023. 1": "Wenzhen Yuan, Shaoxiong Wang, Siyuan Dong, and Ed-ward Adelson. Connecting look and feel: Associating thevisual and tactile properties of physical materials. In Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition, pages 55805588, 2017. 3 Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-drew J Davison. In-place scene labelling and understandingwith implicit scene representation.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1583815847, 2021. 3 Shaohong Zhong, Alessandro Albini, Oiwi Parker Jones,Perla Maiolino, and Ingmar Posner. Touching a nerf: Lever-aging neural radiance fields for tactile sensory data genera-tion. In Conference on Robot Learning, pages 16181628.PMLR, 2023. 1, 2"
}