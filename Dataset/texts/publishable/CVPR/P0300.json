{
  "Shagun UppalAnanye AgarwalHaoyu XiongKenneth ShawDeepak PathakCarnegie Mellon University": ". Learning to SPIN: Our robot learns to simultaneously perceive, manipulate, and navigate cluttered unstructured environmentsin a whole-body fashion. The robot has an actuated camera with a limited field of view that it must control to get information about itsenvironment. The motion and perception problem are tightly coupled since what the robot knows about the environment influences how itcan move and vice versa. We show results in a large variety of scenarios both indoors and outdoors with different obstacles like boxes andfurniture. Our robot can pick up different objects like cups, and utensils. Video demos at",
  "Abstract": "While there has been remarkable progress recently in thefields of manipulation and locomotion, mobile manipulationremains a long-standing challenge. Compared to locomo-tion or static manipulation, a mobile system must make adiverse range of long-horizon tasks feasible in unstructuredand dynamic environments.While the applications arebroad and interesting, there are a plethora of challenges indeveloping these systems such as coordination between thebase and arm, reliance on onboard perception for perceiv-ing and interacting with the environment, and most impor-tantly, simultaneously integrating all these parts together.Prior works approach the problem using disentangled mod-ular skills for mobility and manipulation that are triviallytied together. This causes several limitations such as com- pounding errors, delays in decision-making, and no whole-body coordination. In this work, we present a reactive mo-bile manipulation framework that uses an active visual sys-tem to consciously perceive and react to its environment.Similar to how humans leverage whole-body and hand-eyecoordination, we develop a mobile manipulator that ex-ploits its ability to move and see, more specifically to movein order to see and to see in order to move. This allows itto not only move around and interact with its environmentbut also, choose when to perceive what using an activevisual system. We observe that such an agent learns to nav-igate around complex cluttered scenarios while displayingagile whole-body coordination using only ego-vision with-out needing to create environment maps. Videos are avail-able at",
  ". Introduction": "Consider the example shown in . A person is tryingto carry a coffee cup through clutter. This not only requiresnavigational planning from start to goal but planning of thewhole body to avoid obstacles along the way. Furthermore,due to ego-centric vision, the person needs to actively lookaround to gather the presence of obstacles. This generalform of mobile manipulation task necessitates a coupled un-derstanding of whole-body control with active perception.This capability is one of the fundamental and frequently en-countered tasks in embodied cognition.The dominant paradigm to tackle this problem is throughclassical planning-based control which requires aprioriknowledge about the precise location of all the obstaclesalong with a detailed map of the environment.In mostreal-world scenarios, this assumption is impractical due tocomputational reasons, but more importantly, because envi-ronments are dynamic and objects keep moving around ingeneral. Furthermore, relying on precise measurement ofscenes for control does not allow agents to reactively im-provise to changes in their environment. Practically, evenwhen the complete environment map is known apriori, jointplanning for a system with high degrees of freedom, say amobile base with an arm, is often intractable and too expen-sive to be deployed in real-time.Humans, on the other hand, do not rely on precise knownestimates of object locations and instead use ego-centric vi-sion to navigate around obstacles in real-time. In an un-familiar environment, where to look is informed by wherethey want to move (called active perception), and howthey move in return determines what all they can see im-mediately afterward. This integrated mobility and percep-tion allows us to see, adapt, and react to maneuver throughunseen heavily cluttered environments.This paper presents SPIN, an end-to-end approach toSimultaneous Perception, Interaction, and Navigation. Wetrain a single model that not only outputs low-level controlsfor the robot body and arm but also predicts where shouldthe robots ego-centric camera look at each time step whilemoving its whole body by avoiding obstacles. We train ourapproach via reinforcement learning (RL), and to get aroundthe computational bottleneck of rendering depth images, weuse a teacher-student training framework where robot be-havior is first learned using RL with access to visible objectscandots and then distilled into a policy that operates fromego-depth using supervised learning. We evaluate across 6benchmarks in simulation ranging from easy, medium, andhard difficulty, and two real-world environments with a sim-ilar level of clutter as the hard environments in simulationand also add dynamic, adversarial obstacles. We find thatour method outperforms classical methods and baselineswhich do not use active vision. We also observed emergentbehaviors, including dynamic obstacle avoidance which the",
  ". Human and robot illustration of whole-body navigationthrough the clutter": "robot did not see during training time.Our approach presents a radical hypothesis that the tradi-tionally non-reactive planning approach to whole-body con-trol can indeed be cast into a reactive model i.e. singleend-to-end policy trained by RL. Despite a big departurefrom optimal control literature, this hypothesis is not as sur-prising since agile whole-body coordination and fast obsta-cle avoidance in humans are developed into muscle memoryover time. We now discuss our approach in detail.",
  ". Method": "We want our mobile manipulator () to navigate andmanipulate objects while avoiding obstacles in cluttered en-vironments. It shares anatomical similarities with a human,bringing with it many of the same challenges. First, it has alimb in the form of an arm that can be raised and lowered,so the robot must constantly move the arm to avoid any ob-stacles. Second, it has an actuated camera with a very lim-ited field of view (87 horizontal, 58 vertical), so it needsto constantly look around to simultaneously plan ahead andlook out for unexpected obstacles. Imagine yourself walk-ing through a cluttered cabinet, there are too many obsta-cles around to keep track of, and you cant see all of themat once, so you must keep looking all around your bodyto plan a path through the clutter but also make sure youdont hit anything you missed along the way. Unlike regularwalking where our eyes mostly point straight ahead and thepath is clear, here you must actively choose what to perceivefor simultaneously planning ahead and also doing reactivefixes to your planned path. Since all the obstacles cannot beperceived at a single glance, you must have spatial aware-ness and know where the obstacle you saw some time agois right now in relation to your body. Note that this entireprocess is very different from the classical approach, whereperception, planning, and obstacle avoidance are separateprocesses executed separately and in sequence. Further, itis assumed that the output of each is perfect, whereas this israrely the case in practice in our unstructured world.To deal with this challenging, entangled problem setup,",
  "Pre-initialized weights": ". We learn a policy that uses ego-vision to simultaneously perceive, interact, and navigate in cluttered environments. We proposetwo methods: (1) Coupled Visuomotor Optimization (CVO) learns robot and camera actions at the same time. We train an RL policyto predict these. We only provide scandots if they are visible in the agents field-of-view allowing the agent to learn to move its cameraand aggregate information about its environment. This is followed by a phase-2 supervised training where this behavior is distilled intoa student network that operates with ego-centric depth images (2) Decoupled Visuomotor Optimization (DVO) decouples the action andperception learning into two parts: first the agent learns to navigate across clutter assuming access to all obstacles. In phase 1b, the robotlearns to move its camera to estimate the relevant information. This is followed by supervised learning same as above. we take a data-driven approach. We train our robot to navi-gate inside procedurally generated clutter in simulation us-ing RL. The robot is only allowed to perceive the part of itsenvironment that is visible to the camera and learns to co-ordinate its arm, base, and camera motion such that it canplan ahead and reactively adjust to obstacles.In practice, since training with RL requires many sam-ples and rendering depth is inefficient (see supp. ), we divide training into two phases. In the first one, welearn mobile manipulation behaviors via RL using a cheap-to-compute variant of depth and in phase 2 we train a CNNfor perception from depth images as illustrated in .",
  ". Phase 1 - Learning Simultaneous Perception,Interaction and Navigation": "In this stage, we use RL to learn to control all the joints ofthe robot to navigate clutter and pick target objects. Sincerendering depth images directly from the robot camera isexpensive, we must instead use an ersatz version that con- tains the same information and is cheap to compute. We doso using scandots st which are the xyz coordinates of thebounding box of each obstacle. To specify which object topick, we give the initial location of the object (before it istouched by the robot) oi. In lieu of the object image, wegive the current location of the object ot. Here, scandotsst and object location ot are privileged information whichmust later be estimated from depth images. Given this in-formation, we train two separate LSTM policies nav andpick. At test time, the nav policy is activated to reach a tar-get location and we switch to the pick one once the robotgets close to the object.",
  "[arobot, acam] = pick(xt, F(ot, xt), oi)(1)": ". We illustrate one scenario of the simulation benchmark here with many obstacles in a narrow passage. The agent learns todevelop whole-body coordination such as the robots arm movement in the last two frames, to reactively adapt and navigate through suchcluttered scenes by actively moving around the camera and aggregating information for efficient navigation without collisions. where F is a masking function that masks object positionot if it is not in the field of view of the camera. This isrequired since object position can only be estimated fromdepth in phase 2 if it is visible.",
  "Navigation Policy": "Training this policy requires a complex joint visuomotor op-timization since robot motion is dependent on its knowledgeof the environment which in turn depends on how the robotmoves. We present two approaches to tackle this problem.Coupled Visuomotor Optimization (CVO) Here, weset up a partially observable environment for the robot andlet the RL algorithm do the joint optimization using large-scale data. In particular, the policy gets proprioception xtand only visible scandots st = F(st, xt) as observationand has to predict both the camera and the robot actions.Since the scandots are permutation invariant, we pass themthrough a trainable point-net architecture P to obtain com-pressed latent zt = P(st) that we pass to the policy",
  "[arobot, acam] = nav(xt, zt)(2)": "This presents a tough optimization landscape because theobservations at each step are strongly dependent on acam.For instance, if the camera swivels around the observationsat the next timestep may look completely different. Indeed,we observe that this requires billions of samples inside aGPU-accelerated simulator to optimize which may not al-ways be feasible in practice.Decoupled Visuomotor Optimization (DVO) To easethe optimization process, we learn the robot and camera ac-tions separately. First, we learn how to move by giving therobot access to all available scandots zt = P(st) in a localvicinity. Since the robot sees everything, the camera motionis irrelevant and we just predict the robots motion",
  "where gt is the goal with respect to the base. Using this pol-icy as supervision, we train another policy to predict both": "camera and robot motions with access to only visible scan-dots zt = P(F(st, xt)). This policy is trained via RL topredict the robot actions from phase 1 policy arobot. Thisoptimization forces the student policy to learn camera be-haviors that capture information about the environment thatis needed to move optimally. We initialize 1bnav from theweights of 1anav",
  "s.t.[arobot, acam] = 1bnav(xt,zt, gt)(4)": "This decoupled approach learns to move and see in separatephases which eases the optimization burden. In principle,the coupled optimization is better since it is possible thatthe 1a policy may learn to exploit privileged information ina way that the 1b policy cannot estimate it for any set ofcamera movements. However, in our setting, this did notturn out to be the case.We train using PPO with backpropagation throughtime in procedurally generated environments.Rewards: For the navigation task, we use distance togoal reward gt along with a forward progress reward| (vt)g | where (vt)g is velocity along the direction of thegoal.",
  "where [x]+ = max(x, 0) and I is the indicator functionwhich forces the reward to be active only when object con-tact forces fi exceed 10N": "Training environments: We procedurally generate longcorridors with obstacles placed in between the robot and thegoal. The initial joints and orientation of the robot are ran-domized. Near the edges of the corridors, we place ran-domized obstacles and walls to simulate distractors in thedepth image. For the pick task, objects are spawned on ta-bles of varying dimensions. We use five different objects -a banana, mug, can, foambrick, and a bottle. The episodeis terminated if the robot reaches the goal or hits an obsta-cle/table.",
  ". Phase 2 - From Scandots to Depth": "Scandots are not directly observable in the real world andmust instead be estimated from the depth image. We traina convolution network C to convert rendered depth imagesdt to perception latents zt. This latent is passed to a studentpolicy to predict the actions [arobot, acam]. This is super-vised using L2 loss from the phase 1 actions. The weightsfor are initialized using . We train this policy usingDAgger . For the navigation policy, we optimize",
  ". Experimental Setup": "We use the Hello Robot Stretch for all our experiments(). The robot has 10 actuated joints which include 2degrees of freedom for the camera, 2 for base rotation andtranslation, 2 for the arm, 1 for the gripper fingers, and 3for the dexterous wrist. An Intel D435i depth camera ismounted on the top of the robot head which is actuated us-ing two motors. The learned policy operates at 10Hz and wedo velocity control for the robot base and position controlfor all the other joints. Velocity control for the robot base al-lows us to perform simultaneous robot translation and rota-tion for more agile behavior. We train using IsaacGymEnvs using 8192 environments which takes 6 hours of train-ing for phase 1 and 10 hours of training time for phase 2 ona RTX 3090. We compare against the following baselines: FixCam: The camera joints are frozen and the camera isforced to look forward. This baseline shows whether ac-tive vision is useful for the mobile manipulation problemand a fixed viewpoint is not enough.",
  "cameras field-of-view": ". (Left) We compute visible scandots by projecting themto the camera frame and checking if they lie within the image plane(Right) of the stretch RE1 robot that we use experiments. It hastwo DoFs in the base, one each for arm lift and extension, two forthe camera, three for the wrist, and one for the gripper. Classical: This uses a classical stack to control the basemotion. We first teleoperate the robot for 3-5 minutesto construct a map using the onboard 2D RPLidar usinggmapping. Next, move base is used to plan a path throughthe environment. Finally, we move the robot to the start,use a Monte Carlo method to localize, and then exe-cute the plan. Note that this baseline gets an easier versionof the problem since it assumes that the map is known inadvance and does not consider arm motion due to the 2DLidar. This is used to test whether reactive navigation issuperior to planning.",
  ". Results and Analysis": "We evaluate our approach both in simulation as well as real-world. Since doing a lot of in-the-wild real-world experi-ments is more time-consuming and cumbersome due to var-ious practical reasons, we thoroughly evaluate our approachon 6 simulation benchmarks with multiple scenarios. Weexplain each of these benchmarks in detail in .3.While simulation benchmarks are useful for fair com-parison with baselines as well as reproducibility, real-worldexperimenting is essential for determining the efficacy ofour system in truly unstructured and dynamic environments.For this, we test our system on various real-world environ-ments as shown in and benchmark its performanceon 2 real-world setups as described in .2.Through simulation experiments, we aim to answer thefollowing questions: (1) For a mobile agent, is active per-ception with an actuated camera necessary, or is a fixedviewpoint enough? (2) Can an active visual agent outper-form a classical agent that relies on pre-built maps? Whatare the limitations of the latter? (3) What are some practi-cal architectural design choices for optimizing mobility andperception together? We empirically answer each of these",
  ". Emergent Behavior": "Large-scale simulation pre-training allows our robot tolearn emergent behaviors to avoid obstacles in cluttered sce-narios, even in the presence of dynamic obstacles. We seeseveral such behaviors during real-world experimentationwhich were neither planned nor specifically trained for insimulation but emerge as a result of a large diversity of pro- cedural environments seen during training. We illustratethree such scenarios in . As highlighted in sev-eral frames, a depicts robustness to adversariallyplaced dynamic obstacles that constantly block the path ofthe robot. It needs to continuously perceive its environmentin multiple directions and quickly react to those changes.We observe that in cases when there is no feasible path forthe robot to navigate through, it also learns to stop and lookaround in order to replan its path and avoid collisions. Sim-ilarly, in 7b we see that as soon as a floating obstacle is sud-denly placed in front of the robot, it shows spatial awarenessand whole-body coordination and lowers its arm to navi-gate through, instead of turning and replanning the entirebase movement which would take more time. In c,",
  "SPIN(DVO) 5.9216.2528.447.3217.1232.049.247.2218.4534.249.4015.9841.24SPIN(CVO) 6.2414.0023.576.5515.8129.315.746.5113.3927.258.0312.7931.25": ". We evaluate the success rate on 10 random environments with an average of 3 fixed seeds across all difficulty scenarios based onthe obstacle course. We report the success rate of each part of the task including reaching (Reach), picking (Pick), and placing (Place) thetarget object in the desired location. The place task requires the agent to bring back the object across the obstacles near its start location. we see an adaptive rerouting mechanism where the agentchanges its straight-line motion as soon as a person kicksin a box in front of it. These behaviors emerge in real-timeand show the ability of our system to continuously perceive,adapt, and react to changes in its environment which is veryhard for a classical planner.",
  ". Real-world results": "We test on two real-world scenes - an academic lab andan open study area with couches and a kitchenette nextto it with both static as well as dynamic obstacles. Boththese environments have unstructured clutter and humansas dynamic obstacles that make it challenging for the agentto navigate through these spaces. For each environment,we have 4 static obstacles and at most 1 dynamic obstaclethrown adversarially. We compare against a classical base-line that uses an A1 RPLidar with gmapping and move basefor planning. We first teleoperate the robot for 3-5min tocreate a map. Note that this provides the added advantagethat this baseline knows the entire map in advance. Sincethe Lidar cannot see objects above the plane we only teston ground obstacles and ignore floating ones that requirewhole-body coordination. We run the planner to only planthe base motion. In Tab. 2 we compare the success rate andaverage number of collisions. An episode succeeds whenthe robot reaches within 15cm of the specified goal posi-tion. Overall, our method succeeds 20-40% more than theclassical baseline. This is because the classical method suf-fers from noise and is not able to recover from a noisy map,and gets stuck, whereas the learned policy learns to lookat the obstacles again and again to improve its uncertaintyestimates and constantly updates its knowledge of whereobstacles are. This ability is even more apparent in the dy- namic scenario () where the classical has a near zerosuccess rate while our method can succeed. It has the emer-gent ability to avoid a new obstacle in space, whereas theclassical baseline relies on the pre-built map and fails en-tirely. Note that, we do not train our policy with dynamicobstacles in simulations, but this behavior comes out as aby-product of lots of diverse experience in simulation. Wedesign the observation space such that everything is relativeto the robot. This allows the agent to perceive the environ-ment as moving within its local reference frame, allowinggeneralization to dynamic obstacles.",
  ". Simulation results": "The simulation benchmarks have 6 scenes, 2 of each easy,medium, and hard environments. Easy environments have0-1 obstacles within a 5m goal range. Medium environ-ments have 2-3 obstacles within 5m and the hard ones haveheavily cluttered scenes with 5 obstacles within 5m. In eachof these cases, one scene (Scenario 1) comprises of a tight1m wide long corridor which bounds the agent to not takeshortcuts and reach the goal only by navigating through ob-stacles. The second (Scenario 2) is an L-shaped corridorwith the goal at the end. The evaluation metrics are reportedas an average of 10 episodes with random agent and obsta-cle initialization across 3 seeds.We compare against various baselines to study the im-pact of our design decisions in Tab. 1. For each scenario,we report the success rate and average episode length across10 rollouts. Our method achieves 33% higher successrate than the NoPointNet baseline since permutation invari-ant scandots latent makes the optimization problem easierand also generalizes better at test time. Ours achieves 68% higher success rate than the FixCam baseline with the",
  "OursClassicalOursClassicalAverage Success0.80.40.60.2Average # Collisions0.80.61.61.0": ".We compare our method against a classical mappingand planning baseline for navigation in cluttered scenes with bothstatic as well as dynamic obstacles. The classical performs reason-ably in static environments, it quickly breaks with dynamic obsta-cles like humans walking around, whereas our method shows morerobust reactivity to such obstacles even without being trained withdynamic obstacles in simulation. We report the success rate of ourmethod compared with the baseline. For the classical baseline, weteleoperate the robot for 2-3 min. camera pointing straight ahead. This is because in somecases the robot encounters obstacles in its peripheral vi-sion and our policy can change the camera angle to avoidthem. Active vision is necessary for the robot to move ef-fectively through a cluttered environment. Our method issignificantly better than the Mapping baseline because thesystematic noise in the object locations makes it hard forthe robot to avoid them, especially in cluttered environ-ments, whereas our method can continuously estimate theposition of obstacles while it is moving and adapt the mo-tion online. Finally, we compare the decoupled (DVO) op-timization against coupled (CVO) optimization variant ofour method and find that they achieve similar performance.We hypothesize that the partial observability and joint opti-mization for camera and robot actions in CVO training al-lows the agent to quickly discover optimal shortcuts that areotherwise harder to distill from a privileged teacher policy.",
  ". Related Works": "Classical ApproachesThe problem of navigating robotsaround obstacles has been studied for decades. Classicalmethods solve the motion and perception problem sepa-rately.First these methods build a map of the environ-ment using the robots onboard sensors such as cameras,proprioception and Lidar or infrared . Kalman-filter-based techniques are often used to track positions, butthey cant represent multi-modal ambiguities or recover af-ter tracking failure . Grid-based methods solve this butsuffer from high memory usage . Modern SLAM ap-proaches ORBSLAM3 , OpenVSLAM and RTAB use variations of a method that relies on particle fil-ters to hold a multi-modal belief of the robots loca-tion in the map . SLAM is especially challenging in dynamic environments due to the confounding motion ofother agents . Once a map is built, a pathcan be planned over it. Exact paths can be computed usinggraph search algorithms , probabilistic methods whichare faster but yield approximately optimal solutions orpotential-field based methods . All of these assume per-fect perception and re-planning is usually expensive makingthem susceptible to noise and precluding reactive behavior. Learning-based navigationIn recent years, learning hasbeen used to improve the classical navigation stack. Mod-ular approaches still leverage SLAM-basedmethods to build a map but use learning or heuristic changesto get priors for the best possible route to a goal. End-to-end approaches forgo maps entirely and train a policy to gofrom images to robot commands to go to a goal location. We also take the end-to-end approach but un-like prior work where what the robot sees is fixed based onits position, in our case it must move its head and activelychoose what it sees making optimization more challenging. Mobile ManipulationA mobile base and arm togethercan complete useful in-the-wild manipulation but present amore challenging control problem. Imitation learning tech-niques focus on collecting large datasets in a variety of set-tings with a dexterous 6-dof arm and a wheeled mobile baseusing teleoperation . Because of thehigh-dimensionality of mobile manipulation, there is alsocontrol methods that leverage synergies between both thebase and the arm and plans together. .",
  ". Discussion and Limitations": "We present SPIN, an approach to train robots that can si-multaneously perceive, interact, and navigate cluttered en-vironments using a data-driven approach. We show that ourRL-based reactive approach is effective for active whole-body control-perception problem, traditionally addressedvia non-reactive planning methods. With recent interest inhumanoid and other mobile robots with actuated cameras,on neck for instance, SPIN is a cost-effective agile whole-body control solution with limited sensing and compute.Although our robot can perceive geometry and avoid ob-stacles using depth, it still operates on stereo-matched depthinstead of raw RGB. This leads to scenarios where it canbump into glass obstacles or shiny surfaces. In the future,we would like to use RGB for perception. AcknowledgementsWe thank Jared Mejia and MihirPrabhudesai for helping with stress-testing in real-world ex-periments. We are also grateful to Zackory Erickson andthe Hello Robot team for their support with the robot hard-ware. This work was supported in part by grants includingONR N00014-22-1-2096, AFOSR FA9550-23-1-0747, andthe Google research award to DP.",
  "Ananye Agarwal, Ashish Kumar, Jitendra Malik, andDeepak Pathak. Legged locomotion in challenging terrainsusing egocentric vision. In Conference on Robot Learning,pages 403415. PMLR, 2023. 5": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-otar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu,Keerthana Gopalakrishnan, Karol Hausman, et al. Do as ican, not as i say: Grounding language in robotic affordances.arXiv preprint arXiv:2204.01691, 2022. 8 Anthony Brohan, Noah Brown, Justice Carbajal, YevgenChebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.Rt-1: Robotics transformer for real-world control at scale.arXiv preprint arXiv:2212.06817, 2022. 8 Wolfram Burgard, Dieter Fox, Daniel Hennig, and TimoSchmidt. Estimating the absolute position of a mobile robotusing position probability grids. In Proceedings of the na-tional conference on artificial intelligence, pages 896901,1996. 8",
  "Ben Burgess-Limerick, Chris Lehnert Jurgen Leitner, andPeter Corke. Enabling failure recovery for on-the-move mo-bile manipulation. arXiv preprint arXiv:2305.08351, 2023.8": "Anthony R Cassandra, Leslie Pack Kaelbling, and James AKurien. Acting under uncertainty: Discrete bayesian modelsfor mobile-robot navigation. In Proceedings of IEEE/RSJInternational Conference on Intelligent Robots and Systems.IROS96, pages 963972. IEEE, 1996. 8 Matthew Chang, Theophile Gervet, Mukul Khanna, SriramYenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, ChrisPaxton, Saurabh Gupta, Dhruv Batra, et al. Goat: Go to anything. arXiv preprint arXiv:2311.06430, 2023. 8",
  "Zipeng Fu, Xuxin Cheng, and Deepak Pathak. Deep whole-body control: learning a unified policy for manipulation andlocomotion. In Conference on Robot Learning, pages 138149. PMLR, 2023. 8": "Saurabh Gupta, James Davidson, Sergey Levine, Rahul Suk-thankar, and Jitendra Malik. Cognitive mapping and plan-ning for visual navigation. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages26162625, 2017. 8 J-S Gutmann, Wolfram Burgard, Dieter Fox, and Kurt Kono-lige. An experimental comparison of localization methods.In Proceedings. 1998 IEEE/RSJ International Conference onIntelligent Robots and Systems. Innovations in Theory, Prac-tice and Applications (Cat. No. 98CH36190), pages 736743. IEEE, 1998. 8",
  "Jiaheng Hu, Peter Stone, and Roberto Martn-Martn. Causalpolicy gradient for whole-body mobile manipulation. arXivpreprint arXiv:2305.04866, 2023. 8": "Glenn Jocher. YOLOv5 by Ultralytics, 2020. 2 Oussama Khatib. The potential field approach and opera-tional space formulation in robot control. In Adaptive andLearning Systems: Theory and Applications, pages 367377.Springer, 1986. 8 N. Koenig and A. Howard.Design and use paradigmsfor gazebo, an open-source multi-robot simulator. In 2004IEEE/RSJ International Conference on Intelligent Robotsand Systems (IROS) (IEEE Cat. No.04CH37566), pages21492154 vol.3, 2004. 5 Jason Ku, Ali Harakeh, and Steven L Waslander. In defenseof classical image processing: Fast depth completion on thecpu. In 2018 15th Conference on Computer and Robot Vision(CRV), pages 1622. IEEE, 2018. 2 Mathieu Labbe and Francois Michaud. Rtab-map as an open-source lidar and visual simultaneous localization and map-ping library for large-scale and long-term online operation.Journal of field robotics, 36(2):416446, 2019. 8",
  "Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan DTardos. Orb-slam: a versatile and accurate monocular slamsystem. IEEE transactions on robotics, 31(5):11471163,2015. 8": "Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A re-duction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth inter-national conference on artificial intelligence and statistics,pages 627635. JMLR Workshop and Conference Proceed-ings, 2011. 5 Stergios I Roumeliotis and George A Bekey. Bayesian esti-mation and kalman filtering: A unified framework for mobilerobot localization. In Proceedings 2000 ICRA. Millenniumconference. IEEE international conference on robotics andautomation. Symposia proceedings (Cat. No. 00CH37065),pages 29852992. IEEE, 2000. 8",
  "Wetestourframeworkinseveralin-the-wildsce-narios,someofwhichareillustratedinFig-ure1.Qualitativevideoresultsareavailableat": "We see emergent behavior where the robot continuouslyavoids dynamic obstacles without seeing them during train-ing. We also observe generalization heavily cluttered indoorto dim-lit outdoor environments. The agent also demon-strates reactive whole-body coordination where it moves itsarm up or down to efficiently navigate across floating obsta-cles instead of re-routing and re-planning base movement,demonstrating 3D spatial awareness.",
  ". Implementation Details": "We make several design choices for the working of ourframework. Firstly, the robot is only allowed to have lo-cal visibility in order to develop highly reactive and instantbehaviors. At any time instant t, the agent can perceive itsenvironment within a range of 2m in all 4 direction front,back, left and right based on the cameras viewing directionand its field of view. We also empirically observe that givenlarger viewing range, say > 5m which contains informationof more than 4-5 nearest obstacles to the agent makes it asub-global path planning problem which becomes harder tooptimize, leading to degraded behavior and performance asreported in .",
  "5m0.861.21": ". We report the average success rate and average distance togoal for 10 episodes across 3 seeds each with different maximumvisibility range for the agent at any time instant. As reported, withbroader visibility, the agent shows more frequent stalling leadingto higher average distance to goal. Secondly, contrary to standard teacher-student architec-tures which use privileged system information for trainingthe teacher, we restrict the privileged information to onlyelements in the robots field-of-view that can be retrievedfrom the ego-view in the same state. For this, we project theobstacle scandots onto the image plane and pass only thosescandots to the robot observation which lie in the camerasfield-of-view. Note that, if this were not the case, the robot does not learn camera movement in a relevant fashion andalso becomes harder to distill into a depth-conditioned stu-dent policy through only ego-centric view. Similarly, for the3-phase decoupled visuo-motor optimization, we induce in-formation bottleneck with a low-dimensional latent spaceof size 16 for the scandots latent while training the teacherpolicy, which again helps it in attending to most relevantinformation at any time instant t in order to make studentpolicy distillation feasible. Observation Space.The observation space for the robotcomprises of joint positions (q), joint velocities (qvel), end-effector position peef, goal position (pgoal) and depth latent(z) containing visual information about the environment.Note that during phase 1 training in simulation, scandots(z) are used as a proxy for faster depth rendering and laterdistilled into a egocentric depth-conditioned policy. Duringreal-world deployment, peef is obtained via forward kine-matics and other proprioception information is obtained di-rectly from the robot. Action Space.The action space of the robot consists ofthe velocity for base rotation as well as translation and jointpositions for all the other joints including arm, camera aswell as gripper actions. The gripper action is a continu-ously varying scalar which can actuate the gripper to differ-ent extents, unlike a binary action indicating open or closedgripper. Reward Scales.We use a distance and forward progressgoal for reaching, a binary reward for grasping and a contin-uous shaped reward for lifting the object to a certain heightabove the table. We also add a small penalty for the jointvelocities to the arm stretch and camera joints for a tempo-rally smooth gait which permits easier sim2real transfer aswell as more appropriate behaviors leading to less jitter andmore consistent movements on the real hardware.The reward scales used for goal reaching, grasping andlift rewards are reported in . Detailed formulationsof the reward functions are described in .1. Network Architecture and Training Details.The actorand critic for teacher policy are LSTM with 256 hiddenunits, with input as prorpioception, goal and scandots latent.The scandots are compressed using a pointnet architecturefor permutation invariance. The depth network for the stu-dent policy takes as input a low-resolution depth image of",
  "size 58 87 and comprises of 3-layer convolution back-bone followed by 3 fully-connected layers. We use AdamOptimizer with an initial learning rate of 1e 3, entropycoefficient of 5e 4 and as 0.99": "Asynchronous DAgger Training.Since depth render-ing on simulators is a computational bottleneck, we imple-ment an asynchronous version of DAgger algorithm whichsimultaneously collects data in a buffer and trains the stu-dent policy with batches sampled from the collected datausing 2 parallel processes. This provides a 2.5 computa-tional speedup over the non-parallelized version of the algo-rithm, allowing faster convergence of the student network.We also find that freezing the weights of the student actorpre-initialized from the teacher policy for first 1000 itera-tions helps as warm-up steps to the depth convolution back-bone for stable training. Post-processing for clean depth images.To mitigate theissues due to noisy depth, we post-process the depth ob-tained from the Intel RealSense Camera using a real-timefast hole-filling algorithm for depth images . With thecamera constantly in motion, there are additional artefactswith depth images. For this, we additionally use temporalfiltering over the stream of depth images.",
  ". Success rate for 4 FixCam poses in easy (E), medium(M), hard (H) envs": "Object Detection for Pick PolicyOnce the robot reachesnear the goal, we randomly select an object within its fieldof view in order to be grasped and fetched to a target loca-tion. For getting the target object location, we run YOLO, a real-time object detection model with an average in-ference speed of 20ms. We use the corresponding depth im-age to deproject the pixel point into a 3D-coordinate whichis passed as the new goal position to the manipulation pol-icy.",
  ". Analysing camera and base motion": "Fixed Camera BaselineWe run FixCam baseline with 4camera poses () I: Front, II: Down, III: Downand slightly front, IV: Front and slightly down on easy (E),medium (M), hard (H) environments. I, IV with max fovhave much lower success than SPIN, implying active visionis required in clutter. Pose (IV depicts the FixCam baselinereferred in the paper. Camera Movement and Camera Observations:Weshow camera trajectory in .When navigatingthrough clutter (frames 1, 2, 4), it tilts downward to max-imize fov near the base, but with no nearby obstacle (frame3), it faces front. Detailed movement of the camera can beseen on the website along with paired RGBD images forrollouts. RGB frames are only for analysis, the policy onlyobserves depth images. . Camera movement analysis in a trajectory. The agent faces the camera downward when navigating through tightly clutteredvicinity as can be seen in the first, second and fourth frame, whereas the camera points more towards the front when there are no immediateobstacles in the direction of movement, as illustrated in the third frame. . Scenarios where whole-body coordination is essential under heavy obstructions. In the above cases where the obstacles aretightly packed, it is not possible for the robot to navigate through them avoiding collisions without lifting the arm to an appropriate height. Necessity for Active Vision and Whole-body coordina-tionActive Vision: In principle, a multi-camera systemshould be equivalently adequate, however most views willcontain insignificant information and require large modelsto process. With limited onboard compute on most robotsand requirement for real-time reactivity (< 0.1s), it be-comes infeasible to deploy them with larger vision back-bones.Whole-body coordination (WBC): Under heavy obstruc-tions, the robot cannot move without collision if the base& arm control are decoupled. shows such a sce-nario where a fixed arm and gripper close to base would failwithout WBC, which also allows it to use the extra degreeof freedom to find shorter and more efficient paths.",
  ". Directly training from depth images": "We compare training from depth (red) and our 2-phasemethod with scandots (blue) in a medium difficulty envi-ronment as illustrated in . Depth policy has < 1%success after 22h training, whereas total (phase1 + 2) wall-clock time for SPIN is 16h (6+10). The simulator gives 50k fps for scandots (8192 envs) and 820 fps for depth(256 environments maximum parallel environments that . Success rate for scandots (blue) vs depth (red). Thedepth-based policy attains close to 0 performance even after 400kenv steps of training, whereas the policy trained with scandots in-creasingly improves over time. can fit on a single GPU), causing 61 slow-down bottle-neck. This shows the necessity and efficiency of our pro-posed 2-phased coupled visuomotor optimization approachusing scandots over naively training an RL policy fromdepth observations.",
  ". Classical Navigation Baseline": "As discussed in , we compare our method witha classical map-based baseline which uses the 2D RPLidarto build an environment map and then creates a plan usingMonte Carlo method. We observe that for > 90% of thecases, the robot is able to build a map and find a feasiblepath, however it is not able to execute the planned path for> 85% cases. This issue arises due to noisy control or unex-pected wheel motion due to terrain differences. Our methodis able to overcome such failures due to a constant feedbackand reactive improvisation through proprioception as wellas depth, which allows it to deal with uncertainties withoutrequiring a pre-built environment map. Moreover, due to lo-calization inaccuracies, the baseline method is often unableto reach the intended goal if not initialized in the same ori-entation as was used before building the map. Contrary tothat, we heavily randomize all degrees of freedom as well asthe robot orientation at the beginning of every rollout dur-ing test time. Moreover, since the robot has a 2D Lidarinstalled on it, we do not test it in environments with float-ing obstacles which would require 3D understanding andwhole-body coordination to navigate through clutter. Weshow some visualizations of the map built and plans createdby the robot in ."
}