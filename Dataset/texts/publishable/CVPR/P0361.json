{
  "Abstract": "As an important and practical way to obtain high dy-namic range (HDR) video, HDR video reconstruction fromsequences with alternating exposures is still less explored,mainly due to the lack of large-scale real-world datasets.Existing methods are mostly trained on synthetic datasets,which perform poorly in real scenes. In this work, to fa-cilitate the development of real-world HDR video recon-struction, we present Real-HDRV, a large-scale real-worldbenchmark dataset for HDR video reconstruction, featuringvarious scenes, diverse motion patterns, and high-qualitylabels. Specifically, our dataset contains 500 LDRs-HDRsvideo pairs, comprising about 28,000 LDR frames and4,000 HDR labels, covering daytime, nighttime, indoor, andoutdoor scenes. To our best knowledge, our dataset is thelargest real-world HDR video reconstruction dataset. Cor-respondingly, we propose an end-to-end network for HDRvideo reconstruction, where a novel two-stage strategy isdesigned to perform alignment sequentially. Specifically,the first stage performs global alignment with the adap-tively estimated global offsets, reducing the difficulty of sub-sequent alignment. The second stage implicitly performs lo-cal alignment in a coarse-to-fine manner at the feature levelusing the adaptive separable convolution. Extensive exper-iments demonstrate that: (1) models trained on our datasetcan achieve better performance on real scenes than thosetrained on synthetic datasets; (2) our method outperformsprevious state-of-the-art methods. Our dataset is availableat",
  "AHDRNet Kalantari19Chen21CA-ViTLAN-HDR": ". Row 1 shows a real-world sample from the Chen21dataset. Row 2-3 show the HDR frames reconstructed by mod-els trained on the synthetic dataset and our Real-HDRV, re-spectively. Obviously, models trained on our dataset are able torecover more and better details of the over-exposed regions. ever, most cameras cannot capture HDR videos directlydue to the limitations of sensors. Therefore, some special-ized hardware devices are developed tocapture HDR videos. However, these devices are typicallybulky and expensive, which are not widely adopted .In contrast, the computational-based HDR video recon-struction is more practical and affordable for ob-taining HDR videos. It captures low dynamic range (LDR)sequences with alternating exposures (e.g., sequences withexposure values of {-3,0,-3,0,...}), which are then used toreconstruct the corresponding HDR video. The commonreconstruction pipeline is to align the input frames and thenmerge the aligned inputs to reconstruct the HDR videos.Before the era of deep learning, some optimization-basedreconstruction methods are proposed. Recently,learning based methods have shown their effec-tiveness on HDR video reconstruction, which significantlyimprove the performance over optimization-based methods.Despite remarkable progress, the development of deepmodels for HDR video reconstruction is relatively slow,mainly due to the lack of suitable training datasets. Theonly publicly accessible labeled real-world dataset of HDR",
  "arXiv:2405.00244v1 [cs.CV] 30 Apr 2024": "video reconstruction is built for evaluating HDR videoreconstruction methods. The number of distinct scenes andthe motion patterns in their dataset are limited, making itunsuitable for supervised training. Therefore, existing mod-els are still trained on synthetic datasets. However, the syn-thetic datasets are not well suited for the study of real-worldHDR video reconstruction.Models trained on syntheticdatasets are hard to generalize to real scenes (see Row2) since the synthetic degradations are far different from thereal degradations (e.g., the noise in under-exposed areas, thesaturation in over-exposed areas). It is highly desired for alarge-scale real-world dataset to facilitate the developmentof real-world HDR video reconstruction.Besides the datasets, another key issue of HDR video re-construction lies in the alignment of input frames. Previousmethods usually use optical flow or both opticalflow and deformable convolution to align the inputs.However, the estimated flows are prone to be inaccurate dueto the noise and saturation in alternatingly-exposed inputs,resulting in ghosting artifacts. Recently, Chung et al. proposed a luminance-based attention module to align theinputs. However, it cannot properly deal with the under-exposed and over-exposed areas of inputs, resulting in un-pleasing artifacts. Additionally, the global motion (causedby camera movements) is not properly modeled in most ex-isting methods , which further increases the difficultyof alignment, leading to inferior performance.Based on the above observations, to facilitate the devel-opment of real-world HDR video reconstruction, we builda large-scale real-world dataset, named Real-HDRV. In or-der to get LDRs-HDRs video pairs and ensure the quality ofHDR labels, we capture the scene in a frame-by-frame man-ner using a camera with high continuous shooting speed (upto 40 frames/sec). Specifically, we carefully select the rel-atively static scenes and manually create different types ofmotion between neighboring frames. For each static frame,we capture a multi-exposure image stack (7 differently-exposed LDR images guarantee the quality of HDR labels).The images in each multi-exposure stack are then used tosynthesize the corresponding HDR label. We collected 500LDRs-HDRs video pairs, comprising about 28,000 LDRframes and 4,000 HDR labels. Our Real-HDRV cannot onlyserve as a benchmark for HDR video reconstruction but alsobe applied to other HDR tasks (e.g., HDR Deghosting ,single-image HDR reconstruction ).Correspondingly, we propose an end-to-end network forHDR video reconstruction, in which we design a two-stage strategy to align the inputs sequentially.Specifi-cally, the first stage performs global alignment with thedesigned global alignment module (GAM), which can ef-fectively handle the global motion and reduce the difficultyof subsequent alignment. The second stage implicitly per-forms local alignment at the feature level in a coarse-to-fine manner with the designed local alignment module (LAM).The pyramid structure of LAM facilitates the feature align-ment under large motion. The adaptive separable convo-lution used in LAM enables flexibly integrating theuseful information in neighboring frames to compensate forthe missing content in the reference frame, which facilitatesthe feature alignment under noise and saturation. Then, areconstruction module is applied to reconstruct the HDRvideo from the aligned features. Our two-stage alignmentnetwork can effectively handle complex motion and recon-struct high-quality HDR video. In summary, our contribu-tions are as follows: We propose a large real-world HDR video reconstructiondataset, featuring various scenes, diverse motion patterns,and high-quality labels. Our dataset cannot only serve asa benchmark for HDR video reconstruction but also beapplied to other HDR imaging tasks. We propose an end-to-end network for HDR video re-construction, in which we design a two-stage strategy toperform alignment sequentially. Our network can effec-tively handle complex motion and achieve high-qualityHDR video reconstruction.",
  "HDR Image ReconstructionMany methods[5, 16, 22,": "33, 34] attempt to perform HDR reconstruction from a sin-gle LDR image.However, these methods cannot effec-tively handle the noise and saturation due to the limitedinformation in a single image. There are methodsfor HDR reconstruction from multi-exposure LDR images.Although these methods work well for static scenes, theygenerally suffer from ghosting artifacts when tackling dy-namic scenes. Therefore, many HDR deghosting methods are proposed to alleviate this issue. HDR Video Reconstruction Datasets Kalantari et al. captured 5 LDR sequences with two alternating exposures.To quantitatively evaluate HDR video reconstruction meth-ods, Chen et al. collected 76 dynamic image pairs, 49static image pairs, and 50 unlabeled LDR sequences withtwo alternating exposures. However, the number of distinctscenes and the motion patterns in their dataset are limited,making it unsuitable for supervised training. Recently, Yueet al. collected 85 real-world LDRs-HDRs video pairsusing a mobile phone, but, until now, they are not publiclyaccessible. In addition, they use two images with differentexposures to generate an HDR label, which may not coverthe full dynamic range of the scene, resulting in limited-quality HDR labels. Due to the lack of publicly accessi-",
  "Oursper frame500GM, LM, FMID, IN, OD, ON": "1. Our dataset contains per-frame HDR labels, while the Chen21 dataset only containsthe HDR labels for the center frames.2. GM and LM denote global motion (where only the camera is moving) and localmotion (where only the foreground is moving), respectively. FM denotes full motion(where both foreground and camera are moving).3. OD, ON, ID and IN denote outdoor daytime, outdoor nighttime, indoor daytimeand indoor nighttime, respectively. ble large-scale real-world datasets, existing models are stilltrained on the synthetic dataset , which hinders the de-velopment of real-world HDR video reconstruction.HDR Video Reconstruction There are mainly two typesof methods to obtain HDR videos: hardware-based methodsand computational-based methods.The hardware-basedmethods typically rely on specialized hard-ware systems (e.g., beam splitter), which are typically tooexpensive to be widely adopted.The computational-based methods reconstruct the HDRvideo from alternatingly-exposed sequences.Kang etal. proposed the first method in this direction, whichused optical flow to align the input frames and then mergedthe aligned frames to generate HDR videos. Mangiat etal. improved by introducing a block-based mo-tion estimation method with a refinement stage for ghost re-moval. Kalantari et al. proposed a patch-based methodto synthesize the missing exposures at each frame, and thesesynthesized images are then fused into an HDR frame.Recently, learning based methods have shown their ef-fectiveness on HDR video reconstruction.Kalantari etal. proposed a flow-based framework, which consistsof an optical flow network for alignment and a weight net-work for merging images. Chen et al. and Yue et al. used both optical flow and deformable convolution to per-form alignment for reconstructing HDR videos. Unfortu-nately, these methods typically generate ghosting artifactssince the estimated flows are prone to be inaccurate due tothe noise and saturation. More recently, Chung et al. proposed a luminance-based alignment network for HDRvideo reconstruction. However, it cannot properly deal withthe under-exposed and over-exposed areas of inputs, result-ing in unpleasing artifacts.",
  ". Proposed Dataset": "To favor the development of real-world HDR video re-construction, we construct a large-scale real-world HDRvideo reconstruction dataset, named Real-HDRV. Actu-ally, it is extremely challenging to simultaneously capturealternatingly-exposed LDR sequences and the correspond-ing HDR sequences for dynamic scenes. One may use abeam splitter and two cameras to build a complex optical system to capture two videos with different exposures si-multaneously and then generate the HDR video using thecaptured two videos. However, the amount of light is halvedby the beam splitter, which limits the quality of HDRlabels. Instead of relying on complex optical systems, to getLDRs-HDRs video pairs and ensure the quality of the HDRlabels, we capture LDRs-HDRs video pairs in a frame-by-frame manner. We manually create motions between neigh-boring frames and capture a multi-exposure image stack (7LDR images) for each static frame.One crucial problem that needs to be addressed is how toensure that the high-quality HDR label can be obtained af-ter capturing each multi-exposure image stack. The detailsare as follows: (1) We use a camera with high continuousshooting speed (up to 40 frames/sec), the Canon R6 Mark2,which enables us to capture a multi-exposure image stackin a very short period of time with one depression of thewireless shutter. During the capturing, the camera can al-most avoid introducing extra motion, such as camera shake,object movement, etc. (2) We carefully select the relativelystatic scenes and manually create different types of motion(i.e., global motion, local motion, and full motion) betweenneighboring frames to make the motion controllable and di-verse. In addition, the camera is mounted on a tripod, and awireless remote controller is used to avoid introducing extramotion caused by the shutter release.Thanks to the high-speed shooting performance of thecamera and the careful shooting procedure, we can ensurethere is almost no motion between the images in each multi-exposure image stack.Also, each multi-exposure imagestack can provide enough images with different exposures(7 images with different exposures guarantee to cover thefull dynamic range of a scene). Then, the per-frame high-quality HDR label can be generated from images in eachmulti-exposure image stack by using the method in , andthe LDR images can be arranged in a periodic exposure togenerate sequences with alternating exposures1. Therefore,we can collect LDRs-HDRs video pairs in a frame-by-framemanner. Specifically, for each frame, we capture a multi-exposure image stack of 7 LDR images spaced by i-EVdifference where i 1, 2, 3 around the reference exposure.Then, we manually create different types of motion betweenneighboring frames to capture the following image stacks.Finally, all the image stacks are grouped in their temporalorder to generate the LDRs-HDRs video pairs.In total, we collected 500 LDRs-HDRs video pairs, eachcontaining 7 to 10 frames. For each frame, 7 differently-exposed LDR images and a high-quality HDR label can be 1Since we focus on HDR video reconstruction from sequences with twoalternating exposures, we selected LDR frames in a periodic exposure (i.e.,{EV-3, EV0, EV-3, EV0, ...}, {EV-2, EV+1, EV-2, EV+1, ...} or {EV-1,EV+2, EV-1, EV+2, ...}.) to generate the sequences with two alternatingexposures. Note that the LDR frames can be selected in different exposureorders for HDR video reconstruction.",
  "(b)(c)(d)(e)": ". (a) Some typical scenes in our dataset, which can be categorized into 4 categories: indoor daytime (ID), indoor nighttime (IN),outdoor daytime (OD), and outdoor nighttime (ON) scenes. (b) Our dataset contains three kinds of motion: global motion (where only thecamera is moving), local motion (where only the foreground is moving), and full motion (where both foreground and camera are moving).(c) Scene and motion distributions of our dataset. (d) Diversity comparison: our dataset vs. the Chen21 dataset . (e) Statistics of motiondirections in our dataset. We plot a circular histogram, where the color of each bin represents the direction of motion, and the height of thebar represents the proportion of specific directions to all the directions. The per-pixel flow in each frame is computed via RAFT.",
  "the highest 2% luminance and the lowest 2% luminance": "Among these aspects, greater extent of HDR represents more probability for the network to learn pixel in ad-vanced HDR volume beyond LDRs capability, higher intra-frame diversity means that the network may learnbetter generalization capability. We use these metrics to verify the diversity of our dataset provided. All the images are captured in RAW format withresolution of 60004000. We performed the demosaicing,white balancing, color correction, and gamma compression( = 2.2) to convert the raw data to RGB data. In this work,we rescaled the images to 15001000 for training and test-ing. shows some typical scenes in our dataset andthe statistical indicators of our dataset. In addition, sinceour dataset provides data in RAW format, the data process-ing pipeline is highly flexible. Therefore, our dataset canbe easily adjusted to make training data for different HDRtasks for future research.Analysis of Our DatasetTo quantitatively evaluate thesuperiority of our dataset, we analyzed the diversity of theChen21 dataset and our dataset. Following , the7 metrics defined in are utilized to assess the diver-sity of different datasets from 3 aspects, including the ex-tent of HDR, the intra-frame diversity and the overall styleof HDR. For each HDR label, 7 different metrics are calcu-lated according to . Then, we use the t-SNE",
  "Chen21 8.852.468.932.7711.355.292.54Ours13.752.729.164.3012.135.052.73": "to project the single frames 7-D vector (consisting of 7metrics from ) to the corresponding 2D-coordinatefor plotting the frame distribution of our dataset and theChen21 dataset. As shown in (d), our dataset con-tains wider frame distribution than the Chen21 dataset, in-dicating that the networks trained with our dataset may bebetter generalized to different scenarios. And the statisticsof different datasets are shown in . In addition, ourdataset contains more diverse motion patterns (see and (c)). The diversity in both scenes and motionpatterns makes that our dataset can naturally be used fortraining deep networks and assessing the generalization ca-pability of the networks across different scenes.",
  ". Proposed Method": "Global motion (caused by camera movements) and localmotion (caused by object motion) are almost inevitablewhen capturing videos, which imposes a core issue for HDRvideo reconstruction: how to perform alignment for thealternatingly-exposed inputs. Without effective alignment,the areas with motion in neighboring frames cannot be prop-erly utilized to reconstruct the HDR frame, leading to severeghosting artifacts. In this work, considering the differencesbetween the global motion and local motion, we introducea two-stage alignment network for HDR video reconstruc-",
  ". The architecture of our proposed network": "tion, which firstly performs alignment for the inputs (fromglobal to local) and then adaptively fuses the aligned fea-tures to reconstruct the HDR video.Overview Given an input LDR video {Ii|i = 1,...,n} withalternating exposures {ti|i = 1,...,n} 2, our target is to recon-struct the corresponding HDR video {Hi|i = 1,...,n}. Fol-lowing , the input images are firstly mapped into thelinear HDR domain by applying gamma correction:",
  "Ii = Ii /ti, ( = 2.2) .(1)": "where ti is the exposure time of Ii. Then, the input im-age Ii and the linear image Ii are concatenated into a 6-channels input Xi.Our network takes three continuousframes {Xi1, Xi, Xi+1} as input and predicts the HDRframe Hi for the center frame. As shown in , our net-work consists of the global alignment module (GAM) forcompensating global motion, the local alignment module(LAM) for compensating local motion, and the reconstruc-tion module for reconstructing the HDR frame.Global Alignment ModuleThe global motion is rela-tively simple, which does not need to be modeled by densepixel-wise optical flow with high Degree-of-Freedoms. In-spired by , we use the pre-defined offset bases with8 Degree-of-Freedoms (with each 2 for translation, rotation,scale, perspective ) to model the global motion. Specif-ically, we design the GAM to estimate a weighted sum of8 pre-defined offset bases for generating the global offsets.The global offsets are then used to spatially transform theinputs. Since all the operations in the GAM are differen-tiable, the GAM can be optimized through end-to-end train-ing. In this way, the GAM can adaptively learn to compen-sate for the global motion between neighboring frames.As shown in , given the input {Xj|j = i 1, i, i +1}, the GAM firstly uses a shared encoding layer to extractfeature maps Gj with 16 channels from inputs. Then, the 2For example, the input sequences can alternate between two exposures{EV0, EV+3, EV0, EV+3, ...} or three exposures {EV-2, EV0, EV+2,EV-2, EV0, EV+2, ...}. In this work, we reconstruct the HDR video fromsequences with two alternating exposures, while our network can be easilyextended to other cases (e.g., three exposures). features {Gj|j = i 1, i + 1} of neighboring frames arefed into the weights estimation module E (.) (see our sup-plementary file for the detailed architecture) along with thefeature map Gi of the reference image to obtain the corre-sponding weights {1k, 2k}, generating the global offsets:",
  "k=12knk(k = 1, 2, ..., 8) .(2b)": "where the pre-defined offset bases nk are computed withthe same settings in . The global offsets are then usedto spatially transform the neighboring frames for compen-sating global motion between neighboring frames.Local Alignment ModuleThe LAM is designed to per-form local alignment, which estimates the kernel weightsat multiple scales in a coarse-to-fine manner and thenperforms a transformation for input features using adap-tive separable convolution with the estimated kernelweights. In this way, the LAM can adaptively learn to inte-grate useful information in neighboring frames to compen-sate the missing content in reference frame, which facili-tates the feature alignment under the noise and saturation.First, the shallow features {Fj|j = i 1, i, i + 1} of in-put images are extracted. Inspired by Mertens et al. , thecontrast maps cj, exposure wellness maps ej, and saturationmaps sj are extracted to provide the exposure informationof the inputs. All three maps are concatenated together togenerate the adaptive masks Mj for input images. To handlelarge motions, the pyramidal processing is adopted, we gen-erate an L-level pyramid of feature representation {F lj |j =i 1, i, i + 1; l = 1, ..., L} for input images and an L-levelpyramid of masks {M lj |j = i 1, i, i + 1; l = 1, ..., L}.Then, we concatenate the corresponding masks and fea-tures along the channel dimension at each level to obtain thepyramid of tensors {Klj|j = i 1, i, i + 1; l = 1, ..., L},which are then utilized to predict kernel weights W lj forneighboring features. With the predicted kernel weights, thealigned features Alj can be obtained after performing adap-",
  "Ali+1 = hASConvF li+1, W li+1,Al+1i+12. (3c)": "where (.)2 is the upscaling operation with a factor of2, [.] is the concat operation, g (.) is the kernel weights pre-dictor consisting of several convolution layers, ASConv (.)denotes the adaptive separable convolution, h (.) is the gen-eral function with several convolution layers. We use three-level pyramid, i.e., L=3, in LAM. The kernel size is set to31 in the adaptive separable convolution.Fusion and ReconstructionThe fusion module is usedto fuse the aligned features, which can suppress the harm-ful features from under-exposed and over-exposed areas. Asshown in , the aligned features are concatenated as theinput of the fusion module, generating fusion masks for fus-ing the aligned features Aj. Then, the fused feature Ffusionis passed through a series of residual blocks. Two skip con-nections are added to concatenate shallow features of thereference frame. Finally, the HDR frame H can be obtainedafter a convolution layer and a sigmoid activation layer.Loss Function Since HDR images are typically displayedafter tonemapping, following , we use the -lawtonemapping function to the HDR image:",
  ". Experimental Settings": "DatasetsThere are three datasets adopted, includingour Real-HDRV, the Chen21 dataset and the syntheticdataset . As for our dataset, it is divided into the trainingcollection (450 videos) and the testing collection (27 videosfor indoor daytime and outdoor daytime scenes, 23 videosfor indoor nighttime and outdoor nighttime scenes). Eachvideo in the testing collection provides 8 LDR frames withtwo alternating exposures and the corresponding HDR la-bels. As for the synthetic dataset, we utilized 21 existingHDR videos from and Vimeo-90K to synthe-size the dataset with the same settings as in . The Chen21dataset contains 76 dynamic image pairs, 49 static imagepairs augmented with random global motion, and 50 unla-beled sequences with two alternating exposures.Implementation DetailsWe generate LDR sequenceswith two alternate exposures separated by three stops foreach video in our training collection. We then sample threeLDR frames as input and produce the HDR label for thecenter frame to generate a training sample. We crop patchesof size 256256 with a stride of 128 from the training setfor training. Random rotation and flipping augmentationare applied. We use Adam optimizer, and set the batch sizeand initial learning rate as 4 and 0.0001, respectively. Weimplement our model using PyTorch with 6 NVIDIA 3090GPUs and train for 100 epochs.Evaluation Metrics We use six common metrics for test-ing, i.e., HDR-VDP-2 , PSNR-, SSIM-, PU-PSNR,PU-SSIM and HDR-VQM . PSNR- and SSIM- arecomputed after tonemapping with -law function (in Equa-tion (4)). PU-PSNR and PU-SSIM are computed after per-ceptually uniform encoding .When computing theHDR-VDP-2, the diagonal display size is set to 30 inches.",
  ". Evaluation of Our Proposed Dataset": "To evaluate the effectiveness of our dataset, we compare ourdataset with the synthetic dataset . We train representa-tive HDR reconstruction models on ourdataset and the synthetic dataset, and evaluate the perfor-mance of trained models on the Chen21 dataset .Quantitative ResultsAs shown in , the modelstrained on our dataset can achieve better performance onthe real-world dataset than the models trained on thesynthetic dataset, demonstrating the effectiveness of our . Quantitative comparison for training on the synthetic dataset or our dataset, while evaluating on the Chen21 dataset . Thebetter results are highlighted in bold. Among these evaluation metrics, the higher quality of the tested HDR image leads to the higher score.",
  ". Visual comparison of different models trained on thesynthetic dataset and our dataset": "dataset. For example, compared with Kalantari19 trainedon the synthetic dataset, the same model trained on ourdataset can achieve more than 1 dB gain (PSNR-) on thedynamic set of Chen21 dataset, which is significant. Similarimprovements can also be observed in other methods. Qualitative ResultsThe visual comparison for the mod-els trained on different datasets is provided in . Obvi-ously, the models trained on our dataset yield better visualquality, while the models trained on the synthetic datasettypically yield severe ghosting artifacts or color distortions.The superior performance of the models trained with ourdatasets comes from the real degradation distribution in ourdataset (more qualitative comparisons are provided in sup-plementary file). In summary, models trained on our Real-HDRV can better handle real-world scenes, demonstratingthe effectiveness of our dataset. Evaluation on Unlabeled Real-World DatasetWe alsoevaluate the varying models on unlabeled sequences of theChen21 dataset . The visual comparison is provided in. As seen, when the reference frame is low-exposure,the models trained on our dataset can recover clear details,while the models trained on the synthetic dataset generate",
  ". Evaluation of Our Proposed Method": "We compare our method with prevalent state-of-the-artHDR video reconstruction methods andstate-of-the-art HDR deghosting methods on ourdataset for a comprehensive evaluation. For a fair compar-ison, we use their officially released codes, if accessible,otherwise, we re-implement their methods based on theirpapers. Note that the AHDRNet and CA-ViT areadapted for HDR video reconstruction by changing the net-work input. In addition, we evaluate our method on theChen21 dataset to demonstrate the generalization of ourmethod (more details can be found in supplementary file).Quantitative ResultsThe quantitative comparison be-tween our method and other methods is listed in .Compared to other methods, our method achieves the bestaverage performance in all the evaluation metrics, demon-strating the effectiveness of our method. In addition, eval-uated in different scenes, our method can also acquire thebest performance, demonstrating that our method can betterhandle sequences under different real scenes.Qualitative ResultsThe visual comparison of varyingmethods on our dataset is shown in . Obviously, our . Quantitative comparison of our method with state-of-the-art methods on our dataset. Red text indicates the best and blue textindicates the second best result, respectively. ID&OD denotes indoor daytime and outdoor daytime scenes. IN&ON denotes indoornighttime and outdoor nighttime scenes.",
  "Params. (M)10.396.443.57.315.98Flops (T)2.135.293.961.242.07Time (s)0.240.870.760.550.34": "method achieves more excellent visual quality than othermethods, which can recover the missing content of the over-exposed areas without introducing artifacts when the ref-erence frame is high-exposure (see the 1st row in ).Also, our method can better remove the noise and faithfullypreserve the structure of the under-exposed areas when thereference frame is low-exposure (see the 2nd row in ).In contrast, due to the inaccurate-prone flow, the flow-basedmethods usually suffer from unpleasing arti-facts for the over-exposed areas, and they cannot faithfullyrecover the details in the under-exposed areas. Additionally,due to the lack of effective alignment, the attention-basedmethods can easily introduce ghosting artifacts. Complexity ComparisonsFor each test method, werecord the quantity of the model parameters, the executiontime and the floating point operations (flops) of generatingan HDR frame with the size of 1500 1000. As shownin , our model can achieve the best trade-off betweenthe performance and the computational cost. Ablation Study To analyze the effectiveness of each com-ponent in our network, we conduct comprehensive ablationstudies on our dataset. As shown in , the GAM andLAM both improve the performance, demonstrating the ef-fectiveness of the GAM and the LAM. On the one hand, themodel (Baseline) performs poorly when directly conducting",
  "Model159.4942.6785.34Model261.7243.0787.62Model362.5843.6188.45Model463.5343.8689.71": "HDR video reconstruction without performing alignment,demonstrating that the alignment is very critical to HDRvideo reconstruction. On the other hand, by using the GAMto perform global alignment, our full model can more ef-fectively handle the complex motion, obtaining the higherHDR-VDP-2 score and the higher HDR-VQM score thanours (w/o GAM). Also, our full model can achieve betterperformance than ours (w/o LAM).",
  ". Conclusion": "We constructed a novel dataset for HDR video recon-struction, which contains various scenes, diverse motionpatterns, and high-quality labels. Our dataset can also beapplied to other HDR tasks for future research. Then, weproposed a novel framework for HDR video reconstruction,which considers the differences between global motionand local motion. The designed GAM enables our frame-work to better handle global motion.And the designedLAM can adaptively integrate the useful informationin neighboring frames to help reconstruct the referenceHDR frame, effectively decreasing the ghosting artifactscaused by large local motion.Extensive experimentsdemonstrate the superiority of our dataset and our method.",
  "ITU,Geneva,Switzerland,RecommendationITU-RBT.500-14: Methodologies for the subjective assessment ofthe quality of television images, 2019. 4": "Nabajeet Barman and Maria G Martini. User generated hdrgaming video streaming: Dataset, codec comparison, andchallenges. IEEE Transactions on Circuits and Systems forVideo Technology, 32(3):12361249, 2021. 1 Sibi Catley-Chandar, Thomas Tanay, Lucas Vandroux, AlesLeonardis, Gregory Slabaugh, and Eduardo Perez-Pellitero.Flexhdr: Modeling alignment and exposure uncertainties forflexible hdr imaging. IEEE Transactions on Image Process-ing, 31:59235935, 2022. 2 Guanying Chen, Chaofeng Chen, Shi Guo, Zhetong Liang,Kwan-Yee K Wong, and Lei Zhang. Hdr video reconstruc-tion: A coarse-to-fine network and a real-world benchmarkdataset. In IEEE International Conference on Computer Vi-sion (ICCV), pages 25022511, 2021. 1, 2, 3, 4, 5, 6, 7,8 Xiangyu Chen, Yihao Liu, Zhengwen Zhang, Yu Qiao, andChao Dong. Hdrunet: Single image hdr reconstruction withdenoising and dequantization. In IEEE Conference on Com-puter Vision and Pattern Recognition Workshops (CVPRW),pages 354363, 2021. 2 Haesoo Chung and Nam Ik Cho. Lan-hdr: Luminance-basedalignment network for high dynamic range video reconstruc-tion. In IEEE International Conference on Computer Vision(ICCV), pages 1276012769, 2023. 1, 2, 3, 6, 7, 8 Paul E. Debevec and Jitendra Malik. Recovering high dy-namic range radiance maps from photographs. In ACM In-ternational Conferenceon Computer Graphics and Interac-tive Techniques (SIGGRAPH), pages 369378, 1997. 3 Jan Froehlich, Stefan Grandinetti, Bernd Eberhardt, SimonWalter, Andreas Schilling, and Harald Brendel.Creatingcinematic wide gamut hdr-video for the evaluation of tonemapping operators and hdr-displays. In Digital photographyX, pages 279288, 2014. 3, 6 Orazio Gallo, Alejandro Troccoli, Jun Hu, Kari Pulli, and JanKautz. Locally non-rigid registration for mobile hdr photog-raphy. In IEEE Conference on Computer Vision and PatternRecognition Workshops (CVPRW), pages 4855, 2015. 2",
  "Yulia Gryaditskaya, Tania Pouli, Erik Reinhard, KarolMyszkowski, and Hans-Peter Seidel. Motion aware expo-sure bracketing for hdr video. Computer Graphics Forum,34(4):119130, 2015. 1": "Cheng Guo, Leidong Fan, Ziyu Xue, and Xiuhua Jiang.Learning a practical sdr-to-hdrtv up-conversion using newdataset and degradation models.In IEEE Conference onComputer Vision and Pattern Recognition (CVPR), pages2223122241, 2023. 4 Jin Han, Yixin Yang, Peiqi Duan, Chu Zhou, Lei Ma, ChaoXu, Tiejun Huang, Imari Sato, and Boxin Shi. Hybrid highdynamic range imaging fusing neuromorphic and conven-tional images. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2023. 1, 3",
  "Jun Hu, Orazio Gallo, Kari Pulli, and Xiaobai Sun.Hdrdeghosting: How to deal with saturation? In IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 11631170, 2013. 2": "Xiangyu Hu, Liquan Shen, Mingxing Jiang, Ran Ma, andPing An. La-hdr: Light adaptive hdr reconstruction frame-work for single ldr image considering varied light conditions.IEEE Transactions on Multimedia, pages 116, 2022. 2, 4 Anthony Huggett, Chris Silsby, Sergi Cami, and Jeff Beck. Adual-conversion-gain video sensor with dewarping and over-lay on a single chip. In IEEE International Solid-State Cir-cuits Conference (ISSCC), pages 5253, 2009. 1",
  "Nima Khademi Kalantari, Eli Shechtman, Connelly Barnes,Soheil Darabi, Dan B Goldman, and Pradeep Sen. Patch-based high dynamic range video.ACM Transactions onGraphics, 32(202):18, 2013. 1, 2, 3, 7, 8": "Sing Bing Kang, Matthew Uyttendaele, Simon Winder, andRichard Szeliski. High dynamic range video. In ACM Inter-national Conferenceon Computer Graphics and InteractiveTechniques (SIGGRAPH), pages 319-325, 2003. 1, 2, 3 Joel Kronander, Stefan Gustavson, Gerhard Bonnet, AndersYnnerman, and Jonas Unger. A unified framework for multi-sensor hdr video reconstruction. Signal Processing: ImageCommunication, 29(2):203215, 2014. 6 Phuoc-Hieu Le, Quynh Le, Rang Nguyen, and Binh-SonHua.Single-image hdr reconstruction by multi-exposuregeneration. In IEEE Winter Conference on Applications ofComputer Vision (WACV), pages 40524061, 2023. 2 Zhen Liu, Yinglong Wang, Bing Zeng, and Shuaicheng Liu.Ghost-free high dynamic range imaging with context-awaretransformer. In European Conference on Computer Vision(ECCV), pages 344360, 2022. 1, 2, 6, 7, 8",
  "Erik Reinhard, Michael Stark, Peter Shirley, and James Fer-werda. Photographic tone reproduction for digital images.ACM Transactions on Graphics, 21(3):267276, 2002. 2": "Allan G. Rempel, Matthew Trentacoste, Helge Seetzen,H. David Young, Wolfgang Heidrich, Lorne Whitehead, andGreg Ward.Ldr2hdr: on-the-fly reverse tone mapping oflegacy video and photographs. In ACM International Con-ferenceon Computer Graphics and Interactive Techniques(SIGGRAPH), pages 39es, 2007. 2 Pradeep Sen, Nima Khademi Kalantari, Maziar Yaesoubi,Soheil Darabi, Dan B Goldman, and Eli Shechtman. Ro-bust patch-based hdr reconstruction of dynamic scenes. ACMTransactions on Graphics, 31(6):2031, 2012. 2",
  "Laurens vd Maaten and Geoffrey Hinton. Visualizing datausing t-sne. Journal of machine learning research, 9(11):25792605, 2008. 4": "Ruixing Wang, Xiaogang Xu, Chi-Wing Fu, Jiangbo Lu, BeiYu, and Jiaya Jia. Seeing dynamic scene in the dark: A high-quality video dataset with mechatronic alignment. In IEEEInternational Conference on Computer Vision (ICCV), pages97009709, 2021. 3 Jonas Wulff and Michael J. Black. Efficient sparse-to-denseoptical flow estimation using a learned basis and layers. InIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 120130, 2015. 5",
  "In IEEE Conference on Computer Vision and Pattern Recog-nition (CVPR), pages 17511760, 2019. 1, 2, 6, 7, 8": "Qingsen Yan, Weiye Chen, Song Zhang, Yu Zhu, Jinqiu Sun,and Yanning Zhang.A unified hdr imaging method withpixel and patch level. In IEEE Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 2221122220,2023. 2 Nianjin Ye, Chuan Wang, Haoqiang Fan, and ShuaichengLiu. Motion basis learning for unsupervised deep homog-raphy estimation with subspace projection.In IEEE In-ternational Conference on Computer Vision (ICCV), pages1309713105, 2021. 5"
}