{
  "Abstract": "In this paper, we propose a new dataset distillationmethod that considers balancing global structure and localdetails when distilling the information from a large datasetinto a generative model. Dataset distillation has been pro-posed to reduce the size of the required dataset when train-ing models. The conventional dataset distillation methodsface the problem of long redeployment time and poor cross-architecture performance. Moreover, previous methods fo-cused too much on the high-level semantic attributes be-tween the synthetic dataset and the original dataset whileignoring the local features such as texture and shape. Basedon the above understanding, we propose a new methodfor distilling the original image dataset into a generativemodel. Our method involves using a conditional generativeadversarial network to generate the distilled dataset. Sub-sequently, we ensure balancing global structure and localdetails in the distillation process, continuously optimizingthe generator for more information-dense dataset genera-tion.",
  ". Introduction": "The expansion of dataset sizes has notably propelled recentadvancements in deep learning, especially within the fieldof computer vision . However, the reliance on largedatasets poses a challenge, as it often leads to consider-able training expenses . This issue can be addressed bytwo main approaches: data selection and dataset distilla-tion. Data selection involves selecting a subset of represen-tative data from the original large dataset . Althoughthis approach can reduce the training cost, it risks losingcritical information. Dataset distillation, on the other hand,offers a more effective solution . Rather than simplyselecting existing data, it involves synthesizing a new andmuch smaller dataset that contains the important informa-tion of the original dataset. This approach can significantlyreduce dataset size without substantially compromising per-formance. Moreover, dataset distillation offers a further ad-vantage in terms of data privacy . Dataset distillation, an emerging area of interest withinthe research community, has evolved significantly in its al-gorithms and applications . Initially, dataset distil-lation creates a smaller dataset to mimic the training perfor-mance of the original dataset using meta-learning . Sub-sequent advancements introduced gradient matching meth-ods, focusing on aligning the gradients of models trained onboth the original and distilled datasets . It was furtherexpanded with the introduction of distribution matchingmethods, which aim to adjust the smaller datasets distri-bution to closely resemble that of the original dataset .Recently, some dataset distillation methods based on match-ing the training trajectories have been proposed . Thetraining trajectory refers to the change in the model weightsduring the training process. The more similar the trainingtrajectories of the teacher and student models are, the closerthe performance of the student model will be to that of theteacher model. With the development of dataset distillation, the appli-cations of dataset distillation have spaned various fields, in-cluding continual learning , privacy preserving ,and federated learning . However, conventional datasetdistillation methods often incur high redeployment costsbecause they rely on a fixed distillation ratio or the often-used image per class (IPC). Another challenge that the con-ventional dataset distillation method faces is the relativelypoor cross-architecture performance. Distilled results onthe small architecture will be hard to apply to a more com-plex architecture, which will lead to poor model generaliza-tion performance. To solve the above issues, a new dataset distillationmethod is introduced, namely distilling the dataset into agenerative model (DiM) . Different from conventionalmethods, DiM distills the information of the whole datasetinto a conditional generative adversarial network (GAN)model rather than images. This shift to model-based stor-age significantly enhances DiMs redeployment efficiency,as it eliminates the need for retraining when IPC or dis-tillation ratios change, thus overcoming the limitation ofconventional distillation methods. In the distillation pro-cess, DiM uses logit matching as the alignment strategy.",
  "arXiv:2404.17732v1 [cs.CV] 26 Apr 2024": "Logit matching focuses on the image category, emphasiz-ing global information and high-level semantic attributes.Therefore, logit matching aligns the distilled image with theoriginal image in terms of their category, rather than exactvisual details . However, it overlooks finer details suchas shapes and textures, which limits the distillation accuracyand performance on cross-architecture generalization.To address the issue of losing finer details, we proposea novel method that considers both global structure and lo-cal details. The motivation of our method is the integra-tion of high-level semantic attributes with attention to localfeatures that can improve the distillation process and hencegenerate more robust distilled datasets. The local featuresare extracted from the intermediate layers of the neural net-work, ensuring a more detailed representation of the data.Our method combines attention to both the broad, global as-pects and the detailed, local features of images. Specifically,the proposed method introduces a novel loss function thatsimultaneously accounts for the final layer logits discrep-ancies and the variance in local features contained in the in-termediate network layers, ensuring a better distillation pro-cess. Therefore, our method offers a more comprehensiveframework for dataset distillation, leading to more effectiveand accurate model training and better robustness. The ef-fectiveness of our method has been verified through experi-ments on three benchmark datasets. Notably, our methodsconsideration of both global and local image aspects resultsin datasets that exhibit enhanced cross-architecture general-ization capabilities, proving effective across various neuralnetwork types.The contributions of this paper can be summarized asfollows. We propose a new dataset distillation method that con-siders both global structure and local details, which cangenerate more robust distilled datasets.",
  ". Dataset Distillation Using Performance Match-ing": "First, we introduce dataset distillation methods using per-formance matching.The goal is to optimize distilleddatasets so neural networks trained on them mirror the lossprofiles of networks trained on original datasets. This par-ity in performance ensures that models leverage the distilleddatasets as effectively as the original ones. Within the meth-ods, it is separated into subclasses like meta-learning meth-ods, exemplified by gradient-based hyperparameter opti-mization, and kernel ridge regression methods.The in-ception of dataset distillation is first introduced by Wanget al. , and they employed meta-learning paradigmsto optimize model weights as functions of distilled im-ages. Enhancements to this method have since emerged,introducing variations with flexible labels , soft-label ap-proaches , and the incorporation of parametrization to improve distillation performance. Meta-learning-basedapproaches employ backpropagation to compute the gra-dient of the validation loss on synthetic datasets, a pro-cess that requires bi-level optimization and can be com-putationally demanding, especially as the number of in-ner loops grows, leading to increased GPU memory us-age . Limited inner loops can result in suboptimal op-timization and performance issues, and scaling such meth-ods to larger models presents further challenges . How-ever, kernel ridge regression methods like kernel inducingpoint (KIP) offer an alternative by enabling convex opti-mization, which yields a closed-form solution that obvi-ates the need for exhaustive inner loop training . Re-cent advancements in this domain have introduced methodsthat significantly enhance distillation efficiency and perfor-mance. These include leveraging infinitely wide convolu-tional networks and employing neural feature regres-sion , each contributing to the evolution of dataset dis-tillation methods.",
  ". Dataset Distillation Using Gradient Matching": "Next, we introduce dataset distillation methods using gra-dient matching.Zhao et al.first proposed a gradientmatching-based method termed dataset condensation .Different from performance matching, which tunes the ef-ficacy of models using synthetic datasets, gradient match-ing refines the performance of networks trained on boththe original and synthetic datasets by aligning their train-ing gradients. Recent advancements have augmented gra-dient matching with strategies like differentiable data aug-mentation , enhancing training adaptability and effi-cacy. Furthermore, contrastive signaling has been in-tegrated to improve feature discrimination, and long-rangetrajectory matching has been employed to align gradi-ent trajectories over extended training cycles. Additionally,approaches such as parameter pruning and self-adaptive pa-",
  ". Dataset Distillation Using Distribution Match-ing": "Finally, we illustrate some dataset distillation methods us-ing distribution matching. Distribution matching aims tosynthesize data whose distribution closely aligns with thatof the original data within a specified embedding space.Zhao et al.first introduced the method of distributionmatching, which utilizes the neural networks output em-beddings, excluding the final linear layer .The ob-jective is to minimize the distance between the meanvectors (centers) of synthetic and original data for eachclass. Building on this, another method has been devel-oped to align the attention across different layers of the net-work . Although these distribution matching methodsreduce synthesis costs and scale well with larger datasets,they require re-optimization with any change in the distil-lation ratio. This necessity for re-optimization can impacttheir efficiency in certain applications. For a deeper explo-ration of dataset distillation, the reader is directed to thelatest survey papers or the Awesome Dataset Dis-tillation project , which provide a thorough overview ofthe field.",
  "(2)": "where x represents real data and z represents random noise.During the GAN training process, the generator G and dis-criminator D compete with each other. The generator Gattempts to generate more realistic images and the discrim-inator D tries to distinguish between the generated and realdata. Different variants of the GAN network were later devel-oped, and in the proposed method, we chose conditionalGAN as the generative model to generate the distilled data.Compared with conventional GAN networks, conditionalGAN introduces specific information into the input to gen-erate images . In the proposed method, we use labelsas specific information. The training process of conditionalGAN can be summarized as follows:",
  "S = G ([z y] ; W) ,(5)": "where z is the random noise and y is the correspondinglabel. denotes the concatenation operation. S representsthe synthetic dataset and W is the parameter of generator G.The synthetic dataset S is initially generated by the genera-tor G and subsequently optimized through the dataset distil-lation process, which distills the information from the orig-inal dataset T . The optimizing method introduced in thenext section will update G to achieve better performance togenerate more efficient images that contain more valuableinformation.The most important difference between conventionaldataset distillation methods and the proposed method isthat the former ultimately saves the distilled images, whichmeans distilling the information into images, whereas thegoal of the proposed method is to save the trained genera-tor, which means distilling the information into a generativemodel. The trained generator can be used to generate anynumber of distilled images, which saves a lot of redeploy-ment costs.",
  ". Dataset Distillation via Balancing Global Struc-ture and Local Details": "The obtained conditional GAN generator can enable thesynthesis of visually convincing images. However, the syn-thetic dataset S often lacks the compressed information inthe original dataset T , hindering the performance of down-stream tasks. Our method tackles this limitation by focusingon the optimization of the generator, aiming to enhance itscapability to produce distilled data that goes beyond simple . Overview of the distillation process. The goal is to train a generator that synthesizes images rich in information (referred to asdistilled images), taking into account both global structure and local details.",
  "visual authenticity, capturing a more potent and discerningrepresentation of the important information": "As shown in , we use a random initial model fromthe model pool to match the global structure and local de-tails between the original dataset T and the synthetic datasetS. Then it continuously optimizes the generator by mini-mizing the loss between the synthetic dataset and the orig-inal dataset, thereby generating data that is more efficientfor downstream classification tasks. Unlike conventionaldataset distillation methods, we use a model pool that con-tains multiple convolutional neural networks and randomlyselect one from them to perform the matching between theoriginal dataset T and the synthetic dataset S. With themodel pool, we can improve the robustness and general-ization performance of dataset distillation. By using dif-ferent models to perform matching steps, the features ofthe original dataset T are more fully utilized. The use ofa model pool makes the proposed method have better cross-architecture stability, avoiding overfitting to specific archi-tectures and making our method more robust. In our method, the matching of synthetic dataset andoriginal dataset can be divided into two parts, the match-ing of global structure and the matching of local details.The matching of global structure aims to analyze whetherthe synthetic dataset is consistent with the original datasetin terms of high-level semantic information, such as cate-gories. The global loss is obtained by comparing the high-level semantic information of the original dataset and thesynthetic dataset. When comparing the global information,we use logical matching to compare whether the syntheticdataset has achieved similar logits to the original dataset.",
  "b=1(lSk,b lTk,b)2,(6)": "where B and K denote the batch size and the number of cat-egories, respectively. lS and lT represent the output logitsof the synthetic dataset and original dataset, respectively.However, focusing only on global information will causethe loss of valuable detailed information in the data. There-fore, we further perform matching on local features such astexture and shape. In this way, the synthetic dataset S con-tains more valuable detailed information. When calculatingthe local loss, we propose using feature matching and se-lecting information from intermediate layers to compare thematching degree between the original data and the syntheticdataset in terms of texture, shape, and other detail aspects.The local loss can be calculated as follows:",
  "b=1(f S,midk,b f T ,midk,b)2,(7)": "where mid denotes an intermediate layer of the randomlyselected network. f S and f T represent the output featuresof the synthetic dataset and original dataset, respectively.The total loss function of the proposed method is a com-bination of global loss Lglobal, local loss Llocal, and condi-tional GAN loss LCGAN. We also defined g and l to rep-resent the weights of global loss and local loss. The cal-culation of total loss Ltotal can be summarized as follows:",
  "W = arg minWLtotal,(9)": "where W is the optimized parameters of the generator G.The proposed method ensures balancing global structureand local details of the original dataset and synthetic datasetduring the dataset distillation process as much as possibleby matching them and making the synthetic dataset containas much detailed information as possible, thereby generat-ing distilled datasets for downstream tasks.",
  ". Deployment Stage": "After the above optimization, the generator cannot onlygenerate visually realistic images but also generate distilledimages. These distilled images contain more key informa-tion that is helpful for downstream tasks such as recognitionand classification. Therefore, during the deployment phase,we provide various random noises z and corresponding la-bels y to the generator and use the generator to dynamicallygenerate various distilled dataset S as follows:",
  "S = G ([z y] ; W) .(10)": "This distilled dataset can be used to serve as the alterna-tive for the original dataset to effectively reduce the volumeof the dataset. Moreover, since we saved the trained gen-erator, the information of the whole dataset was distilledinto the generative model during this process, rather thanstatic images. Therefore, when we apply the new proposedmethod to other architectures or change the distillation ra-tio, there is no need to retrain the model. This improves theefficiency of redeployment a lot. Algorithm 1 summarizesthe proposed method. The generative dataset distillationmethod trains a generator G of conditional GAN first. Thenthe global-local coherence of the original and synthetic wasmatched. Finally, the generator G is updated to generate amore efficient distilled dataset.",
  ": Generate the distilled dataset S:21: S = G ([z y] ; W)": "differentiable siamese augmentation (DSA) , distribu-tion matching (DM) , aligning features (CAFE) ,kernel inducing point (KIP) , matching training trajec-tories (MTT) , and neural feature regression with pool-ing (FrePo) . We also compared with baseline methodCGAN and the generative-based dataset distillationmethod DiM . To improve the generalization performance and avoidover-reliance on a single network, when optimizing the gen-erator, we applied the model pool to get the randomly ini-tialized model. The model pool has several models suchas ConvNet3 , ResNet10, and ResNet18 . Randomlyselected models are used to match the global and localfeatures of the synthetic dataset and the original dataset.When matching local features between two datasets, we fo-cus on specific intermediate layers that demonstrate a su-perior ability to extract local features, such as the secondlayer within ResNet . We conducted three experimentsto verify the effectiveness of the proposed method, includ-ing benchmark comparison, cross-architecture generaliza-tion, and hyperparameter ablation study. All the experimen-",
  ". Benchmark Comparison": "In this subsection, we verify the effectiveness of the pro-posed method by comparing it with other methods on threebenchmark datasets, i.e., MNIST, Fashion MINIST, andCIFAR-10. We designed three sets of experiments for eachdataset. Each set applies IPC = 1, 10, and 50, respectively.ConvNet3 was used as the test model. From , wecan see that our method achieves better performance un-der majority settings and shows better stability. In most ex-periments, the accuracy has been improved by about 0.5%,especially when IPC = 1, the proposed method improvedthe accuracy by about 1% and improved the stability. Fig- ure 2 shows the visualization results on distilled MNIST,fashionMNIST, and CIFAR-10 datasets obtained using theproposed method. Based on the main experimental and vi-sualization results, we can see that the proposed methodcan improve the performance of generative dataset distilla-tion while maintaining the visual authenticity of the distilleddataset.",
  ". Ablation study of l on CIFAR-10 dataset with IPC = 1": "AlexNet and VGG11 . These architectures weretrained on the distilled dataset and tested on the originaldataset. We set the IPC = 10 to keep the same setting asthe previous methods to make a fair comparison. shows that the proposed method outperforms conventionaldataset distillation methods in terms of cross-architecturegeneralization. The distilled dataset demonstrates higheraccuracy across different architectures. In comparison withthe DiM method, the distillation data derived from the pro-posed method shows enhancements in performance on var-ious architectures and exhibits better stability.",
  ". Hyperparameter Ablation Study": "Since DiM has proved that the weight of global loss g =0.01 leads to the best performance. Hence, we used thesame value of g and set up an experiment on CIFAR-10with IPC = 1 to explore the impact of the weight of localloss l. As shown in , when the weight of the localloss Llocal was set to 0.001, the proposed method achievedthe highest average accuracy. When the local loss weightis too large, it reduces the impact of global loss Lglobal andconditional GAN loss LCGAN, thereby reducing the accu-racy, while a local loss weight that is too small will notallow the generator to effectively learn the local features.Although DiM has shown the best value of g, the impactof the different g values is still worth exploring in futureworks.",
  ". Conclusion": "This paper has proposed a novel dataset distillation method.During the dataset distillation process, the proposed methodtakes into account both the global structure and local de-tails, thereby ensuring that high-level semantic informationand mid-level feature information are simultaneously dis-tilled into the generative model. Experimental results showthat the proposed method outperforms other SOTA datasetdistillation methods on three benchmark datasets.",
  "Timothy Nguyen, Roman Novak, Lechao Xiao, and JaehoonLee. Dataset distillation with infinitely wide convolutionalnetworks. In Proc. NeurIPS, pages 51865198, 2021. 2, 5,6, 7": "Ahmad Sajedi, Samir Khaki, Ehsan Amjadian, Lucy Z.Liu, Yuri A. Lawryshyn, and Konstantinos N. Platanio-tis. DataDAM: Efficient dataset distillation with attentionmatching. In Proc. ICCV, pages 1709717107, 2023. 3 Saptarshi Sengupta, Sanchita Basak, Pallabi Saikia, SayakPaul, Vasilios Tsalavoutis, Frederick Atiah, Vadlamani Ravi,and Alan Peters.A review of deep learning with specialemphasis on architectures, applications and recent trends.Knowledge-Based Systems, 194:105596, 2020. 1"
}