{
  "V1V2V3V5V4": ". Identity-aware captioning. Left: To understand the story in a set of videos, captions refer to characters by a unique local identifier(e.g. P1, P2, . . .). The Fill-in-the-blanks (FITB) task provides these captions with blanks (removing names) and asks a model to fill localperson ids. Middle: End-to-end captioning for a videoset is achieved in two stages . First, captions are generated with someone tags,and then the FITB module is applied to fill-in names. Right: We propose a single-stage encoder-decoder id-aware captioning approachthat can switch between generating the caption with ids or filling in the ids in a caption, jointly learning from both tasks.",
  "Abstract": "Characters are an important aspect of any storyline andidentifying and including them in descriptions is neces-sary for story understanding.While previous work haslargely ignored identity and generated captions with some-one (anonymized names), recent work formulates id-awarecaptioning as a fill-in-the-blanks (FITB) task, where, givena caption with blanks, the goal is to predict person id la-bels. However, to predict captions with ids, a two-stageapproach is required: first predict captions with someone,then fill in identities. In this work, we present a new sin-gle stage approach that can seamlessly switch between id-aware caption generation or FITB when given a captionwith blanks.Our model, Movie-Identity Captioner (MI-Cap), uses a shared auto-regressive decoder that benefitsfrom training with FITB and full-caption generation ob-jectives, while the encoder can benefit from or disregardcaptions with blanks as input. Another challenge with id-aware captioning is the lack of a metric to capture subtle differences between person ids. To this end, we introduceiSPICE, a caption evaluation metric that focuses on identitytuples created through intermediate scene graphs. We eval-uate MICap on Large-Scale Movie Description Challenge(LSMDC), where we show a 4.2% improvement in FITB ac-curacy, and a 1-2% bump in classic captioning metrics.",
  ". Introduction": "Building computer vision models that understand the storyof a movie is a long-standing challenge. A step towardsthis is movie description . Given a short clipof 2-5 seconds, models are required to generate a captionthat describes the visual scene. Captions in the Large ScaleMovie Description Challenge (LSMDC) , a combina-tion of , are obtained from audio descriptions (AD)that are used to convey the (visual) story to a visually im-paired audience. The original version of the LSMDC chal-lenge suggests captioning a single clip and anonymizes all",
  "arXiv:2405.11483v1 [cs.CV] 19 May 2024": "character names with someone.While using the someone tag to describe a charactersactivity in a single video is acceptable, the lack of iden-tity continuity across a videoset (group of N consecutivevideos) hampers understanding.To remedy this, Pini etal. extend MVAD as MVAD names where char-acter names are predicted by linking to the appropriate facedetection/track; and Park et al. propose a fill-in-the-blanks (FITB) task to replace someone tags with local clus-ter identities (e.g. P1, P2, . . .) in a videoset ( left).The latter approach provides two advantages: (i) itdoes not require time-consuming ground-truth annotationslinking faces and blanks ; and (ii) using local cluster idshelps convey the story1 without the need for models withworld knowledge (CLIP , GPT , etc.) or an IMDbcastlist with photographs , making the approach appli-cable to indie films or home-edited videos.To generate id-aware captions, proposes a two-stageapproach shown in (middle). The first stage in-gests a videoset and generates a captionset (a set of N cap-tions, one for each video) using the someone tags; whilethe second stage replaces someone with appropriate localperson id labels. While the two-stage setting unites the twoworlds of video description and character identification, it isnot ideal as errors in captioning may adversely affect FITBas both methods are modeled independently. In this work,we propose a single-stage approach ( right) that canseamlessly switch between both tasks. Challenges with Fill-In. For the FITB task, encodesblanks in the ground-truth (GT) captionset using bidirec-tional context through the BERT encoder. These blanks at-tend to the face features clustered within a single video, notaccounting for other faces coming from the videoset.Using the blank representations, the person ids are pre-dicted in an auto-regressive manner.We note some disadvantages with this approach:(i) Faces are clustered within each video. This means iden-tity information across videos is not directly observed bythe model. (ii) When a character is mentioned in the cap-tion, their face need not be present in the clip (e.g. left, C4 and C5 mention P1 whose face is turned and notvisible). (iii) BERT-based blank embeddings provided atthe encoder are unable to capture face information properly,resulting in a model that largely focuses on text embeddingsto solve FITB (e.g., in , FITB accuracy only improvesby 1.5% (64.4 to 65.9) with visual inputs). Proposed model benefits. We overcome these problemsusing a new paradigm for id-aware multi-video descrip-tion through a single-step sequence-to-sequence model. Weunify the two tasks of FITB and caption generation, by auto-",
  "Note, cluster ids can be easily mapped to gender- and culture-appropriate names instead of using P1, P2, . . . for storytelling": "regressively unrolling the descriptions along with their lo-cal character ids, via a Transformer based encoder-decodermodel. Our model, dubbed as the Movie-Identity Captioner(MICap), enables joint training and independent evaluationfor both tasks: (i) given only the videoset, our model gener-ates an id-aware captionset; and (ii) when a captionset withsomeone tags exists, our model fills in local identities.To overcome text-only shortcuts, we propose auto-regressive decoding of the full caption even for FITB andshow that our multimodal model outperforms a text-onlymodel significantly. We teacher force the ground-truth cap-tion containing the blanks (person ids), and predict one to-ken at a time using causal masking. Note, learning happensonly at select tokens where person id labels are predicted.This way the model (decoder) learns to sequentially use theGT (teacher forced) caption for the FITB task with uni-directional (causal) attention. During inference, we switchbetween the two tasks by deciding whether the decoder isteacher forced with a given captionset or not. Identity-aware evaluation.Existing captioning metricslike CIDEr and BLEU do not account for iden-tity sensitive descriptions. For example P1 is walking to-wards P2 and P2 is walking towards P1 will result inhigh n-gram based scores due to common middle words.We propose a new identity-aware caption evaluation metriciSPICE. Specifically, we are motivated by SPICEs abil-ity to parse a caption into a scene graph, and match a pre-dicted caption with ground-truth based on similarity acrossgenerated tuples. To compute iSPICE, we intervene in thisprocess and remove tuples not associated with a person la-bel before computing the F1 scores. Contributions.In summary, (i) we propose a newparadigm for identity-aware multi-sentence movie descrip-tion using a single-stage approach that unifies FITB withfull caption generation.(ii) We formulate this task asan auto-regressive sequence-to-sequence generation that isable to describe the video and use local person id labelsacross a videoset (multiple videos).We show that jointtraining improves knowledge sharing and boosts perfor-mance. (iii) We enable seamless task switching allowing in-dependent evaluation of (a) caption generation with identi-ties, and (b) filling in identity labels given a caption. (iv) Wepropose a new identity-aware captioning metric, iSPICE,that extends SPICE, and show its sensitivity to identitieswhile evaluating captions.(v) Finally, MICap improvesover the state-of-the-art for FITB by 4.2% and identity-aware captioning by 1.4% CIDEr and 1.8% METEOR.",
  "We address related work from three areas: (i) video caption-ing at large, (ii) identity-aware captioning, and (iii) metricsused for evaluating captions": "Video captioning has gained a lot of attention since the ad-vent of deep learning. The typical task is to generate a singlesentence description for a trimmed video, and is formulatedas a sequence-to-sequence problem . A more challenging setup is multi-sentence genera-tion, typically applied to longer videos and requires long-term temporal consistency . Video situationrecognition, VidSitu presents a structured alterna-tive where multiple captions are generated per event basedon the semantic role labeling framework.Different from multi-sentence captioning, dense videocaptioning, requires temporally localizing and generatingcaptions for every event in an untrimmed video . While most approaches for dense video captioninguse a 2-stage approach, i.e. temporal localization with eventproposals then event captioning , recent meth-ods, jointly model the two tasks for better temporal consis-tency . The state-of-the-art, PDVC , learns DETR-style event queries andperforms localization and captioning over each query us-ing 2 separate heads. Recently, Vid2Seq proposed tofurther unify the two tasks by using a single sequence-to-sequence model and generating both the localization andcaptions with a single auto-regressive Transformer decoder.Similar to above ideas, we unify two seemingly differenttasks of character identification and description by formu-lating them as an auto-regressive sequence generation task. Id-aware captioning datasets. None of the above worksfocus on person identity while generating captions. Vid-Situ , perhaps the closest, contains references to peopleby descriptions such as man in a black jacket. This is anissue when the domain is movie description , whereidentities are anonymized to someone which hinders build-ing practical applications like Audio Descriptions forvisually impaired users. While links character names indescriptions with face tracks, they require significant anno-tation effort that is not scalable. A more recent Movie Au-dio Description dataset, MAD , is a popular source formovie descriptions. But it uses real names that require mod-els with world knowledge. Different from above, Park etal. propose identity-aware captioning as a fill-in-the-blanks task where they assign local person ids (cluster ids)to characters appearing in 5 consecutive video clips. Weadopt this setting for our work. Id-aware captioning methods. Identity-aware captioningis a challenging task that has recently started to attract at-tention. Among the first works, proposes a 2-stagepipeline of first captioning with identities anonymized assomeone using a multi-sentence captioning model , fol-lowed by learning an identity prediction FITB model thatfills in the someone with local person identities. However,as discussed in the introduction (Challenges with Fill-In),the specific 2-stage approach suffers from several disad- vantages. Different from , we propose a single stagesequence-to-sequence model, that outperforms the 2-stageapproach. In this area, another work requires ground-truth mapping between person identities (blanks) in the de-scription to face tracks in the videos. However, this ap-proach is not scalable. Very recently, AutoAD-II pro-posed to generate movie descriptions with proper names,on the MAD dataset. While innovative, this approachrequires additional IMDb castlist information with pho-tographs. While modeling proper names directly is useful,tagging names to unique person ids in a local videoset ispossible and is the motivation for works on person cluster-ing as opposed to person identification .Caption evaluation metrics are typically based on n-grammatching, with few differences. CIDEr , BLEU ,and METEOR all evaluate n-gram similarities betweena single or multiple candidate references and the generatedcaption.Recently, Large Language Models (LLMs) areused for reference-based (e.g. BERTScore , CLAIR )or or Large Vision-Language Models (VLMs) for reference-free caption evaluation (e.g. CLIP Score ). However,model-based metrics may be difficult to interpret, and alsorequire the model to be sensitive to identities.Differ-ent from both directions, SPICE evaluates captions byfirst transforming them into a scene graph and analyz-ing presence of shared tuples between the predicted andground-truth (reference) captions. However, none of themetrics reliably evaluate identity-aware captions, as a ro-bust metric should be sensitive to identity manipulations(swap/add/remove). We propose a new metric iSPICE thatfocuses primarily on person-identity specific semantics.",
  ". Method": "We present a single-stage sequence-to-sequence approachfor identity-aware fill-in-the-blanks (FITB). Later, we willshow that this architecture can be easily re-purposed forgenerating video descriptions.Notation. Before we start, we define some notation. Forthe rest of this section, we will operate with a videoset Nconsisting of N video clips Vi and corresponding captionsetC = {Ci}Ni=1, where Ci describes video Vi. As both setscome from consecutive videos, it is very likely that samecharacters appear across them. As an example, consider thevideoset frames and captionset shown in .",
  "Only used in FitB": ". Identity-aware captioning. Left: illustrates the Transformer Encoder used to capture multimodal inputs such as text (blanks),action, semantic, and face. These tokens are used as memory for the Transformer Decoders. Right: the same Transformer Decoder can beused for both tasks of full caption generation and fill-in-the-blanks (FITB). The model is trained end-to-end with losses applied to tokensindicated in purple. Text tokens are not presented to the decoder for full caption generation. Joint training improves knowledge sharingresulting in performance improvements. person-id labels are reusable across videosets, i.e. a charac-ter only needs to be referred consistently by the same iden-tity within a videoset.We present Movie-Identity Captioner (MICap), an auto-regressive Transformer encoder-decoder model for fillingperson blanks. MICap consists of two parts: (i) Featureextractors and a Transformer encoder to build the caption-ing memory ( left); and (ii) A Transformer decoderthat switches between FITB or full captionset generation( right). For clarity, we will highlight differences toprior work throughout this section.",
  "Creating the Captioning Memory": "Visual feature extraction. We extract 3 features from thevideoset to capture semantic, action, and face information.Semantic embeddings are captured using CLIP .From each video Vi, we sub-sample frames fit at 5 fps andencode them with the CLIP image encoder. For efficientbatching, we truncate or pad to T=50 frames per video,and stack them to create semantic features Fs RNT ds.Action embeddings are captured using I3D . Simi-lar to , each video is divided into S=5 segments, andfeatures within each segment are mean pooled. We stackfeatures across the videoset to obtain Fa RNSda.Faces are detected using Retina Face and repre-sented using Arcface . Across the videoset, we collecta maximum of F=300 face detections. With each face de-tection, we associate the video index i (for Vi) from which itis derived and a normalized spatial bounding box location.We stack features to obtain Ff RF df.We bring all these features to a common d dimen-sional space using separate linear projection layers for each",
  "[ CLS, w1, . . . , bk, . . .] = BERT([CLS, w1, . . . , bk, . . .]) .(1)": "The blank embedding is a concatenation of contextualizedtokens: bk = [ CLS, bk]. We stack these to create a ma-trix B R|B|2dbert and transform them to the same spacethrough a linear projection Wbert Rd2dbert. Face clustering. Instead of creating face clusters withineach video and using blank embeddings to attend to them(as done in ) we adopt a soft approach for incorporatingcluster information in MICap. First, we perform clusteringusing DBSCAN across all F detections in the videoset, re-sulting in G, a set of face groups. This allows our model toassociate faces across videos as the same or different per-son. Next, we prevent propagating errors caused by cluster-ing and mean pooling representations by adding a cluster-idbased learnable embedding Efcl to the face representations. Additional embeddings are added to various features toorient the model: (i) Etyp Rd4 disambiguates betweenthe 4 types of features. (ii) Evid RdN consists of Nembeddings to inform the model of the source video indexfor any visual or blank token. (iii) Eseg RdS, togetherwith Evid, allows to localize any feature to the correct videoand segment. (iv) Efcl Rd|G| is the face cluster indexembedding described above, and (v) Ebbox Rd4 trans-forms normalized face detection bounding box coordinatesto provide the model spatial information.",
  "Auto-regressive Identity Prediction": "We now present the process of filling blanks. Similar tothe encoder, we use a couple embeddings for the decoder.(i) Evid (shared with encoder) informs the decoder of thevideo index that is being captioned; and (ii) Epos encodeslearnable position embeddings similar to the original Trans-former .We use the memory embeddings extractedfrom the video as key-value pairs and blanks in the Trans-former decoder (TD) as queries. Given a captionset C, wegenerate the next word as",
  "wj+1 = arg maxVWVhj+1 .(8)": "hj+1 represents the output of TD at the j +1th timestep andis obtained through a series of LD decoder layers that com-pute self-attention to previous words, and cross-attention tothe memory. WV is a linear classifier in RVd, where V isthe word vocabulary.For the FITB task, the captionset already contains thecorrect caption words. Thus, the output prediction is rele-vant only when wj+1 is a blank bk. In such a case, we canuse a smaller output classifier WP that picks one among Pperson-id labels. We rewrite the above equations as:",
  "We first present how MICap can be adapted for generatingthe entire captionset. Then, we will present the opportunityof joint training": "From FITB to generating the captionset. In this scenario,the model is shown the videoset N and expected to generatean id-aware captionset C. We make two small changes:(i) The memory bank is restricted to visual features,M = [Fs, Fa, Ff]. In fact, we cannot compute blank em-beddings B as the captionset needs to be predicted.(ii) When decoding the next word of the captionset, weuse an augmented vocabulary consisting of normal lan-guage tokens (from V) and person-id labels (from P). Wepredict the next word as shown below:",
  "We can use Eq. (14) during inference to predict the entirecaptionset until the end-of-sentence token is triggered": "Joint training. Can we train the same instance of MICap togenerate the captionset and fill-in-the-blanks with identityinformation? Yes, we suggest an efficient way to do so.Given a batch of data consisting of multiple pairedvideosets and captionsets (N, C), we forward it throughthe model twice.In the first forward pass, we replacethe person-id labels with blanks, i.e. create C, and com-pute losses and gradients to predict the blanks labels (seeEq. (11)). In the second forward pass conducted on the samebatch, we assume that C is not available as input and use theaugmented vocabulary V to compute loss and gradients foreach word as in Eq. (15). We can either accumulate gra-dients and optimize parameters at the end of both forwardpasses or optimize parameters after each pass.Note, the classifier parameters WP are subsumed underWV. We find that sharing the classifier WV for bothforward passes works best.Thus, we unite seemingly disparate tasks of filling inperson-id labels in blanks and generating the full caption-set in a single model with a single set of parameters.",
  "SPICE (iSPICE for short) to evaluate the quality of videodescriptions, especially pertaining to identity labels": "WhySPICE?Theclassiccaptioningmetricsbor-rowed from language translation such as BLEU ,ROUGE , METEOR , and CIDEr rely pri-marily on n-gram overlap. However, as indicated in ,n-gram overlap is neither necessary nor sufficient for twosentences to convey the same meaning. SPICE is shownto have a high correlation with human judgement (0.88) ascompared to METEOR (0.53) or CIDEr (0.43) on the MS-COCO image captioning dataset . How is SPICE calculated? SPICE estimates quality of acaption in two stages. First, the reference and predicted cap-tion are converted to scene graphs that explicitlyencode objects, attributes, and relationships. This abstrac-tion provides a list of tuples Tr and Tp for the reference andpredicted captions. SPICE is the F1-score that measureslogical conjunction (overlap):",
  "iSPICE = F1(T p2+r, T p2+p) F1(T p1r , T p1p ) ,(17)": "where T p2+rdenotes the list of tuples with a person-id la-bel having 2 or more elements and T p1ris a set of person-id labels in the reference captionset. The first term scoreswhether the correct person-id label is used together with averb or attribute, while the second term checks that the totalnumber of person-id labels match.. A couple examples ofthe matching process are presented in the supplement. Validation. We validate iSPICE by an experiment that mea-sures sensitivity to changes in identity. Given a referencecaptionset, we compare it against itself to obtain a basescore s. Next, we modify the reference captionset by swap-ping, adding new, or removing existing id labels.1. Swapping: Here, id tokens are replaced with anotherid present in the captionset. The number of these tokensis selected at random for each captionset. We first identifyeligible id tokens whose ids are present more than once inthe captionset. This is done to prevent the case where stan-dalone ids are selected and replaced with each other thatdoes not change the meaning. For example, the caption P1carries P2 is equivalent to P2 carries P1 if P1 and P2 arenot re-used elsewhere in the captionset. When the id occursmultiple times, e.g. P1 carries P2. P2 is unconscious, thereplacement P2 carries P1. P2 is unconscious changes themeaning of the story. Once these eligible tokens are identi-fied, a random subset is replaced with another id present inthe captionset to generate the modified caption.",
  "Swapping0.55 0.85 0.87 0.86 0.61 0.95 0.99Addition0.51 0.86 0.89 0.880.60.95 0.99Removal0.46 0.84 0.87 0.860.60.95 0.99": ". Sensitivity of metrics to id manipulation in the originalcaption. iSPICE shows highest reduction in performance when re-placing, adding, or removing ids, indicating that it is a good met-ric for id-aware captioning iS=iSPICE, S=SPICE, B4=BLEU4,C=CIDEr, M=METEOR, R=ROUGE, BSc=BERTScore. 2. Addition: Here, we select an id token at random andchange it to an id token that is not present in the currentcaptionset, adding new identities. Again, we do not replacetokens whose id appears only once.3. Removal: Here, we replace a single occurrence idtoken (chosen at random) with an id token that exists in thecaptionset, thereby removing the identity.Id normalization. Prior to scoring, a normalization opera-tion is performed on the captionset. The first unique id labelis set to P1, the second to P2 an so on. This ensures that thecaptionsets P2 carries P1 or P4 carries P3, are treated asthe same captionset P1 carries P2.Results. We compute a new score s for each edited cap-tionset by comparing it against the reference. We reportthe drop in performance s/s as the sensitivity of a metricto changing identities. We create 3 manipulated samplesfor each type and report averaged scores over all 1443 cap-tionsets from the validation set in Tab. 1. We observe thatiSPICE obtains the smallest score, indicating the highestsensitivity to manipulating identities, a desirable property.",
  ". Setup": "Dataset. LSMDC consists of 128,118 short video clips ex-tracted from 202 movies. Each video has a caption, eitherfrom the movie script or from transcribed DVS (descriptivevideo services) for the visually impaired. The median videoduration is 3 s, average is 4.2 s, and std dev is 3.1 s. Thedataset is split into 101,079 clips for training, 7,408 for val-idation, 10,053 for public test, and 9,578 for blind test. Wereport and compare results on the validation set as the testset labels are not released and the evaluation server is down.In the Fill-in challenges, the movie descriptions are eval-uated on sets of 5 clips taken at a time. Characters are iden-tified across the clips to provide meaningful narratives. Thetraining videosets use overlapping clips (e.g. 1-5, 2-6) for data augmentation but the val and test videosets are non-overlapping. We train on 98,527 videosets and report re-sults on 1,443 val videosets. All three tasks of the LSMDCchallenge are evaluated on the same sets of 5 clips. Wefocus on task 2: filling in local person ids; and task 3: de-scription generation with local character IDs.Implementation details. Videosets have N=5 clips, we setthe captionset length to 120 tokens. The hidden dimensionfor encoder and decoder in MICap is d=512, and we useLE=2 and LD=3 layers. We train our model with a learn-ing rate of 5105 for 30 epochs. The vocabulary sizes are|P|=11 and |V|=30522. We train on one RTX 2080 GPUwith a batch size of 16 videosets/captionsets.Fill-in metrics. For the Fill-in task we evaluate results us-ing all pairs of blanks in the captionset as proposed by .Pairs that require both ids to be same are called are evalu-ated with same accuracy (Same-acc). Different id pairsare evaluated using Diff-acc. Inst-acc is the combinedaccuracy while Class-acc computes the harmonic mean.Captioning metrics. We use METEOR , CIDEr ,SPICE and our newly proposed metric iSPICE to evalu-ate the quality of our generated captions.",
  ". Evaluating on the Fill-in Task": "MICap makes better use of visual features. In Tab. 2, ourtext-only model (row 2) is comparable to s text-only(R0). While improves by 1.5% (R1), MICap achievesa significant 4.7% improvement (R6).Ablations on visual features. computes face clusterswithin a video and provides mean pooled features of facesin a cluster. R3 of Tab. 2 uses these features in MICap (withembeddings from Eq. (5)). The only decoder model (only-dec) achieves a 0.6% improvement, while the encoder-decoder model (enc-dec) shows 1.4% improvement overR1. Next, in R4, we swap out face cluster features to indi-vidual face detections, while still using FaceNet for a faircomparison; but using embeddings as shown in Eq. (5).This improves the only-dec model by a further 0.9%, butenc-dec shows negligible change.We incorporate CLIPfeatures as additional tokens in the memory, resulting in a0.35% increase in enc-dec (R5). Finally, in R6, swappingFaceNet to Arcface results in a relatively large im-provement of 1.6% (only-dec) and 1.4% (enc-dec).SotA comparison. Tab. 3 reports results on all 4 FITB met-rics. As we do not have access to the test set labels andthe evaluation server is inactive, we use FillIns results asa proxy for comparison. First, in the top half, we see thatFillIn outperforms other works. In the bottom half, onthe validation set, we compare our approach against FillInshowing a significant improvement of 4% on instance ac-curacy and 3.2% on class accuracy. As we teacher forcecaptions through the decoder, our only decoder model also",
  ". Evaluating Joint Fill-in and Captioning": "We evaluate MICap trained jointly for FITB and id-awarecaption generation. Tab. 4 shows that joint training on fill-inand captioning improves the performance on both the tasks.Class accuracy on FITB improves by 0.9% and captioningmetric CIDEr by 1%. We also see a small 0.01% improve-ment in iSPICE, which we think is important consideringthe difficulty of the metric.This suggests that both thetasks are complementary and can help each other in learn-ing a better representation. MICap can seamlessly switchbetween FITB (id prediction) and full caption generation.SotA comparison for captioning. We compare against thetwo-stage baseline , while MICap predicts the captionsand identities in a single stage. Tab. 5 shows that we im-prove over across all metrics.MICaps captions are better. We disentangle identity pre-diction from caption generation by replacing all person id",
  "GT : P3 turns with a start.Pred : P4 is at the kitchen table": ". We show a qualitative example of our joint training approach. The dataset is highly challenging, with shot changes and darkscenes that are typical in movies. Yet our model is able to perform reasonably well in this example. While the predicted captions (Pred)are different from the ground-truth (GT), they capture the overall meaning. MICap predicts diverse ids correctly in this case and does notoverfit to only predicting P1, or P1 and P2. In fact, in the last clip, as P3 turns (indicated in GT), we see P4 sitting at the table (indicated inPred), which is a correct caption! The last clip also highlights challenges of evaluating captions correctly.",
  ". Experiments showing MICap outperforms foundationalmodels T5-Base and GPT2 adapted/fine-tuned for id-aware captioning on the same LSMDC dataset": "labels by the same id or all different ids. This allows usto evaluate captioning performance, independent of identityprediction. We are pleased that our simple encoder-decoderapproach outperforms a complex adversarial multi-sentencecaptioning approach used in stage 1 of . Tab. 5 R1vs. R4, CIDEr goes up from 7.03 to 8.44, and METEOR9.41 to 10.9. Similar improvements hold for R2 vs. R5. Comparison to VLMs. Tab. 6 shows that MICap outper-forms adaptations of T5 (an encoder-decoder framework)and GPT-2 (QFormer prefix tokens like CLIPCap orBLIP2 ), fine-tuned for the id-aware captioning task.We suspect that integrating many diverse visual tokens isnot trivial for VLMs, resulting in comparable performancewhen using only CLIP or all features.",
  "iSPICE changes dramatically when using the same id or alldifferent ids. We hope that this metric will inspire futureworks in this direction of identity-aware captioning": "Attention patterns of MICaps decoder reveal interestinginsights. For the task of full captioning, we see that tokensthat produce id labels cross-attend more to the face tokens(from memory) while normal word tokens cross-attend toCLIP features. We also analyze the attention patterns inFITB and observe that the model attends to the same clus-ters when predicting the same labels and also attends to facedetections across the videoset (not restricted to faces in asingle video). Please refer to the supplement for details. A qualitative example is shown in .We observethat MICap does a decent job at generating captions (al-though it is unable to use a rich vocabulary - smiles insteadof beams cheerily). The challenges of caption evaluationare also clear in the last clip. Several more examples forboth tasks are shown in the supplement.",
  ". Conclusion": "We proposed a new paradigm for identity-aware movie cap-tion generation. As opposed to the two-stage approach offirst captioning with anonymized names and then fillingin the identities, we proposed a single-stage method thatcombines the two tasks via an encoder-decoder sequence-to-sequence generation framework, that can seamlesslyswitch between (i) full caption generation with identities,or (ii) predict the identities given a caption with anonymizednames. We showed that a single auto-regressive model ben-efits both tasks and shows positive transfer, leading to state-of-the-art performance on the LSMDC challenge. We alsoproposed an identity-aware captioning metric, iSPICE, thatis sensitive to subtle perturbations in identity and robustlyevaluates captions. Acknowledgments. The project was supported by funding fromSERB SRG/2023/002544. We thank the Bank of Baroda for par-tial travel support. We thank Amit Pandey for assisting in earlydiscussions. Makarand Tapaswi thanks support from Google IndiaFaculty Award and Naveen Reddy Desanur from Sensara.",
  "Jiankang Deng, Jia Guo, Niannan Xue, and StefanosZafeiriou. ArcFace: Additive Angular Margin Loss for DeepFace Recognition. In Conference on Computer Vision andPattern Recognition (CVPR), 2019. 4, 7": "Deng, Jiankang and Guo, Jia and Ververas, Evangelos andKotsia, Irene and Zafeiriou, Stefanos. Retinaface: Single-shot multi-level face localisation in the wild. In Conferenceon Computer Vision and Pattern Recognition (CVPR), 2020.4 Michael Denkowski and Alon Lavie. Meteor Universal: Lan-guage Specific Translation Evaluation for Any Target Lan-guage. In European Chapter of the Association for Compu-tational Linguistics (EACL), 2014. 3, 6, 7 Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,and Trevor Darrell. Long-term recurrent convolutional net-works for visual recognition and description. In Conferenceon Computer Vision and Pattern Recognition (CVPR), 2015.3",
  "Tengda Han, Max Bain, Arsha Nagrani, Gul Varol, WeidiXie, and Andrew Zisserman. AutoAD: Movie Descriptionin Context. In Conference on Computer Vision and PatternRecognition (CVPR), 2023. 3": "Tengda Han, Max Bain, Arsha Nagrani, Gul Varol, WeidiXie, and Andrew Zisserman. AutoAD II: The Sequel-Who,When, and What in Movie Audio Description. In Interna-tional Conference on Computer Vision (ICCV), 2023. 2, 3 Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,and Yejin Choi. CLIPScore: A Reference-free EvaluationMetric for Image Captioning. In Empirical Methods in Nat-ural Language Processing (EMNLP), 2021. 3 Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li,David Shamma, Michael Bernstein, and Li Fei-Fei. ImageRetrieval using Scene Graphs. In Conference on ComputerVision and Pattern Recognition (CVPR), 2015. 6",
  "Chin-Yew Lin. ROUGE: A Package for Automatic Evalu-ation of Summaries. In Workshop on Text SummarizationBranches Out (WAS), 2004. 6": "Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, ZheGan, Zicheng Liu, Yumao Lu, and Lijuan Wang.Swin-bert: End-to-end transformers with sparse attention for videocaptioning. In Conference on Computer Vision and PatternRecognition (CVPR), 2022. 3 Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, NanDuan, Tianrui Li, Jason Li, Taroon Bharti, and MingZhou. UniVL: A Unified Video and Language Pre-trainingModel for Multimodal Understanding and Generation. arXivpreprint arXiv:2002.06353, 2020. 3",
  "Jae Sung Park, Trevor Darrell, and Anna Rohrbach. Identity-aware multi-sentence video description. In European Con-ference on Computer Vision (ECCV), 2020. 1, 2, 3, 4, 5, 6,7, 8, 12": "Stefano Pini, Marcella Cornia, Lorenzo Baraldi, and RitaCucchiara. Towards video captioning with naming: a noveldataset and a multi-modal approach. In International Con-ference on Image Analysis and Processing (ICIAP), 2017. 1,2, 3 Stefano Pini, Marcella Cornia, Federico Bolelli, LorenzoBaraldi, and Rita Cucchiara. M-VAD names: a dataset forvideo captioning with naming. Multimedia Tools and Appli-cations (MTAP), 78:1400714027, 2019. 2, 3",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan, DarioAmodei, and Ilya Sutskever. Language Models are Unsuper-vised Multitask Learners. 2019. 2, 8": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International Conference on Machine Learning(ICML). PMLR, 2021. 2, 4 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J. Liu.Exploring the Limits of Transfer Learningwith a Unified Text-to-Text Transformer. Journal of MachineLearning Research (JMLR), 21:167, 2020. 8",
  "Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and BerntSchiele. A Dataset for Movie Description. In Conference onComputer Vision and Pattern Recognition (CVPR), 2015. 1": "Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, NiketTandon, Christopher Pal, Hugo Larochelle, Aaron Courville,and Bernt Schiele. Movie description. International Journalof Computer Vision (IJCV), 123:94120, 2017. 1, 3, 6, 7 Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia,and Aniruddha Kembhavi. Visual Semantic Role Labelingfor Video Understanding. In Conference on Computer Visionand Pattern Recognition (CVPR), 2021. 3",
  "Florian Schroff, Dmitry Kalenichenko, and James Philbin.FaceNet: A Unified Embedding for Face Recognition andClustering. In Conference on Computer Vision and PatternRecognition (CVPR), 2015. 7": "Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei, and Christopher D Manning. Generating semanticallyprecise scene graphs from textual descriptions for improvedimage retrieval.In Fourth Workshop on Vision and Lan-guage, 2015. 6 Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, andCordelia Schmid. End-to-end generative pretraining for mul-timodal video captioning. In Conference on Computer Visionand Pattern Recognition (CVPR), 2022. 3 Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, YurongChen, Yu-Gang Jiang, and Xiangyang Xue. Weakly super-vised dense video captioning. In Conference on ComputerVision and Pattern Recognition (CVPR), 2017. 3 Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen,Zhendong Niu, and Ming Zhou. Dense procedure captioningin narrated instructional videos. In Association of Computa-tional Linguistics (ACL), 2019. 3",
  "Andrew Shin, Katsunori Ohnishi, and Tatsuya Harada. Be-yond caption to narrative: Video captioning with multiplesentences. In International Conference on Image Processing(ICIP), 2016. 3": "Soldan, Mattia and Pardo, Alejandro and Alcazar, Juan Leonand Caba, Fabian and Zhao, Chen and Giancola, Silvio andGhanem, Bernard. MAD: A Scalable Dataset for LanguageGrounding in Videos From Movie Audio Descriptions. InConference on Computer Vision and Pattern Recognition(CVPR), 2022. 3 Makarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen.Knock! Knock! Who is it? Probabilistic Person Identifi-cation in TV series. In Conference on Computer Vision andPattern Recognition (CVPR), 2012. 3",
  "Ramakrishna Vedantam, C. Lawrence Zitnick, and DeviParikh. CIDEr: Consensus-based image description evalua-tion. In Conference on Computer Vision and Pattern Recog-nition (CVPR), 2015. 2, 3, 6, 7": "Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Don-ahue, Raymond Mooney, Trevor Darrell, and Kate Saenko.Sequence to sequence-video to text. In International Con-ference on Computer Vision (ICCV), 2015. 3 Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, MarcusRohrbach, Raymond Mooney, and Kate Saenko. TranslatingVideos to Natural Language Using Deep Recurrent NeuralNetworks. In North American Chapter of the Association forComputational Linguistics: Human Language Technologies,2015. 3 Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and YongXu. Bidirectional Attentive Fusion with Context Gating forDense Video Captioning. In Conference on Computer Visionand Pattern Recognition (CVPR), 2018. 3 Teng Wang, Huicheng Zheng, Mingjing Yu, Qian Tian, andHaifeng Hu.Event-centric hierarchical representation fordense video captioning. IEEE Transactions on Circuits andSystems for Video Technology, 31(5):18901900, 2020. 3",
  "Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, RanCheng, and Ping Luo.End-to-end Dense Video Caption-ing with Parallel Decoding. In International Conference onComputer Vision (ICCV), 2021. 3": "Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-toine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, andCordelia Schmid. Vid2seq: Large-scale pretraining of a vi-sual language model for dense video captioning. In Confer-ence on Computer Vision and Pattern Recognition (CVPR),2023. 3 Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and WeiXu. Video paragraph captioning using hierarchical recurrentneural networks. In Conference on Computer Vision and Pat-tern Recognition (CVPR), 2016. 3 Youngjae Yu, Hyungjin Ko, Jongwook Choi, and GunheeKim. End-to-end concept word detection for video caption-ing, retrieval, and question answering.In Conference onComputer Vision and Pattern Recognition (CVPR), 2017. 3",
  "Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.Weinberger, and Yoav Artzi. BERTScore: Evaluating TextGeneration with BERT.In International Conference onLearning Representations (ICLR), 2020. 3": "Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher,and Caiming Xiong. End-to-end dense video captioning withmasked transformer. In Conference on Computer Vision andPattern Recognition (CVPR), 2018. 3 We present additional insights and results in the supplemen-tary material. In Appendix A, we highlight how our auto-regressive Transformer decoder attends to various memoryfeatures. For the id-aware captioning task, we show therelative importance of the 3 visual features, while for theFill-in-the-blanks (FITB) task, we highlight how our modelattends to correct face clusters. Next, in Appendix B, weshow qualitative results for both tasks, FITB and id-awarecaptioning. We also illustrate how our new identity-awaremetric, iSPICE, is calculated on some examples. Finally,we end with discussion of some limitations in Appendix C.",
  "A. Analyzing Model Attention": "In this section, we visualize and discuss the attention scoresfrom MICaps auto-regressive Transformer decoder. In par-ticular, we focus on the cross-attention scores of the lastlayer as they reveal interesting insights about the featuresthat the captioning model uses. Throughout this section, weanalyze MICap trained jointly on id-aware captioning andFITB. All attention scores are obtained in inference mode.",
  "A.1. Attention Patterns in Id-aware Captioning": "In id-aware full captioning, for a particular videoset N ={Vi}Ni=1, we first encode the videos to obtain memory to-kens M and pass them through a Transformer decoder auto-regressively to generate one token (word) at a time. If weconsider that the number of tokens in the predicted caption-set is L, we can compute a matrix of cross-attention scores = L |M|, where |M| is the number of tokens in thedecoder memory. Note, while we use multi-head attention,scores over the heads are averaged obtain .We split the L tokens into 2 groups: (i) one group con-sists of person id label predictions or person tokens (PT);and (ii) the other group consists of all other tokens referredto as caption tokens (CT). For visualization, we sum overthe attention scores for each of the token types (id labels andtext) and convert our attention map to a matrix of 2 |M|.Next, we also group the memory tokens into 3 types ofvisual features used in our work: action (I3D), face (Arc-face), and semantic features (CLIP). Thus, we obtain a 23matrix of cross-attention scores for each sample.Results. We compute attention scores over all samples ofthe validation set and plot them as a probability densityfunction in . PT (red) and CT (green) represent theperson and caption tokens respectively. We observe that:(i) The model relies on CLIP features to predict captions(depicted by the overall high attention scores from 0.5-0.7).(ii) When predicting person tokens (PT) of the identity-aware captions, the model tends to look at face features(0.1-0.6) more than when predicting caption tokens (0-0.4).(iii) Finally, while action features are useful for captioning,",
  "A.2. Attention Patterns in FITB": "For the FITB task, we analyze how the person id predic-tions attend to face features from the decoder memory.For a videoset N = {Vi}Ni=1 and its corresponding cap-tionset with blanks C we obtain a cross-attention map of = |B| F, where |B| is the number of blanks in the cap-tionset, and F is the number of face detections across thevideoset. Each row of this matrix is normalized to sum to 1.The attention scores and captionsets with blanks are pre-sented in . In the next paragraphs, we will analyze the3 types (columns) of the presented scores. Cross-attention scores for face detections. In the left col-umn of , we visualize the attention scores directly foreach face detection. In the plot, x-axis spans time acrossdifferent videos. Our model tends to show a diagonal pat-tern indicating that person id label predictions tend to lookat faces in the same video (facilitated through the Evid em-beddings). However, as seen in captionset 5, left, row 1, themodel may also attend to other face detections of the sameperson across videos. This highlights that being able to at-tend to faces across videos is useful (compared to thatonly looks at faces within the same video). Cross-attention scores for face clusters grouped by videoindex. Shown in the middle column of , we groupthe F face detections into clusters, but split them based onvideo index in the videoset. For example, in captionset 1,we see that faces in cluster 1 appears across videos 1, 2, 4(C1/V1, C1/V2, C1/V4). This allows us to explain some ofthe predictions made by our model.Please note that the face cluster index and person id la-bels need not match numerically. That is, cluster 2 couldbe assigned the label P1 and cluster 1 the label P2. Thesechanges are acceptable as we only consider person id labelsin a local videoset.In cationset 3, we see that cluster 2 corresponds to theprediction P1 (first two rows) and cluster 4 (C4/V3) corre-sponds to person id label P2 (bottom two rows). In the lastrow of captionset 3, we see that our model predicts P2 forthe video id 4 correctly, while looking at cluster 4 in video3 (C4/V3). Previous work is unable to use such cross-video information. Cross-attention scores for clusters. In the right columnsof , we show attention scores directly grouped bycluster ids. Here, the original attention map of |B| Fis grouped to |B| |G|, where |G| is the number of faceclusters obtained after performing DBSCAN on the F facedetections.Captionset 2 is an example with multiple blanks and 4characters. We observe that some confusion in attention 0.00.20.40.60.81.0",
  "Density": "CLIP PTCT . Cross-attention scores density plots for the id-aware captioning task. We group decoder output tokens into two types: person idlabel tokens (PT), and caption tokens that represent other words (CT). Attention scores are grouped across the three input visual featurescapturing actions (I3D, left), faces (Arcface, middle), and semantic content (CLIP, right). Please refer to Appendix A.1 for a discussion. scores leads to errors in the predicted person id labels. Incaptionset 4, we also see 6 blanks, now with 3 characters.In the last row, while the model wrongly predicts P1, themodel does look at cluster 3 (corresponding to P3) correctly.Captionset 1 and 2 are examples of perfect attention scoresand clusters. P1 and C1, and P2 and C2 go together stronglyin these examples. Impact of number of clusters on FITB. shows theresults on FITB class-accuracy for varying the DBSCANepsilon parameter. These results indicate the importance ofclustering across videos and choosing an appropriate num-ber of clusters. Qualitatively, we adopt 0.75 as it is unlikelyto merge characters incorrectly.",
  "B. Qualitative Results": "iSPICE validation examples. To validate our new metric,we propose an experiment that measures similarity betweencaptions when identity names are added, removed, or re-placed (Sec. 4 of the main paper). While the quantitativeresults favor iSPICE, as seen in Tab. 1 of the main paper,we illustrate with examples the process of metric compu-tation in . We observe that the small difference inidentity names is captured correctly by iSPICE, due to thefocus on tuples containing identities, while other metrics donot show this sensitivity. FITB examples. While clearly shows the importanceof cross-attention scores of detected faces and computedclusters, the challenging visual scenarios are not evident.We pick two examples (captionset 3 and 4) from andpair them together with one frame from each video of thevideosets. shows the challenging nature of the videoswhere characters are often not looking at the camera (exam-ple 1 video 1, 3), the scene is dark, or the face may not evenbe visible (example 1 video 4 or example 2 video 3). MI-Cap leverages the ability to look at faces and clusters across",
  "videos to improve results on the FITB task": "Id-aware captioning examples. shows 2 exampleswhere our model does relatively well, while shows2 difficult examples where our model makes mistakes.In the left column of we see that the model rightlyidentifies P1 as the male character and P2 as the female.The last caption is quite interesting while the GT pointsto P1 giving P2 a bowl, our model predicts that P2 gives asad smile, which is not wrong. This also illustrates some ofthe challenges of evaluating captioning. In the right columnof , the predicted caption uses P2 to refer to the man,and is consistent across videos 3, 4, and 5 in the videoset.In the complex visual example of (left), ourmodel assigns P1 to all blanks.Similarly, in the multi-character example of (right), we observe some con-fusion between characters. Nevertheless, P2, identified asthe man on the left in video 3, is correctly identified for thefirst 3 videos. The model is also able to predict that theyare on a plane (caption for video 2). Nevertheless, these ex-amples illustrate the challenges of id-aware captioning. Asfuture work, they also highlight the need to evaluate visualgrounding of the identities beyond captioning performance.",
  "C. Limitation and Future Work": "One limitation of our work, inherited from the task defi-nition in LSMDC, is restricting videosets to local groupsof 5 videos. In the future, we would like to extend this tolarger videosets, perhaps spanning the entire movie. How-ever, the approach will need to be modified to operate onfull movies as: (i) providing features of all movie framesas decoder memory creates a huge number of embeddings;(ii) face clustering across the entire movie could be error-prone; and (iii) auto-regressively generating one caption ata time for hundreds of clips seems challenging, as the modelneeds to be cognizant of all previously generated captions.",
  "Face Detections": "GT: P1 / Pr: P1 / V1 GT: P1 / Pr: P1 / V2 GT: P1 / Pr: P1 / V4 GT: P2 / Pr: P2 / V5 C1/V1 C1/V3 C1/V4 C1/V5 C2/V5 C3/V1 GT: P1 / Pr: P1 / V1 GT: P1 / Pr: P1 / V2 GT: P1 / Pr: P1 / V4 GT: P2 / Pr: P2 / V5 C1C2C3 GT: P1 / Pr: P1 / V1 GT: P1 / Pr: P1 / V2 GT: P1 / Pr: P1 / V4 GT: P2 / Pr: P2 / V5 . We show 5 examples of our models attention scores on the FITB task. For each example (row), we show the captionset(with blanks) and the attention scores grouped in various ways. The left column shows the attention score for each blank across all facedetections in the video. The middle column shows attention scores for face detections grouped by clusters in each video. C1/V1 indicatesfaces appearing in cluster 1 and video 1, while C1/V2 indicates faces of the same cluster 1 appearing in video 2. The right column showsattention scores of each blank for face clusters (across videos). For each row in the attention scores, we indicate the ground-truth (GT) andpredicted (Pr) person id label and the video index (V1 .. V5) in which this blank appeared. See Appendix A.2 for a discussion. 0.40.50.60.70.750.80.91 eps (DBSCAN) Number of clusters Class accuracy 66.35 68.16 69.33 69.79 69.14 68.72 65.47 65.10",
  ". Class-accuracy for the FITB task by varying the DB-SCAN eps distance threshold. We also show a box-plot for thenumber of clusters created at each threshold across samples of thevalidation set": "We believe that a hierarchical model that builds from shotsto scenes to the full movie may be more appropriate here.Second, the tasks for FITB and full captioning do notlearn at the same pace, and choosing a single best check-point for both may be difficult. We posit that the user maychoose two checkpoints, one for each task. Furthermore,we observe that by weighing the FITB and full captioninglosses appropriately, additional performance improvementscan be achieved for one task at the cost of the other task.We have also not considered using external knowledgeor pre-trained large language models (LLMs) or vision-language models (VLMs) built for captioning. We believethat it is interesting to learn what can be achieved by trainingon LSMDC alone. As seen in multiple examples through-out Appendix B, MICap does perform quite well given thechallenging scenarios. Candidate : A path leads from the side of the circle splitting into two prongs. A third crop circle has two straight lines at either side and a circle of maize remaining in the center with another path leading off from its side. It splits into three larger prongs the central one of which points towards a smaller circle. P1 is on the phone as P2 looks out of his window at the yard. P2 bows his head. : A path leads from the side of the circle splitting into two prongs. A third crop circle has two straight lines at either side and a circle of maize remaining in the center with another path leading off from its side. It splits into three larger prongs the central one of which points towards a smaller circle. P1 is on the phone as P1 looks out of his window at the yard. P1 bows his head. Tuples : [[path, side, lead from], ..., [prong], [p1, window, look out of], [p1, phone, on], [window, yard, at], [phone, window, have], [window], [yard], [p1], [phone], [p1], [p1, head, bow], [p1, head, have], [head], [p1]] Tuples : [[path, side, lead from], ..., [prong], [p2, window, look out of], [p1, phone, on], [window, yard, at], [phone, window, have], [window], [yard], [p1], [phone], [p2], [p2, head, bow], [p2, head, have], [head], [p2]]",
  "~ 0.16": "Candidate : Opening a small chest filled with personal items P1 takes out a pair of green drawstring pants. In the common sleeping area P1 sets his bags on a lower bunk. A rat runs along a shelf by the headboard. P1 springs up and hits his head on the top bunk. P1 scrambles wildly off the bed grabbing his duffel and peers after the rat with a fearful stare. : Opening a small chest filled with personal items P1 takes out a pair of green drawstring pants. In the common sleeping area P2 sets his bags on a lower bunk. A rat runs along a shelf by the headboard. P2 springs up and hits his head on the top bunk. P2 scrambles wildly off the bed grabbing his duffel and peers after the rat with a fearful stare. Tuples : [[p1, pants, take out], [p1, chest, take out opening], [chest, item, fill with], ..., [pants], [pair], [p1], [chest], [item], [p2, area, set in], [p2, bag, set], [p2, bunk, set on], ..., [p2], ..., [p2, bunk, hit on], [p2, head, hit], [p2, spring], [bunk, top], [p2, head, have], [head], [p2], [bunk], [p2, bed, scramble off], ..., [p2, duffel, have], [bed], [duffel], [peer], [p2], [rat], [stare]] Tuples : [[p1, pants, take out], [p1, chest, take out opening], ..., [p1], [chest], [item], [p1, area, set in], [p1, bag, set], [p1, bunk, set on], ..., [p1], ..., [p1, bunk, hit on], [p1, head, hit], [p1, spring], [bunk, top], [p1, head, have], [head], [p1], [bunk], [p1, bed, scramble off], ..., [p1, duffel, have], ..., [p1], [rat], [stare]]",
  "~ 0.12": "Candidate : Meanwhile P1 races to his car in the airport parking lot. P2 stows his bags in the trunk then climbs in. As P2 starts the engine his wipers clear a layer of dirt off the windshield. In an exam room at the clinic the dark haired nurse draws his blood. P2 winces. : Meanwhile P1 races to his car in the airport parking lot. P1 stows his bags in the trunk then climbs in. As P1 starts the engine his wipers clear a layer of dirt off the windshield. In an exam room at the clinic the dark hairednurse draws his blood. P2 winces. Tuples : [[p1, car, race to], [p1, lot, race in], [p1, car, have], [lot, airport], [lot, parking], [car], [p1], [p1, stow], [bag, climb], [bag, trunk, in], [p1, bag, have], [bag], [p1], [trunk], [wiper, layer, clear], [wiper, windshield, clear off], [p1, engine, start], [layer, dirt, of], [engine], [p1], [windshield], [layer], [dirt], [wiper], , [nurse, haired], ..., [p2, wince], [p2]] Tuples : [[p1, car, race to], [p1, lot, race in], [p1, car, have], [lot, airport], [lot, parking], [car], [p1], [p2, stow], [bag, climb], [bag, trunk, in], [p2, bag, have], [bag], [p2], [trunk], [wiper, layer, clear], [wiper, windshield, clear off], [p2, engine, start], [layer, dirt, of], [engine], [p2], , [nurse, haired], , [p2, wince], [p2]].",
  "~ 0.57": ". We show the effect of identity on captioning metrics using add, remove, and replacement examples. This corresponds to thevalidation experiment conducted in Tab. 1 of the main paper. For each example, the identity labels are underlined in the candidate andreference captionsets. We also show how iSPICE works by illustrating the tuples, highlighting tuples with identities, and showing thecomputation of term 1 (left) and term 2 (right) corresponding to tuples with size 1 and = 1 respectively. iSPICE takes into the accountthe identity whereas the other metrics show a high score due to high number of n-gram matches.",
  "P1": ". Examples from the Fill-in-the-blanks (FITB) task. On the left, we show one frame from each video of the videoset and thecorresponding caption with blanks. In the middle, we show the ground-truth and predicted person id labels. On the right, we show thecross-attention maps (face detections, clusters, and clusters by video ids), presented in . We pick the examples corresponding tocaptionset 3 and 4 of for better understanding. In general, we observe that person predictions depend strongly on the cluster featuresand their attention. In some cases, the identity may be difficult to predict as seen in the last row of the second example, where our modelpredicts P1 instead of P3, even though the attention masks are correctly focusing on C3/V5.",
  "GT : P1 tenses as the plane jostles again.Pred : P2 looks at him": ". The above examples are relatively difficult cases where there are multiple characters involved with lot of drama or actionhappening in quick succession. The characters faces are also occluded or partly visible (left example) making it harder to predict identity.We observe that the predicted captions do not capture the tension (e.g. plane turbulence) and the identities."
}