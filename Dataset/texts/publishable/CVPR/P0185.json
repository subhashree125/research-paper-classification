{
  "Abstract": "Recent research identified a temporary performancedrop on previously learned tasks when transitioning to anew one. This drop is called the stability gap and has greatconsequences for continual learning: it complicates the di-rect employment of continually learning since the worse-case performance at task-boundaries is dramatic, it limitsits potential as an energy-efficient training paradigm, andfinally, the stability drop could result in a reduced final per-formance of the algorithm. In this paper, we show that thestability gap also occurs when applying joint incrementaltraining of homogeneous tasks. In this scenario, the learnercontinues training on the same data distribution and hasaccess to all data from previous tasks. In addition, we showthat in this scenario, there exists a low-loss linear path tothe next minima, but that SGD optimization does not choosethis path. We perform further analysis including a finerbatch-wise analysis which could provide insights towardspotential solution directions.",
  ". Introduction": "Deep neural networks demonstrate remarkable performanceacross numerous machine-learning tasks.Nevertheless,when trained on non-IID streaming data these networksstruggle to accumulate knowledge, and tend to forget pre-viously acquired knowledge. Continual learning developstheory and methods to address this problem .Itaims to develop algorithms that prevent catastrophic for-getting and achieve a more favorable trade-off betweenstability and plasticity while learning on a data stream.A typical test setting that continual learning considers islearning from a sequence of tasks (each task with anotherdata distribution) . Usually, continual learning methodperformance is evaluated at the end of each of the tasks.Recently, researchers have observed an interestingphenomenon that went unnoticed in this standard evalua- tion setup: at the start of training a new task, the perfor-mance of previous tasks drastically drops, and only slowlyrecovers during the subsequent training of the new task. DeLange et al. coined the term stability gap for this phe-nomenon. This observation should be taken into accountfor the application of continual learning systems (especiallyin safety-critical contexts) since it significantly lowers theworst-case performance of these algorithms. Furthermore,it can potentially worsen the final accuracy of the learner,since it might not recover totally from the knowledge lossincurred during the stability gap. Addressing the stabilitygap is therefore of utmost importance .The underlying mechanism responsible for the stabil-ity gap remains the subject of lively scientific debate, withno clear explanation available yet.Originally, Caccia etal. hypothesized that the cause for the stability gap isbecause old class prototypes receive a large gradient fromclosely lying new class prototypes. However, this hypoth-esis could not fully explain the phenomenon, because thestability gap had also been observed in domain incrementallearning (where the set of classes remains the same) . Apossible remaining explanation is the following. When op-timizing on new data, the objective is to minimize the losson both the available new data and unavailable old data. Theloss on the unavailable previous data is then approximatedwith various continual learning strategies, such as regular-ization and data rehearsal . An explanationfor the stability gap could be the failure to approximate thisideal joint loss on previous and current task data. Surpris-ingly, a recent paper , showed that even in the case ofjoint incremental training the stability gap occurred (in thiscase, we do have access to both old and new task data andcan minimize the joint loss on both old and new data). They,therefore, came to the important realization that we shouldnot only focus on what to optimize but more importantly onhow to optimize our objective.Hess et al. made their important observation whenlearning on heterogeneous tasks, referring to the fact thateach task is drawn from a different distribution. In this pa-",
  ". Summary of the expanding scope of the stability gap:from heterogeneous to homogeneous tasks": "per, we show that the stability gap is even present in the caseof joint incremental learning on homogeneous tasks (whereeach task is drawn from the same distribution). This re-sult in presented in . So, even in the case that bothtasks have the same distribution, SGD optimization does notsucceed in going to the nearby optimal position withoutderailing through a high-loss region. The only differencebetween the new and old data is that the network has seenthe old data (typically for 100 epochs here) and has not yetseen the newly arriving data. We think that this further con-firms the fundamental nature of the stability gap in continuallearning: it even occurs in the most simple continual learn-ing setting when training from an increasing amount of datadrawn from the same distribution. The main contributionsof this work are:1. We show that the stability gap also occurs during jointincremental learning from homogeneous tasks, arguablythe least challenging continual learning setting.",
  ". We show that there exists a linear low-loss path to theoptimal loss, but that SGD is not following this path (thiswas hypothesized in but was not demonstrated)": "3. We perform an analysis at mini-batch level, and discoverthat the gradient just after the task boundary successfullydecreases the mini-batch loss but results in an overallloss increase on the test set. Addressing this might po-tentially lead to a solution to the stability gap problem.This manuscript does not provide a new possible explana-tion for the stability gap. We think the observation that itoccurs even for joint incremental learning of homogeneoustasks is relevant. Our results, confirm those of Hess et al. and we agree with them that the focus should shift to howto optimize rather than what to optimize.",
  ". Experimental setup": "Datasets: We use the standard benchmark train-test split forall the datasets used in this work, that is publicly available.CIFAR-10 dataset consists of 60, 000 images of 32 32size, divided into 10 classes: 50, 000 used for training and10, 000 for testing. CIFAR-100 dataset consists of 60, 000images of 32 32 size, divided into 100 classes: 50, 000used for training and 10, 000 for testing.Architectures: We consider two convolutional network ar- chitectures, VGG-16 and ResNet-18 for our study.Training Setup: Our code base uses the pytorch library.For training we use the SGD optimizer with hyperparame-ters: learning rate (lr) of 0.01, momentum (m) of 0.9, batchsize (bs) of 64.Notation: In this work, we mainly study the two-task set-ting. All results reported will be in the homogeneous tasksetting, where the various tasks are drawn from the samedistribution. We use the notation of A-B to indicate taskA will contain A% of the data and task B will contain B%of the data from the original training dataset. We will usethe notation A-B to identify the joint incremental learningsetting. In this case when training task B, the algorithm hasaccess to all the data of task A. In practice, for this settingfor task B, we just combine the data of both tasks, and con-tinue training on the combined dataset. Note, that the dataof task A and B in our paper are disjoint data sets and donot contain the same data samples.Note on plots:Most plots in this paper are with a warm-started model. This means a model trained on task A withthe data as prescribed in the setting was used to continuetraining on task B. The starting point of the x-axis is thenthe iterations directly after the task-switch. This was doneto better study the effect of the actual stability gap. Note,that we do not show the end of training on task B.",
  ". Stability Gap in Joint Incremental Learning ofHomogeneous Tasks": "To establish the occurrence of the stability gap in the set-ting of joint incremental learning of homogeneous tasks, westudy the 50-50* setting. This setting divides the training setinto two equally sized tasks, A and B. Both tasks are drawnfrom the same distribution. The test accuracy is providedfor two datasets in . We can observe that even infor this case, there is a clear stability gap. The performancedrops from 0.89 to 0.74 on CIFAR-10 and from 0.65 to 0.38on CIFAR-100. We posit a larger gap on CIFAR-100 to berelated to the smaller number of samples per class. Notethat for both these graphs performance has not returned toits task A level consistently even after the 2000 iterationsshowing the long-lasting impact of the stability gap. Af-ter continued training for around 3500-4500 iterations themodels start to achieve more consistently a performanceabove 0.89 and 0.65, respectively.In we provide a summary of the main papers onthe stability gap. The stability gap has been observed in in-creasingly general settings. Here, we show that it is also ob-served for joint incremental training of homogeneous tasks,which is arguably the most simple continual learning set-ting. This observation is relevant since it discards explana-tions for the stability gap which are based on characteristicsthat are not present (e.g. it cannot be uniquely explainedby the presence of disjoint tasks or heterogeneous distribu- . Occurrence of the stability gap in joint incremental learning with homogeneous tasks in the 50-50 setting on (left) CIFAR-10and (right) CIFAR-100 datasets on a ResNet-18 model. This plot starts after training with task A, and the x-axis represents the number ofiterations of training on task B.",
  ". Linear Mode Connectivity": "Garipov et al. were the first to study the mode con-nectivity properties of neural networks weights by connect-ing two independent minima obtained through differentlyseeded optimization processes using a simple curved path oflow loss. Frankle et al. later showed that a simpler kindof path naturally emerges early in training. They observedthat models that are trained from a warm-started model ver-sion on the same dataset but with different SGD-noise leadto two checkpoints that are connected by a linear path oflow loss. Mirzadeh et al. later extended that property tooptima of multitask models trained on incrementally largerdatasets. Hess et al. hypothesized that there exists a low-loss path between the optima when doing joint incremen-tal learning of heterogeneous tasks. However, they do notdemonstrate this in their paper. In this article, we investigatewhether it is the case that training on incremental homoge-neous tasks leads to linearly connected optimas or not (andwe verify this). To do so, we take the initial checkpoint withweights 1 and final checkpoint with weights 2 and inter-polate between the two by taking = 1+(12) with . We later compute and report the test accuracy ofeach to determine if the linear path is of low loss. compares the loss of the models obtained bylinearly interpolating between the initial and final model tothe ones of the model checkpoints along the SGD optimiza-tion trajectory. Unsurprisingly, the path taken by SGD dur-ing optimization is not linear. More surprisingly, it goesthrough areas of higher loss especially during the initial pe-riod that corresponds to the stability gap, while the linear . In the 50-50* setting, we present the loss path withSGD and the linear connectivity loss path between the warm-startand final models using with ResNet-18 model on (left) CIFAR-10,(right) CIFAR-100 dataset. In order to observe the stability gap,we zoom in on the first few iterations of the new task. path between the initial and final model is of low loss. Thelinear path results confirm that a low-loss path exists be-tween the minima achieved after training task A, and theminima after training task B. Surprisingly, SGD does nottake this path and instead passes through a high-loss areabefore converging towards the minima which is optimal fortask A and B data. We have shown here the first few itera-tions after the task-switch. Refer to the supplementary forplots showing the entire path based on 5 epochs of trainingwith task B.Per mini-batch loss analysis. In , we observe witha microscope the learning of the model per mini-batch. Inthis plot we show the training batch accuracy for the currentmini-batch before (blue line) and after (red line) the SGDupdate. We observe that the SGD update results in a lossdecrease(or accuracy increase) for the particular mini-batch(the blue line is below the red line). However, when we lookat the test accuracy (black line), we see that even though ini- . Using CIFAR-100 with ResNet-18, we present the fineranalysis of the local improvement obtained at the batch level byobserving the train accuracy per batch before (blue line) and after(red line) SGD update is applied for the batch in the 50-50* setting.The black line is the corresponding test accuracy.",
  ". Additional Analysis": "Here we verify if the stability gap also occurs for severalother settings.Stability gap using other architectures. While we presenta detailed study of the stability gap on ResNet-18 archi-tecture, in we show this phenomenon is not re-stricted to a specific architecture by using another well-known VGG-16 architecture on the CIFAR-100 dataset.Stability gap in other settings. In .2, we mainlyconsidered the 50-50* setting which is the joint incremen-tal training with homogeneous task. Here, we look at thestability gap with different first task size and include resultsfor the setting 10-90* and 75-25* in . We observethat the gap is larger when starting from a smaller first task.In addition, we conduct experiments with the splits 50-",
  ". Using CIFAR-100 with ResNet-18, stability gap in (left)10-90* (right) 75-25* setting. We can see that the stability gapincreases for a smaller-sized first task": ".Using CIFAR-100 with ResNet-18, stability gap in(left) 50-50, (right) 75-25 setting. We can see that the stabilitygap increases when comparing (left) with the 50-50* setting in(right) and (right) with the 75-25* setting in (right). 50 and 75-25 which is equal to incremental training withnew data from the same distribution (without access to allprevious data). We observe in that the stabilitygap occurs in this setting too and is more pronounced thanthe corresponding 50-50* and 75*-25* setting studied be-fore. The gap is larger from 0.65 to 0.20 and 0.70 to 0.23 asagainst 0.65 to 0.38 and 0.70 to 0.44, respectively.",
  ". Conclusions": "In this article, we present compelling insights into the sta-bility gap phenomenon. In particular, we show that it alsomanifests when applying joint incremental training on a se-quence of homogeneous tasks, which is often consideredthe simplest scenario for continual learning. Through exper-imental evidence, we demonstrate that while the loss alongthe SGD path displays a stability gap, this discrepancy isnot mirrored in the loss along the linear trajectory betweencheckpoints. An analysis at the mini-batch level showedthat the gradient computed on the initial mini-batches (afterthe task-switch) does reduce the loss for each mini-batchbut it results in an increased loss on the test data. We alsoobserve that in the incremental learning with homogeneoustasks, when we remove rehearsal (going 50-50* to 50-50),the stability gap increases. In further research, we will ex-plore this direction to possibly discover the cause of the sta-bility gap and possible remedies. Acknowledgement.We acknowledge projects TED2021-132513B-I00 and PID2022-143257NB-I00 funded byMCIN/AEI/10.13039/501100011033, by European UnionNextGenerationEU/PRTR, by ERDF A Way of Making Eu-rope, and by Generalitat de Catalunya CERCA Program. Lucas Caccia, Rahaf Aljundi, Nader Asadi, TinneTuytelaars, Joelle Pineau, and Eugene Belilovsky.New insights on reducing abrupt representationchange in online continual learning. In InternationalConference on Learning Representations, 2022. 1, 2",
  "Arslan Chaudhry, Marcus Rohrbach, Mohamed Elho-seiny, Thalaiyasingam Ajanthan, Puneet K. Dokania,Philip H. S. Torr, and MarcAurelio Ranzato. On tinyepisodic memories in continual learning, 2019. 1": "Matthias Delange, Rahaf Aljundi, Marc Masana,Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh,and Tinne Tuytelaars.A continual learning survey:Defying forgetting in classification tasks. IEEE Trans.Pattern Anal. Mach. Intell., 2021. 1 Jonathan Frankle, Gintare Karolina Dziugaite, DanielRoy, and Michael Carbin.Linear mode connectiv-ity and the lottery ticket hypothesis. In InternationalConference on Machine Learning, pages 32593269.PMLR, 2020. 3 Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,Dmitry P Vetrov, and Andrew Gordon Wilson. Losssurfaces, mode connectivity, and fast ensembling ofdnns. In Advances in Neural Information ProcessingSystems, 2018. 3",
  "Timm Hess, Tinne Tuytelaars, and Gido M. van deVen.Two complementary perspectives to continuallearning: Ask not only what to optimize, but also how,2023. 1, 2, 3": "James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-nowitz, Joel Veness, Guillaume Desjardins, Andrei A.Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-nieszka Grabska-Barwinska, Demis Hassabis, ClaudiaClopath, Dharshan Kumaran, and Raia Hadsell. Over-coming catastrophic forgetting in neural networks.Proceedings of the National Academy of Sciences,114:3521 3526, 2016. 1 Matthias De Lange, Gido M van de Ven, and TinneTuytelaars. Continual evaluation for lifelong learning:Identifying the stability gap. In The Eleventh Interna-tional Conference on Learning Representations, 2023.1, 2",
  "Supplementary Material": "We have included more detailed empirical results in thesupplementary material.Linear Mode Connectivity In and weshow the full loss trajectory of the second task training for5 epochs for both the linear path and the SGD path forCIFAR-10 and CIFAR-100 datasets, respectively. . In the 50-50* setting, we present the complete loss pathwith SGD and linear connectivity between the warm-start and finalmodels after 5 epochs of training using changed in steps of 0.01on CIFAR-10 dataset with ResNet-18 model. In order to observethe stability gap, we had zoomed on the first 400 iterations in themain paper. . In the 50-50* setting, we present the complete loss pathwith SGD and linear connectivity between the warm-start and finalmodels after 5 epochs of training using changed in steps of 0.01on CIFAR-100 dataset with ResNet-18 model. In order to observethe stability gap, we had zoomed on the first 400 iterations in themain paper."
}