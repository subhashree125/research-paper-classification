{
  "Abstract": "Continual learning methods are known to suffer fromcatastrophic forgetting, a phenomenon that is particularlyhard to counter for methods that do not store exemplars ofprevious tasks. Therefore, to reduce potential drift in thefeature extractor, existing exemplar-free methods are typ-ically evaluated in settings where the first task is signif-icantly larger than subsequent tasks. Their performancedrops drastically in more challenging settings starting witha smaller first task. To address this problem of feature driftestimation for exemplar-free methods, we propose to adver-sarially perturb the current samples such that their embed-dings are close to the old class prototypes in the old modelembedding space. We then estimate the drift in the embed-ding space from the old to the new model using the per-turbed images and compensate the prototypes accordingly.We exploit the fact that adversarial samples are transferablefrom the old to the new feature space in a continual learningsetting. The generation of these images is simple and com-putationally cheap. We demonstrate in our experiments thatthe proposed approach better tracks the movement of proto-types in embedding space and outperforms existing meth-ods on several standard continual learning benchmarksas well as on fine-grained datasets. Code is available at",
  "Adversarial": "attacks Adversarial SamplesNew Samples Remove theChange 'attaperturbed iIncrease thethe clumsine . Illustration of Adversarial Drift Compensation (ADC)and SDC . In SDC, the drift kt1t is estimated as the av-erage of drift of all new task samples after training on a new task.Instead, we propose to move the new task features close to the oldprototype P kt1 of class k by perturbing the new images using tar-geted adversarial attacks. The drift of the adversarial samples fromold to new feature space is used to resurrect all old prototypes. learned information, a phenomenon known as catastrophicforgetting .Recent studies in continual learning(CL) focus on two prevalent scenarios :Task-Incremental Learning (TIL), where task informationis available during testing, and Class-Incremental Learning(CIL), where it is not. Our work aims to address the morechallenging CIL problem. Exemplar-based CIL methods store small subsets of data from each task. These exemplarsare later replayed with current data during training in newtasks. Although effective, these methods necessitate stor-ing input data from previous tasks, leading to multiple chal-lenges in practical settings such as legal concerns with new",
  "arXiv:2405.19074v1 [cs.CV] 29 May 2024": "regulations (e.g. European GDPR where users can requestto delete personal data), and privacy issues when dealingwith sensitive data like in medical imaging. Recently, theexemplar-free CIL (EFCIL) setting is extensively studied. However, unlike exemplar-basedmethods, the EFCIL methods are only effective when start-ing with high-quality feature representations and are thusdependent on having a large initial task which is typicallyhalf of the whole dataset. However, a more practical CILapproach should be able to perform well on training from asmaller initial task and at the same time should not store ex-emplars. We define this as a small-start setting and analyzehow existing EFCIL methods perform in this setting.A critical aspect in CIL is the semantic drift of featurerepresentations after training on new tasks. This re-sults in the movement of class distributions in feature space.Thus, it is crucial to track the old class representations af-ter learning new tasks. While the class-mean in the newfeature space can be effectively estimated using Nearest-Mean of Exemplars (NME) , it is challenging toestimate it without exemplars. Usually, this drift is mini-mized with heavy functional regularization, which conse-quently restricts the plasticity of the network. Another wayis to estimate it from the drift of current data, as done inSDC or by augmenting old prototypes using new classfeatures . In this paper, we propose a novel drift es-timation method using adversarial examples to resurrect oldclass prototypes in the new feature space as shown in .Adversarial examples are maliciouslycrafted inputs that are designed to fool a neural networkinto predicting a different output than the one initially pre-dicted for the original input. Exploiting the concept of tar-geted adversarial attacks , we propose to perturb thenew data such that the adversarial images result in embed-dings close to the old prototypes. Now, the drift from old tonew feature space is estimated using these adversarial sam-ples, which serve as pseudo-exemplars for the old classes.We hypothesize that the pseudo-exemplars behave like theoriginal exemplars in the feature space, and thus we exploitthem to measure the drift. This generation of adversarialsamples is computationally cheaper and much faster (onlya few iterations) compared to data-inversion methods which inverts embeddings to realistic images.Following recent studies , we explore usingclass prototypes with an NCM classifier and show thata simple baseline of logits distillation with an NCMclassifier often outperforms existing EFCIL methods in thesmall-start setting. Applying our proposed drift compen-sation method with this baseline, we obtain state-of-the-artperformance with significant gains over existing methodson standard CL benchmarks using CIFAR-100 , Tiny-ImageNet and ImageNet-Subset as well as fine-grained datasets like CUB-200 and Stanford Cars . Our contributions can be summarized as: We study the challenging EFCIL settings and highlightthe importance of continually learning from small-startsettings instead of assuming the availability of half of thedataset in the first task. We present a novel and intuitive method - AdversarialDrift Compensation (ADC) to estimate semantic drift andresurrect old class prototypes in the new feature space.We also investigate how adversarially generated samplestransfer in CIL settings from old to the new model. We perform experiments on several CIL benchmarks andoutperform state-of-the-art methods by a large margin onseveral benchmark datasets. Especially notable are ourresults on fine-grained datasets, where we report perfor-mance gains of around 9% for last task accuracy.",
  ". Related Work": "Class-Incremental Learning. CIL methods aimto learn new data which arrives incrementally and suffersfrom the catastrophic forgetting problem .Dur-ing evaluation in CIL without the task id, it is difficultto distinguish classes that belong to different tasks .While in general this setting is tackled using rehearsal ap-proaches by storing raw inputs, some at-tempts have been made without storing raw inputs. LwF prevents important changes in the network by pre-venting the output of the current model to drift too muchfrom the output of the previous model. PASS learnsthe backbone using self-supervised learning and later usesfunctional regularization and feature rehearsal, SSRE proposed an architecture organization strategy that aims totransfer invariant knowledge across tasks. In FeTRIL ,the authors freeze the feature extractor and estimate the po-sition of the old class features by using the current task datavariance. Recently, FeCAM leveraged the mean andcovariance of the previous task features and proposed a ma-halanobis distance-based classifier.Drift estimation.When updating the feature extractoron new classes, the representation learned for the oldclass prototypes changes and thus the need to rectify thosedrifts . SDC showed that the new data can be usedto estimate the drift of the old prototype representations.Recent methods also explored how to update theprototypes learned in old tasks to counter the drift. Toldo etal. proposed to learn the relations between old and newclass features to estimate the drift. NAPA-VQ pro-posed to augment the prototypes using the topological in-formation of classes in the feature space. Prototype Rem-iniscence proposed to dynamically reshape old classfeature distributions by interpolating the old prototypes withthe new sample features. In this work, we generate adver-sarial samples which behaves as pseudo-exemplars and isthen used to measure the drift. CL using Adversarial Attacks. Adversarial Attacks hasbeen studied in-depth in recent years , andhas been later harnessed to create realistic looking imagesfrom a trained vision model , including inputs thatcan be later used for training .Some recent meth-ods in exemplar-based CIL borrowed the ideaof adversarial attacks. ASER used the kNN-specificShapley value to obtain more representative buffer samples.GMED edits the exemplars by monitoring the changein loss when training on incoming data. RAR used thepairwise relations between the exemplars and the new sam-ples and perturb the exemplars to obtain samples close to thedecision boundaries. While all these approaches use adver-sarial attacks on the memory samples, we use it to perturbthe new data to simulate the old data.",
  ". Method": "We consider the EFCIL setup where new classes emergeover time and we are not allowed to store samples from oldclasses. These classes come in different tasks, one task ata time, and the tasks contain a mutually exclusive set ofclasses. When training on task t, we have access to cur-rent dataset Dt = {Xt, Yt} with images Xt and labels Yt.The main goal of EFCIL is to learn a model h that correctlyclassifies the data into classes encountered so far. We useht(x) = (Wtft(x)), where ft is the feature extractor pa-rameterized by t learned in task t and Wt is weight matrixof the linear classifier with softmax function .",
  ". Motivation": "In general, for a new feature extractor ft trained on newdata and an old feature extractor ft1 from previous task,we have access to old class prototypes up to task t 1 de-noted by P Y1:t1t1. We compute the prototypes for all newclasses after training in the current task. For a class k in taskt, we compute P kt =1",
  "|Xkt |": "xXkt ft(x), where Xkt is theset of samples from class k, ft(x) is the feature embeddingfor an image x from class k. However, the old class proto-types were computed on the old feature space ft (t < t) inold tasks and have drifted to a different position in the newfeature space ft after training on new data.Previously, SDC proposed to compensate the driftof these prototypes P Y1:t1t1by computing the drift fromold model embeddings ft1(x) to new model embeddingsft(x) corresponding to all images x in the current task.This drift of the current data is then used to approxi-mate the drift of previous task prototypes by consideringa weighted Gaussian window around the prototypes (giv-ing more weights to drift vectors close to the prototype).However, the quality of this drift approximation for previ-ous prototypes is expected to be low when few current taskdata points are close to a given prototype in the embedding . Illustration to show that the cosine distance between em-beddings and old prototype in the old feature space is correlatedwith the cosine distance between embeddings and oracle proto-type in the new feature space. This holds true for embeddings ofboth initial and adversarial samples. For demonstration, we selectfew current-task samples that are closest to the old prototype andchoose the same target old class for all samples. The blue and or-ange points represents the non-modified current class samples andthe modified samples using our proposed approach respectively.In this analysis, we compute the oracle prototype using all old taskdata in the new feature space. space. We show in our experiments that SDC indeed strug-gles to estimate the drift in the small-start settings where thefeature representations change considerably.In a similar fashion, it is possible to estimate this driftwithout using such weights, by simply choosing the closestsamples to each old class prototype and compute the aver-age feature of these samples when fed to the new backbone.We select samples from the current task that are close tothe old prototype of a given class and verify that such sam-ples also lie close to the oracle prototype (in the new featurespace). We analyze in that there exist a correlationbetween the distance to the old prototype in the old featurespace and the distance to the oracle prototype in the newfeature space (see blue dots). This motivates us to leveragecurrent task samples so that their distance to the old proto-type in the old feature space is even smaller, which could inturn improve the drift estimation. We hypothesize that thiscan be done by computing adversarial samples from the cur-rent task samples, aiming for their representation to matchone of the old class prototypes in the old feature space.",
  ". Adversarial Drift Estimation": "To estimate the drift of old class prototypes after updatingthe model on new classes, it is desirable to have the ex-emplars. These exemplars can be passed through the newmodel to compute the oracle prototype position in the newfeature space. However, in the exemplar-free setting, wecan only access the new data. In order to use the new datato represent the old data, we exploit the concept of targeted KnowledgeDistillationLoss New Image (New Class: Cow)",
  "Adversarial Image": "blue dots should be super close to the star since we do the attack directly on the star not somewhere near that.Remove class circles.will be good to increase the drift, symbols are overlapping too much.Add an arrow from adversarial sample to new image with i iterations label.Update the equn from equn 4. . (a) Adversarial Sample Generation: On the old model feature space, the new samples closest to the old prototype are selected anditeratively perturbed in the direction of the target old prototype to generate adversarial samples which are now misclassified as the target oldclass resulting in embeddings closer to the old prototype. We perform this for every old class (we show 2 classes here for demonstration).(b) Model Training with Drift Compensation: The new model is trained using the classification loss for learning new classes and knowledgedistillation loss to prevent forgetting of old classes. After the new model is trained, the adversarial samples generated using the old modelare passed through both the models and the drift from old to new feature space is estimated. This is then used to update the old prototypes. adversarial attacks to target one old class at a timeand perturb the new data in a way that it serves as a sub-stitute of old data to the model. We perform adversarial at-tacks on new data to move its embeddings very close to oldprototypes in the old feature space. Using the adversarialsamples, we can estimate the drift from old to new featurespace and compensate it as illustrated in .To estimate the drift of prototype P kt1 for a target oldclass k, we obtain X k by sampling a set of m data pointsfrom the current task data Xt which are closest to P kt1based on L2 distance between the embeddings of samples inXt and the prototype P kt1. We aim to perturb the samplesx X k and obtain X kadv such that the adversarial samplesxadv X kadv are closer to P kt1 and are now classified toclass k using the NCM classifier in the old feature space:",
  "where xL(ft1, x, P kt1) is the gradient of the objectivefunction with respect to the data x and refers to the stepsize. We perform the attack for i iterations": "Here, the goal is different from conventional adversarialattacks like FGSM and its variants which aimto minimize the perturbation in order to keep the perturbedimage visually similar to the real image by having a fixed-budget generally based on 2 or -norm of perturbation.In our case, we do not need to apply such restrictions on thedistance between initial and final image, instead, we onlyclip the perturbed image in the existing range of pixel val-ues. We show in supplementary materials that indeed thegenerated adversarial images have much higher perturba-tion. We do observe that our formulation is closer to the2-norm based attack as we use 2 normalization of the gra-dient vector to obtain a unit perturbation vector which isscaled using the step size.Continual Adversarial Transferability. An interesting as-pect of our method is that the adversarial samples xadv arecrafted on the old feature extractor ft1 and then passedto the new feature extractor ft expecting that the adver-sarial samples will still be misclassified as the target oldclass k. We define this as continual adversarial transfer-ability where the adversarial samples generated on the oldfeature space still behave in the same way on the new fea-ture space. This is feasible since the old model and the newmodel are not entirely different because of the knowledgedistillation used in order to reduce catastrophic forgetting.This is related with the concept of adversarial transferabil-ity , where an attack obtained on one neuralnetwork also behaves as an attack on other independentlytrained neural network based architectures.We analyze the oracle setting using the old class data tovalidate the continual adversarial transferability. We showin that distance of the adversarial samples from theirtarget prototypes in the old feature space is still correlatedwith their distance to the oracle prototypes of the target classin the new feature space. This suggests that the adversarialsamples crafted using the old feature space are still effectivein the new feature space and therefore allows us to reliably",
  "P kt = P kt1 + kt1t(5)": "After compensating all old prototypes, we use the NCMclassifier in the new feature space for classifying the testsamples. Unlike SDC , we do not perform weightedaveraging based on the distances to the prototype since em-beddings from adversarial images are very close to the pro-totypes and we found no gain by applying this additionalweighting scheme.",
  ". Experiments": "Datasets. We perform experiments on several CIL bench-marks. CIFAR-100 contains 50k training images ofsize 32x32 and 10k test images, divided in 100 classes.TinyImageNet contains 100k training images and 10ktest images from 200 classes and image size of 64x64, takenas a subset of ImageNet . ImageNet-Subset is a subsetof the ImageNet (ILSVRC 2012) dataset containing100 classes with a total of 130k training images and 5k testimages and image size of 224x224. We equally split all",
  ": end for": "these datasets in 5 and 10 tasks. This is different from thebig-start settings with half of the dataset in first task, com-monly used in EFCIL benchmarks . We alsouse two fine-grained datasets for our experiments. CUB-200 contains 200 classes of birds with 224x224 imagesize, 5994 images for training and 5794 images for testing.We use the 5-split and 10-split settings for CUB-200. Stan-ford Cars consists of 196 car models with 224x224 im-ages, 8144 for training and 8041 for testing and we split itinto 7 and 14 tasks. Training Details. We use the PyCIL framework as abasis for all our experiments. The training is performed us-ing the ResNet18 model and the SGD optimizer. ForCIFAR-100, in the first task, we use a starting learning rateof 0.1, momentum of 0.9, batch size of 128 and weight de-cay of 5e-4 for 200 epochs, the learning rate is reduced by afactor of 10 after 60, 120, and 160 epochs. In the subsequenttasks, we use an initial learning rate of 0.05 reduced by afactor of 10 after 45 and 90 epochs and train for 100 epochs.Following , we set the regularization strength to 10 andthe temperature to 2. The network is trained from scratch onCIFAR-100, TinyImageNet and ImageNet-Subset. For theexperiments on fine-grained datasets, we use the ImageNetpretrained weights following standard practice . ForADC, we use a value of 25, iterations i = 3 and num-ber of closest samples m = 100 for all the datasets. Sim-ilar to most existing methods, we store all the class proto-types. Complete details about the training setting for all the",
  ". Evaluation of EFCIL methods on small-start settings. Best results in bold and second best results are underlined": "datasets are given in the supplementary materials.Compared Methods. Since none of the EFCIL methodsare designed to start from a small first task, we implementthose methods in our small-start settings.This includesLwF , PASS , SSRE , FeTRIL and Fe-CAM . Naturally, we also include a comparison to theexisting drift-estimation method SDC and the baselinemodel with NCM classifier. For SDC and NCM results re-ported in Tab. 1 and Tab. 2, we train the models using LwFand perform NCM classification in the feature space. ForFeTrIL and FeCAM, the feature extractor is frozen afterthe first task, while for the other methods, it is continu-ally learned. Note that here we adapt SDC with distillationon the logits, which is different from where they per-formed distillation on the features.Evaluation. We report the average accuracy after the lasttask denoted by Alast and the average incremental accuracywhich is the average of the accuracy after all tasks (includ-ing the first one) denoted by Ainc. Ainc better reflects theperformance of the methods across all the tasks.",
  ". Quantitative Evaluation": "We observe that methods proposed for the big-start settingsof EFCIL are not effective in small-start settings and per-form poorly. A simple baseline trained with LwF and usingNCM classifier is performing better than most of the exist-ing approaches - SSRE, PASS, FeTrIL and FeCAM in sev-eral settings. While SDC improves over NCM, the proposedmethod ADC outperforms all existing methods in both lasttask accuracy and average incremental accuracy across allsettings in Tab. 1 and Tab. 2. ADC outperforms the second-best method SDC by 4.2% on 5-task and by 5.12% on 10-task settings of CIFAR-100 on last-task accuracy. For Tiny-ImageNet, ADC improves over the second-best method by0.95% on 5-task and by 5.17% on 10-task settings.OnImageNet-Subset, ADC is better by 2.58% on 5-task andby 1.72% on 10-task settings after the last task.",
  ". Memory Size vs accuracy comparison of NME and ADCon CIFAR-100 and TinyImageNet (T=10) settings": "We also evaluate the EFCIL methods on the challengingfine-grained datasets of CUB-200 and Stanford Cars. Weobserve in Tab. 2 that LwF is a strong baseline here, par-ticularly in the 5-task and 7-task settings and methods likeNCM and SDC are not much better than LwF. While PASSperforms poorly on both datasets, FeTrIL and FeCAM per-forms better with FeCAM outperforming the other methodson the 10-task setting of CUB-200 and 14-task setting ofStanford Cars. ADC outperforms the runner-up methods by5.78% on 5-task setting and by 6.19% on 10-task settingsof CUB-200. On Stanford Cars dataset, ADC is better by9.68% on 7-task setting and 7.57 % on 14-task setting. Weanalyze how the accuracy after each task varies for all themethods in and observe that ADC consistently out-performs the other methods across all tasks. Comparison to NME. We compare the last-task accuracyof ADC with exemplar-based NME where the exemplars areused to estimate the old class prototype positions in the newfeature space. We show in that ADC outperformsNME using 20 exemplars per class for CIFAR-100 (totalmemory size of 2000 samples) and using 50 exemplars perclass (total memory size of 10k samples) for TinyImageNet.",
  ". Computational overhead of ADC": "Using ADC requires some additional computation to bemade in-between each training session.In this section,we provide an estimation of the additional computation re-quired by our method and compare it to the training timeof a single task. At the end of each task, our method re-quires estimating the drift of each stored prototype (1 perold class) and for each of these, compute several adversar-ial samples starting from available current task samples. Asa consequence, the training time of our method scales lin-early with the number of classes. For each class, we com-pute 100 adversarial samples in a single batch and perform3 training iterations. In order to perform one iteration, weneed to compute the gradient of the adversarial loss with re-spect to the input image, whose cost is equivalent to the oneof a normal training backward pass . So, if we denotethe number of classes by Nc, and the number of iterationsby Ni, we need to perform Nc Ni backward passes. Inthe case of CIFAR-100 and ImageNet-Subset divided in 10tasks each containing 10 classes, this means an overheadof, 9t=1 10 t 3 = 1350 backward passes. In contrast,one new task is trained for 100 epochs with a batch size of128 (39 batches per epochs with 10 tasks on CIFAR-100),which amounts to 3900 backward passes per task, and twotimes more for the first task (trained for 200 epochs). Intotal, our method increases the computational cost by 3.1%on this setting. For the 5-task setting of CIFAR-100 andImageNet-Subset, it increases by 2.5%.",
  ". Ablation Studies": "In Sec. 4.3, we conduct an analysis on the impact of varioushyperparameters, including the number of iterations, , andthe number of closest samples used for ADC, on CIFAR-100 (T=10) setting. Based on the observations in Tab. 3a,we find that choosing even a very low number of iterations,specifically 3, yields favorable results when generating theperturbed images. Additionally, using = 25 achievesgood accuracy for both incremental and final task evalua-",
  ". Impact of hyperparameters on CIFAR100 (T=10) settingusing the proposed ADC method": "tions in Tab. 3b. Regarding the number of closest samplesto the target old prototype, Tab. 3c shows that the accuracyimproves marginally on considering more than 100 sam-ples. So, we take the 100 closest samples for our experi-ments which is computationally cheaper and yet achievesvery good accuracy.Interestingly, even taking only theclosest 25 samples achieves 2.62% better accuracy than therunner-up method SDC.Drift estimation quality:We validate through Tab. 1 and Tab. 2 that the designed ADC method is giving bet-ter accuracy results than the previous SDC method for alldatasets. As an additional verification, we check that thismethod was indeed better than SDC at estimating the oldprototypes drift. To do so, we use both SDC and ADC onthe same trained checkpoints on CIFAR-100 5-task settingsand compare the estimated drift to the true drift computedusing old data. We report the results in , where weshow the distribution of the estimated drift qualities. Onedrift per class is estimated and we compute the cosine sim-ilarity of estimated drift to the true drift. We see that for all",
  ". Accuracy after each incremental task for CIFAR-100, TinyImageNet and CUB-200 datasets on 10 task settings. ADC improvesover the compared methods starting from the initial to the last task": ". Comparison between the two drift estimation methodsSDC , and the proposed ADC, on CIFAR-100 (5 tasks). Wecompute the drift for each class with the two methods and reportthe distribution of drift estimation quality, measured by computingthe cosine similarity between the estimated drift vector and the truedrift (obtained using old data), for all previous class prototypes. training tasks, the drifts estimated with ADC are of betterquality than the ones estimated with SDC. We observe thatsome class drift estimations with SDC have negative cosinesimilarity with the true drift. However, we also see thatthe estimation quality decreases slightly for later trainingtasks. Indeed, as the backbone drifts more and more, it getsharder to estimate the actual drift. The fact that we see thisdecrease more prominently for ADC might be because thesimilarities obtained by SDC are already centered arounda low-value (0.15) after the second task, whereas the betterADC drift estimation is centered first around 0.9, to then de-crease and reach a minimum average of 0.7. This validatesthat ADC is able to track the movement of the prototypes inthe feature space.",
  "In this study, we explored a drift compensation methodfor exemplar-free continual learning. Drawing inspiration": "from adversarial attack techniques, we introduced a novelapproach called Adversarial Drift Compensation.Thismethod involves generating samples from the new taskdata in a manner that adversarial images result in embed-dings close to the old prototypes. This approach allows usto more accurately estimate the drift of old prototypes inclass-incremental learning without the need for any exem-plars. Furthermore, we conducted an analysis of contin-ual adversarial transferability, revealing an intriguing ob-servation: generated samples for the old feature space (pre-vious task) continue to behave similarly in the new fea-ture space (current task). This sheds light on why the Ad-versarial Drift Compensation method performs exception-ally well.Through a series of experiments, we demon-strated that ADC effectively tracks the drift of class distribu-tions in the embedding space, surpassing existing exemplar-free class-incremental learning methods on several standardbenchmarks. Importantly, these improvements are achievedwithout imposing extensive computational overhead or re-quiring a large memory footprint.Limitations. The ADC method, as currently designed, re-quires the access to the task boundaries during training inorder to trigger the computation of the old prototypes driftand to access a big enough quantity of current data. Themethod would for instance be more challenging to use andwould require changes in order to be applied in the onlinecontinual learning setting, or the continual few-shot learn-ing setting where only a small amount of current data isavailable. Future work can explore these directions. Acknowledgement.We acknowledge projects TED2021-132513B-I00 and PID2022-143257NB-I00, financed byMCIN/AEI/10.13039/501100011033 and FSE+ and theGeneralitat de Catalunya CERCA Program. This work waspartially funded by the European Union under the HorizonEurope Program (HORIZON-CL4-2022-HUMAN-02) un-der the project ELIAS: European Lighthouse of AI for Sus-tainability, GA no. 101120237. Bartomiej Twardowskiacknowledges the grant RYC2021-032765-I.",
  ". Training settings and hyperparameters": "Since the current approaches are not designed and opti-mized for the small-start settings used in our work, weadapt relevant methods and optimize them for these settingsand achieve comparable baselines to our approach. We listthe exact experimental settings to enable reproducibility.Augmentations:As implemented in PyCIL , forCIFAR-100, we use the same augmentation policy whichconsists of small random transformations like contrast orbrightness changes. Similarly, for the other datasets, we usethe default set of augmentations which include random cropand random horizontal flip. For a fair evaluation, we use thesame set augmentations for all the methods.LwF: In CIFAR-100, TinyImageNet and ImageNet-Subsetdatasets for the first task, similar to PyCIL , we use astarting learning rate of 0.1, momentum of 0.9, batch sizeof 128, weight decay of 5e-4 and trained for 200 epochs,with the learning rate reduced by a factor of 10 after 60,120, and 160 epochs, respectively. For subsequent tasks,we used an initial learning rate of 0.05 for CIFAR-100 andImageNet-Subset and 0.001 for TinyImageNet. The learn-ing rate is reduced by a factor of 10 after 45 and 90 epochsand trained for a total of 100 epochs. We set the the temper-ature to 2 and the regularization strength to 10 for CIFAR-100 and TinyImageNet and 5 for ImageNet-Subset. For thefine-grained datasets, we use a learning rate of 0.01 for thefirst task and a learning rate of 0.005 for subsequent tasks.The regularization strength is set to 20.For the NCM classifier, SDC and our proposed methodADC, we use the same training settings as LwF.SDC: For SDC , we set the hyperparameters =0.3 for CIFAR-100, TinyImagenet and for the fine-graineddatasets. For Imagenet-Subset, we set = 1.0.PASS: We follow the implementation of PASS fromPyCIL and set fkd = 10 and proto = 10.SSRE: We follow the implementation of SSRE fromPyCIL and set fkd = 10 and proto = 10.FeTrIL: For first task, we use a learning rate of 0.1 forCIFAR-100, TinyImageNet and ImageNet-Subset and fol-low the exact same settings as the original implementa-tion . For the fine-grained datasets, we use a learningrate of 0.01 for the first task.FeCAM: We use the same training setting as LwF for thefirst task training. FeCAM requires no training after thefirst task and stores the prototypes and covariance matricesfrom all the classes. Similar to the original implementation,we use the covariance shrinkage hyperparameters of (1,1)and the Tukeys normalization value of 0.5. . The t-SNE plot demonstrates that the adversarial sam-ples generated using our proposed method lie close to the targetold class prototype compared to the closest current task samples(in green) and can thus be reliably used for drift estimation.",
  ". Robustness to different class orders": "In CIL, the order of classes can influence the performanceand thus we shuffle the class orders and observe how ADCand the existing methods like LwF, NCM, SDC, FeTrILand FeCAM perform. While we used the seed 1993 fol-lowing previous works for the results re-ported in the main paper, here we use four different seeds0, 1, 2, 3 and report the mean and standard deviation us-ing these 5 seeds for both the last task accuracy Alast andthe average incremental accuracy Ainc in Tabs. 4 to 6. Theproposed method ADC outperforms SDC and NCM con-sistently across all settings on CIFAR-100, TinyImageNetand ImageNet-Subset. This demonstrates the robustness ofADC which improves over the existing methods irrespec-tive of the class order.",
  ". Perturbation guarantee": "We specifically select the closest samples to each old proto-type, one at a time (see Algorithm 1) to ensure we generateadversarial samples for all the old classes. On CIFAR100(T=10), we get an average of 59 samples out of 100, whichare successfully perturbed for all old classes after the lasttask. While performing 5 iterations (instead of 3) generatesan average of 69 successful perturbations for old classes,this does not lead to a significant accuracy change (Tab. 3a).",
  "Therefore, we have used 3 iterations in our implementation": "We analyze the position of the closest current task sam-ples and the generated adversarial samples with respect toa target old class prototype in the old feature space using at-SNE plot in . We observe that the adversarial sam-ples lie close to the prototype, while the original samplesare distant from the prototype. This validates the effective-ness of the adversarial attack in the old feature space andshows how new samples obtained using targeted adversarialattacks can be used to represent old classes. These adver-sarial samples behave as pseudo-exemplars and can now beused to estimate the drift of prototypes from the old to thenew feature space.",
  ". Prompt-based Methods": "Prompt-based methods aim to learn promptparameters that can be used with frozen pre-trained mod-els without updating the parameters of the model. A re-cent work, HiDe-Prompt also freezes the pre-trainedViT backbones and proposes an ensemble strategy for us-ing prompts. Different from them, our objective is to con-tinually learn new representations and update the backboneat every task. These methods have static features due tothe frozen backbone and avoid the feature drift problem weare tackling. We think it is unfair to compare the perfor-mance of frozen pre-trained models with our method (train-ing from scratch and updating the backbone). While freez-ing pre-trained models works well for mainstream datasets,it is crucial to update the backbone and learn new represen- tations for training domain-specific models for data that arenot commonly seen in pre-trained data, and thus it is nec-essary to develop drift compensation methods. Janson etal. show that while pre-trained models with a simpleNCM baseline work similar to L2P on Cifar100, they strug-gle on ImageNet-R with data of different styles like cartoon,graffiti, and origami.",
  ". Visualization of adversarial images": "We visualize the original and adversarially perturbed im-ages and the corresponding perturbations for CIFAR-100and TinyImageNet in and . We observe thatthe perturbations are perceptible in most of the adversarialimages generated from low-resolution images of CIFAR-100 and TinyImageNet while for ImageNet-Subset, CUB-200 and Stanford Cars having high-resolution images of224x224, the perturbations are not perceptible.",
  "Eden Belouadah and Adrian Popescu. Il2m: Class incremen-tal learning with dual memory. In International Conferenceon Computer Vision (ICCV), 2019. 1, 2": "Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-than, and Philip HS Torr. Riemannian walk for incrementallearning: Understanding forgetting and intransigence. In Eu-ropean Conference on Computer Vision (ECCV), 2018. 1 Matthias De Lange, Rahaf Aljundi, Marc Masana, SarahParisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, andTinne Tuytelaars. A continual learning survey: Defying for-getting in classification tasks. Transactions on Pattern Anal-ysis and Machine Intelligence (T-PAMI), 2021. 1, 2",
  "Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,Ziyan Wu, and Rama Chellappa. Learning without memoriz-ing. In Conference on Computer Vision and Pattern Recog-nition (CVPR), 2019. 1, 2": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, JunZhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial at-tacks with momentum. In Conference on Computer Visionand Pattern Recognition (CVPR), 2018. 4 Arthur Douillard, Matthieu Cord, Charles Ollion, ThomasRobert, and Eduardo Valle. Podnet: Pooled outputs distilla-tion for small-tasks incremental learning. In European Con-ference on Computer Vision (ECCV), 2020. 1, 2",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Conferenceon Computer Vision and Pattern Recognition (CVPR), 2016.5": "Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, andDahua Lin. Learning a unified classifier incrementally viarebalancing. In Conference on Computer Vision and PatternRecognition (CVPR), 2019. 1, 2 Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, LoganEngstrom, Brandon Tran, and Aleksander Madry. Adversar-ial examples are not bugs, they are features. In Advances inNeural Information Processing Systems (NeurIPS), 2019. 2,3 Nathan Inkawhich, Wei Wen, Hai Helen Li, and Yiran Chen.Feature space perturbations yield more transferable adversar-ial examples. In Conference on Computer Vision and PatternRecognition (CVPR), 2019. 4 Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and MohamedElhoseiny.A simple baseline that questions the use ofpretrained-models in continual learning. In NeurIPS 2022Workshop on Distribution Shifts: Connecting Methods andApplications, 2022. 2, 3",
  ". Visualization of image, perturbation and the corresponding adversarial image for some samples from TinyImageNet": "Yuyang Liu, Yang Cong, Dipam Goswami, Xialei Liu, andJoost van de Weijer. Augmented box replay: Overcomingforeground shift for incremental object detection. In Inter-national Conference on Computer Vision (ICCV), 2023. 1 Chunwei Ma, Zhanghexuan Ji, Ziyun Huang, Yan Shen,Mingchen Gao, and Jinhui Xu. Progressive voronoi diagramsubdivision enables accurate data-free class-incrementallearning. In International Conference on Learning Repre-sentations (ICLR), 2023. 2 Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,Dimitris Tsipras, and Adrian Vladu. Towards deep learningmodels resistant to adversarial attacks. In International Con-ference on Learning Representations (ICML), 2018. 2, 4 Tamasha Malepathirana, Damith Senanayake, and SamanHalgamuge. Napa-vq: Neighborhood-aware prototype aug-mentation with vector quantization for continual learning. InInternational Conference on Computer Vision (ICCV), 2023.2 Marc Masana, Xialei Liu, Bartlomiej Twardowski, MikelMenta, Andrew D Bagdanov, and Joost van de Weijer. Class-incremental learning: survey and performance evaluation.Transactions on Pattern Analysis and Machine Intelligence(T-PAMI), 2022. 1, 2, 5",
  "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.Transferability in machine learning: from phenomena toblack-box attacks using adversarial samples. arXiv preprintarXiv:1605.07277, 2016. 4": "Gregoire Petit, Adrian Popescu, Hugo Schindler, David Pi-card, and Bertrand Delezoide. Fetril: Feature translation forexemplar-free class-incremental learning. In Winter Confer-ence on Applications of Computer Vision (WACV), 2023. 2,5, 6, 7, 1 Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, GeorgSperl, and Christoph H Lampert. icarl: Incremental classi-fier and representation learning. In Conference on ComputerVision and Pattern Recognition (CVPR), 2017. 2, 1",
  "Aditya Khosla, Michael Bernstein, et al.Imagenet largescale visual recognition challenge. International journal ofcomputer vision, 2015. 5": "Dawid Rymarczyk, Joost van de Weijer, Bartosz Zielinski,and Bartlomiej Twardowski. Icicle: Interpretable class in-cremental continual learning. In International Conferenceon Computer Vision (ICCV), 2023. 5 Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi,Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis,Gavin Taylor, and Tom Goldstein. Adversarial training forfree!Advances in Neural Information Processing Systems(NeurIPS), 2019. 7 Wuxuan Shi and Mang Ye.Prototype reminiscence andaugmented asymmetric knowledge aggregation for non-exemplar class-incremental learning. In International Con-ference on Computer Vision (ICCV), 2023. 2 Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott San-ner, Hyunwoo Kim, and Jongseong Jang.Online class-incremental continual learning with adversarial shapleyvalue. In AAAI Conference on Artificial Intelligence, 2021.3 James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, PaolaCascante-Bonilla, Donghyun Kim, Assaf Arbelle, RameswarPanda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Contin-ual decomposed attention-based prompting for rehearsal-freecontinual learning. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2023. 2 Albin Soutif-Cormerais, Marc Masana, Joost Van de Weijer,and Bartlmiej Twardowski. On the importance of cross-task features for class-incremental learning.InternationalConference on Machine Learning (ICML) Workshops, 2021.2 Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, JoanBruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-triguing properties of neural networks. In International Con-ference on Learning Representations (ICLR), 2014. 2, 3",
  "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. Acomprehensive survey of continual learning: Theory, methodand application. arXiv preprint arXiv:2302.00487, 2023. 1": "Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin-cent Perot, Jennifer Dy, et al. Dualprompt: Complementaryprompting for rehearsal-free continual learning. In EuropeanConference on Computer Vision (ECCV), 2022. 2 Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jen-nifer Dy, and Tomas Pfister. Learning to prompt for contin-ual learning. In Conference on Computer Vision and PatternRecognition (CVPR), 2022. 2 Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, ZhizhongLi, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz.Dreaming to distill: Data-free knowledge transfer via deep-inversion. In Conference on Computer Vision and PatternRecognition (CVPR), 2020. 2, 3 Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van deWeijer. Semantic drift compensation for class-incrementallearning.In Conference on Computer Vision and PatternRecognition (CVPR), 2020. 1, 2, 3, 5, 6, 7, 8",
  "Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Deep class-incremental learn-ing: A survey. arXiv preprint arXiv:2302.03648, 2023. 1,2": "Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu.Prototype augmentation and self-supervision forincremental learning. In Conference on Computer Vision andPattern Recognition (CVPR), 2021. 2, 5, 6, 7, 1 Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, and Zheng-Jun Zha. Self-sustaining representation expansion for non-exemplar class-incremental learning. In Conference on Com-puter Vision and Pattern Recognition (CVPR), 2022. 2, 6, 1 Kai Zhu, Kecheng Zheng, Ruili Feng, Deli Zhao, Yang Cao,and Zheng-Jun Zha. Self-organizing pathway expansion fornon-exemplar class-incremental learning.In InternationalConference on Computer Vision (ICCV), 2023. 2"
}