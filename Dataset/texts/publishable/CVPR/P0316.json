{
  "Abstract": "In the realm of practical fine-grained visual classifica-tion applications rooted in deep learning, a common sce-nario involves training a model using a pre-existing dataset.Subsequently, a new dataset becomes available, promptingthe desire to make a pivotal decision for achieving enhancedand leveraged inference performance on both sides: Shouldone opt to train datasets from scratch or fine-tune themodel trained on the initial dataset using the newly releaseddataset? The existing literature reveals a lack of methods tosystematically determine the optimal training strategy, ne-cessitating explainability. To this end, we present an auto-matic best-suit training solution searching framework, theDual-Carriageway Framework (DCF), to fill this gap. DCFbenefits from the design of a dual-direction search (startingfrom the pre-existing or the newly released dataset) wherefive different training settings are enforced.In addition,DCF is not only capable of figuring out the optimal train-ing strategy with the capability of avoiding overfitting butalso yields built-in quantitative and visual explanations de-rived from the actual input and weights of the trained model.We validated DCFs effectiveness through experiments withthree convolutional neural networks (ResNet18, ResNet34and Inception-v3) on two temporally continued commercialproduct datasets. Results showed fine-tuning pathways out-performed training-from-scratch ones by up to 2.13% and1.23% on the pre-existing and new datasets, respectively, interms of mean accuracy. Furthermore, DCF identified re-flection padding as the superior padding method, enhanc-ing testing accuracy by 3.72% on average.This frame-work stands out for its potential to guide the developmentof robust and explainable AI solutions in fine-grained visualclassification tasks.",
  "M": ". A robust and explainable learning framework should berevealed by not only the leveraged FGVC performance over twotemporally continued datasets with the existence of subtle patterndifferences, imbalanced data samples and high sparsity within theregion of interest but also the associated quantitative (via the ac-tual model input, i.e. frequency distribution of the padded image)and visual explanations (through the learned model weights, layer-wise attention) in a putting-through manner. FGVC tends to be more challenging than conventionalimage classification due to the high degree of similar ap-pearance among subordinate categories . Besides, thecoarse granularity of the captured image resulted from il-lumination variations (e.g. low-light or over-exposed",
  "arXiv:2405.05853v1 [cs.CV] 9 May 2024": "environment) and camera motion associated with un-stable hand stability (e.g.blurriness and distortion) may deteriorate FGVC performance.In a real-world scenario depicted in , suppose thatA and B are the two available datasets; this paper aims tofind an optimal solution to leverage the prediction perfor-mance of a model M on their testing sets, i.e.A and B.Such an optimisation process can be expressed by:",
  "Acc(M,( A)), Acc(M,( B)),(1)": "where the optimal hyperparameters were learned bymodel M on the data . The above optimisation problemis usually resolved by training two datasets from scratch ina row or conducting transfer learning .Fundamentally, the choice of training pathway highly re-lies on the data scale and pattern disparity between datasetsA and B. However, those characteristics cannot be preciselyquantified as the existence of challenging factors mentionedbefore the antecedent paragraph and various types of un-certainties (e.g. dataset noise and model randomness) raised during the model training phase. The former wouldbe less preferred if those two datasets had very different dis-tributions. In contrast, the latter one suffers from the is-sue of catastrophic forgetting where the model for-gets important features from dataset A when fine-tuning ondataset B. Besides, though transfer learning is a more com-mon choice due to its much lower computational cost andpossibly better generalisation capability contributed by e.g.the large-scale ImageNet dataset , it may distort the pre-trained features .The challenges mentioned above in selecting the appro-priate training pathway for levering the prediction perfor-mance on A and B that are adjacent in the timeline as de-picted in , this work addresses the problem from adifferent yet explainable perspective. Generally, we presentan automatic framework for selecting the best model train-ing scheme, named Dual-Carriageway Framework (DCF),for an FGVC task given two chronologically continueddatasets, and our contributions are summarised as follows:1) We propose an automatic best-suit training solutionsearching framework (Sec. 3) through five training settings(Sec. 3.3) configured in a dual-direction manner for robustfine-grained visual classification where the prediction per-formance on test sets of two temporally continued datasetsare maximised.2) We prove the feasibility and efficacy of the proposedframework (Sec. 4.3) through not only the prediction accu-racy and degree of confidence yielded by the three trainedCNN models but also quantitative and visual explanationsin a putting-through manner from the actual model inputdata and layer-wise attention.3) We show that determining the potentially most ap-propriate padding scheme (Sec. 3.2) is a reasonable start-",
  "B": ". Sample images of the bottom side of commercial prod-ucts produced by two manufacturers, F1 and F2, in pre-existing(A) and newly released (B) datasets. Target regions (dotted partsto be segmented by U-Net in Sec. 2.2) with ever-evolvinganti-counterfeit code patterns embody spatial variations led byvarying illumination, camera bias , and noisy textures (e.g. dust and stains), affecting the overall FGVC accuracy. Datasets A and B contain 5870 and 3112 RGB im-ages, each of which comes with different resolutions, asthe data-capturing process was performed by varying typesof cameras in uncontrolled scenes. Slightly different fromthe commonly adopted data division approach, we use thedata of the tail time block in both datasets as the test set,i.e. the test set of A ( A) contains 1716 images in which| AF1| = 1389 and | AF2| = 327, and the test set of B ( B)includes 822 images where | BF1| = 220 and | BF2| = 602.",
  ". Semantic Segmentation Models": "A substantial body of literature exists on the employmentof deep neural networks for image semantic segmentation.Given that semantic segmentation is not the primary focusof this work, we briefly review key network architectures inthe order of their introduction and describe our choice.Fully Convolutional Network (FCN) , as an earlyseminal contribution to semantic segmentation, is a simpleyet efficient architecture that preserves spatial informationthrough end-to-end pixel-wise predictions due to the sub-stitution of fully connected layers using the convolutionalones. As the evolutionary successor to FCN, U-Net demonstrates a sophisticated capacity for capturing coarseand fine-grained features, achieving a favourable balancebetween temporal efficiency and richer feature representa-tion. Its distinctive U-shaped architectural configuration in-tegrates contracting and expansive pathways , cul-minating in enhanced accuracy in spatial localisation.DeepLab utilises atrous convolutions and spatialpyramid pooling to capture multi-scale contextual infor-mation, thus enhancing the models ability to consider abroader context and overcome the restricted receptive fieldlimitation associated with U-Net.High-Resolution Net-work (HRNet) mitigates DeepLabs limitations by de-vising high-resolution feature fusion, enhancing accuracy infine details without compromising computational efficiency.Such a fusion addresses challenges related to longer train-ing times and limited generalisation observed in DeepLab.As the region of interest in this work is the anti-counterfeit code (with significant difference relative tobackground) in each of the given raw RGB images (depictedin the dotted part of ), a U-Net , which is pre-trained on 1479 images with bounding boxes that annotatedby the Amazon Mechanical Turk (MTurk) crowdsourc-ing platform and refined by the Maximally Stable GlobalRegion , is employed in this work for semantic segmen-tation (i.e. the input of the padding schemes partially intro-duced in Sec. 2.3 and systematically formulated in Sec. 3.2),as exampled in the leftmost side of , to facilitate thenature of an FGVC task.",
  ". Padding Schemes and Classification Models": "There is a limited amount of literature that proposes or sys-tematically compares padding schemes in FGVC, ex-cept for the most commonly adopted zero padding inthe past decade. Noteworthy, while many works focus onhow to make neural networks deeper or wider, the paddingscheme, which is usually neglected, is also closely related tothe FGVC performance . In CNNs , zero paddingmaintains the spatial size of the output feature maps by adding layers of zeros around the input image before per-forming the convolution operation. However, it may intro-duce artefacts into the data that the model learns, leading topossible overfitting or reduced model generalisability .The reflection padding addresses this by mirroring theimage at the borders, maintaining the continuity of imagefeatures and reducing the edge artefacts . However,it may introduce redundancy and may not be suitable forall types of images. Similarly, padding schemes based onpartial convolution and repetition are robust toboundary artefacts .Onthecontrary,thelearning-based,interpolation-based,anddistribution-basedpadding schemes focus on leveraging the connectivity ofimage borders for more effective padding to reflect thenatural extension of the image content and suppress the riskof distorting the convolution. In addition, a padding schemedynamically adjusted based on the contextual informationaround the border pixels is devised explicitly for semanticsegmentation .Furthermore, padding with a non-zero constant valueis also a viable option, albeit infrequently implemented inpractical applications . Exploring whether segmentedrectangular images and padded parts are complementary ormutually exclusive regarding model perceptibility would beinteresting. To this end, considering that the segmented im-ages are relatively sparse, and their width is much greaterthan their height, we systematically formulate six paddingschemes, including zero padding and three schemes ofpadding with a non-zero constant in Eq. (2) as well as thereflection padding Eq. (4) in Sec. 3.2.The padded images with the resolution of 1024-by-1024pixels, as the actual input to CNNs, are then readily fedforward for convolution operations. To perform the binaryclassification task, we adopt three CNNs for binary classifi-cation, i.e. two residual networks (ResNet18 and ResNet34) and one Inception network (Inception-v3) due totheir advantages of overcoming vanishing gradients and capturing multi-scale features . To take advantageof large-scale model pretraining, all three models initialisethe training process based on the weights derived from Im-ageNet . Detailed training pathways are elaborated inSec. 3.3, and setups are configured in Sec. 4.1.",
  "selected training pathway": ". Workflow of the proposed DCF for robust and explainable fine-grained visual classification. This figure is separated into twoconsecutive components and a shared part by colours corresponding to 1) the Padding Scheme Adaptor (Sec. 3.2), 2) the Training PathwaySelector (Sec. 3.3), and 3) testing sets with pattern variations derived from two chronologically continued datasets (Sec. 2.1). Scheme Adaptor (PSA) (detailed in the upper left side of) and a Training Pathway Selector (TPS) (depicted inthe right-hand side of ). Concretely, PSA is proposedto figure out the appropriate padding scheme to be adoptedin the subsequent TPS where the FGVC accuracies on bothtemporally continued datasets (visualised in the lower leftside of ) are maximised and leveraged. In PSA, we first train a ResNet34 model (one of thethree visual classification models adopted in this work andmentioned in the tail part of Sec. 2.3) on the earlier trainingset A (splitter from its full dataset, A detailed in Sec. 2.1)with the most commonly employed zero padding (revis-ited in the first half of Sec. 2.3).Such a trained modelis tested on each of the six padding schemes (visualisedin ) that are systematically formulated in Sec. 3.2,using Eqs. (2) and (4) in general and Eqs. (3) and (5) inspecific. Given that subtle changes in terms of the anti-counterfeit code pattern existed in datasets A and B, wecan retain the most suitable padding scheme in our latterTPS phase by looking for the maximised inference perfor-mance on both testing sets. In TPS, for each of three vi-sual classifiers (ResNet18, ResNet34 and Inception-v3 adopted, we perform model training using five settings that are introduced in Sec. 3.3 and summarised in Eq. (6) in adual-carriageway manner, i.e. train on A only, fine-tune themodel learned from A using B, train on both A and B, trainon B only, and fine-tune the model learned from B using A.",
  ". Padding Scheme Adaptor": "As the output of a pre-trained semantic segmentation model mentioned in Sec. 2.2, each cropped image is Icin the shape of a rectangle whose width is much largerthan its height.Given the characteristics of a range ofpadding schemes that have been systematically introducedin Sec. 2.3, we determine that a total of six paddingschemes (in ), i.e. three constant value-based ones(zero, white and grey), two mean value-basedones (RGB-mean and LAB-mean), and one mirroring-based method (reflection) (a variant of and ),are evenly performed in the vertical direction merely (ex-cept the reflection padding) with three practical ad-vantages: a) preserving different aspect ratios; b) avoidingunnecessary distortions and c) reducing the computationalcost.Concretely, given a cropped image Ic RHW C",
  "(6)": "in which M A+ B denotes the model trained on A and B to-gether, whereas M A B represents the model trained on Afirst and then fine-tuned using B, and vice versa. In addi-tion, the alias of each training setting is used in the bottomright side of , e.g. s1 denotes setting 1.Specifically, we adopt three relatively light CNN modelsin this work to justify the proposed framework of seeking anoptimal training pathway for robust and explainable FGVC.Noteworthy, for training settings 2 and 4 of Eq. (6), we fine-tune each of the CNNs by layer freezing for simplicity andtime-efficiency . Detailed experimental setupsare presented in Sec. 4.1.",
  ". Experimental Setups and Evaluation Metrics": "We implemented DCF on an NVIDIA RTX 3090 Ti GPU.To ensure uniform input image size of deep neural classi-fiers and compensate for various aspect ratios of the seg-mented images, we scale them to a resolution of 1024 1024 pixels with six padding schemes (detailed in Sec. 3.2)applied. For training, each of the three image classifiers(Sec. 2.2), we adopt Adam optimiser and categoricalcross-entropy loss with a learning rate of 0.0001 and aweight decay of 0. Besides, we adopt random rotation fordata augmentation to improve model generalisation.The result was reported as mean accuracy standard de-viation by training the same deep image classifier five timesfor a fair comparison of FGVC performance. As a valu-able add-on to the proposed DCF, we provide quantitativeand visual explanations of the padded input (i.e. frequencydistribution) and weights (i.e. layer-wise attention) asso-ciated with the trained classifier via mean pixel value andGradient-weighted Class Activation Mapping (GradCAM). More experimental results are available in our supple-mentary materials.",
  "As briefly introduced (Sec. 2.3) and systematically designed(Sec. 3.2), we trained a ResNet34 model with the most com-": ". Prediction performance (in %) comparisons on the testingsets of datasets A (i.e.A) and B (i.e. B) using the baseline models(ResNet34) trained on the training set of dataset A (i.e. A) that wasprocessed by various padding schemes. Results yielded by the op-timal padding scheme are marked in bold and highlighted with(detailed in ).",
  "ResNet34( eB))": ". Prediction performance (in %) comparisons onthe testing sets of datasets A (in the y-axis on the left) andB (in the y-axis on the right) using the baseline models(ResNet34) trained on the training set of dataset A thatwas processed by six padding schemes summarised in Ta-ble 1. Results yielded by the optimal padding scheme reflection are detailed in . Quantitative andvisual explanations of the six padding schemes are shownin . Colour codes are consistent with Figs. 1 and 3. . Quantitative explanation of the relationship between input pixel value and confidence of correct predictions given by ResNet34trained with zero padding under six padding schemes. avg(; ) calculates the average for each of the two elements regarding thecorrectly predicted samples of F1 and F2 within the testing set, p (normalised to the range of ) represents the mean pixel value,Mc denotes the model confidence and Macc corresponds to the model inference accuracy measured in %. The best adapted avg(p)values and their associated Mc ones are marked in bold and highlighted with. Further visual explanation is delivered in .",
  "BF10.20; 0.958 73.68 0.70; 0.941 75.66 0.58; 0.951 74.66 0.91; 0.961 77.37 0.56; 0.959 78.22 0.70; 0.961 81.87F20.23; 0.9090.71; 0.9170.60; 0.9320.90; 0.9320.57; 0.9520.70; 0.940": "monly employed zero padding scheme on the training splitof a pre-existing dataset A as the backbone model, i.e. train-ing setting 1 defined in Eq. (6) and visualised in .From there, we evaluate the transferrable capability of sucha model by adopting six padding schemes (include zeropadding) on the testing split of the pre-existing and lastlyreleased dataset, i.e. A and B. Testing performance on these two tests is summarisedin and visualised in . It could be generallyobserved from that reflection padding con-tributed the most to the inference performance comparedto the rest, which is consistent with the findings presentedin . When testing above-mentioned backbone model onA, i.e. the column of Acc(M AResNet34( A)) in , thereflection padding outperforms zero, RGB-mean,LAB-mean, white and grey padding schemes by 2.53%, 1.51%, 0.99%, 1.69% and 1.48% in terms of mean accu-racy. Impressively, the reflection padding exceeds itscounterparts with a more significant margin, i.e. 8.06%,5.26%, 7.24%, 4.07% and 2.75% improvements, respec-tively, for mean accuracy. Furthermore, for selecting a single model to continue theconduction of the proposed DCF, we decompose the high-lighted performance yielded by reflection padding(available in ) in . One step further, in linewith the columns containing Macc values as summarisedin , the peak inference performance of the backbonemodel on two testing sets resulting from reflectionpadding is 96.60% and 81.87%, as outperformed zeropadding for 1.38% and 8.19%, RGB-mean padding for1.23% and 6.21%, LAB-mean padding for 1.71% and7.21%, white padding for 1.53% and 4.5%, and grey pa- . The relationship between the input image with six different padding schemes applied and the model (ResNet34) predic-tions in the binary FGVC task is revealed by the frequency distribution and model prediction confidence in the testing sets of Aand B (Sec. 2.1), i.e. A and B with consistent colour schemes adopted in Figs. 1 and 3. Best viewed in colour and zoomed mode.",
  "Acc(MInceptionv3( eB))": ". Prediction performance (in %) comparisons on the testing sets of datasets A (i.e. A) and B (i.e. B) using the three deepneural networks under five training settings with reflection padding scheme applied. This figure is a visual presentation of. Colour codes are consistent with Figs. 1 and 3. dding for 1.78% and 3.65%. Noteworthy, the above-peakperformances are qualified following the criteria definedin Eq. (1), i.e. the leveraged performance over two test-ing sets using a single model as opposed to ones that per-formed exceptionally well on one of the two testing sets andmarginally poorly on another. Besides, presented a quantitative explanation ofthe rationale that reflection padding outperforms itscounterpart schemes. Concretely, the averaged mean pixelvalue of images padded using reflection padding inboth subsets of F1 and F2 are constantly identical inthe testing sets of A and B, i.e. 0.60 and 0.78, respec-tively. Such an observation confirms the robustness of thereflection padding schemes in the context of changinganti-counterfeit code patterns. We also provide visual ex-planations in to justify this. Overall, the competitiveperformance is mainly due to the backbone models high de- gree of generalisation capability. This, in turn, reveals thepractical feasibility and advantages of dense input regardingthe more robust averaged mean pixel value, the correspond-ing more even frequency distribution, and the correctness ofmodel attention.",
  ". Optimal Training Pathway Selection": "In this experiment phase, we assess the feasibility of identi-fying the most suitable training pathway selection from twosequential datasets, as delineated in Sec. 3.3. Predictionperformances on two datasets are summarised in and visualised in .Noteworthy, the training pathways 1 and 2 are similar to4 and 5, where 1 and 3 are the starting point of the proposedDCF, each of which was trained on with zero paddingscheme. The third one is different among the five trainingpathways as it was trained on a combination of both training .Summary of prediction performanceyielded by three fine-grained visual classifiersunder five training pathways that defined inEq. (6). Results of the optimal training pathwayare marked in bold and highlighted with.",
  "0.40 93.11 0.48395.96 0.68 91.88 0.38488.69 0.90 90.90 0.69596.97 0.27 89.02 1.13": ".Detailed summary of peak prediction performance yielded by three fine-grained visual classifiers under five training pathways that defined in Eq. (6). Accuracyis measured in %. Peak performance under each training pathway is marked in boldand highlighted with. The visual explanations of ResNet34 models with the peakperformances are provided in as an example.",
  "(95.54, 98.17)96.86(90.91, 95.85)93.383(95.25, 96.94)96.09(86.36, 98.50)92.434(87.69, 92.0)89.87(84.55, 98.34)91.445(94.96, 99.08)97.02(84.09, 96.51)90.30": "sets with reflection padding scheme applied directly.Apart from ResNet34, which was trained as the back-bone model in training setting 1, we also employedResNet18 and Inception-v3 in such a setting. The detailedperformance is reported in , where the model withpeak prediction performances is highlighted and used forthe fine-tuning process required by training setting 2.Unlike training settings 2 and 4, we train a model usingboth training sets from scratch with the qualified paddingscheme (i.e. reflection padding). In the meantime, asthe peak performance presented in and partially ex-plained in , the prediction performance resulting fromtraining setting 2 is much better than that of setting 4, em-phasising that the order of training datasets matters.Overall, the outperformance of training setting 2 not onlyreveals that different padding schemes could compensatewith each other but also addresses the possible overfitting oftraining data and catastrophic forgetting of model .",
  ". Conclusion": "In this paper, we proposed an optimal training pathwayselection framework DCF for robust and explainable fine-grained visual classification in the context where two tem-poral continued datasets are provided, and a model has beenpre-trained on the dataset released earlier. The DCF con-sists of two adjacent steps, i.e.determining the optimalpadding scheme and selecting the optimal training pathway,each equipped with a quantitative and visual explanation ina putting-through fashion. The experimental results first in-dicate that the best-suited padding scheme determination isa feasible starting point for deploying the proposed DCF, asit reveals great potential in enhancing the eventual classifi- . Visual explanations supported by five ResNet34 modelstrained using various training pathways as summarised in .The adopted colour codes are consistent with . The secondtraining pathway (i.e. setting 2) has demonstrated its compatibilitywith blurry and clean images (first two rows) and illustrated itsdiscriminability in coping with finer patterns, e.g. printed dents(last row) in the testing sets of A and B (i.e. A and B). cation performance. The efficiency and efficacy of DCF arethen confirmed by its second step of figuring out the optimaltraining pathway, where the classification accuracy over twodatasets is maximally enhanced and leveraged. Experimen-tal results also inspire further investigations on image-wisequality separation of the optimal learning pathway se-lection for boosted image categorisation with mitigated databias and reduced computational cost.Acknowledgment.This work was supported by Procter& Gamble (P&G) United Kingdom Technical Centres Ltd.",
  "A. Optimal Padding Scheme Determination": ". Detailed prediction performance (in %) comparisons on the testing sets of datasets A and B, i.e. A and B regarding the labels F1and F2, using the ResNet34 (i.e. M) trained on the training set of dataset A with zero padding (i.e. A) that was processed by eachof the six padding schemes. Results yielded by the optimal padding scheme are marked in bold and highlighted with.",
  "B. Optimal Training Pathway Selection": ". Detailed prediction performance (in %) comparisons on the testing sets of datasets A and B, i.e. A and B, using the ResNet18 trained on the training set of dataset A (i.e.A) with reflection padding (as summarised in ) under five different trainingpathways in the proposed DCF. Results yielded by the optimal padding scheme are marked in bold and highlighted with.",
  "(94.74, 97.55)96.14(79.55, 92.86)86.20": ". Detailed prediction performance (in %) comparisons on the testing sets of datasets A and B using the ResNet34 and Inception-v3 trained on the training set of dataset A with reflection padding under five different training pathways. Results yielded by theoptimal padding scheme are marked in bold and highlighted with.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In Proc. CVPR, 2009. 2, 3": "Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, JiyangXie, Zhanyu Ma, Yi-Zhe Song, and Jun Guo. Fine-grainedvisual classification via progressive multi-granularity train-ing of jigsaw patches. In Proc. ECCV, 2020. 1 Ruoyi Du, Jiyang Xie, Zhanyu Ma, Dongliang Chang, Yi-Zhe Song, and Jun Guo. Progressive learning of category-consistent multi-granularity features for fine-grained visualclassification. IEEE Trans. Pattern Anal. Mach. Intell., 44(12):95219535, 2021. 1",
  "Yann Lecun, Leon Bottou, Yoshua Bengio, and PatrickHaffner. Gradient-based learning applied to document recog-nition. Proc. IEEE, 86(11):22782324, 1998. 3": "Chunlei Li, Xiao Li, Xueping Wang, Di Huang, ZhoufengLiu, and Liang Liao. Fg-agr: Fine-grained associative graphrepresentation for facial expression recognition in the wild.IEEE Trans. Circuits Syst. Video Technol., 34(2):882896,2023. 2 Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and JunZhou. A survey of convolutional neural networks: analysis,applications, and prospects. IEEE Trans. Neural NetworksLearn. Syst., 33(12):69997019, 2021. 1",
  "Guilin Liu, Kevin J. Shih, Ting-Chun Wang, Fitsum A. Reda,Karan Sapra, Zhiding Yu, Andrew Tao, and Bryan Catan-zaro. Partial convolution based padding. In arXiv preprintarXiv:1811.11718, 2018. 3": "Guilin Liu, Aysegul Dundar, Kevin J Shih, Ting-Chun Wang,Fitsum A Reda, Karan Sapra, Zhiding Yu, Xiaodong Yang,Andrew Tao, and Bryan Catanzaro. Partial convolution forpadding, inpainting, and image synthesis. IEEE Trans. Pat-tern Anal. Mach. Intell., 45(5):60966110, 2022. 3 Hai Liu, Cheng Zhang, Yongjian Deng, Bochen Xie, Tingt-ing Liu, Zhaoli Zhang, and You-Fu Li. Transifc: invariantcues-aware feature concentration learning for efficient fine-grained bird image classification. IEEE Trans. Multimedia,pages 114, 2023. 1",
  "Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fullyconvolutional networks for semantic segmentation. In Proc.CVPR, pages 34313440, 2015. 3": "Angela Lopez-del Rio, Maria Martin, Alexandre Perera-Lluna, and Rabie Saidi.Effect of sequence padding onthe performance of deep learning models in archaeal proteinfunctional prediction. Sci. Rep., 10(1):14634, 2020. 3 Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo,Liping Kang, Xiaoming Wei, Xiaolin Wei, and ShuqiangJiang. Large scale visual food recognition. IEEE Trans. Pat-tern Anal. Mach. Intell., 45(8):99329949, 2023. 1",
  "Ryosuke Sakai, Tomokazu Kaneko, and Soma Shiraishi.Framework for fine-grained recognition of retail productsfrom a single exemplar. In Proc. ICKST, 2023. 1": "Amir M Sarfi, Zahra Karimpour, Muawiz Chaudhary,Nasir M Khalid, Mirco Ravanelli, Sudhir Mudur, and Eu-gene Belilovsky. Simulated annealing in early layers leads tobetter generalization. In Proc. CVPR, pages 2020520214,2023. 5 Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.Grad-cam:Visual explanations from deep networks viagradient-based localization. In Proc. ICCV, pages 618626,2017. 5",
  "Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, HangSu, Bo Zhang, and Xiaolin Hu. Pruning from scratch. InProc. AAAI, 2020. 2": "Zhenghua Xu, Shijie Liu, Di Yuan, Lei Wang, Junyang Chen,Thomas Lukasiewicz, Zhigang Fu, and Rui Zhang. -net:Dual supervised medical image segmentation with multi-dimensional self-attention and diversely-connected multi-scale convolution. Neurocomputing, 500:177190, 2022. 3 Wenhan Yang, Ye Yuan, Wenqi Ren, Jiaying Liu, Walter J.Scheirer, Zhangyang Wang, Taiheng Zhang, et al. Advancingimage understanding in poor visibility environments: A col-lective benchmark study. IEEE Trans. Image Process., 29:57375752, 2020. 1"
}