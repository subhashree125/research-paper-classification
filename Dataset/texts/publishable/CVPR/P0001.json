{
  "Abstract": "The growing focus on leveraging computer vision for dietary oversight and nutri-tion tracking has spurred the creation of sophisticated 3D reconstruction methodsfor food. The lack of comprehensive, high-fidelity data, coupled with limitedcollaborative efforts between academic and industrial sectors, has significantlyhindered advancements in this domain. This study addresses these obstacles byintroducing the MetaFood Challenge, aimed at generating precise, volumetricallyaccurate 3D food models from 2D images, utilizing a checkerboard for size cal-ibration. The challenge was structured around 20 food items across three levelsof complexity: easy (200 images), medium (30 images), and hard (1 image). Atotal of 16 teams participated in the final assessment phase. The methodologiesdeveloped during this challenge have yielded highly encouraging outcomes in3D food reconstruction, showing great promise for refining portion estimation indietary evaluations and nutritional tracking. Further information on this workshopchallenge and the dataset is accessible via the provided URL.",
  "Introduction": "The convergence of computer vision technologies with culinary practices has pioneered innovativeapproaches to dietary monitoring and nutritional assessment. The MetaFood Workshop Challengerepresents a landmark initiative in this emerging field, responding to the pressing demand for preciseand scalable techniques for estimating food portions and monitoring nutritional consumption. Suchtechnologies are vital for fostering healthier eating behaviors and addressing health issues linked todiet. By concentrating on the development of accurate 3D models of food derived from various visualinputs, including multiple views and single perspectives, this challenge endeavors to bridge thedisparity between current methodologies and practical needs. It promotes the creation of uniquesolutions capable of managing the intricacies of food morphology, texture, and illumination, while alsomeeting the real-world demands of dietary evaluation. This initiative gathers experts from computervision, machine learning, and nutrition science to propel 3D food reconstruction technologies forward.These advancements have the potential to substantially enhance the precision and utility of foodportion estimation across diverse applications, from individual health tracking to extensive nutritionalinvestigations. Conventional methods for assessing diet, like 24-Hour Recall or Food Frequency Questionnaires(FFQs), are frequently reliant on manual data entry, which is prone to inaccuracies and can beburdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression-based methods for estimating food portions directly from images of eating occasions. By enhancing3D reconstruction for food, the aim is to provide more accurate and intuitive nutritional assessmenttools. This technology could revolutionize the sharing of culinary experiences and significantlyimpact nutrition science and public health.",
  "Related Work": "Estimating food portions is a crucial part of image-based dietary assessment, aiming to determine thevolume, energy content, or macronutrients directly from images of meals. Unlike the well-studiedtask of food recognition, estimating food portions is particularly challenging due to the lack of 3Dinformation and physical size references necessary for accurately judging the actual size of foodportions. Accurate portion size estimation requires understanding the volume and density of food,elements that are hard to deduce from a 2D image, underscoring the need for sophisticated techniquesto tackle this problem. Current methods for estimating food portions are grouped into four categories. Stereo-Based Approaches use multiple images to reconstruct the 3D structure of food. Some methodsestimate food volume using multi-view stereo reconstruction based on epipolar geometry, whileothers perform two-view dense reconstruction. Simultaneous Localization and Mapping (SLAM) hasalso been used for continuous, real-time food volume estimation. However, these methods are limitedby their need for multiple images, which is not always practical. Model-Based Approaches use predefined shapes and templates to estimate volume. For instance,certain templates are assigned to foods from a library and transformed based on physical references toestimate the size and location of the food. Template matching approaches estimate food volume froma single image, but they struggle with variations in food shapes that differ from predefined templates.Recent work has used 3D food meshes as templates to align camera and object poses for portion sizeestimation. Depth Camera-Based Approaches use depth cameras to create depth maps, capturing the distance fromthe camera to the food. These depth maps form a voxel representation used for volume estimation.The main drawback is the need for high-quality depth maps and the extra processing required forconsumer-grade depth sensors. Deep Learning Approaches utilize neural networks trained on large image datasets for portionestimation. Regression networks estimate the energy value of food from single images or from an\"Energy Distribution Map\" that maps input images to energy distributions. Some networks use bothimages and depth maps to estimate energy, mass, and macronutrient content. However, deep learningmethods require extensive data for training and are not always interpretable, with performancedegrading when test images significantly differ from training data. While these methods have advanced food portion estimation, they face limitations that hinder theirwidespread use and accuracy. Stereo-based methods are impractical for single images, model-basedapproaches struggle with diverse food shapes, depth camera methods need specialized hardware,and deep learning approaches lack interpretability and struggle with out-of-distribution samples. 3Dreconstruction offers a promising solution by providing comprehensive spatial information, adaptingto various shapes, potentially working with single images, offering visually interpretable results,and enabling a standardized approach to food portion estimation. These benefits motivated theorganization of the 3D Food Reconstruction challenge, aiming to overcome existing limitations and",
  "Dataset Description": "The dataset for the MetaFood Challenge features 20 carefully chosen food items from the MetaFood3Ddataset, each scanned in 3D and accompanied by video recordings. To ensure precise size accuracyin the reconstructed 3D models, each food item was captured alongside a checkerboard and patternmat, serving as physical scaling references. The challenge is divided into three levels of difficulty,determined by the quantity of 2D images provided for reconstruction:",
  "yYminxX x y22(2)": "This metric offers a comprehensive measure of similarity between the reconstructed 3D models andthe ground truth. The final ranking is determined by combining scores from both Phase-I (volumeaccuracy) and Phase-II (shape accuracy). Note that after the Phase-I evaluation, quality issues werefound with the data for object 12 (steak) and object 15 (chicken nugget), so these items were excludedfrom the final overall evaluation.",
  "Overview": "The teams method integrates computer vision and deep learning to accurately estimate food volumefrom RGBD images and masks. Keyframe selection ensures data quality, supported by perceptualhashing and blur detection. Camera pose estimation and object segmentation pave the way for neuralsurface reconstruction, creating detailed meshes for volume estimation. Refinement steps, includingisolated piece removal and scaling factor adjustments, enhance accuracy. This approach provides athorough solution for accurate food volume assessment, with potential uses in nutrition analysis.",
  "The Teams Proposal: VolETA": "The team starts by acquiring input data, specifically RGBD images and corresponding food objectmasks. The RGBD images, denoted as ID = {IDi}ni=1, where n is the total number of frames,provide depth information alongside RGB images. The food object masks, {M fi }ni=1, help identifyregions of interest within these images. Next, the team selects keyframes. From the set {IDi}ni=1, keyframes {IKj }kj=1 {IDi}ni=1 arechosen. A method is implemented to detect and remove duplicate and blurry images, ensuringhigh-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fouriertransform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-ing to detect similar images and retain overlapping ones. Duplicates and blurry images are excludedto maintain data integrity and accuracy. Using the selected keyframes {IKj }kj=1, the team estimates camera poses through a method calledPixSfM, which involves extracting features using SuperPoint, matching them with SuperGlue, andrefining them. The outputs are the camera poses {Cj}kj=1, crucial for understanding the scenesspatial layout. In parallel, the team uses a tool called SAM for reference object segmentation. SAM segmentsthe reference object with a user-provided prompt, producing a reference object mask M R for eachkeyframe. This mask helps track the reference object across all frames. The XMem++ methodextends the reference object mask M R to all frames, creating a comprehensive set of reference objectmasks {M Ri }ni=1. This ensures consistent reference object identification throughout the dataset. To create RGBA images, the team combines RGB images, reference object masks {M Ri }ni=1, andfood object masks {M Fi }ni=1. This step, denoted as {IRi }ni=1, integrates various data sources into aunified format for further processing.",
  "The team converts the RGBA images {IRi }ni=1 and camera poses {Cj}kj=1 into meaningful metadataand modeled data Dm. This transformation facilitates accurate scene reconstruction": "The modeled data Dm is input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes{Rf, Rr} for the reference and food objects, providing detailed 3D representations. The team uses the\"Remove Isolated Pieces\" technique to refine the meshes. Given that the scenes contain only one fooditem, the diameter threshold is set to 5% of the mesh size. This method deletes isolated connectedcomponents with diameters less than or equal to 5%, resulting in a cleaned mesh {RCf, RCr}. Thisstep ensures that only significant parts of the mesh are retained. The team manually identifies an initial scaling factor S using the reference mesh via MeshLab. Thisfactor is fine-tuned to Sf using depth information and food and reference masks, ensuring accuratescaling relative to real-world dimensions. Finally, the fine-tuned scaling factor Sf is applied to thecleaned food mesh RCf, producing the final scaled food mesh RF f. This step culminates in anaccurately scaled 3D representation of the food object, enabling precise volume estimation.",
  "Detecting the scaling factor": "Generally, 3D reconstruction methods produce unitless meshes by default. To address this, the teammanually determines the scaling factor by measuring the distance for each block of the referenceobject mesh. The average of all block lengths lavg is calculated, while the actual real-world length isconstant at lreal = 0.012 meters. The scaling factor S = lreal/lavg is applied to the clean food meshRCf, resulting in the final scaled food mesh RF f in meters. The team uses depth information along with food and reference object masks to validate the scalingfactors. The method for assessing food size involves using overhead RGB images for each scene.Initially, the pixel-per-unit (PPU) ratio (in meters) is determined using the reference object. Subse-quently, the food width (fw) and length (fl) are extracted using a food object mask. To determine thefood height (fh), a two-step process is followed. First, binary image segmentation is performed usingthe overhead depth and reference images, yielding a segmented depth image for the reference object.The average depth is then calculated using the segmented reference object depth (dr). Similarly,employing binary image segmentation with an overhead food object mask and depth image, theaverage depth for the segmented food depth image (df) is computed. The estimated food height fh isthe absolute difference between dr and df. To assess the accuracy of the scaling factor S, the foodbounding box volume (fw fl fh) PPU is computed. The team evaluates if the scaling factorS generates a food volume close to this potential volume, resulting in Sfine. lists the scalingfactors, PPU, 2D reference object dimensions, 3D food object dimensions, and potential volume. For one-shot 3D reconstruction, the team uses One-2-3-45 to reconstruct a 3D model from a singleRGBA view input after applying binary image segmentation to both food RGB and mask images.Isolated pieces are removed from the generated mesh, and the scaling factor S, which is closer to thepotential volume of the clean mesh, is reused.",
  "Implementation settings": "Experiments were conducted using two GPUs: GeForce GTX 1080 Ti/12G and RTX 3060/6G. TheHamming distance for near image similarity was set to 12. For Gaussian kernel radius, even numbersin the range [0...30] were used for detecting blurry images. The diameter for removing isolated pieceswas set to 5%. NeuS2 was run for 15,000 iterations with a mesh resolution of 512x512, a unit cube\"aabb scale\" of 1, \"scale\" of 0.15, and \"offset\" of [0.5, 0.5, 0.5] for each food scene.",
  "VolETA Results": "The team extensively validated their approach on the challenge dataset and compared their resultswith ground truth meshes using MAPE and Chamfer distance metrics. The teams approach wasapplied separately to each food scene. A one-shot food volume estimation approach was used ifthe number of keyframes k equaled 1; otherwise, a few-shot food volume estimation was applied.Notably, the keyframe selection process chose 34.8% of the total frames for the rest of the pipeline,showing the minimum frames with the highest information.",
  "LevelIdLabelSfPPURw Rl(fw fl fh)": "1Strawberry0.089552238810.01786320 360(238 257 2.353)2Cinnamon bun0.10434782610.02347236 274(363 419 2.353)3Pork rib0.10434782610.02381246 270(435 778 1.176)Easy4Corn0.088235294120.01897291 339(262 976 2.353)5French toast0.10344827590.02202266 292(530 581 2.53)6Sandwich0.12765957450.02426230 265(294 431 2.353)7Burger0.10434782610.02435208 264(378 400 2.353)8Cake0.12765957450.02143256 300(298 310 4.706)9Blueberry muffin0.087591240880.01801291 357(441 443 2.353)10Banana0.087591240880.01705315 377(446 857 1.176)Medium11Salmon0.10434782610.02390242 269(201 303 1.176)13Burrito0.10344827590.02372244 271(251 917 2.353)14Frankfurt sandwich0.10344827590.02115266 304(400 1022 2.353)16Everything bagel0.087591240880.01747306 368(458 134 1.176)Hard17Croissant0.12765957450.01751319 367(395 695 2.176)18Shrimp0.087591240880.02021249 318(186 95 0.987)19Waffle0.010344827590.01902294 338(465 537 0.8)20Pizza0.010344827590.01913292 336(442 651 1.176) After finding keyframes, PixSfM estimated the poses and point cloud. After generating scaled meshes,the team calculated volumes and Chamfer distance with and without transformation metrics. Mesheswere registered with ground truth meshes using ICP to obtain transformation metrics. presents quantitative comparisons of the teams volumes and Chamfer distance with andwithout estimated transformation metrics from ICP. For overall method performance, showsthe MAPE and Chamfer distance with and without transformation metrics. Additionally, qualitative results on one- and few-shot 3D reconstruction from the challenge datasetare shown. The model excels in texture details, artifact correction, missing data handling, and coloradjustment across different scene parts.",
  "Scale factor estimation": "The procedure for estimating the scale factor at the coordinate level is illustrated in . Theteam adheres to a method involving corner projection matching. Specifically, utilizing the COLMAPdense model, the team acquires the pose of each image along with dense point cloud data. For anygiven image imgk and its extrinsic parameters [R|t]k, the team initially performs threshold-basedcorner detection, setting the threshold at 240. This step allows them to obtain the pixel coordinatesof all detected corners. Subsequently, using the intrinsic parameters k and the extrinsic parameters[R|t]k, the point cloud is projected onto the image plane. Based on the pixel coordinates of thecorners, the team can identify the closest point coordinates P ki for each corner, where i represents theindex of the corner. Thus, they can calculate the distance between any two corners as follows:",
  "Dkij = (P ki P kj )2i = j(3)": "To determine the final computed length of each checkerboard square in image k, the team takes theminimum value of each row of the matrix Dk (excluding the diagonal) to form the vector dk. Themedian of this vector is then used. The final scale calculation formula is given by Equation 4, where0.012 represents the known length of each square (1.2 cm):",
  "D Reconstruction": "The 3D reconstruction process, depicted in , involves two different pipelines to accommodatevariations in input viewpoints. The first fifteen objects are processed using one pipeline, while thelast five single-view objects are processed using another. For the initial fifteen objects, the team uses COLMAP to estimate poses and segment the food usingthe provided segment masks. Advanced multi-view 3D reconstruction methods are then applied toreconstruct the segmented food. The team employs three different reconstruction methods: COLMAP,DiffusioNeRF, and NeRF2Mesh. They select the best reconstruction results from these methods andextract the mesh. The extracted mesh is scaled using the estimated scale factor, and optimizationtechniques are applied to obtain a refined mesh. For the last five single-view objects, the team experiments with several single-view reconstructionmethods, including Zero123, Zero123++, One2345, ZeroNVS, and DreamGaussian. They chooseZeroNVS to obtain a 3D food model consistent with the distribution of the input image. Theintrinsic camera parameters from the fifteenth object are used, and an optimization method basedon reprojection error refines the extrinsic parameters of the single camera. Due to limitations insingle-view reconstruction, depth information from the dataset and the checkerboard in the monocularimage are used to determine the size of the extracted mesh. Finally, optimization techniques areapplied to obtain a refined mesh.",
  "Alignment": "The team designs a multi-stage alignment method for evaluating reconstruction quality. illustrates the alignment process for Object 14. First, the central points of both the predicted andground truth models are calculated, and the predicted model is moved to align with the central pointof the ground truth model. Next, ICP registration is performed for further alignment, significantlyreducing the Chamfer distance. Finally, gradient descent is used for additional fine-tuning to obtainthe final transformation matrix.",
  "Methodology": "To achieve high-fidelity food mesh reconstruction, the team developed two procedural pipelines asdepicted in . For simple and medium complexity cases, they employed a structure-from-motion strategy to ascertain the pose of each image, followed by mesh reconstruction. Subsequently,a sequence of post-processing steps was implemented to recalibrate the scale and improve meshquality. For cases involving only a single image, the team utilized image generation techniques tofacilitate model generation.",
  "Single-View Reconstruction": "For 3D reconstruction from a single image, the team utilized advanced methods such as LGM, InstantMesh, and One-2-3-45 to generate an initial mesh. This initial mesh was then refined in conjunctionwith depth structure information. To adjust the scale, the team estimated the objects length using the checkerboard as a reference,assuming that the object and the checkerboard are on the same plane. They then projected the 3Dobject back onto the original 2D image to obtain a more precise scale for the object.",
  "Experimental Results": "Through a process of nonlinear optimization, the team sought to identify a transformation thatminimizes the Chamfer distance between their mesh and the ground truth mesh. This optimizationaimed to align the two meshes as closely as possible in three-dimensional space. Upon completionof this process, the average Chamfer dis- tance across the final reconstructions of the 20 objectsamounted to 0.0032175 meters. As shown in , Team FoodRiddle achieved the best scores forboth multi- view and single-view reconstructions, outperforming other teams in the competition.",
  "Conclusion": "This report examines and compiles the techniques and findings from the MetaFood Workshopchallenge on 3D Food Reconstruction. The challenge sought to enhance 3D reconstruction methodsby concentrating on food items, tackling the distinct difficulties presented by varied textures, reflectivesurfaces, and intricate geometries common in culinary subjects. The competition involved 20 diverse food items, captured under various conditions and with differingnumbers of input images, specifically designed to challenge participants in creating robust reconstruc-tion models. The evaluation was based on a two-phase process, assessing both portion size accuracythrough Mean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distancemetric. Of all participating teams, three reached the final submission stage, presenting a range of innovativesolutions. Team VolETA secured first place with the best overall performance in both Phase-I andPhase-II, followed by team ININ-VIAUN in second place. Additionally, the FoodRiddle teamexhibited superior performance in Phase-II, highlighting a competitive and high-caliber field ofentries for 3D mesh reconstruction. The challenge has successfully advanced the field of 3D foodreconstruction, demonstrating the potential for accurate volume estimation and shape reconstructionin nutritional analysis and food presentation applications. The novel methods developed by theparticipating teams establish a strong foundation for future research in this area, potentially leadingto more precise and user-friendly approaches for dietary assessment and monitoring."
}