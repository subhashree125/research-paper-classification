{
  "Abstract": "Lidar has become a cornerstone sensing modality for3D vision, especially for large outdoor scenarios and au-tonomous driving. Conventional lidar sensors are capableof providing centimeter-accurate distance information byemitting laser pulses into a scene and measuring the time-of-flight (ToF) of the reflection. However, the polarizationof the received light that depends on the surface orienta-tion and material properties is usually not considered. Assuch, the polarization modality has the potential to improvescene reconstruction beyond distance measurements. In thiswork, we introduce a novel long-range polarization wave-front lidar sensor (PolLidar) that modulates the polariza-tion of the emitted and received light. Departing from con-ventional lidar sensors, PolLidar allows access to the rawtime-resolved polarimetric wavefronts. We leverage polari-metric wavefronts to estimate normals, distance, and ma-terial properties in outdoor scenarios with a novel learnedreconstruction method. To train and evaluate the method,we introduce a simulated and real-world long-range datasetwith paired raw lidar data, ground truth distance, and nor-mal maps.We find that the proposed method improvesnormal and distance reconstruction by 53% mean angularerror and 41% mean absolute error compared to existingshape-from-polarization (SfP) and ToF methods. Code anddata are open-sourced here1.",
  ". Introduction": "Sensing and reconstructing large scenes is crucial forsafety-critical applications in autonomous driving , drones , remote sensing , scene un-derstanding and dataset generation for 3D vision. Scanning lidar sensors have been broadlyadopted as a cornerstone sensing modality that providesprecise distance information.These sensors operate bymeasuring the ToF of laser pulses emitted into and returned",
  "*These authors contributed equally to this work.1": "from the scene. The emitted light is typically polarized andthe polarization changes upon reflection depending on sur-face normals and material properties . Off-the-shelflidar sensors only detect intensity, as such, ignore the ad-ditional polarization information. In this paper, we revisitthe abandoned geometric and material information in thepolarization state for the reconstruction of large automotivescenes up to 100m range.Although the benefit of polarization has been investi-gated extensively in other fields , polarizationis largely unexplored in the context of lidar sensing in vi-sion and robotics. Specifically, lidar and polarization havebeen explored in meteorology , biology and maritime sciences by analyzing the depolariza-tion. Besides, a line of work investigates polarization cam-era images for shape estimation , stereodepth estimation , depth completion , and dehaz-ing . These methods have in commonthat they utilize passive sensors, making them ineffectiveat night time. Only a few existing works use activepolarimetric ToF systems for scene reconstruction. How-ever, these existing time-resolved polarization methods aredesigned for indoor scenes with object-level contents, pro-hibiting the measurement of large outdoor scenes.In this paper, we introduce a novel sensing modality thatcombines polarization analysis with lidar sensors for scenereconstruction, illustrated in . We devise a polariza-tion wavefront lidar sensor (PolLidar) that is capable of op-erating in outdoor settings. The proposed sensor modulatesthe polarization of the emitted and received light. In con-trast to polarization cameras, the PolLidar is not limited toa discrete number of polarization states but can measure po-larization continuously by finely controlling waveplates andlinear polarizers basically able to perform full ellipsometry. The sensor reads the raw wavefront signal directlyas a voltage from the Avalanche Photodiode (APD). We em-ploy this sensing technique to capture a polarization datasetconsisting of long-range automotive scenes to assess thebenefit of polarization. Along with the raw wavefronts, weprovide pairwise ground truth distance and normal informa-tion from a Velodyne VLS-128 reference sensor, see .",
  "DMD": ". PolLidar sensing and scene reconstruction. We design our PolLidar sensor with a unique capability: it modulates the polariza-tion of light during both the emission and reception stages. To this end, a HWP and QWP are used to emit light of a certain polarization,whereas a QWP and LP are used to determine the polarization of the received light. To capture the received signal, we employ an ADC atthe APD for precise raw wavefront measurement. This is unlike traditional Lidar systems that primarily focus on distance measurementsand do not provide both the polarization characteristics and the wavefront of the light. Subsequently, a novel lidar geometry reconstructionapproach predicting normals, distance and material properties is introduced in Sec. 4. To recover scene properties from polarization wavefrontmeasurements, we combine the proposed sensor with anovel reconstruction approach that operates on the raw po-larimetric wavefronts. The proposed reconstruction methoduses the polarized wavefronts to estimate surface normalsand accurate distance.The estimated normals can thenbe utilized for predicting material properties, including in-dex of refraction, diffuse and specular albedo, and surfaceroughness. For training, we extend the CARLA simula-tor with a realistic polarization model of light to gener-ate a synthetic long-range polarization dataset. We assess the method with experiments on both syn-thetic data and real-world data. We find that the proposedmethod improves distance estimation by 41% mean abso-lute error compared to conventional ToF methods and 53%mean angular error for normal estimation compared to SfPand point cloud baselines on automotive scenes.",
  "We propose a neural reconstruction approach for distanceand normals operating directly on raw wavefronts insteadof post-processed ToF peaks": "We introduce the first automotive polarization lidardataset, consisting of real-world data and simulation data.We validate our model with the proposed dataset for long-range distance estimation and dense normal reconstruc-tion. Compared to baseline methods, our model improvesdistance and normal reconstruction by 41% mean abso-lute error and 53% mean angular error, respectively.",
  ". Related Work": "Polarization Lidars. Polarization lidar sensors have beenexplored in diverse fields. Early studies, such as Schot-lands , leveraged these polarimetric measurements forcloud property analysis, while approaches as studythe bioaerosols in the atmosphere and in the scat-tering coefficient of oceans are measured using polariza-tion lidar .Recently, Baek et al. combine aprototypical polarization lidar with a temporal-polarimetricBRDF model to achieve accurate scene reconstruction. Jeonet al. propose a polarimetric indirect ToF imagingmethod that utilizes polarization to improve depth estima-tions through scattering media. However, the imaging tech-nique, i.e., the design of the optical path in , and theindirect ToF measurement principle in , fundamentallylimit these devices to indoor usage. In contrast, the pro-posed method is the first designed for scene reconstructionin large outdoor scenes up to 100m. Scene Reconstruction with Passive Polarization Sensors.Exploiting the relationship between the polarization of re-flected light and the surface normals, shape from polar-ization (SfP) methods have achieved scene reconstructionfrom polarization images captured by linear-polarizationcameras .Early SfP methods focus on esti-mating the surface normal of objects under assumptions ofeither pure specular reflection or pure diffuse reflec-tion . These methods usually assume an unpolarizedlight source and suffer from polarization ambiguity issues.Recent works leverage deep learning to solvethe ambiguity problem. By training on real-world datasets,the network can better distinguish the ambiguity and mit-igate the need for inputting unknown material propertiessuch as refractive index. Baek et al. perform joint opti- mization of appearance, normals, and refractive index. De-schaintre et al. propose a learning-based inverse learn-ing framework with the front-flash illumination. Dave etal. combine polarization with implicit neural represen-tations to collectively reconstruct the geometry and appear-ance from multiple images. In general, these reconstructionmethods focus on scenes with few objects that are placedto exhibit strong polarization cues with a high degree ofpolarization (DoP). In outdoor scenes, however, the DoPvaries significantly limiting the quality of the reconstruc-tion to high DoP regions. The proposed method allows toexploit the exploitation of polarization cues in both high andlow DoP regions.In , passive polarization sensors are combinedwith other imaging modalities. Kadambi et al. utilizenormals from polarization to enhance the details of depthfrom a Microsoft Kinect sensor. Yoshida et al. use po-larization to fill in missing regions in the depth maps. Fur-thermore, polarization cues are leveraged to augment low-quality depth maps from two-view stereo , recip-rocal image pairs , multi-view stereo , or li-dars . Recently, Huang et al. and Tian et al. propose stereo polarimetric methods, which utilize two po-larization images to solve the ambiguity in SfP. However, aspassive sensors are dependent on ambient light, these meth-ods struggle in low-light conditions. The proposed activesensing method allows for accurate reconstructions inde-pendently of ambient illumination.",
  ". Polarimetric Wavefront Lidar": "In environmental science, polarimetric lidars are employedfor gathering polarization data over extensive ranges, oftenspanning several kilometers but with a trade-off in spatialresolution. Contrarily, polarimetric lidars for scene recon-struction usually support high spatial resolution, yet theirrange is limited to a few meters. The proposed PolLidarsensor in uniquely bridges these application domains.It is designed to allow for a balanced performance optimalfor both long-range capabilities up to 223 m and high spa-tial resolution of 150 rows and 236 columns over a 23.95and 31.53 vertical and horizontal field-of-view, making itparticularly suitable for autonomous driving applications.Our sensor differs from the ToF systems described in. Specifically, we propose separate modules for emis-sion and reception instead of a shared optical setup. Thisseparation allows for a larger optical aperture in each mod-ule, enhancing optical sensitivity and extending the oper-ational range in outdoor scenarios. Instead of the galvo-mirror used in , a MEMS micro-mirror is used in theemitter for scene scanning. The receiver employs a dig-ital micro-mirror device (DMD) following to selec-tively deflect the returning light towards the photodiode.Using the DMD allows for apertures (0.55) comparable",
  "Normals": ". Additional Real-world Results. We show day- and night-time automotive scenes. PCA generates high-quality surfaces in areaswith high point density. However, for fine details or objects far away, where the point cloud is sparse, the reconstruction is of low quality.The proposed method, however, leverages the polarization cues in these areas and thus outperforms PCA in regions of low-point density.This can be seen in the zoom-ins shown on the right, e.g. bumper area of the car in the fourth row or car roofs in the fifth and sixth rows. Distance (m) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Mean angular error (o) PCAProposed Distance (m) Margin (o)",
  "Material ID": ". PolLidar forward model and simulator. Temporal polarimetric reflectance of the scene can be modeled as the sum of specularMs and diffuse Md reflection. Receiver and emitter of the PolLidar can be described with the Mueller matrices Pi and Ai that are func-tions of the rotation angles {1,2,3,4}iof HWP, QWPs and LP, respectively. We employ the resulting PolLidar forward model in a simulatorbased on CARLA that generates synthetic polarimetric raw wavefronts. To this end, we extract material properties and normals fromCARLA and feed them into the forward model. The resulting temporal wavefronts are subsequently downsampled in spatial dimension tomodel beam divergence and noise is added to simulate APD and ADC. urable selection of polarization states by finely controllingthe polarization elements at the expense of measurementtime. The acquisition of a frame, as described in Sec. 4.1,results in a capture time of 5 min. We refer to the Supple-mentary Material for an analysis on how future setups canachieve real-time capability. Although adding polarizationrequires additional complexity, we argue that the potentialbenefits extend beyond the scope of this paper, aiding recon-struction in scenarios with multi-path reflections or scatter-ing media .",
  ". Polarimetric Lidar Forward Model": "We model polarization with the Stokes-Mueller formalism,with light and reflectance described by a Stokes vectors R41 and a Mueller matrix M R44. . Re-cently, Baek et al. introduced a temporal-polarimetricreflectance model M(, i, o) describing how light po-larization and intensity change when impinging on a surfacewith given incident and outgoing direction of light (i ando), and with temporal delay () of diffuse reflection. Asshown in , the reflectance M can be modeled as a sumof specular and diffuse reflection (Ms and Md)",
  "Md (, i, o) = CnoFoT Dd()FiT Cin,(3)": "where h = cos1(h n), i = cos1(n i), o =cos1(n o) and n is the surface normal. D and G arefunctions to describe the surface, where m is the roughness.Cin and Cno are the coordinate-conversion Muellermatrices , and FiT , FoT are the Fresnel transmission Mueller matrices for incident and outgoing light, depend-ing on refractive index . Ds and Dd are the depolarizationMueller matrices for specular and diffuse reflections .Given its long-range working distance, we can assumethat the incident and outgoing direction of light in our sen-sor are identical, and we approximate the reflectance model(1) with a single viewing direction = i = o. Afterscaling M by the cosine shading term cos and attenuationsuch that H (, , ) =cos / d2M (, , ), the lidarforward model can be written as",
  ",(4)": "where t = t 2d/c, d is the distance between laser andscene, and c is the speed of light. slaser denotes the Stokesvector of the emitted laser light. The operator [...]0 denotestaking the first element of the resulting vector.We use rotating ellipsometry to infer all elements of theStokes vectors . As illustrated by , a HWP and aQWP are rotated to modulate the polarization of the emittedlight. Analogous on the receiving side, a QWP and a LP areused to measure light with a specific polarization incidenton the APD. Hence, the image formation of the PolLidarcan be modelled as",
  "Ii(t, ) = [AiH(t, )Pislaser(, 0)]0 ,(5)": "where Ai and Pi are the i-th Mueller matrices of the an-alyzing optics and the polarizing optics defined as Ai =L(4i )Q(3i ) and Pi = Q(2i )W(1i ), with {1,2,3,4}ias therotation angles of the emitter HWP and QWP and the re-ceiver QWP and LP, respectively. W, Q, and L are theMueller matrices of the HWP, QWP, and LP . The inte-gral is omitted as a result of using pulsed laser illumination.",
  "View Encoding": ". Neural polarization wavefront lidar reconstruction. We capture raw polarization wavefronts of the scene I. We apply apeak-based segmentation technique to obtain a sliced polarization wavefront I and distance priors d. Via ellipsometric reconstruction, weestimate a sliced Mueller matrix Hmeas. Finally, we concatenate all the polarization priors with viewing direction V as the input to a neuralnetwork predicting distance and normals for the scene. We supervised the network with a normal loss Lnormal and a distance loss Ldist.",
  ". Polarimetric Lidar Simulator": "In order to use the PolLidar in a learning-based frame-work, a sufficient amount of training data is required. How-ever, the finely controllable polarization elements come atthe cost of longer measurement times as the motors moverelatively slow. To acquire a large polarization wavefrontdataset, we integrate the lidar forward model from Eq. (5)into the CARLA simulator to generate vast amountsof synthetic training data. Specifically, we extend the fullwavefront lidar model for CARLA as introduced by .As presented in , we extract the material properties m,Ds and Dd using custom material cameras. However, ma-terials in CARLA do not have refractive indices assignedby default. We circumvent this problem by extending theray-tracer to return the material ID of each hit point. Basedon the material ID, we look-up the corresponding refractiveindex in a database . Additionally, we extend the ray-tracer to return normals n for each hit point.With material properties and normals in hand, we sim-ulate the scene using the polarimetric lidar forward model.To model the beam divergence of the laser beam, we down-sample neighboring rays to eventually render the temporallyresolved polarimetric raw wavefronts. Next, we model shotand read-out noise by applying Poisson and Gaussian noiseto the wavefronts, respectively. We tune the noise charac-teristics such that they closely resemble the real device. Ad-ditional details are provided in the Supplementary Material.",
  ". Preprocessing Wavefronts": "When capturing a frame, we perform rotating ellipsome-try by collecting raw wavefronts for 36 different rotationangles i subsequently denoted as I = {Ii}36i=1, whereIi RHW T with H=150, W=236 and T=1488. Thetemporal resolution T and the repeated measurement foreach angle i results in 53,568 samples for each ray in I. Totackle this large dimensional space, we first perform peak-based segmentation to obtain sliced wavefronts as shown in. Specifically, to reduce the temporal dimension, wefirst locate the peak within the wavefront. Then, we segmenta window of size 51 centered around the peak, resulting in asliced wavefront I = {Ii}36i=1, where Ii RHW 51. Wepreserve the temporal index of the peak tpeak as it containsthe distance information d R36HW 1.As the raw wavefront I implicitly encodes the polariza-tion optics from emitter and receiver, we apply ellipsometricreconstruction to recover the time-dependent Mueller ma-trix H. To this end, we use the temporal measurements Iicollected at various rotation angles of the polarizing opticsto invert the image formation model presented in Eq. (5).Following the approach of Baek et al. , we recover theMueller matrix Hmeas RHW 5116 by solving a least-squares optimization problem as follows",
  "(Estimation / GT)": ". Qualitative evaluation on synthetic data. Baek et al. is unable to reconstruct normals in areas with low DoP, e.g., walls ofbuildings facing the sensor. PCA applied in this setting are strongly dependent on point cloud density. Thus, distant poles and carsin the second row cannot be reconstructed accurately. The proposed approach leverages polarization cues to reconstruct normals in sparseregions and is robust against low DoP areas. We estimate accurate material properties for different surfaces and objects (right). where denotes concatenation along the feature dimensionand V is the viewing direction. We then predict normalsn and distance d with a neural network. The network is avariation of a TransUnet that combines the U-Net and trans-former architecture components. Specifically, we use 3 en-coder layers to encode the features. At the bottleneck, weuse 8 transformer layers. At last, we use 3 decoder layerswith skip-connection to predict normals and distance.To train the network, we supervise normals and distancepredictions with a cosine similarity loss for the surface nor-mals and a mean absolute loss for distance",
  "Ldist = |c dgt c d)|1,(9)": "where c is the confidence mask for the normals whereground truth normals are not available.We implement the proposed method in PyTorch.Wetrain the model for 200 epochs on a Nvidia A100 GPU. Weuse the Adam optimizer with a learning rate of 1e-4 and weset the batch size to 1. We crop images to 128128 patchesin each iteration for augmentation. We apply different laserpowers and biases during training to increase robustnessagainst saturation and low-intensity readings. More detailsare presented in the Supplementary Documentation.",
  "SfP-DoP 49.8235.0065.394.297.6012.63Baek et al. 31.038.3253.2127.1244.4461.03PCA 18.648.0233.6055.8960.6466.84Proposed8.714.3117.6565.4970.1978.15": ". Quantitative evaluation for normals on synthetic data.The SfP baseline is unable to reconstruct normals in real-worldas the underlying assumptions do not translate to real-world sce-narios. Baek et al. is designed for object-level ToF imagingand fails in low DoP regions. PCA achieves improved resultsbut with quality depending on point cloud density. The proposedmethod leverages both the neighborhood of points and the polar-ization cues; thus outperforming all the baselines. compare against three SfP baseline methods to evaluate thequality of the reconstructed normals. Specifically, we eval-uate against Baek et al. as a baseline designed for object-level scene reconstruction for a polarimetric ToF prototype.This approach fits the recovered Mueller matrix Hmeas tothe polarimetric lidar forward model by jointly estimatingmaterial properties and normals. Next, we compare againstthe classical SfP approach from , which recovers sur-face normals from the DoP by assuming a scene-wide con-stant refractive index and diffusive reflection. As reportedin Tab. 1, classical SfP approaches do not generalize well tooutside scenes. This can be attributed to real-world geom-etry exhibiting regions of high but also very low DoP. LowDoP regions occur when the surface normal and the viewingdirection of the lidar align, see Supplementary Dcoumenta-tion. Highlighted by the qualitative findings in , themethod from Baek et al. is unable to reconstruct nor-mals in low DoP regions, e.g., buildings of walls that facethe sensor, whereas for high DoP regions, as e.g., the sideof a vehicle, satisfying performance is achieved.Moreover, we compare against conventional lidar by av-eraging wavefronts from all polarization states and applying",
  "Scene": ". Acquisition of Ground Truth Data. We move PolLidar and reference lidar through a scene to capture dense reference lidar maps.We interpolate the lidar maps with the viewing direction of the PolLidar to extract GT distance dgt information. For GT normals ngt, wefirst mesh the lidar map and extract normals from the mesh by querying with the GT point cloud. MaskFor certain pixels/view directions, no information can be extracted from the raw wavefront, when e.g. no object washit, or the measured intensity falls below the noisefloor. We use the ToF map d extracted from the sensor with conventionalpeak-finding to exclude these points from training. To this end, we compare if the ToF map and ground truth distance arewithin a certain bound and define the mask of valid pixels / view directions as",
  ". Material Property Estimation": "With estimated surface normals in hand, we reconstruct thematerial properties, namely index of refraction , rough-ness m and the depolarization matrices Ds and Dd, of thepolarimetric lidar forward model. To this end, we followBaek et al. and estimate material properties by renderingthe Mueller matrix Hrender = Hsrender + Hdrender that best ex-plains the reconstructed Mueller matrix Hmeas. In the largescenes we tackle, we find that the DoP is mostly governedby diffuse reflection. We leverage this heuristic to disentan-gle the specular and diffusive Mueller matrices. To this end,we solve the following minimization problem",
  "s (Hmeas Hdrender) Hsrender1 ,(10)": "where d, s are scalar weights and cdop a mask focusingon regions with high diffusive DoP. The weights are chosensuch that in the first phase of the minimization, the diffusiveloss drives the estimation of the index of refraction whichlater helps to better disentangle material properties that oc-cur solely in the specular component of the Mueller matrix.Note that in our simulation, only the scalar amplitude, de-noted by |Ds| and |Dd|, of the depolarization matrices varyand are subsequently optimized for. validates that theproposed approach is able to successfully recover the mate-rial properties of different objects and surfaces. As we donot optimize the surface normals, this further validates the",
  ". Experimental Evaluation": "Next, we evaluate the proposed approach on real-worlddata. We pair the PolLidar sensor with a Velodyne VLS-128 reference lidar. shows PolLidar data with groundtruth distance and normals. In total, we capture 60 frameswith 3 biases each and scene-adjusted laser power pairedwith ground-truth distance and normal information.Forground truth, we accumulate point clouds from the refer-ence lidar, generate dense lidar maps, and extract normalsfrom the meshed lidar map. reports qualitative reconstruction results. Similarto the synthetic evaluations, PCA introduces artifacts,whereas the proposed approach is able to recover the sur-face geometry correctly, e.g., the first row of for thetransition area between ground and metal ramp. Further-more, the proposed approach is able to reconstruct normalsin sparse regions, e.g. for the metal support structure of theroof in the second row. These findings are consistent withthe quantitative results in Tab. 2, where the proposed ap-proach outperforms the best baseline by 16% mean angularerror. For autonomous driving, accurate normals allow us todistinguish obstacles from the road and are crucial for deter-mining if areas of the road can be overridden, e.g., detectinglost-cargo objects on roads . We show such a sce-nario in the last row of , where normals of a roadblockin 50m distance are predicted correctly as facing towardsthe vehicle by the proposed approach. In contrast, PCA estimates the roadblock as flat with downward pointing nor-mals likely misclassifying the object as traversable.For distance estimation, the mean absolute error of con-ventional argmax-peak-finding amounts to 24 cm, whereasour method yields a mean absolute error of 20 cm outper-forming the conventional distance estimation by 17%.",
  ". Ablation studies for different modules on syntheticdata. The quality of the proposed method degrades when the po-larization information, Mueller matrix, or wavefront is withheld": "remove the polarization information by replacing the rawwavefronts I with the mean over the different i. Remov-ing the polarization cues, increases mean angular error by22%. Furthermore, we ablate the ellipsometric reconstruc-tion. Specifically, we remove the Mueller matrix from theinputs. As the network needs to learn to disentangle the po-larization optics of the emitter and receiver from the scene,the mean angular error of surface normal increases by 12%.Finally, we analyze the impact of using raw wavefronts bysetting the window size to 1. Tab. 3 shows that the wave-front carries crucial information for scene reconstruction.",
  ". Conclusion": "This paper introduces a novel long-range polarization wave-front lidar sensor that measures time-resolved polarization-modulated wavefronts. To recover high-resolution scene in-formation from these raw polarimetric wavefronts, we de-vise a learning-based approach to recover distance, surfacenormals, and material properties. To train and evaluate themethod, we introduce a large synthetic dataset and a real-world long-range dataset with paired raw lidar data, groundtruth depth and normal maps. We validate that the proposedmethod improves normal and depth reconstruction by 53%and 41% in mean angular error and mean absolute distanceerror compared to existing shape-from-polarization (SfP)and ToF methods. Confirming the potential of the proposedpolarimetric wavefront sensing method with a sequentialacquisition setup, future work may devise parallelized ac-quisition setups that capture a subset of polarization states,allowing for real-time polarimetric lidar captures. AcknowledgementsThis work was supported by the AI-SEE project with funding from the FFG, BMBF, and NRC-IRA. Chenyang Lei was supported by the InnoHK program.Seung-Hwan Baek was supported by Korea NRF grant (RS-2023-00211658, 2022R1A6A1A03052954).Felix Heidewas supported by an NSF CAREER Award (2047359), aPackard Foundation Fellowship, a Sloan Research Fellow-ship, a Sony Young Faculty Award, a Project X InnovationAward, and an Amazon Science Research Award.",
  "Seung-Hwan Baek and Felix Heide. All-photon polarimetrictime-of-flight imaging. Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), 2022.1, 2, 3, 4, 6, 7, 8": "Seung-Hwan Baek, Tizian Zeltner, Hyunjin Ku, InseungHwang, Xin Tong, Wenzel Jakob, and Min H Kim. Image-based acquisition and modeling of polarimetric reflectance.ACM Trans. Graph., 39(4):139, 2020. 4, 5 Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-zel, Sven Behnke, Jurgen Gall, and Cyrill Stachniss.To-wards 3d lidar-based semantic scene understanding of 3dpoint cloud sequences: The semantickitti dataset. The In-ternational Journal of Robotics Research, 40(8-9):959967,2021. 1",
  "Kai Berger, Randolph Voorhies, and Larry H. Matthies.Depth from stereo polarization in specular scenes for urbanrobotics. In ICRA, 2017. 1": "Alexander Carballo, Jacob Lambert, Abraham Monrroy,David Wong, Patiphon Narksri, Yuki Kitsukawa, EijiroTakeuchi, Shinpei Kato, and Kazuya Takeda.Libre: Themultiple 3d lidar dataset. In 2020 IEEE Intelligent VehiclesSymposium (IV), pages 10941101. IEEE, 2020. 1 Nicholas Carlevaris-Bianco, Arash K Ushani, and Ryan MEustice. University of michigan north campus long-term vi-sion and lidar dataset. The International Journal of RoboticsResearch, 35(9):10231035, 2016. 1",
  "Felix Goudreault, Dominik Scheuble, Mario Bijelic, NicolasRobidoux, and Felix Heide. Lidar-in-the-loop hyperparame-ter optimization. In CVPR, 2023. 2, 5": "Tianyu Huang, Haoang Li, Kejing He, Congying Sui, BinLi, and Yun-Hui Liu.Learning accurate 3d shape basedon stereo polarimetric imaging.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1728717296, 2023. 3 J Alex Huffman, Anne E Perring, Nicole J Savage, BernardClot, Benot Crouzy, Fiona Tummon, Ofir Shoshanim, BrianDamit, Johannes Schneider, Vasanthi Sivaprakasam, et al.Real-time sensing of bioaerosols: Review and current per-spectives. Aerosol Science and Technology, 54(5):465495,2020. 1, 2",
  "Chenyang Lei, Xuhua Huang, Mengdi Zhang, Qiong Yan,Wenxiu Sun, and Qifeng Chen. Polarized reflection removalwith perfect alignment in the wild. In CVPR, 2020. 1": "Chenyang Lei, Chenyang Qi, Jiaxin Xie, Na Fan, VladlenKoltun, and Qifeng Chen. Shape from polarization for com-plex scenes in the wild. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1263212641, 2022. 1, 2 Clemens Linnhoff, Dominik Scheuble, Mario Bijelic, LukasElster, Philipp Rosenberger, Werner Ritter, Dengxin Dai, andHermann Winner. Simulating road spray effects in automo-tive lidar sensor models. arXiv preprint arXiv:2212.08558,2022. 1",
  "Daisuke Miyazaki, Robby T. Tan, Kenji Hara, and KatsushiIkeuchi. Polarization-based inverse rendering from a singleview. In ICCV, 2003. 2": "Daisuke Miyazaki, Takuya Shigetomi, Masashi Baba, RyoFurukawa, Shinsaku Hiura, and Naoki Asada. Surface nor-mal estimation of black specular objects from multiview po-larization images. Optical Engineering, 56(4):041303, 2016.3 Peter Pinggera, Sebastian Ramos, Stefan Gehrig, UweFranke, Carsten Rother, and Rudolf Mester.Lost andfound: detecting small road hazards for self-driving vehi-cles. In 2016 IEEE/RSJ International Conference on Intel-ligent Robots and Systems (IROS), pages 10991106. IEEE,2016. 8",
  "Moein Shakeri,Shing Yang Loo,Hong Zhang,andKangkang Hu. Polarimetric monocular dense mapping us-ing relative deep depth prior. IEEE Robotics and AutomationLetters, 6(3):45124519, 2021. 3": "Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jian-ping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN:Point-voxel feature set abstraction for 3D object detection.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2020. 1 Chaoran Tian, Weihong Pan, Zimo Wang, Mao Mao,Guofeng Zhang, Hujun Bao, Ping Tan, and Zhaopeng Cui.Dps-net: Deep polarimetric stereo depth estimation.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 35693579, 2023. 1, 3 Shishun Tian, Minghuo Zheng, Wenbin Zou, Xia Li, and LuZhang. Dynamic crosswalk scene understanding for the vi-sually impaired. IEEE transactions on neural systems andrehabilitation engineering, 29:14781486, 2021. 1",
  ". PolLidar Prototype": "As the PolLidar is an entirely novel prototype, we provide additional details about its construction (Sec. 1.1); subsequently,we share insight into key characteristics of the sensor (Sec. 1.2); then, we validate that the surface-induced polarizationcues can be captured with the proposed prototype (Sec. 1.3); finally, we discuss improvements that will be made for futureprototypes (Sec. 1.4).",
  ". Prototype Construction": "The PolLidar extends the concept from Beamagines L3CAM lidar 1 starting from traditional ToF and adding polarizationcapabilities. However, the emitter and receiver are a redesign with custom opto-mechanics to fit waveplates and polarizernecessary to modulate the polarization. illustrates a sectioned view of emitter and receiver to showcase the polarizationoptics. All used parts are listed in Tab. 1. The optics are designed to minimize the angle of incidence (AoI) onto thewaveplates and polarizer. The maximum AoI on the emitter side is 2, whereas on the receiving side, the maximum AoI is8. The optics on the emitting side allow for beam divergence of 0.364. On the receiving side, an entrance pupil of 6.8 mmis realized. An overview of sensor key specifications is provided by Tab. 2.The raw wavefront is captured with a National Instruments PCIe-5764 FlexRIO-Digitizer ADC, sampling at 1 Gs/s.The sampling frequency allows for 1 ns wide bins resulting in a 15cm range resolution. The ADC is interfaced from aLabView application which triggers the acquisition after the piezoelectric motors are finished rotating the waveplates orlinear polarizer to their respective rotation angle i. Raw wavefronts for a single frame with 150 rows and 236 columnsamount to approximately 100 MB of raw binary data where two bytes per bin are used to encode the measured intensity. Forthe acquisition, a Python API interfacing the LabView application has been developed that can be leveraged in future worksfor e.g. online optimization as discussed in .",
  ", 1516, 13": ". Sectioned view of emitter (left) and receiver (right). Parts numbers can be referenced from Tab. 1. For the protection ofintellectual property, parts of the sectioned view are covered and replaced by schematic placeholders. of geometry, material and viewing direction, very subtle. Thus, accurate intensity readings are crucial for a successful recon-struction. Hence, we provide inside in the system response of the PolLidar towards the key parameters of laser power andbias voltage of the APD. Furthermore, we investigate the noise characteristics of the sensor to better distinguish polarization-induced intensity change from sensor noise. System ResponseTo obtain meaningful intensity readings, laser power and bias need to be adjusted according to thescene. In contrast to conventional lidar sensors, we allowed access to these low-level parameters. We investigate the systemresponse to laser power and bias in a controlled environment as shown in . Here, we repeatedly measure a single pixelon a calibrated 90% reflection target for different biases and laser powers. We average 50 frames per laser power/bias to limitthe effect of noise. We find that large bias values quickly lead to saturation at around 0.4V. This will render the intensitiesmeaningless for SfP as all polarization states will likely be saturated making reconstruction unfeasible. In contrast, smallbiases result in low intensity readings below the noise floor and subsequently dropped points preventing reconstructionaltogether. As indicated by , we observe an exponential increase in intensity depending on the bias. Thus, the biasoperating point needs to be selected with care to allow for successful reconstruction. On the other hand, we find that thelaser power is well behaved in this regard and find an approximately linear relationship between laser power and intensity.Motivated by the findings in , we opt to select one laser power per captured frame and but always collect frames with 3different biases. Details on acquisition are provided in Sec. 3.2.",
  "% Reflection Target": ". System response towards bias and laser power. We observe an approximately exponential relationship between bias and intensitybut only a linear one between laser power and intensity for a single pixel on a 90% reflection target. We average 50 frames per laser power,bias pair to reduce the effect of noise. Noise ModelFurthermore, we investigate the noise characteristics of the sensor. To this end, we continuously capture thesame scene 800 times as shown by . On the right of , we show intensity distributions for four selected pixels oninteresting targets. We observe that the noise can be described as approximately Gaussian centered around a single mean.Thereby, we ensure to operate the laser diode and detector with sufficient warm up, such that over the measurements notemperature drift is possible. However, there is a substantial deviation of the intensity with standard deviations up to 0.03 mVwhich is coherent with the specifications of the installed APD. Therefore for some regions of a captured frame, the intensitydeviation from noise will likely overshadow the desirable deviation observable for different polarization states. As a result,we opt to average 10 frames to increase resilience from sensor noise, as further discussed in Sec. 3.2. In addition, we testthe sensor for repeatability by returning to the same rotation angles i after setting a series of different rotation angles. Weobserve no drift in this regard.",
  ". Assessment of Polarization Cues": "Our proposed SfP reconstruction method relies on rotating ellipsometry for acquisition. However, the results from rotatingellipsometry are not directly interpretable and do not allow an intuitive assessment weather the PolLidar is able to capturescene-dependent polarization cues. To this end, we follow the experiments from . We fix the rotation angles of HWP andQWP 1 and 2 to 1 = 2 = 0. When assuming the scene to be solely diffusive, the azimuth angle of the surface normal canbe directly found using the LP. Intuitively, the observed intensity I after the LP will have maxima where the rotation angle4 of the LP and the angle of polarization of the reflected light align. In theory, the intensity varies sinusoidally with respectto 4. The phase-shift or position of the maxima translates directly to the azimuth angle of the surface normal up to anambiguity of 180. The measured intensity I at the APD is given as",
  "2cos24 2,(1)": "where Imax and Imin are the maximal and minimal observed intensities, respectively. Note that for our setup, the rotationangle 3 must be set to 3 = 4 to eliminate the effect of the QWP in the receiver.We use this acquisition approach as an intuitive assessment of the PolLidar. To this end, we setup scenes with flat targetsplaced with different azimuth angles in front of the sensor. An example is shown in , where the object in focus is theblue metal plate oriented to the right and left, respectively. Next, we increase the rotation angle 4 of the LP in steps of 2andaverage 10 wavefronts per rotation angle to limit the effect of sensor noise. As shown on the right of , we observe thatthe intensity for both scenes can be described as a sinusoid. Furthermore, the phase-shift between left- and right-orientedtarget is clearly visible and is close to the difference in the azimuth angle of the two metal plates. As such, we find thatPolLidar is able to scan a scene accurately enough to capture polarization cues.",
  "-60 yaw": ". Assessment of Polarization Cues. We increase the rotation angle of the LP 4 in steps of 2and observe a single pixel on theright- and leftwards oriented blue metal plate. We find that the intensity varies sinusoidally with respect to 4 and we observe a phase shiftdepending on the normal, adhering well with the theory and experiments from . combinations of emitted and captured polarization states. As a consequence setting the polarization state requires consid-erable time. However, we imagine running the measurement in parallel as in passive cameras which employ four constantstates.2. In detail, this would require multiple linear polarizers at fixed rotation angles, which are placed in front of an arrayof APDs similar to a Bayer-Pattern for RGB cameras. In summary, this work contributes a first proof showcasing polariza-tion lidar bridging previous application domains allowing required distances and resolutions for autonmous driving vehiclesespecially enhancing the geometric representation for high distant objects.Future works may optimize the number of required rotation angles for real-time capable reconstruction in automotive scenar-ios.",
  ".(8)": "As denoted in the main paper, |Dd| and |Ds| describe the amplitude of the depolarization matrices and tpeak is the temporal in-dex of the wavefront peak. The parameter defines the pulse width which we tune such that the pulse width of downsampledsynthetic rays resemble the pulse width of the real device.",
  ". Implementation Details of Polarization CARLA Simulator": "We implement the previously discussed polarimetric lidar forward in CARLA. To this end, we rely on the full-wavefrontlidar simulator presented by . We extract the necessary normals n, index of refraction , diffuse and specular depolariza-tion amplitude |Dd|, |Ds| and roughness m from CARLA. Analogous to , we use custom material cameras for diffuseamplitude |Dd|, specular amplitude |Ds|, roughness m. When following the notation of , the diffuse amplitude equals d,the specular amplitude equals s and roughness translates to .However, the index of refraction is not assigned to materials in CARLA by default. To this end, we modify the CARLAray-tracer to return a material ID for each hit point. When the ray-tracer returns a hit point, we query the face of the meshat this hit point for the name of the assigned material. Worlds in CARLA have more than 5000 assigned materials. Wecluster the materials based on their name and their respective parent material. In total, we define 10 clusters and assignan index of refraction to each cluster, which we subsequently look-up during rendering. This method is suitable for staticobjects like buildings, infrastructure, or vegetation. However, moving objects such as vehicles in CARLA are consideredonly with a simplified mesh to reduce the computational cost for the ray-tracing. This simplified mesh does not have anymaterials assigned to it as it was not intended for rendering but only for extracting distance information with the ray-tracer.We circumvent this problem by manually assigning materials to each face of a vehicle mesh as shown in .Finally, we extend the ray-tracer to return the surface normal n for each hit point. The Unreal Engine underlying CARLAprovides a straightforward interface for querying the mesh for normals at a hit point. For extracting the normal, we add anextra channel for the normals to the lidar model in the C++ code and adapt the auxiliary Python interface accordingly. As theUnreal Engine returns normals in world-coordinates, we transform the normals into the local sensor frame.",
  ". Ground Truth": "As shown in , for generating ground truth distance and normals, we pair the PolLidar sensor with a Velodyne VLS-128reference lidar. Both lidars are mounted on a movable platform allowing to scan a scene from different positions. AfterPolLidar acquisition is completed, we move the reference lidar through the scene to obtain multiple reference point cloudsfrom different positions. We accumulate the reference point clouds after previous registration with the Iterative-Closest-Point(ICP) algorithm presented in to obtain a dense accumulated lidar map. DistanceThe lidar map is then used to generate ground truth distances dgt. To this end, we first transform the lidar map toPolLidar coordinates using the transformation T R4x4. We obtain T by iteratively aligning reference and PolLidar pointclouds for different scenes by means of the ICP algorithm as implemented by . Next, we can extract the ground truthdistance by calculating azimuth and elevation angle for each point of the lidar map and then applying bilinear interpolationwith the view encoding V of the PolLidar. As we move the prototype through the scene, the lidar map might have severalvalid distances per viewing direction that are occluded from one viewpoint but visible from another. This would severelydistort the ground truth when applying interpolation to the full lidar map. Thus, before interpolation, we remove hiddenpoints that are invisible from the PolLidar viewpoint by means of . Eventually, we output a ground truth distance mapdgt RHW encoding the ground truth distance for every viewing direction. NormalsFor generating ground truth normals, we first mesh the lidar map using and then query the mesh at theground truth point locations for their normals. Compared to ray-tracing with the viewing direction, querying is beneficialas the meshing method often introduces artifacts around object edges enlarging the object. Ray-tracing would produceerroneous normals as the true object is possibly occluded by an enlarged edge. Finally, we output a ground truth normal mapngt RHW 3 encoding the ground truth normal for every viewing direction.",
  "1if |d dGT| < dthresh0otherwise,(16)": "where c RHW denotes the binary mask of valid view directions and dthresh is a threshold we define as 0.8m. Furthermore,the mask c is helpful for eliminating erroneous ground truth. For instance, erroneous ground truth distance are likely toappear around object edges due to accumulation errors and resolution limits of the reference lidar, resulting in a widening ofobject edges. The mask excludes these points as shown in for the silhouette of the car.",
  ". Acquisition": "In the following, we provide further details on how we acquire frames with the PolLidar and describe the settings used. Whenacquiring a frame, we perform rotating ellipsometry as described in e.g. . To this end, we measure 36 different rotationangle combinations i, where the subscript i {0, 1, ..., 35} is used to distinguish the different combinations. The resultingstokes vector of the emitted light is visualized in the Poincare sphere in . As shown, the polarization state of the emittedlight is uniformly distributed along the sphere. For the emitter, the HWP is set to 1i = 0 and the QWP to 2i = 5 i. For thereceiver, the QWP is rotated to 3i = 25 i and the LP to 4i = 0.We opt to capture 10 frames for aggregation per rotation angle i, as we find that it is a good compromise betweenacquisition time, amount of generated data, and denoising.Furthermore, we choose a laser power of 600 mV and 900 mV for indoor and outdoor scenes respectively. Due to the highsensitivity of intensity towards the bias voltage, we capture the same scenario with the bias voltages {1980, 2000, 2020} mV.After acquisition with the PolLidar is complete, we capture a reference lidar map to extract ground truth as discussed inSec. 3.1. To this end, we move the setup through the scenery until we cover a similar area visible from the PolLidar in theinitial position.",
  ". Reconstruction": "The proposed reconstruction approach is a two-step approach. First, we apply classical ellipsometric reconstruction to dis-entangle the scene from the polarizing optics of the emitter and receiver. From this, we obtain the Mueller matrix H of thescene. Additional details on this, are provided in Sec. 4.1. Next, we feed this as an input to a neural network that predictsdistance offsets and normals. Additional details on the network are provided in Sec. 4.3.",
  ". Ellipsometric Reconstruction": "Ellipsometric reconstruction is used to disentangle the Mueller matrix of the scene from the Mueller matrices of emitter Piand receiver Ai. As discussed in the paper, the additional ellipsometric reconstruction helps the network to learn a betterscene reconstruction. In this section, we provide some additional ellipsometric reconstruction results.In order to recover the Mueller matrix of the scene Hmeas, we solve a least-squares optimization problem as defined by",
  "i=1(Ii [Ai, Hmeas, Pislaser]0)2 ,(17)": "see . We visualize the reconstruction approach in , where the individual elements of the Mueller matrix Hmeas areshown on the right. In order to validate the correctness of the reconstructed Mueller matrix, we then render the intensityimage using the lidar forward model and compare it with the measured intensities for the 36 different rotation angles i.This is visualized for the left of .1. On the top, we show the measured intensity for 0. On the bottom, we showfor two selected pixels, the re-rendered / reconstructed intensities over the 36 different measurements using the polarimetriclidar forward model. We find that the reconstructed intensities are in good agreement with the measured ones. The smalldeviations are likely due to the inherently noisy measurements. Aside from the disentanglement from the polarization optics,we thus believe that the ellipsometric reconstruction provides additional benefit as an additional denoising step.",
  ". Network Details": "We present the details of our network architecture in Tab. 3. Specifically, we first use two convolution layers to process theinput features. We then use 4 encoder layers to encode the features, each layer consists of a max-pooling and two convolutionlayers. At the bottleneck, we use 8 transformer layers . At last, we use 4 decoder layers with skip-connection to the priorlayer. Finally, we use a 11 convolution layer to get a four-channel output, which is the normals and distance, respectively.",
  ". Training Details": "We evaluate our method on both the simulation data and experimental data. Our simulated Carla dataset consists of 62different scenes with different IDs. The contents of each scene are generally different. We select 44 scenes and 18 scenesfor training and evaluation respectively, leading to 1430 and 539 test frames. We apply different laser biases during training.Specifically, we random sample the bias of our simulated PolLidar from 10 to 900. The intensities and distances for differentbiases are shown in .",
  "neg.pos": ". Ellipsometric Reconstruction. We show the individual elements of the reconstructed Mueller matrix on the right. To validate thecorrectness of the reconstruction, we render the intensities using the discussed polarimetric lidar forward model, as shown on the left. Wefind agreement between re-rendered / reconstructed and measured intensities. Note that different color scales are applied to each elementfor better visualization.",
  "Intensity [mV]Distance [m]": ". Frames with different biases. When the bias is low, a limited number of points are detected as the intensity falls below the noisefloor. When the bias is large, we see saturation effects in regions close to the sensor or in regions of high reflectivity. We use differentbiases at training time to increase the robustness against saturation and low-intensity readings.",
  ". Details of our network architecture. Concat operation means that we concatenate two elements along the channel dimension": "where the subscript denotes the respective index of the Mueller matrix H. We show the DoP for the two selected scenes of in the main paper in . When normals and viewing direction are aligned, as visualized in the two right columns,the DoP is low. This is true for e.g. buildings and parts of the car that face the sensor. Contrarily, the side or hood of the caris for instance a high DoP region as viewing direction and normal are almost perpendicular. Consequently, Baek et al. are unable to reconstruct normals in these low DoP regions, as shown in . For high DoP regions, however, satisfyingperformance is achieved.We also compare the normal reconstruction to PCA as a point-cloud based method that considers a neighborhood of points.This method performs well in areas with flat geometry and high point density but degrades significantly at long ranges, e.g.,cars in far distances, and geometry transition regions, e.g., the area between road and car. The proposed method leverages theadditional cues from polarization to resolve normals in regions with sparse points and in transition regions. Furthermore, theproposed method achieves satisfying reconstruction results for regions with little polarization information by taking a localneighborhood into account.",
  ". Additional Real-World Results": "We provide additional qualitative results from the real-world dataset in . We find similar trends as in the main paper.The proposed method consistently outperforms PCA in areas where the point cloud is sparse. This is visible in the zoom-inson the fine structures, e.g., car roofs, where the neighborhood is too sparse for PCA to reconstruct correct surface normals.This is visible for the roof structure in the fourth row or the car on the left in the fifth row. The point cloud visualization also",
  ". Additional Distance-Binned Evaluation of Normal Reconstruction": "To further analyze the advantage of our approach over the existing baseline in regions of low point density, we analyze themean angular error for normal reconstruction and bin the results per distance bin. Specifically, this analysis allows us to studyfurther distances in more detail, where point distance due to constant angular sampling increases further, making PCA faildue to the missing neighborhood. Quantitative results are visualized in , showing in (a) an increased performance dueto polarization in close distance by approx. factor of two compared to PCA. Thereby, for further distances PCA degeneratesby 75% being outperformed by a factor of 2.5. Furthermore, our method is able to cope with sparser point clouds and showsa substantially lower dependency of reconstruction performance on distance. In (b), we plot the relative mean angularerror between PCA and the proposed method. We see that the gain in reconstruction quality is closely related to the distance.",
  ". Additional Metrics for Distance Evaluation": "To evaluate the distance estimation, we compare against the conventional argmax-peak-finding typically performed directlyon the device by low-level electronics . We list all evaluated metrics on the distance reconstruction in Tab. 4 and Tab. 5.In addition to the results in the main paper, we present here median and RMSE distance errors. We find that our approachoutperforms conventional peak-finding on these metrics by large margins on both synthetic and real data, outperformingprevious results by 41% on synthetic data and 17% on real-world data for mean absolute distance error. The comparativelylower gain in real-world data is likely related to the quality of the ground truth. For synthetic data, we have perfect groundtruth as we have control over the entire rendering pipeline including the underlying geometry. However, for the real-worlddata, we are limited by our applied geometry reconstruction described in Sec. 3.1. This has inherent sensor noise obfuscatingdistance and inaccuracies of the accumulation of our ground-truth though pose estimation errors, beam divergence broad-ening object dimensions, and many more. Consequently, the median error is increased for both conventional and proposeddistance estimation methods. In contradiction to that is the higher RMSE in the synthetic data which can be explained withoutliers in the distance error. More specifically, if the conventional peak-finding method selects the wrong peak or missesthe peak altogether, e.g., due to low intensities close to the noise floor, the distance error will be of many meters for both theconventional and proposed method, thus increasing the RMSE significantly. For the real data, the effect of increased RMSEis not as prominent, as we apply the mask dependent on dthresh effectively suppressing these outliers.",
  "Ground truthBaek et al.PCAProposed": ". Additional Synthetic Results. Baek et al. is unable to reconstruct normals in areas with low DoP, e.g., walls of buildingsfacing the sensor. PCA applied in this setting is strongly dependent on point cloud density. This is visible for e.g. the poles in fardistances. The proposed approach leverages polarization cues to reconstruct normals in sparse regions and is robust against low DoP areas.",
  "(a) Absolute mean angular error(b) Margin": ". Mean angular error dependent on distance. In (a), PCA achieves reasonable performance for close regions as the point cloudsare more dense here. However, as the distance increases, the mean angular error of PCA increases rapidly. In (b), we plot of the marginbetween PCA and the proposed approach. We can see the performance gain is closely correlated with the distance.",
  "Eric Heitz. Understanding the masking-shadowing function in microfacet-based brdfs. Journal of Computer Graphics Techniques,3(2):3291, 2014. 7": "Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural kernel surface reconstruction. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43694379, 2023. 8 Sagi Katz, Ayellet Tal, and Ronen Basri. Direct visibility of point sets. In ACM SIGGRAPH 2007 papers, pages 24es. 2007. 8 Ignacio Vizzo, Tiziano Guadagnino, Benedikt Mersch, Louis Wiesmann, Jens Behley, and Cyrill Stachniss. Kiss-icp: In defense ofpoint-to-point icpsimple, accurate, and robust registration if done the right way. IEEE Robotics and Automation Letters, 8(2):10291036, 2023. 8 Bruce Walter, Stephen R Marschner, Hongsong Li, and Kenneth E Torrance. Microfacet models for refraction through rough surfaces.In Proceedings of the 18th Eurographics conference on Rendering Techniques, pages 195206, 2007. 7 Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A modern library for 3D data processing. CoRR, abs/1801.09847, 2018. 8 Yufan Zhu, Weisheng Dong, Leida Li, Jinjian Wu, Xin Li, and Guangming Shi. Robust depth completion with uncertainty-drivenloss functions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 36263634, 2022. 14"
}