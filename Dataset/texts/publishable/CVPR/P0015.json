{
  "Abstract": "Multimodal multihop question answering is a complextask that requires reasoning over multiple sources of infor-mation, such as images and text, to answer questions. Whilethere has been significant progress in visual question answer-ing, the multihop setting remains unexplored due to the lackof high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makesthem unsuitable for real-world scenarios such as analyzingmultimodal educational materials, summarizing lengthy aca-demic articles, or interpreting scientific studies that combinecharts, images, and text. To address this gap, we proposea novel methodology, introducing the first framework forcreating a high-quality dataset that enables training modelsfor multimodal multihop question answering. Our approachconsists of a 5-stage pipeline that involves acquiring rele-vant multimodal documents from Wikipedia, syntheticallygenerating high-level questions and answers, and validatingthem through rigorous criteria to ensure quality data. Weevaluate our methodology by training models on our syn-thesized dataset and testing on two benchmarks, our resultsdemonstrate that, with an equal sample size, models trainedon our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. Webelieve our data synthesis method will serve as a strongfoundation for training and evaluating multimodal multihopquestion answering models1.",
  "Existing Datasets": "(e.g. WebQA) FM2DS (Ours) - Large Human Effort: Most documents are linked by human annotators - Limited Context: only relevant information snippets are provided - Little Human Effort: Documents are automatically linked via hyperlinks and topics. - Long Context Challenge: Full documents are provided - Templated Questions: not very realistic questions and are manually collected - No quality check - Free-form: questions are automatically generated and are more reflective of the real world - Quality Check: questions are ensured to ne answerable, multimodal, and multihop - Large Human Effort: Questions are answered manually by human - Little Human Effort: Answers are generated automatically and validated for quality check - No Queries are included to validate the answer and help models learn better - Queries are included to validate the generated answers and help models learn better",
  "Generation": ". In contrast to traditional datasets that depend on humanannotators, templates, and information snippets as sources, FM2DSis a fully automated approach that utilizes complete documents asits sources. FM2DS incorporates validation steps to ensure that thegenerated questions are answerable, multimodal, and multihop. area highlight the potential of large language models (LLMs)and large vision language models (LVLMs) to enhance rea-soning capabilities , yet existing frameworks oftenrely heavily on extensive human-annotated datasets, whichis not always feasible for practical applications. Current methodologies in MMQA typically leverage in-context learning methods, prompting LVLMs to retrieverelevant information from multimodal sources and thenperform reasoning . However, these models often de-mand significant computational resources due to their largeparameter counts, making them costly to deploy even duringinference . This limitation emphasizes the need for moreefficient frameworks that can operate effectively with mini-mal annotated data. A practical solution is to use a smaller",
  "arXiv:2412.07030v2 [cs.CL] 17 Dec 2024": "model capable of retrieving the necessary information fromsources before performing reasoning. This can be achievedby fine-tuning the model on a MMQA dataset. However,existing datasets often consist of short information snippetsrather than full documents , or they rely on repetitivequestion templates , which limits the models general-izability. They also primarily address unimodal or simplermultimodal tasks that do not require long texts with multipleimages . Additionally, creating new similar datasets ischallenging, requiring extensive human annotation .In this work, we propose FM2DS, a novel data syn-thesis framework designed specifically for few-shot mul-timodal multihop question answering. Our approach synthe-sizes MMQA data from documents that are interconnectedthrough various relationships, such as thematic similaritiesor sequential events. This framework leverages naturally oc-curring document relationships and requires minimal hand-crafted data, thereby broadening the range of reasoningtypes employed in question generation. As illustrated in, FM2DS enables the generation of non-templatedquestion-answer pairs based on full documents rather thanbrief information snippets. By integrating a query compo-nent that offers a step-by-step guide for retrieving relevantinformation from various documents, this framework en-ables smaller models to handle complex questions similarlyto larger models. This methodology allows users to create acustom MMQA dataset with fewer than ten human-annotatedsamples, thereby facilitating the fine-tuning of a smallervision-language model (VLM) for specific applications.In our method, we utilize Wikipedia as a primary datasource due to its extensive knowledge base and hyperlinkstructure. Next, by using document pairs with shared topicalrelevance or hyperlink connections, we prompt LVLMs toperform multiple generation tasks, including question gen-eration, question answering, and query generation, similarto previous work in text-only settings . By leveragingfew-shot samples and tapping into the knowledge and rea-soning capabilities of LVLMs, we generate diverse question-answer pairs, accompanied by queries, to streamline andaccelerate the learning process for smaller VLMs.Through empirical evaluation on established benchmarksfor MMQA, we show that FM2DS significantly enhancesmodel performance across various configurations. By fine-tuning smaller VLMs on our synthesized data, we achievesuperior performance on human-annotated test sets com-pared to fine-tuning the same model on the human-annotateddata. This work advances the state-of-the-art in MMQAby introducing a scalable framework that harnesses the po-tential of few-shot learning. By addressing the critical gapbetween single-hop and multi-hop reasoning in multimodalcontexts, our work presents a novel approach that signif-icantly reduces the dependency on extensively annotateddatasets while maintaining robust performance. The pro- posed methodology not only bridges the existing limitationsin MMQA systems but also establishes a foundation for moreefficient and adaptable question answering systems that canreason across diverse modalities and multiple inference steps.The key contributions of this work are: Introducing a novel framework for synthesizing high-quality training data for VLMs to effectively perform Mul-timodal Multi-hop Question Answering (MMQA).",
  ". Related Work": "Within the Question Answering (QA) literature, synthesis oftraining data has been predominantly focused on unimodal(text-only) scenarios. We review various seminal works thathave established the foundation for our work in few-shotdata synthesis. Unimodal Data SynthesisSynthetic data has become awidely used resource for model training. He et al. propose a general framework that utilizes synthetic text gen-erated by language models (LMs) for knowledge distillation,self-training, and few-shot learning, demonstrating that com-bining labeled and pseudo-labeled synthetic data can leadto notable performance gains in NLP tasks. In other cases,rather than synthetic text alone, entire datasets are generatedfor specific tasks such as classification . For instance, Liet al. evaluate the effectiveness of LLMs, specificallyGPT-3.5, in producing synthetic datasets for text classifi-cation, offering insights into the consistency and reliabilityof LLM-generated data. In another work, Chen et al. present a framework that significantly improves the perfor-mance of smaller language models on multi-hop QA tasks,even with minimal human-annotated data. Multimodal Data SynthesisResearch on multimodal datasynthesis using LVLMs is still underexplored, most workfocuses on creating entirely new data based on knowledgeencoded in these models. For instance, Zhang et al. design a multimodal benchmark utilizing large vision andlanguage models and their code capabilities to synthesizemassive abstract images and visual reasoning instructionsacross daily scenarios. In another study, Mehta et al.",
  "Technology": "Entertainment Linked Docs 2 Linked Docs 1 . The Five-Stage Pipeline for FM2DS. First we retrieve relevant documents from Wikipedia to create a pool of related documentsbased on hyperlinks and topics (Stage 1). In Stage2, we select the few-shot samples from MultiModalQA (MMQA in the figure) . Stage3 focuses on generating and validating questions to make sure they are answerable, multihop, and multimodal . In Stage 4, answers aregenerated and validated. Finally, in Stage 5 we generate queries related to the documents, which are also validated to ensure relevance andaccuracy. tackles the scarcity of training data for models that generateboth speech and gestures, using unimodal synthesis modelsto create synthetic multimodal training data, which is thenemployed to pre-train a joint synthesis model. In the field ofmultimodal question answering, Wu et al. introducedSMMQG, a method that generates questions for multimodaldocuments using multimodal RAG. In this work, we presenta novel approach for synthetically generating a questionanswering dataset, leveraging the reasoning capabilities ofLVLMs to create not only multimodal questions but alsomultihop ones, bringing them closer to real-world scenarios.",
  ". Proposed Method: FM2DS": "Our five-stage pipeline for FM2DS shown in isdesigned to synthesize high-quality multimodal question-answer pairs. The process begins with retrieving relevantdocuments through topic matching and Wikipedia hyperlinks.The subsequent stages involve few-shot sample selection,synthesizing and validating multihop questions, generatingcorresponding answers, and constructing relevant documentqueries, with each stage incorporating specific validationmechanisms. We elaborate on individual components in thefollowing sections. Refer to Appendix E for examples ofgenerated data by FM2DS.",
  ". Stage 1: Creating a Pool of Related Documents": "We collect a pool of relevant documents using Wikipediapages leveraging Wikipedias vast repository of intercon-nected information. For this, we used the WikiWeb2Mdataset , which contains nearly 2 million Wikipedia pages,providing a comprehensive source of information. Thesedocuments are linked through two complementary methods:existing hyperlinks and topics extracted using multimodaltopic modeling with Multimodal-Contrast model . Thehyperlink structure provides explicit, human-curated con-nections between related concepts, while the multimodal topic modeling technique extracts latent thematic relation-ships from both textual and non-textual content. This dualapproach ensures a thorough compilation of related infor-mation, capturing both obvious and nuanced connectionsacross documents, while integrating both visual and textualmodalities.",
  ". Stage 2: Creating Few-Shot Samples": "To generate questions, we leverage existing multimodal mul-tihop question-answering dataset MultiModalQA thatrequire reasoning across various modalities, including text,images, and tables. For our purposes, we utilized a customsubset of the MultiModalQA train set. Using the links toWiikpedia pages provided in the dataset, we create a poolfor few-shot samples that include the full Wikipedia pagescomplete with images and tables in HTML formatalongwith the question. In our experiments, we randomly selectup to three of these samples for question generation.",
  ". Stage 3: Question Generation and Validation": "Question GenerationWe use GPT-4-turbo to gener-ate questions based on few-shot samples created from theMultiModalQA dataset. Due to the models context lengthlimitations, we restricted the input to two or three relateddocuments. Our prompt, detailed in Appendix A, querythe model to produce multimodal, multihop questions thatrequired input from at least two modalities and drew oninformation from all provided documents. For instance, iftwo documents were given, the question needed both to beanswerable; if three documents are provided, it can utilizeany two or all three of them. We explicitly instructed themodel that simply concatenating two independent questionswith an and to reference multiple documents would notqualify as a valid multihop question. For instance How didAlbert Einstein contribute to the Theory of Relativity andwhen Princeton established? is not a true multihop question, even though it involves two documents (Einsteins death inPrinceton being the link). A genuine multihop question re-quires integrating information across sources to necessitatereasoning over multiple documents. Question ValidationOur framework incorporates mul-tiple validation layers to verify that synthesized questionssatisfy the requisite criteria for both multihop reasoningcomplexity and multimodal integration. Although the modelwas instructed during question generation to avoid creatingquestions that simply concatenate independent questions an-swerable by a single document, we evaluated this further. Forthis, we employed LLama-3.1-8B to break each questiondown into simpler components. We then assessed whethereach component could be answered using only one docu-ment. If all parts of the question were solvable with a singledocument, we discarded such question that include unrelatedfacts. Here is an example of question with unrelated facts:",
  "Example (Unrelated Facts): In what year did MikeTyson become the youngest heavyweight champion, andwho is the president of the United States?": "Alternatively, we kept only the initial component thatrequired information from multiple documents to ensure thenew question met the multihop criteria. One potential is-sue with this approach was that, although the facts in thequestions are related, they often led to open-ended ques-tions where the answer needed to be an explanation or acombination of two responses. For example:",
  "Example(Concise Multihop Query): Who was the pres-ident of the United States when Mike Tyson became theyoungest heavyweight champion?": "Another critical part of validation was ensuring the mul-timodal nature of the questions. After confirming that aquestion was multihop, we tested whether it could still beanswered if the documents were reduced to a single modality(e.g., text-only, image-only, or table-only). We used GPT-4oto see if the question is answerable with only one modality(refer to .4 for more details on how to check if aquestion is answerable). If a question could be answeredusing just one modality, we discarded it, as it did not meet",
  ". Stage 4: Answer Generation and Validation": "Answer GenerationFor answer generation, we used GPT-4o to produce concise answers based on multiple documents,which included both textual and visual information such asimages. The model was explicitly instructed to answer thequestion in the shortest way possible, providing only thekey information without offering any additional explanation.To help the model focus on specific details of images inthe given documents to answer the multimodal question,we include question-related captions for the images. Forexample, if the question asks about the geometric shapesin an image (see ), the model generates a captiondescribing the shapes. This makes it easier for the model toanswer the question accurately. Answer ValidationWe validated the generated answersusing information extraction techniques, specifically namedentity recognition (NER) and relation extraction, as theyhave been previously used for validating answers .NER identified key named entities (e.g., persons, organiza-tions, locations) and numerical data in both the answer andthe documents, which were compared to ensure alignmentwith the source content. Relation extraction was used toverify whether the relationships between entities in the gen-erated answer matched those in the documents. By ensuringthe correctness of both named entities and their relations, wevalidated that the answers were accurate, concise, and con-sistent with the multimodal data provided 2. For includingthe images in the process, we used captions generated byGPT-4o. Specifically, based on what the question requiresfrom the image (e.g., the color of a building), the captionis generated to include that information. Additionally, tomitigate hallucinations, we prompted GPT-4o five times, andif the model generated the same correct response across allfive prompts, we accepted it. To evaluate the effectiveness ofour answer validation step, we designed a human evaluationwhere the result can be found in .",
  ". Stage 5: Query Generation and Validation": "Query GenerationWe create queries based on thequestion-answer pairs and the related documents to facil-itate a more effective retrieval process. These queries aremeticulously designed to guide the search for relevant in-formation, allowing us to pinpoint specific data points thatare crucial for formulating accurate answers. This strategicapproach helped us narrow down the information within the",
  "We utilized Spacy for relation extraction": "documents, enabling us to identify and extract key detailssuch as entities, relationships and contextual information thatalign with the questions. This targeted querying facilitatedan effective information extraction approach, that the gener-ated answers are comprehensive and directed supported bythe evidence found in the documents. Query ValidationTo validate the queries, we used multi-modal retrieval-augmented generator (MuRAG) , whichintegrates both textual and visual information by encodingdocuments and images into a shared embedding space. Thisapproach allows us to retrieve relevant documents based onboth text and visual context, ensuring a more comprehen-sive validation. The validation process involves queryingthe MuRAG database and retrieving the top-5 most relevantdocuments for each generated query. If more than one sourcedocument appears in the top-5 results across all queries, itindicates that the query is well-formed and effectively cap-tures diverse, relevant information from multiple sources.Moreover, this validation ensures that the queries are correctand serves as a guide for teaching a smaller model how toretrieve the necessary information to answer the questions.",
  ". Proposed Benchmark: M2QA-Bench": "We also propose a benchmark, M2QA-Bench, to assess theLVLMs performance on a more complicated MMQA taskwith full documents. M2QA-Bench consists of 500 Q&Apairs, each designed to challenge the models ability to per-form a complex reasoning task. The questions are not tem-plated into a specific structure (as in some existing works), instead, they are diverse and challenging. Additionally,answering the questions require access to full documents,where both information extraction and reasoning across dif-ferent modalities (e.g., images and tables) are essential. Ta-ble 1 presents the key statistics of M2QA-Bench. Refer toAppendix E for benchmark samples created using FM2DS.",
  ". Key statistics of the proposed multimodal multihop ques-tion answering benchmark": "To create this benchmark, we used the FM2DS pipeline togenerate 1,000 samples which were further verified by threehuman-annotators on whether it was correct, multihop, andmultimodal, as well as whether the generated answer wasaccurate. Samples without issues received a score of 1, whilethose with issues received a score of 0. After completing theannotation process, we removed samples with an average",
  ". Experiments and Results": "This section evaluates our synthesized dataset against human-annotated datasets. In all experiments, the model was pro-vided one in-context example during data synthesis, unlessstated otherwise. We used GPT-4o in the synthesis pipeline.Models were evaluated using Exact Match (EM) for answeraccuracy and F1 score for precision-recall balance in par-tial matches. See Appendix B for additional experimentaldetails.",
  ". Comparison with Human-Annotated Datasets": "Unlike previous dataset creation methods like Multi-ModalQA and WebQA , our approach is fully auto-mated, with no human intervention in data generation. Thissection provides a comparative analysis of the data qual-ity synthesized by our method against these prior human-annotated datasets. We trained three modelsLLaVA-1.6, InternVL-2 , and Idefics-2 withvarying sizes of the WebQA and MultiModalQA datasets andevaluated them on their respective test sets. showsthat samples generated by FM2DS help us train models thatcan handle MMQA better, even though its training samplesinclude long documents. In contrast, training samples fromMultiModalQA and WebQA consist of short, focused infor-mation snippets. We also notice that WebQA might be aneasier task than MultiModalQA, as models tend to performbetter on WebQA with fewer samples, even though it has alarger training set. Across all models and experiments withdifferent numbers of training samples (where models trainedon synthetic data were exposed to an equal or fewer numberof real samples), the average improvement observed in EMwas 1.81 in MultiModalQA and 1.96 in WebQA. While im-provements in EM generally lead to increases in F1 score,there are cases where F1 decreases even as EM improves.This discrepancy may result from hallucinations in incorrectpredictions, which reduce string overlap and, in turn, lowerthe F1 score. Moreover, models achieve the same performance withless synthetic samples generated by FM2DS as those trainedon the full training dataset, demonstrating faster convergencewith fewer samples. We explore this in details in .2.Also note that larger models achieves better performance onthe same number of synthetic samples, see the differencebetween Llava-1.6-13B and Llava-1.6-7B. GPT-4o achievesbetter results among large VLMs, likely because it was alsoused for data generation. For qualitative analysis, refer toAppendix F.",
  "FT (Real/Syn)RealSynRealSynFT (Real/Syn)RealSynRealSyn": "LLaVa-1.6-34B10k/10k79.41 80.92 82.55 83.7110k/10k84.41 84.94 85.58 85.79LLaVa-1.6-34B23.8k/16k84.83 85.29 85.51 86.4234.2k/13k86.48 87.49 88.18 88.18InternVL-2-40B10k/10k82.18 83.56 89.2490.210k/10k87.67 89.77 92.76 93.82InternVL-2-40B23.8k/15k86.63 87.27 91.73 91.4434.2k/14k89.77 90.32 93.19 93.19InternVL-2-76B10k/10k83.95 86.15 89.76 90.7210k/10k88.12 90.32 93.19 94.77InternVL-2-76B23.8k/14k90.82 91.34 92.79 93.8134.2k/14k93.14 93.65 94.06 93.82Idefics-3-8B10k/10k76.42 77.56 86.74 88.2310k/10k82.48 84.62 89.12 90.09Idefics-3-8B23.8k/19k82.18 83.56 90.27 91.8134.2k/15k86.49 87.77 92.55 93.31Phi-3.5-Vision-Instruct-4.2B10k/10k69.43 70.25 75.35 77.5710k/10k78.31 80.2284.585.18Phi-3.5-Vision-Instruct-4.2B23.8k/22k77.85 78.79 82.59 84.6134.2k/19k80.11 81.27 85.34 87.48mPLUG-DocOwl-1.5-8B10k/10k72.24 74.82 78.82 79.4910k/10k81.27 83.86 87.52 88.75mPLUG-DocOwl-1.5-8B23.8k/20k79.41 80.12 84.69 87.0734.2k/17k82.48 84.42 87.93 89.21 LLaVa-1.6-34BNone60.1164.06None64.3370.82InternVL-2-40BNone72.9377.42None76.9882.33InternVL-2-76BNone75.3279.32None78.3185.41Idefics-3-8BNone61.2769.74None69.8876.39Phi-3.5-Vision-Instruct-4.2BNone55.7862.16None63.6869.74mPLUG-DocOwl-1.5-8BNone58.4664.02None66.3871.26 . Comparison of model performance across various architectures, sizes, and sample sources (real vs. synthesized by FM2DS). Themodels were evaluated on 10k samples and the full dataset (23.8k samples for MultiModalQA and 34.2k samples for WebQA). Whencomparing models tuned on synthesized data with those trained on the full training set, the smallest number of synthetic samples (divisibleby 1000) that outperforms models trained on the full datasets is reported. For real sample evaluations, the WebQA training set is used fortesting on the WebQA test set, and the same applies to MultiModalQA. Models trained with synthesized samples consistently outperformthose trained with equivalent numbers of real samples. participating individuals in the acknowledgment section ofthe papers camera-ready version.Upon registration, participants were provided access to acustom evaluation application, as shown in . Thisapplication was designed to streamline the evaluation pro-cess and ensure consistency across participants. For eachsession, the application randomly selected one of the 100questions from our human evaluation set of samples. Foreach question, participants could review the question text,the associated Wikipedia pages, and the generated answersfrom two methodsone method utilizing the answer valida-tion component and the other without it. To minimize userbias, the application randomly alternated the positioning ofthe methods answers (labeling them as Answer A andAnswer B) so that users could not develop a tendency toselect one model over the other based on position alone. Af-ter examining the question and relevant Wikipedia content,users were asked to select one of four options to indicatetheir assessment of answer accuracy: (1) Answer A is cor-rect, (2) Answer B is correct, (3) both answers are correct,or (4) neither answer is correct.In addition to these selections, participants had the optionto provide a brief rationale for their choices. Although theyhave not been investigated for this research, this optional feedbacks were encouraged, as it offers valuable insightsfor qualitative analysis and potential future improvements inanswer validation accuracy. The combination of structuredand open-ended responses enhances the robustness of ourevaluation and offers a more comprehensive view of userjudgments, which we may explore in future iterations of ourdata synthesis methodology.",
  "GPT-4oNone83.5687.91None86.4990.23Llama-3.2-90BNone77.1880.37None82.2286.9Claude-3.5-SonnetNone73.8477.38None76.2979.43": ". The result of comparing synthetic data with a human-annotated dataset across different models. For the smaller versions of themodels, we tested with 5k samples, 10k samples, and the full training set (23.8k for MultiModalQA and 34.2k for WebQA) and for biggermodels we tested on full training set and 10k samples. When comparing the model trained on the full dataset with synthetic data, thementioned number of synthetic samples is the smallest (divisible by 1000) that outperforms the full dataset model. For real samples, weused the WebQA training set for testing on the WebQA test set, and similarly for MultiModalQA. The results show that models trained onsynthetic data achieve higher EM across all experiments. Models with None are the default pretrained models. For additional results, seeAppendix C.",
  ". Learning Efficiency Comparison": "To assess the comparative learning efficiency between syn-thesized and human-annotated datasets, we conducted sys-tematic experiments using InternVL-2-8B across incremen-tal training sample sizes ranging from 1k to 10k instances.For synthesized data, we used the same training data forboth MultiModalQA and WebQA. For the human-collecteddatasets, we trained the model on each corresponding train-ing set. As illustrated in , our synthesized dataset showsa substantial performance over real data at smaller samplesizes. However, as the number of real samples increases, thisgap gradually narrows, though the synthesized dataset stillachieves superior results overall. Notably, as the numberof samples approaches 10k, the learning efficiency of themodel trained on synthesized data declines more noticeablycompared to real data. This slowdown may be attributed tothe broader and more diverse distribution of general knowl-edge present in the synthesized data. While this diversity is beneficial for earlier stages of training, it can eventuallylead to diminishing returns as the model begins to saturate.In contrast, real data, which often contains more focusedand specific information, may continue to provide new, rele-vant patterns for the model to learn, maintaining a steadierlearning pace .In a related experiment with MultiModalQA, we used fullWikipedia pages as training data by leveraging the datasetslinked articles. Unfortunately, this setup could not be repli-cated with WebQA, as its source links predominantly refer-ence WikiMedia pages without sufficient textual information.Results in show that, initially, the model trainedon full Wikipedia pages for real samples performs better(in terms of improvement per 1k samples); however, afterapproximately 3k samples, this gap decreases. This indicatesthat, unlike our synthesized dataset, full-page real data inMultiModalQA lacks the generality offered by synthesizeddata. Synthesized data not only improves model learningby including queries but also maintains consistent learningspeed, likely due to the diversity and quality of the samples.",
  "Sample Size": "Score (%) WebQA Performance EM (Synthesis)F1 (Synthesis)EM (Real)F1 (Real) . Comparison of learning from Synthesis Data and Real Data across 1k to 10k samples for InternVL-2-8B. As shown, with an equalnumber of samples, synthetic data generated by FM2DS outperforms real (human-annotated) samples in terms of both EM and F1 scores.Additionally, the gap between the two datasets narrows as the number of samples increases. Number of Samples",
  "Score": "Comparison of Synthesized and Real Samples on MultiModalQA When Using Full-Pages In Training Synthesized Samples - EMSynthesized Samples - F1Real Samples - EMReal Samples - F1 . Comparison of learning from synthetic Data and RealData across 1k to 10k samples for InternVL-2 8B. In this exper-iment, to make the real samples similar to ours, we used the fullWikipedia pages from Multimodal QA dataset.",
  ". Cross-Dataset Evaluation": "To evaluate the generalizability and robustness of our syn-thesized data, we conducted a cross-dataset evaluation usingthe InternVL-2-8B model. For this experiment, we used ourM2QA-Bench with 500 samples and matched it with 500randomly selected samples from the MultiModalQA test set,including full Wikipedia pages for both. With evaluationsets now having a similar structure, we trained the modelseparately on 5k samples from our synthesized dataset and5k training samples from the MultiModalQA dataset, bothwith full Wikipedia pages as context. As shown in , the model trained on our synthesized data outperformed the model trained on MultiModalQA samples across bothtest sets. This result highlights the broader generalizabilityof our dataset. Additionally, our benchmark proves to bemore complex and diverse than MultiModalQA which haspredominantly template-based questions.",
  ". Human Evaluation of Answer Validation": "To evaluate the accuracy and impact of the automatic answervalidation component of our pipeline, we conducted a humanevaluation study. We randomly selected 100 samples and foreach sample, we generated an answer using two methods:FM2DS and FM2DS w/o answer validation. We then invited10 participants to contribute to our evaluation platform (seeAppendix D) to determine which method produced the cor-rect answer. Participants were instructed to verify whichanswer was correct by reviewing the associated Wikipediapages. Once they determined the accuracy of each answer,they were asked to select one of four options: (1) Method 1s answer is correct, (2) Method 2s answer is correct, (3) bothmethods generated the correct answer, or (4) neither answeris correct. The results of the study are presented in , shows a clear trend that answer validation improves thelikelihood of correct answers. Percentage (%) 23.8%71.4% Human Evaluation Results: Model Preferences (N=168) FM2DS (40, 23.8%)FM2DS w/o Answer Validation (2, 1.2%) Both (120, 71.4%)Neither (6, 3.6%)",
  ". Effect of Key Steps on Model Performance": "To analyze the impact of various data filtering and validationsteps on model performance, we evaluated the model acrossdifferent configurations. demonstrate the importanceof each data refinement step on model performance. Ques-tion validation improved relevance by filtering questionstailored to the complex, multimodal, multihop requirementsof the test sets, reducing hallucinations and boosting F1 bykeeping answers concise and accurate. Answer validationand query generation proved crucial as answer validationfiltered out incorrect samples, and queries supported learn-ing by distilling knowledge from larger models, leading toimproved EM and F1 scores. Finally, query validation rein-forced training consistency by ensuring that queries followedthe correct structure, minimizing confusion and helping themodel accurately interpret questions. Together, these stepsyielded the highest scores, underlining the cumulative impactof careful dataset curation.",
  ". The Effect of the Number of In-Context Docu-ments": "presents the results of evaluating the Intervl-2 8Bmodel with varying numbers of in-context documents on theMultiModalQA and WebQA datasets. In the zero-shot set-ting, FM2DS exhibits limited understanding of multimodalmulti-hop question answering, and occasionally circumventsthe validation step by simply generating a question that is notmultihop. For example, Looking at the image of the EiffelTower, what engineering innovation allowed it to surpassprevious structures in height? prompts the model to use theimage, but the answer is available in the pages text on tallstructures. As we move from zero-shot to one-shot, there isa significant boost in EM and F1 scores, reflecting improvedperformance with minimal context. The improvement fromone-shot to two-shot is marginal, suggesting diminishingreturns. With three in-context samples, the gains becomeminimal, indicating that additional samples beyond two pro-vide little benefit. This diminishing return may stem from themodels limited context window, which restricts its ability tofully utilize large in-context samples .",
  ". Comparing Methods For Data Synthesis": "To evaluate the effectiveness of different methods for syn-thetic data generation, we compared three prominent lan-guage models: GPT-4o, Claude 3.5 Sonnet, and Llama-3.2-90B, as shown in . Using Intervl-2-8B with 5Kfine-tuning samples as our baseline model, we tested thequality of generated data on two distinct datasets: Multi-ModalQA and WebQA. The results, measured using EMand F1 scores, demonstrate that GPT-4o consistently outper-forms other models across both datasets. We also see thatLlama-3.2-90B shows competitive performance as an open-source model with less number of parameters, particularlyin WebQA tasks. Claude 3.5 Sonnet generally yields lowerscores across both datasets.",
  ". Performance comparison of different models for datageneration on test datasets": "of existing methods, which focus on single-hop question an-swering with a single modality, and provides a robust frame-work for generating complex questions and answers thatrequire reasoning over multiple sources of information withminimal human effort. We have demonstrated the effective-ness of our approach in creating a high-quality dataset thatenables training models for multimodal multihop questionanswering. Our experiments have shown that the synthesizeddataset allows models to achieve higher performance thanwhen trained on real data collected by humans, with signifi-cant improvements in test scores. These results highlight thepotential of our approach to improve the state-of-the-art inmultimodal multihop question answering . Furthermore, ourmethodology provides a scalable and efficient solution forgenerating high-quality data, which is essential for traininglarge language models. For future work, we plan to syn-thesize MMQA samples using sources beyond Wikipedia,including multilingual content, code snippets, videos, andother diverse information type. Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadal-lah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, ArashBakhtiari, Jianmin Bao, and Harkirat et al. Behl. Phi-3 techni-cal report: A highly capable language model locally on yourphone. arXiv preprint arXiv:2404.14219, 2024. 12 Abhinav Jauhri Abhimanyu Dubey, Abhinav Pandey, Ab-hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, AkhilMathur, Alan Schelten, Amy Yang, Angela Fan, AnirudhGoyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, ArchieSravankumar, and Artem Korenev et al. The llama 3 herd ofmodels, 2024. 4 Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant,Matthew R Gormley, and Graham Neubig. In-context learn-ing with long-context models: An in-depth exploration. arXivpreprint arXiv:2405.00200, 2024. 8 Andrea Burns, Krishna Srinivasan, Joshua Ainslie, GeoffBrown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, andMandy Guo.A suite of generative tasks for multi-levelmultimodal webpage understanding. In The 2023 Confer-ence on Empirical Methods in Natural Language Processing(EMNLP), 2023. 3",
  "Yinghsan Chang, Mridu Narang, Hisami Suzuki, GuihongCao, Jianfeng Gao, and Yonatan Bisk. WebQA: Multihop andMultimodal QA. 2021. 2, 5": "Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, ChunyuanLi, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Languagemodels are visual reasoning coordinators. In ICLR 2023Workshop on Mathematical and Empirical Understanding ofFoundation Models, 2023. 1 Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang,Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, DahuaLin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024.2 Mingda Chen, Xilun Chen, and Wen-tau Yih. Few-shot datasynthesis for open domain multi-hop question answering. InProceedings of the 18th Conference of the European Chapterof the Association for Computational Linguistics (Volume1: Long Papers), pages 190208, St. Julians, Malta, 2024.Association for Computational Linguistics. 2 Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and WilliamCohen. MuRAG: Multimodal retrieval-augmented generatorfor open question answering over images and text. In Pro-ceedings of the 2022 Conference on Empirical Methods inNatural Language Processing, pages 55585570, Abu Dhabi,United Arab Emirates, 2022. Association for ComputationalLinguistics. 5 Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-ravit Changpinyo, Alan Ritter, and Ming-Wei Chang.Can pre-trained vision and language models answer vi-sual information-seeking questions?arXiv preprintarXiv:2302.11713, 2023. 2 Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and JifengDai.Internvl: Scaling up vision foundation models andaligning for generic visual-linguistic tasks. arXiv preprintarXiv:2312.14238, 2023. 5 Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, ZhangweiGao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo,Zheng Ma, et al. How far are we to gpt-4v? closing the gapto commercial multimodal models with open-source suites.arXiv preprint arXiv:2404.16821, 2024. 5 Xinya Du and Claire Cardie. Harvesting paragraph-levelquestion-answer pairs from Wikipedia. In Proceedings of the56th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 19071917, Mel-bourne, Australia, 2018. Association for Computational Lin-guistics. 2 Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caim-ing Xiong. QAFactEval: Improved QA-based factual con-sistency evaluation for summarization. In Proceedings ofthe 2022 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human LanguageTechnologies, pages 25872601, Seattle, United States, 2022.Association for Computational Linguistics. 4",
  "NLP with synthetic text. Transactions of the Association forComputational Linguistics, 10:826842, 2022. 2": "Zhi Hong, Aswathy Ajith, James Pauloski, Eamon Duede,Kyle Chard, and Ian Foster.The diminishing returns ofmasked language models to science. In Findings of the As-sociation for Computational Linguistics: ACL 2023, pages12701283, Toronto, Canada, 2023. Association for Compu-tational Linguistics. 6 Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang,Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and JingrenZhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, 2024. 12 Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.LoRA: Low-rank adaptation of large language models. InInternational Conference on Learning Representations, 2022.12",
  "Jie Huang and Kevin Chen-Chuan Chang. Towards reasoningin large language models: A survey, 2023. 1": "Botian Jiang, Lei Li, Xiaonan Li, Zhaowei Li, Xiachong Feng,Lingpeng Kong, Qi Liu, and Xipeng Qiu. Understanding therole of llms in multimodal evaluation benchmarks. arXivpreprint arXiv:2410.12329, 2024. 2 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom BBrown, Benjamin Chess, Rewon Child, Scott Gray, AlecRadford, Jeffrey Wu, and Dario Amodei. Scaling laws forneural language models. arXiv preprint arXiv:2001.08361,2020. 8 Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bek-man, Amanpreet Singh, Anton Lozhkov, Thomas Wang,Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela,Matthieu Cord, and Victor Sanh. Obelics: An open web-scalefiltered dataset of interleaved image-text documents, 2023. 5",
  "Pratyush Maini, Skyler Seto, Richard Bai, David Grangier,Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: A": "recipe for compute and data-efficient language modeling.In Proceedings of the 62nd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: Long Papers),pages 1404414072, Bangkok, Thailand, 2024. Associationfor Computational Linguistics. 6 Shivam Mehta, Anna Deichler, Jim Oregan, Birger Moell,Jonas Beskow, Gustav Eje Henter, and Simon Alexanderson.Fake it to make it: Using synthetic data to remedy the datashortage in joint multimodal speech-and-gesture synthesis.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 19521964, 2024. 2 OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anad-kat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Bal-com, Paul Baltescu, Haiming Bao, Mohammad Bavarian, andJeff Belgum et al. Gpt-4 technical report, 2024. 3 Pranav Rajpurkar, Robin Jia, and Percy Liang. Know whatyou dont know: Unanswerable questions for SQuAD. InProceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers), pages784789, Melbourne, Australia, 2018. Association for Com-putational Linguistics. 4 Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, YizhongWang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, andJonathan Berant. Multimodal{qa}: complex question answer-ing over text, tables and images. In International Conferenceon Learning Representations, 2021. 1, 2, 3, 5",
  "Atula Tejaswi, Yoonsang Lee, Sujay Sanghavi, and EunsolChoi. Rare: Retrieval augmented retrieval with in-contextexamples, 2024. 1": "Ken Tsui. AnyClassifier, 2024. 2 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou.Chain of thought prompting elicits reasoning in large lan-guage models. In Advances in Neural Information ProcessingSystems, 2022. 1 Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosen-berg, Sina Khoshfetrat Pakazad, Tongshuang Wu, and Gra-ham Neubig. Synthetic multimodal question generation. InFindings of the Association for Computational Linguistics:EMNLP 2024, pages 1296012993, Miami, Florida, USA,2024. Association for Computational Linguistics. 3",
  "A. Prompts": "In our data generation pipeline, FM2DS, which incorporatesLVLMs, we carefully designed prompts to guide the modelthrough tasks involving cross-modal reasoning and data syn-thesis. Each prompt was carefully designed with specificelements to ensure precision, clarity, and completeness inachieving the tasks objectives, while also minimizing theneed for error correction during the evaluation process. Inthe following sections, we outline the rationale behind thestructure and components of these prompts.",
  "Question Generation Prompt": "Generate a multi-hop questionbased on the provided information.A multi-hop question requiresthe model to utilize informationfrom all available documentsin combination to reach thecorrect answer.Specifically,the question should be designed tobe unanswerable if any one of thedocuments is missing.Furthermore,focus on creating questions thatcompel the model to extract andsynthesize relevant informationacross multiple modalities|such asimages and text.This means thatanswering the question correctlywill demand integrating insightsfrom each source and modality,making it impossible to arrive atan accurate answer using any singledocument or modality alone.",
  "A.1. Question Generation": "Prompt 1 show the prompt used for question generation.Using this prompt, we ask the model to create multi-hopquestions that require information from all provided docu-ments and modalities (e.g., text and images) to answer. Thekey aim is to design questions that are unanswerable if anyone document or one modality is given, promoting the needfor multi-document and multimodal reasoning. It ensuresthe model generates questions that require synthesizing in-formation from diverse sources to form a comprehensiveunderstanding. To avoid duplicate data generation, if thegenerated question was already present in the dataset, wereused the same prompt but included the previously gener-ated questions from the same set of documents. The modelwas then instructed to generate a new, unique question.",
  "A.2. Answer Generation": "The prompt for answer generation directs the model to an-alyze multiple documents, encompassing both text and im-ages, to address the given question. It emphasizes integratingand synthesizing information from all sources to deliver themost accurate and comprehensive response. The prompt en- sures that the model considers all modalities and documentswithout relying solely on a single source or the models pre-trained knowledge, focusing exclusively on the provided ma-terials. Refer to Prompt 2 for the answer-generation prompt.",
  "A.3. Query Generation": "As illustrated in Prompt 3, in query generation, the modelis tasked with explaining the step-by-step process used toextract relevant information from the documents and deter-mine the answer based on the extracted snippets. This taskemphasizes transparency by requiring the model to identifythe relevant sections of each document and describe howinformation from multiple sources is combined to arrive atthe correct answer, promoting accountability in the modelsreasoning process.",
  "B. Experimental Settings": "In this work, we conducted experiments on a cluster of 8NVIDIA H100 80GB GPUs. The distributed setup allowedus to efficiently scale our fine-tuning process across multi-ple devices. The fine-tuning process was carried out usinglow-rank adaptation (LoRA) , a technique for efficient adaptation of pretrained models with low-rank matrices, re-ducing the number of trainable parameters. The key hy-perparameters used in the fine-tuning procedure include alearning rate of 1e-4, a batch size of 8 per device (totaling64 across 8 devices), LoRA rank set to 8, LoRA alpha set to32, a weight decay of 0.01, and the number of epochs was5. Additionally, AdamW optimizer was used with 1 = 0.9,2 = 0.98, and = 1e 8. The models were fine-tunedusing mixed-precision training to take full advantage of the80GB memory on each H100 GPU. For inference time, weset the temperature to 0.7, which strikes a balance betweenrandomness and coherence in the models responses, pro-ducing more varied outputs without sacrificing too muchquality. This setup ensured efficient usage of computationalresources while maintaining high model performance.",
  "C. Investigating FM2DS on Other Models": "In addition to the models discussed in , we exploredother model families, including Idefics3 , mPLUG-DocOwl-1.5 , and Phi-3.5-Vision-Instruct , as wellas larger versions within the explored families presented in. The results in demonstrate the reliabilityof our data synthesis approach, which consistently enhancesmodel performance across all models and sizes compared toan equivalent number of real samples.As shows, within the same model architecture,as the number of parameters increases and the model com-plexity grows (e.g., InternVL-2), the performance generallyimproves, including the pre-trained version. These mod-els also exhibit more effective learning, especially whenprovided with synthesized data generated by FM2DS, whichmakes the learning process more efficient. Moreover, Idefics-3 shows notable improvement over its predecessor, Idefics-2,indicating that the newer version has a better visual reason-ing. When comparing mPLUG-DocOwl-1.5 with modelslike InternVL-2, Idefics-2, and Idefics-3, it demonstratesrelatively lower performance. This could be attributed to thetraining objective of mPLUG-DocOwl-1.5, which focuseson multi-grained text recognition and parsing, potentiallyresulting in weaker performance when visual reasoning isrequired. Nevertheless, this model still outperforms LLaVA-1.6-7B overall, which might be due to the simpler structureof the LLaVA-1.6 family. Finally, Phi-3.5-Vision-Instruct,despite having fewer parameters compared to other mod-els, performs competitively with other models and surpassesLLaVA-1.6-7B in performance.",
  "E. M2QA-Bench Samples": "FM2DS uses LVLMs to generate multimodal and multihopquestions based on the given documents and evaluate theiranswers. These samples aim to emulate few-shot examplestypically provided to guide the models behavior in a struc-tured and relevant manner.In some cases, the questions focus on understanding factsfrom different modalitiessuch as images, text, and ta-bleswithin the grouped documents and finding the answerfrom one of them. For example, in the case of the questionshown in :",
  "How many people died in the event shown in thephotograph Raising the Flag on Iwo Jima fromthe country shown in the picture?": ". The custom evaluation application used for human evaluation. The application presents each participant with a randomly selectedquestion, relevant Wikipedia pages, and two model-generated answers labeled as Answer A and Answer B. One answer is generated bythe pipeline with validation, while the other comes from the pipeline without it. Participants are asked to choose the correct answer andoptionally provide feedback on their choice. To minimize bias, the application randomizes the position of each models answer. Information Pieces Used by LLM ForQuestion Generation and Answering Image From \"Raising the Flag on Iwo Jima\" Document Text From \"Battle of Iwo Jima\"",
  "Document": "Which album was released rst: the one featuringa famous photograph of a man bending on apiano or the album that includes the song \"I Don'tWanna Be a Soldier\"? The album featuring a famous photograph of aman bending on a piano is \"Music from Big Pink\"by The Band, released in 1968. The album thatincludes the song \"I Don't Wanna Be a Soldier\" is\"Imagine\" by John Lennon, released in 1971.Therefore, \"Music from Big Pink\" was releasedrst. Query:(1) Image from Music from Big Pink documentshows Music from Big Pink is the album thatincludes an image of a man bending on a piano.(2) Text From Music From Ping document showsthe album was released in 1968. (3) Text from Imagine document shows the TheBand album album was released in 1971 with IDon't want to be soldier. Recording for the album started on 24 May atAscot Sound Studios. The first songs recorded were \"It's So Hard\" and \"I Don'tWant to Be a Soldier\" in February 1971 atAscot Sound Studios, during sessions forLennon's single \"Power to the People\". Acover of the Olympics' 1958 song \"Well (BabyPlease Don't Go)\", later released on JohnLennon Anthology, was recorded on 16February. Lennon chose to remake \"I Don'tWant to Be a Soldier\" on 24 May 1971, theopening day of the main album sessions. Recording for the album started on 24 May atAscot Sound Studios. The first songs recorded were \"It's So Hard\" and \"I Don'tWant to Be a Soldier\" in February 1971 atAscot Sound Studios, during sessions forLennon's single \"Power to the People\". Acover of the Olympics' 1958 song \"Well (BabyPlease Don't Go)\", later released on JohnLennon Anthology, was recorded on 16February. Lennon chose to remake \"I Don'tWant to Be a Soldier\" on 24 May 1971, theopening day of the main album sessions. . Multimodal multihop reasoning example from M2QA-Bench where the model compares the release dates of two albums, Musicfrom Big Pink and Imagine, using textual and visual cues. The documents are connected through their shared topic, music, and theanswer is determined as the title of the earlier-released album.",
  "InternVL-2-26BSynth0.610.540.870.740.75InternVL-2-40BSynth0.640.50.890.80.78InternVL-2-76BSynth0.720.280.980.950.92": ". Performance comparison of different model families fine-tuned on real and synthesized data on M2QA-Bench. The ratios forEM scores and hallucination were calculated from filtered data (e.g., hallucination as the proportion of hallucinated responses to incorrectanswers). In the table, indicates that larger values are better (all EM values), while indicates that smaller values are better (hallucinationrate). The scores for EM (Table) and EM (Image) may include samples that also contain other modalities. Larger models and those fine-tunedon synthesized data generally show improved performance with reduced hallucination rates. In M2QA-Bench, models demonstrate higherperformance on questions involving table modality compared to those involving images. LVLM is tasked with combining information from two docu-ments: Raising the Flag on Iwo Jima and Battle of Iwo Jima.Here, the hyperlink between the two documents served asthe connection between two docments. The model identifiesthat the photograph depicts American soldiers (based on theUSA flag) and cross-references the table from the Battle ofIwo Jima document to determine that 539 people from theUSA were killed. This demonstrates how the model syn-thesizes information across modalities to form an accurateresponse. Afterward, the model generates queries, serving as a step-by-step guide to extract relevant information fromthe documents. Using the extracted snippets, it then answersthe question. For instance, the model would need to locatethe image Raising the Flag on Iwo Jima to determine thecountry mentioned in the question, which is the USA. Next,by referencing the table in the Battle of Iwo Jima document,it provides the final answer.",
  "InternVL-2-26BSynth0.70.370.790.850.8InternVL-2-40BSynth0.740.350.850.890.87InternVL-2-76BSynth0.80.150.930.940.93": ". Performance comparison of different model families fine-tuned on real and synthesized data on M2QA-Bench. The ratios for EMscores and hallucination were calculated from filtered data (EM(Table) refers to the EM score calculated on samples that include the tablemodality). In the table, indicates that larger values are better (all EM values), while indicates that smaller values are better (hallucinationrate). The scores for EM (Table) and EM (Image) may include samples that also contain other modalities. Larger models and those fine-tunedon synthesized data generally exhibit improved performance with reduced hallucination rates. In MultiModalQA, models demonstrate higherperformance on questions involving the image modality compared to those involving tables.",
  "Which album was released first: the one featuringa famous photograph of a man bending on a pi-ano or the album that includes the song I DontWanna Be a Soldier?": "requires the model to compare temporal information acrosstwo documents: Music from Big Pink and Imagine. Themodel identifies that Music from Big Pink, featuring a pho-tograph of a man bending on a piano, was released in 1968,while Imagine, containing the song I Dont Wanna Be a Sol-dier, was released in 1971. Therefore, the answer is Musicfrom Big Pink. In this case, the documents were connectedthrough their shared topic, music. The query generationin this example is similar to the first but differs slightly, asthree information snippets are key to answering the question,making the query three steps long.",
  "F. Qualitative Analysis": "In the qualitative analysis, we compared three critical fac-tors influencing model responses: model architecture, fine-tuning (FT) dataset (real samples or synthesized samples),and model size. To examine the effects of model architectureand FT dataset, we used InternVL-2-8B, LLaVA-1.6-7B,and Idefics-2-8B, fine-tuning them on both real and syn-thetic data generated by FM2DS. For analyzing the impactof model size, all versions of InternVL-2 were trained on thesynthetic data. All of the mentioned models were fine-tunedon 5k samples.This analysis was conducted for 100 samples from eachof the following benchmarks: (1) M2QA-Bench, (2) Mul-tiModalQA, and (3) WebQA. The results are presented inTables 8, 9, and 10. The responses generated by differentmodels were analyzed across these datasets, focusing on thefollowing metrics: 1. Model accuracy using the exact match (EM) metric.2. Hallucination rate, corresponding to instances where themodel generated wrong answer based on its pre-trainedknowledge instead of the provided document.",
  ". Model accuracy with EM metric for samples includingtable modality (may include other modalities)": "5. Model accuracy with EM metric for samples includingboth image and table modalities.For WebQA, which only incorporates text and imagemodalities, the last three metrics were not applicable. Ad-ditionally, the distribution of modalities across samples forMultiModalQA and M2QA-Bench was as follows: M2QA-Bench: 66 samples included image modality, 62samples included table modality, and 28 samples includedboth image and table modalities. MultiModalQA: 61 samples included image modality, 54samples included table modality, and 15 samples includedboth image and table modalities.Overall, in all benchmarks, model hallucination rates de-creased as model complexity and parameter count increased,resulting in more accurate answers across all modalities (e.g.,see for an example output of these models). Largermodels consistently outperformed smaller models on bothmodalities. Regarding synthetic data, fine-tuning on datagenerated by FM2DS significantly reduced hallucination andimproved performance across all modalities. While the hal-lucination rates among different model families are relativelysimilar, all models occasionally generate answers based ontheir pre-trained knowledge rather than the provided docu-ment. Fine-tuning on data generated by FM2DS effectivelyalleviates this issue. Among the models, as shown in , LLaVA-1.6 exhibited the poorest performance and thehighest likelihood of hallucination, followed by Idefics-2,with InternVL-2 demonstrating the best performance.",
  "InternVL-2-26BSynth0.810.21InternVL-2-40BSynth0.820.11InternVL-2-76BSynth0.850": ". Overall performance of different model families fine-tuned on real and synthesized data on WebQA. Hallucination is theproportion of hallucinated responses to incorrect answers. In thistable, indicates that larger values are better, while indicates thatsmaller values are better. Fine-tuning on synthesized data consis-tently reduces hallucination rates and improves EM scores acrossall models, with larger models achieving the best performance.Unlike M2QA-Bench and MultModalQA, WebQA only includesimage and text modality, as a result no EM(Image) and EM(Table)are reported.",
  "Regarding the effect of modalities, results from Tables": "8 and 9 suggest that the modalities themselves are not themost critical factor. Instead, the complexity of how thequestion integrates the modalities plays a more significantrole. For M2QA-Bench, models performed better when vi-sual understanding was not required, with tables and textbeing the primary contributors to the results. In contrast, forMultiModalQA, models tended to perform better on image-based questions, highlighting the importance of how thequestion leverages the modalities. For questions involvingboth modalities, smaller models struggled more to producecorrect answers, while larger models performed better interms of EM. It is important to note, however, that due to thesubstantial difference in the number of samples containingboth image and table modalities compared to those with onlyone modality, the reported results are not directly compara-ble. Refer to Figures 9 and 10 for the outputs of differentmodel families fine-tuned on either real or synthesized data.",
  "Documents": "The most important work ofQin Shi Huang is the Great Wall of China. Inside of a watchtower in the Great Wall of China. InternVL-2-8B In the most important project of Qin Shi Huang, what geometric shape was used in the watchtowers, when looking from inside? Model Answer: SquareNote: The model's response stated that watchtower windows can have variousshapes, with squares being the most common. Consequently, the model relied onthis knowledge and provided an incorrect answer because of not using thedocuments and hallucinating. Fine-tuned On FM2DS Data InternVL-2-26B Model Answer: RectangleNote: The model indicated that watchtower windows come in various shapes, withrectangular windows being the most typical. Based on this understanding, themodel generated an incorrect answer because of not using the documentsand hallucinating. InternVL-2-40B Model Answer: CircleNote: The model suggested that, based on images of the interiors of watchtowerson the Great Wall of China, the windows are circular due to their curved shape. Asa result, the model produced an incorrect answer. InternVL-2-76B Model Answer: ArchNote: The model provided a detailed description of the shape, stating that basedon the visible features in the image of the watchtower, the window has an archedshape. . Responses from InternVL-2 models of various sizes (8B, 26B, 40B, and 76B) to the question: In the most important project ofQin Shi Huang, what geometric shape was used in the watchtowers when viewed from inside? from M2QA-Bench illustrate that in exampleslike this, which requires detailed visual understanding, smaller models often hallucinate, providing inconsistent answers (e.g., square,rectangle) without grounding in the provided document. Larger models, however, perform better on this task and have less hallucination."
}