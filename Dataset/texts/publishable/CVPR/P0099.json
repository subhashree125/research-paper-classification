{
  "Abstract": "In this report, we present our approach for the Nat-ural Language Query track and Goal Step track of theEgo4D Episodic Memory Benchmark at CVPR 2024. Bothchallenges require the localization of actions within longvideo sequences using textual queries. To enhance local-ization accuracy, our method not only processes the tem-poral information of videos but also identifies fine-grainedobjects spatially within the frames. To this end, we intro-duce a novel approach, termed ObjectNLQ, which incor-porates an object branch to augment the video represen-tation with detailed object information, thereby improvinggrounding efficiency.ObjectNLQ achieves a mean R@1of 23.15, ranking 2nd in the Natural Language QueriesChallenge, and gains 33.00 in terms of the metric R@1,IoU=0.3, ranking 3rd in the Goal Step Challenge. Our codewill be released at",
  ". Introduction": "In the Ego4D Natural Language Query (NLQ) Chal-lenge, participants are provided with an egocentric videoand a natural language question. The objective is to accu-rately localize the video segment that contains the answer tothe question . Conversely, the Goal Step Challenge requires localizing a video segment that corresponds toa natural language description of the step. Both challengesnecessitate precise identification of relevant video contentbased on textual queries.Existing methods have made significant advancementsin several key areas: 1) Developing a robust feature ex-traction backbone through pretraining . These meth-ods finetune a pre-trained multimodal video model on theEgo4D dataset to enhance feature representation for ego-centric videos.2) Implementing efficient data augmen-tation strategies to expand the scale of the dataset .This enhances the performance of downstream models by",
  ". An illustration of object extraction": "effectively increasing data variability.3) Designing tai-lored grounding models for the NLQ task . Some ap-proaches optimize the framework by reducing the numberof frames processed, thereby streamlining video momentlocalization. Others leverage the sophisticated archi-tectures of point prediction and feature pyramids from Ac-tionFormer to process the entire video directly, aimingfor more precise localization.Among the above three methods, the extraction of videofeatures is crucial, and the quality of feature extraction willdirectly determine whether the model can recognize the spe-cific information in the video. However, existing pretrainingmethods commonly finetune the backbone using contrastivelearning between captions and video clips, which means thevideo features depend entirely on the granularity and focusof caption information. Moreover, most of the captions inthe training data focus on the behavior of human, while the",
  "NLQ task concentrates on building an AI helper for findingthings that people often neglect. This mismatch leads to thelack of fine-grained object information in the input": "Based on this observation, we propose a novel methodfor object extraction and design a novel grounding modelthat can fully use the object annotation. Through an objectdetection model, we obtain the fine-grained object informa-tion in the image. Then we use a parallel attention mecha-nism to extract object information and add it to video framefeatures, to reasonably complete the NLQ task.",
  ". Feature Extraction": "Following , we employ a concatenated feature compris-ing InternVideo and EgoVLP for video represen-tations, while utilizing CLIP (ViT-L/14) for textualfeature extraction. For object representation, as illustratedin , we utilize the Co-DETR object detector,which is pretrained on the LVIS v1.0 dataset , to extractobject annotations from the video. Subsequently, we em-ploy CLIP (ViT-L/14) to extract the textual features ofthe object classes that correspond to the given query.",
  ". Structure": "As depicted in , our grounding model incorporatesobject information1 and consists of several key components:a text encoder, an object encoder, a multi-modal encoder, amulti-scale encoder, and a prediction head. In alignmentwith , we employ the same text encoder, multi-scaletransformer encoder, and prediction heads.To improve the comprehension of fine-grained object in-formation, we have meticulously designed an object en-coder. This encoder is tailored to filter and highlight objectinformation relevant to the query. Additionally, we integratean object branch into the multi-modal encoder, thereby sig-nificantly enhancing the representation of video features. Object Encoder.Our object encoder comprises fourtransformer encoder blocks.Each block features a textcross-attention layer followed by a feed-forward net-work (FFN). In this architecture, the object feature acts asthe query in the text cross-attention layer, with the text fea-ture serving as both the key and value. This setup enablesour model to thoroughly extract and utilize object data per-tinent to the specific query, effectively minimizing confu-sion about objects from different categories within the sameframe.",
  "ObjectNLQ227.0219.2823.1543.6630.87": "text branch, an object branch, and a gate fusion module.Within each branch, a cross-attention layer and an FFN areemployed. Subsequently, the outputs from the two branchesare combined using a gate fusion technique.Specifically, the left branch aligns with the design of themulti-modal encoder proposed in , where the text fea-ture acts as both key and value. The right branch, how-ever, utilizes the video feature as the query while employ-ing the object feature as the key and value. The integra-tion is achieved through a gate fusion process, where theweights for merging are computed by a multi-layer percep-tron (MLP). This innovative design seamlessly combinesobject data with video features, maintaining the integrityof the original interactions between text and video content,thus enhancing the overall models capability to interpretand utilize multimodal data.",
  ". Implementation Details": "Video clip features are extracted with a stride of 16 frames,and the dimension is set to 384. We utilize four heads formulti-head attention. During training, configurations dif-fer between tasks: for the NLQ task, we use a mini-batchsize of 4 and a learning rate of 1e-4; for the Goal Step task,the mini-batch size is increased to 8 with a learning rate of2e-4. We employ a cosine decay strategy for learning rateadjustment, with a warm-up period of four epochs and atotal training duration of ten epochs. The training for theNLQ task is conducted on a single L20 GPU over approx-imately four hours, while the Goal Step task requires fourL20 GPUs for around ten hours of training. For model ini-tialization, we use pretrained parameters to ensure a robuststarting point. The original parameters of GroundNLQ areinitialized with weights from a model pretrained on narra-tion data, which has been augmented using the strategy of",
  "ObjectNLQ333.0026.37": ". Within the object encoder, the text cross-attention lay-ers and the FFN are initialized using the MHA layer and theFFN from the text encoder. Similarly, the object branch inthe multi-modal encoder is initialized using the parametersfrom the text branch, ensuring consistency and leveragingpretrained efficiencies across the model architecture.To maximize the use of available data, both the trainingand validation splits are divided into five folds. This datais used to train multiple models for an ensemble approach,enhancing robustness and accuracy. During inference, Soft-NMS is employed for deduplication to improve the preci-sion of localization .",
  ". Performance Comparison": "presents the leaderboard results of the NLQ Chal-lenge. Our ensemble method achieves a mean R@1 scoreof 23.15%, which is 1.22% higher than the 2023 championand secured us a 2nd-place ranking. details theleaderboard for the Goal Step Challenge, where our ensem-ble method attains an R@1 score of 33.00% at an IoU of0.3. This score is 13.96% points higher than the baselineand placed us third. These results underscore the effective-ness of our method in both challenges.",
  ". Ablation Study": "In our ablation experiments conducted on the validationsplit, the results are systematically detailed in toassess various components of our model: 1) At the top of, we examine modifications to the object encoder. Inthis variant, we build the object encoder with an additionalself MHA layer (SA), a text cross-attention layer (CA), andan FFN. Interestingly, the performance slightly decreases,",
  ". Two examples of ObjectNLQ on the validation set ofNLQ": "suggesting that excessive associations among objects withinthe same frame might be detrimental. 2) At the second lineof , we explore the design of the video encoder. Theablation setup, Sequential Object Attention (SOA), consistsof an MHA layer, followed by a text cross-attention layer,an object cross-attention layer, and an FFN. The observeddecline in performance with this configuration reinforcesthe efficacy of our original video encoder design, indicat-ing that simpler sequences may be more effective. 3) Atthe third line of , we ablate the use of a Gaussianweight on the loss function. The slight decrease in per-formance upon removing this feature confirms the utilityof Action Sensitivity Learning (ASL) , which enhancesthe models focus on relevant aspects of the data for im-proved performance.",
  ". Two examples of ObjectNLQ on the validation set ofGoal Step": "multaneously process added object information and inher-ent video action details, clearly highlighting the advantagesof our architectural design. Conversely, b presentsa case where our model did not perform as expected. Whileit correctly identifies the object in the scene, it fails to accu-rately localize the answer, particularly because the groundtruth required the drill machine to be centered in the frame.This failure suggests that the model may not adequatelyunderstand or prioritize human attention cues, indicating apotential area for further refinement to enhance its perfor-mance in complex scenarios.a shows a successful example in Goal Step.Our model exactly locates the target of the text descrip-tion, which contains a sequence of actions including takingmilk from a fridge, pouring milk into a container, and heat-ing milk with a microwave. This case reflects our modelsability to identify the location intent as well as granularity.However, an example of underperformance is demonstratedin b. Our model incorrectly adds the removal ofmilk from the refrigerator to the localization results, result-ing in inaccurate localization. This reflects that our modelmay not be able to fully understand the semantics of fine-grained actions, and improving the models ability to un-derstand and segment long action sequences may furtherimprove its performance.",
  "In this report, we present our approach for the NLQ trackand Goal Step track of the Ego4D Challenge at CVPR 2024.Our primary contribution lies in leveraging existing ob-": "ject detection models to extract detailed object informationfrom video frames and integrating this data into an effec-tive model framework to enrich video representation withthis additional object detail. Employing fine-grained ob-ject features, our method improves upon the foundationalGroundNLQ model in the NLQ Challenge, demonstratingenhanced performance. Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li,Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, BingkunHuang, et al. Internvideo-ego4d: A pack of champion solu-tions to ego4d challenges. arXiv preprint arXiv:2211.09529,2022. 1, 2 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1899519012, 2022. 1 Weili Guan, Xuemeng Song, Haoyu Zhang, Meng Liu,Chung-Hsing Yeh, and Xiaojun Chang. Bi-directional het-erogeneous graph hashing towards efficient outfit recom-mendation. In Proceedings of the 30th ACM InternationalConference on Multimedia, page 268276. Association forComputing Machinery, 2022. 3 Agrim Gupta, Piotr Dollar, and Ross Girshick.Lvis: Adataset for large vocabulary instance segmentation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 53565364, 2019. 2 Zhijian Hou, Lei Ji, Difei Gao, Wanjun Zhong, Kun Yan,Chao Li, Wing-Kwong Chan, Chong-Wah Ngo, Nan Duan,and Mike Zheng Shou. Groundnlq@ ego4d natural languagequeries challenge 2023. arXiv preprint arXiv:2306.15255,2023. 1, 2, 3 Zhijian Hou, Wanjun Zhong, Lei Ji, Difei Gao, Kun Yan, WkChan, Chong-Wah Ngo, Mike Zheng Shou, and Nan Duan.CONE: An efficient coarse-to-fiNE alignment framework forlong video temporal grounding. In Proceedings of the 61stAnnual Meeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 80138028, 2023. Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan,Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao,Rongcheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentricvideo-language pretraining@ ego4d challenge 2022. arXivpreprint arXiv:2207.01622, 2022. 1, 2",
  "Chen-Lin Zhang, Jianxin Wu, and Yin Li. ActionFormer:Localizing moments of actions with transformers. In Euro-pean Conference on Computer Vision, pages 492510, 2022.1": "Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.Span-based localizing network for natural language video lo-calization. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pages 65436554, 2020. 3 Haoyu Zhang, Meng Liu, Zan Gao, Xiaoqiang Lei, YinglongWang, and Liqiang Nie. Multimodal dialog system: Rela-tional graph-based context-aware question understanding. InProceedings of the 29th ACM International Conference onMultimedia, page 695703. Association for Computing Ma-chinery, 2021. 1 Haoyu Zhang, Meng Liu, Yuhong Li, Ming Yan, Zan Gao,Xiaojun Chang, and Liqiang Nie. Attribute-guided collab-orative learning for partial person re-identification.IEEETransactions on Pattern Analysis and Machine Intelligence,45(12):1414414160, 2023. 2 Haoyu Zhang, Meng Liu, Yaowei Wang, Da Cao, WeiliGuan, and Liqiang Nie.Uncovering hidden connections:Iterative tracking and reasoning for video-grounded dialog.arXiv preprint arXiv:2310.07259, 2023. 4 Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, YaoweiWang, and Liqiang Nie. Multi-factor adaptive vision selec-tion for egocentric video question answering. In Proceedingsof the 41st International Conference on Machine Learning,pages 5931059328. PMLR, 2024. 1"
}