{
  "Abstract": "Diffusion models have shown tremendous results in im-age generation. However, due to the iterative nature of thediffusion process and its reliance on classifier-free guid-ance, inference times are slow.In this paper, we pro-pose a new distillation approach for guided diffusion mod-els in which an external lightweight guide model is trainedwhile the original text-to-image model remains frozen.Weshow that our method reduces the inference computation ofclassifier-free guided latent-space diffusion models by al-most half, and only requires 1% trainable parameters of thebase model. Furthermore, once trained, our guide modelcan be applied to various fine-tuned, domain-specific ver-sions of the base diffusion model without the need for addi-tional training: this plug-and-play functionality drasti-cally improves inference computation while maintaining thevisual fidelity of generated images. Empirically, we showthat our approach is able to produce visually appealing re-sults and achieve a comparable FID score to the teacherwith as few as 8 to 16 steps. project page",
  ". Introduction": "Diffusion models represent a novel categoryof generative models that have shown remarkable perfor-mance on a variety of established benchmarks in genera-tive modeling. Specifically, conditional diffusion models emerged with significantly improved sample quality byclassifier-free guidance (CFG) .However, the sampling speed of diffusion models standsout as a significant obstacle to their adoption in practicalscenarios . Specifically, the process of iteratively re-ducing noise in images typically requires a considerablenumber of iterations, posing challenges for efficient execu-tion. For example, even when using widely adopted state-of-the-art diffusion models such as Stable Diffusion ,more than 20 denoising steps are required to generate high-quality images.Moreover, when applying classifier-free",
  ". We trained a guide model to replace classifier-free guid-ance that can be plug-and-play to other base models with differentdomains": "guidance, two forward passes one for the conditionedand another for the unconditioned diffusion model areneeded per denoising step, further increasing the computa-tional cost.One standard approach to address the speed issues isthrough distillation, where a student model, initialized withthe weights of the teacher diffusion model, is trained toregress to the output of the teacher that runs for multipledenoising steps . However, the standard diffusiondistillation approach has the following limitations. First,the number of trainable parameters of the student model isthe same (or comparable) as that of the teacher diffusionmodel. But recent state-of-the-art diffusion models such asImagen , eDiff-I , and SDXL often have billionsof parameters, and distilling these large models requires atremendous amount of computation. Second, it has beenshown that diffusion models can be finetuned to differentdomains. When finetuned on a collection of customizedimages, diffusion models can be adapted to generate con-tent with novel structures and aesthetic styles. When fine-tuned with only a few images, prior work has shown thatnovel concepts can be learned . However, with standarddistillation on the base model, these finetuned models areno longer applicable. Re-training on the distilled student",
  ". Applying our trained guide model to different fine-tuned latent diffusion models (LDM)": "model is required for all the domains of interest.In this paper, we propose a plug-and-play distillation ap-proach to address these issues. Specifically, we introducea novel type of distillation that makes the parameters ofthe base model remain untouched: we propose an exter-nal guide model with a lightweight architecture that injectsfeature maps to enable the diffusion model to generate text-conditioned images on one path.We first experiment with distilling CFG into one forwardpass, which effectively reduces the inference FLOP countsby 32%. We further study different architectural choicesof the lightweight module and show that the proposed ar-chitecture is around 1% of the parameters of the base, thuseffectively halving the inference FLOP counts. Finally, weexperiment with the generalizability of the plug-and-playmodule. Once our lightweight guided module is trained, itcan be readily integrated with existing finetuned diffusionmodels, requiring minimal to no further training.In summary, our approach has the following advantages: Low computational cost for training: The parametersrequired to learn from the distillation approach is only1% (42% for the Full guide model) of the diffusion model,making the computational cost for training very low com-pared to other distillation methods.",
  ". Reducing inference time in diffusion models": "To reduce the expensive computational cost in inferencetime of diffusion models, previous papers have attemptedto improve the sampling speed of diffusion models.One of the straightforward ways is designing accurateODE samplers . For example, Denoising Dif-fusion Implicit Models (DDIM) uses first-order Eulersmethod which enables to reduce the inference timesteps.On the other hand, there are previous works that attemptto incorporate distillation techniques to improve model in-ference efficiency. Distillation in deep learning refers to aprocess where a larger and more complex model (referredto as the teacher model) is employed to train a smallerand simpler model (the student model). The goal of dis-tillation is to transfer the knowledge and information cap-tured by the teacher model to the student model, enablingthe student model to achieve similar performance with re-duced complexity and computational requirements. Golnariet al. proposed optimizing specific denoising steps byrestricting noise computation to conditional noise and elim-inating unconditional noise computation, thus reducing the",
  "!": ". The overview of CFG distillation. Instead of using two feed-forward pass and classifier-free guidance, we train a student modelconditioned with the guidance, cross-attention, and time embedding to predict the output image with only one forward pass. complexity of target iterations. Salimans and Ho andMeng et al. distilled the model to achieve fewer sam-pling steps. However, these methods only focus on reducinginference timesteps and require progressive model distilla-tion, which may require more time and computing resourcesto train these models.Recently, LCM-LoRA proposed a plug-and-playdistillation approach utilizing LoRA, which has garneredsignificant attention. However, their method has the draw-back of requiring double computation due to the use ofclassifier-free guidance. Our work, completed contempo-raneously and accepted by CVPR, addresses similar chal-lenges. We acknowledge the impact and relevance of theircontribution.CoDi, a concurrent work presented at CVPR, excelsin producing high-quality images with very few steps (e.g.,1-4) across multiple tasks, including super-resolution, text-guided image editing, and depth-to-image generation. Weacknowledge their valuable contribution.",
  ". Controlling diffusion models": "Many researchers in the field of diffusion models havedemonstrated the ability to control models using methodsbeyond just text input. Notably, the use of external modelsto inject features into diffusion models has yielded impres-sive results . For instance, ControlNet proposed an external model that uses images, skeletons,edge maps, etc., as conditions to generate correspondingimages. GLIGEN successfully created desired objectswithin specific bounding boxes. IP-adaptor introduceda method for generating images similar to a given image condition. These approaches all successfully manipulatedimages by injecting values into features through externalmodels. However, these methods have focused on condi-tional image generation or editing, with no instances of ap-plying them to distillation.",
  "Et,,x[(t)||(xt) ||22](1)": "where (t) = (log2t /2t) is a pre-defined weightedfunction that takes into the signal-to-noise ratio t, whichdecreases monotonically with t. xt is a latent variable thatsatisfied x q(xt|x) = N(xt; tx, 2t I).After training the model , during the sampling stage,xt can be obtained by applying the SDE / ODE solver. Forexample, using DDIM:",
  "Classifier-free guidance proves to be a highly effectivestrategy for significantly enhancing the quality of samples": "in class-conditioned diffusion models. It adopted an uncon-ditioned class identifier as a substitute for a separate clas-sifier that is traditionally required to create a Gaussian dis-tribution tailored to a specific class. This approach findswidespread application in extensive diffusion models, in-cluding notable examples like DALLE2 , GLIDE ,and Stable Diffusion . In particular, Stable Diffusiondesigns the diffused forward and reverse process in the VAElatent space, z = E(x), x = D(z) where E and D denotethe VAE encoder and decoder. In the process of generatinga sample, classifier-free guidance carries out evaluations onboth conditional score estimates and unconditional score es-timates. Specifically, the computation of the noise sample(zt, c) follows the formulation",
  "(zt, c) = (1 + g)(zt, c) g(zt, ),(3)": "where is the score estimate function that is a parameter-ized neural network (U-Net). (zt, c) represents the text-conditioned term, while ,(zt, ) corresponds to the un-conditional term (null text). The parameter g stands for theguidance value that scales the perturbation. In this paper,we use the Stable Diffusions VAE latent and omit the nota-tion of Encoder and Decoder of VAE for brief.",
  ". Overview": "Inspired by ControlNet , we design the external guidenetwork for CFG distillation by using the guidance num-ber as the input condition. After the first stage of the dis-tillation (i.e. CFG distillation) has been accomplished, wefollow prior distillation techniques to reduce the samplingsteps. This is accomplished by enabling the model to pro-gressively learn how to halve the sampling steps . Thespecifics of the whole process will be elucidated in the fol-lowing sections.",
  "(zt, c; G(g, zt, c)) = (1 + g)(zt, c) g(zt, ) (4)": "where g is the guidance number, G is our student guidedmodel, (zt, ) is the unconditioned U-Net forward pass,and (zt, c) is the conditioned U-Net forward pass. Pre-cisely, G takes the guidance as the input hint, along withtime and text embedding and zt, then injects its output fea-ture maps to the decoder part of the original U-Net. The fea-ture map injection can be viewed as the guidance strengththat helps the U-Net to trade-off between sample quality anddiversity. The pseudo algorithm is listed in Algorithm 1.Typically, distillation involves initializing an entirelynew model that has the same structure as the teacher model",
  "end while": "and trying to make it learn the teachers output and updatethe parameters of the entire student network. Instead, weuse a small guide model on top of the teacher model, whichleads to reduced computational overhead during trainingbecause the number of parameters in the guide models isrelatively small compared to the whole U-Net. Also, thisapproach does not discard the teacher model after distilla-tion training, but uses the trained guide model along withthe teacher U-Net for faster inference without CFG. Thisfeature makes it applicable to plug-and-play to differenttypes of fine-tuned diffusion models directly without re-training the guide model G.",
  "In this section, we introduce two types of external guidemodel, full guide model and tiny guide model": "full guide modelControlNet is one of the well-designedexternal models for image control. When we regard the dis-tillation with an external guide model as the external con-trolling, the straightforward way is using the UNet architec-ture of diffusion model as the guide model. To align withthe original ControlNet architecture, our full guide modelbroadcasts the guidance number into a shape that is thesame as the hint size, e.g. (C, H, W). This straightfor-ward strategy enables the model to have high capacity. Themodel architecture is depicted in . tiny guide modelAlthough full guide model is already awell-designed guide model, this is not an efficient way be-cause there is not as much information needed to encodewith a simple guidance number. As such, we further sim-plify the standard ControlNet structure, tiny guide model,for our guidance-distillation framework:",
  ". Comparison of the full guide model architecture and thetiny architecture": "bedding will also pass through zero convolution layers, de-noted Z(, ). These elements are added together and passedthrough the zero convolutions in the decoding layer to getthe corresponding output of the guide model y. Zero con-volution architecture ensures that undesirable noise or irrel-evant features are not injected into the base model in theearly stage of the training. The tiny guide model simplifies the traditional Control-Net architecture by removing the encoder blocks as shownin . This design drastically reduces the number of pa-rameters as zt no longer needs to be encoded by the guidemodel.",
  ". Sampling steps distillation": "After training the guide model, G, we progressively distillit with fewer sampling steps required by incorporating withexisting sampling-step-based distillation methods . Toelaborate, under the discrete time-step scenario, let N standfor the original number of sampling steps, we trained a stu-dent model to the output of two-step DDIM sampling of theteacher in one step. Precisely, the initial sampler f(z; )maps a random noise to samples x requires N steps, isdistilled into a new sampler f(z; ) that requires N/2 steps.f(z; ) will become the new teacher so that we can learnanother sampler that requires N/4 steps. This procedurewill be repeated several times until the ideal sampling stepsneeded will be achieved. In this section, again, we onlylearn the parameters from the guide model G and fix thebase model (U-Net) throughout the distillation progress.The small size of the guide model enables the parametersto be learned quickly.",
  ". Experiments": "Distilling a diffusion model involves a balance betweenmaking the model generate images faster and maintaininggood quality. Initially, we assess the image fidelity of ourmodel through both qualitative and quantitative analyses,employing FID and CLIP scores. Subsequently,we evaluate the effectiveness of our approach across differ-ent domains with zero additional training. Finally, since ourapproach keeps the original model fixed, we can closely ex-amine latent feature maps from the guide model to betterunderstand how guidance is applied at different timestepsduring the diffusion process.",
  ". Setup": "We trained our model with LAION (512 512) dataset with the Stable Diffusion v1.5 as our score-estimationmodel. In the training stage, a randomly sampled guidancenumber g is broadcast into the shape of (C, H, W),which becomes the input of the guide model. For the tinyarchitecture, the input is a 1d-array with the length of Cthat passes through the zero modules along with timestepsand text embedding.We apply the -prediction modelthrough the whole experiment. Since Stable Diffusion v1.5is trained on 1000 steps, we sample images with 1000 stepsas our Teacher output for our guide model to learn. Weevaluate our methods with the COCO dataset . We com-pare our method with DDIM sampling and PLMS sampling. The FLOPs and number of parameters for ourmodels compared to the teacher classifier-gree guidance arelisted in . We see that our full guide model onlyneeds 0.67 of FLOPs of the teacher while our tiny modelonly computes 0.51 of FLOPs of the teacher model.",
  ". Qualitative and quantitative evaluation": "illustrates the qualitative comparison between stu-dent and teacher models on various text prompts with thesame initial noise. We see the quality of images generatedby our guide model is close to the generated images us-ing classifier-free-guidance while our approach can gener-ate images and nearly half the FLOP counts. A user studyassociated with images from teacher and student models can",
  "be found in the Appendix": "Furthermore, we generate images with fewer timestepson . We do not observe obvious quality degradationwhen decreasing our model steps to 16 and 8 with full guidemodel given a certain level of guidance (g = 8). On the otherhand, since tiny guide model has less capacity, its challeng-ing for the student to fully mimic the teachers output givena continuous guidance input during the sampling steps dis-tillation process (i.e. progressive distillation). We observethat the tiny guide model can achieve almost the same im-age quality as full guide model when the sampling steps arearound 50, but when the number of steps are reduced to 8,then the performance of the tiny model will degrade dras- tically. This can be partially addressed by training the tinymodel with fixed guidance. Furthermore, both of the mod-els can achieve comparable results compared to Classifier-Free Guidance (DDIM N 2 steps). The qualitative resultis shown in .",
  "t = Tt = 0t = Tt = 0": ". Visualizing the feature map injection from the guide model with guidance 8. The early stage of the iteration process has strongerinjection (larger absolute values) strength, and in the later stage, the injections mainly focus on high-frequency details with lower strength.Also, the lower guidance number has lower feature map injections, higher guidance number has stronger feature map injections. els without training. We focus on three different types offine-tuned stable diffusion v1.5 models: watercolor style,realistic style, and 3D cartoon style.Then, we directlyplug in our pretrained guide model G to these fine-tunedmodels to modify their outputs. We run the models with-out classifier-free guidance and pass the guidance value toour guide module. For other distillation approaches , itmay be necessary to distill a new model for each domain,which can be costly in terms of training and computation.Our approach removes this burden and make it easy to makedifferent models finetuned for different domains nearly twotimes efficiently with no additional cost. We validate ourapproach by measuring FID and CLIP scores in generatedimages in different domains. shows the FID scores and CLIP scores of the teacher CFG on these fine-tunedmodels versus the results of our tiny guide model injectionapproach. The generated pictures are sampled with 50 stepswith guidance 8. We see that FID and CLIP scores of ourtiny model which runs two times faster are comparable withCFG. In addition, qualitative results are shown in for the full guide model plug-ins and for tiny guidemodel plug-ins. The results indicate the great generalizabil-ity of our approach without needing to train the model fordifferent domains.",
  ". Plug in the tiny guide model with different fine-tuned U-Nets": "guide model G. To this end, we visualize the feature mapsat various stages of the iteration process and under differentguidance values. To the best of our knowledge, we are thefirst to visualize how classifier-free guidance emphasizesdifferent patches of image generation in different timesteps.We are able to study this due to the architecture choice ofour model that freezes the original model and adds the guidemodule as an additional component. By looking into featuremaps of our guide module, we are able to get a better un-derstanding of how classifier-free guidance impacts imagegeneration.For each layer of feature map injection, we computedthe mean across various channels for each pixel and appliednormalization. The number of DDIM steps used for sam-pling was 50. displays the feature map injections throughoutthe sampling process. The values indicate that the initialstages of the sampling process are more critical with respectto Classifier-Free Guidance (CFG), as this is when the pri-mary structure of the image is formed. In the middle stage,the main subjects of the image (e.g., a panda, bamboo) aremore important. Thus CFG continues to play a role in theseareas, while the background becomes less significant. Fi-",
  "nally, in the last stage of the sampling, the feature map in-jections mainly focus on detail refinement on the edges withlow strengths (i.e. values)": "Additionally, an examination of the feature maps withvarying guidance values, as shown in , reveals aclear trend: with lower guidance, the feature map injectionsare less pronounced, whereas higher guidance results inmore robust injections that more effectively steer the orig-inal diffusion model. Visualizations of other layers of fea-ture maps can be found in the Appendix.",
  ". Limitation": "Although our method can significantly reduce the FLOPcount in a single pass while maintaining image quality, itis important to note that, unlike CFG, our approach is notas simple to run in batch of two. It requires to run U-Netand guide module in parallel. This is a disadvantage fromimplementation point of view, but it is important to mentionthat in practice larger GPU memory consumption result inslower inference time.",
  ". Conclusion": "In this paper, we introduced a method for distilling guideddiffusion models .The approach allows us to effi-ciently train a lightweight model that modifies the outputsof the conditioned diffusion model while maintaining thebase model parameters. We demonstrate that our techniquesubstantially lowers the computational demands for latent-space diffusion models, which are classifier-free, by de-creasing in the FLOP counts by half. Also, our method canbe plug-and-play to different fine-tuned models without re-training and generate visually pleasing figures. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-imagediffusion models with an ensemble of expert denoisers. arXivpreprint arXiv:2211.01324, 2022. 1",
  "Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusionmodels already have a semantic latent space. arXiv preprintarXiv:2210.10960, 2022. 3": "Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.Gligen: Open-set grounded text-to-image generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2251122521, 2023. 3 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 5",
  "Alexander Quinn Nichol and Prafulla Dhariwal. Improveddenoising diffusion probabilistic models.In InternationalConference on Machine Learning, pages 81628171. PMLR,2021. 4": "DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann, Tim Dockhorn, Jonas Muller, Joe Penna, andRobin Rombach.Sdxl: Improving latent diffusion mod-els for high-resolution image synthesis.arXiv preprintarXiv:2307.01952, 2023. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 5",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,and Mark Chen. Hierarchical text-conditional image gen-eration with clip latents. arXiv preprint arXiv:2204.06125,2022. 4": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1, 4, 7 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2250022510, 2023. 1, 6 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour,Burcu Karagol Ayan,S Sara Mahdavi,Rapha Gontijo Lopes, et al.Photorealistic text-to-imagediffusion models with deep language understanding. arXivpreprint arXiv:2205.11487, 2022. 1 Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-imans, David J Fleet, and Mohammad Norouzi.Imagesuper-resolution via iterative refinement. IEEE Transactionson Pattern Analysis and Machine Intelligence, 45(4):47134726, 2022. 1",
  "A. More visualizations on guide model withLatent Diffusion Models": "In this section, we provided more visualizations of ourmethods with stable diffusion v1.5. The figure is shownin . This part aims to show that our approach cangenerate a variety of styles based on the Text prompts. Notethat the initial noise in the images generated by the CFGand Full guide model with 16 steps (i.e. first two rows) isidentical, but the initial noise for other methods is different.",
  "B. User study": "One of the characteristics observed from the injection-basedconditioned model (e.g. ControlNet) is that the generatedimages are more saturated and have higher contrast. Someperceive them as less realistic while others may find themmore visually pleasing. We conducted a user study whereusers were presented with a text prompt along with a pair ofimages generated from that text prompt (Student Full model50 steps vs Teacher 50 steps) in a sequential fashion. Theywere asked to choose the preferred image based on imagequality and text-image alignment. In the study, 90 partici-pants collectively assessed a total of 680 unique text-imagepairs, resulting in the accumulation of 1.8k votes. The votedistribution indicates that users did not strongly favor theteacher, with 1005 votes (55.65 %) in favor of the teacherand 801 votes (44.35%) in favor of the student.",
  "C. Discussion on model performance with lowguidance number": "We observe that FID scores of our methods are relativelyhigh when guidance is small (g=2, 4, 6).Due to theformulation of the guidance model, when the guidancevalue is small, the injection noise is small (as depicted in).Therefore, the g=0 corresponds to not usingCFG at all, which is known to generate low-quality images.However, when guidance is higher (g=8) our model iscomparable to the teacher model.",
  "D. Other Layers in the Feature Maps": "In this section, we tried to display all the other feature mapinjection layers from our guide model. The correspondingfigure is shown in . Generally, other layers alsoshow that at the beginning of the sampling, the feature mapinjections are stronger. But there may also be some layers(6th layer, counting from top to bottom) that show an in-verse trend."
}