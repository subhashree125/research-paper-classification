{
  "Abstract": "Communicating in noisy, multi-talker environments ischallenging, especially for people with hearing impair-ments. Egocentric video data can potentially be used toidentify a users conversation partners, which could be usedto inform selective acoustic amplication of relevant speak-ers.Recent introduction of datasets and tasks in com-puter vision enable progress towards analyzing social in-teractions from an egocentric perspective. Building on this,we focus on the task of identifying conversation partnersfrom egocentric video and describe a suitable dataset. Ourdataset comprises 69 hours of egocentric video of diversemulti-conversation scenarios where each individual was as-signed one or more conversation partners, providing the la-bels for our computer vision task. This dataset enables thedevelopment and assessment of algorithms for identifyingconversation partners and evaluating related approaches.Here, we describe the dataset alongside initial baseline re-sults of this ongoing work, aiming to contribute to the ex-citing advancements in egocentric video analysis for socialsettings.",
  ". Introduction": "Noisy situations with many people talking simultaneouslyare very common in everyday life but engaging in conver-sations in such situations can be a signicant challenge.This is particularly the case for people with hearing impair-ments which can lead to social isolation . Accordingly,there is a high incentive to develop technologies that couldhelp those affected. Speech audio separation technologiesare rapidly progressing and can offer high-quality acousticseparation and selective amplication of individual speechsources.Yet, a key challenge lies in identifying who the con-versation partners are based on wearable sensors.Using egocentric video is a particularly interesting ap- proach to solving this problem because of the richness ofinformation about the conversation context that can be po-tentially harnessed. Information about looking directions,mouth movements, postures and interactions between peo-ple in the egocentric video could be used to solve the taskof identifying social partners. However, it is not clear whatthe optimal way of integrating this rich information wouldbe. Processing of the egocentric video could be effectivelydone by employing deep neural networks to integrate avail-able information without the need for explicit denition ofrelevant features. However, this can potentially limit modelinterpretability and lead to high computational cost, whichis not desirable for a wearable device. Additionally, al-gorithms trained to identify conversation partners may beprone to overtting to certain conversation scenarios theywere trained on. For instance, egocentric video containingonly people in the same conversation group may be muchmore abundant in datasets, and algorithms may fail to gener-alize to those competing speaker situations where the tech-nology is most needed. In recent years, promising work has introduced newdatasets and concepts that enable the analysis of social sit-uations with egocentric video . For example, aspart of the Ego4D project the tasks of identifying whois looking at, or talking to the camera wearer were pro-posed. This development can yield insights into the chal-lenges posed by complex conversation scenarios with per-spectives for smart hearing instrument technology. How-ever, it is not trivial what the best strategy for achieving thisgoal is. The target of auditory attention may not necessar-ily be xed on a single speaker, but could be a group ofpeople. Rather than focusing on a single target at the time,a more distributed approach may be a better amplicationstrategy for a hypothetical smart hearing device. Based onthese considerations we introduce the task of identifying acamera wearers conversation partners. We dene conversa-tion partners as everyone that is part of the camera wearers conversational group.With this work we hope to contribute to the discussionon and development of egocentric computer vision methodsfor social interactions. Our contributions are: We introduce the task of identifying conversation partnersin the camera wearers egocentric video We describe an annotated dataset with diverse multi-conversation scenarios for this task We present our initial baseline results where we evalu-ate how accurately conversation partners can be identiedbased on rudimentary visual features.",
  ". Related Works": "Investigating social interactions from an egocentric videoperspective has been the focus for an increasing amount ofdatasets and tasks. For example, the EasyCom dataset features 5h of conversations of 3-5 participants in a noisyenvironment with a range of labels and the goal of devel-oping solutions to improve the audibility of partners for thecamera wearer. Ego4D includes 45 h of egocentric videoduring social interaction together with labels for the socialinteraction tasks Looking at me (LAM) and Talking tome (TTM). Ryan et al. used a 20 h dataset and thecomputer vision task of selective auditory attention local-ization (SAAL). On the same dataset, Jia et al. pro-posed the task of estimating not just who the camera weareris listening or speaking to but also who everyone else is lis-tening or speaking to (i.e. estimating the exocentric socialgraph) based on one egocentric video stream.All these approaches are highly relevant and complimen-tary to the task of identifying conversation partners. Themain distinction lies in the temporal dimension: conver-sational groups typically remain stable over longer periods(e.g. minutes), while speech activity and gaze behavior canchange rapidly (e.g. seconds). Short-term features basedon models tackling the TTM or LAM tasks could providevaluable insights into identifying conversation partners evenat time points when they are not talking or looking at theuser. Conversely, long-term information on past conver-sation partners could enhance the performance of TTM orLAM models in multi-conversation settings.The SAAL task is closely related to the task of iden-tifying conversation partners. For SAAL the targets of at-tention are dened as persons if they are both speakingand in the wearers conversation group. In contrast, ourtask focuses on identifying all individuals within the camerawearers conversation group and is thus not reliant on activespeaker localization (ASL). Ryan et al. also demonstratethat by decoupling SAAL from ASL, their model can learnwho is part of the camera wearers group . However, thispromising idea is only evaluated in terms of SAAL with per-fect ASL and was not pursued further. Our approach sepa-rates the long-term task of identifying conversation partners . Example of a frame of egocentric video from our datasetand the group arrangement used in this conversation. The videoframe is seen from the perspective of participant G, who is in aconversation with A (green bounding box in the frame) and H (notin view). Approximately 50% of the dataset feature conversationswith some spatial overlap such as the two conversation groupsshown on the right in the seating plan. from the short-term task of ASL. Therefore, we expect thatthe classication of conversation partners may be more sta-ble over time while also not having to make the assumptionthat auditory attention is only directed towards a speakinggroup member. This could make the analysis of egocen-tric video in multi-conversation scenarios more exible andadvantageous in downstream tasksMost existing datasets feature only one conversationgroup or lack annotations for conversation groups .This may limit their ability to capture the complexities ofmulti-conversation situations and may be suboptimal forthe task of identifying conversation partners. The SAALdataset stands out as the only dataset explicitly containingsocial interactions in more than one simultaneous conver-sation group, featuring ve individuals conversing in twogroups . Yet, a more diverse set of communication con-texts is valuable for effectively developing and evaluatinga system to identify conversational partners that generalizewell across contexts.",
  ". The Dataset": "To investigate communication context with egocentricvideo we collected a unique dataset of people conversingin various simultaneous conversations. The dataset was col-lected in an experiment of 6 sessions where a total of 48 sub-jects participated. These experiments were approved by theScience-Ethics Committee for the Capital Region of Den-mark (reference H-16036391) and all participants providedinformed consent to participate.Per recording session, a group of 6-10 individuals was",
  ". Descriptive statistics of the dataset": "seated around a rectangular table where they were in-structed to engage in conversations in dened subgroups.For each conversation a conversation starter and groupassignment were provided by the experimenter.Seat-ing arrangements and conversation groups were pseudo-randomized and changed multiple times per recording ses-sion. The conversations were conducted in 1-5 groups of2-10 people each.Group sizes were chosen to roughlysimulate everyday situations such as in a canteen ,and also included conversations in a bigger group consist-ing of all present participants.50% of conversations insmaller groups featured some spatial overlap of conversa-tion groups to emphasize scenarios that may be less com-mon but are particularly challenging. Additionally, 50% ofconversations featured multi-talker background noise thatwas played via 8 loudspeakers placed in a circle around thetable (55dBA at the center). Each participant took part in20 5 min conversations, which were balanced with regardto group sizes, background noise and spatial overlap.Egocentric video data (1080p in color) was collected foreach participant during the conversations using Tobii Proglasses 2 and 3 (Tobii AB, Sweden) or Zetronix Z-shades(Zetronix Corp., US). shows an example frame ofthe dataset. The video data was reviewed and cut to onlyinclude the actual conversations in the assigned groups. Perframe of the dataset, face bounding boxes were determinedusing YuNet and were matched to participants usingSFace . Temporal and spatial consistency of the detec-tions was ensured by grouping them into tracklets based onthe optical ow between frames and eliminating shorttracklets with a low face recognition match.The resulting dataset consists of 68.9 hours (6.2 millionframes) of egocentric video segmented into 877 conversa-tion clips of 4.7 min. on average. The average conversationgroup size is 4.1 with groups of 2, 3, 4/5 (groups of 5 onlyoccurred with 10 people around the table and are thereforepresented with the groups of 4) and 6 or more individualsmaking up 25%, 30%, 24% and 20% of the data respectively resulting in a diverse dataset of conversation scenarios (Fig-ure 2). On average there are 2.6 faces in each video frameof. By design the assigned conversation partners per clipare known and make up 66% of all detected faces.Additionally to the egocentric video, calibrated eyetracking data was obtained for 74% (52 hours) of the clips.This data can be used to determine where the camera weareris looking and may facilitate an evaluation of gaze estima-tion algorithms on our dataset. With this, the data can bealso used to employ and evaluate LAM algorithms from theEgo4D challenges on the data .Furthermore, we recorded audio with a 32-channel spa-tial microphone (mh acoustics LLC, US) placed in the mid-dle of the table. This audio data can be used to evaluate thepotential of a spatial beamforming system to separate outthe speech audio of conversation partners.The entire dataset is split into a training set, a validationset, a matched test set and an unseen test set, such that allegocentric clips showing the same conversation from dif-ferent perspectives are always part of the same split. Theunseen test set only contains data from one session not usedin the other splits. This session was the only session with8 individuals, which allows to evaluate how models couldgeneralize to new people and slightly different seating ar-rangements. Overall the training, validation, matched testand unseen test sets account for 40%, 20%, 20% and 19%of the frames respectively.For our dataset we formulate the task as: Given egocen-tric video and face bounding boxes, identify who is partof the camera wearers conversation group. For the currentwork, we perform this as a binary classication per face perframe and evaluate using the average precision (AP).",
  "Everyone0.650.410.520.650.64Center distance0.730.780.540.690.77Face box size0.720.590.680.830.66Face det. score0.720.580.590.680.70Face rec. match0.770.700.680.790.78": ". Average precision (AP) on the task of classifying detectedfaces as communication partners for different classication crite-ria and different parts of the test sets. The criterion Everyonedescribes the AP achievable when assuming everyone is part ofthe same conversation group. We do not show the AP for groupsof 6+ individuals since in this case everyone was a communicationpartner. These cases are included in the any column. classify the face as a conversation partner based on: centerdistance (how close is the face to the center of the frame),face bounding box size (how close is the face to the cam-era wearer), condence score of the face detection , andsimilarity score of face recognition ). An AP of 0.65can be achieved by classifying every face as a conversationpartner. As shown in , all simple decision criteriaachieve an AP higher than .7, however the performance ofeach criterion varies greatly by the subset of the data it isevaluated with. This is a feature of the datasets diversityof contexts, making identication of conversation partnersacross contexts and group sizes non-trivial.",
  ". Conclusion": "Our work introduces the novel computer vision problem ofidentifying conversation partners from egocentric video anddescribes a dataset with baseline measures.Baseline measures vary signicantly by the data subsetindicating that the occurrence of conversation partners inegocentric video is highly dependent on the situation andgroup size.The unseen test set is the only part of thedataset with eight people around the table and accordinglyslightly different group arrangements and distributions ofgroup sizes compared to the rest of the dataset which is re-ected in the different AP scores compared to the matchedtest set. This illustrates the challenges in generalizing acrossdifferent conversation scenarios.Future work may employ existing approaches, such aspredictions of gaze , TTM, LAM or SAAL on ourdata. This could establish a direct comparison of the meritsof these approaches. However, to identify the conversationpartners, perfect knowledge of these measures at every in-tances may not be required. Since communication groupsremains stable for a longer time, we believe that the additionof long temporal context to our models should signicantlyimprove performance. This work lays the foundation for future investigations,contributing to a deeper understanding of conversations incomplex environments. Such analyses could inform deci-sions about spatial beamforming or selective speech sepa-ration in hearing aids.",
  "Kristen Grauman et al. Ego4D: Around the World in 3,000Hours of Egocentric Video. Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,2022:1897318990, 2022. 1, 2, 3, 4": "S. Peter Henzi, L. F. de Sousa Pereira, D. Hawker-Bond, J.Stiller, R. I.M. Dunbar, and L. Barrett. Look whos talking:developmental trends in the size of conversational cliques.Evolution and Human Behavior, 28(1):6674, 2007. 3 Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla,James M Rehg, Vamsi Krishna Ithapu, and Ruohan Gao. TheAudio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective. arXiv preprint, arXiv:2312.12870.1, 2 Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-tusik, and Antonio Torralba. Gaze360: Physically uncon-strained gaze estimation in the wild.In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 69126921, 2019. 4 Bruce D Lucas and Takeo Kanade. An Iterative Image Reg-istration Technique with an Application to Stereo Vision. IJ-CAI81: 7th international joint conference on Articial in-telligence, 2:674679, 1981. 3 Fiona Ryan, Hao Jiang, Abhinav Shukla, James M. Rehg,and Vamsi Krishna Ithapu.Egocentric Auditory Atten-tion Localization in Conversations.Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1466314674, 2023. 1, 2, 4"
}