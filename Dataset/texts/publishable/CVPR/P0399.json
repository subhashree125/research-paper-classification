{
  "Abstract": "Comparing a user video to a reference how-to videois a key requirement for AR/VR technology delivering per-sonalized assistance tailored to the users progress. How-ever, current approaches for language-based assistancecan only answer questions about a single video. We pro-pose an approach that first automatically generates largeamounts of visual instruction tuning data involving pairsof videos from HowTo100M by leveraging existing step an-notations and accompanying narrations, and then trains avideo-conditioned language model to jointly reason acrossmultiple raw videos. Our model achieves state-of-the-artperformance at identifying differences between video pairsand ranking videos based on the severity of these differ-ences, and shows promising ability to perform generalreasoning over multiple videos.Project page:",
  ". Introduction": "Instructional how-to videos are an important medium forlearning new skills that offer in-depth visual demonstrationsof complex procedural activities. In turn, they serve as avaluable resource for building AR/VR assistants that canguide a user through a procedural activity, by aligning useractivity to a reference how-to video. Instructional videoshave thus been the subject of several recent datasets andbenchmarks that are driving new research .A key requirement for such systems is the ability to com-pare and contrast the users execution of a step in the activitywith the reference video step, to highlight similarities anddifferences between them. For example, to let the user knowthat they used too much detergent (while doing laundry)or that the gravy is too thick (while cooking). This abil-ity has direct value for personalized assistance applicationssuch as progress tracking, mistake detection and surfacinguser-activity driven tips.More generally, reasoning about a video with respect to areference video is a fundamental problem for video under-standing that has value for fine-grained video retrieval [11,",
  "A: No, keep stirring till they become golden brown like the video": ". Main idea. Top: We train models to compare twovideos showing the same high-level keystep and to describe theirdifferences (e.g., in tools, ingredients, technique). Bottom: Oncetrained, such models can then help answer questions about a usersactivity compared to a reference (e.g., an internet how-to video)like did I do this step right? or am I done yet?. 53, 54, 57] (e.g., to browse internet videos for this moviescene, but in a forest), step detection (e.g., to recognize subtle variations in keysteps) and multi-video question answering and reasoning (e.g., toanswer comparative questions like which video uses theleast amount of oil?).Despite its importance, there has been limited work oncomparing videos. Prior work has explored change cap-tioning in images , however theseworks typically consider pixel-level differences (e.g., miss-ing or moved objects; changed background objects) in staticscenes (e.g., the same parking lot; the same tabletop), or insynthetically generated datasets . They do not considerimportant semantic differences in activities (e.g., differencesin tool use, subtle variations in actions and techniques orvisual differences due to state changes), which together withthe low-level visual differences, form a complete picture ofhuman-object interactions.To address these limitations, we propose a video-conditioned language model (VCLM) approach to directlycompare two videos of same step in a procedural activity.Specifically, we propose the difference question answeringtask: given a reference and a candidate video, a model mustanswer a question that involves reasoning across both videos(e.g., what are the differences in tools? techniques?; do the",
  "arXiv:2404.16222v2 [cs.CV] 27 Jun 2024": "two videos show the same activity?). Such a model thateffectively relates user activity to a reference video, can thenprovide detailed context to answer more general questionssuch as what did I do wrong compared to the reference oram I done yet?. See .An important practical question is how to source the su-pervision to train such a model, given that existing videodatasets only contain individual videos with captions. More-over, meaningful differences are not guaranteed to existbetween arbitrary pairings of videos. We therefore automati-cally generate training data from existing large-scale instruc-tional video datasets annotated with keysteps and speechnarrations describing what instructors are doing . Wepair clips of the same keystep (e.g., two videos of a personstir frying the rice until it is dark yellow) but from distinctvideos to allow for variations between them. For examplethe first video may use a cast iron pan versus a steel wokor the person may be tossing the food in the pan vs stir-ring with a spatula. We then leverage recent large languagemodels to generate questions and answers about the sim-ilarities and differences between the two videos given theirvisual descriptions, speech narrations, and visible objectsas context. Inspired by work in visual instruction tuning oflanguage models , we finally fine-tune a video-pairconditioned language model with the collected dataset. Theresulting model has the ability to cross-reference videos tocompare them, and more generally answer questions thatrequire joint reasoning about both videos simultaneously.To evaluate our model, we collect a manually annotateddataset of 6292 video pairs with 36k difference captionsspanning 5 categories, as well as scores for how severe thedifferences are. We set up the first benchmark for videocomparisons and evaluate models on their ability to describethe differences in specific categories (e.g., What are the dif-ferences in tools? techniques?) and to rank videos based ontheir differences (e.g., Which video shows the least differ-ent technique?). Our models trained with weak-supervisionfrom automatically generated data achieve state-of-the-artresults on our benchmark, highlighting its value for personal-ized assistance applications. Our benchmark will be hostedpublicly, to allow the community to make progress towardsthis under-explored task.",
  ". Related work": "Instructional video understandingRecent large-scale in-structional video datasets havefacilitated research in step captioning , step detec-tion , temporal grounding ,vision-language representation learning andvideo question answering to name a few. Inall these approaches, the goal is to process a single videoand then caption, answer questions or temporally localizean action or text within it. While we are also interested in the space of procedural videos in the context of personalizedlanguage-based assistance, in contrast, we develop methodsto compare and contrast multiple videos namely a ref-erence video and a candidate video in order to identifydifferences and answer comparative questions about them. Visual differences in imagesPrior work has studied vi-sual differences in images in the context of attributes (e.g., which shoe is more formal) to facilitatefine-grained recognition. More relevant to our work, changecaptioning involves describing thedifferences between two images as a text caption. Otherwork defines differences as 2D bounding boxes orsemantic maps for regions that differ. More recently,VCLM models have been trained with spot-the-differencedata from the above with a similar goal of identifying imagedifferences . In all these cases, the two images typicallyinvolve the same scene from multiple viewpoints or over time(e.g., surveillance footage) or are constructed from syntheticimages (e.g., 3D geometric shapes re-arranged on a table).The resulting differences therefore focus on simple visualcues like missing or moved objects. More recent approachesuse visual differences to retrieve videos , however theyassume the difference is known (to retrieve a relevant video)rather than identifying and describing it. In contrast, we com-pare across distinct video clips that show the same high-levelkeystep. As a result, the difference captions characterizecomplex variations that arise naturally from the availabil-ity of tools and ingredients, differing skill / technique orpersonal preference. Visual instruction tuning of language modelsGiventhe recent success of large language models (LLMs), sev-eral efforts have tried to adapt them for use with vari-ous modalities including images, videos, audio etc., typi-cally by aligning captions to modalities or instruction tun-ing . All these approachestypically use text captions or generate instruction tuning databased on a single image or video. In contrast, we generateinstruction data for pairs of videos (a reference, and a targetvideo) to allow vision conditioned language models to jointlyreason about them both. Some approaches do train on mul-tiple images interleaved with text , however theydo not support instructions at inference, and instead rely onin-context few-shot prompting to respond. In contrast, ourapproach can respond to arbitrary questions about a videowith respect to a reference clip.",
  "Video Tokenizer": ". Step differences framework. We first generate a comprehensive step description including information from action captions,object detections and ASR narrations (left panel). We then select pairs of clips with similar step descriptions, and automatically generatequestions and answers that compare the two (center panel, Sec. 3.2). Finally, we instruction-tune an LLM to generate answers conditionedon the generated questions and encoded representations of both videos (right panel, Sec. 3.3). Once trained, the model directly operates onvideo clips to compare them, without the need for captions, ASR or object detections. we source data of pairs of videos with relevant questionsto train such models and what model architectures supporttraining with multiple videos? For the former, we turn toautomatically generating this data using large-language mod-els (LLMs) parsing narrated video from existing datasets.For the latter, we use vision-conditioned language models(VCLMs) a powerful class of models for single-videoquestion answering adapted to our multi-video setting. Inthe following, we first formally define our task (Sec. 3.1).Next, we describe our automatic training data generationpipeline (Sec. 3.2). Finally, we discuss training and down-stream inference (Sec. 3.3).",
  ". Task definition": "We require models that collectively answer questions abouttwo videos. Formally, given a reference video Vr, a can-didate video Vc and a question q, models must produce acorresponding answer a. This formulation is an extension ofstandard video question answering or captioning witha response that is additionally conditioned on a referencevideo. The questions can take various forms, for exampleHow is the dough being prepared differently in Video 2;What is the similarity in mixing techniques between the twovideos?. Critically, these questions all share the assumptionthat a single video alone (either the reference or the candi-date) is insufficient to answer the question reasoning overboth videos is required.In our experiments, we train models with a diverse set ofautomatically-generated question-answer pairs. At test time,we focus on step differences, where the q is of the form whatis the main difference between these videos in the category gand g is the difference category (e.g., ingredients, techniques,etc.). This structure captures a representative range of fine-grained differences, and allows for consistent evaluation of",
  ". Step differences dataset generation": "To train our models, we require a dataset of paired videosalong with questions and answers (QA) relating the twoin the form (Vr, Vc, q, a). However, current video datasetstypically contain individual video clips annotated for actions,narrations, or single-video QA which is incompatible withour task definition. We therefore construct this from existingvideo datasets using large-language models, inspired by priorwork on instruction tuning .Constructing this dataset from existing video datasetsis non-trivial. On the one hand, selecting random pairsof videos showing very different content (e.g., sports vs.cooking) or near-identical videos (e.g., from repetitions ofthe same activity by the same participant) will lead to triv-ial differences. On the other hand, naively selecting videopairs of the same class in action recognition datasets (e.g.,Bookbinding or Mowing the lawn) will not highlightfine-grained differences of interest, and will instead focus onglobal differences (e.g., changes in actors or scenes). More-over, these datasets do not come with text descriptions toconstruct differences from.We therefore propose to use videos from the large-scale procedural video dataset HowTo100M , specifi-cally cooking-themed videos labeled for keysteps from HT-Step . Instructional videos are an ideal data source as theyare narrated and show the same high-level keystep, but withvariations that arise naturally from availability of tools andingredients, differing skill / technique or personal preference.Specifically, for two videos showing the same keystep(e.g., Slowly pour the sauce over the dumplings), we assumeone is the reference Vr and the other is the candidate videoVc, with corresponding speech narrations. First, we gener- ate descriptions of the actions and objects (including theirattributes) using off-the-shelf captioning models . Thesemodels often hallucinate details in their generations, so weadditionally filter object descriptions based on the scores ofa pre-trained detection model and filter action descrip-tions using visual grounding models . Details about thefiltering stage are in Supp. Finally, we aggregate the informa-tion from these three sources (ASR narration, filtered objectsand actions) to synthesize a detailed step description foreach video (, left panel). We then prompt a languagemodel (in our case, Llama 2 ) to generate both questionsand answers comparing the two videos based on their stepdescriptions. In short, the prompt takes the form: Video1: {description1}. Video 2: {description2}. Summarize thedifferences and generate 3 question-answer pairs comparingthe two videos. (, center panel). An overview of thedata generation pipeline with examples at each stage can beseen in . See Supp. for more examples and full stepdescription prompt details.The resulting dataset contains QA instances over videopairs across 87740 unique video clips. Note that the LLM-generated data is noisy they may hallucinate details thatare not present in the video, misunderstand the ASR narra-tions, produce irrelevant questions or incorrect answers toquestions. Despite this, they offer valuable weak supervisionto train our VCLM models, as our experiments will show.",
  ". Paired video instruction tuning": "We require a model that can generate natural language re-sponses to video comparison questions in our dataset. To dothis, we adapt a vision-conditioned language model (VCLM)to our multi-video setting via visual instruction tuning. Inshort, visual instruction tuning aligns the outputs of an im-age (or video) backbone to a powerful LLM to conditionits responses on the visual content. This strategy has beensuccessful in prior work for single image/video captioningand question answering . Weextend this to support comparisons across multiple videos.In our experiments, we use a Llama2 LLM alignedwith an Internvideo backbone following prior work .Note that it is possible to directly provide multiple videos toexisting models by adding extra visual tokens to the inputprompt, however their performance is degraded as they nottrained to support this. We compare against such models.Specifically,foraninstruction-tuninginstance(Vr, Vc, q, a),we generate an instruction prompt inthe Llama2 format as follows. <s> [INST] <<SYS>> You are a helpful AI assistantthat answers questions about a pair of videos.Answer in a single sentence. Here is the firstvideo: {V_r}. Here is the second video: {V_c}.<</SYS>> {q} [/INST] {a}",
  ". Evaluation tasks. We evaluate on describing (DiffCap),recognizing (DiffMCQ) and ranking (DiffRank) differences": "pre-trained text encoder. We encode each video into a se-quence of spatiotemporal tokens using a pre-trained videobackbone MV , and then align them to the LLMs input spaceusing a learnable projection module Mproj. The resultingencoded instruction prompt is a sequence of tokens com-prising a mix of text and visual tokens, which can then beprocessed by the LLM (, right panel). The model is trained using the original auto-regressive ob-jective to maximize the probability of generating the answertokens, conditioned on the question, reference and candidatevideo, and is trained using a standard cross-entropy loss.",
  "Xc = Mproj(MV (Vc)),(3)": "where Xa,i is the i-th answer token in the sequence, Xr(Xc) are the visual tokens corresponding to the reference(candidate) videos, Xq, Xa are tokens of the question andanswer, Xa,<i are answer tokens that occur before Xa,i and are the learnable parameters in Mproj. Note that the video encoder and the LLM weights arefrozen, and the loss is computed only for answer tokens.Only the projection layer is fine-tuned. Once trained, ourmodel will be able to refer to each video, discuss their sim-ilarities and compare them. We evaluate our model by au-toregressively generating text in response to various promptscoupled with reference and candidate videos.",
  "Finally, we use our trained models to identify and rank fine-grained differences between pairs of video. We cast thesetasks into the paired-video QA framework as follows": "Difference captioning (DiffCap)The goal is to generate atextual description of the differences between two videos ina specific category g (e.g., ingredients, tools). The questionq takes the form what is the main difference between thesevideos in the category g. The difference caption is generatedauto-regressively using the trained model. Difference recognition (DiffMCQ)The goal is to selectthe correct video pair that matches the difference caption,from a list of candidate video pairs {(V ir , V ic )}i=1..4. This isa discriminative version of the captioning task above inspiredby recent work in vision-language feature learning . Forthis, we compute p(a|V ir , V ic , q) the likelihood of gener-ating the difference text given the pair of videos followingEqn. 2 and then select the pair with the highest score. Difference ranking (DiffRank)The goal is to rank videoinstances {V ic }i=1..4 based on how different they are to acommon reference video Vr, in terms of a particular categoryof interest g. For this, we set q to be do these two videosshow the same g? Answer YES or NO., and rank eachcandidate video based on the likelihood of generating YESas the response.Together, these tasks are a representative suite of prob-lems for instructional video understanding that require com-paring videos along various axes. DiffCap tests how accu-rately a model can describe differences in natural language, DiffMCQ tests how well it can discriminate differences be-tween videos, and DiffRank tests how well the model canassess the severity of these differences to rank them. Amodel for these tasks can enable applications that guide useraction (e.g., to follow a reference video tutorial) or helpbrowse through large collections of videos (e.g., to find theperfect variation of a recipe). illustrates these tasks.",
  "We evaluate our VCLM model on the three step differencetasks from Sec. 3.4": "DatasetWe construct a test dataset from videos in HT-Step . HTStep contains videos from a large-scale proce-dural video dataset, HowTo100M (Cooking & Entertain-ment), with temporal segments (clips) annotated for keysteps(e.g., fry then onions until golden brown). We manuallyannotate pairs of clips, where each pair corresponds to in-stances of the same labeled keystep, but from distinct videos.Annotators are asked to identify the main differences across5 categories (ingredients, tools/equipment, techniques, vi-sual differences) and write difference captions of a consistentstyle what happens in the target clip, compared to whathappens in the reference (e.g., The person uses a deep fryerto fry the potatoes instead of shallow frying it in a pan).They are then asked to score the difference caption in eachcategory on a scale of 1-5 based on how severe the differenceis, where 1 is a significant difference (e.g., swapping out acritical ingredient that would change the dish entirely) and 5is nearly identical (e.g., minor cosmetic differences that doesnot affect the activity). A rubric is used to ensure consistencyin scoring.Note that this data is only used for evaluation we ex-",
  "StepDiff0.2230.1040.2150.5410.181": ". Results. Our approach outperforms three classes of baselines built on top of state-of-the-art vision-language embedding and VCLMmodels. VLEmbed baselines are excluded from DiffCap as they cannot generate text. clude these pairs from the automatic training data generationpipeline described in Sec. 3.2 to ensure that the model hasnot seen these instances during training. In total, we collect35988 difference captions across 6292 clip pairs, involving8396 unique clips. See for examples. Full collectiondetails and dataset statistics are in Supp.",
  "BaselinesWe compare several classes of models": "VLEmbed is a class of vision-language model that embedsimages or video in the same space as text, and then com-pares their similarity in the shared space. Video pair em-beddings are calculated as the average of individual videoembeddings1. We use CLIP and InternVideo . Socratic is a class of VCLMs that first converts videosinto text using a captioning model, and then prompts atext-only LLM with these captions. These models arepowerful, but often require complex, manually engineeredprompts. We use state-of-the-art visual captioners (BLIP-2 , LLaVA-1.5 ) as well our aggregate step de-scriptions from Sec. 3.2. We use Llama2 to process thecaptions regardless of which model generated them, forfair comparisons. VCLM is a class of visual instruction-tuned languagemodel trained for video captioning and question answering(for a single video). We directly add extra tokens for thereference video into the prompt to be consistent with ourpaired-video QA task. We compare LLaVA-1.5 andAnyMAL . Interleaved is a class of models that are trained with inter-leaved sequences of images/videos and text, and naturallysupport multiple videos as inputs, but are not explicitlytrained to compare them. We compare the recently pro-posed IDEFICS and a model we train on sequencesof (video, ASR) pairs from HowTo100M (training detailsin Supp.).These baselines represent a spectrum of leading strategiesfor vision-language reasoning, including methods that di-",
  "We evaluate other aggregation strategies in Supp": "rectly embed video and language in the same space (VLEm-bed), ones that explicitly convert videos to text and performexclusively text-based reasoning (Socratic) and ones thatperform joint vision-text reasoning on videos (VCLM, In-terleaved). We ensure that each class of baselines includemethods that have been trained on in-domain HowTo100Mvideos, while excluding the evaluation videos, to ensurefair comparisons with our approach. These are InternVideo,Socratic (Step desc.), VCLM (AnyMAL), and Interleaved(AnyMAL). Additional pretraining and implementation de-tails are in Supp. ImplementationdetailsWeusetheLlama2-chat-70B as the base LLM for all our experiments.Following prior work , MV is an Internvideo video encoder that inputs 8 uniformly sampled frames fromeach video clip and generates 2056 spatio-temporal tokens.MP roj is a 2-layer Perceiver module followed by alinear layer head to output 32 tokens in the LLMs inputdimension. During training, all parameters are frozen exceptfor MP roj. StepDiff models are initialized from Interleavedmodel weights before finetuning (interleaved data is retainedduring finetuning).For baselines, we use the largestavailable versions of models InstructBLIP (Vicuna13B),LLaVA (Vicuna13B), AnyMAL (70B), IDEFICS (80B).Full implementation and training details are in Supp.",
  ". Difference captioning": "We first evaluate how well our model can describe differ-ences in video pairs (DiffCap). As mentioned in Sec. 3.1,q is of the form what is the main difference between thesevideos in the category g where g is the difference cate-gory. Since there may be multiple annotated differences inthe same category, we group them together and treat themas a ground truth set, resulting in a dataset with 22292 in-stances. We measure standard text generation metrics in-cluding CIDER and ROUGE-L . Outputs are post-processed using simple string matching techniques to ensure",
  "ReferenceCandidate": ". Extended QA on video pairs. Our model which can describe differences (row 1) can be prompted (i.e., queried without anyform of retraining) for comparative reasoning (e.g., why are they different?, how different are they? row 2-3), or to bootstrap mistakedetection (row 4). A failure case is shown in row 5 due to model hallucination. difference captions are generated in the correct format (de-tails in Supp). For the socratic baselines, we provide thegenerated caption instead of the video tokens in the promptfrom Sec. 3.3. (left) shows our results. The socraticmodels perform poorly as they are limited by the informationcontained in the base captions. It is infeasible to generatecaptions that exhaustively describe every aspect of a video,without knowing what is of interest, and without the risk ofmodel hallucinations. The VCLM models perform better,especially when trained to process multiple videos (i.e., inter-leaved models), however they still fall short of our approachthat can explicitly compare and contrast videos. The exam-ple in highlights the sensitivity of socratic models toinput captions (e.g., the reference caption did not mentionthe use of hands), and shows how VCLM models tend tohallucinate details. Our approach can correctly describe thedifference. See Supp for more examples.",
  ". Difference recognition": "While the captioning metrics are informative, they are basedon word overlap statistics, and do not always capture thesemantics of the text well. To address this, we evaluateon DiffMCQ the discriminative version of the captioningtask. We adapt the same dataset from DiffCap, except we sample a single difference caption for each category if thereare multiple differences present. Further, we sample threenegative video pairs from other instances in the dataset thatinvolve similar objects and actions (details in Supp). For theVLEmbed baselines, we score each video pair using the co-sine similarity between their average visual embeddings andthe text embedding of the difference caption. We comparevariants of this baseline considering only the reference ortarget in Supp. For all LLM-based baselines, we compute thelikelihood of generating the difference caption for each videopair, under each model as discussed in Sec. 3.1. We evaluatetop-1 accuracy. (center) shows our results. The jointfeature embedding models capture some semantics, but areinsufficient for identifying differences. Socratic models havea similar trend to captioning results, however models trainedon step differences show large improvements, highlightingthe value of careful curation for generating captions. AmongVCLM models, ones that have seen in-domain HowTo100Mvideos during training have an edge over the others (i.e.,LLaVA, IDEFICS), with interleaved models again being su-perior. Our model outperforms all these approaches with a5% accuracy improvement over the strongest baseline. shows performance increases by difference category, overthe weakest baseline (Socratic). Our approach shows large",
  ". Difference ranking": "Finally, we evaluate how well our model can rank videosbased on the severity of differences compared to a commonreference (DiffRank). Each reference video in the dataset ispaired with four target videos, scored along each categoryaxis. For example, some videos may be very similar in termsof technique, but very different in terms of ingredients. Weonly retain instances where there is a clear ranking (i.e., nomore than one tie in scores) The resulting dataset contains3932 instances involving 5746 unique clips.As discussed in Sec. 3.1, we rank each target video candi-date based on the likelihood of producing the response YESwhen asked whether it is similar to the reference. We usethe Kendalls rank correlation metric to evaluate how wellthe generated ranking compares to the ground truth rankingannotators provide. (right) shows our results. Un-like the previous tasks, the joint embedding models performbetter than the LLM based baselines for two reasons. First,similarity in the embedding space directly translates to ascore for ranking, rather than relying on computing YEStoken probability as a proxy for this. Second, there is ahigh correlation between the rankings across categories for",
  ". Extending QA beyond atomic differences": "Next, we show how our model can be prompted to answerquestions beyond just describe the differences. LLMs haveshown remarkable abilities for complex, multi-step reason-ing in text our training framework unlocks the same kindof reasoning for multiple videos, based on their differences.In , we show some examples of this. Our model is ableto naturally describe differences as it was trained for thistask (row 1), but also has the ability to perform comparativereasoning (row 2-3) or explain mistakes (row 4). We show afailure case in row 5, where the model hallucinates content a characteristic feature of the LLM models it is built upon.Moreover, our model works with egocentric video (row 1,4), despite being trained on largely third-person video con-tent (HowTo100M), which is promising for AR/VR userassistance applications.",
  ". Ablation experiments": "Finally, we ablate several design choices in our model in. As mentioned in Sec. 4, we finetune models onboth interleaved ASR data as well as our generated pair QAdata. Without the interleaved data, the model performancedrops on two tasks, likely due to catastrophic forgetting (w/ointerleaved data). Next, we show the importance of filteringthe generated QA data (w/o QA filtering), given the highlikelihood of hallucinations produced by the LLM. Finally,we swap out the 70B LLM model for a smaller sized one(w/ 13B LLM), causing the performance to drop, though notsignificantly.",
  ". Conclusion": "We proposed StepDiff, a video-conditioned language model(VCLM) that can compare and contrast videos to reveal fine-grained differences between them. We propose an approachthat can automatically generate instruction-following paired-video QA training data from large-scale procedural videodata, and a manually curated benchmark to evaluate models.Our experiments on describing and identifying differences,as well on ranking videos based on differences demonstrate the value of our approach for personalized assistance appli-cations. Future work can leverage our work for personalizedretrieval (e.g., retrieve content based on user-activity), ormulti-video QA beyond instructional videos. Acknowledgements Thanks to Efi Mavroudi, Huiyu Wang,Triantafyllos Afouras and Yale Song for helpful discussions;Kumar Ashutosh and Suyog Jain for help with annotationtooling and collection; Austin Miller and Honey Manglanifor managing the annotator workforce. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. Advances inNeural Information Processing Systems, 35:2371623736,2022. 2 Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, JosefSivic, Trevor Darrell, and Bryan Russell. Localizing momentsin video with natural language. In Proceedings of the IEEEinternational conference on computer vision, pages 58035812, 2017. 2 Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and Kris-ten Grauman. Hiervl: Learning hierarchical video-languageembeddings. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2306623078, 2023. 2",
  "Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, and KristenGrauman. Detours for navigating instructional videos. InCVPR, 2024. 2": "Ankan Bansal, Yuting Zhang, and Rama Chellappa. Visualquestion answering on image sets. In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXI 16, pages 5167. Springer,2020. 1 Siddhant Bansal, Chetan Arora, and CV Jawahar. My viewis the best view: Procedure learning from egocentric videos.In Computer VisionECCV 2022: 17th European Conference,Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XIII,pages 657675. Springer, 2022. 1, 2",
  "with feedback. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pages1410514115, 2022. 1": "Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, MiaoZheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo,and Kai Chen. Multimodal-gpt: A vision and language modelfor dialogue with humans. arXiv preprint arXiv:2305.04790,2023. 2, 3, 4 Tengda Han, Weidi Xie, and Andrew Zisserman. Temporalalignment networks for long-term video. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 29062916, 2022. 2, 3 Mehrdad Hosseinzadeh and Yang Wang. Image change cap-tioning by learning from an auxiliary task. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 27252734, 2021. 1, 2 Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,Andrew Zisserman, and Joao Carreira. Perceiver: Generalperception with iterative attention. In International confer-ence on machine learning, pages 46514664. PMLR, 2021.6, 3",
  "Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning todescribe differences between pairs of similar images. arXivpreprint arXiv:1808.10584, 2018. 1, 2": "Hoeseong Kim, Jongseok Kim, Hyungseok Lee, HyunsungPark, and Gunhee Kim. Agnostic change captioning withcycle consistency. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 20952104,2021. 1, 2 Hilde Kuehne, Juergen Gall, and Thomas Serre. An end-to-end generative framework for video segmentation and recog-nition. In 2016 IEEE Winter Conference on Applications ofComputer Vision (WACV), pages 18. IEEE, 2016. 2 Hugo Laurenon, Lucile Saulnier, Lo Tronchon, Stas Bek-man, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-dharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.Obelisc: An open web-scale filtered dataset of interleavedimage-text documents. arXiv preprint arXiv:2306.16527,2023. 2, 6, 4",
  "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,Jingkang Yang, and Ziwei Liu.Otter: A multi-modalmodel with in-context instruction tuning.arXiv preprintarXiv:2305.03726, 2023. 2, 4": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozenimage encoders and large language models. arXiv preprintarXiv:2301.12597, 2023. 6, 4 Chin-Yew Lin and Franz Josef Och. Automatic evaluationof machine translation quality using longest common sub-sequence and skip-bigram statistics. In Proceedings of the42nd Annual Meeting of the Association for ComputationalLinguistics (ACL-04), pages 605612, 2004. 6 Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, MichaelWray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wen-zhe Zhao, Weijie Kong, et al. Egocentric video-languagepretraining. Advances in Neural Information Processing Sys-tems, 35:75757586, 2022. 5 Xudong Lin, Fabio Petroni, Gedas Bertasius, MarcusRohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learningto recognize procedural activities with distant supervision.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1385313863, 2022. 2",
  "Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robustchange captioning. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 46244633,2019. 1, 2": "Jin-Man Park, Jae-Hyuk Jang, Sahng-Min Yoo, Sun-KyungLee, Ue-Hwan Kim, and Jong-Hwan Kim. Changesim: To-wards end-to-end online scene change detection in indus-trial indoor environments. In 2021 IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS), pages85788585. IEEE, 2021. 2 Abhirama Subramanyam Penamakuri,Manish Gupta,Mithun Das Gupta, and Anand Mishra. Answer mining froma pool of images: Towards retrieval-based visual questionanswering. arXiv preprint arXiv:2306.16713, 2023. 1",
  "Pengchuan Zhang. Egovlpv2: Egocentric video-languagepre-training with fusion in the backbone. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision,pages 52855297, 2023. 2": "Yue Qiu, Shintaro Yamamoto, Kodai Nakashima, RyotaSuzuki, Kenji Iwata, Hirokatsu Kataoka, and Yutaka Satoh.Describing and localizing multiple changes with transformers.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 19711980, 2021. 1, 2 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 6, 5 Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Ste-fan Thater, Bernt Schiele, and Manfred Pinkal. Groundingaction descriptions in videos. Transactions of the Associationfor Computational Linguistics, 1:2536, 2013. 1, 2",
  "Ragav Sachdeva and Andrew Zisserman. The change youwant to see. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, pages 39934002,2023. 2": "Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.Laion-5b: An open large-scale dataset for training next gen-eration image-text models. Advances in Neural InformationProcessing Systems, 35:2527825294, 2022. 3 Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, KunHe, Dipika Singhania, Robert Wang, and Angela Yao. As-sembly101: A large-scale multi-view video dataset for un-derstanding procedural activities.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2109621106, 2022. 1, 2 Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, AliFarhadi, and Karteek Alahari. Actor and observer: Joint mod-eling of first and third-person videos. In proceedings of theIEEE conference on computer vision and pattern recognition,pages 73967404, 2018. 1, 2",
  "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-YanLiu. Mpnet: Masked and permuted pre-training for languageunderstanding. Advances in Neural Information ProcessingSystems, 33:1685716867, 2020. 5": "Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin:A large-scale dataset for comprehensive instructional videoanalysis. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 12071216,2019. 1, 2 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori BHashimoto.Alpaca:A strong, replicable instruction-following model. Stanford Center for Research on Foundation",
  "Models. stanford. edu/2023/03/13/alpaca. html,3(6):7, 2023. 3": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-jad Almahairi, Yasmine Babaei, Nikolay Bashlykov, SoumyaBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:Open foundation and fine-tuned chat models. arXiv preprintarXiv:2307.09288, 2023. 2, 4, 6 Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-lami, Oriol Vinyals, and Felix Hill. Multimodal few-shotlearning with frozen language models. Advances in NeuralInformation Processing Systems, 34:200212, 2021. 2 Ramakrishna Vedantam, C Lawrence Zitnick, and DeviParikh. Cider: Consensus-based image description evalu-ation. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 45664575, 2015. 6",
  "Lucas Ventura, Antoine Yang, Cordelia Schmid, and GlVarol. Covr: Learning composed video retrieval from webvideo captions. arXiv preprint arXiv:2308.14746, 2023. 1": "Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, LiFei-Fei, and James Hays. Composing text and image forimage retrieval-an empirical odyssey. In Proceedings of theIEEE/CVF conference on computer vision and pattern recog-nition, pages 64396448, 2019. 1 Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang,Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, SenXing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, LiminWang, and Yu Qiao. Internvideo: General video foundationmodels via generative and discriminative learning. arXivpreprint arXiv:2212.03191, 2022. 4, 6, 3, 5 Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,and Chuang Gan. Star: A benchmark for situated reasoningin real-world videos. In Thirty-fifth Conference on NeuralInformation Processing Systems Datasets and BenchmarksTrack (Round 2), 2021. 2 Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, StevenRennie, Kristen Grauman, and Rogerio Feris. Fashion iq: Anew dataset towards retrieving images by natural languagefeedback. In Proceedings of the IEEE/CVF Conference oncomputer vision and pattern recognition, pages 1130711317,2021. 1 Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.Next-qa: Next phase of question-answering to explaining tem-poral actions. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 97779786,2021. 2",
  "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: Aninstruction-tuned audio-visual language model for video un-derstanding. arXiv preprint arXiv:2306.02858, 2023. 2, 4": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xi-aofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,Fei Wu, et al. Instruction tuning for large language models:A survey. arXiv preprint arXiv:2308.10792, 2023. 2, 4 Yue Zhao, Ishan Misra, Philipp Krhenbhl, and Rohit Gird-har.Learning video representations from large languagemodels. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 65866597,2023. 2",
  "Luowei Zhou, Chenliang Xu, and Jason Corso. Towards auto-matic learning of procedures from web instructional videos.In Proceedings of the AAAI Conference on Artificial Intelli-gence, 2018. 1, 2": "Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan GokberkCinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-task weakly supervised learning from instructional videos.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 35373545, 2019. 1, 2 Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang,and Onkar Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In EuropeanConference on Computer Vision, pages 392408. Springer,2022. 1, 2",
  "S1. Training data generation details": "As mentioned in Sec. 3.2, we construct a paired QA datasetusing pairs of video clips that share the same step label fromHTStep . In this section, we provide detailed descriptionsof each phase in the data generation pipeline. Action and object captioningWe use a VCLM model todescribe actions and objects in the video clip (see detailsin Sec. S3). For actions, we sample 8 frames from the clipand use a HowTo100M trained captioning model. Forobject captions, we sample the center frame of the video clipand use an image captioning model . The full promptstructure for each model is shown below",
  "[OBJECT PROMPT]Give a very short list of all objects that arevisible and their attributes, one per line. Onlylist objects being used, NOT in the background": "Despite the prompt asking to only list objects being used,the LLM-based captioning models tend to hallucinate ob-ject details that are not present in the scene. We thereforepost-process the object captions using an off-the-shelf textgrounding model . We retain only the object descriptionsthat have a grounding score greater than zero. Consolidated step descriptionNext, we consolidate allthe information above into a concise step description asshown in (left panel).For this, we use a text-only LLM model (Llama-2-70b-chat) with the followingprompt.",
  "[SYSTEM PROMPT]You are an AI assistant that synthesizes the outputof narration, action and object captioning modelsinto a single description of the content": "[PROMPT]Video narration: {narration}.Possible activity: {action_caption}.Possible objects: {object_caption}.Summarize the captions into a single, descriptivesentence about what the person is doing, and usingwhat objects. Paired video QA generationFinally, we select pairs ofvideo clips, along with their generated step descriptions,and query the Llama-2 model to generate questions andanswers. We generate questions of three types as shownbelow. [SYSTEM PROMPT]You are an AI assistant that asks questionscomparing two videos based on their descriptions,and then answers them. Each question must be on anew line starting with \"Q:\" for question and \"A:\"for the answer. Use diverse language.Video 1: {step_description_1}Video 2: {step_description_2}",
  "[PROMPT_TYPE3]Do the two videos share a similar main action?Answer with a single word: YES or NO": "The final training dataset is the composition of question-answer pairs from all three sources. See Fig. S1 for examplesof this data. Note that this data is used as weakly supervisedtraining data only. For evaluation, a separate, disjoint set ofvideo clips is manually annotated. See Sec. 4 (dataset) andSec. S2 for details.",
  "Q: Which video features a silver metallic KitchenAid stand mixer?A: Video 1": "Q: What is the main difference between the two videos in terms of the mixing process?A: In Video 1, the person is using a manual measuring cup to pour our into a mixing bowl, while in Video 2, the person is using an electric mixer to stir a mixture. Q: What is the main difference between the cooking techniques used in the two videos?A: The rst video shows a person sauting ingredients in a pan, while the second video shows a person using a slow cooker.",
  "Figure S1. Generated paired QA data. Details in Sec. 3.2": "Figure S2. Data annotation interface Annotators first watch two short video clips of a keystep performed by two different people (rightpanel). After that, they write out what they think the common keystep is between the two video clips, and then describe and score thedifferences between the clips them along various categories (left panel). Annotators can reject clips if they are not comparable (differentkeysteps, unclear or short videos). Annotation instructions and rubricsAs mentioned inthe main paper, annotators are presented with pairs of videoclips from the same keystep category and asked to identifythe main differences across 5 categories (ingredients, tools/e-quipment, techniques, visual differences) and then score howsevere the differences per category are on a scale of 1-5. Theannotation interface presented to the user is shown in Fig. S2.Scoring how severe the differences are is a fairly subjectivetask. To avoid ambiguity in this scoring, we present anno-tators with a scoring matrix (Fig. S3) that provides a rubric for scoring differences in each category. We conducted pi-lot experiments to calculate inter-annotator agreement. Wefound that two out of three annotators agree 82% of the time(Cohens kappa = 0.64 on a scale). Moreover, dis-agreements when present are small (on average within 1.2points from each other).",
  "Consistency, significant difference in color e.g, brown and white, red and blue-": "Figure S3. Difference scoring matrix Annotators score how severe the differences are on a scale of 1-5 (1 = very different; 5 = nearlyidentical) using the scoring matrix as reference to avoid ambiguity across annotators. of difference captions collected over the five categories, withTools/Equipment being the most popular category. Thereare fewer differences in Actions which involves variationsin step order, however they still account for a significantproportion of annotated differences (12%). Fig. S5 (middle)shows the aggregate difference score for video pairs in thedataset, computed by averaging the difference score acrossall categories. While all clip pairs are expected to be simi-lar overall by design, since they are paired together if theyshare the same step label (on average, this aggregate score is3.9), they often have significant differences in one or moreindividual category. Fig. S5 (right) shows the distribution ofdifference scores only for categories where annotators labeldifference text, highlighting the spread in scores.In Fig. S6, we show word clouds of prominent conceptscaptured in each difference category, sorted by their TF-IDFscores. We exclude words with a document frequency > 0.25(e.g., person, instead, prefers etc.) to highlight category-specific concepts. We can see these concepts emerge forTools/Equipment (e.g., materials, textures), Ingredients (e.g.,ingredient names and properties), Visuals (e.g., visual at-tributes), Technique (e.g., motion-heavy words) and Actions(e.g., actions and verbs).Examples of these annotations can be seen in andFig. S4. Note that none of these video clips are used inour automatic training data generation pipeline. These area held-out subset of videos that are manually annotated forevaluation purposes only.",
  "In this section, we present complete implementation detailsfor our approach and all baselines listed in Sec. 4": "VCLM baselinesAs mentioned in Sec. 4 (baselines), wetrain our in-house VCLM and Interleaved baselines on clipsfrom HowTo100M. To re-iterate, following prior work ,MV is an Internvideo video encoder that inputs 8 uni-formly sampled frames from each video clip and gener-ates 2056 spatio-temporal tokens. MP roj is a 2-layer Per-ceiver module followed by a linear layer head to output32 tokens in the LLMs input dimension. During training,all parameters are frozen except for MP roj. For the VCLM models, we extract (video, ASR) pairsfrom automatically aligned ASR data from prior work .We use a batch size of 512 for 50k iterations. We use theAdamW optimizer, with a learning rate of 1e-4. For the Inter-leaved models, we sort (video, ASR) instances by their endtimestamp and interleave sequences of 3 clips along withtheir ASR (clip1, ASR1, clip2, ASR2 ...). The Perceivermodel converts each of the clips into 32 tokens. In additionto HowTo100M, we also train on single image captioninginstances using filtered images from LAION2B to im-prove the diversity of the training data beyond instructionalvideo content. We duplicate the single image 8 times to feedto our video backbone. During training, we sample instancesfrom each dataset in a round-robin manner. The batch sizeand number of iterations follow the VCLM models. StepDiff training detailsAs mentioned in Sec. 4 (im-plementation details), we initialize our models from theInterleaved checkpoints above. In addition to LAION andHT100M data, we also train on our generated PairQA datafrom Sec. 3.2. As before, we sample instances in a round-robin manner. We use a batch size of 256 for and train for20k iterations based on validation data.",
  "Actions: The person stirred the cheese as an additional step. Score: 3/5": "Ingredients: The person lling one cookie instead of two; The person lling plain brown cookies rather than with chocolate bits on it; The frosting has running and smooth texture compared to thick and lumpy texture. Score: 3/5 Technique: The person lled the cookie by counter clockwise motion instead of clockwise; The person starts putting frosting in the side of cookie to middle rather than start in middle of cookie; Score: 3/5",
  "Actions": "2.02.53.03.54.04.55.05.5 Agg. difference score # video pairs Difference score # diff captions Figure S5. Annotated data statistics. Left: Distribution of difference captions by category. Middle: Aggregate difference score distributionfor video pairs (averaged over categories). Right: Distribution of difference scores for categories that have annotated differences (1 = verydifferent; 5 = nearly identical).",
  "S4. Additional task formulation details": "In Sec. 3.4, we described the prompts used for downstreamtasks. To ensure that the outputs generated are in a consistentstyle with the collected annotations, we seed the generationstep with partial text, and require the model to complete it.For DiffCap, we seed with The main difference in categoryis that in Video 2,, and for DiffMCQ, we seed with InVideo 2, followed by the difference caption text that isbeing evaluated. Additionally, as mentioned in Sec. 4.1, we post-processthe outputs of each captioning baseline to match the an-notated difference structure. This is important given thesensitivity of captioning metrics to even small structuralchanges. Even with careful prompting, the baselines tend toproduce captions of the form In Video 1/2, the person ...,while in Video 2/1, ..., while the annotations are collectedin a specific format action in candidate video compared toaction in reference video (see ). The parsing involves",
  "We present additional experiments to supplement the mainpaper results in Sec. 4": "Alternate variants of VLEmbedIn our experiments, weassumed that the embeddings of a pair of videos can be repre-sented as the average of their video embeddings. We evaluateother alternatives where a difference caption is matched toa single video (either the reference or the candidate) forDiffMCQ. Note that these variants are not applicable toDiffRank, where the difference caption is not an input. Ourresults in Table S2 show that including information fromboth video clips results in the best performance, thoughthere is a small bias in the queries towards the referencevideo features.",
  "Alternate variants of the DiffMCQ taskAs mentioned inSec. 4.2, we construct the task from the DiffCap annotationsby sampling three negative video pairs for every difference": "caption that are visually similar to the true video pair, but thatdo not exhibit the true difference. We identify the negativesas follows. First, we compute the average visual embedding(CLIP features) for each reference and candidate pair in thedataset, and sort the video pairs based on this distance tothe positive pair embedding. Then, we go down this list andselect pairs that obey two criteria: (1) they do not involve thetrue reference or candidate videos and (2) they do not shareequivalent difference descriptions. For (2), we measure thesentence similarity between the ground truth difference andall of the differences for the selected pair in the category ofinterest, using MPNet embeddings. If any differencetext is too similar (above a threshold of 0.8 cosine similarity),then we ignore the pair. We continue this process until wecollect three negatives. Note that this is not the only method to construct theDiffMCQ task. For example, we can sample video pairsregardless of whether they share a reference or candidatevideo (as long as they are not the exact same pair). Thisresults in a more difficult variant of DiffMCQ, but runs therisk of selecting negatives that may share differences. A thirdalternative is to fix either the reference or candidate clip andrandomly sample the other, regardless of visual similarity ordifference text similarity. We present all three alternativesin Table S4. Across the first two variants, our approachoutperforms baselines. In the third alternative, the secondclip is selected randomly, and so the VLEmbed baselines aresufficient for identifying outliers, and all baselines performsimilarly. Moreover, the lack of constraints may permitnegatives that still match the difference caption, making thisversion unsuitable for benchmarking our models.",
  "StepDiff0.5410.3820.654": "Table S4. DiffMCQ variants for selecting negatives. V1 excludesnegatives that share the true reference or candidate video clip. Thisis the version reported in . V2 permits overlaps in reference/ candidate clips as long as the pair is not identical. V3 fixes eitherthe reference or candidate clip and randomly selects the other. Ablation experiments with lower capacity baselinesInSec. 4.5 of the main paper, we presented our method witha 13B parameter LLM backbone. In Table S3, we showresults of all baseline models with smaller variants, includ-ing Socratic (LLama-13B), AnyMAL-13B, LLaVA-7B, andIDEFICS-9B. Our results show that while smaller capacitymodels perform reasonably well in the captioning task (evenoutperforming their 70B model alternatives on the BLEUmetric), they perform worse overall on the discriminativeand ranking tasks.",
  "S6. Additional qualitative results": "We show additional qualitative samples of our methods out-puts in Fig. S7. We show various kinds of supported prompts.These are standard difference captioning used to evaluateour models (panel 1), comparative reasoning (panel 2) andmistake reasoning (panel 3). Panel 4 highlights some failurecases. These typically arise due to two reasons. First, theunderlying LLM naturally hallucinates details that are notpresent. This can happen due to inaccurate recognition (e.g.,identifying a bell pepper as a jalapeno), or incomplete con-text information (e.g., without knowing the full recipe, themodel assumes the dish is a dessert and the white powder is sugar). The second failure mode occurs when the model isforced to produce an output when differences in that categorydo not necessarily occur. This forces the model to hallucinatedetails as it is not trained to reject a query (e.g., asking whatmistake did I make in the last row). More diverse automat-ically generated training data that explicitly handles thesesituations will likely address these failure modes. Despitethese limitations, our approach can answer a wide varietyof questions and requires reasoning over multiple videos, asshown in the figure."
}