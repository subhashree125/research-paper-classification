{
  "Abstract": "In this technical report we present TrafficBots V1.5, abaseline method for the closed-loop simulation of traf-fic agents.TrafficBots V1.5 achieves baseline-level per-formance and a 3rd place ranking in the Waymo OpenSim Agents Challenge (WOSAC) 2024.It is a simplebaseline that combines TrafficBots, a CVAE-based multi-agent policy conditioned on each agents individual des-tination and personality, and HPTR, the heterogeneouspolyline transformer with relative pose encoding. To im-prove the performance on the WOSAC leaderboard, we ap-ply scheduled teacher-forcing at the training time and wefilter the sampled scenarios at the inference time.Thecode is available at:",
  ". Introduction": "The problem of closed-loop multi-agent traffic simulationcan be addressed by learning a policy for each traffic partic-ipants. Specifically, at each time step, the policy predicts thenext action of each agent, given the historical observationsform previous time steps, including the map, traffic lightsand agent trajectories. Based on TrafficBots , the Traf-ficBots V1.5 policy is shared by all agents. Different behav-iors are generated by conditioning the policy on the individ-ual destination and personality of each agent. In contrastto the SceneTransformer network architecture and inputrepresentation, which are not rotation and translation invari-ant, TrafficBots V1.5 uses the pairwise-relative representa-tion and the HPTR architecture. This greatly improvesthe accuracy of TrafficBots without sacrificing its efficiencyand scalability. Moreover, instead using a recurrent neu-ral network (RNN) to encode the temporal axis, TrafficBots",
  ". TrafficBots": "TrafficBots is a multi-agent policy built upon mo-tion prediction and end-to-end (E2E) driving. Comparedto previous data-driven traffic simulators , TrafficBotsdemonstrates superior configurability and scalability.Togenerate configurable behaviors, for each agent TrafficBotsintroduces a destination as navigational information, anda time-invariant latent personality that specifies the behav-ioral style. Unlike the goal which depends on the predictionhorizon and hence leads to causal confusions, the destina-tion approximates the output of a navigator which is avail-able in the problem formulation of E2E driving . Im-portantly, the destination indicates where the agent wantsto reach eventually, i.e., not necessarily at a specific futuretime step. In order to capture the diverse behaviors fromhuman demonstrations, the personality is learned using theconditional variational autoencoder (CVAE) followingprior works on multi-modal motion prediction . To en-sure the scalability, TrafficBots uses the scene-centric rep-resentation and presents a new scheme of positional en-coding for angles, allowing all agents to share the samevectorized context and the use of an architecture basedon Transformers.However, due to the lack of rotationand translation invariance induced by the scene-centric rep-resentation, TrafficBots does not achieve superior perfor-mance compared to methods using the agent-centric repre-sentation.",
  "arXiv:2406.10898v1 [cs.RO] 16 Jun 2024": "achieve top accuracy but lack scalability, and scene-centricmethods show superior scalability but suffer from poor ac-curacy, the pairwise-relative methods get the best of bothworlds. However, previous pairwise-relative methods aremostly using graph neural networks , which are oftenless efficiently implemented on the graphics processing unit(GPU) compared to Transformers with dot-product atten-tion. To address this problem, a novel attention modulecalled K-nearest Neighbor Attention with Relative PoseEncoding (KNARPE) is introduced in , which allows thepairwise-relative representation to be used by Transform-ers. KNARPE projects the relative pose encoding (RPE) andadds them to the keys and values to obtain zi, the output ofletting token i attend to its K nearest neighbors Ki ,",
  "D(4)": "where ui, uj are the local attributes of token i and j repre-sented in their local coordinates, rij is the relative pose be-tween token i and j, ij are the attention weights, eij are thelogits, W {q,k,v}, b{q,k,v} are the learnable projection matri-ces and biases for query, key and value, and W {k,v},b{k,v}",
  "i {0, . . . , D/2 1},(10)": "where (xij, yij, ij) are the 2D location and yaw headingof token j represented in the coordinate of token i, is thebase frequency, and D is the embedding dimension.Based on KNARPE, a pure Transformer-based frame-work called Heterogeneous Polyline Transformer withRelative pose encoding (HPTR) is presented, whichuses a hierarchical architecture to enable asynchronous to-ken update and avoid redundant computations. By sharingcontexts among traffic agents and reusing the unchangedcontexts in driving scenarios, HPTR is as efficient as scene-centric methods, while performing on par with state-of-the-art agent-centric methods on marginal motion predic-tion tasks. Since our method TrafficBots V1.5 is entirely",
  ". Architecture": "The network architecture of TrafficBots V1.5 is illustratedin . We make minimal changes while applying HPTRto TrafficBots. We remove the temporal RNN from Traf-ficBots, and follow HPTR to use stacked historical observa-tions as input and PointNet to aggregate the tempo-ral axis. The policy network, the personality predictor, andthe destination predictor of TrafficBots are now all basedon the pairwise-relative representation and KNARPE atten-tion module. We keep the intra-map, enhance-traffic-lightand enhance-agent Transformers of HPTR. Following Traf-ficBots, multi-modal outputs are generated by conditioningthe policy on each agents individual destination and per-sonality. Therefore, the anchors and the anchor-to-all Trans-former of HPTR are discarded. Instead of a learnable priorpersonality of TrafficBots, we use a standard Gaussian forthe prior personality. We also tried to add a traffic light statepredictor, but its accuracy was not good enough to improvethe overall simulation performance on the leaderboard.",
  ". Training": "We found two techniques that improve the performance ofTrafficBots V1.5. Firstly, we adopt a larger free nats equals to 1.0 for the KL-divergence between the posteriorand the prior personality. This allows the posterior person-ality to encode more information. Secondly, we use sched-uled sampling and apply teacher-forcing to 30% agentsat the beginning of the training, i.e., these agents will betrained via open-loop behavior cloning. The percentage ofteacher-forcing decreases linearly to 0 during the training.Due to limited computational resources, we train our mod-els for 5 days on 4 NVIDIA RTX 4090 GPUs. Training forlonger time or using more GPUs should improve the perfor-mance further. We use a total batch size of 8 and we batchover scenarios. Each scenario contains 91 time steps and amaximum of 64 agents. Following TrafficBots, the traininguses back-propagation through time and the training loss in-cludes the following terms:1. Reconstruction loss that trains the model to reconstructthe ground-truth (GT) states using the posterior person-ality and the GT destination. It is a weighted sum of: A smoothed L1 loss between the predicted and the GT(x, y) positions.",
  "Stacked History Agent States :": ". Network architecture of TrafficBots V1.5. In the brackets are the tensor shapes, where the hidden dimensions are omittedfor conciseness. B is the batch size, which is also the number of episodes. NM, NC, NA are, respectively, the number of map polylines,traffic light polylines and agent trajectories. Nnode is the number of segments in each polyline. T is the length of the stacked historicalobservations. The destination predictor and personality predictor are not visualized. They have a similar structure to the policy network.",
  "A smoothed L1 loss between the predicted and the GTvelocities": "2. The KL divergence between the posterior and the priorpersonality. We use free nats to clip the KL diver-gence, i.e., if KL(zpost, zprior) is smaller than the freenats, then the KL loss is not applied. 3. The cross entropy loss for destination classification.Since the GT destination is a single class, this loss boilsdown to a maximum likelihood loss, i.e., the destinationdistribution is trained to maximize the log-likelihood ofthe polyline index of the GT destination.During training, we auto-regressively rollout the policy us-ing the GT destination and the posterior personality. Beforethe auto-regressively rollout, we use the reparameterizationtrick to sample the posterior personality, such that it can betrained to reconstruct the GT trajectories. For 10% of theepisodes we rollout with the prior personality instead of theposterior personality. During the auto-regressively rollout,we take the mean of the actions such that the gradient can beback-propagated through the differentiable vehicle dynam-ics. We do not apply a loss that encourages collision avoid-ance because this will bias the model, even thoughit could improve the performance on the leaderboard.",
  ". Inference": "Instead of bias the model directly, we apply a milder ap-proach to bias the models outputs towards safer behav-ior and hence improve the collision-based metrics on theWOSAC leaderboard. Specifically, we sample 128 scenar-ios at the inference time and select 32 scenarios that containthe least collision events. To sample a scenario, we firstsample the personality and destination for each agent, afterthat we start the auto-regressive rollout. We use the mean of the predicted action distribution, hence the rollout is com-pletely deterministic given the sampled personality and des-tination. Except the episode filtering, we do not apply anyother post-processing or model ensembling techniques. Atthe inference time, HPTR allows different types of tokensto be updated asynchronously at different frequency. There-fore, an accurate analysis of FLOPS turns out to be difficult.For simplicity, we only profile the map encoder, which is thecomputationally heaviest module in our model. For a singleepisode, the map encoder uses 20 GFLOPS. Since HPTRallows the map features to be cached and reused during therollout, the map is encoded only once before the simulationstarts. Other modules, e.g. the personality encoder, the traf-fic light encoder and the policy, are computationally muchlighter than the map encoder. Based on the FLOPS of themap encoder, we estimate that each iteration of the auto-regressive policy rollout should require approximately anorder of magnitude fewer FLOPS, i.e., around 2 GFLOPS.",
  ". Implementation details": "Overall, we use a hidden dimension of 128. We use Trans-former with pre-layer normalization, and the attention mod-ule has 4 heads and a feed-forward dimension of 512. Thesampling rate is fixed to 10 FPS at both the training and in-ference time. At each time step, our method predicts theactions one step ahead, based on the previous observations.We do not differentiate between the ego-agent, i.e., the au-tonomous vehicle agent, and other agents. Similar to Traf-ficBots, TrafficBots V1.5 allows agents that are not con-trolled by itself. Since TrafficBots V1.5 predicts the actionfor all observed agents, the actions will still be predicted forthose agents, but then these actions will be discarded; thebehavior of those agents will be overridden by some othercontrol modules, e.g. an AV planner software or log-reply.Each episode contains at most 1024 map tokens, 128 traf-",
  "Method nameRealism metametric Kinematicmetrics Interactivemetrics Map-basedmetrics minADE": "SMART-large 0.75640.47690.79860.86181.5501BehaviorGPT 0.74730.43330.79970.85931.4147GUMP0.74310.47800.78870.83591.6041model predictive submission0.74170.41820.79420.85911.4842MVTE 0.73020.45030.77060.83811.6770VBD0.72000.41690.78190.81371.4743TrafficBots V1.5 (Ours)0.69880.43040.71140.83601.8825cogniBot v1.50.62880.32930.71290.6918N/Alinear extrapolation baseline0.39850.22530.43270.45337.5148 fic light tokens and 64 agent tokens, where invalid tokensare masked. Each map token corresponds to a polyline con-sisting of up to 20 segments, each 1 meter in length. Eachtraffic light token corresponds to the polyline it associated toand the traffic light state. Each agent token corresponds toan agent trajectory. We use a sliding window approach andstack the historical observations from the last 11 steps. Forthe Transformers with K-nearest-neighbor attention, eachmap token attends to the 32 nearest map tokens. Each trafficlight token attends to 24 nearest map tokens and 24 nearesttraffic light tokens. Each agent token attends to 64 map to-kens, 25 traffic light tokens and 25 agent tokens in its prox-imity. More details about the network architecture can befound in the open-source repository of TrafficBots V1.5, aswell as in the TrafficBots and HPTR papers. Ourmodel does not take the z axis, i.e., the altitude dimension,into account. During inference, we assume that the z di-mension of agent trajectories remains constant and is equalto its last observed value.",
  ". Results": "We have not run any ablation studies for our method. How-ever, we would like to point out some promising directionsfor ablations, including the discrete action space, weightsof different losses, and the collision avoidance loss. Wehave done a manual and rough parameter tuning for ourmodel. The performance of our method on the WOSACleaderboard is shown in . More details aboutthe challenge and the metrics can be found in the WOSACpaper . Our method achieves baseline-level performancein terms of the realism meta metric, which is a weightedsum of other metrics except the minADE. We apply a uni-cycle model for the dynamics of all types of agents andselect the parameters heuristically. This allows our sim-ulation to generate smooth trajectories, but it also affectsthe kinematic metrics negatively. Our TrafficBots V1.5 isoutperformed by other methods in terms of interactive met- rics which involve collision avoidance. We believe adding aloss that encourages collision avoidance would help, but itis controversial if a collision-free simulation would be use-ful for the development of autonomous driving algorithms.In terms of map-based metrics that consider off-road driv-ing, our model performs comparably to other methods TheminADE of TrafficBots V1.5 is significantly larger thanother methods on the leaderboard, which is a known prob-lem inherited from TrafficBots. Overall, from weobserve that GPT-based architectures that rely on to-kenization and next-token prediction, such as SMART andBehaviorGPT, achieve top performance. Interestingly, noneof these GPT-based architectures uses goal or personalityconditioning, but they are still able to generate multi-modaloutputs. It seems that the multi-modality in traffic simu-lation can be addressed using tokenization and the cross-entropy loss. This indicates that the poor performance ofTrafficBots might be caused by the CVAE and regressionlosses on continuous states and actions. Another interestingthing is that the GPT-based methods achieve the best perfor-mance without considering the traffic lights. This indicatesthat the dataset might be imbalanced and the evaluation met-rics might be flawed.",
  ". Conclusion": "In this technical report we present TrafficBots V1.5, whichis a baseline method that combines TrafficBots, a priorwork on the closed-loop traffic simulation using CVAE,and HPTR, a prior work on the Transformer-based motionprediction using the pairwise-relative representation. Ourmethod is the only CVAE-based method on the WOSACleaderboard 2024.The performance of our method isslightly worse than the GPT-based methods. However, asa baseline method that involves minor novelty, it achievesthe performance we expected, and there are many possibil-ities to improve this simple baseline. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and NoamShazeer. Scheduled sampling for sequence prediction withrecurrent neural networks. Advances in neural informationprocessing systems (NeurIPS), 28, 2015. 2 Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, RenjieLiao, and Raquel Urtasun. Implicit latent variable model forscene-consistent motion forecasting. In European Confer-ence on Computer Vision (ECCV), 2020. 1 Alexander Cui, Sergio Casas, Kelvin Wong, Simon Suo, andRaquel Urtasun. Gorela: Go relative for viewpoint-invariantmotion forecasting.In IEEE International Conference onRobotics and Automation (ICRA), 2023. 1, 2, 3 Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, DragomirAnguelov, Congcong Li, and Cordelia Schmid. Vectornet:Encoding hd maps and agent dynamics from vectorized rep-resentation. In Conference on Computer Vision and PatternRecognition (CVPR), 2020. 2 Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Ville-gas, David Ha, Honglak Lee, and James Davidson. Learninglatent dynamics for planning from pixels. In Internationalconference on machine learning (ICML), 2019. 2, 3 Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bron-stein, Rebecca Roelofs, Benjamin Sapp, Brandyn White,Aleksandra Faust, Shimon Whiteson, et al. Imitation is notenough: Robustifying imitation with reinforcement learningfor challenging driving scenarios. In International Confer-ence on Intelligent Robots and Systems (IROS), 2023. 3 Nico Montali, John Lambert, Paul Mougin, Alex Kuefler,Nicholas Rhinehart, Michelle Li, Cole Gulino, Tristan Em-rich, Zoey Yang, Shimon Whiteson, et al. The waymo opensim agents challenge. Advances in Neural Information Pro-cessing Systems (NeurIPS), 2024. 4",
  "Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, KratarthGoel, Khaled S Refaat, and Benjamin Sapp.Wayformer:Motion forecasting via simple & efficient attention networks.In ICRA, 2023. 1": "Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zheng-dong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, RebeccaRoelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al.Scene transformer: A unified architecture for predicting fu-ture trajectories of multiple agents. In ICLR, 2021. 1 Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.Pointnet: Deep learning on point sets for 3d classificationand segmentation. In Conference on Computer Vision andPattern Recognition (CVPR), 2017. 2",
  "Chris Zhang, James Tu, Lunjun Zhang, Kelvin Wong, SimonSuo, and Raquel Urtasun. Learning realistic traffic agentsin closed-loop. In Conference on Robot Learning (CoRL),2023. 3": "Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu,and Luc Van Gool. End-to-end urban driving by imitating areinforcement learning coach. In International Conferenceon Computer Vision (ICCV), 2021. 1 Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu,and Luc Van Gool. Trafficbots: Towards world models forautonomous driving simulation and motion prediction. In In-ternational Conference on Robotics and Automation (ICRA),2023. 1, 2, 4 Zhejun Zhang, Alexander Liniger, Christos Sakaridis, FisherYu, and Luc V Gool. Real-time motion prediction via het-erogeneous polyline transformer with relative pose encod-ing.Advances in Neural Information Processing Systems(NeurIPS), 2024. 1, 2, 4 Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang,Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, andChun Jason Xue.Behaviorgpt: Smart agent simulationfor autonomous driving with next-patch prediction. arXivpreprint arXiv:2405.17372, 2024. 4"
}