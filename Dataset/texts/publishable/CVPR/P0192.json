{
  "Input PersonInput GarmentsTry-On OutputInput PersonInput GarmentsTry-On OutputOutput with Layout": ". Given an input person image, multiple garments, M&M VTO can output a virtual try-on visualization of how those garmentswould look on the person. Our model performs well across various body shapes, poses, and garments. In addition, it allows layout to bechanged, e.g., roll up the sleeves (top rightmost column), and tuck in the shirt and roll down the sleeves (bottom rightmost column).",
  "Abstract": "We present M&M VTOa mix and match virtual try-onmethod that takes as input multiple garment images, text de-scription for garment layout and an image of a person. Anexample input includes: an image of a shirt, an image of apair of pants, rolled sleeves, shirt tucked in, and an imageof a person. The output is a visualization of how those gar-ments (in the desired layout) would look like on the given",
  "Work done while author was an intern at Google": "person. Key contributions of our method are: 1) a singlestage diffusion based model, with no super resolution cas-cading, that allows to mix and match multiple garments at1024512 resolution preserving and warping intricate gar-ment details, 2) architecture design (VTO UNet DiffusionTransformer) to disentangle denoising from person specificfeatures, allowing for a highly effective finetuning strategyfor identity preservation (6MB model per individual vs 4GBachieved with, e.g., dreambooth finetuning); solving a com-mon identity loss problem in current virtual try-on meth-ods, 3) layout control for multiple garments via text inputs",
  ". Introduction": "Virtual try-on (VTO) is the task of synthesizing how a per-son would look in various garments based on provided gar-ment photos and a person photo.Ideally the synthesisis high resolution, showcasing the intricate details of gar-ments, while at the same time representing the body shape,pose, and identity of the person accurately. In this paper,we focus specifically on multiple garment VTO and editing.For example, a user of our method would provide one ormore photos for garments, e.g., shirt, pants, and one photoof a person, with additional optional text input to requesta layout, e.g., shirt tucked out, rolled sleeves. The gar-ment photos could be either a product photo (layflat) or thegarment as worn by a different person. The person photowould be a full field-of-view photo showing the person headto toe. Our method, which we named, M&M VTO, outputsa visualization of how the person looks in those garments. shows a couple of examples.Redefining the VTO problem as multiple-garment VTO,rather than the commonly targeted single garment VTO, al-lowed us to deeply rethink architecture design and solveseveral open problems in multi, as well as single VTO net-works, in addition to opening up the new possibilities formix and match and editing layouts.Two of the most challenging VTO problems are (1) howto preserve the small but important details of garmentswhile warping the garment to match various body shapes,and (2) how to preserve the identity of the person withoutleaking the original garments that the person was wearing tothe final result. state-of-the-art methods came close for sin-gle garment VTO by leveraging the power of diffusion, andbuilding networks that denoise while warping, e.g., Parallel-Unet . To address (1), however, the network requiresto max out the number of parameters and a memory heavyParallel-UNet to warp a single garment. For (2) a clothing-agnostic representation is typically used for the person im-age to erase the current garment to be replaced by VTO, butat same time it removes a significant amount of identity in-formation, with the network needing to hallucinate the rest,resulting in loss of characteristics like tattoos, body shapeor muscle information.With more garments, as in multi-garment VTO, the num-ber of pixels needed to go through the network triples, sothe same number of parameters would create a lower qual-ity VTO. Similarly, showing head to toe person and allow-ing multiple garments, means clothing-agnostic represen- tation leaves even less of the identity of the personif justa shirt needs to be replaced, the network can still see howthe bottom part of that person looks like (and shape of thelegs), while if all garments are changing the agnostic wouldpreserve even less information about the person.Our solution, M&M VTO, is three-fold as depicted in. First, we designed a single-stage diffusion modelto directly synthesize 1024512 images with no need forextra super-resolution(SR) stages as commonly done bystate-of-the-art image generation techniques. We found thatas we expand the scope of VTO, having cascaded design isdetrimental as the base models low resolution assumes ex-cessive downsampling of ground truth during training, thuslosing forever garment details; as SR models depend heav-ily on the base model, if the details disappear they can not beupsampled effectively. Training a single stage base modeljust on higher resolution data, however, does not solve theproblem, as the model doesnt converge even with ideasproposed in .Instead we designed a progressivetraining strategy where model training begins with lower-resolution images and gradually moves to higher-resolutionones during the single stage training. Such a design natu-rally benefits training at higher resolutions by utilizing theprior learned at lower resolutions, allowing the model tobetter learn and refine high-frequency details.Second, to solve the identity loss (and/or clothing leak-age) during the clothing-agnostic process, we propose aspace saving finetuning strategy. Rather than finetuning theentire model during post processing, as commonly done bytechniques like DreamBooth , we choose to finetuneperson features only.We designed a VTO UNet Diffu-sion Transformer (VTO-UDiT) to isolate encoding of per-son features from the denoising process. In addition to pro-ducing much higher quality results, this design also drasti-cally reduces finetuned model size per new individual, go-ing from 4GB to 6MB.Third, we created text based labels representing vari-ous garment layout attributes, e.g., rolled sleeves, tuckedin shirt, and open jacket. We formulated attribute extractionas an image captioning task and finetuned a PaLI-3 model using only 1.5k labeled images. This allows us to auto-matically extract accurate labels for the whole training set.Above three design choices are critical in producing highquality VTO results for multi-garment scenarios. We per-form detailed ablation studies, and comparisons to state-of-the-art papers to illustrate each design choice. Our methodsignificantly outperforms others. The user study shows thatour method is chosen as best 78.5% of the time comparedto state-of-the-art on multiple-garment VTO task.",
  "DiT": ". Overview of M&M VTO. Left: Given multiple garments (top and bottom in this case, full-body garment not shown for thisexample), layout description, and a person image, our method enables multi-garment virtual try-on. Right: By freezing all the parameters,we optimize person feature embeddings extracted from the person encoder to improve person identity for a specific input image. Thefine-tuning process recovers the information lost via agnostic computation.",
  "hensive list of recent papers in virtual try-on we also invitethe reader to review this list1": "Image-Based Virtual Try-On.The seminal VITONmethod proposed a warping model that estimates pixeldisplacements between the original garment image and tar-get warp.Based on those displacements, it warped thegarment, and then used a blending model to combine thewarped garment with the person image, showing one ofthe first promising results for VTO. Many works followed,to improve pixel displacement estimation. proposedthin plate splines, predicted target segmentation andparsing for improved warping, student-teacher approachand distillation were proposed by.Other ef-forts include adaptive parsing and second order constrainton thin plate splines , optimization to remove mis-alignments , leveraging dance videos to improve warp-ing , regularizing , and using self and cross atten-tion to improve flow computation . With the rise of Style-GAN, proposed StyleGAN for optical flow, pro-posed a generator-discriminator approach, re-ported improved results for flow compute and inpainting byutilization of landmarks, and incorporated size informa-tion.While results were improving, there was an inherent dif-ficulty in warping garments explicitlypixel wise, as thereis too much variation in folds, logos, texture where a gar-ment image needs to warp to a new body shape. Ratherthan estimating flows directly, proposed to interpolate StyleGAN coefficients to create try-on, still lacking com-plex textures, though, due to the averaging nature of Style-GAN. TryOnDiffusion introduced a diffusion-based Parallel-UNet enabling implicit warping andblending in the same model via cross-attention, showingsignificantly better results. Key limitations of that approachwere incomplete garment details due to base model beingonly 128128 resolution, and identity preservation.Fi-nally, most of those methods are focused on single garmenttry-on only. Finetuning Diffusion. As finetuning is a general conceptand wasnt much used for VTO, we will review recentworks for any general finetuning. Sometimes also calledpersonalization , finetuning is the task of adjusting anexisting, say text to image generation model, to a specifictask, e.g., style transfer. Dreambooth showed fantas-tic results by finetuning on a few images, and accompa-nying text, to bind a unique identifier with a specific sub-ject. learned encoders to transfer visual conceptinto textual embeddings. created a network that mapsnoise timestamp and layer to text token space. To improvemulti-concept composition , Custom Diffusion op-timized concept embeddings along with key and value pro-jection matrices of cross attention layers in the text-to-image model. In contrast, our approach is tailored for VTOand requires only 6MB of parameters per person during theinference phase.",
  "Shared": ". VTO-UDiT architecture. For image inputs, UNet encoders (Ezt, Ep, Eg) extract features maps (Fzt, Fp, F g ) from zt, Ia,Ic , respectively, with {upper, lower, full}. Diffusion timestep t and garment attributes ygl are embedded with sinusoidal positionalencoding, followed by a linear layer. The embeddings (Ft and Fygl) are then used to modulate features with FiLM or concatenated tothe key-value feature of self-attention in DiT similar to . Following , spatially aligned features(Fzt, Fp) are concatenated whereasF g are implicitly warped with cross-attention blocks. The final denoised image x0 is obtained with decoder Dzt, which is architecturallysymmetrical to Ezt. 37, 38, 42, 49]. SDEdit added noise to the inputs andthen subsequently denoised them through a stochastic pro-cess. Palette trained a conditional diffusion model forspecific edit tasks. BlendedDiffusion , inspired by CLIPguided diffusion , utilized CLIP text encoder andspatial masks to edit images by blending noised input im-ages with locally generated contents. Requiring masks isnot applicable to VTO tasks e.g., tuck this shirt in.The success of text to image diffusion models [23, 42, 46, 47] led to text-based image editing . For example, DiffEdit infers a regionmask based on text instructions, and then guides image edit-ing using inverted noise resulted from DDIM inversion pro-cess . Prompt-to-Prompt (P2P) edits images usingonly text by manipulating the cross-attention scores con-ditioned on inverted latents. Null-text inversion opti-mized on null-text embeddings by minimizing differencesbetween latent codes from unconditional inversion processand conditional one. InstructPix2Pix directly manipu-lates image in the denoising process by using finetuned Sta-ble Diffusion trained on paired examples generated usingP2P technique with given editing instructions. Generally,text based editing, while allowing for easier input (com-pared to masks), often creates the edit but fails to preserveoriginal image details, e.g., in VTO case the original gar-ment details are lost with such techniques. We solve it viaVTO specific finetuning on PaLI-3 and then using it as con-dition in the network.",
  ". Method": "Given a person image Ip, an upper-body garment imageIupperg, a lower-body garment image Ilowergand a full-bodygarment image Ifullg , our method synthesizes VTO result Itrfor person p. Optionally, a layout attribute is provided asinput as well. We begin by describing training data and itspreprocessing, and then the model design of M&M VTO.",
  ". Dataset Preparation and Preprocessing": "M&M VTO is trained on pairsperson image Ip, and a gar-ment image Ig. Ig can be an image of a garment laid outon a flat surface (layflat), or an image of a person wearingthe garment (most often in another pose). As the pair as-sumes that they share only one or two garments rather thanall three of upper, lower and full, we do the following sim-ple process. We compute a garment embedding for each ofthe three garments (determined by segmentation) and com-pare which one appears on the person image. The ones thatdo not are set to 0. Each pair is then processed following . Conditionalinputs ctryon includes clothing-agnostic RGB Ia, segmentedgarment Ic , 2D pose keypoints Jp for the person image Ipand 2D pose keypoints Jg for garment images Ig (Jg is avector with all -1s if Ig is a layflat garment image). Tomake sure that background is as tight as possible (allowingfor the model to fully focus on garments) we crop and resizeall images to 1024512, approximately resembling aspectratio of a photograph of a head to toe person. We also introduce a layout input ygl, defining desired at-tributes of the garments. We only focus on attributes thatone can do in real-life, for example: roll up sleeves, tuckin the shirt, etc. rather than changing texture or garmentproperties. Full set of attributes is in the supplementarymaterial. One way to calculate attributes of each garmentis by training a classifier for each attribute. We chose in-stead to finetune a large vision language model (PaLI-3).Specifically, we convert all attributes into a formatted textand formulate it as an image captioning task. There aretwo advantages for this formulation. First, vision languagemodels have strong priors trained on large datasets and canutilize the correlation between different garment layout at-tributes (e.g. the sleeve can not be rolled up if the sleevetype is sleeveless). Second, using a single model can alsoaccelerate the training data generation process. Thanks to",
  "Input GarmentsInput PersonTryOnDiffusionGP-VTONOurs": ". Qualitative Comparison with existing Try-On methods. On the left, we compare with TryOnDiffusion on our test set andfurther evaluate on DressCode dataset, as shown on the right. Our method can generate better garment details and layouts. the strong prior encoded in the PaLI-3 model, we are ableto get very accurate garment attributes by finetuning PaLI-3with only 1,500 images. To get ygl for each training sample,we first extract garment layout attributes relevant to the gar-ment type by running finetuned PaLI-3 on Ig , and thenconcatenate those attributes into a single vector. Refer tothe supplementary for more details.",
  ". Single Stage M&M VTO": "Cascaded diffusion models, i.e., lower resolution diffu-sion base model, followed by super resolution models,have shown great success for text to synthetic image gen-eration .Similarly, for VTO followed asimilar setup where three stages were used.For multi-garment VTO, however, such design is performing poorly,as the base model doesnt have enough capacity to cre-ate intricate warps and occlusions based on persons bodyshape. We observed that high-frequency garment detailsare smoothed and blurred out if images are downsampledby more than 2 times. Thus, it is impossible for base dif-fusion models trained to preserve those garment details astheir groundtruth images do not include them.Ideally we would just synthesize 1024512 images withthe base model directly. This turned out to be a challengingtask, as if the cross-attention is applied at a lower resolution,the high frequency image details are destroyed by excessivedownsampling of feature maps, and the model tends to learna global structure for the warping. On the other hand, apply-ing cross-attention at a higher resolution does not convergeunder random initialization from our initial experiments.To tackle this challenge, we use an effective progressive training paradigm for M&M VTO. The key idea is to ini-tialize the higher resolution diffusion models using a pre-trained lower resolution one.Specifically, we first traina base diffusion model to synthesize 512256 try-on re-sults I512256tr, where the cross-attention happens in 3216.After that, we continue to train the exact same model tosynthesize 1024512 try-on results I1024512tr, where thecross-attention happens in 6432 with the same architec-ture. Note that our training algorithm does not require mod-ifying or adding new components to the architecture, all weneed is to train the model with data in different resolutions,which is easy to implement.",
  "x0 = x(zt, t, ctryon)(1)": "where t is the diffusion timestep, zt is the noisy imagecorrupted from the ground-truth x0 at timestep t, ctryon is thetry-on conditional inputs, and x0 is the predicted clean im-age at timestep t. In practice, we follow to set the net-work output in v-space to avoid color drift issues in higherresolution diffusion models. Given the predicted vt, wecompute x0 = tzt tvt, where t, t (0, 1) controlthe signal-to-noise ratio.Inspired by , we change the Parallel-UNet architec-ture into a UDiT architecture where the transformerblock is implemented as DiT . With the combination ofUNet and DiT, the model benefits from light weight UNetas image encoders and the heavy DiT blocks to process inlower resolution feature maps for attention operations.",
  "Imagen EditorInput GarmentsInput PersonOurs EditingSDXL InpaintingInstructP2PDiffEditP2P + NI": ". Qualitative Comparison for Garment Layout Editing. Top: editing instruction is to tuck out the shirt. Bottom: roll downthe sleeve. Our method enables more accurate layout editing while preserving the details from the inputs. Details are provided in theSupplementary. Moreover, the design of UDiT fully disentangle the en-coding process of ctryon from the denoising process, whichis critical for person feature finetuning described later in.4. More specifically, 1) Different UNet encodersare used to process the input images without informationexchange. 2) Only Ezt takes diffusion timestep t embed-ding as as input, while Ep and Eg do not, to fully disentan-gle conditional features from diffusion denoising. 3)UnlikeParallel-UNet which updates both conditional featuresand noisy image features in parallel, VTO-UDiT fixes theconditional features and only updates diffusion features dur-ing the forward pass of DiT blocks.Also, note that all UNet encoders are fully convolutionaland free of attention operations, which is preferable for pro-gressive training mentioned in .2.",
  ". Efficient Finetuning for Person Identity": "A key challenge of current VTO methods is the loss of per-son identity due to the use of clothing-agnostic representa-tion. To tackle this problem, we propose a space-efficientfinetuning strategy based on our VTO-UDiT architecture.As described in Sec. 3.3, person feature Fp is independentof diffusion or garment related features, and is kept fixedfor DiT blocks where conditioning happens. Thus, we areable to directly finetune the person features instead of thewhole diffusion model. This greatly reduces the optimiz-able weights from 4GB to 6MB. Furthermore, we foundfinetuning on person features will not cause the model to overfit on the particular garments worn by the target personas shown in .The finetuning process needs to learn how to warp gar-ments from varying sizes and poses on the target person,however, acquiring pairs of images of same garment andvarious shapes and sizes is impractical. Instead we use pre-trained M&M VTO to prepare a synthetic dataset. We seg-ment out garments worn by the target person image, andtry-on the garment on multiple person images across var-ious poses (e.g. different torso orientations and arm posi-tions) and body shapes (from 2XS to 2XL), resulting in 150samples. Since our pretrained M&M VTO can accuratelypreserve but warp garment details to new pose and shape,the quality of the synthetic finetuning data is high, and al-lows us to reconstruct the person identity when tested onunseen garments.",
  "In this section, we describe datasets, comparisons and abla-tions. Additional results as well as implementation detailscan be found in supplementary": "Datasets. Our model is trained on two types of datasets:1) garment paired dataset of 17 Million samples, whereeach sample consists of two images of the same garment intwo different poses/body shapes, 2) layflat paired datasetof 1.8 Million samples, where each sample consists of animage with garment laid out on a flat surface and an imageof a person wearing the garment. For testing, we use two",
  "Hard to tellN/AN/A262N/AN/A177": ". Quantitative Comparison. We evaluate on our 8, 300triplets test set and DressCode triplets test set. GP-VTON istrained on layflat garments, thus we report only on DressCode testset. The metrics are FID, KID, and user study (US). All baselinesare run twice sequentially, first for tops then for bottoms try-on(See ).",
  "sets: 1) we collected 8, 300 triplets (top, bottom, person)that are unseen during training, 2) we use DressCode just for comparison with other methods that use it": "Comparison of VTO. We compared with two represen-tative state of the art methods: TryOnDiffusion , andGP-VTON . Other methods dont provide code at thetime of submission. Our 8, 300 triplets test set was usedto compare to TryonDiffusion, and DressCode triplets un-paired test set was used to compare to both GP-VTON andTryonDiffusion. As TryOnDiffusion was trained only ontops, and person images, we retrained it on our datasetfor upper-body, lower-body, and full-body garments sepa-rately. For GP-VTON, we used officially released check-points trained on DressCode. Then we ran inference se-quentially first to produce top VTO, and then bottom VTO. shows that M&M VTO outperforms baselines inaspects such as garment interactions, warping, and detailpreservation. shows that our method outperformsbaselines for FID , KID (scaled by 1000 follow-ing ) and user study (US). In the user study, 16 non-experts were asked to either select the best result or opt forhard to tell. The findings indicate that users generally pre-fer M&M VTO over other methods. We provide results forsingle garment try-on, and other comparisons in supplemen-tary. Comparison of Editing.We evaluate our approach bycomparing with several text-guided image editing methods.Inpainting mask free: Prompt-to-Prompt (P2P) + Nullinversion (P2P + NI) and InstructPix2Pix (IP2P) using a target text prompt and an input image that we wishto perform editing on. With inpainting mask: Imagen edi-tor , DiffEdit and SDXL inpainting . demonstrates that our method can interpret garment layoutconcepts more effectively, allowing for more precise editsof the targeted part without affecting other areas. We pro-vide quantitative comparison and additional details aboutspecific prompts and input masks in supplementary. Finetuning Comparison. We compare to three baselines:non-finetuned model, finetuning the full model and finetun-ing the person encoder. For the latter two baselines, wehave incorporated the class-specific prior preservation loss,",
  "Progressive Training": ". Ablation Comparison. We provide qualitative zoom-in visualization to compare our progressive training with cascadedmodels and the model trained from scratch. Our approach cangenerate better garment details, i.e., more accurate texts. See sup-plementary for full images. as utilized in DreamBooth , to prevent overfitting to theclothing worn by the target person. For our approach, wedont apply such regularization technique as we found ourmethod does not suffer from overfitting. show-cases that our method successfully retains characteristics ofthe human models (e.g., body shape) without compromisingthe details of the garments.Ablation for Single Stage Model vs. Cascaded.Ourmethod generates 1024512 try-on images in a singlestage. For the cascaded variant, we trained a 512256 basediffusion model, followed by a 512256 1024512 SRdiffusion model. Both models share the same architectureas our single-stage model, with the distinction that the SRmodel concatenates the low-resolution image to the noisyimage. illustrates that our single stage model ex-cels at maintaining complex garment details like tiny textsor logos.Ablation for Progressive Training vs. Training fromScratch.We train an identical model from scratch on1024512 data, without leveraging any model pretrainedin lower resolutions. highlights that our pro-gressively trained model more effectively manages garmentwarping under significant pose variations, whereas the ab-lated version struggles with learning implicit garment warp-ing through cross-attention.Limitations. Firstly, our approach isnt designed for lay-out editing tasks, such as Open the outer top. As demon-strated in (left), a random shirt is generated by themodel, as no specific information is provided from inputsabout what should be inpainted in the open area. Secondly,our method struggles with uncommon garment combina-tions found in the real world, like a long coat paired withskirts. As shown in the right example of , the modeltends to split the long coat in an attempt to show the skirts,because it learned from examples where both garments aretypically visible during training. Thirdly, our model faceschallenges when dealing with upper-body clothing from dif-ferent images, e.g. pairing a shirt from one photo with anouter coat from another. This issue mainly stems from thedifficulty in finding training pairs where one image clearlyshows a shirt without any cover, while another displays the",
  "Ours with Fine-tuning": ". Qualitative Comparison on Person Fine-tuning Strategy. We provide a comparison with various types of fine-tuning strategies.Our method shows a better person identity preservation than fine-tuning the whole model or person encoder only. Red boxes highlightexample errors, e.g., sleeves too short, and extra fabric. same shirt under an outer layer. As a result, the model strug-gles to accurately remove the shirt when its covered by anouter layer during testing. Finally, note that our method vi-sualizes how an item might look on a person, accounting fortheir body shape, but it doesnt yet include size informationnor solves for exact fit.",
  ". Conclusion": "We present a method that can synthesize multi-garment try-on results given an image of person and images of upper-body, lower-body and full-body garments. Our novel archi-tecture VTO-UDiT as well as progressive training strategy,enabled better than state-of-the-art results, particularly inpreserving fine garment details and person identity. Fur-thermore, our method allows for explicit control of garmentlayout via conditioning the model with garment attributesobtained from a finetuned vision-language model.",
  "Yuval Alaluf, Elad Richardson, Gal Metzer, and DanielCohen-Or. A neural space-time representation for text-to-image personalization, 2023. 3": ". Failure Cases. Our model could generate random cloth-ing given layout information. As shown in the left example, givenouter top open, the model generates a random inner top. In ad-dition, the model could lead to failures when dealing with raregarment combinations. For example, given a long coat and skirtcombination, it creates a half open coat, shown in the right image. Omri Avrahami, Dani Lischinski, and Ohad Fried. Blendeddiffusion for text-driven editing of natural images. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1820818218, 2022. 3, 4 Shuai Bai, Huiling Zhou, Zhikang Li, Chang Zhou, andHongxia Yang. Single stage virtual try-on via deformableattention flows. In European Conference on Computer Vi-sion, pages 409425. Springer, 2022. 3",
  "Mikoaj Binkowski, Danica J Sutherland, Michael Arbel, andArthur Gretton. Demystifying mmd gans. arXiv preprintarXiv:1801.01401, 2018. 7": "Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-structpix2pix: Learning to follow image editing instructions.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1839218402, 2023.4, 7, 12, 13 Chieh-Yun Chen, Yi-Chung Chen, Hong-Han Shuai, andWen-Huang Cheng.Size does matter: Size-aware virtualtry-on via clothing-oriented transformation try-on network.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 75137522, 2023. 3",
  "Ting Chen. On the importance of noise scheduling for diffu-sion models. arXiv preprint arXiv:2301.10972, 2023. 2": "Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,Jialin Wu, Paul Voigtlaender, Basil Mustafa, SebastianGoodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al.Pali-3 vision language models: Smaller, faster, stronger.arXiv preprint arXiv:2310.09199, 2023. 2, 4, 12 Seunghwan Choi, Sunghyun Park, Minsoo Lee, and JaegulChoo.Viton-hd:High-resolution virtual try-on viamisalignment-aware normalization.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1413114140, 2021. 3",
  "appearance flows.In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages84858493, 2021. 3": "Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry SDavis. Viton: An image-based virtual try-on network. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 75437552, 2018. 3 Sen He, Yi-Zhe Song, and Tao Xiang. Style-based globalappearance flow for virtual try-on.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 34703479, 2022. 3",
  "Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. sim-ple diffusion: End-to-end diffusion for high resolution im-ages. arXiv preprint arXiv:2301.11093, 2023. 2, 5": "Thibaut Issenhuth, Jeremie Mary, and Clement Calauz`enes.Do not mask what you do not need to mask: a parser-freevirtual try-on. In European Conference on Computer Vision,pages 619635. Springer, 2020. 3 Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,Jaakko Lehtinen, and Timo Aila.Training generative ad-versarial networks with limited data.Advances in NeuralInformation Processing Systems, 33:1210412114, 2020. 7 Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, HuiwenChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 60076017, 2023. 4 Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-fusionclip: Text-guided diffusion models for robust imagemanipulation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 24262435, 2022. 4 Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-tiana, Joe Penna, and Omer Levy.Pick-a-pic: An opendataset of user preferences for text-to-image generation.arXiv preprint arXiv:2305.01569, 2023. 13",
  "Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.Common diffusion noise schedules and sample steps areflawed. arXiv preprint arXiv:2305.08891, 2023. 32": "Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, andJoshua B Tenenbaum. Compositional visual generation withcomposable diffusion models. In European Conference onComputer Vision, pages 423439. Springer, 2022. 3 Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. Repaint: Inpaintingusing denoising diffusion probabilistic models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1146111471, 2022. 4 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guidedimage synthesis and editing with stochastic differential equa-tions. arXiv preprint arXiv:2108.01073, 2021. 4 Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, andDaniel Cohen-Or.Null-text inversion for editing real im-ages using guided diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 60386047, 2023. 4, 7, 13 Davide Morelli, Matteo Fincato, Marcella Cornia, FedericoLandi, Fabio Cesari, and Rita Cucchiara. Dress code: High-resolution multi-category virtual try-on. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 22312235, 2022. 5, 7, 12, 18, 19 Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Mar-cella Cornia, Marco Bertini, and Rita Cucchiara. Ladi-vton:latent diffusion textual-inversion enhanced virtual try-on. InProceedings of the 31st ACM International Conference onMultimedia, pages 85808589, 2023. 12, 18, 19 Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. Glide: Towards photorealistic image generationand editing with text-guided diffusion models. arXiv preprintarXiv:2112.10741, 2021. 4",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,and Mark Chen. Hierarchical text-conditional image gen-eration with clip latents. arXiv preprint arXiv:2204.06125,2022. 4": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1068410695, 2022. 4 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration. arXiv preprint arXiv:2208.12242, 2022. 2, 3,7 Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,Jonathan Ho, Tim Salimans, David Fleet, and MohammadNorouzi.Palette: Image-to-image diffusion models.InACM SIGGRAPH 2022 Conference Proceedings, pages 110, 2022. 4 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour,Burcu Karagol Ayan,S Sara Mahdavi,Rapha Gontijo Lopes, et al. Photorealistic text-to-image dif-fusion models with deep language understanding. Advancesin Neural Information Processing Systems, 2022. 4",
  "Dani Valevski, Matan Kalman, Yossi Matias, and YanivLeviathan. Unitune: Text-driven image editing by fine tuningan image generation model on a single image. arXiv preprintarXiv:2210.09477, 2022. 4": "Bochao Wang, Huabin Zheng, Xiaodan Liang, YiminChen, Liang Lin, and Meng Yang. Toward characteristic-preserving image-based virtual try-on network. In Proceed-ings of the European conference on computer vision (ECCV),pages 589604, 2018. 3 Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, SarahLaszlo, David J Fleet, Radu Soricut, et al. Imagen editorand editbench: Advancing and evaluating text-guided im-age inpainting. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1835918369, 2023. 4, 5, 7, 13 Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, HaoyeDong, Xijin Zhang, Feida Zhu, and Xiaodan Liang.Gp-vton: Towards general purpose virtual try-on via collabora-tive local-flow global-parsing learning.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2355023559, 2023. 3, 7, 12, 18, 19 Keyu Yan, Tingwei Gao, Hui Zhang, and Chengjun Xie.Linking garment with person via semantically associatedlandmarks for virtual try-on.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1719417204, 2023. 3 Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wang-meng Zuo, and Ping Luo. Towards photo-realistic virtualtry-on by adaptively generating-preserving image content. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 78507859, 2020. 3 Han Yang, Xinrui Yu, and Ziwei Liu.Full-range virtualtry-on with recurrent tri-level transform. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 34603469, 2022. 3 Ruiyun Yu, Xiaoqi Wang, and Xiaohui Xie.Vtnfp: Animage-based virtual try-on network with body and clothingfeature preservation. In Proceedings of the IEEE/CVF in-ternational conference on computer vision, pages 1051110520, 2019. 3 Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, WilliamChan, Chitwan Saharia, Mohammad Norouzi, and IraKemelmacher-Shlizerman.Tryondiffusion: A tale of twounets. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 46064615, 2023. 2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19",
  ". Training and inference": "M&M VTO is trained in two stages. For the first stage,the model is trained on 512256 images for 600K itera-tions. In the second stage, the model is initialized fromthe pretrained checkpoint of the first stage and trained on1024512 images for an additional 200K iterations. Forboth training stages, the batch size is set to 1024, and thelearning rate linearly increases from 0 to 104 in the first10K steps and is kept unchanged afterwards. We parame-terize the model output in v-space following while theL2 loss is computed in -space. All conditional inputs areset to 0 in 10% of the training time for classifier-free guid-ance (CFG) . Test results are generated by samplingM&M VTO for 256 steps using ancestral sampler .",
  ". Our method trained solely on DressCode vs GP-VTONand LaDI-VTON official checkpoints. We report FID and KID onDressCode triplets test set": "verted question-answer pairs into a formatted text, wheredifferent question-answer pairs are separated by semicolonwhile the question and answer within each pair are sepa-rated by colon. The resulting 1, 500 image-caption sampleswere used to finetune PaLI-3 model. Finally, we ran in-ference of the finetuned model on our train and test data,and converted the formatted text back into class labels.",
  ". Comparison of VTO": "In , 10, 11 and 12, we showcase additional qual-itative results from our 8, 300 triplets test set, compar-ing them against those generated by TryOnDiffusion ,where both methods are trained on our garment pairedand layflat paired dataset.These results highlight ourmethods superior ability to retain garment details and lay-out.We also compare to layflat-VTO methods GP-VTON and LaDI-VTON on DressCode triplets test dataset. To ensure a fair comparison, we trainedour method exclusively on the DressCode dataset. The FIDand KID metrics for the DressCode triplets test set, pre-sented in , demonstrate that our method surpassesGP-VTON and LaDI-VTON in both metrics, even whentrained solely on the DressCode dataset. Further qualita-tive comparisons on the DressCode triplets test set againstall baselines are provided in and 14.",
  ". Comparison of Editing": "We conducted a user study with 200 images to compare gar-ment layout editing. The results in indicate that ourmethod are preferred by users 84.5% of the time, outper-forming the baseline methods. , 16, 17 and 18present qualitative comparisons on different layout editingtasks. These examples demonstrate our methods ability toperform the intended edits accurately while preserving theintegrity of other areas in both the person and the garments.Image editing baselines require different sets of inputs,such as masks. InstructPix2Pix and Prompt-to-Prompt",
  "Hard to tell1": ". User Study for person finetuning. We carried out a userstudy involving 400 images across 4 subjects, where we randomlyselect 100 top + bottom input garments for each subject. The par-ticipants were asked to choose the method that best maintains theidentity of the person (including body pose and shape) as well asthe details of input garments.",
  ". Quantitative results for ablation studies. We reportFID and KID on our 8, 300 triplets test set": "(P2P) with null inversion only requires text editinginstructions. DiffEdit , Imagen Editor , and StableDiffusion XL Inpainting require masks for the regionof interest. To automatically obtain masks for image edit-ing, we use human pose estimations to mask out belly re-gions for tuck in top garment or tuck out top garmentor the arm regions for roll up sleeve or roll down sleeve.",
  ". Finetuning Comparison": "We chose 4 person images with challenging body shapesor poses for our person finetuning comparison. For eachperson image, we randomly picked 100 top and bottom gar-ment combinations, then generated try-on results using allbaseline methods as well as our own. The user study re-sults, detailed in , show our finetuning method sig-nificantly outperforming the baselines. Additionally, Fig-ure 19, 20, 21 and 22 showcase qualitative comparison foreach subject. Without finetuning, the persons arms, legs, ortorso may appear unnaturally slim or wide, and certain chal-lenging poses can not be accurately recovered. However, ifwe finetune the entire model or the person encoder, it tendsto overfit to the clothing worn by the target subject. Ourfinetuning approach successfully retains both the personsidentity and the intricate details of the input garments.",
  ". Single Stage Model vs. Cascaded": "(1st and 3rd rows) presents the FID and KID met-rics on our 8, 300 triplets test set, comparing our single-stage model with the cascaded variant. Additionally, Fig-ure 23 offers more qualitative results. While our methoddoes not surpass the cascaded variant in terms of FID andKID scores with significant margin, the qualitative resultsindicate that it excels at preserving complex garment de-tails, such as texts and logos. This observation aligns withinsights from , which suggest that FID and KIDare more effective at capturing overall visual compositionrather than the nuances of fine-grained visual aesthetics.",
  ". Additional Qualitative Results": "and 27 present try-on results for the dress cat-egory (denoted as Ifullgin the main paper). Note that ourmethod is able to synthesize realistic folds and wrinkles indress, well aligned with the persons pose, while preservingthe intricate details of the garment. visualizes fullimages of in the main paper. providesmore failure cases of our method. Finally, we provide inter-active web demos for the mix and match try-on task in thesupplementary material.",
  "Input GarmentsInput PersonTry-on ResultsInput GarmentsInput PersonTry-on Results": ". More failure cases. Top left: our method sometimes suffers from color drift issues for very dark images, which is recognizedby diffusion literature . Top right: our method fails to generate valid layout for uncommon garment combinations (e.g. long coat andskirt). Bottom left: the model attempts to create a pocket to accommodate the occluded left hand. Bottom right: our model could generatea random inner top given outer top open garment layout. Additionally, it has difficulties in effectively warping small, densely packed,and irregularly distributed texture patterns."
}