{
  "Abstract": "This technical report presents the 1st winning modelfor UG2+, a task in CVPR 2024 UAV Tracking and Pose-Estimation Challenge.This challenge faces difficultiesin drone detection, UAV-type classification, and 2D/3Dtrajectory estimation in extreme weather conditions withmulti-modal sensor information, including stereo vision,various Lidars, Radars, and audio arrays.Leveragingthis information, we propose a multi-modal UAV detec-tion, classification, and 3D tracking method for accu-rate UAV classification and tracking.A novel classifi-cation pipeline which incorporates sequence fusion, re-gion of interest (ROI) cropping, and keyframe selectionis proposed. Our system integrates cutting-edge classifi-cation techniques and sophisticated post-processing stepsto boost accuracy and robustness.The designed poseestimation pipeline incorporates three modules: dynamicpoints analysis, a multi-object tracker, and trajectory com-pletion techniques. Extensive experiments have validatedthe effectiveness and precision of our approach.In ad-dition, we also propose a novel dataset pre-processingmethod and conduct a comprehensive ablation study forour design. We finally achieved the best performance inthe MMUAD dataset in classification and tracking.Thecode and configuration of our method are available at .",
  "security challenges that extend beyond conventional con-cerns": "In recent years, there has been a significant surge in re-search focused on anti-UAV systems. Despite this interest,the majority of existing systems operate on a single-modalbasis. The UG2+ Challenge at CVPR 2024 aims to pushthe boundaries by requiring participants to develop a novelmulti-modal anti-UAV system. This challenge involves thejoint estimation of UAV 3D trajectories and the identifica-tion of UAV types. The UG2+ competition organizers col-lect their own dataset: MMAUD dataset . The core ofthis challenge is how to effectively utilize multi-modal in-formation to achieve both robust 3D UAV position estima-tion and UAV type classification, even in challenging con-ditions where single sensors may fail to acquire valid in-formation. For the classification task, the key challenge iswhen drones operate at high altitudes or encounter extremevisual conditions. Existing methods struggle to detect smalldrones due to their compact size, resulting in a smaller vi-sual presence. For the tracking part, it is also difficult todetect and estimate the 3D position of the small drone withthe reduced Radar Cross Section, noisy lidar points, and in-terference from surrounding dynamic objects. To this end, we propose our multi-modal method, whicheffectively leverages various Lidars and camera informa-tion. There are two parts in our network: the classificationnetwork and the pose estimation pipeline. For the classifi-cation network, We first carry out pre-processing and serial-ization of the dataset to implement data augmentation, in or-der to cope with adverse weather conditions. The 3D UAVpose estimation method proposed leverages lidar data due tothe unreliability of visual depth information and radar data.Instead of direct network training, a pipeline is devised toexploit features such as spatial density, motion signature,and trajectory smoothness in an unsupervised manner, sup-plementing label-provided semantic information. Overall,our contributions are shown as follows:",
  "We propose the first multi-modal UAV classification and3D pose estimation method for accurate and robust anti-UAV system": "A novel classification pipeline is introduced, incorporat-ing sequence fusion, region of interest (ROI) cropping,and keyframe selection. Our system integrates advancedclassification techniques and post-processing steps to en-hance accuracy and robustness. Extensive experimentsvalidate the effectiveness and precision of our approach. A pose estimation pipeline is proposed with dynamicpoints analysis, multi-object tracker, and trajectory com-pletion. Extensive experiments demonstrate the effective-ness and accuracy of our system. We achieve the bestperformance in UG2+ challenge in CVPR 2024.",
  ". Related Work": "Detection and Classification. UAV detection and classification techniques have seen sig-nificant advancements with the integration of deep learn-ing methodologies across various sensor modalities. Highresolution range sensor, such as MIMO radar and Li-dars , which directly utilize the point cloud for classifi-cation. In addition, radar-based detection systems utilize themicro-Doppler effect to identify UAVs by their uniquerotational patterns. Vision-based detection systems utilizeneural networks to process visual data from cameras. Mod-els from the YOLO series exhibit high accu-racy in bounding box classification and regression. Liu. propose a enhance detection and classification methdos us-ing clustering SVM, achieving better performance. Somemethods use segmentation methods toimprove the detection performance.Despite these advancements, challenges persist due tothe diversity in UAV sizes and dynamic behaviors.En-vironmental factors like weather conditions, backgroundnoise, and the presence of other wireless signals can inter-fere with detection systems . Addressing these chal-lenges requires a sensor fusion framework. Some methodsestimate the vehicle pose and shape . Some methodsharness the capabilities of multiple sensor types, blendingtheir strengths to enhance the robustness and accuracy ofUAV detection and classification. For example, integratingradar and vision data combines the radars ability to pen-etrate adverse weather conditions with the high-resolutionimaging provided by cameras. This multi-sensor fusion ap-proach has been successfully applied in various research ef-forts, such as those documented in studies like ,demonstrating superior performance compared to systemsrelying on a single sensor.3D Tracking. UAV 3D tracking has various applicationin real-world such as military, transportation ,and security . Some systems integrate the Bayesiantracking framework, employing techniques like Kalman fil- ter and particle filter to maintain robust trackingperformance. Some methods use learning-based method toimprove the accuracy. Lan et al. applied the sparselearning method to RGB-T tracking, thereby removing thecross-modality discrepancy.Liu et al. proposed amean-shift-based method which transformed the target po-sition to 3D coordinates using RGB and depth images.Moreover, the development of advanced algorithms for datafusion, such as the use of deep neural networks for fea-ture extraction and decision-level fusion, has been a signif-icant area of progress. These algorithms can learn com-plex relationships and correlations between data from dif-ferent sensors, resulting in a more comprehensive under-standing of the environment . Some SLAM meth-ods are also used in 3D tracking method tofurther improve the accuracy.Additionally, the transformer-based algorithms for multi-object tracking could be adapted for UAV detec-tion scenarios. These algorithms , originally from thefield of natural language processing, have shown their ef-fectiveness in handling complex data associations and couldpotentially improve the tracking of UAVs in multi-sensorenvironments . Some methods use joint learning meth-ods to learn the UAV pose and the type si-multaneously.",
  ". UAV Type Classification": "The trajectory of UAV movement typically exhibits conti-nuity, facilitating the utilization of contextual informationfor training the classifier. Additionally, upon examinationof the dataset, we identified numerous sequences as sub-sets of a broader sequence, termed as real sequences. Fur-thermore, our analysis revealed that sensor data concerningUAVs often exhibits sparsity, with merely a few pixels orpoints belonging to the targets at high altitudes. This inher-ent constraint significantly increases the difficulty of single-frame classification.Previous sequential tasks usually extend the modelthrough the temporal axis or employ a transformer archi-tecture. However, for the MMAUD dataset, most frames donot provide valid information as mentioned above. There-fore, we adopt the following strategy to accomplish the clas-sification task in three steps: sequence fusion grounded infeature similarity, region of interest cropping, and keyframe",
  "Sequence Fusion Based on Feature Similarity": "We have formulated two assumptions to guide the construc-tion of the real sequences. Firstly, each real sequence con-tains only one UAV. Secondly, there exists a substantial tem-poral gap between consecutive real sequences.By observing the foreground and background, we sam-ple the data from the original sequence at a ratio of 1/100.Subsequently, we leverage the EfficientNet-B7 pre-trained on ImageNet to extract feature representations fromthe sampled images. Feature aggregation is accomplishedby averaging the representations extracted from the sam-pled images.Then we extract representational featuresfrom each original sequence, compute the cosine similarityof each representation, and apply a threshold to group se-quences to construct the real sequences in both the trainingand test phases.",
  "ROI Crop and Keyframe Selection Based onYOLOv9": "During the training phase, we apply YOLOv9-e with-out finetuning on all images of the real sequences with se-lection of airplane to get the ROI of UAV. Although thereare misclassifcations in zero-shot results, this process stillhelps us automatically select enough UAV images to trainthe classifier. Following the cropping and rescaling of theROI from the detection results, we additionally utilize a ran-dom sampling procedure to mitigate class imbalance duringtraining (N 300 for each real sequence).During the test phase, we also employ YOLOv9-e to detect the UAV in a zero-shot manner, and rank the confi-dence scores for detection == airplane. Ideally, we aim to use the most clearly detected image for per-image classifi-cation as the prediction for the entire real sequence, assum-ing each sequence contains the same type of UAV. Whileconfidence does not directly indicate how clear the UAVimage is, it does reflect the models confidence. Therefore,we use this metric to identify k keyframes and employ a softclassification strategy by aggregating the softmax probabil-ities from these keyframes.",
  "Classification and Post-Processing": "The training dataset is small because only UAVs close tothe ground ( 10m) and centered in the cameras view canbe effectively detected. Therefore, the detection networksare supposed to be light-weight models.Here, we trainEfficientNet-B7 to obtain the initial results.During the test phase, we apply our classification modelon each keyframe of the real sequence and add the softmaxresults of each sequence to form a soft majority vote strat-egy. Finally, we retrieve the data sequence predictions fromthe real sequences.",
  ". UAV Pose Estimation": "Tracking UAVs in complex weather conditions and at highaltitudes presents significant challenges. Given the unre-liability of visual depth information at long distances, ourpose estimation method primarily utilizes point cloud datafrom Lidar and radar. Upon examining the dataset, it be-came apparent that although UAV flight trajectories exhibithigh diversity, the acquisition environment remains fairlyuniform, and the labeling only includes point annotationsfor the UAVs. Consequently, rather than directly traininga neural network for pose estimation, we suggest a pipelinethat investigates features such as spatial density, motion sig-natures, temporal consistency, and trajectory smoothnessin an unsupervised manner to enrich the semantic details",
  "Trajectory Completion": "x y z x y z . The UAV pose estimation pipeline. Initially, we accumulate and cluster data over 20 frames, then proceed with an analysis ofthe dynamic point cloud to segregate background points from dynamic points. Subsequently, we track these dynamic points and performcenter regression on dynamic clusters to derive an initial estimation of the UAV trajectory. The trajectory is further refined and extrapolatedusing a Kalman filter, culminating in a comprehensive and precise UAV trajectory.",
  "Augmentation": ". The pipeline of dynamic point analysis module. Wefirst accumulate 20 frames of point clouds as a temporal window.Then we use an unsupervised clustering method to cluster the pointclouds and extract the feature of each group. The LSTM modulewith MLP head can finally decompose the dynamic points. provided by the labels. Our pose estimation pipeline com-prises three key modules: dynamic point cloud analysis, amultiple object tracking module, and trajectory completion.The pipeline of our pose estimation framework is shown in.",
  "Dynamic Point Analysis": "When UAVs operate at higher altitudes, they constitute onlya small portion of the overall point cloud. Directly traininga segmentation network on this data tends to yield subopti-mal outcomes, such as classifying all points as background.Noting that UAVs generally operate in clear skies awayfrom other objects, we design a two-stage approach to ad-dress these challenges. First, we employed an unsupervisedclustering method to cluster the point cloud data. Then, weextract relevant geometric and motion features from theseclusters to decompose the dynamic points. The frameworkof our dynamic point analysis method is illustrated in .We begin by accumulating 20 frames of point clouds tocreate a temporal window. Within this window, we extractmotion features, such as the velocity vector of the center points. We incorporate temperal dropout, temperal reverse,and spatial rotation to augment the point cloud. Then, weextract seven-dimensional features. Subsequently, we de-sign a network that includes an attention-based LSTM mod-ule for temporal analysis, a Multi-Layer Perceptron (MLP)for sequence classification, and a PointNet-based modulefor center regression of the detected UAV clusters. Thiscomprehensive approach allows for precise tracking andclassification of UAVs.Instead of relying on the last hidden state of the LSTMmodule alone to encapsulate the sequence, we have inte-grated an attention mechanism that combines all hiddenstates into a comprehensive representation. This mecha-nism dynamically assigns importance weights to each ofthe hidden states, effectively combining them into a singleweighted feature set. By selectively highlighting the mostsalient elements, the models ability to cope with long se-quences and recognise complex motion patterns is greatlyenhanced.Cluster classification is conducted using an MLP head,with ground truth generated through the nearest neigh-bor association between the UAV pose labels and the es-timated cluster centers. To mitigate overfitting, we haveimplemented a range of point cloud augmentation tech-niques. These include global rotation, temporal reversion,and frame dropout, which enhance the models robustnessthrough spatial-temporal augmentation.While the cluster center can initially be predicted as thegeometric mean within the cluster, issues such as incom-plete point clouds and potential distance-related measure-ment biases in the dataset necessitate a more robust ap-proach. Therefore, we employ an additional MLP specif-ically for the task of center regression. Our observationsindicate a strong correlation between the regression errorand the cluster center in the training data. To tackle this,we develop a nonlinear model for bias correction. Specifi-cally, we use a third-order polynomial feature transformerto expand the three-dimensional coordinates into a 24- dimensional feature space. We then perform linear regres-sion to delineate the relationship between these expandedfeatures and the observed bias. The corrected cluster cen-ter is determined by adjusting the initial estimate with thepredicted bias, enhancing the accuracy of our localization.",
  "Multiple Object Tracking": "In the detection process, there exists an inherent trade-offbetween accuracy and recall, often leading to results thatinclude both clutter and missed detections. Additionally,despite corrections, the predicted cluster center may exhibita zig-zag pattern, particularly in sparse point clouds. Toaddress these challenges, we implement a multiple objecttracker that helps filter out clutter and smoothens trajecto-ries. We employ the linear Kalman filter as the backbone ofour tracking framework. Within this framework, new tracksare initiated from unassociated measurements, and existingtracks are terminated when their covariance exceeds a pre-defined threshold. This approach enhances the clarity andreliability of the tracking outcomes.Since the effectiveness of our proposed center regressionmodule, we set low diagonal values in the noise covariancematrix. In addition, since we prioritize recall over accuracyin our classification module, there could be some clutterswhich are wrongly classified as UAV. Therefore, we set theassociation threshold low so that the predicted trajectoriesare robust to the possible clutter from the detector.",
  "D Trajectory Completion and Smoothing": "Given the strict threshold applied in the deletion process, theestimated trajectories might appear fragmented. However,for pose estimation tasks, we can access entire trajectorieswithout causal constraints. This access to contextual infor-mation allows for more effective trajectory prediction andcompletion, which helps address issues related to misseddetections and lost trackers. To enhance the trajectory con-tinuity, we employ a third-order autoregressive (AR) modelfor trajectory completion. This model utilizes data from theprevious three time steps to predict the subsequent step inthe sequence, thereby providing a more cohesive and con-tinuous trajectory estimation.When faced with missing observations or lost trackersin the input data, the autoregressive (AR) model is capableof generating predictions using the available information.However, the absence of data introduces uncertainty andcan impact the accuracy of these predictions. To mitigatethese effects, we interpolate the predicted trajectories ac-cording to the specific timestamps of the test data and applysmoothing techniques to enhance the trajectorys continuityand accuracy.Considering that UAVs commonly employ spline ap-proximation in their path planning modules, we choose B- spline interpolation for the smoothing process. This methodis particularly well-suited for creating smooth and flexibletrajectories, making it ideal for adapting the UAVs flightpath to the dynamic conditions typically encountered dur-ing operations. This approach helps ensure that the trajec-tory remains reliable and precise, even in the presence ofdata gaps.",
  ". MMUAD Dataset Analysis": "The competition is based on a subset of data from the com-prehensive MMAUD dataset. Participants are tasked withclassifying four types of drones: Phantom 4, M300, M30T,and Mavic 3, and estimating their pose. The dataset pro-vides unsynchronized measurements from various sensors,including stereo fisheye cameras, two types of Lidars (conic3D Lidar and peripheral 3D Lidar), and 4D millimeter-waveradar. For training, the competition offers 102 training se-quences and 16 validation sequences, each lasting approx-imately 20 seconds and 5 seconds, respectively. The finalevaluation is conducted on a test set consisting of 59 se-quences. The ranking in this challenge is determined basedon two criteria: i) Mean Square Error (MSE Loss) com-pared to the ground truth labels of the test set, and ii) theclassification accuracy of UAV types in the test set.The dataset employs a sensor rig consists of four sensors,including Stereo Fisheye Cameras: These cost-effective camerasoffer a panoramic 180-degree field of view (FoV), cre-ating a dome-shaped detection volume that is instrumen-tal in horizon scanning and providing a wide coveragearea for UAV detection. Their affordability and wide FoVmake them ideal for continuous surveillance. Conic 3D Lidar: This upward-facing Lidar has a 70-degree conic FoV and is adept at detecting objects at dis-tances of up to 300 meters. Its conic scanning patterncomplements the fisheye cameras by focusing on a cen-tral area and extending the range of detection beyond thevisual capabilities, ensuring that distant drones are cap-tured. Peripheral 3D Lidar: With a 360-degree horizontal anda 59-degree vertical FoV, this Lidar provides comprehen-sive peripheral coverage on the ground level, effectivelydetecting nearby threats within a 70-meter range. It col-laborates with the conic Lidar to ensure that the detectionsystem has no blind spots and covers both close and fardistances. Mmwave Radar: Operating at 77GHz, this radar boasts a120-degree horizontal and a 30-degree vertical FoV, ca-pable of sensing moving objects at distances of up to 350meters. The radars ability to detect motion is particu-larly valuable, as it can track the trajectory of drones and",
  ".We visualize the distribution of images in the testdatasets": "is less affected by environmental conditions like lightingor weather.Together, the combination of these sensors expands theperception field-of-view. The stereo fisheye cameras pro-vide broad situational awareness, while the conic and pe-ripheral Lidars offer detailed detection at varying distances.The mmWave radar enhances the systems ability to trackmoving targets over a considerable range.The primarychallenge of this dataset lies in effectively harnessing thecomplementary information from the four types of sensorsto achieve robust perception.In , we visualize some example images of fourtypes of drones at three different altitudes (5m, 10m, 20m).At low altitudes, the UAVs are clearly identified at 5 me-ters with clear details for classification. However, at alti-tudes over 20 meters, the UAV becomes a point target oreven unobservable, thus making visual classification dif-ficult. highlights some challenging cases for low-altitude visual detection, including color similarity, motionblur, sun glare, small objects, incomplete objects, and edgedistortion of fish-eye camera.In , we visualize example sequences of point cloudmeasurements. In (a), we observe the co-existence ofmeasurements from both conic Lidar and radar. However,it is evident that the conic Lidar produces a higher qualitypoint cloud compared to the radar, which appears sparseand inconsistent. In (b), we notice the UAV crossingthe field of view of two Lidars. In (c), we observeposition bias at high altitudes due to insufficient resolution.",
  ". Experimental Results": "We present the 3D pose estimation performance and UAVtype classification performance of our system on testdataset. In Tab. 1, we can see that we achieve the best per-formance on 3D tracking and UAV type classification. OurUAV classification method successfully fuses informationacross sequences, utilizing a soft vote strategy to accuratelyidentify the type.For the point cloud-based UAV detector, the validationaccuracy is 0.9998 and the recall is 0.9184. For the cen-ter regression task, the MSE loss decreased from 0.27 to0.05. These results indicate that our lightweight detectionframework can successfully detect UAVs and predict thecluster centers. The detection results for the test sequencesare shown in . We can see that there are some noisydetections and missed trajectories. After applying the mul-tiple object tracker, the noisy detections are filtered out andthe trajectory is smoothed, as shown by the red curves. Fi-nally, the missed trajectories are interpolated using the con-textual information, as shown by the blue trajectories. Asshown in Tab. 1, the pose MSE is 2.21 on the test dataset.The performance gap between the validation and test sets isdue to extrapolation error, where the trajectories in the ini-tial or ending stages are missed by our detector for a long",
  "In summary, we propose the first multi-modal anti-UAVsystem, achieving accurate 3D UAV tracking and UAV typeclassification. The multi-modal dataset pre-processing and": "sequential method significantly improve the classificationperformance. The proposed tracking module with dynamicpoint analysis, multi-head tracking, and 3D trajectory pre-diction further improve the UAV tracking accuracy. As aresult, we finally achieve the 1st place in Ug2+ challenge inCVPR 2024. We hope our system can provide new insightsand ideas to professionals involved with multi-modal anti-UAV systems.",
  "JJ M de Wit, RIA Harmanny, and G Premel-Cabic. Micro-doppler analysis of small uavs. In 2012 9th European RadarConference, pages 210213. IEEE, 2012. 2": "Tianchen Deng, Siyang Liu, Xuan Wang, Yejia Liu, DanweiWang, and Weidong Chen. Prosgnerf: Progressive dynamicneural scene graph with frequency modulated auto-encoderin urban scenes. arXiv preprint arXiv:2312.09076, 2023. 1 Tianchen Deng, Guole Shen, Tong Qin, Jianyu Wang, Wen-tao Zhao, Jingchuan Wang, Danwei Wang, and WeidongChen.Plgslam:Progressive neural scene represenationwith local to global bundle adjustment.arXiv preprintarXiv:2312.09866, 2023. 1 Tianchen Deng, Hongle Xie, Jingchuan Wang, and WeidongChen. Long-term visual simultaneous localization and map-ping: Using a bayesian persistence filter-based global mapprediction. IEEE Robotics & Automation Magazine, 30(1):3649, 2023. 2",
  "Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang,Shenghai Yuan, Danwei Wang, and Weidong Chen. Compact3d gaussian splatting for dense visual slam. arXiv preprintarXiv:2403.11247, 2024. 2": "Tianchen Deng, Nailin Wang, Chongdi Wang, ShenghaiYuan, Jingchuan Wang, Danwei Wang, and Weidong Chen.Incremental joint learning of depth, pose and implicit scenerepresentation on monocular camera in large-scale scenes.arXiv preprint arXiv:2404.06050, 2024. 2 Tianchen Deng, Yanbo Wang, Hongle Xie, Hesheng Wang,Jingchuan Wang, Danwei Wang, and Weidong Chen. Nes-lam:Neural implicit mapping and self-supervised fea-ture tracking with depth completion and denoising. arXivpreprint arXiv:2403.20034, 2024. 2",
  "point cloud registration. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pages84518460, 2023. 2": "Jiuming Liu, Ruiji Yu, Yian Wang, Yu Zheng, TianchenDeng, Weicai Ye, and Hesheng Wang.Point mamba:A novel point cloud backbone based on state spacemodel with octree-based ordering strategy. arXiv preprintarXiv:2403.06467, 2024. 1 Rui Liu, Xuanzhen Xu, Yuwei Shen, Armando Zhu, ChangYu, Tianjian Chen, and Ye Zhang. Enhanced detection clas-sification via clustering svm for various robot collaborationtask. arXiv preprint arXiv:2405.03026, 2024. 2 Tianrui Liu, Qi Cai, Changxin Xu, Bo Hong, Fanghao Ni,Yuxin Qiao, and Tsungwei Yang. Rumor detection with anovel graph neural network approach. Academic Journal ofScience and Technology, 10(1):305310, 2024. 2",
  "Tianrui Liu, Changxin Xu, Yuxin Qiao, Chufeng Jiang, andJiqiang Yu. Particle filter slam for vehicle localization. Jour-nal of Industrial Engineering and Applied Science, 2(1):2731, 2024. 2": "Ye Liu, Xiao-Yuan Jing, Jianhui Nie, Hao Gao, Jun Liu, andGuo-Ping Jiang.Context-aware three-dimensional mean-shift with occlusion handling for robust object tracking inrgb-d videos. IEEE Transactions on Multimedia, 21(3):664677, 2019. 2 Kenji Okuma, Ali Taleghani, Nando de Freitas, James J. Lit-tle, and David G. Lowe. A boosted particle filter: Multitargetdetection and tracking. In Computer Vision - ECCV 2004,pages 2839, Berlin, Heidelberg, 2004. Springer Berlin Hei-delberg. 2 Yiran Qin, Chaoqun Wang, Zijian Kang, Ningning Ma,Zhen Li, and Ruimao Zhang. Supfusion: Supervised lidar-camera fusion for 3d object detection. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2201422024, 2023. 2 Zheng Qin, Sanping Zhou, Le Wang, Jinghai Duan, GangHua, and Wei Tang. Motiontrack: Learning robust short-term and long-term motions for multi-object tracking.InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1793917948, 2023. 2 Jingyu Ru, Han Yu, Hao Liu, Jiayuan Liu, Xiangyue Zhang,and Hongli Xu.A bounded near-bottom cruise trajectoryplanning algorithm for underwater vehicles. Journal of Ma-rine Science and Engineering, 11(1), 2023. 2 Yi Shen, Hao Liu, Xinxin Liu, Wenjing Zhou, Chang Zhou,and Yizhou Chen. Localization through particle filter pow-ered neural network estimated monocular camera poses.arXiv preprint arXiv:2404.17685, 2024. 2",
  "K. W. Tong, X. Y. Zhao, Y. X. Li, and P. Li. Individual-level fmri segmentation based on graphs. IEEE Transactionson Cognitive and Developmental Systems, 15(4):17731782,2023. 2": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 2 Chien-YaoWang,AlexeyBochkovskiy,andHong-Yuan Mark Liao.Yolov7: Trainable bag-of-freebies setsnew state-of-the-art for real-time object detectors.InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 74647475,2023. 2",
  "Chien-Yao Wang,I-Hau Yeh,and Hong-Yuan MarkLiao.Yolov9:Learning what you want to learn us-ing programmable gradient information.arXiv preprintarXiv:2402.13616, 2024. 2, 3": "Di Wang, Yuhui Wang, and Xiaochen Xian.A latentvariable-based multitask learning approach for degradationmodeling of machines with dependency and heterogeneity.IEEE Transactions on Instrumentation and Measurement,2024. in press. 2 Xiaosong Wang, Yuxin Qiao, Jize Xiong, Zhiming Zhao,Ning Zhang, Mingyang Feng, and Chufeng Jiang. Advancednetwork intrusion detection with tabtransformer. Journal ofTheory and Practice of Engineering Science, 4(03):191198,2024. 2 Yuhui Wang and Di Wang. An entropy- and attention-basedfeature extraction and selection network for multi-target cou-pling scenarios.In Proc. IEEE International Conferenceon Automation Science and Engineering (CASE), 2023. inpress. 2",
  "Yijie Weng and Jianhao Wu. Big data and machine learningin defence. International Journal of Computer Science andInformation Technology, 16(2), 2024. 2": "Mian Wu, Qiushuo Cheng, Yi Shen, and Congjia Chen.Comparisons between different supervised learning modelsfor cell classification using single-cell rna-sequence data.International Core Journal of Engineering, 7(1):366376,2021. 2 Hongle Xie, Tianchen Deng, Jingchuan Wang, and WeidongChen. Robust incremental long-term visual topological lo-calization in changing environments. IEEE Transactions onInstrumentation and Measurement, 72:114, 2022. 1 Hongle Xie, Tianchen Deng, Jingchuan Wang, and WeidongChen. Angular tracking consistency guided fast feature as-sociation for visual-inertial slam. IEEE Transactions on In-strumentation and Measurement, 2024. 1 Yi Xin, Junlong Du, Qiang Wang, Zhiwen Lin, and Ke Yan.Vmt-adapter: Parameter-efficient transfer learning for multi-task dense scene understanding. In Proceedings of the AAAIConference on Artificial Intelligence, pages 1608516093,2024. 2 Yi Xin, Junlong Du, Qiang Wang, Ke Yan, and ShouhongDing.Mmap: Multi-modal alignment prompt for cross-domain multi-task learning. In Proceedings of the AAAI Con-ference on Artificial Intelligence, pages 1607616084, 2024.",
  "Wenpeng Xing and Jie Chen.Temporal-mpi: Enablingmulti-plane images for dynamic scene modelling via tem-poral basis learning. In European Conference on ComputerVision, pages 323338. Springer, 2022. 1": "Wenpeng Xing and Jie Chen.Mvsplenoctree: Fast andgeneric reconstruction of radiance fields in plenoctree frommulti-view stereo. In Proceedings of the 30th ACM Interna-tional Conference on Multimedia, pages 51145122, 2022.1 Shenghai Yuan, Yizhuo Yang, Thien Hoang Nguyen, Thien-Minh Nguyen, Jianfei Yang, Fen Liu, Jianping Li, HanWang, and Lihua Xie.Mmaud: A comprehensive multi-modal anti-uav dataset for modern miniature drone threats.arXiv preprint arXiv:2402.03706, 2024. 1 Ce Zhang, Chengjie Zhang, Yiluan Guo, Lingji Chen, andMichael Happold.Motiontrack: end-to-end transformer-based multi-object tracking with lidar-camera fusion. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 151160, 2023. 2"
}