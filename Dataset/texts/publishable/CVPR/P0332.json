{
  "Abstract": "To build a cross-modal latent space between 3D hu-man motion and language, acquiring large-scale and high-quality human motion data is crucial. However, unlike theabundance of image data, the scarcity of motion data haslimited the performance of existing motion-language mod-els. To counter this, we introduce motion patches, a newrepresentation of motion sequences, and propose using Vi-sion Transformers (ViT) as motion encoders via transferlearning, aiming to extract useful knowledge from the im-age domain and apply it to the motion domain. These mo-tion patches, created by dividing and sorting skeleton jointsbased on body parts in motion sequences, are robust tovarying skeleton structures, and can be regarded as colorimage patches in ViT. We find that transfer learning withpre-trained weights of ViT obtained through training with2D image data can boost the performance of motion analy-sis, presenting a promising direction for addressing the is-sue of limited motion data. Our extensive experiments showthat the proposed motion patches, used jointly with ViT,achieve state-of-the-art performance in the benchmarks oftext-to-motion retrieval, and other novel challenging tasks,such as cross-skeleton recognition, zero-shot motion classi-fication, and human interaction recognition, which are cur-rently impeded by the lack of data.",
  ". Introduction": "The cross-modal analysis of 3D human motion and natu-ral language has opened up new avenues for tasks such asmotion recognition and text-to-motion synthesis , which can benefit the appli-cations like animating avatars or humans . The keyto these tasks is constructing a cross-modal latent space thatcaptures the intricate relationship between human motionsand language semantics, allowing systems to interpret andgenerate human-like motions based on textual descriptions.Despite the promising advancements in this area, one of",
  "N Points": ".Overview of the existing methods and the proposedmethod. The existing methods train an original Transformer withthe joint information from the motion sequences directly, whilethe proposed method converts them into motion patches and thentrains the ViT, which can be initialized with pre-trained weights. the most challenging aspects is the scarcity of data, becausethe process of collecting and annotating 3D human motiondata is labor-intensive and time-consuming. While recentyears have seen an increase of motion capture datasets ,some of which are categorized by classes or even la-beled with free text , these resources are still notsufficient for deep learning algorithms to fully understandhuman motions. Moreover, because various motion cap-ture systems and skeleton structures are used in differentdatasets, it has been difficult to build a large-scale datasetwith a unified representation.To build a motion-language model, existing methods [38, 50] attempt to incorporate text embeddings into motion au-toencoders. Due to the lack of large-scale data, they trainthe motion encoder from scratch on each dataset and try touse motion synthesis in autoencoders to improve the mo-tion features. Tevet et al. attempt to apply pre-trainedimage-language models to motion data, but they onlyrender a single frame as the input image. Consequently,these methods are not yet robust enough to handle the vari-",
  "arXiv:2405.04771v1 [cs.CV] 8 May 2024": "ations and subtleties present in 3D human motions. Theyalso need to deal with different skeleton structures by train-ing an individual model for each dataset separately, despiteall these data representing human motion. This leads to lowperformance on the small-scale datasets.To overcome these challenges, we introduce an approachto building motion-language models by leveraging VisionTransformers (ViT) as the motion encoder. This ap-proach extends the conventional use of ViTs, which wereoriginally designed for 2D image classification, to the morecomplex domain of 3D human motion analysis. With thetransfer learning of ViT pretrained on ImageNet to mo-tion data, the training process of the motion encoder can beaccelerated, while at the same time, overcoming the issueof limited data scale and achieving better correspondencebetween the motion features and the language features.To efficiently transfer knowledge of the image domainto the motion domain, we also propose motion patches, anovel unified representation for various skeleton structuresin motion sequences. We design the motion patches to belikened to image patches in ViT, with joint positions in xyzcoordinates simply converted to image colors in rgb space.We first partition the joints of the skeleton into five bodyparts: torso, left arm, right arm, left leg, and right leg. Then,motion patches are formed by sampling N points from eachbody part through linear interpolation, and stacking thesepoints for each part across N frames by sliding window.This results in a patch for each part with a size of N N.We then train a motion-language model with a contrastivelearning framework . The comparison between existingmethods and the proposed method is shown in .We evaluate the versatility and effectiveness of our pro-posed method through comprehensive experiments and ap-plications of motion-language tasks. This study makes thefollowing contributions: We propose a new framework for building motion-language models using ViT, which extends the applica-tion of ViT to obtain a cross-modal latent space betweenmotions and language. We introduce a novel method of representing 3D humanmotion data as motion patches, which can be processedby the ViT architecture with its pre-trained weights fortransfer learning, and are also resilient to variations in hu-man skeleton structures. Our approach not only significantly improves the per-formance of text-to-motion retrieval, but also illustratesthe potential for other novel applications, such as cross-skeleton recognition, zero-shot motion classification, andhuman interaction recognition.",
  ". Summary of recent related methods for motion-languagemodels. Only our proposed method utilizes pre-trained motion en-coders and a unified representation for various skeleton structures": "motions, there is a scarcity of motion-language datasets. Al-though datasets do exist for action recognition and poseestimation , they lack detailed textual descriptions foreach motion. Notably, the KIT dataset offers 11 hoursof motion capture sequences, each paired with descriptivesentences. The recently released HumanML3D dataset provides around 29 hours of motion data with natural lan-guage labels for AMASS and HumanAct12 col-lections. Compared to the scale of image-text pairs used fortraining image-language models like CLIP (e.g., datasetswith 400 million images), motion-text pairs remain notablylimited in scale (e.g., 14,616 motions in HumanML3D).Creating motion-language datasets presents challenges, in-cluding the need for expensive motion capture and annota-tion systems, as well as issues related to variations in skele-ton structures across different datasets.Motion-Language Models.In recent years, vision-language foundation models have garnered significant at-tention, driven by the availability of vast collections ofimage-text pairs gathered from the internet. These modelshave adopted various pre-training schemes .A recent representative in this field is CLIP , whichaims to learn joint representations of vision and language bytraining on a large-scale dataset of image-text pairs. How-ever, the field of motion-language models is relatively lessexplored. Petrovich et al. use contrastive training dur-ing motion generation to align text features with motionfeatures. Because of the limited scale of motion-text pairs,these methods are trained from scratch and cannot capturethe differences between similar motions. Moreover, thesemethods cannot be applied to cross-skeleton recognition,as different motion encoders need to be trained for eachdataset. There are some attempts to utilize external knowl-edge from different modalities to analyze human motion.Tevet et al. attempt to render a single frame as a staticimage to be used as input to CLIP, in order to obtain vi-sual features and align them with motion features, but theperformance is limited by the use of a single frame.Motion Generation and Retrieval. Motion-languagemodels find valuable application in text-conditioned motiongeneration. Unlike unconstrained motion generation , action-conditioned or text-conditioned mod-els introduce se-mantic controls to generate motion sequences correspond-",
  "Text #1Text #2Text #3": ". Overview of the proposed framework, which consists of a motion encoder and a text encoder. We transform the raw motionsequences into motion patches as the input of the ViT-based motion encoder. We calculate the similarity matrix between text-motion pairswithin a batch to train the model. To illustrate this concept, we provide an example batch containing three samples for clarity. ing to input textual descriptions. Recent advancements intext-to-image generation through diffusion generative mod-els have led to methods such as , aimingto apply diffusion models to text-to-motion generation us-ing the HumanML3D dataset, along with other approachesbased on Large Language Models . Text-to-motionretrieval is an alternative and potentially complemen-tary approach to generating motions corresponding to agiven textual description, as a retrieval model can alwaysreturn a realistic motion. Text-to-motion retrieval is alsoused as a tool to evaluate the performance of text-to-motiongeneration . We also use the task of text-to-motion retrieval as an important evaluation of the proposedmethod. Transfer Learning. Transfer learning involves takingmodels trained for a specific task using a large dataset, andextending their capabilities to address new tasks by lever-aging prior knowledge to extract relevant features. Someexamples of these attempts can be found in image segmen-tation and medical image analysis . Besidesthe image domain, transfer learning using ImageNet pre-trained weights also performs well in video recogni-tion and even in audio classification , where timesequences are transformed into images as the input of themodel. Some methods attempt to use convolutionalneural networks for action recognition. However, our focusis on leveraging the pre-trained knowledge from the imagedomain to construct motion-language models. To address the challenges of motion-language models,we aim to transfer the ViT pre-trained in the image domainto the motion domain with a unified representation of mo-tion sequences, to overcome the data scale problem. We",
  ". Problem Statement": "Given a set of motion sequences M and a set of captionsT , our target is to learn a function s(mi, tj) to calculatethe similarity between the motion mi M and the cap-tion tj T . The objective of the s(mi, tj) is to calculatea high similarity for relevant motion-text pairs and a lowsimilarity score for irrelevant ones. The motion sequencemi M is represented as a sequence of skeleton jointsin this paper. Formally, the motion sequence is denoted bymi RT J3, where T represents the length of the se-quence, J 3 represents the position of the skeleton jointsin Cartesian coordinates, (x, y, z).To build a motion-language model, we adopt the CLIPframework , which consists of a motion encoder FMand a language model FT . Using these encoders, we encodethe motion sequence mi as FM(mi) and the caption tj, andthen calculate the similarity as follows:",
  "ExtractInterpolate": ". Process of building the motion patches for each motionsequence. Given a skeleton, we mark different body parts in dif-ferent colors. We show the method to construct the motion patchof the right leg. The same process is applied to other body parts. of knowledge from the image domain via pre-trained mod-els, motions need to be represented in a similar manner asimages. However, in contrast to image data usually having aunified representation of size 2242243 for deep learningmodels, the size of motion sequences is T J D as men-tioned in .1. Because the number of the framesT differs for each sequence and the number of the jointsJ depends on the skeleton structure in the dataset, there isno consensus on how to obtain a unified representation formotion data in different skeleton structures.We propose motion patches as a new representation,which can be further used as input to ViT for motionfeature extraction. To build motion patches similar to imagepatches with size N N in ViT, we divide the joints of 3Dmotion skeletons into body parts, interpolate between jointsin each body part to obtain N sample points, and use theN consecutive frames in the sequence as shown in .First, the joints are partitioned into five body parts: the torso(including the head), left arm, right arm, left leg, and rightleg. This type of partitioning is commonly used andcan be implemented on any human skeleton. Each bodypart comprises a subset of joints corresponding to that partof the body according to the kinematic chain of the skeleton.Next, within each body part, we arrange the joints basedon their distance from the torso. For example, in the case ofarms, we order the joints as the upper chest the shoulder upper arm lower arm hand. This sequence main-tains the spatial structure of the skeleton. We standardizethe number of sample points in each body part to N us-ing linear interpolation. To normalize these sample pointsas image data, we calculate the mean and variance of eachpoint across the dataset and perform the z-score normaliza-tion using these mean and variance values.Finally, we form motion patches by stacking se-quences of sample point positions across N frames. Werepeat this process for every sequence of N frames using asliding window, creating a series of motion patches. Thesepatches, which are robust to variations in skeleton struc-tures, can be analogized to image patches in ViT, and allowus to represent skeletons from various structures in a unified",
  "Left Arm": ". Visualization of motion patches by regarding the jointcoordinates as RGB pixels. We show the rendered motions andtheir text label on the left and the processed motion patches onthe right. We can observe different motions reflected in differentmotion patches. format.We provide a visualization of motion patches by depict-ing them as RGB images in . We interpret joint co-ordinates as RGB pixels to create a visual representation ofeach motion patch. The figure displays the rendered mo-tions and their corresponding text labels on the left, and theprocessed motion patches on the right. We can observe dif-ferent motions resulting in distinct motion patches, demon-strating the capacity of our method to capture unique char-acteristics of each motion in the form of motion patches.",
  ". Motion Encoder": "For image data, many well-established architectures andpre-trained models can be used for CLIP. Meanwhile, formotion data, there is no standard architecture and no large-scale pre-trained model. Existing methods use theiroriginal motion encoders and train them from scratch to ex-tract the motion representations. However, with the novelmotion patches proposed in .2, we are able to en-code motions by extending ViT for 2D images to 3D motiondata to overcome the limited scale of motion data.ViT first extracts non-overlapping image patches fromthe image data. Then, a projection head is used to projectthese patches onto 1D tokens. The transformer architectureis used to model the interaction between each image patchto obtain the final representation. To apply ViT to motionsequences, we first transfer the motion sequences into mo-tion patches and then regard these motion patches as imagepatches. In this paper, we adopt the ViT-B/16 with 12 lay- ers and the patch size 16 pre-trained on ImageNet-21k as our motion encoder. Hence, we set N = 16 to obtainthe motion patches with size 16 16. We have additionallyincluded an investigation of the choices related to the ViTbackbone and patch sizes in the supplementary material.Following ViT and CLIP, the [class] token is added tothe inputs and we resize the position embedding to matchthe number of patches. The output from the [class] to-ken is projected onto a multi-modal embedding space as themotion representation.",
  ". Text Encoder": "In the context of text encoding, it is crucial to extract fea-tures related to motion. Following TMR , we adopt Dis-tilBERT for this purpose, utilizing a pre-trained modelwith a projection head. The output from the [class] tokenis used as the text representation. An alternative is utilizingthe text encoder of CLIP , commonly used in motiongeneration methods .However, vision-languagemodels, including CLIP, face challenges in distinguishingbetween entities and verbs . Despite exploringthis option, our experiments showed that DistilBERT out-performed CLIP, with detailed comparisons available in thesupplementary material.",
  ". Datasets": "We utilize two standard datasets in our experiments: theHumanML3D dataset and the KIT Motion-Languagedataset .HumanML3D Dataset: The HumanML3D dataset en-riches the AMASS and HumanAct12 motioncapture collections with natural language labels describingthe motions. We follow the same motion pre-processingmethod proposed in . Furthermore, the dataset is aug-mented through the mirroring of both left and right motions, along with their corresponding textual descriptions. Sub-sequently, following the official dataset split, we acquire atotal of 23,384, 1,460, and 4,380 motions for the training,validation, and test sets, respectively. On average, each mo-tion receives 3.0 distinct textual annotations. During thetraining phase, we randomly choose one annotation as thematching text, while for testing, we only use the first one.KIT Motion-Language Dataset (KIT-ML): The KIT-ML dataset, which primarily focuses on locomotion, is alsoderived from motion capture data.To prepare the mo-tion data for analysis, we apply the identical pre-processingprocedure as employed in the HumanML3D dataset. Thedataset is partitioned into training, validation, and test sets,consisting of 4,888, 300, and 830 motions, respectively. Onaverage, each motion is annotated 2.1 times.",
  ". Evaluation Protocol": "To evaluate the performance of the motion-language model,we adopt the retrieval task between the motion sequence andthe text description. Following , our evaluation of re-trieval performance employs standard metrics, specificallyRecall at various ranks (R@1, R@2, etc.), for both text-to-motion and motion-to-text tasks. Recall at rank k indicatesthe percentage of instances where the correct label appearswithin the top k results 1, with higher values indicating bet-ter performance. Additionally, we calculate the median rank(MedR), where a lower value shows better performance. Itis important to note that the evaluation of retrieval perfor-mance is conducted using an unseen gallery of real motions,specifically the test set.We used several evaluation protocols to calculate Recall,primarily altering the composition of the gallery set:All: In this protocol, the entire test set is used withoutany modifications. However, repetitive texts across motionsor minor textual differences (e.g., person vs. human,walk vs. walking) will affect the results. We use thisprotocol as the default protocol in this paper.Small Batches: This protocol is designed by Guo etal. .It involves randomly selecting batches of 32motion-text pairs and then reporting the average perfor-mance.While this approach introduces randomness, itserves as a benchmark for comparison. It is worth notingthat a gallery size of 32 is relatively manageable comparedto the other protocols, making it a less challenging scenario.",
  ". Results of text-to-motion and motion-to-text retrieval benchmark on KIT-ML": "batch size of 256 is used during the training. The latent di-mension of the embeddings after the projection is set to 256.We set the temperature parameter to 0.07 following CLIP.The number of frames in each motion sequence is limited to224, following the existing methods , which means14 5 = 70 motion patches are used as the input of ViT.",
  ". Results": "In our evaluation of text-to-motion and motion-to-text re-trieval benchmark across HumanML3D () and KIT-ML () datasets, encompassing all evaluation proto-cols, we provide comparisons against prior works, specifi-cally TEMOS , T2M , TMR and the proposedmethod trained from scratch without using pre-trained ViTweights.The experimental results of TEMOS andT2M are sourced from the TMR paper. Mean-while, we re-evaluate the official models of TMR usingour evaluation code to ensure a fair comparison.It is important to note that TEMOS is not explic-itly designed for retrieval tasks. The cross-modal embed-ding space of TEMOS is primarily trained with pos-itive pairs. In contrast, T2M applied their method toretrieval by employing contrastive learning, which includesnegative pairs as well.TMR is the state-of-the-art method for text-to-motion retrieval, which extends TEMOSby incorporating a contrastive loss between the motionfeatures and the text features in the latent space.Remarkably, our model consistently outperforms priorwork across all evaluation sets in various degrees of diffi-culty. This indicates that our model can capture the nuancednature of motion descriptions. The substantial performanceenhancements we achieve over the state-of-the-art can be at-tributed to several factors: (1) the design of motion patchesto capture the temporal-spatial motion representation and(2) the utilization of ViT and transferring its pre-trainedweights to the motion domain. In the subsequent sections,we conduct controlled experiments to analyze the impact ofthese components on our results.",
  ". Ablation Studies": "In this section, we explore various settings to better under-stand the factors influencing the performance of our model.Pre-trained ViT: We conduct experiments to comparethe performance of our model when utilizing pre-trainedViT representations against a setting where ViT pre-trainingis not employed.Motion Representation: Another aspect we investigateis the use of motion representations. We investigate differ-",
  ". Results of ablation studies. We experiment with differentsettings (1) with/without the pre-trained ViT and (2) whether touse motion patches as the representation of the motion": "ent scenarios where we either employ the proposed motionpatches as the input for ViT or directly feed the raw mo-tion sequences into the Transformer component, along withpositional encodings.The results of HumanML3D and KIT-ML are shownin . It is noticeable that when the motion patchesare used, training the model from pre-trained weights leadsto much better results than training from scratch. Comparedwith using motion patches as input, using motion sequenceswithout preprocessing leads to worse performance. Thisanalysis shows the impact of ViT pre-training on the ca-pabilities of our model and the advantages that our motionpatches bring to retrieval tasks.",
  ". Qualitative results": "In , we present the qualitative results of text-to-motionretrieval on the entire test set of HumanML3D. Each querytext is displayed on the left, and on the right, we showcasethe top-3 retrieved motions along with their correspondingground-truth text labels. The gallery of motions for retrievalremains unseen during training.In the first two examples, we successfully retrieve theground-truth motion in the top-2 results. Note that in thesecond example, the differences between the motions arevery small and they all present motions similar to the textquery. For the free-form prompt in the last example, wherethe exact text is not present in the gallery, our method alsosucceeds in retrieving correct motions.",
  "around": ". Qualitative results of text-to-motion retrieval. For eachquery, we show the retrieved motions ranked by text-motion sim-ilarity and their accompanying ground-truth text labels. Note thatthese descriptions are not used in the retrieval process. All motionsin the gallery are from the test set and were unseen during training.For the first two examples, the text queries are sampled from thedata. For the last example, we query with a free-form text. have 21 joints in KIT-ML, and the skeleton structure of KIT-ML is different from SMPL (detailed in the supplementary).Existing methods cannot deal with these two datasets simul-taneously because the dimension of the input vector variesaccording to the dimension of the pose vector. However, ourmethod is able to convert the motion sequence to 1616motion patches according to the kinematic chain of eachbody part, which means the motion features learned on adataset can be transferred to another dataset even when theskeleton structure is different.To evaluate the performance of the proposed method forcross-skeleton recognition, we prepare two scenarios. Thefirst one is a zero-shot setting, where we directly applythe motion-language model trained on the HumanML3Ddataset to the text-to-motion retrieval task of the KIT-MLdataset. The other one is a transfer learning setting, wherewe further fine-tune the HumanML3D model with the KIT-ML dataset, because the scale of the KIT-ML dataset issmaller than that of the HumanML3D dataset.The results are shown in , which compares thezero-shot setting and the transfer learning setting with otherexisting methods and the proposed method trained on KIT-ML. The performance of zero-shot prediction with the Hu-manML3D model is lower than the models trained on KIT-ML, because the language domains (i.e., KIT-ML is morefocused on locomotion descriptions) and the skeleton struc-",
  "OursTransferred15.28 38.7152.659.0016.51 35.7847.4711.00": ". Results of cross-skeleton recognition. We evaluate thetext-to-motion and motion-to-text retrieval on the KIT-ML datasetwith the HumanML3D model and the transferred model.Thetransfer learning method achieves better performance than themethod of training only on KIT-ML. tures of these two datasets are different. However, it stillachieves acceptable performance compared to TEMOS and T2M , especially on the task of text-motion re-trieval. It is noticeable that the transfer learning method ob-tained the best results, which shows that training the modelwith HumanML3D is helpful in recognizing the motion se-quences of KIT-ML. This highlights the potential of our ap-proach to improve the performance on small-scale datasetsby pre-training the model on large-scale datasets.",
  ". Zero-shot Motion Classification": "Furthermore, we demonstrate the effectiveness of thesemantically structured latent spaces generated by ourmotion-language model via action recognition. We followthe BABEL 60-classes benchmark , containing 10,892sequences, and 20% of them are used as test sets. We pre-processed the motion sequences with the same procedure asHumanML3D . Because this is a zero-shot classifica-tion setting, we did not train the model with BABEL andonly applied the model trained on HumanML3D to the testdata. For the text prompts, the action names in BABEL areused as A person {action}. We calculate the cosine dis-tance between a given motion and all 60 text prompts.In , we show a comparison of the Top-1 and Top-5 accuracy achieved by our zero-shot classifier with that ofthe 2s-AGCN classifier and MotionCLIP . As evi-dent from the results, our framework performs comparablyto the state-of-the-art supervised methods, despite the factthat our method was not initially designed for action recog-nition tasks nor trained on the action label set of BABEL.This highlights the versatility and adaptability of our ap-proach across various applications.",
  "Ours9.5121.2732.4127.008.2622.6532.6624.00": ". Results of human interaction recognition. For TMR and our method, we concatenate the motion features of each per-son and get the multi-person motion feature through a projectionhead of the concatenated feature. training and 1,557 sequences for testing. We processed themotion sequences of each individual with the same proce-dure as HumanML3D . To obtain the features of theinteractions, we apply a shared motion encoder to each per-son and simply concatenate the motion features before theprojection head. We evaluate the performance of the pro-posed method via text-to-motion retrieval. The results areshown in and our method outperforms TMR .",
  ". Limitations": "In this paper, we primarily evaluate our method on mo-tion recognition, focusing on text-to-motion retrieval. Fu-ture work includes applying our method to text-to-motiongeneration. Despite leveraging pre-trained vision models tohandle small-scale motion datasets, the generalization per-formance of our method may be limited due to the com-paratively smaller size of motion-text data versus image-text data. However, our proposed motion patch, a skeleton-robust representation, aids in constructing large-scale mo-tion datasets from diverse motion capture systems.",
  ". Conclusion": "In this paper, we introduced a novel unified motion rep-resentation called motion patches and applied the ViTarchitecture with its pre-trained weights to build motion-language models. Our approach effectively addresses chal-lenges related to limited data scales in 3D human motiondata and diverse skeleton structures, characterized by com-plex spatial-temporal dependencies. As a result, we havemade significant advancements in motion recognition, in-cluding text-to-motion retrieval and other applications.",
  "Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-ard Pons-Moll, and Michael J Black. Amass: Archive ofmotion capture as surface shapes. In CVPR, 2019. 1, 2, 5": "Anna Majkowska, Sid Mittal, David F. Steiner, Joshua JayReicher, Scott Mayer McKinney, Gavin E Duggan, Kr-ish Eswaran, Po-Hsuan Cameron Chen, Yun Liu, Sreeni-vasa Raju Kalidindi, Alexander Ding, Greg S Corrado, Daniel Tse, and Shravya Shetty.Chest radiograph in-terpretation with deep learning models: Assessment withradiologist-adjudicated reference standards and population-adjusted evaluation. Radiology, 2019. 3",
  "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,Dan Jurafsky, and James Zou.When and why vision-language models behave like bag-of-words models, and whatto do about it? In ICLR, 2023. 5": "Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, ShaoliHuang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and XiShen. T2m-gpt: Generating human motion from textual de-scriptions with discrete representations. In CVPR, 2023. 2,3 Mingyuan Zhang, Zhongang Cai, Liang Pan, FangzhouHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-fuse: Text-driven human motion generation with diffusionmodel. arXiv preprint arXiv:2208.15001, 2022. 3, 5",
  "A. Motion Patches": "In , we show the process of constructing motionpatches for SMPL skeletons in the HumanML3D dataset.For the KIT-ML dataset, the skeleton structure is differentbut the process is the same as shown in . Becausethe motion patches use the kinematic chain of the skele-ton to extract the spatial-temporal information in motionsequences, our model can be used in cross-skeleton recog-nition as detailed in .1 of the main paper.",
  "B.1. Visualization of Attention Maps": "In this paper, we find that pre-trained image ViT can helpthe learning of motion data with the proposed motionpatches. As shown in , the motion patches can beregarded as a kind of spectrogram, where certain patternsrelated to motions can be observed. Pre-trained ViT helpsdetect these patterns, which makes transfer learning work.We additionally visualize the attention maps extracted fromthe ViT trained by our method in , where the impor-tant patterns are activated in the attentions. An analogousapproach is audio recognition by rendering the spectrogramof audio as the input into pre-trained image models .",
  "B.2. ViT Backbones": "We evaluated our method with different ViT backbones.In the main paper, we used ViT-B/16 as the motion en-coder. We additionally tried ViT with tiny, small, and largesizes provided in TIMM 2, and the results are shown in Ta-ble 8. We can find that ViT-Tiny and ViT-Small perform alittle worse when compared to ViT-base in both datasets.The largest model, ViT-Large, performs well in the Hu-manML3D dataset, but not well in the KIT-ML Dataset,which may be due to the limited scale of the data. Overall,our proposed method works well on all the ViT backbones.",
  "B.3. Motion and Text Encoders": "In the paper, we employed the ViT pre-trained on ImageNetas the motion encoder and the pre-trained DistilBERT as the text encoder. Additionally, we explored an alternativeapproach by utilizing the image encoder and text encoderof CLIP as the motion and text encoders in our methodfor comparison. The results are shown in . We can",
  ". Visualization of attention maps extracted from ViT": "find that the pre-trained weights affect the performance ofthe model and the combination of ViT with ImageNet andDistilBERT achieved the best results. When the model ofCLIP is used as the motion encoder or the text encoder, wefind that the performance drops a little, which shows thatCLIP is not effective for capturing motion representations.This might be because CLIP is pre-trained to focus on thesemantic features of real-world images, while the motionpatches resemble a type of spectrogram with color patterns.",
  "C. Additional Qualitative Results": "In this section, we present qualitative results of the text-to-motion retrieval and motion-to-text retrieval tasks with thecomparisons between TMR and the proposed methodon the challenging HumanML3D dataset. The results of thetext-to-motion retrieval are shown in . We can findthat our method succeeded in finding the motion match-ing the text descriptions including the details, e.g., ducksin the first sample and with right arm up in the secondsample. Regarding the motion-to-text retrieval tasks shownin , each query motion is displayed on the left, and onthe right, we showcase the top-5 retrieved text descriptionsalong with the ground-truth text labels of query motions.We successfully retrieved the ground-truth descriptions inthe top-5 results, and the descriptions in the top-5 resultsseem to be reasonable to describe the motion sequences ex-cept for some mirror-augmented ones. When compared tothe results of TMR , our method is better at catchingthe details of the motion such as jumps twice in the firstsample and moves backward then forwards in the thirdsample.",
  "diagonally across a room with their arms swinging hands down": ". Comparisons of text-to-motion retrieval between TMR and the proposed method. For each query, we show the retrievedmotions ranked by text-motion similarity and their accompanying ground-truth text labels. Note that these descriptions are not used in theretrieval process. All motions in the gallery are from the test set and were unseen during training.",
  "TMROurs": ". Comparisons of motion-to-text retrieval between TMR and the proposed method. For each query motion, we show theretrieved descriptions ranked by motion-text similarity and their accompanying ground-truth text labels. Note that these ground-truth textsare not used in the retrieval process. All motions in the gallery are from the test set and were unseen during training. For all the samples,our proposed method retrieved reasonable descriptions."
}