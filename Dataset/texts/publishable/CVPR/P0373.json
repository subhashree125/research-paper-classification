{
  "Department of Computer Science, ETH ZurichMax Planck Institute for Intelligent Systems, Tubingen": ". Overview of 4D-DRESS. We propose the first real-world 4D dataset of human clothing, capturing 64 human outfits in more than520 motion sequences. These sequences include a) high-quality 4D textured scans; for each scan, we annotate b) vertex-level semanticlabels, thereby obtaining c) the corresponding garment meshes and fitted SMPL(-X) body meshes.",
  "Abstract": "The studies of human clothing for digital avatars have pre-dominantly relied on synthetic datasets. While easy to col-lect, synthetic data often fall short in realism and fail tocapture authentic clothing dynamics. Addressing this gap,we introduce 4D-DRESS, the first real-world 4D datasetadvancing human clothing research with its high-quality4D textured scans and garment meshes. 4D-DRESS cap-tures 64 outfits in 520 human motion sequences, amount-ing to 78k textured scans. Creating a real-world clothingdataset is challenging, particularly in annotating and seg-menting the extensive and complex 4D human scans. Toaddress this, we develop a semi-automatic 4D human pars-ing pipeline. We efficiently combine a human-in-the-loopprocess with automation to accurately label 4D scans in di-",
  ". Introduction": "Human clothing is crucial in various applications such as3D games, animations, and virtual try-on. Researchers areactively investigating algorithms for clothing reconstruc-tion and simulation , to achieve re-alistic clothing behavior, enhance user engagement, and en-able cross-industry applications. These algorithms are fre-quently developed and assessed using synthetic datasets , since they comprise a) meshes covering variousgarment types and outfits and b) parametric body mod-",
  "D-DRESS (Ours)6478kScans + SMPL(-X)+ Garments": ". Summary of 4D clothed human datasets. The datasets highlighted in gray color are synthetic datasets while the others are real-world scans. # of Outfits: number of outfits included; # of Frames: total number of 3D human frames; Data Format: 3D representationsof human bodies and garments; Textured: with textured map or not; Semantic Labels: with semantic labels for clothing or not; LooseGarments: containing challenging loose clothing such as dresses or not. 4D-DRESS demonstrates outstanding features against others. els with diverse motions.While synthetic datasets leadin outfit quantity and the number of frames provided (re-fer to Tab. 1), there also presents a significant challengein bridging the domain gap between the synthetic and realgarments. Despite the recently released real-world 4D hu-man datasets such as X-Humans , ActorsHQ , and4DHumanOutfit , a key limitation persists: they lack ac-curately segmented garment meshes, offering only raw hu-man scans. Moreover, these datasets are limited in the num-ber of loose garments (e.g., jackets and dresses) or dynamicmotions, which reduces their applicability as test benches.These challenges highlight the need for a real-world 4Ddataset that provides semantic annotations and captures di-verse garments across various body motions.In this work, we contribute 4D-DRESS, the first real-world dataset of human clothing with 4D semantic segmen-tation. We aim to provide an evaluation testbench with real-world data for tasks related to human clothing in computervision and graphics. We capture over 520 human motionsequences featuring 64 distinct real-world human outfits ina high-end multi-view volumetric capture system, similarto the one used in . The complete dataset comprises atotal of 78k frames, each composed of an 80k-face trianglemesh, a 1k resolution textured map, and a set of 1k resolu-tion multi-view images. As illustrated in , we providea) high-quality 4D textured scans, b) vertex-level semanticlabels for various clothing types, such as upper, lower, andouter garments, and c) garment meshes along with their reg-istered SMPL(-X) body models.Capturing real-world 4D sequences of humans wearingvarious clothing and performing diverse motions requiresdedicated high-end capture facilities. Moreover, processingthese clips into accurately annotated and segmented 4D hu-man scans presents significant challenges. To develop our dataset, we tackled the task of labeling 78k high-resolutionmeshes at the vertex level. Given that the mesh topologiesof consecutive frames do not inherently correspond, con-sistently propagating 3D vertex labels from one frame tothe next is non-trivial. While previous methods at-tempted to fit a fixed-topology parametric body model to thescans, these template-based approaches still struggle withscenarios such as a jacket being lifted to reveal a shirt or theemergence of new vertices on a flowing coat as illustratedin the example shown in . Consequently, we opted foran alternative approach. We developed a semi-automaticand template-free 4D human parsing pipeline. Leveragingsemantic maps from a 2D human parser and a seg-mentation model , we extended these techniques to 4D,considering both multi-view and temporal consistency. Ourpipeline accurately assigns vertex labels without manual in-tervention in 96.8% of frames. Within the remaining scans,only 1.5% of vertices require further rectification, addressedvia a human-in-the-loop process. The quality of the ground-truth data in 4D-DRESS al-lows us to establish several evaluation benchmarks for di-verse tasks, including clothing simulation, reconstruction,and human parsing. Our evaluation and analysis demon-strate that 4D-DRESS offers realistic and challenging hu-man clothing that cannot be readily modeled by existing al-gorithms, thereby opening avenues for further research. Insummary, our contributions include:",
  "evaluation benchmarks showing the utility of our dataset": "2. Related Work4D clothed human dataset.Datasets featuring clothedhumans can be divided into two categories. Firstly, syn-thetic datasets create large volumeof synthetic data using graphic engines and simula-tion tools (Tab. 1 top).These datasets are easy toscale with ground truth semantic labels available by de-sign. However, they often lack realism in human appear-ances, clothing deformations, and motion dynamics. Eventhough recent work attempted to achieve photoreal-istic human textures with manual efforts, it is challengingto precisely mimic the way real-world clothing moves anddeforms. Therefore, it is essential to create datasets of real-world human clothing by capturing these intricate details.The second category (Tab. 1 bottom) involves usingmulti-view volumetric capture systems to collectdatasets of people dressed in real-world clothing . However, the resources re-quired for capturing, storing, and processing this data aresubstantial, which limits the size of these publicly availabledatasets . Moreover, these methods do not in-herently provide labeled annotations, offering only tempo-rally uncorrelated scans. This makes the raw data on thesedatasets less suitable for research focusing on human cloth-ing. 4D-DRESS gathers a variety of human subjects andoutfits providing accurate semantic labels of human cloth-ing, garment meshes, and SMPL/SMPL-X fits. Human parsing.Human parsing is a specific taskwithin semantic segmentation aimed at identifying detailedbody parts and clothing labels. Conventionally, this chal-lenge is tackled using deep neural networks, trained on im-ages with their corresponding semantic labels .Although these methods have been successful in 2D , applying them to annotate 3D and 4D scansis still a challenge. Previous work has explored it using twodistinct strategies. One strategy, used by SIZER andMGN , involves rendering multi-view images and pro-jecting parsing labels onto 3D meshes through a voting pro-cess. While this method considers consistency across multi-ple views, it overlooks temporal consistency and falls shortof accurately labeling 4D scans. Another approach, used byClothCap , registers all scans to a fixed-topology SMPLmodel with per-vertex displacements. Yet, this methodstruggles with handling large motions and complex clothingdue to limited template resolutions and model-fitting capa-bilities. This results in noisy labels near boundaries andloose garments. In contrast, our approach combines multi-view voting and optical warping in a template-free pipeline,achieving both multi-view and temporal consistency.",
  ". MethodologyTo accurately label each vertex within our 4D textured scansequences, we leverage a semi-automatic parsing pipeline": "that incorporates but minimizes manual efforts during thelabeling process. depicts the overall workflow of ourpipeline. We first render 24 multi-view images of the cur-rent frame textured scan. We combine those images withthe previous frames multi-view images and labels to de-ploy three state-of-the-art tools to vote candidate labels foreach rendered pixel (Sec. 3.1): a) human image parser, b)optical flow transfer, and c) segmentation masks. Next, were-project and fuse all the 2D label votes via a Graph Cutoptimization to obtain vertex-level semantic labels, consid-ering neighboring and temporal consistency (Sec. 3.2). Forthose challenging frames where further labeling refinementis needed (around 3% in our dataset), we refined their se-mantic labels with a manual rectification step that we feedback into the optimization (Sec. 3.3). We describe the de-tails of the pipeline within this section. 3.1. Multi-view ParsingAt each frame k {1, ..., Nframe}, we render the 3D-meshinto a set of multi-view images, consisting of twelve hori-zontal, six upper, and six lower uniformly distributed views.We note this as Iimg,n,k with n {1, ..., Nview = 24}.Within the multi-view space, we tackle the problem of as-signing a label vote l to each pixel p using multi-viewimage-based models. The label l varies for human skin,hair, shoes, upper clothing (shirts, hoodies), lower clothing(shorts, pants), and outer clothing (jackets, coats). For clar-ity, we omit the frame index (k) in the following unless theyare strictly needed. Please refer to and the Supp. Mat.for more label definitions and the versatility of our parsingmethod with new labels like belts and socks. Human image parser (PAR).Our primary source of la-bels is a deep-learning image parser, which provides pixel-level votes for body parts and clothes. Specifically, we ap-ply Graphonomy to each view n and store the labels asa new set of images {Ipar} (see ). These labels arethen accessible by the vote function fpar,n(p, l) that checksif the image Ipar,n matches the value l at the pixel p, inwhich case returns 1, or 0 otherwise. This vote functionand the other two defined below will be crucial later whensetting our full-mesh optimization (Sec. 3.2). Optical flow transfer (OPT).This block leverages theprevious frames multi-view labels to provide temporal con-sistency.Specifically, we use the optical flow predictorRAFT to transfer multi-view labels in the k 1 frameto the current k frame using the texture features on the ren-dered multi-view images.Similarly to the image parserabove, the optical flow output goes to a set {Iopt}. These la-bels are accessible via the vote function fopt,n(p, l), whichchecks Iopt,n and returns 1 if label l is in p and 0 otherwise.",
  "Segmentation masks (SAM).The multi-view votes gen-erated by the Human Image Parser sometimes lack 3D con-sistency, particularly when dealing with open garments un-": ". 4D Human parsing method. We first render current and previous frame scans into multi-view images and labels. Then collectmulti-view parsing results from the image parser, optical flows, and segmentation masks (Sec. 3.1). Finally, we project multi-view labelsto 3D vertices and optimize vertex labels using the Graph Cut algorithm with vertex-wise unary energy and edge-wise binary energy(Sec. 3.2). The manual rectification labels can be easily introduced by checking multi-view rendered labels. (Sec. 3.3). der dynamic motions (cf. ). While the votes derivedfrom the optical flows provide a cross-frame prior, they maynot accurately track every human part and cant identifynewly emerging regions. Therefore, we introduce segmen-tation masks to regularize the label consistency within eachmasked region. We apply the Segment Anything Model to each rendered image and obtain a self-define group ofmasks Mm,n, with the index m {1, ...Nmask,n}. Withina mask Mm,n we compute the score function S(l, Mm,n)that fuses the votes of the image parser and the optical flow,normalized by the area of the mask:",
  "m1:Nmask,nC(p, Mm,n)S(l, Mm,n). (2)": "3.2. Graph Cut Optimization for Vertex ParsingThe next step in our semi-automatic process is combiningall the labels obtained in Sec. 3.1 to assign a unique la-bel to each scan vertex vi, with i {1, ..., Nvert}. Weframe this 3D semantic segmentation problem as a graphcut optimization: each 3D frame is interpreted as a graphG, where vertices are now nodes and mesh edges are con-nections. Note that in a traditional Graph Cut, the values ofthe nodes are fixed, and the optimization computes only the cost of breaking a connection. In our case, we have severalvotes for a vertex label, coming from three different toolsand from concurrent multi-view projections. We define ourcost function that consists of two terms,",
  "i,j1:NvertEedge(li, lj), (3)": "where L = {li} represents all the vertex labels in currentframe. As described below, the term Evert combines thedifferent votes into a single cost function, while Eedge eval-uates neighboring labels for consistent 3D segmentation.We follow an approach similar to .Vertex-wise unary energy.The cost function per node orUnary energy comes from combining the different votes ob-tained in the multi-view image processing (see Sec. 3.1):",
  "pP (vi,n) wX (p, vi) fX,n(p, li),(5)": "meaning that energy of the method X, calculated for a pro-posed label li, is obtained by summing over those pixelsp P(vi, n) whose projections are within a triangle of vi.The weights for the cases of Epar and Eopt are set to thebarycentric distance from the projected pixel p to the ver-tex vi, which means wpar = wopt = u as in . ForEsam instead, we set the weight wsam to the constant value1 given that we look for an across-vertex regularization. . Qualitative ablation study. We visualize the effectiveness of our 4D human parsing method on our 4D-DRESS dataset. From leftto right, we show the improvements after adding the optical flow labels and mask scores to the multi-view image parser labels. The manualrectification efforts can be easily introduced from multi-view rendered labels, with which we achieve high-quality vertex annotations. Theproblem of isolated labels can be relieved by introducing the edge-wise binary energy term. Edge-wise binary energy.The Binary energy term penal-izes the case of adjacent vertices with different labels, en-couraging neighboring vertices to take the same label. Be-ing A the adjacency matrix of the graph G and the Diracdelta function, the edge cost can be calculated as follows:",
  "which increases the energy by b in the case that the adja-cent vertices vi, vj take different labels li = lj": "3.3. Manual Rectification of 3D LabelsWhen manual rectification is needed, we introduce it backinto the multi-view space as an additional 2D annotation,and we recalculate the steps in Sec. 3.2. Concretely, we ranthe graph cut optimization for the first time. Then, we ren-dered the vertex labels into multi-view labels, from whichwe let a person introduce corrections by comparing the re-sulting labels with the textured multi-view images. Simi-larly to the vote functions of the image parser and opticalflow, we create a vote function fman(p, l) that accesses thisset of images with rectified annotations and returns 1 if thelabel l is assigned to the pixel p and 0 otherwise.Similar to previous cases, we define a per-view manualenergy (Eman) by using the variable X = man in Eq. (5),and we added it to the global per-node energy Evert inEq. (4). We use a constant large weight for wman to favorthe manual annotation over other sources of voting wherewe rectified the labels. The final vertex labels L = {li}are obtained after the second round of graph cut optimiza-tion. This manual rectification process finally changed 1.5%of vertices within 3.2% of all frames. The rectification pro-cess is detailed in Supp. Mat.4. ExperimentsTo validate the effectiveness of our method, we con-ducted controlled experiments on two synthetic datasets,",
  ". Baseline and ablation study. Mean accuracy of 4Dhuman parsing methods applied on synthetic datasets. The Innerand Outer outfits are selected according to our definition in Sec. 5": "CLOTH4D and BEDLAM , where ground-truthsemantic labels are available. We first compare our pars-ing method with a template-based baseline , that uses asemantic template (SMPL model with per-vertex displace-ments) to track and parse the clothed human scans. Due tothe limited resolution and the fixed topology nature of theSMPL+D model, its parsing accuracy is lower than 90% onall synthetic outfits (see Tab. 2). We then compare our 4D parsing pipeline with severalablations and report them in Tab. 2. We use an examplescan from 4D-DRESS to support the visualization of theablation study in . Using PAR only shows reason-able results for upper and lower clothes. Yet, it predictsinconsistent labels at open garments like jackets and coats( PAR Only), resulting in only 71.4% parsing accu-racy on the BEDLAM dataset. The optical flow labels fromthe previous frame can serve as a cross-frame prior, yetaccuracy may vary, particularly in fast-moving arms andcloth boundaries ( PAR+OPT). By fusing both of theprevious multi-view labels via the segmentation masks, weachieve better boundary labels ( PAR+OPT+SAM),with 98.8% accuracy on the outer outfits in BEDLAM, withchallenging open garments. Finally, we show the effect of . Qualitative examples for clothing simulation methods. On the left are templates used for simulations. On the right are ground-truth geometries and original scans, LBS baseline results in body penetrations and overly stretched areas. Compared to other methods,HOOD better models dresses and jackets and, with tuned material parameters, HOOD* achieves simulations closest to the ground truth. introducing manual efforts to rectify incorrect labels (With Manual). Our parsing method can also be deployed toannotate other existing 4D human datasets. We present ex-amples of BUFF, X-Humans , and ActorsHQand additional qualitative results in Supp. Mat.5. Dataset Description4D-DRESS contains 520 motion sequences (150 frames at30 fps) in 64 real-world human outfits with a total of 78kframes. Each frame consists of multi-view images at 1kresolution, an 80k-face triangle 3D mesh with vertex an-notations, and a 1k-resolution texture map. We also pro-vide each garment with its canonical template to benefit theclothing simulation study. Finally, each 3D scan is accu-rately registered by SMPL/SMPL-X body models.To record 4D-DRESS we recruited 32 participants (18female), with an average age of 24. The dataset consists of 4dresses, 30 upper, 28 lower, and 32 outer garments. Partici-pants were instructed to perform different dynamic motionsfor each 5-second sequence. For each participant, we cap-ture two types of outfits: Inner Outfit comprising the innerlayer dress/upper, and lower garments; and Outer Outfitwith an additional layer of garment, such as open jacketsor coats. A unique feature of 4D-DRESS is the challeng-ing clothing deformations we captured. To quantify thesedeformations, we compute the mean distances from the gar-ments to the registered SMPL body surfaces. The inner andouter outfits exhibit distance ranges up to 7.12 cm and 14.76cm over all frames. This is twice as much as what we ob-served in the X-Humans dataset , for example. In the10% most challenging frames, this increases to 20.09 cmfor outer outfits, highlighting the prevalence of challenginggarments. Please refer to Supp. Mat. for dataset details.6. Benchmark EvaluationWith high-quality 4D scans and diverse garment meshesin dynamic motions, 4D-DRESS serves as an ideal ground",
  ". Clothing simulation benchmark. CD is Chamfer Dis-tance between the simulation and ground truth.Estr denotesstretching energy with respect to the template": "truth for a variety of computer vision and graphics bench-marks. In our work, we outline several standard bench-marks conducted in these fields using our dataset.Ourprimary focus is on tasks related to clothing simulation(Sec. 6.1) and clothed human reconstruction (Sec. 6.2). Ad-ditionally, benchmarks on human parsing and human repre-sentation learning are included in our Supp. Mat. 6.1. Clothing SimulationExperimental setup.We introduce a new benchmark forclothing simulation, leveraging the garment meshes from4D-DRESS, which capture dynamical real-world clothingdeformations.This benchmark evaluates three methodsfor modeling garment dynamics: PBNS , Neural ClothSimulator (NCS ), and HOOD , as well as a base-line method that applies SMPL-based linear blend-skinning(LBS) to the template.We ran the simulations usingT-posed templates extracted from static scans and com-pared the results to the ground-truth garment meshes acrossvarious pose sequences.Our evaluation metrics includethe Chamfer Distance (CD), which compares the resultingmesh sequences with ground-truth point clouds, and the av-erage stretching energy (Estr) calculated by measuring thedifference in edge lengths between the simulated and tem-plate meshes. The experiments were conducted across fourcategories of garments (Lower, Upper, Dress, and Outer),",
  "SiTH": ". Examples of clothed human reconstruction on 4D-DRESS. We evaluate state-of-the-art methods using both inner (Top) andouter (Bottom) outfits. We show that existing methods generally struggle with the challenging loose garments. Moreover, these approachescannot faithfully recover realistic details such as clothing wrinkles. with four garment templates in each category. We simu-lated clothing deformation for each garment in six differentpose sequences, providing a comprehensive comparison oftheir ability to generate realistic motions. Fine-tuning material parameters.To demonstrate theadvantages of real-world garment meshes in 4D-DRESS,we also introduce a simple optimization-based strategy forinverse simulation using HOOD. Specifically, we optimizethe material parameters fed into the HOOD model to mini-mize the simulations Chamfer Distance to the ground-truthsequences and their stretching energy. This optimized ver-sion is denoted as HOOD*. For more details on the materialoptimization experiments, please refer to Supp. Mat. Evaluation results.The quantitative and qualitative com-parisons of the clothing simulation methods are presentedin Tab. 3 and respectively. The LBS baseline andLBS-based approaches (PBNS and NCS) perform betterwith upper and lower garments, which exhibit limited free-flowing motions compared with the dress and outer gar-ments. Conversely, HOOD excels with dresses, generat-ing more natural, free-flowing motions and achieving lowerstretching energy. However, if HOOD fails to generate re-alistic motions for a single frame, this error propagates toall subsequent frames.This issue does not occur in theLBS-based methods, which generate geometries indepen-dently for each frame. With finely-tuned material parame-ters, HOOD* produces garment sequences that more faith-fully replicate real-world behavior. We anticipate that futureresearch in learned garment simulation will increasingly fo-cus on modeling real-world garments made from complexheterogeneous materials. This will be a major step in cre-ating realistically animated digital avatars, and we believe4D-DRESS will be highly instrumental in this task.",
  "MethodCDNCIoUCDNCIoU": "PIFu 2.6960.7920.6902.7830.7590.697PIFuHD 2.4260.7930.7392.3930.7630.743PaMIR 2.5200.8050.7062.6080.7770.715ICON 2.4730.7980.7522.8320.7620.756PHORHUM 3.9440.7250.5803.7620.7050.603ECON 2.5430.7960.7362.8520.7600.728SiTH 2.1100.8240.7552.3220.7940.749 . Clothed human reconstruction benchmark. We com-puted Chamfer distance (CD), normal consistency (NC), and Inter-section over Union (IoU) between ground truth and reconstructedmeshes obtained from different baselines.6.2. Clothed Human ReconstructionExperimental setup.We create a new benchmark forevaluating state-of-the-art clothed human reconstructionmethods on the 4D-DRESS dataset. This benchmark is di-vided into three subtasks. First, we evaluate single-viewhuman reconstruction utilizing images and high-quality3D scans from our dataset. In addition, benefiting from thegarment meshes in our dataset, we establish the first real-world benchmark for evaluating single-view clothing re-construction. Finally, we assess video-based human re-construction approaches leveraging the sequences in 4D-DRESS that capture rich motion dynamics of both humanbodies and garments. In all the experiments, we report 3Dmetrics including Chamfer Distance (CD), Normal Consis-tency (NC), and Intersection over Union (IoU) to comparethe predictions with ground-truth meshes.Single-view human reconstruction.We use the two testsets defined in Sec. 5 (denote as Outer and Inner) toevaluate the following single-view reconstruction meth-ods: PIFu , PIFuHD , PaMIR , ICON ,PHORHUM , ECON , and SiTH . The evalu-ation results are summarized in and Tab. 4. We ob-",
  ". Clothing reconstruction benchmark. We report Cham-fer Distance (CD), and Intersection over Union (IoU) between theground-truth garment meshes and the reconstructed clothing": "served that methods leveraging SMPL body models as guid-ance (i.e., ICON, ECON, SiTH) performed better in recon-structing inner clothing. However, their performance sig-nificantly declined when dealing with outer garments. Onthe other hand, end-to-end models like PIFu and PIFuHDdemonstrated more stability with both clothing types. Thisleads to an intriguing research question: whether the humanbody prior is necessary for reconstructing clothing. Qualita-tively, we see that even the best-performing methods cannotperfectly reconstruct realistic free-flowing jackets as shownin Tab. 4. We believe 4D-DRESS will offer more valuableinsights for research in clothed human reconstruction.Single-view clothes reconstruction.Clothes reconstruc-tion has received relatively little attention compared tofull-body human reconstruction. Leveraging the garmentmeshes in 4D-DRESS, we introduce the first real-worldbenchmark to assess prior art, including BCNet , SM-PLicit , and ClothWild . The results of differentclothing types, as shown in , indicate a significantgap between the reconstructed and real clothing. Firstly, theclothing sizes produced by these methods are often inaccu-rate, suggesting a lack of effective use of image informationfor guidance. Moreover, the results typically lack geometricdetails like clothing wrinkles compared to full-body recon-struction. We report quantitative results in Tab. 5. We ob- .Video-based human reconstruction.Qualitativeresults of video-based human reconstruction methods on 4D-DRESS. Prior works struggle to reconstruct 3D human with chal-lenging outfits and cannot recover the fine-grained surface details.",
  ". Video-based human reconstruction. Results of video-based human reconstruction methods on 4D-DRESS": "served that the data-driven method (BCNet) performs bet-ter with inner clothing, while the generative fitting method(SMPLicit) shows more robustness to outer clothing, suchas coats. However, none of these methods is designed for ortrained on real-world data. The domain gap between syn-thetic and real data still limits their capability to produceaccurate shapes and fine-grained details.We expect ourbenchmark and dataset will draw more research attentionto the topic of real-world clothing reconstruction. Video-based human reconstructionLeveraging the se-quential 4D data in our dataset, we create a new benchmarkfor evaluating video-based human reconstruction methods.We applied Vid2Avatar and SelfRecon to obtain4D reconstructions and compared them with the providedground-truth 4D scans. As observed in , both methodsstruggle with diverse clothing styles and face challenges inreconstructing surface parts that greatly differ in topologyfrom the human body, such as the open jacket. Moreover,there remains a noticeable discrepancy between the real ge-ometry and the recovered surface details. Quantitatively,the existing methods cannot achieve satisfactory reconstruc-tion results with outer garments, as demonstrated by alarge performance degradation in Tab. 6. We believe 4D-DRESS provides essential data for advancing video-basedhuman reconstruction methods, particularly in achievingdetailed geometry recovery for challenging clothing.",
  ". Clothed Human Parsing": "We design a benchmark for the image-based human parser.Concretely, we project each scan frames vertex labels to themulti-view captured images using corresponding cameraparameters and rasterizer, which provide the ground-truthpixel labels for evaluating the image-based human pars- . Human representation learning. Qualitative results of the novel pose synthesis of state-of-the-art human representation learningapproaches together with the GT of 4D-DRESS. All Baseline methods fail to learn the large non-rigid surface deformations and are boundedby the skeletal deformations.",
  ". Human representation learning. Results of human rep-resentation learning approaches on 4D-DRESS": "ing methods: SCHP , CDGNet , and Graphon-omy . In Tab. 7, we report the mean Pixel Accuracy(mAcc.) and mean Intersection over Union (mIOU) be-tween the prediction and the ground-truth labels. We con-ducted our human image parsing experiments on one subsetof our 4D-DRESS dataset, which contains 128 sequencesof 64 outfits (2 sequences for each of the inner and outeroutfits). The qualitative parsing results are shown in .",
  ". Human Representation Learning": "We design a new benchmark for evaluating the human rep-resentation learning task. Unlike physics-based methods,this line of work directly takes 3D human scans as train-ing input and obtains an animation-ready human avatar. Wefollow the split strategy mentioned before and evaluate priorworks, SCANimate , SNARF , X-Avatar onthe novel-pose synthesis. shows that state-of-the-arthuman representation learning approaches cannot correctlylearn the large non-rigid surface deformations (e.g., foldedskirt) due to the strong skeletal dependency and the lack ofmodeling for temporal dynamics. This effect can also be . Human parsing comparison. We use the ground-truthsemantic labels to evaluate state-of-the-art human parsing models.These methods generally failed to predict correct clothing labelsfrom different view angles.reflected in Tab. 8 quantitatively where all baseline meth-ods produce higher errors on the split of more challenginggarments (outer outfits).",
  ". Discussion": "Limitations. Our current pipeline requires substantial com-putational time. The offline manual rectification processand garment mesh extraction also demand expertise in 3Dediting and additional human efforts. These factors con-strain the scalability of our dataset. With a goal of expand-ing more diverse subjects and clothing, real-time 4D anno- tation and rectification/editing will be exciting future work.Conclusion. 4D-DRESS is the first real-world 4D clothedhuman dataset with semantic annotations, aiming to bridgethe gap between existing clothing algorithms and real-worldhuman clothing. We demonstrate that 4D-DRESS is notonly a novel data source but also a challenging bench-mark for clothing simulation, reconstruction, and other re-lated tasks. We believe that 4D-DRESS can support a widerange of endeavors and foster research progress by provid-ing high-quality 4D data in life like human clothing.Acknowledgements. This work was partially supported bythe Swiss SERI Consolidation Grant AI-PERCEIVE. AGwas supported in part by the Max Planck ETH CLS. Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu.Photorealistic monocular 3d reconstruction of humans wear-ing clothing.In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2022. 7 Matthieu Armando, Laurence Boissieux, Edmond Boyer,Jean-Sebastien Franco, Martin Humenberger, ChristopheLegras, Vincent Leroy, Mathieu Marsot, Julien Pansiot, SergiPujades, Rim Rekik, Gregory Rogez, Anilkumar Swamy,and Stefanie Wuhrer.4dhumanoutfit: a multi-subject 4ddataset of human motion sequences in varying outfits ex-hibiting large displacements. Computer Vision and ImageUnderstanding, 2023. 2, 3",
  "Hugo Bertiche, Meysam Madadi, and Sergio Escalera. Neu-ral cloth simulation. ACM Transactions on Graphics (TOG),41(6):114, 2022. 1, 6, 17": "Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,and Gerard Pons-Moll. Multi-garment net: Learning to dress3d people from images. In Proceedings of the IEEE Interna-tional Conference on Computer Vision (ICCV). IEEE, 2019.2, 3 Michael J. Black, Priyanka Patel, Joachim Tesch, and Jin-long Yang. BEDLAM: A synthetic dataset of bodies exhibit-ing detailed lifelike animated motion. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 87268737, 2023. 1, 2, 3, 5, 15",
  "body parts. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), 2014. 3": "Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,and Andreas Geiger. Snarf: Differentiable forward skinningfor animating non-rigid neural implicit shapes. In Proceed-ings of the IEEE International Conference on Computer Vi-sion (ICCV), 2021. 9 CLO. 2022. 3 Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-nis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,and Steve Sullivan. High-quality streamable free-viewpointvideo. ACM Transactions on Graphics (TOG), 34(4):113,2015. 2, 3, 16",
  "Blender Online Community. Blender - a 3D modelling andrendering package. Blender Foundation, Stichting BlenderFoundation, Amsterdam, 2018. 17": "Enric Corona, Albert Pumarola, Guillem Aleny`a, Ger-ard Pons-Moll, and Francesc Moreno-Noguer.Smplicit:Topology-aware generative model for clothed people.InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2021. 1, 8 Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, MingYang, and Liang Lin. Instance-level human parsing via partgrouping network. In Proceedings of the European Confer-ence on Computer Vision (ECCV), pages 770785, 2018. 3 Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, MengWang, and Liang Lin. Graphonomy: Universal human pars-ing via graph transfer learning. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2019. 2, 3, 9, 13 Artur Grigorev, Bernhard Thomaszewski, Michael J. Black,and Otmar Hilliges. Hood: Hierarchical graphs for general-ized modelling of clothing dynamics. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 1696516974, 2023. 1, 6, 17 Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and OtmarHilliges. Vid2avatar: 3d avatar reconstruction from videosin the wild via self-supervised scene decomposition. In Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2023. 8 Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang,Young-Jae Park, and Hae-Gon Jeon. High-fidelity 3d humandigitization from single 2k resolution images. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2023. 3 Haoyu He, Jing Zhang, Qiming Zhang, and Dacheng Tao.Grapy-ml: Graph pyramid mutual learning for cross-datasethuman parsing. In Proceedings of the AAAI Conference onArtificial Intelligence (AAAI), 2020. 3",
  "Jie Song Hsuan-I Ho, Lixin Xue and Otmar Hilliges. Learn-ing locally editable virtual humans. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), 2023. 3": "Mustafa Isk, Martin Runz, Markos Georgopoulos, TarasKhakhulin, Jonathan Starck, Lourdes Agapito, and MatthiasNiener. Humanrf: High-fidelity neural radiance fields forhumans in motion. ACM Transactions on Graphics (TOG),42(4):112, 2023. 2, 3, 6 Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, LigangLiu, and Hujun Bao. Bcnet: Learning body and cloth shapefrom a single image. In Proceedings of the European Con-ference on Computer Vision (ECCV). Springer, 2020. 1, 8 Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Sel-frecon: Self reconstruction your digital avatar from monoc-ular video. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), 2022. 8 Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, LeiTan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, BartNabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, andYaser Sheikh. Panoptic studio: A massively multiview sys-tem for social interaction capture. IEEE Transactions on Pat-tern Analysis and Machine Intelligence (TPAMI), 2017. 3 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment anything. In Proceedings of theIEEE International Conference on Computer Vision (ICCV),pages 40154026, 2023. 2, 4, 13",
  "Peike Li, Yunqiu Xu, Yunchao Wei, and Yi Yang.Self-correction for human parsing. IEEE Transactions on PatternAnalysis and Machine Intelligence (TPAMI), 2020. 3, 9": "X. Liang, K. Gong, X. Shen, and L. Lin. Look into per-son: Joint body parsing & pose estimation network and anew benchmark. IEEE Transactions on Pattern Analysis andMachine Intelligence (TPAMI), 41(04):871885, 2019. 3 Kunliang Liu, Ouk Choi, Jianming Wang, and WonjunHwang. Cdgnet: Class distribution guided network for hu-man parsing.In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR), pages44734482, 2022. 3, 9",
  "Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-lor Gordon, Wan-Yen Lo, Justin Johnson, and GeorgiaGkioxari.Accelerating 3d deep learning with pytorch3d.arXiv:2007.08501, 2020. 13": "Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-alignedimplicit function for high-resolution clothed human digitiza-tion. In Proceedings of the IEEE International Conferenceon Computer Vision (ICCV), 2019. 7 Shunsuke Saito, Tomas Simon, Jason Saragih, and HanbyulJoo. Pifuhd: Multi-level pixel-aligned implicit function forhigh-resolution 3d human digitization. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), 2020. 7 Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.Black. SCANimate: Weakly supervised learning of skinnedclothed avatar networks. In Proceedings of the IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR),2021. 9",
  "Yidi Shao, Chen Change Loy, and Bo Dai. Towards multi-layered 3d garments animation. In Proceedings of the IEEEInternational Conference on Computer Vision (ICCV), 2023.2, 3": "Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Zarate,Julien Valentin, Jie Song, and Otmar Hilliges. X-avatar: Ex-pressive human avatars. In Proceedings of the IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR),2023. 2, 3, 6, 9, 17 Zhaoqi Su, Tao Yu, Yangang Wang, and Yebin Liu. Deep-cloth: Neural garment representation for shape and styleediting. IEEE Transactions on Pattern Analysis and MachineIntelligence (TPAMI), 45(2):15811593, 2023. 3",
  "Unreal Engine 5. 3": "Wenguan Wang, Hailong Zhu, Jifeng Dai, Yanwei Pang,Jianbing Shen, and Ling Shao.Hierarchical human pars-ing with typed part-relation reasoning. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), 2020. 3 Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, SebastianDziadzio, Thomas J Cashman, and Jamie Shotton. Fake ittill you make it: face analysis in the wild using synthetic dataalone. In Proceedings of the IEEE International Conferenceon Computer Vision (ICCV), pages 36813691, 2021. 3 YuliangXiu,JinlongYang,DimitriosTzionas,andMichael J. Black. ICON: Implicit Clothed humans Obtainedfrom Normals. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2022. 7 Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, andMichael J. Black. ECON: Explicit Clothed humans Opti-mized via Normal integration. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2023. 7",
  "Lu Yang, Wenhe Jia, Shan Li, and Qing Song. Deep learningtechnique for human parsing: A survey and outlook. arXivpreprint arXiv:2301.00394, 2023. 3": "Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-hai Dai, and Yebin Liu. Function4d: Real-time human vol-umetric capture from very sparse consumer rgbd sensors. InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2021. 3 Chao Zhang, Sergi Pujades, Michael J. Black, and GerardPons-Moll. Detailed, accurate, human shape estimation fromclothed 3d scan sequences.In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2017. 2, 3, 6 Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.Pamir: Parametric model-conditioned implicit representa-tion for image-based human reconstruction. IEEE Transac-tions on Pattern Analysis and Machine Intelligence (TPAMI),2021. 7 Xingxing Zou, Xintong Han, and Waikeung Wong. Cloth4d:A dataset for clothed human reconstruction.In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 1284712857, 2023. 1, 2, 3, 5,15",
  ". Multi-view Parsing": "Multi-viewrendering.Foreachframek{1, ..., Nframe}, we render twelve horizontal, six up-per, and six lower images Iimg,n,k that are uniformlydistributed on a sphere by rasterizing the textured scanwith Pytorch3D , where n {1, ..., Nview = 24}.Each scan is centralized according to its bounding boxcenter and then placed at the camera sphere center. Therendered images have a resolution of 512 512. Examplesof 24-view rendered images are shown in .",
  ". Label mapping between 4D-DRESS and LIP dataset.We define 6 label categories based on LIP dataset": "Human image parser (PAR).We apply the pre-trainedGraphonomy to each rendered image Iimg,n,k and savethe label results as a new image Ipar,n,k. Concretely, wemanually classify the 20 classes of Graphonomy labels into6 classes that are used in our dataset: skin (0), hair(1),shoes(2), upper(3), lower(4), and outer(5) clothes. The cor-responding labels between Graphonomy (LIP) and ours areshown in Tab. 9. Specifically, we map the background la-bel from Graphonomy to our setting with a label value -1,and the color code of white. These background labels willreturn 0 in the vote function fpar,n(p, l). Optical flow transfer (OPT).To establish connectionswith previous frames, we project previous frame vertex la-bels to multi-view labels Ilab,n,k1 using the same ren-dering cameras and rasterizer from Pytorch3D. Then, wewarp these previous multi-view labels to the current frameIopt,n,k using the optical flow vectors predicted by theRAFT model. The vertex labels at the first frame do notinvolve this process thanks to our first-frame initialization(see Sec. 8.3). Concretely, each pixel label with locationp within Ilab,n,k1 will be warped to a new pixel locationp + v at the current frame, through the optical flow vectorv = RAFT(Iimg,n,k1, Iimg,n,k, p). The new labels at thecurrent frame are determined by voting. If there is no cor-responding label found in the previous frame, the new labelwill be set to -1.",
  ". Example of SAM predictions. The input image is the first view (upper-left) of . We filter out the segmentation masksthat contain background, full body, and only small regions (marked as red)": ". Example of manual rectification. An annotator se-lects a region in the rendered images and gives a correct label.The label is projected to 3D and used for correcting the 3D ver-tices through a second round of graph cut optimization. tra prompts, where m {1, ..., Mmask,n}. Then we com-pute the score function S(l, Mm,n) within each mask foreach label by fusing the votes from the image parser andoptical flow, normalized by the area of the mask. depicts the predicted segmentation masks from a renderedimage. A pixel p within the rendered image Iimg,n may be-long to multiple segmentation masks. In this case, the SAMvote function fsam,n(p, l) is calculated by summing all thescores of masks that contain this pixel.",
  ". Graph Cut Optimization": "The energy Eq. (5) in the main paper is optimized throughthe graph cut algorithm (alpha-expansion). The vertex-wiseunary energy is normalized among all labels and then addedto the edge-wise binary energy. The weights are empiricallyset as p = 0.5, o = 0.5, po = 1.5, s = 1, and b = 1.",
  ". Manual Rectification Process": "Manual rectification on segmentation masks.In ourdataset, each scan mesh has around 80k vertices. Manu-ally annotating their vertex labels on the 3D scans is veryexpensive and time-consuming. Thus, we introduce a man-ual rectification process within the 2D image space. Af-ter the first graph cut optimization, we render vertex labelsto multi-view images, from which we let an annotator cor-rect labels with the segmentation masks and a painting tool.More specifically, the annotator is asked to identify an in-correctly labeled region by checking the multi-view imagesand labels. Once an incorrect labeling is found, the annota-tor will look for its corresponding segmentation masks forlabel correction. If such a mask does not exist, the anno-tator will manually paint the region using a painting tool.Finally, the images with rectified labels are projected to3D vertices and are formulated as the manual vote func-tion fman,n(p, l). The energy Eman,n term will be addedto the second round of graph cut optimization, with a largeweight wman = 10. We note that for each 150-frame 4Dsequence, the rectification process takes about 30 minuteson a desktop with an RTX 2080Ti GPU whereas the humanparsing and the graph cut optimization take two and onehour, respectively. An example of our rectification processis shown in . First-frame initialization of vertex labels.To ensure agood label initialization, the motion sequences always startfrom the A pose, which is easier for human parsing andpose registration. We obtain the first-frame vertex labels",
  ". 4D Parsing on Synthetic Datasets": "We conducted controlled 4D parsing experiments on twosynthetic datasets, CLOTH4D and BEDLAM ,where the cloth meshes are simulated from cloth templateson top of the parameterized body models.Since withinthese synthetic datasets, some inner body and cloth verticesare always invisible from the outside, we report our label-ing accuracy only on the vertices that are visible from our24 views of rendered images. Baseline comparison.We first compare our 4D humanparsing method with a template-based baseline method that utilizes a semantic SMPL+D template to first track theclothed human shape, and then project the template labelsto neighboring scan vertices. Since ClothCap didntrelease their 4D parsing code, we implemented their pars-ing method following their descriptions. We first registerthe SMPL+D model to all frames. Then we initialize thefirst frame template label using the nearby scan vertex la-bels obtained through our first-frame initialization process.At each frame, we update the template labels using thebody prior, previous frame prior, and the Gaussian Mixture Model trained from the vertex colors of each labeled cate-gory. Finally, the scan vertex labels are assigned from thenearest template label. The quantitative parsing results fromthis baseline method are shown in the main paper. Here, weshow more qualitative results in .The main issue of this template-based baseline methodis fitting the SMPL+D template to loose human outfits. Thespatial mismatch between template and loose garments gen-erates incorrect labels, especially in the open area of thejackets. Besides this, precisely updating the template labelsusing the Gaussian mixture model of labeled vertex colorsis also difficult, especially in front of garments that havesimilar colors. The limited template resolution also resultsin noisy boundary labels at the higher-resolution clothedhuman meshes. The parsing accuracy from this baselinemethod is below 90% for all synthetic outfits. Ablation studies.We then compare our 4D human pars-ing method (without manual rectifications) with severalablations of the multi-view parsing inputs (PAR Only,PAR+OPT, PAR+OPT+SAM), as shown in . Similarto in the main paper, we observed similar qualitativeresults on the synthetic datasets.",
  ". 4D Parsing with New Labels": "The six classes in our 4D-DRESS are strategically definedto ensure a consistent benchmark evaluation for clothingsimulation and reconstruction. We showcase the general-ization ability of our parsing method with new labels in, by effectively distinguishing a belt from pants andsocks from shoes. Initiated during the first-frame initial-ization, these new labels can integrate into the 4D parsingpipeline. However, refining labels for these smaller clothesand objects may entail additional manual efforts for rectifi-",
  ". Data Capturing Steup": "We captured our dataset with a volumetric capture sys-tem equipped with 106 synchronized cameras (53 RGBand 53 IR cameras). The sequences are filmed at 12 MP, 30FPS, and within an effective capture volume of 2.8 m in di-ameter and 3 m in height. Each frame consists of a meshwith 80k faces and a texture map.",
  ". Clothing Distribution": "We compute the mean distances from the outfits to the reg-istered SMPL body surfaces. The inner and outer outfitsexhibit distance ranges of up to 7.12 cm and 14.76 cm, re-spectively, over all frames. The distribution of the distanceon the SMPL body is shown in . In the 10% mostchallenging frames that have a larger Chamfer distance be-tween scan mesh and SMPL mesh, the distance range in-creases to 20.09 cm for outer outfits. We further visualizethe mean distances of each garment category, as shown in. The average Chamfer distance between the clothedhuman scans and SMPL body meshes are 3.30 cm and 5.28cm for the inner and outer outfits in our 4D-DRESS dataset,and 2.21 cm in the X-Humans dataset .",
  ". Clothing Simulation": "4D-Dress provides diverse garments and challenging hu-man pose sequences, which serves as a great asset for futureresearch in clothing simulation. Unlike the synthesized gar-ment templates with smooth surfaces and simple topologies,we provide templates extracted from scans, with realistic wrinkles and complex structures. Using these templates,we evaluated the performance of recent unsupervised clothsimulators, including PBNS , Neural Cloth Simulator(NCS) and HOOD , and a baseline method, linearblend-skinning. We quantitatively and qualitatively com-pared the generated garments with our scanned garments.We also demonstrated the potential of HOOD by simply op-timizing the material parameters, which again confirmed thevalue of our dataset. In the following sections, we elaborateon each step of our experiments.",
  ". Template Extraction": "Current clothing simulation algorithms rely on a predefinedgarment template, deforming it to generate realistic simula-tions under various poses. They typically utilized synthe-sized garment templates, with unnaturally smooth surfacesand basic topologies. In our work, we provided templatesdirectly extracted from real-world scans, offering a more re-alistic foundation for deformation.Firstly, we select from pose sequences the frames closestto the canonical pose, in other words, T-pose. We alsomake sure that the body in this frame is static and garmentsare in rest status. Then we apply inverse LBS to convertthe scans into exact canonical pose. After extracting gar-ment meshes from the unposed scans, we made some man-ual efforts to recover the garment shape in Blender .Specifically, we erased unwanted faces, solved penetrationsbetween clothing and body, and smoothed rigid wrinklesand coarse boundaries. Synthesized templates used by cur-rent simulators usually have 4-5k vertices. We observed inexperiments that too many vertices in the template are com-putationally expensive for simulation and may erode per-formance. Therefore, we downsampled each template to30-50%, which now has 3-8k vertices in total depending oneach garments surface area, while keeping them in theiroriginal shapes. To use lower garments in simulators, likepants and lower skirts, pinned vertices are compulsory forthem to stay on the body. We extract the loop around thewaist as pinned vertices and provide their indexes.",
  ". Evaluation Details": "In the clothing simulation benchmark, we compared fourdifferent clothing simulators: LBS, PBNS , NCS , andHOOD . The training and evaluation of each methodwere conducted using the SMPLX model, which providesmore details in visualization. The final evaluation is doneon four types of garments(Upper, Outer, Dress, and Lower),with each having 2 garments and 6 sequences in total. Forqualitative evaluation, we employed Chamfer distance andstretching energy, scaling vertex positions by a factor of 100to use centimeters as the unit.The Chamfer distance, shown in equation 7, is computedby summing the squared distances between nearest neigh-",
  "iei ei2(8)": "We provide more details on implementing each method:LBS blends joint transforms with skinning-weights. Foreach garment template, we find the nearest body node onthe canonical SMPLX human, and get the skinning weightson this point. Then, we follow the same forward LBS pro-cess in SMPLX to get deformed template meshes.PBNS and NCS, both are deformation-based methods,predict vertex-wise deformation on the template and em-ploy LBS to transform the deformed garment into desiredposes. Given their One model for one garment nature, wetrained each garment from scratch. We also used identicalAMASS sequences mentioned in the NCS paper to ensurefairness. As both PBNS and NCS developed using SMPL,we made slight adjustments to the data-loading pipeline toensure their compatibility with SMPLX. And we assignedzero poses to joints that are exclusive in SMPLX.Meanwhile, we also kept the same training settings usedin their original papers.For PBNS, default parameterswere used, and each garment underwent training for 20-50 epochs to ensure convergence. For NCS, a batch sizeof 2048 was employed across all training instances, as sug-gested in their paper. In the case of tight garments, default parameters were maintained with a temporal window sizeof 0.5 and 10 iterations for blend weights smoothing. Inthe case of loose garments like outerwear and dresses, wemade slight parameter adjustments for stable training, typi-cally using a temporal window size of 0.75 and 1, with 50iterations for blend weights smoothing, as suggested by theauthor in a GitHub issue.HOOD, as a simulation-based method, predicts physi-cally realistic fabric dynamics and is agnostic to garmenttopology.Hence, we directly used a pre-trained pub-licly available model to evaluate our garments. Unlike thedeformation-based methods, which convert the template incanonical pose to any pose instantly, HOOD predicts gar-ment motion frame by frame.Therefore, to apply ourcanonical template for simulating each sequence, we haveto convert the template into the pose of the first frame. In theHOOD paper, they used LBS to convert templates, whichworks for tight synthesized garments.However, for ourreal-world garments, it usually results in large stretchingon mesh, especially around joint areas. Therefore, alter-natively, we insert extra frames from the canonical pose tothe first frame and simulate the prolonged sequence to get anatural transform from the canonical pose. The first posesfor all sequences in our dataset are in A-pose. Generally,we insert 30 frames to transfer from canonical to A-pose,which makes it slow enough for the garment to stay in reststatus with minimum dynamics.",
  ". HOOD*: Material Optimization": "HOOD provides 4 local material parameters for each vertex,including and evaluating the ability of stretching andarea preservation, mass m computed from the fabric den-sity, and the bending coefficient kbending penalizing foldingand wrinkles. For each edge, there are three material pa-rameters, including , , and kbending. Assuming we havev vertices, e edges, and coarse edges in total, we define thematerial parameters as M R4v+3e.In the fine-tuning process, we freeze the pre-trainedHOOD model H and only update material parameters M.Using all 6 sequences of each garment for training, we feedthem into model f to get simulated outputs. Then, withGround Truth garment mesh G, we compute Chamfer dis-tance and stretching energy, as described in equation 9.",
  "L = LCD(f(M, H), G) + wLEstr(f(M, H), G)(9)": "We used the stretching energy from HOOD and set w as1 in our experiments. Chamfer distance LCD is described inequation 10, measuring the average distance between sim-ulation and ground-truth garment. We use V, ( [s, g])to represent the simulated and ground truth vertices and useN as the total number of vertices. . Additional qualitative results for clothing simulation. Left are templates used for simulations. Right are simulations andground-truth scans. HOOD presents more dynamic while getting overly stretched. HOOD* matches well with ground truth."
}