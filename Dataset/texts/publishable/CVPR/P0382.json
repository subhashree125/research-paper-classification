{
  "Abstract": "Generalizable NeRF aims to synthesize novel views forunseen scenes.Common practices involve constructingvariance-based cost volumes for geometry reconstructionand encoding 3D descriptors for decoding novel views.However, existing methods show limited generalizationability in challenging conditions due to inaccurate geome-try, sub-optimal descriptors, and decoding strategies. Weaddress these issues point by point.First, we find thevariance-based cost volume exhibits failure patterns as thefeatures of pixels corresponding to the same point canbe inconsistent across different views due to occlusionsor reflections.We introduce an Adaptive Cost Aggrega-tion (ACA) approach to amplify the contribution of con-sistent pixel pairs and suppress inconsistent ones.Un-like previous methods that solely fuse 2D features intodescriptors, our approach introduces a Spatial-View Ag-gregator (SVA) to incorporate 3D context into descriptorsthrough spatial and inter-view interaction. When decod-ing the descriptors, we observe the two existing decodingstrategies excel in different areas, which are complemen-tary. A Consistency-Aware Fusion (CAF) strategy is pro-posed to leverage the advantages of both. We incorporatethe above ACA, SVA, and CAF into a coarse-to-fine frame-work, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu).GeFu attains state-of-the-artperformance across multiple datasets. Code is available at",
  "MVSNeRFENeRFOurs": ". Comparison with existing methods. (a) With three in-put source views, our generalizable model synthesizes novel viewswith higher quality than existing methods in the severe oc-cluded area. (b) Circle area represents inference time. The X-axisrepresents the PSNR on the DTU dataset and the Y-axis rep-resents the PSNR on the Real Forward-facing dataset . Ourmethod attains state-of-the-art performance.",
  "for each scene, which limits its applications.To address this issue, some recent methods [5, 7, 9, 15,": "19, 20, 23, 3032, 38, 44] generalize NeRFs to unseenscenes. Instead of overfitting the scene, they extract fea-ture descriptors for 3D points in a scene-agnostic manner,which are then decoded for rendering novel views. Pio-neer methods utilize 2D features warped from thesource images. As this practice avoids explicit modeling of3D geometric constraints, its generalization capability forgeometry reasoning and view synthesis in new scenes islimited. Hence, subsequent methods introduce ex-plicit geometry-aware cost volume from multi-view stereo(MVS) to model geometry at the novel view. Despite signif-icant progress achieved by these methods, synthesis resultsremain unsatisfactory, especially in challenging areas, suchas the occluded area illustrated in (a). The modelstruggles to accurately infer the geometry of these regions,",
  "arXiv:2404.17528v1 [cs.CV] 26 Apr 2024": "as variance-based cost volumes cannot perceive occlusions.Generalizable NeRFs pipeline consists of two phases:radiance field reconstruction and rendering. The reconstruc-tion phase aims to recover scene geometry and encode 3D-aware features for rendering, i.e., creating descriptors for3D points. For geometry reasoning, similar to MVS, if thegeometry (i.e., depth) is accurate, features across variousviews are supposed to be similar with low variance. How-ever, this variance-based cost metric is not universal, espe-cially in occluded and reflective regions. Due to inconsis-tent features in these regions, equally considering the con-tributions of different views is unreasonable, leading to mis-leading variance values. Inspired by MVS methods ,we propose an Adaptive Cost Aggregation (ACA) mod-ule. ACA adaptively reweights the contributions of differentviews based on the similarity between source views and thenovel view. Since the novel view is unavailable, we employa coarse-to-fine framework and transfer the rendering viewfrom the coarse stage into the fine stage to learn adaptiveweights for the cost volume construction.With the geometry derived from the cost volume, we canre-sample 3D points around the surface and encode descrip-tors for sampled points. For descriptors encoding, previ-ous methods directly aggregate inter-viewfeatures into descriptors for subsequent rendering. Thesedescriptors lack 3D context awareness, leading to disconti-nuities in the descriptor space. To this end, we design theSpatial-View Aggregator (SVA) to learn 3D context-awaredescriptors. Specifically, we encode spatially context-awareand smooth features by aggregating 3D spatial informa-tion. Meanwhile, to preserve geometric details, we utilizesmoothed features as queries to reassemble high-frequencyinformation across views to create final descriptors.With the scene geometry and point-wise descriptors, thesubsequent rendering phase aims to decode descriptors intovolume density and radiance for rendering a novel view. Forthe radiance prediction, predict blending weightsto combine color values from source views, while directly regress from features.However, the analysis ofthese two approaches has not been conducted in existingworks. In this paper, we observe that the blending approachperforms better in most areas ( (a)), as the color val-ues from source views provide referential factors. However,as shown in (b)&(c), in challenging areas such as re-flections and boundaries, the regression approach producessuperior results with fewer artifacts, while the blending ap-proach leads to suboptimal rendering due to unreliable ref-erential factors. To unify the advantages of both strategies,we propose to separately predict two intermediate views us-ing two approaches and design a weighted structure namedConsistency-Aware Fusion (CAF) to dynamically fuse theminto the final view. The fusing weights are learned by check-ing multi-view consistency, following an underlying princi-",
  "Error: (a) < (b)Error: (a) > (b)": ". Comparison of two rendering strategies. (a) The viewobtained using the blending approach that combines color valuesfrom source views. (b) The view obtained using the regressionapproach that directly regresses color values from features. (c)Accuracy comparison between two rendering strategies. The for-mer strategy performs better in the white regions, while worse inthe green ones. ple that if color values are close to the ground truth, themulti-view features corresponding to the correct depth aresupposed to be similar.By embedding the above ACA, SVA, and CAF into acoarse-to-fine framework, we propose GeFu. To demon-strate the effectiveness, we evaluate GeFu on the widely-used DTU , Real Forward-facing , and NeRF Syn-thetic datasets.Extensive experiments show thatGeFu outperforms other generalizable NeRFs by large mar-gins without scene-specific fine-tuning as shown in (a)&(b).After per-scene fine-tuning, GeFu also outper-forms other generalizable NeRFs and achieves performancecomparable to or even better than NeRF . Additionally,GeFu is capable of generating reasonable depth maps, sur-passing other generalization methods.Our main contributions can be summarized as follows: We propose ACA to improve geometry estimationand SVA to encode 3D context-aware descriptors forgeometry-aware reconstruction.",
  ". Related Work": "Multi-View Stereo.Given multiple calibrated images,multi-view stereo (MVS) aims to reconstruct a dense 3Drepresentation. Traditional MVS methods primarily rely on hand-crafted features and similarity met-rics, which limits their performance, especially in chal-lenging regions such as weak-texture and repetitive ar-eas.Powered by the impressive representation of neu-ral networks, MVSNet first proposes an end-to-endcost volume-based pipeline, which quickly becomes themainstream in the MVS community. Following works ex-plore the potential capacity of this pipeline from various aspects. e.g., reducing memory consumption with recur-rent approaches or coarse-to-fine paradigms , enhancing feature representations andmodeling output formats . Another important lineis to optimize the cost aggregation by adaptivelyweighting contributions from various views. In this paper,following the spirit, we introduce the adaptive cost aggre-gation tailored for the NVS task to mitigate the issue of in-consistent features caused by reflections and occlusions.Generalizable NeRF. With implicit continuous represen-tation and differentiable volume rendering, NeRF achieves photo-realistic view synthesis.However, NeRFand its downstream expansion works require an expensive per-scene optimization process. Toaddress this issue, some generalizable NeRF methods havebeen proposed, following a reconstruction-and-renderingpipeline. In the reconstruction phase, each sampled pointis assigned a feature descriptor. Specifically, according tothe descriptors, generalizable NeRF methods can be catego-rized into the following types: appearance descriptors ,aggregated multi-view descriptors , cost vol-ume interpolated descriptors , and correspon-dence matching descriptors . Despite different forms,these descriptors only aggregate inter-view information orare interpolated from the low-resolution cost volume, lack-ing the ability to effectively perceive 3D spatial context. Toremedy the issue, we utilize a proposed aggregator to facil-itate the interaction of spatial information. In the renderingphase, volume density is obtained by decoding descriptors.For radiance, predict blending weights to combinecolor from source views, while directlyregress features. In this paper, we observe that these twostrategies benefit different regions and thus propose a uni-fied structure to integrate their advantages.",
  ". Preliminaries": "Learning-based MVS. Given a target image and N sourceimages, MVS aims to recover the geometry, such as thedepth map of the target image. The key idea of MVS isto construct the cost volume from multi-view inputs, aggre-gating 2D information into 3D geometry-aware representa-tion. Specifically, each voxel-aligned feature vector fc ofcost volume can be computed as:",
  "fc = (ft, f 1s , ..., f Ns ) ,(1)": "where ft and f is represent the target feature vector and thewarped source feature vector, respectively. And denotes aconsistency metric, such as variance. The underlying prin-ciple is that if a sampled depth is close to the actual depth,the multi-view features of the sampled point are supposedto be similar, which naturally performs multi-view corre-spondence matching and geometry reasoning, facilitatingthe generalization to unseen scenes. Generalizable NeRF. In generalizable NeRFs, each sam-pled point is assigned a geometry-aware feature descriptorfp, as (x, d, fp) (, r), where x and d represent thecoordinate and view direction used by NeRF . andr denote the volume density and radiance for the sampledpoint, respectively. Specifically, the volume density canbe obtained from the descriptor via = MLP(x, fp). Forthe radiance r, one approach is to predict blendingweights to combine color values from source views, as:",
  "exp(wi)ciNj=1 exp(wj), where wi = MLP(x, d, fp, f is) ,(2)": "where {ci}Ni=1 are color values from source views. Thesecolors provide referential factors, facilitating convergenceand better performance in most areas.However, in oc-cluded and reflective regions, these colors introduce mis-leading bias to the combination, resulting in distorted col-ors.Besides, for pixels on object boundaries, it is hardto accurately locate reference points for blending, result-ing in oscillated colors between the foreground and back-ground. Another approach is to directly regressthe radiance from the feature per r = MLP(x, d, fp). Asfewer inductive biases are imposed on the output space, themodel can learn to predict fewer artifacts in challenging ar-eas (). With the volume density and radiance of sam-pled points, the color values c of each pixel can be computedby volume rendering, given by:",
  ". Method": "Given a set of source views {Iis}Ni=1, NVS aims to gener-ate a target view at a novel camera pose. As illustrated in, our method consists of an NVS pipeline wrapped in acoarse-to-fine framework. In the pipeline, we first employ afeature pyramid network to extract multi-scale featuresfrom the source views. Then, we propose an Adaptive CostAggregation module (Sec. 4.1) to construct a cost volume,which is further processed by a 3D-CNN to infer the geom-etry. Guided by the estimated geometry, we re-sample 3Dpoints around the surface and apply the Spatial-View Ag-gregator (Sec. 4.2) to encode 3D-aware feature descriptorsfp for sampled points. Finally, we decode fp into two inter-mediate views using two strategies introduced in Sec. 3 andfuse them into the final target view through Consistency-Aware Fusion (Sec. 4.3). This pipeline is iteratively called.Initially, the low-resolution target view is generated and therough scene geometry is captured. Then, in the subsequentrefining stage, high-resolution results with fine-grained ge-ometry are obtained.",
  "||depth": ". The overview of GeFu. In the reconstruction phase, we first infer the geometry from the constructed cost volume, and thegeometry guides us to further re-sample 3D points around the surface. For each sampled point, the warped features from source imagesare aggregated and then fed into our proposed Spatial-View Aggregator (SVA) to learn spatial and inter-view context-aware descriptors fp.In the rendering phase, we apply two decoding strategies to obtain two intermediate views and fuse them into the final target view in anadaptive way, termed Consistency-Aware Fusion (CAF). Our pipeline adopts a coarse-to-fine architecture, the geometry from the coarsestage (ls = 1) guides the sampling at the fine stage (ls > 1), and the features from the coarse stage are transferred to the fine stage forACA to improve geometry estimation. Our network is trained end-to-end using only RGB images.",
  ". Adaptive Cost Aggregation": "The core process of geometry reasoning is to construct acost volume that encodes the multi-view feature consis-tency. Previous works treat different views equallyand employ the variance operator to construct a cost vol-ume.However, due to potential occlusions and varyinglighting conditions among different views, multi-view fea-tures of the same 3D point may exhibit notable disparities,thereby resulting in misleading variance values. Inspiredby , we propose to adaptively weight the contributionof different views to the cost volume, termed Adaptive CostAggregation (ACA). ACA will suppress the cost contribu-tion from features that are inconsistent with the novel viewscaused by reflections or occlusions, and enhance the con-tribution of better-matched pixel pairs. The voxel-alignedfeature fc of cost volume can be computed as:",
  "i=1(1 + (f ic)) f ic , where f ic = (f is ft)2 ,(4)": "where ft and f is denote the target feature vector and thewarped feature vector of source image Iis, respectively. denotes Hadamard multiplication and (.) represents theadaptive weight for each view. However, an important chal-lenge is that the target view is available in MVS, but not forNVS. To remedy this, we adopt a coarse-to-fine frameworkwhere a coarse novel view is first generated and serves asthe target view in Eq. (4). Specifically, we obtain the coarse-stage feature fb by accumulating descriptors fp of sampled",
  ". Spatial-View Aggregator": "With the estimated geometry, 3D points around objects sur-faces can be re-sampled, and the subsequent step is encod-ing descriptors for these sampled points. Existing meth-ods aggregate inter-view features with a poolingnetwork to construct the descriptors fp = ({f is}Ni=1).However, these descriptors only encode multi-view infor-mation and lack the awareness of 3D spatial context, lead-ing to discontinuities in the descriptor space. To introduce3D spatial information, a feasible approach is to interpo-late the low-resolution regularized cost volume, but it lacksfine-grained details. To address these issues, we design a3D-aware descriptors encoding approach. We first utilize a3D U-Net termed sm to aggregate 3D spatial context, as:",
  ". Consistency-Aware Fusion": "With the descriptor fp, the volume density is first acquiredthrough an MLP, and color values can be obtained usingtwo decoding approaches discussed in Sec. 3. We observethat these two approaches exhibit advantages in differentareas. To combine their strengths, we propose to predict twointermediate views using these two approaches separately,and then fuse them into the final target view.Specifically, for the blending approach, the radiance r ofeach sampled point can be computed using Eq. (2). Andthen the pixel color cb and feature fb are obtained via thevolume rendering manner per Eq. (3) and Eq. (5), respec-tively.For another regression approach, one practice ispredicting radiance for sampled points and then accumu-lating it into pixel color. In contrast, to reduce computa-tional costs, we accumulate point-wise descriptors into fea-tures and then decode them into pixel color. Specifically,we feed the accumulated feature into a 2D U-Net for spa-tial enhancement to obtain pixel feature fr, followed by anMLP to yield the pixel color cr.Since these two approaches excel in different areas, in-stead of using a fixed operator, such as average, we pro-pose dynamically fusing them via ct = wbcb +wrcr, wherewb and wr are predicted fusing weights.As the targetview is unavailable, comparing the quality of cb and cr be-comes a chicken-and-egg problem. A naive practice is to di-rectly predict fusing weights from features, which is under-constrained (Sec. 5.5). In contrast, we propose to learn fus-ing weights by using the multi-view feature consistency asa hint. The underlying motivation is that if the predictedcolors closely resemble the ground-truth colors, the corre-sponding features of the predicted view and source viewsunder the correct depth are supposed to be similar.Specifically, we first obtain the final predicted depthdf in a volume rendering-like way per df = k k(1 exp(k))dk, where dk represents the depth of sampledpoint. With the depth df, we can obtain the warped features{f is}Ni=1 from source views, and the multi-view consistencycan be computed using variance:",
  ". Settings": "Datasets. Following MVSNeRF , we divide the DTU dataset into 88 training scenes and 16 test scenes. We firsttrain our generalizable model on the 88 training scenes ofthe DTU dataset and then evaluate the trained model onthe 16 test scenes.To further demonstrate the general-ization capability of our method, we also test the trainedmodel (without any fine-tuning) on 8 scenes from the RealForward-facing dataset and 8 scenes from the NeRFSynthetic dataset, both of which have significant dif-ferences in view distribution and scene content compared tothe DTU dataset. The image resolutions of the DTU, theReal Forward-facing, and the NeRF Synthetic datasets are512 640, 640 960, and 800 800, respectively. Thequality of synthesized novel views is measured by PSNR,SSIM , and LPIPS metrics.Baselines. We compare our methods with state-of-the-artgeneralizable NeRF methods . Forgeneralization with three views and per-scene optimization,",
  "-view": "24.030.9140.19220.220.7630.28720.560.8560.243NeuRay 24.510.8250.20322.730.7200.23622.420.8650.228ENeRF 25.480.9420.10722.780.8210.19124.830.9310.117GNT 24.320.9030.20120.910.6830.29323.470.8770.151MatchNeRF 25.030.9190.18120.590.7750.27620.570.8640.200Ours26.980.9550.08123.390.8390.17625.300.9390.082 . Quantitative results under the generalization setting. We show the average results of PSNRs, SSIMs, and LPIPSs on threedatasets under two settings for the number of input views. ENeRF represents results borrowed from the original paper. The comparisonmethods are organized based on the year of publication.",
  ". Quantitative results under the per-scene optimization setting. The best result is in bold, and the second-best is underlined": "we follow the same setting as and borrow the re-sults of from . We evaluate using the official code and trained models. To keep consis-tent with the same settings for a fair comparison, such asthe number of input views, dataset splitting, view selection,and image resolution, we use the released code and trainedmodel of and retrain with the released code, andevaluate them under our test settings. For generalizationwith two views, we borrow the results of from .For other baselines , we evaluate them using thereleased model or the retrained model . Implementation Details. Following , the number ofcoarse-to-fine stages Ns is set to 2. In our coarse-to-fineframework, we sample 64 and 8 depth planes for the coarse-level and fine-level cost volumes, respectively.And wesample 8 and 2 points per ray for the coarse-level and fine-level view rendering, respectively. We set p = 0.1 and s = 0.1 in Eq. (11), while 1 = 0.5 and 2 = 1 inEq. (12). We train our model on four RTX 3090 GPUs us-ing the Adam optimizer. Refer to the supplementarymaterial for more implementation and network details.",
  ". Generalization Results": "We report quantitative results on DTU, Real Forward-facing, and NeRF Synthetic datasets in under gen-eralization settings.PixelNeRF , which applies ap-pearance descriptors, has reasonable results on the DTUtest set, but insufficient generalization on the other twodatasets. Other methods that modelthe scene geometry implicitly or explicitly by aggregat-ing multi-view features can maintain relatively good gen-eralization. Thanks to our proposed modules tailored forboth the reconstruction and rendering phases, our methodachieves significantly better generalizability. As shown in",
  ". Per-scene Fine-tuning Results": "The quantitative results after per-scene optimization areshown in and we report the results of our methodafter 15 minutes and 1 hour of fine-tuning. Due to the ex-cellent initialization provided by our generalization model,only a short period of fine-tuning is needed to achieve goodresults. Our results after 15 minutes of fine-tuning are com-parable to or even superior to those of NeRF optimized",
  "MVSNet 3.600.6030.955---PixelNeRF 490.0370.17647.80.0390.187IBRNet 3380.0000.9133240.0000.866MVSNeRF 4.600.7460.9137.000.7170.866ENeRF 3.800.8370.9394.600.7920.917Ours2.470.9000.9712.830.8790.961": ". Quantitative results of depth reconstruction on theDTU test set. MVSNet is trained with depth supervision whileother methods are trained with only RGB image supervision. Abserr represents the average absolute error and Acc(X) means thepercentage of pixels with an error less than X mm. for substantially longer time (10.2 hours), and also outper-form the results of other generalization methods after fine-tuning. With a longer fine-tuning duration, such as 1 hour,the rendering quality can be further improved. Qualitativeresults can be found in the supplementary material.",
  ". Ablations and Analysis": "Ablation studies. As shown in , we conduct abla-tion studies to investigate the contribution of each proposedmodule. Each individual component can benefit the base-line model in both view quality and depth accuracy, withCAF having the highest gain. An interesting phenomenon isthat CAF greatly improves depth accuracy, indicating that awell-designed view decoding approach also facilitates depthprediction. Combining all components results in the great-est gain, with a 5.9% increase in PSNR and a 34.5% im-provement in depth error compared to the baseline model.CAF working mechanism. As shown in , we visu-alize the fusion weights of two decoding approaches. Theregression approach exhibits higher confidence in challeng-ing areas such as object edges and reflections, while theblending approach shows higher confidence in most otherareas, which is consistent with the observation in .As shown in , we define challenging areas as thosewith high confidence in Wr and divide them by a series ofthresholds. A smaller threshold X indicates a more diffi-cult region. When X = 5%, our method improves PSNRby 2.55db and Abs err by 3.47mm. When the thresholdincreases, such as X = 50%, our method improves PSNRby 1.97db and Abs err by 1.54mm, which further demon-strates the superiority of our method in challenging areas.Fusion strategy. As shown in , we investigate theperformance of different fusion strategies. Comparing No.1",
  ". Comparison of different fusion strategies. The AErepresents the refinement by an autoencoder. The DWF representsdirect weighted fusion. The CAF is our proposed Consistency-Aware Fusion": "and No.2, the result of using the blending approach aloneis better than that of using the regression approach alone.For No.4, the DWF represents fusion weights derived di-rectly from features of the two intermediate views, whichgreatly degrades performance compared to our proposedway of checking the multi-view consistency (No.5). Ourfusion approach utilizes the advantages of the two decodingapproaches to refine the synthesized view. In No.3, we re-fine the synthesized view decoded in a single way, where thesynthesized view is fed into an auto-encoder for refinement,which has limited improvement.",
  ". Conclusion": "In this paper, we present a generalizable NeRF method ca-pable of achieving high-fidelity view synthesis.Specifi-cally, during the reconstruction phase, we propose AdaptiveCost Aggregation (ACA) to improve geometry estimationand Spatial-View Aggregator (SVA) to encode 3D context-aware descriptors. In the rendering phase, we introduce theConsistency-Aware Fusion (CAF) module to unify their ad-vantages to refine the synthesized view quality. We inte-grate these modules into a coarse-to-fine framework, termedGeFu. Extensive evaluations and ablations demonstrate theeffectiveness of our proposed modules. Henrik Aanaes, Rasmus Ramsbol Jensen, George Vogiatzis,Engin Tola, and Anders Bjorholm Dahl. Large-scale data formultiple-view stereopsis. Int. J. Comput. Vis., 120:153168,2016. 1, 2, 5, 6, 11, 14, 15, 18 Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall,Kalyan Sunkavalli, Milos Hasan, Yannick Hold-Geoffroy,David Kriegman, and Ravi Ramamoorthi.Neural re-flectance fields for appearance acquisition. arXiv preprintarXiv:2008.03824, 2020. 3 Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Bar-ron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectancedecomposition from image collections. In Proc. IEEE Int.Conf. Comput. Vis., pages 1268412694, 2021. 3 Di Chang, Aljaz Bozic, Tong Zhang, Qingsong Yan, Ying-cong Chen, Sabine Susstrunk, and Matthias Niener. Rc-mvsnet: Unsupervised multi-view stereo with neural render-ing. In Proc. Eur. Conf. Comput. Vis., 2022. 13 Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-izable radiance field reconstruction from multi-view stereo.In Proc. IEEE Int. Conf. Comput. Vis., pages 1412414133,2021. 1, 2, 3, 4, 5, 6, 7, 11, 12, 15, 16, 17, 18, 19, 21, 22",
  "Anpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su,and Jingyi Yu. Sofgan: A portrait image generator with dy-namic styling. ACM Trans. Graph., 41(1):126, 2022. 3": "Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng,Tat-Jen Cham, and Jianfei Cai.Explicit correspondencematching for generalizable neural radiance fields.arXivpreprint arXiv:2304.12294, 2023. 1, 2, 3, 5, 6, 7, 16, 17,18, 19 Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Er-ran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo usingadaptive thin volume representation with uncertainty aware-ness.In Proc. IEEE Conf. Comput. Vis. Pattern Recogn.,pages 25242534, 2020. 3 Julian Chibane, Aayush Bansal, Verica Lazova, and GerardPons-Moll. Stereo radiance fields (srf): Learning view syn-thesis for sparse views of novel scenes. In Proc. IEEE Conf.Comput. Vis. Pattern Recogn., pages 79117920, 2021. 1 Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang,Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnetglobal context-aware multi-view stereo network with trans-formers. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn.,pages 85858594, 2022. 3",
  "Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, ChanghuWang, and Gim Hee Lee. Mine: Towards continuous depthmpi with nerf for novel view synthesis. In Proc. IEEE Int.Conf. Comput. Vis., 2021. 1": "Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,Hujun Bao, and Xiaowei Zhou.Efficient neural radiancefields for interactive free-viewpoint video. In SIGGRAPHAsia Conference Proceedings, 2022. 1, 2, 3, 4, 5, 6, 7, 11,12, 15, 16, 17, 18, 19, 21, 22 Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,Bharath Hariharan, and Serge Belongie.Feature pyra-mid networks for object detection.In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 21172125, 2017. 3 Tianqi Liu, Xinyi Ye, Weiyue Zhao, Zhiyu Pan, Min Shi, andZhiguo Cao. When epipolar constraint meets non-local oper-ators in multi-view stereo. In Proc. IEEE Int. Conf. Comput.Vis., pages 1808818097, 2023. 3 Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, PengWang, Christian Theobalt, Xiaowei Zhou, and WenpingWang.Neural rays for occlusion-aware image-based ren-dering. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn.,2022. 1, 3, 5, 6, 13, 15, 16, 21, 22 Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, andAbhishek Kar. Local light field fusion: Practical view syn-thesis with prescriptive sampling guidelines.ACM Trans.Graph., 38(4):114, 2019. 1, 2, 5, 6, 11, 12, 13, 14, 15, 18 Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. In Proc. Eur. Conf. Comput. Vis., 2020. 1, 2, 3, 5, 6,7, 11, 12, 13, 14, 15, 18, 21, 22 Keunhong Park, Utkarsh Sinha, Jonathan T Barron, SofienBouaziz, Dan B Goldman, Steven M Seitz, and RicardoMartin-Brualla. Nerfies: Deformable neural radiance fields.In Proc. IEEE Int. Conf. Comput. Vis., pages 58655874,2021. 3",
  "Alex Trevithick and Bo Yang. Grf: Learning a general radi-ance field for 3d representation and rendering. In Proc. IEEEInt. Conf. Comput. Vis., pages 1518215192, 2021": "Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser.Ibrnet:Learning multi-view image-based rendering. In Proc. IEEEConf. Comput. Vis. Pattern Recogn., 2021. 1, 2, 3, 4, 5, 6, 7,11, 15, 21, 22 Xiaofeng Wang, Zheng Zhu, Guan Huang, Fangbo Qin, YunYe, Yijia He, Xu Chi, and Xingang Wang. Mvster epipo-lar transformer for efficient multi-view stereo. In Proc. Eur.Conf. Comput. Vis., pages 573591. Springer, 2022. 2, 3, 14 Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-moncelli. Image quality assessment: from error visibility tostructural similarity. IEEE transactions on image processing,13(4):600612, 2004. 5, 11 Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, andGuoping Wang. Aa-rmvsnet adaptive aggregation recurrentmulti-view stereo network. In Proc. IEEE Int. Conf. Comput.Vis., pages 61876196, 2021. 2, 3, 4, 14",
  "Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and ChangilKim. Space-time neural irradiance fields for free-viewpointvideo. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn.,pages 94219431, 2021. 3": "Fanbo Xiang, Zexiang Xu, Milos Hasan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, and Hao Su. Neutex: Neuraltexture mapping for volumetric neural rendering. In Proc.IEEE Conf. Comput. Vis. Pattern Recogn., pages 71197128,2021. 3 Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proc. IEEE Conf. Comput.Vis. Pattern Recogn., pages 54385448, 2022. 1 Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding,Runze Zhang, Yisong Chen, Guoping Wang, and Yu-WingTai. Dense hybrid recurrent multi-view stereo net with dy-namic consistency checking. In Proc. Eur. Conf. Comput.Vis., pages 674689. Springer, 2020. 3",
  "Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.Mvsnet depth inference for unstructured multi-view stereo.In Proc. Eur. Conf. Comput. Vis., pages 767783, 2018. 2, 7": "Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang,and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proc. IEEE Conf. Comput.Vis. Pattern Recogn., pages 55255534, 2019. 3 Xinyi Ye, Weiyue Zhao, Tianqi Liu, Zihao Huang, ZhiguoCao, and Xin Li.Constraining depth map geometry formulti-view stereo:A dual-depth approach with saddle-shaped depth cells. In Proc. IEEE Int. Conf. Comput. Vis.,pages 1766117670, 2023. 3",
  "I. Implementation and Network Details": "Implementation Details.Our generalizable model istrained on four RTX 3090 GPUs using the Adam optimizer, with an initial learning rate of 5e 4.Thelearning rate is halved every 50k iterations. As shown inFig. I, we present the metrics of the DTU test set vary-ing with the number of iterations.The model tends toconverge after approximately 150k iterations, taking about25 hours.It is worth noting that, with only 8k itera-tions, our model can achieve metrics of 27.68/0.961/0.080,surpassing the metrics of the SOTA method , whichis 27.61/0.956/0.091 (PSNR/SSIM /LPIPS ). Fol-lowing ENeRF , during training, we select 2, 3, and 4source views as inputs with probabilities of 0.1, 0.8, and0.1. To save computational costs, the Consistency-awareFusion (CAF) is exclusively employed in the fine stage,while in the coarse stage, a blending approach is used tosynthesize a low-resolution target view, supervised by theground-truth target view.During training, the final tar-get view is generated by fusing two intermediate views asIt = WbIb + WrIr. However, during evaluation for otherdatasets , which have a different domain comparedwith DTU, increasing the weights of Ib can lead to slightlybetter performance. Therefore, we obtain the final view asIt = (Ib +(WbIb +WrIr))/2. Our evaluation setup is con-sistent with ENeRF and MVSNeRF . The results ofthe DTU test set are evaluated using segmentation masks.The segmentation mask is defined based on the availabilityof ground-truth depth at each pixel. Since the marginal re-gion of images is typically invisible to input images on theReal Forward-facing dataset , we evaluate the 80% areain the center of the images. Incidentally, all the inferencetime presented in of the main text is measured at an",
  "input image resolution of 512 640. The number of ourmodels parameters is 3.15M": "Network Details.Here, we will introduce the network de-tails of the pooling network (Sec. 4.2) and ACA (Sec. 4.1)mentioned in the main text.Pooling network. apply a pooling network to aggregate multi-view features to obtain the descriptor viafp = ({f is}Ni=1). The implementation details are as fol-lows: initially, the mean and variance v of {f is}Ni=1 arecomputed. Subsequently, and v are concatenated witheach f is and an MLP is applied to generate a weight. Thefp is blended via a soft-argmax operator using obtainedweights and multi-view features ({f is}Ni=1).ACA. Per the Eq. (4) in the main text, (.) represents theadaptive weight for each view, and the network architecturethat learns these weights is shown in Table I. Inference Speed.For an image with 512640 resolution,the inference time of our method is 143ms. We decomposethe inference time in Table II and results demonstrate thatthe inference time of our modules is only 71ms, with theremaining 72ms spent saving results.",
  "II. Additional ablation experiments": "Numbers of Views.As shown in Table III, we evaluatethe performance of our trained generalization model andENeRF with different numbers of input views on theDTU test set . With an increase in the number of inputviews, the performance improves as the model can leveragemore multi-view information. In terms of both overall per-formance and the magnitude of performance improvement,our method outperforms ENeRF, indicating its superior ca-pability in leveraging multi-view information for recon-",
  "Table II. Time overhead for each module (in milliseconds). Theterm save in dictionary refers to storing tensor results in dictio-nary form for subsequent evaluation of various metrics": "structing scene geometry and rendering novel views. Ad-ditionally, we also present the performance of our methodon the Real Forward-facing and NeRF Synthetic datasets under different numbers of input views, as shownin Table IV. The results demonstrate the same trend, indi-cating the capability of our model in leveraging multi-viewinformation is generalizable. Features for Intermediate Views.In Sec. 4.3 of the maintext, fb and fr are utilized as the feature representations forthe two intermediate views Ib and Ir, respectively. Subse-quently, their consistency with source views is individuallycomputed to learn fusion weights. The choice of using fb asthe feature for Ib is based on their similar volume renderinggeneration manners, while the selection of fr as the featurefor Ir is driven by their direct projection relationship. Here,we will discuss different selection strategies for the featuresof intermediate views. An alternative approach for the fea-tures of Ib is to blend features from source views. Similarto Eq. (2) in the main text, the calculation of the features fbfor Ib is as follows:",
  "F[b,r] = eI[b,r] ,(II)": "where e represents a feature extractor, instantiated as a 2DU-Net. fb and fr are the pixel-wise features of Fb and Fr,respectively. As shown in Table V, the strategy employedin the main text is slightly superior to the other two alter-native strategies. For the first alternative Eq. (I), fb is ob-tained by blending features from source views. fb lacks 3Dcontext awareness, leading to some information loss in thesubsequently accumulated pixel features. For the second al-ternative Eq. (II), fb and fr are extracted from scratch at theRGB level. This practice is disadvantageous for the subse-quent learning of 3D consistency weights, due to the lackof utilization of 3D information. Additionally, the introduc-tion of a feature extractor also increases the burden on themodel. However, the strategy in the main text maximallyutilizes the obtained 3D-aware descriptors, while also hav-ing the smallest computational cost compared to the othertwo alternative approaches. Intermediate View Supervision.In the main text, weonly supervise the images fused through CAF. However,simultaneously supervising intermediate results is also acommon practice, whose final result is 29.15/0.968/0.065.This result is slightly inferior to supervising only the fusedview (29.36/0.969/0.064). Because each of the two inter-mediate views has its own advantages, supervising only thefused view allows the network to focus on the fusion pro-cess, leveraging the strengths of both. However, simulta-neously supervising the intermediate views burdens the net-work, diminishing its attention to the fusion process. In the-ory, if both intermediate views are entirely correct, the finalfused view will be accurate regardless of the fusion process.The network prioritizes predicting two accurate intermedi-ate views, which is a more challenging task.",
  "ViewsPSNRSSIMLPIPSAbs errAcc(2)Acc(10)": "226.98 / 25.480.955 / 0.9420.081 / 0.1073.86 / 5.530.835 / 0.7560.942 / 0.107329.36 / 27.610.969 / 0.9570.064 / 0.0892.83 / 4.600.879 / 0.7920.961 / 0.917429.77 / 27.730.971 / 0.9590.062 / 0.0892.73 / 4.260.880 / 0.8040.961 / 0.929529.91 / 27.540.971 / 0.9580.062 / 0.0912.69 / 4.290.882 / 0.8000.961 / 0.928 Table III. The performance of our method and ENeRF with different numbers of input views on the DTU test set. Each itemrepresents (Ours/ENeRFs). Abs err denotes the average absolute error and Acc(X) means the percentage of pixels with an error lessthan X mm.",
  "Self-supervision29.210.9680.0643.210.8730.957Supervision29.310.9690.0642.950.8750.957None29.360.9690.0642.830.8790.961": "Table VI. The comparison of different depth supervision sig-nals. The self-supervision represents using unsupervised depthloss and the supervision represents using ground-truth depth forsupervision. The term None refers to training without any depthsupervision signals. 23], including our method, aim to improve the quality ofview synthesis by enhancing the geometry prediction. Byonly supervising RGB images, excellent geometry predic-tions can be achieved (Sec. 5.4 in the main text). Here,we will discuss the impact of incorporating depth supervi-sion signals on the model. We introduce supervision signalsfor depth in two ways: one through self-supervised and theother through supervision using ground-truth depth.Following , the unsupervised depth loss is:",
  "where LP C represents the photometric consistency loss": "LSSIM and LSmooth are the structured similarity loss anddepth smoothness loss, respectively. 1, 2, and 3 are setto 12, 6, and 0.18 in our implementation, respectively. Referto for more details. Ld is used to supervise the finaldepth, i.e., df (Sec. 4.3 in the main text). Since the DTUdataset provides ground-truth depth, another approach is toutilize the ground-truth depth for supervision. The depthloss is as follows:",
  "Ld = (df, dgt)(IV)": "where df and dgt represent the final predicted and ground-truth depth, respectively. denotes a loss function. Fol-lowing , is instantiated as the Smooth L1 loss .The quantitative results are presented in the Table VI. Theperformance of the three strategies in the table is compa-rable, indicating that supervising only the RGB images issufficient, and there is no need for additional introductionof depth supervision signals. More Comprehensive Depth Analysis.As shown in in the main text, our pipeline first infers the geom-etry from the cost volume, re-samples 3D points around ob-jects surfaces, and finally encodes 3D descriptors for ren-dering. We can obtain two depths: one inferred from thecost volume and the other obtained through volume render-ing, which is the final depth. Here, we report the depthobtained from the cost volume and the final depth as shownin Table VII. Compared to the baseline, our method per-forms better on both depth metrics. Thanks to AdaptiveCost Aggregation (ACA), the depth obtained from the costvolume has been significantly improved. Based on this, asthe Spatial-View Aggregator (SVA) encodes 3D-aware de-scriptors, the final depth has also been further improved. Inaddition, the well-designed decoding approach, i.e., CAF,greatly facilitates the depth prediction of the model (Sec. 5.5in the main text). Visualization of ACA.Previous approaches using vari-ance struggle to encode efficient cost information for chal-lenging areas, such as the occluded areas marked in theblack box in Fig. II. Our proposed ACA module learns an",
  "Figure II. Visualization of ACA": "adaptive weight for different views to encode accurate ge-ometric information. As illustrated in Fig. II, the weightslearned by ACA amplify the contribution of consistent pixelpairs, such as the visible areas in source image 1 and 3,while suppressing inconsistent ones, as shown in the oc-cluded areas in the source image 2. Different ACA networks.The primary challenge of ap-plying ACA to the NVS task is the unavailability of thetarget view, which we addressed by adopting a coarse-to-fine framework. In the main text, the weight learningnetwork utilized in ACA is illustrated in Table I, follow-ing the MVS method, i.e., AA-RMVSNet .More-over, other networks can also be embedded into our coarse-to-fine framework to learn inter-view weights. Here, weadopt another MVS method, i.e., MVSTER , to learnadaptive weights.The result on the DTU test set is29.31/0.969/0.064 (PSNR/SSIM/LPIPS), which is compa-rable with the result obtained using . In summary, ourmain contribution is to propose an approach for applyingACA to the NVS task, without specifying a particular net-work for learning weights. Analysis of SVA.Previous approaches directly uses apooling network to aggregate multi-view 2D features forencoding 3D descriptors, which are not spatially context-aware, leading to discontinuities in the decoded depth mapand rendered view (see Fig. III (a)). To address this issue,convolutional networks can be used to impose spatial con-straints on adjacent descriptors. However, due to the smoothnature of convolution, some high-frequency details may belost. Since detailed information comes from the multi-viewfeatures, we employ a divide-and-conquer approach to ag-gregate descriptors. Firstly, we employ a 3D U-Net to ag-gregate spatial context and obtain smooth descriptors. De-spite resolving the issue of discontinuities, an unsharpenedobject edge occurs (Fig. III (b)). Secondly, we propose us-ing smoothed features as queries, with multi-view featuresserving as keys and values. Applying the attention mecha-nism allows us to gather high-frequency details adaptively.",
  "III. More Qualitative Results": "Qualitative Results under the Generalization Setting.As shown in Fig. IV, V, and VI, we present the qualita-tive comparison of rendering quality on the DTU , NeRFSynthetic , and Real Forward-facing datasets, re-spectively. Our method can synthesize views with higherfidelity, especially in challenging areas. For example, inoccluded regions and geometrically complex scenes, ourmethod can reconstruct more details while exhibiting fewerartifacts at objects edges and in reflective areas. Qualitative Results under the Per-scene Optimiza-tion Setting.Benefiting from the strong initialization ofour generalizable model, excellent performance can beachieved within just a short fine-tuning period, such as 15minutes. As shown in Fig. VII, we present the results af-ter fine-tuning.After per-scene optimization, the modeldemonstrates enhanced capabilities in handling scene de-",
  "tails, resulting in views with higher fidelity": "Qualitative Comparison of Depth Maps.As shown inFigs. VIII, IX, and X, we present the qualitative comparisonof depth maps on the DTU , NeRF Synthetic , andReal Forward-facing datasets, respectively. The depthmaps generated by our method can maintain sharper objectedges and preserve more details of scenes, which verifiesthe strong geometry reasoning capability of our method. Fusion Weight Visualization.As shown in Fig. XI, wepresent the fusion weights of the Consistency-aware Fusion(CAF) module. The blending approach generally demon-strates higher confidence in most areas, while the regressionapproach shows higher confidence in challenging regionssuch as object boundaries and reflections. Error Map Visualization.As shown in Fig. XII, wepresent the error maps obtained by two decoding ap-proaches, as well as the error maps of the fused views. Theblending approach tends to exhibit lower errors in most ar-eas, while the regression approach may have lower errorsin some regions with reflections and edges. In addition, wealso present quantitative results, as shown in Table VIII. Theviews fused through Consistency-aware Fusion (CAF) inte-grate the advantages of both intermediate views, achievinga further improvement in quality.",
  "V. Limitations": "Although our approach can achieve high performance forview synthesis, it still has the following limitations. 1) Likemany other baselines , our method is tailored specifi-cally for static scenes and may not perform optimally whenapplied directly to dynamic scenes. 2) During per-scene op-timization, the training speed and rendering speed of NeRF-based methods, including our method, are time-consuming.We will explore the potential of Gaussian Splatting ingeneralizable NVS to address this issue in the future.",
  "Image": "Figure XII. Visualization of error maps. Errb and Errr represent the error maps of the views obtained through the blending approachand the regression approach, respectively. Errt is the error map of the final fused target view. The yellow boxes indicate that the blendingapproach outperforms the regression approach, while the orange boxes indicate regions where the regression approach outperforms theblending approach."
}