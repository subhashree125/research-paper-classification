{
  "Speakercondition": ". Our framework integrates Talking Face Generation (TFG) and Text-to-Speech (TTS) systems, generating synchronised naturalspeech and a talking face video from a single portrait and text input. Our model is capable of variational motion generation by conditioningthe TFG model with the intermediate representations of the TTS model. The speech is conditioned using the identity features extracted inthe TFG model to align with the input identity. AbstractThe goal of this work is to simultaneously generate nat-ural talking faces and speech outputs from text. We achievethis by integrating Talking Face Generation (TFG) andText-to-Speech (TTS) systems into a unified framework. Weaddress the main challenges of each task: (1) generatinga range of head poses representative of real-world scenar-ios, and (2) ensuring voice consistency despite variationsin facial motion for the same identity. To tackle these is-sues, we introduce a motion sampler based on conditionalflow matching, which is capable of high-quality motion codegeneration in an efficient way.Moreover, we introducea novel conditioning method for the TTS system, whichutilises motion-removed features from the TFG model toyield uniform speech outputs. Our extensive experimentsdemonstrate that our method effectively creates natural-looking talking faces and speech that accurately match theinput text. To our knowledge, this is the first effort to build amultimodal synthesis system that can generalise to unseenidentities.",
  ". Introduction": "In recent years, the field of talking face synthesis has at-tracted growing interest, driven by the advancements indeep learning techniques and the development of serviceswithin the metaverse. This versatile technology has diverseapplications in movie and TV production, virtual assistants,video conferencing, and dubbing, with the goal of creatinganimated faces that are synchronised with audio to enablenatural and immersive human-machine interactions.Previous studies in deep learning-based talking face syn-thesis have focused on enhancing the controllability of fa-cial movements and achieving precise lip synchronisation.Some notable works incor-porate 2D or 3D structural information to improve motionrepresentations. From this, recent research has naturallydiverged into two primary strands along the target appli-cations of TFG: one strand concentrateson generating expressive facial movements only from audioconditions. Meanwhile, the other strand aims to enhance the controllability of talking faces byintroducing a target video as an additional condition. De-spite these advancements, the audio-driven TFG methods",
  "arXiv:2405.10272v1 [cs.CV] 16 May 2024": "exhibit limitations, especially in scenarios like video pro-duction and AI chatbots, where video and speech must begenerated simultaneously.An emerging area of research is text-driven TFG, whichis relatively under-explored compared to audio-driven TFG.Several studies have attempted to merge TTSsystems with TFG using a cascade approach, but sufferedfrom issues like error accumulation or computational bot-tleneck. A very recent work uses latent features fromTTS systems for face keypoint generation, yet still requiresan additional stage for RGB video production. It highlightsthe challenges and complexities in integrating TFG and TTSsystems into a cohesive and unified framework.In this paper, we propose a unified framework, namedText-to-Speaking Face (TTSF), which integrates text-driven TFG and face-stylised TTS. The key to our methodlies in analysing mutually complementary elements acrossdistinct tasks and leveraging this analysis to construct animproved framework. As illustrated in , our frame-work is capable of simultaneously generating talking facevideos and natural speeches given text and a face portrait.To combine the different tasks in a single model, we tacklethe primary challenges inherent in each task, TFG and TTS.Firstly, our approach enables the generation of a range ofhead poses that reflect real-world scenarios. To encompassdynamic and authentic facial movements, we propose a mo-tion sampler based on Optimal-Transport Conditional FlowMatching (OT-CFM). This approach learns Ordinary Dif-ferential Equations (ODEs) to extract precise motion codesfrom a sophisticated distribution. Nonetheless, considera-tions need to be taken into account to apply OT-CFM to themotion sampling process. Direct prediction of target mo-tion by OT-CFM results in the generation of unsteady facialmotions. To address this issue, we employ an auto-encoder-based noise reducer to mitigate feature noise through com-pression and reconstruction of latent features. The com-pressed features serve as the target motions for our motionsampler. This demonstrates an enhanced quality of the gen-erated motion, particularly in terms of temporal consistency.Secondly, we focus on the challenge of producing con-sistent voices, specifically when the input identity remainsthe same but facial motions differ.This problem arisesfrom a fundamental inquiry in face-stylised TTS: How canwe extract more refined speaker representations, influenc-ing prosody, timbre, and accent, from a portrait image?We observe that facial motion in the source image affectsthe ability to identify the characteristics of the target voice.Nevertheless, this issue has been overlooked in all previousworks , as they commonly omit a facial motiondisentanglement module, a crucial component in the TFGsystem. With the benefit of integrating the TFG and TTSmodels into a system, we present a straightforward yet ef-fective approach to condition the face-stylised TTS model. By eliminating motion features from the input portrait, ourframework can generate speeches with the consistency ofspeaker identity.In addition to the previously mentioned advantages ofour framework, there are further benefits compared to cas-cade text-driven TFG systems: (1) our framework does notrequire an additional audio encoder, as it can be substitutedwith the text encoder in our system, and (2) the joint trainingeliminates the need for the fine-tuning process and yieldsbetter-synchronised lip motions in the generated outcomes.Our contributions can be summarised as follows:",
  ". Related Works": "Audio-driven Talking Face Generation.Audio-drivenTalking Face Generation (TFG) technology has capturedconsiderable attention in the fields of computer vision andgraphics due to its broad range of applications . Inthe early works , the focus is on situations with in-dividual speakers, where a single model generates varioustalking faces based on a single identity. Recently, advance-ments in deep learning have facilitated the creation of moreversatile TFG models . Thesemodels can generate talking faces by incorporating identityconditions as input. However, these studies overlook headmovements, grappling with the difficulty of disentanglinghead poses from facial characteristics linked to identity. Toenhance natural facial movements, some studies integratelandmarks and mesh or leverage 3D infor-mation . Despite these efforts, performancedegradation occurs, especially in wild scenarios with lowlandmark accuracy. Recent research branches focus on generating vivid facial movements only fromaudio conditions. Another branch demonstrates improved controllability by introducing a tar-get video as an additional condition. These studies show-case the creation of realistic talking faces with various facialmovements, encompassing head, eyes, and lip movements.However, these approaches rely on audio sources for TFG,limiting their applicability in multimedia scenarios lacking",
  "EMB(": ". Overall architecture of our framework. The TTS model receives identity representations from the TFG model, while theTFG model takes conditions for natural motion generation from the TTS model. These complementary elements enhance our modelscapabilities in generating both speech and talking faces. The EMB block denotes an embedding operation. The grey dashed arrowrepresents a path used only during the training process, and the red arrows represent paths used only during the inference process.",
  "an audio source": "Text-driven Talking Face Generation. Text-driven TFGis relatively less explored compared to the field of audio-driven TFG. Most previous works pri-marily focus on generating lip regions for text-based redub-bing or video-based translation tasks. Recent works have tried to incorporate Text-to-Speech (TTS) tech-nology into the process of TFG through a cascade method.However, its worth noting that the cascade method encoun-ters bottlenecks in terms of both performance and inferencetime . To tackle this issue, the latest study hasdelved into the latent features of TTS to generate keypointsfor talking faces. This exploration provides evidence thatleveraging the latent features of a TTS model is advanta-geous in substituting the latent of an audio encoder for TFG.In this paper, we unify TTS and TFG tasks to generatespeech and talking face videos concurrently. Furthermore,we extend the application of TTS in TFG by conditioningthe target voice with the input identity image.As a re-sult, our model can generate a diverse range of talking facevideos using only a static face image and text as input. Text-to-Speech. Text-to-Speech (TTS) systems aim to gen-erate natural speech from text inputs, evolving from earlyapproaches to recent end-to-end methods .Despite their success, unseen-speaker TTSsystems face a challenge in requiring substantial enroll-ment data for accurate voice reproduction.While priorworks extract speaker representationsfrom speech data, obtaining sufficient high-quality utter-ances is challenging.Recent studies have incorporatedface images for speaker representation , aimingto capture correlations between visual and audio features.However, these models often neglect motion-related factorsin face images, leading to challenges in generating consis-tent desired voices when the input identity remains constantbut the motion varies. In this paper, to tackle this issue, we leverage the motionextractor of TFG to eliminate the motion features from thesource image. The motion-normalised feature is then fedinto the TTS system as a conditioning factor, aiding the TTSmodel in producing consistent voices.",
  ". Method": "In , we propose a unified architecture, named TTSF,which integrates TFG and TTS pipelines. In the TTS model,the text input is embedded as et by an embedding layer.The text encoder Et maps this embedding to the text fea-ture ft Rltd, where lt and d denote the token lengthand hidden dimension, respectively. The duration predic-tor then upsamples ft to ft Rlmd to align with the tar-get mel-spectrograms length lm. The ft is subsequentlypassed into the TTS decoder DT T S to predict the targetmel-spectrogram. Both Et and DT T S are conditioned withthe identity feature fid from the TFG model to incorporatethe characteristics of the target speaker. In the TFG model,the source image Is and driving frames Id Rtchw pass through the shared visual encoder Ev, yielding visualfeatures fs and fd for the source and target, respectively.The motion extractor encodes motion features from the in-put, obtaining the identity feature fid by subtracting the mo-tion feature from fs. The target motion feature is denoted asfm. With the motion fusion module, fid, fm, and the audiomapper output flip are aggregated and then, input into theTFG generator G to generate videos Id with desired mo-tions. To produce variational facial movements during in-ference, we propose a conditional flow matching-based mo-tion sampler. Additionally, we introduce an auto-encoder-based motion normaliser aimed at reducing the noise in thesampled motions. The feature fc, compressed by the nor-maliser, serves as the motion samplers target during train-ing. Consequently, our framework synthesises natural talk-ing faces and speeches from a single portrait image and text",
  ". Baseline for Talking Face Generation": "Motion Extractor. Previous research in the fields of mo-tion transfer and TFG has identifiedthe presence of a reference space that only contains individ-ual identities. Formally, we can express this as Ev(I) =fid + fm, where I is the input image, Ev is the visual en-coder, fid is an identity feature, and fm is a motion fea-ture. In our framework, the motion extractor Em learns thesubtraction of identity feature fid from the visual feature:Em(Ev(I)) = fm = f fid. Our motion extractor followsthe architecture of LIA , featuring a 5-layer MLP andtrainable motion codes under an orthogonality constraint.This constraint facilitates the representation of diverse mo-tions with compact channel sizes. Unlike LIA, which com-putes relative motion between source and target images, ourmotion extractor independently extracts identity and motionfeatures. This distinction is crucial for integrating TFG andTTS models, where the identity feature conditions TTS togenerate consistent voice styles robust to facial motions. Motion Fusion and Generator. To establish a baseline forgenerating both talking faces and speeches, we consider twokey aspects in designing the TFG generator G: (1) mem-ory efficiency and (2) resilience to unseen identity gener-ation. To reflect these, we avoid using an inversion net-work, known for its computational heaviness, and opt fora flow-based generator that focuses on learning coordinatemapping. For our generator, we choose LIAs one, whichemploys a StyleGAN -styled generator as a baseline.However, LIA is explicitly tailored for face-to-face mo-tion transfer and does not account for generating lip move-ments synchronised with an audio source. To apply LIAto TFG, specific considerations are needed. In the trainingprocess, the lack of augmentation for target frames leadsto the model replicating lip motions from the target framesrather than from audio sources. In response to this, inspiredby FC-TFG , we regulate lip motions by incorporatingaudio features into specific n-th layers of the decoder. Thefusion process involves a straightforward linear operation:",
  "fz,n =fid + fmi {non-lip motion layers}fid + flipi {lip motion layers},(1)": "where, fm denotes the target motions extracted from tar-get frames and flip denotes the output of the audio mapper,representing lip motion features. In the end, we generatethe final videos Id by inputting the style feature fz,n intothe TFG generator G. Audio Mapper. Unlike the cascade text-driven TFG, ourframework does not require extracting acoustic features us-ing an audio encoder. Instead, we utilise the intermediaterepresentations of the TTS system, serving a definite pur-",
  ". The architecture of the audio mapper. The conditiondenotes the concatenated feature of text embedding et, upsampledtext feature ft, and energy, which is a norm of ft": "pose: Generating natural lip motion with the TFG genera-tor. This feature is crafted by aggregating the concatenatedfeatures of text embedding et, the upsampled text featureft, and energy which is an average from ft along the chan-nel axis. The text embedding enables the TFG model tograsp phoneme-level lip representation, while the upsam-pled text feature and energy contribute to capturing intri-cate lip shapes aligned with the generated speech sound. Toaggregate these different types of features, we use Multi-Receptive field Fusion (MRF) module . As illustratedin , the MRF module comprises multiple residualblocks, each characterised by 1D convolutions with dis-tinct kernel sizes and dilations.This diverse configura-tion enables the module to observe both fine and coarsedetails in the input along the time axis. To avoid poten-tial artifacts at the boundaries of motion features causedby temporal padding operations, we intentionally removethe padding operation and introduce temporal interpolation.Consequently, our framework achieves well-synchronisedlip movements while effectively capturing the characteris-tics of the generated speech.Training Objectives. We use a non-saturating loss inadversarial training:",
  "i=1 (Id)i (Id)i 2,(3)": "where is a pretrained VGG19 network, and Nf isthe number of feature maps. To preserve facial identity af-ter motion transformation, we apply an identity-based sim-ilarity loss using a pretrained face recognition networkLid = 1 cosEidId, Eid (Id). Finally, To generatewell-synchronised videos according to the input audio con-ditions, We use the modified SyncNet introduced in toenhance our models lip representations. We minimise the",
  ". Variational Motion Sampling": "Preliminary: Conditional Flow Matching. In this sub-section, we present an outline of Optimal-Transport Condi-tional Flow Matching (OT-CFM). Our exposition primaryadheres to the notation and definitions in .Let x Rd be the data sample from the target distribu-tion q(x), and p0(x) be tractable prior distribution. Flowmatching generative models aim to map x0 p0(x) to x1by constructing a probability density path pt : Rd R>0, such that p1(x) approximates q(x). Consider an arbi-trary Ordinary Differential Equation (ODE):",
  "ddtt(x) = vt(t(x)),0(x) = x,(4)": "where the vector field vt : Rd Rd generates theflow t : Rd Rd. This ODE is associated withpt, and it is sufficient to produce realistic data if a neuralnetwork can predict an accurate vector field vt.Suppose there exists the optimal vector field ut that cangenerate accurate pt, then the neural network vt(x; ) canbe trained to estimate the vector field ut. However, in prac-tice, it is non-trivial to find the optimal vector field ut andthe target probability pt. To address this, leverages thefact that estimation of conditional vector field is equivalentto estimation of the unconditional one, i.e.,",
  "(6)": "where is the predicted frame-wise mean of x1 andOTt(x0) = (1 (1 min)t)x0 + tx1 is the flow fromx0 to x1.The target conditional vector field becomeuOTt(OTt(x0)|x1) = x1 (1 min)x0, which enablesthe improved performance with its inherent linearity. In ourwork, we use fixed value of min = 104.Prior Network. The prior serves as the initial conditionfor OT-CFM, facilitating the identification of the optimalpath to x1. During training, our prior network takes the firstmotion fm,0 of target motion sequence fm and the acoustic feature flip as inputs. We structure the prior network witha 4-layer conformer , where the input is formed by thesummation of fm,0 and flip. Note that the first motion isreplaced as the source images motion in inference. OT-CFM Motion Sampler. The objective of our motionsampler is to sample a sequence of natural motion codesfrom the prior . During training, this module aims to pre-dict target motions fm. However, in our experiments, weobserved that directly regressing fm (equivalent to settingx1 as fm) leads to producing shaky motions during infer-ence. We expect that this is due to the characteristics ofthe StyleGAN-styled decoder. Each channel of the decoderplays a semantically meaningful role in generating detailedfacial attributes. Therefore, when the motion sampler failsto successfully estimate the vector field, it directly impactsthe final outcomes. To address this issue, we introduce anauto-encoder-based motion normaliser that compresses fea-ture and reconstructs them into the target motion fm. Thecompressed motion features fc serve as x1 in OT-CFM. Training Objectives.The reconstruction loss for train-ing our motion normaliser is defined as Mean Square Error(MSE) loss between the target motion fm and the recon-structed motion fm as follows: LAE = fm fm 2 .Moreover, as motion decoding commences from randomnoise N(, I) at inference, our objective is to minimise thedistance between the prior and compressed target motionfc. Considering the output of prior network as parameter-ising the input noise for the decoder, it is natural to view theencoder output as a normal distribution N(, I). Follow-ing , we compute a negative log-likelihood prior loss:",
  ". Text-to-Speech Synthesis": "Our TTS system aims to produce well-stylised speech froma single portrait, acquired in in-the-wild setting. In this con-text, we define the in-the-wild environment as follows: (1)The model is exposed to previously unseen facial data, and(2) the facial images exhibit various facial poses.First,since we cannot access to the identity labels to unseenspeakers, we condition our model with image embedding.Second, our emphasis is on the advantages of our frame-work. By integrating TFG and TTS systems, we can utilisethe identity feature fid, a motion-removed feature, obtainedfrom the TFG model. Consequently, our TTS model is ca-pable of generating speeches robust to various facial mo-tions in image, maintaining consistency in the style of voice.Our system is based on Matcha-TTS , an OT-CFM-based TTS model known for synthesising high-quality",
  "(energy & !\"!)": ". Qualitative Results. We compare our method with several baselines listed in . Our approach outperforms all the baselinesin terms of generating natural facial motions, encompassing lip shape and head pose. MakeItTalk and SadTalker exhibit smaller variancein head poses, while Audio2Head fails to preserve the source identity. We emphasis that our TTSF system can generate sophisticated lipshapes, reflecting both linguistic and acoustic information from our TTS model. speeches in a few synthesis steps. We input the identityfeature fid to both encoder and decoder. With this mini-mal variation, our model is trained with prior, duration, andOT-CFM losses, as outlined in . These losses are collec-tively denoted as LT T S. Finally, we convert the generatedmel-spectrogram by using a pretrained vocoder .",
  "challenging examples than LRS3 since many videos areshot outdoors. We randomly select a subset of videos fromeach dataset to evaluate the performance of our framework": "Implementation Details. First of all, we pretrain a Matcha-TTS model on the LRS3 dataset for 2,000 epochs andthen jointly train with the talking face generation model for40 epochs. Our focus is on the manipulation of seven spe-cific layers within the generator, namely layers 1 to 7. Fur-thermore, we exclusively input the audio feature into twospecific layers, namely layers 6 and 7. Our motion sampleris trained with 32-frame videos and then inferences with allframes of each video. Audio data is sampled to 16kHz, andconverted to mel-spectrogram with a window size of 640, ahop length of 160, and 80 mel bins. To update our model,we employ the Adam optimiser with a learning rate setat 1e4. The entire framework is implemented using Py-Torch and is trained across eight 48GB A6000 GPUs. Evaluation Metrics. In our quantitative assessments forTFG, we employ a range of evaluation metrics introducedin previous works. To assess the visual quality of the gen-erated videos, we employ the Frechet Inception Distance(FID) score and ID Similarity (ID-SIM) score using a pre-trained face recognition model . To measure the accu-racy of mouth shapes and lip sync, we utilise the Lip SyncError Confidence (LSE-C), a metric introduced in . Forthe diversity of the generated head motions, we calculate the",
  ". Comparison with the state-of-the-art methods on Vox-Celeb2 in the one-shot setting. The previous audio-driven TFGmodels are cascaded with our TTS model to generate talking facesfrom text": "standard deviation of the head motion feature embeddingsextracted from the generated frames using Hopenet ,following the approach introduced in .For the evaluation of TTS performance, we computeWord Error Rate (WER), Mel Cepstral Distortion (MCD),the cosine similarity (C-SIM) between x-vectors of thetarget and synthesised speech, as well as the Root MeanSquare Error (RMSE) for F0. WER and MCD representthe intelligibility and naturalness of speech, respectively.C-SIM and RMSE measure the voice similarity to the tar-get speaker. For WER, we use a publicly available speechrecognition model of .",
  ". Comparison with State-of-the-art Methods": "Text-driven Talking Face Generation.We compareseveral state-of-the-art methods (MakeitTalk , Au-dio2Head , and SadTalker ) for the text-driven talk-ing head video generations by attaching our TTS modelto the previous audio-driven TFG models in the cascademethod. To simulate a one-shot talking face generation sce-nario, we evaluate the baselines on the in-the-wild datasets,LRS2 and VoxCeleb2. As shown in , the proposedmodel outperforms every audio- and cascade text-drivenmethod in terms of video quality (FID, ID-SIM) on LRS2.Additionally, we present experimental results on the Vox-Celeb2 dataset in . Since this dataset does not con-tain text transcription according to the speech in the video,our framework generates both speech and a talking face",
  ". Quantitative results of synthesised speech. Intel. and Nat.denote intelligibility and naturalness of audio, respectively": "by inputting a single frame from a VoxCeleb2 video anda randomly selected transcription from LRS3. Similar tothe experimental results on LRS2, our framework exhibitssuperior performance in ID-SIM score. On the other hand,the proposed model records a lower synchronisation scorecompared to SadTalker using the LSE-C metric. However,given that the LSE-C metric relies significantly on a pre-trained model, a more useful evaluation of lip synchronisa-tion can be achieved through perceptual judgement by hu-mans, as assessed in user studies. The qualitative assess-ment in .3 shows that our method produces per-ceptually better synchronised output compared to the base-line. Although Audio2Head shows the best diversity score,it records the lowest scores in video quality metrics. Wealso observe that Audio2Head completely fails to generatea natural video when the input source image is not locatedin the centre of the screen. On the other hand, our pro-posed method achieves high scores in both video qualityand diversity metrics. Considering the aforementioned is-sues, our framework demonstrates robust generalisation tounseen data when conducting multimodal synthesis encom-passing both video and speech. Face-stylised Text-to-Speech. To evaluate the generalis-ability of our TTS system, we compare our model to Face-TTS , which is a state-of-the-art method of face-stylisedTTS. For the evaluation, we simulate two scenarios onLRS2 dataset: (1) w/ motion, where the TTS model is condi-tioned with source image embedding, i.e., fid +fm; (2) w/omotion, where the model is conditioned with only identityfeature fid. The results are shown in . While the pro-posed model shows slight deviance in MCD, it clearly out-performs the baseline in WER, C-SIM, and RMSE, demon-strating its superiority in intelligibility and voice similarity.More importantly, when we consider motion features to-gether as our speaker condition, the generation performanceis significantly degraded, especially in voice similarity. Itindicates the benefits of unifying TFG and TTS systems,highlighting the advantages of their integration.",
  ". MOS evaluation results. MOS is presented with 95% con-fidence intervals. Note that the previous audio-driven TFG modelsare cascaded with our TTS model": "ions on 20 videos. Reference images and texts were ran-domly selected from the LRS2 test split to create videosusing MakeItTalk , Audio2Head , SadTalker ,and our proposed method. Mean Opinion Scores (MOS) areused for evaluation, following the approach in .Participants rate each video on a scale from 1 to 5, consid-ering lip sync quality, video realness, and head movementnaturalness. The order of methods within each video clip israndomly shuffled. The results in indicate that ourmethod outperforms existing methods in generating talkingface videos with higher lip synchronisation and natural headmovement. Analysis on Qualitative Results. We visually present ourqualitative results in . MakeItTalk fails to produceprecise lip motions aligned with the synthesised speech,and Audio2Head struggles to preserve identity information.SadTalker can generate well-synchronised lip motions butis limited in facial movements. In contrast, our approachexhibits more dynamic facial movements and can generatevivid lip motions that reflect both linguistic and acoustic in-formation. For instance, it can be seen that our models lipmotions are precisely aligned to the pronunciation of thespeeches (refer to the yellow arrows). The accuracy and thedetails demonstrate that our method can generate realisticand expressive talking faces. The Effectiveness of Identity Features. To verify the ef-fectiveness of identity feature-based conditioning, we visu-alise the feature space of synthesised audio. shows t-SNE plots of x-vectors from Face-TTS and our method.As shown in a, Face-TTS fails to cluster features de-rived from the same speaker. This implies the potential fail-ure to generate the target voice with different styles. In con-trast, as depicted in b, the proposed TTS system ef-fectively clusters features derived from the same speaker de-spite the variety in head motions. This demonstrates that ourmethod is capable of synthesising consistent voices, even inthe presence of varying motions.",
  ". Speaker representation space of (a) Face-TTS and (b)Ours. Each colour represents a different speaker": "audio mapper. w/o (energy & ft) indicates the TFG modelconditioned with text embedding et from the audio mapper.In this case, the TFG model can incorporate only linguisticinformation and it leads to our model failing to generate pre-cise lip motions. When we additionally input the upsampledtext feature ft to our TFG model, the synchronisation scoreimproves significantly. This is because our TTS model isoptimised by reducing the prior loss between ft and thetarget mel-spectrogram. This indicates that the ft featurecontains acoustic information. Finally, when we add theenergy feature to the previous condition, our model exhibitsthe best performance across all metrics. This indicates thatthe energy of speech significantly impacts generation of de-tailed lip movements.",
  ". Conclusion": "Our work introduces a unified text-driven multimodal syn-thesis system that exhibits robust generalisation to unseenidentities. The proposed OT-CFM-based motion sampler,coupled with an auto-encoder-based noise reducer, pro-duces realistic facial poses. Notably, our method excels inpreserving essential speaker characteristics such as prosody,timbre, and accent by effectively removing motion fac-tors from the source image. Our experiments demonstratethe superiority of our proposed method over cascade-basedtalking face generation approaches, underscoring the effec-tiveness of our unified framework in multimodal speechsynthesis.",
  "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-nications of the ACM, 2020. 4": "Shunsuke Goto,Kotaro Onishi,Yuki Saito,KentaroTachibana, and Koichiro Mori. Face2speech: Towards multi-speaker text-to-speech synthesis using an embedding vectorpredicted from a face image. In Proc. Interspeech, 2020. 2,3 Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-dong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. In Proc. In-terspeech, 2020. 5",
  "Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and ZhouZhao. Generspeech: Towards style transfer for generalizableout-of-domain text-to-speech synthesis. In Proc. NeurIPS,2022. 3": "Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu,Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang.Curricularface: Adaptive curriculum learning loss for deepface recognition. In Proc. CVPR, 2020. 6 Geumbyeol Hwang, Sunwon Hong, Seunghyun Lee, Sung-woo Park, and Gyeongsu Chae.Discohead: Audio-and-video-driven talking head generation by disentangled controlof head pose and facial expressions. In Proc. ICASSP, 2023.1, 2 Youngjoon Jang, Kyeongha Rho, Jongbin Woo, HyeongkeunLee, Jihwan Park, Youshin Lim, Byeong-Yeol Kim, andJoon Son Chung. Thats what i said: Fully-controllable talk-ing face generation. In Proc. ACM MM, 2023. 1, 2, 4, 8 Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, JonathanShen, Fei Ren, Patrick Nguyen, Ruoming Pang, IgnacioLopez Moreno, Yonghui Wu, et al. Transfer learning fromspeaker verification to multispeaker text-to-speech synthesis.In Proc. NeurIPS, 2018. 3",
  "Ji-Hyun Lee, Sang-Hoon Lee, Ji-Hoon Kim, and Seong-Whan Lee. PVAE-TTS: Adaptive text-to-speech via progres-sive style adaptation. In Proc. ICASSP, 2022. 3": "Sang-Hoon Lee, Hyun-Wook Yoon, Hyeong-Rae Noh, Ji-Hoon Kim, and Seong-Whan Lee. Multi-spectroGAN: High-diversity and high-fidelity spectrogram generation with ad-versarial style combination for speech synthesis. In Proc.AAAI, 2021. 3 Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, ZhibinHong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding,and Jingdong Wang. Expressive talking head generation withgranular audio-visual control. In Proc. CVPR, 2022. 1, 2, 8"
}