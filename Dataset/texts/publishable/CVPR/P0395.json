{
  "Abstract": "Adversarial patch attacks present a significant threat toreal-world object detectors due to their practical feasibil-ity.Existing defense methods, which rely on attack dataor prior knowledge, struggle to effectively address a widerange of adversarial patches. In this paper, we show twoinherent characteristics of adversarial patches, semantic in-dependence and spatial heterogeneity, independent of theirappearance, shape, size, quantity, and location.Seman-tic independence indicates that adversarial patches oper-ate autonomously within their semantic context, while spa-tial heterogeneity manifests as distinct image quality of thepatch area that differs from original clean image due to theindependent generation process. Based on these observa-tions, we propose PAD, a novel adversarial patch localiza-tion and removal method that does not require prior knowl-edge or additional training. PAD offers patch-agnostic de-fense against various adversarial patches, compatible withany pre-trained object detectors. Our comprehensive digitaland physical experiments involving diverse patch types, suchas localized noise, printable, and naturalistic patches, ex-hibit notable improvements over state-of-the-art works. Ourcode is available at",
  ". Introduction": "Adversarial attacks substantially challenge the security ofobject detectors, leading to potentially severe consequencesin various fields (e.g., autonomous driving). Traditional ad-versarial attacks typically involve adding perturbations tothe entire image. However, modifying every pixel is unre-alistic in real-world attack scenarios. Adversarial patch at-tacks, on the other hand, focus on introducing disturbancesin a limited area. Their practical feasibility makes them oneof the most threatening forms of adversarial attacks.Defenses against adversarial patch attacks on object de-tectors can be broadly categorized into three main types: i)modifying or intervening within detection models , ii) locating and eliminating adversarial patch regionsin images, and iii) certifiably robust de-",
  "Clean image(a) Denoising-based defense": ". When attacked by natural-looking adversarial patches,(a) locates high-frequency areas, and eliminates edge lines insteadof patches ; (b) fails to detect the existence of patches since nosuch patch data in the training set ; (c) produces a heat mapwhere the patch area and background are difficult to distinguish,failing to locate the patches . Our proposed PAD achieves ac-curate patch location and removal. fenses . Among these, methods falling under the sec-ond category, which act as preprocessing, offer the broadestrange of applications.Researchers have explored various patch localizationmethods to effectively remove adversarial patches from im-ages. Denoising-based defenses smooth out noise-likeregions in images, providing an effective defense againstearly localized noise patches. However, they fail to addressnatural-looking patches. External segmenter-based defenses train an adversarial patch segmentation model forpatch localization, using adversarial images generated byexisting attack techniques. However, the reliance on train-ing data makes them ineffective against unseen patch types.Entropy-based defenses achieve patch localizationby identifying high entropy kernels and patch shape recon-struction.Nevertheless, the entropy threshold setting re-quires prior knowledge of the distribution for clean data andpatches, and shape reconstruction relies on training data,posing challenges in practical applications. Despite progressin certain aspects, these methods face a common challengeof locating various adversarial patches without relying onprior attack knowledge. As shown in , these threecategories of methods fail to effectively remove patch areas.In this paper, we propose a new approach for various ad-versarial patch localization without relying on prior attack",
  "arXiv:2404.16452v1 [cs.CV] 25 Apr 2024": "knowledge (e.g., appearance, shape, size, or quantity). Theproposed approach is derived from two inherent characteris-tics of adversarial patches: semantic independence and spa-tial heterogeneity.Semantic independence implies zero information gainfrom the surrounding semantic space, while spatial hetero-geneity refers to inconsistent image quality in the samespace introduced by adversarial patches. In both digital andphysical attacks, the adversarial patch, added as a separatecomponent to the image or environment, is semantically in-dependent in the image. The surroundings provide no in-formation about the content of adversarial patches, and viceversa. Additionally, different imaging devices, generationprocesses, and compression methods may lead to variationsin image quality. With a source different from the originalclean data, the quality of patch regions exhibits heterogene-ity compared to other areas in space.Based on these observations, we propose to identifypatch areas by quantifying local semantic independence andspatial heterogeneity. We measure the information gain be-tween adjacent regions based on mutual information, andevaluate residuals from recompression at different qualityfactors to address the unequal impact upon areas of vary-ing quality. While complex backgrounds confuse entropy-based methods since the high information densitycaused by complicated textures, our method exhibits morerobustness as semantic correlations remain between adjacentbackground areas. In addition, to eliminate the reliance ontraining data, we present a patch localization and removalpipeline that requires no prior knowledge or additional train-ing. Different from current works, our defense accuratelyidentifies the patch region mask without any reference to ex-isting adversarial images and imposes no limitations on thequantity or proportion of patches in the image.Our contributions can be summarized as follows: We reveal two inherent characteristics of adversarialpatches, semantic independence and spatial heterogeneity,and propose patch locating based on mutual informationand recompression, which is agnostic to patch appearance,shape, size, location, and quantity. We propose a patch-agnostic defense (PAD) method foradversarial patch localization and removal, which requiresno prior attack knowledge or additional training and iscompatible with any object detector. We conduct experiments on adversarial patches with dif-ferent appearances, shapes, sizes, locations, and quanti-ties, evaluating the defense effectiveness of PAD in bothdigital and physical scenes. Experimental results demon-strate our superior defense performance compared to thecurrent state-of-the-art methods.",
  "The concept of adversarial patch attacks is first intro-duced by . They develop a generic patch capable of de-": "ceiving image classifiers and demonstrate the feasibility ofphysical attacks by attaching the patch in real-world scenar-ios. Early research on adversarial patch attacks primarilyfocuses on localized noise . DPatch is the pio-neering work on patch attacks specifically designed for ob-ject detection, targeting both bounding box regression andobject classification components of the detection system.PatchAttack proposes a reinforcement learning-basedattack method to induce misclassification by superimposingsmall texture patches on the input image. Some work focuson physical attacks . and attachpatches to traffic signs, leading to the misidentification ofthose signs. proposes a printable adversarial patch forpedestrian detection, introducing non-printable loss in theoptimization process. and explore the integrationof adversarial patches into wearable clothing. , and train generative adversarial networks (GANs) to gener-ate natural-looking patches that match the visual propertiesof normal images.",
  ". Defenses against Patch Attacks": "Adversarial training , whichenhances model robustness by adding adversarial examplesduring training, is one of the most popular and effective de-fenses against digital attacks. However, such methods arenot suitable for defending pre-trained models already in useand require significant resources for retraining when new at-tacks emerge, making them not so practical.Some defense methods involve modification of specificmodels . investigates the use of spatialcontext constraints in YOLOv2 to enhance defense ro-bustness against adversarial patches. introduces a patchclass into YOLOv2, enabling the detection of objects of in-terest as well as adversarial patches. proposes adver-sarial patch feature energy(APE), and defense is achievedby incorporating an APE discovery and suppression mod-ule into the network. Although good defense effects can beachieved on specific detection models, they cannot directlyprovide defense for a wide range of object detectors.To provide more general defense, researchers have ex-plored locating patch areas in images and eliminating theireffects . Since early adversar-ial patches are usually in the form of localized noise, somedefense methods focus on reducing the impact of noise-likeareas in input images.LGS observes that patch at-tacks introduce concentrated high-frequency noise and pro-poses gradient smoothing for regions with significant gradi-ents. APM and SAC train external segmentationnetworks to locate noise-like regions. While these meth-ods effectively defend against localized noise-based patchattacks, they struggle to counter the new types of natural-looking patches. DW and Jujutsu utilize saliencymaps to identify patch areas and cover them to mitigate theirimpact on classification. In object detection tasks, whichinvolve bounding box regression in addition to classifica-tion, accurately localizing patches becomes challenging us-",
  "inpaint": ". Overview of our proposed PAD. Semantic Independence Localization and Spatial Heterogeneity Localization find patch regionsfrom two views, generating heat maps feeding into Fusion Block. The patch localization map output by Fusion Block is then matched withall masks from SAM, getting more accurate patch boundaries. Feeding the defended image into object detectors for robust prediction. ing saliency maps. Jedi and use entropy to locatepatch areas, but prior knowledge of entropy distribution ofclean dataset and patch area is required.In recent years, some researchers have proposed certi-fiably robust defenses against adversarial patches .DetectorGuard is an attack detection defense that raisesan alert when an attack is detected without removing the ad-versarial patches, resulting in a loss of model functionalityduring an attack. ObjectSeeker requires ensuring that,under at least one partition, the remaining images do notcontain any adversarial patch pixels. As a result, there willbe trouble handling attack scenarios with large patch pro-portions or multiple patches.Different from these methods, PAD is derived from twogeneral characteristics of patches that are independent oftheir appearance, shape, size, quantity, and location, allow-ing us to eliminate various patches without relying on priorattack knowledge.",
  ". Preliminaries": "Differing from traditional adversarial attacks, adversar-ial patch attacks impose restrictions on the attacker, limitingthe area where perturbations can be introduced. Within thisconstraint, the attacker has the flexibility to manipulate thepixels within the designated patch region.We denote a clean image with dimensions w h c asX Rwhc. The adversarial patch, denoted as Padv, cantake any shape. The generation of Padv is typically con-trolled by the loss function Lpatch, which varies dependingon the specific attack objective. Since our goal is to defendagainst various patch attacks, irrespective of their intentionto conceal the target object, misclassify it, or generate falsedetections of non-existent objects, we do not make assump-tions about Lpatch.The resulting adversarial image Xadv Rwhc, pro-",
  "Xadv = MpatchA(Padv, X, l, t)+(1Mpatch)X, (1)": "where Mpatch {0, 1}wh represents the patch region inimage X, with elements set to 1 within Padv and 0 else-where. A(Padv, X, l, t) denotes the patch application func-tion, incorporating patch transformations such as scalingand rotation denoted by t, and patch location denoted byl. refers to element-wise multiplication. In the case ofattack methods that can be used physically, A(Padv, X, l, t)typically involves completely replacing the image area at po-sition l with the transformed patch.",
  ". Defense Pipeline": "In this section, we introduce the pipeline of PAD, asshown in . Firstly, we analyze the input image usingthe two inherent characteristics that all adversarial patchespossess to obtain heat maps, Hmi and Hcd, which highlightthe regions in the image that exhibit semantic independenceand spatial heterogeneity, respectively. Next, we employ theFusion block to merge Hmi and Hcd, generating the patchlocalization map Hp that accurately reflects the areas in theimage that possess both of these characteristics.To make the patch masks more accurate, we introduce theSegment Anything Model (SAM) . Different from thesegmentation models introduced in prior methods, since wedo not need it to have recognition capabilities for adversarialpatches, SAM can be replaced with any pre-trained segmen-tation models with similar capabilities, without additionaltraining. In other words, we do not rely on known patchattack methods to generate adversarial images for training,preventing our defense from losing effectiveness when en-countering new attack methods. With SAMs zero-shot seg-mentation capability, we segment the edges of all regions in the image and obtain masks for each region. We then matcheach mask with Hp and consider all masks with Intersectionover Area (IoA) greater than threshold tm as the final patchmasks. The calculation of IoA can be stated as follows:",
  "area(mask).(2)": "If the localization of adversarial patches is accurateenough, the removal process only needs to be able to elim-inate the impact of the patches. Therefore, we employ asimple and fast inpainting method that is commonly used inprevious works : filling the patch area with all blackpixels. We also compare the coherence transport-based in-painting method with all-black, more details can befound in supplementary material.",
  ". Semantic Independence Localization": "Semantic independence evaluation. The semantic inde-pendence of a region can be measured by its semantic cor-relation with surrounding regions. The smaller the semanticcorrelation, the stronger the independence. For two adjacentregions, A and B, their semantic correlation can be definedas the information gain they provide to each other. Givenknowledge about region A, it quantifies how much infor-mation can be inferred about region B, i.e., the reductionin uncertainty about B given knowledge of A. This can beexpressed using mutual information:",
  "I(A; B) = H(A) H(A|B) = H(B) H(B|A)(3)": "where H() represents information entropy, and H(|)represents conditional entropy.Semantic independence localization based on mutual in-formation.Building on the analysis above, we performadversarial patch discovery by computing the semantic in-dependence of local regions across the entire image. Weset up a sliding window, and calculate the mutual informa-tion between each window and its four neighboring win-dows (up, down, left, right). The average value of thesemutual information scores is used as the heat value forthe current window, generating a heat map for the entireimage. We use Wcur to denote the current window, andWup, Wdown, Wleft, Wright represent the four neighbor-ing windows of the same size respectively. For each i in{up, down, left, right}, the mutual information betweenWi and Wcur can be expressed as follows:",
  "(5)": "where(xcur, ycur) denotes the coordinates of the upper leftcorner of the window Wcur, Hmi denotes the heat map gen-erated by Semantic Independence Localization module, ddenotes the size of the sliding window, and n denotes thenumber of neighboring windows. n equals 4 for most win-dows, 2 or 3 for windows located at the edge of the image.",
  ". Spatial Heterogeneity Localization": "Impact of compression on image quality. Image compres-sion leverages the insensitivity to certain components of hu-man eyes to reduce storage space. Taking the most com-monly used JPEG compression as an example, after colorspace transformation, it undergoes block-based Discrete Co-sine Transform (DCT) to the image, converting the spatialdomain into the frequency domain. The transformed low-frequency components have larger values, mainly concen-trated in the upper-left corner, while high-frequency com-ponents have smaller values, distributed in lower-right re-gions. Subsequently, the DCT coefficients are quantized sothat smaller coefficients close to 0 completely become 0, andnon-zero values also generate a large number of repetitions,thereby reducing the coding length. Using F(x, y, i) to rep-resent the DCT coefficient of channel i at location (x, y), thequantization process can be expressed as:",
  "Q(x, y, i)),(6)": "where Q(x, y, i) represents the corresponding quantizationstep size. A larger Q leads to greater quantization loss andpoorer image quality.Spatial heterogeneity localization through recompres-sion. For an image containing regions of varying quality,compressing the entire image will affect each quality regiondifferently, providing valuable clues for identifying abnor-mal regions in the image . Inspired by this,we locate adversarial patches based on the quality differ-ences during recompression. For a clean image with qualityfactor Qc, the patch area with quality factor Qp, we set dif-ferent quality factors Qr to re-compress the attacked imageXadv, and calculate the squared difference of pixel valuesbefore and after re-compression, as follows:",
  "i=1[f(x, y, i) fQr(x, y, i)]2 ,(7)": "where c denotes the number of channels. When Qr is closeto Qp, the D values of the patch region are minimized.When Qr is close to Qc, the D values of the uncovered re-gion are minimized. To enhance the robustness to texturevariations in the image, we apply convolutional smoothingand perform normalization.",
  "and 4.3 aim to identify potential adversar-ial patch regions from the perspectives of semantic inde-pendence and spatial heterogeneity.In the fusion block,": "we merge the heat maps obtained from these two methods.Additionally, to mitigate the influence of cluttered back-grounds, we apply adaptive thresholding and morphologicaloperations to further process the fused results.Heatmap fusion. Given the local mutual information heatmap Hmi and the recompression difference heat map Hcdwhich have different value ranges, we first normalize themindividually to scale the values into the range . Thenormalization process can be expressed as follows:",
  "Hfuse(x, y) = rmi Hmi(x, y) + (1 rmi) Hcd(x, y),": "(10)where rmi denotes the weight of mutual informationheat map.Adaptive thresholding. Since the heat values are signifi-cantly affected by the image content, using a static thresholdmay filter out adversarial patches or incorrectly treat otherregions as adversarial patches, thus degrading the perfor-mance of the model. Therefore, we automatically set anadaptive threshold based on the distribution of the heat mapfor each image, and then set elements in Hfuse that are be-low the threshold to 0. The threshold here can be expressedas follows:",
  "i = (n 1) p , j = (n 1) p i,(12)": "where p is a fixed hyperparameter, n represents the numberof elements in Hfuse, and Sort(Hfuse) represents sortedHfuse in ascending order based on the heat values.Morphological operations. To eliminate the interferenceof background with high heat values but unrelated to adver-sarial patches, we apply an OPEN-CLOSE-OPEN operationto the heat map after thresholding. The opening operationinvolves erosion followed by dilation and is mainly used toremove isolated small dots and bridges between different re-gions in the heat map. The closing operation involves dila-tion followed by erosion and is mainly used to fill in a fewconcave regions in the patch area that were filtered out bythe threshold. The kernel size for the opening and closingoperations is adaptively selected based on the image size,more details can be found in the supplementary material.",
  "Target object detectors and dataset. In our experiments,we use Faster R-CNN with a ResNet-50 backbone,": "YOLOv2 , YOLOv3 , YOLOv5s and YOLOv8n as our target object detectors. All models are pre-trainedon MS COCO . Since most existing adversarial patchattacks that can be used physically are developed for pedes-trian detectors , we mainly focus on the INRIA Per-son dataset which consists of 614 person detection im-ages for training and 288 for testing. Only test images areadopted since there is no training part in PAD. Experimentson other datasets can be found in supplementary material.Adversarial patch attacks. To evaluate the defense effec-tiveness of PAD against different types of patches, we em-ploy 11 distinct patches generated by DPatch , YOLOadversarial patch , and Naturalistic Patch , cover-ing localized noise, printable, and natural-looking patches.DPatch generates a specific-sized patch (75 75 and 100 100 in our experiments) located in the upper left corner ofeach image, using 200 iterations with a learning rate of 0.01.The YOLO adversarial patch (P1-P6) and Naturalistic Patch(OBJ, OBJ-CLS, and Upper) generate multiple patches ofvarying sizes and positions based on the detectable pedes-trians in the image. We also conduct defense experimentsagainst two more attacks , the relevant results canbe found in the supplementary material.Implementation details. Throughout our experiments, weused fixed hyperparameter values for different patch typeswithout any adjustments. We set rmi to 0.5 in Heatmapfusion, which assigns equal weights to Semantic Indepen-dence Localization and Spatial Heterogeneity Localization.The value of p in Adaptive thresholding is set to 0.8. TheIoA threshold tm for mask matching is set to 0.5.We compare PAD with four state-of-the-art adversarialpatch defenses: LGS , SAC , Jedi , and Ob-jectSeeker , corresponding to denoising-based, externalsegmenter-based, entropy-based and certifiably robust de-fenses respectively. For LGS, we set the block size to 30,overlap to 5, threshold to 0.1, and smoothing factor to 2.3.For Jedi, due to the reliance on the prior entropy distribu-tion values of the clean dataset and the patch region, usingdefault parameters in code is less effective, we perform pa-rameter tuning for some patches.",
  ". Overall Defense Performance": "In object detection tasks, Average Precision (AP) is awidely used evaluation metric that assesses the area underthe Precision-Recall Curve, representing the overall perfor-mance of a model.Therefore, we utilize mean AveragePrecision (mAP) at Intersection over Union (IoU) 0.5 todemonstrate the effectiveness of the attacks and defenses.We conduct experiments on different detectors, attacks, anddefenses mentioned above and report the results in .Due to space limitations, it only shows results on Faster R-CNN, YOLOv3, and YOLOv5s. Results on YOLOv2 andYOLOv8n can be found in the supplementary material.The results demonstrate that PAD achieves the best de-fense performance against various adversarial patch attackson different detectors. For natural-looking patches (P1-P6)",
  "PAD (Ours)96.1793.9793.0384.0183.6284.5442.0158.3869.8778.9767.3161.08": ", which are more challenging to detect by both humansand machines, the mAP increases by more than 10% on av-erage (absolute) compared to the suboptimal method.From the experimental results, it can be observed thatObjectSeeker performs poorly under these attacks,some even worse than undefended.This is because Ob-jectSeeker can only defend against hiding attacks, andthe assumption does not hold when encountering multiplepatches. SAC is best at defending against localizednoise patches since its segmenter is trained on noise-likepatch data. However, its performance significantly dropswhen facing natural-looking patches, with almost no de-fense capabilities against some of the patches. The perfor-mance of Jedi is unstable, due to the influence of non-patch high-entropy regions. In contrast, PAD demonstratesrobustness against various patches, benefiting from the uni-versality of semantic independence and spatial heterogene-ity and the complete independence from prior knowledge ofattacks.We also report the mAP of clean samples after defense in. PAD achieves a similarly high clean performance asthe vanilla object detectors (0.02% drop on Faster R-CNN,0.34% drop on YOLOv3, and 0.45% rise on YOLOv5s). Forclean images without patches, although areas with relativelyhigh values may remain after heat map threshold processing,they are usually scattered and will be eroded during subse-quent morphological operations, thus not significantly im-pacting the models performance.",
  "M.(14)": "Since there is no patch localization process in certifiablyrobust defense, and the localization results of LGS arenot continuous regions, we primarily compare PAD withSAC and Jedi . The results are presented in Ta-ble 2, showing a significant improvement ( 30%-55% abso-lute and 2-3x relative) over existing state-of-the-art works.In the case of YOLO adversarial patch and NaturalisticPatch , the ground truth masks include small regions thatcan be easily mistaken for the background. This is becausethe attack involves covering almost every visible pedestrianin the image with a patch, including small individuals in the",
  ". Visualization examples illustrating the patch localization process of PAD across different adversarial patch types": "distance. As a result, achieving high Patch Localization Re-call values becomes more challenging. However, PAD stilldemonstrates effective performance against these attacks.SAC exhibits good performance for DPatch ,as its segmenter is trained on adversarial images generatedwith PGD , which falls under the category of local-ized noise. However, it struggles when faced with natural-looking patches that it has not encountered before, leadingto almost zero Patch Localization Recall. Jedi performspoorly on DPatch-75, which may caused by the difficulty inthe prior entropy distribution values adjustment. In contrast,PAD achieves high Patch Localization Recall for differenttypes of patches, as it does not rely on prior knowledge orexisting attack data.According to the definition of Patch Localization Recall,It is natural to think that a defense method can achieve a highPatch Localization Recall by generating a mask with thewidest possible coverage. However, removing a large num-ber of non-patch areas from the image will inevitably resultin a decrease in detection mAP. PAD achieves the highestvalues in both Patch Localization Recall and detection mAP,showcasing superior defense performance. We provide vi-sualization of the patch localization process in .",
  ". Ablation Study": "To investigate the individual impacts of semantic in-dependence and spatial heterogeneity in PAD, we con-duct an ablation study that involves using only the Hmifrom Semantic Independence Localization and only the Hcdfrom Spatial Heterogeneity Localization. Partial results onYOLOv8n are presented in . It can be observed thatthe full method, which combines semantic independenceand spatial heterogeneity, achieves more stable overall per-formance.Additionally, we have observed that spatial heterogeneity",
  "LGS47.582.053.179.462.4SAC81.958.151.878.253.5Jedi57.684.666.965.964.2PAD-MI only78.686.776.377.474.0PAD-CD only86.389.876.185.482.2PAD-all87.588.978.785.481.5": "tends to outperform semantic independence in digital attackexperiments. In digital attacks, patch generation is com-pletely independent of the original clean image and less af-fected by interference, resulting in more pronounced hetero-geneity, leading to better performance. However, in physi-cal attacks where the patch is physically printed and imagedalongside other parts of the scene, the manifestation of het-erogeneity may become weaker, and the role of semanticindependence becomes more significant. In such cases, se-mantic independence outperforms spatial heterogeneity.In this paper, to validate the defense performance of PADwithout any parameter tuning, equal weights are assignedto Semantic Independence Localization and Spatial Hetero-geneity Localization. By adjusting the weight allocation fordigital attacks and physical attacks respectively, PAD canachieve even better results.",
  ". Evaluation on APRICOT": "APRICOT consists of 1,011 photos with high res-olution captured in real-world environments, encompass-ing both indoor and outdoor scenes. Each photo containsa printed physical adversarial patch, which varies in size,shape, location, viewing angle, and lighting conditions.These patches are generated to cause false detection of non-existent objects, targeting 10 specific classes.We use Faster R-CNN model pretrained on MSCOCO as our target object detector and evaluate thedefense performance on the development set. Since this isa targeted attack, we use the Attack Success Rate (ASR) asour evaluation metric, setting the IoU threshold to 0.10 andthe confidence threshold to 0.30. We present the results afterapplying different defense methods in .The results show that PAD, without any prior attackknowledge or training data of adversarial patches, signifi-cantly reduces the attack success rate to 2.27%. Moreover,SAC utilizes APRICOT data with accurate masks totrain its patch segmenter, getting an attack success rate ofonly 0.1% lower than PAD, highlighting the superiority ofPAD. We provide examples of defended images producedby different defense methods in , demonstrating our",
  ". Evaluation on physical attack videos": "To further evaluate the effectiveness of PAD against awider range of patch types in the physical world, we printnine different patches, including P1-P6 , OBJ, OBJ-CLS, and Upper , and capture videos in five differentindoor and outdoor scenes while holding these patches.Due to the significant impact of lighting, distance, andangles on the success rate of physical attacks, we con-duct extensive practical filming and testing to select a sub-set comprising images with relatively higher attack successrates. The final defense test set consists of 1100 photos,more details about the data distribution can be found in thesupplementary material.We use YOLOv8n as the target object detector and com-pare the defense performance of PAD with Jedi andSAC on this test set.The PR curves in a",
  ". Conclusion": "In this paper, we identify two inherent characteristicsof adversarial patches that are independent of their appear-ance, shape, size, location, and quantity. Leveraging thesecharacteristics, we propose a patch-agnostic defense (PAD)method, which perform adversarial patch localization andremoval without prior attack knowledge. PAD offers patch-agnostic defense against a wide range of adversarial patches,significantly enhancing the robustness of various pre-trainedobject detectors. Without training, PAD eliminates the re-liance on existing attack data, making it more adaptable andcapable of defending against novel patch attacks that havenot been encountered yet. Our experimental results demon-strate the effectiveness in both digital space and the physicalworld, highlighting the practicality of PAD across differentattack scenarios.",
  "Tom B Brown, Dandelion Mane, Aurko Roy, Martn Abadi,and Justin Gilmer.Adversarial patch.arXiv preprintarXiv:1712.09665, 2017. 2": "Niklas Bunzel, Ashim Siwakoti, and Gerrit Klause. Adver-sarial patch detection and mitigation by detecting high en-tropy regions. In IEEE/IFIP International Conference on De-pendable Systems and Networks Workshops, pages 124128,2023. 1, 2, 3 Shang-Tse Chen,Cory Cornelius,Jason Martin,andDuen Horng Chau. Shapeshifter: Robust physical adversar-ial attack on faster r-cnn object detector. In European Con-ference on Machine Learning and Knowledge Discovery inDatabases, pages 5268, 2019. 2",
  "Chong Xiang,Saeed Mahloujifar,and Prateek Mittal.{PatchCleanser}: Certifiably robust defense against adver-sarial patches for any image classifier. In USENIX Security22, pages 20652082, 2022": "Chong Xiang, Alexander Valtchanov, Saeed Mahloujifar, andPrateek Mittal. Objectseeker: Certifiably robust object de-tection against patch hiding attacks via patch-agnostic mask-ing. In IEEE Symposium on Security and Privacy (SP), pages13291347, 2023. 1, 3, 5, 6 Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, MengshuSun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, and Xue Lin.Adversarial t-shirt! evading person detectors in a physicalworld. In ECCV, pages 665681, 2020. 2"
}