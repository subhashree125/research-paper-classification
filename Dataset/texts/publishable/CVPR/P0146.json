{
  "Abstract": "Extracting planes from a 3D scene is useful for down-stream tasks in robotics and augmented reality.In thispaper we tackle the problem of estimating the planar sur-faces in a scene from posed images. Our first finding isthat a surprisingly competitive baseline results from com-bining popular clustering algorithms with recent improve-ments in 3D geometry estimation. However, such purelygeometric methods are understandably oblivious to planesemantics, which are crucial to discerning distinct planes.To overcome this limitation, we propose a method that pre-dicts multi-view consistent plane embeddings that comple-ment geometry when clustering points into planes. We showthrough extensive evaluation on the ScanNetV2 dataset thatour new method outperforms existing approaches and ourstrong geometric baseline for the task of plane estimation.",
  ". Introduction": "While only parts of the real world are perfectly planar, a3D reconstruction made out of planes is a useful parame-terization for many downstream tasks. A planar scene re-construction is a common representation for applications inrobotics , path planning , and augmented reality(AR) . For example, both ARKit and ARCore ,two of the most used AR platforms, provide 3D plane esti-mation from scenes as part of their frameworks.Broadly, there are two families of approaches for 3Dplane extraction from images: geometric versus learning-based methods. Geometry-based pipelines assume accessto a point cloud or mesh of the scene, e.g., as estimatedfrom multi-view stereo or LIDAR. This geometry is thenpartitioned into planes using geometric cues, e.g., usingRANSAC . The disadvantages of these approaches arethat they can be sensitive to noisy data and they do not typ-ically encode learned priors to facilitate robust plane esti-mation. In contrast, learning-based methods make use ofsupervised data to develop models that can predict planeparameters from raw images. Many prior works have fo-",
  "+ poses": ". We create planar scene representations using onlyposed RGB images as input. Existing systems can predict per-pixel planar embeddings for each image, but these are not 3D con-sistent. We learn a per-scene function which maps points on thesame plane to nearby positions in an embedding space. Clusteringthese embeddings, using strong geometrical priors, gives accurateplanar reconstructions. cused on the task of extracting planes from single input im-ages . In practice though, it is morecommon to have a sequence of input images of the scene ofinterest, e.g., in AR applications where the user is interact-ing with new parts of the scene in real-time. There is how-ever, only limited work that extends these learning-basedsingle image methods to the multi-image setting .Inspired by recent work in interactive labeling , wepropose an alternative approach to discovering planes in3D. We train a small MLP network for each scene, whichmaps any 3D location in that scene to an embedding vec-",
  "arXiv:2406.08960v1 [cs.CV] 13 Jun 2024": "tor.Using various 2D and 3D cues, we train the MLPto produce embeddings which are 3D consistent and canbe easily clustered to uncover distinct and accurate planarregions. By exploiting learned cues when decomposing ascene into planes, our method can adapt to different defini-tions of what constitutes a plane. This is important becausethe concept of what counts as a plane is application depen-dent. For example, a painting on the wall can be consideredeither a distinct plane or part of the wall plane, dependingon the application. Unlike purely geometric definitions, ourmethod learns what is considered a plane based on what isencoded in the training data.Our core contribution is a new method that estimates3D-consistent plane embeddings from a sequence of posedRGB images, and then groups them into planar instances.We demonstrate via extensive evaluation that our methodis more accurate than recent end-to-end learning-based ap-proaches, and can run at interactive speeds. We also makea surprising observation by proposing an additional stronggeometry plus RANSAC baseline.It can achieve im-pressive accuracy, outperforming existing baselines, rank-ing second place behind our proposed method.",
  ". Related work": "Planes from single images. Although estimating 3D planesfrom single images is an ill-posed problem, multiple deeplearning solutions have been proposed.Top-down ap-proaches directly predict a mask and the param-eters of each plane. In contrast, bottom-up approaches first map pixels into embeddings, which can subsequentlybe clustered into planes (e.g., via clustering methods ).More recent works leverage the query learningmechanism of Vision Transformers to achieve state-of-the-art single-image results. These methods process framesindependently, so are unable to produce temporally and 3D-consistent planes.As a result, they would require non-trivial plane tracking mechanisms to match the same planeacross different frames over time. In contrast, we leveragemulti-view image sequences, which enables planes to be es-timated in 3D rather than just from single images.We note that some works use planarity assumptions toregularize depth maps or to improve 3Dscenes and poses . In contrast, our aim is to finda high quality planar decomposition of the scene, rather thanto use planarity for regularization in downstream tasks.Planes from 3D and multi-view images. The extractionof geometric primitives, such as planes, from 3D pointclouds is an established problem . RANSAC andthe Hough transform are popular strategies to help fitplanes, and other 3D shapes , to 3D data.While a small number of works start from multi-viewstereo estimated point clouds , the vast majority ofplane extraction methods assume access to higher-quality 3D LIDAR scans . These methods canbe slow, i.e., not suitable for real-time AR applications, andthey cannot easily cope with non-trivial amounts of noise inthe input point clouds. To address noise, existing methodshave attempted to enforce simple to define priors during re-construction such as a Manhattan-world assumption ,object/scene symmetry , or via user interaction .Methods that only use geometry are fundamentally limitedby the quality of the 3D information provided to them. Incontrast, learning-based methods can learn to compensatefor such issues and can also generate planar decompositionsthat better align with the semantic content of the scene.Learning-based methods have been proposed for esti-mating planes from a limited number of input images . However, extending these methods to entire videosis not trivial. Most related to us, PlanarRecon is oneof the first learning-based methods to predict a planar rep-resentation of entire 3D scenes.They incrementally de-tect and reconstruct 3D planes from posed RGB sequences,where 3D planes are detected in video fragments before thefragments are fused into a consistent planar reconstruction.The pipeline is somewhat complex and contains expensiveoperations such as 3D convolutions, recurrent units, and dif-ferentiable matching. In contrast, we trade such complex-ity for an efficient non-plane-based 3D scene reconstruc-tion method , which provides reliable scene geometryestimates suitable for input to our plane estimation method.Finally, room layout estimation can also leverage multipleimages . However, their extreme scene simplifi-cation is only suitable for a limited number of applications.2D and 3D segmentation. Our task of dividing a 3D sceneinto planes has some similarities with 3D semantic orpanoptic instance segmentation. These methodsare less applicable to our problem because they aim to seg-ment objects or semantic regions without special regard forgeometric properties. Recent works have leveraged NeRFsto obtain a consistent semantic or panoptic scene representation. Our method follows this direction byalso using test time optimization. However, unlike weuse an online rather than offline reconstruction method, anddo not need to perform linear assignment for every frame.Scene-level embeddings. Our key innovation is to use per-scene 3D embeddings to represent planes. We are inspiredby previous works, e.g., iMap and iLabel , whoshowed how emergent embeddings can be used for interac-tive reconstruction and labeling. We are inspired by theseworks, but instead of encoding scene geometry or semanticlabels, we encode plane embeddings trained to be multi-view consistent. Related, there are works that optimize 3Dembeddings from 2D supervision, e.g., to ground 2D vision-language features in 3D . However, un-like our reconstruction focus, their aim is to ground open-vocabulary semantic queries in 3D. Representations for 3D reconstruction. The focus of ourwork is planar scene representations, but there are manyalternatives to planes. For example, TSDFs encode shapevolumetrically. They can be generated by estimating depth,e.g., from multi-view stereo , or directlyvia more expensive 3D convolutions .Subsequentmethods have extended neural TSDF estimation tothe online setting. Implicit functions are an alternative rep-resentation which have been used to map from 3D spaceto occupancy . In the context of SLAM,implicit neural strategies have been developed that are able to encode scene geometry using a multi-layerperceptron (MLP). Finally, further from our task, the recentsuccess of NeRFs for realistic novel view synthesis haspaved the way for methods that apply volume rendering torepresent a scene using a neural network .",
  ". Method": "We take as input a sequence of color images, each associ-ated with a known camera pose. We aim to predict a repre-sentation of the imaged 3D scene, where surfaces are seg-mented into constituent planes. We follow the definition ofplanes from previous work where there can besemantic separation between parts, e.g., nearby table-topsshould each have a different plane and a closed door shouldhave a different plane to the wall enclosing it.Our approach estimates planes by first reconstructing the3D geometry of the scene using a mesh representation. Wethen train a network that maps each point on the mesh toa 3D-consistent embedding space, such that points on thesame plane map to nearby places in the space. These em-beddings implicitly encode semantic instance informationand geometric cues.Semantics complement the 3D ge-ometry information provided by the mesh computed viaa lightweight multi-view stereo system.We then use aclustering algorithm on the geometry and embeddings tocompute accurate plane assignments. All the steps in ourmethod support online inference. An overview of our ap-proach is shown in .",
  ". Learning 3D planar embeddings": "Our key innovation is to learn a mapping from each 3Dpoint p in a reconstructed scene to an embedding ep, suchthat points on the same plane map to nearly the same placein embedding space, while points on different planes mapto different places. We denote these as 3D embeddings,where 3D refers to the fact that the embeddings encode per-scene, and not per-image, planar information. We first re-view how existing single image embedding networks aretrained, before describing how we distill these pixel-wiseembeddings into a 3D-consistent embedding.Single image embeddings. In the case of plane estimationfrom monocular images, train a feedforward network to",
  "dist = 0.001": ".Per-image planar embeddings are not temporallyconsistent. While they can segment planes within a single image,plane embeddings in (b) from do not result in 3D consistentembeddings for a full scene. Our method (c) gives a per-scene em-bedding which is consistent across many views of that scene. map a single color image to per-pixel embeddings. Pixels iand j in the same image are mapped to embeddings xi andxj respectively, where xi is similar to xj, if and only if iand j are in the same plane. This is achieved by traininga network which takes as input a single image and outputsa per-pixel embedding, using two losses: a pull loss penal-izing pixel embeddings xi that are different from the meanembedding of their corresponding plane; and a push lossencouraging mean embeddings for each plane to be differ-ent from each other. One option to obtain 3D embeddingscould be to find all pixels that correspond to the reprojec-tion of a 3D point across multiple views and average theirper-pixel embeddings. The issue with this approach can beseen in . Here, the per-pixel embeddings are not con-sistent across views, despite encoding valuable planar in-stance information for each individual view. This is ablatedin Sec. 5.3 as embeddings w/o test-time optimization. Consistent 3D embeddings.Our goal is to learn embed-dings that preserve the properties of the per-pixel embed-dings, while being consistent across views. We achieve thisgoal by learning a per-scene mapping function , which isparameterized as an MLP and is optimized at test time, fol-lowing recent work . Our network takes as input a3D point p and predicts its 3D embedding ep = (p). Single image embeddings distillation loss.Our network is trained to distill information contained in the per-pixelembeddings x. For a pair of pixels i and j in a single im-age, we take their embeddings xi and xj. We also know",
  "(x,y,z)": ". Our method for 3D plane estimation. For each RGB keyframe we estimate per-pixel depth, planar probability and planarembedding following . We fuse the depths and planar probabilities into a TSDF and extract a mesh. We then train a per-scene MLP todistill the per-pixel embeddings into 3D-consistent embeddings. These are finally grouped via clustering into 3D planes. their corresponding 3D positions pi and pj and their image-space normals ni and nj. We can then train the network such that (pi) is similar to (pj), if and only if theircorresponding embeddings in image space xi and xj aresimilar and their normals (ni and nj) are also similar. In-spired by the push-pull loss used for the single image em-beddings , we use the following loss to encourage this:",
  ". 3D geometry estimation": "To estimate planes, we use our 3D embeddings alongside aninitial estimate of scene geometry. To estimate an accurate3D mesh we use SimpleRecon , a state-of-the-art 3D re-construction system that requires posed images as input. Init, depth maps are estimated using a multi-view stereo net,then fused into a 3D mesh via a truncated signed distancefunction (TSDF) .We adapt their network to additionally predict aplanar/non-planar probability, assigning a per-pixel valueindicating if that pixel belongs to a planar or non-planar re-gion, trained equivalently to the single-image plane estima-tor of . Our novelty is to then combine these per-pixelpredictions into 3D as an additional channel in the TSDF.When extracting the mesh, we exclude voxels that have anaggregated non-planar value of less than p = 0.25, so thatnon-planar regions are not part of the final mesh. This ex-tracted mesh is one of the inputs to the next steps.",
  "Given an embedding for each vertex in our 3D mesh, ournext step is to cluster vertices into plane instances based": "on those embeddings and on geometry information definedby the mesh. For this clustering step we rely on sequen-tial RANSAC . RANSAC works by randomly samplingplane instance proposals, checking the inlier count for eachproposal, and selecting the plane instance with the most in-liers. This process is done sequentially, where at each it-eration the points associated with the last predicted planeare removed from the pool. Each plane instance proposalis created by sampling a single mesh vertex, which togetherwith its associated normal, defines a plane. A different meshvertex is considered an inlier to this plane proposal if: (i)the distance to the plane is smaller than a threshold rd and(ii) the euclidean difference between embeddings is smallerthan a threshold re. After convergence, we merge planeswith highly similar embeddings and normals, i.e., where thedistance between average embeddings is < 0.2 and the dotproduct between average normals is > 0.6. Next, we runa connected components algorithm on the mesh representa-tion of each discovered plane in turn, to separate out non-contiguous planes.Since the non-planar vertices have already been removedas explained in Sec. 3.2, we expect all remaining vertices tobe assigned a plane instance label. RANSAC, however doesnot guarantee this. For this reason, we run a post-processingstep that iteratively propagates labels to connected unla-beled points from the RANSAC step. Finally, we removeplanes with fewer than 100 vertices.",
  ". Online inference": "All components of our method are designed so that theycan run online with little adaptation.The 3D geometryestimation steps, i.e., depth estimation, fusion into TSDF,and mesh extraction, are commonly used in online sys-tems . Our per-scene embedding network is alwaysupdated in an online fashion, similar to . Given thecurrent 3D mesh and the current embedding network, em-beddings can be predicted for all mesh vertices. We thenperform clustering to extract plane instances. To achieveinteractive speeds for our online method, we replace ourRANSAC clustering method, which takes 131ms on aver-",
  ". Planes can be estimated online at interactive rates. Asnew RGB frames are acquired, we can update the weights of ourMLP and recompute plane assignments. See Sec. 5.5 for timings": "age per scene, with the mean-shift algorithm using theefficient implementation from , which takes 25ms. Weevaluate this alternative clustering algorithm in the experi-mental section. Finally, each time we recompute planes, weuse Hungarian matching between the previous and cur-rent plane assignments to encourage consistency of planesacross time (visible in the figure as stability of colors overtime, while new planes are computed). shows an on-line reconstruction obtained with our method for a Scan-NetV2 scene.",
  ". Sequential RANSAC: A strong baseline": "Given recent advances in 3D scene reconstruction from im-age inputs, e.g., , the question arises: How good a pla-nar decomposition can we achieve if we run RANSAC onthe mesh only, without the contribution of our 3D embed-dings? Surprisingly, later results show this simple baselineperforms very well. However, while this naive approachtakes geometry into account, it does not leverage semanticor appearance-based cues, leading to plane over- and under-segmentation issues (see ). Our method, using 3Dplane embeddings, addresses these problems.",
  ". Implementation details": "Depth, plane probabilities, and per-pixel embeddingnetwork architecture. We use the SimpleRecon ar-chitecture for depth estimation. Encoder features are sharedbetween the depth estimation, plane probabilities, and per-pixel embedding tasks, though they have separate decoders.Full architecture details are in the supplementary material.Embedding MLP network. We use a three-layer MLPwith 128 dimensions for each hidden layer. Following ,we lift the input to the MLP to 48 periodic activation func-tions before it is input to the first linear layer. Our final em-bedding has three dimensions. We use te = 0.9, tn = 0.8and tp = 1.0, tuned on the validation set. Similarly to, the MLP is always trained in an online fashion. Foreach new keyframe we sample 400 pixels from it and applyEqn. (1) to each pair of points, together with the pairs fromthe 10 most recent keyframes. We then run backpropagationten times to optimize the MLP.Grouping thresholds. For RANSAC we set re = 0.5 and",
  "(c) Planes from geometry + RANSAC do not respect object instance boundaries": ".Sequential RANSAC alone is not enough to seg-ment planar instances.Sequential RANSAC (with geometryfrom ) does well, but fails to segment adjacent co-planar in-stances. Our method can segment these, e.g., this picture frame. rd = 0.1. We set the mean-shift bandwidth to 0.25.Mesh planarization. Given our final assignment of pointsto planes, we perform mesh planarization to convert our 3Dmesh into a planarized mesh. First, we estimate the planeequation for each plane. Next, each point is moved alongthe normal of its assigned plane such that it lies on the planeit is assigned to. This is the mesh which is geometricallyevaluated against the ground truth planarized mesh.",
  ". Experiments": "We train and evaluate on ScanNetV2 , because provided ground truth plane annotations for most of it.Plane annotations are unavailable for the ScanNetV2 testset. We therefore split the official ScanNetV2 validation setinto new plane evaluation validation and test splits, dubbedvalplanes and testplanes, with 80 and 100 scenes respectively.For a fair comparison with prior work, we re-evaluate base-lines on our new test split. The new splits and our evaluationcode are available at",
  ". Evaluation metrics": "Geometric evaluation.Here, we evaluate how well thepredicted planar mesh approximates the geometry of theground truth planar mesh. Following we adopt con-ventional 3D metrics . To compare a predicted meshwith the ground truth mesh, we first sample N = 200, 000points from each mesh. We then compare the two sampledpoint clouds to each other using chamfer distance and f1score. See for details.Fully volumetric methods such as predict geometryfor the whole scene, including unobserved regions. To pre-vent such methods from being penalized unfairly, we en-",
  ". Our 3D embeddings can be used in combination with avariety of different 3D geometry estimators, leading to improvedresults for all methods": "force a visibility mask to handle unseen points differentlywhen computing metrics, following . This visibilitymask is applied to all methods for fair comparison. We alsomask out 3D points sampled on faces that connect two ormore planes, as these points have ambiguous labeling. Forfull transparency, we report numbers in the supplementarymaterial using the evaluation method from without ouradditions. Plane segmentation evaluation.Following previouswork on plane estimation , we also re-port the following clustering metrics: Variation of Informa-tion (VOI), Rand Index (RI), and Segmentation Covering(SC). Given a predicted mesh, we use the protocol proposedin to map the plane ID of each vertex to the closest ver-tex in the ground truth mesh. See for full details. Planar metrics. To better evaluate how well the main, i.e.,large planes in the ground truth scene, are reconstructed weadditionally propose the following protocol. We select thek = 20 largest planes from each ground truth mesh. Foreach such plane qj, we find the predicted plane pi that mostclosely matches according to the completion metric. Wereport the fidelity between qj and pi as completion(qj, pi),where completion is the completion metric from . Theaverage of this score over all k ground truth planes overall scenes is our planar fidelity score. We also report thegeometric accuracy between qj and pi as planar accuracy,and the average of the two as planar chamfer.",
  ". Comparisons with baselines": "We evaluate our 3D plane estimation method against vari-ous baselines (). PlanarRecon is the existingstate-of-the-art method for 3D plane estimation from posedRGB images. We outperform their approach in geometry,segmentation, and planar metrics. We also compare with theleading baseline for 3D plane estimation from a single im-age, PlaneRecTR . For each scene, we run this singleimage predictor for selected keyframes. Planes from eachincoming image are matched to the closest world planes bycomparing planar normals, offsets, and plane positions. We compare with our own implementation of sequen-tial RANSAC, applied to meshes from SimpleRecon .SimpleRecon (SR in tables) is the same method we use forgeometry estimation, as detailed in Sec. 3.2, making thisthe closest baseline to our method, but without using thebenefits of our 3D consistent embeddings. In addition, wealso apply the sequential RANSAC method to geometryfrom . See supplementary material for imple-mentation details of the baselines. Our method outperforms all other methods on the seg-mentation metrics. While the results for the geometric met-rics are comparable with the SR + RANSAC baseline,we significantly outperform this baseline on the segmenta-tion and planar metrics, clearly demonstrating the benefitof using our 3D consistent embeddings. Surprisingly, Pla-narRecon is outperformed by several of our sequentialRANSAC baselines. This is in contrast with the results pre-sented in , and we discuss this difference in more detailin the supplementary material. Our embeddings benefit other geometry methods.Tovalidate the usefulness of our 3D embeddings, we use themin combination with different geometry estimation methods. We compare using only 3D geometry ver-sus using 3D geometry plus the embeddings derived fromour test-time optimized MLPs without retraining. We showthe results for this experiment in . For all methods,we observe that the additional information encoded in theembeddings improves over the baseline of using geometry+ RANSAC only.",
  ". Ablations": "We ablate our method to validate that our contributions leadto higher scores.These results are in .Fusedper-pixel embeddings w/o test time optimization is ourmethod but using the embeddings directly from , with-out our 3D distillation.These embeddings are fused asadditional channels into the TSDF. Fused per-pixel em-beddings with training time multi-view consistency isa variant of our method, where we attempt to train a sin-gle feed-forward embedding network which predicts multi-view consistent 3D embeddings directly, without perform-ing test-time optimization. Ours without planar probabil-ity is our method but all points are assigned a planar prob-ability of 1, meaning non-planar points are still part of themesh. For this reason, we do not run the post-processingstep that assigns unlabeled points after RANSAC.The first two ablations show that it is not trivial to pre-dict 3D consistent embeddings using a feed-forward net-work applied to each frame independently. This motivatesour use of a per-scene MLP optimized at test time to achieveconsistent embeddings. The last of these ablations showsthat our fusion of planar probabilities into the TSDF im-proves geometry metrics. We note that some computationalsavings could be made, at the price of 1% drop in geome-try scores, without this.We also compare the two different plane grouping algo-rithms. Using the Mean-shift variant of our method leads toonly a small degradation of results versus RANSAC, whileachieving interactive speeds (see Sec. 5.5 for timings).RANSAC oracle methods are variants of RANSACwhich have access to ground truth semantic and instanceinformation.SR + RANSAC + ground truth seman-tic labels uses ground truth semantic labels (transferred to the closest vertex in the predicted mesh) in the sequentialRANSAC loop to separate planes. Specifically, points canbe associated with a plane candidate only if they are geo-metrically consistent and have the same label. We addition-ally compare with a RANSAC variant with predicted se-mantic labels, where we predict N = 20 semantic classesand we fuse their probabilities in the TSDF. It is worthnoting that explicitly predicting semantics is beneficial andleads to better planar scores compared to its geometry-onlycounterpart in . However, our method provides bet-ter results across all metrics, and requires fusion of onlyplanar probabilities instead of N semantic classes, whichmight be challenging as N increases.Finally, we alsoshow an oracle with ground truth instance labels, whichpresents an upper bound for plane estimation.",
  ". Qualitative results": "shows results of our method compared to the closestpublished competitor PlanarRecon and our SR +RANSAC baseline. We can see that our method has closerfidelity to the ground truth versus , and avoids oversim-plification of geometry. By more closely adhering to thegeometry of the real scene, our planes can appear to havejagged edges when compared to the more simplified out-puts from . Our planar meshes have gaps where planesintersect because we remove triangles that connect verticesfrom different planes. If needed for a specific application,our outputs could be further post-processed, e.g., using .A qualitative comparison of our method with the SR +RANSAC baseline shows that we are able to recover sepa-rate semantic planes that have a common planar geometry.Finally, shows more results of our method, with im-ages and camera poses from an iPhone running ARKit .",
  ". Planes at interactive speeds": "The online variant of our method, which uses mean-shiftclustering, takes a total of 152ms per keyframe on average,on an RTX A6000 GPU. This comprises 65ms to obtainthe per-pixel depth, planar probability, and 2D planar em-bedding and separately 1ms for TSDF fusion, 61ms to up-date the MLP, and 25ms to run the clustering. As the av-",
  "Ground truthOursSR + RANSACPlanarRecon": ". Qualitative results on ScanNetV2. Our predictions are more faithful to the ground truth both in terms of geometry andsegmentation. Here we have removed ceiling planes (i.e., those with normals facing downwards) for visualization. In the first row, werecover the size and shape of the table well when compared to the baselines. In the second row, the sink is well separated from thecountertop in our results. The bottom row shows a failure case where we do not recover small pillows on the beds.",
  ". Limitations": "Our method shows notable improvements compared toother 3D plane estimation methods, but limitations remain.Errors in the geometry from our MVS system might havesevere consequences when extracting 3D planes. We alsofit planes in a greedy manner. Instead, global optimiza-tion e.g., may further improve results. Unlike ,we only estimate planes for visible geometry. Completingunobserved regions, like , could be a useful ex-tension for some applications.",
  ". Conclusion": "We propose a new approach which takes a sequence ofposed color images as input, and outputs a planar repre-sentation of the 3D scene. Surprisingly, we demonstratethat a strong baseline for this task is to simply run sequen-tial RANSAC on a lightweight 3D reconstruction. How-ever, this baseline is likely too limited for AR and roboticsuse-cases. Our approach addresses this, by training a 3Dembedding network to map 3D points to 3D-consistent andmeaningful plane embeddings, which can then be clusteredinto 3D planes. Our approach gives state-of-the-art planeestimation performance on the ScanNetV2 dataset.Acknowledgements.We are extremely grateful to SakiShinoda, Jakub Powierza, and Stanimir Vichev for their in-valuable infrastructure support."
}