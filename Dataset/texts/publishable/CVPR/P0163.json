{
  "Btm. 50%": ". (a) Large motions in optical flows are highlighted in blur maps. (b) Comparison of FLOPs between the standard spatio-temporaltransformer and the blur-aware spatio-temporal transformer. (c-d) Summary of the standard spatio-temporal transformer and the blur-awarespatio-temporal transformer. (e-f) Summary of the standard flow-guided feature alignment and blur-aware feature alignment. (g) In thevisual comparisons on the GoPro dataset, the proposed BSSTNet restores the sharpest frame.",
  "Abstract": "Video deblurring relies on leveraging information fromother frames in the video sequence to restore the blurred re-gions in the current frame. Mainstream approaches employbidirectional feature propagation, spatio-temporal trans-formers, or a combination of both to extract informationfrom the video sequence.However, limitations in mem-ory and computational resources constraints the temporalwindow length of the spatio-temporal transformer, prevent-ing the extraction of longer temporal contextual informa-tion from the video sequence. Additionally, bidirectionalfeature propagation is highly sensitive to inaccurate op-tical flow in blurry frames, leading to error accumula-",
  "Corresponding author:": "tion during the propagation process. To address these is-sues, we propose BSSTNet, Blur-aware Spatio-temporalSparse Transformer Network. It introduces the blur map,which converts the originally dense attention into a sparseform, enabling a more extensive utilization of informationthroughout the entire video sequence. Specifically, BSSTNet(1) uses a longer temporal window in the transformer, lever-aging information from more distant frames to restore theblurry pixels in the current frame. (2) introduces bidirec-tional feature propagation guided by blur maps, which re-duces error accumulation caused by the blur frame. The ex-perimental results demonstrate the proposed BSSTNet out-performs the state-of-the-art methods on the GoPro andDVD datasets.",
  ". Introduction": "Video deblurring aims to recover clear videos from blurryinputs, and it finds wide applications in many subsequentvision tasks, including tracking , video stabiliza-tion , and SLAM . Therefore, it is of great interestto develop an effective algorithm to deblur videos for abovementioned high-level vision tasks.Video deblurring presents a significant challenge, as itnecessitates the extraction of pertinent information fromother frames within the video sequence to restore the blurryframe.In recent years, there have been noteworthy ad-vancements in addressing this challenge.Flow-guided bidirectional propagation methods employ flow-guided deformable convolution andflow-guided attention for feature alignment. However, inac-curate optical flow in blurry frames causes the introductionof blurry pixels during bidirectional propagation. VRT andRVRT use spatio-temporal self-attention with tem-poral window to fuse the information from video sequence.Due to the high memory demand of self-attention, theseapproaches frequently feature restricted temporal windows,limiting their ability to incorporate information from distantsections of the video.Analyzing videos afflicted by motion blur reveals a cor-respondence between the blurry regions in the video and ar-eas with pixel displacement, where the degree of blurrinessis directly associated with the magnitude of pixel displace-ment. Moreover, The blurry regions are typically less fre-quent in both the temporal and spatial aspects of the blurryvideos. By leveraging the sparsity of blurry regions, thecomputation of the spatio-temporal transformer can focussolely on these areas, thereby extending the temporal win-dow to encompass longer video clips. Moreover, bidirec-tional feature propagation based on blurry regions enablesthe minimization of error accumulation. As shown in Fig-ure 1a, the green box area represents the blurry region inthe frame. Similarly, both the forward and backward opticalflows in the same location are also maximized, indicating acorrelation between the motion and blurry regions.By introducing blur maps, we propose BSSTNet, Blur-aware Spatio-temporal Sparse Transformer Network. Com-pared to methods based on spatio-temporal transformer,BSSTNet introduces Blur-aware Spatio-temporal SparseTransformer (BSST) and Blur-aware Bidirectional FeaturePropagation (BBFP). The proposed BSST efficiently uti-lizes a long temporal window by applying sparsity on inputtokens in the spatio-temporal domain based on blur maps.This enables the incorporation of distant information in thevideo sequence while still maintaining computational ef-ficiency. BBFP introduces guidance from blur maps andchecks for flow consistency beforehand. This aids in mini-mizing the introduction of blurry pixels during bidirectionalpropagation, ultimately enhancing the ability to gather in-",
  "formation from the adjacent frames.The contributions are summarized as follows": "We propose a non-learnable, parameter-free method forestimating the blur map of video frames. The blur mapprovides crucial prior information on motion-blurry re-gions in the video, enabling sparsity in the transformerand error correction during bidirectional propagation. We propose BSSTNet, comprising two major compo-nents:BSST and BBFP. BSST incorporates spatio-temporal sparse attention to leverage distant informationin the video sequence while still achieving high perfor-mance. BBFP corrects errors in the propagation processand boosts its capability to aggregate information fromthe video sequence.",
  ". Related Work": "Many methods in video deblurring have achieved impres-sive performances. The video deblur methods can be cate-gorized into two categories:RNN-based Methods.On the other hand, some re-searchers are focusing on theRNN-base methods. STRCNN adopts a recurrent neu-ral network to fuse the concatenation of multi-frame fea-tures. RDN develops a recurrent network to recurrentlyuse features from the previous frame at multiple scales.IFRNN adopts an iterative recurrent neural network(RNN) for video deblurring. STFAN uses dynamicfilters to align consecutive frames. PVDNet containsa pre-trained blur-invariant flow estimator and a pixel vol-ume module. To aggregate video frame information, ES-TRNN employs a GSA module in the recurrent net-work. Recently, the BiRNN-based method has achieved impressive deblur results through aggressivebidirectional propagation. BasicVSR++ adopts aggres-sive bidirectional propagation.Based on BasicVSR++,RNN-MBP introduces the multi-scale bidirectional re-current neural network for video deblurring. STDANet and FGST employ the flow-guided attention to alignand fuse the information of adjacent frames. However, dueto error accumulation, these methods do not effectively fusethe information from long-term frames. Ji and Yao de-velop a Memory-Based network, which contains a multi-scale bidirectional recurrent neural network and a memorybranch. However, the memory branch introduces a largesearch space of global attention and ineffective alignment.Transformer-based Methods. The Spatio-temporal trans-former is widely used in video deblurring .VRT utilizes spatio-temporal self-attention mechanismto integrate information across video frames. Due to the",
  "Estimator": ". Overview of the proposed BSSTNet. BSSTNet consists of three major components: Blur Map Estimation, Blur-aware Bidirec-tional Feature Propagation (BBFP), and Blur-aware Spatio-temporal Sparse Transformer (BSST). computational complexity of self-attention, VRT employs a2-frame temporal window size and utilizes a shifted windowmechanism for cross-window connections. However, theindirect connection approach with a small window size failsto fully exploit long-range information within the video se-quence.RVRT divides the video sequence into 2-frame clips, employing small-window spatio-temporal self-attention within each clip and Flow-guided biderectionalpropagation and alignment between clips. However, dueto the small window constraint of spatio-temporal self-attention and the error accumulation caused by the opticalflow of blurred frames in Flow-guided biderectional propa-gation, RVRT still falls short of fully utilizing the informa-tion from the entire video sequence.",
  ". Overview": "As shown in , the BSSTNet contains three keycomponents: Blur Map Estimation, Blur-aware Bidirec-tional Feature Propagation (BBFP), and Blur-aware Spatio-temporal Sparse Transformer (BSST). First, the forwardand backward optical flows, denoted as {Ot+1t}T 1t=1 and{Ott+1}T 1t=1 , are estimated from the downsampled videosequence X = { Xt}Tt=1. Then, Blur Map Estimation gen-erates the blur maps B = {Bt}Terated based on {Ot+1t}T 1t=1 for each frame are gen-t=1 and {Ott+1}T 1t=1 . Next,BBFP produces the aggregated features F using Blur-awareFeature Alignment (BFA). After that, BSST generates therefined features F from F with the Blur-aware SparseSpatio-temporal Attention (BSSA) layers. Finally, the de-coder reconstructs the sharp video sequence R = {Rt}Tt=1.",
  ". Blur-aware Bidirectional Feature Propagation": "Within BBFP, bidirectional feature propagation propagatesthe aggregated features F in both the forward and back-ward directions, incorporating Blur-aware Feature Align-ment (BFA). BFA is designed to align features from neigh-boring frames to reconstruct the current frame. As shownin e and f, the standard flow-guided fea-ture alignment aligns all pixels in the neighboring frames,whereas BFA selectively integrates information from sharppixels guided by blur maps. This prevents the propagationof blurry regions from the features of neighboring framesduring bidirectional feature propagation.Bidirectional Feature Propagation. Assuming the currenttime step is the t-th step, and the corresponding propagation",
  "Ott1, Ott2, At1, At2)(3)": "where BFA and W denote the BFA and Backward Warpoperations, resepectively. Fj1trepresents the feature ag-gregated from the t-th time step in the (j 1)-th branch.Fjt1 and Fjt2 are the features generated from the previ-ous and the second previous time step. The aforementionedprocess progresses forward through the time steps until itreaches t = T. The backward propagation process mirrorsthe forward propagation process. Blur-aware Feature Alignment. Different from the stan-dard flow-guided feature alignment that aligns all pix-els in neighboring frames, BFA introduces sharp maps toprevent the introduction of blurry pixels in the neighboringframes. As illustrated in , along with features Fjt1and Fjt2 from previous time steps, the corresponding op-tical flows Ott1 and Ott2, and the warped featuresW(Fjt1) and W(Fjt2), sharp maps At1 and At2 areadditionally introduced. These sharp maps serve as addi-tional conditions to generate the offsets and masks of the de-formable convolution layers . Moreover, the sharp mapacts as a base mask for DCN by being added to the DCNmask. This ensures that only sharp regions of features arepropagated.",
  "PP": ". The details of BSST. Note that P denotes the WindowPartition operation. Flatten by window indicates that query to-kens are flattened for each query window, and K/V tokens are gen-erated in a similar manner. Multi-head Self Attention is also com-puted on the query and K/V tokens generated for each window.",
  ". Blur-aware Spatio-temporal Sparse Trans-former": "The spatio-temporal attention is commonly employed invideo deblurring and demonstrates remarkable perfor-mance, as shown in c.However, the standardspatio-temporal attention method often restricts its temporalwindow size due to computational complexity, thereby con-straining its capability to capture information from distantparts of the video sequence. To overcome this limitation,we introduce the Blur-aware Spatio-temporal Sparse Trans-former (BSST). As illustrated in d, BSST filters outunnecessary and redundant tokens in the spatio and tempo-ral domain according to blur maps B. As shown in Fig-ure 1b, allowing BSST to include a larger temporal windowwhile maintaining computational efficiency. The detailedimplementation of BSST is illustrated in .GiventheaggregatedfeaturesF={FtRH/4W/4C}Tt=1 from the last branch of BBFP,we employ a soft split operation to divide each ag-gregated feature into overlapping patches of size p p witha stride of s. The split features are then concatenated, gen-erating the patch embeddings z RT MNp2C. Next,the blur map B are downsampled by average pooling witha kernel size of p p and a stride of s, resulting in B RT MNp2C. For simplicity, p2C is denoted as Cz. After that, z is fed to three separate linear layer transformations,resulting in zq RT MNCz, zk RT MNCz, andzv RT MNCz, where M, N, and Cz respectively de-note the number of patches in the height and width domains,and the number of channels. Subsequently, zq, zk, zv arepartitioned into m n non-overlapping windows, generat-ing partitioned features Eq, Gk, Gv RT mnhwCz,where mn and hw are the number and size of the win-dows, respectively. Utilizing the embedding z and incor-porating depth-wise convolution, the generation of pooledglobal tokens gk and gv takes place as follows",
  "gv = lv(DC(z))(4)": "where DC represents depth-wise convolution, and gk, gv RT hpwpCz.Following that, we repeat and concate-nate gk with Gk and gv with Gv, resulting in Ek, Ev RT mn(h+hp)(w+wp)Cz. Note that for the key/valuewindows, we enlarge its window size to enhance the recep-tive field of key/value . For simplicity, we ignore itin the following discussion.Spatial Sparse in Query/Key/Value Spaces. We observethat the blurry regions are typically less frequent in boththe temporal and spatial aspects of the blurred videos. Mo-tivated by this observation, we only choose the tokens ofblurry windows in Eq and tokens of sharp windows inEk, Ev to participate in the computation of spatio-temporalattention. This ensures that the spatio-temporal attentionmechanism focuses solely on restoring the blurred regionsby the utilization of sharp regions in video sequences. First,the blur maps of windows U RT mn are generated bydownsampling B RT MN using max pooling. Next, the spatial sparse mask of windows is obtained as follows",
  "(5)": "where , Clip, S Rmn are the threshold for consideringrelated windows as blurry windows, a clipping function thatset S to 1 if Tt=1 Qt > 0, and the spatial sparse maskfor Eq, Ek and Ev, respectively. Then, the spatial sparseembedding features Iq, Ik, and Iv are generated using thefollowing equations",
  "(6)": "where Concat denotes the Concatenation operation. IfSi,j = 0, it indicates that the windows position indexedby (i, j) in the video sequence does not encompass blurrytokens. This allows us to exclude the tokens within thosewindows from the spatio-temporal attention mechanism.Iq RT msnshwCz, while both Ik and Iv share thesize of RT msns(h+hp)(w+wp)Cz, where ms and ns rep-resenting the number of selected windows in m and n do-mains, respectively.Temporal Sparse in Query Space. Along the temporal do-main, we choose the windows of the blurry region for queryspace, ensuring that the spatio-temporal attention mecha-nism is dedicated to restoring only the blurry regions of thevideo sequence. Given the spatial sparse embedding fea-tures Iq, the spatio-temporal sparse embedding yq is gener-ated as follows",
  "i [1, ms], j [1, ns]}Tt=1Yq = Concat(Hq)(7)": "where Yq RKqmsnshwCz. Top(Kq, ) represents theoperation of finding the Kq-th largest element in a vector.For each window located at position (i, j) in Iq, within thetemporal domain, we selectively chose the top Kq windowswith the highest blur levels for deblurring.Temporal Sparse in Key/Value Spaces.In contrast tothe query space, we select the sharp regions in Ik, Iv forkey/value spaces.Due to the high similarity in texturesbetween adjacent frames, we alternately choose temporalframes with a stride of 2 in each BSST. In BSSTNet, con-sisting of multiple BSSTs, odd-numbered BSSTs selectframes with odd numbers, while even-numbered BSSTschoose frames with even numbers, resulting in a 50% re-duction in the size of the key/value space. Given the spatialsparse embedding features Ik and Iv, the spatio-temporalsparse embedding features yk and yv are generated as fol-lows",
  "zs = MSA( Yq, Yk, Yv)(10)": "where MSA is the Multi head Self-Attention function.After applying our sparse strategy to eliminate unnecessaryand redundant windows, we use self-attention followingEq. 9 on the remaining windows to extract fused features.Specially, standard window spatio-temporal attention is ap-plied to unselected (less blurry) windows, allowing featuresto be restored to their original size. Subsequently, these fea-tures are gathered through a soft composition operation to serve as the input for the next BSST. The output of thefinal BSST is denoted as F.",
  ". Datasets": "DVD. The DVD dataset comprises 71 videos, consist-ing of 6,708 blurry-sharp pairs. These are divided into 61training videos, amounting to 5,708 pairs, and 10 testingvideos with 1,000 pairs.GoPro.The GoPro dataset consists of 3,214 pairsof blurry and sharp images at a resolution of 1280720.Specifically, 2,103 pairs are allocated for training, while1,111 pairs are designated for testing.",
  "Runtime (ms)234528": "to 4 104. The network is optimized with L1 loss usingAdam optimizer , where 1 = 0.9 and 2 = 0.999. Theflow estimator in BSSTNet uses pre-trained weights fromthe official RAFT release and remains fixed duringtraining. During testing, T, Kq, and Kkv are set to 48, 24,and 24, respectively. During training, they are 24, 12, and12, respectively. In the training phase, input images are ran-domly cropped into patches with resolutions of 256 256,along with the application of random flipping and rotation.Hyperparameters To strike a better balance between videodeblurring quality and computational efficiency, the valueof is set to 0.3. The patch size p and stride z are set to 4and 2, respectively.",
  ". Main Results": "DVD. The quantitative results on the DVD dataset areshown in . The proposed method demonstrates su-perior performance in terms of both PSNR and SSIM com-pared to existing state-of-the-art methods. Specifically, incomparison to the best-performing state-of-the-art method,Shift-Net+, the proposed BSSTNet achieves an improve-ment of 0.26 dB in PSNR and 0.0013 in SSIM. Examplesfrom the DVD dataset are presented in a, demon-strating that the proposed method generates images with in-creased sharpness and richer visual details. This highlightsthe robustness of the method in eliminating large blur in dy-namic scenes.GoPro. In , the proposed BSSTNet shows favor-able performance in terms of both PSNR and SSIM when",
  "(b) Qualitative comparison on the GoPro dataset": ". Qualitative comparison on the GoPro and DVD datasets. Note that GT stands for Ground Truth. The proposed BSSTNetproduces images with enhanced sharpness and more detailed visuals compared to competing methods. compared to state-of-the-art methods on the GoPro dataset.BSSTNet achieves higher PSNR and SSIM values com-pared to Shift-Net+. The visual results in b furtherillustrate that the proposed method restores finer image de-tails and structures.FLOPs and Runtime. We conducted a comparison of thecomputational complexity (FLOPs) and runtime betweenour method, RVRT, and Shift-Net+, as presented in Ta-ble 3. In contrast to the state-of-the-art Shift-Net+, our ap-proach demonstrates a 13 GFLOPs reduction in FLOPs andachieves a speedup of 1.6 times.",
  ". Ablation Study": "Effectiveness of BBFP. To evaluate the effectiveness ofBBFP, we conduct an experiment by excluding BBFP fromBSSTNet. As illustrated in , the omission of BBFPin Exp.(b) results in a reduction of 0.21 dB in PSNRand 0.0011 in SSIM. BFA plays an important role in pre-venting the introduction of blurry pixels from neighboringframes. As shown in , replacing BFA with Stan-dard Flow-guided Feature Alignment results in a decline inperformance. To highlight the improved feature alignmentcapability of BBFP, we visualize the aligned features in Fig-ure 6, comparing them with the standard feature bidirec-",
  "Previous Frame": ".Comparison of feature alignment between BBFPand Standard Flow-guided Bidirectional Propagation (SFBP).Compared to SFBP, BBFP prevents the propagation of blurry re-gions from the features of neighboring frames during propagation. . Comparison of different temporal lengths in terms of PSNR, SSIM, Runtime, Memory, and GFLOPs between the StandardSpatio-temporal Transformer (SST) and BSST.. The results are evaluated on the DVD dataset. Note that TL. and Mem. denoteTemporal Length and the used memory on GPU, respectively. SST runs out of memory for a temporal length of 60.",
  "Top 25%34.780.9694127Top 50% (Ours)34.950.9703133": "tional propagation (SFBP). Benefiting from the incorpora-tion of blur maps, BBFP prevents the propagation of blurryregions from the features of neighboring frames during thepropagation process, resulting in sharper features.Effectiveness of BSST. To evaluate the effectiveness ofBSST, we conduct an experiment by excluding BSST fromBSSTNet. As shown in , the omission of BSST inExp. (c) results in a notable degradation of 0.85 dB in PSNRand 0.0042 in SSIM. To further evaluate the effectivenessand efficiency of BSST, we compare different token spar-sity strategies. demonstrates that using fewer tokennumbers or randomly selecting tokens will result in a sig-nificant decline in performance. This result suggests thatwithout guidance from the blur map, discarding tokens in the spatio-temporal domain results in the loss of valuableinformation in the video sequence.Moreover, our spar-sity strategy, which involves using the top 25% of tokens,achieves performance comparable to using all tokens whileutilizing only approximately 43% of the FLOPs. This indi-cates that our sparsity strategy effectively leverages tokensin sharp regions within the video sequence. Comparison of Different Temporal Length. In ,we present a comparison of the Standard Spatio-temporalTransformer (SST) under different sequence lengths interms of PSNR, SSIM, Runtime, Memory, and GFLOPs.As the sequence length increases, the computational com-plexity of SST grows rapidly. In contrast, BSSTs computa-tional complexity is less affected by the sequence length, al-lowing BSST to utilize longer sequences and boost deblur-ring performance. Specifically, when the sequence length is60, BSST shows a modest gain in PSNR and SSIM. Consid-ering the balance between performance and computationalload, we ultimately choose 48 as the length for the inputvideo sequence.",
  ". Conclusion": "In this paper, we present a novel approach for video de-blurring, named BSSTNet. Utilizing an understanding ofthe connection between pixel displacement and blurred re-gions in dynamic scenes, we introduce a non-learnable,parameter-free technique to estimate the blur map of videoframes by employing optical flows. By introducing Blur-aware Spatio-temporal Sparse Transformer (BSST) andBlur-aware Bidirectional Feature Propagation (BBFP), theproposed BSSTNet can leverage distant information fromthe video sequence and minimize the introduction of blurrypixels during bidirectional propagation. Experimental re-sults indicate that the proposed BSSTNet performs favor-ably against state-of-the-art methods on the GoPro andDVD datasets, while maintaining computational efficiency.",
  "Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang,Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool.VRT: A video restoration transformer. arXiv: 2201.12288,2022. 2, 6": "Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan,Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, RaduTimofte, and Luc Van Gool.Recurrent video restorationtransformer with guided deformable attention. In NeurIPS,2022. 2, 3, 6 Jing Lin, Yuanhao Cai, Xiaowan Hu, Haoqian Wang, You-liang Yan, Xueyi Zou, Henghui Ding, Yulun Zhang, RaduTimofte, and Luc Van Gool. Flow-guided sparse transformerfor video deblurring. In ICML, 2022. 2 Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, LeweiLu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hong-sheng Li. Fuseformer: Fusing fine-grained information intransformers for video inpainting. In ICCV, 2021. 4, 5, 6"
}