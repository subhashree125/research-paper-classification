{
  "Abstract": "Recent advances in generative models trained on large-scale datasets have made it possible to synthesize high-quality samples across various domains.Moreover, theemergence of strong inversion networks enables not onlya reconstruction of real-world images but also the modifi-cation of attributes through various editing methods. How-ever, in certain domains related to privacy issues, e.g., hu-man faces, advanced generative models along with stronginversion methods can lead to potential misuses. In this pa-per, we propose an essential yet under-explored task calledgenerative identity unlearning, which steers the model notto generate an image of specific identity.In the gener-ative identity unlearning, we target the following objec-tives: (i) preventing the generation of images with a cer-tain identity, and (ii) preserving the overall quality of thegenerative model.To satisfy these goals, we propose anovel framework, Generative Unlearning for Any IDEntity(GUIDE), which prevents the reconstruction of a specificidentity by unlearning the generator with only a single im-age.GUIDE consists of two parts: (i) finding a targetpoint for optimization that un-identifies the source latentcode and (ii) novel loss functions that facilitate the unlearn-ing procedure while less affecting the learned distribution.Our extensive experiments demonstrate that our proposedmethod achieves state-of-the-art performance in the gen-erative machine unlearning task. The code is available at",
  "SourceUnlearned": ". Given a single source image containing a specific iden-tity, we remove that identity from the pre-trained 3D generativeadversarial network (e.g., EG3D ). Our method effectively un-learns identity even from in-the-wild images where the source im-age is absent in the pre-training dataset. or AFHQ , have drawn substantial attention due totheir remarkable generation performance and highly disen-tangled representation space. However, their advancementshave raised privacy concerns , especially regarding thepotential misuse of generative models to represent and ex-ploit individual identities. For instance, deepfakes can create very believable images or videos of people inmade-up situations, causing major concerns about ethicsand privacy.To alleviate privacy issues in generative models, machineunlearning task has been actively studied.Machine un-learning involves the process of selectively removing spe-cific knowledge or erasing the influence of certain data fromthe training dataset of pre-trained models. It is beneficialespecially when the data are harmful, private, or biased",
  "arXiv:2405.09879v1 [cs.CV] 16 May 2024": ". Despite a focus on discriminative tasks inmost machine unlearning research, a few studies have ven-tured into generative models, attempting to erase high-levelconcepts such as socially inappropriate content or artisticstyles that present copyright challenges .Nevertheless, generative models still exhibit ongoingprivacy issues. Even if an identity of someone is not usedin the pre-training of the generative models, it can be easilyreconstructed in the pre-trained models via GAN inversionmodels . Furthermore, the recon-structed image can be manipulated or edited easily via im-age editing methods . To prevent potential ex-ploits of an identity, it is necessary to erase a certain identityfrom the pre-trained generative models.To consider the above issue, we introduce an essentialtask of unlearning any identity from the pre-trained 2D or3D GANs , called generative identity unlearning.Unlike typical machine unlearning tasks, which focus onunlearning the training samples our generative identity un-learning task unlearns any identity on pre-trained GANs,even if it was not shown during the pre-training. Our goal isto remove the whole identity associated with a given singleimage from the generator while minimally impacting theoverall performance of the pre-trained model.To achieve our goal, we propose a novel generativeunlearning framework, Generative Unlearning for AnyIDEntity, named GUIDE. GUIDE replaces the source iden-tity with an anonymous target identity, erasing the originalidentity effectively. To this end, we propose a new explo-ration method to determine an effective target latent code,called Un-Identifying Face On latent space (UFO). UFOutilizes the GAN inversion method to embed the givenidentity into the source latent, and then decides the targetlatent using both the source and the average latent codes.We empirically find that the proposed UFO can identify thepromising target to erase any given source identity robustly.Given the source and target latent code, we update thegenerator to shift from the source identity to the target iden-tity. To this end, we propose three novel loss functions: (i)local unlearning loss, (ii) adjacency-aware unlearning loss,and (iii) global preservation loss. (i) guides our model di-rectly shifting the source identity to the target identity. (ii)utilizes other latent codes adjacent to the source and targetlatent codes to effectively unlearn the entire identity from asingle image. To minimize side effects from the unlearningprocess, (iii) additionally regularizes the generator to retaingeneration performance for latent codes relatively far fromthe source and target latent codes. Through comprehensiveexperiments on diverse identities, including Random, InD,and OOD, we confirm that GUIDE can successfully removethe identity of the source image from the pre-trained gener-ative model, and shows qualitatively and quantitatively su-perior performances.",
  "Our contributions can be summarized as follows:": "For the first time, we propose a novel task, generativeidentity unlearning, which tackles machine unlearning ingenerative models in the aspect of privacy protection. Inour task, we aim to prevent the pre-trained generativemodels from synthesizing the given identity by utilizingonly a single image. For the effective and robust elimination of the identity, wepropose a novel method - Un-Identifying Face On LatentSpace (UFO). We configure the unlearning procedure byformulating how to represent and shift the identity in thelatent space. We find that setting the extrapolated latentcode between the source and average latent codes as anoptimization target facilitates the unlearning procedure. We propose three loss functions - local unlearning loss,adjacency-aware unlearning loss, and global preserva-tion loss to effectively unlearn the identity from the pre-trained model while less affecting the generation perfor-mance on other identity. We show that our proposed framework, GUIDE achievesstate-of-the-art performance both qualitatively and quan-titatively, through extensive experiments.We demon-strate that GUIDE can remove the specific identity suc-cessfully in the generative models while minimizing thenegative effect on other identities.",
  ". Related Work": "Generative Models and Privacy Issue.In image synthe-sis field, GAN-based generative models have achieved re-markable performance not only in 2D but also in 3D domain. The applica-tion of various image editing methods to stronggenerative models, people can easily generate edited imagesof specific individuals and various artistic styles , aswell as extract copyrighted content without the permis-sion of the individual or the original creator.Recently, with the rise of the importance of AI ethics,several works have addressed this issue .ESD erases specific visual concepts from diffusionmodel by using negative guidance about the undesired con-cepts. Kumari et al. modifies the conditional distri-bution of the model a specific target concepts to match theanchor concept. Forget-Me-Not fine-tunes U-Net tominimize each of the intermediate attention associated withthe target concepts to remove. Additional works in GANs focus on unlearning specific features, e.g., Bang,Hat or Beard rather than forgetting specific identity.The above methods primarily concentrate on the elimina-tion of specific concepts or high-level features. In otherwords, these cannot preclude models from generating spe-cific individuals while maintaining the generation perfor-mance of realistic human faces. Unlike the existing works,our work targets to unlearn only specific individuals without",
  "shifting the overall distribution of generated images": "Machine Unlearning.Machine unlearning aims to selec-tively forget specific acquired knowledge or diminish theimpact of certain training data subsets on a trained model.Since previous research shows that machinelearning models might accidentally share private informa-tion when faced with certain attacks or inputs, machine un-learning becomes crucial.While previous machine unlearning is mainly focusedon supervised learning tasks , the interestin unlearning techniques within unsupervised learning, i.e.,generative models, is growing . However, mostof the existing methods need full dataset access for retrain-ing, which is hard to acquire and computationally expensive. For example, Kong and Chaudhuri uti-lizes data redaction and augmentation algorithms, which re-quires a full training dataset. Despite the existence of a fea-ture unlearning model which does not need full datasetaccess, unlearning only an individual feature is not enoughto forget a whole specific identity. To this end, we pro-pose an algorithm that enables forgetting the specific iden-tity only with a single image. Furthermore, our approachdistinguishes itself from existing research by applying un-learning to unseen images, enabling the erasure of specificidentities without prior exposure to those images.",
  ". Method": "Firstly, in .1, as shown in , we introducethe problem we aim to address, named generative identityunlearning. In .2, we introduce un-identifying faceon latent space, which designates an appropriate target la-tent for unlearning. Then, in .3, we introduce la-tent target unlearning, along with our proposed novel lossesto unlearn the generator. The total overview of our methodis illustrated in .",
  ". Problem Formulation": "Given a set of images x depicting a specific identity, we ran-domly select a single source image xu x as an exemplarof the identity. Initially, using off-the-shelf inversion net-work E corresponding to the unconditional generator,i.e., EG3D , we embed xu to the source latent code wuin the latent space of EG3D:",
  "! (#)": ". An illustration of generative identity unlearning. UponGUIDE, the identity of the image generated from wu, i.e., inver-sion of the source image xu by inversion network E, should ex-hibit a distinct identity when passed through the pre-trained gen-erator Gs compared to the unlearned generator Gu. Furthermore,other images xo, not used in unlearning but sharing the same iden-tity with xu, also should vary an identity through GUIDE. where c denotes camera poses. For convenience, we omitthe explicit notation of camera poses in this paper, i.e., x =(R G)(wu). We target to derive an unlearned G, i.e., Gu,from the pre-trained EG3D generator G, i.e., Gs, while fix-ing Map and R. With proper unlearning, an image gener-ated by unlearned EG3D using wu, i.e., xu = (RGu)(wu)should have a distinct identity from the image generated byoriginal EG3D using wu, i.e., (R Gs)(wu).Inourtaskformulation,twoconsiderationsareparamount. First, we aim to eliminate the entire identityfrom the generator only utilizing a single image. To vali-date this, we incorporate other multiple images xo x fortesting and its corresponding latent code wo = E(xo). Byutilizing xo, we can verify whether Gu has successfully un-learned the identity as a whole, rather than just unlearningthe specific image xu. Second, we strive to maintain thegeneration performance of the pre-trained model. To assessthis, we sample multiple images from fixed latent codes us-ing both the unlearned and pre-trained generators. We thenestimate the distribution shift between the images generatedbefore and after the unlearning process.",
  ". Un-Identifying Face On Latent Space": "The successfully unlearned model should not generate theimage with the identity of x, even when wu is used as a la-tent. Consequently, we initiate our approach to manipulatexu to be another image rather than the image with iden-tity in x by unlearning G. To design the objective functionfor unlearning, we first need to establish the objective forunlearning, which involves defining the target image xt, de-rived from wt, which the unlearned image xu, derived fromwu, should mimic after unlearning. While there exist var-ious options for setting xt, e.g., a random face or even anon-human image, we choose the mean face generated by",
  "=": ". An overview of GUIDE. Starting with a source image, we employ a GAN inversion network E, specifically GOAE , toembed this image into the latent space of a pre-trained generative model, namely EG3D , obtaining the source latent code wu. Thetarget latent code wt is designated through the UFO process. To facilitate identity removal in wu, we shift its identity to match that ofwt with our Latent Target Unlearning (LTU) process. Three loss functions of LTU are designed for this purpose: (i) The generator isoptimized to produce an image from the source latent code, denoted as (R Gu)(wu), that is similar to the image from the target latentcode, represented as (R Gs)(wt). (ii) To achieve unlearning across the entire identity, we consider latent codes near both the source andtarget latent codes, denoted as wu,a and wt,a, respectively. (iii) To prevent model corruption during the unlearning process, we additionallysample latent codes from a random noise vector, represented as wr,g, and optimize Gu to preserve its generation ability on wr,g.",
  "||": ". An illustration of Un-identifying Face On Latent Space(UFO). We define the identity of the source latent code by subtractit from the average latent code. We set the target latent code forour unlearning process by measuring an extrapolation between thesource and average latent code with a fixed distance d. the mean latent w of Map, i.e., (R Gs)( w), as xt. Thissetting is inspired by inversion methods which ap-ply the identity to the mean face through inversion stages.Since our goal is to de-identify the image, we argue that it isintuitive and reasonable to send the image back to the meanface, which is opposite to the inversion.However, this simple approach might be problematicwhen wu is close to w, where the unlearned image mightstill resemble the original identity. To this end, we proposea novel method named Un-identifying Face On latent space(UFO), which can set xt robustly regardless of the distancebetween wu and w, as shown in . The following arethe processes of UFO: First, in the de-identification process,we extract the identity latent wid = wu w by subtractingw from wu. Then we propose a process of en-identification, setting the target in the opposite direction of the existingwid to foster a more pronounced change in identity, creat-ing an entirely new identity. In other words, we chose was the stopover point and established the final target pointat the extrapolation of wu and w. The finalized target pointcan be expressed as:",
  "wid2,(3)": "where d is defined as the distance that balances un-identification with preservation of the source distribution,as determined by our empirical results.Finally, we canset xt = (R Gs)(wt). We empirically demonstrate thatwhether the given identity is in close proximity to the aver-age latent code within the latent space or not, UFO sets thedesirable target latent code for the generative unlearning.",
  ". Latent Target Unlearning": "After setting xt, we need to robustly design the unlearn-ing process that can effectively make xu to be similar toxt, while maintaining the generation performance of Gu.We coin this process as Latent Target Unlearning (LTU),which targets to unlearn images from the specific latentwhile keeping generation results from other latent codes.LTU utilizes the following three losses to achieve this goal:",
  ", !\"#$,\" = $,\" , $,\"~(0, 1)": ". An illustration of determining latent codes near a latentcode w in adjacency-aware unlearning loss. We first sample a la-tent code wr,a which is derived from a random noise vector zr,avia the mapping network Map(), i.e. wr,a = Map(zr,a). Next,we compute the direction between w and wr,a, and we scale it tofall within range between 0 and max. This yields the distancevector to compute the adjacent latent code wa = w + . perceptual loss Lper , and identity loss Lid . UsingLrecon, we compare the tri-plane features Fu = Gu(wu)and Ft = Gs(wt), derived from source and target latentcodes, respectively. The local unlearning loss is defined as:",
  "By adopting Llocal, we can successfully un-identify thegiven source identity in xt": "Adjacency-Aware Unlearning Loss.The above equationconsiders only one pair of source and target latent codes.However, images of a similar identity to the source iden-tity can be obtained by introducing marginal perturbationsto the latent code.For the successful unlearning of thegiven identity, we need to consider the neighborhood ofboth the source and the target latent codes. Consequently,as shown in , we sample Na latent codes in thevicinity of the wu. Specifically, with the scale i sampledfrom the uniform distribution with hyperparameter max,i.e. i U(0, max), we define the distances to com-pute the adjacent latent codes as:",
  "where F iu,a = Gu(wiu,a), F it,a = Gs(wit,a) denotes for tri-plane features, and Llocal in Equation 4. From Ladj, we canfurther consider possible variations of the source identity": "Global Preservation Loss.While the local unlearningloss and adjacency-aware unlearning loss mentioned abovefacilitate the removal of the source identity, we propose aglobal preservation loss to mitigate side effects arising fromthese unlearning loss functions. In the global preservationloss, we constrain the generator to maintain generation per-formance for latent codes that are relatively distant fromboth the source and target latent codes.To be precise, we sample Ng latent codes {wir,g}Ngi=1from random noise vectors {zir,g}Ngi=1. We ensure that thesedo not overlap with the adjacent latent codes used in theadjacency-aware unlearning loss.Unlike the unlearningloss functions, we find that adopting only Lper achievesa balanced performance between identity shift and modelpreservation. The global preservation loss is computed as:",
  ". Experimental Setup": "Baseline.Since we propose generative identity unlearn-ing task for the first time, to evaluate the effectiveness ofGUIDE, we constructed a simple baseline. In the baseline,we used the target latent code as the average one for the un-learning. During the unlearning, we updated the pre-trainedgenerator using Llocal as described in Equation 4. Implementation Details.We built GUIDE based on the3D generative adversarial network pre-trained on FFHQdataset . We used GOAE as a GAN inversion net-work to obtain the latent code from an image. The imageresolution we used in our experiments is 512x512 with arendering resolution of 128x128. We used Adam optimizer",
  "with a learning rate of 104 in the unlearning proce-dure. The hyperparameters used in the experiments were:d = 30, max = 15, L2 = 102, per = 1, id = 101,Na = Ng = 2, and adj = global = 1": "Dataset and Scenarios.We evaluated GUIDE in threescenarios:Random, where we set an unlearning targetimage from a randomly sampled noise vector; InD (in-domain), where we sampled an image from the FFHQdataset used for pre-training;and OOD (out-of-domain), where the unlearning target image was sampledfrom the CelebAHQ dataset . For InD and OOD, weused the GAN inversion network to obtain correspondinglatent codes. For OOD scenario, we also conducted multi-image test since there were multiple images with a sameidentity in CelebAHQ. On the other hand, we performedonly single-image test in the Random and InD scenarios. Evaluation Metrics.We evaluated GUIDE on two keyaspects. Firstly, we estimated the efficacy of our approachin preventing the generator from producing images similarto the unlearning target. We quantitatively measured simi-larity of identities (ID) using face recognition network, Cur-ricularFace , between images generated from the samelatent codes before and after unlearning. Moreover, we uti-lized IDothers to estimate the erasure of a identity from im-ages which are unseen during training but containing thesame identity of the source image. Secondly, we assessedwhether our method preserves overall generation perfor-mance using the Frechet Inception distance (FID) score. Different from the existing usages, we utilized twovariants of FID. First, we evaluated the distribution shiftof generated images the pre-trained generator and the un-",
  "Qualitative Results": "We conducted a comparative analysis of GUIDE against thebaseline in the generative identity unlearning task. Initiat-ing from the provided source image, we aimed to eliminatethe identity within the pre-trained generator, as illustratedin . We presented the resulting unlearned image,along with the target image optimized in our loss functions.Notably, GUIDE effectively erases identities whether syn-thetic, presented during pre-training, or unseen.",
  "UnlearnedSynthesized from \"": ". Qualitative comparison between GUIDE and the base-line on the preservation of the generation quality of other iden-tities.GUIDE generates images almost identical to those syn-thesized by Gs, whereas the baseline often results in noticeablechanges, e.g., beard shape, hairstyle change, hat. To evaluate the thoroughness of identity removal, weperformed a multi-image test using identities from the Cele-bAHQ dataset. This test involved assessing the ID similar-ity not only for the unlearned image derived from the sourceimage but also for other images sharing the same identity.As shown in , GUIDE showed superior general-ization for unseen images compared to the baseline. Thisimprovement is attributed to the adjacency-aware unlearn-ing, which facilitated the unlearning process not just for thegiven images but also for their neighborhood.In , we conducted an experiment to assess the ef-fect of the unlearning process on other identities. While thebaseline had a significant impact on other identities throughthe unlearning, GUIDE showed a relatively lesser effect.We attribute this to the global preservation loss, which con-strained the distribution shift on other latent codes.",
  "+ Lglobal (GUIDE)0.03 0.050.17 0.087.88 1.963.34 1.10": ". Quantitative results of GUIDE and the baseline in thegenerative identity unlearning in a multi-image setting, i.e., usinga single image for unlearning and the other images for testing. Weused CelebAHQ dataset for this test. random scenario, indicating that in cases where a latentcode was close to w, i.e., as in the random scenario, therewas insufficient removal of identity. The effectiveness ofemploying an extrapolation between the source latent codeand the average latent code was evident in such instances.The adjacency-aware unlearning loss further enhancedthe unlearning an identity. This loss was designed to coverthe vicinity of the source latent code, thereby promoting un-learning on the source latent code itself. Finally, the appli-cation of the global preservation loss effectively reduced theestimated distribution shift using FIDpre and FIDreal.Moreover, we conducted a multi-image test in an OODscenario. In this particular experiment, we introduced addi-tional metric - IDothers aimed at quantifying ID similaritiesfor the unseen images associated with the source identity.As presented in , the introduction of the adjacency-aware unlearning loss resulted in a remarkable improve-ment in IDothers, emphasizing the effectiveness of this un-learning approach for handling unseen images.",
  ". Ablation Study": "Effect of d in Determination of wt.We conducted an ab-lation study by comparing target images derived from var-ied values of d. Setting d to 0 denotes utilizing the w aswt in the unlearning process. For d < 0, we designatedwt as an interpolated latent code between the ws and thew. Conversely, for d > 0, we employed an extrapolatedwt, as detailed in .2. As illustrated in ,when d < 0, the target image closely aligns with the givensource images. However, as d deviates from 0, the qualityof the target image rapidly deteriorates, resulting in a pro-nounced collapse in the distribution of the pre-trained gen-erator. Consequently, the effectiveness of unlearning withsuch target images diminishes in removing identity from the",
  "= (Ours)": ". Ablation study to figure out the effectiveness of d. Wevisualized target images corresponding to each source image withdifferent values of d. The target images were generated using tar-get latent codes derived from interpolated latent codes, the averagelatent code (d = 0), or extrapolated latent codes (d > 0). Interpo-lation and extrapolation were carried out between the source andthe average latent code. In the case of interpolation, the center be-tween the source and the average latent code was computed.",
  "0.06030.2754 0.0791100.0892 0.06200.2123 0.0762150.0878 0.03750.2094 0.0692200.0900 0.05380.2105 0.0924300.0926 0.05610.2111 0.0653": ". Ablation study to figure out the effectiveness of Ladj andmax. We compared the performance based on how successfullythe given identity was erased, using ID and IDothers metric. Therow where max = 0 denotes the baseline. We used CelebAHQdataset in this experiment. source images. Setting d to 0 might suggest the use of w asan effective target for erasing identity. However, our ab-lation studies indicate that when the source image closelyaligns with w, the unlearning procedure fails to thoroughlyeliminate the identity. Conversely, when d > 0, wt con-tains a distinct identity compared to the source image whilemaintaining a consistent distance from w. Among the in-stances where d > 0, our ablation studies reveal that settingd = 30 achieves a balanced performance between effectiveunlearning and preservation of the generation performanceof the pre-trained model. Effect of max in Ladj.In , we scrutinized theeffectiveness of the adjacency-aware unlearning loss. Toensure a fair comparison, we employed w as wt in this ex-periment, and we used Llocal and Ladj in the unlearningprocedure. Rows corresponding to max = 0 represent ex-perimental results without the incorporation of Ladj in theunlearning procedure. The introduction of Ladj resulted inconsistent performance gains in IDothers. This observationhighlights the efficacy of considering not only the pair ofsource and target latent codes but also their surroundingsfor unlearning the entire identity.",
  "1.534.75 0.894.63 0.431.48 0.29": ". Ablation study to figure our the effectiveness of Lglobal.We compared how preserved the performance of the pre-trainedmodel through the unlearning process, via FIDpre and FIDreal.We used CelebAHQ dataset in this experiment. Effect of Lglobal.To assess the effectiveness of the globalpreservation loss, a similar experiment was conducted asthe previous experiment, i.e., setting w as wt. The resultsare presented in . The application of Lglobal demon-strated consistent performance improvements in both FIDpreand FIDreal. This suggests that imposing constraints onthe generator to maintain its generation performance in la-tent codes distant from our primary focus is effective in re-ducing distribution shifts in generative models.",
  ". Conclusion": "In this paper, we introduced a novel task, referred to as gen-erative identity unlearning, designed to address privacy con-cerns in pre-trained generative adversarial networks. Thistask requires thoroughly removing the identity of a singlesource image from the pre-trained generator. To achievethis, we proposed a new framework, GUIDE (GenerativeUnlearning for any IDEntity). To unlearn the single iden-tity, we first defined the target latent code via extrapolation,moving away from the average latent by the pre-defined dis-tance in the direction from the source to the average latent.Using this, our GUIDE successfully unlearned the givenidentity via Latent Target Unlearning (LTU), which opti-mized the pre-trained model to preserve the overall genera-tive ability but not to generate the same identity within thelocal space. Experimental results demonstrated the effec-tiveness of GUIDE with promising outcomes. We antici-pate that our work will be widely applied in research or theindustry field, providing users with a sense of freedom fromprivacy concerns through identity removal.",
  "Acknowledgement": "This work was supported by MSIT (Ministry of Scienceand ICT), Korea, under the ITRC (Information TechnologyResearch Center) support program (IITP-2024-RS-2023-00258649) supervised by the IITP (Institute for Information& Communications Technology Planning & Evaluation),and in part by NRF-2023S1A5A2A21083590, and in partby the IITP grant funded by the Korea Government (MSIT)(Artificial Intelligence Innovation Hub) under Grant 2021-0-02068, and by the IITP grant funded by the Korea gov-ernment (MSIT) (No.RS-2022-00155911, Artificial Intelli-gence Convergence Innovation Human Resources Develop-ment (Kyung Hee University)).",
  "A.2. Distribution of Identities within CelebAHQ": "In designing GUIDE, we assume that images sharing thesame identity tend to cluster together. Consequently, con-sidering the proximity of latent codes aids in a more com-prehensive erasure of identity. illustrates the re-lationship between images and their respective identities,utilizing 5 images per identity. Our observation reveals aclose grouping of images from the same identity in the la-tent space. This finding aligns with the effectiveness of the",
  "A.3. Generative Identity Unlearning in AFHQv2": "In this section, we validated GUIDE in a different dataset- AFHQv2-Cat .We used the generator architecture and the GAN inversion network pre-trained onAFHQv2-Cat. The pre-trained weights are publicly avail-able at their official implementations. Since the identityloss used in our main experiment were designed to cap-ture the dissimilarities between identities in human faces,we only adopted to use the reconstruction loss and the per-ceptual loss in this experiment. The qualitative resultsare shown on . We could show the effectiveness ofGUIDE in a different domain - faces of cats. In , weadditionally show that GUIDE can preserve performance ofpre-trained model on AFHQv2-Cat.",
  ". Illustration of target images from source images with different d in In-domain (FFHQ) and Out-of-domain (CelebAHQ) scenario": "sualized target images derived from a given source imageat multiple d values. In , we utilized various dvalues to sample the corresponding target image in the Ran-dom scenario, while is for the In-domain and Out-of-domain scenarios. Our results illustrate that adjustingd allows us to obtain diverse target images. However, as mentioned in our main paper, target images derived frominterpolated latent code, where d is less than 0, exhibit sim-ilarity to the given source image. Conversely, target imageswith d 50 tend to be corrupted. Therefore, our choice ofd = 30 appears to strike a visually balanced representationfor the target image.",
  "A.7. Generative Unlearning in StyleGAN2": "In addition to our primary experiments employing a 3D gen-erative adversarial network as the generator architecture, weobserved the effectiveness of our framework in unlearningidentity in 2D generative adversarial networks. In this sec-tion, we utilized the widely-used StyleGAN2 as thebackbone architecture and pSp as a GAN inversionnetwork for latent code extraction from images. Both thebackbone and the GAN inversion network were pre-trainedon FFHQ . We refer to our framework built on top ofStyleGAN2 as GUIDE-SG2. In GUIDE-SG2, we employed images from the Style-GAN2 generator for calculating loss, instead of tri-planefeature maps.We present results of GUIDE-SG2 quali-tatively in and quantitatively in . Bothresults demonstrated GUIDE-SG2 successfully erased thegiven identity in a 2D GAN architecture with minimal im-pact on the performance of the pre-trained generator.",
  "0.0670.180 0.0807.875 2.0173.491 1.11820.030 0.0510.174 0.0817.882 1.9583.442 1.10440.031 0.0550.181 0.0757.705 1.8683.359 1.079": ". Ablation study to figure out the optimal Ng. To findoptimal, we performed the analysis with different Ng. As can beseen, when Ng increase, GUIDE can preserve the performanceof pre-trained model more effectively. When Ng = 2, GUIDEhave achieved a balanced performance in our metric. We usedCelebAHQ dataset in this experiment.",
  "B.1. Number of Latent Codes in Loss Functions": "In the computation of Ladj and Lglobal, as outlined in Sec-tion 3.3 of our main paper, we incorporated Na and Ng la-tent codes, respectively. In this section, we investigate theinfluence of varying Na and Ng. Due to an out-of-memoryissue in VRAM, these experiments were conducted on anNVIDIA A6000 GPU. presents the results of vary-ing Na in Ladj while keeping Ng fixed at 2. Our findingsindicate that using Na = 2 yields the best performance inerasing the given identity among different values of Na,while maintaining comparable performance in preservinggeneration quality. Conversely, in , we varied Ng inLglobal while keeping Na fixed at 2. Results show that usingNg = 2 achieves a balanced performance between erasingthe given identity and preserving generation performance.Importantly, all cases experimented upon outperformed thebaseline in generative identity unlearning task.",
  "B.2. Scaling Factors of Loss Functions": "In Llocal and Ladj, as proposed in .3 of our mainpaper, we set the scaling factors as L2 = 102 andid = 101. In this section, we conducted ablation stud-ies to determine the effective scaling factors.In , we varied L2 while fixing the other scalingfactors as default. For small L2, the generator architec-ture could not successfully erase the given identity. On theother hand, for larger L2, the generator architecture lostgeneration performance significantly. Based on these ob-servations, we decided to use L2 = 102 for balancedperformance. In , we varied id while fixing theother scaling factors as default. Our findings indicate thatusing id = 101 is the most effective.",
  "C. Additional Implementation Details": "Besides the .1 in our main paper, in this section,we additionally describe the implementation details thatare omitted in the main paper due to space limit.Weran GUIDE and the baseline using a single NVIDIAA5000 GPU. Erasing the given source identity usingGUIDE takes about 20 minutes.We utilized only asingle image to represent a certain identity;GUIDEunderwent 1,000 iterations throughout our experiments. Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle:A residual-based stylegan encoder via iterative refinement.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 67116720, 2021. 4",
  "Thomas Baumhauer, Pascal Schottle, and Matthias Zep-pelzauer.Machine unlearning: Linear filtration for logit-based classifiers.Machine Learning, 111(9):32033226,2022. 3": "Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagiel-ski, Vikash Sehwag, Florian Tramer, Borja Balle, DaphneIppolito, and Eric Wallace. Extracting training data from dif-fusion models. In Proceedings of the 32nd USENIX SecuritySymposium (USENIX Security 23), pages 52535270, 2023.2 Eric R. Chan, Connor Z. Lin, Matthew A. Chan, KokiNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, TeroKarras, and Gordon Wetzstein.Efficient geometry-aware3D generative adversarial networks.In Proceedings of",
  "Vikram S Chundawat, Ayush K Tarun, Murari Mandal, andMohan Kankanhalli. Zero-shot machine unlearning. IEEETransactions on Information Forensics and Security, 2023.3": "Jiankang Deng, Jia Guo, Niannan Xue, and StefanosZafeiriou. Arcface: Additive angular margin loss for deepface recognition. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 46904699, 2019. 5, 9 Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.Model inversion attacks that exploit confidence informationand basic countermeasures. In Proceedings of the 22nd ACMSIGSAC conference on computer and communications secu-rity, pages 13221333, 2015. 3 RohitGandikota,JoannaMaterzynska,JadenFiotto-Kaufman, and David Bau.Erasing concepts from diffu-sion models. In Proceedings of the 2023 IEEE InternationalConference on Computer Vision (ICCV), pages 24262436,2023. 2",
  "Antonio Ginart,Melody Guan,Gregory Valiant,andJames Y Zou. Making ai forget you: Data deletion in ma-chine learning. Advances in Neural Information ProcessingSystems, 32, 2019. 3": "Aditya Golatkar, Alessandro Achille, and Stefano Soatto.Eternal sunshine of the spotless net: Selective forgetting indeep networks. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages93049312, 2020. 2 Aditya Golatkar, Alessandro Achille, and Stefano Soatto.Forgetting outside the box: Scrubbing deep networks of in-formation accessible from input-output observations. In Pro-ceedings of the European Conference on Computer Vision(ECCV), pages 383398. Springer, 2020. 2 Aditya Golatkar, Alessandro Achille, Avinash Ravichan-dran, Marzia Polito, and Stefano Soatto. Mixed-privacy for-getting in deep networks. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 792801, 2021. Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth,Saeed Sharifi-Malvajerdi, and Chris Waites. Adaptive ma-chine unlearning. Advances in Neural Information Process-ing Systems, 34:1631916330, 2021. 3",
  "Jamie Hayes, Luca Melis, George Danezis, and EmilianoDe Cristofaro. Logan: Membership inference attacks againstgenerative models. arXiv preprint arXiv:1705.07663, 2017.1": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. Gans trained by atwo time-scale update rule converge to a local nash equilib-rium. Advances in Neural Information Processing Systems,30, 2017. 6 Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu,Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang.Curricularface: adaptive curriculum learning loss for deepface recognition. In proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 59015910, 2020. 6 Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.Progressive growing of gans for improved quality, stabilityand variation. In Proceedings of the International Confer-ence on Learning Representations (ICLR), 2018. 2, 6 Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2019. 1, 3, 5, 6 Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,Jaakko Lehtinen, and Timo Aila.Analyzing and improv-ing the image quality of stylegan.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2020. 2, 12 Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen,Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-freegenerative adversarial networks. Advances in Neural Infor-mation Processing Systems, 34:852863, 2021. 1, 2 Mahdi Khosravy, Kazuaki Nakamura, Yuki Hirose, NaokoNitta, and Noboru Babaguchi. Model inversion attack by in-tegration of deep generative models: Privacy-sensitive facegeneration from a face recognition system. IEEE Transac-tions on Information Forensics and Security, 17:357372,2022. 3",
  "Zhifeng Kong and Kamalika Chaudhuri.Data redactionfrom pre-trained gans.In Proceedings of the 2023 IEEEConference on Secure and Trustworthy Machine Learning(SaTML), pages 638677. IEEE, 2023. 3": "Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, EliShechtman, Richard Zhang, and Jun-Yan Zhu. Ablating con-cepts in text-to-image diffusion models. In Proceedings ofthe 2023 IEEE International Conference on Computer Vision(ICCV), pages 2269122702, 2023. 2 Ronak Mehta, Sourav Pal, Vikas Singh, and Sathya N Ravi.Deep unlearning via randomized conditionally independenthessians. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages1042210431, 2022. 2 Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, andGyeong-Moon Park. Online class incremental learning onstochastic blurry task boundary via mask and visual prompttuning. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision (ICCV), pages 1173111741,2023.",
  "Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng,Rana Hanocka, and Ben Y Zhao. Glaze: Protecting artistsfrom style mimicry by text-to-image models. arXiv preprintarXiv:2302.04222, 2023. 2": "Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. In-terpreting the latent space of gans for semantic face editing.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 92439252,2020. 2 Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou.Interfacegan: Interpreting the disentangled face representa-tion learned by gans. IEEE transactions on pattern analysisand machine intelligence, 44(4):20042018, 2020. 2 Gowthami Somepalli, Vasu Singla, Micah Goldblum, JonasGeiping, and Tom Goldstein. Diffusion art or digital forgery?investigating data replication in diffusion models. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 60486058, 2023. 2 Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, YongZhang, Hongwen Zhang, and Yebin Liu. Next3d: Genera-tive neural texture rasterization for 3d-aware head avatars. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 2099121002,2023. 2",
  "Laurens van der Maaten and Geoffrey Hinton. Visualizingdata using t-sne. Journal of Machine Learning Research, 9(86):25792605, 2008. 9": "Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, andQifeng Chen. High-fidelity gan inversion for image attributeediting.In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages1137911388, 2022. 2 Yue Wu, Yu Deng, Jiaolong Yang, Fangyun Wei, QifengChen, and Xin Tong. Anifacegan: Animatable 3d-aware faceimage generation for video avatars. Advances in Neural In-formation Processing Systems, 35:3618836201, 2022. 2 Yuting Xu, Jian Liang, Gengyun Jia, Ziming Yang, Yan-hao Zhang, and Ran He. Tall: Thumbnail layout for deep-fake video detection. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages2265822668, 2023. 1 Zhiyuan Yan, Yong Zhang, Yanbo Fan, and Baoyuan Wu.Ucf: Uncovering common features for generalizable deep-fake detection.In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision (ICCV), pages2241222423, 2023. 1 Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, ByeongkyuKang, and Jung-Woo Ha. Photorealistic style transfer viawavelet transforms.In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages90369045, 2019. 2",
  "Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, andHumphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591,2023. 2": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 586595, 2018. 5, 9 Xuanmeng Zhang, Zhedong Zheng, Daiheng Gao, BangZhang, Yi Yang, and Tat-Seng Chua. Multi-view consistentgenerative adversarial networks for compositional 3d-awareimage synthesis. International Journal of Computer Vision,pages 124, 2023. 2 Xiaoming Zhao, Fangchang Ma, David Guera, Zhile Ren,Alexander G Schwing, and Alex Colburn. Generative mul-tiplane images: Making a 2d gan 3d-aware. In Proceedingsof the European Conference on Computer Vision (ECCV),pages 1835. Springer, 2022. 2"
}