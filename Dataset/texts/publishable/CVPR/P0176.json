{
  "Abstract": "Dataset bias is a significant challenge in machine learn-ing, where specific attributes, such as texture or color of theimages are unintentionally learned resulting in detrimentalperformance. To address this, previous efforts have focusedon debiasing models either by developing novel debiasingalgorithms or by generating synthetic data to mitigate theprevalent dataset biases. However, generative approachesto date have largely relied on using bias-specific samplesfrom the dataset, which are typically too scarce. In thiswork, we propose, DiffInject, a straightforward yet power-ful method to augment synthetic bias-conflict samples usinga pretrained diffusion model. This approach significantlyadvances the use of diffusion models for debiasing purposesby manipulating the latent space. Our framework does notrequire any explicit knowledge of the bias types or labelling,making it a fully unsupervised setting for debiasing. Ourmethodology demonstrates substantial result in effectivelyreducing dataset bias.",
  ". Introduction": "Deep learning networks and algorithms often inadvertentlylearn biases from extensive benchmark datasets. These bi-ases, such as textures or colors, enable models to adoptshortcuts, leading to incorrect image classification.Forinstance, in a scenario where the majority of images de-picting alligators are set against backgrounds of rivers orponds, with scant examples of alligators on land, deep learn-ing classifiers may rely on background features (e.g., riverequates to alligator, land equates to horse) as shortcuts forclassification. In this context, we classify the background(river or land) as task-irrelevant features, whereas the sub-",
  "*Equal ContributionCorresponding author": "ject of the image (the alligator) is a task-relevant feature.Features such as skin tones, genders, and colors also consti-tute task-irrelevant features that can obstruct the classifiersability to accurately represent objects within an image.Prior approaches to reducing bias have explored super-vised learning techniques, which depend on the annotationof biases or labels. An alternative strategy involves unsu-pervised learning, where generative models are employedto enhance the biased data without prior knowledge of thebiases. This presents a significant advantage, as it suggeststhat bias mitigation should be performed without direct hu-man supervision.Diffusion-based models , have demonstratedsuperior performance compared to GANs in gener-ating synthetic images. In particular, variants of Stable Dif-fusion have achieved remarkable success in producinghigh-quality synthetic images. Recent research has shown that synthetic datasets generated by thesemodels can significantly contribute to the learning or en-hancement of visual representations in deep learning mod-els. Consequently, leveraging generative models to translatebias-conflict features and augment data presents a promis-ing avenue for enabling biased image classifiers to accu-rately learn and represent bias-conflict features.In this paper, we propose a novel framework, DiffInject,where we inject or translate bias-conflict features into thedata sample and generate synthetic dataset via leveragingthe diffusion model. Our framework is composed of fourmajor steps: 1) Overfit an image classifier into the biaseddataset and extract samples with top-K loss in which weassume are bias-conflict images, 2) Train a diffusion modelusing a pronounced image benchmark dataset that generallyencompasses the domain of the biased benchmark dataset,3) Translate or inject the bias-conflict features content byleveraging the h-space of the diffusion model and translat-ing the feature into the original bias-aligned image, 4) De-",
  "arXiv:2406.06134v1 [cs.CV] 10 Jun 2024": "bias the biased-classifier with the augmented dataset. In-jecting the content of the top-K loss samples will allowthe data to be translated and follow the distribution of bias-conflict images, allowing the biased model to capture thetask-relevant features.To the best of our knowledge, we believe our method isthe first to explore leveraging the diffusion model in modeldebiasing via unsupervised learning. Our extensive exper-iments demonstrate that style injection of top-K loss sam-ples to the original biased dataset allows to learn visual rep-resentation of biased-conflict samples extensively, thus de-biasing the classifier.",
  ". Preliminaries": "Extracting Samples with High Losses. To extract bias-conflict samples from the dataset, we select top-K loss sam-ples, where K denotes number of samples with top lossvalues, by intentionally overfitting the classifier where wenamed as bias classifier, fB. By utilizing generalized cross-entropy (GCE) loss , we train the classifier from scratchto maximize the representation to become biased by priori-tizing samples in the dataset that are easy-to-learn with high probability values which we believe are bias-align samples. Thus, GCE loss is formulated as follows:",
  "q,(1)": "where p(x; ), y) denotes softmax output of the classifier,py(x; ) is the probability of the target attribute y andq (0, 1] is a hyperparameter which controls the strengthof amplification to make the model easy-to-learn. Clas-sifiers parameters are denoted as . The GCE loss allowsthe classifier to gradually become biased by maximizing theweights on the gradients of the samples with higher proba-bility py, formulated as follows:",
  "(2)": "Overfitting the Classifier to become Biased.This al-lows the model have higher GCE losses and maximizes theloss when the model sees bias-conflict samples, eventuallylearning shortcuts provided by abundant bias-align samplesin the dataset.After overfitting the classifier to become biased, we areable to select samples with top-K losses by calculatingcross-entropy loss of all the train data output. We chose Kas 10 following the method of AmpliBias and for faircomparison with other generative model methods as well.Selection process of the top-K loss samples from the over-",
  "(k + SNR(t)) ,(6)": "where SNR(t) = t/(1 t), t = ts=1(1 s) fromstandard diffusion model notation . Both hyper-parameters and k are set as 1, where controls the degreeof learning perceptually rich contents and k determines thesharpness of the weighting scheme. We compare the qualityof generated images with baseline diffusion models and further validate the use of P2 weighting as our model forsynthetic data generation.",
  ". Injecting Biased Contents": "In our approach, we utilize images with high loss values togenerate synthetic bias-conflict samples by integrating theirbiased content into randomly chosen bias-aligned samples.To achieve this, we manipulate the bottleneck layer of theU-Net architecture, denoted as h-space , as a methodof content injection during the DDIM reverse process pro-posed in InjectFusion :",
  "hcontentt ht, (8)": "where horiginaltand hcontenttis the h-space of the originaland content image respectively, and denotes con-tent injection ratio.We apply content injection at both global and local lev-els. Local content injection is performed by masking thetargeted area of the h-space before applying Slerp, with theresulting interpolated ht subsequently inserted back into theoriginal feature map. This image editing process occursduring the early stage of the generative process [T, tedit]followed by the stochastic noise injection during interval[tboost, 0]. Then, we generate synthetic data samples to rep-resent a certain proportion of the overall dataset, with theterm bias-conflict ratio used throughout this paper. Fur-ther details are provided in Appendix B.",
  ". Training Unbiased Classifier": "Following the synthetic data generation method describedin .3, we construct an unbiased dataset Dsyn. Sub-sequently, we mitigate the bias in our biased classifier bytraining on a combined dataset comprising both syntheticand original data, denoted as Dtotal = Dsyn Dorig. Thisenables the model to learn more general visual representa-tions of task-relevant features within in the dataset, therebyenhancing the debiasing of the learning process. It is im-portant to note that labels for synthetic data is automaticallyassigned based on the bias-conflicted samples from whichthey were generated.",
  "Datasets We conduct experiments on four datasets withtheir matching class and bias attributes.Details are as": "follows: Colored MNIST: (Number-Color), CorruptedCIFAR-10: (Object-Noise), BFFHQ: (Age-Gender), Dogs& Cats: (Animal-Fur Color).Baselines We compare our method with vanilla network,LfF, DisEnt, BiasEnsemble, A2, and AmpliBias as ourbaselines. Vanilla network is defined as multi-layer percep-tron (MLP) with three hidden layers for Colored MNIST,and ResNet-18 for the remaining datasets.Implementation Details We pretrained ADM withP2-weighting on four widely recognized computer vi-sion benchmark datasets: MNIST, CIFAR-10, FFHQ, andAFHQ. Subsequently, we use the pretrained model weightsto apply InjectFusion on our benchmark datasets: ColoredMNIST (CMNIST), Corrupted CIFAR-10 (CCIFAR-10),BFFHQ, and Dogs & Cats. Further implementation detailsare described in Appendix B.",
  ". Analysis": "Quantitative Results presents image classificationaccuracies on the unbiased test sets of synthetic datasets andreal world datasets, where the ratio of bias-conflicting sam-ples is varied at 1% and 5%. Most notably, DiffInject out-performs baseline methods such as LfF and DisEnt on real world datasets by a substantial margin. DiffInjectalso achieves state-of-the-art performance on CMNIST, butlacks performance in CCIFAR-10. It is important to notethat our method does not require prior knowledge in biastypes and manual labeling of synthetic data, yet demonstrat-ing superior performance across most benchmark datasetscompared to the baselines.Qualitative Results We analyze the quality of syntheticsamples generated from DiffInject, as illustrated in .The three columns depict bias-aligned samples from theoriginal dataset, original samples with top-K loss, andwell-generated bias-conflict samples via DiffInject, respec- tively.Synthetic data generated with DiffInject providehigher quality and realism, facilitating the model to learndebiased representation. For instance, the generated sam-ple in BFFHQ effectively captures the bias-conflict at-tribute of young-male, enriching the visual features ofthe dataset. Successful injection of bias-conflict attributesis also demonstrated in CMNIST and Dogs & Cats datasets.Limitations demonstrates synthetic samples gen-erated with artifacts. To ensure fair comparison and pre-vent selective sampling of synthetic data, we included bias-conflict data with artifacts in Dsyn for training the classi-fier. For instance, in the case of Dogs & Cats, the generatedsample with artifacts fails to transfer the dark fur color ob-served in the bias-conflict samples. However, the presenceof artifact may have positively contributed to the debiasingprocess, as the mixed fur color (black & white) enables themodel to learn diverse visual representation. For CCIFAR-10, our generated sample fails to effectively capture the biasattributes, which may have negatively impacted the modelperformance as demonstrated in . Further works canbe explored to successfully extract and inject texture biasattributes presented in CCIFAR-10.",
  ". Conclusion": "We propose, DiffInject, a novel framework for debiasing theimage classifier by augmenting synthetic data through se-mantic manipulation of the latent space within the diffusionmodel. Our approach eliminates the need for manual label-ing of synthetic data and explicit knowledge of bias types inthe samples, yet generating high quality bias-conflict syn-thetic samples. With our results demonstrating significantperformance increase over benchmark datasets, we believeour work inspires the future work of leveraging diffusionmodels in model debiasing. AcknowledgementThis research was supported by the MSIT(Ministry of Sci-ence and ICT), Korea, under the ICAN(ICT Challengeand Advanced Network of HRD) support program(IITP-2024-RS-2023-00259497) supervised by the IITP(Institutefor Information & Communications Technology Planning& Evaluation), Institute of Information & communicationsTechnology Planning & Evaluation (IITP) grant fundedby the Korea government(MSIT) (No.RS-2023-00254129,No.RS-2023-00230561, Development of ConversationalIntegrated AI Search Engine based on Multi-modal Tech-nology),Graduate School of Metaverse Convergence(Sungkyunkwan University)), Aim Future, Minds and Com-pany, and PseudoLab. Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim,Daesik Kim, Seung-Hun Nam, and Kibeom Hong. Dream-styler: Paint by style inversion with text-to-image diffusionmodels. arXiv preprint arXiv:2309.06933, 2023. 1 Jaeju An, Taejune Kim, Donggeun Ko, Sangyup Lee, andSimon S Woo. A 2: Adaptive augmentation for effectivelymitigating dataset bias. In Proceedings of the Asian Confer-ence on Computer Vision, pages 40774092, 2022. 2, 1",
  "Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-hammad Norouzi, and David J Fleet. Synthetic data fromdiffusion models improves imagenet classification.arXivpreprint arXiv:2304.08466, 2023. 1": "Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo,and Seong Joon Oh. Learning de-biased representations withbiased representations. In International Conference on Ma-chine Learning, pages 528539. PMLR, 2020. 1 Jooyoung Choi, Jungbeom Lee, Chaehun Shin, SungwonKim, Hyunwoo Kim, and Sungroh Yoon.Perception pri-oritized training of diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1147211481, 2022. 2, 1",
  "Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi,Phillip Isola, and Yonglong Tian.Scaling laws of syn-thetic images for model training... for now. arXiv preprintarXiv:2312.04567, 2023. 1": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.An image is worth one word: Personalizing text-to-image generation using textual inversion.arXiv preprintarXiv:2208.01618, 2022. 1 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. Advances inneural information processing systems, 27, 2014. 1",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural informationprocessing systems, 33:68406851, 2020. 1, 2": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.Lora: Low-rank adaptation of large language models. arXivpreprint arXiv:2106.09685, 2021. 1 Jaeseok Jeong, Mingi Kwon, and Youngjung Uh. Training-free content injection using h-space in diffusion models. InProceedings of the IEEE/CVF Winter Conference on Appli-cations of Computer Vision, pages 51515161, 2024. 2, 1 Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,Jaakko Lehtinen, and Timo Aila.Analyzing and improv-ing the image quality of stylegan.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 81108119, 2020. 1 Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim,and Junmo Kim. Learning not to learn: Training deep neuralnetworks with biased data. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 90129020, 2019. 1",
  "Kangyeol Kim, Sunghyun Park, Junsoo Lee, and JaegulChoo.Reference-based image composition with sketchvia structure-aware diffusion model.arXiv preprintarXiv:2304.09748, 2023. 1": "Donggeun Ko, Dongjun Lee, Namjun Park, Kyoungrae Noh,Hyeonjin Park, and Jaekwang Kim. Amplibias: Mitigatingdataset bias through bias amplification in few-shot learningfor generative models. In Proceedings of the 32nd ACM In-ternational Conference on Information and Knowledge Man-agement, pages 40284032, 2023. 2, 1 Nupur Kumari, Bingliang Zhang, Richard Zhang, EliShechtman, and Jun-Yan Zhu. Multi-concept customizationof text-to-image diffusion. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 19311941, 2023. 1",
  "Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusionmodels already have a semantic latent space. arXiv preprintarXiv:2210.10960, 2022. 2": "Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, andJaegul Choo. Learning debiased representation via disentan-gled feature augmentation. Advances in Neural InformationProcessing Systems, 34:2512325133, 2021. 4, 1 Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.Gligen: Open-set grounded text-to-image generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2251122521, 2023. 1 Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, JianZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.T2i-adapter:Learning adapters to dig out more controllableability for text-to-image diffusion models.arXiv preprintarXiv:2302.08453, 2023. 1 Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, andJinwoo Shin. Learning from failure: De-biasing classifierfrom biased classifier. Advances in Neural Information Pro-cessing Systems, 33:2067320684, 2020. 2, 4, 1",
  "Alexander Quinn Nichol and Prafulla Dhariwal. Improveddenoising diffusion probabilistic models.In InternationalConference on Machine Learning, pages 81628171. PMLR,2021. 1, 2": "Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros,Yong Jae Lee, Eli Shechtman, and Richard Zhang. Few-shotimage generation via cross-domain correspondence. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1074310752, 2021. 1 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2250022510, 2023. 1 Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, andPercy Liang.Distributionally robust neural networks forgroup shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.1 Enzo Tartaglione, Carlo Alberto Barbano, and MarcoGrangetto.End: Entangling and disentangling deep rep-resentations for bias correction.In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, pages 1350813517, 2021. 1",
  "A. Related Work": "Previous Debiasing Methods.Prior approaches to de-biasing have employed supervised training using explic-itly defined bias labels . These methods ex-tracted bias features and attributes from datasets under theassumption that bias labels were explicitly predefined. Re-cent works has sought to tackle biases without depend-ing solely on pre-established bias labels.Instead, thesestrategies aim to reduce human intervention through tech-niques such as augmentation and re-weighting of properties.LfF pinpoints bias-conflict samples by deploying twoconcurrently trained and updated models, fD and fB, withthe debiased model fD adjusting CE loss based on a rela-tive difficulty score. Rebias strives to mitigate bias bydisentangling and interchanging features within the latentspace. A2 leveraged StyleGAN to produce aug-mented bias-conflict samples through a few-shot adaptationmethod . AmpliBias employed FastGAN for few-shot learning in generating synthetic bias-conflict samples.Yet, no further exploration on diffusion models were under-gone in context of debiasing.Content injection using diffusion models Numerousmethods has been introduced to enhance controllability inimage generation through diffusion-based models . Recent works have explored the incorporation of ei-ther text guidance or structure guid-ance as a method of content injection. How-ever, these approaches typically rely on textual descriptionsor structure maps as conditioning inputs. Concurrently, al-ternative methodologies propose the utilization ofreference images for image editing guidance. In contrast,InjectFusion explores a novel approach by leveragingthe latent space of a frozen, pretrained diffusion model as ameans of content injection from a reference image. We fur-ther investigate the exploitation of the semantic latent spaceas a source of control to generate synthetic images, aimingto mitigate biases in classification tasks.",
  "B.2. Injecting Biased Contents": "The parameter tedit is empirically defined such thatLPIPS(x, Ptedit) = 0.33, while tboost is fixed as 200. Weset content injection ratio as 0.9, 0.3, 0.7, and 0.2 for CM-NIST, CCIFAR-10, BFFHQ, and Dogs & Cats, respectively.We apply local content injection for CMNIST, BFFHQ, andDogs & Cats, and global content injection for CCIFAR-10.Bias-conflict ratio is set as 0.6 for BFFHQ and Dogs & Cats,and 0.1 for CMNIST and CCIFAR-10. Ablation studies onbias-conflict ratio can be explored in future work.InjectFusion takes approximately 7-10 seconds (2-3seconds for computing inversion for each of the two imagesand applying content injection, respectively) per generatedsample for BFFHQ and Dogs & Cats, and 90 seconds (30seconds for computing inversion for each of the two imagesand applying content injection, respectively) per generatedsample for CMNIST and CCIFAR-10, based on NVIDIAA100 and NVIDIA H100 GPUs. We use multiprocessingto accelerate the content injection process.",
  "B.3. Training Unbiased Classifier": "We implement the preprocessing techniques described inDisEnt : We apply random crop and horizontal fliptransformations for CCIFAR-10 and BFFHQ, and applynormalization with the mean of (0.4914, 0.4822, 0.4465)and standard deviation of (0.2023, 0.1994, 0.2010) for eachchannel. We do not implement any augmentations for CM-NIST and Dogs & Cats. We use cross entropy loss as ourloss function, and use Adam optimizer with the learningrate of 0.001 for CMNIST and CCIFAR-10, and 0.0001 forBFFHQ and Dogs & Cats.",
  "B.4. Additional Generated Synthetic Images": "In this section, we provide additional samples generatedfrom DiffInject. includes synthetic samples forCMNIST and CCIFAR-10. and includesgenerated samples for BFFHQ and Dogs & Cats, respec-tively. Each figure consists of three columns representing,from left to right, the original samples from the dataset,top-k loss samples, and synthetic samples generated fromDiffInject, respectively. . Examples of generated bias-conflict samples with DiffInject for CMNIST and CCIFAR-10 dataset. The three columns representsamples from the original dataset, top-k loss samples and generated samples, respectively."
}