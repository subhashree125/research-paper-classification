{
  "Abstract": "State-of-the-art approaches for autonomous driving in-tegrate multiple sub-tasks of the overall driving task into asingle pipeline that can be trained in an end-to-end fash-ion by passing latent representations between the differentmodules. In contrast to previous approaches that rely ona unified grid to represent the belief state of the scene, wepropose dedicated representations to disentangle dynamicagents and static scene elements. This allows us to explic-itly compensate for the effect of both ego and object motionbetween consecutive time steps and to flexibly propagate thebelief state through time.Furthermore, dynamic objectscan not only attend to the input camera images, but alsodirectly benefit from the inferred static scene structure viaa novel dynamic-static cross-attention. Extensive experi-ments on the challenging nuScenes benchmark demonstratethe benefits of the proposed dual-stream design, especiallyfor modelling highly dynamic agents in the scene, and high-light the improved temporal consistency of our approach.Our method titled DualAD not only outperforms indepen-dently trained single-task networks, but also improves overprevious state-of-the-art end-to-end models by a large mar-gin on all tasks along the functional chain of driving.",
  ". Introduction": "Autonomous systems have evolved from strictly modularand largely hand-crafted pipelines towards a more holisticlearning-centric paradigm . While the former relieson explicitly defined interfaces between modules, the lat-ter tackles the entire driving task in an end-to-end fashion.Nevertheless, recent work has shown the benefits of retain-ing a modular structure including typical sub-tasks such asperception, prediction and planning while allowing latentfeatures to serve as interfaces between the modules .In contrast to independent, task-specific modules withfixed pre-defined interfaces, an end-to-end approach en-ables the joint optimization of the entire pipeline, learningnot only the parameters in each module but also the inter-",
  "Dual Stream-based(Ours)": ". Comparison of Representation Design of unified grid-based approaches and our dual-stream design. By explicitly dis-entangling dynamic and static representations, the dynamic streamcan aggregate highly descriptive features. This is achieved throughdirect attention to image features, as well as explicit compensationfor object and ego motion, which is not feasible with unified grids. faces between them. The chosen space of each moduleslatent representation restricts the set of interfaces whichcan be learned, allowing to model inductive biases aboutthe scene structure, e.g. consistent motion of dynamic ele-ments, or to incorporate task specific properties. However,this places additional importance on choosing well-suitedintermediate representations, since they heavily affect in-formation flow and the performance of subsequent modules.Hence, these representations should be carefully tailored totheir corresponding semantic entities in the driving scene toachieve a high performing end-to-end architecture.",
  "arXiv:2406.06264v1 [cs.CV] 10 Jun 2024": "To model dynamic agents in the scene, a prevalent ap-proach is to leverage attention with object-centric queriesthat detect an individual object in the environment . Furthermore, recent works have demon-strated the benefits of incorporating temporal informationto consistently model object dynamics and to account fortemporal occlusions. In such work, object-queries providededicated latent representations that each describe a singleobject. Its belief state can then be propagated through timeby explicitly compensating for the ego and estimated objectmotion between two consecutive timestamps .The most common alternative is to use birds-eye view(BEV)-grid queries as an intermediate repre-sentation, with subsequent tasks relying solely on this rep-resentation. However, such a grid is not coupled to semanticinstances and instead represents a spatial area of the scene.Hence, the motion of agents cannot be explicitly modeledand compensated for, see . This is due to the fact thateach grid-cell could potentially represent multiple entitieswith different rigid motion transforms or even completelystatic elements, depending on grid resolution and objectsizes. While grid-based representations are well-suited forstatic world perception , exclusively relying on themto aggregate sensor measurements and temporal informa-tion hampers the perception of highly dynamic agents. Contributions: In this work, we propose a dual-stream ap-proach to leverage the potential of object-centric representa-tions for dynamic agents combined with a BEV-grid repre-sentation for static scene elements. This dual-stream designexplicitly applies object and ego motion compensation todynamic agents and allows object-queries and BEV-queriesto simultaneously attend to the camera images of the currenttimestamp. Besides self-attention and cross-attention withcamera images, we introduce a new dynamic-static cross-attention-block that allows object-queries to attend to theBEV-queries fostering the consistency between the streams.Our proposed approach termed DUALAD allows for ro-bust and temporally consistent perception.On the chal-lenging nuScenes dataset DUALAD outperforms spe-cialized state-of-the-art (SOTA) models for various percep-tion tasks by a large margin. The integration with recentend-to-end frameworks reveals the importance ofdisentangled representations for dynamic agents and staticworld elements, and exhibits significant performance gainsalong the entire functional chain. Extensive ablation stud-ies highlight the importance of the dual-stream design forall driving tasks, while especially improving temporal con-sistency and the perception of highly dynamic agents.",
  "Accurate and consistent perception forms the basis for au-tonomous driving. We structure related literature into three": "categories: (i) specialized models for dynamic agents thatperform 3D object detection and 3D multiple object track-ing, (ii) models that reason about static scene elementsand perform online mapping, and (iii) multi-task end-to-endmodels that jointly perform the aforementioned tasks in asingle model and can be optimized end-to-end. Perception of Dynamic Agents:Based on pioneeringworks , recent specialized models for 3D object de-tection utilize a transformer-based architecture with a set ofobject-queries to detect objects in the scene .Several extensions have been proposed e.g. to reduce thememory footprint and to increase the convergence speed, re-sulting in improved overall performance . Incorpo-rating temporal information for the perception of dynamicagents can be achieved by query propagation and there-fore implicit tracking combined with varioustracking-by-detection approaches , or by tracking-by-attention . For such query propagation, it is cru-cial to follow an object-centric paradigm. This allows to ag-gregate descriptive features for each object by performingattention directly between object-queries and sensor mea-surements, as well as explicitly compensate for the motionof objects between consecutive time steps .Another line of work utilizes an intermediate grid ofBEV-queries to propagate information through time . In such approaches, each BEV-query always representsthe same area in the grid and is not coupled to a specificsemantic element. Dynamic agents are then detected usingqueries attending to this grid. However, since compensatingfor the motion of dynamic agents in the grid is not directlypossible, we opt for an object-centric approach to modeldynamic agents in our dual-stream design. Perception of Static Scene Elements: Inspired by recentworks on 2D panoptic segmentation , current worksthat perform online map segmentation rely on BEV-grid-queries, coupled with a transformer-decoder architecture toperform BEV map segmentation .Another class of approaches tries to model map percep-tion tasks in a vectorized fashion, where map elements aredirectly modeled as a sequence of points, e.g. by leveragingmap queries . As both variants rely on a tem-poral BEV-grid to achieve a temporally consistent perfor-mance, we follow this concept for static world perception. Multi-Task End-to-End Models: Most recently, differentapproaches proposed to model the driving task as amodular pipeline that is trainable end-to-end. This allows tooptimize the individual modules as well as their interfacestowards the final driving task. The modules are typicallyconnected by transformer mechanisms, effectively defininginterfaces in terms of query, key and value triplets.Inspired by the aforementioned works, we propose adual-stream transformer that can be used as the foundation for various perception tasks as well as for end-to-end multi-task driving. We simultaneously use object-centric queriesto represent dynamic agents in the scene, while modellingstatic scene elements with BEV-grid-queries. This explic-itly disentangles the representation of static and dynamicelements in the scene, resulting in a higher temporal consis-tency, especially for highly dynamic agents. The resultingarchitecture combines the potential of SOTA approaches fordynamic object perception as well as static perception ina single model, and can directly be integrated with recentmulti-task models to train the entire stack end-to-end.",
  ". Method": "As shown in a, our proposed approach DUALADcomprises a transformer-decoder-based perception architec-ture that uses two streams to explicitly model dynamic ob-jects in an object-centric and static scene elements in a grid-based fashion. The resulting dynamic and static world rep-resentations enable various tasks relevant to driving suchas 3D object detection and tracking, map segmentation,motion prediction as well as planning. Furthermore, ourapproach permits an end-to-end optimization of the entiredriving stack as proposed in .At each time step t a set of N multi-view camera im-ages It is fed into a shared image feature extractor. Theresulting image features Ft are used by both, the dynamicobject as well as the static stream.The former reasonsabout dynamic agents in the scene, like cars or pedes-trians.These agents are represented by a set of object-queries qobj Qobj that can be decoded into a boundingbox bt =x, y, z, w, l, h, , vx, vyas well as the predictedclass c of the agent.In parallel, a grid of BEV-queriesqBEV QBEV with dimensions HBEV WBEV uses Ft toreason about the static scene. The resulting BEV represen-tation is used to perform panoptic segmentation of the roadtopology, e.g. drivable space or lane markings, utilizing asegmentation head as proposed in .Interaction between the two streams is enabled by noveldynamic-static cross-attention blocks (see b) wherethe object-queries qobj attend to the BEV-queries qBEV rep-resenting the static scene structure. Since the temporal rep-resentations for the dynamic and the static world are disen-tangled, we can explicitly compensate object and ego mo-tion for the object-centric queries of dynamic agents whilethe static BEV-queries only require a propagation that is de-pendent on the motion of the ego vehicle.",
  ". Dual Stream Design for End-to-End Driving": "Finding a well-suited representation for the belief state ofthe scene is key for any transformer-based end-to-end train-able driving stack as motivated in . In comparisonto traditional pipelines, the end-to-end paradigm allows theinterfaces to be optimized towards subsequent modules in the pipeline. Nevertheless, the chosen space of latent rep-resentations heavily affects the ability to model the relevantsemantic entities and their relations .Whereas unified BEV-grid representations can appropri-ately handle static content, representing highly dynamic ob-jects in a BEV-grid is ill-posed, since each cell might de-scribe multiple entities with different motion patterns, staticscene elements or even a combination of both. We thereforeargue that dynamic objects and static scene content shouldbe represented separately, and propose a dual-stream archi-tecture consisting of a dynamic and a static stream. Dynamic Stream: In DUALAD dynamic objects are mod-elled with an object-centric representation by using a sin-gle object-query qobj to describe an individual object in thescene . To obteain a highly descriptiverepresentation, we propose that each object-query shoulddirectly perform cross-attention to the image features Ft.In contrast to unified methods in which only the BEV-gridqueries directly attend to images , this enablesto exploit the high spatial resolution of the image featuresfor more precise detection and tracking.Following the arguments in , we propose to prop-agate the latent queries qobj to the next timestamp by com-pensating for the motion between the two timestamps viaa latent transformation that depends on the geometric mo-tion.In contrast to static scene parts, the observed mo-tion of objects consists of two separate components: (1) themotion egot+1Tegot of the ego vehicle and (2) the motionobjt+1Tobjt of the dynamic object itself. For more detailson query propagation, we kindly refer the reader to .In detail, our approach uses the top-k propagated object-queries of each time step as priors in the subsequent frame,following an implicit tracking approach as in Stream-PETR to account for temporary occlusions and to con-sistently track objects in the scene. In contrast to tracking-by-attention in which only matched objects arepropagated to the next frame, this allows our model to main-tain multiple hypotheses for the same object and does notrequire explicit track handling. To obtain explicit objectidentities, our model can be combined with any tracking-by-detection approach. Static Stream: We use a BEV grid-based representation tomodel static scene elements. A dense, spatially regular rep-resentation is well-suited for non-moving objects in the sur-rounding area. Since all elements in the grid are assumed tobe static, updates over time are incorporated by applying arigid transform to the BEV-grid computed from the ego mo-tion egot+1Tegot. We sample grid features differentiably viainterpolation and use deformable temporal grid-attention asproposed in . Map segmentation is then performed witha decoder-only segmentation head . This signifi-cantly simplifies the map segmentation head as compared",
  "(b) Dual-stream transformer": ". DUALAD Architecture: Two separate representations are chosen for dynamic agents (Qobj) and static elements (QBEV) as shownin a. Self- and cross-attention is simultaneously performed in the proposed dual-stream transformer as shown in b, paired withthe novel dynamic-static cross-attention block to allow the dynamic agents to benefit from the inferred scene structure.",
  ". Modelling Interactions between the Dynamicand Static World": "Explicitly disentangling dynamic agents and static scene el-ements results in two independent streams of the model.Both streams rely on shared image features Ft, but per-form self-attention and cross-attention to these features sep-arately. To enable the network to leverage mutual infor-mation between static scene elements and dynamic agents,we propose an additional attention block that performsdynamic-static cross-attention between the streams.As shown in b, this is achieved by performingdeformable attention between object-queries qobj andBEV-queries qBEV of the current timestamp that are closeto the position of the object . In doing so, the dynamicobjects can infer their state update more precisely by con-sidering not only the sensor information but also the aggre-gated static BEV-grid, e.g. by incorporating the estimatedinformation on road layout and lane topology. Movable Belief State Through Space and Time:Ourdual stream design enables incorporating even unsynchro-nized sensor input. Whenever sensor information is avail-able, potentially at arbitrary time intervals, the belief stateof static and dynamic parts can be propagated to that times-tamp considering ego and object motion. The novel sensordata is then easily integrated via cross-attention to the avail-able image features to update the inferred scene state.Hence, our approach facilitates incorporating sensormeasurements at different points in time while simultane-ously keeping a temporally consistent representation of thescene. Our model can also handle cases, where the set ofsensors varies at each time step.This is especially rel-evant for non-synchronized sensors, e.g. due to differentsensing rates, or even sensor failures. Additionally, this en-ables to use ground truth annotations that are synchronized",
  ". Experiments": "We evaluate the performance of DUALAD on the challeng-ing and well-established nuScenes dataset . Additionally,we integrate our proposed approach into two SOTA end-to-end trainable driving frameworks, i.e. UniAD andVAD . We perform extensive ablation studies to evalu-ate the effect of our design choices and provide additionalinsights as well as qualitative results. Dataset:We utilize the large-scale nuScenes dataset consisting of 1000 scenes and use the official train- andval-set split. We adopt the official task definitions for theobject detection task and object tracking task , re-spectively, and follow other recent works for thedefinition of the motion prediction and planning objectives. Metrics:For object detection, we report the main met-rics mean Average Precision (mAP) and nuScenes Detec-tion Score (NDS) computed on all ten classes of the dataset,as well as true positive metrics such as the mean AverageTranslation Error (mATE), mean Average Orientation Er-ror (mAOE), and mean Average Velocity Error (mAVE) asdefined in . For object tracking, we follow the officialmetric definition in and report Average Multi ObjectTracking Accuracy (AMOTA) and Average Multi ObjectTracking Precision (AMOTP) as well as recall and num-ber of identity switches (IDS). For map segmentation, weclosely follow and report BEV segmentation Intersec-tion over Union (IoU) for different classes. We refer thereader to for additional details on the metric and classdefinitions. For motion prediction, we report the End-to-end Prediction Accuracy (EPA) as main metric as wellas the true positive metrics minimum Average DisplacementError (minADE) and minimum Final Displacement Error (minFDE). For open-loop planning, we report the L2 dis-tance to the ego trajectory as well as collision rates for 1 sand 3 s, respectively. A more detailed evaluation includingadditional metrics for the different configurations of our ap-proach can be found in the supplementary. Training Configuration:We closely follow the settingsin to increase comparability. Unless otherwisespecified, we utilize a VovNet-V2-99 and an image res-olution of 800320 pixels. For details on the backbone andFPN configuration, we refer the reader to . Fol-lowing , we utilize streaming video training. All modelsare trained for 24 epochs utilizing a batch size of eight oneight NVIDIA A100 GPUs with AdamW , a learningrate of 2e4 and a cosine annealing schedule. DUALADperforms object detection, tracking, map segmentation, andmotion prediction, as well as open-loop planning. Note thatall tasks are performed jointly in one multi-task model. Fol-lowing , we train the model in a two-stage fashion: thestage-I model only performs object detection, tracking andmap segmentation, while the stage-II model is optimized forall tasks in an end-to-end fashion with a frozen image back-bone for numerical stability. In all perception experiments,we append -I or -II for clarity e.g. DUALAD-II. Baselines:Given that our approach follows the object-query propagation technique used by StreamPETR ,and considering that StreamPETR reaches SOTA perfor-mance on the nuScenes benchmark , we choose it as themain baseline for dynamic object perception. To demon-strate the performance gains of our proposed architecturealong the entire functional chain, we evaluate the perfor-mance of DUALAD on downstream tasks for driving, suchas motion prediction and open-loop planning. We choosethe two recent SOTA approaches UniAD and VAD as baselines since they perform all tasks in an end-to-endtrainable fashion while also following the two-stage trainingparadigm. If not all metrics are reported in the correspond-ing publications, we utilize the published training logs andcode to reproduce the results. For tasks without an officialbenchmark, we adopt the evaluation scheme of . Ithas to be noted that VAD utilizes a ResNet-50 and asmaller detection range for the perception and motion pre-diction tasks. For all comparisons with VAD , we con-figure our model to exactly follow their settings for a faircomparison. We refer the reader to for additionaldetails on the configuration and used detection ranges.",
  "UniAD-II38.10.680.380.3849.8DUALAD-II48.10.570.410.2856.6": "pared to other multi-task models like UniAD , DU-ALAD yields an improvement of +10 (+20 %) in terms ofmAP and +7.2 (+12 %) in NDS respectively. Comparedto StreamPETR , which is only trained on object de-tection, DUALAD gains an improvement of +1.3 mAP and+0.7 NDS, resulting in SOTA performance for object detec-tion. While our model builds on StreamPETR, we attributethe key improvements over StreamPETR to the newly in-troduced dynamic-static cross-attention that allows object-queries to benefit from the static scene structure. Map Segmentation:The results for map segmentationof different classes are shown in . DUALAD yieldscomparable performance to SOTA approaches on all classeswhile improving the lane segmentation by +2.9 IoU ascompared to UniAD using the same image backbone. Asindicated in , the observed improvements in lane seg-mentation are vital for the accurate perception of dynamicagents. We want to highlight that our two-stream designallows the use of a single static BEV encoder instead ofhaving a unified BEV encoder and an additional static mapencoder. This results in 2.7 M (15 %) fewer parametersfor map segmentation as compared to UniAD. Multiple Object Tracking: The tracking performance ofour model is shown in . In contrast to explicit track-ing as in our model only performs implicit trackingthrough query propagation. We utilize the widely adoptedtracking approach presented in for a fair comparison toStreamPETR . DUALAD reaches SOTA performancein terms of AMOTA and outperforms specialized trackingapproaches like PF-Track by a large margin. Com-pared to UniAD , our approach heavily improves theAMOTA by +15.8 (+28 %) and by +2.5 (+4 %) comparedto the specialized StreamPETR, respectively. In particular,the dual-stream layout leads to a higher temporal consis-tency of tracked objects, manifesting in a reduction of IDSby 25 % compared to UniAD and StreamPETR . . Map Segmentation. DUALAD achieves competitive per-formance, especially improving the segmentation of lanes. indi-cates a version that uses a ResNet-101 as in UniAD , aversion only trained on the mapping task without dynamic agents.We report performance for perception as well as stage-II results forend-to-end models. *Results taken from official repository .Segmentation IoU(%) is reported for different classes.",
  ". Integration to End-to-End Pipelines": "To demonstrate the performance gains for downstreamtasks like motion prediction and planning, we integrateour proposed dual-stream architecture into recent end-to-end trainable driving frameworks that reach SOTA results:UniAD and VAD . Due to the training focus onthe final motion prediction and planning performance, weobserve, similar to UniAD , a slight degradation in per-ception performance when training in the end-to-end settingas shown in , and . . Motion Prediction. DUALAD remarkably improves themotion prediction task within different frameworks. V denotes thevehicle category and P pedestrians, respectively. *Results takenfrom official repository . Please note that VAD andhence our integration DUALVAD use a smaller detection rangeand that results are not directly comparable with other approaches.",
  "VAD0.411.050.720.070.410.22DUALVAD0.300.820.550.110.360.22": "Motion Prediction: The results for motion prediction areshown in .Our model significantly outperformsUniAD by +6.8 (+12 %) EPA for the vehicle class and by+9.7 (+21 %) for pedestrians, respectively. A similar effectis observed for the vectorized framework VAD whereour model improves motion prediction by +5.1 (+7 %)EPA for vehicles. We attribute this to improved modellingof dynamic agents in the scene and improved motion queuesby direct object to image attention. Open-Loop Planning:The evaluation of the open-loopplanning performance is provided in . While we re-port those results for completeness, we want to highlightthe issues on open-loop planning in nuScenes recentlyidentified in . We integrate our approach into the plan-ning modules proposed in and that do not use theego status as direct input to planning and follow their cor-responding evaluation protocols . As shown in Ta-ble 5, DUALAD reaches comparable performance in termsof L2 distance compared to UniAD and heavily re-duces the collision rate up to a factor of two for longer plan-ning horizons. Similarly, for VAD the L2 error is re-duced by up to 0.23 m (21 %) depending on the planninghorizon. This is in line with our model design since improv- . Ablation on the Interaction Design. denotes UniAD with the detection head of StreamPETR instead of the tracking-by-attention head proposed in . denotes a variant of our approach without stream interaction and a variant that uses bidirectionalstream interaciton.",
  "ing temporal consistency, especially for dynamic agents,might become more relevant for longer planning horizons": "Qualitative Results: visualizes the performance ofDUALAD in a complex traffic scene. The proposed dual-stream design enables a temporally consistent perceptionof the surrounding area, including highly dynamic agentsand allows for precise motion prediction and planning. Acomparison to the perception of UniAD with respect tohighly dynamic agents is shown in . Additional exam-ples for both integrated frameworks can be foundin the supplementary.",
  ". Ablations": "Effect of Interaction Design: An evaluation of the differ-ent design choices of DUALAD is shown in . First,using a version of our approach with two separate streamswithout the proposed dynamic-static cross-attention mod-ule especially decreases the object detection and trackingperformance. This observation confirms the benefits of theinteraction with the static branch for dynamic agents. Sec-ond, using a bidirectional stream interaction, where alsothe static BEV-queries attend to the dynamic agents, doesnot yield significant improvements.This is in line withour hypothesis that it is sufficient for the static world rep-resentation to perform cross-attention to the images pairedwith temporal self-attention. Third, we incorporate Stream-PETRs query propagation as used in our approachto UniAD , which yields consistent improvements butheavily increases the IDS which might be a result of thesimple greedy tracker .Fourth, DUALAD benefitsas expected from using temporal attention for the BEV-queries, but we observe the opposite for UniAD . Aversion without temporal attention within the unified BEV-grid in UniAD has a performance for dynamic agent per-ception that is increased by +3.7 NDS and 4.4 AMOTA, re- . Ablation on Temporal Consistency. DUALAD achievesconsistent tracks even without sensor measurements from eachsensor in each frame. We mimic the effect of non-synchronizedsensors by using the front and back facing cameras in an alternat-ing fashion (denoted by ).",
  "spectively, while IDS are drastically reduced by 55 %. Thissupports our claim that the unified grid is not well-suited topropagate information about dynamic agents through time": "TemporalConsistency,Non-SynchronizedSensors:Since our proposed dual-stream design can flexibly prop-agate the belief state through time, we run DUALAD in asetting with non-synchronized sensors. To do so, we splitthe camera images into two sets Cfront containing the threefront-facing cameras and Cback for the rear cameras respec-tively. We use the inputs of Cfront and Cback in an alter-nating fashion, leading to three camera inputs per time stepand an effective refresh rate per camera of 1 Hz. The re-sulting performance is shown in . The restriction ofsensor data to one half of the scene per time step decreasesthe performance for both approaches. We observe that theIDS of UniAD doubles, while DUALAD keeps consis-tent tracks with only 29 % increase in IDS. This observationagain highlights the effectiveness of the dual-stream design.",
  "Ground-truthPrediction": ".Performance Comparison ofDUALADandUniAD for two different scenes. Predictions are shown inorange, ground-truth annotations in blue, ego location with a redcross. While highly dynamic agents cause perception errors suchas track losses or distorted objects for UniAD, DUALAD consis-tently captures them due to the proposed dual-stream design. duct an experiment in which we focus on highly dynamicagents.More specifically, we evaluate the object detec-tion performance on objects of the type car, where both,the absolute and the relative velocity with respect to theego vehicle, are higher than 10 m/s. The performance ofUniAD drops to 36.3 (38 %) mAP while DUALADdrops to 47.3 (25 %) mAP, yielding an increased perfor-mance delta of 5.5mAP. We observe on the one hand thatfor both approaches the detection of highly dynamic objectsis particularly challenging. On the other hand, these resultsconfirm the importance of explicit motion modelling for theperception of dynamic agents as conducted in DUALAD.",
  ". Conclusion": "This paper presents DUALAD, a novel approach that ex-plicitly models dynamic agents and static scene elementsin a dual-stream design, where both can directly access thesensor information. This split explicitly accounts for ob-ject and ego motion within the dynamic stream, while onlycompensating for ego motion within the static stream. Thestreams can interact by the newly introduced dynamic-staticcross-attention, facilitating object detection by utilizing theinferred scene structure around the object. Our approach not only excels in early-stage perceptiontasks such as object detection and online map learning, butalso demonstrates seamless integration with recent end-to-end models to tackle downstream tasks. In our experimentalevaluation, DUALAD yields significant improvements overspecialized models and reaches SOTA performance for ob-ject detection, map segmentation, and multiple object track-ing. Additionally, the integration into end-to-end modelsrevealed improvements in motion prediction and planning,highlighting the importance of our dual-stream design forthe entire functional chain. Whilst our approach results in a robust and temporallyconsistent perception of the scene, the integration of othermodalities such as LiDAR could boost the performanceeven further, especially combined with the potential of ourmodel to flexibly move the belief state to different pointsin time to incorporate even unsynchronized sensors. Theintegration of additional information like traffic signs ortraffic lights, as well as the integration of additional taskssuch as depth-estimation or lane topology reasoning, remainpromising research directions. Acknowledgement:This work is a result of the joint re-search project STADT:up (19A22006O). The project is sup-ported by the German Federal Ministry for Economic Af-fairs and Climate Action (BMWK), based on a decision ofthe German Bundestag. The author is solely responsible forthe content of this publication. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-ancarlo Baldan, and Oscar Beijbom.nuscenes: A multi-modal dataset for autonomous driving. In Proc. IEEE Conf.on Computer Vision and Pattern Recognition (CVPR), 2020.2, 4, 5, 6, 1 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proc. of theEuropean Conf. on Computer Vision (ECCV), 2020. 2",
  "MMDetection3D Contributors.MMDetection3D: Open-MMLab next-generation platform for general 3D object de-tection. 1": "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo-pher Re. Flashattention: Fast and memory-efficient exact at-tention with io-awareness. Advances in Neural InformationProcessing Systems (NIPS), 2022. 1 Simon Doll, Richard Schulz, Lukas Schneider, Viviane Ben-zin, Markus Enzweiler, and Hendrik P.A. Lensch.Spa-tialdetr: Robust scalable transformer-based 3d object de-tection from multi-view camera images with global cross-sensor attention. In Proc. of the European Conf. on Com-puter Vision (ECCV), 2022. 2, 3 Simon Doll, Niklas Hanselmann, Lukas Schneider, RichardSchulz, Markus Enzweiler, and Hendrik P.A. Lensch. Star-track: Latent motion models for end-to-end 3d object track-ing with adaptive spatio-temporal appearance representa-tions. arXiv.org, arXiv:2306.17602, 2023. 2, 3, 5 Junru Gu, Chenxu Hu, Tianyuan Zhang, Xuanyao Chen,Yilun Wang, Yue Wang, and Hang Zhao. Vip3d: End-to-end visual trajectory prediction via 3d agent queries.InProc. IEEE Conf. on Computer Vision and Pattern Recog-nition (CVPR), 2023. 4, 6",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proc. IEEEConf. on Computer Vision and Pattern Recognition (CVPR),2016. 5, 6, 3": "Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, WenhaiWang, et al. Planning-oriented autonomous driving. In Proc.IEEE Conf. on Computer Vision and Pattern Recognition(CVPR), 2023. 1, 2, 3, 4, 5, 6, 7, 8 Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, JiajieChen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang,and Xinggang Wang. Vad: Vectorized scene representationfor efficient autonomous driving. 2023. 1, 2, 3, 4, 5, 6, 7",
  "Youngwan Lee, Joong-won Hwang, Sangrok Lee, YuseokBae, and Jongyoul Park. An energy and gpu-computationefficient backbone network for real-time object detection.2019. 5, 1": "Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni,and Lei Zhang. Dn-detr: Accelerate detr training by intro-ducing query denoising. In Proc. IEEE Conf. on ComputerVision and Pattern Recognition (CVPR), 2022. 2, 1 Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:Learning birds-eye-view representation from multi-cameraimages via spatiotemporal transformers. In Proc. of the Eu-ropean Conf. on Computer Vision (ECCV), 2022. 2, 3, 4, 5,6, 1 Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, AnimaAnandkumar, Jose M Alvarez, Ping Luo, and Tong Lu.Panoptic segformer: Delving deeper into panoptic segmen-tation with transformers. In Proc. IEEE Conf. on ComputerVision and Pattern Recognition (CVPR), 2022. 2, 3",
  "NuScenes Benchmark.nuScenes Tracking Task. . Accessed: 2.11.23. 4, 1, 3": "Ziqi Pang, Jie Li, Pavel Tokmakov, Dian Chen, SergeyZagoruyko, and Yu-Xiong Wang. Standing between past andfuture: Spatio-temporal modeling for multi-camera 3d multi-object tracking. In Proc. IEEE Conf. on Computer Vision andPattern Recognition (CVPR), 2023. 5, 6 LIAN Qing, Tai Wang, Dahua Lin, and Jiangmiao Pang.Dort:Modeling dynamic objects in recurrent for multi-camera 3d object detection and tracking. In Proc. Conf. onRobot Learning (CoRL), pages 37493765. PMLR, 2023. 2 Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu,Pranaab Dhawan, and Raquel Urtasun. Perceive, predict, andplan: Safe motion planning through interpretable semanticrepresentations. In Proc. of the European Conf. on ComputerVision (ECCV), 2020. 1",
  "Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xi-angyu Zhang. Exploring object-centric temporal modelingfor efficient multi-view 3d object detection. 2023. 2, 3, 5, 6,7, 1": "Xiaofeng Wang, Zheng Zhu, Yunpeng Zhang, Guan Huang,Yun Ye, Wenbo Xu, Ziwei Chen, and Xingang Wang. Are weready for vision-centric driving streaming perception? theasap benchmark. In Proc. IEEE Conf. on Computer Visionand Pattern Recognition (CVPR), 2023. 4 Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang,Yilun Wang, Hang Zhao, and Justin Solomon.Detr3d:3d object detection from multi-view images via 3d-to-2dqueries. In Proc. Conf. on Robot Learning (CoRL), 2022.2, 3 Zitian Wang, Zehao Huang, Jiahui Fu, Naiyan Wang, andSi Liu. Object as query: Lifting any 2d object detector to3d detection. In Proc. of the IEEE International Conf. onComputer Vision (ICCV), pages 37913800, 2023. 2",
  "Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In Proc. IEEE Conf.on Computer Vision and Pattern Recognition (CVPR), 2021.2, 5, 7": "Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao,Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye,and Jingdong Wang. Rethinking the open-loop evaluationof end-to-end autonomous driving in nuscenes. arXiv.org,arXiv:2305.10430, 2023. 6 Tianyuan Zhang, Xuanyao Chen, Yue Wang, Yilun Wang,and Hang Zhao. Mutr3d: A multi-camera tracking frame-work via 3d-to-2d queries. In Proc. IEEE Conf. on ComputerVision and Pattern Recognition (CVPR), 2022. 2, 3, 7 Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,Guan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified per-ception and prediction in birds-eye-view for vision-centricautonomous driving. arXiv.org, arXiv:2205.09743, 2022. 6",
  "Supplementary Material": "In this supplementary document, we first provide imple-mentation details of our proposed approach. Furthermore,we present additional evaluation metrics for all perceptiontasks tackled by DUALAD. Next, we discuss experimentalfindings regarding our design choices and temporal consis-tency. Finally, we provide a detailed runtime analysis fordifferent variants of our model and show additional qualita-tive results in the attached video file.",
  ". Implementation Details": "Our work is built using the MMDetection3D framework.Furthermore, we inherit various design choices fromStreamPETR , UniAD and VAD .We truly thank all authors and contributors of those projects.Our main model configuration closely follows Stream-PETR since our dynamic stream design inheritsthe proposed query propagation through time as well as thegeometric positional encodings for object-to-image cross-attention. All choices for the static stream are adopted fromUniAD . Data Augmentation: We use the six surround camera im-ages of nuScenes as input, down scaled to a resolution of800 320 pixels. During training, we apply a random cropaugmentation by choosing a random crop of 47 % 62.5 %of the image before down scaling. Model Settings:We use a VovNet-V2-99 as im-age backbone and use the last two feature scales as inputto the FPN .As in previous work, a latent dimen-sion L = 256 is adopted for all latent embeddings of ourmodel.We use |Qobj| = 900 object queries consistingof the top-k propagated from the previous time step withk = 256 and 644 newly spawned objects queries respec-tively.For the BEV-queries we follow UniAD anduse |QBEV| = 200 200.The used detection range is[51.2 m, 51.2 m] for x and y direction, resulting in an ef-fective grid resolution of 0.512 m.The proposed dual-stream transformer utilizes six con-secutive layers and performs self-attention within Qobj,cross-attention of Qobj, temporal self-attention of QBEV andthe interpolated grid queries from the last frame ,cross-attention from QBEV to image features as in anddynamic-static cross-attention of Qobj and QBEV. For thedynamic object cross-attention to the image features weonly choose the highest spatial resolution feature scale asin .During training, we adopt query-denoising andstreaming video training as proposed in to accelerate the convergence as well as Flash-Attention to reduce thememory requirements. With the aforementioned settings,the training for 24 epochs requires 18 GB of GPU memoryand takes approximately one day for stage-I and two daysfor stage-2 on eight NVIDIA A100 GPUs.",
  ". Performance Evaluation": "We provide evaluation results for various model configu-rations of DUALAD. As in the main paper, we indicateall stage-I models that are trained on perception tasks onlye.g. object detection, map segmentation and multiple ob-ject tracking as DUALAD-I and the configuration that wastrained on all tasks in an end-to-end fashion as DUALAD-II respectively. Furthermore, we adopt the notation intro-duced in to denote different configurations of DU- ALAD. The version marked with does not use the pro-posed dynamic-static cross-attention, while describes aversion that uses bidirectional stream interaction by usingglobal attention for the interaction from the static to the dy-namic stream. The version of our model that is trained onthe reduced sensor set by using front and back facing cam-eras in an alternating fashion only is indicated with .",
  "DUALAD-I31.7367.5226.5710.99DUALAD-I33.9769.3529.4912.33DUALAD-I33.8667.7829.1112.18DUALAD-I34.2669.7129.7113.87DUALAD31.5366.6026.7710.14DUALAD-I34.6870.5030.2912.82DUALAD-II34.1770.0129.9612.25": "proposed dynamic-static cross-attention. Adding anothercross-attention block to perform bidirectional interactiondoes not significantly improve the performance of staticmap perception or overall temporal consistency, which is inline with our hypothesis that map segmentation might notbenefit from dynamic agent perception. We leave the inves-tigation of other interaction designs and other dense tasksthat depend on the dynamic agent perception e.g. free-spaceestimation for future work. The stage-II configuration of our approach yields aslightly decreased perception performance when comparedto the stage-I model. This could result from the fact thatin stage-II the model might focus on certain scene parts thatare more relevant for the currently planned trajectory. Addi-tionally, a fast detection of highly dynamic agents and tem-poral consistency might be crucial for longer planning hori-zons, which is in line with the improvements of the stage-IImodel in terms of Track Initialization Duration (TID) andLongest Gap Duration (LGD) as shown in . The DUALAD-I version of our model that only has ac-cess to front or back facing cameras in an alternating fashionmaintains high temporal consistency by query propagationeven without sensor data for some areas in the scene. We re-fer to the attached video for a qualitative example. However,the initial detection of newly appeared object is not possibleif no sensor data for the corresponding scene area is avail-able or consistent tracking might be challenging, especially for highly dynamic or hardly visible agents in the scene.Since our base model especially improves over previous ap-proaches in such challenging cases, this explains the drop inperception performance by 6.7 mAP and 10.7 AMOTArespectively (see , ).",
  ". Runtime Analysis": "We evaluate the runtime of the stage-II configuration ofDUALAD. The results of the entire system as well as theruntime of the intermediate task modules are shown in Ta-ble 12. DUALAD runs with 4.12 FPS on a single NVIDIAA100 GPU. The dual stream transformer uses a significantamount of the models total runtime due to the expensiveattention operations from object queries and BEV-queriesto sensor data. Since all downstream tasks use the result-ing representations, the task heads only add a small amountof additional runtime. Please note that our codebase con-tains various operations which could be further optimized.However, improving the runtime and memory requirementsof end-to-end approaches remains a challenging topic forlarge scale application of such approaches.",
  "Total242": "configurations, VAD relies on a ResNet-50 as imagebackbone, an input resolution of 1280 720 pixels and a shorter detection range around the ego vehicle of[30 m, 30 m] in x and [15 m, 15 m] in y respectively. Adetailed evaluation of the perception performance is givenin . DUALVAD outperforms VAD by +2.7mAP for dynamic object perception and achieves a slightlyhigher vectorized map perception performance while alsoheavily improving downstream tasks such as motion predic-tion (see ) and open-loop planning (see ). Theruntime of DUALVAD-II is shown in . In this con-figuration, our model runs at 3.32 FPS on a single NVIDIA .Runtime evaluation of DUALVAD-II on a singleNVIDIA-A100 for 500 frames of the nuScenes validation set.Misc describes various non-optimized computations e.g. bound-ing box decoding and positional encodings.",
  ". Qualitative Results": "Together with this document, we provide a video that showsqualitative results of our approach for various scenes fromthe nuScenes validation set. Those include complex trafficscenes, a setting with unsynchronized sensors, challenginglighting and adverse weather conditions and results for thevectorized map representation. DUALAD-II demonstratesrobust and consistent performance for all perception tasks,as well as downstream performance for motion predictionand open-loop planning."
}