{
  "Abstract": "Diffusion models have demonstrated significant potential inimage generation. However, their ability to replicate train-ing data presents a privacy risk, particularly when the train-ing data includes confidential information. Existing mitiga-tion strategies primarily focus on augmenting the trainingdataset, leaving the impact of diffusion model architectureunder explored. In this paper, we address this gap by ex-amining and mitigating the impact of the model structure,specifically the skip connections in the diffusion models U-Net model. We first present our observation on a trade-offin the skip connections. While they enhance image genera-tion quality, they also reinforce the memorization of trainingdata, increasing the risk of replication. To address this, wepropose a replication-aware U-Net (RAU-Net) architecturethat incorporates information transfer blocks into skip con-nections that are less essential for image quality. Recogniz-ing the potential impact of RAU-Net on generation quality,we further investigate and identify specific timesteps dur-ing which the impact on memorization is most pronounced.By applying RAU-Net selectively at these critical timesteps,we couple our novel diffusion model with a targeted train-ing and inference strategy, forming a framework we refer toas LoyalDiffusion. Extensive experiments demonstrate thatLoyalDiffusion outperforms the state-of-the-art replicationmitigation method achieving a 48.63% reduction in replica-tion while maintaining comparable image quality.",
  ". Introduction": "Diffusion models have emerged as powerfultools for generating high-quality images, with their perfor-mance further enhanced through conditional-guidance , a technique that allows models to generate imagesbased on specific text input conditions, such as descriptiveprompts or categorical labels. Starting with a pure noiseimage, prevalent diffusion models utilize a U-Net archi-tecture to predict and remove the noise iteratively across",
  ". Comparison of replication scores between our proposedLoyalDiffusion with and without dataset optimization (DO) andprior methods including MC , GN , RC , CWR ,GC&DF": "a predetermined number of timesteps, gradually generat-ing an image that corresponds to the specified input con-ditions .Leading examples include Stable Dif-fusion , DALLE , and Imagen , which havegained widespread recognition for their ability to gener-ate art, photorealistic images, and illustrations from textualprompts .However, a significant vulnerability of diffusion mod-els is replication, where the model generates images thattoo closely resemble its training data .Thisissue becomes particularly concerning when fine-tuning alarge pre-trained model on customized datasets , a com-mon scenario in many applications, because customizeddatasets often contain a substantial amount of sensitive orprivate data which should not appear in the generated im-ages . Previous studies have proposed various strategies to mitigate replication indiffusion models by optimizing the datasets. Deduplica-tion , for instance, helps reduce overfittingby removing duplicate content but is computationally de-manding and struggles with large-scale datasets. Noise ad-dition techniques including differential privacy effectively limit privacy risks but introduce noise that de-grades model quality. Lastly, prompt disturbing modifies user inputs to prevent replication, yet this can leadto reduced control over output quality and introduce bi-",
  "arXiv:2412.01118v1 [cs.CV] 2 Dec 2024": "ases. These methods largely focus on external factors likedatasets or inputs rather than the core model mechanismsthat cause replication, highlighting the need for mitigatingreplication from the root, the model itself.In this paper, we investigate replication in diffusion mod-els from two new perspectives: the model structure andthe timesteps during which the impact on memorizationare significant but less important on generation quality.We propose a new diffusion model framework, referred toas LoyalDiffusion, to mitigate the replication issue moti-vated by the hypothesis that the skip connections in the U-Net model may contribute to replication because they di-rectly transfer the outputs of down-sampling blocks to up-sampling blocks. We first verify our hypothesis and discov-ered a trade-off with these skip connections: while they en-hance the quality of image generation, they also reinforcethe memorization of training data, thereby increasing therisk of replication. Thus motivated, we introduce a modi-fied replication-aware U-Net structure, referred to as RAU-Net, that replaces the direct copying between down- andup-sampling layers with information transfer blocks. In par-ticular, we propose several information transfer blocks andempirically select the one that minimizes replication whilepreserving the highest quality of image generation. More-over, because replacing all skip connections with an infor-mation transfer block significantly degrades the quality ofgenerated images, we empirically investigate the optimalplacement for these blockers.Orthogonally, we show that the sensitivity to generationquality and replication varies across timesteps. Thus moti-vated, we selectively apply our RAU-Net to only a specificrange of timesteps to optimize the balance between reducedreplication and generation quality. Thus, LoyalDiffusion in-corporates both the standard U-Net and our newly proposedRAU-Net that we apply (and train) for different ranges oftimesteps.Finally, we demonstrate that LoyalDiffusion can be ef-fectively used alongside prior replication mitigation meth-ods , achieving near-optimal performance while sub-stantially reducing replication risks. comparesour LoyalDiffusion with prior methods showing that our ap-proach achieves the smallest replication score.To the best of our knowledge, LoyalDiffusion is the firstframework to consider addresses replication in diffusionmodels by focusing on the U-Net structure and image gen-eration evolvement, providing a novel perspective on repli-cation mitigation. We summarize our contributions as fol-lows: (1) We propose a replication-aware U-Net (RAU-Net) which mitigates replication by limiting direct infor-mation flow between down-sampling and up-sampling. (2)We identify the impact of timesteps, and selectively applythe RAU-Net during those timesteps, minimizing replica-tion while maintaining model performance. We also de- velop a corresponding training method to optimize modelperformance. (3) Our approach outperforms prior methodsby achieving the lowest replication score with minimal lossin image quality. (4) We use Bing image search to obtainreal world images that closely align with the tested promptsfrom which we estimate a lower bound of the replicationscore Rreal and find that our optimized models yield repli-cation scores close to this estimate (see ).",
  ". Diffusion Models": "Denoising Diffusion Probabilistic Models (DDPMs) have shown significant success in generative tasks , which use a parameterized Markov chain to reverse anoise-adding process, reconstructing data from noise. How-ever, their high computational complexity led to the de-velopment of Latent Diffusion Models (LDMs), such asStable Diffusion , which reduce this cost by train-ing in a compressed latent space. LDMs first encode im-ages into a lower-dimensional latent space using an autoen-coder, where a DDPM is then trained. Additionally, build-ing on existing guidance techniques in diffusion models(DMs) , LDMs employ cross-attention to enablemulti-modal guidance by injecting conditional informationat each timestep to better control generation.We refer to a pre-trained Stable Diffusion v2.1 model as SD, which is used as the baseline model forthe rest of this paper. The fine-tuning dataset is typicallyformed with a number of image-caption pairs, denoted asDF T = {(xn, yn)}Nn=1, where xn is the n-th image, andyn is its corresponding caption. Training a diffusion modelinvolves two phases: forward process and reverse trainingprocess. The forward process adds noise iteratively to eachtraining image x over T timesteps, gradually transformingit into pure Gaussian noise. We use xt and t to denote thenoisy input data and the noise added to the input at time stept, respectively. In the reverse training process, SD learns toreverse this transformation, gradually denoising the noisydata to reconstruct the original input. Specifically, a U-Netparameterized by learns to estimate the noise t addedat timestep t, and SD can then subtract the predicted noisefrom xt to iteratively refine the data. The loss function fortraining the U-Net is as follows:",
  "SC 4": ". Overview of the proposed LoyalDiffusion framework. (a) Standard U-Net architecture. (b) Replication-Aware U-Net (RAU-Net)where the skip connection (SC) 3 & 4 being modified to one single Conv 33 layer. (c) Training strategy for LoyalDiffusion where thetraining data optimization block includes generalized captioning and dual fusion . (d) Image generation process using LoyalDiffusion. play a crucial role in diffusion models. The encoder pathconsists of multiple convolutional layers and cross-attentionblocks that perform downsampling operations, progres-sively reducing the spatial resolution while capturing in-creasingly abstract features. At the center of the network,the bottleneck provides a highly compressed, low-level rep-resentation of the input data, containing the most essentialfeatures. The decoder path then uses upsampling layers toprogressively reconstruct the spatial dimensions back to theoriginal resolution. In this upsampling path, each layersoutput is concatenated with the corresponding feature mapfrom the encoder via skip connections. Skip connection were introduced to mitigate the degra-dation problem by helping stabilize gradients during train-ing and enhancing information flow across layers ,and have been applied to various U-Net designs . These skip connections help retain high-resolution de-tails from the encoder, enabling more precise reconstruc-tions and improving the models ability to denoise, segmentor reconstruct the data accurately. However, while skip con-nections improve feature retention, they may also increasethe risk of replication by enabling the diffusion model tomemorize specific details from training data .",
  ". Replication and Mitigation Strategies": "We use X = {xn}Nn=1 and G = {gn}Nn=1 to represent thetraining image set and the generated image set, respectively.The degree of replication between X and G can be evalu-ated using the replication score , which quantifies theextent to which generated images replicate or closely re-semble images from the training set.Replication Score. Let S represent the set of similarityscores of all possible pairs of images x X and g G.The replication score, denoted as R, is defined as the 95thpercentile of the similarity score distribution. Formally,",
  "Pr(s < R) = 0.95, s S.(2)": "This means that 95% of the similarity scores in s S arebelow the replication score R, ensuring that low similarityscores do not distort the overall dataset replication score.The similarity score between two images is calculated basedon features extracted by the SSCD . A higher value ofR indicates a more pronounced degree of replication.Mitigation Methods. As prior research indicates thatreplication is largely driven by image duplication andhighly specific captions in the training data, exist-ing mitigation strategies primarily focus on optimizing thetraining data. apply image deduplication to identifyand remove both exact duplicates and semantically simi-lar images. To generalize captions, Somepalli et al. pro- pose methods including using multiple captions to train-ing DMs (MC), adding gaussian noise to text embeddings(GN), randomly replacing captions (RC), randomly replac-ing or adding tokens or words in captions (RTA), repeat-ing a random word in random location (CWR), and ran-domly adding a number to captions (RNA) .Addi-tionally, Li et al. introduce a generality score to quantifycaption abstraction and use LLM to generalize captions,along with a dual-fusion technique that merges each train-ing image with an external image-caption pair based on aset weighting, thereby reducing memorization risks. Thisapproach focuses on a combination of caption generaliza-tion and image deduplication, thereby significantly reducesreplication. Anti-Memorization Guidance (AMG) in-cludes three guidance strategies, despecification guidance,caption deduplication guidance, and dissimilarity guidance,all aiming at addressing memorization arising from im-age and caption duplication and over-specification. Wen etal. suggest using perturbed prompt embeddings to min-imize text-conditional prediction magnitude at diffusion in-ference, and screen out potential memorized pairs based onprediction magnitude during training. However, these data-centric strategies predominantly address replication exter-nally, without examining the models internal mechanismsthat contribute to memorization. Machine unlearning isable to erase and remove specific content from the modelsand several studies have introduced it to mitigate replica-tion . However, it can be computationally expensiveand has potential to introduce new vulnerabilities. To address those shortcomings, in this paper, we proposea novel approach to mitigate replication by focusing on thediffusion model structure, specifically the U-Net architec-ture and training strategy.",
  ". The Proposed LoyalDiffusion Framework": "Our approach to reducing replication in diffusion models, asshown in , has three components. First, we intro-duce a replication-aware U-Net (RAU-Net), which replacesthe U-Nets skip connections with more complex informa-tion transfer block, mitigating the transference of direct fea-tures between the encoder and decoder of the model. Sec-ondly, we propose a two-stage training strategy that appliesthe RAU-Net only at higher timesteps, where the impacton fidelity and quality of generation is minimal while stillmaintaining significant replication reduction, using a stan-dard U-Net at lower timesteps to avoid fidelity of finer de-tails being destroyed. Finally, we show how these tech-niques can be combined with generalized captioning anddual-fusion enhancement to further improve generalizationand sample quality.",
  ". Replication-Aware U-Net": "Skip Connection (SC) and Replication. Skip connectionsin U-Net architectures are designed to preserve spatial fea-tures from the encoder by passing them directly to the de-coder, effectively bypassing the bottleneck layer .This direct transfer helps retain high-resolution details thatmight otherwise be lost during the encoding and decodingprocesses. However, the bypassing also means the decoderblock relies less on the most compressed and abstract fea-tures from the bottleneck layer which help reduce the re-liance on specific features. As a result, the decoder can be-come overly dependent on these unaltered encoder features.Instead of learning to generate new content based on la-tent representations, the decoder might start copying detailsdirectly from the encodera copy-paste behavior. Thismechanism impacts the information flow by allowing high-resolution details to take precedence, limiting the networksability to learn general, abstract representations, making themodel more prone to reproducing specific features from thetraining data rather than creating new, varied outputs.To test this hypothesis, we compare the replication scoreof SD with and without skip connections, as shown in Ta-ble 1. We use the Frechet Inception Distance (FID) to assess the quality and diversity of generated images.The detailed experimental setup is provided in .1.Based on the results, we conclude that removing skip con-nections can indeed reduce the replication score, however,at the expense of generated image quality.Information Transfer Block. To maintain image gen-eration quality, we propose adding information transferblocks to selective skip connections rather than removing allskip connections indiscriminately. The information transferblock candidates include:1. Max Pooling (MP): Max pooling reduces the spatial di-mensions of features in the skip connections, emphasiz-ing prominent elements and condensing encoder infor-mation. It helps prevent the decoder from accessing de-tailed patterns, while still preserving essential spatial in-formation. Specifically, we use Max pooling with kernelsize 2 2, followed by a MaxUnpool that set all no-maximal values to 0. 2. Convolutional Layer (Conv): Adding a convolutionallayer to the skip connections allows the model to trans-form encoder features before passing them to the de-coder, suppressing redundant patterns that could lead to replication. By adjusting the features, the convolutionallayer helps balance the detailed information and the ab-stract representations. For Conv, we use a single convo-lutional layer with kernel size 3 3, without changingthe output feature shape. 3. Multiple Convolutional Layers (Multi-Conv): ExtendingConv with multiple convolutional layers offers deeperfeature processing, creating complex abstractions to fur-ther reduce replication artifacts. We use 3 Convolutionallayers with kernel size 3. We analyze the effectiveness of different informationtransfer blocks for reducing replication. For this evalua-tion, we applied each type of block uniformly across all skipconnections within the U-Net and also tested each type on asingle skip connection. The results are presented in .The results indicate that the Conv block provides the mosteffective mitigation against replication compared to otheralternatives, leading us to select it as the information trans-fer block in our RAU-Net.Noting that applying the information transfer block to allskip connections results in either high FID or minimal repli-cation improvement, we selectively apply the Conv block tospecific skip connections in RAU-Net. As shown in ,applying Conv to SC 3 achieves the lowest replication scorewhile maintaining a comparable FID, leading us to adoptthis design in RAU-Net. Interestingly, modifying SC 4 us-ing a Conv resulted in almost no change in both replica-tion and FID. This suggests that the information carried bySC 4 may not contribute significantly to replication issues.Given this finding, we experimented with modifying bothSC 3 and SC 4 together. The result showed a replicationscore of 0.386, further improving upon modifying only SC3 by 13.07% and 37.44% better than baseline, without in-creasing FID. We thus apply Conv to both SC 3 and SC 4in RAU-Net as shown in Figuer 2(b). We present exper-imental results on the impact of applying the informationtransfer block to various skip connection combinations inthe Appendix.",
  ". Training LoyalDiffusion": "We recognize that applying the RAU-Net uniformly acrossall timesteps in the diffusion process may negatively impactoverall model performance. To address this, we propose atwo-stage diffusion training approach that strategically ap-plies RAU-Net only at the least influential timesteps forgeneration quality, thereby balancing the trade-off betweenreplication reduction and model fidelity.As shown in (c), we analysis the behavior ofgenerating process for a highly replicated generated image.We use pairwise similarity score to represent the quality ofgenerated image and CLIP score to manage how well itmatches the prompt. The similarity increases most rapidlyduring the later timesteps, when t is small, but is stable",
  "Diffusion generating process": ". Similarity between a training image and the generatedimage that replicates it and CLIP score calculating using the cap-tion during the diffusion generating process. It shows that sim-ilarity and CLIP score do not change too much during the earlyinference interval with large timesteps. at early timesteps, which suggests that early timesteps areless important to image quality than later timesteps. TheCLIP score curve shows a similar trend, remaining almostunchanged during early timesteps. Intuitively, during theseearly timesteps, the noise level remains high, making it dif-ficult for the model to capture detailed features that define the generated image quality or precise alignment with theprompt. At this stage, the model primarily operates in a gen-eralized manner, with limited specificity, as the high noiseoverwhelms finer details from the training data. This ob-servation suggests that these early timesteps are less criticalfor maintaining high-quality generation results. By strategi-cally applying RAU-Net only during these high-noise, low-impact timesteps, we can minimize any potential adverse ef-fects on generating quality while still benefiting from RAU-Nets replication reduction capabilities.On the other hand, during the early timesteps, the modelbegins by generating high-level information, such as objectoutlines and shapes, as shown in . These founda-tional elements establish the general direction of the subse-quent generation process. Given that our studys replicationscore is based on object-level, we hypothesize that this high-level information generated in the early stages is a criticalfactor in replication.Based on these insights, we propose a two-stage trainingstrategy with a predefined threshold timestep . When t >, the RAU-Net Ura is employed and only trained on noisytraining samples within t (, T). For t , the standardU-Net Ust is used and only trained on t trainingsamples.Mathematically, the denoising process at each timestep tcan be defined as:",
  "Ura(xt, t),if t > ,Ust(xt, t),if t ,(3)": "where xt is the noisy image at timestep t. During training,both U-Nets are optimized concurrently but each is respon-sible for different ranges of timesteps.The image generation procedure of LoyalDiffusion isillustrated in (d), where RAU-Net is utilized fortimesteps greater than the threshold, while the standard U-Net is applied for timesteps below the threshold.",
  ". Layering Dataset Optimization Methods": "To further enhance replication mitigation, we show our ap-proach can be integrated with the state-of-the-art datasetoptimization methods, i.e., generalized caption and dual-fusion techniques (GC&DF) .Generalized caption-ing reduces replication by transforming specific captionsinto more generalized descriptions using a large languagemodel, thereby encouraging the model to learn broad pat-terns instead of memorizing specific details. Dual-fusionfurther supports this by fusing a training image with anotherimage from a different resource, using a weighted fusionof both the images and their captions. These approachestend to diversify the training data, making it harder for themodel to replicate exact training samples. Together withour LoyalDiffusion, this combination addresses replicationfrom both data and model perspectives.",
  ". Evaluation using Bing Search Images": "To further evaluate replication, we utilize the Bing searchengine to retrieve images from the internet using the sameprompts used during training and inference, forming a newdataset named RepliBing, for which we calculate a replica-tion score Rreal comparing RepliBing to our training im-ages.Because the RepliBing dataset is diverse, closelyaligns with the provided prompts, and is unlikely to havedirect replications of training images (an assumption weverify manually), we assert Rreal is a good estimate of thelower bound of the replication score. In fact, as described in, we found the replication score of all generativemodels tested is above Rreal, but for our proposed modelsnot by much, suggesting our models avoid replication well.",
  ". Setup": "To evaluate our strategy, we fine-tune large pre-trainedmodels on small datasets, as this approach tends to inten-sify data replication issues . This worst-case scenarioallows us to assess the limits of our approach in mitigatingreplication.Baseline.We use Stable Diffusion v2.1 as ourbaseline model, which is currently one of the state-of-the-art text-to-image generation models trained on the LAIONdataset . Stable Diffusion consists of: a text encoder,an image autoencoder, a diffusion module, and an imagedecoder.For our experiments, we set the timestep withT = 1000, focusing specifically on fine-tuning U-Net.Dataset. For training, we randomly sample 10,000 in-stances from the LAION-2B dataset. Each instancecontains an image paired with its corresponding caption,providing a diverse set of content covering various objectsand contexts.Training and Evaluation. In our setup, all componentsexcept the U-Net are frozen during fine-tuning. We adoptmost of the hyperparameters from , including train-ing for 100,000 iterations with a learning rate of 5e6. Forevaluation, we generate 10,000 images and use three pri-mary metrics: the replication score R to quantify the extentof replication in generated samples, the Frechet InceptionDistance (FID) to assess the quality and diversity ofgenerated images, and the CLIP score to evaluate thesemantic similarity between generated images and their cor-responding captions.Real World Replication Score. We calculate the repli-cation score between RepliBing and the training image set,resulting in a replication score of Rreal = 0.275. Inter-estingly, the replication score is not zero because even real-world images may share similar high-level structures or fea-tures, especially when generated from similar prompts.",
  ". Comparison with Prior-Art": "We compare our approach with a two-stage training thresh-old of =300 with several prior-art methods de-signed to mitigate replication, including multiple cap-tions (MC) , Gaussian noise (GN) , randomcaption replacement (RC) , caption word repetition(CWR) , Generalized Captions and Dual-Fusion En-hancement (GC&DF) . The results are shown in . Specifically, our method outperforms the best prior-artmethod, GC&DF, by 24.58%, without significantly impact-ing FID. Moreover, our best-performing model achieves areplication score of 0.316 that is close to the real worldreplication score Rreal of 0.275, indicating that our ap-proach is highly effective in mitigating replication.",
  ". Two-Stage Training Threshold Analysis": "Building on the effective reduction of replication achievedby RAU-Net, which replaces skip connection groups 3 and4 with Conv, we selected this architecture for further exper-imentation. To evaluate our two-stage training strategy, thestandard U-Net Ust is trained on timesteps t , while theRAU-Net Ura handles timesteps t > . Then we test differ-ent {100, 300, 500, 700, 900} and show the results in.We see that the best performance is achieved when =300. This configuration results in an 38.74% reduction inthe replication score compared to the baseline model andan 2.07% reduction compared to training without two-stagestrategy. Additionally, this approach reduces the FID scoreby 2.14 compared to using only the Ura (that yields a FIDof 21.31 as seen in , indicating improved model gen-eration ability.",
  "Training ImagesBaselineGC&DFOurs w/o GC&DF Ours w/ GC&DF": ". Images in the first row are from training dataset thatare replicated. Images in the second row are generated using fine-tuned baseline model, which show some replication with trainingimages. Images in the third row are generated by our best perfor-mance model, with less content replicated with training images. replication. For dual-fusion, we perform image fusion inthe latent space using a weight wlat = 0.1 and text fusionat the embedding level with wemb = 0.1, 0.3, 0.5 . Theexperimental results are presented in .The combination of GC&DF with LoyalDiffusion sig-nificantly improves the performance.With wlat = 0.1,wemb = 0.5 and = 300, it achieves a reduction in replica-tion of 48.78% with only 0.97 FID and a 0.005 CLIP scoreloss, compared with the baseline. Furthermore, even witha strict FID that should be almost the same as the baseline,our method still achieves a 43.11% reduction of replication,with wlat = 0.1, wemb = 0.3 and = 300. These resultssupport the claim that GC&DF and our architectural mod-ifications complement each other, GC&DF mitigates repli-cation from the data perspective, while our methods targetthe model structure itself.",
  ". Ablation Studies": "shows the ablation studies.Applying RAU-Netamong all timesteps reduces replication comparing withbaseline but increases FID, indicating lower fidelity. Whentwo-stage training strategy is introduced, it significantly re-covers FID without impacting replication reduction. Addi-tionally, LoyalDiffusion achieve a better replication scorethan GC&DF and combining LoyalDiffusion with GC&DFoutperforms only using one of them.FID-Centric vs. R-Centric Two-Stage Training. Inthe previous section, we showed that early timesteps (whent is large) have less impact on FID which motivated apply-ing RAU-Net to those timesteps. We refer to this approachas FID-centric. However, the similarity curve in can also serve as a measure of replication, which showsthat similarity increases fast during late timesteps (when tis small). So later timesteps are critical to generate repli-cated images which could to some degree motivate apply-ing RAU-Net to later timesteps, an approach we refer to asR-centric. For this reason, we conduct an ablation exper-iment in which we apply Ura when t and Ust whent > . The results are shown in and confirm that",
  ". Conclusions and Future Work": "In this work, we introduced LoyalDiffusion, a novel frame-work designed to mitigate replication risks in diffusionmodels while preserving image generation quality. Our ap-proach leverages a replication-aware U-Net (RAU-Net) thatselectively incorporates convolutional information transferblocks at specific skip connections, effectively balancingquality and privacy. Additionally, we propose a novel train-ing strategy applies RAU-Net selectively across timesteps,allowing for targeted mitigation of replication during lessquality-sensitive stages of the generation process. Throughextensive experiments, LoyalDiffusion demonstrated a sig-nificant reduction in replication scores compared to base-line models and prior state-of-the-art methods, achievingthis with minimal impact on FID and generation quality.WhileourapproachwithRAU-Netdemonstratespromising results, there are limitations to our current de-sign.The RAU-Net architecture, specifically the choiceand placement of convolutional information transfer blocks,was determined through empirical evaluation of a limitedset of transfer block configurations.Consequently, it ispossible that other information transfer mechanisms couldyield better results. Future research could explore a broaderrange of information transfer blocks or optimize RAU-Net architecture through automated search techniques toachieve an optimal balance between replication reductionand fidelity. Despite these limitations, our work representsa critical advancement for the field. In particular, to the bestof our knowledge, this study is the first to explicitly examinethe link between skip connections in diffusion models andtraining data leakage, thereby providing a new perspectiveon mitigating privacy risks within generative models.",
  "Saeed Anwar and Nick Barnes. Real image denoising withfeature attention.In Proceedings of the IEEE/CVF inter-national conference on computer vision, pages 31553164,2019. 4": "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.SegNet: A deep convolutional encoder-decoder architecturefor image segmentation. IEEE transactions on pattern anal-ysis and machine intelligence, 39(12):24812495, 2017. 3 Lucas Bourtoule, Varun Chandrasekaran, Christopher AChoquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang,David Lie, and Nicolas Papernot. Machine unlearning. In2021 IEEE Symposium on Security and Privacy (SP), pages141159. IEEE, 2021. 4 Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagiel-ski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ip-polito, and Eric Wallace. Extracting training data from diffu-sion models. In 32nd USENIX Security Symposium (USENIXSecurity 23), pages 52535270, 2023. 1",
  "Jonathan Ho, Tim Salimans, Alexey Gritsenko, WilliamChan, Mohammad Norouzi, and David J Fleet. Video dif-fusion models. Advances in Neural Information ProcessingSystems, 35:86338646, 2022. 2": "Nisha Huang, Fan Tang, Weiming Dong, and ChangshengXu. Draw your art dream: Diverse digital art synthesis withmultimodal guided diffusion.In Proceedings of the 30thACM International Conference on Multimedia, pages 10851094, 2022. 1 Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, EliShechtman, Richard Zhang, and Jun-Yan Zhu. Ablating con-cepts in text-to-image diffusion models. In Proceedings ofthe IEEE/CVF International Conference on Computer Vi-sion, pages 2269122702, 2023. 4",
  "Jean-Marie Lemercier, Julius Richter, Simon Welker, EloiMoliner, Vesa Valimaki, and Timo Gerkmann.Dif-fusion models for audio restoration.arXiv preprintarXiv:2402.09821, 2024. 2": "Chenghao Li, Dake Chen, Yuke Zhang, and Peter ABeerel. Mitigate replication and copying in diffusion mod-els with generalized caption and dual fusion enhancement.In ICASSP 2024-2024 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages72307234. IEEE, 2024. 1, 2, 3, 6, 7, 8 Zhangheng Li, Junyuan Hong, Bo Li, and Zhangyang Wang.Shake to Leak: Fine-tuning diffusion models can amplify thegenerative privacy risk. In 2024 IEEE Conference on Secureand Trustworthy Machine Learning (SaTML), pages 1832.IEEE, 2024. 1",
  "Mario Lucic, Karol Kurach, Marcin Michalski, SylvainGelly, and Olivier Bousquet. Are GANS created equal? Alarge-scale study. Advances in neural information processingsystems, 31, 2018. 4, 6": "Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. RePaint: Inpaintingusing denoising diffusion probabilistic models. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1146111471, 2022. 2 Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang.Imagerestoration using very deep convolutional encoder-decodernetworks with symmetric skip connections. Advances in neu-ral information processing systems, 29, 2016. 3 Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, PriyaGoyal, and Matthijs Douze. A self-supervised descriptor forimage copy detection. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1453214542, 2022. 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 5, 6 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.Zero-shot text-to-image generation. In International confer-ence on machine learning, pages 88218831. Pmlr, 2021. 1",
  "synthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1, 2, 6": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmen-tation. In Medical image computing and computer-assistedinterventionMICCAI 2015: 18th international conference,Munich, Germany, October 5-9, 2015, proceedings, part III18, pages 234241. Springer, 2015. 2, 3 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in neural informationprocessing systems, 35:3647936494, 2022. 1 Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, andKristian Kersting. Safe latent diffusion: Mitigating inappro-priate degeneration in diffusion models. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2252222531, 2023. 1 Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon,Ross Wightman,Mehdi Cherti,TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, et al. LAION-5B: An open large-scale dataset for train-ing next generation image-text models. Advances in NeuralInformation Processing Systems, 35:2527825294, 2022. 4,5, 6, 2 Gowthami Somepalli, Vasu Singla, Micah Goldblum, JonasGeiping, and Tom Goldstein. Diffusion art or digital forgery?Investigating data replication in diffusion models. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 60486058, 2023. 1, 6 Gowthami Somepalli, Vasu Singla, Micah Goldblum, JonasGeiping, and Tom Goldstein. Understanding and mitigatingcopying in diffusion models. Advances in Neural Informa-tion Processing Systems, 36:4778347803, 2023. 1, 3, 4, 6,7",
  "Ryan Webster, Julien Rabin, Loic Simon, and Frederic Ju-rie. On the de-duplication of LAION-2B. arXiv preprintarXiv:2303.12733, 2023. 1, 3": "Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. De-tecting, explaining, and mitigating memorization in diffusionmodels. In The Twelfth International Conference on Learn-ing Representations, 2024. 1, 3, 4 Guoping Xu, Xiaxia Wang, Xinglong Wu, Xuesong Leng,and Yongchao Xu. Development of skip connection in deepneural networks for computer vision and medical image anal-ysis: A survey. arXiv preprint arXiv:2405.01725, 2024. 3,4",
  "A.1. Additional Experimental Results": "In this section, we provide additional experimental resultsfocusing on different combinations of modified skip con-nections (SC) with information transfer blocks, extendingthe analysis presented in .1. .1 analyzeda limited combinations of applying different informationtransfer blocks to different skip connections (SC) and de-termined applying Conv to SC 3 and SC 4 simultaneouslygives the best replication reduction without increasing FID.Here, we explore additional combinations of modified skipconnections, as shown in .The results reveal two distinct trends: in some cases,replication is significantly reduced, but at the cost of a no-table degradation in FID; in other cases, the impact on FIDis minimal or even slightly improved, but the reduction inreplication is negligible. Only two configurations achievedsatisfactory results: applying the Conv information trans-fer block to SC 3 and 4, and applying the Multi-Conv in-formation transfer block to SC 3 and 4. Both configura-tions effectively reduced replication while maintaining al-most unchanged FID. Among these, the Conv informationtransfer block applied to SC 3 and 4 demonstrated a morepronounced reduction in replication than Multi-Conv, vali-dating the choice made in .1.",
  "A.2. Visualize Generated Images for Various FIDs": "In this section, we provide visualizations of generated im-ages corresponding to different FID values, as shown in Fig-ure 5. The third column shows images generated by base-line model with FID= 18.01. The visual examples illustratethat when the FID is within the range of 18.01 2, the gen-erative ability remains comparable, producing visually sim-ilar and acceptable images. However, with FID larger than20, the image quality drops significantly. This demonstratesthat an FID within (16, 20) does not significantly impact im-age quality, making it an acceptable range for evaluating theperformance of our proposed LoyalDiffusion.",
  "A.3. RepliBing Dataset": "To further evaluate replication in generative models, we cre-ated the RepliBing dataset mentioned in .4. Thisdataset is constructed using the Bing search engine to re-trieve publicly available images from the internet, based onthe same prompts used during training and inference. Repli-Bing is designed to provide a diverse set of images that alignclosely with the input captions while minimizing the chanceof direct replication of the training data. showcases examples from the RepliBing datasetalongside corresponding training images that share thesame captions. As shown, the RepliBing dataset capturesa broad spectrum of visual diversity while maintaining fi-delity to the provided prompts. These comparisons high-light the visual similarity in the conceptual alignment be-tween RepliBing images and the training data, without anydirect replication. We plan to open-source this dataset tofacilitate further research in mitigating replication in gener-ative models upon acceptance of the paper.",
  "Beach on the tropical island. Clear blue water sand and palm trees. Beautiful vacation spot treatment and aquatics": ". Examples of generated images with different FIDs. The images come from the following models: column (1) MP 3&4 from; column (2) GC&DF from ; column (3) Baseline from ; column (4) wlat = 0.1, wemb = 0.5, and = 300 from; column (5) Remove skip connection 4 from ; column (6) Two-stage result with = 100 from ; and (7) Multi-Conv1&4 from ."
}