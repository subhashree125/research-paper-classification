{
  "(e) Full-Body Recon": ". Stratified avatar generation from sparse observations. Given the sensory sparse observation of the body motion: 6-DoFposes of the head and hand marked by RGB axes in (a), our method leverages a disentangled body representation in (b) to reconstruct theupper-body conditioned on the sparse observation in (c), and lower-body conditioned on the upper-body reconstruction in (d) to accomplishthe full-body reconstruction in (e).",
  "Abstract": "Estimating 3D full-body avatars from AR/VR devices isessential for creating immersive experiences in AR/VR ap-plications. This task is challenging due to the limited in-put from Head Mounted Devices, which capture only sparseobservations from the head and hands. Predicting the full-body avatars, particularly the lower body, from these sparseobservations presents significant difficulties. In this paper,we are inspired by the inherent property of the kinematictree defined in the Skinned Multi-Person Linear (SMPL)model, where the upper body and lower body share onlyone common ancestor node, bringing the potential of de-coupled reconstruction. We propose a stratified approach todecouple the conventional full-body avatar reconstructionpipeline into two stages, with the reconstruction of the up-per body first and a subsequent reconstruction of the lowerbody conditioned on the previous stage. To implement thisstraightforward idea, we leverage the latent diffusion modelas a powerful probabilistic generator, and train it to fol-low the latent distribution of decoupled motions explored bya VQ-VAE encoder-decoder model. Extensive experimentson AMASS mocap dataset demonstrate our state-of-the-artperformance in the reconstruction of full-body motions.",
  ". Introduction": "Generating 3D full-body avatars from observations of HeadMounted Devices (HMDs) is crucial for enhancing immer-sive AR/VR experiences. HMDs primarily track the headand hands, while leaving the rest of the body unmonitored.This limited motion tracking poses a challenging scenariofor accurately reconstructing full-body 3D avatars, partic-ularly in representing the lower body.The high degreeof freedom in body movements compounds this difficulty,making the task of reasoning human motion from suchsparse observations significantly complex. Tremendous efforts have been made to obtain moretracking signals by adding sensors at Pelvis orboth Pelvis and Legs . While these ap-proaches provide more data points for avatar construction,they can diminish the users experience. Wearing extra de-vices can be cumbersome, potentially interfering with theusers comfort and immersion in the virtual environment.This trade-off highlights the need for innovative solutionsthat can deliver detailed body tracking without compromis-ing the users comfort and immersion in AR/VR settings.Accordingly, we are interested in the problem of generat-ing 3D full-body avatars from sparse observations of HMDsthat track the motion of the head and two hands, by develop-ing a neural solution that learns the distribution of full-bodyposes given the sparse observations as the input condition.",
  "arXiv:2405.20786v2 [cs.CV] 3 Jun 2024": "Recent studies have attempted to address the challengeof sparse observations in HMD-based full-body avatar gen-eration by employing regression-based techniques, as seenin , or by adopting generation-based approacheslike . These methods typically use deep neural net-works to predict human motion within a single, expansivemotion space. However, due to the limited data provided bysparse observations, these networks often struggle to fullycapture the complexities of human kinematics across such abroad and unified motion space. This limitation frequentlyresults in reconstructions that are unrealistic and lack phys-ical plausibility. We introduce a new method for reconstructing full-bodyhuman motions from sparse observations, called StratifiedAvatar Generation (SAGE). Instead of the upper-body mo-tion prediction that has tracking signals of certain upperjoints from sparse observations, predicting lower-body mo-tion is not straightforward as no direct tracking signalsabout any lower-body joint is given. It is noteworthy thatSMPL model connects the upper and lower half-bodyby a single root joint, as shown in (b), which moti-vates us to split the full-body motions into upper and lowerhalf-body parts. The benefits are two-fold: 1) the smallersearch space achieved by disentanglement facilitates learn-ing and prediction; 2) our stratified design makes the mod-eling and inferring for lower-body motions more accurateand visually appealing by explicitly modeling the correla-tion and constraint between two half-body motions. To thisend, we use VQ-VAE to encode and reconstruct upperand lower body motions separately. With the disentangled latent representation of the upperand lower body motions, we aim to recover the accuratefull-body motions from sparse observations with a body-customized latent diffusion model (LDM) in a strati-fied manner. Specifically, as shown in (c), (d),and (e), we first find the latent of upper-body motioncondition on the sparse observations (i.e., tracking signalsof the head and hands in (a)). Then, the latent oflower-body motion is inferred condition on both the pre-dicted upper-body latent and sparse observations. Finally,a full-body decoder takes the two half-body latents as inputand outputs the full-body motion. In the experiments, we comprehensively justified our in-tuitive design of disentangling the upper and lower bodymotion in a stratified manner. On the large-scale motioncapture benchmark AMASS , our proposed SAGE isexhibiting superior performance in different evaluation set-tings and particularly performs well in terms of the evalua-tion metrics for lower-body motion estimation compared toprevious state-of-the-art methods.",
  ". Motion Reconstruction from Sparse Input": "The task of reconstructing full human body motion fromsparse observations has gained significant attention in re-cent decades within the research community . For instance, re-cent works focus on reconstructing fullbody motion from six inertial measurement units (IMUs).SIP employs heuristic methods, while DIP pio-neers the use of deep neural networks for this task. PIP and TIP further enhance performance by incorporatingphysics constraints. With the rise of VR/AR applications,researchers turn their attention toward reconstructing fullbody motion from VR/AR devices, such as head-mounteddevices (HMDs), which only provide information about theusers head and hands, posing additional challenges. LoB-STr , AvatarPoser , and AvatarJLM approachthis task as a regression problem, utilizing GRU andTransformer Network to predict the full body posefrom sparse observations of HMDs. Another line of meth-ods employs generative models .For ex-ample, VAEHMD and FLAG utilize VariationalAutoEncoder (VAE) and Normalizing flow , re-spectively. Recent works leverage more powerfuldiffusion models for motion generation, yieldingpromising results due to the powerful ability of diffusionmodels in modeling the conditional probabilistic distribu-tion of full-body motion.Contrasting with previous methods that model full-bodymotion in a comprehensive, unified framework, our ap-proach acknowledges the complexities such methods im-pose on deep learning models, particularly in capturing theintricate kinematics of human motion. Hence, we proposea stratified approach that decouples the conventional full-body avatar reconstruction pipeline, first for the upper bodyand then for the lower body under the condition of theupper-body.",
  ". Human Motion Generation": "Human motion generation is explored under various inputconditions, including text , action la-bels , 3D scenes and objects , and mo-tion itself . Our work shares similarities with twoprimary research streams. The first involves diffusion-basedmotion generation. For instance, is the first to utilize adiffusion model for text-to-motion generation witha transformer network. develops a hierarchical genera-tion pipeline for human-object interaction by generating ini-tial keyframes in the motion sequence and then interpolat-ing between them. Secondly, our approach parallels workslike PoseGPT , MotionGPT , and T2M-GPT in terms of representing human motions with discrete la- tent. These studies also utilize a VQ-VAE to encodehuman motion into a discrete latent space, facilitating thesubsequent generation process.The task we aim to address significantly deviates fromtraditional motion generation tasks like text-to-motion gen-eration, which typically aims to create motions that alignwith textual descriptions. Our goal is distinctly different:we focus on accurately reconstructing human motion usingsolely sparse observations.",
  ". SAGE: Stratified Avatar Generation": "This section introduces the proposed SAGE Network, fol-lowing our observation about the connection relationshipbetween upper-body and lower-body motions. The overallarchitecture of our SAGE Net is shown in . Disentan-gled latent representations for upper-body and lower-bodymotions are learned in (a). Subsequently, as illus-trated in (b), we employ a stratified latent diffusionprocess for full body motion reconstruction.",
  ". Problem Statement and Notation": "Input Signals.Our paper follows the common setting ofHead Mounted Devices (HMDs) inputs for motion gener-ation, in which three sensors mounted on the head and,left and right hands are employed to perceive the corre-sponding joint motions.Formally, the raw input signalsare denoted by a time-dependent vector function X(t) =(xh(t), xl(t), xr(t)), where the subscripts h, l and r indi-cate the head, left hand, and right hand, and all these func-tions are with six degree of freedom for 3D rotation andtranslation under the global coordinate system. Joint rota-tions are represented by a six-axis representation, which hasbeen demonstrated to be more suitable for network learningin previous works . Given a time interval with T sam-pling points, the raw input signals can be denoted in a ma-trix Xraw RT (3(3+6)). To enhance the input signals,we follow to compute the positional velocities and an-gular velocities. This augmentation process adds a 9D inputsignal for each observed joint, resulting in an 18D input sig-nal per joint at every timestamp. By combining these sig-nals for all joints over all timestamps, we form the completesparse input signal, represented as X RT 54. Kinematic Tree and SMPL Representation.As shownin (b), SMPL represents a human pose by a stan-dard skeletal rig, which is widely adopted by current motiongeneration works. A pose j(t) represents the relative rota-tion of joint j at tth frame with respect to its parent in thekinematic tree. The global rotations G(j(t)) can be calcu-lated by:",
  "kA(j)k(t)(1)": "where A(j) denotes the ordered set of joint ancestors ofjoint j.Articulated motion representation based on the humankinematic tree, is key for realistically simulating humanmotions and enables efficient control over joint parameters.However, such an intricate articulated motion representa-tion poses a significant challenge for models to learn effec-tively. In this work, we seek to disentangle this complexrepresentation to enable the model to focus on a limited setof motions and interactions, thereby simplifying the learn-ing process.Nevertheless, separating full-body human motions intodistinct parts is nontrivial due to the complex correlationsamong joints. We revisit the human kinematic tree definedin SMPL model, where the upper and lower half-body isconnected solely via a root joint. This insight from SMPLmodel provides a natural solution to separate the articulatedfull-body motion into two distinct parts: upper-body motionand lower-body motion. Notably, the root joint is includedin both two parts as a central element since the parametersof all other joints in each half-body are defined in the localcoordinate system of the root joint. The Outputs.As discussed in the last paragraph forSMPL representation , the problem of 3D body avatargeneration comes down to the full-body motion estima-tion of 22 joints (including the root joint), denoted in theset function (t) =i(t) SE(3)|t {t1, . . . , tT }asthe expected output of our problem. Based on the discus-sion of SMPL model with the disentangling nature of up-per and lower body, we redefine the set function (t) inthe disentangled way by (t) = upper(t) lower(t),where upper(t)={0(t), . . . , buu (t)},lower={0(t), . . . , bll (t)}. These two subsets have only one in-tersected joint: root joint 0, and bu = 13 and bl = 8 de-note the number of rest joints in the upper and lower body,respectively. For the final output of our method, the dimen-sion of the underlying motion variables is 22 6 = 132 atevery timestamp.",
  ". Disentangled Motion Representation": "In this section, our objective is to disentangle full-body hu-man motions into upper-body and lower-body parts and en-code them to discrete latent spaces. This can effectivelyreduce the complexity and burden of encoding since eachencoding takes care of only half-body motions.We employ two autoencoders, i.e., VQ-VAE ,with identical architecture to learn the discrete latent spacesfor upper-body and lower-body motions, respectively. Asshown in (a), our VQ-VAE model consists of an en-coder and a decoder. The encoder E takes the motion se-quence = {i}Ti=1 as input and encodes it into a series of . The overall architecture of our SAGE Net. It mainly contains two components: (a) Disentangled VQ-VAE for discrete humanmotion latent learning. To facilitate visualization, we incorporate zero rotations as padding for the lower body in the Upper VQ-VAE, andvice versa for the Lower VQ-VAE. Consequently, in the visualizations of the Upper VQ-VAE, the lower body remains in a stationary pose,whereas in the visualizations of the Lower VQ-VAE, the upper body is maintained in a T-pose. (b) The stratified diffusion model, whichmodels the conditional distribution of the latent space for upper and lower motion. This model sequentially infers the upper and lowerbody latents, capturing the correlation between upper and lower motions. By employing a dedicated full-body decoder on the concatenatedupper and lower latents, we can obtain full-body motion. continuous latent E() = H, where H = {hi}T/li=1, and l isthe temporal down-sampling rate of input motion sequence.To quantize the continuous latent, we define the discretemotion latent space by a codebook C = {cj}Nj=1 RND,where N = 512 is the number of entries in the codebookand D = 384 is the dimension of each element cj. Theoperation Q quantizes the continuous latent hi into discretelatent zi by finding its most similar element in C:",
  "zi = Q(hi) = arg mincjChi cj2(2)": "Since continuous latent from all data samples share thesame codebook C, all the real motions in the training setcould be expressed by a finite number of bases in latentspace.Subsequently, the quantified latents Z are fed into thedecoder to reconstruct the original motions, given by =D(Z). The training process involves the joint optimizationof the encoder and decoder by minimizing the followingloss over the training dataset:",
  ". Stratified Motion Diffusion": "After encoding and expressing different human motions aslatents, we aim to properly sample from the latent spacefor full-body motion reconstructions and match the sparseobservations.Although disentangling the full-body motions into upperand lower parts enhances effectiveness and efficiency formotion representation learning, its crucial to include thecorrelation between two body parts during generation. Oth-erwise, severe inconsistency would be witnessed in recon-structed full-body motions. To this end, we propose Strat-ified Motion Diffusion to sample upper-body and lower-body latent in a cascaded manner with explicit considera-tions of the correlations mentioned above.Since the sparse observations are all from the upper body(e.g., head and hand sensors), we first generate upper-bodylatentzup by upper diffusion model conditioning on thesparse observations X.Thus the training objective of the up-",
  "(4)": "where is the random noise from the normal distribution, is the noise predictor of the diffusion model with networkparameters , and k is the diffusion time step.Compared with upper-body latent prediction, directlypredicting the lower-body latentzlow from the same sparseobservations is more challenging due to the absence ofdirect tracking or supervision for any of the lower-bodyjoints. To make the prediction more physically meaning-ful, as shown in (b), we take both the sparse obser-vations X and the generated upper-body latent zup as con-ditions for lower-body latent prediction by lower diffusionmodel. This design considers the correlation between twohalf-body parts and allows more information to be involvedfor the lower-body inference. The objective for lower diffu-sion model training is as follows:",
  "(5)": "Once two half-body latentzup andzlow are obtained,the full-body motions can be recovered with a decoder = Dfull( zup,zlow). Instead of directly using pre-trainedupper and lower decoders in (a) to recover the corre-sponding half-body motions, we train this full-body decoderEfull from scratch together with our stratified motion diffu-sion, which is further optimized to capture the correlationsbetween half-body motions.",
  ". Implementation Details": "Since both sparse observations and human motion occursequentially, we utilize the widely adopted sequential net-work, i.e., transformer , as the backbone network for theencoder and decoder in the disentangled VQ-VAE , andthe denoise network in the stratified diffusion model. We settemporal down-sampling rate l = 2 to balance the compu-tational cost and the performance. In our transformer-basedmodel for upper-body and lower-body diffusion, we inte-grate an additional DiT block as described in . Duringthe training of the latent diffusion model, instead of pre-dicting noise k as formulated by the standard latent dif-fusion model , we follow and directly predictthe latent z itself, as we find that this operation can signifi-cantly reduce the sampled time steps during inference stage.For training decoders, i.e., Eup, Elow and Efull, in additionto the rotation-level reconstruction loss, we incorporate theforward kinematic loss proposed in and the hand lossdescribed in .For the inference stage, we evaluate our model in an on-line manner. Specifically, we fix the sequence length at 20for both the input and the output of our model, and only the last pose in the output motion sequence is retained. Givena sparse observation sequence, we apply our model using asliding window approach. For the first 20 poses in the mo-tion sequence, we predict by padding the sparse observationsequence x at the beginning with the first available observa-tion. We make this choice considering the practicality andrelevance of online inference in real-world application sce-narios. This allows the motion sequences to be predicted ina frame-by-frame manner.In addition, we employ a simple two-layer GRU onthe top of the full body decoder as a temporal memory tosmooth the prediction of the output sequence with minimalcomputational expense, and we term it as a Refiner. To trainthis Refiner, we use the same velocity loss as . Ourmodel takes 0.74ms to infer 1 frame on a single NVIDIARTX3090 GPU.",
  ". Dataset and Evaluation Metrics": "We train and evaluate our method on AMASS , whichunifies multiple motion capture datasets as SMPL representations.We report several metrics for evaluations and compar-isons: mean per joint rotation error (MPJRE) and mean perjoint position error (MPJPE) for measuring the average rela-tive rotation and position error across all joints respectively,as well as the average position error of the root joints (RootPE), hand joints (Hand PE), upper-body joints (Upper PE),and lower-body joints (Lower PE).Besides the above reconstruction accuracy, we also eval-uate the spatial and temporal consistency of the generatedsequences, as it significantly contributes to the visual qual-ity. Specifically, we calculate the mean per joint velocityerror (MPJVE) and Jitter, where MPJVE measures the av-erage velocity error of all body joints, and Jitter quantifiesthe average jerk (time derivative of acceleration) of all bodyjoints. In both cases, lower values indicate better results.",
  ". Quantitative and Qualitative Results": "For a fair comparison, we follow two settings used in previ-ous works for quantitative and qual-itative assessment. Moreover, we propose a new setting inthis paper for a more comprehensive evaluation on currentmethods.In the first setting, as previous works , sub-sets CMU , BMLrub , and HDM05 datasets arerandomly divided into 90% for training and 10% for testing.Besides sparse observations of three joints, we also evalu-ate the performance of all compared methods by using fourjoints as input, including the root joint as an additional in-put, the same as in . We term this setting as S1 in thefollowing.",
  ". Visualization results compared with other methods. All models are trained under setting S1": "Tabs. 1 and 2 show that our method outperforms ex-isting methods on most evaluation metrics, confirming itseffectiveness. For the MPJVE metric, only AGRoL surpasses our method when employing an offline strategy.In this scenario, specifically, AGRoL processes the entiresparse observation sequence in one pass and outputs thepredicted full-body motions simultaneously. This enableseach position in the sequence to utilize the information fromboth preceding and subsequent time steps, offering an ad-vantage in this particular metric. However, its important",
  ". Visualization results on real data": "As shown in Tab. 3, our method achieves comparableperformance with previous works on S2. However, we ob-serve that the testing set of S2 is disproportionately small(i.e., only 1% of the training set). Such a small fractioncannot represent the overall data distribution of the largedataset and may not include sufficiently diverse motions toevaluate the models scalability, causing unconvincing eval-uation results. We introduce a new setting, S3, which adopts",
  ". The visualization comparison for disentanglement. Thedarker the red color, the greater the deviation is between the pre-dicted result and the ground truth": "the same training and testing splitting ratio used in S1. Inthis setting, we randomly select 90% of the samples fromthe 15 subsets of S2 for training, while the remaining 10%are for testing. We train and evaluate the compared methodswith this new setting. reveals that under S3, the per-formance differences between the compared methods aremore significant than S1 and S2. Since the test set has morediverse motions in S3, this benchmark evaluates the mod-els scalability in a more objective way. In this context, ourmethod outperforms existing methods in most metrics, es-pecially in the critical metric of Lower PE, highlighting thesuperiority of our stratified design for lower-body modelingand inference. presents a visual comparison between our SAGENet and baseline methods, all trained under the S1 pro-tocol, which is commonly used by baselines for releasingtheir trained checkpoints. These visualizations demonstratethe significant improvements that our model offers in re-constructing the lower body. For example, in the first rowof samples, baseline methods typically reconstruct the feettoo close to the ground, restricting the avatars leg move-ments. Our model, however, overcomes this limitation, en-abling more flexible leg movements. In the third row, for asubject climbing a ladder, the baseline methods often resultin avatars with floating feet, failing to capture the detailedmotion of climbing. In contrast, our SAGE Net accuratelyreplicates complex foot movements, resulting in more re-alistic and precise climbing animations. We also evaluateour model on the real data, and for fair comparison, we di-rectly use the real data release by . As shown in ,our method also achieves better reconstruction results onthe real data.",
  ". Evaluation results on the conditional strategy of the dif-fusion model under setting S1": "Disentangled Codebook:We establish a baseline usinga unified motion representation to evaluate the disentanglestrategy. Specifically, we developed a full-body VQ-VAEmodel that encodes full-body motion into a single, unifieddiscrete codebook. Other components are the same as theoriginal model. Results shown in the first and the last rowsin , demonstrate that our approach employing dis-entangled latents significantly outperforms the baseline onall evaluation metrics. This demonstrates that the disentan-glement can simplify the learning process by allowing themodel to focus on a more limited set of movements andinteractions. Additionally, shows the visualizationcomparison between our model and baseline model, verify-ing that the disentangle can significantly improve the recon-struction results for the most challenging lower motions.Full-Body Decoder and Refiner:The second and thirdrows of Tab. 5 demonstrate the impact of the full-body de-coder and the refiner, respectively. Compared with utiliz-ing the upper and lower decoder from VQ-VAEup and VQ-VAElow, the full-body decoder facilitates the integration offeatures from both the upper and lower body, improving theoverall accuracy of full-body motion reconstruction. On theother hand, the refiner acts as a temporal memory, smooth-ing out the motion sequence to yield better visualization re-sults.Disentanglement Strategy:To investigate the optimaldisentanglement strategy, we explore an extreme disentan-glement configuration by following the path from the root",
  ". Failure cases. All models are trained under setting S1": "(Pelvis) node to each leaf node along the kinematic tree.Specifically, we break down the body into five segments:the paths from the root to the left hand (a), right hand (b),head (c), left foot (d), and right foot (e). As reported inthe last two rows of Tab. 5, the natural joint interconnec-tions within the upper (or lower) body were disrupted whenfurther disentangling the human body, resulting in perfor-mance drops and complicating the model design.Stratified Inference:Tab. 6 highlights the influence ofour stratified design on the accuracy of lower body pre-dictions. For comparison, we design a baseline that onlyuses the sparse observation for lower body latent generationwithout predicted upper body latent (term as Parallel Dif-fusion in the table). As we focus solely on the reconstruc-tion quality of the lower body here, we use the decodingresults on the generated lower latents from VQ-VAElow toisolate the impact of other modules such as the full-bodydecoder and refiner. We report Lower PE and Jitter of thelower body for comparison. Results show that our strati-fied design markedly improves the accuracy of lower bodypredictions.Limitation:In , both the previous state-of-the-artmethod and our model encounter difficulties in two mainsituations: (1) External Force-Induced Movements (the toprow). (2) Unconventional Poses (the bottom row). The ad-dition of more varied samples to the training dataset canpotentially enhance the models performance in these areas.",
  "We study the problem of human avatar generation fromsparse observations. Our key finding is that the upper and": "lower body motions should be disentangled with respect tothe input signals from the upper-body joints. Based on this,we propose a novel stratified solution where the upper-bodymotion is reconstructed first, and the lower-body motion isreconstructed next and conditioned on the upper-body mo-tion. Our proposed stratified solution achieves superior per-formance on public available benchmarks.",
  "RootMotion Final IK., 2018. 2, 6, 7 Advanced Computing Center for the Arts and Design.ACCAD MoCap Dataset. 5, 6": "Karan Ahuja, Eyal Ofek, Mar Gonzalez-Franco,Christian Holz, and Andrew D. Wilson. Coolmoves:User motion accentuation in virtual reality. Proc. ACMInteract. Mob. Wearable Ubiquitous Technol., 5(2):52:152:23, 2021. 2, 7 Ijaz Akhter and Michael J. Black. Pose-conditionedjoint angle limits for 3d human pose reconstruction. InIEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 14461455, 2015. 5,6 Sadegh Aliakbarian, Pashmina Cameron, FedericaBogo, Andrew W. Fitzgibbon, and Thomas J. Cash-man.FLAG: flow-based 3d avatar generation fromsparse observations.In IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR),pages 1324313252, 2022. 1, 2, 5, 6, 7",
  ", 6": "Angela Castillo, Maria Escobar, Guillaume Jeanneret,Albert Pumarola, Pablo Arbelaez, Ali Thabet, and Art-siom Sanakoyeu. Bodiffusion: Diffusing sparse obser-vations for full-body human motion synthesis. In Pro-ceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV) Workshops, pages 42214231, 2023. 2, 5 Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang,Xiaobo Xia, and Tongliang Liu. Humanmac: Maskedmotion completion for human motion prediction. InIEEE/CVF International Conference on Computer Vi-sion (ICCV), pages 95109521, 2023. 2",
  "Junyoung Chung, C aglar Gulcehre, KyungHyun Cho,and Yoshua Bengio. Empirical evaluation of gated re-current neural networks on sequence modeling. CoRR,abs/1412.3555, 2014. 5": "Andrea Dittadi, Sebastian Dziadzio, Darren Cosker,Ben Lundell, Thomas J. Cashman, and Jamie Shotton.Full-body motion from a single head-mounted device:Generating SMPL poses from partial observations. InIEEE/CVF International Conference on Computer Vi-sion (ICCV), pages 1166711677, 2021. 1, 2, 5, 6,7 Yuming Du, Robin Kips, Albert Pumarola, Sebas-tian Starke, Ali K. Thabet, and Artsiom Sanakoyeu.Avatars grow legs: Generating smooth human motionfrom sparse tracking inputs with diffusion model. InIEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 481490, 2023. 2, 5,6, 7, 8, 1 Saeed Ghorbani, Kimia Mahdaviani, Anne Thaler,Konrad P. Kording, Douglas James Cook, GunnarBlohm, and Nikolaus F. Troje.Movi:A largemultipurpose motion and video dataset.CoRR,abs/2003.01888, 2020. 5, 6 Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou,Qingyao Sun, Annan Deng, Minglun Gong, and LiCheng. Action2motion: Conditioned generation of 3dhuman motions. In ACM International Conference onMultimedia (MM), pages 20212029, 2020. 2 Mohamed Hassan, Duygu Ceylan, Ruben Villegas,Jun Saito, Jimei Yang, Yi Zhou, and Michael J.Black. Stochastic scene-aware motion prediction. InIEEE/CVF International Conference on Computer Vi-sion (ICCV), pages 1135411364, 2021. 2",
  "Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu,and Tao Chen. Motiongpt: Human motion as a foreignlanguage. Advances in Neural Information ProcessingSystems (NeurIPS), 36, 2024. 2": "Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender,Larissa Laich, Patrick Snape, and Christian Holz.Avatarposer: Articulated full-body pose tracking fromsparse motion sensing. In European Conference onComputer Vision (ECCV), pages 443460, 2022. 2, 3,5, 6, 7, 8, 1 Yifeng Jiang, Yuting Ye, Deepak Gopinath, JungdamWon, Alexander W. Winkler, and C. Karen Liu. Trans-former inertial poser: Real-time human motion recon-struction from sparse imus with simultaneous terraingeneration. In SIGGRAPH Asia 2022 Conference Pa-pers, pages 3:13:9. ACM, 2022. 1, 2",
  "Eyes JAPAN Co. Ltd. Eyes Japan MoCap Dataset. 5,": "Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel,and Gregory Rogez. Posegpt: Quantization-based 3dhuman motion generation and forecasting. In Euro-pean Conference on Computer Vision (ECCV), pages417435, 2022. 2, 1 Naureen Mahmood, Nima Ghorbani, Nikolaus F.Troje, Gerard Pons-Moll, and Michael J. Black.AMASS: archive of motion capture as surface shapes.In IEEE/CVF International Conference on ComputerVision (ICCV), pages 54415450, 2019. 2, 5, 6 Christian Mandery, Omer Terlemez, Martin Do, Niko-laus Vahrenkamp, and Tamim Asfour.The KITwhole-body human motion database. In IEEE Inter-national Conference on Advanced Robotics (ICAR),pages 329336, 2015. 5, 6 Wei Mao, Miaomiao Liu, and Mathieu Salzmann.Generating smooth pose sequences for diverse humanmotion prediction. In IEEE/CVF International Con-ference on Computer Vision (ICCV), pages 1328913298, 2021. 2",
  "Danilo Jimenez Rezende and Shakir Mohamed. Vari-ational inference with normalizing flows.In Inter-national Conference on Machine Learning (ICML),pages 15301538, 2015. 2": "RobinRombach,AndreasBlattmann,DominikLorenz, Patrick Esser, and Bjorn Ommer.High-resolution image synthesis with latent diffusion mod-els.In IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1067410685, 2022. 2, 5 Leonid Sigal, Alexandru O. Balan, and Michael J.Black. Humaneva: Synchronized video and motioncapture dataset and baseline algorithm for evaluationof articulated human motion. Int. J. Comput. Vis., 87(1-2):427, 2010. 5, 6",
  "Xinyu Yi, Yuxiao Zhou, and Feng Xu.Transpose:real-time 3d human translation and pose estimationwith six inertial sensors. ACM Trans. Graph., 40(4):86:186:13, 2021. 1, 2": "Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shi-mada, Vladislav Golyanik, Christian Theobalt, andFeng Xu. Physical inertial poser (PIP): physics-awarereal-time human motion tracking from sparse inertialsensors. In IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1315713168, 2022. 1, 2 Jianrong Zhang, Yangsong Zhang, Xiaodong Cun,Shaoli Huang, Yong Zhang, Hongwei Zhao, HongtaoLu, and Xi Shen. T2M-GPT:generating human mo-tion from textual descriptions with discrete represen-tations. In IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), 2023. 2",
  "Chuanxia Zheng and Andrea Vedaldi.Online clus-tered codebook.In IEEE/CVF International Con-ference on Computer Vision (ICCV), pages 2274122750, 2023. 3": "Xiaozheng Zheng, Zhuo Su, Chao Wen, Zhou Xue,and Xiaojie Jin.Realistic full-body tracking fromsparse observations via joint-level modeling.InIEEE/CVF international conference on computer vi-sion (ICCV), 2023. 2, 5, 6, 7, 8, 1 Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang,and Hao Li. On the continuity of rotation represen-tations in neural networks. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR),pages 57455753, 2019. 3",
  "A.1 Input sequence length": "Our model adheres to the online inference setting, where itprocesses sparse tracking signals from the past N framesand predicts the full body motion of the final frame as donein . As indicated in , the length of theinput sequence is a critical factor affecting the models per-formance, involving a balance between efficiency and ef-fectiveness. Therefore, it is essential for our model to effec-tively tackle shorter sequences, as this not only maintainsperformance but also significantly reduces computationalcosts.We examine AvatarJLM and our method with differ-ent input lengths N under setting S1, as presented in Tab. A.The results demonstrate that our proposed SAGE Net ismore robust to variations in the input sequence length com-pared to the baseline method, AvatarJLM . Notably,SAGE Net is able to exceed AvatarJLMs performance evenwhen utilizing just a quarter of their sequence length (10frames for our method compared to 40 frames for Avatar-JLM).",
  "A.2 Predicting noise": "Our SAGE Net follows the approach of previous meth-ods by directly predicting the raw data during thediffusion process, specifically the clean latent z0 in our con-text. In this subsection, we adapt the diffusion process topredict the residual noise instead of z0, while maintainingall other components as they are, to validate the effective-ness of this design choice. Results are detailed in Tab. B.We observe that compared with predicting the noise , thisstrategy leads to enhanced performance.",
  "B.1 Disentangled VQ-VAE": "The VQ-VAEup and VQ-VAElow follow the architecturein , unitizing a 4-layer transformer network . Eachof these transformer layers includes a 4-head self-attentionmodule and a feedforward layer with 256 hidden units.For the training of VQ-VAEs, we employ a set of lossterms including a rotation-level reconstruction loss, a for-ward kinematic loss as proposed in , and a hand loss asproposed in with batch size of 512. Adam optimizer",
  "B.2 Stratified Diffusion": "In our transformer-based model for upper-body and lower-body diffusion, we integrate an additional DiT block as de-scribed in . Each model features 12 DiT blocks, eachwith 8 attention heads, and an input embedding dimensionof 512. The full-body decoder is structured with 6 trans-former layers.The diffusion process is trained with 1000 samplingsteps, employing the squaredcos cap v2 beta schedule.For this schedule, we set the starting beta value at 0.00085and the ending beta value at 0.012.The training of theupper-body diffusion model, lower-body diffusion model,and the full-body decoder Dfull, is conducted sequentially.Each component is trained with a batch size of 400, usingthe Adam optimizer. We set the weight decay at 1e-4 andbegin with an initial learning rate of 2e-4. The learning rateundergoes a reduction by a factor of 0.25 at the milestoneepochs of 20 and 30.",
  "final = + res(6)": "For achieving a balance between smoothness and accuracyin the predicted motion sequences, we adopt various lossterms previously utilized in related research . Theseinclude the rotation-level reconstruction loss Lrec, the ve-locity loss Lvel, and the forward kinematic loss Lfk.In addition, we design a new loss term jitter loss Ljitterto directly control the jitter:"
}