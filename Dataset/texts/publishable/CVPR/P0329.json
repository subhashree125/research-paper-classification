{
  "Abstract": "Automating visual inspection in industrial productionlines is essential for increasing product quality across var-ious industries.Anomaly detection (AD) methods serveas robust tools for this purpose. However, existing pub-lic datasets primarily consist of images without anomalies,limiting the practical application of AD methods in pro-duction settings. To address this challenge, we present (1)the Valeo Anomaly Dataset (VAD), a novel real-world in-dustrial dataset comprising 5000 images, including 2000instances of challenging real defects across more than 20subclasses.Acknowledging that traditional AD methodsstruggle with this dataset, we introduce (2) Segmentation-based Anomaly Detector (SegAD). First, SegAD leveragesanomaly maps as well as segmentation maps to computelocal statistics. Next, SegAD uses these statistics and anoptional supervised classifier score as input features for aBoosted Random Forest (BRF) classifier, yielding the finalanomaly score. Our SegAD achieves state-of-the-art perfor-mance on both VAD (+2.1% AUROC) and the VisA dataset(+0.4% AUROC). The code and the models are publiclyavailable1.",
  ". Introduction": "Within the manufacturing process, industrial visual inspec-tion plays a crucial role in identifying defects in producedcomponents. This operation holds significant importance inminimizing costs by identifying and removing faulty partsearly in the production stages and, more importantly, in pre-venting the dispatch of defective components to customers.Traditionally, this task has relied on human operators; how-ever, the likelihood of overlooking certain defects can be ashigh as 25% for specific defect types .When the inspected product comprises numerous com-ponents, its examination may create a bottleneck in the pro-duction process, causing delays across the entire line. While conventional computer vision methods applied to this taskdemonstrate superior speed and lower error rates comparedto human operators , their inflexibility and lack of sat-isfactory accuracy limit their effectiveness.Industrial deep learning anomaly detection has been anactive research field in recent years. Most of anomaly de-tection methods use only good images for training and tryto detect deviations from the training data .Development of the new methods is restrained by pub-licly available datasets. Recent industrial anomaly detec-tion datasets typically contain approximately one hundred(or even fewer) abnormal images, showcasing defects in thetesting set only . This poses a challenge for super-vised anomaly detection methods aiming to utilize both nor-mal and defective parts during training. Supervised modelsoften undergo training with just ten abnormal images fromthe testing set, resulting in overfitting and reduced sensitiv-ity to previously unseen defects .In response to the limitations of current datasets, weintroduce and publicly release VAD (Valeo AnomalyDataset), which contains 1000 bad and 2000 good parts inthe training set and 1000 bad and 1000 good parts in thetest set, see (a). Among the defective parts used fortesting, 165 contain specific defect types not present in thetraining data; these parts are explicitly labeled in the re-leased dataset. All images in VAD are captured from anactual production line, showcasing a diverse range of de-fects, from highly obvious to extremely subtle. This datasetbridges the gap between the academic community and theindustry, offering researchers the chance to advance the per-formance of methods in tackling more intricate real-worldchallenges.Current approaches to supervised anomaly detection ei-ther yield unsatisfactory results; see Tab. 3, or demandpixel-level labels for defective parts, as seen in .Consequently, we introduce a novel method named SegAD(Segmentation-based Anomaly Detector), which is de-scribed in (b). The employed approach eliminates theneed for pixel-level labels, requiring only a flag for eachimage. Anomaly map scores from each segment are used",
  "arXiv:2405.04953v2 [cs.CV] 11 May 2024": ". Overview of our contributions. (a) VAD, a real-world industrial dataset designed for supervised anomaly detection with complexdefects. (b) SegAD, our method that leverages anomaly maps extracted from segmented outputs of one or more anomaly detectors.Higher-level statistical features are computed on these maps, such as skewness, kurtosis, or mean, to generate the local anomaly features.Additionally, our SegAD provides the flexibility to use the output of a supervised classifier score with the local anomaly features, creatinginput for a final Boosted Random Forest (BRF) classifier that yields the final score. to delineate the parameters of the distribution, emphasizingrelatively low scores in important areas while disregardinghigher scores in noisy regions. In this context, the preciseposition and appearance of the anomaly diminishes in im-portance as long as it resides within a designated segment.Our main contributions can be summarized as follows: We propose a supervised anomaly detection dataset withcomplex objects and a large variety of defects. As an ad-dition, we establish a one-class anomaly detection bench-mark, which is more challenging than most of the cur-rent ones, a supervised benchmark with a high number ofbad images for training (1000 images) and a supervisedbenchmark with a low number of bad images for train-ing (100 images). We share the dataset freely with theresearch community. We create a novel supervised anomaly detection methodcalled SegAD. This approach performs noticeably bet-ter than recent anomaly detectors alone while retainingthe ability to detect unknown defects. SegAD reachesSOTA results on VAD as well as on the established VisAdataset .",
  "In 2019, the MVTec AD dataset was introduced. Thisdataset significantly advanced anomaly detection methods": "by providing images with authentic defects and realistic ob-jects, in contrast to earlier datasets predominantly compris-ing textures and simple defects . MVTec LOCOAD highlighted the challenge of logical defects, such asmissing or misplaced parts, necessitating anomaly detectionmethods capable of capturing the global context of the im-age. The more recent VisA dataset expands on this byincorporating diverse objects with complex structures, mul-tiple instances, and variations in scale.Existing datasets often contain simulated defects [3, 4, 38], creating a domain gap between research and practicalapplications. This complicates the deployment of anomalydetection methods in real-world settings. To promote super-vised anomaly detection, we propose the Valeo AnomalyDataset (VAD), a real-world industrial dataset with variousdefects, including logical ones.Another notable issue stems from the saturation ofsolved datasets, where other methods are influenced moreby dataset-specific design choices than general applicabil-ity. Compared to this, our dataset introduces a broader rangeof challenging and diverse anomalies and high intra-classvariability of good images.",
  ". Methods": "To define the terms we are working with, one-class meth-ods are commonly called unsupervised in the context ofanomaly detection . We use one-class as amore accurate name since these methods use labeled im-ages for training. It is important to distinguish such meth-ods from ones that work with unlabelled data, as in .Models that use bad and good images for training are usu-ally called supervised, which is the name we also adopt. . Comparison of different anomaly detection datasets. Tr. good and Tr. bad show the average number of images per class fortraining. tex = texture, obj = object, Log. def. = Logical defects, Pix. labels = pixel-level labels and wl = weakly labelled (ellipse aroundthe defect).",
  "DatasetYearClassesImagesTypesDefectsTr. goodTr. badLog. def.Pix. labels": "DAGM 20071011500texsynthetic50075nowlKSSD 20191399texreal-world11517noyesMVTec AD 2019155354tex, objsimulated2420noyesKSSD2 202113335texreal-world2085246noyesBTAD 202132830tex, objreal-world6000noyesLOCO AD 202253644objsimulated3540yesyesVisA 20221210821objsimulated7200noyesVAD (ours)202315000objreal-world20001000yesno One-class Anomaly Detection.PatchCore relies ona pretrained feature extractor model to extract features fromthe training set into a memory bank and reduce the sizeof a memory bank using coreset subsampling.Featuresextracted from the new input are compared to their near-est neighbors in the memory bank. FastFlow uses asimilar feature extractor model and maps extracted featuresto the Gaussian distribution using normalizing flow .RD4AD utilizes a teacher-student architecture, whichcombines features from several layers of feature extractorto eliminate redundant ones. EfficientAD introducesmany innovations, including using an autoencoder to detectlogical defects as an addition to a classic teacher-student ar-chitecture and their own pretrained feature extractor, whichimitates the behavior of a bigger model with a noticeablylesser inference time. Supervised Anomaly Detection. In theory, leveragingdefective parts for training is advantageous for refining classboundaries . However, practical challenges arise, suchas the small size of anomalies and the impossibility of col-lecting all potential defects . Several recent anomaly de-tection methods use both good and bad images ,but some of them overfit to seen defects, and others re-quire pixel masks for defects to calculate loss or to gen-erate new defects, which can be problematic in a real-worldcase. Real-world datasets (as VAD) might contain logicaldefects that cannot be generated by existing defects genera-tion strategies, which usually cut and paste existing defectswith various modifications , which is not going toimprove results for a wrong wire shape as or other similartypes of defects. Very few supervised anomaly detection benchmarks areavailable; the most popular is Supervised Anomaly Detec-tion on MVTec AD. DevNet is one of the first su-pervised anomaly detection models that tried to solve in-dustrial MVTec AD dataset, compared to earlier methodswhich were mostly addressing Out-of-Distribution problem using non-industrial datasets such as MNIST or CIFAR-10 . DRA uses several heads to learn bothseen and pseudo anomalies, as well as normal examples,which should reduce overfitting on seen anomalies. Thesemethods perform well on MVTec AD but might fail on morecomplex problems, as shown in Tab. 3 and Tab. 5. Com-pared to modern supervised anomaly detection methods,SegAD (ours) performs better than current SOTA methods,even on complex problems without pixel-level labels, ex-tensive augmentation, and long training.",
  ". Valeo Anomaly Dataset (VAD)": "VAD consists of one class with predefined training and test-ing sets. The training set contains 1000 bad and 2000 goodimages, and the testing set contains 1000 bad, 165 of themare unseen bad, and 1000 good images. Unseen bad im-ages in the test dataset refer to several rare defect typesthat are not present in the training data, several examplesof which are shown in . Having such images is impor-tant to avoid turning this anomaly detection problem intoa supervised classification problem. Some images mighthave a thin black border at the bottom due to how they werefilmed, an example can be seen in . Defects mightoccur in the whole area of the image. Image-level annota-tion is provided, but there is no pixel-level annotation dueto the complexity of defects and the fact there is no exactposition for missing or misplaced components. All imagesare 512 512 pixels in PNG format. Examples of all typesof defects can be found in Appendix C.Good parts consist of two wires connected to two pinson one side and two soldering dots on the other, placed ona piezo. Piezo means piezoelectric element, a big roundarea under the other components. Wires should have theright shape, not too straight, too long, or bent too much.Wires should connect the soldering dots as close to theircenters as possible. Some amount of deviation is allowed,but if the wire is too close to the side of a solder dot, it isa defect. Soldering should be well-rounded, not too small,and placed in the correct location, not overlapping with the",
  ". Good parts. Small scratches on the piezo are allowed, aswell as wire being closer to the side of the soldering. (VAD)": ". Variety of defects: logical, from the top left: wire onthe side, wire out of solder dot, missing wire, bad wire shape (benttoo much). Structural: soldering paste pollution, crack on a piezo,burned area under the solder dot, broken piezo. (VAD) piezo contours. The wire should be properly connected tothe pin without any pieces of it visible on the side of thepin. The piezo can have different textures. Any kind ofpollution is considered to be an anomaly, which makes thepart defective.Logical defects result from components being misplacedor misshapen rather than being damaged, e.g.bad wireshape, bad soldering shape, wrong wire position, wrongsoldering position, and missing parts. Structural defects in-clude various cracks and broken parts of the piezo, as wellas a variety of burns and pollution. For both types of de-fects, see . The importance of logical defects is of-ten overlooked in the existing datasets (excluding MVTec",
  ". Benchmarks": "We establish three benchmarks for the VAD dataset. Thefirst is one-class anomaly detection. As shown in Tab 2,even SOTA methods struggle to perform well on our datasetcompared to other popular datasets, which leaves an open-ing for further research. For this benchmark, no bad imagesare used for training. The second benchmark is high-shotsupervised anomaly detection, which uses all 1000 badimages from the training set. The last benchmark is low-shot supervised anomaly detection. Only 100 bad imagesfrom the training set are used. These images are randomlyselected using 5 seeds, with the average result over all seedsbeing expected. The same 2000 good parts are used for thetraining in all benchmarks.",
  "SegAD works with image data at its input, and after infer-ence, it is expected to output a score that describes a prob-ability of anomaly appearance. Let I =0, . . . , 255W,H,3": "be a space of all images of width W, height H and with 3color channels. The ith image from a dataset is marked asI(i) I. We denote x to be a 2D pixel coordinate vector.SegAD inference consists of three consecutive stages in apipeline. First is anomaly map(s) calculation with optionalsupervised classifier score calculation. Next is the calcula-tion of simple statistics from anomaly maps over individ-ual segments. Lastly, the obtained statistics, and optionallyalso the classifier score, are used as input features for a BRFclassifier that delivers the final result.Let us denote K as the number of used pixel-wiseanomaly detectors and mark them as functions fk(I) : I RW,H, k {1, . . . , K}. A supervised classifier is a func-tion g(I) : RW,H R.Next, we introduce a set ofL mutually exclusive segments S = {s1, . . . , sL}, wheresl 0, 1W,H.Afterthecalculationofanomalymapsforallfk(I),themasksslareusedforsegment-wise extractionofresults,followedbythecalcula-tion of several statistics.The final feature vectorf=g(I), [qL]Kk=1, [zL]Kk=1, [cL]Kk=1, [mL]Kk=1,ex-tracted from one image, is the concatenation of 4 vectorscontaining different statistics.qLk stands for a vectorof L numbers where each stands for a 99.5% quantilefrom anomaly map fk(I) over pixels where sl(x) = 1.Similarly, for every anomaly map fk(I) we calculate avector for skew zLk , kurtosis cLk and mean mLk . The lengthof the feature vector f equals to K L 4 + 1.For notation simplicity, we keep the supervised classifierscore g(I) always present in f, but it may be omitted, whichis clearly defined in every experiment setup. Feature vectorsf (i) are used as inputs for training the final classifier ,which, at inference time, delivers the final result.",
  ". Training": "Training SegAD requires two separate subsets from thetraining dataset. The first subset is used to train base mod-els fk, the second subset is used to train the BRF classifier.To create these subsets from one training dataset, we splitavailable good images into two parts. For VAD, we reportaverage results over 5 different splits to show more reliableresults.Each model (fk, g, BRF) is trained independently. Train-ing can be separated into two stages.In the first stage,anomaly detector(s) fk are trained using the same data. Inthe same stage, a segmentation map S must be defined. Aswill be elaborated in more detail in Sec. 5, the segmentationmap can be static or produced by a segmentation model, de-pending on the dataset. A supervised classifier g is trainedseparately on 80% of the full training set (1600 good and800 bad parts for VAD) to ensure that the training sets forthis model and SegAD are not fully overlapping. The nextstage is to train a final BRF classifier in SegAD, which re-quires both good and bad images, bad images can be re-placed with artificial defects in some cases, as shown in Tab.2. BRF is trained with the output of the models from theprevious stage.An important thing to note is that SegAD is very fast totrain as long as you already have other models available.Extraction of features and training the final classifier takesnoticeably less time than training anomaly detectors. Thesame is true for the inference; SegAD adds a negligibletime to processing the image while improving results sig-nificantly.",
  ". Setups for SegAD": "SegAD can use any anomaly detector(s) fk as long as it pro-duces an anomaly map showing pixel-level anomaly scores,although, in this paper, we use only one-class anomaly de-tectors because they require less data to train. Any super-vised classifier g that returns a classification score can be used. In experiments, we use several setups, which will bedescribed here in detail. SegAD is denoted as Ours. Super-vised classifier g is not used unless stated otherwise.Anomaly Detector + Ours means K= 1, f1=Anomaly Detector name. This setup allows us to showimprovement for one anomaly detection model in particularand demonstrate that this improvement can be achieved fordifferent types of anomaly detectors.All AD + Ours denotes that all anomaly detectors fromthe list AllAD of length N are used to produce anomalymaps. Such a list can be found in the caption of the table.In that case K = N, f1 = AllAD1, . . . , fN = AllADN.Anomaly detectors might use an ensemble of feature extrac-tors, similarly to , which makes them slower but can im-prove the result. In our case, anomaly detectors themselvescan be successfully assembled in a supervised way.All AD + Supervised Classifier + Ours denotes a setupin which g = Supervised Classifier name, and the rest isthe same as in the previous setup. A supervised classifierallows us to detect seen defects, which can be invisible toa one-class classifier, similarly to . Such a setup showsthe best result for academic purposes, but the speed of infer-ence can be unsatisfying for real-world applications. We ex-pect that, in that case, the number and selection of anomalydetectors can be optimized to fit the problem as well as thetime constraints.Anomaly Detector + Supervised Classifier + Oursmean a setup in which g = Supervised Classifier name,K = 1, f1 = Anomaly Detector name. It shows an ex-ample of an optimization mentioned in the previous setup.Only one anomaly detector gives a higher inference speedcompared to the ensemble of anomaly detectors while giv-ing a satisfying result.",
  ". Experiments": "Evaluation metrics. We compare results using the follow-ing metrics: (1) Area Under the Receiver Operating Charac-teristic Curve (AUROC) , (2) False Positive Rate (FPR)at 95% True Positive Rate (TPR) denoted as AUROC, which describes the performance onthe image level for multiple thresholds, denoted by Cl. AU-ROC. FPR@95TPR shows the percentage of misclassifiedgood parts (false positive) at 95% of bad parts classified cor-rectly (true positive). We report mean results standarddeviation.Implementation and evaluation details. SegAD usesthe output of one-class detectors PatchCore , FastFlow, Reverse Distillation (referred to as RD4AD), Ef-ficientAD either alone or several anomaly detectors atonce. SegAD results are compared to the results of thesemodels alone, as well as supervised anomaly detectors De-vNet and DRA . We also compare our results tothe supervised classifier Wide ResNet50 (denoted as WRN). The final classifier BRF was trained with the sameparameters for all datasets using XGBoost library . Moredetails on the implementation can be found in AppendixesA and B.Images are resized to W = H = 256 pixels to make thecomparison less biased. The DRA model originally uses aresolution 448 448, which shows better results on VADthan 256 256, 92.8 Cl. AUROC, which is still lower thanthe top results. The higher resolution also improves resultsfor some of the other models, so we do not use it. 256 256 is sufficient for defects to remain visible, but it alsoallows to have a reasonable speed of inference, especiallyfor anomaly detection models.For VAD, results averaged for 5 seeds and training runsare reported. These seeds are used for training and split-ting the training set for SegAD into two sets, as describedin Subsection 4.1. 2000 good parts from the train set weresplit in half: 1000 for training the base model(s) and 1000for training SegAD. 0, 1000, or 100 bad parts (dependingon the benchmark) were also used to train SegAD. Detailson the VisA benchmark can be found in Subsection 5.4No augmentations were used for training, and existing aug-mentations were removed from used methods. Augmenta-tions tend to be suitable only for specific tasks (as rotationin DRA or DevNet, removing it improved results by thesemodels on the VAD drastically), and using augmentationsmakes it hard to compare models themselves.We include three benchmarks for the new VAD and oneadditional for the VisA dataset . We compare resultsfor VAD for one-class, high-shot, and low-shot supervisedbenchmarks, as described in Subsection 3.1. The bench-mark for the VisA dataset is explained in 5.4. The segmen- . Static segmentation map for objects in VAD dataset,overlayed over the image. The image is separated into L = 7segments: background, outer half-circle, piezo border, solder dotsarea, wires area, pins area, and piezo in the middle.",
  "tation maps for VAD is the same static image for every inputimage from the dataset. The segmentation map can be seenin . Due to the fact that the object in the VAD is al-": "ways centered with the constant size and component place-ment, this is the simplest choice, which may be sub-optimal,but serves the purpose and does not require any manual ormodel-inferred annotation of the data. It was created to sep-arate the anomaly maps into several meaningful parts, suchas piezo border, wire areas, soldering area, etc., which cancontain different defects and different levels of falsely highscores on the anomaly map. Another source of segmenta-tion maps can be a specially trained segmentation model ora zero-shot segmentation model.",
  ". VAD, one-class benchmark": "This benchmark allows only good images for training, butSegAD requires bad images as well, so we try to replacethem with artificially generated defects. Because SegADworks with anomaly maps, generating realistic defects isunnecessary.For this reason, we have applied severalsimple augmentations to the 1000 good images availablefor training SegAD. These augmentations include Gaussianblur or a randomly placed, randomly sized thin rectangle ofa random shade of gray. They were applied to randomly se-lected segments or random parts of the segments; segmentsare defined with the segmentation map. Code for generatingdefects can be found in the SegAD GitHub repository.",
  ". One-class benchmark (VAD). The best result is markedin bold. All AD means PatchCore, FastFlow, and RD4AD": "Results in show that even such a naive approachimproves results compared to anomaly detection methodsalone. Even greater improvement can be achieved by us-ing several anomaly detectors to produce anomaly maps forSegAD. This strategy fails for EfficientAD + Ours, anomalymaps produced by EfficientAD for the generated defects aretoo different from anomaly maps for real defects, creatinga large discrepancy between training and test distributionsfor SegAD and causing worse results compared to anomalydetector alone.",
  ". High-shot supervised benchmark (VAD). The best re-sult is marked in bold.All AD means PatchCore, FastFlow,RD4AD, and EfficientAD. Improvement calculated compared tobase method results in Tab. 2": "The high-shot benchmark allows training supervisedclassifier g, which shows competitive results as can be seenin Tab. 3 in the row WRN, but it wont be able to detectunseen defects. Histograms in visualize this problem.On the left is the one-class anomaly detector, which candetect unseen defects (in red) but shows a bad separationbetween good and bad parts. In the center, the supervisedclassifier g shows a better separation of good and bad parts,but unseen defects distribution is shifted to the left becausetheir scores are relatively low and such defects cannot bedetected. On the right, SegAD shows a good separation be-tween classes, but also unseen defects have a similar distri-bution to the seen defects. We still compare the supervisedclassifier Wide ResNet50 (WRN) with other methods, butthe ability to detect unseen defects is crucial in real-worldapplications. . Distributions of scores in the VAD test set. Green colorshows good parts, orange bad parts, and red bad parts with unseendefects. (a) shows one-class anomaly detector PatchCore, (b) su-pervised classifier Wide ResNet, (c) SegAD (PatchCore + WRN +Ours). X axis is score values, Y axis is frequency.",
  ". VAD, low-shot supervised benchmark": "This benchmark uses just 100 bad images for training,which can be a relatively low number for a dataset withmore than 20 types of defects. However, such a setting iscloser to real-world applications, where a limited amount ofdefective images can be available. In , WRN showsmuch lower results due to a smaller training dataset, as wellas DevNet and DRA. SegAD also performs worse, thanwith a high-shot benchmark, but it still can visibly improvethe performance of other models. We do not use WRN inSegAD for this benchmark, because it showed low resultsby itself.",
  ". Results on VisA dataset": "VisA contains fewer good images per class for training thanVAD (500-1000 in VisA, compared to 2000 in VAD). Thetraining set consists of good images only. For this reason,we use a different strategy to split the training dataset tomake sure we have enough images to train the base anomalydetection model with a competitive result. For RD4AD,the training set was divided into 90% of images to trainthe RD4AD itself and 10% to train SegAD. EfficientADuses a validation set (10% of good images from the trainset) to normalize anomaly maps. In our case, the same im-ages were used to train SegAD. The segmentation maps forthis dataset were created with the Segment Anything Model(SAM) . For most classes, it is a binary mask for theobject and background, and for pcb1-4, static componentswere mapped on top of it; see .",
  ". Results on VisA dataset, supervised benchmark. The bestresult is marked in bold. All AD means RD4AD and EfficientAD": "With VisA, we create a new, challenging, supervisedbenchmark on a public dataset with 12 different classes.In a similar way to the supervised benchmark on MVTecAD , we move 10 randomly selected bad images fromthe test set to the train set, reporting results over 10 differ-ent seeds. In , this benchmark shows how SegADperforms on a different data from VAD and creates a newpossibility to compete on a difficult supervised anomaly de-tection problem. Detailed results per class can be found inAppendix D.",
  ". Ablation study": "We perform an ablation study on the VAD high-shot super-vised benchmark. We use the average result of the Patch-Core, FastFlow, RD4AD, and EfficientAD as the baseline inthe , denoted as An.Det.. These anomaly detectorsuse the maximum value from the anomaly map to calculatethe score. On the contrary, we compute our proposed fea-tures f, which are described in , and use BRF toyield a final score. This shows an improvement of 1.3 Cl.AUROC in the row One Seg.. In addition, we evaluatethe segmentation maps impact by computing the anomalymaps maximum value per segment. Doing this gives an im- provement in 2.1 Cl. AUROC, which can be seen in the rowMax.. This shows how both features and segmentationmaps are important for SegAD. We also show the strengthof BRF by comparing it against other algorithms. We com-pare BRF with BT and RF using a segmentation map andfeatures. The results show that the BRF is better than thesemethods by 1.7 and 0.3 Cl. AUROC, respectively.",
  "SegAD90.1": ". Ablation study on the methodical differences betweenour method (SegAD) and anomaly detection methods. An.Det. =Anomaly Detectors. One Seg. = One Segment. Max. means usingmaximum value. BT = Boosted Tree. RF = Random Forest. BRF= Boosted Random Forest. SegAD denotes the average result fromPatchCore + Ours, FastFlow + Ours, RD4AD + Ours, EfficientAD+ Ours. Limitations:SegAD (ours) requires to have segmenta-tion maps. Static maps can be used for objects like the prod-uct in VAD. Obtaining segmentation maps for unalignedobjects can be more difficult, yet SAM or a speciallytrained segmentation model may be a solution. The perfor-mance improvement of SegAD depends on the complexityand structure of the objects. For less complex objects inVisA, improvement is lower than for VAD, as can be seenin Tab. 3 and Tab. 5. Another possible limitation can be thedifference in the anomaly maps between training and testdata. Due to the nature of anomaly detection, this problemmight be impossible to mitigate, because our task is to de-tect such differences.",
  ". Conclusion": "This study introduces VAD, a brand new supervisedanomaly detection dataset derived from real production, of-fering challenging benchmarks for the research communityto address real-world defect detection.Additionally, wepropose SegAD, an innovative supervised anomaly detec-tion method that achieves state-of-the-art performance onVAD as well as on a supervised benchmark on standardVisA dataset. Experimental results reveal the limitationsof existing supervised anomaly detection methods in han-dling complex problems, while the integration of SegADwith top-performing one-class anomaly detection modelsfurther enhances their results.",
  "A.1. Final Classifier": "We use the same Boosted Random Forest (BRF) for all experiments with SegAD, except for the Ablation Study, which alsouses Random Forest (RF) and Boosted Tree (BT). Implementation by XGBoost is used for all three of them. BRF in allexperiments uses 10 estimators and 200 parallel trees with a learning rate of 0.3 and binary:logitraw objective. Maximumdepth is set to 5 to avoid overfitting; for the same reason, we set the subsample ratio of columns when constructing each tree(colsample bytree) to 0.6, the same as the subsample ratio per node (colsample bynode) and subsample. We also use the L1regularization with a value of 1. For RF, the learning rate is 1.0, the number of estimators is 1, and the number of treesis 2000. For BT, the number of estimators is 2000, the number of trees is 1, colsample bytree is default.",
  "A.2. Generating Defects": "Generating defects for the one-class benchmark is not optimized and is used just as a demonstration that SegAD can work withone-class tasks as well. In our case, it was sufficient to generate disturbances in the image, which caused anomaly detectorsfk to show higher anomaly scores in their output, which was sufficient to train SegAD. The code to generate defects can befound in the SegAD GitHub repository. Several examples of generated defects can be seen in . Advanced strategies forgenerating defects, which are described in , can be investigated in future work.",
  "B. Implementation Details for Other Evaluated Methods": "In this section, we describe implementation details and training parameters for other methods used in this paper. All modelsuse resolution 256 256 and pretrained feature extractor WideResNet-50-2 by TorchVision unless stated otherwise. Noaugmentations were used. All existing augmentations were removed to have a more fair comparison and to avoid fitting aspecific problem. These removed augmentations include center crop for PatchCore, which improves results as long as defectsare close to the center of the image, e.g., for MvTec AD, see EfficientAD paper for more details and random rotation forDRA and DevNet which made their results worse for VAD. Models were trained without early stopping.Wide ResNet. WideResNet-50-2, pretrained weights by TorchVision . Trained with a batch size of 4, the learningrate is 0.025, uses focal loss . The optimizer is SGD with a momentum of 0.9.PatchCore. Unofficial implementation by Akcay et al. . The feature extractor uses layers 2 and 3. The coreset samplingratio is 0.01, and the number of neighbors is 9.Reverse Distillation. Unofficial implementation by Akcay et al. . The feature extractor uses layers 1, 2, and 3. Beta1is 0.5, beta2 is 0.999. Anomaly maps are combined through addition. Trained for 200 epochs.FastFlow. Unofficial implementation by Akcay et al. . Trained for 200 epochs (similar as in EfficientAD paper ),learning rate is 1e 3, weight decay is 1e 5.EfficientAD. Unofficial implementation by Nelson3, the original code was not published. Trained for 70000 iterationswithout an early stopping. 10% of the training dataset was used for normalization. Uses padding in convolutional layers. The model size is medium.DevNet. Official implementation by Pang et al. . The feature extractor is ResNet18 by TorchVision . For VAD,the code was modified to use bad images from a separate folder instead of a testing set. For VisA, the code was modified tosplit bad images from the test set with predefined seeds, the same as for SegAD (ours). The parameter n anomaly (number ofbad images used for training) was set to 1000 for high-shot, 100 for low-shot VAD, and 10 for VisA. Trained for 50 epochs.DRA. Official implementation by Ding et al. . The feature extractor is ResNet18 by TorchVision . The code wasmodified similarly to DevNet, with the same values for n anomaly. Trained for 30 epochs.",
  "D. Detailed results for VisA": "In this section, we put the results for our supervised benchmark for VisA per class. It is worth noticing that the VisA paper already has two supervised benchmarks, which were not widely adopted by the anomaly detection community. Theirfirst supervised benchmark includes 60% of all images, good and bad, used for training and the rest for testing. VisA paperalready reports results of 99.7 Cl. AUROC with the supervised classifier used together with their method SPD. Such a highresult for a supervised method can be explained by the fact that VisA contains a low number of anomaly types per class (4.7on average), so 60 bad images per class for training were sufficient to recognize the rest. Their second supervised benchmarkuses just 10 good and 10 bad images, and it also has not become too popular because anomaly detection methods usually tryto utilize many good images, which are much easier to get than bad for real-world problems. There are very few methods thatcan tackle such problems successfully, such as PatchCore . Our supervised benchmark for VisA is an attempt to fix theseproblems. We have both a high number of good parts, as well as 10 randomly selected bad parts for training, in a similar wayto the popular supervised MVTec AD benchmark 4. Results per class are shown in .",
  ". Results on VisA dataset, Cl. AUROC values for different classes. The best results are shown in bold. All AD means RD4AD andEfficientAD": "Our method falls short with one class, capsules. This might be explained by some of the bad images having dark spots inthe background, which are not defects, but together with image-level labels they provide SegAD with misleading informationabout the importance of the background. In any case, it shows that our method can sometimes make the results worse ifthe training data has semantic differences from the testing data. The supervised method from the original VisA high-shotsupervised benchmark, mentioned in their paper, also shows lower results on the capsules class, 97.2 Cl. AUROC comparedto the average result of 99.7, which might also indicate a difference between test and training data.",
  "E. Additional experiments with MVTec AD": "This section shows the results of additional experiments with MVTec AD per class. The task is to investigate how our methodwill work on another dataset, it is not a benchmark. 20 random bad images from the test set are selected for training SegADover ten different seeds for the selection process. 10% of good images from the training dataset are used to train SegAD aswell. MVTec AD includes 15 classes, 5 of which are textures and 10 are simple objects . Training datasets consist of 242good images per class on average (compared with 720 for VisA and 2000 for VAD), which leaves us with fewer good imagesfor training SegAD. EfficientAD + Ours shows worse results compared to EfficientAD. This displays that some anomalydetectors might be not working with our method in some cases for reasons that require further investigation. Nevertheless,other anomaly detectors show improvement."
}