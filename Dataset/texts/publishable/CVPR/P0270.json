{
  "Abstract": "The image matching field has been witnessing a contin-uous emergence of novel learnable feature matching tech-niques, with ever-improving performance on conventionalbenchmarks. However, our investigation shows that de-spite these gains, their potential for real-world applica-tions is restricted by their limited generalization capabili-ties to novel image domains. In this paper, we introduceOmniGlue, the first learnable image matcher that is de-signed with generalization as a core principle. OmniGlueleverages broad knowledge from a vision foundation modelto guide the feature matching process, boosting general-ization to domains not seen at training time.Addition-ally, we propose a novel keypoint position-guided atten-tion mechanism which disentangles spatial and appear-ance information, leading to enhanced matching descrip-tors. We perform comprehensive experiments on a suiteof 7 datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlues novelcomponents lead to relative gains on unseen domains of20.9% with respect to a directly comparable reference model,while also outperforming the recent LightGlue method by9.5% relatively. Code and model can be found at",
  ". Introduction": "Local image feature matching techniques provide fine-grained visual correspondences between two images ,which are critical for achieving accurate camera pose estima-tion and 3D reconstruction . The pastdecade has witnessed the evolution from hand-crafted to learning-based image features . Morerecently, novel learnable image matchers have been pro-posed , demonstrating ever-improvingperformance on conventional benchmarks .",
  "SIFT+MNNSuperGlueOmniGlue (ours)": ". OmniGlue is a generalizable learnable matcher. In-troducing foundation model guidance and an enhanced attentionmechanism, OmniGlue learns effective image matching that trans-fers well to image domains not seen during training. We compare itagainst reference methods SIFT and SuperGlue , with sub-stantial improvements on a suite of diverse datasets: outdoor scenes(MegaDepth-1500 pose AUC@5), indoor scenes (ScanNet pose accuracy @5), aerial scenes (DeepAerial PCK@1%)and object-centric images (GSO-Hard and NAVI-MultiView /NAVI-Wild , pose accuracy @5). Despite substantial progress, these advancements over-look an essential aspect: the generalization capability ofimage matching models. Today, most local feature match-ing research focuses on specific visual domainswith abundant training data (e.g., outdoor and indoor scenes),leading to models that are highly specialized for the trainingdomain. Unfortunately, we observe that the performance ofthese methods usually drops dramatically on out-of-domaindata (e.g., object-centric or aerial captures), which may noteven be significantly better than traditional approaches insome cases. For this reason, traditional domain-agnostictechniques, such as SIFT , are still widely used to obtainposes for downstream applications . Due to thecost of collecting high-quality correspondence annotations,we believe it is unrealistic to assume that abundant trainingdata would be available for each image domain, like in someother vision tasks . Thus, the community should focus",
  "arXiv:2405.12979v1 [cs.CV] 21 May 2024": "on developing architectural improvements to make learnablematching methods generalize.Motivated by the above observations, we proposeOmniGlue, the first learnable image matcher that is de-signed with generalization as a core principle. Buildingon top of domain-agnostic local features , we introducenovel techniques for improving the generalizability of match-ing layers: foundation model guidance and keypoint-positionattention guidance. As shown in , with the introducedtechniques, we enable OmniGlue to generalize better onout-of-distribution domains while maintaining quality per-formance on the source domain.Firstly, we incorporate broad visual knowledge of a foun-dation model. By training on large-scale data, the foundationmodel, DINOv2 , performs well in diverse image do-mains on a variety of tasks, including robust region-levelmatching . Even though the granularity of match-ing results yielded from foundational models is limited, thesemodels provide generalizable guidance on potential match-ing regions when a specialized matcher cannot handle thedomain shift. Thus, we use DINO to guide the inter-imagefeature propagation process, downgrading irrelevant key-points and encouraging the model to fuse information frompotentially matchable regions.Secondly, we also guide the information propagationprocess with keypoint position information. We discoverthat previous positional encoding strategies hurt per-formance when the model is applied to different domains which motivates us to disentangle it from the matchingdescriptors used to estimate correspondence. We proposea novel keypoint-position guided attention mechanism de-signed to avoid specializing too strongly in the training dis-tribution of keypoints and relative pose transformations.Experimentally, we assess OmniGlues generalizationacross diverse visual domains, spanning synthetic and realimages, from scene-level to object-centric and aerial datasets,with small-baseline and wide-baseline cameras. We demon-strate significant improvements compared to previous work.In more detail, our contributions are as follows.Contributions. (1) We introduce foundation model guid-ance to the learnable feature matching process, which lever-ages broad visual knowledge to enhance correspondencesin domains that are not observed at training time, boostingpose estimation accuracy by up to 5.8% (14.4% relatively).(2) A new strategy for leveraging positional encoding ofkeypoints, which avoids an overly reliant dependence ongeometric priors from the training domain, boosting cross-domain transfer by up to 6.1% (14.9% relatively). (3) Weperform comprehensive experiments on 7 datasets from var-ied domains, demonstrating the limited generalizability ofexisting matching methods and OmniGlues strong improve-ments, with relative gains of 20.9% on average in all noveldomains. (4) By fine-tuning OmniGlue using limited amount",
  ". Related Work": "Generalizable Local Feature Matching. Prior to the deeplearning era, researchers focused on developing generaliz-able local feature models. For example, SIFT , SURF and ORB have been widely used for image match-ing tasks across diverse image domains. Still today, manycomputer vision systems ignore recent advances in learnablelocal features and rely on hand-crafted methods, for example,to obtain poses for downstream applications .One of the main reasons for such old hand-crafted meth-ods to continue being adopted is that most of the recentlearning-based methods are specializedto domains with abundant training data, such as outdoorbuilding scenes, and do not generalize well to other domains.Recently, the community shifted the main focus to developlearnable image matchers, which associate local features pro-duced by off-the-shelf methods or jointly learn featuredescription and association . While they demonstratebetter performance compared with hand-crafted matchingsystems, they make the entire image matching pipeline evenmore domain-specific. Our experiments show that learnablematchers specialize strongly in the training domain, with lim-ited generalization. Our proposed OmniGlue improves thegeneralization capability of existing learnable matchers byintroducing guidance from foundation models and improvedpositional encoding.Sparse Learnable Matching.Sparse learnable imagematching methods associate sparse keypoints, pro-duced by keypoint detectors. For example, SuperGlue uses SuperPoint for keypoint detection and leverages theattention mechanism to perform intra- and inter-imagekeypoint feature propagation. However, SuperGlues showslimited generalization capability. One reason is that it entan-gles the local descriptors and positional information of thekeypoints, making the matching process overly dependenton learned positional patterns. It hinders the generalizabilityto data with different position-related matching patterns. Tosolve this problem, OmniGlue proposes to disentangle themduring the feature propagation, releasing the reliance on po-sitional patterns and improving the generalization capabilityto images from diverse domains.(Semi-)Dense Learnable Matching. Dense image matchingmethods jointly learn the image descriptors and the matchingmodule, performing pixel-wise matching on the entire inputimages . They benefit from the end-to-end learning pipeline and demonstrate better performance inthe training domain. For example, the semi-dense methodLoFTR introduces a coarse-to-fine correspondence predic-tion paradigm . Another line of work directly predicts",
  "{, }": "sharedshared . OmniGlue overview. We use frozen DINO and SuperPoint to detect keypoints and extract features. Then, we build denselyconnected intra-image keypoint graphs and leverage DINO features to build inter-image graphs. We refine the keypoint features based on theconstructed graphs, performing information propagation. In this process, we use keypoint positions solely for guidance, disentangling themfrom the keypoint local descriptors. Finally, the matching results are produced based on the updated keypoint local descriptors. the matching results as a 4D correlation volume .However, we notice that some of them generalize worseon new domains compared with sparse methods. Thus,OmniGlue chooses to focus on sparse methods, which canhave better potential to be generalizable due to the use ofdomain-agnostic local descriptors.Matching with Additional Image Representations. Lever-aging robust image representations is a promising avenuetoward generalizable image matching. One line of workuses geometric image representations, e.g., depth map and NOCS map , to augment the image matching pro-cess. However, they are dependent on a highly accuratemonocular estimation of these geometric representations.Differently, SFD2 uses semantic segmentation resultsto reject indistinguishable keypoints in background regions.Nevertheless, the semantic segmentation model has to betrained on each specific target domain. Recently, large visionmodels, e.g., self-supervised vision backbones and Diffusion models demonstrate robust seman-tic understanding properties. By training on large data, thesemodels showcase strong generalization capability across di-verse domains , which enables them to obtaincoarse patch-level matching results. However, performingmatching using image features extracted by these modelsdemonstrates limited performance on regions/keypoints with-out strong semantic information and the accuracy is lim-ited . Instead of directly incorporating these coarsesignals into the keypoint features and using them to performmatching, OmniGlue uses DINOv2 features to identify po-tentially related regions and guide the attention-based featurerefinement process. Thanks to the wide domain knowledgeencoded in this model, OmniGlue can boost the generaliza-tion ability of our method to diverse domains.",
  ". Model Overview": "presents a high-level overview of our OmniGluemethod, with four main stages. First, image features areextracted using two complementary types of encoders: Su-perPoint , focusing on generic fine-grained matching;and DINOv2 , an image foundation model which en-codes coarse but broad visual knowledge. Second, we buildkeypoint association graphs using these features, both intraand inter-image. In contrast to previous work, our inter-image graph leverages DINOv2 guidance, which providesa coarse signal capturing general similarity between Super-Point keypoints. Third, we propagate information among thekeypoints in both images based on the built graphs, using selfand cross-attention layers for intra and inter-image communi-cation, respectively. Crucially, we disentangle positional andappearance signals at this stage, different from other modelsthat overlook this aspect. This design enables feature re-finement to be guided by both keypoint spatial arrangementand their feature similarities, but without contaminating thefinal descriptors with positional information, which hindersgeneralizability. Finally, once the refined descriptors areobtained, optimal matching layers are applied to produce amapping between the keypoints in the two images. Thesestages are described in more detail in the following section.",
  ". OmniGlue Details": "Feature Extraction. The inputs are two images with sharedcontent, denoted as IA and IB. We denote the SuperPointkeypoint sets of the two images as A := {A1, ..., AN} andB := {B1, ..., BM}. Note that N and M are the numberof identified keypoints of IA and IB, respectively. Eachkeypoint is associated with its SuperPoint local descriptord RC. Additionally, normalized keypoint locations areencoded with positional embeddings, and we further refinethem using MLP layers. We denote the resulting positionalfeatures of a keypoint as p RC. Furthermore, we extractdense DINOv2 feature maps of the two images. We inter-polate the feature maps using the location of SuperPoint Pairwise DINO similarity",
  "Softmax": ". (Left) Building inter-image graph. We prune the densepairwise graph based on the DINO feature similarity. (Right)Position-guided attention. The keypoint position is involved incomputing attention weights, while the output attention update isonly composed of local descriptor components. keypoints to obtain DINOv2 descriptors for each keypoint,denoted as g RC. For clarity, we denote the three featuresof the ith keypoint in set A as dAi , pAi and gAi . The fea-tures of the keypoints in set B are denoted accordingly. Thegoal of our OmniGlue model is to estimate correspondencesbetween the two keypoint sets.Graph Building Leveraging DINOv2. We build four key-point association graphs: two inter-image graphs and twointra-image graphs. The two inter-image graphs representthe connectivity between the keypoints of the two images,from IA to IB and vice versa. We denote them as GABand GBA, respectively. The two inter-image graphs aredirected, where information is propagated from the sourcenode to the target node.We leverage DINOv2 features to guide the building ofthe inter-image graphs. As depicted in (left), we takeGBAi as an example. For each keypoint Ai in keypointset A, we compute its DINOv2 feature similarities with allkeypoints in set B. Note that we perform channel-wise nor-malization on the DINOv2 features gAi and gB before com-puting the similarities. We select the top half of keypointsin set B with the largest DINOv2 similarities to connectwith Ai, which prunes the densely-connected pairwise graphbetween the keypoints of the two images. We perform thesame operation on all keypoints in A to obtain GBA, andthe graph GAB is built in a similar manner.Similarly, the intra-image graphs represent the connec-tivity between keypoints belonging to the same image. Wedenote them as GA and GB, which are undirected in-formation is propagated bi-directionally between connectedkeypoints. Each keypoint is densely connected with all otherkeypoints within the same image.Information Propagation with Novel Guidance. We per-form information propagation based on the keypoint graphs.This module contains multiple blocks, where each block hastwo attention layers. The first one updates keypoints based on the intra-image graphs, performing self-attention; Thesecond updates keypoints based on the inter-image graphs,performing cross-attention. In particular, this stage intro-duces two novel elements compared to previous work, whichwe show are critical towards generalizable matching: suit-able guidance from DINOv2 and from keypoint positions.First, DINOv2 guidance: during cross-attention, for key-point Ai, it only aggregates information from the DINOv2-pruned potential matching set selected from B, instead ofall its keypoints. This is particularly helpful for generalizedimage matching, where DINOs broad knowledge may guidethe feature matching process in a domain that the model hasnot seen at training time. In this manner, information fromirrelevant keypoints will not be fused into the query keypointfeatures. This process also encourages the cross-attentionmodule to focus on distinguishing the matching point in thesmaller potential matching set. Note, however, that we do notforcibly limit the matching space to the potential matchingsets, as DINO may also be incorrect in some cases.Second, we introduce refined keypoint guidance. Weobserve that prior methods entangle keypoint positional fea-tures and local descriptors during feature propagation ,which makes the model overly dependent on learned position-related priors our ablation experiments in high-light this issue. The learned priors are vulnerable underimage pairs with matching patterns that were not seen attraining time, limiting the generalization capability. To dealwith this issue, we propose a novel position-guided attention,which disentangles the keypoint positional features p andthe local descriptors d. The positional information is usedas spatial context in this module and is not incorporated inthe final local descriptor representation used for matching.With these novel elements, our attention layer, illustratedin (right), is defined as follows, where we take theexample of keypoint Ai:",
  "vS = Wv(dS) + bv RKC.(5)": "As described in Eq. 1, the attention has a residual connection,which integrates the attention update value dAi . The nota-tion is the updating operation and [|] is the channel-wiseconcatenation. To compute the attention update value, asdescribed in Eq. 2, we compute the feature similarity be-tween the keypoint Ai and its source connected keypointsin a graph, which is denoted as S containing K keypoints.The query, key and value of the attention are qAi , kS, andvS, respectively. Specifically, as shown in Eq. 3-5, the queryand key are computed by fusing both local descriptors and positional features. The value, however, is transformed fromonly the local descriptors. We note that the weights (W)and bias (b), which map features into query, key and valuetokens in attention, are not shared across different attentionlayers. In self-attention (GA and GB), S is composed byall keypoints; in cross-attention (GAB and GBA), Scontains the keypoints identified by DINO.Intuitively, the query and key compute the attentionweights, where both feature affinity and spatial correlationsare considered. However, the attention update value, dAi , iscomposed of local descriptor components only. This designallows the model to reason about spatial correlation betweenkeypoints using their positional features while avoiding anover-reliance on it.Matching Layer and Loss Function. We use the refinedkeypoint representations to produce a pairwise similaritymatrix S RNM, where Si,j = dAi (dBj )T . Then we usethe Sinkhorn algorithm to refine the similarities, whichproduces the matching matrix M NM, where Mi,jrepresents the matching probability between keypoint Aiand Bj. To train OmniGlue, we minimize the negative log-likelihood of the matching matrix with ground truth .",
  ". Comparison Against SuperGlue and LightGlue": "It is important to highlight differences between our modeland reference sparse learnable feature matching methods,SuperGlue and LightGlue . While neither of theseis designed to target generalizability to multiple domains,there are common elements in the model structure, so wewould like to emphasize our novelty.Both works use attention layers for information propaga-tion. Differently, OmniGlue leverages a foundation modelto guide this process, which significantly helps with transfer-ring to image domains that are not observed during training.In terms of local descriptor refinement, OmniGlue departsfrom SuperGlue to disentangle positional and appearancefeatures. For reference, SuperGlue represents keypoint withentangling the two features as d + p, where positional fea-tures are also used to produce matching results. Similarto our design, LightGlue removes the dependency of theupdated descriptors on the positional features. However, itproposes a very specific positional encoding formulation,based on rotary encodings, only in self-attention layers.Overall, SuperGlue is the closest model to OmniGlue,serving as a directly comparable reference where our con-tributions can be clearly ablated. For this reason, in thefollowing section, we use SuperGlue as the main referencecomparison for experimental validation.",
  "DeepAerialSceneAerialN/AImage Reg": ". Dataset and task comparisons on: (1) The general type;(2) The background scene type; (3) Use of real () or rendered() images; (4) Whether the pose transformation is synthetic; (5)Whether foreground masks are used to filter correspondence predic-tions; (6) The camera baseline type; (7) Whether two input imageshave different backgrounds; (8) Evaluated tasks: CorrespondenceEstimation, Pose Estimation or Image Registration.",
  "We list the datasets and tasks used for evaluating OmniGluein . We include details of datasets as follows:": "Synthetic Homography (SH) contains images from theOxford and Paris dataset . We generate random cropsand homography transformations to sample image patchpairs, similar to . Two subsets are generated, SH100and SH200, wherein the perturbations of the image cor-ners for homography generation are within 100 and 200pixels, respectively. For each subset, we generate roughly9 million training pairs and 10K test pairs. MegaDepth (MD) is a large-scale outdoor imagedataset. The ground-truth matches are computed usingSfM . We follow the train/test split of prior works ,with roughly 625K training pairs and 1500 test pairs. Google Scanned Objects (GSO) comprises 1400daily object model scans of 17 categories. We rendersynthetic images with large (60- 90) rotation (Hard sub-set) and small (15- 45) rotation (Easy subset) camerabaselines, intentionally distinct from the training distribu-tion. We produce 50 image pairs for each object model,resulting in around 140K test cases. NAVI focuses on objects and encompasses a varietyof both indoor and outdoor images. It is divided into twosubsets: the multiview subset (25K image pairs), featur-ing input images captured in the same environment; andthe wild subset (36K image pairs), where the two inputimages are taken in different environments with distinctbackgrounds, lighting conditions and camera models.",
  "Tasks and metrics. We assess the models across three": ". Visualization of correspondences predicted by OmniGlue on the MegaDepth-1500 benchmark. We distinguish the matches bydifferent colors. We show results for scene \"0022\" and \"0015\" on the top and bottom rows, respectively. tasks:(1) Correspondence estimation, evaluated withcorrespondence-level precision and recall (for sparse meth-ods only). Following SuperGlue , we employ thresholdsof < 3px and > 5px to label a correspondence as correctand incorrect, respectively. (2) Camera pose estimation,evaluated with pose accuracy (% of correct poses within{5, 10, 20} of error) and AUC, with accuracy being usedby default unless otherwise specified. The poses are derivedfrom the estimated correspondences using RANSAC ,and we use Rodrigues formula to calculate relative rotationerror between the predicted/ground truth rotation matrices;(3) Aerial image registration, evaluated with percentage ofcorrect keypoints (PCK). We use RANSAC-based affineestimation from the estimated correspondences, and applythe predicted/ground truth affine transformations to 20 testkeypoints with fixed positions to calculate the PCK within max(h, w) pixels of error, for {0.01, 0.03, 0.05}.",
  "Baselines. We compare OmniGlue against:": "SIFT and SuperPoint provide domain-agnosticlocal visual descriptors for keypoints.We generatematching results using both nearest neighbor + ratio test(NN/ratio) and mutual nearest neighbor (MNN), with thebest outcomes being reported. Sparse matchers: SuperGlue employs attention lay-ers for intra- and inter-image keypoint information aggre-gation, using descriptors derived from SuperPoint . Itis the closest reference of OmniGlue. LightGlue im-proves SuperGlue with better performance and speed.Besides, we also test with DINOv2 +SuperGlue, bysubstituting SuperPoint descriptors with DINO features.",
  ". Results": "Following SuperGlue and LightGlue, we first initializeOmniGlue by training it on SH100. Then we further pre-train OmniGlue on SH200, and finally train OmniGlue onMegaDepth (MD). We evaluate OmniGlue and all baselinemethods on the test splits of each training domain, and testtheir generalization to both subsequent training datasets orout-of-domain test datasets. Finally, we experiment withadapting OmniGlue to out-of-domain images with limitedtarget domain training data.From Synthetic Homography to MegaDepth. As depictedin , in comparison to the base method SuperGlue,OmniGlue not only exhibits superior performance on thein-domain data but also demonstrates robust generalization.Even with a minimal data distribution shift from SH100 toSH200, SuperGlue experiences substantial drops in perfor-mance with a 20% reduction in precision and recall. Thisresult implies that SuperGlue is overly dependent on learnedposition-related patterns and is unable to handle further im-age warping distortion. In contrast, OmniGlue showcasesstrong generalization capability, surpassing SuperGlue with . Zero-shot generalization to novel domains. The top and middle row show results on GSO and NAVI, the last row shows results onScanNet and DeepAerial. We draw the correct and incorrect estimated correspondences as green and red, respectively.",
  "DENSE AND SEMI-DENSE METHODS": "PDCNet 51.5 / 67.5 / 78.25.1 / 8.9 / 14.924.8 / 36.7 / 49.33.9 / 7.1 / 11.66.6 / 11.6 / 17.038.6 / 60.0 / 71.314.0 / 20.9 / 22.6LoFTR 52.8 / 69.2 / 81.27.6 / 14.0 / 22.938.2 / 54.1 / 67.512.5 / 22.7 / 34.29.8 / 18.4 / 29.836.2 / 56.1 / 68.617.8 / 23.7 / 25.0",
  "SuperGlue 42.2 / 61.2 / 76.07.2 / 13.2 / 21.632.3 / 48.4 / 62.911.8 / 21.9 / 34.410.6 / 19.8 / 31.825.5 / 43.4 / 57.316.4 / 26.2 / 28.8": "LightGlue 47.6 / 64.8 / 77.97.5 / 13.8 / 21.736.4 / 53.2 / 66.913.2 / 24.0 / 34.89.7 / 17.6 / 25.936.7 / 59.4 / 71.618.1 / 25.8 / 27.3OmniGlue (ours)47.4 / 65.0 / 77.88.6 / 15.3 / 25.038.4 / 54.8 / 68.813.2 / 24.8 / 37.712.4 / 22.8 / 35.031.3 / 50.2 / 65.022.4 / 33.5 / 36.6",
  "rel. gain (%) over +12.3 / +6.2 / +2.4+19.4 / +15.9 / +15.7+18.9 / +13.2 / +9.4+11.9 / +13.4 / +9.6+16.7 / +15.2 / +10.1+22.0 / +15.7 / +13.4+36.6 / +27.9 / +27.0": ". Results for in-domain (left, measured with AUC) and zero-shot generalization to out-of-domain datasets (right, measured with poseaccuracy / PCK), for models trained on the MegaDepth dataset. We highlight the best results on out-of-domain data and show our relativeimprovement against our base method SuperGlue. All sparse methods use 1024 keypoints. a 12% improvement in precision and a 14% boost in recall.Similarly, during the transfer from SH200 to Megadepth,OmniGlue outperforms SuperGlue with a drastic 15% im-provement in recall.From MegaDepth to other Domains. As shown in ,OmniGlue not only achieves comparable performance onMegaDepth-1500 with the state-of-the-art sparse matcherLightGlue, but also demonstrates better generalization ca-pability on 5 out of 6 novel domains, when compared to allother methods. In detail, on MegaDepth-1500, OmniGlueshowcases 12.3% relative gain (pose AUC @5) over thebase method SuperGlue. On the 6 novel domains, OmniGlueshows 20.9% and 9.5% averaged relative gains (for pose andregistration accuracy at the tightest thresholds) over Super-Glue and LightGlue, respectively. Moreover, OmniGluedemonstrates larger performance gains on harder novel do-mains against LightGlue, i.e., on GSO-Hard, NAVI-Wild,and DeepAerial. We show visualization in and Fig 4for zero-shot generalization on novel domains and its perfor-mance on the source domain.Notably, the reference dense matchers, which achieve bet-ter performance on the in-domain MegaDepth dataset, gen- eralize worse. Their performances are close, or even worse,to SuperGlue, which has 10% lower in-domain conjecture this may be due to the joint learning of visualdescriptors and the matching module, making them easier tospecialize strongly to the training domain.Low-Shot Fine-tuning on Target Domain. In certain real-world scenarios, a limited set of target domain data maybe available for fine-tuning. To test this scenario, we fine-tune OmniGlue on the target domain (object-centric GSOdataset), comparing its performance with the base model,SuperGlue. We create small training subsets by utilizingonly a few dozen object scans. Notably, these small trainingsets consist of instances from the sneaker object categoryonly, covering a significantly minor subset of the testingobject category distribution.As depicted in , OmniGlue is more readily adaptedto the target domain. In detail, when scaling from 0 to 30instances for training, OmniGlue consistently exhibits en-hanced performance for both test subsets. With just 10 in-stances for training, OmniGlue improves pose estimationaccuracy by 5.3% and 4.0% on the two subsets. Expandingthe training sets by incorporating 10 more objects leads to",
  "rel. gain (%)+94.2 / +90.2 / +69.2+19.3 / +14.1 / +10.5": ". Fine-tuning results of SuperGlue (SG) and our methodOmniGlue (OG) on Google Scanned Object (GSO) dataset. We usedozens of sneaker object instances to generate training data and teston all 17 GSO categories. We also show a relative gain comparedwith the zero-shot performance. a further performance improvement of 2%. Furthermore,OmniGlue consistently surpasses SuperGlue, achieving arelative gain of approximately 10% across all experiments.The results collectively demonstrate the applicability ofOmniGlue in real-world scenarios as a versatile and gen-eralizable method.",
  ". Ablation Study and Insights": "We conduct a comprehensive ablation study on each pro-posed module, as detailed in . Please note that thenumbers reported on the GSO dataset are based on a subset,encompassing half of all test cases, for rapid evaluation.The effectiveness of each proposed technique. The resultsin (1) highlight the effectiveness of our foundationmodel guidance, which enhances the generalization capa-bility on out-of-domain data. Additionally, the third rowof (2) illustrates the impact of the position-guidedattention, showcasing improvement in both in-domain andout-of-domain data. Furthermore, we conduct ablations withdifferent approaches to disentangling keypoint positionalfeatures. The first two rows of (2) demonstratethat performance degrades when either not using any posi-tional features or applying the position-guidance only on self-attention (without positional guidance on cross-attention).This emphasizes the effectiveness of our position-guidedattention in facilitating information propagation within bothintra- and inter-image contexts. Besides, after removing thepositional embeddings, the model shows better generaliza-tion even though the in-domain performance drops. Thisresult implies that the inappropriate way that SuperGlue usespositional information limits its generalization.The ways of incorporating DINO features. As shown in (3), we explore different methods of incorporatingDINOv2. The first involves merging DINO features andSuperPoint local descriptors. This integration is performedbefore the information propagation module using an MLP.",
  "(5)(2) + DINO-guide-0.5 (full)66.2 / 74.111.0 / 20.4 / 32.048.7 / 68.4 / 82.3": ". Ablation study on (1) only with DINO guidance, (2) onlywith the disentangled keypoint representation variants, (3) DINOguidance variants analysis (based on (2) with position guidance), (4)DINO guidance threshold analysis, and (5) full model OmniGlue. The experiment reveals a decline in performance, suggestingthat the two features are not compatible, likely due to thecoarse granularity of DINO. The manner in which thesefeatures can be effectively merged remains an open problem.The second method entails applying DINOv2 guidancefor constructing both intra and inter-image graphs, demon-strating diminished performance compared to (5). We hy-pothesize that the reason lies in the fact that intra-imageinformation propagation (self-attention) requires a globalcontext, particularly for distinguishing all keypoints in thefeature space. Reducing connectivity on the intra-imagegraph adversely affects the global context, aligning withfindings in the study of attention span in SuperGlue.Details of foundation model guidance. We ablate the hyper-parameter used to determine the number of source keypointin a graph, as presented in (4). The results indicatethat selecting the top half of keypoints in the other image forbuilding inter-image graphs is the optimal choice.",
  ". Conclusions and Future Work": "We propose OmniGlue, the first learnable image matcherthat is designed with generalization as a core principle. Weintroduce the broad visual knowledge of a foundation model,which guides the graph-building process. We identify thelimitation of the previous descriptor-position entangled rep-resentation and present a novel attention module to dealwith it. We demonstrate that OmniGlue outperforms priorwork with better cross-domain generalization. Moreover,OmniGlue can also be easily adapted to a target domain witha limited amount of data collected for fine-tuning. For futurework, it is also worth exploring how to leverage unannotateddata in target domains to improve generalization. Both ofbetter architectural designs and better data strategies canpave the way for a foundational matching model.",
  "A. Additional Model Details": "OmniGlue undergoes training with 750, 000 iterations usinga batch size of 48 on 8 NVIDIA Tesla V100 GPUs. Theinitial learning rate is set at 3e 5, with a decay rate of0.999991 and a hinge step of 55000. For DINOv2 feature extraction, we use the images with a maximum reso-lution (long side) of 630, maintaining the aspect ratio duringimage resizing, for reduce the computation. The DINOv2backbone employed ViT-14-base . We use the improvedpositional embedding scheme proposed in LFM-3D .",
  "D. Latency analysis": "We note that novel OmniGlue modules do not hurt latency ascompared the baseline SuperGlue model. Even though DI-NOv2 introduces additional computation, we use its featuresto prune the graphs and reduce the computation accordingly.Theoretically, the computation that DINOv2 introduces isO(n1(hw)2), where n1 = 9 (number of DINOv2 attentionlayers), h = H",
  "and w = W": "14 (H and W are input resolu-tion to DINOv2). The computation that pruning saves isO(2n2kk), where n2 = 9 (number of information propa-gation blocks), k = 1024 (number of target keypoints inone image), k =k2 (number of pruned keypoints in theother image) and the coefficient 2 is multiplied because thereare 2 inter-graph aggregation modules in each block. It issimplified as O(n2k2). With the resolution W = 630 and atypical aspect ratio of 16:9, the hw k = 1024. Thus, theintroduced and saved computation are balanced.We report the empirical speed results in , whichshows that OmniGlue runs at a similar frame rate as thebaseline SuperGlue model (no graph pruning). Inference wasperformed on an NVIDIA A40 GPU with FlashAttention.The result is reproduced with using Glue-Factory.",
  "E. Additional Qualitative Results": "We additionally present qualitative results of OmniGlue in. We compare our method (last column) with two ref-erence matching methods: mutual nearest neighbors (MNN,first column) and SuperGlue (second column). We showMNN with SIFT features for two domains, and withSuperPoint features for one. We observe that OmniGlueproduces improved matches for image pairs with significantchanges in viewing conditions, across a range of domains.",
  "Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf:Speeded up robust features. In European Conference onComputer Vision, 2006. 1, 2": "Csar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif,Davide Scaramuzza, Jos Neira, Ian D. Reid, and John J.Leonard. Past, present, and future of simultaneous localiza-tion and mapping: Toward the robust-perception age. IEEETransactions on Robotics, 32:13091332, 2016. 1 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. 2021IEEE/CVF International Conference on Computer Vision(ICCV), pages 96309640, 2021. 3 Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, XuyangBai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning tomatch features with seeded graph matching network. 2021IEEE/CVF International Conference on Computer Vision(ICCV), pages 62816290, 2021. 2 Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, MingminZhen, Tian Fang, David N. R. McKinnon, Yanghai Tsin, andLong Quan. Aspanformer: Detector-free image matchingwith adaptive span transformer. In European Conference onComputer Vision, 2022. 2 Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber,Thomas Funkhouser, and Matthias Niener. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Com-puter Vision and Pattern Recognition (CVPR), IEEE, 2017. 1,5, 9, 10 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiFei-Fei. Imagenet: A large-scale hierarchical image database.In 2009 IEEE conference on computer vision and patternrecognition, pages 248255. Ieee, 2009. 1 Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. In Proceedings of the IEEE conference oncomputer vision and pattern recognition workshops, pages224236, 2018. 1, 2, 3, 6, 7, 9, 10",
  "SIFT +MNN3.4 / 6.5 / 11.516.7 / 30.1 / 40.83.3 / 6.9 / 12.82.8 / 5.9 / 11.71.7 / 4.8 / 10.3SuperPoint +MNN2.5 / 5.3 / 10.015.2 / 26.1 / 38.84.5 / 9.7 / 17.83.7 / 8.0 / 15.17.7 / 17.8 / 30.6": "DINOv2 +SG 1.8 / 3.6 / 7.45.5 / 11.6 / 21.33.3 / 9.7 /.155.63.8 / 8.4 / 16.33.3 / 10.0 / 22.0SuperGlue 3.4 / 6.9 / 12.217.5 / 30.1 / 42.65.1 / 11.2 / 19.94.8 / 10.2 / 18.310.4 / 22.9 / 37.2LightGlue 3.5 / 7.1 / 12.618.9 / 32.3 / 46.75.7 / 12.4 / 21.24.3 / 9.2 /15.715.1 / 32.6 / 50.3OmniGlue (ours)4.1 / 8.2 / 14.320.7 / 34.1 / 48.45.8 / 12.6 / 22.25.6 / 11.8 / 20.714.0 / 28.9 / 44.3",
  "vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 9": "Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-man, Ryan Michael Hickman, Krista Reymann, Thomas Bar-low McHugh, and Vincent Vanhoucke. Google scanned ob-jects: A high-quality dataset of 3d scanned household items.2022 International Conference on Robotics and Automation(ICRA), pages 25532560, 2022. 1, 5, 9, 10 Johan Edstedt, Ioannis Athanasiadis, Mrten Wadenbck, andMichael Felsberg. Dkm: Dense kernelized feature matchingfor geometry estimation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1776517775, 2023. 1, 2, 3",
  "Martin A. Fischler and Robert C. Bolles. Random sampleconsensus: a paradigm for model fitting with applications toimage analysis and automated cartography. Commun. ACM,24:381395, 1981. 6": "Michael Goesele, Brian Curless, and Steven M Seitz. Multi-view stereo revisited. In 2006 IEEE Computer Society Confer-ence on Computer Vision and Pattern Recognition (CVPR06),volume 2, pages 24022409. IEEE, 2006. 1 Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B.Girshick. Momentum contrast for unsupervised visual repre-sentation learning. 2020 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 97269735,2019. 3",
  "Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle-feys. LightGlue: Local Feature Matching at Light Speed. InProc. ICCV, 2023. 1, 2, 5, 6, 7, 10": ". Target domain examples. We share some example image pairs from each of the target image datasets. From top row to bottomrow, the domains are: Google Scanned Objects (Hard), NAVI Wild Set, NAVI Multiview, ScanNet-1500, and DeepAerial. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shotone image to 3d object. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 92989309, 2023. 3",
  "Yuki Ono, Eduard Trulls, Pascal V. Fua, and Kwang MooYi. Lf-net: Learning local features from images. In NeuralInformation Processing Systems, 2018. 2": "Maxime Oquab, Timothee Darcet, Tho Moutakanni, Huy Q.Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-moud Assran, Nicolas Ballas, Wojciech Galuba, RussHowes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra,Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, HuijiaoXu, Herv Jgou, Julien Mairal, Patrick Labatut, ArmandJoulin, and Piotr Bojanowski. Dinov2: Learning robust visualfeatures without supervision. ArXiv, abs/2304.07193, 2023.2, 3, 6, 7, 9, 10",
  "G. Potje, F. Cadar, A. Araujo, R. Martins, and E. Nascimento": ". Qualitative matching comparison. We compare the following methods: mutual nearest neighbor (MNN, left), SuperGlue (center)and OmniGlue (right). Green lines denote correct correspondences, while red ones denote incorrect predictions. The first two rows presentresults on Google Scanned Objects (Hard), the following two rows on the NAVI Wild Set, and the final two rows on DeepAerial. The MNNresults use SuperPoint features in the first two rows, and SIFT features in the others.",
  "Enhancing Deformable Local Features by Jointly Learning toDetect and Describe Keypoints. In IEEE Conf. Comput. Vis.Pattern Recog., 2023. 1": "Filip Radenovic, Ahmet Iscen, Giorgos Tolias, YannisAvrithis, and Ondrej Chum. Revisiting oxford and paris:Large-scale image retrieval benchmarking. In Proceedings ofthe IEEE conference on computer vision and pattern recogni-tion, pages 57065715, 2018. 5 Jrme Revaud, Philippe Weinzaepfel, Csar Roberto deSouza, Noe Pion, Gabriela Csurka, Yohann Cabon, and M.Humenberger. R2d2: Repeatable and reliable detector anddescriptor. ArXiv, abs/1906.06195, 2019. 1, 2 Barbara Roessle and Matthias Niener. End2end multi-viewfeature matching with differentiable pose optimization. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 477487, 2023. 1",
  "Qing Wang, Jiaming Zhang, Kailun Yang, Kunyu Peng, andRainer Stiefelhagen. Matchformer: Interleaving attention intransformers for feature matching. In Asian Conference onComputer Vision, 2022. 2": "Shuzhe Wang, Juho Kannala, Marc Pollefeys, and DanielBarath. Guiding local feature matching with surface curvature.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 1798117991, 2023. 3 Fei Xue, Ignas Budvytis, and Roberto Cipolla.Sfd2:Semantic-guided feature detection and description. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 52065216, 2023. 3"
}