{
  "Boyao Zhou, Shunyuan Zheng, Hanzhang Tu, Ruizhi Shao, Boning Liu, Shengping Zhang,Liqiang Nie and Yebin Liu": "AbstractDifferentiable rendering techniques have recently shown promising results for free-viewpoint video synthesis of characters.However, such methods, either Gaussian Splatting or neural implicit rendering, typically necessitate per-subject optimization whichdoes not meet the requirement of real-time rendering in an interactive application. We propose a generalizable Gaussian Splattingapproach for high-resolution image rendering under a sparse-view camera setting. To this end, we introduce Gaussian parametermaps defined on the source views and directly regress Gaussian properties for instant novel view synthesis without any fine-tuning oroptimization. We train our Gaussian parameter regression module on human-only data or human-scene data, jointly with a depthestimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable with both depth and renderingsupervision or with only rendering supervision. We further introduce a regularization term and an epipolar attention mechanism topreserve geometry consistency between two source views, especially when neglecting depth supervision. Experiments on severaldatasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.",
  "INTRODUCTIONF": "REE-Viewpoint Video (FVV) synthesis from sparse inputviews is a challenging and crucial task in computervision, which is largely used in sports broadcasting, stageperformance and telepresence systems , . However,early attempts , try to solve this problem through aweighted blending mechanism by using a huge numberof cameras, which dramatically increases computational costand latency. On the other hand, NeRF-like differentiable vol-umetric rendering techniques , , , can synthesizenovel views under sparse camera setting , but typicallysuffer from per-scene optimization , , , , slowrendering speed , and overfitting to input views .In contrast, point-based rendering , , , has drawn long-lasting attention thanks to its high-speed,and even real-time, rendering performance. Once integratedwith neural networks, point-based graphics , realizea promising explicit representation with comparable realismand extremely superior efficiency in FVV tasks , .Recently, 3D Gaussian Splatting (3D-GS) introduces anew representation that the point clouds are formulated as3D Gaussians with a series of learnable properties including3D position, color, opacity and anisotropic covariance. Byapplying -blending , 3D-GS provides not only a morereasonable and accurate mechanism for back-propagating indicates equal contribution.Boyao Zhou, Hanzhang Tu, Ruizhi Shao, Boning Liu and Yebin Liu are withDepartment of Automation, Tsinghua University, Beijing 100084, P.R.China.Shunyuan Zheng and Shengping Zhang are with the School of ComputerScience and Technology, Harbin Institute of Technology, Weihai 264209,P.R.China. Liqiang Nie is with the School of Computer Science and Tech-nology, Harbin Institute of Technology, Shenzhen 518055, P.R.China.Corresponding author: Shengping Zhang (). 3D-GSENeRF GPS-Gaussian PSNR:24.852K@25FPS PSNR:23.211K@5FPS PSNR:22.851K@ - FPS GPS-Gaussian+MVSplat4D-GS PSNR:32.311K@25FPS PSNR:27.52512@8FPS PSNR:30.191K@ - FPS : High-fidelity and real-time rendering. On the top,GPS-Gaussian produces 2K-resolution rendering of charac-ter, while GPS-Gaussian+ renders novel views of human-centered scenes on the bottom. Our methods outperformthe state-of-the-art feed-forward implicit rendering methodENeRF , explicit rendering method MVSplat andoptimization-based methods 3D-GS and 4D-GS .",
  "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 20242": "the gradients but also a real-time rendering efficiency forcomplex scenes. Despite realizing a real-time inference,Gaussian Splatting relies on per-scene or per-frame parameter optimization for several minutes. It is thereforeimpractical in interactive scenarios as it necessitates the re-optimization of Gaussian parameters once the character orthe scene changes.More recently, some generalizable Gaussian Splattingmethods , , , have been proposed to explorenovel-view synthesis in a feed-forward way. In general,such methods leverage a learnable geometry prior with adifferentiable rendering pipeline to achieve feed-forwardinference. For example, Splatter Image regresses directlyGaussians positions and other properties from a singleimage. In such an ill-posed setting, 3D consistency is hardlyheld and image quality is extremely low. PixelSplat ,MVSplat and MVSGaussian utilize probabilisticdepth estimation and multiplane sweeping to representgeometry from multiple source view images. Although suchgeometry cues allow 3D-GS to generalize to some staticscenes, the probabilistic representations can easily makefloating artifacts due to uncertain Gaussian positions. More-over, they dramatically increase the computational cost forinference and no longer render novel view image in real-time, even with a low resolution of 256 256.In this paper, we propose to integrate binocular stereo-matching , , as a geometry cue with 3D-GS ren-dering pipeline to achieve a generalizable Gaussian Splat-ting. Given a pair of images, stereo-matching can determi-nately calculate the disparity by searching the cost volumebuilt on the extracted features of two source views. Basedon the epipolar geometry, it is straightforward to transformdisparity into depth given camera parameters. This determi-nant geometry representation can be better supervised withrendering loss than probabilistic ones because GaussianSplatting necessitates a certain position of each primitive.Specifically, we introduce 2D Gaussian parameter (depthresidual, color, scaling, rotation, opacity) maps which aredefined on source view image planes, instead of unstruc-tured point clouds. These Gaussian parameter maps allowus to represent a scene with pixel-wise parameters, i.e. eachpixel corresponding to a specific Gaussian point. Addition-ally, it enables the application of efficient 2D convolutionnetworks rather than expensive 3D operators. Given theestimated depth map via binocular stereo-matching, it isefficient to lift 2D parameter maps to 3D Gaussian points.Such unprojected Gaussian points from both the two sourceviews constitute the representation of scene and novel viewimages can be rendered with splatting technique . Sincethe whole pipeline is fully differentiable, we can jointly trainan iterative stereo matching-based depth estimation along with our Gaussian parameter regression with bothdepth and rendering loss or with only rendering loss.A preliminary version of this work has been publishedas a highlight paper in CVPR 2024, in which we pro-pose a real-time framework of human novel view synthesisand train this framework on synthetic human-only datawith depth and rendering loss. In the current version, weaim to extend it to human-scene scenarios and no longerrequire matting technique or depth supervision. Note thatwrong matting leads to floating artifacts and it is not triv- ial to acquire high-quality geometry or depth informationfrom complex human-centered scene data. To achieve high-quality rendering without depth supervision, an epipolarattention is applied in the shared feature extraction module(Sec. 4.2), which can improve stereo-matching accuracy andrendering consistency. In addition, a depth residual map(Sec. 4.4.3) is introduced in order to recover high-frequencydetails from the predicted depth of stereo-matching whenlacking depth supervision. Furthermore, we propose a reg-ularization term in Sec. 4.5.2 to preserve geometry consis-tency between the two source views and improve the overallstability of the training process when lacking the groundtruth of depth. Thanks to these adaptive components, ournetwork can be trained with only rendering loss, making itscalable for more general human-scene scenarios.In practice, we are able to synthesize high-fidelity free-viewpoint video around 25 FPS on a single modern graphicscard. Leveraging the rapid rendering capabilities and broadgeneralizability inherent in our proposed method, an un-seen character with or without background can be instantlyrendered without necessitating any fine-tuning or optimiza-tion. In summary, our contributions can be summarized as:",
  "We introduce a generalizable 3D Gaussian Splattingmethodology that employs pixel-wise Gaussian pa-rameter maps defined on 2D source image planes toformulate 3D Gaussians in a feed-forward manner": "We propose a fully differentiable framework com-posed of an iterative depth estimation module and aGaussian parameter regression module. The interme-diate depth prediction bridges the two componentsand allows them to benefit from joint training. We introduce a regularization term and an epipolarattention mechanism to preserve geometry consis-tency between the two source views when usingonly rendering loss. Our method generalizes well tounseen characters even in complicated scenes.",
  "RELATED WORK": "Neural Implicit Representation. Neural implicit functionhas recently aroused a surge of interest to represent com-plicated scenes, in form of occupancy fields , , ,, radiance fields , , , , , and signeddistance functions , , , , . Implicit repre-sentation shows the advantage in memory efficiency andtopological flexibility for human representation , , or scene reconstruction , , especially in a pixel-aligned feature query manner , . However, eachqueried point is processed through the full network, whichdramatically increases computational complexity. More re-cently, numerous methods have extended Neural RadianceFields (NeRF) to static human modeling , and dy-namic human modeling from sparse multi-view cameras ,, or a monocular camera , , . However,these methods typically require a per-subject optimizationprocess and it is non-trivial to generalize these methodsto unseen subjects. Previous attempts, e.g., PixelNeRF ,",
  "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 20243": "IBRNet , MVSNeRF and ENeRF resort to image-based features as potent priors for feed-forward scene mod-eling. Despite the great progress in accelerating the scene-specific NeRF , , , , efficient generalizable NeRFfor interactive scenarios remains to be further elucidated.Deep Image-based Rendering. Image-based rendering, orIBR in short, synthesizes novel views from a set of multi-view images with a weighted blending mechanism, which istypically computed from a geometry proxy. , deploymulti-view stereo from dense input views to produce meshsurfaces as a proxy for image warping. DNR directlyproduces learnable features on the surface of mesh proxiesfor neural rendering. Obtaining these proxies is not straight-forward since high-quality multi-view stereo and surfacereconstruction requires dense input views. Point cloudsfrom SfM , or depth sensors , can also beengaged as geometry proxies. These methods highly dependon the performance of 3D reconstruction algorithms or thequality of depth sensors. FWD designs a network torefine depth estimations, then explicitly warps pixels fromsource views to novel views with the refined depth maps.FloRen utilizes a coarse human mesh reconstructed byPIFu to render initialized depth maps for novel views.Arguably FloRen is most related to our preliminarywork GPS-Gaussian , as it also realizes 360 free viewhuman performance rendering in real-time. However, theappearance flow in FloRen merely works in 2D domains,where the rich geometry cues and multi-view geometricconstraints only serve as 2D supervisions. The difference isthat our approach lifts 2D priors into 3D space and utilizesthe point representation to synthesize novel views in a fullydifferentiable manner.Point-basedGraphics.Point-basedrepresentationhasshown great efficiency and simplicity for various 3Dhuman-centered tasks , , , , , , .Previous attempts integrate point cloud representation with2D neural rendering , or NeRF-like volume ren-dering , . Still, such a hybrid architecture does notexploit the rendering capability of point clouds and takesa long time to optimize on different scenes. Then differen-tiable point-based and sphere-based , renderinghave been developed, which demonstrates promising ren-dering qualities, especially attaching them to a conventionalnetwork pipeline , . In addition, isotropic pointscan be substituted by a more reasonable Gaussian pointmodeling , , to realize a rapid differentiable render-ing framework with a splatting technique. This advancedrepresentation has showcased prominent performance inconcurrent 3D human-centered work , , , ,. However, a per-scene or per-subject optimization strat-egy limits its real-world application. Although , accelerate partly the optimization process by using an on-the-fly strategy, they struggle to handle topology change indynamic scenes. In this paper, we go further to generalize3D Gaussians across diverse subjects while maintaining itsfast and high-quality rendering properties.Free-Viewpoint Video. Targeting different applications,there are two feasible schemes to produce free-viewpointvideos, one uses a compact 4D representation , ,, , , , and the other formulate an individual3D representation for each discrete timestamp, which can be further subdivided into on-the-fly optimization meth-ods , , , and feed-forward inference meth-ods , , , . The 4D representations , ,, cater to volumetric video, which can be playedback and viewed from any viewpoint at any time, but theperformance degrades when the capturing time goes longer.On the contrary, the on-the-fly optimization method excels at handling long-time sequences. They can realizesimilar experiences after applying customized compressiondesigns but they typically have higher memory costs than4D methods. Nevertheless, despite having been accelerated,the essential optimization process is far from real-time.Thus, we orient towards feed-forward methods for inter-active scenarios. Among them, MonoFVV and Func-tion4D implement RGBD fusion with depth sensorsto attain real-time human rendering. The large variationin pose and clothing makes the feed-forward generalizablefree-view rendering a more challenging task, thus recentwork , , , , simplifies the problem byleveraging human priors , . However, an inaccurateprior estimation would mislead the final result. For moregeneral dynamic scenarios, relies on expensive proba-bilistic geometry estimation, thus they can hardly achievereal-time free-viewpoint rendering, even integrated withGaussian Splatting , , . 3PRELIMINARYSince the proposed GPS-Gaussian+ harnesses the power of3D-GS , we give a brief introduction in this section.3D-GS models a static 3D scene explicitly with pointprimitives, each of which is parameterized as a scaledGaussian with 3D covariance matrix and mean",
  "j=1(1 i)(4)": "where ci is the color of each point, and density i isreasoned by the multiplication of a 2D Gaussian with co-variance and a learned per-point opacity . The coloris defined by spherical harmonics (SH) coefficients in .To summarize, the original 3D Gaussians methodol-ogy characterizes each Gaussian point by the followingattributes: (1) a 3D position of each point X R3, (2) acolor defined by SH c Rk (where k is the freedom of SHbasis), (3) a rotation parameterized by a quaternion r R4,(4) a scaling factor s R3+, and (5) an opacity .",
  "Feature Extraction": "The selected images are encoded with a feature extractionmodule in order to search the corresponding features fromone view to another. Once two source view images are recti-fied, Il, Ir HW 3 are fed to a shared image encoderEimg with several residual blocks and downsampling layersto extract dense feature maps f s RH/2sW/2sDs whereDs is the dimension at the s-th feature scale",
  "METHOD": "The overview of our method is illustrated in . GivenRGB images of a human-centered scene with sparse cam-era views, our method aims to generate high-quality free-viewpoint video of the performer in real-time. Once givena target novel viewpoint, we select the two neighboringviews from sparse cameras (Sec. 4.1). Then, image featuresare extracted from the two input images with a sharedimage encoder by using epipolar attention (Sec. 4.2), andthey are further used to predict the depth maps for bothsource views with a binocular depth estimator (Sec. 4.3).The colors of 3D Gaussians are directly determined by thecorresponding source view pixels, while other parametersof 3D Gaussians are predicted in a pixel-wise manner whenfeeding the predicted depth values and the former imagefeatures into a network (Sec. 4.4). Combined with RGB mapof the source view image, these parameter maps formulatethe Gaussian representation in 2D image planes and arefurther unprojected to 3D space with the estimated depth.The unprojected Gaussians from both views are aggregatedand rendered to the target viewpoint in a differentiable way,which allows for end-to-end training (Sec. 4.5), even withonly rendering loss.",
  "Source View Selection": "As a binocular stereo method, we synthesize the target novelview with two adjacent source views. Given N input images{In}Nn=1, with their camera position {Cn}Nn=1, source viewscan be represented by Vn = Cn O, where O is the centerof the scene. Similarly, the target novel view rendering canbe defined as Itar with camera position Ctar and viewVtar = Ctar O. By conducting a dot product of all",
  "{f sl }Ss=1, {f sr }Ss=1 = Eimg(Il, Ir)(5)": "where we set S = 3 in our experiments.Since the image encoder Eimg is independent of eachother view, it struggles to extract informative features whenlacking depth supervision. Thus, we propose to conduct anepipolar attention module on the bottleneck features f Sl,r,in order to exchange useful information from each otherview. Note that corresponding pixels from the two rectifiedimages are located on the same horizontal epipolar line. Inpractice, we rearrange feature map into H/2S line featuresf e RW/2SDS and employ multi-head attention Attalong each epipolar line",
  "Depth Estimation": "The depth map is the key component of our frameworkwhich lifts the 2D image planes to 3D Gaussian represen-tation. Note that, depth estimation in binocular stereo isequivalent to disparity estimation. For each pixel coordinatex = (u, v) in one view, disparity estimation disp aimsto find its corresponding coordinate (u + disp(u), v) inanother view, considering the displacement of each pixelis constrained to a horizontal line in rectified stereo. Sincethere is a one-to-one mapping between disparity maps anddepth maps given camera parameters, we do not distinguishthem in the following sections. Inspired by , we imple-ment this module in an iterative manner mainly because itavoids using prohibitively slow 3D convolutions to filter thecost volume. Given the processed feature maps f Sl ,f Sr , wecompute a 3D cost volume C RH/2SW/2SW/2S usingmatrix multiplication",
  "h(f Sl )ijh (f Sr )ikh(7)": "Then, an iterative update mechanism predicts a sequenceof depth estimations {dtl}Tt=1 and {dtr}Tt=1 by looking up involume C, where T is the update iterations. For more detailsabout the update operators, please refer to . The outputsof the final iterations (dTl , dTr ) are upsampled to full imageresolution via a convex upsampling. The depth estimationmodule depth can be formulated as",
  "Dl, Dr = depth(f Sl ,f Sr , Kl, Kr)(8)": "where Kl and Kr are the camera parameters, Dl, Dr RHW 1 are the depth estimations. The classic binocularstereo methods estimate the depth for reference view only,while we pursue depth maps for both input views with ashared-weight network to serve as the position of Gaussianpoints, which results in a decent efficiency increase.",
  "Pixel-wise Gaussian Parameters Prediction": "In 3D-GS , each Gaussian point in 3D space is charac-terized by attributes G = {X, c, r, s, }, which represent 3Dposition, color, rotation, scaling and opacity, respectively. Inthis section, we introduce a pixel-wise manner to formulate3D Gaussians in 2D image planes. Specifically, the proposedGaussian maps G are defined as",
  "Rotation, Scaling and Opacity Map": "The remaining Gaussian parameters are related not only tothe extracted features {f s}Ss=1 in Sec. 4.2 but also to thespatial cues from estimated depth in Sec. 4.3. The formerone provides a global context with image encoder Eattimg andthe latter one should focus on structural details so thatGaussian parameters can be predicted in a feed-forwardmanner. Hence, we construct an additional encoder Edepth,which takes the depth map D as input, to complementthe coarse geometric awareness for each pixel. The imagefeatures and the spatial features are fused by a U-Net likedecoder Dparm to regress pixel-wise Gaussian features infull image resolution",
  "Md(x) = Tanh(hd((x)))(14)": "where hd represents the depth residual head and we usethe Tanh function with a scaling factor = 0.5 to activatethe predicted value in a small range. Given the predicteddepth map D in Eq. 8 and the residual value in Eq. 14,a pixel located at x can be immediately unprojected fromimage planes to 3D space using projection matrix P R34",
  "Joint Training": "The pixel-wise Gaussian parameter maps defined on bothsource views are then lifted to 3D space and aggregated torender photo-realistic novel view images using the GaussianSplatting technique in Sec. 3. Our whole framework isfully differentiable so that we jointly train depth estimation(Sec. 4.3) and Gaussian parameters prediction (Sec. 4.4)which typically benefit each other. The full pipeline canbe trained with only rendering loss or a combination ofdepth loss and rendering loss when ground truth depth isavailable during training. When depth supervision is absent,we should also take into account the geometry consistencybetween two point clouds unprojected from left-view andright-view depth maps. 4.5.1Training LossRendering loss. First, we use rendering loss composed ofL1 loss and SSIM loss , denoted as Lmae and Lssimrespectively, to measure the difference between the renderedand the ground truth image",
  "Lrender = 1Lmae + 2Lssim(16)": "where we set 1 = 0.8 and 2 = 0.2 in our experiments.Depth loss. When ground truth depth is available, weminimize the L1 distance between the predicted and groundtruth depth over the full sequence of predictions {dt}Tt=1with exponentially increasing weights, as shown in .Given ground truth depth dgt, the loss is defined as",
  "where we set = 0.9 in our experiments": "4.5.2Geometry RegularizationGround truth depth is not trivially accessible, especially forcomplex scene data. Only rendering loss can not ensure ge-ometry consistency between the two input views. Thus wetry to minimize Chamfer distance between the unprojectedGaussian points of the two source views as a regularizationterm to boost the stereo-matching in two directions",
  "Datasets and Metrics": "Human-Scene data. To train and evaluate our network, wecollect character performance data in the scene from DyN-eRF and ENeRF-outdoor datasets. We take 4 motionsequences from DyNeRF, each of which contains 300 frames. The first 220 frames are used as training data, and we eval-uate our method on the rest of the data. For ENeRF-outdoordata, we take 4 motion sequences of 300 frames as trainingdata, and 2 motion sequences of unseen characters as testdata. We also capture motion sequences of single-characteror multiple-character performance in 3 different scenes witha forward-facing camera rig, as shown in (a), to testthe robustness of our method across scenes. In particular,10 cameras are positioned in a line, spanning 1.6 meters.Four cameras with red circles in (a), are used asinputs which compose 3 pairs of source views, and theothers serve as novel views during validation. For renderingcontinuity across source-view pairs, we use all 10 views assupervision during training. For each scene, our captureddataset consists of 3 sequences for training and 2 sequencesfor test, so there are 15 sequences in total. We train a modelon each dataset and the models can generalize to unseencharacters in the scene. For our captured data, our modelis able to handle all three backgrounds. In terms of human-scene ratio, our data and ENeRF-outdoor capture full-bodycharacters with small-focal cameras, while DyNeRF focuseson upper-body. Due to the original resolution of raw data,we set all images to 1K resolution.Human-only data. To learn human priors from a largeamount of data, we collect 1700 and 526 human scansfrom Twindom and THuman2.0 , respectively. Werandomly select 200 and 100 scans as validation data fromTwindom and THuman2.0, respectively. In addition, weuniformly position 8 cameras in a cycle, thus the anglebetween two neighboring cameras is about 45. To test therobustness in real-world scenarios, we capture real data of 4characters in the same 8-camera setup and prepare 8 addi-tional camera views for evaluation, as shown in (b).For synthetic data, we render images on 2K resolution asrendering supervision during training and as ground truthduring the test.Evaluation metric. Following ENeRF , we evaluate ourmethod and other baselines with PSNR, SSIM andLPIPS as metrics for the rendering results in validregions of novel views.",
  "Implementation Details": "Our method is trained on a single RTX3090 graphics cardusing AdamW optimizer with an initial learning rateof 2e4. For real-captured human-scene data, we train thewhole network from scratch for around 100k iterations withrendering loss and Chamfer distance. For DyNeRF data,we set = 0.005 for its large range of depth, while weset = 0.5 for ENeRF-outdoor and our captureddata. For synthetic human-only data, THuman2.0 andTwindom , we have 2 strategies to train networks. Wecan still train the whole network from scratch for 100kiterations with rendering loss and Chamfer distance. Whendepth information is available during training, the depthestimation module can be firstly trained for 40k iterationsand we then jointly train depth estimation and Gaussianparameters prediction for 100k iterations.",
  "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 20247": "TABLE 1: Quantitative comparison on human-scene datasets. All methods are evaluated on an RTX 3090 GPU to reportthe speed of synthesizing one novel view with two 1024 1024 source images, except MVSplat with two 512 512images due to memory cost. Our method uses TensorRT for fast inference. 4D-GS requires per-sequence optimization,while the other methods perform feed-forward inferences. The best , the second best and the third best are highlightedwith different colors.",
  ": Qualitative comparison on human-scene data. Our method produces high-quality renderings with respect to others": "izable methods including Gaussian Splatting-based methodMVSplat , implicit method ENeRF , image-basedrendering method FloRen and hybrid method IBR-Net . In particular, it is difficult to train MVSplat onmasked human-only data for its probabilistic modeling andFloRen relies on human prior, so we compare MVSplat onlyon human-scene data and FloRen only on human-only data.Following our setting, all baselines are trained on the sametraining set from scratch and take two source views as inputfor synthesizing the targeted novel view. The preliminarywork, GPS-Gaussian, and FloRen use ground truth depthsof synthetic human-only data for supervision. We furtherprepare the comparison with the original 3D-GS forstatic human-only data and with 4D-GS for sequentialhuman-scene data which are optimized on all input viewsusing the default strategies in the released code.",
  "Results on Human-Scene Data": "We compare state-of-the-art methods on 3 real capturedhuman-scene datasets. In , our approach achievessuperior or competitive results at the fastest speed withrespect to other methods. In particular, our approach makesa great improvement on metric LPIPS which reveals betterglobal rendering quality. We notice that camera parame-ters are not perfectly calibrated in the ENeRF-outdoor dataset. Although the bad calibration has a tough impacton the back-propagation of rendering loss, our methodcan still synthesize fine-grained novel view images withmore detailed appearances in . Due to the lack ofconsistent geometry prior, ENeRF and IBRNet can easilymake floating artifacts illustrated in . MVSplat andENeRF rely on multiplane sweeping to infer geometry from",
  "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 20248": "TABLE 2: Quantitative comparison on human-only datasets. All methods are evaluated on an RTX 3090 GPU to report thespeed of synthesizing one novel view with two 10241024 source images. Our methods and FloRen use TensorRT forfast inference. 3D-GS requires per-subject optimization, while the other methods perform feed-forward inferences.The best , the second best and the third best are highlighted with different colors. denotes training with depthsupervision.",
  ": Qualitative comparison on human-only data. Our method produces more detailed human appearances and canrecover more reasonable geometry": "sparse views, such representation can hardly handle thinstructure object, e.g. knife in . Although 4D-GS ac-celerates the optimization process from the original 3D-GSby decomposing spatial-temporal deformation into multi-resolution planes, such decomposition produces blurry re- sults under fast movements, see and . Thanks todeterminant stereo-matching and geometry regularization,our generated geometry is consistent from the two sourceviews so that our rendering results are more decent withfewer floating artifacts.",
  "Results on Human-Only Data": "We illustrate comparisons on two synthetic datasets andour collected real-world data in . Our method out-performs all baselines on all metrics and achieves a muchfaster rendering speed. Once occlusion occurs, some targetregions under the novel view are invisible in one or both ofthe source views. ENeRF and IBRNet are not able to renderreasonable results due to depth ambiguity. The unreliablegeometric proxy in these cases also makes FloRen produceblurry outputs even if it employs the depth and flow refin-ing networks. In our method, the efficient stereo-matchingstrategy and the geometry regularization help to alleviatethe adverse effects caused by occlusion. In addition, it takesseveral minutes for 3D-GS parameter optimization for asingle frame and produces noisy renderings of novel views,see , from such sparse views. Also, we demonstrate theeffectiveness of our method on thin structures, such as thehockey stick and robe in .",
  "Ablation Studies": "In this part, we evaluate the effectiveness of our proposedcomponents in GPS-Gaussian and GPS-Gaussian+ throughablation studies. The efficacy of joint training and depth en-coder proposed in GPS-Gaussian are validated on aforemen-tioned synthetic human-only data. As depth informationis accessible in synthetic data, we further evaluate depth(identical to disparity) estimation, other than renderingmetrics, with the end-point-error (EPE) and the ratio ofpixel error in 1 pix level, following . Furthermore, theadditional components in GPS-Gaussian+ are evaluated onour captured human-scene data because our data includessingle and multiple characters in different scenes. We alsoconduct a comparison between GPS-Gaussian and GPS-Gaussian+ on both datasets in order to illustrate the effi- Ground TruthGPS-GaussianGPS-Gaussian+GPS-Gaussian w/ Depth Sup.w/o Depth Sup.w/o Depth Sup.",
  "ciency of adaptive integration under the setting of trainingwithout depth supervision": "5.4.1Effects of Depth SupervisionA consistent geometry prior is essential to rendering reason-able images in novel views, especially for explicit GaussianSplatting. Although the rendering metrics in are notdramatically degraded for GPS-Gaussian when neglectingdepth supervision, the geometry metrics are not competitivewith respect to the models using depth supervision. In ad-dition, a wrong geometry prior could produce unreasonablerendering results in occluded regions, see . Therefore,how to exploit accurate geometry proxy in the case of thelack of depth supervision is the key to our extensions. 5.4.2Effects of Joint Training MechanismFor GPS-Gaussian trained on synthetic human-only data,we compare jointly training both depth estimation and ren-dering modules with separately training them. We designa model substituting the differentiable Gaussian renderingwith point cloud rendering at a fixed radius. Since the pointcloud renderer is no longer differentiable, the renderingquality is merely based on the accuracy of depth estimationwhile the rendering loss could not conversely promote thedepth estimator. The rendering results in witnessfloating artifacts due to the depth ambiguity in the marginarea of the source views where the depth value changesdrastically. In , joint training makes a more robustdepth estimator with a 5% improvement in EPE.",
  "Target": ": Qualitative ablation study on designed components in GPS-Gaussian+ for geometry. We show the effectivenessof the geometry regularization and the depth residual in the full pipeline for geometry reconstruction. 5.4.3Effects of Depth EncoderWe claim that merely using image features is insufficientfor predicting Gaussian parameters. Herein, we ablate thedepth encoder from our model, thus the Gaussian param-eter decoder only takes as input the image features topredict Mr, Ms, M simultaneously. As shown in ,the ablated model fails to recover the details of humanappearance, leading to blurry rendering results. The scaleof Gaussian points is impacted by comprehensive factorsincluding depth, texture and surface roughness, see Sec. 5.6.The absence of spatial awareness degrades the regression ofscaling map Ms, which deteriorates the visual perceptionreflected on LPIPS, even with a comparable depth estima-tion accuracy, as shown in . 5.4.4Effects of Geometry RegularizationIn GPS-Gaussian+ trained on real captured data withoutdepth supervision, geometry regularization is designed topreserve geometry consistency between Gaussian points ofthe two source views. Due to the lack of depth supervision, marginal regions in and regions with view-dependentreflection in are hardly reconstructed when missingour proposed geometry regularization. Such geometric con-straints can boost the unsupervised depth learning of thetwo source views to reach a mutual optimum. A good geom-etry prior also improves the rendering results, as reportedin . 5.4.5Effects of Depth ResidualWithoutdepthsupervision,GPS-Gaussiangeneratesscarcely reasonable geometry, as shown in the fourth rowof . Although GPS-Gaussian+ integrates the afore-mentioned adaptions, the geometry of the second row in is still not acceptable. This problem is caused bytwo reasons. First, a downsampling operator is used inthe stereo-matching module to minimize time cost. Second,the differentiability of point position in Gaussian Splattingis not satisfactory enough. By using our proposed depthresidual map, our model recovers more details and correctsgeometry artifacts, see the third row of .",
  "Effects of Epipolar Attention": "Compared with GPS-Gaussian, GPS-Gaussian+ incorpo-rates epipolar attention into the feature extraction module toachieve a solid stereo-matching result with only renderingloss. Epipolar attention allows the encoder to exchangeuseful information between source views so that we canbuild a compact cost volume for stereo-matching. Even if thedisparity (identical to depth) is not accessible during train-ing, our proposed model corrects floating artifacts causedby wrong matching in . Since we apply such attentionmechanism only along epipolar line, the time cost is on parwith GPS-Gaussian, see .In general, our adaptive integration is designed tocompensate for the absence of depth supervision. In Ta-ble 3, GPS-Gaussian+ without depth supervision achievescompetitive rendering results against GPS-Gaussian withdepth supervision and produces reasonable geometry re-sults. Moreover, the adaptive integration works better onfull-scene images than masked human-only images. Whenthe background is concerned, GPS-Gaussian+ can largelyimprove all rendering metrics with respect to GPS-Gaussian,as shown in .",
  "Visualization of Opacity Maps": "We discover that the joint regression with Gaussian param-eters eliminates the outliers by predicting an extremely lowopacity for the Gaussian points centered at these positions.The visualization of opacity maps is shown in . Sincethe depth prediction works on low resolution and upsam-pled to full image resolution, the drastically changed depthin the margin areas causes ambiguous predictions (e.g. thefront and rear placed legs and the crossed arms in ).These ambiguities lead to rendering noise on novel viewswhen using a point cloud rendering technique. Thanks tothe learned opacity map, the low opacity values make theoutliers invisible in novel view rendering results, as shownin (e). (a) (b)(c)(d)(e) : Visualization of opacity maps. (a) One of the sourceview images. (b) The predicted opacity map related to (a).(c)/(d) The directly projected color/opacity map at novelviewpoint. (e) Novel view rendering results. A cold colorin (b) and (d) represents an opacity value near 0, while ahot color near 1. The low opacity values predicted for theoutliers make them invisible. (a) Input Image(b) Depth(c) Scaling Map(d) Gaussian Points : Visualization of scaling map and the shape ofGaussian points. (a) One of the source view images. (b) Thedepth of (a). (c) The scaling map shown in heat map, wherea hotter color represents a larger value. (d) The zoom-inGaussian points of the boxed area in (a). The depth andscaling map are normalized.",
  "Visualization of Scaling Maps": "The visualization of the scaling map (mean of three axes)in (c) indicates that the Gaussian points with lowerdepth roughly have smaller scales than the distant ones.However, the scaling property is also impacted by compre-hensive factors. For example, as shown in (c) and(d), fine-grained textures or high-frequency geometries leadto small-scaled Gaussians.",
  "Robustness to Random Camera Views": "We evaluate the robustness of our method to the randomlyplaced source-view cameras in the first row of . Themodel trained under a uniformly placed 8-camera setup inSec. 5 shows a strong generalization capability to randomcamera setup with a pitch in range of [20, +20] and yawin range of [25, +25] for human-only data. In (f) and (h), our method achieves reasonable renderings ofnovel views with a pitch angle of about 10 for human-scene data, even without any supervision of views withpitch angles during training.",
  "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 202412": "(d) Ground Truth(c) Output(b) Source View 2(a) Source View 1 Pitch +20Yaw -25 Pitch -20Yaw +25 Pitch 0Yaw 0 Pitch 0Yaw 0 Pitch 0Yaw +10 (f) Output 1(g) Output 2(h) Output 3 Pitch +10Yaw 0 Pitch 0Yaw 0 Pitch -10Yaw 0 (e) Source Views Pitch 0Yaw -10 : Results on random camera views. In the first row,(a) and (b) are the source view images with an extreme pitchand yaw, (c) is the novel view rendering result and (d) is thenovel view ground truth. In the second row, we show (e)source views and target-view renderings with pitch anglesof (f) +10, (g) 0 and (h) 10.",
  "Comparison on Run-Time": "We conduct all experiments of our method and other base-line methods on the same machine with an RTX 3090 GPUof 24GB memory, except memory-consuming MVSplat .Even if we prepare another machine with a V100 GPU of32GB memory, MVSplat can only be fed with input imagesof 512 512 resolution during training. In , theoverall run-time can be generally divided into two parts:one correlating to the source views and the other concerningthe desired novel view. The source view correlated compu-tation in FloRen refers to coarse geometry initializationwhile the key components, the depth and flow refinementnetworks, operate on novel viewpoints. IBRNet usestransformers to aggregate multi-view cues at each samplingpoint aggregated to the novel view image plane, which istime-consuming. ENeRF constructs two cascade costvolumes on the target viewpoint, then predicts the targetview depth followed by a depth-guided sampling for vol-ume rendering. Once the target viewpoint changes, thesemethods need to recompute the novel view correlated mod-ules. However, the computation on source views dominatesthe run-time of GPS-Gaussian+, which includes binoculardepth estimation and Gaussian parameter map regression. TABLE 5: Run-time comparison. We report the run-timecorrelated to the source views and each novel view on anRTX 3090 GPU. Input resolution is 512 512 for MVSplat,while all other methods take two 10241024 source imagesas input. Our methods can render multiple novel viewsconcurrently in real-time.",
  ": Result on unseen scenes. In the case of unseenbackground during training, our method achieves reason-able rendering without any finetuning": "Similar to our method, MVSplat spends the majority ofrun-time on the multi-view stereo process of source views,which includes multiple CNN and transformer operations.As reported in , it takes only 1.9 ms to renderthe 3D Gaussians to the desired novel view of human-scene data for GPS-Gaussian+, while this can be reducedto 0.8 ms when rendering human-only data with fewerGaussian points. This allows us to render multiple novelviews simultaneously, which caters to a wider range ofapplications such as holographic displays. Suppose thatn = 10 novel views are required concurrently, it takes ourmethod T = Tsrc + n Tnovel = 49ms to synthesize, while135ms for MVSplat and 1261ms for ENeRF. In a real-worldcapture system, we should also consider I/O process andhuman matting, thus the frame rate is slightly degraded in and .",
  "LIVE-DEMO SYSTEMS": "We prepare a machine equipped with an RTX 3090 GPUto run our algorithm and build two capture systems toshoot live demo. For human-scene data, our capture systemconsists of ten cameras positioned on a 1.6-meter beam, apiece of illumination equipment and two synchronizers, asshown in (a). In (b), we position all cameras ina circle of a 2-meter radius to capture human-only data. Forlive demos, capture and rendering processes are run on thesame machine. We only use the cameras with red circles, 4cameras in (a) and 6 cameras in (b), as inputsource views. Our method enables real-time high-qualityrendering, even for challenging human-scene, human-objectand multi-human interactions.",
  "DISCUSSION": "Conclusion. In this paper, we present GPS-Gaussian+, afeed-forward rendering method for both human-only dataand human-scene data. By leveraging stereo-matching andpixel-wise Gaussian parameter map regression, our methodtakes a significant step towards a real-time photo-realistichuman-centered free-viewpoint video system from sparseviews. When lacking depth supervision during training, aregularization term and depth residual module are designedto ensure geometry consistency and high-frequency details.An adaptive integration, epipolar attention, is proposed inGPS-Gaussian+ to improve stereo-matching accuracy withonly rendering supervision. We demonstrate that our GPS-Gaussian+ notably improves both quantitative and qualita-tive results compared with baseline methods and extendsoriginal GPS-Gaussian from human-only synthesis to morescalable and general scenarios of human-centered scenes.Limitations. We notice some ghost artifacts on the whitewall or the light yellow ground in the supplementaryvideo. This is mainly because less textured regions couldincrease the difficulty of stereo-matching. Capturing moredata covering more complex backgrounds to expand thediversity of the training scenes is a general solution tothis issue. To achieve this, a portable system composed ofmobile phones (e.g. Mobile-Stage dataset ) could breakthrough the limitations of the fixed in-door capture system.Another feasible solution is using the massive monocularvideos of static scenes captured by moving cameras (e.g.RealEstate10k ) to pre-train the network. However, sinceour method requires accurate camera calibration and strictsynchronization, additional effort is required when makingit practical to leverage the aforementioned data. Jason Lawrence, Danb Goldman, Supreeth Achar, Gregory MajorBlascovich, Joseph G Desloge, Tommy Fortes, Eric M Gomez,Sascha Haberling, Hugues Hoppe, Andy Huibers, et al. Projectstarline: A high-fidelity telepresence system. ACM TOG, 40(6):116, 2021. 1 Hanzhang Tu, Ruizhi Shao, Xue Dong, Shunyuan Zheng, HaoZhang, Lili Chen, Meili Wang, Wenyu Li, Siyan Ma, ShengpingZhang, et al. Tele-aloha: A telepresence system with low-budgetand high-authenticity using sparse rgb cameras. In SIGGRAPH,pages 112, 2024. 1",
  "Byong Mok Oh, Max Chen, Julie Dorsey, and Fredo Durand.Image-based modeling and photo editing. In SIGGRAPH, pages433442, 2001. 1": "Bennett Wilburn, Neel Joshi, Vaibhav Vaish, Eino-Ville Talvala,Emilio Antunez, Adam Barth, Andrew Adams, Mark Horowitz,and Marc Levoy. High performance imaging using large cameraarrays. ACM TOG, 24(3):765776, 2005. 1 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan TBarron, Ravi Ramamoorthi, and Ren Ng.Nerf: Representingscenes as neural radiance fields for view synthesis. In ECCV, pages405421, 2020. 1, 2, 3 Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, QingShuai, Hujun Bao, and Xiaowei Zhou.Neural body: Implicitneural representations with structured latent codes for novel viewsynthesis of dynamic humans. In CVPR, pages 90549063, 2021. 1,2",
  "Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.pixelnerf: Neural radiance fields from one or few images. In CVPR,pages 45784587, 2021. 1, 2": "Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, HujunBao, and Xiaowei Zhou.Efficient neural radiance fields forinteractive free-viewpoint video. In SIGGRAPH Asia, pages 19,2022. 1, 3, 6, 7, 8, 12 Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, MarcPollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat:Efficient 3d gaussian splatting from sparse multi-view images. InECCV, 2024. 1, 2, 3, 7, 12",
  "generalizable gaussian splatting reconstruction from multi-viewstereo. In ECCV, 2024. 2, 3": "Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, LeiYang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practicalstereo matching via cascaded recurrent network with adaptivecorrelation. In CVPR, pages 1626316272, 2022. 2 Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, YohannCabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, LeonidAntsfeld, Boris Chidlovskii, and Jerome Revaud. Croco v2: Im-proved cross-view completion pre-training for stereo matchingand optical flow. In ICCV, pages 1796917980, 2023. 2",
  "Jeong Joon Park, Peter Florence, Julian Straub, Richard New-combe, and Steven Lovegrove.Deepsdf: Learning continuoussigned distance functions for shape representation.In CVPR,pages 165174, 2019. 2": "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Ko-mura, and Wenping Wang. Neus: Learning neural implicit sur-faces by volume rendering for multi-view reconstruction. NeurIPS,34:2717127183, 2021. 2 Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis,Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neuralimplicit surfaces for multi-view reconstruction. In ICCV, pages32953306, 2023. 2",
  "Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, andAnurag Ranjan. Neuman: Neural human radiance field from asingle video. In ECCV, pages 402418, 2022. 2": "Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srini-vasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla,Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In CVPR, pages 46904699, 2021. 3,7, 8, 12 Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, FanboXiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radi-ance field reconstruction from multi-view stereo. In ICCV, pages1412414133, 2021. 3",
  "Zhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu.Animat-able gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling.In CVPR, pages 1971119722,2024. 3": "Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng,Dahua Lin, Xihui Liu, and Ziwei Liu. Humangaussian: Text-driven3d human generation with gaussian splatting. In CVPR, pages66466657, 2024. 3 Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao,and Wei Xing.3dgstream: On-the-fly training of 3d gaussiansfor efficient streaming of photo-realistic free-viewpoint videos. InCVPR, pages 2067520685, 2024. 3 Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green,Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Love-grove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv.Neural 3d video synthesis from multi-view video. In CVPR, pages55215531, 2022. 3, 6, 7 Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zoll-hoefer, Johannes Kopf, Matthew OToole, and Changil Kim. Hy-perreel: High-fidelity 6-dof video with ray-conditioned sampling.In CVPR, pages 1661016620, 2023. 3",
  "Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussianfeature splatting for real-time dynamic view synthesis. In CVPR,pages 85088520, 2024. 3": "Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong,Yingliang Zhang, Jingyi Yu, and Lan Xu.Hifi4g: High-fidelityhuman performance rendering via compact gaussian splatting. InCVPR, pages 1973419745, 2024. 3 Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Minye Wu,Kaiwen Guo, and Lan Xu.Neuralhumanfvv: Real-time neuralvolumetric human performance rendering using rgb cameras. InCVPR, pages 62266237, 2021. 3 Youngjoong Kwon, Baole Fang, Yixing Lu, Haoye Dong, ChengZhang, Francisco Vicente Carrasco, Albert Mosella-Montoro, Jian-jin Xu, Shingo Takagi, Daeil Kim, et al.Generalizable humangaussians for sparse view synthesis. In ECCV, 2024. 3"
}