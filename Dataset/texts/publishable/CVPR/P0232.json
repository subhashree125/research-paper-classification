{
  "Abstract": "We introduce PlausiVL, a large video-language modelfor anticipating action sequences that are plausible in thereal-world. While significant efforts have been made to-wards anticipating future actions, prior approaches donot take into account the aspect of plausibility in an ac-tion sequence. To address this limitation, we explore thegenerative capability of a large video-language model inour work and further, develop the understanding of plau-sibility in an action sequence by introducing two objec-tive functions, a counterfactual-based plausible action se-quence learning loss and a long-horizon action repetitionloss.We utilize temporal logical constraints as well asverb-noun action pair logical constraints to create implau-sible/counterfactual action sequences and use them to trainthe model with plausible action sequence learning loss.This loss helps the model to differentiate between plausi-ble and not plausible action sequences and also helps themodel to learn implicit temporal cues crucial for the task ofaction anticipation. The long-horizon action repetition lossputs a higher penalty on the actions that are more proneto repetition over a longer temporal window. With this pe-nalization, the model is able to generate diverse, plausibleaction sequences. We evaluate our approach on two large-scale datasets, Ego4D and EPIC-Kitchens-100, and showimprovements on the task of action anticipation.",
  ". Introduction": "Having the ability to predict future events is a critical com-ponent in the decision-making process of an AI agent. Forexample, for an autonomous driving car, being able to antic-ipate the next sequence of actions for cars, pedestrians, andother agents in the scene can ensure safety of pedestrians aswell as vehicles. To enable this, the model should be ableto reason effectively from the spatial as well as temporal in-",
  "arXiv:2405.20305v1 [cs.CV] 30 May 2024": "reason that crack eggs could be one of the plausible futureactions.However, action anticipation is challenging because theuncertainty in precisely predicting the future makes the tasknon-deterministic in nature. In other words, given what hashappened so far, there are infinitely many possibilities forwhat future actions might happen. Moreover, action antic-ipation is accompanied by an additional challenge of un-derstanding the implicit temporal information present in anaction sequence, which makes the sequence plausible in thereal-world. For example, the model should be able to un-derstand that an action like crack eggs will always happenbefore cook omelette as shown in .To this end, there has been some progress for the actionanticipation task. Earlier works have explored an LSTMbased approach by summarizing the past and inferring thefuture , by logging the past history actions intext , or using RNN-based approaches by learn-ing goals. However, such LSTM/RNN-based approachesare unable to effectively capture the temporal relationsamong the actions over a long horizon due to their sequen-tial nature. Recent works have also explored transformer-based approaches , with a memory-based sys-tem or leveraging multiple-modalities . Whiletransformer-based approaches are able to model longer tem-poral understanding, they can still become confined to theinformation present in the training data and cannot modelthe diverse nature of the future actions. They rely on theability of the transformer encoder to learn from the giventraining data which limits their generalization and scalingcapability.To overcome the above challenges, recent methods [3, 34, 35, 64] have attempted to leverage the autoregressivetext generation capabilities of generative large-languagemodels (LLMs) to improve generalizability for various vi-sion tasks. Taking inspiration from these works and to ad-dress the challenges present in anticipating plausible ac-tions, we introduce PlausiVL, Plausible action anticipationthrough a large Video-Language model.Given the generative capabilities of large language mod-els, in this work, we introduce a video-large-languagemodel which can efficiently model and leverage the tem-poral cues present in a video to generate plausible actionsequences for the task of action anticipation. We use a Q-former based transformer architecture to embed videosinto spatio-temporal visual representations. This architec-ture ensures an effective alignment between the visual fea-tures and the desired text in the LLM embedding space. Inaddition to the alignment, we try to address the challengesthat are specifically present in the task of action anticipationand thus, introduce a method with the following importantcharacteristics: 1). The ability to understand the temporalcorrelations present among the actions in a sequence which in turn makes the action sequence temporally plausible inthe real-world, 2). Being able to model the diverse, possi-ble actions that can happen in the future. For example, forthe former characteristic, a model should follow a temporalconstraint that an action X has to happen before for the ac-tion Y to happen to make the sequence action X action Yplausible in the real-world.To build such temporal understanding required forgenerating plausible action sequences,we design acounterfactual-based plausible action sequence learningloss where we create temporal logic constraints and trainthe model to be able to differentiate between the plausibleand not plausible action sequences. Additionally, we alsouse verb-noun action logical constraints to further improvethe models understanding about which verbs are possiblewith which nouns to create a plausible action in the real-world (for example, cook spoon is not a plausible action).To our knowledge, the aspect of plausibility in generatingan action sequence has not been explored for the task ofaction anticipation. While this loss is helpful for efficienttemporal understanding, we also aim for the model to beable to understand the diverse nature of actions and generateplausible action sequences with less repeated actions as lan-guage models are prone to the issue of repetition. To resolvethis, we devise a long-horizon action repetition loss wherethe later actions that are more prone to repetition have ahigher penalty and the earlier, immediate actions have lowerpenalty. We summarize our contributions as follows:1. We present PlausiVL, a large video-language modelwhich leverages the spatial-temporal information presentin videos for anticipating plausible future action se-quences. 2. To learn the temporal cues and understand the temporaldependencies among actions in a plausible sequence, wedesign a counterfactual-based plausible action sequencelearning loss. We create temporal logic rules and verb-noun action pair logic constraints for the model to beable to understand plausibility in action sequences.",
  ". Related Works": "Large Language Models. Language Modeling is a methodto model the generative likelihood over the word token se-quences and predict the probabilities of the next/future to-kens. Large language models (LLMs) aretransformers with billions of parameters that have beentrained on massive amounts of data and have shown im-pressive capabilities on the task of question-answering andchat-conversation with humans.Methods like in-contextlearning , prompt tuning , chain-of-thought reason-ing , and reinforcement learning with human feed- back have improved the language models to per-form very well on few-shot tasks. While these models showgreat capabilities in understanding the input and solvingcomplex tasks via text generation, these models can onlyunderstand the text modality and are at a loss of the richinformation that is present in other modalities like video,audio. In our work, we utilize videos as input and learnfrom the visual and temporal information present in them.Large Vision-Language Models.Recent strides in thisdomain have seen diverse pre-training methods leveragingextensive multimodal datasets driving the progress of largevision-language models. Some models merge visual and linguistic modalities by co-training textand image encoders using contrastive loss on large datasetscontaining image-caption pairs.Meanwhile, other ap-proaches integrate visual input directly into languagemodel decoders through a cross-attention mechanism, es-chewing the use of images as additional prefixes. Anothercategory of vision-language models leverage Masked-Language Modeling (MLM) and Image-Text Matching (ITM) objectives to align image segmentswith text. BLIP-2 was one of the works which pro-posed a Qformer-based method to ensure visual-text align-ment. Since these works explore the image-text alignment,they are unable to model and understand the temporal in-formation that is present in videos. There have been ef-forts towards video-text alignment by using a linear layerto project the video space to the LLMs textual space inVideo-LLM or by using a Q-former based module inVideo-LLaMA. While these works explore video-text align-ment, these models can be ineffective for the task of actionanticipation as they do not understand the temporal correla-tions among the actions in a sequence.Temporal and symbolic logic reasoning. Symbolic logicreasoning is a method to create a system of rules andsymbols in the form of logical expressions.Temporallogic reasoning specifically designs logical expressions forrepresenting and reasoning about time.Linear tempo-ral logic , metric temporal logic , signal temporallogic , and interval temporal logic are some meth-ods for develop temporal logical rules. We take inspirationfrom the work DTL to generate temporal logic rulesand create counterfactual sequences of actions.Action Anticipation.This task has been explored forthird-person videos as well as egocentricvideos . Standard approaches forthis task can be divided into LSTM/RNN-based ap-proaches and transformer-based approaches. LSTM-basedapproaches mainly use a rolling LSTM to encodethe observed video and store an updated summary. For in-ference, an unrolling LSTM is initialized with the hiddenand cell state of the rolling LSTM to predict the next action.While LSTM/RNNs have shortcomings in modeling long- horizon temporal dependencies, some approaches mitigatethis issue via goal-based learning , diverse attentionmechanism , skip-connections , message passingframework , memory-based modules or simi-larity metric . Recent works have explored transformer-based approaches with global attention , mod-elling apperance change in human-object interactions ,conditioning on intention , hierarchical feature aggre-gation . While most of the works explore it in a uni-modal setting by using the visual modality, other works alsopresent a multi-modal approach for this task by using opti-cal flow , object-based features or au-dio . Other works explore uncertainty-based meth-ods and GAN-based approach . We takeinspiration from the object detection literature for therepetition loss. Concurrent to our work, there have beentext-based LLM approaches which explore the taskof action anticipation, however, they only operate in the tex-tual space and lose the visual-temporal information presentin video.",
  ". Model Architecture": "Given a video clip of N frames, V = [v1, v2, v3....vN], weuse a frozen visual encoder (ViT) to extract video-frame-level representations, V= [v1, v2, v3....vN].After this,each frame feature is passed through a Q-former withk number of query tokens, to get the dq-dimensional visualrepresentation as vi Rkdq. These queries are helpfulin extracting the visual features with the most informationaligned to the text. For the frames to have an understandingof the temporal relations among them, a frame position em-bedding layer is applied to each Q-former feature. At thesame time, we also apply a clip-position embedding layerto infuse more grouping information about the frames thatbelong to a clip. These features are then passed through avideo Q-former to aggregate the spatio-temporal informa-tion of the video. Finally, a linear projection layer is usedto project these output representations to the LLM text em-bedding space of dl dimension, vi Rkldl. These videoembeddings can be considered as visual prompts which areconcatenated with the input text embeddings ti to make theLLM generate text conditioned on the video content.",
  "(c) Objective Functions and Training": ". Model diagram:(a) PlausiVL: Given a video, a frozen visual encoder a Q-former with k number of query tokens is used to extractframe level representations which are further concatenated with a frame position embedding layer to add temporal understanding. Next,the representations are passed through the video Q-former and a linear layer is added to project these features into the LLM space. Thesevisual embeddings (visual prompts) and are concatenated with text-prompts to get the desired output text (Sec 3.1), (b) Augmentation: Forplausible action anticipation, we use logical rules to create counterfactual implausible action sequences. Given an input video, we create apositive augmentation of the video and a negative augmentation by using temporal logical and verb-noun action pair constraints (Sec 4.1).(c) Objective Functions and Training: We train our model with two losses: (i) Plausible Action Sequence Learning Loss (Lplau)which aligns the original video-plausible text pair closer to the positive augmentation of video-plausible text, and brings the original video-plausible text far apart from the video-counterfactual text. (Sec 4.1), (ii) long-horizon action repetition loss that ensures diverse and lessrepetitive actions by adding a higher penalty to the later tokens (mix mixture and wipe hands) and lower penalty to immediate futureactions (pour water, pour water). The graph shows the linearly increasing penalty for the tokens over the long-horizon (Sec 4.2). horizon temporal dependencies among the actions which iscrucial for plausible action anticipation. To develop suchtemporal understanding in a model, we train our system tooptimize two losses, (1). Plausible Action Sequence Learn-ing loss Lplau and (2). Long-horizon action repetition lossLrep. With these two losses, the model can understand the",
  "For a model to be able to understand the plausible natureof an action sequence, it should be able to leverage the im-": "plicit temporal information present in input videos. Thus,we design a self-supervised plausible action sequence learn-ing loss, Lplau. The key idea is to create counterfactualsbased on temporal logical constraints as well as verb-nounaction pair logical constraints and optimize the network byminimizing a loss with two negative log-likelihood terms:(1) increase the probability of associating the visual modal-ity with the temporally correct and plausible sequence ofactions, and (2) decrease the probability of associating thevideo with the action sequences that are not plausible in thereal-world and temporally incorrect. Here, sequences of ac-tion that satisfy the temporal as well as verb-noun actionpair logic constraints are considered as logically correct.Temporal logical constraints: In our work, we define atemporal constraint for an action sequence as follows: anaction X that has to happen before an action Y to make ita plausible sequence in the real-world. Consider for exam-ple, given a sequence of take eggs crack eggs whiskeggs cook omelette, a counterfactual of this sequence ofactions would be, take eggs cook omelette whisk eggs crack eggs since crack eggs would always happen beforecook omelette. Mathematically, we define it as follows:",
  "(1)": "where CF temp(ai, aj) is an action pair matrix with a valueof 1 if ai always happens before aj for all the ground truthsequences t T, a value of -1 if ai always happens afteraj, and 0 otherwise if there is no relation between the twoactions. With this temporal logical constraint, given a textsequence t, we perform a swap operation if there is a for-ward or backward relation between an action pair. Hence,given a ground truth text sequence t, we define the operationif aj always happens before ap as follows:",
  "(4)": "where CF act(ai, aj) is a verb-noun pair matrix with a valueof 1 if for a verb, the corresponding noun is not plausible orvice-versa in all the ground truth actions t T and 0 other-wise if the verb-noun pair is plausible. Similar to the tempo-ral constraints mentioned above, with this verb-noun actionpair constraint, given an action, we swap either the verb ornoun with a uniform probability to create implausible verb-noun action pairs. Given a text action pair t, we define theoperation of a counterfactual, implausible verb-noun actionpair as follows:",
  "(8)": "where vi and vi are the visual embeddings of the originalvideo and augmented video (respectively), ti and tcfiarethe text embeddings of the ground truth text sequence andcounterfactual text (respectively), is the temperature, is the sigmoid function, p(., .) is the cross-modalvideo-text representation from LLM after passing througha MLP projection layer (absorbed in the equation for betterreadability), and sim is the similarity function.",
  ". Long-Horizon Action Repetition Loss": "While the plausible action sequence learning loss Lplauhelps the model to understand the implicit temporal infor-mation present in the action sequences, we consider anotheraspect of plausibility by reducing the repetition of actionsand in turn generating more diverse actions. We observethat while the model is able to generate accurate, temporallycorrect, and diverse actions over a short temporal window,it starts repeating the same actions over a longer horizon.To mitigate this, we train the model by enforcing a largerpenalty on the actions that happen over a longer horizonin the temporal window and lesser penalty to the actionsthat are immediately near to the observed video. We add apenalty of t over the negative log-likelihood of the proba-bility as follows:",
  "Lrep(pt) = tlog(pt)(10)": "where yt is the output from the language model for the tthtoken over which softmax operation is applied to get theprobability pt. t is the value temporally unique to thetth token following the order, 0 < 1 < 2 < < N.In summary, by optimizing the Lrep loss, the model ispenalized more for the actions that happen over a longerhorizon and less penalized for immediate actions. This ishelpful in regulating repetition and ensuring more diverseactions in the generated text.Finally, we train our model with the overall loss as:",
  ". Implementation Details": "We process the videos of size 224 224 with Ego4Dcontaining 8 clips with 4 frames, making a total of 32frames, and EPIC-Kitchens-100 with 32 frames as well. Weuse the pretrained Qformer model, BLIP2-FlanT5xxl fromBLIP2 with number of query tokens as 32 and ViT-G/14 as our vision encoder. We train our method end-to-endwith a learning rate of 1e5, for 100 epochs, and = 0.5and = 0.5. We use LLaMA-2-7B as our language model. For long-horizon action repetition loss, Lrep, we use inthe uniform distribution from with number of stepsequal to the number of output tokens from the languagemodel. For plausible action sequence learning loss Lplau,we use video augmentation of color jitter, random horizon-tal flip, and a random rotation of 10 degrees.",
  ". Experimental Setup": "Datasets: We evaluate on two action anticipation datasets:Ego4D and EPIC-Kitchens-100 . Ego4D is a large-scale egocentric dataset covering diverse indoor and out-door scenarios like home, workplace, etc. It consists of3670 hours of videos with 115 verbs and 478 nouns. Toevaluate our method on Ego4D, we use videos from theForecasting and Hand-Object interaction subset and showresults on the validation set. In Ego4D, a video and a stop-ping time is given, and the model predicts N sets of se-quences having Z number of actions in the form of verb-noun pairs, {{(vz,n, nz,n)}Zz=1}Nn=1, where, vz,n is the pre-dicted verb and nz,n is the predicted noun.EPIC-Kitchens-100 is an egocentric dataset of akitchen-based environment. It consists of 100 hours of ego-centric videos with 97 verbs and 300 nouns. For this dataset,given an action segment that starts at time s, the model hasto predict the anticipated action by observing a video seg-ment of duration [s (o + a), s a] where o is theobservation time and a is the anticipation time. The antic-ipation time a means how much in advance the model hasto anticipate the action.Baselines:We compare our method with large video-language models , Video-LLaMA and Video-LLM .We also compare our method with the transformer andLSTM-based approaches for action anticipation along withtext-based large language models for a more exhaustiveanalysis.Ablation Study: In the ablation study, we present resultsof PlausiVL with and without Lplau and Lrep objectivefunctions to show the effect of each component on the finalperformance of the model.",
  ". Discussion of Results": "Referring to and , we can observe that Plau-siVL is able to perform better when compared with thebaselines. This can be attributed to its ability to be ableto understand the plausibility in the action sequences andleverage the temporal correlations among the actions in asequence. We present a closer analysis of the results in ourdiscussion following next.PlausiVL shows performance gain towards action antic-ipation: Prior large video-language models haveonly explored the visual-text alignment and lack the tem-poral understanding needed for the action anticipation. Toshow that our model is able to learn the temporal un-",
  "Ground Truth100.004.33": ". BLEU score and Repetition Score on the Ego4D dataset.For BLEU score, : Higher is better, and for repetition score, :lower is better. We can observe that both the BLEU score andrepetition score are better for PlausiVL than Video-LLaMA. EPIC-Kitchens-100 in . The improvement in theperformance emphasizes that the model is able to learn thetemporal dependencies among the actions to generate moreaccurate and plausible action sequences. Qualitative resultspresented in also show the quality of our generatedsequence in comparison to the ground truth. We can seethat our method is able to understand the activity happeningthe video and anticipate the temporal future action sequenceaccordingly. We also exhaustively compare PlausiVL withprior approaches in and that utilize trans-former and LSTM-based architectures and show that ourmethod is able to perform better.Lplau helps the model to learn plausible future actionsequences: We hypothesize that for generating accurate fu-ture action sequences, a model should have an understand-ing about the temporal plausibility of an action sequence inthe real-world. To assess if our devised loss function, plau-sible action sequence learning loss, Lplau is able to createsuch understanding in the model, we compare our method,row (1) and our method without Lplau, rows (3) and (4)in . We observe by removing this module, there isa drop in the performance of 1.2 % on verbs for Ego4Dand 1.47 % for verbs of EPIC-Kitchens-100 (row(1) androw(3) are compared). This shows that training a modelwith Lplau as an objective function helps the model tolearn the implicit temporal information of action correla-tions in a sequence. Through learning to differentiate be-tween the plausible and not plausible action sequences andaligning the video representations closer to the plausibleaction sequences, the model learns an effective video-textalignment which helps in generating more accurate, plausi-ble future action sequences.",
  "PlausiVL49.5053.9027.0148.4441.2922.10": ". Performance of action anticipation on EPIC-Kitchens-100 Unseen Participants and Tail Classes on class-mean Top-5 recall (%)): Higher is better. Our method is able to outperform all the previous baselines. Prediction: take iron, take pants, put pants, adjust pants, take iron, press pants, put iron, adjust pants, take iron, press pants, turn pants, adjust pants, take iron, press pants, put iron, adjust pants, take iron, turn pants, put iron, adjust pants Ground Truth: take iron, press pants, hold iron, press pants, put iron, take iron, press pants, turn pants, arrange pants, take iron, press pants, adjust pants, turn pants, arrange pants, take iron, turn pants, put pants, touch pants, take pants, fold pants",
  "Video": "Prediction: put card, take card, touch card, take card, put card, take card, put card, adjust card, pack card, take card, put card, take card, put card, adjust card, take card, adjust card, take card, put card, pack card, put card Ground Truth: put card, touch card, take card, put card, take card, put card, pack card, take card, put card, pack card, take card, put card, take card, put card, put card, pack card, arrange card, take card, shuffle card, put card",
  "Time": ". Qualitative Results over videos of diverse environments like kitchen, construction sites, etc. and their respective anticipatedactions from our method. Given a video, the top blue box shows the prediction from PlausiVL and the green box contains the ground truthaction sequence for reference. The model is able to generate plausible action sequences.",
  "Training with Lrep loss vs prompt tuning: We performan analysis where instead of training the model with Lrepobjective function, we simply prompt the model with the": "phrase: Do NOT repeat actions (DNR). We comparePlausiVL trained with Lplau and Lrep losses (row 2)and PlausiVL trained with Lplau and DNR prompt (row1) and present the results of this analysis for Ego4D andEPIC-Kitchens-100 in . We can observe that simplyprompting the model with DNR in the prompt does notgive much improvement in the performance as comparedto training the model with long-horizon action repetitionloss (Lrep) as objective function.Training the modelLrep penalizes the model for repeating the actions andmakes the model learn to generate more diverse actions.This penalty is helpful in reducing repetition of the actionsover a long-horizon. Simply stating DNR in the promptonly gives an instruction/command to the model, whereas,training the model with Lrep loss influences the learningof the model which is needed for the task of action antici-pation. Large Video-language model vs Text-large-language-model: Given the exploration of text-only large languagemodels, we also address the comparison between text-basedLLM and large video-language models for the task of ac-tion anticipation. We compare PlausiVL with AntGPT which is a text-based LLM and observe a performance gainof 2.1% on verbs and 3.6% on nouns for Ego4D from ourmethod. We reason that a major drawback of text-basedLLM for this task is that they completely discard the vi-sual as well as temporal information present in the videos.Whereas, the task of action anticipation is highly dependenton the visual spatio-temporal information to understand thereal-world temporal flow of actions and anticipate actionsaccurately. Incorporating visual modality can give crucialinformation such as the environment of the agent, the ob-jects interacted with, and other objects in the scene whichmight be interacted with later in the future. Such vital in-formation is lost when converting a video into textual ac-tions or into a summary . Summarizing a video intotext-based information can only provide the high-level de-tails about a video, but it doesnt give a signal about the real-world temporal flow of the actions and objects in a video.PlausiVL is able to generate plausible action sequences:To further emphasize the plausibility,less repetitionand quality our generated text, we compute the BLEUscore and repetition score.The repetition score isan average of the number of actions that are repeated inan action sequence and the BLEU score measures thesimilarity between our generated text and ground truth. Wereport the results in .By having a better BLEUscore than the baseline, we show that the generated textfrom our method is a more plausible action sequence,thus emphasizing the efficacy of our objective functions.Similarly, by having a lower repetition score than thebaseline, we show that the model has lesser repetitiveactions in the generated sequence.Our method repeats5.87 actions in an action sequence on average whereasVideo-LLaMA repeats an average of 7.12 actions. We alsoobserve an average repetition of 4.33 actions in groundtruth action sequences.Moreover, a lower edit distancemetric in also indicates less repetition and moreplausibility in the generated text as a lower metric wouldmean less substitutions were made to bring the output textcloser to the ground truth. Generalization and robustness to long-tail: We evaluateour method on the unseen participants and tail classes ofEPIC-Kitchens-100 and present the results in .Unseen participants consists of those participants that arenot present in the train set and tail classes are definedto be the smallest classes whose instances are around20% of the total number of instances in the train set. We",
  ". Analysis of a vs. verb-noun class-mean Top-5 recall(%) accuracy () on EK100": "observe that a better performance of our approach on theunseen participants as compared to the other baselinesshows the generalizability of our model across unseen data.Similarly, a better performance on the tail classes showsthat our model is robust to the long-tail distribution of theEPIC-Kitchens-100 dataset. Anticipation time a vs Accuracy: a is the anticipationtime between the end time of observed video and the start-ing time of the first action to be anticipated. The video dur-ing the anticipation period a is unobserved. For EK100,a=1s and for Ego4D, a=2.20s on an average. We analyzechanging a versus accuracy on EK100 in . We canobserve that the method is quite robust till a=3.5s whereasVideo-LLaMA is only robust till a=2.0s for EK100. Thisshows that the model can predict future actions even with afar anticipation time.",
  ". Conclusion": "In this work, we leverage the generative capabilities of largevideo-language models for plausible action anticipation. Inaddition to the abilities of large video-language models , forthe model to better understand the plausibility in an actionsequence, we introduce a plausible action sequence learn-ing loss which helps the model to differentiate betweenthe plausible and not plausible action sequences, and thuslearn anticipation related temporal cues.We further de-vise a long-horizon action repetition loss that puts a higherpenalty on the actions that happen over a longer temporalwindow and are more prone to repetition, thus mitigatingaction repetition and ensuring more diverse actions. Exper-imental results show that our model is able to perform betterby generating more plausible and accurate action sequenceson Ego4D and EPIC-Kitchens-100. While our method is aninitial step towards plausible action anticipation, there canbe further exploration mitigating the issue of hallucinatingimplausible action sequences in the future work.",
  "Yazan Abu Farha and Juergen Gall. Uncertainty-aware antic-ipation of activities. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision Workshops, pages00, 2019. 3": "Yazan Abu Farha, Alexander Richard, and Juergen Gall.When will you do what?-anticipating temporal occurrencesof activities.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 53435352,2018. 3 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. Advances inNeural Information Processing Systems, 35:2371623736,2022. 2, 3 Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, andKristen Grauman.Hiervl:Learning hierarchical video-language embeddings.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 2306623078, 2023. 7 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners. Advances in neural in-formation processing systems, 33:18771901, 2020. 2 Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, YifeiHuang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, TongLu, et al. Videollm: Modeling video sequence with largelanguage models. arXiv preprint arXiv:2305.13292, 2023.3, 6, 7, 8 Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed El-hoseiny. Visualgpt: Data-efficient adaptation of pretrainedlanguage models for image captioning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1803018040, 2022. 3 Junwen Chen, Gaurav Mittal, Ye Yu, Yu Kong, and MeiChen. Gatehub: Gated history unit with background sup-pression for online action detection.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1992519934, 2022. 3 Liunian Harold Chen, Yukun Zhu, Yen-Chun Shen, HengGao, Xiaodong Liu, Xiaohui Shen, Zhe He, Ricardo Henao,Renjie Miao, Yuan Guo, et al. Uniter: Universal image-textrepresentation learning. In ECCV, 2020. 3 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, SebastianGehrmann, et al.Palm: Scaling language modeling withpathways. arXiv preprint arXiv:2204.02311, 2022. 2",
  "Georgios E Fainekos and George J Pappas.Robustnessof temporal logic specifications for continuous-time signals.Theoretical Computer Science, 410(42):42624291, 2009. 3": "Basura Fernando and Samitha Herath. Anticipating humanactions by correlating past with the future with jaccard simi-larity measures. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1322413233, 2021. 3 Antonino Furnari and Giovanni Maria Farinella.Rolling-unrolling lstms for action anticipation from first-personvideo. IEEE transactions on pattern analysis and machineintelligence, 43(11):40214036, 2020. 2, 3",
  "Antonino Furnari and Giovanni Maria Farinella. Towardsstreaming egocentric action anticipation. In 2022 26th Inter-national Conference on Pattern Recognition (ICPR), pages12501257. IEEE, 2022. 3": "AntoninoFurnari,SebastianoBattiato,and GiovanniMaria Farinella. Leveraging uncertainty to rethink loss func-tions and evaluation measures for egocentric action anticipa-tion. In Proceedings of the European Conference on Com-puter Vision (ECCV) Workshops, pages 00, 2018. 3 Andreas Furst, Elisabeth Rumetshofer, Johannes Lehner,Viet T Tran, Fei Tang, Hubert Ramsauer, David Kreil,Michael Kopp, Gunter Klambauer, Angela Bitto, et al.Cloob: Modern hopfield networks with infoloob outperformclip. Advances in neural information processing systems, 35:2045020468, 2022. 3 Harshala Gammulle, Simon Denman, Sridha Sridharan, andClinton Fookes. Predicting the future: A jointly learnt modelfor action anticipation.In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 55625571, 2019. 3",
  "Rohit Girdhar and Kristen Grauman.Anticipative videotransformer. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 1350513515, 2021.2, 3, 7, 8": "Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha,and Minsu Cho.Future transformer for long-term actionanticipation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 30523061, 2022. 2, 3 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1899519012, 2022. 3, 6, 7,13 Hongji Guo, Nakul Agarwal, Shao-Yuan Lo, KwonjoonLee, and Qiang Ji.Uncertainty-aware action decouplingtransformer for action anticipation. In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, 2024. 3",
  "Daoji Huang, Otmar Hilliges, Luc Van Gool, and Xi Wang.Palm: Predicting actions through language models@ ego4dlong-term action anticipation challenge 2023. arXiv preprintarXiv:2306.16545, 2023. 3, 9": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In Internationalconference on machine learning, pages 49044916. PMLR,2021. 3 Qiuhong Ke, Mario Fritz, and Bernt Schiele.Time-conditioned action anticipation in one shot. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 99259934, 2019. 3",
  "Vladimir I Levenshtein et al. Binary codes capable of cor-recting deletions, insertions, and reversals. In Soviet physicsdoklady, pages 707710. Soviet Union, 1966. 13": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models.arXivpreprint arXiv:2301.12597, 2023. 2, 3, 6 Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, ZichengLiu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 2311923129, 2023. 2",
  "Esteve Valls Mascaro, Hyemin Ahn, and Dongheui Lee.Intention-conditioned long-term human egocentric actionforecasting@ ego4d challenge 2022.arXiv preprintarXiv:2207.12080, 2022. 7": "Esteve Valls Mascaro, Hyemin Ahn, and Dongheui Lee.Intention-conditioned long-term human egocentric actionanticipation. In Proceedings of the IEEE/CVF Winter Con-ference on Applications of Computer Vision, pages 60486057, 2023. 3 Himangi Mittal, Pedro Morgado, Unnat Jain, and AbhinavGupta. Learning state-aware visual representations from au-dible interactions. Advances in Neural Information Process-ing Systems, 35:2376523779, 2022. 3, 7",
  "Angelo Montanari. Metric and layered temporal logic fortime granularity. University of Amsterdam, 1996. 3": "Nada Osman, Guglielmo Camporese, Pasquale Coscia, andLamberto Ballan.Slowfast rolling-unrolling lstms for ac-tion anticipation in egocentric videos. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 34373445, 2021. 2, 3 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-roll Wainwright, Pamela Mishkin, Chong Zhang, SandhiniAgarwal, Katarina Slama, Alex Ray, et al.Training lan-guage models to follow instructions with human feedback.Advances in Neural Information Processing Systems, 35:2773027744, 2022. 3 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu. Bleu: a method for automatic evaluation of machinetranslation. In Proceedings of the 40th annual meeting of theAssociation for Computational Linguistics, pages 311318,2002. 9",
  "Amir Pnueli. The temporal logic of programs. In 18th An-nual Symposium on Foundations of Computer Science (sfcs1977), pages 4657. ieee, 1977. 3": "Zhaobo Qi, Shuhui Wang, Chi Su, Li Su, Qingming Huang,and Qi Tian.Self-regulated learning for egocentric videoactivity anticipation. IEEE Transactions on Pattern Analysisand Machine Intelligence, 2021. 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. arXiv preprint arXiv:2103.00020, 2021. 3 Mamshad Nayeem Rizve, Gaurav Mittal, Ye Yu, MatthewHall, Sandra Sajeev, Mubarak Shah, and Mei Chen.Piv-otal: Prior-driven supervision for weakly-supervised tempo-ral action localization. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages2299223002, 2023. 3 Ivan Rodin, Antonino Furnari, Dimitrios Mavroeidis, andGiovanni Maria Farinella. Untrimmed action anticipation.In International Conference on Image Analysis and Process-ing, pages 337348. Springer, 2022. 3",
  "Debaditya Roy, Ramanathan Rajendiran, and Basura Fer-nando. Interaction visual transformer for egocentric actionanticipation. arXiv preprint arXiv:2211.14154, 2022. 2, 3,7, 8": "Fadime Sener, Dipika Singhania, and Angela Yao. Temporalaggregate representations for long-range video understand-ing. In Computer VisionECCV 2020: 16th European Con-ference, Glasgow, UK, August 2328, 2020, Proceedings,Part XVI 16, pages 154171. Springer, 2020. 7, 8 Yuge Shi, Basura Fernando, and Richard Hartley. Action an-ticipation with rbf kernelized feature mapping rnn. In Pro-ceedings of the European Conference on Computer Vision(ECCV), pages 301317, 2018. 2, 3 Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-laume Couairon, Wojciech Galuba, Marcus Rohrbach, andDouwe Kiela. Flava: A foundational language and visionalignment model.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1563815650, 2022. 3 Tsung-Ming Tai, Giuseppe Fiameni, Cheng-Kuang Lee, Si-mon See, and Oswald Lanz. Unified recurrence modeling forvideo action anticipation. In 2022 26th International Con-ference on Pattern Recognition (ICPR), pages 32733279.IEEE, 2022. 3",
  "Hui Tan and Mohit Bansal. Lxmert: Learning cross-modalityencoder representations from transformers.In EMNLP,2019. 3": "Ross Taylor, Marcin Kardas, Guillem Cucurull, ThomasScialom, Anthony Hartshorn, Elvis Saravia, Andrew Poul-ton,Viktor Kerkez,and Robert Stojnic.Galactica:A large language model for science.arXiv preprintarXiv:2211.09085, 2022. 2 Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothee Lacroix, BaptisteRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.Llama:Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. 2 Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. An-ticipating visual representations from unlabeled video.InProceedings of the IEEE conference on computer vision andpattern recognition, pages 98106, 2016. 3",
  "Exploring unified video-language pre-training. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 65986608, 2023. 2": "Lan Wang, Gaurav Mittal, Sandra Sajeev, Ye Yu, MatthewHall, Vishnu Naresh Boddeti, and Mei Chen.Protege:Untrimmed pretraining for video temporal grounding byvideo temporal grounding. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 65756585, 2023. 3 Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large lan-guage models. Advances in Neural Information ProcessingSystems, 35:2482424837, 2022. 2 Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Car-los Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalogto enhance prompt engineering with chatgpt. arXiv preprintarXiv:2302.11382, 2023. 2 Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, HaoqiFan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.Memvit: Memory-augmented multiscale vision transformerfor efficient long-term video recognition. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1358713597, 2022. 2, 3, 7, 8 Ziwei Xu, Yogesh Rawat, Yongkang Wong, Mohan SKankanhalli, and Mubarak Shah. Dont pour cereal into cof-fee: Differentiable temporal logic for temporal action seg-mentation. Advances in Neural Information Processing Sys-tems, 35:1489014903, 2022. 3 Ce Zhang, Changcheng Fu, Shijie Wang, Nakul Agarwal,Kwonjoon Lee, Chiho Choi, and Chen Sun. Object-centricvideo representation for long-term action anticipation.InProceedings of the IEEE/CVF Winter Conference on Ap-plications of Computer Vision (WACV), pages 67516761,2024. 2, 3",
  "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: Aninstruction-tuned audio-visual language model for video un-derstanding. arXiv preprint arXiv:2306.02858, 2023. 3, 6,7, 8, 13": "Qi Zhao,Shijie Wang,Ce Zhang,Changcheng Fu,Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, and ChenSun. AntGPT: Can large language models help long-termaction anticipation from videos?In The Twelfth Interna-tional Conference on Learning Representations, 2024. 3, 7,9 Zeyun Zhong, David Schneider, Michael Voit, RainerStiefelhagen, and Jurgen Beyerer. Anticipative feature fusiontransformer for multi-modal action anticipation. In Proceed-ings of the IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 60686077, 2023. 2, 3, 7, 8",
  "A.1. Metrics": "Edit Distance (ED@(Z=20)) : This metric is com-puted over a sequence of verb and noun predictions usingthe Damerau-Levenshtein distance and takes intoaccount the sequential nature of the action anticipationtask.A prediction is considered correct if it matchesthe ground truth at a specific time step using the editdistance operations - insertion, deletion, substitution, andtransposition. A total of K predictions are evaluated andthe smallest edit distance between a prediction and groundtruth is reported . We consider the value of Z = 20 andK = 5 which is the same as Ego4D . Class-mean Top-5 Recall (%) : This metric evalu-ates if the ground truth class is within the top-5 predictionsand averages the per-class performance to equally weightall the classes. The top-k criterion takes into account theuncertainty/multi-modality in the future action predictionand class-mean is helpful for balancing the long-tail dis-tribution.",
  "B. Quantitative Analysis": "Analysis of plausibility in generated action sequence:To evaluate if the generated text is a plausible actionsequence and additionally, the efficacy of the Lplauand Lrep objective functions, we calculate the averagenumber of temporal and action constraints followed inthe generated text.We compare the average number ofconstraints followed by PlausiVL versus the baselineVideo-LLaMA and present the graph visualizationin . We report the average number of constraintsfollowed over the training and show the number over thecheckpoints from beginning till the end of training. Fromthe figure, we can observe that as the training of the modelwith Lplau and Lrep losses progresses, the averagenumber of constraints followed increases in the generatedtext. Morever, the average number of PlausiVL is higherthan that of Video-LLaMA. This indicates that by trainingthe model with Lplau and Lrep objective functions, themodel can generate more plausible action sequences andthey help the model learn the implicit temporal informationneeded for plausible action anticipation.",
  "Lrep loss is dataset independent: We perform an anal-ysis to highlight that repetition loss is independent of the": ". Analysis of plausibility in generated action sequence:Black line represents our method and orange is the baseline,Video-LLaMA. Comparing the two line plots, we can observethat PlausiVL follows more number of temporal and action con-straints over training than Video-LLaMA indicating that the objec-tive functions Lplau and Lrep are helping the model to learntemporal cues needed to generate plausible action sequences foraction anticipation.",
  ".Contrastive Loss with negative sample from othervideos (CLR Paradigm) for Ego4D on ED@(Z=20)": "dataset. In other words, the performance of the repetitionloss does not depend on the number of repeated actions ina dataset. We present this analysis in . We observeno strong correlation between n rep and performance,showing data-independency and also show that PlausiVLw/ repetition loss reduces repetition and outperforms thebaseline. Different videos as negative samples for Lplau loss: Forthe Lplau loss, we use an implausible action sequence asa negative sample. We perform an analysis of using neg-ative samples from other videos and show the results in. This setting performs worse than Row 2,3 as it",
  "C. Qualitative Analysis": "In this section, we present more qualitative results of ourmethod. Given a video, the top blue box shows the predic-tion from PlausiVL and the green box contains the groundtruth action sequence for reference. We can observe thatour method is able to understand the activity happening inthe video and then, generate action sequences accordingly.Additionally, PlausiVL is able to generate action sequencesthat satisfy the temporal logic constraints and are diversewith less repetitions. The predicted action sequence is alsocloser to the ground truth action sequence. Prediction: take tape, measure ladder, put tape, put metal, weld metal, take metal, put tape, measure metal, take rod, weld metal, take tape, put tape, measure metal, weld metal, take welding, weld metal, put welding, take rod, weld metal, put tape Ground Truth: take tape, measure metal, put tape, hold metal, weld metal, take metal, hit metal, move needle, take tape, measure metal, mark metal, mark metal, measure metal, mark metal, measure metal, mark tape, mark metal, mark metal, mark metal, put pencil"
}