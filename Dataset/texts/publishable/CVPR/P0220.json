{
  "Abstract": "Pixel-level Scene Understanding is one of the funda-mental problems in computer vision, which aims at recog-nizing object classes, masks and semantics of each pixelin the given image.Compared with image scene pars-ing, video scene parsing introduces temporal information,which can effectively improve the consistency and accuracyof prediction,because the real-world is actually video-basedrather than a static state. In this paper, we adopt semi-supervised video semantic segmentation method based onunreliable pseudo labels. Then, We ensemble the teachernetwork model with the student network model to generatepseudo labels and retrain the student network. Our methodachieves the mIoU scores of 63.71% and 67.83% on devel-opment test and final test respectively. Finally, we obtain the1st place in the Video Scene Parsing in the Wild Challengeat CVPR 2024.",
  ". Introduction": "The Video Scene Parsing in the Wild (VSPW) isa video semantic segmentation dataset with 3536 videosand annotations of 124 categories .Thanks to the availabil-ity of various semantic segmentation datasets, significantprogress has been made in image semantic segmentation.The challenge aims to assign pixel-wise semantic labels toeach video frame of the test set videos in the VSPW. Theprominent evaluation metric for the challenge is the meanIntersection over Union (mIoU).With the development of deep neural networks and theavailability of large-scale labeled data, the capabilities ofVideo Semantic Segmentation (VSS) have been notablyextended . VSS is a spatial-temporal variation of im-age segmentation on videos that aims to predict a pixel-wise label across sequential video frames .Comparedwith image semantic segmentation, most existing meth-ods of VSS methods emphasize the exploitation of tem-poral information.Several approaches utilized opti- cal flow prediction to model temporal information betweenframes.However, optical flow may lead to unbalanced la-tency. ETC used the temporal loss and the new tempo-ral consistency knowledge distillation on the per-frame seg-mentation predictions as an efficient replacement for opticalflow. MRCFA mining the relationship of cross-frameaffinities to achieve better temporal information aggrega-tion.TMANet is the first work using a temporal memoryattention module to capture the temporal relations betweenframes in VSS. LLVSS designed an efficient frame-work involving both adaptive feature propagation and adap-tive key-frame schedule.DVIS streamlines the processby framing VSS as an initial segmentation task followedby tracking, subsequently refining segmentation outcomesusing comprehensive video data. The 1st Place Solutionfor CVPR 2023 PVUW VSS Track focusing on en-hancing spatial-temporal correlations with contrastive lossand leveraging multi-dataset training with label mapping toboost model performance.",
  ". Approach": "In this section, we describe the overall architecture ofour network. And then, we introduce a semi-supervisedvideo semantic segmentation method based on unreliablepseudo labels . We first train the teacher network andstudent network on labeled data, then use the teacher net-work to generate pseudo labels, combine them with theoriginal dataset to form a new dataset, and then retrain thestudent network. Through semi-supervised training, we im-prove the performance of the model on unlabeled datasets.",
  ". Overview": "Transformer is a neural network model based on at-tention mechanism, which has achieved significant suc-cess in natural language processing and other sequencedata processing tasks. In recent years, with the develop-ment of transformer technology, it has also made signifi-cant progress in the field of segmentation. Given that theOne-peace algorithm achieves state-of-the-art perfor-",
  ". Semi-supervised method": "With the development of deep learning methods, thereis a qualitative improvement in segmentation performance.However, high-performance deep learning models require alarge amount of data and labeling, especially pixel level la-beling, which requires a huge investment of manpower andtime costs. Therefore, methods based on semi-supervisedlearning are highly favored by researchers. The core prob-lem of semi-supervised learning is to effectively utilize un-labeled samples as a supplement to labeled samples, therebyimproving the performance of the model. The conventionalsemi- supervised methods retain high confidence predic-tion results by screening samples, but these results in alarge amount of unlabeled data not being effectively uti-lized, leading to insufficient model training. It is difficultto assign the correct labels to unlabeled pixels for some un-predictable categories. Therefore, we treat unreliable pre-diction results as negative samples to participate in modeltraining, allowing all unlabeled samples to play an effectiverole in the training process.",
  ". The pipeline of our method": "As shown in , how to extract effective informa-tion from unmarked data is a crucial factor, so we use semi-supervised learning methods. Specificly,in the first step, wetrain the teacher network model and student network modelusing labeled training data, and then use multi-scale andhorizontal flipping to enhance testing and model ensembleto generate pseudo labels. Then we combine the unlabeledand labeled datasets into a new dataset, and continue fine-tuning the student network model. For pseudo labels, pixellevel entropy is used to filter reliable pixels and unreliablepixels. For unreliable pixels as negative samples, compar-ative loss training is used to ensure that the entire pseudolabel can be effectively used during the training process.",
  ". Experiments of different crop size with ViT-Adapter-L onPVUW2024 challenge test part 1": "we first use a teacher model for prediction. Then, we usepixel level entropy to ignore unreliable pixel level pseudolabels and unsupervised losses in Eq(5). We use contrastiveloss to fully utilize the excluded unreliable pixels inEq(6).To achieve better segmentation performance, weminimize overall loss to the greatest extent possible, it canbe formalized as:",
  "The VSPW Dataset annotates 124 categories of real-world scenarios, which contains 3,536 videos, with 251,633": "frames totally. Among these videos, there are 2806 videosin the training set, 343 videos in the validation set, and387 videos in the testing set. In order to enrich our train-ing samples, both the training and validation set are usedfor training.Due to the large number of parameters inthe transformer model, increasing the number of trainingsamples is beneficial for improving the performance of themodel. We introduce additional data to train our model,such as the ADE20k and COCO datasets. Duringthe training phase, the backbone of our model is pretrainedon the ImageNet22K dataset. The COCO dataset isused to train the entire model during the pre-training phase.The COCO and ADE20k dataset labels are mapped to theVSPW dataset through label remapping, and categories thatdo not exist in the VSPW dataset are marked as 255.",
  ". Training Configuration": "All models in our experiments are implemented in thePyTorch framework. For data augmentation, we per-form random resizing within ratio range [0.5, 2.0], randomcropping, random horizontal flipping, and color jitter on im-ages. An AdamW optimizer is applied with the initial learn-ing rate of 1e-5, beta = (0.9, 0.999), and the weight decay of0.05. A linear warmup is used. To prevent overfitting, thetraining iterations is set to 80k.",
  ". Ablation Studies": "With the rapid development of transformer technology,models based on transformer technology have shown strongfeature expression ability in the field of dense object detec-tion and segmentation, and even maintain good robustnessin some complex scenes. Therefore, we explore the appli-cation of models based on transformer in video semanticsegmentation tasks. The experimental results of differentbackbones and methods are shown in . From thetable, it can be seen that selecting One-peace as the back-bone has significantly better performance than Swin-L ,BEiT-L and ViT-Adapter-L . In subsequent exper-iments, we continue to explore the effects of network inputresolution, multi-scale and flip enhancement testing, semi-supervised training, and model ensemble on segmentationperformance.",
  ". Semi-supervised training": "Semi-supervised learning aims to extract effective in-formation from unlabeled data, thereby improving the per-formance of the model. Taking inspiration from this, wechose one-place as the teacher network and ViT-Adapter asthe student network. Firstly, we train teacher and studentnetworks on labeled datasets, and generate pseudo labelsthrough multi-scale and flip enhanced testing and model fu-sion. We combine unlabeled and labeled datasets into a newtraining set to continue fine-tuning the student network. We",
  ". Comparison with other teams on the PVUW2024 chal-lenge final test set": "believe that in semi-supervised model training, each pixel ofthe pseudo label is important, even if its prediction is am-biguous. Intuitively, unreliable predictions may be directlyconfused in the category with the highest probability, butthey should have credibility for pixels that do not belong toother categories. Therefore, such pixels can be judged asnegative samples in the least likely category. From ,",
  ". Inference Augmentation": "In the inference stage, we continue to explore the fac-tors that affect model performance.We achieve higherscores on the mIOU metric by using multi-scale and hor-izontal flipping for each scale where the selected scalesare [512./896., 640./896., 768./896., 896./896., 1024./896.,1152./896., 1280./896., 1408./896.]. From Tables 2 and 3, itcan be seen that The multi-scale and horizontal flipping re-sults increase the mIoU indicator by 0.4 percentage pointscompared to the single scale results. In order to furtherimprove the performance of the model, we ensemble theteacher model and student model with a crop size of 896and achieve the highest score on the mIOU of PVUW testpart 1. We achieve state-of-the-art results on the final test setof the PVUW semantic segmentation challenge by combin-ing multi-scale and horizontal flipping enhancement testing, semi-supervised training learning, and multi model ensem-ble techniques. Finally, we obtain the 1st place in the finaltest set, as shown in . Qualitative result on VSPWtest set of our method is shown in .",
  ". Conclusion": "In this paper, we start by selecting a strong baselinemodel that is well-suited for the task of multi-class seman-tic segmentation. We adopt a semi-supervised video seman-tic segmentation method based on unreliable pseudo labels.Our method effectively utilizes unlabeled samples as a sup-plement to labeled samples to improve model performance.We propose a ensemble method to get more accurate proba-bility by fusing the results of different models. These tech-niques are combined to create a comprehensive solution thatachieve 1st place in the VSS track of PVUW challenge atCVPR 2024 conference. The results demonstrate the effec-tiveness and versatility of our solution for addressing multi-task semantic segmentation problems. Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guan-grui Li, and Yi Yang. Vspw: A large-scale dataset forvideo scene parsing in the wild. In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, pages 41334143, 2021. 1, 4",
  "Q. Ning M. Yan and Q. Wang.Semantic seg-mentation on vspw dataset through contrastive lossand multi-dataset training approach. arXiv e-prints,arXiv:2306.03508, 2023. 1": "Yuchao Wang, Haochen Wang, Yujun Shen, JingjingFei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, andXinyi Le. Semi-supervised semantic segmentation us-ing unreliable pseudo labels. In Proceedings of theIEEE/CVF International Conference on Computer Vi-sion and Pattern Recognition (CVPR), 2022. 1 Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai,Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, andChang Zhou. One-peace: Exploring one general rep-resentation model toward unlimited modalities. arXivpreprint arXiv:2305.11172, 2023. 1 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler,Adela Barriuso, and Antonio Torralba. Scene parsingthrough ade20k dataset. In Proceedings of the IEEEconference on computer vision and pattern recogni-tion, pages 633641, 2017. 2, 3",
  "Yazhe Li Aaron van den Oord and Oriol Vinyals. Rep-resentation learning with contrastive predictive cod-ing. arXiv preprint arXiv:1807.03748, 2018. 3. 3": "Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollar,and C Lawrence Zitnick.Microsoft coco:Com-mon objects in context. In Computer VisionECCV2014: 13th European Conference, Zurich, Switzer-land, September 6-12, 2014, Proceedings, Part V 13,pages 740755. Springer, 2014. 3 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, KaiLi, and Li Fei-Fei. Imagenet: A large-scale hierarchi-cal image database. In 2009 IEEE conference on com-puter vision and pattern recognition, pages 248255.Ieee, 2009. 3"
}