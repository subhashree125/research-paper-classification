{
  "Abstract": "Scene Graph Generation (SGG) aims to identify enti-ties and predict the relationship triplets <subject, predi-cate, object> in visual scenes.Given the prevalence oflarge visual variations of subject-object pairs even in thesame predicate, it can be quite challenging to model andrefine predicate representations directly across such pairs,which is however a common strategy adopted by most exist-ing SGG methods. We observe that visual variations withinthe identical triplet are relatively small and certain rela-tion cues are shared in the same type of triplet, which canpotentially facilitate the relation learning in SGG. More-over, for the long-tail problem widely studied in SGG task,it is also crucial to deal with the limited types and quan-tity of triplets in tail predicates. Accordingly, in this paper,we propose a Dual-granularity Relation Modeling (DRM)network to leverage fine-grained triplet cues besides thecoarse-grained predicate ones. DRM utilizes contexts andsemantics of predicate and triplet with Dual-granularityConstraints, generating compact and balanced representa-tions from two perspectives to facilitate relation recogni-tion. Furthermore, a Dual-granularity Knowledge Trans-fer (DKT) strategy is introduced to transfer variation fromhead predicates/triplets to tail ones, aiming to enrich thepattern diversity of tail classes to alleviate the long-tailproblem. Extensive experiments demonstrate the effective-ness of our method, which establishes new state-of-the-artperformance on Visual Genome, Open Image, and GQAdatasets. Our code is available at",
  "*Corresponding author": "<elephant, eating, leaf> <man, eating, pizza><bird, eating, fruit> <elephant, eating, leaf> . The illustration of large visual variations within the pred-icate eating. Identical predicate can appear differently under dis-tinct subject-object pairs, encompassing a different set of visualcues within each manifestation. Identifying discriminative rela-tion cues that are shared across diverse subject-object pairs withinthe same predicate can be challenging. Yet, they can be easily cap-tured when the scope is narrowed to the identical triplet. their pairwise relationships, encapsulating them into <sub-ject, predicate, object> triplets . The generatedcompact graph-structured image representation can be uti-lized in a range of applications, e.g. embodied navigation, image retrieval , visual question answering, etc., so the SGG task has received widespread at-tention in recent years .Existing SGG methods are mostly dedicated to generat-ing discriminative predicate representations for the detectedentities, based on their appearances, relative positions, con-textual cues, etc. . However, as shownin , large visual variations due to different subject-object combinations are inherent even in the same predicate,presenting an obstacle for SGG methods to capture robustpredicate cues across distinct triplet types. To alleviate thisproblem, PE-Net utilizes textual semantics of predi-cate categories as the prototype and model predicate cuesby reducing the intra-class variance and inter-class similar-ity. Despite the improved prediction accuracy, PE-Net stillfollows the previous strategy of straightforwardly amalga-",
  "arXiv:2406.02038v1 [cs.CV] 4 Jun 2024": "mating predicate cues that probably contain extensive visualvariations of diverse triplets.Moreover, many recent efforts have been devoted to thelong-tail problem in SGG task. Insufficient samples withlimited triplet types lead to the reduced diversity observedin tail predicate categories, making it a challenging taskto learn and adapt to their distributions.Existing meth-ods boost models attention towards tail predicate categoriesthrough re-sampling , re-weighting, or the utiliza-tion of mixture of experts . However, they mostly fallshort of directly tackling the core of the long-tail issue, i.e.,the insufficient patterns for tail predicates, leaving space forfurther improvement.Reflecting on similarities and differences among predi-cates, we find that despite the non-negligible or even largevariations inherent in the same predicate, the visual diver-sity of the same triplet is relatively small (e.g. the two in-stances of <elephant, eating, leaf> in ). Accord-ingly, considering the more fine-grained triplet cues in ad-dition to the coarse-grained predicate ones can help preventthe model from getting stuck in the refinement process ofpredicate features with potentially large variations, and pro-mote it to strike a balance between different granularitiesduring the relationship learning process. For the long-tailproblem in SGG task which primarily emerges due to theinsufficient tail predicate patterns and corresponding lim-ited types and quantity of triplets, it then becomes a naturalchoice to enrich tail predicate patterns using head predicatesand their triplets.Based on the aforementioned insights, we propose aDual-granularity Relation Modeling (DRM) network thatmodels triplet cues to facilitate predicate learning and trans-fer knowledge from the head classes to tail classes for rela-tion recognition. In our DRM network, as shown in Fig-ure 2, 1) besides a predicate branch that models coarse-grained predicate cues by leveraging their contextual pred-icates and subject-object pairs, a triplet branch is also pre-sented to strengthen fine-grained triplet representations viajointly exploring their visual contents and corresponding la-bel semantics. We also devise dual-granularity constraintsto prevent the degradation of predicate and triplet featurespaces during model training. Subsequently, 2) the Dual-granularity Knowledge Transfer (DKT) strategy is proposedto transfer the class variance from head classes to tail onesfrom both the predicate and triplet perspectives for unbiasedSGG. Distributions of tail predicates/triplets are designed tobe calibrated using variance from head ones that are mostsimilar to them. New predicate/triplet samples are gener-ated as well based on the calibrated distributions to enrichthe pattern diversity of tail predicates/triplets. Extensive ex-periments conducted on the widely used Visual Genome, Open Image , and GQA datasets for SGGdemonstrate the state-of-the-art performance of our method.",
  ". Scene Graph Generation": "Scene graph is a structured representation of the image con-tent, where its essential constituents are the relationships (ortriplets <subject, predicate, object>). The direct predictionof triplet categories presents a significant challenge giventhe extensive combinations of subjects, predicates, and ob-jects.Existing Scene Graph Generation (SGG) methods decompose this prediction targetinto two components: entities and predicates. Early SGGapproaches explore to integrate multiple modal-ities like positional information and linguistic features intorelationships. Later methodologies iden-tify the value of visual context in SGG. Some of them en-code contextual information utilizing techniques e.g. mes-sage passing , LSTM , graph neural net-works , and self-attention modules . Oth-ers refine the detected scene graph and optimize thefeatures of refined predicates based on high-confidence pre-dictions. PE-Net proposes to utilize text embeddingsas the centroid of predicates, aiming to minimize the intra-class variance and the inter-class similarity. Despite the im-proved prediction accuracy, it is still hard to extract discrim-inative relation cues across the various subject-object pairsin the same predicate. The core of this issue lies in the factthat the identical predicates can manifest differently underdistinct subject-object pairs, encompassing a unique array",
  "Concatenation Add Operation": ". Illustration of the proposed Dual-granularity Relation Modeling (DRM) network. The learning procedure of DRM is composedof two stages. In the first stage, we capture the coarse-grained predicate cues shared across different subject-object pairs and learn thefine-grained triplet cues under specific subject-object pairs. In the second stage, the Dual-granularity Knowledge Transfer (DKT) strategytransfers the variation from head predicates with their associate triplets to the tail. Then DRM exploits the real instances along withsynthetic samples from the calibrated tail distribution to fine-tune the relation classifier, which alleviates the long-tail problem in SGG. of visual cues within each manifestation. The direct aggre-gation of predicate features tends to overlook the inherenttriplet cues present under the various subject-object pairs.We propose to explicitly model both coarse-grained predi-cate cues and fine-grained triplet cues for biased and unbi-ased SGG.",
  ". Unbiased Scene Graph Generation": "Real-world data tends to obey a long-tailed distribution, andimbalanced samples in scene graph present the challengeto learn and adapt to the distribution in the tail predicates. Recently, many methods have been pro-posed to deal with the biased prediction problem in scenegraphs, which can be roughly divided into three categories.The first category of methods use re-balancing strategiesthat alleviate the long-tail problem by re-sampling imagesand triplet samples to enhance the performance of tail predi-cates or by enhancing the loss weights of tail predicatesand easily misclassified categories through the pre-definedPredicate Lattice .The second category of methodsutilize noisy label learning to explicitly re-label relationaltriplets missed by annotators and correct the mislabeledpredicates . The last category exploit the mixtureof experts to let different experts separately deal with a sub-part of predicates and merge their outputs . Differentfrom these approaches, we mitigate the long-tail problem in SGG by exploiting knowledge from the head predicatesand triplets. Our method involves transferring the abun-dant variations from the head predicates and their associatetriplets to the tail, thus enriching patterns of the tail predi-cates.",
  ". Method": "An overview of our Dual-granularity Relation Modeling(DRM) network is illustrated in . DRM aims to bal-ance and integrate coarse-grained predicate cues and fine-grained triplet cues for relation recognition. To this end, apredicate cue modeling module and a triplet cue modelingmodule are designed to extract predicate cues shared acrossdiverse subject and object pairs, and triplet cues sharedby specific subject-object pairs, respectively.Moreover,the Dual-granularity Knowledge Transfer (DKT) strategyis proposed for unbiased SGG. This strategy transfers theknowledge of both predicates and triplets from head cate-gories to tail ones, with the objective of enriching the pat-tern of tail predicates and their associated triplets.",
  "The backbone of our DRM network is composed of a pro-posal network and an entity encoder. Features generated bythe backbone are further fed into subsequent predicate andtriplet cue modeling modules": "Proposal Network. Given an image I, the proposal net-work generates N entities along with their correspondingvisual features, label predictions, and spatial features fromtheir bounding boxes. A pre-trained Faster RCNN isadopted as the proposal network in this paper. And follow-ing previous works , we initialize the entityrepresentations {vi}Ni=1 with their visual and spatial fea-tures, encode the union feature ui,j between the i-th entityand the j-th entity with their relative spatial representationand the ROI feature of their union box, and obtain the se-mantic features {si}Ni=1 of entities using word embeddingsof their class labels.Entity Encoder. The entity encoder is designed to refinefeatures of entities with their contexts for further predic-tions. Inspired by Dong et al. , we utilize the Hybrid At-tention (HA) to incorporate semantic cues {si}Ni=1 into en-tities {vi}Ni=1 while modeling the scene context. Each layerof Hybrid Attention is composed of two Self-Attention (SA)units and two Cross-Attention (CA) units, which is builtupon the multi-head attention module . The Hybrid At-tention at the l-th layer can be formulated as:",
  ". Predicate and Triplet Cue Modeling": "Previous approaches tend to categorize predicates in acoarse manner, most of which only focus on predicate cuesshared among various subject-object pairs, and thus cannoteffectively deal with the potentially large visual variationsinherent in the identical predicate. In contrast to these ap-proaches, our method considers both coarse-grained predi-cate cues and fine-grained triplet cues, leveraging and strik-ing a balance between the dual-grained cues for accuratepredicate categorization.Our DRM network involves aPredicate Cue Modeling module and a Triplet Cue Model-ing module for extracting features at these two granularitiesrespectively. Additionally, we introduce Dual-GranularityConstraints to decrease the intra-class variance and increaseinter-class distinguishability, explicitly enforcing the predi-cate and triplet branches to concentrate on cues from their corresponding granularities and preventing the degradationof dual-granularity space.Predicate Cue Modeling.Our predicate cue model-ing module Encprd, comprising a 2-layer Hybrid Atten-tion, aims to capture predicate cues across different subject-object pairs. In each HA layer of Encprd, the two Self-Attention units are designed to model predicate and entitycontextual information. And the two Cross-Attention unitsare designed to capture the dependency between entities andpredicates, where the predicate pi,j solely queries the con-texts of its corresponding subject ei and object ej, and theentity ei only queries predicates related to it. Our PredicateCue Modeling process can thus be formulated as:",
  "{pi,j}Mi=j = Encprd({pi,j}Mi=j, {ei}Ni=1),(3)": "where the predicate representation pi,j is initialized withthe union feature ui,j, the entity representation ei is ob-tained with the concatenation of vi and vi, M = N (N 1) is the number of subject-object pairs, and pi,j is the cor-responding output of pi,j at the last HA layer.Triplet Cue Modeling. Our triplet cue modeling mod-ule Enctpt is also constructed using a 2-layer Hybrid Atten-tion. This module is responsible for obtaining fine-grainedtriplet cues that are shared by specific subject-object pairs.The two Self-Attention units in each HA layer of Enctptare designed to model visual and semantic contextual infor-mation of triplets, respectively, and the two Cross-Attentionunits aim at fusing semantic cues into the visual informationof triplets. We initialize the triplet representation ti,j withthe concatenation of the subject representation ei, predi-cate representation pi,j, and object representation ej. OurTriplet Cue Modeling process is formulated as:",
  "{ti,j}Mi=j = Enctpt({ti,j}Mi=j, {[si, sj]}Mi=j),(4)": "where si is the word embedding of predicted entity label,and denotes the concatenate operation. ti,j is the con-textually and semantically aware triplet feature, and is de-rived from the addition of outputs from the last HA layer.Dual-granularity Constraints. Although we explicitlymodel predicate and triplet cues with Encprd and Enctpt,they may degrade beyond our desires with a single predicatecross-entropy loss. To prevent this degradation, we proposethe dual-granularity constraints to guide the predicate andtriplet cue modeling modules to refine representations intheir desired granularities. Specifically, we generate twoviews of an input relation and impose a predicate category-aware supervised contrastive learning loss on predicate rep-resentations to capture the coarse-grained predicate cues as:",
  "where pi,j is obtained by passing pi,j through a projectionlayer comprising two fully connected layers, p is the tem-": "perature, posp denotes the subscripts of positive samplesbelonging to the same predicate category as pi,j, , is thecosine similarity function, and B(i, j) denotes the subscriptset of samples within the same batch.Similarly, a triplet category-aware supervised contrastivelearning loss is applied on triplet representations, aiming atcapturing the fine-grained triplet cues as:",
  "bB(i,j) exp(ti,j, tb /t),(6)": "where t is the temperature, post denotes subscripts of pos-itive samples from the same triplet category as ti,j, and ti,jis obtained by passing ti,j through a projection layer.Scene Graph Prediction.For each relationship pro-posal, our predicate classifier utilizes two fully connectedlayers to integrate the coarse-grained and fine-grained cues,i.e. pi,j and ti,j, to obtain the final relation label prediction.For each entity, we also introduce a fully connected layerwith softmax function to get its refined label prediction.Training Loss. During the training in the first stage ofDRM, the overall loss function L is defined as:",
  ". Dual-granularity Knowledge Transfer": "The SGG task typically suffers from the long-tail distribu-tion problem. This problem primarily originates from thetail predicate classes, which possesses limited types andquantities of triplets. The head predicate classes containa relatively larger number of samples and an abundance oftriplet types. Thus to alleviate the long-tail problem, wepropose the Dual-granularity Knowledge Transfer (DKT)strategy to transfer knowledge in predicate and triplet fea-ture spaces from head predicate class to the tail one. DKTgenerates samples belonging to the tail predicates with theirassociated predicate and triplet features, thereby enrichingand diversifying the patterns of tail predicates.Specifically, DKT first calculates the distributions ofpredicate and triplet features. We assume that the distribu-tion of each category c follows a multidimensional Gaus-sian distribution. Formally, it can be expressed as {c =N(c, c)|c C}, where c and c denote the mean andcovariance of c, and c denotes the predicate or triplet cate-gory. After the first-stage pre-training of DRM network, wefreeze the proposal network, entity encoder, and the pred-icate and triplet cue modeling modules. Subsequently, thecompact predicate and triplet features, i.e. p and t, are ex-tracted to calculate the predicate and triplet feature distribu-tions, respectively. The mean, denoted as c, is calculated",
  "calculated as c =1": "Nc1Nck (xck c)(xck c)T . Here,xci denotes the feature of category c, and Nc is the numberof xck. The feature x can be either p or t.We then transfer knowledge of feature distributions ofhead predicate classes to the tail ones. Specifically, we ar-range the predicate classes in descending order based ontheir sample numbers, choosing half of the predicate classesas head predicates and the remaining ones as tail predicates.We further select triplets that appear more than certain timesin the head predicates to be the head triplets (we use 64times as the threshold in this paper), and those in the tailpredicates to be the tail triplets. For each tail category i C,we compute the euclidean distance di,j between its centeri and the center j of head category j C. The closer thecenters of two categories are to each other in either predicateor triplet space, the more similar they are, which also in-creases the likelihood of knowledge sharing between them.Based on di,j, we achieve the knowledge transfer as:",
  "ji,jj,(8)": "where i,j denotes the softmax normalized form of di,j, Qidenotes the desired number of predicate/triplet instances oftail class i and it is identical for every predicates in the tail.It suggests that the tail category with fewer samples requiresmore knowledge from the head for calibration.After dual-granularity knowledge transfer, we gener-ate synthetic samples in tail predicate using correspondingpredicate and triplet features from calibrated distributions:",
  ". Experimental Settings": "Datasets.We evaluate our method on three commonlyused SGG datasets, namely Visual Genome , Open Im-age , and GQA . For the Visual Genome dataset,we adopt the VG150 split following previous approaches, which contains the most frequent 150object categories and 50 predicate categories. We use 70%of images for training, 30% images for testing and 5k im-ages from the training set for validation. As for Open Im-age, we apply the Open Image V6 protocol, which has 301object categories and 31 predicate categories. It contains",
  "R@50/100mR@50/100R@50/100mR@50/100R@50/100mR@50/100": "VTransE CVPR1755.7 / 57.914.0 / 15.033.4 / 34.28.1 / 8.727.2 / 30.75.8 / 6.6MOTIFS CVPR1865.3 / 66.816.4 / 17.134.2 / 34.98.2 / 8.628.9 / 33.16.4 / 7.7VCTREE CVPR1963.8 / 65.716.6 / 17.434.1 / 34.87.9 / 8.328.3 / 31.96.5 / 7.4SHA CVPR2263.3 / 65.219.5 / 21.132.7 / 33.68.5 / 9.025.5 / 29.16.6 / 7.8VETO ICCV2364.5 / 66.021.2 / 22.130.4 / 31.58.6 / 9.126.1 / 29.07.0 / 8.1 VTransE+GCL CVPR2235.5 / 37.430.4 / 32.322.9 / 23.616.6 / 17.415.3 / 18.014.7 / 16.4MOTIFS+GCL CVPR2244.5 / 46.236.7 / 38.123.2 / 24.017.3 / 18.118.5 / 21.816.8 / 18.8VCTREE+GCL CVPR2244.8 / 46.635.4 / 36.723.7 / 24.517.3 / 18.017.6 / 20.715.6 / 17.8SHA+GCL CVPR2242.7 / 44.541.0 / 42.721.4 / 22.220.6 / 21.314.8 / 17.917.8 / 20.1",
  ". Comparison results with state-of-the-art SGG methodson Open Image V6. The best and second best results under eachmetric are respectively marked in red and underline blue": "126,368, 1,183, and 5,322 images for training, validation,and testing, respectively. For the GQA dataset, we followprevious works and utilize the GQA200 split, whichincludes 200 object categories and 100 predicate categories.Tasks.We adopt three SGG tasks for evaluation: 1)Predicate Classification (Predcls) infers the predicates ofentity pairs with ground-truth bounding boxes and cate- gories. 2) Scene Graph Classification (SGCls) aims to pre-dict the triplet categories with ground-truth bounding boxes.3) Scene Graph Detection (SGDet) detects bounding boxesof entity pairs and infers their predicate categories. Evaluation Metrics.We use Recall@K (R@K) andmean Recall@K (mR@K) as evaluation metrics on VG150and GQA200 datasets, following recent works .R@K tends to prioritize frequent predicates, while mR@Kexhibits a preference for less frequent predicates.Re-sults on Open Image dataset are evaluated using Recall@50(R@50), weighted mean AP of relations (wmAPrel),weighted mean AP of phrase (wmAPphr), and a weightedscore of them scorewtd = 0.2R@50+0.4wmAPrel +0.4 wmAPphr, following previous works . Implementation Details.Following previous works, we adopt the pre-trained Faster RCNN withResNeXt-101-RPN in the proposal network to detect en-tities in the image. GloVe is applied to embed the se-mantic features. We set the loss weight parameters r, e,t, and p as 3, 0.5, 0.1, and 0.1, respectively. Tempera-tures p and t are set as 0.2, and 0.1 considering that thepredicate feature space exhibits greater variety than its asso-",
  ". Comparison with State-of-the-art Methods": "To evaluate the performance our model, we compare it withseveral state-of-the-art SGG approaches on Visual Genome,Open Image, and GQA datasets. The comparison methodsinclude IMP , MOTIFS , VCTREE , RU-Net, HL-Net , PE-Net , and VETO , whichfocus on the prediction of every relationship in an image.We also compare with methods for unbiased scene graphgeneration, including TDE , CogTree , NICE ,INF , CFA , EICR , BGNN , SHA+GCL, SQUAT , and CaCao. In addition, we compare our method with VETO+MEET in the setting withoutgraph constraint in the Supplementary Material.Visual Genome. shows the comparison re-sults of different approaches on VG150. From these re-sults, we have the following observations:1) our pro-posed method significantly outperforms all baselines on allthree tasks. More specifically, our DRM w/o DKT outper-forms the recent PE-Net by 2.0%, 2.9%, and 2.0% atR@100 on PredCls, SGCls, and SGDet, respectively. Un-like approaches such as MOTIFS and VCTREE ,which only utilize a predicate classifier to predict predicatesand overlook the triplet cues, our method leverages coarse-grained predicate cues and fine-grained triplet cues for re-lation recognition and thus achieves superior results.2)Our DRM method also has considerably better performancecompared to the baseline unbiased SGG methods. Notably,based on the proposed DKT strategy in DRM, our methodoutperforms the recent multi-expert method SHA+GCL by 5.5%, 4.9%, and 3.2% at mR@100 on three tasks. Thisdemonstrates that transferring knowledge from head pred-icates to tail predicates at dual granularities and enrichingthe patterns in tail predicates can effectively mitigate thelong-tail problem.Open Image. Compared to the VG dataset, Open Imageprovides a relatively complete labeling of relationships inthe images. Consequently, the models capability to gener-ate scene graphs can be evaluated at a fine-grained level uti-lizing the AP metric. To evaluate the generalizability of ourmethod across various datasets, we conduct experiments onOpen Image V6. Since the metrics in Open Image V6 tendto prioritize frequent predicates, we just compare our DRMw/o DKT with state-of-the-art approaches. The compari-son results shown in indicate that our DRM w/oDKT significantly outperforms the recent approaches, i.e.,SQUAT and PE-Net .GQA. Compared to Visual Genome and Open Imagedatasets, GQA200 contains a broader range of predicates.So we further confirm the generality of our model on the",
  "(e) DRM, Predicate": ". The comparison of t-SNE visualization results on predicate and triplet feature distributions within the VG dataset. MOTIFS,Triplet and DRM w/o DKT, Triplet visualize the same set of samples, where each unique color represents a different type of triplet. GQA200 dataset. As shown in , our method sig-nificantly outperforms the recent VETO at R@100 onthree tasks. When equipped with DKT, our DRM outper-forms SHA+GCL by an average of 0.4% at mR@100across three tasks. These results demonstrate the consistenteffectiveness of our method in handling relation recognitionunder different data distributions.",
  ". Ablation Study": "To verify the contributions of different components in ourDRM network, we conduct the following ablation studies.Predicate and Triplet Cue Learning. We first performan ablation study on the leverage of predicate and tripletcues.As shown in , we incrementally incorpo-rate one component into the baseline to verify their effec-tiveness.Compared with the application of predicate ortriplet cue modeling in isolation, leveraging both predicateand triplet together improves the performance. Our dual-granularity constraints include a constraint loss componentand a two-view augmentation component to generate pos-itive pairs in each training batch. The constraints compelpredicate and triplet cue modeling to concentrate on cor-responding granularities. We observe from the results thatboth the loss component and augmentation component con-tribute to the performance improvements.Dual-Granularity Knowledge Transfer. As shown in, we conduct an ablation study for the DKT. We ob-serve an obvious performance gain on mR@K when trans-ferring knowledge at either predicate or triplet granularity.The mR@K is further improved when predicate and tripletgranularities are simultaneously employed. In comparisonto the scenario without the application of DKT, we observea performance decrease in R@K when using it. This is at-tributed to the capability of our model to reasonably classifyambiguous head predicates, e.g. on, into more specific tailpredicates, e.g. sitting on. Consequently, decreases at theR@K for these head predicates are inevitable .",
  "To illustrate the ability our method to learn triplet cues andimpact of the DKT strategy, we visualize the predicate and": "triplet feature distributions using t-SNE. The visualizationresults are shown in . Comparing a withb, we observe that our DRM generates compactand distinguishable triplet representations, while MOTIFS appears to overlook the triplet cues, leading to a chal-lenge in distinguishing various triplet types. By comparingb with c, it can be observed that DKT cangenerate synthetic samples with diverse distributions for thetail triplets. Figures 4d and 4e demonstrate that DKT alsotransfers the knowledge of head predicates to tail predicates,increasing the tail predicate patterns. The visualization ofpredicate and triplet feature distributions demonstrates theinterpretability of our method in leveraging predicate andtriplet cues and transferring the dual-granularity knowledge.",
  ". Conclusion": "In this paper, we propose a Dual-granularity Relation Mod-eling (DRM) network to address two issues in SGG, i.e. thediverse visual appearance within the same predicate and thelack of patterns in tail predicates. Our DRM network cap-tures the coarse-grained predicate cues shared across differ-ent subject-object pairs and fine-grained triplet cues underspecific subject-object pairs for relationship recognition.The Dual-granularity Knowledge Transfer (DKT) is furtherproposed to transfer the variation from head predicates tothe tail to enrich the tail predicate patterns. Quantitative andqualitative experiments demonstrate that our method estab-lishes new state-of-the art performances on Visual Genome,Open Image and GQA datasets.",
  "Acknowledgement": "This work was supported in part by the National KeyR&D Program of China under Grant 2022ZD0161901,theNationalNaturalScienceFoundationofChinaunderGrants62276018andU20B2069,theBei-jingNovaProgramunderGrant20230484297,theFundamental Research Funds for the Central Univer-sities,and Research Program of State Key Labora-toryofComplex&CriticalSoftwareEnvironment. Hedi Ben-Younes, Remi Cadene, Nicolas Thome, andMatthieu Cord.Block: Bilinear superdiagonal fusion forvisual question answering and visual relationship detection.In Proceedings of the AAAI Conference on Artificial Intelli-gence, pages 81028109, 2019. 1",
  "Bashirul Azam Biswas and Qiang Ji. Probabilistic debias-ing of scene graphs. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1042910438, 2023. 6, 7, 2": "Jun Chen, Aniket Agarwal, Sherif Abdelkarim, Deyao Zhu,and Mohamed Elhoseiny. Reltransformer: A transformer-based long-tail visual relationship recognition. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1950719517, 2022. 2 Bo Dai, Yuqi Zhang, and Dahua Lin. Detecting visual re-lationships with deep relational networks. In Proceedingsof the IEEE Conference on Computer Vision and PatternRecognition, pages 30763086, 2017. 1 Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab,Gregory D Hager, Federico Tombari, and Christian Rup-precht. Semantic image manipulation using scene graphs.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 52135222, 2020. 1 Xingning Dong, Tian Gan, Xuemeng Song, Jianlong Wu,Yuan Cheng, and Liqiang Nie. Stacked hybrid-attention andgroup collaborative learning for unbiased scene graph gener-ation. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1942719436,2022. 2, 3, 4, 6, 7, 8",
  "Yutian Guo, Jingjing Chen, Hao Zhang, and Yu-Gang Jiang.Visual relations augmented cross-modal retrieval. In Pro-ceedings of the International Conference on Multimedia Re-trieval, pages 915, 2020. 1": "Yuyu Guo, Lianli Gao, Xuanhan Wang, Yuxuan Hu, XingXu, Xu Lu, Heng Tao Shen, and Jingkuan Song. From gen-eral to specific: Informative scene graph generation via bal-ance adjustment. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1638316392,2021. 6, 2 Drew A Hudson and Christopher D Manning. Gqa: A newdataset for real-world visual reasoning and compositionalquestion answering. In Proceedings of the IEEE/CVF con-ference on Computer Vision and Pattern Recognition, pages67006709, 2019. 2, 5, 1 Tianlei Jin, Fangtai Guo, Qiwei Meng, Shiqiang Zhu, Xiang-ming Xi, Wen Wang, Zonghao Mu, and Wei Song. Fast con-textual scene graph generation with unbiased context aug-mentation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 63026311, 2023. 2",
  "graph generation.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1866418674, 2023. 1, 6, 7, 2": "Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, SamiAbu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Ui-jlings, Stefan Popov, Andreas Veit, et al. Openimages: Apublic dataset for large-scale multi-label and multi-class im-age classification.Dataset available from 2(3):18, 2017. 2, 5, 1 Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-tidis, Li-Jia Li, David A Shamma, et al.Visual genome:Connecting language and vision using crowdsourced denseimage annotations. International Journal of Computer Vi-sion, 123:3273, 2017. 1, 2, 5 Sanjoy Kundu and Sathyanarayanan N Aakur. Is-ggt: Itera-tive scene graph generation with generative transformers. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 62926301, 2023. 1",
  "Jiankai Li, Yunhong Wang, and Weixin Li. Zero-shot scenegraph generation via triplet calibration and reduction. ACMTransactions on Multimedia Computing, Communicationsand Applications, 2023. 1, 2": "Lin Li, Long Chen, Yifeng Huang, Zhimeng Zhang,Songyang Zhang, and Jun Xiao. The devil is in the labels:Noisy label correction for robust scene graph generation. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1886918878, 2022. 3,6, 7, 8, 2 Lin Li, Guikun Chen, Jun Xiao, Yi Yang, Chunping Wang,and Long Chen.Compositional feature augmentation forunbiased scene graph generation.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2168521695, 2023. 6, 7, 2 Rongjie Li, Songyang Zhang, Bo Wan, and Xuming He.Bipartite graph network with adaptive message passing forunbiased scene graph generation.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1110911119, 2021. 1, 2, 3, 4, 6, 7 Rongjie Li, Songyang Zhang, and Xuming He. Sgtr: End-to-end scene graph generation with transformer. In proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1948619496, 2022. 2 Wei Li, Haiwei Zhang, Qijie Bai, Guoqing Zhao, Ning Jiang,and Xiaojie Yuan. Ppdl: Predicate probability distributionbased loss for unbiased scene graph generation. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1944719456, 2022. 6, 2 Xin Lin, Changxing Ding, Jinquan Zeng, and Dacheng Tao.Gps-net: Graph property sensing network for scene graphgeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 37463753, 2020. 1, 6, 2 Xin Lin, Changxing Ding, Yibing Zhan, Zijian Li, andDacheng Tao.Hl-net: Heterophily learning network forscene graph generation.In proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1947619485, 2022. 5, 6, 7, 2 Xin Lin, Changxing Ding, Jing Zhang, Yibing Zhan, andDacheng Tao. Ru-net: Regularized unrolling network forscene graph generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1945719466, 2022. 6, 7, 2 Hengyue Liu, Ning Yan, Masood Mortazavi, and Bir Bhanu.Fully convolutional scene graph generation. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1154611556, 2021. 2",
  "Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Visual relationship detection with language priors. InEuropean Conference on Computer Vision, pages 852869.Springer, 2016. 1, 2": "Yichao Lu, Himanshu Rai, Jason Chang, Boris Knyazev,Guangwei Yu, Shashank Shekhar, Graham W Taylor, andMaksims Volkovs.Context-aware scene graph generationwith seq2seq transformers. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1593115941, 2021. 2 Xinyu Lyu, Lianli Gao, Yuyu Guo, Zhou Zhao, Hao Huang,Heng Tao Shen, and Jingkuan Song.Fine-grained predi-cates learning for scene graph generation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1946719475, 2022. 2, 3 Yukuan Min, Aming Wu, and Cheng Deng. Environment-invariant curriculum relation learning for fine-grained scenegraph generation. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1329613307,2023. 5, 6, 7, 2 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-perative style, high-performance deep learning library. Ad-vances in Neural Information Processing Systems, 32, 2019.1 Jeffrey Pennington, Richard Socher, and Christopher D Man-ning. Glove: Global vectors for word representation. In Pro-ceedings of the Conference on Empirical Methods in NaturalLanguage Processing, pages 15321543, 2014. 6",
  "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-cnn: Towards real-time object detection with regionproposal networks.Advances in Neural Information Pro-cessing Systems, 28, 2015. 4": "Jiaxin Shi, Hanwang Zhang, and Juanzi Li. Explainable andexplicit visual reasoning over scene graphs. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 83768384, 2019. 1 Kunal Pratap Singh, Jordi Salvador, Luca Weihs, andAniruddha Kembhavi. Scene graph contrastive learning forembodied navigation. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1088410894, 2023. 1 Gopika Sudhakaran, Devendra Singh Dhami, Kristian Ker-sting, and Stefan Roth.Vision relation transformer forunbiased scene graph generation.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2188221893, 2023. 2, 3, 6, 7, 8, 1 Mohammed Suhail, Abhay Mittal, Behjat Siddiquie, ChrisBroaddus, Jayan Eledath, Gerard Medioni, and Leonid Si-gal. Energy-based learning for scene graph generation. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1393613945, 2021. 2",
  "Shuzhou Sun, Shuaifeng Zhi, Qing Liao, Janne Heikkila,and Li Liu. Unbiased scene graph generation via two-stagecausal modeling. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2023. 2": "Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo,and Wei Liu. Learning to compose dynamic tree structuresfor visual contexts. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages66196628, 2019. 1, 2, 3, 4, 5, 6, 7 Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, andHanwang Zhang. Unbiased scene graph generation from bi-ased training. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 37163725, 2020. 3, 6, 7, 2 Hongshuo Tian, Ning Xu, An-An Liu, Chenggang Yan,Zhendong Mao, Quan Zhang, and Yongdong Zhang. Maskand predict: Multi-step reasoning for scene graph genera-tion. In Proceedings of the ACM International Conferenceon Multimedia, pages 41284136, 2021. 2 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in NeuralInformation Processing Systems, 30, 2017. 4 Wenbin Wang, Ruiping Wang, Shiguang Shan, and XilinChen. Sketching image gist: Human-mimetic hierarchicalscene graph generation. In European Conference on Com-puter Vision, pages 222239. Springer, 2020. 2",
  "Sanghyun Woo, Dahun Kim, Donghyeon Cho, and In SoKweon. Linknet: Relational embedding for scene graph. Ad-vances in Neural Information Processing Systems, 31, 2018.2": "Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.Scene graph generation by iterative message passing. In Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition, pages 54105419, 2017. 2, 5, 6, 7 Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, LijuanWang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 30603069, 2021. 1",
  "Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,Wayne Zhang, and Ziwei Liu. Panoptic scene graph gen-eration. In European Conference on Computer Vision, pages178196. Springer, 2022. 1": "Yuan Yao, Ao Zhang, Xu Han, Mengdi Li, Cornelius Weber,Zhiyuan Liu, Stefan Wermter, and Maosong Sun. Visual dis-tant supervision for scene graph generation. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1581615826, 2021. 6 Jing Yu, Yuan Chai, Yujing Wang, Yue Hu, and Qi Wu.Cogtree: Cognition tree loss for unbiased scene graph gener-ation. In International Joint Conference on Artificial Intelli-gence, pages 12741280, 2021. 6, 7, 2 Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, andYueting Zhuang.Visually-prompted language model forfine-grained scene graph generation in an open world. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 2156021571, 2023. 3, 6, 7 Rowan Zellers, Mark Yatskar, Sam Thomson, and YejinChoi. Neural motifs: Scene graph parsing with global con-text. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 58315840, 2018. 2,4, 5, 6, 7, 8, 1 Ao Zhang, Yuan Yao, Qianyu Chen, Wei Ji, Zhiyuan Liu,Maosong Sun, and Tat-Seng Chua. Fine-grained scene graphgeneration with data transfer. In European Conference onComputer Vision, pages 409424. Springer, 2022. 3, 6, 1 Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-Seng Chua. Visual translation embedding network for visualrelation detection. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 55325540, 2017. 1, 2, 6, 7 Ji Zhang, Kevin J Shih, Ahmed Elgammal, Andrew Tao,and Bryan Catanzaro. Graphical contrastive losses for scenegraph parsing. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1153511543, 2019. 2, 6",
  "Supplementary Material": "In the supplementary material, we provide the followingcontents for the proposed Dual-granularity Relation Mod-eling (DRM) network which leverages predicate and tripletlearning for Scene Graph Generation (SGG): (1) more im-plementation details of our method; (2) more comparisonresults, including comparisons on the M@K and F@K met-rics and comparisons with VETO+MEET ; (3) moreablation studies, including the ablation on Dual-granularityKnowledge Transfer (DKT) strategy and dual-granularitylearning; (4) hyper-parameter analysis; and (5) qualitativevisualization.We will make the code publicly availableupon acceptance of this paper.",
  ". Additional Implementation Details": "We implement DRM using Pytorch and the offi-cial code-base Scene-Graph-Benchmark.pytorch1 with aNVIDIA A800 GPU. In the initialization of entity repre-sentations {vi}Ni=1 and union features {si}Ni=1, we adoptthe same strategy as VCTREE and PE-Net , whichinvolves a fusion of their visual and spatial features. Wefollow Xu et al. to augment input images for modeltraining. The expect number Qi of each tail predicate inEquation 8 is equal to the count of the head predicates withthe smallest number.",
  ". Trade-off Results between R@K and mR@K": "Due to the imbalanced data distribution of Visual Genome, Open Image , and GQA datasets , there is atrade-off between Recall R@K and mean Recall mR@Kmetrics. To measure the trade-offs of the scene graph gen-eration methods, Zheng et al. introduce the Mean@K(M@K), which averages the R@K and mR@K, whileZhang et al. propose the F@K, the harmonic mean ofR@K and mR@K. Note that these two metrics only mea-sure the trade-off between R and mR, and it is feasible fordiverse methods, even with significant differences in R andmR, to still arrive at the same trade-off results. Evaluatingeither R@K or mR@K better aligns with the practical needto predict the highest number of relationships or to forecastrelationships as uniformly as possible.Visual Genome. shows the performance of dif-ferent methods in terms of M@50/100 and F@50/100 onVG150.Our DRM outperforms state-of-the-art methodsat F@K measurements and DRM w/o DTK also achieves",
  "state-of-the-art performance at M@K. It indicates that al-though the recall of DRM degrades, the trade-off betweenthe recall and the mean recall is the best in the state-of-the-art methods": "GQA. To evaluate the generalizability of our method acrossvarious datasets, we present the performance of M@50/100and F@50/100 on GQA200. As shown in , DRMoutperforms all of the state-of-the-art methods at bothM@50/100 and F@50/100 metrics. These results demon-strate that our method remains effective in dealing with re-lation recognition, regardless of the variations of data dis-tributions.",
  ". Comparison with VETO+MEET": "The MEET method assigns multiple relationships toeach subject-object pair during inference. This is in ac-cordance with the testing protocol termed without graphconstraint. The setting of without graph constraint, asproposed by Zellers , permits the output scene graph tohave multiple edges between the subject and object. Betterperformance is typically achieved without the graph con-straint since the model is allowed to make multiple guessesfor challenging relations. In the following, ng- denote theNo Graph Constraint variant of the metric. As shown in , we compare our DRM withVETO+MEET under the setting of without graph con-straint on VG150 and GQA200 datasets.We have thefollowing observations: 1) Compared to the performancewith graph constraint, our method consistently exhibits pro-motion without graph constraint. 2) Our proposed DRMw/o DKT has considerably better performance comparedto VETO+MEET. More specifically, our DRM w/o DKToutperforms VETO+MEET by an average of 11.9% and4.9% at ng-R@100 and ng-mR@100, respectively.3)Based on the proposed DKT strategy, DRM significantlyoutperforms VETO+MEET by 21.1%, 16.2%, 16.7% atng-mR@100 on three tasks of VG150 datasets.It alsosurpasses VETO+MEET by 25.2%, 14.4%, 14.8% at ng-mR@100 on three tasks of VG150 datasets. We also present the comparison results at ng-M@50/100and ng-F@50/100 to demonstrate the trade-off performanceunder the setting of without graph constraint. The resultsare shown in . Our DRM consistently and signif-icantly outperforms the recent VETO+MEET in terms ofboth ng-M50/100 and ng-F@50/100 metrics. These resultsindicate the consistent effectiveness of our DRM under thesetting of without graph constraint.",
  "M@50/100F@50/100M@50/100F@50/100M@50/100F@50/100": "VTransE CVPR1734.9 / 36.522.4 / 23.820.8 / 21.513.0 / 13.916.5 / 18.79.6 / 10.9MOTIFS CVPR1840.9 / 42.026.2 / 27.221.2 / 21.813.2 / 13.817.7 / 20.410.5 / 12.5VCTREE CVPR1940.2 / 41.626.3 / 27.521.0 / 21.612.8 / 13.417.4 / 19.710.6 / 12.0SHA CVPR2241.4 / 43.229.8 / 31.920.6 / 21.313.5 / 14.216.1 / 18.510.5 / 12.3VETO ICCV2342.9 / 44.131.9 / 33.119.5 / 20.313.4 / 14.116.6 / 18.611.0 / 12.7",
  ". Results in terms of Recall@100 of all predicate classes of Predicate-Only and DRM w/o DKT on the PredCls task. Predicatesare sorted according to their frequency": "knowledge from head to tail. As shown in , theincorporation of DKT substantially boosts the performanceof PE-NET in three tasks at further demonstrate the effectiveness of our method inmodeling triplet clues, we provide the R@100 performance of our methods predicate-only and DRM w/o DKT oneach predicate.As shown in , our method out-performs the predicate-only baseline on each predicate.We also present the performance of the Recall@100 for thepredicate riding and the predicate eating at the fine- . Results in terms of Recall@100 for triplets belonging to predicate riding of Predicate-Only and DRM w/o DKT on the PredClstask. The terms seen and unseen represent whether the triplets appear in the training set or not, respectively. . Results in terms of Recall@100 for triplets belonging to predicate eating of Predicate-Only and DRM w/o DKT on the PredClstask. The terms seen and unseen represent whether the triplets appear in the training set or not, respectively. grained triplet level. The results are illustrated in Figures 6and 7. We can observe that the performance is significantlyimproved in the seen triplets, especially for <giraffe, eat-ing, leaf> and <bird, eating, fruit>, where our method can accurately predict multiple eating expressions that cannotbe captured using only predicates. This suggests that ourmethod is able to learn the triplet cues in the training, andutilize them to reason about the relationships under specific",
  ". The results on three tasks of PE-Net equipped with our DKT on VG150 dataset": "street car truck sign building car hill mountain leg tree bus number sign windshield poeple sign bus sheep2 sheep1 sheep1sheep2 leghill mountain onhas treeon sheep1sheep2 leghill mountaintree onhas on sheep1sheep2 leghill mountaintree growing on walking inhas building truck car sign street car on on onon building truck car sign street car bus people signnumber windshield bussign with on hason in on bus people signnumber windshield bussign bus people signnumber windshield bussign with on hason on on says on haspainted on in painted on building truck car sign street car on on onon hanging from parked on parked onparked on ImageGround TruthDRM w/o DKTDRM",
  "(c)": ". Scene graphs generated by our DRM w/o DKT and DRM in the PredCls Task. DRM tends to generate more precise fine-grainedrelations than DRM w/o DKT, which leads to the performance degradation on R@K. The use of green and red colors indicates whether theprediction matches the ground truth or not, respectively. nose ear2ear1 bear bed pillow she on 1sheep2 hill ntaintree ons on sheep1sheep2 leghill mountaintree growing on walking inhas pillow ear1 bed bear on ear2 of nose of oflaying on pillow ear1 bed bear ear2 nose pillow ear1 bed bear ear2 nose on ofof ofonabove ofof oflaying on Ground TruthDRM w/o DKTDRMImage . Scene graphs generated by our DRM w/o DKT and DRM in the PredCls Task. Triplet <bear, laying on, bed> only appears oncein the VG150 training set. The use of green and red colors indicates whether the prediction matches the ground truth or not, respectively.",
  ". Hyper-parameters analysis of the loss weights p andt": "the more compact predicate and triplet representations. Weobserve that the model achieves the best performance whenp = 0.2 and t = 0.1, indicating a compact aggregationof the triplet compared to the predicate. This observationaligns with the fact that variations in triplet are significantlysmaller than those in the predicate.During pre-training, the predicate classification loss ismuch smaller than the entity classification loss due to thelong-tailed distribution of predicates. To balance the scalesbetween these losses, we set p to be larger than e. Exper-imental results in show that the model performsbest when p = 3 and e = 0.5.",
  ". Visualization Results": "We present a visualization of the results of our DRM w/oDKT and our DRM on the PredCls task on the VG150dataset. The results are shown in and 9. As the VGdataset is incompletely labeled, we focus our analysis onlyon labeled relations in the dataset. We observe that DRMw/o DKT can accurately predict the ground-truth relation-ships.However, our DRM often predicts these relation-ships differently. This difference arises from the tendencyof our DRM to predict coarse-grained relations as more pre-cise fine-grained relations. For instance, for the subject-object pair tree-mountain, DRM generates the more accu-rate predicate growing on while the DRM w/o DKT pre-dicts the coarse-grained predicate on. These fine-grainedpredictions lead to a decrease in R@K and an increase inmR@K. This degradation in R@K is an inevitable resultdue to the large number of coarsely labeled relations in thetest dataset."
}