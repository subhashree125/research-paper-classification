{
  "Abstract": "Training deep models for LiDAR semantic segmentationis challenging due to the inherent sparsity of point clouds.Utilizing temporal data is a natural remedy against the spar-sity problem as it makes the input signal denser. However,previous multi-frame fusion algorithms fall short in utilizingsufficient temporal information due to the memory constraint,and they also ignore the informative temporal images. Tofully exploit rich information hidden in long-term temporalpoint clouds and images, we present the Temporal Aggre-gation Network, termed TASeg. Specifically, we proposea Temporal LiDAR Aggregation and Distillation (TLAD)algorithm, which leverages historical priors to assign dif-ferent aggregation steps for different classes. It can largelyreduce memory and time overhead while achieving higheraccuracy. Besides, TLAD trains a teacher injected withgt priors to distill the model, further boosting the perfor-mance. To make full use of temporal images, we design aTemporal Image Aggregation and Fusion (TIAF) module,which can greatly expand the camera FOV and enhance thepresent features. Temporal LiDAR points in the camera FOVare used as mediums to transform temporal image featuresto the present coordinate for temporal multi-modal fusion.Moreover, we develop a Static-Moving Switch Augmenta-tion (SMSA) algorithm, which utilizes sufficient temporalinformation to enable objects to switch their motion statesfreely, thus greatly increasing static and moving trainingsamples. Our TASeg ranks 1st on three challenging tracks,i.e., SemanticKITTI single-scan track, multi-scan track andnuScenes LiDAR segmentation track, strongly demonstrat-ing the superiority of our method. Codes are available at",
  "* Corresponding authors. On the date of CVPR deadline, i.e., 2023-11-18 07:59 AM UTC": "in the autonomous driving . With the adventof deep learning, a large quantity of LiDAR segmentation al-gorithms have been put forward. Despitetheir impressive results, the segmentation performance is stillconstrained by the inherent sparsity of point clouds. To alle-viate this, it is desirable to aggregate temporal data. Previousmulti-frame models can only fuse a few LiDARframes due to the GPU memory constraint, which restrictsthem from utilizing rich information hidden in the long-term temporal point clouds. Although MSeg3D stacksdozens of LiDAR frames, the memory and time overheadare intolerable. Moreover, previous attempts concentrateon utilizing temporal point clouds, ignoring the valuableinformation hidden in temporal images. To better leverage long-term temporal information, wepropose a Temporal LiDAR Aggregation and Distillation(TLAD) algorithm. It can perform efficient multi-frame ag-gregation while achieving higher accuracy. TLAD consistsof Flexible Step Aggregation (FSA) and Mask Distillation.FSA is based on the observation that the model needs dif-ferent amounts of temporal points for different classes. Fordifficult classes, such as bicyclists, more points are needed toyield accurate predictions. However, for easy classes, suchas cars and roads, it is unnecessary to aggregate too manypoint cloud frames for them. Therefore, we propose to as-sign different aggregation steps for different classes in pointcloud sequences, which can significantly save memory andcomputation overhead on easy classes while providing suffi-cient temporal points for difficult classes. To identify classesof temporal point clouds, historical predictions are used,considering the temporal data is processed in chronologicalorder, which is consistent with practical applications. Interestingly, by assigning different classes with differentsteps, FSA actually injects historical priors into the aggre-gated point clouds. For classes with different steps, theirpatterns can be more discriminative due to their differentdensities. This makes the point cloud frames accumulated byFSA more conducive for segmentation. Experimental resultsshow that FSA can not only reduce memory and time costsbut also enhance overall performance. To further verify our idea, we directly replace historical predictions of temporalpoint clouds with their ground truth labels for FSA. We findthe performance boosts greatly. This inspires us to use themodel injected with gt priors to distill the model injectedwith historical priors, which we term Mask Distillation.To fully use the informative temporal images, we devisea Temporal Image Aggregation and Fusion (TIAF) module.Previous multi-modal fusion methods usually suffer fromlimited image features due to the different FOVs (field ofviews) between the LiDAR and the camera. We observe thatas the ego-vehicle moves forward, cameras can capture dif-ferent FOVs. By aggregating images of different timestamps,the FOV of the present camera can be enlarged greatly. Inaddition, temporal images can provide different views forthe same region at different timestamps, which can supplyricher information. Our TIAF leverages temporal LiDARpoints in the FOV of the corresponding temporal camera totransform historical image features to the present coordinatewith the pose matrix. After aggregating all temporal imagefeatures, we use a series of 3D sparse convolutions to fusethem and convert them to voxel representation. Ultimately,we use temporal LiDAR points to gather temporal imagefeatures for temporal multi-modal fusion. The fused fea-tures embrace both temporal LiDAR and temporal imageinformation, which contribute to more precise segmentationresults. To our knowledge, we are the first to leverage tem-poral point clouds and temporal images simultaneously forLiDAR semantic segmentation.Moreover, in the multi-scan task, the model needs todistinguish the motion states (moving or static) of a movableobject, which is very challenging. In this paper, we makefull use of temporal information to remarkably increase themulti-scan perception ability of the model. Specifically, wepropose a data augmentation strategy named Static-MovingSwitch Augmentation (SMSA). It manipulates temporal pointclouds of a movable object to switch its motion state. Inthis way, we can increase the quantity of static and movingsamples significantly, even in the absence of training samplesof static or moving classes.In summary, our major contributions are listed as follows: We propose a Temporal LiDAR Aggregation and Distilla-tion algorithm, which uses Flexible Step Aggregation andMask Distillation techniques to largely reduce memoryand time costs while achieving higher accuracy. We devise a Temporal Image Aggregation and Fusionmodule, which exploits temporal images to enlarge thecamera FOV and enhance present features. It also deliversa scheme for temporal multi-modal fusion. We design a Static-Moving Switch Augmentation algorithmto enable static and moving objects to switch their motionstates freely. With this technique, we can greatly increasestatic and moving training samples.",
  ". Related Work": "LiDAR Semantic Segmentation. LiDAR semantic seg-mentation aims to assign a unique class label to eachpoint in the input point cloud sequence.Recent yearshave witnessed an explosion of LiDAR segmentation al-gorithms . For ex-ample, is the pioneering work that approximates thepermutation-invariant function with a per-point and sharedMulti-Layer Perceptron. changes traditional cubic gridsto cylindrical grids and designs a network of asymmetrical3D convolutions. divides the space with the radial win-dow, which increases the receptive field smoothly and helpsimprove the performance. Despite their good segmenta-tion performance, these methods still take the single LiDARframe as input, which does not utilize the rich semantic andgeometric information hidden in temporal data. Multi-Frame LiDAR Perception. Compared to a singleLiDAR scan, multiple LiDAR scans can provide more suf-ficient information. Recently, many research efforts havebeen put on temporal LiDAR segmentation . For example, leverages a Bayes filter toexplore the temporal consistency. shunts the historicalpoints into two groups to utilize historical frames efficiently.Despite the success of previous multi-frame methods, theycan not leverage the valuable information hidden in long-term temporal point clouds due to the GPU memory con-straint. Although stacks dozens of LiDAR scans, thememory and time overhead are intolerable. In this paper,we present an efficient multi-frame aggregation algorithm,which can greatly save memory and computation consump-tion while achieving higher performance. Multi-Modal Fusion. Since LiDAR and camera are twocomplementary sensors for 3D semantic segmentation, multi-modal fusion has gained increasing attention in recent years. However, these multi-modal fusionmethods usually suffer from limited overlapped regions be-tween the LiDAR and camera due to their different FOVs. proposes a cross-modal knowledge distillation method,which is free from images at inference, while it causes muchloss of the RGB information. completes the missingcamera features using predicted pseudo-camera features,while the used image information is still restricted to thepresent camera FOV. In this paper, we fully take advantageof temporal images to enlarge the FOV of the present cameraand enhance present image features. Knowledge Distillation for LiDAR Perception. Knowl-edge distillation is widely used in various fields in thatit can improve the performance of the student without sac-rificing inference efficiency. In LiDAR perception, compress a cumbersome teacher to a lightweight student toreinforce the representation learning of the student as wellas maintain high efficiency. distills the prior of 2Dimages to 3D point clouds with well-designed cross-modalknowledge distillation module. utilizes a multi-frame teacher to help a single-frame student learn dense 3Dfeatures. By contrast, our method aims to transfer knowl-edge from a multi-frame teacher injected with gt priors to amulti-frame student injected with historical priors.",
  "Yt = concat(Lt, Lt1, . . . , Ltt),(1)": "where Pti and Lti denote the (t i)th point cloud frameand the corresponding point-wise label. Tti is the trans-formation matrix that transforms the coordinate from the(t i)th frame to the tth frame. t is the window size oftemporal point clouds and concat(.) denotes the concatena-tion operation. Xt and Yt are the aggregated LiDAR frameand point-wise label. Although simple, there are some prob-lems. On the one hand, direct concatenation consumes muchGPU memory. On the other hand, the huge memory cost con-strains the multi-frame model from utilizing more historicalframes, thereby limiting the ultimate performance.",
  "Flexible Step Aggregation": "To reduce the GPU memory consumption, we can sampletemporal frames with a step, while this method is also sub-optimal. A small step introduces huge memory overhead,whilst a large step cannot utilize sufficient temporal infor-mation. To this end, we propose Flexible Step Aggregation(FSA). Our method is based on the observation that for differ-ent classes, the model needs different amounts of temporalinformation. For difficult classes, such as bicycles, morepoints are needed to yield accurate predictions. For easyclasses, such as cars and buildings, it is unnecessary to ag-gregate many point cloud frames for them. Based on theabove analysis, we propose to assign different steps for differ-ent classes according to their learning difficulty. Specifically,we leverage historical predictions to divide the temporalpoint clouds into several class groups and assign a specificsampling step for each group. Then, we aggregate temporal points for each group with the corresponding step. Eventu-ally, temporal points of all groups are concatenated with thecurrent frame, resulting in an aggregated frame.The group division is not strict as long as it follows theprinciple that more difficult classes need smaller steps. Forexample, we can simply divide all classes into three groupsaccording to their segmentation performance, such as [0, 80)mIoU, [80, 90) mIoU and mIoU. Then, we assignthe three groups with a step of 2, 4 and ( means we donot aggregate temporal points for the group). To save morememory and computation without sacrificing performance,a more fine-grained division can be used. Formally, supposewe divide temporal point clouds into g groups. We aggregatetemporal points for the kth group as below:",
  "(2)": "Here o = {oi|oi = tisk, i = 1, 2, ..., n, n = t/sk}.sk is the sampling step for the kth class group and . is thefloor operation. M koi is the group mask that indicates whichpoint of Poi belongs to the kth class group. It can be obtainedfrom historical predictions. Finally, we concatenate pointsand labels of all groups with the current frame:",
  "(3)": "In this way, we discard massive redundant temporal pointswhile maintaining essential temporal information. Sinceeasy classes are usually large and hold a large quantity ofpoints, the GPU memory overhead can be further reduced.Moreover, by assigning different classes with different steps,FSA actually injects historical priors into the aggregatedpoint clouds. For classes with different steps, their patternscan be more discriminative due to their different densities(steps), which makes it easier to segment the multi-framepoint clouds aggregated by FSA. Experiments in verify that the proposed FSA can not only save memory andtime costs but also achieve better performance.",
  "Mask Distillation": "In FSA, we use historical predictions to generate groupmasks, which we call pseudo masks. Since ground truthlabels are more accurate than historical predictions, a naturalquestion arises: what if we use gt masks (the group masksgenerated by ground truth labels) for FSA? Our experimentshows that the performance can be improved greatly. Actu-ally, using gt masks for FSA can produce more discrimina-tive patterns for classes with different steps. This motivatesus to simulate features of a model trained on temporal pointclouds aggregated with gt masks. In this way, our model isguided to learn more discriminative features to distinguishdifferent classes. We call this Mask Distillation. Specifically, . Overview of our Temporal Aggregation Network (TASeg). (1) Temporal LiDAR Aggregation and Distillation leverages theproposed Flexible Step Aggregation (FSA) to assign different temporal steps for different classes, and it utilizes a teacher injected withgt priors for knowledge distillation. (2) Temporal Image Aggregation and Fusion takes temporal LiDAR points as mediums to transformhistorical image features to the present coordinate. 3D sparse convolutions are employed to fuse temporal image features. Finally, we usetemporal LiDAR points to gather voxel-wise temporal image features for temporal multi-modal fusion.",
  "LKD = E[F sms F tmt2].(4)": "Since temporal point clouds aggregated with pseudo masksand gt masks are different, we use masks ms and mt to selectvoxels that appeared in both F s and F t. Note that our MaskDistillation is distinct from the methods that distill a multi-frame model to a single-frame model . Both thestudent and teacher in our method are multi-frame models.",
  ". Temporal Image Aggregation and Fusion": "Previous multi-modal fusion methods only focus on lever-aging present images while ignoring the precious value oftemporal images. Temporal images can provide broadercamera FOVs and richer information. Besides, they can en-able more robust multi-modal fusion under the malfunctioncondition on some cameras. In this section, we provide aneffective solution for aggregating temporal image featuresand performing temporal multi-modal fusion.Temporal Aggregation and Fusion. Since temporal im-ages are in different feature spaces, it is difficult to establishthe relationship between different images for feature aggre-gation. In our method, we take temporal LiDAR points asmediums to transform temporal image features to the present coordinate with the pose information. This way, temporalimage features are unified to the present 3D space. Specif-ically, given an image Itt RHW 3 and point cloudPtt RN3, we use an image network to extract theimage feature Ztt RHW C. According to the sen-sor calibration, we can establish the pixel-to-point mappingbetween 2D pixels and 3D points. Hence, we can projectimage feature Ztt to 3D space, resulting in point-wiseimage feature Qtt RMC, where M is the number ofLiDAR points located on Ztt. By transforming Qtt tothe present coordinate with the pose matrix, we realize theaggregation of temporal image features:",
  "Xt = concat(Qt, To1Qo1, . . . , TonQon),(5)": "where o = {oi|oi = t i s, i = 1, 2, ..., t/s} and s isthe sampling step for temporal images. With temporal im-ages aggregated, the FOV of the present camera is expandedgreatly. Moreover, temporal image feature fusion becomesconvenient because they are unified to the same 3D space.Concretely, we can use several 3D sparse convolutions tofuse aggregated temporal image features, which also endowsthem with geometric information, as shown in Equation 6. Inaddition, feature map downsampling is also utilized to gener-ate multi-scale voxel features, providing richer informationfor subsequent temporal multi-modal fusion.",
  "V t = SparseConv3D(Voxelization(Xt)).(6)": "Temporal Multi-Modal Fusion. Benefiting from the tem-poral image aggregation, associating temporal image fea-tures with temporal LiDAR points also becomes convenient.Specifically, given temporal image features converted tounified voxel representation V t, we can establish a point-to-voxel association between temporal LiDAR points andV t. For each temporal LiDAR point, we generate its imagefeature by pooling its nearby voxel-wise temporal imagefeatures with trilinear interpolation instead of hard indexing.To extract richer image features, we perform interpolation onmulti-scale feature maps. Finally, we concatenate the pointcloud features and aggregated multi-scale image features, re-sulting in fused features, which convey powerful informationof both temporal point clouds and temporal images. 2D and 3D Supervision. To make the extracted imagefeatures more informative, we add 2D supervision and 3Dsupervision on the 2D backbone and 3D convolutions in theimage branch, respectively. The 3D supervision is just thelabel of point clouds. The 2D supervision is obtained byprojecting labels of point clouds to the image plane.",
  ". Static-Moving Switch Augmentation": "In the multi-scan task, the model is required to distin-guish the motion state of movable objects. To enable themodel to explore a large data space, we design an effec-tive data augmentation, Static-Moving Switch Augmenta-tion (SMSA). SMSA enables a movable object to switchits motion state freely, which can remarkably increase thesample quantity of static and moving objects. Concretely,considering that a unique object b has the same instanceid in all frames of a sequence, we can use its instancemask to crop its temporal point clouds, which is denotedas B = {Boi|i = 0, 1, ..., t/s, Boi RN3}. Hereo = {oi|oi = t i s, i = 1, 2, ..., t/s}, s is thesampling step and Boi is the temporal part of b at toi. Bymanipulating B, we can change the motion state of b. Static to Moving. If b is static, all temporal parts of blocate at the same position, as shown in the upper-left of. To change b to a moving object, we can shift eachtemporal part of b with an offset. Considering that objectstypically move at a constant speed within a short time, weset the offset between adjacent temporal parts to be the same.For offset itself, it is a random value to increase the diversityof resulting moving samples. As for the direction of theoffset, it can be roughly estimated by comparing the widthand height of the minimum bounding box of b. Since staticobjects often park on the side of the road, which is crowded,the shifted temporal parts of static objects may overlap withother objects. To alleviate this, we define a set of anchorpoints around b and a coverage area for each anchor point.",
  ". Visualization of the augmented samples by our Static-Moving Switch Augmentation (SMSA). Our SMSA switches themotion state by manipulating the temporal parts of objects": "Then, we shift all temporal parts of b to the anchor pointwhose coverage area contains the fewest LiDAR points (referto the supplementary material for more details).Moving to Static. If b is moving, different temporal partsof b locate at different positions. To switch its motion stateto static, we can shift all its temporal parts to the sameposition. In particular, we calculate the centers of eachtemporal part of b, which are denoted as C = {Coi|Coi R3}.Considering the trajectory of moving objects in a short timeis approximate to a line and the speed is a constant, wesimply use q = Co0 Co1 as the offset of adjacent temporalparts of b. Eventually, we can obtain a static object byshifting all temporal parts of b to Co0 (see ).It should be noted that our SMSA is an online and plug-and-play data augmentation strategy, which consumes negli-gible storage and computation costs.",
  ". Experiments": "Datasets & Evaluation Metrics. Following , weevaluate the performance on two popular LiDAR segmen-tation benchmarks, i.e., SemanticKITTI and nuScenes. Se-manticKITTI contains 22 point cloud sequences, where se-quences 0-10, 8, 11-21 are selected as training, validationand testing, respectively. As for nuScenes, it collects 1,000 driving scenes, where 850 scenes are chosen for train-ing and validation, and the remaining 150 scenes are usedfor testing. We conduct experiments on three tracks, i.e.,SemanticKITTI single-scan and multi-scan semantic seg-mentation and nuScenes semantic segmentation.",
  "traffic sign": "SqueezeSegV2 39.7 81.8 18.5 17.9 13.4 14.0 20.1 25.1 3.9 88.6 45.8 67.6 17.7 73.7 41.1 71.8 35.8 60.2 20.2 26.3RangeNet53++ 52.2 91.4 25.7 34.4 25.7 23.0 38.3 38.8 4.8 91.8 65.0 75.2 27.8 87.4 58.6 80.5 55.1 64.6 47.9 55.9RandLA-Net 55.9 94.2 29.8 32.2 43.9 39.1 48.4 47.4 9.4 90.5 61.8 74.0 24.5 89.7 60.4 83.8 63.6 68.6 51.0 50.7SqueezeSegV3 55.9 92.5 38.7 36.5 29.6 33.0 45.6 46.2 20.1 91.7 63.4 74.8 26.4 89.0 59.4 82.0 58.7 65.4 49.6 58.9JS3C-Net 66.0 95.8 59.3 52.9 54.3 46.0 69.5 65.4 39.9 88.9 61.9 72.1 31.9 92.5 70.8 84.5 69.8 67.9 60.7 68.7SPVNAS 67.0 97.2 50.6 50.4 56.6 58.0 67.4 67.1 50.3 90.2 67.6 75.4 21.8 91.6 66.9 86.1 73.4 71.0 64.3 67.3Cylinder3D 68.9 97.1 67.6 63.8 50.8 58.5 73.7 69.2 48.0 92.2 65.0 77.0 32.3 90.7 66.5 85.6 72.5 69.8 62.4 66.2RPVNet 70.3 97.6 68.4 68.7 44.2 61.1 75.9 74.4 43.4 93.4 70.3 80.7 33.3 93.5 72.1 86.5 75.1 71.7 64.8 61.4(AF)2-S3Net 70.8 94.3 63.0 81.4 40.2 40.0 76.4 81.7 77.7 92.0 66.8 76.2 45.8 92.5 69.6 78.6 68.0 63.1 64.0 73.3PVKD 71.2 97.0 69.3 53.5 67.9 60.2 75.1 73.5 50.5 91.8 77.5 70.9 41.0 92.4 69.4 86.5 73.8 71.9 64.9 65.82DPASS 72.9 97.0 63.6 63.4 61.1 61.5 77.9 81.3 74.1 89.7 67.4 74.7 40.0 93.5 72.9 86.2 73.9 71.0 65.0 70.4SphereFormer 74.8 97.5 70.1 70.5 59.6 67.7 79.0 80.4 75.3 91.8 69.7 78.2 41.3 93.8 72.8 86.7 75.1 72.4 66.8 72.9UniSeg 75.2 97.9 71.9 75.2 63.6 74.1 78.9 74.8 60.6 92.6 74.0 79.5 46.1 93.4 72.7 87.5 76.3 73.1 68.3 68.5",
  "moto.-m": "LatticeNet 45.2 91.1 54.8 29.7 3.5 23.1 0.66.8 49.9 0.0 44.6 0.0 64.3TemporalLidarSeg 47.0 92.1 68.2 39.2 2.1 35.0 12.4 14.4 40.4 0.0 42.8 0.0 12.9(AF)2-S3Net 56.9 91.8 65.3 15.7 5.6 27.5 3.9 16.4 67.6 15.1 66.4 67.1 59.6MarS3D 52.7 95.1 78.4 39.7 5.1 36.6 10.0 16.2 58.0 1.2 67.3 0.0 36.3SVQNet 60.5 96.0 80.1 41.0 5.1 60.4 7.1 28.7 85.1 0.0 72.4 0.0 88.12DPASS 62.4 96.2 82.1 48.2 16.1 52.7 3.8 35.4 80.3 7.9 71.2 62.0 73.1",
  ". Comparison to the state-of-the-art methods on SemanticKITTI test set (multi-scan). -s indicates static and -m stands for moving": "Implementation Details. We use the MinkowskiNet re-implemented by PCSeg as our baseline. Our TASeg istrained with the SGD optimizer on 4 A100 GPUs withbatch size 6 for 12 and 36 epochs on SemanticKITTI andnuScenes datasets. The learning rate and weight decay areset to 0.02 and 0.0001. The window size of temporal pointclouds is set to 16. Due to a large redundancy of images,we use a step of 12 and a window size of 48 for temporalimages. Our data augmentation strategy includes randomflipping, rotation, scaling, transformation, LaserMix and PolarMix . During the inference, the LiDAR seman-tic predictions and image features of the previous timestampcan be saved, so there is no time cost for generating histori-cal predictions and processing temporal image aggregation.More details are provided in the supplementary material.",
  "We compare our TASeg with state-of-the-art LiDAR segmen-tation methods on SemanticKITTI single-scan and multi-": "scan track, and nuScenes LiDAR semantic segmentationtrack, as summarized in , 2 and 3.On the Se-manticKITTI single-scan track, our TASeg is 1.3 mIoUhigher than UniSeg , the best-performing published algo-rithm on the SemanticKITTI single-scan leaderboard. On theSemanticKITTI multi-scan track, our approach outperformsthe best multi-scan method 2DPASS by 3.3 mIoU. FornuScenes LiDAR semantic segmentation, we also achievesuperior performance over previous algorithms. As shownin , the proposed approach surpasses the UniSeg by 1.1 mIoU. In particular, our approach holds the highestentry on all three tracks. These encouraging results stronglyshow the effectiveness of our approach.",
  "vegetation": "Cylinder3D 77.2 82.8 29.8 84.3 89.4 63.0 79.3 77.2 73.4 84.6 69.1 97.7 70.2 80.3 75.5 90.4 87.6SPVCNN 77.4 80.0 30.0 91.9 90.8 64.7 79.0 75.6 70.9 81.0 74.6 97.4 69.2 80.0 76.1 89.3 87.12D3DNet 80.0 83.0 59.4 88.0 85.1 63.7 84.4 82.0 76.0 84.8 71.9 96.9 67.4 79.8 76.0 92.1 89.22DPASS 80.8 81.7 55.3 92.0 91.8 73.3 86.5 78.5 72.5 84.7 75.5 97.6 69.1 79.9 75.5 90.2 88.0LidarMultiNet 81.4 80.4 48.4 94.3 90.0 71.5 87.2 85.2 80.4 86.9 74.8 97.8 67.3 80.7 76.5 92.1 89.6MSeg3D 81.1 83.1 42.5 94.9 92.0 67.1 78.6 85.7 80.5 87.5 77.3 97.7 69.8 81.2 77.8 92.4 90.1SphereFormer 81.9 83.3 39.2 94.7 92.5 77.5 84.2 84.4 79.1 88.4 78.3 97.9 69.0 81.5 77.2 93.4 90.2UniSeg 83.5 85.9 71.2 92.1 91.6 80.5 88.0 80.9 76.0 86.3 76.7 97.7 71.8 80.7 76.7 91.3 88.8",
  ". Performance comparison with start-of-the-art methods on nuScenes test set": "Effect of Flexible Step Aggregation.In , wecompare our FSA with other multi-frame algorithms.MSeg3D utilizes temporal point clouds by directly con-catenating all points, which introduces a large memory con-sumption and achieves limited improvement. SVQNet leverages cross-attention to fuse temporal features while itcan only handle fewer LiDAR scans. When increasing Li-DAR scans, the memory overhead is also huge. Our FSAassigns different classes with different temporal steps, whichcan leverage long-term temporal information with less mem-ory and time costs. Moreover, thanks to the utilization ofhistorical priors, our FSA achieves better results (71.3 mIoU)than directly concatenating (69.9 mIoU), given the same tem-poral window size. To explore the effect of different windowsizes on FSA, we present . Results show that theperformance is saturated with a window size of 24. Effect of Group Division.To investigate the effect of dif-ferent group divisions for FSA, we provide . Fordivision1, we simply divide all classes into three groupsaccording to their performance on the val set. The group1consists of the classes in [90, 100) mIoU, such as cars androads. The group2 consists of classes in [80, 90) mIoU, suchas motorcycles. The group3 contains the remaining classes.We assign group1, group2 and group3 with the step of ,4 and 2. A step of means we do not aggregate temporalpoints for classes in the group due to their near-saturate per-formances. The result shows that division1 can reduce thetime and memory costs largely compared to directly stack-ing (the 2st row in ). We can also use performancetop1-6, top6-12, and top12-19 for division, resulting in di-vision2. The group division is robust as long as it followsthe principle that more difficult classes need smaller steps.To further reduce the memory consumption, we can finetunethe group division, such as moving large-size classes (e.g.other-ground and terrain) in group3 to group2 (division3)or moving large-size classes in group3 and group2 to a new",
  ". Ablation on different group divisions for FSA": "group with step of 8 (division4). Besides, we can divideeach class group into a close and a distant group accordingto the distance (such as 30 m). Considering close areas needfewer temporal points, we can use twice the original step fortemporal aggregation of close groups (division5). Effect of Mask Distillation.At the 5th row of , weuse gt masks of historical frames for FSA. The performanceis greatly boosted. Considering we cannot get ground truthat inference, we use it as a teacher to distill the model trainedwith pseudo masks. The result shows that the student isimproved by 0.5 mIoU after distillation. With 75% modelcomplexity, we can further reduce the time and memory costand still achieve a leading performance. Effect of Temporal Image Aggregation and Fusion.TIAF delivers a temporal multi-modal fusion scheme tomake full use of temporal images. As shown in ,with zero image, only TLAD is used, achieving 71.8 mIoU.With one image, only present images are utilized. Due tothe limited FOV of the present camera, only part of theLiDAR points can gather image features, which limits themulti-modal fusion. Our TIAF leverages temporal images toenlarge the FOV of the camera and enhance present features.The results show that with the number of images increasing,the performance rises gradually, which verifies the effec-tiveness of our TIAF. When using seven temporal images,we achieve 1.0 mIoU improvement on the strong baseline.Note that there is no extra cost to generate historical imagefeatures because we can save them at the past moments.",
  ". Effect of different supervisions and multi-scale for TIAF": "We also provide the ablation on different supervisionsand multi-scale features for TIAF in . In TIAF, weleverage 2D and 3D supervision to guide the extracted imagefeatures to be more conducive for segmentation. Multi-scaleis also used to provide more discriminative features. Theresult shows that each of the designs is beneficial for finalperformance. Note that our temporal multi-modal method isorthogonal to other single-frame multi-modal methods. Weuse a simple pixel-to-point mapping at the feature projectionstage. More complex methods can also be utilized,but it is not the focus of our TIAF. Effect of Static-Moving Switch Augmentation.To verifyour SMSA, we present . From the results, we canfind that without SMSA, the accuracy of the model on manyclasses is lower than 10 IoU. After using SMSA, the per-formance of moving truck, moving other-vehicle and staticbicyclist classes is improved by more than 20 IoU, and theoverall performance boosts from 61.3 mIoU to 65.7 mIoU,which strongly demonstrates the efficacy of our SMSA.",
  ". Performance of applying our TASeg to different baselines. represents our re-implementation": "Comparison on Complexity, Latency and Accuracy.Asshown in , when not utilizing temporal images, ourTASeg achieves superior performance than other methodswith comparable or less complexity and latency. With tem-poral images, TASeg can maintain lower complexity andlatency than UniSeg while achieving higher accuracy.Besides, with 75% parameters, the latency of TASeg can befurther reduced, and the accuracy is not affected much.",
  ". Acknowledgements": "This work was supported in part by The National Nature Sci-ence Foundation of China (Grant Nos: 62273301, 62273302,62036009, 61936006, 62273303), in part by YongjiangTalent Introduction Programme (Grant No: 2022A-240-G,2023A-197-G), in part by the National Key R&D Programof China (NO.2022ZD0160100). This work was done duringthe first authors internship at Shanghai AI Laboratory. Mehmet Aygun, Aljosa Osep, Mark Weber, Maxim Maximov,Cyrill Stachniss, Jens Behley, and Laura Leal-Taixe.4dpanoptic lidar segmentation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 55275537, 2021. 2",
  "Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger,Andreas Geiger, and Hongyang Li.End-to-end au-tonomous driving: Challenges and frontiers. arXiv preprintarXiv:2306.16927, 2023. 1": "Xuechao Chen, Shuangjie Xu, Xiaoyi Zou, Tongyi Cao, Dit-Yan Yeung, and Lu Fang. Svqnet: Sparse voxel-adjacentquery network for 4d spatio-temporal lidar semantic segmen-tation. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 85698578, 2023. 1, 2, 6,7 Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and Bing-bing Liu. (af)2-S3Net: Attentive Feature Fusion with Adap-tive Feature Selection for Sparse Semantic Segmentation Net-work. In IEEE Conference on Computer Vision and PatternRecognition, pages 1254712556, 2021. 2, 6",
  "Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4dspatio-temporal convnets: Minkowski convolutional neuralnetworks. In CVPR, 2019. 2": "Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4DSpatio-Temporal ConvNets: Minkowski Convolutional Neu-ral Networks. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 30753084, 2019. 2, 6 Ayush Dewan and Wolfram Burgard. Deeptemporalseg: Tem-porally consistent semantic segmentation of 3d lidar scans.In 2020 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 26242630. IEEE, 2020. 2 Fabian Duerr, Mario Pfaller, Hendrik Weigel, and JurgenBeyerer. Lidar-based recurrent 3d semantic segmentationwith temporal memory alignment. In 2020 InternationalConference on 3D Vision (3DV), pages 781790. IEEE, 2020.1, 2, 6 Khaled El Madawi, Hazem Rashed, Ahmad El Sallab, OmarNasr, Hanan Kamel, and Senthil Yogamani. Rgb and lidarfusion based 3d semantic segmentation for autonomous driv-ing. In IEEE Intelligent Transportation Systems Conference,pages 712, 2019. 2 Kyle Genova, Xiaoqi Yin, Abhijit Kundu, Caroline Panto-faru, Forrester Cole, Avneesh Sud, Brian Brewington, BrianShucker, and Thomas Funkhouser. Learning 3d semanticsegmentation with only 2d image supervision. In 3DV, 2021.7",
  "Vision and Pattern Recognition, pages 84798488, 2022. 2,3, 5, 6": "Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, YulanGuo, Zhihua Wang, Niki Trigoni, and Andrew Markham.Randla-net: Efficient semantic segmentation of large-scalepoint clouds. Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, 2020. 1, 6 Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma,Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Ziwei Liu.Rethinking range view representation for lidar segmentation.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 228240, 2023. 2 Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu. Laser-mix for semi-supervised lidar semantic segmentation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2170521715, 2023. 6 Lars Kreuzberg, Idil Esen Zulfikar, Sabarinath Mahadevan,Francis Engelmann, and Bastian Leibe. 4d-stop: Panopticsegmentation of 4d lidar using spatio-temporal object pro-posal generation and aggregation. In European Conferenceon Computer Vision, pages 537553. Springer, 2022. 2 Georg Krispel, Michael Opitz, Georg Waltner, Horst Pos-segger, and Horst Bischof. Fuseseg: Lidar point cloud seg-mentation fusing multi-modal data. In IEEE/CVF WinterConference on Applications of Computer Vision, pages 18741883, 2020. 2",
  "Jiale Li, Hang Dai, Hao Han, and Yong Ding. MSeg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving.arXiv preprint arXiv:2303.08600, 2023. 1, 2, 7, 8": "Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang.Depthformer: Exploiting long-range correlation and local in-formation for accurate monocular depth estimation. MachineIntelligence Research, 20(6):837854, 2023. 1 Jiahui Liu, Chirui Chang, Jianhui Liu, Xiaoyang Wu, LanMa, and Xiaojuan Qi. Mars3d: A plug-and-play motion-aware model for semantic segmentation on multi-scan 3dpoint clouds. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 93729381,2023. 1, 2, 6",
  "Youquan Liu, Yeqi Bai, Lingdong Kong, Runnan Chen, Yue-nan Hou, Botian Shi, and Yikang Li. Pcseg: An open sourcepoint cloud segmentation codebase. 2023. 6": "Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong, YuchenYang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, Yuexin Ma,Yikang Li, et al. Uniseg: A unified multi-modal lidar segmen-tation network and the openpcseg codebase. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision,pages 2166221673, 2023. 1, 2, 6, 7, 8 Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill Stach-niss. Rangenet++: Fast and accurate lidar semantic segmen-tation. In Proc. of the IEEE/RSJ Intl. Conf. on IntelligentRobots and Systems (IROS), 2019. 6",
  "Jiexiong Tang,John Folkesson,and Patric Jensfelt.Sparse2dense: From direct sparse odometry to dense 3-dreconstruction. IEEE Robotics and Automation Letters, 4(2):530537, 2019. 3, 4": "Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, andKurt Keutzer. Squeezesegv2: Improved model structure andunsupervised domain adaptation for road-object segmentationfrom a lidar point cloud. In 2019 International Conference onRobotics and Automation (ICRA), pages 43764382. IEEE,2019. 6 Dong Wu, Man-Wen Liao, Wei-Tian Zhang, Xing-GangWang, Xiang Bai, Wen-Qing Cheng, and Wen-Yu Liu. Yolop:You only look once for panoptic driving perception. MachineIntelligence Research, 19(6):550562, 2022. 1",
  "Aoran Xiao, Jiaxing Huang, Dayan Guan, Kaiwen Cui, Shi-jian Lu, and Ling Shao. PolarMix: A General Data Augmen-tation Technique for LiDAR Point Clouds. arXiv preprintarXiv:2208.00223, 2022. 6": "Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Va-jda, Kurt Keutzer, and Masayoshi Tomizuka. Squeezesegv3:Spatially-adaptive convolution for efficient point-cloud seg-mentation. In European Conference on Computer Vision,pages 119. Springer, 2020. 6 Jianyun Xu, Ruixiang Zhang, Jian Dou, Yushi Zhu, Jie Sun,and Shiliang Pu. RPVnet: A Deep and Efficient Range-Point-Voxel Fusion Network for Lidar Point Cloud Segmentation.In IEEE International Conference on Computer Vision, pages1602416033, 2021. 6 Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, RuiHuang, and Shuguang Cui. Sparse single sweep lidar pointcloud segmentation via learning contextual shape priors fromscene completion. In Proceedings of the AAAI Conference onArtificial Intelligence, pages 31013109, 2021. 6",
  "Assisted Semantic Segmentation on LiDAR Point Clouds. InEuropean Conference on Computer Vision, 2022. 1, 2, 3, 6, 7": "Jihan Yang, Shaoshuai Shi, Runyu Ding, Zhe Wang, and Xiao-juan Qi. Towards efficient 3d object detection with knowledgedistillation. Advances in Neural Information Processing Sys-tems, 35:2130021313, 2022. 3 Dongqiangzi Ye, Zixiang Zhou, Weijia Chen, Yufei Xie, YuWang, Panqu Wang, and Hassan Foroosh. Lidarmultinet:Towards a unified multi-task network for lidar perception.arXiv preprint arXiv:2209.09385, 2022. 7",
  "Ekim Yurtsever, Jacob Lambert, Alexander Carballo, andKazuya Takeda. A survey of autonomous driving: Commonpractices and emerging technologies. IEEE access, 8:5844358469, 2020. 1": "Zhiwei Zhang, Zhizhong Zhang, Qian Yu, Ran Yi, YuanXie, and Lizhuang Ma. Lidar-camera panoptic segmentationvia geometry-consistent and semantic-aware alignment. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 36623671, 2023. 2 Wu Zheng, Li Jiang, Fanbin Lu, Yangyang Ye, and Chi-WingFu. Boosting single-frame 3d object detection by simulatingmulti-frame point clouds. In Proceedings of the 30th ACMInternational Conference on Multimedia, pages 48484856,2022. 3, 4 Minghan Zhu, Shizhong Han, Hong Cai, Shubhankar Borse,Maani Ghaffari, and Fatih Porikli. 4d panoptic segmentationas invariant and equivariant field prediction. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 2248822498, 2023. 2 Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, YuexinMa, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical andAsymmetrical 3D Convolution Networks for Lidar Segmen-tation. In IEEE Conference on Computer Vision and PatternRecognition, pages 99399948, 2021. 1, 2, 5, 6, 7, 8"
}