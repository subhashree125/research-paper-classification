{
  "Abstract": "In this work, we introduce a novel method for calculat-ing the 6DoF pose of an object using a single RGB-D image.Unlike existing methods that either directly predict objectsposes or rely on sparse keypoints for pose recovery, our ap-proach addresses this challenging task using dense corre-spondence, i.e., we regress the object coordinates for eachvisible pixel. Our method leverages existing object detec-tion methods. We incorporate a re-projection mechanism toadjust the cameras intrinsic matrix to accommodate crop-ping in RGB-D images. Moreover, we transform the 3D ob-ject coordinates into a residual representation, which caneffectively reduce the output space and yield superior per-formance. We conducted extensive experiments to validatethe efficacy of our approach for 6D pose estimation. Ourapproach outperforms most previous methods, especiallyin occlusion scenarios, and demonstrates notable improve-ments over the state-of-the-art methods. Our code is avail-able on",
  ". Introduction": "Estimating the pose of every object in an image is anessential task. This paper introduces a simple but effec-tive 6DoF object pose estimation method based on RGB-D images. Object pose estimation is a crucial techniquein many applications, such as augmented reality , au-tonomous driving , and robotic manipulation .Traditional methods use handcrafted features toestablish 2D-3D correspondences between the image and3D object meshes. However, they face challenges such assensor noise, variable lighting conditions, and visual ambi-guity. Recent methods leverage deep learning using con-volutional neural networks (CNNs) or Transformer-based architectures in RGB images to address these chal-lenges. These methods typically provide pixel-level corre-spondences or predict 2D image locations for predefined 3D keypoints , resulting in more ro-bust results. Nonetheless, they still face challenges such ashandling textureless objects and severe occlusion.To address the issues mentioned above, and because thecost of RGB-D sensors continues to decrease, recent stud-ies such as PVN3D , FFB6D , RCVPose , andDFTr have employed RGB-D images for 6DoF objectpose estimation.Given the large output space in 6DoF,existing methods based on RGB-D images predict prede-fined keypoints in the 3D space (i.e., establishing 3D-3Dsparse correspondences).The pose is then solved usingleast squares fitting methods.Although sparse keypointmethods exhibit essential clues that are explainable for ob-ject pose estimation, they still suffer from some drawbacks.First, they often involve time-consuming procedures such askeypoint voting. Second, predicting the keypoints that areheavily occluded remains challenging. Third, they mightfail to find object landmarks under viewpoint changes .Another approach in the RGB-D scheme aims to directlypredict the object poses from the feature representationsgenerated by the neural networks, such as DenseFusion and ES6D . Uni6D and Uni6Dv2 have alsointegrated RGB and depth feature extraction for direct poseprediction. The pose-deriving procedure is less explainablebecause the object pose is regressed directly from the fea-ture embedding. The object poses estimated are often lessaccurate using the direct pose estimation approaches. Thisweakness is also observed in a related track of study, Cam-era Pose Estimation (aka Ego-motion Estimation) .Reasons why pose regression methods are less accurate areprovided in .To overcome the limitations of the above-mentionedRGB-D image-based methods, we propose to use densecorrespondence in 3D space to mitigate the shortcomingsof sparse correspondence and pose regression.This in-volves the utilization of both dense 2D-3D correspondenceand 3D-3D correspondence, where the former employs theRGB-channels and the latter employs the D-channel in-puts to establish the correspondences in association withthe point clouds of 3D object models.To better com-",
  "arXiv:2405.08483v1 [cs.CV] 14 May 2024": "bine the RGB and depth information, we modify the ap-proach of ES6D .Our approach generates interme-diate feature representations for the RGB images and thepositionally encoded depth map (camera xyz map) at first.Then, it uses the concatenated feature maps to producethe final embedding jointly.Instead of directly regress-ing the pose from the embedding, we use the embeddingto bring about the dense correspondence from the joint in-termediate representation.This avoids the shortcomingsof the pose regression approach, which is more related topose approximation than to accurate pose estimation. Ourdense-correspondence method maintains explainability andcan achieve better accuracy than the sparse-correspondencemethod because complete 2D-3D and 3D-3D correspon-dence information is used. An overview is given in .Furthermore, to avoid the need to predict the coordinatesin a large and relatively unlimited space when finding thecorrespondence, we transform the coordinates of the 3Dpoints in the object database into residual-based represen-tations. A set of scattered anchor points is selected fromthe 3D model, and the coordinate of an object point is ex-pressed as the residue vector for its nearest anchor point,as illustrated in . The representation ensures that theprediction range is more condensed and focused. Subse-quently, the object pose is estimated using a neural networkbased on the dense correspondence, which is more efficientthan the time-consuming process such as PnP-RANSAC.To evaluate our proposed residual-based dense pointnetwork (RDPN) approach in a broad range of scenarios,we conducted experiments using four benchmark datasets:MP6D , YCB-Video , LineMOD , and Occlu-sion LineMOD . Our approach generally outperformsmost state-of-the-art methods on the four datasets, espe-cially on heavily occluded datasets. Specifically, our ap-proach achieves the best ADD-S AUC on the YCB-Videoand MP6D datasets and the best ADD(-S) 0.1d on theLineMOD and Occlusion LineMOD datasets.",
  ". Related work": "Two approaches are mainly used to infer the object posesrelated to the 3D models stored in the object database: ap-proaches based on RGB images and RGB-D images.In RGB-based approaches, SSD6D and T6D-Direct adopt object detection networks to infer an additionaloutput for the object pose. YOLOPose uses keypointmatching to derive the rotation and translation. DPOD projects the 3D model points onto a 2-channel texture mapand then predicts the coordinates on the corresponding tex-ture map. Pix2Pose uses an encoder-decoder architec-ture to predict the 3D coordinates and also uses a GAN toimprove stability. CDPN predicts correspondences intwo steps and directly predicts translations from images in-stead of relying on the results of PnP-RANSAC. To address 2D UV map & Camera xyz map RGB-D ImageNetworkPixel-wise 2D/3D (to) 3D MatchingNetwork tR Predicted object coordinate . Overview of our approach. Our method predicts the 3Dcoordinates of each pixel on the objects surface, resulting in pixel-wise (or dense) correspondence. The object pose is then estimatedbased on the pixel-wise correspondence. symmetric objects, EPOS discretizes the object surfaceinto fragments and predicts a probability distribution overfragments. GDR-Net utilizes a simple yet effective 2Dconvolutional Patch-PnP to regress the 6D pose directly. Italso introduces a symmetry-aware module from EPOS to handle symmetric objects. ZebraPose proposes a ahierarchical surface encoding technique and then efficientlysolves the pose through the one-to-one correspondence.Despite their excellent performance, approaches basedon RGB images are still limited by texture-less objects andsevere occlusions, which can result in wrong 2D-3D cor-respondences.With the help of depth-sensing informa-tion, approaches based on RGB-D images can generallyperform better. Most RGB-D based methods assume pre-segmentation of object instances in the input image. Af-ter instance segmentation, the pose of each object is esti-mated individually. Existing approaches can be divided intotwo main categories: Direct Pose Prediction and Keypoint-based Prediction. The former regresses the poses of thesegmented objects from the feature embedding layer di-rectly in the image. It is computationally more efficient,but the pose obtained relies directly on the feature embed-ding, which would be less accurate and lacks explainability.The latter learns to select a set of keypoints (or proposals)for each object in the database. Given a segmented objectin the input image, the corresponding points are found andmatched at first. The object pose is then estimated basedon the keypoint correspondence between the input imageand the 3D models using closed-form solutions with outlierremoval (e.g., RANSAC) or a network for pose estimation.The approach is more explainable since the keypoints actingas proposals are learnable and we know the correspondencebetween the input images and the database objects. How-ever, the sparsity of keypoints may restrict the performance.They are reviewed respectively in the following.",
  "Researches in opt for a direct predictionof rotation and translation. DenseFusion introducesthe densefusion module, which fuses RGB and depth fea-": "tures at the pixel level and predicts the object pose accord-ingly. The final predicted object pose is determined by se-lecting the pixel with the highest confidence level. ES6D presents XYZNet as a solution to mitigate the needfor random memory accesses in the densefusion module toimprove time efficiency. Additionally, they introduce theA(M)GPD loss, specifically designed to address the chal-lenges posed by symmetric objects, as an improvement overthe previous ADD-S loss. Experimental results demonstratethat the A(M)GPD loss yields greater efficacy in handlingsymmetric objects. In Unit6D and Unit6Dv2 , theauthors address the issue of projection breakdown by in-troducing supplementary UV data as an input, enabling aunified backbone to estimate the object pose accurately.While direct pose prediction (or regression) methodsare time-efficient, they usually have degraded performancecompared to keypoint-based methods due to sensor noise.",
  ". Keypoint-based Prediction": "Unlike direct pose prediction methods, keypoint-basedapproaches improve robustness using theprojection equation. These methods define predeterminedkeypoints on the objects surface and predict their posi-tions in the image frame or camera coordinate system. Theobjects pose is then computed based on these correspon-dences through PnP-RANSAC or least-square fitting .PVN3D extends PVNet to predict the key-points in the 3D space because errors that may appear smallin the projection can significantly impact the real world.FFB6D enhances PVN3D by incorporating bidi-rectional fusion modules to share information between thetwo modalities at an early stage. RCVPose proposesa novel keypoint voting scheme that uses 1D radial vot-ing to mitigate cumulative errors in each channel, whichcan significantly improve the accuracy of keypoint localiza-tion predictions. DFTr fuses two cross-modal featuresusing a transformer block to communicate global informa-tion between the two modalities. Moreover, instead of us-ing the MeanShift algorithm to perform keypoint voting,DFTr proposes non-iterative weighted vector-wise vot-ing scheme to reduce computational costs.",
  ". Method": "Given an RGB-D image I and the set of 3D CAD mod-els M = {Mi|i = 1, ..., N}, the primary objective of 6Dofobject pose estimation is to accurately determine the rota-tion R SO(3) and the translation t R3 that correspondto the camera coordinate system for each detected objectO = {Oi|i = 1, ..., O} in the image I.",
  "Anchor": ". Our purposed residual representation. We use dis-tributedly located anchor points and a fine-level residual vector (tothe nearest anchor) to map each point on the objects surface. Thiseliminates the need for the network to directly predict the exactcoordinates where the range is extensively large, and makes thecorrespondence prediction more robust. tation network or an integrated module designedfor object segmentation . To address the de-mands of processing efficiency and the challenges posed bysmall objects, we adopt a two-step approach similar to theapproaches proposed in that rely on object detec-tion. However, unlike those that use RGB images, our ap-proach uses RGB-D images. Processing the depth informa-tion (represented as the camera xyz map) is crucial to thesuccess of object pose estimation.Our proposed network, RDPN, is illustrated in .First, we use a readily available object detector to derive de-tection results. Afterward, for each detection, we crop theRGB-D image using the detected bounding box to obtainresized RGB-D images (Irgb and Idepth). We first use thecamera intrinsic matrix to derive the camera xyz map to uti-lize the depth information better. To obtain an accurate pro-jected camera xyz map, we adjust Korg, the original cam-era intrinsic matrix, to the adjusted intrinsic matrix Kcropfor the cropped window, and apply it to obtain the cameraxyz map (ICxyz). Then, we feed Irgb and ICxyz to our net-work. The network predicts both coarse anchors (Acoarse)and residual vector (Fresidual) for each visible pixel of theobject, as well as a mask (Fmask) for removing backgroundpixels. Leveraging these predictions, in conjunction withthe 2D UV map (IUV ) and down-sampled camera xyz map(ICxyz64), our model predicts the object pose jointly usingthe 2D-3D and 3D-3D dense correspondences established.",
  "where d is a scale factor": ". Framework of RDPN. (i) Starting with an RGB-D image, our initial step involves utilizing the outcomes of object detectionto crop the region of interest (ROI), which results in a zoomed-in view (Irgb, Idepth), In order to obtain the accurate projected cameraxyz map ICxyz, it is necessary to adjust the original camera intrinsic Korg to Kcrop. (ii) Once we have prepared the Irgb and ICxyz, theRGB-D feature extractor is responsible for capturing the RGB-D fusion features Frgbd, and feed them into a feature decoder to obtain boththe mask (Fmask) and per-pixel prediction to the point coordinates in the 3D model of the object. This includes a (K + 1)-dimensionalregion probability (Fregion), 3-dimensional corresponding nearest anchors, and the residual vector (Fresidual). (iii) Finally, based onthe mask and object coordinates, we utilize an image uv map (IUV ) and a downsampled camera xyz map (ICxyz64) to establish densecorrespondences. These correspondences are then input into the pose predictor to regress the object pose R and t. However, once we have acquired the object boundingboxes, we must crop and resize the images to obtain theRegions of Interest (RoI). This can be seen as a sub-imageobtained by applying an affine transformation to the origi-nal image or modifying the intrinsic matrix of the originalimage. When transforming the depth image into a cameraxyz map using the original intrinsic matrix, the projectedlocation will be incorrect within the camera. Therefore, it isnecessary to adjust the original camera intrinsic matrix Korgby left multiplying it by an affine matrix A (as illustrated inthe upper left part of ), which is a 3 3 matrix withthe last row . This gives us Kcrop = AKorg, whichensures accurate projection for the RoI.In addition to achieving accurate projections, adjustingthe intrinsic matrix addresses the diverse distribution ofobject positions between the training and testing datasets.Specifically, if we directly use Korg, the object 3D pointscould appear in a wide range of locations in the image, suchas the upper-left or lower-right. However, after adjusting theintrinsic matrix to Kcrop, the object will always be locatedin the center of the image, making it easier for the subse-quent networks to robustly learn from uniformly distributedcoordinates for correspondence finding.",
  "A possible approach to dense correspondence is to pre-dict the 3D object coordinates of all object pixels in a singlestep. It is efficient in this way but requires the model to iden-": "tify each mesh point within the image and store its coordi-nates in an extensively large range of the object coordinatesystem. Learning to predict them is challenging because ofthe large output space, which can lead to suboptimal perfor-mance for objects with complex shapes or symmetric fea-tures . Additionally, symmetric features could causeambiguities in the mapping, as there may be multiple pointson the object similar to the given pixel in an image. To address this concern, we propose a novel residual-based strategy that decouples the prediction of the object co-ordinates into the coarse and fine parts. For the coarse part,we first establish multiple anchors for the object M usingFarthest Point Sampling (FPS) . FPS effectively subdi-vides the objects surface into distinct subregions. Specifi-cally, we can establish the anchor set Ai for the given objectMi as follows:",
  "Ai = {Aki |Aki FPS(Mi, K), k = 1, ..., K},(2)": "where K is the number of anchors. Each object in the 3Dmodel database can be divided into separate regions basedon its anchor points, determined by nearest-neighbor group-ing. For each point on the objects surface, we then usethese anchors as reference points and compute the residualvector (i.e., fine part) to its nearest anchor, as illustrated in. By utilizing a residual representation, we can refor-mulate the projection equation as Eq. (3) below:",
  "(3)": "where (Rx, Ry, Rz)Ai represents the closest an-chor point of the given point (X, Y, Z) on the objectssurface and (rx, ry, rz) = (X Rx, Y Ry, Z Rz)denotes the residual of the point. This residual projectionenables a condensed and uniform range to represent thecorrespondence relationship between the camera frame andthe object coordinate system.",
  ". RDPN": "Our proposed RDPN takes as input a cropped RGB im-age Irgb, a camera xyz map ICxyz, and the predicted goalcoarse part and fine part for the visible object pixels. Thenetwork is designed using an encoder-decoder architectureto predict the object coordinates for each pixel. The pre-dicted object coordinates, the 2D UV map (IUV ), and thedownsampled camera xyz map (ICxyz64) are then used tocompute the object pose via a simple but effective network.RGB-D Feature Encoder. Our approach to fusing the in-formation from the two modalities draws inspiration fromES6D .Nevertheless, a critical distinction betweentheir method and ours is that we do not concatenate Irgband ICxyz initially and then feed them into CNNs to extractlocal features. We avoid this approach because backgroundnoise may hinder our detection-based approachs effective-ness for fusion of the two modalities.Instead, we extract local texture features (Frgb) from theRGB image Irgb using CNNs. We then combine these fea-tures with the downsampled camera xyz map (ICxyz32) andinput them into a PointNet-like CNNs architecture, using1 1 convolutions to compress both position and textureinformation for individual pixels to get the spatial features(Fdepth). We empirically concatenate the two features at a32 32 resolution to leverage the depth information effi-ciently. Finally, we obtain the global RGB-D fusion feature(Frgbd) by concatenating the Frgb and Fdepth, as shown inthe upper right part of .RGB-D Feature Decoder. The residual representation ofthe 3D model object coordinates is predicted by the RGB-Dfeature decoder, as illustrated in the lower part of . Toutilize the dense correspondences accurately, we also let thedecoder network predict the visible object mask Fmask andguide it with ground truth Fmask by applying L1 loss:",
  "Lmask = ||Fmask Fmask||1.(4)": "For the coarse part in the residual representation, we ap-proach it as a classification problem, opting to choose theanchor with the highest probability for each pixel. Considerthe anchor set of the object i, Ai = {Aki |k = 1, ..., K}.The object points can be divided into regions by these K an-chors according to nearest-neighbor grouping. This definesthe ground-truth labels of the point in a region: if the regionis associated with anchor k, every point inside this regionis labeled by a one-hot vector with the k-th element 1 andthe others 0. The one-hot vector is of length K + 1, wherethe (K +1)-th element denotes the background. Hence, ev-ery spatial site of the RGB-D fusion feature Frgbd can beclassified as one of the classes in {1, , (K + 1)} afterdecoding. We use Fregion to represent the classification la-bels thus obtained. Hence, we can supervise the predictedFregion with cross-entropy loss, formulate as:",
  "Lcoarse = Cross Entropy(Fmask Fregion, Fregion),": "(5)where denotes the Hadamard product, Fregion recordsthe probability of being the k-th class (k = 1 (K + 1))for every spatial site. To infer coarse correspondence lo-cations, we need anchor positions in addition to class (oranchor label) probabilities. We employ the anchor coordi-nate Aki = (Rx, Ry, Rz) as feature representation with kthe most probable class. This yields an additional 3 chan-nels. An illustration can be found in the lower part of .For the fine part in the residual representation, we ap-proach it as a regression problem. To generate the residualFfine, we employ a straightforward approach by applyingan L1 loss to the ground truth residual Ffine:",
  ". Benchmark Datasets": "LineMOD contains sequences of 13 objects with mildocclusion, cluttered scenes, texture-less objects, and varia-tions in lighting conditions, presenting challenges for accu-rate pose estimation in real-world scenarios. Our approachfollows the protocol established in , whichemploys the standard 15%/85% training/testing split. Dur-ing training, we leverage further 1k rendered images foreach object, as described in the settings in .Occlusion LineMOD is an extension of LineMOD thatcontains challenging test images of partially occluded ob-jects. It involves training on LineMOD images and testingOcclusion LineMOD to assess the robustness of handlingheavily occluded objects.YCB-Video is an extensive dataset including 130K keyframes from 92 videos featuring 21 objects with varyinglighting conditions and occlusions. Following ,we split the dataset into 80 training videos and select 2,949keyframes from the remaining 12 videos for testing.MP6D is a challenging dataset for the 6D pose estima-tion of metal parts in industrial environments. It contains 77RGB-D video segments with simultaneous multi-target, oc-cluded, and illumination changes. All objects are symmet- ric, textureless, and of complex shape, high reflectivity, anduniform color, which makes them difficult to distinguish.We follow to split the training and testing sets.For Occlusion LineMOD and YCB-V, we also follow that utilizes synthetic data with physically-basedrendering for training.",
  ". Comparison with State-of-the-Art Methods": "Results on LineMOD & Occlusion LineMOD. The quan-titative results of our proposed RDPN are presented inTab. 1 and Tab. 2 for the LineMOD and OcclusionLineMOD datasets, respectively.Our method achievesstate-of-the-art performance on both datasets. Particularly,in occlusion scenes, our approach outperforms most previ-ous methods extensively, demonstrating its effectiveness.Results on YCB-V. Results of our proposed RDPN on theYCB-Video dataset are shown in Tab. 3. Our approach out-performs the previous state-of-the-art approaches, achiev-ing a superior ADD-S AUC metric by 1.7%. It also demon-strates competitive performance in the ADD(-S) AUC with-out time-consuming post-processing.Results on MP6D. The results of our RDPN on the MP6Ddataset are shown in Tab. 4. Again, our method outper-forms the previous state-of-the-art methods in the ADD-SAUC metric, demonstrating its generalizability across dif-ferent datasets. Compared to other RGB-D based solutions,RDPN is more robust towards objects with texture-less orheavy reflective surfaces and severe occlusions.Qualitative Results. Furthermore, to illustrate the robust-ness of our RDPN in handling occlusion, we present some",
  ". Ablation study on the effectiveness of adjusting the in-trinsic parameter": "the performance is inferior to the previously employed di-rect regression methods. However, replacing IUV with thecamera xyz map ICxyz64 (i.e., 3D-3D correspondences) sig-nificantly improved all ADD(-S) metrics. This observationis intuitive, as the pose predictor can effectively predict theobjects pose without considering the camera intrinsic ma-trix K through Eq. (1). Finally, utilizing dense correspon-dence with both IUV and ICxyz, along with the predictedobject coordinates (2D/3D to 3D correspondences), furtherenhances the performance.We attribute that the sensornoise in collected depth images can be rectified by incor-porating IUV , when both IUV and ICxyz64 are available.Effectiveness of adjusting intrinsic Korg is shown inTab. 7, which reveals better performance when using Kcropto achieve accurate projection. This suggests that the intrin-sic K adjustment step is necessary in our method.Effectiveness of different number of anchors is shownin . We observe a noticeable improvement when in-creasing the numbers of anchors from 4 to 16, with the bestperformance achieved at 32 anchors. However, the perfor-mance starts to decline after using 32 anchors. We believethat as the number of anchors increases, the output space ofthe fine part is reduced, while the output space of the coarsepart inevitably increases. Therefore, there is a trade-off be-tween the number of anchors and overall performance.",
  ". Limitation": "Our method demonstrates remarkable speed, achievingpose estimation for 8 objects in a 640x480 image within amere 29ms/frame. However, the overall processing time iscurrently constrained by the object detection stage, whichrequires 66ms/frame using the YOLOX-x detector. Tofurther enhance the efficiency of our method, we plan toexplore the development of a unified network for object de-tection and pose estimation in future work.",
  ". Conclusion": "We introduce a novel Residual-based Dense Point-wiseNetwork (RDPN) for precise and efficient object pose es-timation from RGB-D images. By employing an intrinsicadjustment and a residual representation for object coor-dinates, RDPN effectively condenses and concentrates theoutput range, eliminating the need to predict coordinateswithin a vast and relatively unbounded space. Leveragingboth 2D-3D and 3D-3D dense correspondences, our methodachieves state-of-the-art performance on public benchmarks . Qualitative results on Occlusion LineMOD. The im-ages are rendered by projecting the 3D object model onto theimage plane using the estimated pose. Our two-step dense cor-respondence method can accurately capture the object and pre-dict its pose, even under heavy occlusion. This contrasts previouskeypoint-based methods, which often struggle in such scenarios.",
  ". Ablation study on the number of anchors. The 10, 10cm metric measures whether the rotation and the translation erroris less than 10 and 10 cm, respectively": "with high efficacy.Acknowledgement This work was supported in part bythe National Science and Technology Council, Taiwan un-der Grant NSTC 112-2221-E-002-182-MY3 and 112-2634-F-002-005, and also under Grant UR2205 of Delta-NTUjoint R&D center. We thank to National Center for High-performance Computing (NCHC) of National Applied Re-search Laboratories (NARLabs) in Taiwan for providingcomputational and storage resources.",
  "Mean shift: A robust approach toward feature space analysis.IEEE Transactions on pattern analysis and machine intelli-gence, 24(5):603619, 2002. 3": "Arash Amini, Arul Selvam Periyasamy, and Sven Behnke.T6d-direct: Transformers for multi-object 6d pose direct re-gression. In DAGM German Conference on Pattern Recog-nition, pages 530544. Springer, 2021. 1, 2 Arash Amini, Arul Selvam Periyasamy, and Sven Behnke.Yolopose: Transformer-based multi-object 6d pose estima-tion using keypoint regression.In International Confer-ence on Intelligent Autonomous Systems, pages 392406.Springer, 2022. 1, 2 Eric Brachmann, Alexander Krull, Frank Michel, StefanGumhold, Jamie Shotton, and Carsten Rother.Learning6d object pose estimation using 3d object coordinates. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part II 13, pages 536551. Springer, 2014. 2, 6",
  "Long Chen, Han Yang, Chenrui Wu, and Shiqing Wu. Mp6d:An rgb-d dataset for metal parts 6d pose estimation. IEEERobotics and Automation Letters, 7(3):59125919, 2022. 2,6, 7": "Shuai Chen, Xinghui Li, Zirui Wang, and Victor Prisacariu.Dfnet: Enhance absolute pose regression with direct featurematching. In Proceedings of the European Conference onComputer Vision (ECCV), 2022. 1 Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, and AlesLeonardis. G2l-net: Global to local network for real-time 6dpose estimation with embedding vector features. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 42334242, 2020. 6, 7 Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.Multi-view 3d object detection network for autonomousdriving. In Proceedings of the IEEE conference on ComputerVision and Pattern Recognition, pages 19071915, 2017. 1 Alvaro Collet, Manuel Martinez, and Siddhartha S Srinivasa.The moped framework: Object recognition and pose estima-tion for manipulation. The international journal of roboticsresearch, 30(10):12841306, 2011. 1 Yan Di, Fabian Manhardt, Gu Wang, Xiangyang Ji, NassirNavab, and Federico Tombari.So-pose: Exploiting self-occlusion for direct 6d pose estimation. In Proceedings ofthe IEEE/CVF International Conference on Computer Vi-sion, pages 1239612405, 2021. 1, 6",
  "Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and JianSun. Yolox: Exceeding yolo series in 2021. arXiv preprintarXiv:2107.08430, 2021. 5, 8": "Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the kitti vision benchmarksuite. In 2012 IEEE conference on computer vision and pat-tern recognition, pages 33543361. IEEE, 2012. 1 Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, andJian Sun. Ffb6d: A full flow bidirectional fusion network for6d pose estimation. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages30033013, 2021. 1, 3, 5, 6, 7 Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, HaoqiangFan, and Jian Sun. Pvn3d: A deep point-wise 3d keypointsvoting network for 6dof pose estimation. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1163211641, 2020. 1, 3, 4, 5, 6, 7 Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Ste-fan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab.Model based training, detection and pose estimation oftexture-less 3d objects in heavily cluttered scenes. In Com-puter VisionACCV 2012: 11th Asian Conference on Com-puter Vision, Daejeon, Korea, November 5-9, 2012, RevisedSelected Papers, Part I 11, pages 548562. Springer, 2013.1, 2, 6 Tomas Hodan, Daniel Barath, and Jiri Matas. Epos: Esti-mating 6d pose of objects with symmetries. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 1170311712, 2020. 1, 2 Tomas Hodan, Jir Matas, and Stepan Obdrzalek. On evalua-tion of 6d object pose estimation. In Computer VisionECCV2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, pages 606619. Springer, 2016. 6 Tomas Hodan, Martin Sundermeyer, Bertram Drost, YannLabbe, Eric Brachmann, Frank Michel, Carsten Rother, andJir Matas. Bop challenge 2020 on 6d object localization.In Computer VisionECCV 2020 Workshops: Glasgow, UK,August 2328, 2020, Proceedings, Part II 16, pages 577594. Springer, 2020. 6 Tomas Hodan, Xenophon Zabulis, Manolis Lourakis, StepanObdrzalek, and Jir Matas. Detection and fine 3d pose es-timation of texture-less objects in rgb-d images.In 2015IEEE/RSJ International Conference on Intelligent Robotsand Systems (IROS), pages 44214428. IEEE, 2015. 7 Xiaoke Jiang, Donghai Li, Hao Chen, Ye Zheng, Rui Zhao,and Liwei Wu. Uni6d: A unified cnn framework without pro-jection breakdown for 6d pose estimation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1117411184, 2022. 1, 2, 3, 6, 7 Wadim Kehl, Fabian Manhardt, Federico Tombari, SlobodanIlic, and Nassir Navab. Ssd-6d: Making rgb-based 3d detec-tion and 6d pose estimation great again. In Proceedings ofthe IEEE international conference on computer vision, pages15211529, 2017. 2",
  "Eric Marchand, Hideaki Uchiyama, and Fabien Spindler.Pose estimation for augmented reality: a hands-on survey.IEEE transactions on visualization and computer graphics,22(12):26332651, 2015. 1": "Ningkai Mo, Wanshui Gan, Naoto Yokoya, and ShifengChen.Es6d:A computation efficient and symmetry-aware 6d pose regression framework.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 67186727, 2022. 1, 2, 3, 5, 7 Kiru Park, Timothy Patten, and Markus Vincze. Pix2pose:Pixel-wise coordinate regression of objects for 6d pose esti-mation. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 76687677, 2019. 2 Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hu-jun Bao. Pvnet: Pixel-wise voting network for 6dof poseestimation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 45614570, 2019. 1, 3, 6 Nuno Pereira and Lus A Alexandre. Maskedfusion: Mask-based 6d object pose estimation. In 2020 19th IEEE Inter-national Conference on Machine Learning and Applications(ICMLA), pages 7178. IEEE, 2020. 7 Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and LauraLeal-Taixe.Understanding the limitations of cnn-basedabsolute camera pose regression.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), June 2019. 1",
  "Ivan Shugurov,Sergey Zakharov,and Slobodan Ilic.Dpodv2: Dense correspondence-based 6 dof pose estima-tion.IEEE transactions on pattern analysis and machineintelligence, 44(11):74177435, 2021. 6": "Chen Song, Jiaru Song, and Qixing Huang. Hybridpose: 6dobject pose estimation under hybrid representations. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 431440, 2020. 6 Yongzhi Su, Mahdi Saleh, Torben Fetzer, Jason Rambach,Nassir Navab, Benjamin Busam, Didier Stricker, and Fed-erico Tombari.Zebrapose: Coarse to fine surface encod-ing for 6dof object pose estimation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 67386748, 2022. 1, 2",
  "Uni6dv2: Noise elimination for 6d pose estimation. In Inter-national Conference on Artificial Intelligence and Statistics,pages 18321844. PMLR, 2023. 1, 2, 3, 6, 7": "Jonathan Tremblay, Thang To, Balakumar Sundaralingam,Yu Xiang, Dieter Fox, and Stan Birchfield. Deep object poseestimation for semantic robotic grasping of household ob-jects. arXiv preprint arXiv:1809.10790, 2018. 1 Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martn-Martn,Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6dobject pose estimation by iterative dense fusion. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 33433352, 2019. 1, 2, 3, 6, 7 Gu Wang, Fabian Manhardt, Federico Tombari, and Xi-angyang Ji. Gdr-net: Geometry-guided direct regression net-work for monocular 6d object pose estimation. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1661116621, 2021. 1, 2, 3, 5, 6 Yangzheng Wu, Mohsen Zand, Ali Etemad, and MichaelGreenspan. Vote from the center: 6 dof pose estimation inrgb-d images by radial keypoint voting. In European Con-ference on Computer Vision, pages 335352. Springer, 2022.1, 3, 6, 7",
  "A.1. Network Architecture": "The detailed architecture of the proposed RDPN isshown in . In this figure, conv(n n, c) denotes a2D convolution with kernel size n and output channel c.bn denotes batch normalization, relu denotes ReLU activa-tion, Upsample(s) denotes 2D upsampling with scale factors. and maxpool(k, s, p) denotes 2D max pooling with ker-nel size k, stride s, and padding p, respectively. The outputof adaptive avgpool(h, w) or adaptive maxpool(h, w) is ofsize hw for any input size. convTranspose(nn, c) denotesa 2D transposed convolution with kernel size n and outputchannel c. gn denotes group normalization [?], Leakyreludenotes LeakyReLU activation, and Linear(c) denotes afully connected layer with output channel c.To represent rotations, we adopt the solution proposedin [?] to address the issue of rotation discontinuity, whichresults in a 6-dimensional output.",
  "A.3. Training Enhancements": "We employ two strategies to enhance the models abilityto handle objects of varying sizes. First, we dynamicallyadjust the receptive field of the Fresidual based on the sizeof the corresponding tight 3D bounding box of the CADmodel. This allows the model to focus more effectively onobjects of different scales.Second, we adopt the Dynamic Zoom-In technique pro-posed in [?,?] to alleviate the impact of varying object sizesfurther. During training, we randomly shift the center andscale of the ground-truth bounding boxes by a ratio of 25%.Subsequently, we zoom in the input Regions of Interest (RoIs) with a ratio of r = 1.5 while maintaining their orig-inal aspect ratio. This ensures that the area containing theobject occupies approximately half of the RoIs. This dy-namic zooming approach effectively normalizes the objectsize distribution and improves the models generalizationability across different object sizes.",
  "B.1. Quantitative Results under the same detectionson the YCB-V Dataset": "To comprehensively assess the effectiveness of RDPN,we compare it with several baseline methods while ensur-ing a fair comparison. However, it is essential to note thatwhile other methods utilize segmentation masks or built-indetection techniques, RDPN incorporates detection prepro-cessing specifically designed for RGBD images. Therefore,we adopt PoseCNNs [?] RoI results for RDPN and seg-mentations for other methods to maintain consistency andimpartiality. Despite this disparity in detection pipelines,RDPN exhibits robust accuracy, as evidenced in Tab. 1. Thisfinding underscores its efficacy even when operating underdifferent detection paradigms.",
  "B.5. Visualization on Predicted Pose on the YCB-Video and MP6D Datasets": "We provide several qualitative comparison results be-tween our method and the previous state-of-the-art methodFFB6D [?] in for the YCB-Video dataset. Addition-ally, we provide several qualitative results on the MP6Ddataset in .The results demonstrate the effectiveness of our methodon both datasets, including texture-less and high-reflectivityobjects. conv(7*7, 64), bn, relu maxpool(3,2,1) Upsample(4) conv(1*1, 64), bn, relu adaptive_maxpool(1,1) adaptive_avgpool(32,32) convTranspose(3*3, 256), bn, relu conv(1*1, 37) conv(3*3, 128), gn, relu Linear(1024), Leakyrelu Linear(256), Leakyrelu Linear(6)Linear(3) 3X conv(3*3, 64), bn,relu conv(3*3, 64), bn 4X conv(3*3, 128), bn,relu conv(3*3, 128), bn 6X conv(3*3, 256), bn,relu conv(3*3, 256), bn 3X conv(3*3, 512), bn,relu conv(3*3, 512), bn conv(1*1, 128), bn, relu conv(1*1, 256), bn, relu conv(1*1, 512), bn",
  "FFB6DRDPN (Ours)Ground Truth": ". Qualitative results on YCB-Video dataset. The first column shows the ground truth pose. The second column shows the poseestimated using the keypoint-based method FFB6D [?]. The third column shows the pose estimated using our RDPN approach. Insidethe bounding box, we see that our dense correspondence method outperforms the keypoint-based method FFB6D [?] in handling poseestimation under occlusion conditions."
}