{
  "Abstract": "In this paper, we explore the existing challenges in 3Dartistic scene generation by introducing ART3D, a novelframework that combines diffusion models and 3D Gaus-sian splatting techniques. Our method effectively bridgesthe gap between artistic and realistic images through an in-novative image semantic transfer algorithm. By leveragingdepth information and an initial artistic image, we generatea point cloud map, addressing domain differences. Addi-tionally, we propose a depth consistency module to enhance3D scene consistency. Finally, the 3D scene serves as initialpoints for optimizing Gaussian splats. Experimental resultsdemonstrate ART3Ds superior performance in both con-tent and structural consistency metrics when compared toexisting methods. ART3D significantly advances the field ofAI in art creation by providing an innovative solution forgenerating high-quality 3D artistic scenes.",
  ". Introduction": "With the progress of Artificial Intelligence Generated Con-tent (AIGC) and advancements in 3D vision technology, AI-driven art creation has become a hot topic fordesigners and artists. The latest generative models in the realm of 2D art are capable of producing high-qualityimages based on artistic prompts, making it easier for usersto create 2D art and promoting the growth of AI art. How-ever, despite success in the 2D field, 3D art creation stillfaces significant challenges.The current approaches face a series of challenges whenattempting to apply similar generative models to create 3Dmodels from textual descriptions. While some emergingmethods strive to address the issues from textto 3D models, significant difficulties arise due to the lackof available 3D art training data , especially datatailored to artistic creations rather than real-world scenes.These methods encounter substantial challenges when deal-ing with domain differences between real-world scenes andartistic works. Furthermore, some approaches adopt a two-step process of generating images first and thenconverting them to 3D. However, the 3D information re-",
  "arXiv:2405.10508v1 [cs.CV] 17 May 2024": "quired by these models is often based on training with real-world datasets, making accurate information prediction forartistic inputs a difficult task.All these challenges con-tribute to the fact that research on 3D technology in artisticcreation is still in its early exploratory stages.In this paper, we aim to explore a novel research ques-tion: how to generate high-quality 3D scenes with artisticstyles based on text or reference RGB images. As shownin , leveraging the powerful generative capabilitiesof Stable Diffusion models , we introduce an innovative3D Gaussian splatting technique to produce artistic 3Dscenes from user-provided textual input by updating a pointcloud map.Specifically, we propose an image semantic transfer al-gorithm to bridge the gap between artistic and realistic im-ages. This algorithm extracts feature maps with the samesemantic layout obtained through UNet from user-providedtext or reference images. Subsequently, we use text promptsto generate new realistic images, ensuring they adhere to thesame semantic layout. While realistic images play a role inthe intermediate layers, once we predict the depth informa-tion of realistic images, we can transfer this information totheir corresponding artistic image. Using the depth map andthe initial artistic image to generate an initial point cloud,we successfully address the domain gap between artisticand realistic images.Next, we generate a reasonable intrinsic matrix and cam-era pose, using the camera pose to reproject the initial pointcloud onto a new camera plane. Employing the inpaintingtechnique of the Stable Diffusion model , we completethe hollow areas of the novel view image, ultimately ob-taining a complete image. By iteratively performing pointcloud generation and projection, we finally obtain an initialpoint for the scene point cloud optimized using 3D Gaussiansplatting . In this process, we introduce a depth con-sistency module to enhance consistency between multipleviews. Aligning depth domains, this module ensures seam-less integration of the newly generated point cloud into theentire system, improving consistency in multi-view genera-tion. Ultimately, we render an impressive 3D scene basedon a continuous representation of the point cloud map using3D Gaussian splatting.Our experiments demonstrate that our approach excelsin both style consistency and continuity metrics for artis-tic images compared to existing methods, exhibiting greatervisual appeal. We further include the ablation studies in-volving different core components. Finally, we extend ourmethod to various application scenarios, such as text-drivenfusion of different artistic styles. This paper provides aninnovative approach to high-quality artistic 3D scene gen-eration based on textual descriptions or reference images,making significant contributions to the intersection of artand technology. Our contributions can be summarized as",
  "We introduce ART3D, which achieves high-quality 3Dartistic scene generation through diffusion models and 3DGaussian splatting techniques": "Our method compensates for the domain gap betweenartistic and realistic images through an image semantictransfer algorithm, and the introduction of a depth consis-tency module improves the overall consistency of globalscene generation. We innovatively address the generation of high-quality3D artistic scenes from text or reference images, mak-ing a significant contribution to the development of theinterdisciplinary field of AI in art creation.",
  ". Related Works": "In this section, we provide a brief review of the literaturerelated to diffusion models, text-guided 3D generation and3D scene synthesis.Diffusion Models. In recent years, large-scale models havebecome a hot topic in the field of AI. Among them, diffu-sion models are widely recognized as powerful toolsin the areas of complex data modeling and generation. Theirremarkable and stable capabilities in complex data model-ing have led to successful applications in various domains,including image restoration , image translation ,image /video generation , super-resolution , andimage editing . P2P and PnP furtherexplore the crucial role of attention mechanisms and fea-tures in the process of image generation, proposing the useof cross-channel attention or self-attention mechanisms forglobal image editing tasks. Recent research indicates thatthese techniques have successfully extended from the 2Ddomain to the 3D domain, as demonstrated in . Ourapproach maximizes the advantages of diffusion models inboth 2D and 3D domains, leveraging their powerful restora-tion and generation capabilities, as well as feature mecha-nisms. This has led to successful applications in the field ofAI artistic creation.Text-to-3D. With the emergence of pre-trained text-conditioned image generation diffusion models, the trend ofextending these 2D models to achieve text-to-3D generationhas become increasingly popular. Some methods employsupervised training of text-to-3D models using 3D data ; however, due to the lack of large-scale aligned textand 3D datasets, this direction remains challenging.Toachieve open-vocabulary 3D generation, pioneering workby DreamFusion has paved the way, where some ap-proaches propose the application of text-to-image diffusionmodels to 3D generation . Subse-quently, numerous extensive improvements have been madeto DreamFusion . ProlificDreamer in-troduces Variational Score Distillation (VSD) and generates",
  "^": ". Pipeline of ART3D. We introduce the process of our method. First, we input textual descriptions or reference images and usesemantic transfer algorithms to obtain accurate depth 3D information. Then, construct a point cloud and enhance multi-view consistencythrough consistency/alignment algorithms. Finally, we render high-quality 3D artistic scenes using 3D Gaussian splatting technology. high-fidelity texture results. Magic3D adopts a coarse-to-fine strategy and utilizes DMTET as a 3D repre-sentation to achieve texture refinement through SDS loss.Recent methods combine large-scale text-to-image diffusion models with neural radiancefields to generate 3D objects without the need for train-ing. However, most of these methods are tailored to real im-ages or common stylistic works. Approaches for text-driven3D scene generation, particularly for diverse artistic styles,remain largely unexplored.3D Scene Synthesis. The typical methods for presenting3D scenes include explicit approaches like point clouds,meshes, and voxels. Recently, there has been a focus onimplicit methods, such as signed distance functions and neural radiance fields , for expressing 3D scenes.The 3D Gaussian splatting technique , by cleverly com-bining Gaussian splats, spherical harmonics, and opacity,achieves a fast and high-quality reconstruction of completeand boundaryless 3D scenes. Its features align well withour approach to freely generate scenes, so we opt to use thisrendering method in this paper.",
  ". Method": "In this section, we provide a detailed overview of ART3D,which consists of four key components, as illustrated in. Firstly, we leverage the attention mechanism ofStable Diffusion models and design an image seman-tic transfer algorithm to enhance the accuracy of obtaining depth information from artistic images. In .2, weestablish a point cloud map that transforms depth informa-tion Di into a point cloud, and through camera reprojec-tion, generates new images Ii+1. Subsequently, we applythe operations from the first part to the inpainting imageIi+1 to obtain depth map Di+1. In .3, we intro-duce a depth consistency module to improve consistencybetween multiple views and seamlessly integrate the newpoint cloud into the existing point cloud map. Finally, em-ploying the 3D Gaussian splatting technique, we success-fully render high-quality 3D artistic scenes and novel views.",
  ". Image Semantic Transfer": "The pre-trained stable diffusion model can generate anartistic image with a corresponding style based on inputprompts such as mosaic, Van Gogh, etc. However, sim-ply appending keywords of realistic styles at the end of theprompts makes it nearly impractical to generate images thatalign with semantic information due to the stochastic na-ture of the diffusion model. Currently, conditional gener-ation methods like ControlNet , which leverage depthand edge information as conditional inputs, struggle whenapplied to artistic images, as these artistic images inherentlypossess fuzzy characteristics. Therefore, adopting such anapproach is still challenging.To address this issue and generate real images with a se-mantic layout similar to artistic images, similar to , weutilize the internal features of the Stable Diffusion model to align the semantic information of the two images. The in-termediate layers of the UNet in the Stable Diffusion modelconsist of residual blocks, self-attention layers, and cross-attention layers. The self-attention layer of the i-th block attime t computes features as follows:",
  "d),(1)": "where Qit, Kit, V it represent the query, key, and value.Similar to PnP , we note that the self-attention mapsAt at each time step t during the denoising process in Sta-ble Diffusion control the spatial structure of the resultingimage. Therefore, while generating the artistic image x, weretain the intermediate self-attention maps At for all timesteps t. Injecting the output residual block features f t isemployed to enhance structural alignment. Specifically, westore the features from the 4-th layer in the output blocks.Then, we use the self-attention maps during the denoisingprocess of Stable Diffusion to control the layout of the gen-erated image. We inject the output residual block featuresf t and self-attention maps At into the UNet module. Thisprocess can be represented as follows:",
  "xt1 = (xt, c, t; At, f t),(3)": "where is a standard denoising UNet. xt and xt arenoisy images at time step t corresponding to the artisticand realistic images, respectively. is the modified UNet,which takes injected features as input.During the generation process, the alignment of semanticfeatures between artistic and realistic images is guaranteedby leveraging the stored features f t and self-attention mapsAt.",
  ". Point Cloud Map": "Our approach achieves view-consistent 3D scene generationby updating a point cloud map. Initially, we set up the cam-eras intrinsic and extrinsic parameters as T. Next, utilizinga monocular scale depth estimation model, ZoeDepth ,we acquire a depth map Di and lift pixels of Ii to 3D spacebased on depth information, thus creating an initial pointcloud Pi.Along the cameras trajectory, we project thepoint cloud Pi and then obtain a new image Ii+1 by repro-jecting it back onto the camera plane. This process can berepresented as follows:",
  "(d) x-t depth slices (after)": ".We demonstrate the effectiveness of our depth con-sistency module. Frames from (a) to (b) represent consecutiveframes, while (c) and (d) illustrate depth value slices evolving overtime at the red line in (a) and (b). A smoother x-t slice indicatesmore consistent depth. Our approach significantly enhances depthconsistency. aforementioned process and , obtaining scene imagesfrom every viewpoint along the cameras trajectory. Due tothe independence among these 3D point clouds and the dis-continuity in the depth estimation model, the scale informa-tion among these point clouds varies. Therefore, in .3, we introduce an efficient depth consistency algorithmto standardize the initial depth scale information obtainedfrom the depth estimation model, ensuring the maintenanceof a scale-consistent point cloud map.Next, it is essential to align the projected point cloudscorresponding to each pose to ensure the generation of acontinuous 3D artistic style scene. The 3D point cloud Pi,projected from the camera pose of the previous time step,results in the image Ii+1. After inpainting, a new imageIi+1 (representing the point cloud to be aligned) is obtainedfrom Ii+1. In Ii+1, a significant portion of content overlapswith Ii, and we denote this overlapping region as the M.Consequently, the 3D point clouds Pi+1 and Pi, projectedfrom the current pose of Ii+1, also share many redundantpoints. The point alignment can be represented as follows:",
  ". Depth Consistency Module": "According to the analysis in .2, achieving a con-sistent 3D point cloud scene necessitates relying on consis-tent depth information. Due to the independence of predic-tions from depth estimation networks, the depth consistencyalgorithm faces two challenges: the global distribution ofdepth ranges for different depths is inconsistent, makingit impractical to simply employ a global scaling factor Sfor standardization.Additionally, methods that involve the re-optimization of depth information basedon camera poses require RGB domain image information,making their application challenging in the artistic domain. To address these challenges, we propose DCM, whicheliminates the dependence on RGB domain image informa-tion, thereby enhancing depth consistency across differentviews of the same 3D scene. Specifically, DCM takes thedepth Di from the previous view and the initial depth Di+1from the current view as input, producing the updated depthDi+1 as output. This can be represented as follows:",
  "Di+1 = Di + DCM (Di+1, Di) .(6)": "Based on this formula, DCM attempts to learn a depth resid-ual to update D and minimize the inconsistency between Diand Di+1. To ensure computational efficiency, we employshallow convolutional layers and residual blocks to imple-ment DCM. Its architecture comprises an encoder and a de-coder, connected via skip connections.While the structure of DCM is concise, we encountera scarcity of real multi-view datasets containing groundtruth depth from different perspectives, posing challengesto DCMs training. Considering that our scenario only in-volves static scenes without dynamic objects, we train themodel using the virtual multi-frame dataset IRS . Thisdataset includes highly accurate depth information for staticscenes. Specifically, we select a set of 7 frames as trainingdata and the initial depth map is resized and cropped to aresolution of 384. To improve the depth accuracy, we alsoemploy a depth domain loss similar to MiDaS , whichconstrains the same depth range. Additionally, we design adepth consistency loss as follows:",
  "MsDi+1 Di1 ,(8)": "where Mi = exp(|Oi+1 Oi|2) represents the oc-clusion weight between Di+1 and Di, and is set to 50. Oiand Oi+1 are RGB frames. Di is derived by warping Di toDi+1 based on the backward optical flow Fi+1i, which iscomputed using GMFlow . Furthermore, Oi is obtainedby warping Oi to Oi+1 according to Fi+1i. This involvesutilizing the optical flow to transform visualized images, en-suring alignment between Oi and Oi+1. It is important tonote that Di is warped by the optical flow Fi+1i.As shown in , we demonstrate how our approacheffectively enhances depth consistency, where (c) and (d)represent x-t slices of scene motion, with more smooth in-dicating better depth consistency.",
  ". 3D Gaussian Splatting for Rendering": "Following the point cloud alignment stage, we obtain acomplete point cloud map, which serves as the initial pointcloud for the Structure from Motion (SfM) required in train-ing the 3D Gaussian splatting model. Each Gaussian splats point is initialized with these point cloud values andoptimized for volume and position using the ground truthfrom the projection images as supervision. In contrast totraditional SfM methods , which may exhibit shortcom-ings in the domain of artistic images, our approach not onlyyields more accurate initial point clouds but also acceleratesthe convergence of the network, facilitating deeper learn-ing of more nuanced features. Additionally, consideringthat regions generated by the Stable Diffusion Inpaintingmodel may contain inaccurate information, we inten-tionally ignore these areas when computing the loss func-tion. Given that points in the point cloud are represented byGaussian distributions, these regions will naturally be filledduring the training process.",
  ". Experiment Setup": "Implementation DetailsFor our text-to-image model,we adopt the Stable Diffusion model and its inpaint-ing version, fine-tuned on the image inpainting task withan additional mask input.Concurrently, we employ theZoeDepth model as our monocular depth estimator, rep-resenting the SOTA in scale-depth estimation models. Sub-sequently, we provide a detailed overview of the trainingparameters for our DCM below. Details of DCMThe DCM adopts an encoder-decoderarchitecture, where the encoder consists of two downsam-pling strided convolutional layers, followed by five resid-ual blocks. In the decoding process, two transposed con-volutional layers are employed, incorporating essential skipconnections from the encoder to the decoder. Instance Nor-malization is consistently applied, with the exception of thelast layer. To confine the output within the range of -1 to1 after decoding, a Tanh layer is employed. The model isimplemented using PyTorch and is trained with the Adamsolver for 20,000 iterations, maintaining a steady learn-ing rate of 1e-4. Throughout the training phase, a batchsize of 4 is utilized, and the training data undergoes randomcropping to dimensions of 384384. Evaluation MetricsWe need to ensure that the renderedimages maintain consistency with the reference imagesstyles.To achieve this, we employ CLIP-I metricsto calculate the similarity of image features. The secondmetric needs to evaluate the consistency between the ren-dered image and textual descriptions. Additionally, we uti-lize the CLIP-T metric to calculate the cosine similarity be-tween text prompts and CLIP embeddings. Furthermore,",
  "(a) (b) Rendered novel view synthesis images(c) 3D artistic scene": ". Visualization results of our method. (a) represents the inputs, where we can use only text or a combination of a reference imageand text as input. (b) is a novel view image rendered from the generated 3D artistic scene, showing stylistic consistency. (c) demonstrates3D artistic scenes generated by our method through a predefined camera trajectory. Our approach can accurately and high-quality generatestructurally consistent and diverse 3D artistic scenes.",
  "As shown in , we illustrate the results generatedby our method when provide with multiple sets of artis-tic textual descriptions. Our ART3D successfully produces": "consistent 3D artistic scenes, as depicted in (d). Leverag-ing the generative capabilities of the diffusion model, ourapproach achieves stylistic coherence in scene generationbased on textual prompts. Additionally, as shown in (b),we can render images from various perspectives by incor-porating given camera poses, presenting a novel approachfor novel view image synthesis tasks. To further assess the superiority of our method in text-driven 3D artistic scene generation, we conduct a compar-ison with another method utilizing the diffusion model for3D scene generation (). In contrast to ap-proaches applied in real-world domains or those closelysimulating real-world texture, our method produces morecontinuous multi-view images and more consistent andplausible 3D scenes. The structural consistency in our gen-erated 3D scenes surpasses that of other methods, whichmay exhibit errors in spatial positioning due to depth align-ment issues, as indicated within the red circles in .Specifically, LucidDreamer exhibits some depth align-ment problems, while Text2Room shows reduced ro-bustness for outdoor scenes, leading to potential inaccura-",
  "(c) Ours. Text: City Streets in Chinese Landscape Painting Style": ". Qualitative comparison with 3D scene generation methods. LucidDreamer and Text2Room perform poorly on artisticimages due to the gap between the artistic and realistic domains. As highlighted in the red circle, they struggle to obtain accurate 3Dinformation, leading to structural errors caused by depth accuracy or alignment. These issues result in the generation of unsatisfactory 3Dscenes. In contrast, our method excels in the artistic domain.",
  "(b)": ". We demonstrated the effectiveness of our depth consis-tency module. In (a), we optimize only the global depth scale fac-tor S, leading to discontinuities and structural distortions withinthe white region. After processing with our depth consistency al-gorithm, the generated 3D scenes exhibit improved consistencybetween different views. reference images, and textual descriptions. Simultaneously,we conduct a user study, selecting 25 sets of images for 20users to rate (0 to 5 represents worst to best score). Specifi-cally, we focus on structural consistency metrics(SC), such as the presence of incoherent elements in the scenes, such asholes, and content consistency metrics (CC), assessing thealignment of images with textual descriptions. Across allthese metrics, our approach consistently achieves the high-est scores, highlighting its ability to generate structurallyconsistent high-quality 3D artistic scenes accurately.",
  ". Ablation Studies": "The key components of our method include the image se-mantic transfer algorithm (.1), point cloud map(.2), and depth consistency (.3). We con-duct an extensive ablation study to validate the effectivenessof each component. Effects of Image Semantic TransferAs we know, mostdepth estimation models are typically trained on realdatasets, posing challenges in accurately estimating depthinformation for artistic images. However, accurate depthinformation is crucial for generating point clouds, ensuringthe fidelity of the generated scenes. Our designed imagesemantic transfer algorithm can generate realistic scene im-ages with the same semantic layout as corresponding artisticstyle images. As shown in , directly predicted depthmap (a) struggles to capture scene information accurately,while our method can generate a depth map (b) with richdetails, leading to more accurate point clouds and avoidingissues like object misplacement. Our approach effectivelybridges the gap between artistic and real-world image ap-plications and provides new inspiration and insights for AIartistic creation. Effects of Point Cloud MapWe evaluate the point cloudgeneration strategy we employed for 3D Gaussian Splattinginitialization. We compare it with point clouds obtained us-ing COLMAP . For the task of artistic scene gener-ation, our point cloud exhibits exceptionally high quality,providing a substantial number of high-quality points for3D Gaussian Splatting initialization and significantly accel-erating the reconstruction speed. As shown in , weevaluate our generated artistic results using image qualitymetrics, demonstrating the effectiveness of our method inenhancing the rendering quality of artistic scenes. Effects of Depth Consistency ModuleDue to the factthat the 3D information required for point cloud generationis predicted from a monocular depth estimation model, in-consistencies in scale and depth range exist in the predic-tions of monocular depth across consecutive frames. Ourproposed monocular depth consistency algorithm ensuresthe generation of a coherent 3D scene, thereby avoiding is-sues such as discontinuities, holes, gaps, and distortions inthe final 3D representation. This algorithm learns residual",
  "LPIPS COLMAP0.2580.2370.224Ours0.2290.2210.214": ".Quantitative comparison.We compare our methodwith the 3D artistic scene reconstruction results obtained usingCOLMAP. Underline shows that we achieve competitive perfor-mance with few iterations. Our approach optimizes 3D Gaussiansplats more efficiently and exhibits superior reconstruction met-rics. information between depths from two different viewpoints,effectively unifying the depth range and scale. In (a), we present visual results where a simple global scal-ing factor for depth alignment leads to noticeable disconti-nuities and distortions in the 3D structure within the whiteboxes.",
  ". Conclusion": "In conclusion, ART3D represents an advancement in AI-driven 3D art creation. By effectively addressing challengesin domain gaps and global scene consistency, our approach,utilizing diffusion models and 3D Gaussian splatting, excelsin generating high-quality 3D artistic scenes from textualdescriptions. Beyond quantitative metrics, ART3D signifi-cantly contributes to the intersection of AI and art by pro-viding a novel solution for creating visually appealing 3Dscenes. Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-ter Talbott, Alexander Toshev, Zhuoyuan Chen, LaurentDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al.Gaudi: A neural architect for immersive 3d scene genera-tion. Advances in Neural Information Processing Systems,35:2510225116, 2022. 1, 2",
  "Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, PeterWonka, and Matthias Muller.Zoedepth: Zero-shot trans-fer by combining relative and metric depth. arXiv preprintarXiv:2302.12288, 2023. 4, 5": "Eric R Chan, Koki Nagano, Matthew A Chan, Alexander WBergman, Jeong Joon Park, Axel Levy, Miika Aittala, ShaliniDe Mello, Tero Karras, and Gordon Wetzstein. Generativenovel view synthesis with 3d-aware diffusion models.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 42174229, 2023. 2 Kevin Chen, Christopher B Choy, Manolis Savva, An-gel X Chang, Thomas Funkhouser, and Silvio Savarese.Text2shape: Generating shapes from natural language bylearning joint embeddings. In Computer VisionACCV 2018:14th Asian Conference on Computer Vision, Perth, Australia,",
  "Pengzhi Li, Baijuan Li, and Zhiheng Li.Sketch-to-Architecture: Generative AI-aided Architectural Design. InPacific Graphics Short Papers and Posters. The Eurograph-ics Association, 2023. 2": "Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu,Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, andFeng Zheng. Tuning-free image customization with imageand text guidance. arXiv preprint arXiv:2403.12658, 2024.2 Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolutiontext-to-3d content creation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 300309, 2023. 2, 3 Andrew Liu, Richard Tucker, Varun Jampani, AmeeshMakadia, Noah Snavely, and Angjoo Kanazawa. Infinite na-ture: Perpetual view generation of natural scenes from a sin-gle image. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1445814467, 2021.1 Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. Repaint: Inpaintingusing denoising diffusion probabilistic models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1146111471, 2022. 2",
  "Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-hall.Dreamfusion: Text-to-3d using 2d diffusion.arXivpreprint arXiv:2209.14988, 2022. 1, 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 5, 7 Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-man, Michael Rubinstein, Jonathan Barron, et al. Dream-booth3d:Subject-driven text-to-3d generation.arXivpreprint arXiv:2303.13508, 2023. 1, 2 Rene Ranftl,Katrin Lasinger,David Hafner,KonradSchindler, and Vladlen Koltun. Towards robust monoculardepth estimation: Mixing datasets for zero-shot cross-datasettransfer. IEEE transactions on pattern analysis and machineintelligence, 44(3):16231637, 2020. 5 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1, 2, 3, 4, 5 Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,Jonathan Ho, Tim Salimans, David Fleet, and MohammadNorouzi.Palette: Image-to-image diffusion models.InACM SIGGRAPH 2022 Conference Proceedings, pages 110, 2022. 2 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. 3 Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-imans, David J Fleet, and Mohammad Norouzi.Imagesuper-resolution via iterative refinement. IEEE Transactionson Pattern Analysis and Machine Intelligence, 45(4):47134726, 2022. 2",
  "Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited.In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages41044113, 2016. 5, 8": "Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, andSanja Fidler.Deep marching tetrahedra: a hybrid repre-sentation for high-resolution 3d shape synthesis. Advancesin Neural Information Processing Systems, 34:60876101,2021. 3 Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-BinHuang. 3d photography using context-aware layered depthinpainting.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 80288038, 2020. 1",
  "Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and GangZeng. Dreamgaussian: Generative gaussian splatting for effi-cient 3d content creation. arXiv preprint arXiv:2309.16653,2023. 2": "Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity3d creation from a single image with diffusion prior.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 2281922829, 2023. 2 Narek Tumanyan, Michal Geyer, Shai Bagon, and TaliDekel.Plug-and-play diffusion features for text-drivenimage-to-image translation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 19211930, 2023. 2, 4 Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,and Greg Shakhnarovich. Score jacobian chaining: Liftingpretrained 2d diffusion models for 3d generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1261912629, 2023. 3 Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng,Kaiyong Zhao, and Xiaowen Chu. Irs: A large naturalisticindoor robotics stereo dataset to train deep models for dis-parity and surface normal estimation. In 2021 IEEE Interna-tional Conference on Multimedia and Expo (ICME), pages16. IEEE, 2021. 5 Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, ChongxuanLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity anddiverse text-to-3d generation with variational score distilla-tion. arXiv preprint arXiv:2305.16213, 2023. 2 Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, andDacheng Tao.Gmflow: Learning optical flow via globalmatching. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 81218130,2022. 5 Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Addingconditional control to text-to-image diffusion models.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 38363847, 2023. 3 Minda Zhao,Chaoyi Zhao,Xinyue Liang,LinchengLi, Zeng Zhao, Zhipeng Hu, Changjie Fan, and XinYu.Efficientdreamer:High-fidelity and robust 3d cre-ation via orthogonal-view diffusion prior.arXiv preprintarXiv:2308.13223, 2023. 1, 2 Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generationand completion through point-voxel diffusion. In Proceed-ings of the IEEE/CVF international conference on computervision, pages 58265835, 2021. 2"
}