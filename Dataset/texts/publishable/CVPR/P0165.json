{
  "Abstract": "Recent years have seen immense progress in 3D com-puter vision and computer graphics, with emerging toolsthat can virtualize real-world 3D environments for numer-ous Mixed Reality (XR) applications. However, alongsideimmersive visual experiences, immersive auditory experi-ences are equally vital to our holistic perception of an en-vironment. In this paper, we aim to reconstruct the spatialacoustic characteristics of an arbitrary environment givenonly a sparse set of (roughly 12) room impulse response(RIR) recordings and a planar reconstruction of the scene,a setup that is easily achievable by ordinary users. To thisend, we introduce DIFFRIR, a differentiable RIR renderingframework with interpretable parametric models of salientacoustic features of the scene, including sound source di-rectivity and surface reflectivity.This allows us to syn-thesize novel auditory experiences through the space withany source audio. To evaluate our method, we collect adataset of RIR recordings and music in four diverse, real en-vironments. We show that our model outperforms state-of-the-art baselines on rendering monaural and binaural RIRsand music at unseen locations, and learns physically inter-pretable parameters characterizing acoustic properties ofthe sound source and surfaces in the scene.",
  ". Introduction": "Much of the impetus to realize immersive virtual reality(VR) stems from the desire to recreate and share real scenesand experiences. Motivated by this goal, recent progress in3D computer vision and computer graphics has led to toolsthat can virtualize real-world 3D environments using sim-ple consumer devices (e.g., cellphone cameras) for numer-ous Mixed Reality (XR) applications. Alongside immer-sive visual experiences, immersive auditory experiences areequally vital to our holistic perception of an environment.For instance, while the interior of Carnegie Hall in New",
  "*Equal contribution": "York City is visually beautiful, one cannot fully appreci-ate the majesty of its design without experiencing a musicalperformance in-person and hearing its unique acoustics.In this paper, our goal is to capture the acoustic intrinsicsof a real-world scene using a sparse set of measurements,in order to render arbitrary source audio at any location,hence the name, Hearing Anything Anywhere. This isanalogous to the task of sparse-view novel view synthesis(NVS) in computer vision and graphics .However, there are two key differences between lightand sound that make common approaches to visual NVSinapplicable to audio. First, light is typically emitted fromcontinuous sources and travels steadily and almost instantlythrough space, resulting in a largely stationary visual scene.In contrast, sound signals are usually time-varying andtravel through space at a much slower pace, resulting in aconstantly changing 4D acoustic field with both numerousearly reflections and late reverberations. Second, a singlecamera captures millions of pixels in a split second, eachrecording a distinct light ray from a particular direction. Incontrast, a typical microphone only records an amalgama-tion of sound waves arriving to a single location from all di-rections, with different times-of-arrival. Therefore, while itis possible to capture the appearance of a 3D scene by sim-ply walking through it with a camera, the same approachfalls short to record the entire 4D acoustic field.Thus, capturing a fully immersive acoustic field oftennecessitates setting up hundreds of microphones denselyacross the space , which is impractical formany consumer use cases. In this work, we attempt to cap-ture real-world acoustic spaces with a basic hardware setup,e.g., 12 microphones, which can be easily scaled to arbitraryenvironments.To capture the acoustic properties of the scene, we mea-sure a room impulse response (RIR) between the soundsource and each microphone location. An RIR is a time-series signal that estimates how a perfect impulse emittedfrom the source, traveling and bouncing in the room, wouldbe perceived at the listener location. RIRs effectively cap-ture a rooms intrinsic acoustic properties between source",
  "arXiv:2406.07532v1 [cs.SD] 11 Jun 2024": "and listener points, and are thus widely used in acousticsimulation . In order to simulate the sound of an arbi-trary source for a particular listener location in a room, theRIR associated with the source-listener pair is simply con-volved with the source audio . We thus formulate our Hearing Anything Anywhere taskas inferring RIRs and music at novel listener locationsfrom a sparse set of RIRs measured between a singlesource and a small set of microphone locations spatially dis-tributed within the scene. Towards this goal, we introduce afully differentiable impulse response rendering frameworkDIFFRIR that reasons about the individual contributions ofeach acoustic reflection path between the source and the re-ceiver, including the time delay and magnitude of the soundon each path, as well as the influence of reflections fromeach surface in the scene. By explicitly modeling the sound source location, the di-rectivity map of the source, and the reflection properties ofthe surfaces in the scene in a fully differentiable audio ren-dering framework, we can characterize the parameters ofeach model through an analysis-by-synthesis paradigm byoptimizing the output of DIFFRIR against the known sub-set of measured RIRs. After optimizing the interpretableparameters of our model, we can estimate the RIR from anyunseen location in the scene. To validate our method, we collect a dataset that con-tains RIR measurements from four real-world environmentsthat represent a diverse range of room materials, shape, andcomplexity. Through experiments comparing our frame-work with current state-of-the-art methods, DIFFRIR showsgreater robustness in real, data-limited scenarios. Moreover,with the explicit and interpretable models of source and sur-face reflection properties, we can easily synthesize novelauditory experiences with different speaker orientations andlocations, which can be useful in applications such as virtualreality and acoustics-aware interior design. In addition, thedifferentiable and interpretable models of our framework al-low us to estimate acoustic parameters of the sound sourceand surfaces in the room, which can be useful in applica-tions like robotics and architectural design for acoustics. Our contributions are threefold.First, we contributeDIFFRIR, a differentiable acoustic inverse rendering frame-work that can recover the fully immersive acoustic field ofa room from a set of 12 sparsely located RIR measure-ments. Second, we contribute a new dataset of real-worldRIRs measured from hundreds of locations in four differentreal environments. Third, we compare our method to ex-isting methods across various settings, demonstrating thatour method is more effective than existing methods on realdata in our data-limited scenarios, predicting more accu-rate RIRs and music at unseen locations. Code and dataare available at the project website.",
  ". Related Work": "Learning-Based Room Acoustics Prediction.Whilemany acoustical learning frameworks model room acousticsimplicitly, others explicitly interpolate and predict RIRs atnovel points. Frameworks that predict RIRs at novel pointsin a room vary not only in their underlying techniques, butalso in their inputs. Some methods do not use vision orgeometry to make their estimates, but instead learn to di-rectly approximate a function mapping spatial coordinatesto RIRs . These methods can require large train-ing set sizes on the order of 1,000 RIRs from a room toeffectively interpolate RIRs to novel points within the sameroom. Alternatively, some methods use geometric featuresof the scene , such as , which learns a diffuse re-flection model from a small subset of points in the meshof the environment, to achieve a performance improvementover pure audio-based methods. Our method uses environ-ment geometry to explicitly model specular reflections oneach surface. To validate our approach, we compare againstthree baselines, including one audio-only method andtwo methods that use scene geometry . Audio-Visual (AV) Room Acoustics Prediction.Othermethods learn relationships between visual inputs and roomacoustics to perform tasks such as predicting the dereverber-ated signal from an audio recording and a panoramic im-age of the recording environment , or predicting howan input audio signal would sound in a target space basedon an image of the space . Many works use visual in-puts to explicitly perform the novel view acoustic synthe-sis (NVAS) task. For instance, Chen et al. proposedthe Visually-Guided Acoustic Synthesis (ViGAS) network,which outputs the spatial audio of the speech of a human incorresponding visual frames. Furthermore, by using audio-visual features as well as geometric ones, Ahn et al. show that the important sub-tasks of NVAS, e.g., soundsource localization, separation, and dereverberation, can bejointly solved. AV-NeRF improved the performance ofboth NVS and NVAS tasks via multi-task training by usingan audio-based Neural Radiance Field (NeRF). Their au-dio NeRF estimates variations in the magnitudes of audioperceived from varying locations, whereas we explicitly es-timate the RIR, a much more holistic characterization of theenvironment acoustic properties.Similar to our binaural prediction task, Garg et al. predict binaural audio from an AV scenes monaural au-dio and visual features extracted from the scenes videoframes. Although AV approaches can sometimes outper-form uni-modal audio-only models at estimating environ-ment acoustics, collecting large enough datasets of synchro-nized audio-visual pairs for these models can be laborious.Perhaps for this reason, many such models, even one boast-ing few-shot generalization , present results from eval-",
  "uating exclusively on simulated data": "Geometry-Based RIR Simulation.Many of the afore-mentioned works use datasets of simulated RIRs generatedby the SoundSpaces framework , a fast acoustic sim-ulator based on geometric acoustic methods.They sim-ulate the acoustics of virtualized versions of real roomsfrom datasets of meshes reconstructed from RGBD scansof real rooms in home and workplace environments, suchas the Matterport3D dataset or the Replica dataset .The Geometric-Wave Acoustic (GWA) dataset uses a hybridpropagation algorithm combining wave-based methods with geometric acoustic methods, intending to model low-frequency wave effects more accurately, albeit at the costof longer run-time. The input meshes are from a datasetof professionally designed virtual home layouts . TheMesh2IR framework uses the GWA dataset to learn a con-ditional generative adversarial network (cGAN) to morequickly predict RIRs from meshes of rooms . The au-thors do not show how their cGANs estimates of RIRscompare to measured RIRs from real rooms. DifferentiableAcoustics.Thepreviouslymentionedsimulators are not differentiable, which precludes gradient-based optimization techniques which can be used in solv-ing inverse problems. Differentiable audio rendering tech-niques have been used to solve such inverse problems esti-mating acoustic properties of musical instruments andeveryday objects , as well as the reverberation proper-ties of the environments they are in. The authors of implemented a differentiable acoustic ray tracer for inversetasks in underwater acoustics, such as estimating the ab-sorption of the seabed on simulated 2D data. We use similarprinciples for estimating absorption parameters of surfacesin 3D environments from our real, airborne sound data.",
  ". Task Formulation": "To achieve our goal of virtualizing real acoustic spaces, ourmethod should require information about the room that is aseasy as possible to obtain. With this objective in mind, weshow that our method produces accurate results, while onlyrequiring the following:1. A small set of omnidirectional RIR recordings capturedat sparse locations (e.g., 12), with the xyz coordinates atwhich they were captured. 2. The rooms rough geometry, expressed as a small num-ber of planes.RIRs can be easily captured by playing a sine sweepfrom the source location and recording it from a microphoneat the listener location. In our setup, we assume a stationary audio source whose orientation and position are unknown.With this information, our goals are to simulate monoauraland binaural RIRs and music at arbitrary listener locationsand orientations in the room.",
  ". The DIFFRIR Framework": "To achieve this task, we design a differentiable RIR render-ing framework, dubbed DIFFRIR. As an overview of theDIFFRIR framework, we use the sound source and micro-phone location, along with the planar decomposition of theenvironment, to trace all specular reflection paths betweenthe source and a listener location, up to a certain number ofreflections. We estimate the sound arriving to the listenerfrom each path using a series of parametric models for thesound source directivity and impulse response, as well asthe acoustic reflection of each surface. Each model is fullydifferentiable, with interpretable parameters. We computeeach RIR as the sum of contributions of the sound arriv-ing from each path, combined with a learned residual. Weuse these models in a differentiable audio renderer to opti-mize parameters according to a loss function comparing ourestimates to the known subset of ground-truth RIRs. We de-scribe each model in detail below.",
  "Characterizing the Sound Source": "Source Localization.We first estimate the location of thesound source for all subsequent steps. Based on the knownsubset of RIRs we use their locations and the timing of thefirst peak to localize the source using a traditional time-of-arrival method. More details are provided in Appendix E. Source Directivity.Most real sound sources do not radi-ate sound uniformly in all directions. For instance, a loud-speaker will usually be much louder from the front, andhuman speakers also have distinct directivity patterns .The sources directivity describes the way in which thesource radiates sound differently in different directions andis generally frequency dependent.For example, a loud-speaker will overall sound much louder from the front,with the higher-frequency components radiating in espe-cially narrow beams and lower-frequency components moreomnidirectionally. The sound sources directivity has a sig-nificant impact on the acoustic field of the room and is there-fore important to model.We model the filtering effect of exiting the sound sourcein any particular direction with the directivity response. Letdp be the absolute direction (given as a unit vector) in whichthe sound path exits the speaker. Our goal is to fit D( dp),a function mapping dp to a magnitude frequency responsethat accounts for the effect of exiting the speaker in the di-rection of dp. When a sound exits the speaker in the di-rection of dp, the frequency content of the sound wave ismultiplied by D( dp).",
  "Differentiable Path Tracing": "* . Differentiable Room Impulse Response Rendering Framework (DIFFRIR). Our model renders the contribution to the RIRof a single traced reflection path. After computing a reflection path, we characterize it by the direction at which it exits the speaker, itslength, and the surfaces on which it reflects. The sound source has a learned frequency response that depends on the outgoing direction,and each surface has a different learned frequency response. We multiply each of these responses to estimate the overall path response. Todetermine the reflection paths time-domain contribution to the final RIR, we apply a minimum-phase inverse-Fourier transform to the pathresponse, convolve it with the source impulse response, and then shift the result in time based on the path length and the speed of sound. To model the direction-dependent frequency response,we fit F different heatmaps on unit spheres centered on thespeaker, one heatmap for each of F octave-spaced centerfrequencies comprising vector f. To do this, we distribute128 points evenly along the surface of the unit sphere, us-ing a Fibonacci lattice . We denote this set of pointsL. Let Ax,fo be the log-amplitude gain for sound travel-ing out of the speaker in the direction of x at frequency fo.To determine the log-amplitude gain at fo in direction dp,we interpolate between the points on the heatmap using aspherical Gaussian weighting function, inspired by :",
  "where represents linear interpolation on the vector of deci-bel values Ad indexed by center frequencies f, based onquery frequency fo": "Source Impulse Response.Since the room impulse re-sponse relates the source signal fed to the speaker to thesound heard in the room, we must also account for the waythat the source modifies the source signal being fed to it. Forinstance, if the source is a loudspeaker, it may attenuate orboost certain frequencies. We model these effects by learn-ing a source impulse response IRs in the time domain, thus",
  "We trace each specular reflection path and model the acous-tic effects of each reflection along the path, with unique re-flection parameters for each surface in the environment": "Reflectivity.When a sound wave encounters a surface, afraction of the sound waves energy will be specularly re-flected, while the remaining energy will be absorbed, trans-mitted, diffusely reflected, or diffracted. These effects varyby frequency, depending on the texture and material prop-erties of each surface.For each surface s, we fit a vector Vs of F differentvalues representing the magnitude of sound specularly re-flected by the surface at each of F octave-spaced centeredfrequencies in vector f. We apply the sigmoid function tothese values to determine the energy reflection coefficients(the proportion of specularly reflected sound energy) at eachfrequency. Next, we determine the amplitude reflection co-efficients (the amount that the surface attenuates the incom-ing sound at each frequency in terms of linear amplitudegain) by taking the square root of the energy reflection co-efficients . Using the amplitude reflection coefficientsat the F center frequencies, we obtain the amplitude gainsfor arbitrary frequencies through linear interpolation. Thisgives us the reflection response Rs, a magnitude frequencyresponse representing the surfaces effect on incoming au-dio of different frequencies. Thus, the formula for Rs is:",
  "Here, denotes the sigmoid function, and is a linear in-terpolation from the coefficients Vs based on the relation ofthe query frequency fr to the center frequencies f": "Reflection Paths.Given the estimated source locationSxyz, a listener location Lxyz, and a planar representation ofthe rooms geometry, we use the image-source method to efficiently compute all of the specular reflection paths be-tween the source and listener in the room, up to a particularorder N (e.g., 5). The method considers all permutationsfrom 1 to N of these surfaces with repetition and, for eachpermutation, determines if there is a valid reflection paththat travels from the source to the listener after reflectingspecularly off of each of the surfaces in order. For eachvalid reflection path p from source to listener, we track thelength of the reflection path lp, the ordered list Sp of reflec-tion surfaces along the path, and the direction from whichthe path exits the source dp.Rooms often contain parallel surfaces, which lead toprominent higher-order reflections.These reflections re-sult in axial modes, which are powerful room resonanceswith especially long reverberation times . Thus, in ad-dition to computing all N th-order reflection paths for allpossible orderings of surfaces, our image-source algorithmalso computes all valid reflection paths for pairs of parallelwalls, up to a much higher order, e.g., 50. This modifica-tion, which we call axial boosting, improves the modelsperformance (see Appendix D.4) in adversarial cases likethe Hallway, with a computational overhead that scales lin-early rather than exponentially with reflection order. Wediscuss additional surface interactions, such as diffuse re-flection, in .2.3.",
  "Combining Models": "We combine these reflection and sound source models toestimate the contribution of each reflection path. We thensum the contributions across all paths and add a residual toestimate the RIR for a given source and listener location. Contribution of a Single Reflection Path.In summary,for each individual reflection path p, the outgoing directiondp from the source, the ordered list Sp of reflected surfaces,and the total path length lp each have distinct effects onrendering the paths contribution. D( dp) characterizes thefrequency response of the source from the paths outgoingdirection. The reflection of each surface s Sp attenu-ates the amplitude of the sound in a frequency-dependentfashion parameterized by Rs. The total reflection-based at-tenuation is the product of the frequency response across alls Sp. Finally, we use the path length lp to compute thetime of arrival tp by dividing the path length lp by the speedof sound. We also use lp to estimate the attenuation of theamplitude due to spherical propagation, where the ampli-tude is inversely proportional to lp, as well as air absorption,",
  ",": "where FFT and IFFT are the Fast-Fourier Transform andits inverse respectively, a[t] is the digital recording of thesine sweep, and l[t] is the digital loopback signal. Notethat we deconvolve the loopback signal from the recording,instead of deconvolving the source signal sent to the speakerfrom the recording. We assume that the loopback signal isthe same as the source signal, but delayed in time by thelatency of the system. Deconvolving from a delayed copy ofthe source signal instead of directly from the source signalthus corrects for the delay in the system. We remove the last0.1 seconds of the 14-second RIR to eliminate anti-causalartifacts. In addition, to account for differences in microphonesensitivity, we adjust the volume of each sweep record-ing according to the sensitivity of the microphone used torecord it. Specifically, we look up each EMM6s micro-phones response at 1000 Hz in dB from its calibrationsheet, and reduce the overall volume of its recordings bythe same amount.",
  "+(1)r": "(5)In this formula, denotes convolution, and P is the setof all paths between the source and listener locations. Asr is intended to capture higher-order reflections, its effectsare likely to become more dominant later in the impulseresponse, whereas the traced paths are intended to char-acterize the early-stage reflections. For this reason, we fit16 points on a temporal spline that interpolates a relativeweighting between the contributions of the late-stage resid-ual and those of explicitly computed reflection paths.",
  "Fitting and Inference": "We estimate the parameters of each acoustic model in theenvironment in an iterative analysis-by-synthesis process.Inspired by and , we optimize according to a multi-scale log-spectral loss comparing rendered RIR W with theground-truth RIR W measured at the same location. Thespecific loss formulation is in Eq. 6 in Appendix E. For inference, we simply compute Equation 5 for a pointat a novel location, computing all the specular paths belowthe maximum order between the source and the novel loca-tion, etc., and using the parameters we determined from theanalysis-by-synthesis process. Binauralization.We train our model on single-channelRIRs recorded using omnidirectional microphones. How-ever, immersive spatial audio requires binauralization - theprocess of converting single-channel audio into left andright channels, in a way that mimics human perception. Theshape of the head, the acoustic shadow it casts, and the dif-ferences in time-of-arrival between the left and right earsall result in distinct perceptual cues that help place the lis-tener in the scene . These effects are typically mod-eled by head-related impulse responses (HRIRs). There is adifferent HRIR for each incoming audio direction. To ren-der binaural audio, the incoming audio from each reflectionpath is convolved with an HRIR sampled from the SADIEII dataset corresponding to its incoming direction. Thisallows our model to approximate perceptually accurate bin-aural audio, which captures the effects of the human head,with merely monaural supervision.",
  ". The DIFFRIR Dataset": "To evaluate methods of rendering and interpolating RIRs,we collect a novel dataset of real monoaural and binauralRIRs and music data in four different rooms, as illustratedin . further summarizes the dimensions andreverberation time measurements of each room. In partic-ular, we choose the following rooms to represent a widerange of room layouts, sizes, geometric complexities, andreverberation effects:1. Classroom. A standard classroom with 13 rectangular",
  ". Hallway. A narrow, highly reverberant hallway, withtwo wooden doors, a tile floor, and drywall ceiling andwalls": "4. Complex Room. A room with an irregular shape thatresembles a pentagonal prism. Portions of the side walland ceiling are covered with acoustic panels. There arethree pillars in the middle of the room, one slanted diag-onally. A portion of the rear wall is glass which is inter-nally covered with paper posters. There are 7 tables, oneof which is in a figure-eight shape. There are exposedair ducts, six hanging lights, water pipes, monitors, andchairs, as well as various large objects, such as a shelf.There is significant ventilation noise. To collect audio recordings, we place a QSC K8.2 Loud-speaker in a particular location and orientation in the roomand play sine sweeps to measure real RIRs in several hun-dred precisely-measured listener locations using a custom-built microphone array. In addition, we play and recordseveral 10-second music clips selected from the Free Mu-sic Archive dataset from the same listener and speakerlocations. The music and RIRs are recorded using multipletime-synchronized Dayton Audio EMM6 omnidirectionalmicrophones, as well as a 3Dio FS XLR microphone, whichfeatures ear-shaped silicone microphones to model humanhearing and captures binaural audio. Additional Configurations.We also collect additionalsubdatasets in some rooms where we slightly modify eachroom configuration. In each such subdataset, we vary thelocation and/or orientation of the speaker, or the presenceand location of standalone whiteboard panels in the room.We use these additional configurations to evaluate zero-shotvirtual speaker rotation and translation, and panel insertionand relocation. We include these evaluations and details onthese configurations in Appendix C. While previous RIRdatasets include varying room configurations",
  ". Experiments": "For each room in our collected dataset, we evaluate our per-formance on the tasks of rendering both omnidirectionalRIRs and music at unseen listener locations. In each roomconfiguration, we select 12 omnidirectional RIRs to trainour model. We then use our model to render RIRs at unseenlocations in the test set, and compare our rendered RIRs tothe ground-truth RIRs using metrics we detail in .1. To simulate music playing in the room, we convolveour rendered RIRs with five different source music files, andcompare the result to real recordings of the same music filesbeing played in the room, across the same metrics. Baselines.We compare our method with nearest neighbor(NN) and linear interpolation baselines, which are widelyused to interpolate RIRs . We also comparewith Deep Impulse Response (DeepIR) and NeuralAcoustic Fields (NAF) , which are both deep-neural-network-based (DNN-based) frameworks. DeepIR predictsthe monaural RIR at novel locations based only on thelocations coordinates, while NAF uses the location com-bined with local geometric features to estimate the RIR.In addition, NAF was originally designed for binaural ren-dering. Thus, we modify NAF to output monaural audiofor the monaural RIR estimation task. We also compareour method with Implicit Neural Representation for AudioScenes (INRAS) , which uses a combination of DNNsto more explicitly model specular and diffuse reflections ata subset of points in a scenes 3D mesh.Additional details on baselines and any necessary adjust-ments we made to them are included in Appendix F.",
  ". Results": "Metrics.We compare rendered audio to ground-truth au-dio using two metrics:1. Multiscale Log-Spectral L1 (Mag). A comparison ofrendered and GT waveforms in time-frequency domainat multiple temporal and frequency resolutions . 2. Envelope Distance (ENV). The L1 distance betweenthe log-energy envelopes of the ground-truth and ren-dered waveforms. Energy decay envelopes are used toextract the decay curve of the RIR, which characterizesthe rooms reverberant qualities . We compute thesignals energy envelope by taking the envelope of thesquared signal .Satoh et al. directly use thislog-energy (squared) envelope of an RIR to measure therooms RT60 reverberation time, which is a commonway of characterizing the rooms acoustics . Analysis.Our results for the base monaural predictiontask are shown in .For the monaural predictiontask, our model significantly outperforms all baselines onour metrics, across all rooms. Results for the binaural pre-diction task are shown in Appendix D.1.",
  "Reflection Amplitudes": "Classroom WallClassroom FloorDampened Wall . Visualization of our models learned parameters. Theleft images show sample spherical heatmaps that our model fits tothe speakers directivity pattern when trained on 12 points fromthe Classroom subdataset. The green dot indicates the directionthe speaker is facing, and the yellow regions indicate higher vol-ume. The right image shows reflection amplitude responses thatour model learns for various surfaces. Directivity Maps.The left side of shows thesource directivity heatmaps at various frequencies, learnedfrom 12 training points in the Classroom subdataset. Thearea near the front of the speaker emits the loudest soundacross most frequencies, as expected. The figures also con-firm that higher frequencies are more directionally emittedthan lower ones, evident in the narrowing yellow directivitybeam with increasing frequency. Additionally, the factthat higher frequencies are typically emitted by the loud-speakers tweeter at the top front of the speaker, is reflectedin our heatmaps, where the yellow regions appear above thespeakers center for higher frequencies.",
  "MagENVMagENVMagENVMagENVMagENVMagENVMagENVMagENV": "DIFFRIR5.220.9422.711.361.210.5551.591.199.132.952.591.254.860.9172.251.41w/o Time-of-Arrival Perturbation5.190.9622.701.431.230.5821.611.369.132.932.601.274.860.9132.231.42w/o Axial Boosting5.190.9692.711.431.220.5551.591.209.142.952.591.304.860.9342.251.44w/o Hop Size 1 Loss5.260.9882.741.411.250.5591.671.169.222.982.601.244.900.9622.271.42w/o Interpolation Spline5.600.9732.721.411.630.5651.531.169.472.922.561.245.240.9202.211.42 . Ablation results from the task of predicting monaural RIRs and music at an unseen point. In the Interpolation Spline ablation, theResidual Component and the contributions from explicitly computed reflection paths are simply added together, instead of being blendedusing the learned temporal spline . Lower is better for all metrics. Errors for RIRs are multiplied by 10.",
  ".RIR loudness heatmaps generated from DIFFRIRtrained on 12 points in the Dampened Rooms base subdataset": "Virtual Rotation and Translation.Since our modellearns physically interpretable parameters, we can simulatechanges to the room layout that are unseen in the trainingdata. In , we train our model on the Dampenedsubdataset, and use it to simulate virtual speaker rotationand translation. We visualize these changes by plotting RIRloudness heatmaps. Since the DIFFRIR Dataset also in-cludes real data where the speaker is rotated or translated,we include quantitative evaluations on virtual speaker rota-tion and translation in the Appendix C.3, as well as evalua-tions on virtual panel insertion and relocation.",
  ". Additional Experiments and Visualizations": "Along with additional RIR loudness maps, Appendix B.2shows that our model can reconstruct the modal structure ofthe soundfield at a low frequency. In Appendix D.2, weshow that our model trained on 6 points outperforms allbaselines trained on 100 points. Appendix D shows that ourmodel is robust to geometric distortions and experimentswith modeling the effects of transmission.",
  "S. Butterworth. On the Theory of Filter Amplifiers. Experi-mental Wireless & the Wireless Engineer, 7:536541, 1930.14": "Diego Caviedes-Nozal, Nicolai A.B. Riis, Franz M. Heuchel,Jonas Brunskog, Peter Gerstoft, and Efren Fernandez-Grande. Gaussian processes for sound field reconstruction.Journal of the Acoustical Society of America, 149(2):11071119, 2021. Funding Information: The authors would liketo thank Manuel Hahmann for the fruitful discussions. Thiswork is part of the MONICA project and has received fund-ing from the European Unions Horizon 2020 research andinnovation program under grant agreement No. 732350. Itis partly supported by the VILLUM foundation (Grant No.19179, Large scale acoustic holography). Publisher Copy-right: 2021 Acoustical Society of America. 13 Angel Chang, Angela Dai, Thomas Funkhouser, MaciejHalber, Matthias Niessner, Manolis Savva, Shuran Song,Andy Zeng, and Yinda Zhang.Matterport3d: Learningfrom rgb-d data in indoor environments.arXiv preprintarXiv:1709.06158, 2017. 3 Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vi-cenc Amengual Gari, Ziad Al-Halah, Vamsi Krishna Ithapu,Philip Robinson, and Kristen Grauman.Soundspaces:Audio-visual navigation in 3d environments. In ECCV, 2020.22",
  "Mandar Chitre. Differentiable ocean acoustic propagationmodeling.In OCEANS 2023-Limerick, pages 18. IEEE,2023. 3": "Samuel Clarke, Negin Heravi, Mark Rau, Ruohan Gao, Ji-ajun Wu, Doug James, and Jeannette Bohg.Diffimpact:Differentiable rendering and identification of impact sounds.In Conference on Robot Learning, pages 662673. PMLR,2022. 3, 6, 7, 20 Orchisama Das, Paul Calamia, and Sebastia V. Amen-gual Gari.Room impulse response interpolation from asparse set of measurements using a modal architecture. InICASSP 2021 - 2021 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages960964, 2021. 13",
  "Rishabh Garg, Ruohan Gao, and Kristen Grauman. Visually-guided audio spatialization in video with geometry-awaremulti-task learning. International Journal of Computer Vi-sion, pages 115, 2023. 2": "Michele Geronazzo, Erik Sikstrom, Jari Kleimola, FedericoAvanzini, Amalia De Gotzen, and Stefania Serafin. The im-pact of an accurate vertical localization with hrtfs on shortexplorations of immersive virtual reality scenarios. In 2018IEEE International Symposium on Mixed and AugmentedReality (ISMAR), pages 9097. IEEE, 2018. 6 Georg Gotz, Sebastian J Schlecht, and Ville Pulkki. A datasetof higher-order ambisonic room impulse responses and 3dmodels measured in a room with varying furniture. In 2021Immersive and 3D Audio: from Architecture to Automotive(I3DA), pages 18. IEEE, 2021. 6",
  "V.D. Landon. A study of the characteristics of noise. Pro-ceedings of the Institute of Radio Engineers, 24(11):15141521, 1936. 4": "Eric A. Lehmann and Anders M. Johansson. Prediction ofenergy decay in room impulse responses simulated with animage-source model. The Journal of the Acoustical Societyof America, 124(1):269277, 2008. 7 Yan Li, Peter F. Driessen, George Tzanetakis, and Steve Bel-lamy. Spatial sound rendering using measured room impulseresponses. In 2006 IEEE International Symposium on Sig-nal Processing and Information Technology, pages 432437,2006. 2",
  "Shu-Nung Yao. Headphone-based immersive audio for vir-tual reality headsets. IEEE Transactions on Consumer Elec-tronics, 63(3):300308, 2017. 6": "Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, andNoah Snavely.PhySG: Inverse rendering with sphericalgaussians for physics-based material editing and relighting.In The IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), 2021. 4 Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-lik, and Alexei A Efros. View synthesis by appearance flow.In Computer VisionECCV 2016: 14th European Confer-ence, Amsterdam, The Netherlands, October 1114, 2016,Proceedings, Part IV 14, pages 286301. Springer, 2016. 1",
  "Please see the supplementary video on the website for anin-depth qualitative analysis and comparative evaluationagainst baseline models. This video showcases a simulation": "of a song played in two distinct environments: the Damp-ened Room and the Hallway. The purpose is to demonstratethe immersive quality and perceptual accuracy of the audiorendered by our model, reflecting the true characteristics ofthe real scenes. To achieve this, we rendered 100 room im-pulse responses at various locations, convolved them withthe chosen source audio, and smoothly interpolated betweenthese convolved signals. For an optimal experience of thesequalitative results, we recommend using earbuds or head-phones while viewing the video. Furthermore, the video features a side-by-side compari-son of our binaural audio results with those from baselinemodels, highlighting the enhanced realism and compellingnature of the audio generated by our model. This compar-ison underscores the significant qualitative improvementsour model offers in creating an immersive auditory expe-rience. In addition, the video provides visualizations ex-plaining our method, and the task setup.",
  "B.1. Broadband RIR Heatmaps": "After our model is trained, we can use it to visualize howthe loudness of the rendered acoustic field varies spatially.To do this, we use the model trained on each of the basesubdatasets to render RIRs on a dense 2D-grid of listenerlocations. We visualize of the root mean square (RMS) vol-ume level of the RIRs in , on a decibel (logarithmic)color scale. The visualizations shown are similar to those in. We observe several differences in the heatmaps for thedifferent rooms. In the Dampened Room, the surfaces areless reflective, and thus, much of the soundfields loudnessis concentrated in the region in front of the speaker. Thiseffect is reduced in the Classroom, where the soundfieldis more spread out. In the Hallway, which is the most re-flective room, the soundfields volume is even more spreadout, and the region behind the speaker is significantly louderthan it is in any of the other rooms. . Visualization of RIR loudness maps generated from our model trained in each of the four base subdatasets. We measure loudnessby rendering an RIR at a given listener location and measuring its RMS volume level. For each RIR rendered, we fix the height of thelistener location to be 1 meter above the floor. The resolution of each xy-grid is approximately 5 centimeters in both the x and y directions.We fix the location and orientation of the speaker (indicated by the black icon) to where it was during RIR measurement. The color scaleis in decibels and is consistent between rooms. The green dots indicate the xy locations of the 12 training points, which are projected ontothe z = 1 plane.",
  "B.2. Soundfield Reconstruction": "When observed at a single frequency, the spatial variationsin sound pressure for a given sound field often exhibit modalpatterns. Reconstructing the pressure levels of a sound fieldfrom a sparse set of observations is a problem of longstand-ing theoretical and practical interest . Usingthe RIRs measured in the Classroom subdataset, we calcu-late the sound pressure level at 70 Hz at all locations in oursubdataset, plotted in a. We also use the predictedRIRs from each method to predict the sound pressure levelat 70 Hz at every spatial location. We find that our modellearns to predict the modal structure of the RIR sound fieldwithout explicitly modeling it, while other baselines fail todo this. Note that our model approximately predicts the lo-cations of the sound fields nodes and anti-nodes (regionsof high and low intensity), even without observing trainingdata in those locations.",
  "To test our methods effectiveness on various room lay-outs, including those where the speaker is occluded": "To evaluate acoustic interpolation methods on the task ofzero-shot generalization to changes in room layouts, byvirtually simulating speaker rotation and translation, andpanel relocation and insertion.The locations and orientations of the speakers as wellas the positions of the panel(s), are provided as part of theDIFFRIR Dataset. Photographs of each additional configu-ration are shown in . Rotation Subdatasets.In the Dampened Room, Hallway,and Complex Room, we collected 120, 72, and 132 addi-tional datapoints where the speaker was rotated by 225,90, and 90clockwise, respectively. The location of thespeaker and all surfaces otherwise remain the same. Translation Subdatasets.In the Dampened Room, Hall-way, and Complex Room, we collected 120, 72, and 132additional datapoints where the speaker was translated toanother part of the room, but the orientation of the speakeris was kept the same. In the Dampened Room, we movethe speaker such that it is near one corner of the room andfacing a wall. In the Hallway, we move the speaker to thefar end of the Hallway, such that the speaker faces the entirelength of the Hallway. The Complex Room is roughly di-vided into two halves by the table and pillars in the middleof the room. In the Complex Room, we collect additionaldatapoints where the speaker is translated from the one half X (m) Y (m) Z = 0.32m X (m) Y (m) Z = 0.98m",
  "(g) DeepIR": ". Visualization of RIR loudness at 70 Hz in the Classroom subdataset. The sound field intensity at a given location is measured byfiltering the ground-truth or predicted RIR around 70 Hz using a 2nd order Butterworth filter and measuring the RMS volume level ofthe filtered signal. Subfigure a) shows the intensity of the 70hz sound field at all locations in the subdataset. Subfigure b) shows predictedintensities at these same locations using our model trained on 12 points. We indicate the spatial locations of these 12 training points withgreen dots, and the speakers location and orientation with a black icon. Subfigures c) through g) show the sound field intensity as predictedby each of our baseline models. Note that in subfigure d), the Linear baseline underestimates the soundfield intensity at locations far awayfrom the training locations, since the linear interpolation at these locations is a weighted average of roughly uncorrelated signals whosemean is roughly zero.",
  "to the other": "Panel Subdatasets.In the Dampened Room and Hallway,we place 1-2 whiteboard panels in the room. In the Damp-ened Panel subdataset, we place the panel directly in frontof the speaker. In the Hallway subdataset, there are threepanel configurations. In Hallway Panel 1, we place onewhiteboard panel in front of the speaker at a slanted an-gle. In Hallway Panel 2, we place one whiteboard paneldirectly behind the speaker. In Hallway Panel 3, we placewhiteboard panels both in front of and behind the speaker.",
  "C.3. Quantitative Results on Virtual Room LayoutModifications": "Since our model learns physically interpretable parametersfor the speakers directivity, we expect to be able to virtu-ally simulate rotations or translations of the speaker that areunobserved in the training data. We simulate rotating thespeaker by rotating the speakers learned directivity map,and translation by moving the speakers estimated locationduring path-tracing.These predicted changes in the speakers location ororientation can be evaluated against real data, since theDIFFRIR Dataset contains additional configurations that",
  "Room/ConfigurationMagENVMagENV": "Damp. Hall. 1.Hall. 1 Model8.472.992.581.32Virtual Insertion9.322.962.691.33Damp. Hall. 2.Hall. 2 Model8.523.612.631.36Virtual Insertion9.313.452.621.38Hall. 1. Damp.Damp. Panel Model1.230.6001.621.47Virtual Insertion1.840.6603.701.56Hall. 2. Damp.Damp. Panel Model1.230.6001.621.47Virtual Insertion1.840.6603.701.56 . Results on Virtual Panel Insertion. Damp.Hall 1.means that take the DIFFRIR model from the Hallway Base sub-dataset (no panel). Then, we virtually insert a panel to simulatethe Hallway Panel 1 subdataset, by borrowing the reflection co-efficients of the panel from the DIFFRIR model trained on theDampened w/ Panel subdataset. We then evaluate the virtual in-sertion on the recordings from the Hallway Panel 1 subdataset. Asa baseline, we compare to a model that is trained on the same panelsubdataset that it is tested on. Virtual Speaker Rotation.As an experiment, we takethe DIFFRIR model trained on each base subdataset witha corresponding rotated subdataset, virtually rotate thespeaker by rotating the learned directivity heatmap, andpredict RIRs and music at locations in each of the corre-sponding rotated subdatasets. We evaluate these predictionsagainst ground-truth RIRs and music recordings from therotated subdatasets. In addition, we compare our virtualrotation with the performance of the DIFFRIR model bothtrained and tested on the rotated subdatasets. The resultsare shown in . Although the model both trained andtested on the rotated subdatasets outperforms our virtually-",
  "Virtual Speaker Translation.We perform a similar ex-periment with virtual speaker translation, evaluating againstground-truth recordings from the corresponding subdataset.The results are shown in": "Virtual Panel Relocation.We would like to see if wecan learn the reflective characteristics of a surface in oneroom, then virtually move the surface to another locationin the same room.In the Hallway, we collect two sub-datasets (Hallway Panel 1 and Hallway Panel 2 in ),where the room layouts are identical except for the locationand orientation of a single whiteboard panel. In our exper-iments, we train on the first panel configuration, then movethe location of the whiteboard panel to that of the secondconfiguration before performing inference. We then eval-uate our predicted audio against ground-truth audio from",
  "the second configuration. Results are shown in .The baseline shown is one where we train on the same sub-dataset that we evaluate on": "Virtual Panel Insertion.We would like to see if we canlearn the reflective characteristics of a surface in one room,then virtually insert the surface into another room. Threeof our base subdatasets also include a version with a sin-gle inserted whiteboard panel. In each of our four exper-iments, we take the base subdataset (e.g., the DampenedBase subdataset), and the coefficients learned for the white-board panel from another room (e.g., the Hallway PanelConfig. 1 subdataset). We then virtually insert the white-board panel into the base subdataset, and evaluate the vir-tual insertion against the version of the base dataset with apanel in it (e.g., the Dampened Panel subdataset). Resultsare shown in . The baseline shown is one where wetrain on the same subdataset that we evaluate on.",
  "D.1. Results on Binaural Rendering": "We evaluate our method on the task of rendering a binau-ral RIR at an unseen location. We collect binaural RIRsat several locations in all rooms using our 3Dio binauralmicrophone, and compare these to predicted RIRs that webinauralize from single-channel audio as described in theMethods section.We compare our binauralized audio with the ground-truth audio using the left-right energy ratio error betweenthe ground-truth and predicted recordings, which is usedin . To compute the left-right energy ratio, we computecompute the ratio of total energy between the left and rightchannels of the RIR or music recordings. We then computethe mean squared error between the left-right energy ratioof the predicted and ground-truth RIRs or music. Resultsare shown in .Since the baselines do not have a way of generating bin-aural RIRs from monaural ones, we binauralize these base-lines by rendering two monaural RIRs at the locations of theleft and right ears of the 3Dio microphone, and combiningthem into left and right channels.Our method outperforms our baselines across most met-rics.Note that it is difficult to compare a binaural RIRrecorded from our binaural microphone with binauralizedaudio originally recorded from a different microphone. Ourrendered binaural audio will have characteristics of themonaural microphone and the microphones used in theSADIE dataset used to record the HRIRs that we con-volve our monaural recordings with. The binaural record-ings in our dataset will h ave different characteristics, sincethey are recorded using a different microphone with dif-ferent spectral characteristics and directionality. Becauseof this, we include qualitative binauralization results in the",
  "D.2. Performance vs Number of Training Points": "We conduct an ablation study with varying numbers oftraining points N on each subdataset and compare againstthe baselines. As shown in , the performance in-creases with N, and our model consistently outperforms thebaselines when N 2. Note that in all rooms, our modeltrained on only 6 locations outperforms all baselines trainedon 100.Note that our models hyperparameters are optimized forperformance in data-limited scenarios. When the numberof training points is higher, it is possible that increasing thenumber of parameters (for instance, increasing the resolu-tion of the heatmap or the number of reflection coefficients)leads to even better performance.",
  "D.3. Robustness to Inaccurate Geometry": "Our method requires measuring the rooms geometry. In ourdataset, we do this using a tape measure or laser distancemeasure, which both provide sufficiently accurate measure-ments. In order to explore the effect of inaccurate geomet-ric measurements, we conduct an additional experiment tomeasure the performance after adding random artificial dis-tortions to the surfaces. In the Classroom, we select 8 ran-dom directions to move each of the 11 vertices defining thewalls, ceiling, floors and the corners of the tables that areexposed. We move each vertex by 0-2 meters in its corre-sponding random direction. Results are shown in .Observe that unless we distort all vertices in the room byover 1.5 meters, our model outperforms the best baseline(Nearest Neighbors). We conclude that our method is ro-bust to geometric distortion.Geometric distortion can affect our models rendering inone of three ways: It can change the distance of reflectionpaths, which affects its time-of-arrival and amplitude; it caneliminate reflection paths, or it can add new reflection paths.Since our model is optimized against a frequency-domainloss whose smallest window size is 256 samples (or 1.8 me-ters at the speed of sound), our model is robust to perturba-tions in times-of-arrival.",
  "D.4. More Ablations": "In the Methods section, Section E.4, and Section E.3,we discuss several minor components of our model (axialboosting, time-of-arrival perturbation, hop size 1 loss, etc.)that provide a boost to our models performance and/or ro-bustness. Results with each of these components ablatedare in . Our model performs the best on a plu-rality of evaluations, proving that these performance boostsare good on balance. However, we should also observe thateven in evaluations where our model does not perform thebest, it is never worse than the best performing ablation by a",
  "RIRMusicRIRMusicRIRMusicRIRMusic": "NN1.270.5165.642.570.0620.0340.3450.166Linear1.290.5315.482.090.0450.0080.3350.157DeepIR1.100.5296.205.900.0480.0360.3500.397NAF1.930.7435.932.370.1080.0120.3200.176INRAS1.250.3835.864.351.604.410.3320.183DIFFRIR (ours)0.430.0912.940.3160.0970.0120.2870.288 . Experimental results from the task of predicting binaural RIRs and music at an unseen point from a model trained on 12 monoauralRIRs. We use the left-right energy ratio error metric . Lower is better. All errors are multiplied by 10. Num. Train Points 0.50 0.55 0.60 0.65 0.70 Spectral Error (Mag)",
  "Complex": "NNLin.NAFINRASOurs . Evaluations of our method and baselines with differentnumbers of training points. We use the Multiscale Log-SpectralL1 Loss (Mag), and train with N {1, 2, 6, 12, 25, 100}. Alltraining locations are selected as nested subsets of one another,and we evaluate on a fixed test set. Note that the DeepIR baselineserror was too large to fit into the range of the plot. significant margin. We cannot say the same for the Interpo-lation Spline ablation, which also performs the best in thesame number of evaluations (six), but significantly under-performs our model in several settings.",
  "D.5. Modeling the Effects of Transmissions": "Our model assumes that sound energy encountering a sur-face is either reflected or absorbed by the surface. This isfor the sake of simplicity. We also conduct an experiment inwhich we consider surface transmission as well. This meansthat we modify our tracing algorithm to consider reflectionpaths that pass through surfaces, and assume that a propor-tion of the sound energy at each frequency can be transmit-ted through these surfaces in a frequency-dependent man-ner. Our modified training procedure then fits surface trans-mission coefficients in a manner identical to the way it fitssurface reflection coefficients. contains quantita-tive results from a model that models both transmission and 0.00.51.01.52.0 Geometric Distortion (m) 5.2 5.4 5.6 5.8 6.0 6.2 Multiscale Log-Spectral L1 Error 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30",
  "Envelope Distance": "Effect of Geometric Distortion Ours (Spectral Error)Ours (Envelope Error)NN (Spectral Error)NN (Envelope Error) . Effect of geometric distortion on RIR prediction per-formance in the Classroom subdataset The blue line shows ourmodels performance according to the Multiscale Log-Spectral L1metric, and the red line shows our models perfomance accord-ing to the envelope distance metric. The red and blue dashed linesindicate the performance of the nearest-neighbors baseline accord-ing to the multiscale log-spectral L1 metric and the envelope dis-tance metric, respectively.",
  "D.6. Comparison to Traditional Acoustic Simula-tions": "We compare our method to a widely-used image-source au-dio simulator, Pyroomacoustics . For each room in ourdataset, we simulate RIRs by providing the dimensions andthe speaker location, and selecting the closest material coef-ficients for each surface from its pre-defined database (e.g.,drywall, ceiling tiles, carpet). reports the accuracyof the simulated RIRs compared to the ground truth.",
  "Our method does not require a ground-truth source location": "measurement. Instead, we use a simple time-of-arrival tech-nique to estimate the sound sources location to a degree ofaccuracy sufficient for the subsequent steps of the method.For each Room Impulse Response (RIR) in the training set,we determine its first peak, which is proportional to the dis-tance of the direct path between the microphone and sourcelocations. We locate the first peak of the RIR by measuringwhen the RIR first exceeds a quarter of its absolute max-imum. We then determine the distance from the source tothe target microphone by multiplying by the speed of sound,assumed to be 343 m/s.We use a gradient descent optimization method to fitthe optimal source location. We initialize the source loca-tion to the origin, which is at a corner of the room. Weperform an optimization process that updates the optimalsource locations position at each step. At each iteration ofthe optimization process, we compute the estimated times-of-arrival for each of the microphone locations, based onthe current estimate for the source location. We then cal-culate the L1 loss between the estimated times-of-arrivaland the times-of-arrival as measured by locating the firstpeak of the ground-truth RIR as described in the previousparagraph. We perform a gradient update on the estimatedsource location to minimize this L1 loss. We optimize for1000 steps, and use the final estimate for the source locationas our estimated source location. In all of the base configurations, our method is ableto predict a source location that is inside the location ofour QSC loudspeaker.We used the estimated locationin all configurations except for the Complex Rotation andComplex Translation configurations, where our localizationmethod failed.",
  "E.2. Minimum-Phase Transform": "Our model learns the frequency-domain response curve foreach of the surfaces in the room and for each outgoing di-rection from the source, allowing us to determine how thefrequency profile of sound traveling along that reflectionpath is altered. However, this frequency-domain response isnot enough to determine the reflection paths time-domaincontribution to the RIR, because it contains magnitude in- formation, but no phase information. In order to invert ourreflection paths frequency profile into a time-domain sig-nal, we need to provide phase values at each frequency, sowe can perform the inverse-Fourier transform. In our analysis, we adopt the minimum-phase assump-tion to calculate phase values for acoustic reflections, amethod widely recognized and justified within acoustic re-search . This assumption posits that for each fre-quency, the phase delay introduced by the reflection is min-imal, implying that the time delay contributed by the pathof reflection at any given frequency is as short as possible.From a physical standpoint, this is akin to assuming thatsound is reflected off surfaces with negligible delay, therebybehaving as if the reflections are instantaneous while stillpreserving the unique frequency-dependent characteristicsof the reflection. We compute the phase values using themethod described in .",
  "E.3. Specific Loss Formulation": "Loss Formulation and Equations.We define the loss fora given short-time Fourier transform (STFT) window sizesw and hop size h in Equation (6).This is the sum ofthe L1 distance between the magnitude-spectrograms of theground-truth and synthesized RIRs and the log-magnitudespectrograms of the ground-truth and synthesized RIRs.In the formula, W and W indicate the ground-truth andpredicted RIRs, respectively. h indicates the hop length,sw indicates the STFT window size, and S is the short-timeFourier transform, or spectrogram, whose arguments are thetime-domain signal to transform, the window size, and thehop length, respectively. H indicates the hop ratio, or thehop length divided by the window size. We set H = 0.25.Equation (7) provides the total loss, which sums acrossmultiple window sizes, and adds a loss term that uses a hopsize of 1. Hop Size 1 Loss.We use a spectral loss term with hopsize 1 to ensure that the early part of the RIR has accu-rate time-domain characteristics, since the hop length of 1allows for high-resolution in the time domain. We take in-spiration from for this term, and discover it improvesperformance, as seen in .",
  "Modifications from Related Work.Our multi-scale": "spectral plus log-spectral loss is identical to those usedin and , with two exceptions: First, is the intro-duction of the loss term with hop size one. Second, theminimum window size in our loss is 256, instead of 32 or64. This is because there will be error in the time-of-arrivalof certain reflection paths, due to geometric measurementerror (which increases with reflection order) or errors in thespeed of sound approximation. This means that the place-ment in time of a reflection paths contribution to our syn-thesized RIR may be off from its placement in the ground- truth RIR by some amount. Using larger window sizes com-pensates for this error, since larger windows are more likelyto contain both the reflection paths contribution to our syn-thesized RIR and its contribution to the ground-truth RIR.",
  "E.4. Small Efficiency and Performance Boosts": "Efficiency Boosts.Since each rendered RIR combineshundreds of reflection paths, we compute all the reflectionpath contributions in parallel to minimize runtime. In addi-tion, all reflection paths for the training points are precom-puted before training starts. Time-of-Arrival Perturbation.Since our measurementsof each room are not necessarily precise, to make our modelmore robust, especially with an extremely limited numberof measurements, we would like to perturb the surfaces dur-ing training. However, reflection paths for all training loca-tions are precomputed before the training process begins.Perturbing each surface would require retracing at each it-eration, which is computationally inefficient. As a proxy tothis, we perturb the time of arrival of all paths by addingGaussian noise to it, with a standard deviation of 7 samples.We found that this improved the interpretability of the es-timated parameters and led to minor perfomance boosts, asshown in . Regularization via Convolution with Pink Noise.SinceRIRs are often used as a means to simulate sounds in anacoustic environment, we would like to not only ensure thatour rendered RIRs are accurate, but also that the sounds wesimulate via convolution with the RIR are accurate. Mini-mizing the spectral loss between ground-truth and predictedRIRs does not always accomplish this, since convolving theRIRs with other waveforms results in significant changes inthe spectrograms.Pink Noise is a special type of noise whose power spec-tral density is inversely proportional to frequency. It is ubiq-uitious in nature , and is often used as a test signal to cal-ibrate sound systems and loudspeakers, since its frequencyprofile is similar to that of music and other sounds thespeaker might play.To encourage our model to maintain accuracy post-convolution, we implement a regularization strategy usingpink noise. For the latter half of training iterations, we con-volve both our predicted and the ground-truth RIRs withfive seconds of randomly generated pink noise, compute theloss between them, and add it to the loss computed betweenRIRs at each iteration. Convolving RIRs with pink noisesimulates the speaker playing of a pink noise test signal. Itis equivalent to reshaping the RIRs spectrum according tothe profile of pink noise, and applying a random phase shiftat each frequency. shows that this form of regularization results in",
  "+ L256,1(W, W)(7)": "improvements in both RIR prediction and music prediction.Such forms of regularization should be the study of futurework and theoretical study.With the goal of improving rendered music in mind,we also tried a similar form of regularization, where weconvolve both our ground-truth and predicted RIRs withfive seconds of music randomly sampled from the FMAdataset at each iteration after training is halfway done.Convolution with the music files simulates the speaker play-ing them. Results for this form of regularization are alsoshown in , although we prefer the performance andsimplicity of pink noise regularization.",
  "E.5. Computational Cost": "Training and Path-Tracing Time.In all of our experi-ments, we trained our model for 1000 epochs. In ,we report the amount of time it took for our model to trainon each of the base room configurations. Note that sincethe Complex Room is only traced up to order 4, there aresubstantially fewer valid reflection paths, and thus trainingis faster. In all other rooms, we trace up to order 5. Tracingis slower in rooms with more surfaces. Main Contributions to Training Time.We also mea-sured the different steps in the training process to see whichones took the longest. Each training location is associatedwith hundreds of reflection paths that must be added to-gether to form the the RIR. While rendering these contri-butions is done in parallel, compiling them requires plac-ing them in at the right locations in time and is done se-quentially. In practice, 37.7% of the time during the 1000epochs is spent on this compilation, 61.9% on the back-wards passes, and 0.4% on everything else.",
  "F. Baseline Implementation Details": "Linear.The Linear baseline computes a RIR at a giventest location by taking a linear combination of the fournearset points in the training data. The weights on each ofthese four training points are inversely proportional to thedistance to the test location. We also experimented with tak-ing a weighted combination of all the training data, wherethe weights are inversely proportional to distance. This al-ternative linear baseline performs quite poorly, with errorincreasing with the number of training points. This is be-cause the training RIRs are roughly uncorrelated with mean",
  "zero, so the average of N RIRs tends towards zero as Nincreases": "Neural Acoustic Fields (NAF) .To compare ourmethod to NAF, we utilized NAFs official code,1 as open-sourced by authors. However, in order to apply NAF to ourdataset and experimental settings, we modified this code insome minor ways. Specifically, the original NAF was de-signed to estimate arbitrary stereo RIRs constrained to lieon a 2D horizontal plane within a 3D room, i.e., it did notconsider a z-axis and thus does not output RIRs at arbi-trary heights. Therefore, we added the height on the z-axisas an input, embedding it by using the same positional en-coding as the authors code. The correspondingelements of the network architecture, e.g., the number ofunits in the input layer, were also modified. The architec-ture we used for NAF in our experiments consisted of 8linear layers with Leaky-ReLU activations . Note thatwe only changed the number of the number of units in theinput layer, from 126 to 168, due to the aforementioned ad-dition of z-axis features. In addition, the NAF we used inour experiments was designed to output only magnitude-spectrograms, i.e., without any phase information, becausethe official code also does not have the phase-related lossand corresponding phase output. We utilized the Griffin-Lim algorithm to estimate the phase of each magnitudespectrogram and render the time-domain RIRs. For train-ing, we followed the same process in their official code andused the models weights after the final training epoch forinference and evaluation. Finally, we used a 48000 Hz sam-ple rate rather than the original 22050 Hz. All other settings,such as the optimizer, number of epochs, learning rate, etc.,are the same as their official implementation. Deep Impulse Responses (DeepIR) .Unlike NAF,the authors of DeepIR have not open-sourced an officialcodebase. Therefore, we implemented DeepIR ourselves,based on the details in their paper. Specifically, we built asimple multi-layer perceptron (MLP) consisting of 6 linearlayers, each followed by leaky-ReLU activations. The inputfeature vector consists of (x, y, z, t), which are the desiredspatial coordinates and the time index, respectively. Similarto NAF, we applied positional encoding to all inputs beforepassing them into the MLP. Hence, the number of units inthe input layer is demb, whereas all other layers have 512",
  "Classroom9.610.9094.3874Dampened5.750.5660.83675Hallway8.970.9061.5853Complex2.820.373347439": ". In all of our experiments, we train our model for 1000 epochs and report the training time that this takes in each room, in thebase configuration. In addition, we report the inference time, or the time it takes our model to render a single RIR. Before training begins,we precompute the reflection paths that go between the source and listener locations, up to a certain maximum reflection order, so we alsoreport this tracing time to trace reflection paths, per listener location of each room and its corresponding subdataset. We also report thenumber of valid reflection paths found by the tracing algorithm, as an average across all points in the subdataset. units. DeepIR directly outputs the tth time sample of theRIR to produce an estimate IR of the full RIR. We thenconvolve this with the arbitrary dry source audio x, to pro-duce an estimate y of the sound of the arbitrary audio beingrecorded from the specified source and listener location inthe room, i.e., y = x IR. We optimized y according toan L2 loss comparing the log-magnitude spectrogram withthat of the corresponding ground-truth audio ygt. We omit-ted the noise model, since our dataset did not include ar-tificially added noise, and the noise in our recordings wasminimal. We set other hyperparameters for DeepIR such asthe optimizer, learning rate, the number of epochs, etc., tothe same values as NAF. Implicit Neural Representation for Audio Scenes (IN-RAS) .The authors of the INRAS baseline releasedtheir code in the Supplementary Materials of their submis-sion.2 We use their code with some minor modifications.The framework is originally trained and tested on data fromthe SoundSpaces dataset , which provides simulatedbinaural recordings within virtual environments. The ar-chitecture is built around consuming this data, where eachsimulated recording represents a stereo, binaural recordingwith the head positioned at one of the four cardinal an-gles. Our training sets use exclusively monaural record-ings from omnidirectional microphones. Thus, in order tomake our changes to the network as minimal as possible, weduplicated our mono-channel recordings to stereo-channel recordings and assumed them to all be at the 0 angle. Wethen took only the left channel of the stereo output as theframeworks estimate of the monaural RIR. Since INRASconsumes environment meshes, we provide it with a 3Dscan of each room. Otherwise, we used mostly the samehyperparameters as the original, with the exception that weincreased the sample rate from 22050 to 48000 Hz. Sinceour training set of 12 recordings per subdataset was approxi-mately four orders of magnitude smaller than the datasets onwhich the authors had trained, we increased the initial learn-ing rate from to 0.001 instead of 0.0005, slowed the learningrates exponential decay schedule to decay rate = 0.1 over3000 epochs rather than 50, and trained for 5000 epochsrather than 100. We evaluated the model against a valida-tion set every 100 epochs. For our test evaluations of IN-RAS, we used the weights and consequent outputs of themodel with the best performance across all such validationevaluations.",
  "G. Data Collection Procedure Details": "We use a custom-built microphone frame designed to ac-commodate 12 Dayton Audio EMM6 measurement micro-phones, as well as one 3Dio FS XLR binaural microphone,all of which were rigidly mounted at precisely measuredpositions on the frame. shows a photo of the mi-crophone frame used to collect the data. We set the originof each room such that there is one wall representing x = 0and one wall representing y = 0. Before each recording,we positioned the frame within the room and measured the",
  ". A photo of the data collection procedure in the Damp-ened Room. The custom microphone frame holds 12 microphones,as well as a 3Dio FS XLR binaural microphone": "distance from the edge of the frame to each of the originwalls using a tape measure or a Bosch GLM20 laser dis-tance measure, which have 1 and 3 millimeter measurementresolutions, respectively. We use the measured position ofthe frames corner as well as the pre-measured offset of eachmicrophone from the frames corner in order to annotateeach microphones position in the room to sub-centimeterprecision for our dataset.",
  "G.1. Estimating the Room Impulse Response (RIR)": "In order to measure each RIR, we played a logarithmic sinesweep through the speaker. The sweep spanned from 20 Hzto 24 kHz for 10 seconds, followed by 4 seconds of silence.This sine sweep was recorded from each of the microphonessimultaneously at each gantry position. While sending thesine sweep signal from the audio interface to the speaker,we also recorded loopback signal by wiring the audio in-terfaces output to one of its inputs. We used this loopbacksignal to estimate and correct for the latency in the system.",
  "G.2. Room Geometry Estimation": "As the wavelengths of audible sound typically range from2 cm - 17 m , the prominent sound waves are likely tobypass or diffract around smaller surfaces. Hence, we onlyfocus on modeling salient surfaces (e.g., walls, pillars, tabletops), which are often characterized by planes, and simplytrace the reflection paths using image source methods. Forthe rooms we captured in our dataset, we also measured thewalls and surfaces and manually created planar mesh-basedreconstructions of them. With the recent progress in visual3D scene reconstructions , our geometric estimation canalso easily be replaced by automatic algorithms or even ma-ture customer tools such as Polycam. X (m) Y (m) NearCornerSpread",
  "H. Guidelines for Microphone Placement": "To maximize efficiency, we found it empirically beneficialto spread our RIR locations in all three dimensions. Thisallows us to 1) cover a variety of angles around the speaker,which likely leads to better speaker directivity estimates, 2)disentangle the effects of individual reflections, and 3) bet-ter estimate the diffuse sound field, which is approximatedin our model as spatially uniform.To study this effect, we conducted an experiment inthe Classroom subdataset.We select three different setsof training locations (shown in ), each of whichcontain 6 RIR recordings from 6 different locations. Forsimplicity, these training locations were selected in the 2Dplane defined by Z = 0.98. We evaluated DIFFRIR trainedon each of these sets of training locations on a test set com-prised of other points selected in the Z = 0.98 plane.Our best performance across all metrics is achieved inthe Spread configuration of training points, confirmingour intuition. Interestingly enough, the Near Configura-tion performed the worst. We believe this could be due tothe model overfitting to the near-field of the speaker ,which can be substantially different than the sound field atother locations in the room."
}