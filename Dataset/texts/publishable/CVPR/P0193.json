{
  "Abstract": "Scene coordinate regression (SCR) methods are a fam-ily of visual localization methods that directly regress 2D-3D matches for camera pose estimation. They are effec-tive in small-scale scenes but face significant challenges inlarge-scale scenes that are further amplified in the absenceof ground truth 3D point clouds for supervision. Here, themodel can only rely on reprojection constraints and needsto implicitly triangulate the points. The challenges stemfrom a fundamental dilemma: The network has to be in-variant to observations of the same landmark at differentviewpoints and lighting conditions, etc., but at the sametime discriminate unrelated but similar observations. Thelatter becomes more relevant and severe in larger scenes.In this work, we tackle this problem by introducing the con-cept of co-visibility to the network. We propose GLACE,which integrates pre-trained global and local encodingsand enables SCR to scale to large scenes with only a sin-gle small-sized network. Specifically, we propose a novelfeature diffusion technique that implicitly groups the repro-jection constraints with co-visibility and avoids overfittingto trivial solutions. Additionally, our position decoder pa-rameterizes the output positions for large-scale scenes moreeffectively. Without using 3D models or depth maps for su-pervision, our method achieves state-of-the-art results onlarge-scale scenes with a low-map-size model. On Cam-bridge landmarks, with a single model, we achieve 17%lower median position error than Poker, the ensemble vari-ant of the state-of-the-art SCR method ACE. Code is avail-able at:",
  "*Equal contribution": "Map Size (MB) Position Error (cm) DSAC* (Full)ACEPoker (ACEx4)Ours Map Size (MB) OursACE Ensemble . Left: Quantitative comparison of map size and positionerror with state-of-the-art SCR methods on Cambridgelandmarks . Our method outperforms DSAC , ACE and Poker (4 ACE models) with a moderate model size. Right:Relationship between map size and position error. Note that ourmethod with the smallest map size (3.2 MB) still performs betterthan Poker (4 ACE models, map size is 16.0 MB). Currently, most state-of-the-art localization methods arestructure-based , including featurematching based methods and most scene coordinate regres-sion methods. Both techniques have in common to buildmaps from images with known poses. For localization theyestablish matches between 2D pixel positions in the queryimage and 3D points in the maps. Finally, embedded intoRANSAC , a Perspective-n-Point (PnP) solver is used to predict the camera pose from the 2D-3D corre-spondences. Both methodologies differ in the representa-tion of the map and the estimation of correspondences.Given a database of images, methods based on featurematching typically represent the 3Dscene by reconstructing the 3D geometry, e.g. point cloud,using structure-from-motion (SfM) . At test time, theyestablish 2D-3D matches between pixels in a query imageand 3D points in the 3D model using descriptor matching.However, these methods need to store point-wise visual de-scriptors for the whole point cloud, which may cause stor-age issues when the scenes scale up.In contrast, scene coordinate regression (SCR) meth-ods implicitly encode the map infor-mation inside a deep neural network. Instead of comput-ing 2D-3D matches via explicit descriptor matching, these",
  "arXiv:2406.04340v1 [cs.CV] 6 Jun 2024": "methods directly regress the matches. Though achievingsuperior performance in small scenes , it is difficult toscale these methods to large-scale scenes due to the limitedcapacity of a single network . A common solution isto train multiple networks on sub-regions of the scene .But this certainly increases the model size, training time,and query time. Recent works avoid the need fordepth maps or a complete 3D model for training. In ad-dition, ACE proposes a method to train a 4MB-sizednetwork in 5 minutes, while achieving state-of-the-art per-formance for smaller scenes . Although it has impres-sive efficiency, ACE still possesses the same problemof scaling to larger problem sizes and requires the use ofan ensemble of networks for large-scale scenes , whichlessens efficiency and practicality.In this work, we propose GLACE, a novel methodthat enables the scene regression methodology to workon large-scale scenes with only a single network.Ourmethod achieves state-of-the-art results on several large-scale datasets while using only a single model ofsmall size and without using 3D models for supervision.Our contribution can be summarized as follows:i) To our knowledge, our method is the first attempt ofan SCR method to achieve state-of-the-art performance onlarge-scale scenes without using an ensemble of networksor 3D model supervision.ii)We propose a novel feature diffusion technique for thepre-trained global encodings that implicitly groups the re-projection constraints with co-visibility, which avoids over-fitting to trivial solutions.iii) We propose a positional decoder that parameterizesthe output positions for large-scale scenes more effectivelythan previous work.",
  ". Related Work": "Pose Regression. Pose regression approaches encode the scene into a neural net-work and are trained end-to-end. At test time they regressan absolute or relative pose from a query image.With-out geometric constraints, absolute pose regression meth-ods usually do not generalize well to novelviewpoints or appearances. Besides, these methods do notscale well when limiting network capacity . Operat-ing differently, relative pose regression methods regress a camera pose relative to one or more database im-ages. While being scene-agnostic, they are often limited inaccuracy. Feature Matching Based Localization.Localizationmethods based on feature matching (FM) are often still considered state-of-the-art for vi-sual localization. Those methods establish 2D-3D corre-spondences between pixels in a query image and 3D points in a scene model using descriptor matching. To scale tolarge scenes and handle challenging problems, such as day-night illumination change and seasonal change, these meth-ods first perform a form of coarse localization. For instance,using image retrieval , to first identify a small set ofpotentially relevant database images and only then performdescriptor matching with the 3D points visible in these im-ages. However, these methods need to store all the descrip-tor vectors of the 3D model to perform matching, whichmay cause storage issues for large maps. Recently, severalworks try to avoid storing descriptors explicitly andinstead propose to match directly against the geometry, e.g.,given as point cloud or mesh. Scene Coordinate Regression. Given a query image, thisfamily of localization methods regresses for a 2D pixel thecorresponding 3D coordinates in the scene . Usually,these methods implicitly store the information about thescene within the weights of a machine learning model. Toregress 2D-3D matches, SCR methods are mainly based onrandom forests or convolutional neu-ral networks .Recently, ACE only uses posed RGB images for mapping. The trainingis performed only from the images using a loss based onthe image reprojection error while completely avoiding theexplicit reconstruction of a 3D model. It achieves state-of-the-art performance on several small-scale scenes ,and demonstrates impressive efficiency in training time andmap size. However, a single model based on SCR is usu-ally limited to only working on scenes of small-scale .Larger scenes require techniques like an ensemble of SCRnetworks to scale, which demands additional main-tenance, training time, and memory. In contrast, our methodscales SCR methods to large-scale scenes without requiringan ensemble of networks or 3D model supervision.",
  ". Method": "In this section, we first introduce the basic concepts forscene coordinate regression with ACE . We follow bydiscussing how the system performs implicit triangulationwhen training without ground truth scene coordinate su-pervision and describe challenges in large-scale scenes forSCR methods. We conclude by introducing co-visibility toSCR in the form of global encodings and explain how weeffectively enable the network to utilize this information.Finally, we discuss our novel position decoding techniquethat removes a bias in the SCR toward producing solutionsnear the center of training camera positions.",
  "Visual Localization. We consider visual localization fromRGB images. For training, we require a set of images withcorresponding ground truth poses{(Itrain, htrain)}, where": ". Pipleine of GLACE. Besides the buffer of ACE lo-cal encodings, we extract global features of training images withimage retrieval model . During training, we sample a batch oflocal encodings, look up their global encoding according to theirimage index and perform feature diffusion by adding Gaussiannoise. The global and local encodings are concatenated as inputto an MLP head. The output of the MLP is further processed bya position decoder to yield the final coordinate predictions. Theglobal encoding with feature diffusion facilitates the grouping ofreprojection constraints, enabling effective implicit triangulationin large-scale scenes. Best viewed when zoomed in. htrain denotes the rigid transformation from world coor-dinates to camera coordinates. During testing, our systemestimates the camera pose htest for a query image Itest. Tothat end, we follow the SCR methodology. Specifically, wemainly consider the setting with a pretrained local featureextractor and without ground truth scene coordinate super-vision, established by ACE . We first briefly review theSCR pipeline. SCR Pipeline. SCR methods belong to the structure-basedmethods, which first predict 2D-3D correspondences andthen solve for the pose with PnP and RANSAC. Traditionalstructure-based methods usually explicitly store a triangu-lated point cloud with corresponding features and matchthem with query image features to obtain 2D-3D correspon-dences. Instead, SCR methods implicitly learn the 2D-3Dcorrespondence, usually in a convolutional neural network,which outputs the corresponding 3D coordinate for each im-age patch:",
  "e(xi, yi, h) = ||xi (K h yi)||1,(2)": "where h is the ground truth pose of the image K is the cam-era intrinsic matrix and performs the mapping from ho-mogeneous to pixel space. The reprojection loss is usuallycombined with a robust loss function to reduce the influ-ence of outliers. We use the dynamic tanh loss introducedin ACE :",
  "(3)": "where V is the set of valid predictions, defined as points thatare between 0.1m to 1000m in front of the camera and havea reprojection error e(xi, yi, h) less than 1000px. yi is thepseudo ground truth scene coordinate defined by the inverseprojection of the pixel with the ground truth pose and a fixedtarget depth at 10m. During training the threshold (t) isadjusted dynamically based on the relative training time t:",
  "t2max + min.(4)": "Reprojection Loss as Implicit Triangulation.In stan-dard reconstruction, 2D-3D correspondences are explicitlyestablished through matching. Observations of the same3D point are grouped into a track, and the 3D point is tri-angulated by minimizing their reprojection error. In con-trast, in SCR methods such as ACE and ours, thereis no explicit grouping of 2D observations for the same3D point. Instead, each 2D observation independently re-gresses to a 3D point. Though initially seems like an under-determined problem, these methods demonstrate practicalefficacy, which we attribute to an implicit triangulation pro-cess. This process is driven by the inherent prior of neuralnetworks to deliver smooth functions , where simi-lar inputs tend to produce similar outputs and undergo sim-ilar supervision. Thus, the reprojection loss for similar in-puts is collectively minimized, leading to the triangulationof their corresponding output points. This insight explainsthe practical success of such methods, but also underlinesthe problem of applying SCR on large maps, which possessunrelated, yet, visually similar image observations and pro-vides the motivation for our feature diffusion techniques.",
  ". Global Local Encoding": "Challenges in Large-scale Scenes.SCR methods pos-sess state-of-the-art accuracy in small indoor scenes. How-ever, they struggle in larger environments, especially whenno ground truth scene coordinate supervision is available,and the network needs to perform implicit triangulation ofthe coordinates from scratch. Consider the trade-off be-tween invariance and discriminative power. Specifically, thedilemma of the receptive field. A model with a smaller re-ceptive field satisfies the invariance assumption better and can generalize to new observations, but suffers from am-biguity when there are different locations with similar lo-cal appearances, which intuitively occurs more frequentlyin large-scale scenes. On the other hand, a model with alarger receptive field may be able to disambiguate similarpatches in different locations, but this breaks the invarianceassumption: the network will also distinguish observationsof the same scene coordinate. This can lead to overfitting totrivial solutions, e.g., producing an arbitrary point along theray, instead of triangulating the point from different obser-vations. This also leads to poor generalization, since novelviews of a point observed in training cannot be associatedwith it anymore. Global Encoding. In order to solve the dilemma, we pro-pose to carefully introduce global information, only includ-ing what is necessary. First, we analyze what exactly isneeded from global information. Without global informa-tion, ambiguous patches that may belong to different scenepoints will together, via the reprojection loss, affect the tri-angulation of the same point. Also, the robustness in theloss function Eq. 3 can only mitigate, but not solve suchproblems. Therefore, we need global information to effec-tively group the reprojection constraints. Specifically, weonly want to triangulate points in two images if and onlyif they are looking at the same thing, i.e. the views sharesufficient co-visible structure. To effectively measure co-visibility, we utilize a global feature from an image retrievalmodel R2Former , pretrained on MSLS dataset andsupervised by triplet margin loss with margin m:",
  "Lretrieval = max(||EqEp||2||EqEn||2+m, 0), (5)": "where Eq, Ep, En are global features of query, positive, andnegative samples. The global features are 256-dimensionalvectors normalized to the unit sphere.Here, we furtheranalyze the relationship between feature distance and co-visibility using the SfM reconstruction of a large scene. Agenerative modeling analysis in depicts the distribu-tion of the angular feature distance() d = 180 arccos(uv)conditioned on the number of co-visible points. It stronglyreminds us of a mixture of Gaussians, where the distributionof co-visible pairs possesses a lower mean. The discrimina-tive model in shows that the conditional probabilityof co-visibility c conditioned on feature distance d resem-bles P(c|d) Bernoulli(p), where the parameter p equalsa sigmoid-like function of the feature distance. p is highbefore a threshold, then starts to decrease quickly to a lowlevel afterward, which implies that it is possible to discrim-inate co-visibility based on the feature distance. Naive Concatenation. With the co-visibility informationcontained in the global encoding, we still need to effec-tively integrate global and local features. First, considerthe naive concatenation. In our discussion above, we as- 0.00 0.01 0.02 0.03 0.04 =60.42=8.43 =48.95=9.61 N = 15 P(d | n < N) P(d | n N) =60.37=8.45 =46.67=9.57 N = 100 P(d | n < N) P(d | n N) . Distribution of angular feature distance(), conditionedon co-visibility. Two images are considered co-visible, if the num-ber of co-visible points n at least reaches a threshold N. The x-axis depicts the angular distance d in degrees (left: N=15, right:N=100). 0.0 0.2 0.4 0.6 0.8 1.0 N = 15 P(n N | d < D) N = 100 P(n N | d < D) . Distribution of co-visibility conditioned on the angularfeature distance(). Two images are considered co-visible, if thenumber of co-visible points n at least reaches a threshold N. Thex-axis depicts the angular threshold D (left: N=15, right: N=100). sume that patches with similar input encoding will trian-gulate the same point together. When we concatenate lo-cal and global encoding together, inputs will triangulate thesame point when both the local and global encoding aresimilar. However, as shown in , the feature distancebetween co-visible image pairs, although generally smallerthan non-covisible pairs, may still be quite large. Intuitively,views of the same point with some angle between them willonly partially overlap and thus possess global descriptorsthat do not match as well as local descriptors, and the con-catenated descriptors of matching patches will not have asmall distance as before. Those images that have almostthe same global encoding, possess a very similar pose, witha small baseline, and contribute only little to the triangu-lation. Hence, if we simply concatenate global local en-codings together, only a few images with small baselinesare grouped together, which leads to large triangulation er-rors. Furthermore, the network might struggle to associateunseen views (w.r.t. to spatial coverage) during testing andgeneralize badly.",
  "Explicit Clustering. A simple idea to solve this problem is": "to explicitly cluster the global features, associate each fea-ture with its cluster center, and use this as global encoding.This forces a grouping into hard clusters of features withthe same global encoding. However, this hard clusteringapproach requires to decide on an appropriate number ofclusters. The number has to be large enough to ensure eachcluster has a sufficient number of observations per point fortriangulation and small enough to avoid ambiguous localencodings within a cluster, as shown in Tab. 6. Implicit Grouping with Feature Diffusion. We proposea novel feature diffusion technique to perform the groupingimplicitly. The idea is simple: instead of using a single fixedglobal feature for each image, we add some noise to make ita distribution. For the simplicity of sampling, we add Gaus-sian noise with a standard deviation of = m, where m isthe margin for the image retrieval loss in Eq. 5. After addingthe noise, the encoding is mapped back to the unit sphere.This method can be viewed as a form of feature metric dataaugmentation that imposes a stronger smoothness prior onglobal encoding, which prevents the neural network fromeasily discriminating co-visible pairs, thereby promotingimplicit triangulation. Distinct from traditional image met-ric augmentations that typically involve alterations in the in-put image space, such as color jittering. Our approach oper-ates directly within the feature space, where distances moreaccurately reflect covisibility relationships. The choice ofhyperparameters, grounded in the metric space propertiesof the pretrained encoder, eliminates the need for scene-specific tuning, thereby ensuring robust performance acrossdifferent scenes.",
  ". Position Decoding": "Research shows that the final layer has an importanteffect on the prior of CNNs that regress spatial positions,if the direct output of the last linear layer is a linear com-bination of bases in its weight. Therefore, it is importantto effectively parameterize the final position by the networkoutput, especially when there is no ground truth scene co-ordinate supervision, and we rely on the prior of the modelto perform implicit triangulation. The network output ofACE ( d, w) defines an offset in homogeneous coordi-nates from the center of training camera positions c:",
  "S1max is the parameter for thesoftplus. It can better parameterize points at different scales,": "Camera positions 50 cluster centersMean camera position ACE Ours coarse Ours fine . Comparison between decoder output of random Gaus-sian input samples. We use 50 cluster centers in this example ofthe Aachen dataset, shown in the top left (cluster assignments arecolor-coded, and cluster centers occur as red star). but still suffers from an unimodal prior, preferring localiza-tion near the center c (, top right). Here, we proposean effective position decoder that predicts a convex combi-nation of cluster center positions to replace the fixed centerc in Eq. 6. We use K-Means to distribute the training camerapositions into k clusters with centers {ci}. The final linearlayer of our MLP outputs k logits {si}, one for each clus-ter center and one homogeneous coordinate with parametersd, w to define an offset. The final output is calculated simi-larly to Eq. 6. We only replace the center of training camerapositions with the convex combination (using the softmaxof logits) of cluster centers:",
  "j esj ci.(8)": "We demonstrate the idea of our model and compare it tothe encoding of in . We sample from a Gaus-sian distribution as input and compare the decoded outputfor different decoders. Because of the unimodal prior ofthe ACE decoder, most of the samples are concentrated atthe center. As a convex combination of clusters centers ourmodel is inherently multimodal, but the samples are stillconcentrated at the modes. After adding the offset, the sam-ples are distributed more evenly (, bottom right). Al-though the output of an MLP may not be a simple Gaussiandistribution, this still can show that our decoder can betterparameterize the output. We also designed a simple toy ex-",
  ". Integrated rooms dataset evaluation with SfM poses.We report the percentage of frames below a 5cm, 5 pose error": "periment in supplementary material using a simplified 2Dtask that predicts the coordinates of the center pixel of a 2Dimage patch. The results show that even with strong super-vision, the original decoder cannot regress the coordinatewell when the scale is large. In contrast, with the help ofour positional decoder, the performance is improved signif-icantly. Please refer to the supplement for details.",
  "Scenes and 12 Scenes are two standard datasetsfor room-scale indoor RGB-D localization. They contain 7": "and 12 scenes respectively, each with a set of RGB-D se-quences. There are two sets of ground truth poses for eachscene, one from SfM and one from depth-based SLAM.Since they both have some bias , we report results onboth of them following prior work . To evaluate local-ization in large-scale indoor scenes, previous works have proposed to integrate multiple rooms from 7 Scenesand 12 Scenes into a single scene, denoted by i7, i12, andi19. We strictly follow , placing the scenes inside a 2Dgrid with a cell size of 5m.Cambridge Landmarks is a large-scale outdoordataset, with RGB sequences of landmarks in Cambridge. Itincludes ground truth poses and a sparse 3D reconstructiongenerated via SfM. The dataset is notable for its large-scaleand outdoor setting, providing a different set of challengescompared to small-scale indoor datasets.The Aachen Day-Night dataset is a city-scaledataset, which is particularly challenging for SCR methodsdue to its large scale and sparsity. It contains only limitedimages of Aachen city and ground truth poses provided viaSfM. Here, we only consider Aachen Day, because there isno night-time training data.",
  ". Implementation": "Architecture. We implement our method in PyTorch basedon the official implementation of ACE . The MLP ar-chitecture is the same as ACE, except that the networkwidth is adjusted to match the input dimension of concate-nated encoding. In addition, we use more residual blocksand increase the hidden size of the residual block for largeoutdoor scenes such as Cambridge and Aachen to increasemodel capacity, while still maintaining a comparable mapsize as baseline methods. We also tried concatenating theSuperpoint descriptor to the original ACE local en-coder for the Aachen dataset to provide a more discrimi-",
  "native local descriptor": "Training. Most of the training parameter choices are thesame as ACE, but we use larger buffer sizes for largerscenes, because there is more training data to be cached.In addition, we also use a larger batch size. As shown inSec. 3.1, the reprojection supervision acts as an implicit tri-angulation. Therefore, it is desirable to have multiple ob-servations of the same point in one batch to get stable andaccurate supervision. In order to cache these larger buffers,we use distributed training with multiple GPUs. Specifi-cally, we use a batch size of 160K and a training buffer sizeof 64M for the Cambridge dataset, a batch size of 320K anda training buffer size of 128M for Aachen and i19. For theSuperpoint version on Aachen, we also perform impor-tance sampling according to its corner detection likelihoodin order to select more salient structures. We train 30k it-erations for Cambridge and 100k iterations for Aachen andi19.",
  "Scenes and 12 Scenes. As indicated in Tab. 1, our ap-proach retains the benefits of accuracy and compact mapsize observed in SCR methods when applied to small room-scale scenes": "Integrated Rooms. As shown in Tab. 2 and Tab. 3, pre-vious SCR methods need a much larger map size, or de-mand an ensemble of networks in order to achieve satis-factory performance on large indoor scenes. Our methodachieves comparable performance by a single model with amuch smaller total map size. During test time, we only needto query a single model instead of all the ensemble models,which also makes our method more efficient and practical. Cambridge Landmarks. This real-world outdoor datasetcan fully demonstrate the advantages of our method. Asshown in Tab. 4, our method significantly outperforms state-of-the-art SCR methods and closes the gap withFM methods . Particularly, the largest scene in thisdataset, GreatCourt, is very challenging for SCR methods,but our method can still achieve comparable performance toFM methods with a small model size. Aachen Day. We also evaluate our method on the Aachendataset. The challenges of this dataset are not only the scalebut also the sparsity. There are only about 4K discrete im-ages for a city-scale scene, while the other datasets consistof several sequences with thousands of images for a smallscene. Previous methods usually rely on the ground truthscene coordinate supervision, however, we can still achievecomparable results without ground truth scene coordinatesupervision and a much smaller map size.Other meth-ods that also feature small map sizes and no scene coor-dinate supervision will fail with a similar map size as ours.They cannot achieve a similar performance even with an en-semble of 50 models. In addition, we also tried concatenat-ing SuperPoint features to the original ACE localfeatures to increase the discriminative power and achievedbetter performance with smaller map size.",
  ". Ablation of Global Encoding. Performance of GLACE on the Cambridge Landmarks with different kinds of global encodinginput. We report median rotation and position errors": "we use K-Means to cluster the global encoding to certaindiscrete center values, we can explicitly force the groupingof the reprojection constraints. However, it is non-trivial tochoose a suitable number of clusters, which may require alot of tuning. In contrast, our feature diffusion techniqueachieves the best performance and additionally avoids tun-ing any hyperparameter. Decoder. In Fig 6, we show the performance of our methodwith different numbers of decoder clusters K on the i19dataset with SfM ground truth. When K = 1, which isequivalent to the original ACE decoder, the networkhas an unimodal prior, which only learns the center sceneswell and almost completely fails on several border scenesthat are away from the center. When we increase the num-ber of decoder clusters, the model is allowed to better pa-rameterize a multimodal distribution and have increasingperformance in border scenes. Note that different from theensemble methods that splits the scene into clustersand trains multiple models, our method of increasing thenumber of decoder clusters only needs to add a few outputchannels for the last linear layer and shows no significantincrease in inference time and model size.",
  ". Conclusion": "In this paper, we have presented GLACE, a novel scene co-ordinate regression method that is able to work on large-scale scenes with a single network and without ground truthscene coordinate supervision. We propose a feature diffu-sion technique that effectively utilizes co-visibility informa-tion in the form of global encoding from image retrieval net- 0.089.779.90.00.0 52.999.072.876.514.3 33.693.3100.093.20.0 92.889.889.116.5 K=1 Mean=57.5% 21.8100.091.838.772.7 84.6100.087.199.193.9 77.594.8100.095.694.2 97.592.593.732.9 K=4 Mean=82.6% 22.799.899.299.6100.0 98.2100.096.698.893.3 75.999.3100.095.795.8 96.591.893.140.9 K=19 Mean=89.3% 99.2100.099.699.694.2 98.1100.090.298.293.3 98.695.8100.095.094.8 96.090.593.439.0 K=50 Mean=93.4%",
  "A. Network Architecture": "The detailed structure of our method is shown in .Our network has a fully-connected network architecture. Bydefault, we set the hidden size h of all layers as 768, whichis the size of the concatenation of local encoding (512) andglobal encoding (256).In the beginning, the network takes as input the concate-nation of local encoding and global encoding, and trans-forms it with a residual block which has 3 fully-connectedlayers. Then N residual blocks with 3 fully-connected lay-ers are sequentially applied, where we choose N based onthe scene scale.Specifically, we set N = 1 for all in-door scenes, and N = 2 for Cambridge Landmarks .For Aachen Day dataset, we set N = 3 and dou-ble the hidden size h, i.e. 1536, for the second layer ofeach residual block. In addition, for evaluation of AachenDay dataset with additional SuperPoint fea-ture, we set N = 2 and do not double the hidden size hin order to maintain a similar map size. Finally, we apply3 fully-connected layers to get k logits {si}, one for eachcluster center, and one homogeneous coordinate with pa-rameters d, w to define an offset. The final 3D coordinate yis estimated with Eq. 8 in the main paper.To get the k cluster centers from training data, we clustertraining camera positions with K-Means++ . We set k =50 for scenes that have a more multimodal distribution, in-cluding integrated rooms and Aachen Day dataset ,and k = 1 for scenes that have a more unimodal distribu-tion, including individual scenes in the 7 Scenes , 12Scenes and Cambridge Landmarks .",
  "B. Experiment Details": "Following , we allocate a training buffer on the GPU,which stores local encodings and corresponding meta-data, i.e. image indices and ground truth poses. This bufferis filled by iterating over the training images. Each imageis first converted to grayscale and then subjected to a se-ries of data augmentations: random scaling between 2 3 and32, brightness and contrast jitter by 10%, and random ro-tations up to a maximum of 15. From each augmentedimage, we extract and uniformly sample 1024 local encod-ings. For the version using SuperPoint , importancesampling based on corner detection probability is employedinstead. We also continuously update the training bufferduring each training iteration when the number of trainingimages is large.Global features for each training image are extractedwithout any data augmentation and stored in a lookup ta-",
  ". Mapping Times of our method on different scenes. Weuse Nvidia Quadro RTX 6000 GPUs in experiments": "ble to avoid unnecessary duplication. During each trainingiteration, a batch of local encodings is randomly selectedfrom the training buffer. Corresponding global encodingsare then retrieved based on the image index.For theseglobal encodings, we add Gaussian noise with a standarddeviation of = m = 0.1, where m is the margin used inthe triplet margin loss by the global feature extractor .Subsequently, the global encodings are normalized back tothe unit sphere. We use AdamW optimizer with a One Cycle learn-ing rate scheduler that increases the learning rate from2 104 to 5 103 and then decreases to 2 108. Thedetailed mapping times and number of GPUs for training isshown in Tab. 7. During evaluation, we use a 10px inlier threshold and 64RANSAC hypotheses for all experiments, except that weuse 3200 RANSAC hypotheses for Aachen Day dataset to match the number of RANSAC hypotheses of theACE 50 baseline.",
  "C. Position Decoding in 2D Toy Example": "We designed a simplified 2D toy example to show the ef-fect of our position decoder. We randomly select 19 im-ages from the 7 Scenes and 12 Scenes datasetsand place them in a grid with a similar layout as the i19scene.The images are resized and cropped to a size of480 x 640 for convenient batch processing.We use thesame pretrained ACE encoder and train the MLP headwith similar architecture, except that the output coordinateis now 2D instead of 3D. We use 19 decoder cluster cen-ters, which are actually the centers of the 19 images. Theoutput coordinate is directly supervised by the ground truthpixel location. shows that, even for this simple exam-ple with strong supervision, our position decoder can allowthe model to fit the training data with a multi-modal outputdistribution better.",
  "Offset": ". Detailed structure of our fully-connected network architecture for GLACE. In the beginning, a residual block (blue) transformsthe concatenation of global and local encodings, which is followed by N sequential residual blocks (orange). Finally, three fully-connectedlayers (pink) are applied to get the k logits and offset for estimating the 3D position. 0.900.840.850.880.98 0.760.540.820.900.73 0.550.520.440.430.48 0.670.590.590.65",
  "D. Reconstruction Visualization": "In , 10 and 11, we visualize the implicit reconstruc-tions by accumulating the predicted 3D scene coordinatesof the training images, and filter the outliers according to a5px reprojection error threshold. The point cloud color isobtained from the center pixel of each image patch. As wecan see, the implicit triangulation allows the model to learnmeaningful 3D structures from reprojection loss only.",
  "Federico Camposeco, Andrea Cohen, Marc Pollefeys, andTorsten Sattler. Hybrid scene compression for visual local-ization. In CVPR, 2019. 7": "Tommaso Cavallari, Stuart Golodetz, Nicholas A Lord,Julien Valentin, Luigi Di Stefano, and Philip HS Torr. On-the-fly adaptation of regression forests for online camera re-localisation. In CVPR, 2017. 2 Tommaso Cavallari, Luca Bertinetto, Jishnu Mukhoti, PhilipTorr, and Stuart Golodetz. Lets take this online: Adaptingscene coordinate regression network predictions for onlinergb-d camera relocalisation. In 3DV, 2019. 1",
  "Yoli Shavit, Ron Ferens, and Yosi Keller. Learning multi-scene absolute pose regression with transformers. In ICCV,2021. 2, 7": "Jamie Shotton, Ben Glocker, Christopher Zach, ShahramIzadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene co-ordinate regression forests for camera relocalization in RGB-D images. In CVPR, 2013. 2, 6, 9 Leslie N. Smith and Nicholay Topin.Super-convergence:very fast training of neural networks using large learningrates. In Artificial Intelligence and Machine Learning forMulti-Domain Operations Applications, 2019. 9",
  "Linus Svarm, Olof Enqvist, Fredrik Kahl, and Magnus Os-karsson. City-scale localization for cameras with known ver-tical direction. IEEE TPAMI, 39(7):14551461, 2016. 2": "Hajime Taira, Masatoshi Okutomi, Torsten Sattler, MirceaCimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-ihiko Torii.Inloc: Indoor visual localization with densematching and view synthesis. In CVPR, pages 71997209,2018. 1, 2 Hajime Taira, Ignacio Rocco, Jiri Sedlar, Masatoshi Oku-tomi, Josef Sivic, Tomas Pajdla, Torsten Sattler, and AkihikoTorii. Is this the right place? geometric-semantic pose ver-ification for indoor visual localization. In ICCV, 2019. 1,2 Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, SaraFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-mamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea-tures let networks learn high frequency functions in low di-mensional domains. NeurIPS, 2020. 3 Carl Toft, Will Maddern, Akihiko Torii, Lars Hammarstrand,Erik Stenborg, Daniel Safari, Masatoshi Okutomi, MarcPollefeys, Josef Sivic, Tomas Pajdla, et al. Long-term vi-sual localization revisited. IEEE TPAMI, 44(4):20742088,2020. 1"
}