{
  "Abstract": "Segment Anything Model (SAM) has achieved impressiveperformance in many computer vision tasks. However, as alarge-scale model, the immense memory and computationcosts hinder its practical deployment. In this paper, we pro-pose a post-training quantization (PTQ) framework for Seg-ment Anything Model, namely PTQ4SAM. First, we investi-gate the inherent bottleneck of SAM quantization attributedto the bimodal distribution in post-Key-Linear activa-tions. We analyze its characteristics from both per-tensorand per-channel perspectives, and propose a Bimodal In-tegration strategy, which utilizes a mathematically equiv-alent sign operation to transform the bimodal distributioninto a relatively easy-quantized normal distribution offline.Second, SAM encompasses diverse attention mechanisms(i.e., self-attention and two-way cross-attention), resultingin substantial variations in the post-Softmax distributions.Therefore, we introduce an Adaptive Granularity Quanti-zation for Softmax through searching the optimal power-of-two base, which is hardware-friendly. Extensive experimen-tal results across various vision tasks (instance segmenta-tion, semantic segmentation and object detection), datasetsand model variants show the superiority of PTQ4SAM. Forexample, when quantizing SAM-L to 6-bit, we achieve loss-less accuracy for instance segmentation, about 0.5% dropwith theoretical 3.9 acceleration. The code is available at",
  "(b) post-Softmax distribution": ".The histogram of two special distributions in SAM:(a) bimodal distribution in post-Key-Linear activations. (b)post-Softmax distributions of self-attention, token-to-image cross-attention and image-to-token cross-attention. and other downstream tasks . How-ever, the transformer architectures in SAM require intensivecomputation and memory footprint, which hurdles the prac-tical deployment on resource-constrained edge-devices.To address this issue, several quantization approaches [5, 10, 17, 26, 27, 42, 43, 57, 60] were proposed to con-vert weights and activations from floating-point to low-bit. There are two categories of quantization methods: 1)Quantization-Aware Training (QAT) and 2) Post-TrainingQuantization (PTQ). QAT retrains a model by utilizingthe whole labeled training dataset, which will be time-consuming due to the corresponding massive dataset (SA-1B). On the other hand, PTQ is more promising because itonly requires small unlabeled samples to calibrate the pre-trained networks. In this paper, we focus on designing thePTQ approach as it is more effective in practical usage.Although previous PTQ methods have showcased signif-icant accomplishments in various scenarios, including con-volutional neural networks (CNNs) , vi-sion transformers (ViTs) as well as large lan-guage models (LLMs) , directly adopting thesemethods to Segment Anything Models will raise two uniquechallenges that necessitate a revisiting of traditional PTQschemes: 1) we observe the bimodal distribution appearsin post-Key-Linear activations, i.e., the output activa-tions of key linear, as shown in a. The two peaks",
  "arXiv:2405.03144v1 [cs.CV] 6 May 2024": "and their central void interval severely enlarge the rangeof the entire distribution, which negatively affects quanti-zation performance. 2) Owing to the diverse kinds of at-tention mechanisms, SAMs exhibit more complicated post-Softmax distributions when compared to VITs, as shownin b. Statistically, about 72.5% post-Softmax acti-vations of image-to-token are more than 0.01 while only0.4% in token-to-image. Prior works havenot adequately addressed this discrepancy and treated themequally, causing the potential loss of inherent information.Therefore, it is desirable to design specialized componentsfor post-Softmax distribution in SAM.Based on the above observations, in this paper, we pro-pose a novel post-training framework called PTQ4SAMspecifically designed for Segment Anything Model quanti-zation. First, we present a Bimodal Integration (BIG) strat-egy to deftly eliminate the bimodal distribution. Specifi-cally, we conduct an in-depth analysis of bimodal distri-bution from both per-tensor and per-channel perspectives,finding that the bimodal distribution is the crucial obstacleto SAM quantization. We leverage its per-tensor character-istic to determine whether a distribution bimodal distribu-tion, and use its per-channel characteristic to transfer thisbimodal distribution to a normal distribution by simultane-ously absorbing sign factor into query linear and key linearoffline. This transformation is mathematically equivalentand significantly narrows the distribution range for betterquantization performance. Second, we propose an AdaptiveGranularity Quantization (AGQ) explicitly tailored for di-verse post-Softmax distributions, which affords a suitabletrade-off in granularity for both lower and higher attentionscores. We provide theoretical proof for its efficiency onhardware by searching the optimal power-of-two base. In-stead of minimizing quantization errors of attention scores,we design the objective by the matrix multiplication outputbetween attention scores and values, which is more robustand beneficial to the ultimate performance.We conduct extensive experiments on fundamental tasksand different model variants to demonstrate the versatil-ity of PTQ4SAM. Our PTQ4SAM can seamlessly pluginto both statistic-based and learning-based PTQ methods,achieving 3.9 FLOPs and 4.9 storage savings while main-taining lossless performance on 6-bit SAM-L and SAM-H.Our major contributions are summarized as follows: To our best knowledge, our work is the first post-training quantization solution tailored for Segment Any-thing Model, dubbed PTQ4SAM. We observe a challenging bimodal distribution for quan-tization and analyze its characteristics. To overcome it,we propose a Bimodal Integration (BIG) strategy, whichautomatically detects it and transforms the bimodal dis-tribution to normal distribution equivalently.",
  ". Segment Anything": "Recently, Meta AI Research has revolutionarily approacheda general, promptable Segment Anything Model (SAM).Pre-training on web-scale datasets (SA-1B), SAMdemonstrates the capability to generalize across diversedownstream tasks . HQ-SAM de-signs learnable tokens and global-local fusion schemes toobtain high-quality masks. SEEM extends the refer-ring image to prompt types and integrates a joint visual-semantic space. In the realm of medical research, MedSAM and SAM-Med2D fine-tune SAM through large-scale medical image datasets. Combined with a series ofvisual-language models , Anything-3D andSA3D applies SAM to the single-view 3D reconstruc-tion task while Seal to the 3D point cloud segmenta-tion task. Suffering from substantial computational require-ments, some efficient SAMs, including MobileSAM ,FastSAM and TinySAM are successively intro-duced. However, SAM still undergoes untenable resource-intensive consumption. Its real-time processing capabilitieshave received widespread expectations.",
  ". Post-Training Quantization": "As a predominant compression approach , the main-stream post-training quantization (PTQ) methods can bebroadly divided into two categories : statistic-basedPTQ and learning-based PTQ. Statistic-based PTQ methodssolely seek optimal quantization parameters to minimizequantization errors, whereas learning-based PTQ methodsfine-tune both weights and quantization parameters. Ourapproach is out-of-the-box on both kinds of methods.",
  "Statistic-Based PTQ": "Numerous classic statistic-based quantization methods have been shown to achieve minimalloss in precision primarily for convolutional neural net-works. However, with the widespread popularity of net-works featuring novel architectures, researchers have intro-duced quantization schemes specifically designed for thesenetworks.When quantizing ViTs, twin uniform quanti-zation , Log-Int-Softmax , scale reparameteriza-tion and matthew-effect preserving quantization are proposed to tackle the output distribution from soft- max. The LLM quantization techniques include weight-only quantization , weight and activation quanti-zation , aiming to settle the outlier issue fromthe activations. PTQ4DM , Q-Diffusion discoverthe variations in the activations during multiple denoisingsteps in Diffusion and design specialized calibration strate-gies. However, there is a notable gap between these distinc-tive distributions and SAMs, and we are the first to explorequantizing SAMs for efficient inference.",
  "Learning-Based PTQ": "Based on statistic-based PTQ methods, several learning-based PTQ schemes were also proposed. AdaRound optimizes the rounding operation when quantizing weightsto minimize the overall loss of the model.Subse-quently, many methods are proposed based on AdaRound.BRECQ proposes a block-wise reconstruction algo-rithm to optimize the quantized model. QDrop intro-duces drop operation during the reconstruction process toincrease the flatness of the optimized model. PD-Quant introduces global information when optimizing quantiza-tion parameters. MRECG focuses on the oscillationproblem in PTQ and FlexRound proposes a new learn-able weight rounding scheme on large language models forthe first time. Unfortunately, these techniques are mainlycarried out based on CNN architecture models. Transformerarchitecture models like SAM remain unexplored.",
  ". Preliminaries": "Basic Notations. We use X to represent a matrix, whereasthe vectors are marked by x. The operator is used torepresent element-wise multiplication between matrices orvectors and operator denotes scalar multiplication. Also,we use XW to denote matrix multiplication.Post-training Quantization. Post-training quantization isa prevalent approach to compress the pre-trained neural net-work. In this paper, we merely study the hardware-efficientquantization methods. For uniform quantization, quantiza-tion and de-quantization operations can be defined as:",
  "x = s (xq z) x,(2)": "where s and z denote the scaling factor and zero point, re-spectively. is the round-to-nearest operator. x and x arefloating-point and de-quantized values, and xq is mappedinteger.clamp function clips the values fall outside therange of a k-bit integer.In light of rapid bit-shifting operations, Log2 Quantiza-tion has emerged as an alternative hardware-oriented quan-tization approach. Due to the Log2 Quantization is exclu-sively employed on post-Softmax activations, it is simplyformulated as:",
  ". Bimodal Integration": "Intuitively, the bimodal distribution poses significant chal-lenges for quantization. The two peaks, accompanied bytheir central void or sparse interval considerably expand thedistribution range, leading to over 5 quantization errorscompared to normal distribution experimentally. Therefore,we first make an in-depth analysis of bimodal distribution inSAM from two perspectives: 1) From per-tensor perspec-tive: the distribution contains two peaks and their centersare symmetric, e.g., -8 and 8 in a. 2) From per-channel perspective: the activations of each channel onlypersist in a fixed peak, indicating pronounced asymmetryinside one channel. As shown in , for instance,the activations of the 0-th channel correspond to a negativepeak (i.e., -8), while the activations of 1-th channel belongto a positive peak (i.e., 8). Generally, about half channels(e.g., 46.1% in SAM-B) cluster in the positive peak and theremaining channels cluster in the negative peak.Recently, some methodologies have been intro-duced to overcome this channel-wise asymmetry by equiv-alently adjusting the weights of LayerNorm and the subse-quent linear. However, we observe that the bimodal distri-bution does not exist in post-LayerNorm activations butrather prominently concentrates in post-Key-Linear ac-tivations (), rendering the aforementioned methodsinapplicable. To precisely estimate the influence of bimodaldistribution, the following matrix multiplication between Qand K can be formulated as",
  ",(5)": "where W Rmn and b Rn are the weight and bias oflinear (i.e., fully connected layer). m and n are input andoutput feature dimensions. The subscripts q and k denotethe query and key linears, respectively. Note that the matrixmultiplication of Q and K essentially represents the matrixmultiplication of normal and bimodal distributions. Moti-vated by the above analysis, we adopt a channel-wise sign value LinQ LinQ",
  "+1,if mean(K:,j) 01,otherwise,(6)": "where K:,j denotes the post-Key-Linear activations inj-th channel. And we compute the j through its sign ofmean value. Specifically, if the j-th channel is negativepeak, j will be -1. Then we multiply it to the correspond-ing channel of query linear and key linear simultaneously tomaintain equivalence. After that, K:,j will be transferred tothe positive. Conversely, if one channel is positive peak, itwill remain invariant as its sign factor is 1. As shown in, after using our BIG strategy, the negative peak inK has been merged into the positive peak, thereby transfer-ring the bimodal distribution to a normal distribution. On",
  ".(7)": "Note the sign factor can be easily absorbed into previ-ous query linear and key linear offline without any compu-tation overhead, i.e., W = W and b = b .bimodal discovery: However, not all post-Key-Linearactivations in SAM is bimodal distribution. To discrimi-nate the bimodal distribution, we first adopt the Gaussiankernel density estimation to compute the probability den-sity function (PDF) by the whole tensor. Based on thecontinuous and smooth function, we quantitatively describethe peaks as local maxima. To avoid recognizing two smallbumps as two peaks, we constrain the peak height and thedistance between two peaks. More implementation detailsare described in supplementary materials.In summary, our Bimodal Integration (BIG) strategycomprises three steps: bimodal discovery, computationand equivalent transformation. Due to the strong asymme-try, only one sample is enough to compute the sign factor, described in Algorithm 1. Therefore, our BIG is efficientand the extra computational burden can be ignored.",
  ": end for19: return MQ": "identify the extremely unbalanced power-lawdistribution as the rationale of quantization difficulty anddevise a specialized quantizer to address it. However, theaforementioned methods are mainly designed for the soft-max in self-attention mechanism. SAM also incorporatescross-attention in two directions, i.e., token-to-imageand image-to-token cross-attention, amplifying the re-markable discrepancy between post-softmax distributionsin b. For example, there are more ultra-low-valuesin token-to-image, displaying a smooth distribution un-der a logarithmic scale. On the contrary, the distributionsin image-to-token and self-attention exhibit higherkurtosis and exist more high values.To tackle this discrepancy, we revisit the logarithmicquantizer and propose an Adaptive Granularity Quantiza-tion (AGQ) with an adaptive parameter to adjust the base.As shown in , a smaller can represent lower atten-tion scores. And as increases, the higher attention scoresbecome more fine-grained. Our AGQ, equipped with a suit-able , achieves a flexible trade-off between the granularityof low and high values under diverse post-Softmax scenar-ios and different bit-widths. The corresponding quantiza-tion and de-quantization operations of AGQ for attentionscore a (one element in the attention map for simplicity)can be rewritten as:",
  "where LUT denotes the small lookup table, which isavailable on various Neural Network Accelerators, e.g. onFPGA. The entries in the LUT are only determined by(aq)% and vq >> aq": ". The first term can be repre-sented with an n-bit number, and the second term with ak-bit (activation bit-width). Notably, since the LUTs for {20, 21, . . . , 2n1} can be incorporated into the LUTfor = 2n, the entire network only requires a single LUT.Considering a scenario with 8-bit activation and maximum from the set (22 for implementation), the size of the LUTis computed as 28+24 bytes = 4KB, which is negligiblecompared to quantized SAMs.With the theoretical validation established, our aim is todefine an objective to select the optimal . To this end, anatural choice is to minimize the local quantization error ofattention map A directly. However, we discover it is incon-sistent with the quantization error of its associated attentionblock, which induces instabilities in the global quantizationperformance, especially at low-bit (see more details in sup-plementary materials). Therefore, to alleviate this incon-sistency, we design the objective function to measure thequantization error of the matrix multiplication output be-tween attention map A and values V :",
  ". Experimental Setup": "Tasks, datasets and metrics. We conduct experiments onthree mainstream vision tasks. For the instance segmenta-tion task, we utilize predicted boxes generated by the detec-tor as box prompts for SAM to gain accurate binary masksand evaluate its effectiveness on MS-COCO datasetwith the metric mean Average Precision (mAP). For seman-tic segmentation task, the overall framework comprises twobranches, and we leverage the fine-grained mask producedby SAM to refine the blurry and imprecise mask bound-aries generated by the original segmentor. We evaluate itseffectiveness on ADE20K dataset using the mean Inter-section over Union (mIOU) as the performance metric. Fororiented object detection task, we obtain the final rotatedRBoxes by the minimum circumscribed rectangle operationon the masks generated by SAM. Our evaluation of its ef-fectiveness on the DOTA-v1.0 dataset uses mAP. Implementation details. We choose CNN-based Faster R-CNN , YOLOX , FCOS and transformer-basedH-Deformable-DETR , DINO as detectors and ad-vanced SegFormer as segmentor. The adaptive param-eter is searched from the set {20, 21, 22}. We set the boxthreshold to 0.05 for CNN-based detectors and affix a set of100 adaptive anchors for transformer-based detectors. Werandomly sample 32 training images as calibration set andonly the first sample is utilized for the determination of thebimodal distribution. For a fair comparison, we adopt per-channel asymmetric quantization for weights and per-tensorasymmetric quantization for activations . Followingthe common settings , the first and last layer/blockare not quantized. To verify the effectiveness of PTQ4SAMin two kinds of PTQ methods, we integrate our methodinto statistic-based OMSE and learning-based QDrop, called PTQ4SAM-S and PTQ4SAM-L, respectively.For learning-based methods, we adopt MinMax calibrationstrategy and design attention block, MLP block for block-",
  ". Instance Segmentation Results": "We compare our PTQ4SAM with statistic-based methodssuch as MinMax , Percentile , OMSE andlearning-based methods such as AdaRound , BRECQ, QDrop . lists the performance of allmethods. Our method consistently outperforms other meth-ods by a large margin among different detectors. Our 4-bit PTQ4SAM-S achieves comparable results, and exceedsthe baseline OMSE by over 10% mAP (e.g., from 5.4% to18.1% with Faster R-CNN) on SAM-L and about 20% mAP(e.g., from 8.3% to 30.5% with DINO) on SAM-H, recover-ing them to a usable level. Our PTQ4SAM-S even remark-ably surpasses the state-of-the-art learning-based methodQDrop, at W6A6 on SAM-L. Meanwhile, our PTQ4SAM-L encouragingly achieves lossless accuracy. For instance,at W6A6 setting, our PTQ4SAM-L achieves 40.3% and41.2% when applying YOLOX and H-Deformable-DETRon SAM-L, with only 0.1% and 0.3% performance dropcompared to full-precision models.When quantizing tomore challenging case W4A4, AdaRound and BRECQ be-come infeasible while our method surpasses the QDropby 5.1% on SAM-B with YOLOX and 6.3% on SAM-Lwith H-Deformable-DETR. Compared with SAM-B andSAM-L, SAM-H exhibits greater robustness when intro-ducing quantization noise, but our PTQ4SAM-L still pro-vides about 2% improvement at W4A4. Applying the state-of-art detector DINO , our 6-bit PTQ4SAM-L yields40.4% mAP on SAM-B and achieves a mAP nearing 50%on SAM-L and SAM-H.",
  ". Quantization results of oriented object detection": "mentor, i.e., bring 1.37%, 1.83%, 1.85% enhancement forfull-precision SAM-B, SAM-L and SAM-H. As shown in, quantized SAMs generally still contribute to the fi-nal masks. Our method retains the capability of SAM to thegreatest extent. In particular, we are surprised to find that6-bit PTQ4SAM-L even achieves better performance thanthe full-precision models on both SAM-L and SAM-H. AtW4A4 setting, our method provides 1.04% accuracy pro-motion on SAM-L, outperforming QDrop by 0.15%.",
  ". Object Detection Results": "To further demonstrate versatility across other tasks, wetest PTQ4SAM-L in oriented object detection. Notably, asshown in , our method consistently performs bet-ter than other learning-based PTQ methods. For instance,when quantizing the network to W6A6, experiments indi-cate PTQ4SAM-L slightly drops about 0.3% compared withthe full-precision model on SAM-L and SAM-H. At themost challenging W4A4 bit-width, AdaRound and BRECQsuffer from non-trivial performance degradation.Con-trastively, our method can still obtain satisfactory perfor-mance. We achieve over 44% and 56% accuracy on SAM-Band SAM-L, surpassing the baseline QDrop 2.2% and 6.2%.",
  ". Ablation Studies": "Ablation for components: lists the results of dif-ferent components. We demonstrate that each componentcontributes to PTQ4SAM, while the best performance isachieved when both components are jointly applied. Specif-ically, at relatively higher bit-widths, like W6A6 setting,both BIG and AGQ strategies can bring performance im-provement, making the quantized model comparable to thefull-precision one.When quantizing SAM to lower bit-widths, i.e., W4A4, BIG can significantly improve the per-formance, which is 3.4% higher than baseline, as it can min-imize the more serious quantization perturbation.Ablation for different quantizers: We also report the re-sults of different quantizers in . Notably, simply em-",
  ". (a) Theoretical acceleration rate (100 prompts) vs. allSAM models. (b) Accuracy vs. storage": "ploying Log2 quantizer for attention scores is unsta-ble under different settings (e.g., lower result at W6A6 forSAM-B compared with uniform quantizer). Our AGQ, bycontrast, surpasses the uniform quantizer and Log2 quan-tizer in different cases, boosting 3.5% for 4-bit SAM-L.Apart from the encouraging performance, our AGQ is avail-able on various hardware, ensuring efficient execution.",
  ", 6": "32} of the amountof {4,6}-bit multiplications. Surprisingly, our 6-bit SAM-B (2.96) achieves better acceleration than FastSAM (1.98) while maintaining a close performance. At W4A4,our method reduces computational FLOPs by over 70% andstorage by over 85%. As models scale up, both accelerationratio and memory savings become more significant, whilethe performance drop becomes less.",
  ". Qualitative Results": "To exhibit the superiority of our PTQ4SAM, especially onlow-bit quantization (W4A4), we visualize the instance seg-mentation results in compared with other existingSOTA methods on COCO dataset. We can perceive thatmost methods fail to produce clear boundaries and miss thesalient pixels in the center. For example, other methods donot distinguish the complex edge, even erroneously catego-rizing the sky as foreground (train on row 1). Besides,these schemes are unable to detect the objects against com-plicated backgrounds (surfboard on row 3). Furthermore,our method outperforms other methods on ensuring the in-tegrity of the object (toilet and person on rows 2, 4).",
  ". Conclusion": "In this paper we first propose a novel post-training quantiza-tion framework, PTQ4SAM for Segment Anything Model.To begin with, we observe the significant bottleneck liesin bimodal distribution and explore its occurrence position.We introduce Bimodal Integration strategy to eliminate thenegative effect of quantization caused by such bimodal dis-tribution. We also present the Adaptive Granularity Quanti-zation which can fit diverse post-Softmax distribution bysearching the hardware-friendly base.Extensive experi-ments demonstrate that our method consistently yields grat-ifying results across various tasks. Nevertheless, the reasonfor bimodal distribution in SAM remains unclear. This di-rection serves as a potential avenue for our future research.",
  "J. Guo, W. Ouyang, and D. Xu. Channel pruning guided byclassification loss and feature importance. In AAAI, 2020. 2": "Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,Matthew Tang, Andrew Howard, Hartwig Adam, and DmitryKalenichenko. Quantization and training of neural networksfor efficient integer-arithmetic-only inference. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 27042713, 2018. 1, 2, 6, 7 Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu,Weihong Lin, Lei Sun, Chao Zhang, and Han Hu.Detrswith hybrid matching. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1970219712, 2023. 6",
  "Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dong-soo Lee. Flexround: Learnable rounding based on element-wise division for post-training quantization. arXiv preprintarXiv:2306.00317, 2023. 3": "Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu,Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao.Semantic-sam: Segment and recognize anything at any gran-ularity. arXiv preprint arXiv:2307.04767, 2023. 1 Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for uni-fied vision-language understanding and generation. In In-ternational Conference on Machine Learning, pages 1288812900. PMLR, 2022. 2 Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, ZhenDong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.Q-diffusion: Quantizing diffusion models. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1753517545, 2023. 3 Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, QiZhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushingthe limit of post-training quantization by block reconstruc-tion. arXiv preprint arXiv:2102.05426, 2021. 1, 3, 6, 7 Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq-vit: Scale reparameterization for post-training quantizationof vision transformers. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1722717236, 2023. 1, 2, 3, 5",
  "Shunchang Liu, Jiakai Wang, Aishan Liu, Yingwei Li, YijieGao, Xianglong Liu, and Dacheng Tao. Harnessing percep-tual adversarial patches for crowd counting. In ACM CCS,2022. 1": "Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wen-wei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segmentany point cloud sequences by distilling vision foundationmodels. arXiv preprint arXiv:2306.09347, 2023. 2 Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu,and Kwang-Ting Cheng.Bi-real net: Enhancing the per-formance of 1-bit cnns with improved representational ca-pability and advanced training algorithm. In Proceedings ofthe European conference on computer vision (ECCV), pages722737, 2018. 8",
  "Jun Ma and Bo Wang. Segment anything in medical images.arXiv preprint arXiv:2304.12306, 2023. 2": "Yuexiao Ma, Huixia Li, Xiawu Zheng, Xuefeng Xiao, RuiWang, Shilei Wen, Xin Pan, Fei Chao, and RongrongJi.Solving oscillation problem in post-training quantiza-tion through a theoretical perspective.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 79507959, 2023. 3 Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallab-hula, Makram Chahine, Daniel M Vogt, Robert J Wood, An-tonio Torralba, and Daniela Rus. Follow anything: Open-set detection, tracking, and following in real-time.arXivpreprint arXiv:2308.05737, 2023. 1 Jeffrey L McKinstry, Steven K Esser, Rathinakumar Ap-puswamy, Deepika Bablani, John V Arthur, Izzet B Yildiz,and Dharmendra S Modha. Discovering low-precision net-works close to full-precision networks for efficient infer-ence.In 2019 Fifth Workshop on Energy Efficient Ma-chine Learning and Cognitive Computing-NeurIPS Edition(EMC2-NIPS), pages 69. IEEE, 2019. 2 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. Communications of the ACM, 65(1):99106, 2021.2",
  "Ziwei Wang, Ziyi Wu, Jiwen Lu, and Jie Zhou.Bidet:An efficient binarized object detector.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 20492058, 2020. 8": "Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, andFengwei Yu. Qdrop: Randomly dropping quantization forextremely low-bit post-training quantization. arXiv preprintarXiv:2203.05740, 2022. 1, 3, 6, 7 Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, RuihaoGong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xian-glong Liu. Outlier suppression: Pushing the limit of low-bittransformer language models. Advances in Neural Informa-tion Processing Systems, 35:1740217414, 2022. 1, 3 Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang,Ruihao Gong, Jinyang Guo, and Xianglong Liu.Outliersuppression+: Accurate quantization of large language mod-els by equivalent and optimal shifting and scaling.arXivpreprint arXiv:2304.09145, 2023. 1, 3 Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, andPaulius Micikevicius. Integer quantization for deep learn-ing inference: Principles and empirical evaluation.arXivpreprint arXiv:2004.09602, 2020. 1, 6, 7 Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-pei Zhang. Dota: A large-scale dataset for object detectionin aerial images. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 39743983,2018. 6 Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, JulienDemouth, and Song Han. Smoothquant: Accurate and effi-cient post-training quantization for large language models.In International Conference on Machine Learning, pages3808738099. PMLR, 2023. 1, 3 Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,Jose M Alvarez, and Ping Luo.Segformer: Simple andefficient design for semantic segmentation with transform-ers.Advances in Neural Information Processing Systems,34:1207712090, 2021. 6",
  "Conference on Computer Vision, pages 191207. Springer,2022. 1, 2, 5, 6": "Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.Faster segment anything: Towards lightweight sam for mo-bile applications. arXiv preprint arXiv:2306.14289, 2023.2 Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, JunZhu, Lionel M Ni, and Heung-Yeung Shum.Dino: Detrwith improved denoising anchor boxes for end-to-end objectdetection. arXiv preprint arXiv:2203.03605, 2022. 6, 7",
  "F.1. Bimodal Discovery": "As we mentioned in the main paper, we utilize the contin-uous probability density function to characterize the peaks.However, merely using the naive local maxima will inducean over-detection issue. We summarize the issue in two sit-uations: 1) Two neighboring bumps in one peak are recog-nized as two peaks (Figure S6(a)). 2) Wrongly consider thesmall bump as a peak (Figure S6(b)). To address it, we im-pose constraints stipulating that both the peak height and thedistances between two peaks must exceed a predeterminedthreshold in Figure S6(c). Smaller peaks are removed firstuntil the condition is fulfilled for all remaining peaks.",
  "F.2. Effect of Sign Operation": "To verify the effectiveness of our BIG strategy, we show therepresentative real distributions of query and key activationsbefore and after sign operation. As shown in Figure S7, af-ter sign operation, the bimodal post-Key-Linear distri-bution will be transferred to a normal distribution, narrow-ing the range from -1314 to 314 (row 1). Meanwhile,the query activations remain normal distribution invariantly,slightly reducing the range from -843848 to -848296(row 2). Intuitively, our BIG is beneficial for quantizationand the sign operation can be performed in advance.",
  "Figure S8. Pie charts depicting the optimal across various atten-tion mechanisms in SAM-L": "higher attention scores can be quantized in a more fine-grained fashion.For simplicity, we conduct a statis-tical analysis of optimal across diverse post-Softmaxdistributions at W4A4.As illustrated in Figure S8, intoken-to-image, our AGQ uniformly favors =1 becausethere are more low attention scores (see in themain paper).In image-to-token, =2 is prominentlyselected to accurately quantize more high scores. And inself-attention, there is a coexistence of =1 and =2for the combination of both high and low attention scores.",
  "H. More Qualitative Results": "More instance segmentation results are given in Figure S9produced by 4-bit BRECQ , QDrop , PTQ4SAMand full-precision SAM-L. Notably, our model demon-strates superior performance in terms of both completenessand clarity when compared to other methodologies. In asimple scenario with a single object, such as the personin row 1 and the kite in row 2, our method is capable ofproviding a more comprehensive description of the objectboundaries, without missing any pixels. In cases where ob-jects overlap, as observed in rows 3 and 4, our quantizedmodel accurately distinguishes each individual object andsuccessfully separates them from complex backgrounds.Conversely, other methods often struggle to segment oc-cluded objects accurately, capturing unnecessary details.Particularly when recognizing background objects like thedining table, as depicted in row 5, the results obtainedfrom alternative approaches exhibit notable incompleteness.Conversely, our approach excels in effectively identifyingthe entire object, showcasing a significant advantage overother methods."
}