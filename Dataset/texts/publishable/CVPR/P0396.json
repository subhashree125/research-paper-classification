{
  "Abstract": "Satellite imaging generally presents a trade-off betweenthe frequency of acquisitions and the spatial resolution ofthe images. Super-resolution is often advanced as a wayto get the best of both worlds. In this work, we investi-gate multi-image super-resolution of satellite image timeseries, i.e. how multiple images of the same area acquiredat different dates can help reconstruct a higher resolutionobservation. In particular, we extend state-of-the-art deepsingle and multi-image super-resolution algorithms, suchas SRDiff and HighRes-net, to deal with irregularly sam-pled Sentinel-2 time series. We introduce BreizhSR, a newdataset for 4 super-resolution of Sentinel-2 time seriesusing very high-resolution SPOT-6 imagery of Brittany, aFrench region. We show that using multiple images signif-icantly improves super-resolution performance, and thata well-designed temporal positional encoding allows usto perform super-resolution for different times of the se-ries. In addition, we observe a trade-off between spectralfidelity and perceptual quality of the reconstructed HR im-ages, questioning future directions for super-resolution ofEarth Observation data. The source code is available at",
  ". Introduction": "Satellite imagery is one of the most powerful and effectivetools to monitor and study the surfaces of the Earth, con-tributing to various applications such as weather forecasting,urban planning, or tracking natural disasters. However, itsefficiency is constrained by trade-offs between spatial andtemporal resolutions. On the one hand, recent constellationscan capture the same area with a high-revisit time, formingcomplex spatio-temporal data cubes coined as satellite imagetime series (SITS). A notable example is the two Sentinel-2satellites that capture all land surfaces every five days at SR image x4 LR time series",
  ". Multi-image super-resolution with an upscaling factorof 4. The irregular low-resolution (LR) satellite time series is usedto predict a super-resolved (SR) image for a given acquisition date": "the equator at a 10-meter spatial resolution at best. TheSITS data are commonly used to precisely monitor land-scape dynamics, such as in land cover mapping or changedetection, but their low to medium spatial resolution mightbe inadequate for some applications, such as urban mappingof buildings, roads, or sparse vegetation. On the other hand,very high-spatial resolution sensors, such as Pleiades Neo orWorldView, capture images at a metric or sub-metric resolu-tion, useful for small object detection or individual buildingcounting. For example, SPOT-6 acquires images at 1.5 mspatial resolution. As well as often being commercial andthus necessitating the purchase of images, the lack of denseacquisitions poses challenges for continuous monitoring ofdynamic landscapes, such as crops or forests.One possible solution to make the most of high-temporal",
  "arXiv:2404.16409v1 [cs.CV] 25 Apr 2024": "low-spatial data and low-temporal high-spatial data is super-resolution, an image processing technique that aims to im-prove the spatial resolution of an image. In this context, thevanilla super-resolution ill-posed inverse problem consists ofreconstructing a high-spatial resolution (HR) image from alow-spatial resolution (LR) image. This problem, coined assingle-image super-resolution (SISR), has been tackled withan ever-growing corpus on deep learning, from CNN toGAN , and now diffusion models , yet withonly preliminary applications to remote sensing .One challenge with super-resolution is that models are of-ten trained on synthetic data, where (LR,HR) pairs are artifi-cially created by downsampling an HR image . Inremote sensing, the synthetic LR image does not reflect whatan actual LR sensor would capture in situ . Besides, thissupposes that (LR,HR) were acquired simultaneously. Thecross-sensor setting that uses different LR and HR sourcesis a way to overcome this issue, by training models robust toatmospheric effects, geometric distortions, and differencesin spectral characteristics .Additionally, access to multiple images of the same geo-graphical area at different times can be beneficial to enhancethe generation of an HR image by fusing information frommultiple views . This approach, coined as multi-imagesuper-resolution (MISR), applies super-resolution on a se-quence of satellite images instead of a single image (see). MISR techniques usually extend SISR techniqueswith a dedicated module to learn from the sequence of LRimages . Among notable approaches, HighRes-net introduced a recursive fusion module and residual attentionmodel (RAMS) proposed the use of residual channelattention to model the temporal relationships between LRimages. However, these approaches consider regularly tem-poral sampled images while SITS are irregular and unaligned(not the same acquisition dates for different SITS) due to themultitude of satellite orbits and meteorological conditions.In this work, we study cross-sensor super-resolution tech-niques for SISR and MISR scenarios with diffusion models.To exploit irregular and unaligned time series, we introducea time-equivariant fusion module to model the temporal rela-tionships between LR images. Specifically, we improve onthe light-weight temporal attention encoder (L-TAE), origi-nally designed for SITS classification and later on forSITS panoptic segmentation , to integrate the temporaldimension of SITS into MISR. We evaluate our approacheson a new dataset, BreizhSR, that consists of LR Sentinel-2time series paired with HR SPOT-6 images acquired overthe Brittany region located in northwest France.Our contributions are summarized as follows:1. We introduce BreizhSR, a new dataset for real-worldsuper-resolution of Sentinel-2 time series.",
  ". Related Work": "Super-resolution (SR) techniques have witnessed a shift fromtraditional interpolation-based methods to data-driven deep-learning approaches in recent years. Traditional methods,such as bicubic interpolation or Lanczos resampling, yieldedlimited improvements and failed to capture the intricate de-tails and high-frequency information in the super-resolvedimages . In this section, we present the state-of-the-artmethods for SISR and MISR. We will focus on deep learningapproaches since they outperform traditional methods .Single-image super-resolution (SISR) techniques in-clude standard neural networks, attention-based models, gen-erative adversarial networks (GANs), and diffusion models.Pioneer deep super-resolution based on convolutional neuralnetworks quickly evolved to integrate architecture im-provements such as residual learning and channelattention . State-of-the-art models are now based on gen-erative models, such as GANs. For example, SRGAN combines a perceptual loss with an adversarial loss, and isone the first techniques reaching a 4 upscaling factor. Itwas refined in ESRGAN using the residual-in-residualdense block (RRDB) generator and an improved GAN formu-lation. The latest models rely on diffusion models, definedby denoising diffusion probabilistic models (DDPM) .In brief, they gradually inject random noise into the inputimage and then learn to reverse the diffusion process to gen-erate new samples from the noise. State-of-the-art diffusionmodels such as have shown their ability to model com-plex image distributions and to outperform both CNN andGAN-based approaches. Newer works, such as SRDiff ,experiment with conditioning the diffusion model using aclassical super-resolution model, e.g. RRDB.In remote sensing, super-resolution has been proposedas a way to extract HR information from LR imagery .Similarly to classical computer vision, CNN orGAN can be trained to super-resolve downsampledremote sensing images. The drawback of these approaches isthat the LR data is synthetic, and simulated by degrading realimages. To close the gap with real data, many works haveproposed ways to improve the synthetic data by using betterspectral models or simply different sensors for the LRand HR images . This setup, called cross-sensorsuper-resolution is more realistic and promising to applySISR models to real-world data, but poses other problems.It requires co-registered LR and HR acquisitions of the samearea, at similar dates to avoid changes. Several datasets havebeen introduced, focusing mainly on Sentinel-2 , whosealiasing and inter-band shift characteristics have shown tobe particularly well-suited for super-resolution . Multi-image super-resolution (MISR) extends SISR byexploiting multiple LR images to generate an HR image. Theidea is that each LR image in the sequence contains a differ-ent portion of the HR information. Most approaches fuse theinformation from various LR perspectives captured within alimited time frame to produce an HR image that depicts thelandscape during this period. Formally, we aim at generat-ing a super-resolved image HR applying some function hparameterized by to the set of LR images (LR1, ..., LRT ),with T the number of images in the sequence, i.e.",
  "HR = h(LR1, ..., LRT )(1)": "Most MISR approaches are designed for video processing, which typically assume a fixed time interval betweenframes and a high frame rate. These hypotheses are notreasonable for satellite imagery perturbed by the presence ofclouds or saturated pixels. Using the information from multi-ple images should help generate images that are more robustto these perturbations. MISR datasets in remote sensing areless common. The most known is the Proba-V challenge, acompetition organized by the European Space Agency ,that sparked a wave of interest in MISR for remote sens-ing. However, Proba-V uses a fixed number of images foreach series, with a constant time gap between two images.Closest to our work, MISR MuS2 and WorldStrat datasets use Sentinel-2 SITS as LR data, and WorldView-2or SPOT-6 images, respectively, for the HR reference.With these new datasets, novel architectures have beenproposed to incorporate the temporal dimension in super-resolution models. Most models are built upon an encoder-decoder structure, as in SISR, but with a modified encoderto incorporate multiple images as inputs. One of the seminalworks in MISR for remote sensing is HighRes-net , a net-work composed of an encoder, a recursive fusion network,and a decoder. More precisely, a reference image (ref) de-fined as the median of the LR image time series is computedand concatenated with each LR image LRi (1 i T).A hidden state is then computed for each concatenation[LRi, ref]. The fusion network then fuses the hidden statesrecursively, by halving by two the number of LR states ateach fusion step. The decoder reconstructs the HR imagefrom the fused state with a deconvolution layer followedby a final convolutional layer. The RAMS network uses a combination of residual feature attention to reducethe temporal dimension of the input and channel attention toweigh the different images. 3DRRDB extends RRDBto multiple images by replacing the 2D convolutional layersby 3D convolutions. Finally, TR-MISR adds learnablechannel attention to the HighRes-net encoder. A limitationof these methods is that they assume a fixed number of inputimages and a constant acquisition rate. While this holds forProba-V, this is a problem for many practical applicationswith Sentinel-2 imagery, that we will address in this work.",
  ". From single- to multi-image super resolution": "Diffusion models are already very popular in SISR applied tosynthetic data, such as the two DDPM models: SR3 andSRDiff . In this work, we propose to study SRDiff on a cross-sensor scenario. SRDiff uses a U-Net net-work in the reverse process to estimate the noise, which is fedwith the noisy image conditioned (i.e. concatenated) by LRinformation. While a simple option is to condition the U-Netmodel directly with the LR image , SRDiff conditionsit with an LR encoder to allow the extraction of valuablefeatures from the LR image. The LR encoder used in SRDiffis RRDB, the generator of ESRGAN , which combinesmulti-level residual networks and dense connections.In the case of SRDiff, the diffusion model acts like adecoder: the model is conditioned by the output of the back-bone model (e.g. RRDB) to predict the residual betweenthe upsampled LR image and the HR image. Therefore,extending SRDiff to the multiple image setting is a simplematter of switching RDDB to a MISR backbone. A solutionis to use an existing MISR backbone such as HighRes-netor to extend RRDB to the MISR case. This is possible byindividually encoding each image in the sequence with theRRDB encoder, then proceeding to fuse the hidden stateswith a MISR fusion module, such as the recursive moduleof HighRes-net or our time-equivariant module, and finallyusing the decoder to obtain the super-resolved image. Wewill use both replacement and extension strategies in the restof this work.",
  ". Time-Equivariant Super-Resolution": "The MISR formulation provided by Eq. (1) assumes regularacquisitions within a small time frame. It sets a lot of con-straints on the acquisition, as satellite image acquisition islimited by several factors, from the revisit rate to the presenceof clouds or other artifacts, making some (parts of) imagesunusable. Besides, SITS can present temporal and spatialdecorrelations, i.e. the image content can change betweentwo acquisitions due to gradual (e.g. vegetation growth) andabrupt changes (e.g. harvests and urban expansion). A caveatof the MISR approaches is that they assume that the time gapbetween two images is constant or all images are equal, i.e.all images in the sequence contribute the same to the recon-struction of the HR output. However, as we have seen before,none of these assumptions hold. Learning to weigh eachimage in the LR time series should enable super-resolutionmodels to take account of the perturbations affecting the LRimage and to decide which LR images should contributemost to the generation of the HR image.To deal with this problem, we propose a time-dependentMISR approach that relies on the acquisition dates of LR andHR images. The idea is to acknowledge the time differences within the LR time series, but also their time difference withthe HR acquisition. Therefore, the generated solution willconsider information from the entire set of images, assigningdifferent weights to each image. Based on a set of T imageswith acquisitions dates at t1, ..., tT , and a given time tHRwithin the time span, we generate the HR image at a timetHR through a deep neural network h of parameters :",
  "HR(tHR) = h(LR1, t1), ..., (LRT , tT ), tHR(2)": "This formulation enables the generation of an image for anyspecified time within the given time frame.We use the structure of MISR techniques and propose asa fusion module the lightweight temporal attention encoder(L-TAE) , developed originally for crop-type identifica-tion. It is a simplified version of the multi-head self-attentionnetwork designed for fast computation and efficiencypurposes. The self-attention allows each image in the se-quence to contribute differently to the model output. Thishelps deal with changes in the time series: images that arenot in agreement with the HR image will receive less atten-tion. In practice, we use the 2D version of L-TAE adoptedfor panoptic segmentation . It applies a shared L-TAEwith multiple heads applied independently at each pixel. Theidea is to enable individual weighting of the pixels within thepatch to account for the diverse landscapes and the presenceof local perturbations such as clouds and their shadows. Forsimplification, we will use L-TAE to designate L-TAE 2D inthe following.To include the temporal information, Sainte Fare Garnotand Landrieu originally define the positional encodingused in the Transformer layer as the number of days elapsedsince the beginning of the sequence, i.e. for k = 1, . . . , T:",
  "i=1(3)": "where day(tk) denotes the number of days between tk andthe beginning of the sequence, = 1000 is the characteristictime scale, ce is the embedding dimension, and H the num-ber of attention heads. This positional encoding is absoluteregarding the beginning of the time series, and thereforemight not be well-suited to changes in the support of thetime series. For example, removing the first image of thesequence changes the temporal encoding.In contrast, we redefine the positional encoding to use theacquisition date of the target HR image as a reference point.In practice, we replace day(tk) in Eq. (3) with the differencebetween the date of the HR reference and the date of eachLR image in the sequence. Formally, for k = 1, . . . , T,the positional embedding vector p(k) of dimension ce/H isgiven by:",
  ". Study area with the minimal time difference of Sentinel-2and SPOT-6 acquisitions": "Changing the temporal support of the time series, i.e. re-moving or adding an image, does not alter the encoding ofthe other images. At training time, the model h is trainedusing the actual tHR date from the reference HR image. Atinference, the reference time can be set to any time betweent1 and tT . This means that the same sequence of images(LR1, t1), . . . , (LRT , tT ) can be super-resolved at variousreference times and produce different images HRt.",
  ". The BreizhSR dataset": "To evaluate the methods proposed in Sec. 3, we proposeBreizhSR, a cross-sensor MISR dataset. The study area is theregion of Brittany (Breizh in the local language), located onthe northwestern coast of France with an oceanic climate (see). It covers about 35 000 km2 with mostly agriculturalareas (about 80 %). The dataset comprises cross-sensorsatellite data, with low-spatial but high-temporal resolutionimage time series sourced from Sentinel-2 satellites and thecorresponding low-temporal high-spatial resolution imagesprovided by the SPOT-6 satellite.Sentinel-2 constellation has twin satellites launched bythe European Space Agency (ESA) in 2015 and 2017 thatcover all Earths surfaces every five days at the equator.Level-2A images of the BreizhSR dataset are gathered via theTHEIA platform, which employs the MAJA pre-processingalgorithm to obtain atmospherically corrected ground re-flectance . To match the SPOT-6 spectral characteristics,only RGB bands at a 10-meter spatial resolution (B4, B3,and B2) are used in the analysis. The images were collectedfor the nine tiles covering the Brittany region from the 1st",
  ". Comparison of dataset characteristics for SISR/MISR of remote sensing images": "of April 2018 to the 31st of August 2018, filtering imageswith a cloud cover under 5 %. Since the SPOT-6 data was ac-quired in the summer of 2018, the Sentinel-2 time period waschosen to include images from before and after the SPOT-6acquisitions while staying in a range of similar seasonal andclimate conditions.",
  "SPOT-6 is an HR optical Earth-imaging satellite systemoperated by the French national space agency (CNES) andlaunched in 2012. It can capture daily about 6 million km2": "at a spatial resolution of up to 1.5 meters. SPOT-6 is used forcommercial purposes and thus operates mainly on customerdemand. In this work, we focus on pan-sharpened RGBimages with a spatial resolution of 1.5 meters. The usedproducts are radiometrically and geometrically correctedL2A images with an 8-bit value range. They consist of 3 352RGB image tiles of size 3000 3000 px, acquired betweenthe 19th of April and the 3rd of August 2018. Temporal discrepancy Given the high temporal resolu-tion of the Sentinel-2 data, the vast majority of tiles showa low temporal difference from the SPOT-6 image to thetemporally closest Sentinel-2 image, with over 78 % of theSPOT-6 images having a corresponding Sentinel-2 imagewith a difference of 10 days or closer, as can be seen in .The median number of Sentinel-2 images in the time seriesis 8, and the maximum is 17. BreizhSR uniqueness Compared to existing datasets,BreizhSR exhibits several unique characteristics as high-lighted in Tab. 1. It complements existing datasets withwider coverage, smaller than Proba-V but at a significantlyhigher spatial resolution, on a limited geographical regionbut without focusing on specific areas such as WorldStart or MuS2 that cover only urban areas. In addition, World-Strat performs no filtering of cloudy images, reducing thenumber of effectively usable images in the series. Finally,BreizhSR uses LR and HR acquisitions that are close intime (less than two months apart, with a median time dif-ference below 10 days) to mitigate seasonal effects. Thisstrategy differs from the MuS2 dataset, which depends onopen WorldView-2 HR images separated by several yearsfrom the LR series.",
  ". Pre-processing": "We perform super-resolution with an upscaling factor of 4(see ). To reach this factor, the SPOT-6 images weredownsampled through bicubic interpolation from 1.5 m to2.5 m. Sentinel-2 and SPOT-6 images were respectivelycropped into 3 74 74 px and 3 296 296 px. In total,there are 52 255 S2 time series and SPOT-6 image pairs.The images stem from two different sensors, Sentinel-2and SPOT-6, for which the value range and distribution aredifferent. This is not only caused by differences in weatherand radiation conditions at the time of acquisition but alsoby inner differences in terms of spectral bands, atmosphericcorrection, radiometric resolution and data processing. Tomitigate the spectral discrepancy and help the models con-vergence, we first min-max normalise both data sources in-dependently, with statistics computed on the whole trainingset. In practice, we compute 2 % and 98 % percentile as anestimation of minimum and maximum values of Sentinel-2data to take into account the presence of outliers due to arti-facts such as clouds and their shadows. Then, for each pairof images (LR, HR) in the SISR setting or ([LRi]Ti=1, HR)in the MISR setting, we perform a histogram matching ofthe HR image from SPOT-6 towards the distribution of thetemporally closest Sentinel-2 LR image.To evaluate the performance of the proposed methods,the dataset is split into training and test sets using a randomblock sampling strategy. Blocks of 4 4 SPOT-6 tiles (i.e.10 10 km2) are formed to ensure independence betweentraining and testing sets. Out of 52 255 image pairs, 37 234compose the training set (70 %), including 10% reserved forvalidation, and 15 021 compose the test set (30 %).",
  ". Implementation and architectures": "For the SISR setting, we assess the performance of diffu-sion models and conditioning by comparing two existingmodelsSRDiff conditioned with the LR image upsampledthrough bicubic interpolation and the output of the RRDBpre-trained model with the original RRDB super-resolutionmodel. For the MISR setting, we assess the benefits of thefusion module based on temporal attention mechanisms (i.e.L-TAE) by experimenting with three models: (i) HighRes-",
  ". Quantitative results for a 4 upscaling factor. Bold values highlight the best results": "net, (ii) RRDB, and (iii) SRDiff conditioned by HighRes-netwith L-TAE as the fusion module. We compare the results tothe original HighRes-net using the recursive fusion module.All networks are trained using the L1 loss and Adam op-timizer on an NVIDIA A100 80GB. HighRes-net recursivefusion uses two encoding layers and is trained for 300k train-ing steps with a batch size of 32, a learning rate of 6e 4and a 0.7 decay every 50k. The same architecture is used totrain HighRes-net L-TAE. In both SISR and MISR settings,RRDB has 8 blocks and is trained with a learning rate of2e 4 and a batch size of 10. SRDiff uses 500 diffusionsteps and a batch size of 64. We train for 400k steps in SISRand 325k in MISR (SRDiff HighRes-net L-TAE). In MISRexperiments, the length of the time series is set to 8.",
  ". Evaluation metrics": "To assess the performance of the SISR and MISR methods,we compute widely used quality measures averaged over allthe test images. In particular, we use three reconstruction-based metricsmean absolute error (MAE), shift-MAE, androot mean square error (RMSE) and three perception-basedmetricspeak signal-to-noise ratio (PNSR), learned percep-tual image patch similarity (LPIPS) , and structural sim-ilarity index measure (SSIM) .Shift-MAE Slight misalignments between satellite im-ages can occur, especially when using two differentsources. To compensate for the error induced by thosepixel shifts, we consider the Shift-MAE metric, in-spired by the scoring method used in the Proba-V chal-lenge .It consists in computing the MAE on sub-images shifted, and keeping the best value: Shift-MAE =minu,v{0,...,} MAE(HRu,v, SRcenter), where is themaximum shift allowed (in this work, = 6, i.e. 15 me-ters) and SRcenter is the super-resolution image cropped toavoid the border effect and HRu,v the sub-images with itsupper left corner at coordinates (u, v).",
  "We report quantitative evaluations of our models in Tab. 2,both in SISR and MISR settings. First, we observe that MAE": "and Shift-MAE follow the same trends, pointing to a strongsub-pixel co-registration of the two sensors. Then, we ob-serve that SRDiff produces images with the best LPIPS val-ues by a large margin ( 0.3 compared to 0.6 for the RRDBbackbone alone). This indicates a higher perceptual qualityof the super-resolved images. However, SRDiff also obtainsworse pixel-based metrics compared to the other models.Conversely, while the other methods have better pixel-basedmetrics, they present poorer LPIPS results, indicating lowerperceptual quality. These observations regarding LPIPS val-ues are indeed corroborated by the qualitative results from. This observation raises an interrogation: should super-resolution strive for perceptual improvements or pixel-wisefidelity? In other applications, such as photography or videoprocessing, perceptual losses, including LPIPS, are a popularchoice as the focus is more on the visual quality . Inremote sensing, balancing visual quality and pixel accuracyis crucial for some tasks, requiring further investigation intocombining perceptual and pixel-wise losses. Second, we observe that MISR models tend to outperformtheir SISR counterparts. For example, RRDB L-TAE outper-forms the classical RRDB on all metrics, both reconstruction-based and perceptual ones. Another notable conclusion isthat the temporal attention mechanism added to the timeseries through L-TAE achieves the best pixel-based met-rics values among the methods. In particular, HighRes-netL-TAE outperforms the recursive fusion baseline for all re-ported metrics, demonstrating the importance of our time-equivariant design. Although using stronger conditioning with RRDB en-hances the results of SRDiff when it is conditioned sim-ply by the upsampled LR image, SRDiff conditioned byHighRes-net L-TAE produces poorer metrics. MISR modelsare inherently more computationally expensive and challeng-ing to optimize due to their design, as they process multipleimages at once. Their combination with DDPMs, which arealso resource-intensive, makes it difficult to achieve optimalresults. Therefore, the cost of MISR models is also a concernthat must be addressed in future work.",
  "Temporal irregularity To further assess whether L-TAEeffectively captures temporal information, we conduct in": ". Boxplots of MAE results as a function of the time differ-ence between the acquisitions of the SPOT-6 image and the closestSentinel-2 image. From left to right: time difference of less than10 days (10 758 images), between 10 and 30 days (3 633 images),over 30 days (630 images). a performance analysis where we compare the MAEresults of each model depending on the time difference be-tween the acquisitions of the SPOT-6 image and the closestSentinel-2 image. Although the dataset has irregular tem-poral sampling, we observe that time differences betweenSPOT-6 acquisitions and the closest Sentinel-2 image inthe test set are predominantly within 10 days (10 758 im-ages), with 3633 images between 10-30 days, and 630 im-ages above 30 days. This explains why the average scoresof SISR models are similar to MISR models. Still, whenevaluating the subset of images where the time differenceis >30 days, MISR models with L-TAE demonstrate lowerMAE values than SISR models. It indicates that L-TAE isindeed capturing temporal information in the time series,contrary to HighRes-net, which performs worst.Length of the series As SITS in real settings can havevarious lengths, we evaluate our model using different num-bers of images. We report in Tab. 3 the super-resolutionmetrics of HighRes-net L-TAE trained for 8 images and eval-uated on time series of length T = {2, 4, 8}, by dropping theimages furthest away from tHR. Intuitively, we observe thatmore images in the time series result in better reconstructionand perceptual metrics. Yet, we also observe that the drop inperformance is not that steep: even with only 2 images, ourmodel outperforms the HighRes-net recursive fusion trainedon 8 LR images (Tab. 2). This shows that our careful han-dling of the temporal dimensions helps the network selectthe relevant images, making it less dependent on additionalimages that are temporally far from the reference point.Time-equivariant results As detailed in Sec. 3.2, ourmethod can produce HR images at different times. We showthis ability in , with various super-resolved outputsat different timestamps in the series. We observe that thesuper-resolved images follow the dynamics of the time series,with the field getting greener over time. This illustrates theflexibility of our time-equivariant positional encoding, which",
  ". Cloud impact": "To minimize the impact of clouds obscuring the landscapeand making super-resolution predictions challenging, wegathered Sentinel-2 images with a maximum cloud cover of5%. While this criterion significantly reduced the presence ofclouds, some images may still contain cloud cover. As shownin , MISR models can effectively generate images withreduced cloud cover compared to the input Sentinel-2 image,even in cases where the latter is heavily obscured by clouds.Among the MISR models, HighRes-net recursive fusionprovides the best visual result, with a completely cloud-freeoutput. However, when comparing the outputs of the MISRmodels to the corresponding Sentinel-2 time series, the out-put of HighRes-net L-TAE is more coherent with the timeseries. In the original image, the crop in the central region isexpected to be green, but the HighRes-net model with recur-sive fusion produces an output with a brown crop. In otherwords, while HighRes-net recursive fusion produces themost visually pleasing image, HighRes-net L-TAE producesan image that is more faithful to the actual changes in thelandscape. This shows that L-TAE is effectively assigningmore weight to images close to the date on which we chooseto apply the SR. Compared to the other MISR models, thesuper-resolved image generated by SRDiff HighRes-net with",
  ". Conclusion": "In this work, we proposed a time-equivariant formulation forMISR to handle temporally irregular SITS. Our approachtakes advantage of the acquisition dates to generate coherentsuper-resolution images at any specified date and outper-forms SISR techniques over our dataset BreizhSR. This flex-ibility can be useful in practice for downstream tasks. Thiswork paves the way for cross-sensor MISR research but alsoraises several questions. We hypothesize that the use of jointreconstruction and perceptual losses as well as enhanced dif-fusion models should improve our super-resolution models.Besides, we believe that the super-resolution of the full spec-trum offered by Sentinel-2, beyond RGB, deserves furtherinvestigation. AcknowledgementsWe thank the support of GDR IASIS forfunding this work under the SESURE project, the DINAMIS con-sortium, CNES/Airbus and IGN for access to the SPOT-6 data, andESA for access to Sentinel-2 data. During the conduct of this re-search, Simon Donike received a European scholarship to engage inMaster Copernicus in Digital Earth, Erasmus Mundus Joint MasterDegree (EMJMD). We thank Dirk Tiede (Uni. Salzburg) for hishelp and feedback on BreizhSR. This work was performed usingHPC resources from GENCIIDRIS (grant 2022-AD011013003). Tai An, Xin Zhang, Chunlei Huo, Bin Xue, Lingfeng Wang,and Chunhong Pan. TR-MISR: Multiimage super-resolutionbased on feature fusion with transformers. IEEE Journal ofSelected Topics in Applied Earth Observations and RemoteSensing, 15:13731388, 2022. 3",
  "Saeed Anwar, Salman Khan, and Nick Barnes. A deep journeyinto super-resolution: A survey. ACM Computing Surveys(CSUR), 53(3):134, 2020. 2": "Jose Caballero, Christian Ledig, Andrew Aitken, AlejandroAcosta, Johannes Totz, Zehan Wang, and Wenzhe Shi. Real-time video super-resolution with spatio-temporal networksand motion compensation. In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, pages47784787, 2017. 3 Julien Cornebise, Ivan Orsolic, and Freddie Kalaitzis. Openhigh-resolution satellite imagery: The WorldStrat datasetwith application to super-resolution. Advances in NeuralInformation Processing Systems, 35:2597925991, 2022. 3, 5 Michel Deudon, Alfredo Kalaitzis, Israel Goytom, Md Ri-fat Arefin, Zhichao Lin, Kris Sankaran, Vincent Michal-ski, Samira E Kahou, Julien Cornebise, and Yoshua Ben-gio.HighRes-net:Recursive fusion for multi-framesuper-resolution of satellite imagery.arXiv preprintarXiv:2002.06460, 2020. 2, 3",
  "Chao Dong, Chen Change Loy, Kaiming He, and XiaoouTang. Image super-resolution using deep convolutional net-works. IEEE Transactions on Pattern Analysis and MachineIntelligence, 38(2):295307, 2015. 2": "Mikel Galar, Ruben Sesma, Christian Ayala, Lourdes Albizua,and Carlos Aranda. Super-resolution of sentinel-2 imagesusing convolutional neural networks and real ground truthdata. Remote Sensing, 12(18):2941, 2020. 2 Olivier Hagolle, Mireille Huc, David Villa Pascual, and Ger-ard Dedieu. A multi-temporal and multi-spectral methodto estimate aerosol optical thickness over land, for the at-mospheric correction of FormoSat-2, LandSat, VENS andSentinel-2 images. Remote Sensing, 7(3):26682691, 2015. 4 Muhammad Haris, Gregory Shakhnarovich, and NorimichiUkita. Recurrent back-projection network for video super-resolution. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 38973906,2019. 3 Juan Mario Haut, Ruben Fernandez-Beltran, Mercedes EPaoletti, Javier Plaza, and Antonio Plaza. Remote sensingimage superresolution using deep residual channel attention.IEEE Transactions on Geoscience and Remote Sensing, 57(11):92779289, 2019. 2",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-sion probabilistic models. Advances in Neural InformationProcessing Systems, 33:68406851, 2020. 2": "Mohamed Ramzy Ibrahim, Robert Benavente, Felipe Lumbr-eras, and Daniel Ponsa. 3DRRDB: Super resolution of mul-tiple remote sensing images using 3D Residual in ResidualDense Blocks. In 2022 IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops (CVPRW), pages322331. IEEE, 2022. 3 Younghyun Jo, Sejong Yang, and Seon Joo Kim. Investigatingloss functions for extreme super-resolution. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) Workshops, 2020. 6 Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng,Robin Rombach, Marshall Burke, David Lobell, and Ste-fano Ermon. Diffusionsat: A generative foundation model forsatellite imagery. arXiv preprint arXiv:2312.03606, 2023. 2 Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurateimage super-resolution using very deep convolutional net-works. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 16461654, 2016. 2 Pawel Kowaleczko, Tomasz Tarasiewicz, Maciej Ziaja, DanielKostrzewa, Jakub Nalepa, Przemyslaw Rokita, and MichalKawulok. A real-world benchmark for Sentinel-2 multi-imagesuper-resolution. Scientific Data, 10(1):644, 2023. 2, 3, 5 Charis Lanaras, Jose Bioucas-Dias, Silvano Galliani, Em-manuel Baltsavias, and Konrad Schindler. Super-resolutionof Sentinel-2 images: Learning a globally applicable deepneural network. ISPRS Journal of Photogrammetry and Re-mote Sensing, 146:305319, 2018. 2 Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,Andrew Cunningham, Alejandro Acosta, Andrew Aitken,Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative ad-versarial network. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 46814690,2017. 2 Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, HuajunFeng, Zhihai Xu, Qi Li, and Yueting Chen. SRDiff: Singleimage super-resolution with diffusion probabilistic models.Neurocomputing, 479:4759, 2022. 2, 3 Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, andKyoung Mu Lee. Enhanced deep residual networks for singleimage super-resolution. In Proceedings of the IEEE Confer-ence on Computer Vision and Pattern Recognition Workshops,pages 136144, 2017. 2",
  "Julien Michel, Juan Vinasco-Salinas, Jordi Inglada, andOlivier Hagolle. Sen2vens, a dataset for the training ofSentinel-2 super-resolution algorithms. Data, 7(7):96, 2022.2, 5": "Ngoc Long Nguyen, Jeremy Anger, Lara Raad, BrunoGalerne, and Gabriele Facciolo. On the role of alias andband-shift for Sentinel-2 super-resolution. In InternationalGeoscience and Remote Sensing Symposium (IGARSS), 2023.2 Nathanael Carraz Rakotonirina and Andry Rasoanaivo. ESR-GAN+: Further improving enhanced super-resolution genera-tive adversarial network. In IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pages36373641, 2020. 2",
  "Intervention (MICCAI) Conference, pages 234241. Springer,2015. 3": "Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-mans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions onPattern Analysis and Machine Intelligence, 45(4):47134726,2022. 2, 3 Vivien Sainte Fare Garnot and Loic Landrieu. Lightweighttemporal self-attention for classifying satellite images timeseries. In Advanced Analytics and Learning on TemporalData (AALTD): 5th ECML PKDD Workshop, pages 171181.Springer, 2020. 2, 4 Vivien Sainte Fare Garnot and Loic Landrieu. Panoptic seg-mentation of satellite image time series with convolutionaltemporal attention networks. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 48724881, 2021. 2, 4 Francesco Salvetti, Vittorio Mazzia, Aleem Khaliq, and Mar-cello Chiaberge. Multi-image super resolution of remotelysensed images using residual attention deep neural networks.Remote Sensing, 12(14):2207, 2020. 2, 3 Tomasz Tarasiewicz, Jakub Nalepa, and Michal Kawu-lok. Semi-simulated training data for multi-image super-resolution. In IEEE International Geoscience and RemoteSensing Symposium (IGARSS), pages 481484, 2022. 2 Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, HeewonKim, Seungjun Nah, Kyoung Mu Lee, Xintao Wang, YapengTian, Ke Yu, Yulun Zhang, Shixiang Wu, Chao Dong, LiangLin, Yu Qiao, Chen Change Loy, Woong Bae, Jaejun Yoo,Yoseob Han, Jong Chul Ye, Jae-Seok Choi, Munchurl Kim,Yuchen Fan, Jiahui Yu, Wei Han, Ding Liu, Haichao Yu,Zhangyang Wang, Honghui Shi, Xinchao Wang, Thomas S.Huang, Yunjin Chen, Kai Zhang, Wangmeng Zuo, ZhiminTang, Linkai Luo, Shaohui Li, Min Fu, Lei Cao, Wen Heng,Giang Bui, Truc Le, Ye Duan, Dacheng Tao, Ruxin Wang,Xu Lin, Jianxin Pang, Jinchang Xu, Yu Zhao, Xiangyu Xu,Jinshan Pan, Deqing Sun, Yujin Zhang, Xibin Song, YuchaoDai, Xueying Qin, Xuan-Phung Huynh, Tiantong Guo, Hoj-jat Seyed Mousavi, Tiep Huu Vu, Vishal Monga, CristovaoCruz, Karen Egiazarian, Vladimir Katkovnik, Rakesh Mehta,Arnav Kumar Jain, Abhinav Agarwalla, Ch V. Sai Praveen,Ruofan Zhou, Hongdiao Wen, Che Zhu, Zhiqiang Xia, Zheng-tao Wang, and Qi Guo. NTIRE 2017 Challenge on SingleImage Super-Resolution: Methods and Results.In 2017IEEE Conference on Computer Vision and Pattern Recogni-tion Workshops (CVPRW), pages 11101121, 2017. 2 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in NeuralInformation Processing Systems, 30, 2017. 4",
  "Piper Wolters, Favyen Bastani, and Aniruddha Kembhavi.Zooming Out on Zooming In: Advancing Super-Resolutionfor Remote Sensing, 2023-11-29. 2, 5": "Yi Xiao, Qiangqiang Yuan, Kui Jiang, Jiang He, XianyuJin, and Liangpei Zhang. EDiffSR: An efficient diffusionprobabilistic model for remote sensing image super-resolution.IEEE Transactions on Geoscience and Remote Sensing, 2023.2 Wenming Yang, Xuechen Zhang, Yapeng Tian, Wei Wang,Jing-Hao Xue, and Qingmin Liao. Deep learning for singleimage super-resolution: A brief review. IEEE Transactionson Multimedia, 21(12):31063121, 2019. 2 Kexin Zhang, Gencer Sumbul, and Begum Demir. An ap-proach to super-resolution of Sentinel-2 images based ongenerative adversarial networks.In Mediterranean andMiddle-East Geoscience and Remote Sensing Symposium(M2GARSS), pages 6972. IEEE, 2020. 2 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,and Oliver Wang. The unreasonable effectiveness of deepfeatures as a perceptual metric. In Proceedings of the IEEEConference on Computer Cision and Pattern Recognition,pages 586595, 2018. 6 Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, BinengZhong, and Yun Fu. Image super-resolution using very deepresidual channel attention networks. In Proceedings of theEuropean Conference on Computer Vision (ECCV), pages286301, 2018. 2"
}