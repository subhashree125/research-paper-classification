{
  "Abstract": "Extensions of Neural Radiance Fields (NeRFs) to modeldynamic scenes have enabled their near photo-realistic,free-viewpoint rendering.Although these methods haveshown some potential in creating immersive experiences,two drawbacks limit their ubiquity: (i) a significant re-duction in reconstruction quality when the computing bud-get is limited, and (ii) a lack of semantic understandingof the underlying scenes. To address these issues, we in-troduce Gear-NeRF, which leverages semantic informationfrom powerful image segmentation models. Our approachpresents a principled way for learning a spatio-temporal(4D) semantic embedding, based on which we introducethe concept of gears to allow for stratified modeling of dy-namic regions of the scene based on the extent of their mo-tion. Such differentiation allows us to adjust the spatio-temporal sampling resolution for each region in propor-tion to its motion scale, achieving more photo-realistic dy-namic novel view synthesis. At the same time, almost forfree, our approach enables free-viewpoint tracking of ob-jects of interest a functionality not yet achieved by ex-isting NeRF-based methods.Empirical studies validatethe effectiveness of our method, where we achieve state-of-the-art rendering and tracking performance on multi-ple challenging datasets. The project page is available at:",
  "TimeViewpoint": ". (a) Our method takes RGB videos captured from a cam-era array as input. (b) Trained Gear-NeRF achieves photo-realisticreal-time free-viewpoint rendering of a dynamic scene. (c) Withusers giving a single click at any time and from any viewpoint, ourmethod can perform free-viewpoint tracking of the target object. reconstruct dynamic scenes, as the world around us is char-acterized by a constant state of flux, with many objects in it- in a state of motion.Recent advances in novel view synthesis, such as Neu-ral Radiance Fields (NeRFs) have inspired numerousstudies to extend them to dynamic 3D scenes.Existingapproaches either employ a deformation field to map neu-ral fields from a given time to a canonical space , or directly model dynamic scenes as a4D space-time grid . Though these methods of-fer improved rendering quality by utilizing more accessibleinputs compared to previous solutions , theystill struggle to ensure rendering quality in low-resource set-tings, requiring carefully engineered efforts. Further, mostdynamic radiance field approaches adopt a naive spatio-temporal sampling strategy, without discerning the differentscales of motion across different regions in the scene.We propose to fix this issue by leveraging a seman-tic understanding of dynamic scenes. Intuitively, a recon-struction system aware of the distinction between staticand dynamic regions in a scene can perform more focused",
  "Gear 4": ". Pipeline of Gear-NeRF: Gear-NeRF takes multi-view videos as input. After optimizing the serial 4D feature volumes (Sec-tion 4.1), it maps space-time coordinates to a 4D semantic embedding (.2), in addition to the volume density and view-dependentradiance color. Regions with larger motion are automatically assigned higher gear levels (.3) and as a result, receive higher-resolution spatio-temporal sampling (.4). Furthermore, Gear-NeRF is capable of performing free-viewpoint tracking of a targetobject with prompts as simple as a user click (.5). sampling in the dynamic regions, which inherently requiremore resources per unit volume than static regions, due totheir time-evolving nature. Accordingly, dynamic regionscan be further stratified according to their scale of mo-tion. To this end, this paper presents Gear-NeRF, a frame-work that leverages semantic embedding from powerful im-age segmentation models for stratified modeling of 4Dscenes. Gear-NeRF optimizes for a 4D semantic embed-ding, based on which we introduce the concept of gearto smartly determine the appropriate region-specific reso-lution of spatio-temporal sampling in the NeRF. Regionswith larger motion scales are assigned higher gears, throughour gear determination scheme and we accordingly per-form higher-resolution spatio-temporal sampling. Empiri-cal studies reveal that this motion-aware sampling strategyimproves the quality of synthesized images, over competingapproaches.As a by-product of our semantically embeddedrepresentation, we achieve free-viewpoint object tracking,given user prompts. presents an overview of thecapabilities of our method.Gear-NeRF makes two primary advancements: (i) en-hanced dynamic novel view synthesis by resorting tosmarter spatio-temporal sampling, and (ii) the ability forfree-viewpoint tracking of objects of interest. The latteris a capability not yet realized by existing NeRF methodsfor dynamic scenes. We perform extensive experiments onmultiple datasets to validate the generalizability and robust-ness of our method, which shows state-of-the-art perfor-mances for both tasks across all datasets.",
  "Neural Radiance Fields: NeRF is a recent break-through among novel view synthesis methods that uses mul-tilayer perceptrons (MLPs) to parameterize the appearance": "and density for each point in 3D space, given any viewingdirection of the scene. Researchers have extended NeRFalong various dimensions , including improving ren-dering quality , handling challenging con-ditions such as large scenes , view-dependentappearances , and sparse inputs .NeRF-like neural representations have alsofound applications in semantic segmentation and 3D content generation . Recent workhas shown that replacing the deep MLPs with a featurevoxel grid can significantly improve training and inferencespeed . On the other hand, a more recentapproach to further improve visual quality, rendering time,and performance entails representing the scene with 3DGaussians . Our approach, while drawing upon manyof these approaches, deals with dynamic scenes which isbeyond the scope of these methods.Neural Representations for Dynamic Scenes: NeRF-likerepresentations have recently been extended to model dy-namic scenes in high fidelity .One straightforward approachto do this is to directly condition the radiance field ontime . Alternatively, several methods modela deformation field to map coordinates from different timestamps to a common canonical space . Some recent approaches rep-resent the scene using a 4D space-time grid, which is de-composed into sets of planar representations for trainingefficiency.Other techniques for improving rendering fi-delity and frame rate include Fourier PlenOctrees , ray-conditioned sample prediction networks , 4D space de-composition (static/dynamic/newly appeared regions) ,and explicit voxel grids . 3D Gaussians have also beenadapted to model dynamic scenes . Whilethese approaches paved the initial path for rendering dy- namic scenes, semantically aware modeling of the scene isabsent, a caveat that our proposed method seeks to address.Segment Anything Model (SAM): SAM is a pow-erful promptable image segmentation model, which show-cases remarkable zero-shot generalization abilities and canproduce semantically consistent masks, given a single fore-ground point on the image. HQ-SAM is an improve-ment on SAM that enhances the quality of masks, espe-cially on objects with intricate boundaries and structures.Recent works have extended SAM to perform in-teractive video object segmentation. These methods utilizeSAM for mask initialization or correction and then employstate-of-the-art mask trackers for mask tracking andprediction . Recent methods have also leveraged SAMfor tracking multiple reference objects in a video .This work uses SAM to profile the scene into semantic re-gions, which are then grouped based on motion scales.3D Semantic Understanding:Existing methods for3D visual understanding mainly focuson closed set segmentation of point clouds or voxels.NeRFs ability to integrate information across multipleviews has led to its applications in 3D semantic segmen-tation , object segmentation , panopticsegmentation , and interactive segmentation .Kobayashi et al. explored the effectiveness of embed-ding pixel-aligned features into NeRFs for 3D ma-nipulations. LERF fuses CLIP embeddings andNeRFs to enable language-based localization in 3D NeRFscenes. Recent work has enabled click/text-based 3D seg-mentation by learning a 3D SAM embedding or in-verse rendering of SAM-generated masks . We, on theother hand, seek to utilize the synergy of dynamic NeRFsand SAM segments to derive a semantic understanding of adynamic 3D scene for tracking objects of interest in novelviews a first of its kind effort.",
  ". Preliminaries": "Neural Radiance Fields (NeRFs): Vanilla NeRFs employ a multi-layer perceptron (MLP) with sinusoidalpositional encoding to map a 3D-spatial coordinate x =(x, y, z) and a viewing direction d = (, ) to a volumedensity and an emitted RGB, c R3. Renderingeach image pixel involves casting a ray r(t) = o + td fromthe camera center o through the pixel along direction d. Thepredicted color for the corresponding pixel is computed as:",
  "rR C(r) C(r)22.(2)": "where R is the set of all rays projected from the input image.Planar-Factorized 4D Volumes: A recent emerging trendof handling dynamics using radiance field representations isto directly adapt them to be conditioned on a frame index t(denoting time) in addition to x and d. This can be accom-plished by learning a mapping from (x, d, t) to (, c) usingplanar-factorized 4D volumes . These meth-ods attempt to learn a 4D feature vector for every (x, t), byprojecting it to a set of 2D-planes. Embeddings of theseprojections on these planes can then be integrated to obtainthe embedding for the 4D point. This can be mathematicallyrepresented as follows:",
  "+ B3(h3(y, z) k3(x, t)).(3)": "where hi(, ) and ki(, ) are functions (evaluated by bilin-ear interpolation on regularly spaced 2D feature grids) em-bedding coordinate tuples to features of dimension M, denotes an element-wise product, and Bi() denotes a lin-ear transform which maps the products to feature vectors.Subsequently, a tiny MLP can map the feature vector f(, )to the volume density, , and the view-dependent emittedcolor, c, given the viewing direction d.",
  ". Proposed Method": "Given a set of W input videos, V = {V1, V2, , VW } of adynamic scene, with calibrated camera poses, our approachrepresents the scene using a series of 4D feature volumes(.1) along with 4D semantic embeddings (Sec-tion 4.2).Analogous to multiple gears in motor vehicles for op-timizing engine performance, Gear-NeRF stratifies this se-mantically embedded scene representation into Ngear lev-els, based on the motion scales. Each of these levels iscalled a gear. Through our training scheme, regions withlarger motion are assigned higher gear levels (.3)and as a result, are more densely sampled (.4) forimproved dynamic novel view synthesis. Our 4D seman-tic embedding also enables a new functionality, almost forfree free-viewpoint tracking of target objects, given sim-ple user prompts like clicks (.5). showsthe overall pipeline of Gear-NeRF.",
  "+ B3(h3(y, z) kG3 (x, t)).(4)": "The vector-valued functions hj(, ) and linear transformsBj() are shared by all gears, while each gear has itsown spatio-temporal embedding kGj (, ), in M-dimensionalspace. Therefore, each gear describes regions of a certainscale of motion while the purely spatial features can beshared among all gears.We obtain the gear level at any spatio-temporal coor-dinate also from a planar-factorized 4D feature volume.Specifically, the gear level at (x, t) is computed as:",
  ". 4D Semantic Embedding": "Gear-NeRF leverages the strong object priors of theSAM model to acquire a semantic understanding of thescene, for improved photometric rendering (.3 and.4) as well as free-viewpoint object tracking (Sec-tion 4.5). Toward this end, we utilize the SAM encoder toobtain 2D feature maps from the frames of each video. Wethen optimize a 4D SAM embedding field by supervising itwith these 2D feature maps. In particular, the MLP above,F, is configured to output a 4D semantic embedding s for",
  "4th Gear Assignment Update": ". Illustration of Gear Assignment Update: For eachgear assignment update, we calculate the rendering loss map be-tween the rendered RGB-SAM map and the ground truth and iden-tify the centers of the patches with the maximum and minimumlosses, marked in red and green (second column). These points arethen fed into the SAM decoder as positive and negative promptsto generate an upshift mask representing the areas that need to beshifted to a higher gear (last column). After the first gear assign-ment update, we see that the next candidate region for upshift issituated where the horse is located, and so on. Upshift mask colorsimply the gear levels after the update (blue-2, green-3, red-4). a given space-time coordinate in addition to the density, ,and color, c. To render 2D semantic feature maps in a givenview, we compute the semantic feature of a pixel in the fea-ture map by tracing a ray through it and perform volumerendering analogous to Equation 1, as follows:",
  ". Training Scheme with Gear Assignment": "With gear initialization g(x, t) = 1, x, t, the (semanticallyembedded) radiance field optimization and gear assignmentupdating take place in an alternating fashion.Gear Assignment Update: As illustrated in , whenupdating the gear assignment after a period of radiance field optimization, we find the regions rendered most poorly fromthe rendering loss maps and increment their gears for denserspatio-temporal sampling. The following steps lay out theprocess for updating gear assignments to regions: We sample a number of viewpoints and time steps andrender 2D-images/SAM features for it. For every ren-dered RGB-SAM map, we compute a rendering loss map.Each pixel of the rendering loss map is computed as:L(r) := Lpho(r) + LSAM(r). See for exam-ple loss maps. Next, we patchify each rendering loss map to find patcheswith the top-k largest/smallest average loss.The cen-ter coordinate of these patches serve as positive/negativeprompts for the next step. See for example posi-tive (red) / negative (green) prompts. We then feed the ground truth RGB images together withpositive and negative prompts into the SAM decoder to estimate an upshift mask. These masks tend to coverregions that have motions and are not satisfactorily ren-dered with the current sampling resolution. We have mul-tiple upshift masks at different viewpoints and time steps. For every pixel of an upshift mask, we trace a ray andsample points along it and update the gear assignment bypushing g(x, t) towards incremented values.In particular, in the last step, for each pixel within anupshift mask, a corresponding ray is traced that connectsit with the camera center o, along direction d.Next, aset of points are sampled along this ray.The collectionof sampled points, that lie on the rays emanating fromwithin the masked region constitutes the set Supshift={(xupshifti, tupshifti)}Nupshifti=1, where Nupshift represents the totalcount of points sampled from rays pertaining to the maskedregion. We then follow a similar procedure to sample a setof points pertaining to the unmasked region. We denote thisset as: Sstay = {(xstayi, tstayi)}Nstayi=1 , with Nstay indicating thetotal number of points sampled from rays that pertain tothe unmasked area. Next, for each sample point in eachof Supshift and Sstay, we query its current gear level p(x, t).In order to assign new gear levels, we need to update thegear assignment function g(, ), which proceeds with theobjective function:",
  "(x,t)Sstayg(x, t; ) p(x, t)22,": "(11)where denotes the set of optimizable parameters forg(, ). The minimization of the first term encourages sam-ple points within the masked region Supshift to have a gearlevel equal to their current gear incremented by one, re-sulting in an upshift of gear. Conversely, minimizing thesecond term encourages the points in Sstay to maintain theirgear values, thereby encouraging the remaining regions tokeep their current gear levels and avoiding unwanted up-",
  "Lupshift,(12)": "where is the learning rate. Since, g(, ) is essentiallyderived from the embedding functions, hi(, ) and ki(, )for i {1, 2, 3}, the aforementioned optimization stepamounts to updating these embedding functions. Once up-dated, we proceed with a fresh round of gear assignmentto increase the spatio-temporal sampling resolution for theregions that end up at a higher gear level than before.Radiance Field Optimization: With the updated gear as-signment, we increase the resolution of spatio-temporalsampling for the gear-shifted regions (.4) and thenresume optimizing the radiance field. We alternate betweenthe two processes: radiance field optimization (each timefor L epochs), and gear assignment updates until the aver-age variance of each rendering loss map is below a predeter-mined threshold. After this, we optimize the radiance fieldfor an additional L epochs without further gear assignmentupdates.",
  ". Motion-aware Spatio-Temporal Sampling": "In this subsection, we explain our motion-aware spatio-temporal sampling strategy based on assigned gears, per-mitting differential processing of regions at different gearlevels. By temporal sampling, we imply the choice of tem-poral resolution for planar-factorized 4D feature volumes,and by spatial sampling, we mean the strategy used tochoose sampling points along each ray for volume render-ing.Motion-aware Temporal Sampling: To handle the in-creasing intensity of object motion, as reflected by theirgrowing gear levels, we increment the temporal resolutionfor voxel grids. Specifically, kGj in Equation 4 has increas-ing resolution along the time axis, thereby empowering the4D feature volumes to better model the dynamics along thetemporal axis.This ensures fast-moving objects can bemore faithfully modeled without unsightly blurring. Thetemporal resolution for each gears feature volume is deter-mined by linear interpolation between 1 (for G = 1) and thetotal number of frames (for G = Ngear).Motion-aware Spatial Sampling: While denser samplingof points can improve reconstruction accuracy, increasingthe number of sampling points throughout the scene canlead to prohibitive computational costs. Therefore, we pro-pose a 3D point-splitting strategy as illustrated in .We begin by sampling a relatively small number, n, of sam-ples along each ray, assuming it is at the lowest gear level.If a sampled point belongs to a region with a higher gear, asdetermined by p(x, t), we then sample more densely in thatregion. For every sampled point in that region, we split itinto 2p(x,t)1 points, equally spaced within the correspond-ing ray segment (at that gear level).",
  ". Free-Viewpoint Tracking with User Prompts": "Our 4D SAM embedding enables another useful functional-ity, almost for free free-viewpoint object tracking, wherethe user only needs to provide as few as one click to extractthe target object based on the 4D embedding. Next, we de-scribe how, given a user-supplied point click at any arbitraryviewpoint and time step, we obtain the corresponding objectmask at a novel viewpoint and time step.Masks for Novel Viewpoints: The first step for this taskentails finding the 3D correspondence of the user click. Wetrace a ray through the selected pixel, and by utilizing thevolume density, we determine the depth at which the rayintersects with the first object surface it encounters. Thisyields the 3D coordinates of the point of intersection. Sub-sequently, the 3D coordinates of this intersection can be eas-ily mapped into a 2D coordinate within any novel viewpointimage, using the camera pose of the new viewpoint. Along-side the rendered SAM feature map of the novel view, wefeed this coordinate into the SAM decoder to generate theobject mask for the novel view.Masks for Novel Time Steps: For this task, we propagatean object mask to its neighboring time step. Specifically,with an object mask for a specific frame t, we calculate thebounding box of this mask and use this bounding box asa prompt to SAM for neighboring frames t = t + 1 ort = t1. By inputting this prompt along with the renderedSAM feature map at t into the SAM decoder, we can obtainthe object mask for t. Combining the above two processes,we can start from a single click and get the object mask inany viewpoint and time step.",
  ". Experiments": "We assess the performance of our proposed Gear-NeRFfor dynamic novel view synthesis and free-viewpoint ob-ject tracking across a range of challenging datasets, com-paring it with state-of-the-art methods. Through ablationstudies, we provide empirical evidence of the effectivenessof its fundamental components. We kindly refer the readerto our supplementary material for additional experimentaldetails and results, including videos for free-viewpoint ren-dering and tracking.",
  ". Experimental Setup": "Implementation Details: We implement our method usingPyTorch and conduct experiments on an NVIDIA RTX4090 GPU with 24 GB RAM. We divide each input videointo chunks of 100 frames. For every chunk, we train amodel for approximately 2.5 hours. Our 4D feature volumesyield embeddings with a dimension of M = 32. We set thegear number Ngear to 4. We find patches with top-k = 3largest/smallest average loss for gear assignment updates toobtain prompts. The rendering loss map is computed with = 0.01. For the optimization of radiance fields, L = 3and L = 10. In our motion-aware spatial sampling, eachray initially has n = 64 sampling points. We use an ini-tial learning rate of 0.02 for all the parameters and optimizethem using ADAM . For Equation 12, we use = 0.02.Datasets:(i) The Technicolor light field dataset includes diverse indoor environment videos captured by a44 camera rig. We evaluate on 4 sequences (Train, The-ater, Painter, Birthday) at the original 20481088 reso-lution, holding out the same view as prior work (thesecond row and second column) for evaluation. (ii) TheNeural 3D Video dataset includes indoor multi-viewvideo sequences captured by 20 cameras at a resolution of27042028 pixels. We experiment on 6 sequences (CutRoasted Beef, Flame Steak, Coffee Martini, Cook Spinach,Flame Salmon, Sear Steak), downsampling by a factor of 2and holding out the central view (akin to prior work ) forevaluation. (iii) The Google Immersive dataset con-tains light field videos of indoor and outdoor scenes cap-tured by a time-synchronized 46-fisheye camera rig, witha resolution of 25601920 pixels. We experiment with 9sequences from it (Flames, Truck, Horse, Car, Welder, Ex-hibit, Face Paint 1, Face Paint 2, Cave). We downsamplethe video resolution by a factor of 2 and hold out the centralview (like prior work ) for evaluation. For our experi-ments, we adopted the same resolution and held-out viewselection as prior work .Evaluation Metrics: For the task of novel view synthesisof dynamic scenes, we evaluate using the following stan-dard metrics: (i) Peak Signal-to-Noise Ratio (PSNR), (ii)Structural Similarity Index Measure (SSIM) and (iii)Learned Perceptual Image Patch Similarity (LPIPS) ,by comparing the reconstructed frames against ground truthimages. These metrics are computed on the held-out viewand averaged across all frames. For the free-viewpoint ob-ject tracking task, we designate a specific viewpoint andtime step for the user to give prompting clicks.Subse-quently, we assess the quality of the predicted object masksat novel viewpoints.The quality of the object mask isquantified in terms of the Mean Intersection over Union(mIoU) and accuracy (Acc.), which are calculated againstthe ground truth mask, manually annotated utilizing AdobePhotoshop. Additionally, we present the same metrics com-puted for novel time steps, denoted as t-mIoU and t-Acc.Baselines: We run a comprehensive comparison of our",
  "Ground TruthHyperReelOursNeural 3D Video": ". Qualitative comparisons for novel view synthesis on the Technicolor dataset : We qualitatively compare our approachagainst HyperReel and Neural 3D Video . Our approach better recovers fine details like patterns on the toys or stripes on the shirt. method against a range of recent NeRF-based baselinemethods: (i) ST-NeRF , (ii) HexPlane , (iii) Hy-perReel , and (iv) MixVoxels . As the first method toenable promptable free-viewpoint object tracking under theNeRF setting, there is no established direct baseline for thisspecific task. However, SA3D , a recent method for seg-menting static scenes, is treated as a baseline for predictingmasks at novel viewpoints corresponding to the promptedtime step.",
  ". Results": "Dynamic Novel View Synthesis: As shown in ,Gear-NeRF produces high-quality novel view synthesis ofdynamic scenes, accurately modeling real-world sceneswith intricate motions and fine details. For example, pat-terns on the toys or stripes on the shirt, are faithfully ren-dered, resulting in more photo-realistic images comparedto all of the baselines. , presents quantitative com-parisons of our method against the baselines. While Gear-NeRF has longer training (Tr. Time) / inference times (FPS)compared to some baselines, it almost always achieves thebest performance in terms of rendering quality.Free-Viewpoint Tracking:In , we present theobject masks obtained by our method based on the userprompts provided at a specified viewpoint and time step.Specifically, the first row displays masked objects at theprompted viewpoint and time step. The second row showsnovel view masks at the prompted time step. The third rowshows novel view masks at novel time steps. We see thatmasks obtained from Gear-NeRF show precise boundaries, . Quantitative comparisons for dynamic novel view syn-thesis: Our method outperforms all baselines across all datasets onall metrics. We report means over all scenes for each dataset. Bestand second best results are highlighted.",
  "Ours31.800.9360.058204 min6.8": "compared to SA3D. presents quantitative assess-ment of the quality of the masks generated of our methodversus SA3D. Our method exceeds 90% across all met-rics and datasets, demonstrating the effectiveness of our ap-proach. Our gains over SA3D can be attributed to the factthat SA3D, as a static scene segmentation method, does notutilize information across all time frames, whereas our ap-proach does. SA3D is incapable of predicting masks fornovel time steps, and as such, the corresponding entries aremarked as not applicable, a shortcoming which our methoddoes not have.",
  "User Prompt": ". Qualitative comparisons of free-viewpoint object tracking on Technicolor and Neural 3D Video datasets: Ourmethod can obtain desirable object masks, with clear edges, from prompting points provided by users at desired time steps and viewpoints. . Quantitative comparisons for free-viewpoint track-ing: t-mIoU and t-Acc are metrics used for evaluating novel viewmasks at novel time steps, not applicable to SA3D. Reported met-rics are averages over all scenes for each dataset.",
  "Ours93.494.390.692.3": "be used to perform object tracking in the target view usingthe mapped click(s) as prompts. As the quantitative resultsin Table D indicate, our method outperforms SAM-Trackacross all metrics on all datasets for the task of desirednovel/target view object tracking. This may be attributedto our methods capability of learning the semantics of thescene by leveraging the 4D SAM embedding field. A ren-dered SAM feature map is fed into the SAM decoder toobtain the mask of the target object at every time step. Incontrast, SAM-Track uses SAM to acquire the object maskonly for the first frame and employs a mask tracker to obtain masks for subsequent time steps.This is alsodemonstrated in Figure B where our approach better ren-ders the scene without introducing artifacts as opposed toSAM-Track. More qualitative results can be seen in the at-tached video. Table E reveals that our approach better segments thetarget object, given a rendered frame, as compared toSA3D . We attribute this gain to the fact that our methodunlike SA3D reasons about the temporal dynamics of thescene and can thus better assess/predict the location of thetarget object.",
  "Ours (Ngear = 5)27.460.9010.131": "bedding for guiding gear assignment reduces the modelsrendering quality.Number of Gears: We ablate on the numbers of gears. In-creasing the number of gears allows for more fine-grainedmotion-aware spatio-temporal sampling, while increasingthe computational cost. As shown in , a choice of upto 4 gear levels seems optimal, further increasing the num-ber of gears does not result in significant improvements.",
  ". Conclusions": "In this work, we introduced Gear-NeRF, an extensionof dynamic NeRFs that leverages semantic informationfrom powerful segmentation models for stratified model-ing of dynamic scenes. Our approach learns a 4D (spatio-temporal) semantic embedding and introduces the con-cept of gears for differentiated modeling of scene re-gions based on their motion intensity.With determinedgear assignments, Gear-NeRF adaptively adjusts its spatio-temporal sampling resolution to improve the photo-realismof rendered views. At the same time, Gear-NeRF providesthe new functionality of free-viewpoint object tracking withuser prompts as simple as a click. Our empirical studies un-derscore the effectiveness of Gear-NeRF, showcasing state-of-the-art performance in both rendering quality and objecttracking across multiple challenging datasets.",
  "A. Appendix": "Webeginthisappendixbyreportingper-scene rendering resultsofGear-NeRFcomparedto competing methods, both qualitatively and quantita-tively. In Section A.2, we present performance comparisonsfor the task of tracking in novel views, a new contributionof this work, and compare against baselines adapted forthis task.We then present additional ablation studies,discussing the sensitivity of our method to the choice ofappropriate hyper-parameters in Section A.3.",
  "A.1. Per-Scene Rendering Results": "In this section, we present a quantitative evaluation of Gear-NeRF and competing techniques for the task of renderingdynamic scenes from novel views, on a per-scene basis foreach of the three datasets we conduct experiments on: (i)The Technicolor Lightfield Dataset (ii) The Neural 3DVideo Dataset , and the (iii) The Google ImmersiveDataset . Moreover, to further demonstrate the gener-alizability of our method vis-a-vis our closest competingbaseline, HyperReel , we report its performance versusthat of our method on some additional sequences for eachof these three datasets. Table A, Table B, and Table C show per-scene quanti-tative comparison results of our approach against compet-ing methods on the Technicolor dataset , the Neural3D Video dataset , and the Google Immersive dataset, respectively. The averaged results are presented in Ta-ble 1 of the paper and are derived from these per-scene re-sults. We see that in all but a couple of sequences (CutRoasted Beef from the Neural 3D video dataset oe The-ater from the Technicolor dataset) our proposed approach",
  "Ours29.680.8800.144": "outperforms all other competing methods, across all themetrics, attesting to the effectiveness of our method. Evenunder occasional circumstances when that is not the case,our method still reports performance comparable to Hyper-Reel.Figure A presents qualitative comparisons of ren-dering results, by our method and HyperReel for some se-quences from the Google Immersive and the Neural3D Video datasets. As is evident from the figure, theframes synthesized by our method look less blurry and bet-ter preserves the details (for instance the eye of the lady, theflame, the stem of the glass, or the glasses of the man withthe hat) which underscores the effectiveness of our method.More qualitative results can be seen in the attached video.",
  "A.2. Novel-View Tracking Results": "Being the first method to achieve free-viewpoint tracking oftarget objects in the NeRF setting, our approach does nothave direct baselines, to the best of our knowledge. Hence,we use the following as baselines for benchmarking: (i)The static scene segmentation approach, SA3D men-tioned in of the main paper. (ii) We also compare",
  "Figure C. Gear selection and rendering of non-Lambertian ob-jects": "against a monocular video tracking baseline called SAM-Track a method based on SAM for object track-ing in monocular videos. Since SAM-Track only takes amonocular video as input and does not consider the 3D in-formation, we adopted the following procedure to use it asa baseline: Given user-provided click(s) in an input view,we utilize our radiance field representation to map theseclicks to a desired target/novel view. SAM-Track can then",
  "A.3. Additional Ablation Studies": "In this section, we present some additional ablation resultson the hyper-parameters of our model.Top-k in Gear Assignment Updates: For gear assignmentupdates, we employ a patch-based approach to identify re-gions with the top-k highest or lowest average renderingloss to obtain positive or negative prompts for subsequentsteps. We perform an ablation study on the Truck sceneof the Google Immersive dataset . Table F reveals that",
  "Ours (2p(x, t) 1)26.460.8150.140": "both excessively high or low values of k do not yield opti-mal performance. We note that a selection of k = 4 or 5leads to gear upshifts for inappropriate regions, weakeningthe efficacy of our motion-aware spatio-temporal samplingstrategy. In our experiments, we uniformly applied k = 3across all scenes, which yielded satisfactory results.Sampling Point Splitting: In our motion-aware spatialsampling, we adopt a 3D sampling point-splitting strategy.Specifically, we split each sampled 3D point into 2p(x,t)1 points. We conduct an ablation study on the number ofpoints a sampling point is split into. To elaborate, in ad-dition to splitting one point into 2p(x,t)1 points, we ex-plore variants, including splitting into 3p(x,t)1 points and2p(x, t) 1 points, on the Truck scene of the Google Im-mersive dataset . As shown in Table G, the additionalsampling points generated by the 2p(x, t) 1 strategy areinsufficient, resulting in a decrease in rendering quality. Incontrast, 3p(x,t)1 achieves better quality than 2p(x,t)1.However, an excessive number of sampling points leads toa reduction in training speed, while providing a marginalperformance boost, which is why we stick with the strategyof splitting into 2p(x,t)1 points. BenjaminAttal,Jia-BinHuang,ChristianRichardt,Michael Zollhoefer, Johannes Kopf, Matthew OToole, andChangil Kim. HyperReel: High-fidelity 6-DoF video withray-conditioned sampling.In IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2023.1, 2, 3, 6, 7, 8, 9, 10 Jonathan T Barron, Ben Mildenhall, Matthew Tancik, PeterHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.Mip-nerf: A multiscale representation for anti-aliasing neu-ral radiance fields. In IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 58555864, 2021. 2",
  "anti-aliased neural radiance fields. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 54705479, 2022": "Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul PSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In IEEE/CVF InternationalConference on Computer Vision (ICCV), pages 1969719705, 2023. 2 Michael Broxton, John Flynn, Ryan Overbeck, Daniel Er-ickson, Peter Hedman, Matthew Duvall, Jason Dourgarian,Jay Busch, Matt Whalen, and Paul Debevec. Immersivelight field video with a layered mesh representation. ACMTransactions on Graphics (TOG), 39(4):861, 2020. 6, 7,8, 9, 10, 11",
  "Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, WeiShen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, andQi Tian. Segment anything in 3d with nerfs, 2023. 3, 7, 8,10, 11": "Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,and Gordon Wetzstein.pi-gan: Periodic implicit gener-ative adversarial networks for 3d-aware image synthesis.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 57995809, 2021. 2 Eric R Chan, Connor Z Lin, Matthew A Chan, KokiNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis,et al. Efficient geometry-aware 3d generative adversarialnetworks. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1612316133, 2022. 2",
  "Jiaben Chen and Huaizu Jiang. Sportsslomo: A new bench-mark and baselines for human-centric video frame interpo-lation. IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), 2024. 3": "Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang,Ziyao Zeng, and Jianbo Shi. iquery: Instruments as queriesfor audio-visual sound separation. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 1467514686, 2023. 3 Xiaokang Chen, Kwan-Yee Lin, Chen Qian, Gang Zeng,and Hongsheng Li. 3d sketch-aware semantic scene com-pletion via semi-supervised structure prior. In IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), 2020. 3",
  "Xiaokang Chen, Jiaxiang Tang, Diwen Wan, Jingbo Wang,and Gang Zeng. Interactive segment anything nerf with fea-ture imitation. arXiv preprint arXiv:2305.16233, 2023. 3": "Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, XiaoyuLi, Yu Guo, Jue Wang, and Fei Wang. Uv volumes for real-time rendering of editable free-view human performance.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1662116631, 2023. 2 Zhang Chen, Zhong Li, Liangchen Song, Lele Chen, JingyiYu, Junsong Yuan, and Yi Xu.Neurbf: A neural fieldsrepresentation with adaptive radial basis functions.InIEEE/CVF International Conference on Computer Vision(ICCV), pages 41824194, 2023. 2",
  "Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li,Zongxin Yang, Wenguan Wang, and Yi Yang. Segment andtrack anything. arXiv preprint arXiv:2305.06558, 2023. 3,10, 11": "Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett,Dennis Evseev, David Calabrese, Hugues Hoppe, AdamKirk, and Steve Sullivan.High-quality streamable free-viewpoint video. ACM Transactions on Graphics (TOG),34(4):113, 2015. 1 Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenen-baum, and Jiajun Wu. Neural radiance flow for 4d viewsynthesis and video processing. In IEEE/CVF InternationalConference on Computer Vision (ICCV), pages 1430414314. IEEE Computer Society, 2021. 1, 2 Zhiwen Fan, Peihao Wang, Xinyu Gong, Yifan Jiang, De-jia Xu, and Zhangyang Wang. Nerf-sos: Any-view self-supervised object segmentation from complex real-worldscenes. International Conference on Learning Represen-tations (ICLR), pages arXiv2209, 2023. 3 Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-aopeng Zhang, Wenyu Liu, Matthias Niener, and Qi Tian.Fast dynamic radiance fields with time-aware neural vox-els. In ACM Transactions on Graphics (SIGGRAPH Asia),pages 19, 2022. 2 Sara Fridovich-Keil, Alex Yu, Matthew Tancik, QinhongChen, Benjamin Recht, and Angjoo Kanazawa.Plenox-els: Radiance fields without neural networks. In IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 55015510, 2022. 2 Sara Fridovich-Keil, Giacomo Meanti, Frederik RahbkWarburg, Benjamin Recht, and Angjoo Kanazawa.K-planes: Explicit radiance fields in space, time, and appear-ance. In IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1247912488, 2023. 1,2, 3",
  "Rahul Goel, Dhawal Sirikonda, Saurabh Saini, and P.J.Narayanan. Interactive Segmentation of Radiance Fields.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2023. 3": "Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-Hai Zhang. Nerfren: Neural radiance fields with reflections.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1840918418, 2022. 2 Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, LinGao, Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip repre-sentation for efficient anti-aliasing neural radiance fields. InIEEE/CVF International Conference on Computer Vision(ICCV), pages 1977419783, 2023. 2",
  "Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Re-current slice networks for 3d segmentation on point clouds.IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2018. 3": "Mustafa Isk, Martin Runz, Markos Georgopoulos, TarasKhakhulin,JonathanStarck,LourdesAgapito,andMatthias Niener. Humanrf: High-fidelity neural radiancefields for humans in motion. ACM Transactions on Graph-ics (SIGGRAPH), 2023. 2 Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerfon a diet: Semantically consistent few-shot view synthe-sis. In IEEE/CVF International Conference on ComputerVision (ICCV), pages 58855894, 2021. 2",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization. ICLR, 2015. 6": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao, SpencerWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segmentanything.IEEE/CVF International Conference on Com-puter Vision (ICCV), 2023. 2, 3, 4, 5, 10 Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, TimWalter, and Matthias Niener. Nersemble: Multi-view ra-diance field reconstruction of human heads. ACM Transac-tions on Graphics (SIGGRAPH), 2023. 2 Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-mann. Decomposing nerf for editing via feature field distil-lation. Advances in Neural Information Processing Systems(NeurIPS), 35:2331123330, 2022. 2, 3",
  "Hao Li, Bart Adams, Leonidas J Guibas, and Mark Pauly.Robust single-view geometry and motion reconstruction.ACM Transactions on Graphics (TOG), 28(5):110, 2009.1": "Hao Li, Linjie Luo, Daniel Vlasic, Pieter Peers, JovanPopovic, Mark Pauly, and Szymon Rusinkiewicz. Tempo-rally coherent completion of dynamic shapes. ACM Trans-actions on Graphics (TOG), 31(1):111, 2012. 1 Tianye Li, Mira Slavcheva, Michael Zollhoefer, SimonGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,Steven Lovegrove, Michael Goesele, Richard Newcombe,et al.Neural 3d video synthesis from multi-view video.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 55215531, 2022. 2, 6, 7, 8, 9,10, 11 Zhengqi Li, Simon Niklaus, Noah Snavely, and OliverWang. Neural scene flow fields for space-time view syn-thesis of dynamic scenes. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 64986508, 2021. 1, 2 Zhengqi Li, Qianqian Wang, Forrester Cole, RichardTucker, and Noah Snavely.Dynibar: Neural dynamicimage-based rendering. In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 42734284, 2023. 2 Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,Hujun Bao, and Xiaowei Zhou. Efficient neural radiancefields for interactive free-viewpoint video. In ACM Trans-actions on Graphics (SIGGRAPH Asia), pages 19, 2022. Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He,Hujun Bao, and Xiaowei Zhou. Im4d: High-fidelity andreal-time novel view synthesis for dynamic scenes. ACMTransactions on Graphics (SIGGRAPH Asia), 2023. 2 Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object.In IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages92989309, 2023. 2 Xinhang Liu, Jiaben Chen, Huai Yu, Yu-Wing Tai, and Chi-Keung Tang.Unsupervised multi-view object segmenta-tion using radiance field propagation.Advances in Neu-ral Information Processing Systems (NeurIPS), 35:1773017743, 2022. 2, 3 Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai,and Chi-Keung Tang.Deceptive-nerf: Enhancing nerfreconstruction using pseudo-observations from diffusionmodels. arXiv preprint arXiv:2305.15171, 2023. 2",
  "Jonathon Luiten, Georgios Kopanas, Bastian Leibe, andDeva Ramanan.Dynamic 3d gaussians:Trackingby persistent dynamic view synthesis.arXiv preprintarXiv:2308.09713, 2023. 2": "Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-BinHuang, Changil Kim, Min H Kim, and Johannes Kopf.Progressively optimized local radiance fields for robustview synthesis. In IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1653916548,2023. 2 Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis.InEuropean Conference on Computer Vision(ECCV), 2020. 1, 2, 3 Ashkan Mirzaei, Tristan Aumentado-Armstrong, Kon-stantinos G Derpanis, Jonathan Kelly, Marcus A Brubaker,Igor Gilitschenski, and Alex Levinshtein. Spin-nerf: Mul-tiview segmentation and perceptual inpainting with neuralradiance fields. In IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 2066920679,2023. 3",
  "Thomas Muller,Alex Evans,Christoph Schied,andAlexander Keller. Instant neural graphics primitives witha multiresolution hash encoding.ACM Transactions onGraphics (TOG), 41(4):115, 2022. 2": "Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan.Regnerf: Regularizing neural radiance fields for view syn-thesis from sparse inputs.In IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages54805490, 2022. 2 Keunhong Park, Utkarsh Sinha, Jonathan T Barron, SofienBouaziz, Dan B Goldman, Steven M Seitz, and RicardoMartin-Brualla. Nerfies: Deformable neural radiance fields.In IEEE/CVF International Conference on Computer Vi-sion (ICCV), pages 58655874, 2021. 1, 2 Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan TBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz.Hypernerf:A higher-dimensional representation for topologically varying neu-ral radiance fields. ACM Transactions on Graphics (TOG),2021. 1, 2 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, Zem-ing Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:An imperative style, high-performance deep learning li-brary. Advances in Neural Information Processing Systems(NeurIPS), 32, 2019. 6 Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xi-aowei Zhou. Representing volumetric videos as dynamicmlp maps. In IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 42524262, 2023.2",
  "Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-hall. Dreamfusion: Text-to-3d using 2d diffusion. Inter-national Conference on Learning Representations (ICLR),2023. 2": "Albert Pumarola, Enric Corona, Gerard Pons-Moll, andFrancesc Moreno-Noguer. D-nerf: Neural radiance fieldsfor dynamic scenes. In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 1031810327, 2021. 1, 2 Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-BinHuang, and Ming C Lin. Dynamic mesh-aware radiancefields. In IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 385396, 2023. 2",
  "Krueger, and Ilya Sutskever. Learning transferable visualmodels from natural language supervision. In Proceedingsof Machine Learning Research (PMLR), pages 87488763,2021. 3": "Konstantinos Rematas,Andrew Liu,Pratul P Srini-vasan, Jonathan T Barron, Andrea Tagliasacchi, ThomasFunkhouser, and Vittorio Ferrari.Urban radiance fields.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1293212942, 2022. 2 Zhongzheng Ren,Aseem Agarwala,Bryan Russell,Alexander G. Schwing, and Oliver Wang. Neural volumet-ric object selection. In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2022. 3 Neus Sabater, Guillaume Boisson, Benoit Vandame, PaulKerbiriou, Frederic Babon, Matthieu Hog, Remy Gendrot,Tristan Langlois, Olivier Bureller, Arno Schubert, et al.Dataset and pipeline for multi-view light-field video.InIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) Workshops, pages 3040, 2017. 6, 7,8, 9, 11 Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural4d decomposition for high-fidelity dynamic reconstructionand rendering. In IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1663216642,2023. 2, 3 Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul`o, Nor-man Muller, Matthias Niener, Angela Dai, and PeterKontschieder. Panoptic lifting for 3d scene understandingwith neural fields. In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 90439052,2023. 3 Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, LeleChen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerf-player: A streamable dynamic scene representation withdecomposed neural radiance fields.IEEE Transactionson Visualization and Computer Graphics (TVCG), 29(5):27322742, 2023. 2 Cheng Sun, Min Sun, and Hwann-Tzong Chen.Directvoxel grid optimization: Super-fast convergence for radi-ance fields reconstruction.In IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages54595469, 2022. 2 Matthew Tancik, Vincent Casser, Xinchen Yan, SabeekPradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan TBarron, and Henrik Kretzschmar.Block-nerf: Scalablelarge scene neural view synthesis. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 82488258, 2022. 2",
  "Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-bardi, et al. Advances in neural rendering. In ComputerGraphics Forum, pages 703735. Wiley Online Library,2022. 2": "Edgar Tretschk,Ayush Tewari,Vladislav Golyanik,Michael Zollhofer,Christoph Lassner,and ChristianTheobalt. Non-rigid neural radiance fields: Reconstructionand novel view synthesis of a dynamic scene from monocu-lar video. In IEEE/CVF International Conference on Com-puter Vision (ICCV), pages 1295912970, 2021. 2 Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zick-ler, Jonathan T Barron, and Pratul P Srinivasan.Ref-nerf: Structured view-dependent appearance for neural ra-diance fields. In IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 54815490. IEEE,2022. 2",
  "Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, andHuaping Liu. Mixed neural voxels for fast multi-view videosynthesis. IEEE/CVF International Conference on Com-puter Vision (ICCV), 2023. 2, 7, 9, 10": "Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao,Yanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu,and Lan Xu.Fourier plenoctrees for dynamic radiancefield rendering in real-time. In IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages1352413534, 2022. 2 Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero PSimoncelli. Image quality assessment: from error visibilityto structural similarity. IEEE Transactions on Image Pro-cessing (TIP), 13(4):600612, 2004. 6 Ziyan Wang, Giljoo Nam, Tuur Stuyck, Stephen Lom-bardi, Chen Cao, Jason Saragih, Michael Zollhofer, Jes-sica Hodgins, and Christoph Lassner. Neuwigs: A neuraldynamic model for volumetric hair capture and animation.In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 86418651, 2023. 2 Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xi-aopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xing-gang Wang. 4d gaussian splatting for real-time dynamicscene rendering. arXiv preprint arXiv:2310.08528, 2023. 2 Rundi Wu, Ben Mildenhall, Philipp Henzler, KeunhongPark, Ruiqi Gao, Daniel Watson, Pratul P Srinivasan, DorVerbin, Jonathan T Barron, Ben Poole, et al.Reconfu-sion: 3d reconstruction with diffusion priors. arXiv preprintarXiv:2312.02981, 2023. 2 Jamie Wynn and Daniyar Turmukhambetov. Diffusionerf:Regularizing neural radiance fields with denoising diffusionmodels. In IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 41804189, 2023. 2 Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and ChangilKim. Space-time neural irradiance fields for free-viewpointvideo. In IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 94219431, 2021. 2",
  "Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, andLi Zhang. Real-time photorealistic dynamic scene repre-sentation and rendering with 4d gaussian splatting. arXivpreprint arXiv:2310.10642, 2023. 2": "Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, andKwan-Yee Lin.Monohuman: Animatable human neu-ral field from monocular video.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1694316953, 2023. 2 Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, and StevenLovegrove. Star: Self-supervised tracking and reconstruc-tion of rigid objects in motion with neural rendering. InIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1314413152, 2021. 2 Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yan-shun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, andJingyi Yu.Editable free-viewpoint video using a lay-ered neural representation. ACM Transactions on Graphics(TOG), 40(4):118, 2021. 1, 2, 7, 9, 10 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 586595, 2018. 6"
}