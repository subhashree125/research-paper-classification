{
  "Abstract": "Visual Language Tracking (VLT) enhances single objecttracking (SOT) by integrating natural language descriptionsfrom a video, for the precise tracking of a specified object.By leveraging high-level semantic information, VLT guidesobject tracking, alleviating the constraints associated withrelying on a visual modality. Nevertheless, most VLT bench-marks are annotated in a single granularity and lack a co-herent semantic framework to provide scientific guidance.Moreover, coordinating human annotators for high-qualityannotations is laborious and time-consuming. To addressthese challenges, we introduce DTLLM-VLT, which auto-matically generates extensive and multi-granularity text toenhance environmental diversity. (1) DTLLM-VLT gener-ates scientific and multi-granularity text descriptions us-ing a cohesive prompt framework. Its succinct and highlyadaptable design allows seamless integration into variousvisual tracking benchmarks. (2) We select three prominentbenchmarks to deploy our approach: short-term tracking,long-term tracking, and global instance tracking. We of-fer four granularity combinations for these benchmarks,considering the extent and density of semantic informa-tion, thereby showcasing the practicality and versatility ofDTLLM-VLT. (3) We conduct comparative experiments onVLT benchmarks with different text granularities, evalu-ating and analyzing the impact of diverse text on track-ing performance. Conclusionally, this work leverages LLMto provide multi-granularity semantic information for VLTtask from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers. In the future,we believe this work can be extended to more datasets tosupport vision datasets understanding. c",
  "MGIT 362Sequence length: 12703 Official annotation: A white goose walks to a room in the yard,": "and then the goose is fed by a man with blue jeans in the room. After that, the goose walks to a basin filled with water, andplays in the basin. Then the goose walks to a small pond with many goldfish in the yard, and plays in the pond. Finally, the goose walks to a lake, and plays in the lake. . Examples of video content and semantic descriptionson OTB99 Lang , LaSOT , and MGIT benchmarks.The green bounding box (BBox) indicates ground truth, while thered dashed BBox indicates other objects that satisfy the seman-tic description. (a) and (b) are short sequences in OTB99 Langwith simple narrative content. Besides, their semantic annotationsmainly describe the first frame, which may misguide the algo-rithm. (c) Comparison of different text annotations, video length,and content on three benchmarks. The VLT environment is com-plex, variable and most of them suffer from issues of inconsistenttext styles and single annotation granularity.",
  "arXiv:2405.12139v2 [cs.CV] 9 Oct 2024": "sual modality greatly constrains the versatility of such sys-tems. Consequently, several studies have begun providingsemantic annotations for the SOT task, leading to the emer-gence of the visual language tracking (VLT) task. The pro-posal of VLT task helps the research of SOT to be morehuman-like and broaden its application prospects. Naturallanguage, in contrast to bounding boxes (BBox), provides amore user-friendly and intuitive way of describing objects,allowing for precise descriptions ranging from spatial lo-cations to high-level semantic details to improve trackingperformance. When defining the VLT task, researchers in-corporate text annotations from two main viewpoints:(1) Short text annotation. Representative VLT bench-marks such as OTB99 Lang , TNL2K , and LaSOT primarily employ short text. This concise styleof description is clear and uncomplicated, facilitating thelearning and comprehension of VLT trackers. The utiliza-tion of short text offers the benefit of simplicity and en-hanced comprehension for VLT trackers. However, thesemethods are prone to imprecise semantic descriptions andpotential ambiguities. As illustrated in (a) and (b),the description only captures the state of the object at thesequence beginning. As the object moves, the positionalconstraint in the semantic information becomes misleading.The reason lies in the benchmark focus primarily on the ini-tial state of the object, neglecting changes in the objectsmotion throughout the video. Consequently, semantic de-scriptions may become restrictive later in the sequence.(2) Long text annotation.MGIT adopts a multi-granular semantic annotation strategy from the perspectiveof more precise semantic descriptions, providing a way toannotate complex spatio-temporal causal relationships inlong videos. Compared to other benchmarks, this style ex-hibits two characteristics: longer text and periodic updates,evolving from simple to dense, detailed descriptions. How-ever, this approach faces challenges like time-intensive textannotations and the need for algorithms with robust textprocessing and multi-modal alignment capabilities to effec-tively utilize the information. As shown in (c), thetext in MGIT is overly long and complex. Clearly, althoughthe motivation of these works is to extend SOT task to multi-modal one to enhance tracking performance, the disparatestyles and singular granularity across most studies not onlyhinder algorithms from achieving the desired outcomes butalso escalate the complexity of research on VLT task.In summary, diverse motivations in existing research re-sult in varying approaches to integrating textual informa-tion. In (c), the three prominent benchmarks differin sequence length, text style, and annotation granularity.Imposing a single standard mechanism for VLT researchappears impractical, given the inherent flexibility and vari-ability in human comprehension and processing of multi-modal information. Humans can adeptly leverage various types of multi-modal information. Rather than enforcinga rigid task format, optimal design should furnish algo-rithms with comprehensive environmental data to exploretheir capabilities and limitations.By offering diverse text descriptions of the environ-mentencompassing short, long, sparse, and dense for-matsand evaluating algorithm performance across thesedescriptions, we can effectively discern the strengths andweaknesses of existing methods under different semanticgranularities, thereby guiding the enhancement of multi-modal algorithms. What excites us is that the Large Lan-guage Model (LLM) can facilitate the achievement of thisgoal. By seamlessly integrating the LLM into the text gen-eration process, we can offer a varied multi-modal environ-ment conducive to VLT research.Our work focuses on the aforementioned motivationsand designs DTLLM-VLT to achieve diverse text genera-tion for tracking datasets. Specifically, we combine textlength and generation density to form four granularitieswith a uniform style. Based on this, we select MMTrack, a state-of-the-art (SOTA) VLT tracker, for experimen-tal analysis to verify the impact of diverse texts on algorithmperformance.The experimental results not only demon-strate that this diversified environment can assist in fine-grained evaluation and analysis of algorithm capabilities butalso suggest the possibility of further enhancing the multi-modal learning capabilities of algorithms using generateddata in the future.The contributions of this paper can be summarized in thefollowing three aspects: We develop DTLLM-VLT, a model based on LLM, aimedat efficiently generating high-quality scientific text fortracking datasets at scale. DTLLM-VLT can seamlesslyapply to various tracking tasks. We generate diverse text for three prominent VLT bench-marks, addressing four levels of granularity.This ap-proach overcomes the limitations of previous bench-marks, which focused on a single granularity and lackeda unified semantic framework. We conduct an experimental analysis to evaluate the im-pact of diverse texts on algorithm performance. The re-sults highlight the benefits of a diversified environmentand indicate the potential for enhancing multi-modallearning through generated text data.",
  ". Single Object Tracking Benchmark": "The SOT task involves initializing and tracking a specificobject within a video sequence. It begins by identifying theobject through its BBox in the first frame and then proceedsto locate and follow the object across subsequent frames.Since 2013, several benchmarks such as OTB . Summary of current popular tracking benchmarks and Comparison number of language description between official and ourgenerated text. Italics indicate automatic generation. We provide far more diverse semantic information than the original annotations forrepresentative environments.",
  "As the ground truth of the MGIT test set is not open-sourced, we only generated text for 120 video of the training and validation sets": "and VOT have been introduced, providing standard-ized datasets and scientific evaluation mechanisms to sup-port SOT research.However, with the advancements indeep learning techniques, these short-term and small-scalebenchmarks have faced challenges in adequately accom-modating data-driven trackers. Consequently, researchershave started designing larger-scale datasets such as GOT-10k and TrackingNet . Additionally, efforts havebeen made to gather data featuring long videos, leading tothe creation of long-term tracking benchmarks like OxUvA and VOT LT . Some work has also focused onSOT in drone scenarios, such as BioDrone , a visionbenchmark for SOT based on bionic drones. Recently, re-searchers have acknowledged that traditional approaches toboth short-term and long-term tracking are based on thepremise of constant movement, a factor that restricts test-ing to situations involving a single camera view and a staticscene. To expand beyond these limitations, they have intro-duced the global instance tracking task along with a novelbenchmark called VideoCube , which enables the track-ing of arbitrary moving objects in various types of videos.To scientifically evaluate the performance of trackers underdifferent challenging factors, researchers have introducedSOTVerse , a user-defined space for SOT task.",
  ". Visual Language Tracking Benchmark": "While visual benchmarks have undergone significant evo-lution over the past decades, benchmarks integrating vi-sual and semantic information, known as VLT benchmarks,have only recently gained traction.OTB99 Lang stands out as the first VLT benchmark, enhancing sequencesfrom the OTB100 benchmark with additional natu-ral language descriptions. However, the limited scale ofthe dataset has hindered the widespread adoption of theVLT task. Subsequently, the release of LaSOT ,a long-term tracking benchmark with natural language an-notations, marked a significant development. Concurrently,researchers introduced the TNL2K benchmark in thesame year, aiming to enhance object tracking flexibilityand accuracy through text descriptions.Following theseefforts, researchers proposed a new multi-modal bench- mark named MGIT , which fully represents the complexspatio-temporal and causal relationships present in long nar-rative content through a multi-granular annotation strategy.These three benchmarks have enriched the pool of availabledata and facilitated the development of various VLT track-ers.",
  ". Algorithms for Visual Language Tracking": "VLT emerges as a burgeoning multi-modal task aiming toachieve tracking by leveraging both a language descriptionand an initial template patch. Following the principle ofsimilarity-matching, most existing VLT methods utilize language descriptions and template patches as ref-erences to identify the most similar object in the searchframe.Among these methods, SNLT presents anadaptable language-based region proposal network that im-proves tracking accuracy by employing a dynamic aggre-gation mechanism. Meanwhile, MMTrack introducesa streamlined and effective tracking method, treating theVLT task as a sequence of token generation.However,these methods often fail to capture the dynamic proper-ties of the object, which becomes a critical issue for robusttracking when the objects appearance undergoes signifi-cant changes. To overcome this shortcoming, some VLTtrackers have begun to integrate temporal data to estab-lish a more dynamic reference. For instance, GTI andAdaSwitcher identify object by merging tracking andlocalization outcomes at every time interval. JointNLT also takes a step towards this by including temporal infor-mation as queries during the prediction phase. Most benchmarks for VLT provide only one natural lan-guage description per video.Additionally, the existingbenchmarks suffer from inconsistent text annotation styles,leading to varied mechanisms for incorporating text in-formation. These discrepancies hinder algorithm evalua-tion and comprehension of video content. Moreover, theseworks all provide semantic information in the form of man-ually annotated data, which is a time-consuming and labor-intensive process. . Comparison of Manual Annotation and Automatic Generation and Framework of DTLLM-VLT. (a) Manual annotation relieson human labor, only provides one text annotation for each video segment, and cannot guarantee a uniform style. The cost of large-scaleannotation is too high. (b) Automatic Generation can generate diverse text on a large-scale in a unified style. (c) The DTLLM-VLT canprovide dense concise/detailed text generation based on given video frames and BBox of object.",
  ". Generation Strategy": "The volume and linguistic annotations of the VLT datasetdetermine the quality and generality of learned visual lan-guage representations. illustrates that the datasetcomprises only 3,649 videos, specifically 1,400 from La-SOT , 2,000 from TNL2K , 99 from OTB99 Lang, and 150 from MGIT , which are used for trainingand testing. These videos are accompanied by 5,252 officialtext descriptions. However, this amount of data is deemedinsufficient for algorithms to effectively learn.These official annotation suffers from inconsistency instyle, and are only able to describe short-term changes forthe object. The varying annotation styles of the text descrip-tions make it difficult for trackers to learn general visuallanguage information, resulting in a significant performancedrop when inferring on new videos with non-official anno-tations or different language description styles. Moreover,inaccurate text descriptions hinder object tracking, turningnatural language annotations into a hindrance rather than asupport.To enhance the accuracy and generality, we proposeDTLLM-VLT, which generates text in a consistent style forfour datasets, establishing a robust foundation for VLT. This a person walking on the sidewalk1 A person is seen walking away from the camera. She is wearing a white shirt and a helmet, indicating she might be a pedestrian. She is located towards the right side of the image, a bit further in the background1 a person walking on the sidewalk1100 a person in a white 200 shirta person in white300 A person is seen walking away from the viewer's perspective. She is wearing a white shirt and appears to be in motion. She islocated towards the left side of the image, and her back is turned towards us.200 A person is seen walking away from the camera. She is wearing a white shirt and black pants, and his back is turned towards us. She appears to be in motion, perhaps walking towards the right side of the image. A person is seen walking away from the camera. She is wearing a white shirt and a helmet, indicating she might be a pedestrian. She is located towards the right side of the image, a bit further in the background1 A person, dressed in a white shirt, is seen walking across the busy street. She is located in the middle of the scene, amidst the bustling traffic, and is one of the several pedestrians making their way across the street. Initial concise first frame Initial detailed first frame Dense conciseevery 100 frame Dense detailedevery 100 frame C D A B a person in a white shirt . Examples of the four types of generated text. We provide four different natural language descriptions for each video. The objectto be tracked is determined in the first frame and does not change throughout the video sequence.",
  "generation approach can be expanded to additional VLTdatasets and even applied to text generation in SOT datasets": "Initial and dense text descriptions. Following the textannotations method in OTB99 Lang and TNL2K ,we generate text for the initial frame of each video. Addi-tionally, given that 4 seconds marks the threshold betweenhuman instant memory and short-term memory ,we consider the worst situation and infer that the algorithmlacks an efficient memory system. Consequently, at 25 FPS,equating to every 100 frames in 4 seconds, we supply thealgorithm with relevant generated text. We posit that thisupdate frequency optimally sustains the memory state of al-gorithm and enhances tracking performance. Concise and detailed text descriptions. For the algo-rithm, if the BBox already sufficiently describes the tempo-ral and spatial changes of the object, the text descriptionsshould focus on providing essential semantic details likethe category and positions of the object. In cases wherethe BBox lacks sufficient information for effective learningby the tracker, more elaborate texts are necessary to com-pensate for the missing temporal and spatial relationships.Consequently, we generate two types of text descriptions:concise and detailed. As illustrated in , the concisetext conveys essential information about the object, such asits category (bear) and position (in the water), while the",
  ". DTLLM-VLT": "The traditional VLT datasets rely on manual text annota-tions, as shown in (a), providing a correspondingnatural language description for each video. This methodincurs high annotation costs, lacks uniformity in style, in-volves a single annotation granularity, and cannot be usedfor large-scale data annotation. To address these issues, wedesign DTLLM-VLT based on SAM and Osprey ,which can provide large-scale and diverse text generationlike (b). The framework of the DTLLM-VLT is illustrated in (c). Input video frames and corresponding objectBBox, SAM utilizes image encoder, prompt encoder,and mask decoder to obtain masks of the corresponding ob-ject and then input the video frames and mask into Osprey. Osprey encodes the images and masks, combines withpreset prompts, and generates concise and detailed descrip-tions of the corresponding object through LLM .Through this approach, we can generate large-scale, di-verse granularities, and uniform style text for SOT and VLTdatasets at very low costs.",
  ". Generation Analysis": "Combining the aforementioned strategies, we offer fourgranularities of natural language descriptions for eachvideo, namely initial concise description, initial detailed de-scription, dense concise description, and dense detailed de-scription, as illustrated in . Our goal is to incorporatemultiple granularities of text to enrich the environment foralgorithm to learn and evaluate, while also providing guid-ance for algorithm design and model optimization.Leveraging the DTLLM-VLT, we generate text descrip-tions comprising 7,238 initial descriptions (3,619 conciseand 3,619 detailed descriptions each) and 128.4K dense de-scriptions (64.3K concise and 64.3K detailed descriptionseach). Our dense texts are 24.4 times the quantity of the offi-cial annotations. Further details regarding the number of se-mantic descriptions are presented in . The semanticdescriptions contain 1.9M words with 14.8K non-repetitivewords. The vocabulary is rich, allowing for a comprehen-sive description of changes in the object during the trackingprocess. Word cloud and more detailed analyses have beenillustrated in .",
  ". Speed and Memory Usage": "We generate diverse text for visual language trackingdatasets on RTX-3090 GPUs, with approximately 16GB ofVRAM usage. It takes about 2 seconds to generate a textentry for each frame.Compared to manual annotation, DTLLM-VLT can gen-erate texts of various granularities for large-scale trackingdatasets in a short period of time. And it can seamlesslyapply to various tracking tasks.",
  ". Datasets and Evaluation Methods": "Datasets.We selected three representative datasets,OTB99 Lang , LaSOT , and MGIT , for evaluat-ing short-term tracking, long-term tracking, and global in-stance tracking task. OTB99 Lang and LaSOT areexpanded from the traditional SOT benchmark by addinglanguage annotations. OTB99 Lang serves as a represen-tative dataset for short-term tracking task, providing a textdescription for the initial frame of each video sequence. La-SOT is a representative dataset for long-term tracking task.Its text annotations only describe the appearance of the tar-get, omitting relative positions. MGIT is a novel large-scale benchmark specifically tailored for the global instancetracking task. Text annotations of each sequence containcomplex spatio-temporal causal relationships with a multi-granular annotation strategy.Evaluation Methods. As shown in , we followgeneration granularities to design various mechanisms. Weselect a SOTA visual language tracker, MMTrack as a baseline model and evaluate it on three benchmarks (asshown in and ). Compared with other al-gorithms, MMTrack does not impose restrictions onthe length of the text and does not truncate excessively longtext. Additionally, it unifies the VLT task as a form of to-ken generation, which is more conducive to learning visuallanguage information.To fairly compare the tracking performance on threedatasets, we directly use the officially provided weights totest with the official annotations, initial concise texts, initialdetailed texts, dense concise texts, and dense detailed texts.We also retrain and test the model under the correspondingsettings to evaluate Area Under the Curve (AUC), trackingprecision (P), and normalized precision (PNorm).",
  "Testing Directly": "We directly use the model provided by the official for test-ing, and the test results are as shown in .Short-term tracking. In , when comparing re-sults on OTB99 Lang , which only provides the textdescription of the initial frame and will interfere with thetracking of the object in the later stage, our initial concisetext achieves gains of 1.6 %, 2.2 %, and 1.6 % in area un-der the curve, normalized precision, and precision score,respectively. At the same time, we find that dense concisetext also helps improve tracking performance, for example,our generated text achieves improvements of 1.2 % in thearea under the curve. We think that the short-term trackingdatasets represented by OTB99 Lang , their BBox caneffectively describe the temporal and spatial relationships inthe visual modality. If only the text from the initial frameis used and cannot describe the temporal and spatial rela-tionships of the object in the following frame, it will causesignificant interference. The same problem arises in our de-tailed initial concise/dense text description testing. In thiscase, the text only needs to be as concise as possible to assistin improving tracking performance.Long-term tracking. The official text annotation of La-SOT only describes the appearance of the object, ig-noring the relative position.Compared to OTB99 Lang, the text description of the object is more accurate.Compared with MGIT , there is no excessive interfer-ence from relative position information. It represents a bal-ance between the two and is most in line with the currentalgorithm learning method. Therefore, the test performance",
  "Official69.082.089.573.577.254.369.982.275.7": "Initial Concise70.084.390.573.677.454.269.681.875.4Initial Detailed70.385.691.474.178.354.569.481.575.1Dense Concise71.386.092.574.077.654.269.581.675.3Dense Detailed69.884.890.674.478.554.669.882.175.6 with official annotation is the best. However, we believe thatfor long-term tracking, providing only a single sentence oftext is not conducive to algorithm learning. And the spa-tial relationships of the object are crucial. When there arelarge-scale and diverse VLT datasets and better approachesto enhancing video understanding capabilities of algorithm,this situation observed in LaSOT will soon change. Global instance tracking.The same situation asOTB99 Lang appeared on MGIT , that is, the per-formance is improved when tested under initial/dense con-cise text annotations. Particularly, dense concise annota-tion excels over the official text, surpassing it by 0.7 %, 0.7%, and 0.7 % in area under the curve, normalized preci-sion, and precision score, respectively. MGIT provideshigh-quality, multi-granularity long texts containing com-plex temporal and spatial relationships. From the test re-sults, we think that the handling of long texts and multi-modal alignment in the current algorithm requires improve-ment, as it fails to fully leverage temporal and spatial rela-tionships. Therefore, concise text can actually help improveperformance. However, temporal and spatial informationare crucial for long-term tracking and global instance track-ing. When the temporal-spatial information of the BBoxcannot stably determine the object, detailed text is needed toprovide additional high-level semantic information to iden-tify the object.",
  "Retraining and Testing Respectively": "As mentioned earlier, when the dataset text becomes denserand more accurate, it can compensate for BBox shortcom-ings. The algorithm gains additional knowledge throughtext updates, potentially improving performance.There-fore, we retrained and tested MMTrack using variedgenerated texts, with tracking results shown in .Short-term tracking.It can be seen that on theOTB99 Lang benchmark, the testing results after re-training with dense concise text have shown further im-provement. Compared with the official text, it gains 2.3 %,4.0 %, and 3.0 % in area under the curve, normalized pre-cision, and precision score, respectively. This indicates thatproviding dense concise text on short-term datasets can fur-ther improve tracking performance. It also reflects the ca-pability of the current algorithm to achieve better trackingeven when provided with more accurate text, without theneed for matching learning methods. However, we believethat the current method of training algorithms to memorizehigh-frequency text for enhancing memory capabilities stillneeds improvement, the potential of text has not been fullyexploited yet.Long-term tracking. The results on the LaSOT benchmark show that official annotations are still more ad-vantageous for tracking. However, after retraining, the re-sults on dense detailed text are only 0.1 % from the optimal results, indicating an improvement in the algorithms un-derstanding of dense text compared to direct testing, but itis still unable to fully learn all temporal and spatial infor-mation.Global instance tracking. The test results after retrain-ing based on different texts show that the algorithm can im-prove its tracking ability on the MGIT benchmark bylearning from dense detailed text, which differs from theresults of direct testing. For global instance tracking task,it is beneficial for tracking if the algorithm can learn morecomprehensive temporal and spatial relationships.Comparing the above results, we can draw the followinginsights:(1) The existing algorithm tends to learn and under-stand short text. The results of direct testing show thatconcise text is more beneficial for performance improve-ment on the OTB99 Lang and MGIT benchmarks.For OTB99 Lang , inaccurate natural language descrip-tions in official annotations create interference for tracking,while concise text provides further assurance for BBox thatalready expresses temporal and spatial relationships well,reducing interference. For MGIT , the algorithm is un-able to understand complex temporal relationships and canonly extract semantic information from concise text. Offi-cial text annotations of LaSOT lie between the two andare most conducive to the current algorithm, resulting in thebest performance.(2) For short-term tracking task, dense concise textwill bring greater gains. While dense detailed text ismore suitable for the other two tasks. Looking at the re-sults of testing after retraining with different texts, denseconcise text has the greatest impact on OTB99 Lang .We think this is because the text provides precise objectdescriptions, further compensating for the shortcomings ofBBox. The algorithm can further improve its performanceon MGIT by learning from dense detailed text, becausethey can provide high-level semantic information that BBoxcannot exhibit, such as temporal and spatial relationships.By text updating that best suits the memory system of al-gorithm, we provide the algorithm with precise and timelyhigh-level semantic information, which is more helpful forunderstanding long video.(3) The text processing method and multi-modalalignment ability need to be adjusted and improved. Thecurrent algorithm cannot fully understand and learn com-plex temporal and spatial relationships. When the text pro-cessing and multi-modal alignment abilities of algorithmare adjusted and improved, text with more information willshow even greater potential.",
  ". Visualization of tracking results on dense concise textannotations retrained algorithm": "challenging sequences from OTB99 Lang . In these se-quences, the official text annotations can only cover a shorttime for the changes in the object. The scenes contain dis-tractors, and the appearance of the object undergoes signif-icant changes. The retrained model exhibits greater robust-ness with dense concise text compared to the official one.This validates that our generated text helps tracker to ad-dress these challenges.",
  ". Conclusions": "Object tracking is the basis for advanced tasks such asvideo understanding, and VLT may offer a potential pathfor enhancing tracking capabilities. In this paper, we pro-pose DTLLM-VLT, a unified prompt framework, and gener-ate diverse multi-granularity text descriptions. We analyzethe results under different natural language descriptions forthree representative benchmarks, aiming to provide new in-sights for the evaluation of different tracking tasks.In our perspective, enhancing algorithm performance re-quires a comprehensive understanding of the properties ofthe datasets. We explore how leveraging the generative ca-pabilities of LLM can help us improve VLT datasets andprovide a new analytical approach from a multi-modal per-spective for the field of video understanding.We hopethis work can be expanded to incorporate more datasets,thereby enhancing support for vision datasets understand-ing research.",
  ". Acknowledgement": "This work is jointly supported by the National Scienceand Technology Major Project (No.2022ZD0116403);theNationalNaturalScienceFoundationofChina(No.62176255); the Strategic Priority Research Programof Chinese Academy of Sciences (No.XDA27000000). M. Mueller, N. Smith, and B. Ghanem, A benchmarkand simulator for uav tracking, in Computer VisionECCV2016: 14th European Conference, Amsterdam, The Nether-lands, October 1114, 2016, Proceedings, Part I 14.Springer, 2016, pp. 445461. R. Bibliographie Goecke, R. Stolkin, S. Lim, S. Maher,S. Poullot, S. Wong, S. Satoh, W. Chen, W. Hu, X. Zhanget al., The visual object tracking vot2013 challenge results,in Computer Vision Workshops (ICCVW), 2013 IEEE Inter-national Conference on, 2013, pp. 98111. M. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Ce-hovin, G. Fernandez, T. Vojir, G. Hager, G. Nebehay, andR. Pflugfelder, The visual object tracking vot2015 challengeresults, in Proceedings of the IEEE international conferenceon computer vision workshops, 2015, pp. 123. 3 M.Kristan,A.Leonardis,J.Matas,M.Felsberg,R. Pflugfelder, L. Cehovin Zajc, T. Vojir, G. Bhat,A. Lukezic, A. Eldesokey et al., The sixth visual objecttracking vot2018 challenge results, in Proceedings of theEuropean conference on computer vision (ECCV) work-shops, 2018, pp. 00. 3 M.Kristan,J.Matas,A.Leonardis,M.Felsberg,R. Pflugfelder, J.-K. Kamarainen, L. Cehovin Zajc, O. Dr-bohlav, A. Lukezic, A. Berg et al., The seventh visual ob-ject tracking vot2019 challenge results, in Proceedings ofthe IEEE/CVF international conference on computer visionworkshops, 2019, pp. 00. 3 M. Muller,A. Bibi,S. Giancola,S. Alsubaihi,andB. Ghanem, Trackingnet: A large-scale dataset and bench-mark for object tracking in the wild, in Proceedings of theEuropean conference on computer vision (ECCV), 2018, pp.300317. 3 M. Kristan, J. Matas, A. Leonardis, T. Vojr, R. Pflugfelder,G. Fernandez, G. Nebehay, F. Porikli, and L. Cehovin, Anovel performance evaluation methodology for single-targettrackers, IEEE transactions on pattern analysis and ma-chine intelligence, vol. 38, no. 11, pp. 21372155, 2016. 3 S. Hu, D. Zhang, M. Wu, X. Feng, X. Li, X. Zhao, andK. Huang, A multi-modal global instance tracking bench-mark (mgit):Better locating target in complex spatio-temporal and causal relationship, in Advances in NeuralInformation Processing Systems, vol. 36, 2023, pp. 25 00725 030. 1, 2, 3, 4, 6, 7, 8",
  "S. Hu, X. Zhao, and K. Huang, Sotverse: A user-definedtask space of single object tracking, International Journalof Computer Vision, vol. 132, p. 872930, 2024. 3": "H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai,Y. Xu, C. Liao, and H. Ling, Lasot: A high-quality bench-mark for large-scale single object tracking, in Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, 2019, pp. 53695378. 1, 2, 3, 4, 6, 7, 8 H. Fan, H. Bai, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu,Harshit, M. Huang, J. Liu et al., Lasot: A high-qualitylarge-scale single object tracking benchmark, InternationalJournal of Computer Vision, vol. 129, pp. 439461, 2021. 2,3 X. Wang, X. Shu, Z. Zhang, B. Jiang, Y. Wang, Y. Tian, andF. Wu, Towards more flexible and accurate object trackingwith natural language: Algorithms and benchmark, in Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2021, pp. 13 76313 773. 2, 3, 4, 5 Z. Li, R. Tao, E. Gavves, C. G. Snoek, and A. W. Smeulders,Tracking by natural language specification, in Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, 2017, pp. 64956503. 1, 2, 3, 4, 5, 6, 7,8",
  ", Object tracking benchmark, IEEE Transactions onPattern Analysis and Machine Intelligence, vol. 37, no. 09,pp. 18341848, 2015. 2, 3": "L. Huang, X. Zhao, and K. Huang, Got-10k: A large high-diversity benchmark for generic object tracking in the wild,IEEE Transactions on Pattern Analysis and Machine Intelli-gence, vol. 43, no. 5, pp. 15621577, 2021. 3 J. Valmadre,L. Bertinetto,J. F. Henriques,R. Tao,A. Vedaldi, A. W. Smeulders, P. H. Torr, and E. Gavves,Long-term tracking in the wild: A benchmark, in Proceed-ings of the European conference on computer vision (ECCV),2018, pp. 670685. 3",
  "S. Hu, X. Zhao, L. Huang, and K. Huang, Global instancetracking: Locating target more like humans, IEEE Transac-tions on Pattern Analysis and Machine Intelligence, vol. 45,no. 1, pp. 576592, 2023. 3": "X. Zhao, S. Hu, Y. Wang, J. Zhang, Y. Hu, R. Liu, H. Ling,Y. Li, R. Li, K. Liu et al., Biodrone: A bionic drone-basedsingle object tracking benchmark for robust vision, Interna-tional Journal of Computer Vision, pp. 126, 2023. 3 L. Zhou, Z. Zhou, K. Mao, and Z. He, Joint visual ground-ing and tracking with natural language specification, in Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2023, pp. 23 15123 160. 3 Q. Feng, V. Ablavsky, Q. Bai, and S. Sclaroff, Siamesenatural language tracker: Tracking by natural language de-scriptions with siamese trackers, in Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, 2021, pp. 58475856. 3",
  "H. Zhao, X. Wang, D. Wang, H. Lu, and X. Ruan, Trans-former vision-language tracking via proxy token guidedcross-modal fusion, Pattern Recognition Letters, vol. 168,pp. 1016, 2023": "Y. Li, J. Yu, Z. Cai, and Y. Pan, Cross-modal target re-trieval for tracking by natural language, in Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, 2022, pp. 49314940. X. Wang, C. Li, R. Yang, T. Zhang, J. Tang, and B. Luo, De-scribe and attend to track: Learning natural language guidedstructural representation and visual attention for object track-ing, arXiv preprint arXiv:1811.10014, 2018.",
  "Q. Feng, V. Ablavsky, Q. Bai, and S. Sclaroff, Robust visualobject tracking with natural language region proposal net-work, arXiv preprint arXiv:1912.02048, vol. 1, no. 7, p. 8,2019": "Q. Feng, V. Ablavsky, Q. Bai, G. Li, and S. Sclaroff, Real-time visual object tracking with natural language descrip-tion, in Proceedings of the IEEE/CVF Winter Conferenceon Applications of Computer Vision, 2020, pp. 700709. 3 A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland,L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Loet al., Segment anything, in Proceedings of the IEEE/CVFInternational Conference on Computer Vision, 2023, pp.40154026. 5",
  "Y. Yuan, W. Li, J. Liu, D. Tang, X. Luo, C. Qin, L. Zhang,and J. Zhu, Osprey: Pixel understanding with visual instruc-tion tuning, arXiv preprint arXiv:2312.10032, 2023. 5": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro,F. Azhar et al., Llama: Open and efficient foundation lan-guage models, arXiv preprint arXiv:2302.13971, 2023. 5 W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez et al., Vi-cuna: An open-source chatbot impressing gpt-4 with 90%*chatgpt quality, See lmsys. org (accessed 14April 2023), vol. 2, no. 3, p. 6, 2023. 5",
  "R. C. Atkinson and R. M. Shiffrin, Human memory: A pro-posed system and its control processes, in Psychology oflearning and motivation.Elsevier, 1968, vol. 2, pp. 89195. 5": "P. Voigtlaender, J. Luiten, P. H. Torr, and B. Leibe, Siamr-cnn: Visual tracking by re-detection, in Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, 2020, pp. 65786588. D. Guo, J. Wang, Y. Cui, Z. Wang, and S. Chen, Siamcar:Siamese fully convolutional classification and regression forvisual tracking, in Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, 2020, pp. 62696277.",
  "M. Danelljan, L. V. Gool, and R. Timofte, Probabilis-tic regression for visual tracking, in Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, 2020, pp. 71837192": "C. Mayer, M. Danelljan, D. P. Paudel, and L. Van Gool,Learning target candidate association to keep track of whatnot to track, in Proceedings of the IEEE/CVF internationalconference on computer vision, 2021, pp. 13 44413 454. Y. Cui, C. Jiang, L. Wang, and G. Wu, Mixformer: End-to-end tracking with iterative mixed attention, in Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, 2022, pp. 13 60813 618. B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, High performancevisual tracking with siamese region proposal network, inProceedings of the IEEE conference on computer vision andpattern recognition, 2018, pp. 89718980."
}