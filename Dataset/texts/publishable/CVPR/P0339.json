{
  "Wei-Bin Kou, Qingfeng Lin, Ming Tang, Sheng Xu, Rongguang Ye, Yang Leng,Shuai Wang, Guofa Li, Zhenyu Chen, Guangxu Zhu*, Yik-Chung Wu*": "AbstractDeep learning-based Autonomous Driving (AD)models often exhibit poor generalization due to data hetero-geneity in an ever domain-shifting environment. While FederatedLearning (FL) could improve the generalization of an AD model(known as FedAD system), conventional models often strugglewith under-fitting as the amount of accumulated training dataprogressively increases. To address this issue, instead of conven-tional small models, employing Large Vision Models (LVMs) inFedAD is a viable option for better learning of representationsfrom a vast volume of data. However, implementing LVMsin FedAD introduces three challenges: (I) the extremely highcommunication overheads associated with transmitting LVMsbetween participating vehicles and a central server; (II) lackof computing resource to deploy LVMs on each vehicle; (III)the performance drop due to LVM focusing on shared fea-tures but overlooking local vehicle characteristics. To overcomethese challenges, we propose pFedLVM, a LVM-Driven, LatentFeature-Based Personalized Federated Learning framework. Inthis approach, the LVM is deployed only on central server, whicheffectively alleviates the computational burden on individualvehicles. Furthermore, the exchange between central server andvehicles are the learned features rather than the LVM param-eters, which significantly reduces communication overhead. Inaddition, we utilize both shared features from all participatingvehicles and individual characteristics from each vehicle toestablish a personalized learning mechanism. This enables eachvehicles model to learn features from others while preservingits personalized characteristics, thereby outperforming globallyshared models trained in general FL. Extensive experimentsdemonstrate that pFedLVM outperforms the existing state-of-the-art approach by 18.47%, 25.60%, 51.03% and 14.19% interms of mIoU, mF1, mPrecision and mRecall, respectively.",
  "Index TermsLarge Vision Model (LVM), Latent Feature,Personalized Federated Learning, Autonomous Driving": "Wei-Bin Kou, Qingfeng Lin, Yang Leng and Yik-Chung Wu are with theDepartment of Electrical and Electronic Engineering, The University of HongKong, Hong Kong, China.Guangxu Zhu is with Shenzhen Research Institute of Big Data, Shenzhen,China.Ming Tang and Rongguang Ye are with the Department of ComputerScience and Engineering, Southern University of Science and Technology,Shenzhen, China.Sheng Xu is with Research Institute of Electronic Science and Technology,University of Electronic Science and technology, Chengdu, China.Shuai Wang is with Shenzhen Institute of Advanced Technology, ChineseAcademy of Sciences, Shenzhen, China.Zhenyu Chen is with the State Key Laboratory for Novel Software Tech-nology, Nanjing University, Nanjing, China.Guofa Li is with the College of Mechanical and Vehicle Engineering,Chongqing University, Chongqing, China(Corresponding author: Guangxu Zhu and Yik-Chung Wu.)",
  "I. INTRODUCTION": "Autonomous Driving (AD) is a highly complex task. Oneof the major challenges in developing an effective AD systemis the poor model generalization due to significant data het-erogeneity , which results from frequent domain shifting.For example, an AD vehicle transitioning into an unfamiliarenvironment may experience a notable decline in model per-formance compared to operations in usual and known settings.Federated Learning (FL) , is an effective solution tomake use of diverse data , from locationally distributedvehicles for improving the AD model generalization whilepreserving data privacy. Generally known as the FedAD , the FL-based AD system typically composes of oneCentral Server and multiple Vehicles. The FedAD trainingprocedure involves the following steps: (I) Vehicle Updates:each vehicle trains its local model using continuously collecteddata. (II) Server Aggregation: after every several updates atvehicles, the central server receives all participating vehiclesmodel and aggregates them as a weighted average, and thenredistributes the aggregated model to all participating vehicles.(III) Life-Long Learning: steps (I) and (II) are iterated tolearn from dynamically changing data. This FedAD system ispictorially illustrated in .As time progresses, the amount of data incorporated in theFedAD system continually expands, which helps to achievesubstantial improvement in AD model generalization com-pared to deep learning model trained on data from eachindividual vehicle. However, such ever-expanding data is a",
  "IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. XX, NO. X, APRIL 20242": "double-edged sword. In general, the Bias-Variance Tradeoff in classical regime states that when the volume of data isless than what the model can comfortably accommodate, themodel tends to overfit. Conversely, if the data volume exceedsthe models capacity, the model is susceptible to under-fitting.In the context of the FedAD system, as more and more data isused to train the FL model, the amount of information wouldeventually exceeds the model capacity, increasing the risk ofunder-fitting and leading to poor generalization performance.To tackle such under-fitting problem, recent research entersthe modern regime of Large Models (LMs). For example,Large Language Models (LLMs) have marked a significantmilestone in the advancement of natural language processing(NLP), exhibiting proficiency across a variety of applications,such as language comprehension and generation , inter-pretation of user intentions , solving question answeringtasks based on structured data , and complex reasoning. Similarly, Large Vision Models (LVMs) demonstrate thecapability to interpret visual input and extract comprehensivesemantic insights from image data , .However, deploying LMs within FedAD system presentsits own set of challenges. Firstly, due to the extremely largenumber of parameters in LMs, the communication betweenvehicles and the central server can lead to unbearable over-heads. Secondly, the computing resource at vehicles wouldnot be sufficient to train a large model locally. Thirdly, theinherent focus of LMs on extracting general features oftenleads to the inadvertent neglect of unique, vehicle-specificlocal characteristics. This issue primarily stems from theintrinsic weakness of generic overgeneralization in LMs ,. Such overgeneralization issue arises due to the LMstraining on widely varying data aimed at capturing universallyapplicable patterns. Consequently, the nuanced details thatare critical for distinguishing specific local attributes may beoverlooked.To overcome these challenges in the context of FedADsystem, we propose the pFedLVM framework which involvesa two-fold strategy. On the one hand, since a major partof the AD system is vision-based, we propose deployingLarge Vision Models (LVMs) on the central server but not atvehicles. To alleviate the computational burden on individualvehicles and communication overhead, each vehicle wouldtrain on a small model. Furthermore, the FL exchange betweenvehicles and the central server would be on features ratherthan the LVM parameters. On the other hand, to ensure thateach vehicles unique characteristics are not neglected, theproposed framework incorporates a feature-based personal-ized FL (pFL) mechanism. This mechanism leverages bothshared features of all involved vehicles and each vehiclesindividual characteristics, allowing for models to be tailoredto each vehicle. Such pFL mechanism is particularly usefulin FedAD because vehicles behavior or driving pattern variessignificantly for different vehicles. The proposed pFedLVMframework is summarized in .With the proposed pFedLVM aiming to address the chal-lenges posed by using LVMs in FedAD and pave the wayfor more effective and better autonomous driving systems, themain contributions of this paper are summarized as follow:",
  "This work leverage LVMs in FedAD system. LVMs canovercome under-fitting problem in the ever-increasingtraining dataset in FedAD": "To effectively alleviate the computational burden of eachvehicle in FedAD system, we propose using LVMs asbackbone on the central server only but not at the vehiclelevel. In addition, we exchange only the extracted features(instead of the parameters of LVMs), reducing communi-cation overhead while sharing diverse knowledge learnedat each vehicle. To avoid LVMs overlooking local vehicle characteristics,we propose utilizing both shared features from all partic-ipating vehicles and individual characteristics from eachvehicle to establish a personalized learning mechanism.This enables each vehicles model to learn features fromothers while preserving its personalized characteristics,improving the inference performance in non-independentand identically distributed (non-i.i.d.) autonomous drivingscenarios. Extensive experiments show that our proposed meth-ods outperforms current existing state-of-the-art (SOTA)benchmarks by 18.47%, 25.60%, 51.03% and 14.19% interms of mIoU, mF1, mPrecision and mRecall, respec-tively. Additional empirical analyses are also conductedto explain the superiority of the proposed method.The remainder of this paper is organized as follows. Sec-tion II provides an overview of the related work. Section IIIelaborates on the proposed pFedLVM framework, while Sec-tion Section IV analyzes the communication overhead andcomplexity of pFedLVM. Subsequently, Section V presentsa comprehensive set of experiments along with an analysis ofthe empirical results. The paper concludes in Section VI.",
  "A. Federated Autonomous Driving (FedAD) System": "Current AD systems are commonly grouped into twocategories in various studies: modular-based , andlearning-based . Modular-based methods, althoughstructured, are plagued by error propagation due to potentialinaccuracies in both the modeling and problem-solving phases.In contrast, typical learning-based end-to-end approaches offer a promising alternative that mitigates error propagation.These methods directly transform sensory inputs, such as Li-DAR point clouds and camera imagery, into vehicular controlactions, encompassing throttle, brake, and steering commands.On the other hand, learning-based models can also be usedwithin a modular pipeline, for example, using a learningmodel for semantic segmentation within the perception module, . Nevertheless, the intrinsic challenge for learning-based paradigms lies in their generalization capabilities, oftenresulting in performance limitations to specific scenarios.FL presents itself as a novel approach aimed at enhancingthe generalization ability of learning-based systems .It achieves this through the aggregation of model param-eters.Within the realm of AD, FL capitalizes on vehicularnetworks to combine insights from diverse vehicles operat-ing across varied environments. Consequently, when an AD",
  "SharedFeatures": ": The illustration of the proposed pFedLVM framework. The proposed pFedLVM is composed of one central server and |V| vehicles. For each vehicle(e.g., V ehiclev), the feature compressor extracts the compressed features which are transmitted to central server. The central server then uses LVM as abackbone to extract the shared features of all participating vehicles, and returns the extracted shared features to all involved vehicles. Once each vehiclereceived the shared features from the central server, the downstream perception head, taking such shared features as input, is optimized by using the lossbetween the output of the perception head and ground truth via back propagation, while the feature compressor is updated by using the distance between theshared features and the compressed features via back propagation. system encounters a new data sample or edge case, it candisseminate newfound knowledge to the centralized serverand subsequently other vehicles, all while safeguarding dataprivacy . A notable instance is the cloud federated roboticsystem proposed in , which augments the behavior cloningtechnique to yield precise control commands by leveragingRGB imagery, depth perception, and semantic segmentation.Recently, communication-efficient researches de-velop rapidly so that FedAD could be more feasible.",
  "B. Personalized Federated Learning (pFL)": "To surmount the challenge of discrepancies in local datadistribution in FL, pFL has been proposed as a solution, . This approach customizes the model in each clientto account for the unique characteristics of local data. Oneof the most popular and effective methods for achievingpFL is the architecture-based approach . This methoddecouples the models parameters, allowing only a subsetof parameters to be shared and aggregated among clients,while the remaining private parameters are selected basedon model architecture , or data similarities , to learn solely on local data. Another option enableseach client to fine-tune global model locally , . Forexample, considers the global model as an initial sharedmodel. By performing a few additional training steps locally,all the clients can easily fine-tune the initial shared model. implements the above strategy by splitting the backboneinto a global model (representation) and a client-specific headand fine-tunes the head locally to achieve personalization.In contrast, unlike the methods mentioned earlier, this paperintroduces a novel strategy for pFL that is centered on featuremaps, allowing each vehicle to learn concurrently from otherswhile also maintaining its unique characteristics. C. Large Vision Models (LVMs)Recently, LLMs have achieved great success in the NLPfield in various scenarios, such as user intent understanding, knowledge utilization and complex reasoning in a zero-shot/few-shot setting. Inspired by the achievementsof pre-trained LLMs in NLP field, researchers have turnedtheir attention to exploring pre-trained LVMs. These models,pre-trained on extensive image datasets, hold the ability todecipher image content and distill rich semantic information. By learning representations and features from a signif-icant volume of data, these models enhance the ability ofcomputers to comprehend and analyze images, facilitatinga range of diverse downstream applications , . Inthis paper, by leveraging on their exceptional capabilitiesof semantic understanding, we propose the use of LVMs toextract and integrate the shared representations and featuresfrom all participating vehicles. III. METHODOLOGYA. pFedLVM OverviewThe key notations in pFedLVM formulation are summarizedin Table I. We consider a FedAD system, which includesa cloud server and |V| vehicles. V ehiclev denotes the v-thvehicle connected to the cloud server, where v = 1, 2, , |V|.V ehiclev has a local dataset Dv with size |Dv|. The Cen-tral Server virtually covers dataset D |V|v=1Dv with size|D| = |V|v=1 |Dv|. The proposed pFedLVM consists of twokey elements.Firstly, we suggest deploying LVMs exclusively on the cen-tral server, and the central server and vehicles share collectiveknowledge via exchanging features. This strategy aims notonly to significantly reduce the computational load at the vehi-cle level, but also allows knowledge sharing while alleviating",
  "(j)The counterpart of D(j)vfor symbol . For example,F(j)v,c means the Fv,c due to D(j)v": "communication overheads. Specifically, on the vehicle side,we propose a feature compressor (with parameters v,c) oneach vehicle to extract compressed features (denoted as Fv,c).Then such features are transmitted to the central server. On theserver side, we propose to use LVMs as backbone to fuse andextract all participating vehicles shared features. Precisely,when the central server receives the compressed features Fv,cfrom V ehiclev, where v = 1, 2, , |V|, it concatenates suchfeatures to form Fcat which is then fed into the LVM backbone(with parameters lvm) to produce the shared feature mapsFshd of all participating vehicles. After that, the central serverredistributes Fshd to all participating vehicles.Secondly, to acknowledge and incorporate the unique char-acteristics of each vehicle, we introduce a personalized learn-ing mechanism. This mechanism utilizes both the sharedfeatures (denoted as Fshd) of all participating vehicles andthe local characteristics (denoted as Fv,c) of each vehicle.As aforementioned, we utilizes a feature compressor (withparameters v,c) to extract compressed features Fv,c in orderto reduce communication overheads. The feature compressorparameters v,c is optimized by the loss depending on thediscrepancy between Fv,c and Fshd via back propagation.In addition, we also design downstream personalized controlmodule to enhance the inference performance. The controlmodule in general consists of two blocks: modular heads(including sequentially connected perception head, planninghead and control head) and end-to-end head. Such heads areupdated by the loss taking into account these heads outputand corresponding heads ground truth. By introducing suchpersonalized mechanism, it allows for the models (includingfeature compressor and various downstream heads) to becustomized for each vehicle. This mechanism, with its superiorability to capture unique patterns and preferences, enables eachvehicle to perform better than a global model trained througha general FL. This is particularly advantageous in FedAD,where the behavior and patterns among vehicles can varysubstantially. The proposed feature-based pFL is illustrated in.",
  ") LVM Backbone: LVMs tend to perform exceptionallywell for a few reasons: I. Depth and Width: Large models": "have more layers (depth) and more neurons per layer (width).This allows them to form a hierarchy of features, from simpleto complex, and capture more intricate patterns in the data.II. Learning Capacity: The large number of parameters inLVMs allows them to effectively capture and model the un-derlying distribution of the data, especially for complex taskslike autonomous driving. III. Attention Mechanisms: LVMsgenerally incorporate attention mechanisms, which allow themodel to focus on different parts of the input when generatingeach part of the output. This leads to a more context-awarerepresentation of the data and results in more meaningfulfeature extraction.Pretrained ImageGPT , often abbreviated as iGPT, isan outstanding representative of LVMs and selected as thebackbone. It is proposed to fuse and extract shared features in azero-shot fashion. These features are high-dimensional vectorsthat capture the models understanding of the image content.They can be used as input for a variety of downstream tasks.Overall, by pretraining on a large-scale image dataset, iGPTlearns rich representations of images that can be leveraged fora variety of image-processing tasks.2) Shared Feature Extraction and Fusion: Recall that allthe participating vehicles extract their compressed features(termed as F(j)v,c where v = 1, 2, , |V|) and send themto the central server. The central server concatenates suchfeatures together to form F(j)cat which is then fed into theLVM backbone to produce the shared feature maps F(j)shd. Thisprocess is given by",
  "F(j)shd = lvm(F(j)cat),(2)": "where symbol is defined as concatenation operation of allinvolved items.From Eqs. (1) and (2), it is obvious that F(j)shd containsthe fused features of all participating vehicles. It is worthnoting that due to the powerful capabilities of representationof LVMs, the generated fusion features F(j)shd are generallystable and effective instead of under-fitting for ever-increasingvast amount of data. Once the shared feature maps F(j)shdhave been extracted, the central server sends them back toall participating vehicles. The shared features serves twopurposes: 1) It enables each vehicle to benefit from the sharedknowledge of all the other vehicles, improving all participatingvehicles inference performance; 2) The shared feature can beused for personalized learning, which is covered next.",
  "Server": ": This figure illustrates the proposed feature-based pFL. We utilize thecompressed features (denoted by Fv,c) and the shared features (denoted byFshd) to compute the loss for the feature compressor (with parameters v,c)update. Meanwhile, the perception head (with parameters v,p), taking theshared feature (denoted by Fshd) as input, is trained based on the outputof the perception head (denoted by Ov,p) and the ground truth (denoted byGv,p). As a result, the feature compressor and the perception head both learnshared features while maintaining local uniqueness. 1) Feature Compressor: For the feature compressor, eachvehicle undertakes the training based on the onboard dataset(termed as Dv) as well as the shared feature maps (termedas Fshd). The loss function for optimizing v,c is defined asEc(v,c, F(j)v,c, F(j)shd), as shown in Eqs. (3) and (4):",
  "Ec(v,c, F(j)v,c, F(j)shd).(4)": "As detailed in Eq. (3), D(j)vis fed into the feature compres-sor v,c to extract the compressed features (termed as F(j)v,c).Once each vehicle has extracted the compressed features, suchfeatures are sent to the central server, which consumes muchsmaller communication overheads compared to transferringraw images and preserves privacy as well. Then the server per-forms a critical role in merging these features and distributingthe shared features (termed as F(j)shd) back to all vehicles.Upon receipt of these shared features F(j)shd, each vehiclecalculates the loss in Eq. (4) which represents the discrepancybetween the compressed features F(j)v,c and the shared featuresF(j)shd. Subsequently, this loss is optimized to update v,cthrough back propagation. Eq. (4) is a general expression, andthe specific loss function depends on the applications involved.In the implementation in Section V, we employ the mean-square error loss (see Table III for details). 2) Personalized Downstream Heads: Besides for learningthe individual feature compressor, the shared features F (j)shdfrom the central server are also used to train the downstreamheads at each vehicle. In general, the heads are commonlydivided into two catogrogies: modular heads (including per-ception head, planning head and control head) and end-to-endhead. As all such heads are trained in a similar manner, in thispaper, we just focus on the perception head to demonstrate theproposed feature-based personalized FL mechanism. Trainingof other heads can be easily added as an extension.For the perception head (with parameters v,p), the loss isthe distance between the output (termed as O(j)v,p) and groundtruth (termed as G(j)v,p) of perception head which is given byEqs. (5) and (6):",
  "Ep(v,p, G(j)v,p, O(j)v,p).(6)": "As outlined in Eq. (5), once V ehiclev receives the sharedfeatures F(j)shd from the central server, these features are fedinto the perception head with parameters v,p to generate theoutput O(j)v,p. Subsequently, as detailed in Eq. (6), this outputalong with the ground truth G(j)v,p is employed to calculate theloss Ep. In the implementation, we employ the cross entropyloss for Ep, but the loss function can take other forms to suitthe specific application. Based on this loss, the perceptionhead model v,p can be updated via back propagation. Itis clear that the trained v,p exhibits properties associatedwith personalized FL thanks to its integration of both sharedfeatures and unique characteristics specific to each vehicle dueto proprietary perception task data.",
  "D. Summary of the pFedLVM Algorithm": "The proposed pFedLVM framework in AD is summarizedas Algorithm 1. The algorithm is designed to facilitate thetraining of personalized models, which includes a featurecompressor and the downstream perception head, based onmini-batches of data in an iterative fashion. The processunfolds as follows:1) On the Vehicle Side: Each vehicle begins by computingthe compressed features of its data. These compressed featuresare then transmitted to the central server. Vehicles enter awaiting state until the server completes its computations andreturns the shared features. Upon receiving the shared featuresfrom the central server, each vehicle proceeds to further trainits feature compressor using both the newly acquired sharedfeatures and the vehicles own previously compressed features.Simultaneously, each vehicle updates the perception head byleveraging the shared features in conjunction with its localground truth data.2) On the Server Side: The central server collects thecompressed features from all participating vehicles. Utilizinga LVM backbone, the central server extracts shared featuresfrom the concatenated compressed features. Once the sharedfeatures have been extracted, the server dispatches them backto the vehicles.",
  "A. Communication Efficiency Analysis of pFedLVM": "shows the communication paradigms of two cases:I) the proposed pFedLVM which trains personalized modelsof each vehicle via feature sharing with the central server;II) deploying the LVMs in the FedAD system and train theglobal LVM in a typical FL way, where LVMs are exchangedbetween vehicles and the central server.In order to compare the communication overheads of thetwo schemes, we introduce some symbols. Firstly, let Smaxrepresents the size of the largest dataset (dubbed as Dmax)among all involved vehicles included in the FedAD system,i.e., Smax = |Dmax| = maxvV{|Dv|}. To align with the twoconsidered cases, we assume they both execute a total of Nbiterations of Dmax. Furthermore, we denote Bs, Mb and Fb asthe min-batch size, the size of the LVMs, and feature size ofone mini-batch of input images, respectively. Generally, Mb isdetermined by the architecture of LVMs, while Fb is dependanton the architecture of the feature compressor in pFedLVM. Atlast, let represents the number of local iterations of eachvehicle between two adjacent aggregations in the typical FL.Based on the above defined symbols, we can derive thecommunication overhead of the proposed pFedLVM. As il-lustrated in a, in each round of local update, there are Smax Bs mini-batches of features to exchange between eachvehicle and the central server, where the symbol is athe floor function, which returns the largest integer that is lessthan or equal to the value between and . Therefore, foreach vehicle, the communication overhead in each round oflocal update for both upload and download is Fb Smax",
  "(b) The protocol of exchanging LVMs in typical FL": ": The illustration for communication overhead analyses. In this figure,we assume the V ehicle|V| has the largest dataset among all involved vehicles,i.e., Smax = |D|V||. (a) In the proposed pFedLVM, the vehicles with smallerdataset may repeat more than one iteration to align with the V ehicle|V|. (b)In the typical FL, the vehicles with smaller dataset need to wait to align withthe V ehicle|V|. On the other hand, for the typical FL, as showcased inb, it exchanges the parameters of the LVM once after local updates at the vehicle level. Therefore, the number ofrounds for parameter exchange is Nb",
  "Bs ) 100%.(9)": "From Eq. (9), we can observe that communication resourcereduction of the proposed pFedLVM depends on multiplefactors, including the mini-batch size Bs, the size of the largestdataset among all involved vehicles Smax, the feature size ofeach mini-batch Fb, the size of the adopted LVMs Mb, andthe aggregation interval in typical FL.",
  "IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. XX, NO. X, APRIL 20247": "1) Space Complexity Analysis: Recall that in the proposedpFedLVM, there are one central server and |V| AD vehicles.Assume that the compressed features F()v,c in each vechicle andthe shared feature F()shd of each mini-batch share the samesize (denoted as Fb in the previous subsection). Therefore,for each vehicle, it needs extra Fb space units to store thecompressed feature, and in total it needs |V| Fb space unitsfor all involved vehicles. For the central server, it also needs|V|Fb space units to store the received features from involvedvehicles. Therefore, the space complexity of the proposedpFedLVM is O(|V|).2) Time Complexity Analysis: As illustated in , for theb-th min-batch, the forward time and backward time of featurecompressor and perception head for V ehiclev are denoted astfv,c, tbv,c, tfv,p, and tbv,p, respectively; the computing timein the central server to extract the shared feature is denotedas ts; the communication time for uploading and downloadingfeatures are denoted as tuv,b and tdv,b, respectively.As demonstrated in , the total time for the b-th mini-batch can be divided into three stages: I) for the first stage,the central server needs to take maxvV{tfv,c + tuv,b} toreceive the compressed features from all involved vehicles.II) for the second stage, once the central server received allfeatures, it takes the time of ts to extract the shared features.III) for the third stage, V ehiclev takes tdv,b to download theshared features, and takes max(tbv,c, tfv,p+tbv,p) to optimizeits feature compressor and perception head. Therefore, thetotal time in third stage is max(tbv,c, tfv,p + tbv,p) + tdv,b.For all involved vehicles, the total time of the third stage ismaxvV{max(tbv,c, tfv,p +tbv,p)+tdv,b}. Therefore, we canobtain the time of pFedLVM in the b-th mini-batch is",
  "V. EXPERIMENTS": "In this section, we present experimental results to verify theproposed pFedLVM framework for the street scene semanticunderstanding task in the context of AD. Our experimentsassess the performance of LVM backbone and the proposedpersonalized learning mechanism compared with some exist-ing FL benchmarks, including FedAvg , FedProx andFedDyn . Given that the control, planning, and end-to-end heads could employ update mechanism analogous to thatused in the perception head, we focus on the perception headas a representative demonstration on the effectiveness of ourproposed pFedLVM framework.",
  "A. Datasets, Metrics and Implementation": "1) Datasets: We employ two public datasets in our ex-periments: Cityscapes dataset and CamVid dataset .Cityscapes dataset, captured across multiple cities, comprises2,975 training images and 500 validation images. The trainingdataset features pixel-level labels for 19 classes, such as vehi-cles, pedestrians, etc. To simulate the practical scenario wheredifferent vehicles might have different amount of data, wepartition this training dataset randomly for multiple vehicles.On the other hand, the CamVid dataset includes a total of 701samples, each with a pixel-level label for 11 classes. In theexperiments, we divide randomly selected 600 samples intodifferent vehicles. The remaining samples serve as test dataset.2) Evaluation Metrics: We assess the proposed pFedLVMusing four metrics: mIoU: the mean of intersection overunion; mPrecision (mPre for short): the mean ratio of truepositive pixels to the total predicted positive pixels; mRecall(mRec for short): the mean ratio of true positive pixelsto the total positive ground truth pixels; mF1: the mean ofharmonic mean of precision and recall, providing a balancedmeasure of these two metrics. Such metrics are evaluatedacross all semantic classes, offering a comprehensive view ofpFedLVMs performance. These metrics are formally listed in",
  "Prec + Recc,(12)": "where TP, FP, TN and FN stand for True Positive, FalsePositive, True Negative and False Negative, respectively. Cdenotes the number of semantic classes within the test dataset,with values set to 19 for the Cityscapes dataset and 11 for theCamVid dataset. Similarly, N signifies the size of the testdataset, which amounts to 500 for Cityscapes and 101 forCamVid.3) Implementation Details: The main hardware and soft-ware configurations are summarized in Table II, and the majortraining details are given in Table III.",
  "B. Evaluation of LVM backbone and Perception Head": "The publicly available pre-trained iGPT outputs multiplelayers of hidden features. These features are essentially rep-resentations of the input data as understood by the modelat various levels of abstraction, complexity and capability.In this section, we will present comprehensive experimentalresults to evaluate the performance of iGPT for the streetscene semantic understanding task within the context ofAD. Furthermore, we also conduct additional experiments toevaluate the performance of the LVM+Head framework. Ourobjective is to benchmark its efficacy against current SOTAmodels: BiSeNetV2 and SegNet , with all the metricsmentioned in subsection V-A2.1) Hidden Feature Selection from iGPT: iGPT mentionedearlier contains multiple hidden layers included in its output.Previous research suggested that features in the middleof the output hidden layers perform the best for training alinear classification model. However, which layer (or layers)performs optimally for street scene semantic understandingtask in the context of AD remains an open question.In our experiments, we aim to identify the most ef-fective features by comparing the following six differentoptions: I) Using features from the last layer (termed asiGPT Last); II) Using features from the middle layer (termedas iGPT Middle 1); III) Using the averaging features from all layers (termed as iGPT ALL Avg); IV) Using the av-eraging features from the middle four layers (termed asiGPT Middle 4 Avg); V) Using features from the middle fourlayers (termed as iGPT Middle 4); VI) Using features fromall the layers (termed as iGPT ALL). This comprehensivecomparison will help us identify the optimal layer(s) configu-ration in the AD context.The results of this experiment are presented in ,with Figs. 6a to 6d illustrating four different metrics for theCityscapes dataset, while Figs. 6e to 6h shows the correspond-ing results for CamVid dataset. It is noticed that all fourmetrics show similar conclusions. In particular, it is evidentthat multiple layers performs better than single layer. For in-stance, from a to d, iGPT All and iGPT Middle 4exhibit better mIoU values and fluctuate less compared to otheroptions. Furthermore, the averaging of multiple layers, suchas iGPT All Avg and iGPT Middle 4 Avg, can outperformsingle layer but underperform multiple layers, as they smoothout the details of features in different layers. Moreover, acomparison between iGPT Last and iGPT Middle 1 revealsa notable distinction: features extracted from the middle layerexhibit better performance over those from the last layerin the context of downstream semantic segmentation tasks.This observation is in line with the results reported by .Notably, the performance disparity between iGPT Last andthe other considered options is markedly more pronounced,underscoring the conclusion that iGPT Last represents theleast favorable option for tasks involving street scene semanticunderstanding. Analyzing the evaluation metrics on CamViddataset in Figs. 6e to 6h, we observe that they follow similarpatterns to that of Cityscapes dataset. It is worth noting thatthe gap between iGPT Last and the other options in CamVidis narrower than that of Cityscapes, which is likely due to thesmaller data complexity of CamVid than that of Cityscapesdataset.For gaining insight of the aforementioned observationsand analysis, Table IV provides a more quantitative per-spective. Specifically, for both Cityscapes and CamViddatasets, the multiple layers options (termed as iGPT Alland iGPT Middle 4) achieve the best performance in almostall evaluation metrics. It is noteworthy that iGPT All andiGPT Middle 4 exhibit comparably high performance, withonly a narrow difference across all evaluation metrics. Forinstance, for the Cityscapes dataset, iGPT Middle 4 surpassesiGPT All with marginal improvements of 1.05 in mIoU, 0.79in mF1, and 0.9 in mPrecision. However, it falls short by 0.38in mRecall. In the case of the CamVid dataset, iGPT Middle 4again outperforms iGPT All with increments of 0.31 in mIoU,1.39 in mF1, and 0.4 in mPrecision, yet it lags by 0.2 in mRe-call. Based on these observations, the subsequent experimentsadopt iGPT Middle 4 for further exploration, which strikes atrade-off between predictive performance and computationalresources for street scene semantic understanding task withinthe realm of AD. presents a t-SNE visualization of the output ofiGPT Middle 4 for Cityscapes test dataset and CamVid testdataset, respectively. This visualization highlights that certainsemantic classes have been distinctly separated. Others, despite",
  "mIoUmF1mPrecisionmRecallmIoUmF1mPrecisionmRecall": "iGPT All Avg43.7053.4554.1654.7148.5960.3769.4056.67iGPT Last39.1549.3751.9449.9245.0756.1864.5753.69iGPT Middle 143.4953.2454.4153.8748.2860.0069.0356.37iGPT Middle 4 Avg43.2253.0255.1454.1648.8460.5969.9956.74iGPT Middle 445.8155.1556.1856.0049.2960.9070.5257.27iGPT All44.7654.3455.2856.3848.9859.5170.1257.47 being intertwined, still exhibit a sufficient spread among datapoints. More specifically, a offers a visualization of theCityscapes dataset, revealing that classes depicted in blue, lightblue and brown tend to cluster together, while other classesare more dispersed, yet still maintain considerable separation.In a similar way, b for the CamVid dataset shows thatclasses identified by blue, green, and red are well separatedfrom others. The remaining classes are not grouped closertogether, yet they are sufficiently spaced apart to allow fordistinct categorization. 2) The proposed iGPT+Head vs Existing SOTA Models:Building upon our previous discussions, it was found thatmiddle four layers (termed as iGPT Middle 4) provides thebest choice for street semantic understanding task in thecontext of AD. In this experiment, our goal is to examinewhether iGPT+Head could outperform existing state-of-the-art models. Specifically, we will compare the performance ofiGPT+Head with existing models: BiSeNetV2 and SegNet.Figs. 8a to 8d present the performance across variousmetrics on CamVid dataset. There are two obvious observa-tions: I) iGPT combined with perception head outperformsthe current leading models BiSeNetV2 and SegNet in almostall metrics. II) iGPT+Head always converge faster than thecompared SOTA benchmarks. This is because the pretrainedLVM backbone has learned rich representations of various rawnatural images. Table V compares the performance between iGPT+Head and its competitors quantitatively. The table indi-cates the following clear patterns: I) For classes that containobjects with large sizes, such as Sky, Building, Road, Side-walk, Tree, and Car, almost all models demonstrate high per-formance, often exceeding 90% across various metrics. II) Theperformance drops significantly for classes with narrow objectslike Pole and Fence. For example, the models consistentlyshow lower scores, with metrics for the Pole class approachingzero for all models. III) For the classes characterized by ahigh degree of shape variability, like Bicyclist, iGPT+Headsurpasses the performance of BiSeNetV2 and SegNet. Thisbetter performance may be due to iGPTs exposure to a morediverse dataset during training, which likely includes a widerange of Bicyclist gestures, in contrast to the more constrainedCamVid dataset used for BiSeNetV2 and SegNet. Thesefindings highlight the importance of the scale and diversityof training data for iGPT, which contribute significantly toits robust generalization capabilities, particularly for complexclasses with variable shapes.",
  "iGPT+Head94.0493.760.0597.2979.1880.3525.8247.4171.804.7552.08": "Prox include a hyperparameter that requires careful tuning.In addition, the SegNet model has been verified to achievecomparable performance to iGPT+Head on both Cityscapesand CamVid datasets, as detailed in the previous subsection.Therefore, the SegNet model is employed as the underlyingarchitecture for all FL algorithms under consideration in thisstudy. The hyperparameters for FedDyn and FedProx are setto 0.005 or 0.01. The notations FedDyn-0.005, FedDyn-0.01,FedProx-0.005, and FedProx-0.01 correspond to these modelswith the specified hyperparameters. Additionally, pFL-Vehicle-#1, pFL-Vehicle-#2, and pFL-Vehicle-#3 represent the person-alized models from the proposed framework for Vehicles 1, 2,and 3, respectively.Figs. 9a to 9d depict the inference performance of allconsidered models across all evaluation metrics on Cityscapesdataset. The results are clear: I) The personalized model tailored for Vehicle 1, Vehicle 2, and Vehicle 3 demonstratesbetter performance when compared to other benchmarks,underscoring the efficacy of the feature-based personalizedlearning approach. II) At the beginning, the metric scoresfor Vehicle 1, 2, and 3 are higher compared to the otherbenchmarks. This initial advantage for Vehicles 1, 2, and 3is due to the utilization of a pretrained iGPT for featureextraction, in contrast to the other models that begin theirtraining from scratch. III) Although Vehicle 1, 2, and 3surpass other benchmarks in overall performance, there arenoticeable differences in their performance, with each vehicleshowing a distinct accuracy. This variation in performance isattributed to the different sizes of datasets used for each vehicle(shown in ??). This suggests that larger datasets typicallyprovide more comprehensive training, leading to better model",
  ": Performance comparison of the proposed pFedLVM against other FL algorithms on Cityscapes and CamVid datasets": "performance, whereas smaller datasets may limit a modelsability to learn and generalize, resulting in a relatively poorperformance. IV) The quantitative improvements of the pFL-Vehicle-#3 compared to FedAvg are 10.05%, 11.85%, 10.71%and 9.70% in terms of mIoU, mF1, mPrecision and mRecall,respectively. Figs. 9e to 9h show the corresponding resultsof CamVid dataset, and the same conclusion can be drawnas in the Cityscapes dataset. In particular, the quantitativeimprovements are 18.47%, 25.60%, 51.03% and 14.19% interms of mIoU, mF1, mPrecision and mRecall, respectively.It is important to highlight that within the feature-basedpFL mechanism, the personalized feature compressor (withparameters v,c) and the personalized downstream head (withparameters v,p) collectively play a pivotal role in achievingsuperior performance in comparison to other baselines. The ex-periments conducted in this study, therefore, serve to validatethe effectiveness of both the personalized feature compressorand the personalized downstream head in enhancing modelperformance.2) t-SNE Visualization of pFedLVM against other existingSOTA FL algorithms: presents a t-SNE visualization, of the pixel embeddings for the Cityscapes testdataset, facilitating a comparative analysis of the modelspFL-Vehicle-#1, pFL-Vehicle-#2, and pFL-Vehicle-#3 againstFedAvg, FedDyn-0.005, and FedProx-0.005. The visualizationreveals that while FedAvg, FedDyn-0.005, and FedProx-0.005models exhibit limited capability, distinguishing only someof the semantic classes with others remaining interspersed,the pFL-Vehicle-#2 and pFL-Vehicle-#3 models demonstratea marked superiority in almost all the semantic classes.Conversely, pFL-Vehicle-#1 lags somewhat behind in perfor-mance, a discrepancy that can be traced back to its smallerdata volume relative to pFL-Vehicle-#2 and pFL-Vehicle-#3.These observations are in line with the performance metricsdetailed in Figs. 9a to 9d. visualizes the pixel embeddings of the CamVid test dataset using t-SNE , . It can be seen that pFL-Vehicle#2 and pFL-Vehicle#3 models show the best separationof semantic classes, which is similar to that of Cityscapesdataset and aligns with the performance metrics presented inFigs. 9e to 9h. 3) Evaluation of Communication Efficiency of the pro-posed pFedLVM algorithm: shows the communicationoverheads of exchanging features in the proposed pFedLVMagainst the communication overheads of exchanging LVMsin typical FL. For both Cityscapes dataset (a) andCamVid dataset (b), as the training progresses, thecommunication overheads of both exchanging schemes in-crease linearly yet with different growth rate, which suggeststhat the communication overhead saving by the proposedpFedLVM becomes more and more pronounced as the trainingprogresses.To explore how mini-batch size, feature size and LVM sizecontribute to communication overhead saving achieved by theproposed pFedLVM against the typical FL, shows theoverhead saving versus these parameters. The observations andimplications are summarized as follow: I) a displaysthe result of how batch size contributes the communicationoverhead saving. We can observe that the communicationoverhead saving fluctuates in a small range as the batch sizeincreases. As mini-batch size has little impact on improvingthe communication efficiency of pFedLVM, we can adjust itaccording to the performance need of the personalized models.II) b tells us that the feature size has an importantrole in affecting communication overhead saving. For exam-ple, for the Cityscapes dataset, the communication reductiondrops from approximately 60% to 20% when the feature sizeincreases from 1.0MB to 2.0MB. As shrinking the feature sizeimproves the communication efficiency substantially whileit will decrease the personalized models performance, weshould consider the trade-off carefully. III) LVM size alsoposes a significant effect on the communication overhead",
  "VI. CONCLUSION": "This paper introduced pFedLVM framework, which inte-grates FL with LVM in autodriving context. The proposedframework deploys LVMs only on a central server to reducecomputational burden of vehicles, and exchanges learnedfeatures instead of LVMs to reduce communication over-heads. In addition, a personalized learning mechanism wasincorporated, leading to superior performance than a globalmodel trained using typical FL. The communication efficiency,space and time complexities of the proposed pFedLVM werealso analyzed. Experimental results showed that pFedLVMoutperforms currently existing SOTA approaches by largemargins. Future work could incorporate multi-modal data, suchas natural language, into pFedLVM. C. Yang, M. Xu, Q. Wang, Z. Chen, K. Huang, Y. Ma, K. Bian,G. Huang, Y. Liu, X. Jin, and X. Liu, Flash: Heterogeneity-awarefederated learning at scale, IEEE Transactions on Mobile Computing,vol. 23, no. 1, pp. 483500, 2024. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,Communication-efficient learning of deep networks from decentralizeddata, in Artificial Intelligence and Statistics.PMLR, 2017, pp. 12731282. G. Zhu, Y. Du, D. Gunduz, and K. Huang, One-bit over-the-air aggre-gation for communication-efficient federated edge learning: Design andconvergence analysis, IEEE Transactions on Wireless Communications,vol. 20, no. 3, pp. 21202135, 2021.",
  "G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, Towardan intelligent edge: Wireless communication meets machine learning,IEEE Communications Magazine, vol. 58, no. 1, pp. 1925, 2020": "G. Zhu, Z. Lyu, X. Jiao, P. Liu, M. Chen, J. Xu, S. Cui, and P. Zhang,Pushing ai to wireless network edge: An overview on integratedsensing, communication, and computation towards 6g, Science ChinaInformation Sciences, vol. 66, no. 3, p. 130301, 2023. L. Fantauzzo, E. Fan`, D. Caldarola, A. Tavera, F. Cermelli, M. Ciccone,and B. Caputo, Feddrive: Generalizing federated learning to semanticsegmentation in autonomous driving, in 2022 IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS).IEEE, 2022, pp.11 50411 511. W.-B. Kou, S. Wang, G. Zhu, B. Luo, Y. Chen, D. W. K. Ng, andY.-C. Wu, Communication resources constrained hierarchical federatedlearning for end-to-end autonomous driving, in 2023 IEEE/RSJ Inter-national Conference on Intelligent Robots and Systems (IROS).IEEE,2023, pp. 93839390. S. Wang, C. Li, D. W. K. Ng, Y. C. Eldar, H. V. Poor, Q. Hao, andC. Xu, Federated deep learning meets autonomous vehicle perception:Design and verification, IEEE Network, vol. 37, no. 3, pp. 1625, 2023. H.-T. Wu, H. Li, H.-L. Chi, W.-B. Kou, Y.-C. Wu, and S. Wang,A hierarchical federated learning framework for collaborative qualitydefect inspection in construction, Engineering Applications of ArtificialIntelligence, vol. 133, p. 108218, 2024. Y. Hui, J. Hu, N. Cheng, G. Zhao, R. Chen, T. H. Luan, and K. Al-dubaikhy, Rcfl: Redundancy-aware collaborative federated learning invehicular networks, IEEE Transactions on Intelligent TransportationSystems, vol. 25, no. 6, pp. 55395553, 2024. B. Li, Y. Jiang, Q. Pei, T. Li, L. Liu, and R. Lu, Feel: Federated end-to-end learning with non-iid data for vehicular ad hoc networks, IEEETransactions on Intelligent Transportation Systems, vol. 23, no. 9, pp.16 72816 740, 2022.",
  "J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J.-R. Wen,Structgpt: A general framework for large language model to reasonover structured data, 2023": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,D. Zhou et al., Chain-of-thought prompting elicits reasoning in largelanguage models, Advances in neural information processing systems,vol. 35, pp. 24 82424 837, 2022. L. Wang, B. Huang, Z. Zhao, Z. Tong, Y. He, Y. Wang, Y. Wang, andY. Qiao, Videomae v2: Scaling video masked autoencoders with dualmasking, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2023, pp. 14 54914 560.",
  "Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. Lopez,Multimodal end-to-end autonomous driving, IEEE Transactions onIntelligent Transportation Systems, vol. 23, no. 1, pp. 537547, 2020": "A. Nguyen, T. Do, M. Tran, B. X. Nguyen, C. Duong, T. Phan,E. Tjiputra, and Q. D. Tran, Deep federated learning for autonomousdriving, in 2022 IEEE Intelligent Vehicles Symposium (IV).IEEE,2022, pp. 18241830. G. Li, Y. Lin, D. Ouyang, S. Li, X. Luo, X. Qu, D. Pi, and S. E. Li,A rgb-thermal image segmentation method based on parameter sharingand attention fusion for safe autonomous driving, IEEE Transactionson Intelligent Transportation Systems, vol. 25, no. 6, pp. 51225137,2024.",
  "Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. Lopez,Multimodal end-to-end autonomous driving, IEEE Transactions onIntelligent Transportation Systems, vol. 23, no. 1, pp. 537547, 2022": "W. Zheng, X. Jiang, Z. Fang, and Y. Gao, Tv-net: A structure-level feature fusion network based on tensor voting for road cracksegmentation, IEEE Transactions on Intelligent Transportation Systems,vol. 25, no. 6, pp. 57435754, 2024. K. Muhammad, T. Hussain, H. Ullah, J. D. Ser, M. Rezaei, N. Kumar,M. Hijji, P. Bellavista, and V. H. C. de Albuquerque, Vision-basedsemantic segmentation in scene understanding for autonomous driving:Recent achievements, challenges, and outlooks, IEEE Transactions onIntelligent Transportation Systems, vol. 23, no. 12, pp. 22 69422 715,2022.",
  "E. Bakopoulou, B. Tillman, and A. Markopoulou, Fedpacket: A feder-ated learning approach to mobile packet classification, IEEE Transac-tions on Mobile Computing, vol. 21, no. 10, pp. 36093628, 2022": "S. Zhang, J. Li, L. Shi, M. Ding, D. C. Nguyen, W. Tan, J. Weng, andZ. Han, Federated learning in intelligent transportation systems: Recentapplications and open problems, IEEE Transactions on IntelligentTransportation Systems, vol. 25, no. 5, pp. 32593285, 2024. H. Liao, Z. Zhou, W. Kong, Y. Chen, X. Wang, Z. Wang, andS. Al Otaibi, Learning-based intent-aware task offloading for air-groundintegrated vehicular edge computing, IEEE Transactions on IntelligentTransportation Systems, vol. 22, no. 8, pp. 51275139, 2021. Y. Xiao, R. Xia, Y. Li, G. Shi, D. N. Nguyen, D. T. Hoang, D. Niyato,and M. Krunz, Distributed traffic synthesis and classification in edgenetworks: A federated self-supervised learning approach, IEEE Trans-actions on Mobile Computing, vol. 23, no. 2, pp. 18151829, 2024. B. Liu, L. Wang, M. Liu, and C.-Z. Xu, Federated imitation learning:A novel framework for cloud robotic systems with heterogeneous sensordata, IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 35093516, 2020. S. S. Shinde and D. Tarchi, Joint air-ground distributed federatedlearning for intelligent transportation systems, IEEE Transactions onIntelligent Transportation Systems, vol. 24, no. 9, pp. 999610 011,2023. Q. Lin, Y. Li, W.-B. Kou, T.-H. Chang, and Y.-C. Wu, Communication-efficient joint signal compression and activity detection in cell-freemassive mimo, in ICC 2023 - IEEE International Conference onCommunications, 2023, pp. 50305035. S. Liu, J. Yu, X. Deng, and S. Wan, Fedcpf: An efficient-communicationfederated learning approach for vehicular edge computing in 6g com-munication networks, IEEE Transactions on Intelligent TransportationSystems, vol. 23, no. 2, pp. 16161629, 2022. Q. Lin, Y. Li, W.-B. Kou, T.-H. Chang, and Y.-C. Wu, Communication-efficient activity detection for cell-free massive mimo: An augmentedmodel-driven end-to-end learning framework, IEEE Transactions onWireless Communications, pp. 11, 2024. K. Pillutla, K. Malik, A.-R. Mohamed, M. Rabbat, M. Sanjabi, andL. Xiao, Federated learning with partial model personalization, inInternational Conference on Machine Learning.PMLR, 2022, pp.17 71617 758.",
  "S. Ren, Z. Wang, H. Zhu, J. Xiao, A. Yuille, and C. Xie, Rejuvenatingimage-gpt as strong visual representation learners, 2023": "J. Wang, Y. Ge, R. Yan, Y. Ge, K. Q. Lin, S. Tsutsui, X. Lin, G. Cai,J. Wu, Y. Shan et al., All in one: Exploring unified video-languagepre-training, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2023, pp. 65986608. X.-A. Bi, K. Chen, S. Jiang, S. Luo, W. Zhou, Z. Xing, L. Xu, Z. Liu, andT. Liu, Community graph convolution neural network for alzheimersdisease classification and pathogenetic factors identification, IEEETransactions on Neural Networks and Learning Systems, 2023.",
  "D. A. E. Acar, Y. Zhao, R. Matas, M. Mattina, P. Whatmough, andV. Saligrama, Federated learning based on dynamic regularization, inInternational Conference on Learning Representations, 2021": "M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-son, U. Franke, S. Roth, and B. Schiele, The cityscapes dataset forsemantic urban scene understanding, in Proc. of the IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), 2016. G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, Segmentationand recognition using structure from motion point clouds, in Com-puter VisionECCV 2008: 10th European Conference on ComputerVision, Marseille, France, October 12-18, 2008, Proceedings, Part I 10.Springer, 2008, pp. 4457. C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, Bisenetv2: Bilateral network with guided aggregation for real-time semanticsegmentation, International Journal of Computer Vision, vol. 129, pp.30513068, 2021. V. Badrinarayanan, A. Kendall, and R. Cipolla, Segnet: A deep con-volutional encoder-decoder architecture for image segmentation, IEEEtransactions on pattern analysis and machine intelligence, vol. 39,no. 12, pp. 24812495, 2017."
}