{
  "Abstract": "Recent advancements in decentralized learning, such asFederated Learning (FL), Split Learning (SL), and SplitFederated Learning (SplitFed), have expanded the poten-tials of machine learning. SplitFed aims to minimize thecomputational burden on individual clients in FL and par-allelize SL while maintaining privacy. This study investi-gates the resilience of SplitFed to packet loss at model splitpoints. It explores various parameter aggregation strategiesof SplitFed by examining the impact of splitting the modelat different pointseither shallow split or deep splitonthe final global model performance. The experiments, con-ducted on a human embryo image segmentation task, reveala statistically significant advantage of a deeper split point.",
  ". Introduction": "Federated learning (FL) allows multiple clients totrain machine learning models without sharing data, whichis particularly advantageous for privacy-sensitive domainslike healthcare. However, FL requires all clients to trainmodels locally, causing challenges for resource-constrainedclients. Split Learning (SL) addresses this by split-ting the model between clients and a server. Split FederatedLearning (SplitFed) combines FLs privacy preser-vation with SLs model balancing, offering the best of both.Error resilience is crucial in decentralized learning. Re-cent research has explored SplitFeds robustness to anno-tation errors and noisy communication links , butpacket lossa common transmission errorhas not paidattention yet.Prior work in FL has tackled packet lossby modelling communication links as packet erasure chan-nels or implementing loss-tolerant strategies . InSL, packet loss occurs at model split points, challenging theoptimal split points selection for loss resilience. Researchon optimal split points selection in SL and in collabo-rative intelligence exists . Loss resilience studies inthese domains are separate . Analyzing split pointchoices based on loss resilience is not previously studied.",
  ". Methodology": "We utilized a Split U-Net model for human embryo com-ponent segmentation, as in . Three split models were:client-side front-end (CS(FE)), server-side (S), and client-side back-end (CS(BE)). Two split points were: the shal-low split and the deep split. We used the Blastocyst dataset, which has 781 human embryo images with ground-truth segmentation masks for five components: background,zona pellucida (ZP), trophectoderm (TE), inner cell mass(ICM), and blastocoel (BL). 70 images were assigned fortesting. Data were non-uniformly distributed among fiveclients. Each client allocated 85% of its data for training and15% for validation. During training, resizing, horizontalflipping, and vertical flipping were applied. The loss func-tion was Soft Dice, and the optimizer was Adam with aninitial learning rate of 1 104. The Jaccard index withoutbackground was employed as the performance metric. Thesystem was trained for 12 local and 15 global epochs. Eachexperiment was repeated for 10 runs and the mean JI (MJI)",
  "arXiv:2405.19453v1 [cs.AI] 29 May 2024": "was recorded. Packet loss was simulated, with each lostpacket representing a zeroed-out row of feature and gradi-ent maps. PL {0.1, 0.3, 0.5, 0.7, 0.9}, and was indepen-dent and identically distributed (iid). Nc {0, 1, 2, 3, 4, 5}.Paramagg {naive averaging , federated averaging(FedAvg) , auto-FedAvg , fed-NCL V2 , fed-NCL V4 }.",
  ". Experiments without packet loss": "Our U-Net model, centrally trained on the same dataset as BLAST-NET (the sole network accessible for hu-man embryo component segmentation), achieved aMJI of 81.70%, compared to BLAST-NETs MJI, whichwas 79.88%.This shows our models favourable per-formance compared to BLAST-NET. The SplitFed U-Netmodels MJI for nave averaging, FedAvg, auto-FedAvg,fed-NCL V2, and fed-NCL V4 were 82.78%, 82.57%,82.99%, 83.02%, and 82.95%, respectively. We performed20 (522) pairwise t-tests for the difference in these MJIs.Jparam agg1 and Jparam agg2 are MJIs of two of Paramagg.The two-tailed t-test is",
  ". Experiments with packet loss": "Then we trained our SplitFed U-Net model with the twosplits across all PL. The deep split model outperformed theshallow split model, as in . To statistically confirmthis, we conducted 125 (5 5 5) pairwise t-tests for eachcombination of PL and Nc. JD and JS are MJIs of deep andshallow splits, respectively. The one-tailed t-test is:",
  "H0 : JD JSH1 : JD > JS(2)": "In all cases, p-value < 0.05, so the null-hypothesis H0 canbe rejected and we can conclude that deep split produces ahigher MJI than shallow split. Finally, we analyzed whethercertain Paramagg perform better than others in the pres-ence of packet loss for the deep split model. Conducting250 (52 5 5) pairwise comparisons for each combina-tion of paramagg, PL, and Nc, we found performance vari-ations among methods, but no consistent pattern emerged toprove one methods superiority over others.",
  ". MJI vs. PL for shallow split (left) and deep split (right),with variying Nc in the SplitFed U-Net model": "experiments, utilizing five advanced parameter aggregationmethods, revealed a statistically significant advantage of adeeper split. Two reasons contribute to this: Firstly, thedeep split model provides the CS(BE) with additional layersto recover the lost data. Secondly, in our deep split U-Netmodel, the initial skip connection is entirely at the client,enabling features and gradients transfers without subjectingto packet loss. Our SplitFed U-Net model showed resilienceto packet loss, up to 50%, in both shallow and deep splits.This resilience may be due to the utilization of ReLU activa-tion , and the phenomenon of packet loss acting similarlyto dropout. No significant performance gap was observedbetween models trained with packet loss probabilities up to0.5, but those trained with higher packet loss probabilitiesexhibited diminished performance. More research wouldfocus on exploring multiple SplitFed networks, investigat-ing realistic packet loss models, and developing robust ag-gregation and recovery methods. This study will initiatefurther exploration of packet loss effects on SplitFed.",
  "Zahra Hafezi Kafshgari, Ivan V. Bajic, and Parvaneh Saeedi.Smart split-federated learning over noisy channels for em-bryo image segmentation. In Proc. IEEE ICASSP, pages 15,2023. 1": "Zahra Hafezi Kafshgari, Chamani Shiranthika, ParvanehSaeedi, and Ivan V Bajic. Quality-adaptive split-federatedlearning for segmenting medical images with inaccurate an-notations. arXiv preprint arXiv:2304.14976, 2023. 1 Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovin-ski, Trevor Mudge, Jason Mars, and Lingjia Tang. Neuro-surgeon: Collaborative intelligence between the cloud andmobile edge. ACM SIGARCH Computer Architecture News,45(1):615629, 2017. 1 Juyong Kim, Yookoon Park, Gunhee Kim, and Sung JuHwang. Splitnet: Learning to semantically split deep net-works for parameter reduction and model parallelization. InProc. Mach. Learn. Res., pages 18661874. PMLR, 2017. 1",
  "Li Li, Liang Gao, Huazhu Fu, Bo Han, Cheng-Zhong Xu,and Ling Shao. Federated Noisy Client Learning, Nov. 2021.arXiv:2106.13239 [cs]. 2": "Lisette Lockhart, Parvaneh Saeedi, Jason Au, and Jon Have-lock. Multi-Label Classification for Automatic Human Blas-tocyst Grading with Severely Imbalanced Data.In Proc.IEEE MMSP, pages 16, Kuala Lumpur, Malaysia, Sept.2019. 1 Brendan McMahan, Eider Moore, Daniel Ramage, SethHampson, and Blaise Aguera y Arcas.Communication-efficient learning of deep networks from decentralized data.In Artificial intelligence and statistics, pages 12731282.PMLR, 2017. 1, 2"
}