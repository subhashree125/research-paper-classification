{
  "Abstract": "Text-to-video generation task has witnessed a notableprogress, with the generated outcomes reflecting the textprompts with high fidelity and impressive visual qualities.However, current text-to-video generation models are in-variably focused on conveying the visual elements of a sin-gle scene, and have so far been indifferent to another im-portant potential of the medium, namely a storytelling. Inthis paper, we examine text-to-video generation from a sto-rytelling perspective, which has been hardly investigated,and make empirical remarks that spotlight the limitationsof current text-to-video generation scheme. We also proposean evaluation framework for storytelling aspects of videos,and discuss the potential future directions.",
  ". Introduction": "Text-to-video generation has gained attention with its in-creasingly better controllability, visual aesthetics, and po-tential for a wide range of applications, including scene gen-eration and video editing. Creating a novel video whoseduration ranges from a few seconds to a few minutes cannow be done within a few minutes by simply providing textprompts, and a plethora of videos generated from text haveappeared on social network services and streaming services,many of which demonstrate high-quality visuals.However, current text-to-video generation models al-most invariably focus on generating visual components of a single scene or movement, while mostly disregarding othermodalities including speech or text. As such, current text-to-video generation shares a high resemblance with moviesfrom the silent film era, in which dialogues were either con-veyed through text or completely forgone, with backgroundmusic inserted to deliver the atmosphere. The differencelies in that, while most silent films strove to tell a concretestory with structured plots in spite of their lack of auditorytools, current text-to-video generation models tend to fo-cus on presenting an array of imagery centered around theprompt, rather than storytelling. It thus becomes our cen-tral question whether contemporary text-to-video genera-tion can also mimic the storytelling aspects of silent filmsthat share similar limitations.In this paper, we examine current text-to-video gener-ation models from a perspective of storytelling. We firstexamine generating videos from short stories generated bya large language model, one of which is entitled The LostMelody (), for which we make frequent referencesthroughout the paper in order to illustrate important points1.We also examine generating videos from script version ofeach short story, in order to illustrate a striking performancedrop stemming from asymmetry in training data. We furtherexamine generating videos from captions that are aimed toperform storytelling for existing videos, in order to contrastour generated results with actual videos and captions aimedfor storytelling.",
  "arXiv:2405.08720v1 [cs.CV] 13 May 2024": "Subsequently, We propose and conduct an extensiveevaluation to properly assess the videos from various as-pects, including components of a story, visual qualities, andhow they correlate to each other. We also propose a novelevaluation framework T2Vid2T, where captions are gener-ated from videos and are compared to input text promptsor ground truths, in a cyclical manner. Our evaluation pro-tocols utilize both automatic and human evaluation metricsextensively, in order to account for the videos both qualita-tively and quantitatively.Upon discussion and analysis of the generation and eval-uation carried out as described above, we further make em-pirical remarks as to the limitations that make it difficult togenerate videos that successfully tell a story, and discusspotential future directions that we believe would help criti-cally refine the performance.Note that, since our primary focus is on storytelling, wedeliberately disregard visual artifacts (which frequently ap-pear throughout our experiments) in most cases, althoughour experiments suggest that they may also play a non-negligible role in storytelling.We also disregard othershooting or editing techniques frequently used in modernvideos, such as changing shots and angles for more dramaticimpact.Our primary contributions can be summarized as follow-ing:",
  ". Related Work": "Text-to-Video Generation: Text-to-video has becomean actively deployed research topic. Frequently employedarchitectures include diffusion model and transformer, and also models that enable learning of common em-bedding for text and visual inputs, such as CLIP .Make-A-Video , built on top of text-to-image gen-eration model DALLE 2 , which in turn is based onCLIP, generates videos using spatio-temporal convolutionand frame interpolation networks. Temporal corrections aremade using a U-net-style diffusion model, and frame inter-polation network fills in the gaps between frames generatedby spatio-temporal decoder, resulting in a smoothly movingvideo. CogVideo , also based on text-to-image genera-tion model CogView2 , utilizes VQVAE to converteach frame of a video into image tokens. Since applying the same frame rate to all videos can lead to mismatches be-tween the content of the videos and the text, and consecutiveframes tend to have very similar content, making it chal-lenging to learn long-term dependencies with a fixed framerate, CogVideo inserts a token indicating the frame rate intothe text and samples frames at the specified frame rate. Ima-gen Video utilizes a cascade of spatio-temporal super-resolution, generating a total of 128 frames with a resolu-tion of 1280768 at a frame rate of 24fps. The advantage ofsuch cascade model is that it can independently learn eachsuper-resolution diffusion model.The text encoder usesa pre-trained T5 model with fixed weights. ImagenVideo is particularly noteworthy for its ability to representtext within videos, a task that was challenging for traditionalvideo generation models.Sora is a diffusion model using transformer archi-tecture, and enables generation of videos up to 1 minutethat demonstrate high fidelity to input prompts with highlyplausible graphics. Recaptioning technique from DALLE3 has been said to have helped generating videos withhigh fidelity to users text input prompts.While many text-to-video generation models do not al-low for public examination and explicitly state the limita-tions, it is generally fair to say that most of the text-to-videogeneration models above presuppose that the input textualprompt aims to describe a single scene or motion, such that,upon receiving prompts that contain multiple scenes as nec-essary for a storytelling, the models usually end up reflect-ing only a limited portion of the input, failing to generate re-sults that successfully perform storytelling. Its causes maybe attributed to various factors including limitations on in-put length, duration of output, and training data.Visual Storytelling: As a more advanced task than con-ventional image captioning task, visual storytelling was proposed to generate a story based on a sequence ofimages. In contrast, proposed the story visualizationtask, attempting to generate a sequence of images from astory. Examining video generation from stories is a naturalextension of the line of works above that attempt to bridgethe gap between vision and storytelling. Recent works havetackled the task of consistent multi-scene text-to-video gen-eration , but their focus is on multi-scene generationfrom a single prompt, rather than a wider scope of story-telling.Story Evaluation: Automatic evaluation of stories is anessential research topic for tasks related to stories. How-ever, in textual story evaluation, it has been pointed outthat existing metrics correlate poorly with human evalua-tion . In order to overcome such low correlation, novelevaluation metrics including UNION and StoryER have been proposed and demonstrated improved correla-tions with human perception.Such limitation of conventional evaluation metrics has also been pointed out in visual storytelling . proposed a metric for visual storytelling from three perspec-tives; namely relevance, coherence, and expressiveness. In-spired by , proposed the similar three perspectivesto evaluate visual storytelling; visual grounding, coherence,and non-redundancy.To assess story quality, usedstory-specific metrics in addition to lexical-matching met-rics.",
  ". Components of a Story": "While various definitions exist as to what constitutesa story, common elements include character, plot, setting,conflict, theme, and point of view. We briefly describe eachcomponent and discuss the challenges it poses on videosgenerated from text, which will also be examined explicitlylater in the paper.Character is the person in the story, around whichthe story evolves. In this simple sense, we can say thatvideos generated from text also frequently contain charac-ters. However, there is rarely a single character in a story,and multiple characters including a protagonist, the maincharacter, and an antagonist, an opposition to the protago-nist, are typically present. Furthermore, character can alsorefer to the qualities of a person. Direct characterization,where the qualities are directly presented to the audience,such as their appearances, can be easily manipulated invideos generated from text, and certain aspects of their per-sonalities can often be specified by adjusting facial expres-sions. Indirect characterization, however, where the qual-ities are formed through the persons statements or behav-iors, inevitably necessitates a development of a plot.Setting refers to time and location in which the storytakes place. The degree of specification required varies de-pending on the story, and videos generated from text in-herently provide direct visual clues for the setting. Also,setting may contain the social conditions under which thecharacter is placed, which is presented less directly and re-quires context. Although not entirely impossible, it is cer-tainly more challenging for videos generated from text toaccurately present.Plot refers to the sequence of events that occur through-out the story, and develops through multiple stages includ-ing introduction, rising action, climax, falling action, andresolution. While it is certainly possible in theory to demon-strate these multiple stages solely with visual elements, aswas the case with silent movies, it turns out to be difficultwith current text-to-video generation, as we will see later.Conflict, often considered to be an element of plot, refersto the primary opposition that the character undergoes, andmay be internal, i.e. versus self, or external, e.g. with othercharacters or circumstances. In any case, insomuch as theplot is nearly absent, it is evident that conflict is also dif-ficult to convey. Theme refers to the central message of the story, and may not always be directly expressed. Whilevisual elements often strongly contribute to the highlight-ing of a theme, it is typically developed through the plot,e.g. the outcomes of a characters behaviour. As such, itis highly challenging for current text-to-video generation toeffectively express a theme. Point of view in a story of textformat refers to the narrator; first person if the narrator isa character within the story, and third person if the narra-tor is not. Point of view takes a slightly different meaningwhen applied to videos. Implementing a truly first personspoint of view would be to use views seen from the charac-ters eyes, and while such shot is rare in commercial filmsand television shows, it can be generated by specifying itin input prompts. Third persons point of view, where theaudience oversees what is happening as an observer, is inmost cases the default point of view for videos generatedfrom text, unless specified otherwise.In addition, other elements may be included, such asstyle or tone of the story, and symbolism. The former can beconveyed through the visual atmosphere of the video, whichcan also be controlled with specifications in input prompts.The latter is more of a derivative of a plot, and thus canhardly exist in the absence of a plot.In summary, current text-to-video generation, while in-herently containing some elements of a story to varyingextents, may struggle to fully attain the indispensable ele-ments that constitute a story, as we will empirically observelater. In this paper, we primarily focus on character, set-ting, and plot, as other components can be considered sub-components of one of these, and without accounting for thethose three components, reflecting the remaining compo-nents will be simply out of question.",
  ". Generation from Short Story": "We used ChatGPT with GPT3.5 to generate a shortstory from which to generate a video. The generated shortstory consists of multiple scenes.As discussed in Sec-tion 2, with current text-to-video generation, it is nearly in-feasible to generate a video for the entire story from a sin-gle round of generation, in terms of both maximum tokenlength acceptable and the models ability to compose multi-ple scenes. As such, we need to convert the generated shortstory into a sequence of prompts, where each prompt cor-responds to a scene. Concatenating the generated video foreach prompt will be the resulting video of the short story.Note that each scene is generated independently of eachother, without any explicit adherence to previously gener-",
  ". Overall workflow of generating videos from a short story generated by a large language model": "ated outcomes. As such, each prompt for video generationmodel needs to specify the characters and the setting repeat-edly. Otherwise, the context is easily lost and the genera-tion results in incoherent videos. An alternative would beto generate the videos conditioned on previously generatedoutcomes. We did examine conditioning video generationon an image from previously generated results, but the re-sult turned out to be unreliable as we will further discuss in.Since each scene is generated independently, we also in-sert character description for proper nouns repetitively ineach prompt. For example, Lily shares a special bond with... is replaced with Lily, a young girl, shares a specialbond with..., and so forth for each prompt. Otherwise, apartfrom the first appearance of the character, none of the visualdescriptions will be available in subsequent scenes, result-ing in immense inconsistency, as illustrated in . Aswe will re-visit in , a scheme to integrate previouscontext, frequently used in LLMs, may help achieve visualcoherence more easily.Finally, video is generated for each prompt with Gen-2 , after which the generated videos are temporally con-catenated. In order to examine how the comprehensibilityof the story changes with linguistic aid, we generate speechfrom each prompt using OpenVoice . We adjuste theplaying speed of each scene in order to match the dura-tion of narration. The same duration is applied to the ver-sion without narration, as the varying duration can affectthe viewers understanding of the story. For aesthetic pur-pose, we also generate the background music with StableAudio using the scene description as the prompt, whichwas applied to all of our videos in an identical manner. shows the overall workflow of generatingvideos for a short story created by an LLM. See Supple-mental Material for actual prompts we used and the gener-ated outcomes.",
  ". Generation from Script": "Text-to-image generation or text-to-video generationhave mostly evolved around descriptive text prompts, mostof which are concerned with vividly conveying the visualcomponents to be generated. Yet, humans can also visualizethe scene from dialogue, which is in fact a strong mediumfor conveying a narrative along with novel. In fact, dra-mas or plays written in script format have longer historythan novels, and have been around for millennia. In or-der to examine to what extent video generation can handlestorytelling given text prompt in script format, we gener-ate a script version of the short story generated as describedabove. We use LLM to generate a sequence of prompts forvideo generation model, explicitly requiring the dialoguesspoken by characters, with non-dialogue parts for indicat-ing setting or transitions. As before, each scene is gener-ated independently, so proper nouns in each dialogue aresupplemented with description for each prompt.",
  ". Generation from Captions": "In order to make a comparison with how story is per-ceived from real videos, we sampled a subset from VideoStorytelling dataset , whose ground truth captions wereused for video generation. While other datasets that ex-amined visual storytelling exist, such as SIND , mostof them contain a sequence of images rather than videos,and are thus not apt for our purpose. Since videos in VideoStorytelling dataset usually span several minutes, which arelengthier than our generated videos, we either used the first5 captions, or the first 100 seconds of the video, selectingwhichever one results in shorter duration.",
  ". Evaluation": "As shown in , while there have been evaluationmetrics proposed for textual storytelling or video caption-ing, evaluation of videos from a storytelling perspective hashardly been examined, to the best of our knowledge. In thissection, we propose and conduct a wide range of evaluationmetrics to assess storytelling ability of videos both qualita-tively and quantitatively, involving both automatic and hu-man evaluations.We first evaluate the visual quality of the generatedvideos with FVD (Frechet video distance) and incep-tion score (IS) . For FVD, following , we used I3D trained on Kinetics-600 to compute the real videostatistics. For inception score, following , we used pre-trained C3D , first trained on Sports-1M and fine-tuned on UCF101 . We extracted 10 frames per secondfrom the target videos and report the average values. Whilethese metrics are not directly relevant to storytelling per se,we will later use them to examine how they correlate to theaudiences perception of the story.We also propose a novel cyclical evaluation frame-work, namely T2Vid2T (text-to-video-to-text), where weautomatically generate captions that describe the generatedvideo, and compare it against the original text prompt thatwas used to generate the video. We used a fine-tuned ver-sion of TimeSFormer for video captioning to generatecaptions from each scene. Popular evaluation metrics in-cluding BLEU , METEOR , ROUGE , CIDEr, and SPICE were employed for comparison against text prompt. Note that we replace the proper nouns in textprompt with general descriptive phrase, e.g. Lily with ayoung girl, as the proper noun is out of scope with videocaptioning framework. For Video Storytelling, we disre-garded the parts where ground truth captions were not pro-vided, and extracted only the frames that were accompaniedby captions. illustrates the workflow of T2Vid2Tusing Video Storytelling as the reference.We further conducted human evaluation, where we askedthe workers to rate the video in terms of the components ofthe story, namely character, setting, plot, as outlined in Sec-tion 3, out of 1 to 5 scale. Each component of the story wasrated according to three distinct aspects, namely expressive-ness, relevance, and coherence, as suggested by , yield-ing a 3 3 evaluation matrix per video. Expressivenessrefers to richness and diversity of expression styles. Rel-evance refers to the extent to which the video reflects thetext prompt in terms of the story component of concern.Coherence refers to the extent to which the consistency ofrespective component is retained throughout the video. Inaddition, a worker was asked to rate the video in terms ofoverall comprehensibility of the story.We used Amazon Mechanical Turk for our experiment.We presented the workers with videos generated from ashort story, a script version of the story, and a caption fromVideo Storytelling, along with an actual video to whichthe caption belongs. Each generated video is presented intwo variations; the generated video as is, and the generatedvideo with narration added to it. Note that human evaluationwas performed with 2 conditions, where the worker was ei-ther explicitly asked to ignore the visual artifacts, or to takevisual artifacts into consideration, in order to examine howthey contribute to perception of visual storytelling.Finally, we conducted a mixture of human and automaticevaluation, where the worker was asked to write a summaryof the video, with one sentence per scene, and the resultingsummary was compared against the input textual promptusing automatic evaluation metrics.",
  ". Results": "shows FVD and inception score for each typeof videos. Real videos from Video Storytelling outperformother generated videos by far. While such result is highlypredictable, it is particularly of our interest that higher vi-sual qualities also correlate to perception of story, as will beshown in other results. Also, videos generated from cap-tions for Video Storytelling displayed better results thanvideos generated from short stories or scripts, suggestingthat current text-to-video generation models are more com-patible with factual descriptions. shows the results of T2Vid2T. Note that, inputprompts were used as ground truths for results other thanVideo Storytelling. Video captioning is still widely consid-",
  "Generated (caption)0.10450.06350.12440.06150.0952Real videos0.26300.16830.25720.26040.3519": "ered a relatively difficult task, as can also be seen by no-tably low scores throughout all videos. Setting aside thereal videos, which performed far better as predicted, it isnotable that videos generated from stories and scripts per-formed substantially poorly, even when compared to videosgenerated from captions. This suggests that video caption-ing model struggles even to match a few tokens with theinput stories or scripts, hinting at a severe imbalance in thetraining corpora in terms of textual styles and formats. summarizes the results from human evaluationin terms of how well the video reflects components of astory. While trends from previous results generally con-tinue, adding linguistic elements via narration improved theoverall scores. It is notable that in some categories, videoswith narrations received lower scores than videos withoutnarrations. We conjecture that this is due to mismatch be-tween narration and the generated scene. For example, text-to-video generation frequently struggles with multiple char-acters, and easily ends up generating incoherent charactersor skipping the generation of some characters present in in-put prompt. Such mismatch is likely to exacerbate, ratherthan aid, the perception of story. Asking the workers to takevisual artifacts into consideration resulted in performance drop, suggesting that visual qualities also play a role in per-ception of stories. In fact, our investigation showed that vi-sual artifacts also had an influence on automatic evaluationsby incorrectly classifying the objects, e.g. violin to guitar,etc. shows the results of running automatic evalua-tion metrics on human-written summary upon watching thevideos, with input prompts as the ground truths. Its over-all trend is coherent with , suggesting that the lowperformance of videos generated from stories and scriptsis not solely attributed to problems with video captioningmodels, but is also highly relevant to limitations with text-to-video generation models as well. shows examplehuman-written summary for each type of videos. Note that,for The Lost Melody, later parts of the plot were oftencompletely misunderstood by the workers. This reinforcesour claims and experiment results that current text-to-videogeneration models are yet to generate convincing videos forstorytelling. visualizes how the plot for a scenewas misunderstood by the worker.",
  ". Example of human-written summary that is substantiallydifferent from input": "telling as observed in our experiments, and discuss potentialfuture directions that would enhance the performance.Video generation with visual reference: We generatedvideos for the story simply by concatenating multiple gener-ated videos for its constituents. Such workflow is bound toincur inconsistency in visual aspects, as we have witnessedin our experiments. An alternative would be to condition thetext-to-video generation on a reference visual, from whichto learn how the characters should look like, and reflect itin the generated videos. Indeed, a story generation schemewith additional source frame has been proposed for text-to-image generation , and some of the text-to-video gener-ation models also enable both image and text inputs. How-ever, it must be noted that, in storytelling perspective, thepurpose of conditioning the generation on image and text isfairly different from current editing schemes based on im-age and text inputs. In text-based image editing, for ex-ample, input images themselves are targets to be directly modified via directions in the text prompt. A direct exten-sion of this to video generation, which is currently deployedby some of the text-to-video generation models, also entailsan assumption that input images are targets to be directlymodified in the generated video, e.g. generating motionson the image as specified by input prompt. As such, whenconfronted with text inputs that do not imply a direct editingof the input image, the resultant video utterly fails to reflectany component of the input prompt, and merely ends upmaking subtle motions on the input image (see ). Instorytelling perspective, image or video inputs conditionedfor the generation are frequently not the targets to be di-rectly modified, but are references that provide clues as tohow the main components of the video should appear, sothat the consistency of visual storytelling is preserved. Assuch, a scheme to condition the video generation on imagenot by directly modifying it, but using it as a reference toadjust the visual appearances will be necessary to enablea recurrent video generation, which is more desirable thancurrent scheme of concatenating independently generatedscenes. Script-to-video generation:As discussed in Sec-tion 4.2, script is a familiar format for humans to perceivestories. As shown in .2, nonetheless, generatingvideos from dialogue is as of now far from delivering a sen-sible and coherent visuals, which implies a severe asymme-try in terms of the types of text in training corpus. In ourattempts, the model frequently fails with who is present inthe scene, when given a script as input prompt, as shownin . This implies that the current model does notunderstand how the script format is structured, particularlythe role of speaker identification token. Once dialogue-to-video generation is established, a much more powerful sto-rytelling will be possible, as it can be more easily incor-porated with audible modality using text-to-speech genera-",
  "Gen.(story)": "A little girl is smiling in front of a house in forest. The girl is sitting next to her grandma. The girl isplaying a violin in a room. Now, the girl goes outside and starts playing violin again. The girl is lost inthe forest and people are searching for her. The girl manages to come back home safely. The girl playsviolin again with joy of returning home.",
  "Gen.(script)": "A young girl and grandma are enjoying their time next to fireplace. The girl starts playing a violin. Thegirl goes out to forest. In the forest, she is fascinated by nature around her. Back at home, the girl andher mother start growing flowers. Now she plays violin with a different meaning of loving nature.",
  ". Generating videos from script format frequently resultsin complete failure of understanding the scene": "tion, which turned out to be helpful in conveying a narrativein our experiments. Lip-syncing the characters to given textis also an active research topic , and can be more easilyintegrated with videos if dialogue is given.Global story representation:Our premise so far hasbeen a concatenation of separately generated videos. Ide-ally, however, a scheme for a global understanding of struc-ture of the story will be necessary. In music generation, forexample, a hierarchical architecture is frequently employed in order to learn the global structure of a musical piece,consisting of ascendance, descent, and repetition of theme,which are also observed in stories.Naively porting thesame approach to video domain, however, will require pro-hibitively large amount of computational resources, and itmay thus be necessary to come up with a resource-efficientapproach that can compactly encode the structure of a story.One possibility is to represent the structure of a story withgraphs, as have been examined by previous works .Incorporating such global representation into video genera-tion can potentially enhance the generation of more coher-ent videos. Evaluation metrics: While we proposed and conducteda number of evaluation protocols in this paper, they still re-lied heavily on manual efforts by human. Fully automaticevaluation metrics, such as T2Vid2T, also suffered from stillvery low similarity between generated captions and groundtruth stories, and it also does not account for visual con-sistency of the character.Such issues may be mitigatedby building a more story-oriented video captioning model,which in turn requires a construction of large-scale datasetswith videos and corresponding stories. In any case, it isinarguable that more reliable and standardized evaluationmetrics for storytelling aspects of videos are indispensablefor the field to make substantial advances.",
  ". Conclusion": "We examined current text-to-video generations from anovel storytelling perspective, by generating videos fromshort stories and scripts, as well as existing captions forvideo storytelling. We performed an extensive evaluationof generated videos by proposing and conducting a wide ar-ray of evaluation protocols. Finally, we made a number ofempirical observations and suggestions for potential futuredirections that we believe would help enhance text-to-videogeneration for storytelling.",
  "Joao Carreira and Andrew Zisserman.Quo vadis, actionrecognition? a new model and the kinetics dataset. 2017IEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 47244733, 2017. 5": "Hong Chen, Duc Vo, Hiroya Takamura, Yusuke Miyao, andHideki Nakayama. StoryER: Automatic story evaluation viaranking, rating and reasoning. In Yoav Goldberg, ZornitsaKozareva, and Yue Zhang, editors, Proceedings of the 2022Conference on Empirical Methods in Natural Language Pro-cessing, pages 17391753, Abu Dhabi, United Arab Emi-rates, Dec. 2022. Association for Computational Linguistics.2",
  "Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, andJordi Pons. Fast timing-conditioned latent audio diffusion.ArXiv, abs/2402.04825, 2024. 4": "Jian Guan and Minlie Huang.UNION: An UnreferencedMetric for Evaluating Open-ended Story Generation.InBonnie Webber, Trevor Cohn, Yulan He, and Yang Liu,editors, Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pages91579166, Online, Nov. 2020. Association for Computa-tional Linguistics. 2 Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, WenbiaoDing, Xiaoxi Mao, Changjie Fan, and Minlie Huang. Open-MEVA: A benchmark for evaluating open-ended story gen-eration metrics.In Chengqing Zong, Fei Xia, Wenjie Li,and Roberto Navigli, editors, Proceedings of the 59th An-nual Meeting of the Association for Computational Linguis-",
  "Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, andJie Tang. Cogvideo: Large-scale pretraining for text-to-videogeneration via transformers. ArXiv, abs/2205.15868, 2022.2": "Ting-Yao Hsu, Chieh-Yang Huang, Yen-Chia Hsu, and Ting-Hao Huang. Visual story post-editing. In Anna Korhonen,David Traum, and Llus M`arquez, editors, Proceedings ofthe 57th Annual Meeting of the Association for Computa-tional Linguistics, pages 65816586, Florence, Italy, July2019. Association for Computational Linguistics. 3 Junjie Hu, Yu Cheng, Zhe Gan, Jingjing Liu, Jianfeng Gao,and Graham Neubig. What makes a good story? designingcomposite rewards for visual storytelling. In AAAI Confer-ence on Artificial Intelligence, 2019. 3, 5 Ting-HaoKennethHuang,FrancisFerraro,N.Mostafazadeh, Ishan Misra, Aishwarya Agrawal, JacobDevlin, Ross B. Girshick, Xiaodong He, Pushmeet Kohli,Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, LucyVanderwende, Michel Galley, and Margaret Mitchell. Visualstorytelling. In North American Chapter of the Associationfor Computational Linguistics, 2016. 2, 5 Andrej Karpathy, George Toderici, Sanketh Shetty, ThomasLeung, Rahul Sukthankar, and Li Fei-Fei. Large-scale videoclassification with convolutional neural networks.2014IEEE Conference on Computer Vision and Pattern Recog-nition, pages 17251732, 2014. 5",
  "Zengyi Qin, Wenliang Zhao, Xumin Yu, and Xin Sun.Openvoice:Versatile instant voice cloning.ArXiv,abs/2312.01479, 2023. 4": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision. In ICML, 2021.2 Colin Raffel, Noam M. Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,and Peter J. Liu. Exploring the limits of transfer learningwith a unified text-to-text transformer. J. Mach. Learn. Res.,21:140:1140:67, 2019. 2",
  "Khurram Soomro, Amir Zamir, and Mubarak Shah. Ucf101:A dataset of 101 human actions classes from videos in thewild. ArXiv, abs/1212.0402, 2012. 5": "Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre-sani, and Manohar Paluri. Learning spatiotemporal featureswith 3d convolutional networks. 2015 IEEE InternationalConference on Computer Vision (ICCV), pages 44894497,2014. 5 Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-wards accurate generative models of video: A new metric &challenges. ArXiv, abs/1812.01717, 2018. 5",
  "Ashish Vaswani, Noam M. Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, andIllia Polosukhin. Attention is all you need. In NeurIPS, 2017.2": "Ramakrishna Vedantam, C. Lawrence Zitnick, and DeviParikh. Cider: Consensus-based image description evalua-tion. 2015 IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 45664575, 2014. 5 Eileen Wang, Caren Han, and Josiah Poon. RoViST: Learn-ing robust metrics for visual storytelling. In Marine Carpuat,Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz,editors, Findings of the Association for Computational Lin-guistics: NAACL 2022, pages 26912702, Seattle, UnitedStates, July 2022. Association for Computational Linguis-tics. 3 XinWang,WenhuChen,Yuan-FangWang,andWilliam Yang Wang.No metrics are perfect:Adver-sarial reward learning for visual storytelling.In IrynaGurevych and Yusuke Miyao, editors, Proceedings of the56th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 899909, Mel-bourne, Australia, July 2018. Association for ComputationalLinguistics. 3",
  "A.1. Short Story Generation": "Prompt:Write a plot for a 5-minute animation. It should involvebasic elements of a story, including characters, setting, plot,conflict, and theme. The overall mood should be warm andheartfelt, but it may be good to add an element of eeriness.Output:Title: The Lost MelodySetting: A cozy, rustic cottage in a peaceful forest.Characters:Lily: A young girl with a bright smile, staying at the cot-tage with her grandmother. Grandma Rose: A kind and wiseelderly woman who lives in the cottage. The Enchanted Vi-olin: An old, mysterious violin found in the attic. Plot: The story begins with Lily visiting her Grandma Rose inthe tranquil cottage deep in the forest. They share a specialbond, and Lily loves hearing her grandmothers stories.One day, while exploring the attic, Lily stumbles uponan old, dusty violin. Grandma Rose explains that its anenchanted violin, but it has been silent for years because itsmissing a magical melody.Intrigued by the idea of an enchanted violin, Lily is de-termined to find the lost melody. She embarks on a questthrough the forest, seeking magical elements to create themelody. Along the way, she encounters talking animals whooffer clues, and the forest itself seems to come to life.As Lilys journey unfolds, an eerie, mystical atmosphereenvelopes the forest. Strange and enchanting occurrences,like glowing fireflies and ethereal music, add an element ofeeriness to the story.Finally, with the help of the forest creatures, Lily finds allthe ingredients needed for the magical melody and returnsto the cottage.She plays the melody on the enchanted violin. It re-sponds with beautiful, heartwarming music that fills the for-est, causing flowers to bloom and animals to dance.The message of the story becomes clear: The magic is inthe connection between generations and the beauty of pass-ing down traditions and stories.Theme: The central theme of The Lost Melody is thepower of tradition, the importance of family bonds, and theenchanting wonder that can be found in even the most unex-pected places. It combines warmth and heartfelt momentswith an element of mystery and magic, creating an unfor-gettable and heartwarming tale.",
  "A.2. Prompt Sequence Generation": "Prompt:I want to generate a video for the generated short storyabove using AI text-to-video generation tools. So I need todivide them into a sequence of texts, each of which is tobe the prompt for the generation. For the prompt to suc-cessfully generate coherent video, each prompt will have tocontain the character and setting, since each prompt is inde-pendent of others from the tools perspective. Please makethe prompt sequences.Output:Introduction to Setting and Character:Meet Lily, a young girl with a bright smile, staying ata cozy, rustic cottage nestled in a peaceful forest. Lily isvisiting her grandmother, Grandma Rose.Special Bond and Grandmas Stories: In the tranquilforest cottage, Lily shares a special bond with GrandmaRose. Lily cherishes hearing her grandmothers enchantingstories.Discovery of the Enchanted Violin: While exploring theattic one day, Lily discovers an old, dusty violin in the cozy cottage. Grandma Rose reveals its mystery its an en-chanted violin, silent for years without its magical melody.Quest for the Lost Melody Begins: Intrigued by the en-chanted violin, Lily is determined to find the lost melody.She embarks on a quest through the mystical forest sur-rounding the cozy cottage, seeking magical elements to cre-ate the melody.Encounters in the Enchanted Forest: As Lily exploresthe enchanted forest, she encounters talking animals whooffer clues. The forest itself seems to come to life aroundthe cozy cottage. Strange occurrences, like glowing fire-flies and ethereal music, add an eerie yet enchanting atmo-sphere.Culmination of Lilys Journey: During Lilys journey,an eerie, mystical atmosphere envelops the forest aroundthe cozy cottage. With the help of the forest creatures, Lilygathers all the magical ingredients and returns to the cot-tage.Magical Melody Unveiled: Back at the cozy cottage,Lily plays the magical melody on the enchanted violin. Theviolin responds with beautiful, heartwarming music thatfills the forest. Flowers bloom, and animals dance in re-sponse.The Message of the Story: In this enchanting tale, dis-cover the magic in the connection between generations andthe beauty of passing down traditions and stories at the cozy,rustic cottage in the peaceful forest. Now, each promptcontains both the character (Lily, Grandma Rose) and thesetting (cozy cottage in the peaceful forest). Adjustmentscan still be made based on the specific requirements of thetext-to-video generation tool.",
  "A.3. Script Generation": "Prompt:Convert the story above into a script with dialogue to bespoken by characters, with non-dialogue part for indicat-ing setting, transition, etc. Write the script as a prompt se-quence for AI text-to-video generation tools, while includ-ing characters in each prompt as you did before.Output:INT. COZY COTTAGE - DAYSetting: A cozy, rustic cottage in a peaceful forest. Char-acters: Lily (young girl), Grandma Rose (elderly woman)INT. COZY COTTAGE - LIVING ROOM - DAYLily and Grandma Rose sit by the fireplace. Dialogue:Lily: Grandma, your stories always make the cottage feeleven cozier. Grandma Rose: Well, my dear, stories havea way of adding warmth to our surroundings. INT. COZYCOTTAGE - ATTIC - DAYLily explores the attic, discovering an old, dusty violin.Dialogue: Lily: Whats this, Grandma? Grandma Rose:Ah, that, Lily, is no ordinary violin. Its enchanted, but ithas been silent for years. EXT. COZY COTTAGE - DAY Lily sets out on a quest through the mystical forest.INT/EXT. ENCHANTED FOREST - VARIOUS LOCA-TIONS - DAYLily encounters talking animals, gathers magical ele-ments, and experiences the mystical atmosphere of the for-est. Dialogue: Lily (to a talking bird): Have you heard ofthe lost melody? I need your help. Talking Bird (chirping):Follow the fireflies. They hold the secret. EXT. COZYCOTTAGE - DAYLily returns to the cottage, the enchanted forest echoingwith mysterious sounds. INT. COZY COTTAGE - LIVINGROOM - DAYLily, with gathered ingredients, stands before GrandmaRose. Dialogue: Lily: Grandma, Im going to play thelost melody on the enchanted violin! Grandma Rose: Oh,Lily, that melody has been silent for far too long. Let theforest hear its song again. EXT. COZY COTTAGE - DAYLily plays the magical melody. The forest responds withblooming flowers and dancing animals. INT. COZY COT-TAGE - LIVING ROOM - DAYGrandma Rose watches with joy. Dialogue: GrandmaRose (whispering): The magic is in the connection be-tween generations, Lily. Youve brought the melody backto life. EXT. COZY COTTAGE - DAYThe forest, now alive with enchantment, surrounds thecozy cottage."
}