{
  "Abstract": "A unified and versatile LiDAR segmentation model withstrong robustness and generalizability is desirable for safeautonomous driving perception. This work presents M3Net,a one-of-a-kind framework for fulfilling multi-task, multi-dataset, multi-modality LiDAR segmentation in a univer-sal manner using just a single set of parameters. To betterexploit data volume and diversity, we first combine large-scale driving datasets acquired by different types of sensorsfrom diverse scenes and then conduct alignments in threespaces, namely data, feature, and label spaces, during thetraining. As a result, M3Net is capable of taming hetero-geneous data for training state-of-the-art LiDAR segmenta-tion models. Extensive experiments on twelve LiDAR seg-mentation datasets verify our effectiveness. Notably, usinga shared set of parameters, M3Net achieves 75.1%, 83.1%,and 72.4% mIoU scores, respectively, on the official bench-marks of SemanticKITTI, nuScenes, and Waymo Open.",
  ". Introduction": "Dense and structural 3D surrounding scene understand-ing provides crucial information for autonomous vehicles tomake proper decisions . With the recent advancementsin sensing technologies, especially the Light Detection andRanging (LiDAR) sensor, a holistic scene perception can beachieved by segmenting the acquired sensor data .Most existing LiDAR segmentation models [1, 38, 113, 123, 130] are trained and tested in a single-task, single-dataset, single-modality manner. Despite achieving com-mendable results in the single domain, there is a signifi-cant performance drop when transitioning to new domains. The limited generalization capability hinders theirfacilitation of real-world applications . In re-ality, LiDAR datasets are marred by significant variances,encompassing variations in data patterns due to different",
  "The first two authors contributed equally to this work": ". Performance comparisons among M3Net [], Single-Dataset Training [], and Nave Joint Training [] across twelveLiDAR segmentation datasets. For better comparisons, the radiusis normalized based on M3Nets scores. The larger the area cover-age, the higher the overall performance. Best viewed in colors. sensor types and weather conditions, diverse class distribu-tions arising from varying capture scenarios, and distinctlabel spaces shaped by specific annotation protocols. Thesefactors collectively pose a formidable challenge in harmo-nizing disparate LiDAR point clouds and jointly optimiz-ing model parameters to effectively address multiple tasksacross a range of sensor modalities . Empiricalevidence in further reveals that navely combiningheterogeneous data to train a LiDAR segmentation model without strategic alignments often leads to sub-opt results.Recent works resort to un-supervised domain adaptation (UDA) for utilizing trainingdata from both source and target domains to optimize oneparameter set. Nevertheless, they either focus on only thesharing mapping between two domains (by ignoring dis-",
  "arXiv:2405.01538v1 [cs.CV] 2 May 2024": "joint classes) or directly merge source domain labels toalign with the target domain . The overlook ofthe performance degradation on the source dataset and thedestruction of original label mappings inevitably constrainssuch a learning paradigm. Furthermore, there have beenefforts to employ multi-dataset learningstrategies to bolster the generalization prowess of 3D per-ception models. However, they either necessitate dataset-specific fine-tuning, deviating from a truly universal learn-ing approach, or converge label spaces to a coarser set, re-sulting in the dilution of fine-grained segmentation capabil-ities across diverse semantic categories.In this work, we define a novel paradigm towards lever-aging LiDAR point clouds from different datasets to tamea single set of parameters for multi-task LiDAR segmenta-tion. Sibling to image segmentation communities , we call this paradigm universal LiDAR segmenta-tion. The ultimate goal of such a synergistic way of learningis to build a powerful segmentation model that can absorbrich cross-domain knowledge and, in return, achieve strongresilience and generalizability for practical usage. Giventhe substantial differences among datasets in terms of datacharacteristics, feature distributions, and labeling conven-tions, we introduce a comprehensive multi-space alignmentapproach that encompasses data-, feature-, and label-levelalignments, to effectively pave the path for efficient and uni-versally applicable LiDAR segmentation. In particular, themulti-modal data, including images and texts, is fully ex-ploited to assist the alignment process with the guidanceof more general knowledge. Through aforementioned pro-cesses, we propose M3Net to learn common knowledgeacross datasets, modalities, and tasks, thereby significantlyenhancing its applicability in practical scenarios.To substantiate the efficacy of M3Net and the utility ofeach module developed, we have carried out a series of thor-ough comparative and ablation studies across an extensivearray of driving datasets, as shown in . Notably, ourbest model achieves state-of-the-art LiDAR segmentationperformance with 75.1%, 83.1%, 72.4% mIoU scores onSemanticKITTI , nuScenes , Waymo Open , re-spectively, using a shared set of parameters. Moreover, ourapproach also performs well for direct knowledge transferand out-of-distribution adaptations, further underscoring itsrobust capability for effective knowledge transfer.",
  ". Related Work": "LiDAR Segmentation. A holistic perception of 3D scenesis crucial for safe autonomous driving .Various LiDAR segmentation models have been proposed,with distinct focuses on aspects include LiDAR repre-sentations , model archi-tectures , sensor fusion, post-processing , data aug- mentations , etc. Most recently, researchersstarted to explore data efficiency , annotation ef-ficiency , annotation-free learning , zero-shot learning , domain adaptation, and robustness in LiDARsegmentation, shedding lights for practitioners.Existingpursues, however, learn separate parameter sets for eachdataset, impeding the scalability. This motivates us to ex-plore LiDAR segmentation in a multi-task, multi-dataset,multi-modality manner with just a single set of parameters. Multi-Task Learning. A proper pipeline design could en-able the model to generate suitable predictions to fulfill mul-tiple tasks simultaneously . The current researchendeavors mainly focus on building image or video seg-mentation models to handle semantic, instance, and panop-tic segmentation tasks .Recently, several attempts have been made to enable multi-task segmentation on LiDAR point clouds.MaskRange and MaskPLS extend the mask classificationparadigm for joint semantic and panoptic LiDAR seg-mentation. LidarMultiNet uses global context pool-ing and task-specific heads to handle LiDAR-based detec-tion and segmentation. P3Former proposed a spe-cialized positional embedding to handle the geometry am-biguity in panoptic LiDAR segmentation. Our frameworkalso supports multi-task learning. Different from existingapproaches, the proposed M3Net stands out by combiningknowledge from different sensor data across multiple datasources, which achieves superior performance on each task. Multi-Dataset Learning. Leveraging data samples fromdifferent sources for training has been proven effective inenhancing robustness and generalizability . Various ap-proaches have been proposed to merge image datasets forobject detection , image seg-mentation , depth estimation ,etc. Due to large domain gaps, the image-based methodsare often hard to be transferred to 3D. To combine multi-ple LiDAR datasets for 3D object detection, MDT3D defines a coarse label set to handle the label space conflictsin different point cloud datasets. MS3D++ ensem-bles pre-trained detectors from different source datasets formulti-domain adaptation. Uni3D resorts to dataset-specific detection heads and feature re-coupling for traininga unified 3D object detector. Recently, PPT proposedto pre-train a point cloud segmentation network using datafrom multiple datasets. However, the pre-trained weightsare then fine-tuned on each specific dataset, which breaksthe universal learning manner. The closest work to us isCOLA , which trains a single model across multiplesources by converting dataset-specific labels to a commoncoarse set. Such a conversion, however, leads to the loss offine-grained segmentation across the various semantic cate-gories. Differently, our M3Net is tailored to tame a single . Statistical analysis of six sharing semantic classes in the nuScenes [], SemanticKITTI [], and Waymo Open [] datasets. Eachviolin plot shows the class distribution across LiDAR scenes spanning 50 meters, centered around the ego-vehicle. Best viewed in colors. parameter set to fulfill multi-task prediction across multipledatasets while still maintaining the original label mappings.Multi-Modality Learning. Recent trend favors synergisticlearning from data of different modalities, such as vision,language, and speech . For LiDARsegmentation, several works exploredthe distillation of image features to point clouds. Recently,OpenScene and CLIP2Scene proposed to leveragepoint clouds along with multi-view images and languagefor open-vocabulary learning. PPKT , SLidR , andSeal form cross-sensor contrastive learning objectivesto pre-train the LiDAR segmentation models. The advan-tages of sensor fusion have been consistently proven. In thiswork, to pursue universal LiDAR segmentation, we proposeto align multi-space point clouds via images and texts.",
  ". Approach": "Our study serves as an early attempt at combining multi-task, multi-dataset, multi-modality knowledge into a singleset of parameters to fulfill universal LiDAR segmentation.We start with a pilot study to unveil the difficulties in merg-ing heterogeneous LiDAR point clouds (cf. Sec. 3.1). Wethen present M3Net, a versatile LiDAR segmentation net-work tailored to pursue i) statistical consistency in the dataspace (cf. Sec. 3.2), ii) cross-modality-assisted alignmentin the feature space (cf. Sec. 3.3), and iii) language-guidedunification in the label space (cf. Sec. 3.4).",
  ". Pilot Study": "The current de facto of training a LiDAR segmenta-tion network adopts a task-by-task and dataset-by-datasetpipeline. Despite the superior performance achieved undersuch standalone settings, the trained parameter sets cannotbe shared to satisfy out-of-domain requirements and, there-fore, limits their use cases for practical applications.Nave Joint Training. A natural alternative to breaking theabove constraint is to jointly train a network across multiple datasets for better generalizability. However, as depicted in, it is often non-trivial to navely combine heteroge-neous data with large data distribution gaps to train a univer-sal LiDAR segmentation model without proper alignments.To testify this, we conducted a pilot study using the priorart MinkUNet for both standalone and joint training onthree large-scale datasets . As shown in (a)and (d), a brutal combination undermines the segmentationperformance. Due to large discrepancies in aspects like sen-sor configurations, data acquisitions, label mappings, anddomain shifts, the jointly trained representations tend to bedisruptive instead of being more general.LiDAR Sensor Discrepancy. To understand the root causeof performance degradation, we conducted another studythat controls point cloud density discrepancies when merg-ing datasets. As shown in (b) and (c), joint training ondata collected by sensors with different beam numbers tendsto suffer more severely than merging less density variantdata. We hypothesize that this is mainly caused by the datastatistical variations. In light of these observations, we pro-pose a bag of suitable operations in the following sectionsto alleviate the large domain gaps among different LiDARsegmentation datasets .",
  ". Data-Space Alignment": "Given a total of S datasets Ds = {(xs, ys)|1 s S},where (xs, ys) denotes the data-label pairs constituting adataset. For the LiDAR segmentation task, xs often en-compasses the LiDAR point cloud P s = {px, py, pz}s RN3 and synchronized multi-view camera images V s ={I1, ..., Il}|l = 1, ..., L}, where It RHW 3, N is thenumber of points, L denotes the number of camera sensors,H and W are the height and width of the image, respec-tively. ys RN denotes point cloud labels in the labelspace Ys, we unify the label space as Yu = Y1Y2...YS.Cross-Modality Data Alignment. As a multi-sensing sys-tem, the information encoded in P si and V si are intuitively",
  "cd": ". The t-SNE plots of learned features before and af-ter the feature-space alignment in merging the nuScenes [], Se-manticKITTI [], and Waymo Open [] datasets. We show imagefeatures from (a) standalone networks; (b) SAM , and pointcloud features (c) before and (d) after the feature-space alignment. . Feature-space alignment in M3Net. We leverage bothimage features Fv and LiDAR point cloud features Fp extractedfrom image encoder Eimg and point encoder Epcd to employ theregularization via V2P loss and achieve feature-space alignment. it exclusively to image and point features from the samedataset.In this mode, point features solely learn frommatching image features, restricting their knowledge ac-quisition.Ideally, we aim to ensure that image featuresencompass not only scenes identical to those representedin point clouds but also scenes from other datasets.Toaddress this, we propose a domain-aware cross-modalityguided alignment, as illustrated in . Specifically, wefirst extract, for each dataset, Fv and Fp from the same im-age encoder Eimg and point encoder Epcd during the cross- modality assisted alignment. The sets of features from alldatasets are concatenated along the channel dimension toform Fv Rcvhw. Subsequently, we sequentially feedFv through a branch that consists of a global average pool-ing and an MLP. Simultaneously, Fv is fed to an auxiliarybranch that undergoes the same processing flow and gener-ates an output after the softmax function G(). The outputsfrom both branches are multiplied to obtain Fm Rcv11.The overall process can be described as follows:",
  "where T R44 is the camera extrinsic matrix that consistsof a rotation matrix and a translation matrix, and Ts R34": "is the camera intrinsic matrix. As we will show in the fol-lowing sections, such a cross-sensor data alignment servesas the foundation for alignments in other spaces.Cross-Sensor Statistical Alignment. To mitigate the dis-crepancies in sensor installations across different datasets,we incorporate a point coordinate alignment operation.Specifically, drawing upon insights from prior domainadaptation approaches , we adjust the coordinateorigins of point clouds from different datasets by introduc-ing an offset R13 to the ground plane. We find empir-ically that such an alignment can largely reduce the degra-dation caused by the variations in different sensor setups.Dataset-Specific Rasterization. It is conventional to ras-terize LiDAR point clouds P s using unified rasterizationparameters, e.g., voxel size or horizontal rangeview resolution . However, the point clouds ac-quired in different LiDAR datasets naturally differ in den-sity, range, intensity, etc., which tends to favor differentrasterization parameters. To meet such a requirement, weselect dataset-specific parameters for rasterization on eachdataset through empirical experiments and analyses. Decoupled BN. Another challenge in training across mul-tiple datasets is the presence of domain gaps, which can re-sult in significant statistical shifts of feature learning amongdatasets. Such shifts can hinder the convergence and af-fect the models ability to generalize well across diversedatasets. We adopt a decoupled batch norm (BN) for pointcloud features in each dataset. Instead of using the tradi-tional BN, which calculates mean and variance across allsamples in a mini-batch, the decoupled BN tends to adapteach datasets specific characteristics independently.",
  ". Feature-Space Alignment": "We aim to acquire a generalized feature representationfor downstream tasks. Compared to point clouds, imagescontribute stronger visual, textural, and semantic informa-tion.Thus, the collaboration between pixels and pointscould enrich the overall representation. Previous research has consistently demonstrated that such a com-bination results often leads to improved performance.Cross-Modality Assisted Alignment.In the context ofmulti-dataset joint training, our objective is to establish aunified feature space by leveraging image features to assistpoint cloud features. Acknowledging that images used intraining lack ground truth labels , we utilize imagefeatures from a pre-trained model as an alternative, facilitat-ing a more universally applicable representation. We feedcamera images V s into a pre-trained DeepLab anda vision-language model (VLM) and visualize the outputimage features by t-SNE . As shown in , we ob-serve that image features from DeepLab appear disorderlyand lack semantics. In contrast, features from VLM share amore unified feature space. Motivated by this, we proposea cross-modality assisted alignment that uses VLM to helpalign the feature space. Specifically, the camera images V s are fed to the frozen image encoder from VLM to obtain im-age features Fv = {F1v, F2v, ..., Fsv}, where Fsv Rchw.The LiDAR point clouds P s, on the other hand, are fed tothe point encoder followed by a projection layer to gen-erate the point features Fp = {F1p, F2p, ..., Fsp}, whereFsp Rmc; m denotes the number of non-empty grids.We then leverage the paired image features Fv Rmpc",
  ". Label-Space Alignment": "Label Conflict. In multi-dataset joint training settings, la-bel conflicts emerge as a significant challenge. This oftenrefers to the inconsistencies in class labels across differentdatasets involved in the training process. The discrepancycan arise due to variations in annotation conventions, label-ing errors, or even differences in the underlying semanticsof classes between datasets. In our baseline, we unionizethe different label spaces across datasets into Yu, where alldatasets share a single LiDAR segmentation head. How-ever, this may introduce several potential drawbacks: Loss of granularity: Unified label spaces could lose se-mantic granularity, particularly when dealing with subtlecategory differences in between different datasets.",
  "Information loss: During label space consolidation, de-tails unique to each dataset may be obscured or lost, es-pecially for those related to domain-specific categories": "Increased complexity: Handling a unified label space maynecessitate more complex model architectures or trainingstrategies, thereby increasing overall complexity.To address these issues, we introduce a language-guidedlabel-space alignment to facilitate a more holistic seman-tic correlation across datasets. Given the natural correspon-dence between images and texts and the strong correlationbetween images and point clouds, we aim to strategicallyutilize the image modality as a bridge to establish language-guided alignments. Such a process consists of a text-driven . Label-space alignment in M3Net. We leverage imagefeatures Fv, point cloud features Fp, and text embedding Ft ex-tracted from Eimg, Epcd, and Etxt, respectively, for regularizationvia the I2P, P2T, and V2T losses in the label-space alignment.",
  "point alignment, a text-driven image alignment, and a cross-modality-assisted label alignment.Text-Driven Alignments. As depicted in , images V s": "are fed into the frozen image encoder Eimg to extract theimage features Fv. Concurrently, the LiDAR point cloudsP s are processed by the point encoder Epcd to generate thepoint features Fp. Additionally, given the text input T s, textembedding features Ft RQc are obtained from a frozentext encoder Etxt, where Q represents the number of cate-gories across datasets. The text is composed of class namesfrom unified label space Yu placed into pre-defined tem-plates, and the text embedding captures semantic informa-tion of the corresponding classes. Subsequently, pixel-textpairs {vk, tk}Mk=1 and point-text pairs {pk, tk}Mk=1 are gen-erated, where M represents the number of pairs. Leverag-ing the semantic information contained in the text, we selec-tively choose positive and negative samples for both imagesand points for contrastive learning. It is noteworthy thatnegative samples are confined to the specific dataset cate-gory space. The overall objective of the text-driven pointalignment function is shown as follows:",
  "(7)": "Cross-Modality-Assisted Label Alignment.After text-driven alignments, the subsequent crucial step entails align-ing the point and image modalities within the label space.We first obtain image logits Fvl= {F1vl, F2vl, ..., Fsvl}and point logits Fpl = {F1pl, F2pl, ..., Fspl} from text-drivenalignments, where Fsvl RQHW , Fspl RNQ. Sub-sequently, we conduct cross-modality alignment to obtainpaired image logits Fvl RmpQ and paired point log-its Fpl RmpQ. Formally, the cross-modality-assistedalignment in the label space is formulated as follows:",
  ". Universal LiDAR Segmentation": "We enhance the versatility of M3Net via multi-taskinglearning. This integration involves an instance extractor toenable joint semantic and panoptic LiDAR segmentation.Panoptic LiDAR Segmentation. Motivated by DSNet , our instance extractor comprises an instance head anda clustering step. The instance head encompasses severalMLPs designed to predict the offsets between instance cen-ters. The clustering step uses semantic predictions to filterout stuff points, thereby retaining only those associated withthing points. The remaining points undergo a mean-shiftclustering , utilizing features from the instance head todiscern distinct instances. Lastly, we employ the L1 lossLl1 to optimize the thing point regression process.Overall Objectives. Putting everything together, the over-all objective of M3Net is to minimize the following losses:",
  ". Experimental Setups": "Datasets. Our M3Net framework and baselines are trainedon a combination of nuScenes , SemanticKITTI ,and Waymo Open . Meanwhile, we resort to anotherfive LiDAR-based perception datasets and two 3D robustness evaluation datasets to verify thestrong generalizability of M3Net. Due to space limits, ad-ditional details regarding the datasets are in the Appendix.Implementation Details.M3Net is implemented basedon Pointcept and MMDetection3D . We use twobackbones in our experiments, i.e., MinkUNet and . Ablation study on the M3Net alignments happen in the Data, Feature, and Label spaces, respectively, when combining theSemanticKITTI , nuScenes , and Waymo Open datasets. The mAcc and mIoU scores are in percentage. Best scores are in bold.",
  ". Ablation Study": "Multi-Space Alignments. The effectiveness of three pro-posed alignments over the joint training baselines is shownin Tab. 1. We observe that the data-space alignment playsthe most crucial role in improving the universal LiDAR seg-mentation performance. Without proper data alignments,joint training with either MinkUNet or the strongerPTv2+ will suffer severe degradation, especially onsparser point clouds . On top of the data-space align- ment, the combinations of multi-view images at the featurespace and the language-guided knowledge at the label spacefurther enhance the learned feature representations. The re-sults show that they work synergistically in merging knowl-edge from heterogeneous domains during joint training.Panoptic LiDAR Segmentation. In Tab. 2, we present an-other ablation study focusing on panoptic LiDAR segmen-tation. All three alignments incorporated in M3Net demon-strate significant improvements over the baselines.Thishighlights the pronounced efficacy of our multi-space align-ments.Moreover, our approach outperforms the single-dataset state-of-the-art method Panaptic-PHNet by anotable 2.17% PQ on Panoptic-SemanticKITTI andachieves compelling results on Panoptic-nuScenes .Visual Feature Alignments. We conduct a qualitative anal-ysis of the learned visual feature distributions in the formof t-SNE . (a) and (b) represent the distribu-tions of learned visual features among three datasets fromDeepLab and VLM backbones, respectively. The featuresobtained by the latter exhibit more distinct semantics infeature space. The concentrated distribution space is ad-vantageous for achieving feature alignments across multipledatasets. Additionally, (c) and (d) illustrate the distri-bution of point cloud features before and after feature-spacealignment. As can be seen, the feature distribution distances . Knowledge transfer and generalization analyses across five LiDAR segmentation datasets and two 3D robustness evaluationdatasets. All scores are given in percentage. The best and second-best scores are highlighted in bold and underline, respectively.",
  ". Comparative Study": "Comparisons to State of the Arts. In Tab. 4, we compareM3Net with current best-performing models on the bench-marks of SemanticKITTI , nuScenes , and WaymoOpen . Remarkably, M3Net consistently outperformsexisting approaches across all three datasets. Specifically,on SemanticKITTI , M3Net achieves a 72.0% mIoU onthe validation set, surpassing the closest method by a no-table margin of 1.7% mIoU. Similarly, on nuScenes ,M3Net achieves 80.9% mIoU and 83.1% mIoU on the val-idation and test sets, demonstrating its robustness and gen-eralization capabilities. Additionally, the performance ofM3Net on Waymo Open is competitive with prior arts.We achieve a mIoU of 72.4% and a mAcc of 81.1%. Theseresults highlight again the superiority of M3Net in handlingcomplex diverse LiDAR segmentation tasks.Direct Knowledge Transfer. To further validate the strong knowledge transfer capability of M3Net, we conduct exten-sive experiments on five different LiDAR-based perceptiondatasets . These datasets have uniquedata collection protocols and data distributions. As shownin and the first ten columns in Tab. 3, our frameworkconstantly outperforms the prior arts, the nave joint train-ing, and the single-dataset baselines across all five datasets.This concretely supports the strong knowledge transfer effi-cacy brought by multi-space alignments in M3Net.Out-of-Distribution Generalization. Evaluating the gen-eralization ability of models on out-of-training-distributiondata is crucial, particularly in safety-critical fields like au-tonomous driving . In this context, we resortto the two corruption datasets from the Robo3D bench-mark, i.e., SemanticKITTI-C and nuScenes-C, to conductour assessment. From the last four columns of Tab. 3, weobserve that M3Net achieves better results than the navejoint training and other single-dataset approaches, provingthe strong generalizability of the learned representations.",
  ". Conclusion": "In this work, we presented M3Net, a universal frame-work capable of fulfilling multi-task, multi-dataset, multi-modality LiDAR segmentation using a single set of param-eters. Through extensive analyses, we validated the effec-tiveness of applying data-, feature-, and label-space align-ments to handle such a challenging task. In addition, ourcomprehensive analysis and discourse have delved into thefundamental challenges of acquiring the general knowledgefor scalable 3D perception, which holds substantial poten-tial to propel further research in this domain. Our futurestrides focus on combining more data resources to furtherenhance the alignments and adaptations in our framework. Acknowledgements. This work was partially supported by NSFC(No.62206173) and MoE Key Laboratory of Intelligent Percep-tion and Human-Machine Collaboration (ShanghaiTech Univer-sity). This work was also supported by the Ministry of Education,Singapore, under its MOE AcRF Tier 2 (MOET2EP20221- 0012),NTU NAP, and under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, aswell as cash and in-kind contribution from the industry partner(s).",
  "A.1. Overview": "In this work, we resort to ten driving datasets for achiev-ing i) multi-dataset training and evaluations, ii) knowledgetransfer and generalization, and iii) out-of-distribution gen-eralization. A summary of the datasets used in this work is shown in Tab. 5. For multi-dataset training and evalua-tions, we use the LiDAR and camera data from the nuScenes, SemanticKITTI , and Waymo Open datasets. nuScenes is a large-scale public dataset for autonomousdriving, created by Motional (formerly nuTonomy). Itis widely used in the research and development of au-tonomous vehicles and related technologies. The datasetincludes a comprehensive range of sensor data crucial forautonomous driving. It typically contains data from mul-tiple cameras, LiDAR, RADAR, GPS, IMU, and othersensors. This multimodal data collection is essential fordeveloping and testing algorithms for perception, predic-tion, and motion planning in autonomous vehicles. Oneof the strengths of the nuScenes dataset is its diversity.The data encompasses various driving conditions, includ-ing different times of day, weather conditions, and ur-ban environments.This diversity is crucial for train-ing robust algorithms that can handle real-world drivingscenarios.In this work, we use the LiDAR semanticand panoptic segmentation data from the lidarseg1 sub-set in the nuScenes dataset, which includes segmenta-tion labels for the entire nuScenes dataset, encompass-ing thousands of scenes, each a 20-second clip capturedfrom a driving vehicle in various urban settings.32classes are manually labeled, covering a wide range ofobjects and elements in urban scenes, where 16 of themare typically adopted in evaluating the segmentation per-formance. More details of this dataset can be found at SemanticKITTI is a well-known dataset in the field ofautonomous driving and robotics, specifically tailored forthe task of semantic and panoptic segmentation using Li-DAR point clouds.It is an extension of the originalKITTI Vision Benchmark Suite2 , with annotationsfor over 20 sequences of driving scenarios, each contain-ing tens of thousands of LiDAR scans. The dataset coversa variety of urban and rural scenes. This includes citystreets, residential areas, highways, and country roads,providing a diverse set of environments for testing al-gorithms.The dataset provides labels for 28 differentsemantic classes, including cars, pedestrians, bicycles,various types of vegetation, buildings, roads, and so on.19 classes are typically adopted for evaluation. In total,around 4549 million points are annotated, and such ex-tensive labeling provides a dense coverage for each Li-DAR scan. More details of this dataset can be found at Waymo Open is a large dataset for autonomous driving,provided by Waymo LLC, a company that specializes inthe development of self-driving technology. This datasetis particularly notable for its comprehensive coverage . Summary of the datasets used in this work. We split different datasets into three categories: i) The nuScenes , SemanticKITTI, and Waymo Open datasets are used for multi-dataset training and evaluations. ii) The RELLIS-3D , SemanticPOSS ,SemanticSTF , SynLiDAR , and DAPS-3D datasets are used for knowledge transfer and generalization (w/ fine-tuning).iii) The SemanticKITTI-C and nuScenes-C datasets are used for out-of-distribution generalization (w/o fine-tuning).",
  "SemanticSTF SynLiDAR DAPS-3D SemanticKITTI-C nuScenes-C [Link][Link][Link][Link][Link]": "of various scenarios encountered in autonomous driving.The data is collected using Waymos self-driving vehi-cles, which are equipped with an array of sensors, includ-ing high-resolution LiDARs, cameras, and radars. Thismultimodal data collection allows for comprehensive per-ception modeling. The dataset includes a wide range ofdriving environments and conditions, such as city streets,highways, and suburban areas, captured at different timesof day and in various weather conditions. This varietyis crucial for developing robust autonomous driving sys-tems. In this work, we use its 3D Semantic Segmenta-tion subset, which specifically provides point-level an-notations for 3D point clouds generated by LiDAR sen-sors. 22 semantic classes are used during evaluation, en-compassing a wide range of object classes, such as vehi-cles, pedestrians, and cyclists, as well as static objects likeroad signs, buildings, and vegetation. More details of thisdataset can be found at To validate that the learned features from our multi-dataset training setup are superior to that of the singe-dataset training in knowledge transfer and generalization,we conduct fine-tuning experiments on the following fivedatasets: RELLIS-3D , SemanticPOSS , Semantic-STF , SynLiDAR , and DAPS-3D .",
  "RELLIS-3D is a dataset focusing on off-road environ-": "ments for autonomous navigation and perception, de-veloped by Texas A&M University.It contains mul-timodal sensor data, including high-resolution LiDAR,RGB imagery, and GPS/IMU data, providing a compre-hensive set for developing and evaluating algorithms foroff-road autonomous driving.The dataset features di-verse terrain types, such as grasslands, forests, and trails,offering unique challenges compared to urban scenar-ios. RELLIS-3D includes annotations for 13 semanticclasses, including natural elements and man-made ob-jects, crucial for navigation in off-road settings. Moredetails of this dataset can be found at SemanticPOSS focuses on panoramic LiDAR scans,which include urban scenes, highways, and rural areas.The dataset contains annotations for 14 semantic classes,covering vehicles, pedestrians, cyclists, and various roadelements. Its panoramic view provides a 360-degree un-derstanding of the vehicles surroundings, which is ben-eficial for comprehensive scene analysis. More detailsof this dataset can be found at",
  "SemanticSTF studies the 3D semantic segmentation ofLiDAR point clouds under adverse weather conditions,including snow, rain, and fog. It is built from the real-": "world STF dataset with point-wise annotations of 21semantic categories. The original LiDAR data in STFwas captured by a Velodyne HDL64 S3D LiDAR sen-sor. In total, SemanticSTF selected 2076 scans for denseannotations, including 694 snowy, 637 dense-foggy, 631light-foggy, and 114 rainy scans. More details of thisdataset can be found at SynLiDAR is a synthetic dataset for LiDAR-based se-mantic segmentation.It is generated using advancedsimulation techniques to create realistic urban, suburban,and rural environments. SynLiDAR offers an extensiverange of annotations for a variety of classes, includingdynamic objects like vehicles and pedestrians, as wellas static objects like buildings and trees.This datasetis useful for algorithm development and testing in sim-ulated environments where real-world data collection ischallenging. More details of this dataset can be found at DAPS-3D is a dataset focusing on dynamic and staticpoint cloud segmentation. It includes LiDAR scans fromdiverse urban environments, providing detailed annota-tions for dynamic objects such as vehicles, pedestrians,and cyclists, as well as static objects like buildings, roads,and vegetation.DAPS-3D is designed to advance re-search in dynamic scene understanding and predictionin autonomous driving, addressing the challenges posedby moving objects in complex urban settings. More de-tails of this dataset can be found at",
  "Meanwhile, we leverage the SemanticKITTI-C andnuScenes-C datasets in the Robo3D benchmark toprobe the out-of-training-distribution robustness of": "SemanticKITTI-C is built upon the validation set ofthe SemanticKITTI dataset. It is designed to coverout-of-distribution corruptions that tend to occur in thereal world. A total of eight corruption types are bench-marked, including fog, wet ground, snow, motion blur,beam missing, crosstalk, incomplete echo, and cross-sensor cases.For each corruption, three subsets thatcover different levels of corruption severity are created,i.e. easy, moderate, and hard. The LiDAR segmentationmodels are expected to be trained on the clean sets whiletested on these eight corruption sets. The performancedegradation under corruption scenarios is used to mea-sure the models robustness. Two metrics are designedfor such measurements, namely mean Corruption Error(mCE) and mean Resilience Rate (mRR). mCE calculatesthe relative robustness of a candidate model compared tothe baseline model, while mRR computes the absoluteperformance degradation of a candidate model when itis tested on clean and corruption sets, respectively. Intotal, there are 97704 LiDAR scans in SemanticKITTI-",
  "A.2.1nuScenes": "The LiDAR point clouds in the nuScenes datasetare acquired by a Velodyne HDL32E with 32 beams, 1080(+/10) points per ring, 20Hz capture frequency, 360-degree Horizontal FOV, +10-degree to 30-degree Verti-cal FOV, uniform azimuth angles, a 80m to 100m range,and up to around 1.39 million points per second. There area total of 16 semantic classes in this dataset. The distribu-tions of these classes across a 50 meters range are shownin Tab. 6. As can be seen, most semantic classes distributewithin the 20 meters range. The dynamic classes, such asbicycle, motorcycle, bus, car, and pedestrian,show a high possibility of occurrence at round 5 meters to10 meters. The static classes, on the contrary, are often dis-tributed across a wider range. Typically examples includeterrain and manmade. Different semantic classes ex-hibit unique distribution patterns around the ego-vehicle.",
  "A.2.2SemanticKITTI": "The LiDAR point clouds in the SemanticKITTI dataset are acquired by a Velodyne HDL-64E with 64beams, providing high-resolution data. The Velodyne HDL-64E features a 360-degree Horizontal Field of View (FOV),a Vertical FOV ranging from +2 to 24.33 degrees, and anangular resolution of approximately 0.080.4 degrees (ver-tically) and 0.08 0.35 degrees (horizontally). The sensoroperates at a 10Hz capture frequency and can detect objectswithin a range of up to 120m, delivering densely sampled,detailed point clouds with approximately 1.3 million pointsper second. There are a total of 19 semantic classes in thisdataset. The distributions of these classes across a 50 me-ters range are shown in Tab. 7. It can be seen from thesestatistical plots that the distributions are distinctly differ-ent from each other; points belonging to the road class . The statistical analysis of the 16 semantic classes in the nuScenes dataset. Statistics are calculated from the training splitof the dataset. Each violin plot shows the LiDAR point cloud density distribution in a 50 meters range. Best viewed in colors.",
  "A.2.3Waymo Open": "The 3D semantic segmentation subset of the WaymoOpen dataset features LiDAR point clouds obtainedusing Waymos proprietary LiDAR sensors, which includemid-range and short-range LiDARs. There are five LiDARsin total - one mid-range LiDAR (top) and four short-rangeLiDARs (front, side left, side right, and rear), where the mid-range LiDAR has a non-uniform inclination beam an-gle pattern. The range of the mid-range LiDAR is trun-cated to a maximum of 75 meters.The range of theshort-range LiDARs is truncated to a maximum of 20 me-ters. The strongest two intensity returns are provided forall five LiDARs.An extrinsic calibration matrix trans-forms the LiDAR frame to the vehicle frame. The pointclouds of each LiDAR in Waymo Open are encoded as arange image. Two range images are provided for each Li-DAR, one for each of the two strongest returns. There arefour channels in the range image, including range, inten-sity, elongation, and occupancy. The distributions of theseclasses across a 50 meters range are shown in Tab. 8. Ascan be seen. the class distributions of Waymo Open aremore diverse than those from nuScenes and SemanticKITTI. . The statistical analysis of the 19 semantic classes in the SemanticKITTI dataset. Statistics are calculated from the trainingsplit of the dataset. Each violin plot shows the LiDAR point cloud density distribution in a 50 meters range. Best viewed in colors.",
  "B.1. Overview": "A proper pipeline design could enable the model to gen-erate suitable predictions to fulfill multiple tasks simultane-ously. In the context of LiDAR segmentation, we are es-pecially interested in unifying semantic and panoptic seg-mentation of LiDAR point clouds. Such a holistic way of3D scene understanding is crucial for the safe perception inautonomous vehicles.",
  "B.2. Mean Shift": "In this work, we enhance the versatility of our frame-work in an end-to-end fashion through the integration of amulti-tasking approach. This adaptation involves the mod-ification of the instance extractor on top of the semanticpredictions, which enables a dual output for both LiDARsemantic and panoptic segmentation. Specifically, draw-ing inspiration from DS-Net , our instance extrac-tor comprises an instance head, succeeded by a point clus-tering step. The instance head encompasses a sequence ofmulti-layer perceptrons designed to predict the offsets be-tween instance centers. This point clustering step strategi-cally employs semantic predictions to filter out stuff points,thereby retaining only those associated with thing instances,such as pedestrian, car, and bicyclist. Subse-quently, the remaining points undergo mean-shift cluster-ing , utilizing features from the instance head to dis-cern distinct instances. This meticulous process enhancesthe frameworks capacity for accurate instance segmenta-tion. The bandwidth for mean-shift in the SemanticKITTIand Panoptic-nuScenes datasets is set to 1.2 and 2.5, re-spectively.",
  "C.1. Datasets": "In our multi-dataset training pipeline, we train ourM3Net framework on the three most popular large-scaledriving datasets, i.e., the SemanticKITTI , nuScenes ,and Waymo Open datasets. These three datasets con-sist of 19130, 29130, and 23691 training LiDAR scans,and 4071, 6019, and 5976 validation LiDAR scans, respec-tively. Besides, we leverage the synchronized camera im-ages from the corresponding datasets as our 2D inputs inthe M3Net training pipeline for cross-modality alignments. The SemanticKITTI, nuScenes, and Waymo Open datasetscontain 19130, 174780, and 118455 camera images in thetrain set, respectively, where SemanticKITTI has single-camera (front-view) data, nuScenes is with a six-camera(three front-view and three back-view) systems, and WaymoOpen has five camera views in total.For multi-task experiments on SemanticKITTI andPanoptic-nuScenes , we follow the official data prepa-ration procedures to set up the training and evaluations.Specifically, these two datasets share the same amount ofdata with their semantic segmentation subsets, i.e., 19130and 29130 training LiDAR scans, and 4071 and 6019 vali-dation LiDAR scans, respectively. Each LiDAR scan is as-sociated with a panoptic segmentation map which indicatesthe instance IDs. For additional details, kindly refer to theoriginal papers.For the knowledge transfer fine-tuning experiments onthe RELLIS-3D , SemanticPOSS , SemanticSTF, SynLiDAR and DAPS-3D datasets, we fol-low the same procedure as Seal to prepare the trainingand validation sets. Kindly refer to the original paper formore details on this aspect.For the out-of-training-distribution generalization exper-iments on SemanticKITTI-C and nuScenes-C, we follow thesame data preparation procedure in Robo3D . Thereare eight different corruption types in each dataset, includ-ing fog, wet ground, snow, motion blur, beam missing,crosstalk, incomplete echo, and cross-sensor cases, whereeach corruption type contains corrupted data from threeseverity levels. In total, there are 97704 LiDAR scans inSemanticKITTI-C and 144456 LiDAR scans in nuScenes-C. For additional details, kindly refer to the original paper.",
  "C.2. Text Prompts": "In this work, we adopt the standard templates along withspecified class text prompts to generate the CLIP text em-bedding for the three datasets used in our multi-datasettraining pipeline.Specifically, the text prompts associ-ated with the semantic classes in the nuScenes , Se-manticKITTI , and Waymo Open datasets are shownin Tab. 9, Tab. 10, and Tab. 11, respectively.",
  "16vegetationtree, trunk, tree trunk, bush, shrub, plant, flower, woods": "environments, like autonomous driving, where understand-ing the temporal evolution of the scene is crucial. A keyfeature of the Minkowski convolution, and by extensionMinkUNet, is its ability to perform convolutional operationson sparse data. This is achieved through the use of a gener-alized sparse convolution operation that can handle data inhigh-dimensional spaces while maintaining computationalefficiency. The implementation of Minkowski convolutionsis facilitated by the Minkowski Engine, a framework forhigh-dimensional sparse tensor operations. This engine en-ables the efficient implementation of the MinkUNet andother similar architectures. In this work, we resort to thePointcept implementation of MinkUNet and adopt thebase version as our backbone network in M3Net.Moredetails of this used backbone can be found at",
  "C.3.2PTv2+": "PTv2+ introduces an effective grouped vector at-tention (GVA) mechanism. GVA facilitates efficient infor-mation exchange both within and among attention groups,significantly enhancing the models ability to process com-plex point cloud data. PTv2+ also introduces an improvedposition encoding scheme. This enhancement allows forbetter utilization of point cloud coordinates, thereby bol-stering the spatial reasoning capabilities of the model. The additional position encoding multiplier strengthens the po-sition information for attention, allowing for more accu-rate and detailed data processing. Extensive experimentsdemonstrate that PTv2+ achieves state-of-the-art perfor-mance on several challenging 3D point cloud understandingbenchmarks. In this work, we resort to the Pointcept implementation of PTv2+ implementation of MinkUNetand adopt the base version as our backbone network inM3Net. More details of this used backbone can be found at",
  "C.4. Training Configuration": "In this work, we implement the proposed M3Net frame-work based on Pointcept and MMDetection3D .We trained our baselines and M3Net on four A100 GPUseach with 80 GB memory. We adopt the AdamW optimizer with a weight decay of 0.005 and a learning rate of0.002. The learning rate scheduler utilized is cosine decayand the batch size is set to 6 for each GPU.In the data-specific rasterization process, we rasterize thepoint clouds with voxel sizes tailored to the dataset char-acteristics. Specifically, we set the voxel sizes to [0.05m,0.05m, 0.05m], [0.1m, 0.1m, 0.1m], and [0.05m, 0.05m,0.05m] for the SemanticKITTI , nuScenes , andWaymo Open datasets, respectively.For data augmentation, we leverage several techniques,including random flips along the X, Y , and XY axes, and",
  "19traffic signtraffic sign": "random jittering within the range of [-0.02m, 0.02m]. Addi-tionally, we incorporate global scaling and rotation, choos-ing scaling factors and rotation angles randomly from theintervals [0.9, 1.1] and , respectively. Furthermore,we integrate Mix3D into our augmentation strategyduring the training. There also exists some other augmen-tation techniques, such as LaserMix , PolarMix ,RangeMix , and FrustumMix .Forthenetworkbackbones,wehaveoptedforMinkUNet and PTv2+ . In the case of MinkUNet,the encoder channels are set as {32, 64, 128, 256}, and thedecoder channels are {256, 128, 64, 64}, each with a kernelsize of 3. Meanwhile, for the PTv2+, the encoder chan-nels are {32, 64, 128, 256, 512}, and the decoder channelsare {64, 64, 128, 256}. For additional details, kindly referto the original papers.For the loss function, we incorporate the conventionalcross-entropy loss and the Lovasz-softmax loss to pro-vide optimization for the LiDAR semantic and panopticsegmentation task. Additionally, we employ the L1 loss tooptimize the instance head, aiding in the regression of pre-cise instance offsets.",
  "TPc + FPc + FNc.(11)": "Here, TPc, FPc, and FNc represent the true positive, falsepositive, and false negative of class c, respectively. ThemIoU score on each dataset is calculated by averaging theIoU scores across every semantic class. Notably, followingrecent works , we report mIoU with Test TimeAugmentation (TTA). For additional details, kindly refer tothe original papers.For panoptic LiDAR segmentation, we follow conven-tional reporting and utilize the Panoptic Quality (PQ) as ourprimary metric. The definition and calculation of the Panop-tic Quality (PQ), Segmentation Quality (SQ), and Recogni-",
  "(b) M3Net (w/ PTv2+ backbone)": ". Performance comparisons among M3Net [], Single-Dataset Training [], and Nave Joint Training [] across twelve LiDARsegmentation datasets. Subfigure (a): M3Net w/ a MinkUNet backbone. Subfigure (b): M3Net w/ a PTv2+ backbone. Forbetter comparisons, the radius is normalized based on M3Nets scores. The larger the area coverage, the higher the overall performance.",
  "D.1. Pilot Study": "In the main body of this paper, we conduct a pilot studyto showcase the potential problems in the Single-DatasetTraining and Nave Joint Training pipelines. Specifically,we observe that it is non-trivial to navely combine hetero-geneous data from different driving datasets with large datadistribution and sensor configuration gaps to train a univer-sal LiDAR segmentation model.We show in our pilot study with the MinkUNet backbone in subfigure (a) and the PTv2+ back-bone in subfigure (b), for both standalone and joint train-ing setups.As can be seen, using either the classicalMinkUNet or the most recent PTv2+ as the backbone, thebrutal combination will undermine the segmentation perfor-mance. Due to large discrepancies in aspects like sensorconfigurations, data acquisitions, label mappings, and do-main shifts, the jointly trained representations tend to be disruptive instead of being more general. Such degradationis particularly overt using navely combining LiDAR dataacquired by different sensor setups, such as the direct mergeof nuScenes (Velodyne HDL32E with 32 laser beams)and SemanticKITTI (Velodyne HDL-64E with 64 laserbeams).Meanwhile, we also supplement the complete compari-son results among the Single-Dataset Training, Nave JointTraining, and our proposed M3Net pipelines and show theresults in . As can be seen, compared to the Single-Dataset Training baselines, a nave merging of heteroge-neous LiDAR data will cause severe performance degrada-tion. This observation holds true for both the MinkUNet backbone as in a and the PTv2+ backboneas in b, which highlights again the importance of con-ducting alignments when merging multiple driving datasetsfor training. Notably, after proper data, feature, and labelspace alignments, we are able to combine the advantageof leveraging the diverse training data sources and achievebetter performance than the Single-Dataset Training base-lines. Such improvements are holistic, as shown in the radarcharts, our proposed M3Net achieves superior performancegains over the baselines under all the tested scenarios acrossall twelve LiDAR segmentation datasets.",
  "D.3.1Panoptic-SemanticKITTI": "For the detailed PQ, RQ, and SQ scores of our compara-tive study on the SemanticKITTI dataset, we supplementTab. 12 to facilitate detailed comparisons with state-of-the-art LiDAR segmentation approaches on the validation set.We observe that the proposed M3Net is capable of achiev-ing new arts on the validation set, especially for the morefine-grained metrics like RQ and SQ. The results verify theeffectiveness of the proposed M3Net compared to the singe-dataset training and nave joint training baselines.",
  "D.4. Out-of-Distribution Generalization": "In this section, we supplement the class-wise CE andRR scores of the out-of-training-distribution generaliza-tion experiments on the SemanticKITTI-C and nuScenes-C datasets in the Robo3D benchmark. Specifically,Tab. 13 and Tab. 14 show the per-corruption IoU scoresof prior works, our baselines, and the proposed M3Neton the SemanticKITTI-C and nuScenes-C datasets, respec-tively. We observe that M3Net sets up clear superiority overprior arts across almost all eight corruption types. Suchrobust feature learning is crucial to the safe operation ofautonomous vehicles under out-of-training-distribution sce-narios, especially in safety-critical areas .",
  ". Ablation study of the data, feature, and label space alignments in the proposed M3Net (w/ PTv2+ backbone)": "datasets are shown in , , and , respec-tively. As we can see, the proposed M3Net shows superiorperformance than the baseline under different driving sce-narios. Such results highlight the effectiveness of the pro- posed M3Net in enhancing performance in the multi-task,multi-dataset, multi-modality training setting. Additionally,we present qualitative results in to showcase the ca-pability of M3Net in tackling both the LiDAR semantic seg- mentation and panoptic segmentation tasks. As we can see,the proposed M3Net demonstrates effectiveness in makingaccurate predictions among the complex object and back-ground classes in the driving scenes, underscoring its effec-tiveness in handling multi-task LiDAR segmentation.",
  "F.1. Positive Societal Influence": "In this work, we present a versatile LiDAR segmentationframework dubbed M3Net for conducting multi-task, multi-dataset, multi-modality LiDAR segmentation in a unifyingpipeline. LiDAR segmentation is crucial for the develop-ment of safe and reliable autonomous vehicles. By accu-rately interpreting the vehicle surroundings, LiDAR helpsin obstacle detection, pedestrian safety, and navigation,thereby reducing the likelihood of accidents and enhancingroad safety. LiDAR segmentation contributes significantlyto societal welfare through its applications in various fields.Its ability to provide accurate, detailed 3D representationsof physical environments enables more informed decision-making, enhances safety, and promotes sustainability.",
  "F.2. Potential Limitation": "Although our proposed M3Net is capable of leveragingmultiple heterogeneous driving datasets to train a versatileLiDAR segmentation network and achieve promising uni-versal LiDAR segmentation results, there still exists roomfor improvement.Firstly, our framework leverages cali-brated and synchronized camera data to assist the align-ments. Such a requirement might not be met in some olderLiDAR segmentation datasets. Secondly, we do not han-dle the minority classes during multi-dataset learning, espe-cially for some dynamic classes that are uniquely definedby a certain dataset. Thirdly, we do not consider the com-bination of simulation data with real-world LiDAR pointclouds.We believe these aspects are promising for fu-ture work to further improve our multi-task, multi-dataset,multi-modality LiDAR segmentation framework.",
  "We acknowledge the use of the following public datasets,during the course of this work: nuScenes3 . . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0": "nuScenes-devkit4 . . . . . . . . . . . . . . . . . Apache License 2.0 SemanticKITTI5 . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0 SemanticKITTI-API6 . . . . . . . . . . . . . . . . . . . MIT License Waymo Open Dataset7 . . . . . . . . Waymo Dataset License RELLIS-3D8 . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 3.0 SemanticPOSS9 . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown SemanticSTF10 . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0 SynLiDAR11 . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License DAPS-3D12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License Robo3D13 . . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0",
  "G.2. Public Models Used": "We acknowledge the use of the following public imple-mentations, during the course of this work: MinkowskiEngine14 . . . . . . . . . . . . . . . . . . . . .MIT License PointTransformerV215 . . . . . . . . . . . . . . . . . . . . . Unknown spvnas16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License Cylinder3D17 . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 SLidR18 . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 OpenSeeD19 . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 segment-anything20 . . . . . . . . . . . . . . . Apache License 2.0 Segment-Any-Point-Cloud21 . . . . . . . CC BY-NC-SA 4.0 Mix3D22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Unknown LaserMix23 . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0",
  "G.3. Public Codebases Used": "We acknowledge the use of the following public code-bases, during the course of this work: mmdetection3d24 . . . . . . . . . . . . . . . . . Apache License 2.0 Pointcept25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License OpenPCSeg26 . . . . . . . . . . . . . . . . . . . . Apache License 2.0 . The class-wise panoptic segmentation scores on the val sets of the Panoptic-SemanticKITTI and Panoptic-nuScenes datasets. All scores are given in percentage (%). For each evaluated metric: bold - best in column; underline - second best in column.",
  "M3Net (Semantic Segmentation)Ground-TruthM3Net (Panoptic Segmentation)": ". Qualitative comparisons between the Ground-Truth and the proposed M3Net for LiDAR panoptic segmentation on the Se-manticKITTI dataset . To highlight the panoptic segmentation effect, the semantic predictions in the third column are painted in gray.For panoptic segmentation predictions, each color-coded cluster represents a distinct instance. Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy,Alexandre Boulch, and Renaud Marlet. Rangevit: Towardsvision transformers for 3d semantic segmentation in au-tonomous driving. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 52405250, 2023. 1,2, 8 Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu,Jiatao Gu, and Michael Auli. Data2vec: A general frame-work for self-supervised learning in speech, vision and lan-guage. In International Conference on Machine Learning,pages 12981312, 2022. 3 Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-zel, Sven Behnke, Cyrill Stachniss, and Juergen Gall. Se-mantickitti: A dataset for semantic scene understanding oflidar sequences. In IEEE/CVF International Conference onComputer Vision, pages 92979307, 2019. 2, 3, 4, 6, 7, 8,9, 10, 11, 13, 15, 16, 17, 19, 20, 23, 24, 27 Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-zel, Sven Behnke, Jurgen Gall, and Cyrill Stachniss. To-wards 3d lidar-based semantic scene understanding of 3dpoint cloud sequences: The semantickitti dataset. Interna-tional Journal of Robotics Research, 40:95996, 2021. 2,3 Maxim Berman, Amal Rannen Triki, and Matthew BBlaschko. The lovasz-softmax loss: a tractable surrogatefor the optimization of the intersection-over-union measurein neural networks. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 44134421, 2018. 6,17 Mario Bijelic, Tobias Gruber, Fahim Mannan, FlorianKraus, Werner Ritter, Klaus Dietmayer, and Felix Heide.Seeing through fog without seeing fog: Deep multimodalsensor fusion in unseen adverse weather.In IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1168211692, 2020. 11 Alexandre Boulch, Corentin Sautier, Bjorn Michele, GillesPuy, and Renaud Marlet.Also: Automotive lidar self-supervision by occupancy estimation. In IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1345513465, 2023. 1, 2 Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-modal dataset for autonomous driving. In IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1162111631, 2020. 2, 3, 4, 9, 11 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin.Emerging properties in self-supervised vision transformers.In IEEE/CVF International Conference on Computer Vi-sion, pages 96509660, 2021. 3 Jun Cen, Shiwei Zhang, Yixuan Pei, Kun Li, Hang Zheng,Maochun Luo, Yingya Zhang, and Qifeng Chen.Cmd-fusion: Bidirectional fusion network with cross-modalityknowledge distillation for lidar semantic segmentation.arXiv preprint arXiv:2307.04091, 2023. 3 Runnan Chen, Youquan Liu, Lingdong Kong, NenglunChen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wen-ping Wang. Towards label-free scene understanding by vi-sion foundation models. In Advances in Neural InformationProcessing Systems, 2023. 2 Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu,Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and WenpingWang. Clip2scene: Towards label-efficient 3d scene under-standing by clip. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 70207030, 2023.2, 3, 4 Runnan Chen, Xinge Zhu, Nenglun Chen, Wei Li, YuexinMa, Ruigang Yang, and Wenping Wang. Bridging languageand geometric primitives for zero-shot point cloud segmen-tation. In ACM International Conference on Multimedia,pages 53805388, 2023. 2 Tian Chen, Shijie An, Yuan Zhang, Chongyang Ma,Huayan Wang, Xiaoyan Guo, and Wen Zheng. Improvingmonocular depth estimation by leveraging structural aware-ness and complementary datasets. In European Conferenceon Computer Vision, pages 90108, 2020. 2 Yanbei Chen, Manchen Wang, Abhay Mittal, Zhenlin Xu,Paolo Favaro, Joseph Tighe, and Davide Modolo. Scaledet:A scalable multi-dataset object detector. In IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages72887297, 2023. 2 Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmen-tation. In Advances in Neural Information Processing Sys-tems, pages 1786417875, 2021. 2 Bowen Cheng, Ishan Misra, Alexander G. Schwing,Alexander Kirillov, and Rohit Girdhar. Masked-attentionmask transformer for universal image segmentation.InIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 12901299, 2022. 2 Huixian Cheng, Xianfeng Han, and Guoqiang Xiao. Cenet:Toward concise and efficient lidar semantic segmentationfor autonomous driving. In IEEE International Conferenceon Multimedia and Expo, pages 16, 2022. 2 Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, andBingbing Liu.Af2-s3net: Attentive feature fusion withadaptive feature selection for sparse semantic segmentationnetwork. In IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1254712556, 2021. 2, 8 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebas-tianGehrmann,ParkerSchuh,KensenShi,SashaTsvyashchenko, Joshua Maynez, Abhishek Rao, ParkerBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, JamesBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-mawat, Sunipa Dev, Henryk Michalewski, Xavier Gar-cia, Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Bar-ret Zoph, Alexander Spiridonov, Ryan Sepassi, David Do-han, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, AitorLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polo-zov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, JasonWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, SlavPetrov, and Noah Fiedel. Palm: Scaling language modelingwith pathways. arXiv preprint arXiv:2204.02311, 2022. 3 Christopher Choy, JunYoung Gwak, and Silvio Savarese.4d spatio-temporal convnets:Minkowski convolutionalneural networks. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 30753084, 2019.2, 3, 4, 6, 7, 8, 15, 17, 18, 19, 20, 21",
  "Christoph Feichtenhofer, Yanghao Li, and Kaiming He.Masked autoencoders as spatiotemporal learners. In Ad-vances in Neural Information Processing Systems, 2022. 3": "Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lub-ing Zhou, Holger Caesar, Oscar Beijbom, and Abhinav Val-ada. Panoptic nuscenes: A large-scale benchmark for lidarpanoptic segmentation and tracking. IEEE Robotics andAutomation Letters, 7:37953802, 2022. 2, 3, 4, 6, 7, 8, 9,10, 11, 12, 15, 16, 19, 20, 23, 25 Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, andHuijing Zhao. Are we hungry for 3d lidar data for semanticsegmentation? a survey of datasets and methods.IEEETransactions on Intelligent Transportation Systems, 2021.1 Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the kitti vision benchmarksuite. In IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 33543361, 2012. 9 Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan,Xuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo,Huizhong Chen, Liang-Chieh Chen, and David A. Ross.Dataseg: Taming a universal multi-dataset multi-task seg-mentation model. arXiv preprint arXiv:2306.01736, 2023.2",
  "Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-shick. Mask r-cnn. In IEEE/CVF International Conferenceon Computer Vision, pages 29612969, 2017. 2": "Fangzhou Hong, Hui Zhou, Xinge Zhu, Hongsheng Li, andZiwei Liu. Lidar-based panoptic segmentation via dynamicshifting network. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1309013099, 2021.6, 7, 15, 18, 23 Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu,Hongsheng Li, and Ziwei Liu. Unified 3d and 4d panopticsegmentation via dynamic shifting networks. IEEE Trans-actions on Pattern Analysis and Machine Intelligence, 46(5):34803495, 2024. 2, 6, 15 Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy,and Yikang Li. Point-to-voxel knowledge distillation forlidar semantic segmentation.In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 84798488, 2022. 8 Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, YulanGuo, Zhihua Wang, Niki Trigoni, and Andrew Markham.Randla-net: Efficient semantic segmentation of large-scalepoint clouds. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1110811117, 2020. 2 Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, NikiTrigoni, and Andrew Markham.Towards semantic seg-mentation of urban-scale 3d point clouds: A dataset, bench-marks and challenges. In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 49774987,2021. 1",
  "Juana Valeria Hurtado, Rohit Mohan, Wolfram Burgard,and Abhinav Valada. Mopt: Multi-object panoptic track-ing. arXiv preprint arXiv:2004.08189, 2020. 7, 23": "Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, NikitaOrlov, and Humphrey Shi. Oneformer: One transformer torule universal image segmentation. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pages29892998, 2023. 2 Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Em-ilie Wirbel, and Patrick Perez. xmuda: Cross-modal unsu-pervised domain adaptation for 3d semantic segmentation.In IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1260512614, 2020. 1, 2, 3 Maximilian Jaritz, Tuan-Hung Vu, Raoul De Charette,Emilie Wirbel, and Patrick Perez.Cross-modal learningfor domain adaptation in 3d semantic segmentation. IEEETransactions on Pattern Analysis and Machine Intelligence,45(2):15331544, 2023. 1, 2, 3 Peng Jiang, Philip Osteen, Maggie Wigness, and SrikanthSaripallig. Rellis-3d dataset: Data, benchmarks and anal-ysis. In IEEE International Conference on Robotics andAutomation, pages 11101116, 2021. 6, 8, 10, 15",
  "Learning semantic segmentation from multiple datasetswith label shifts.In European Conference on ComputerVision, pages 2036, 2022. 2": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao, SpencerWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar,and Ross Girshick. Segment anything. In IEEE/CVF In-ternational Conference on Computer Vision, pages 40154026, 2023. 5 Alexey Klokov, Di Un Pak, Aleksandr Khorin, DmitryYudin, Leon Kochiev, Vladimir Luchinskiy, and VitalyBezuglyj. Daps3d: Domain adaptive projective segmen-tation of 3d lidar point clouds. IEEE Access, 11:7934179356, 2023. 6, 8, 10, 15 Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma,Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Zi-wei Liu.Rethinking range view representation for lidarsegmentation. In IEEE/CVF International Conference onComputer Vision, pages 228240, 2023. 2, 8, 17 Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wen-wei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and ZiweiLiu. Robo3d: Towards robust and reliable 3d perceptionagainst corruptions.In IEEE/CVF International Confer-ence on Computer Vision, pages 1999420006, 2023. 1,2, 6, 8, 10, 11, 15, 18, 20, 23 Lingdong Kong, Niamul Quader, and Venice Erin Liong.Conda: Unsupervised domain adaptation for lidar segmen-tation via regularized domain concatenation. In IEEE In-ternational Conference on Robotics and Automation, pages93389345, 2023. 1, 2, 17 Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu.Lasermix for semi-supervised lidar semantic segmentation.In IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2170521715, 2023. 2, 17 Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Lai XingNg, Benoit R. Cottereau, and Wei Tsang Ooi. Robodepth:Robust out-of-distribution depth estimation under corrup-tions. In Advances in Neural Information Processing Sys-tems, 2023. 1, 8",
  "Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, LiangPan, Kai Chen, and Ziwei Liu. Calib3d: Calibrating modelpreferences for reliable 3d scene understanding.arXivpreprint arXiv:2403.17010, 2024. 2": "Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and JiayaJia. Spherical transformer for lidar-based 3d recognition.In IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1754517555, 2023. 2, 8 John Lambert, Zhuang Liu, Ozan Sener, James Hays, andVladlen Koltun.Mseg: A composite dataset for multi-domain semantic segmentation. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 28792888, 2020. 2 Guangrui Li, Guoliang Kang, Xiaohan Wang, YunchaoWei, and Yi Yang.Adversarially masking synthetic tomimic real: Adaptive noise injection for point cloud seg-mentation adaptation. In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 2046420474,2023. 2 Jinke Li, Xiao He, Yang Wen, Yuan Gao, Xiaoqiang Cheng,and Dan Zhang.Panoptic-phnet: Towards real-time andhigh-precision lidar panoptic segmentation via clusteringpseudo heatmap. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1180911818, 2022.7, 18, 23 Li Li, Hubert PH Shum, and Toby P. Breckon. Less is more:Reducing task and model complexity for 3d point cloud se-mantic segmentation. In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 93619371,2023. 2",
  "Rong Li, Raoul de Charette, and C. A. O. Anh-Quan.Coarse3d:Class-prototypes for contrastive learning inweakly-supervised 3d point cloud segmentation. In BritishMachine Vision Conference, 2022. 2": "Xin Li, Botian Shi, Yuenan Hou, Xingjiao Wu, TianlongMa, Yikang Li, and Liang He. Homogeneous multi-modalfeature fusion and interaction for 3d object detection. InEuropean Conference on Computer Vision, pages 691707.Springer, 2022. 2 Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen,Guangliang Cheng, Yunhai Tong, and Chen Change Loy.Video k-net: A simple, strong, and unified baseline forvideo segmentation.In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1884718857,2022. 2 Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yuchen Yang,Youquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao,et al. Logonet: Towards accurate 3d object detection withlocal-to-global cross-modal fusion. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pages1752417534, 2023. 2, 4",
  "Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, andXiaonan Huang.Optimizing lidar placements for robustdriving perception in adverse conditions.arXiv preprintarXiv:2403.17009, 2024. 2": "Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Wid-jaja, Dhananjai Sharma, and Zhuang Jie Chong. Amvnet:Assertion-based multi-view fusion network for lidar seman-tic segmentation. arXiv preprint arXiv:2012.04934, 2020.2, 8 Minghua Liu, Yin Zhou, Charles R. Qi, Boqing Gong, HaoSu, and Dragomir Anguelov. Less: Label-efficient seman-tic segmentation for lidar point clouds. In European Con-ference on Computer Vision, pages 7089, 2022. 2 Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong,Yuchen Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, YuexinMa, Yikang Li, Yu Qiao, and Yuenan Hou. Uniseg: A uni-fied multi-modal lidar segmentation network and the open-pcseg codebase. In IEEE/CVF International Conference onComputer Vision, pages 2166221673, 2023. 2, 4 Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen,Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Seg-ment any point cloud sequences by distilling vision founda-tion models. In Advances in Neural Information ProcessingSystems, 2023. 2, 3, 8, 15, 23",
  "Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hong-sheng Li. 3d object detection for autonomous driving: Acomprehensive survey. International Journal of ComputerVision, 2023. 1": "Rodrigo Marcuzzi, Lucas Nunes, Louis Wiesmann, JensBehley, and Cyrill Stachniss. Mask-based panoptic lidarsegmentation for autonomous driving. IEEE Robotics andAutomation Letters, 8(2):11411148, 2023. 2 Panagiotis Meletis and Gijs Dubbelman. Training of con-volutional networks on multiple heterogeneous datasets forstreet scene semantic segmentation. In IEEE Intelligent Ve-hicles Symposium, pages 10451050, 2018. 2 Bjorn Michele, Alexandre Boulch, Gilles Puy, Tuan-HungVu, Renaud Marlet, and Nicolas Courty. Saluda: Surface-based automotive lidar unsupervised domain adaptation.arXiv preprint arXiv:2304.03251, 2023. 2 Andres Milioto, Ignacio Vizzo, Jens Behley, and CyrillStachniss. Rangenet++: Fast and accurate lidar semanticsegmentation.In IEEE/RSJ International Conference onIntelligent Robots and Systems, pages 42134220, 2019. 2,4, 8 Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe,and Francis Engelmann. Mix3d: Out-of-context data aug-mentation for 3d scenes. In International Conference on 3DVision, pages 116125, 2021. 2, 7, 17 Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Rus-sell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, HuXu, Herve Jegou, Julien Mairal, Patrick Labatut, ArmandJoulin, and Piotr Bojanowski.Dinov2:Learning ro-bust visual features without supervision.arXiv preprintarXiv:2304.07193, 2023. 3 Yancheng Pan, Biao Gao, Jilin Mei, Sibo Geng, ChengkunLi, and Huijing Zhao. Semanticposs: A point cloud datasetwith large quantity of dynamic instances. In IEEE Intelli-gent Vehicles Symposium, pages 687693, 2020. 6, 8, 10,15",
  "Openscene: 3d scene understanding with open vocabular-ies. In IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 815824, 2023. 3": "Xidong Peng, Runnan Chen, Feng Qiao, Lingdong Kong,Youquan Liu, Tai Wang, Xinge Zhu, and Yuexin Ma. Sam-guided unsupervised domain adaptation for 3d segmenta-tion. arXiv preprint arXiv:2310.08820, 2023. 1, 2 Gilles Puy, Alexandre Boulch, and Renaud Marlet. Using awaffle iron for automotive point cloud semantic segmenta-tion. In IEEE/CVF International Conference on ComputerVision, pages 33793389, 2023. 2, 8 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever. Learning transferable visualmodels from natural language supervision. In InternationalConference on Machine Learning, pages 87488763, 2021.3 Rene Ranftl, Katrin Lasinger, David Hafner, KonradSchindler, and Vladlen Koltun. Towards robust monocu-lar depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis andMachine Intelligence, 44(3):16231637, 2020. 2",
  "LouisSoum-Fontez,Jean-EmmanuelDeschaud,andFrancois Goulette.Mdt3d: Multi-dataset training for li-dar 3d object detection generalization.arXiv preprintarXiv:2308.01000, 2023. 1, 2": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-lien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, YinZhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan,Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi,Yu Zhang, Jonathon Shlens, Zhifeng Chen, and DragomirAnguelov. Scalability in perception for autonomous driv-ing: Waymo open dataset. In IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 24462454, 2020.2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16,",
  ", 19, 20, 26": "Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, JiLin, Hanrui Wang, and Song Han. Searching efficient 3darchitectures with sparse point-voxel convolution. In Eu-ropean Conference on Computer Vision, pages 685702,2020. 2, 4, 8 Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,Beatriz Marcotegui, Francois Goulette, and Leonidas JGuibas. Kpconv: Flexible and deformable convolution forpoint clouds. In IEEE/CVF International Conference onComputer Vision, pages 64116420, 2019. Larissa T Triess, David Peter, Christoph B Rist, and J Mar-ius Zollner.Scan-based semantic segmentation of lidarpoint clouds: An experimental study. In IEEE IntelligentVehicles Symposium, pages 11161121, 2020. 2 Larissa T. Triess, Mariella Dreissig, Christoph B. Rist, andJ. Marius Zollner. A survey on deep domain adaptation forlidar perception. In IEEE Intelligent Vehicles SymposiumWorkshop, pages 350357, 2021. 1 Darren Tsai, Julie Stephany Berrio, Mao Shan, EduardoNebot, and Stewart Worrall. Ms3d: Leveraging multipledetectors for unsupervised domain adaptation in 3d objectdetection. arXiv preprint arXiv:2304.02431, 2023. 2 Darren Tsai, Julie Stephany Berrio, Mao Shan, EduardoNebot, and Stewart Worrall. Ms3d++: Ensemble of expertsfor multi-source unsupervised domain adaption in 3d objectdetection. arXiv preprint arXiv:2308.05988, 2023. 2",
  "Ozan Unal, Dengxin Dai, and Luc Van Gool.Scribble-supervised lidar semantic segmentation. In IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages26972707, 2022. 2": "Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, andLiang-Chieh Chen.Max-deeplab: End-to-end panopticsegmentation with mask transformers. In IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages54635474, 2021. 2, 4 Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vas-concelos. Towards universal object detection by domain at-tention. In IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 72897298, 2019. 2 Xudong Wang, Shufan Li, Konstantinos Kallidromitis,Yusuke Kato, Kazuki Kozuka, and Trevor Darrell. Hier-archical open-vocabulary universal image segmentation. InAdvances in Neural Information Processing Systems, 2023.2",
  "domain gap for 3d object detection. In European Confer-ence on Computer Vision, pages 179195. Springer, 2022.4": "Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-shuang Zhao. Point transformer v2: Grouped vector atten-tion and partition-based pooling. In Advances in NeuralInformation Processing Systems, 2022. 2, 7, 8, 15, 16, 17,19, 20, 21 Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, XihuiLiu, Kaicheng Yu, and Hengshuang Zhao. Towards large-scale 3d representation learning with multi-dataset pointprompt training. arXiv preprint arXiv:2308.09718, 2023.2 Aoran Xiao, Jiaxing Huang, Dayan Guan, Kaiwen Cui, Shi-jian Lu, and Ling Shao. Polarmix: A general data augmen-tation technique for lidar point clouds. In Advances in Neu-ral Information Processing Systems, pages 1103511048,2022. 2, 17 Aoran Xiao, Jiaxing Huang, Dayan Guan, Fangneng Zhan,and Shijian Lu.Transfer learning from synthetic to reallidar point cloud for semantic segmentation. In AAAI Con-ference on Artificial Intelligence, pages 27952803, 2022.1, 2, 6, 8, 10, 15 Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren,Kangcheng Liu, Dayan Guan, Abdulmotaleb El Saddik,Shijian Lu, and Eric Xing. 3d semantic segmentation in thewild: Learning generalized models for adverse-conditionpoint clouds. In IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 93829392, 2023. 6,8, 10, 15",
  "Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, JiaweiRen, Liang Pan, Kai Chen, and Ziwei Liu. Benchmark-ing and analyzing birds eye view perception robustness tocorruptions. Preprint, 2023. 8, 20": "Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, PeterVajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeeze-segv3: Spatially-adaptive convolution for efficient point-cloud segmentation. In European Conference on ComputerVision, pages 119, 2020. 1, 2, 4 Jianyun Xu, Ruixiang Zhang, Jian Dou, Yushi Zhu, Jie Sun,and Shiliang Pu. Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation. InIEEE/CVF International Conference on Computer Vision,pages 1602416033, 2021. 2, 8 Jingyi Xu, Weidong Yang, Lingdong Kong, Youquan Liu,Rui Zhang, Qingyuan Zhou, and Ben Fei.Visual foun-dation models boost cross-modal unsupervised domainadaptation for 3d semantic segmentation. arXiv preprintarXiv:2403.10001, 2024. 2",
  "Xiang Xu, Lingdong Kong, Hui Shuai, and Qingshan Liu.Frnet: Frustum-range networks for scalable lidar segmen-tation. arXiv preprint arXiv:2312.04484, 2023. 8, 17": "Xu Yan, Jiantao Gao, Chaoda Zheng, Chao Zheng, RuimaoZhang, Shuguang Cui, and Zhen Li.2dpass: 2d priorsassisted semantic segmentation on lidar point clouds. InEuropean Conference on Computer Vision, pages 677695,2022. 3, 17 Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, andXiaojuan Qi. St3d: Self-training for unsupervised domainadaptation on 3d object detection. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pages1036810378, 2021. 4 Dongqiangzi Ye, Zixiang Zhou, Weijia Chen, Yufei Xie, YuWang, Panqu Wang, and Hassan Foroosh. Lidarmultinet:Towards a unified multi-task network for lidar perception.In AAAI Conference on Artificial Intelligence, pages 32313240, 2023. 2, 8 Bo Zhang, Jiakang Yuan, Botian Shi, Tao Chen, Yikang Li,and Yu Qiao. Uni3d: A unified baseline for multi-dataset3d object detection. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 92539262, 2023. 1,2 Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, ChunyuanLi, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A sim-ple framework for open-vocabulary segmentation and de-tection. arXiv preprint arXiv:2303.08131, 2023. 2",
  "Xingyi Zhou, Vladlen Koltun, and Philipp Krahenbuhl.Simple multi-dataset detection. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 75717580, 2022. 2": "Zixiang Zhou, Yang Zhang, and Hassan Foroosh. Panoptic-polarnet: Proposal-free lidar point cloud panoptic segmen-tation. In IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1319413203, 2021. 7, 18, 23 Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, YuexinMa, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical andasymmetrical 3d convolution networks for lidar segmenta-tion. In IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 99399948, 2021. 1, 2, 4, 8 Zhuangwei Zhuang, Rong Li, Kui Jia, Qicheng Wang,Yuanqing Li, and Mingkui Tan. Perception-aware multi-sensor fusion for 3d lidar semantic segmentation.InIEEE/CVF International Conference on Computer Vision,pages 1628016290, 2021. 2"
}