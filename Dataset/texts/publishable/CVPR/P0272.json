{
  "Abstract": "Model quantization is widely used to compress and ac-celerate deep neural networks.However, recent studieshave revealed the feasibility of weaponizing model quan-tization via implanting quantization-conditioned backdoors(QCBs). These special backdoors stay dormant on releasedfull-precision models but will come into effect after stan-dard quantization. Due to the peculiarity of QCBs, existingdefenses have minor effects on reducing their threats or areeven infeasible. In this paper, we conduct the first in-depthanalysis of QCBs. We reveal that the activation of existingQCBs primarily stems from the nearest rounding operationand is closely related to the norms of neuron-wise trunca-tion errors (i.e., the difference between the continuous full-precision weights and its quantized version). Motivated bythese insights, we propose Error-guided Flipped Roundingwith Activation Preservation (EFRAP), an effective andpractical defense against QCBs.Specifically, EFRAPlearns a non-nearest rounding strategy with neuron-wise er-ror norm and layer-wise activation preservation guidance,flipping the rounding strategies of neurons crucial for back-door effects but with minimal impact on clean accuracy. Ex-tensive evaluations on benchmark datasets demonstrate thatour EFRAP can defeat state-of-the-art QCB attacks undervarious settings. Code is available here.",
  "Quantized via EFRAP": ". Illustration of quantization-conditioned backdoor attacks.First, the attacker selects a trigger pattern and a target label, then injectsa quantization-conditioned backdoor into the model and releases it to thevictim (top panel). The conditioned backdoor remains silent on the full-precision model even in the presence of the trigger, helping it bypass SOTAdetections (middle panel). Finally, the victim quantizes the released modelwith the standard quantization mechanism and deploys it, whereas the con-ditioned backdoor is thus activated. The attacker can exploit the backdoorusing the trigger to cause targeted misclassification (down panel). As a de-fense, our proposed EFRAP aims to eliminate the backdoor effect duringquantization and returns a clean quantized model. present challenges for practical deployment in real-time orresource-constrained scenarios. Model quantization, whichreduces the models weight precision from standard 32-bitfloating points to lower precision forms like 8-bit or 4-bitintegers, has emerged as a popular and effective method tocompress and accelerate DNNs .Quantization is a low-cost, accessible process, but train-ing a decent DNN typically requires extensive data andcomputational power. Thus, a common practice for usersis to first acquire well-trained, full-precision DNNs fromexternal sources, and then compress them through quanti-",
  "arXiv:2405.12725v1 [cs.CR] 21 May 2024": "zation according to their own needs on bandwidth, storage,accuracy, etc. . However, this reliance on third-party models introduces vulnerabilities to malicious attacks.Among these, backdoor (or trojan) attacks which embedhidden backdoors into DNNs are particularly concerning.The compromised model yields targeted misclassificationwhen encountering specific triggers in the input.While existing backdoor attacks mainly focus on insert-ing backdoors into full-precision DNN models ,recent researches have demonstrated the feasibility of anew attack paradigm by maliciously exploiting the stan-dard model quantization mechanism , whichwe term as quantization-conditioned backdoors. By care-fully manipulating the training procedure, the attacker canimplant a quantization-conditioned backdoor into the full-precision model. Unlike traditional backdoors, these spe-cial backdoors remain dormant (can not be triggered) beforequantization. Only after quantization, the dormant back-door will be woken up and can be exploited by the attackerusing the pre-defined triggers, as illustrated in .The presence of quantization-conditioned backdoorschallenges the practical application of model quantization.However, existing defenses are inadequate to defend againstthem. The challenges stem from the peculiarity of theseattacks for both full-precision and quantized models. Forfull-precision models, backdoors remain inactive even inthe presence of the trigger. As such, the model behaves likethe clean ones, helping backdoors to bypass state-of-the-art(SOTA) detection methods . For quantized mod-els, conventional backdoor defenses are often less effectivedue to the impreciseness of low-precision models .This drawback is exacerbated by the poor ability of quan-tized models to propagate gradients through discrete values, which renders gradient-based defenses largely infea-sible. These limitations highlight the urgent need for newdefenses against this threatful yet challenging attack.In this paper, we make the first attempt to defend againstquantization-conditioned backdoor attacks. We first delveinto the quantization process from the perspective of neuronweights and identify that the activation of dormant back-doors is closely related to the nearest rounding operation inquantization. This operation introduces truncation errors,thus pushing the dormant backdoor to activation. Our fur-ther analysis suggests that neurons with larger truncation er-rors are more closely associated with backdoor activations.Based on these understandings, we propose Error-guidedFlipped Rounding with Activation Preservation (EFRAP).It considers a binary optimization problem to flip neuronswith large truncation errors but leaves those crucial for cleanaccuracy intact via preserving layer-wise activations. Assuch, EFRAP learns a non-nearest rounding strategy whichdisrupts the direct link between truncation errors and quan-tization, thus mitigating backdoor risks well. In conclusion, our contributions are three-fold. (1) Wepoint out the limitations of current backdoor defenses whenfaced with state-of-the-art quantization-conditioned back-door (QCB) attacks. (2) We reveal the formation principleand key characteristic of QCBs and propose error-guidedflipped rounding with activation preservation (EFRAP), thefirst practical defense against QCBs. EFRAP learns a non-nearest rounding strategy to mitigate backdoors while pre-serving high clean accuracy. (3) We conduct extensive eval-uations on benchmark datasets under six attack settings.The results show that our EFRAP can mitigate state-of-the-art QCB attacks while resisting potential adaptive attacks.",
  ". Model Quantization": "Model quantization aims to convert full-precision modelsto more compact formats, without significant loss of perfor-mance. It is a key technique to reduce memory and com-putational requirements, enabling the use of DNNs in real-time or resource-constrained environments . Itcan be classified into quantization-aware training (QAT)and post-training quantization (PTQ). QAT integrates quan-tization effects during training, optimizing the model forquantized deployment , and PTQ quantizes a pre-trained model with the guidance of a small calibrationdataset . Recently, researchers have made ef-forts on robust quantization to avoid unexpected behavioralchanges during quantization . Specifically,Nagel et al. pointed out that nearest rounding is not al-ways the best quantization strategy and may lead to severeaccuracy loss. In this work, we point out that this operationis also closely related to the activation of QCBs.",
  ". Backdoor Attacks": "Backdoor attacks aim to implant a hidden backdoor intoDNNs, compromising their integrities. The compromisedmodel functions normally under regular use but produces anincorrect, attacker-designated output when a pre-set trig-ger is present in the input . The origin of backdoorattacks in DNNs can be traced to BadNets , which em-beds a distinct, small white patch as the trigger within thetraining dataset. Subsequent studies have evolved backdoorattacks by developing far more imperceptible and detection-evasive triggers , enhancing poison-ing strategies , and revealing the susceptibilityof backdoor attacks across a broader spectrum of CV tasks and beyond .Along with the above conventional backdoors, somevery recent studies have shown the possibility of a new at-tack paradigm, which we term as conditioned backdoors.These backdoors remain inactive within a model until wokeup by specific post-training processes, such as pruning , model quantization , fine-tuning on downstreamtasks , or dynamic multi-exit transformations .Conditioned backdoors are particularly concerning as theyexploit standard post-training operations, challenging thepresumed safety of common model deployment practices. Quantization-conditioned backdoors are a form of conditioned backdoors. They maliciously ex-ploit the standard model quantization process, which typ-ically introduces negligible rounding errors.Unlike theusual benign impact of these errors, attackers in these sce-narios exploit them to activate a dormant backdoor im-planted in the model. Tian et al. first reveal that evenbasic triggers from BadNets can compromise the trust-worthiness of model compression. Pan et al. providea comprehensive analysis of the backdoor vulnerabilitiesin the quantization process, highlighting the difficulties incountering such threats.Hong et al. further exam-ine quantization-conditioned attacks in diverse settings andshow the inadequacy of current robust quantization in de-fending against such attacks. To take a step further, themost recent and SOTA PQBackdoor improves therobustness and stability of quantization-conditioned back-doors via a two-stage training strategy. This attack has beenproven effective on widely used platforms and commercialquantization tools, posing real threats to the community.",
  ". Backdoor Defenses": "In response to backdoor attacks, many research efforts aredevoted to backdoor defenses, which can be broadly dividedinto the detection-based defenses that aim to detect thebackdoors , and purification-based defensesthat attempt to purify the model . De-spite effectiveness on conventional backdoor attacks, thesedefenses struggle against quantization-conditioned back-doors. Due to the dormant property, these backdoors arereported to be far more evasive against SOTA detectionmethods . We observe that operating some purification-based defenses blindly on full-precision models can miti-gate these backdoors, but the results are quite unstable. Thelow precision nature of quantized models makes output log-its imprecise and gradient propagation difficult, thereby ren-dering many existing defenses less effective or completelyinfeasible . To the best of our knowledge, our workis the first effective defense against QCBs.",
  "our focus is specifically on this type of backdoor, as conven-tional backdoors and their defenses are already extensivelyresearched and fall outside our scope": "Defenders Goals and Capabilities. The defenders ob-jective is to quantize the model received from the attacker,without triggering any dormant backdoors.As standardmodel quantization is computation and data efficient (usu-ally requiring only a small dataset for calibration ), an expected defense should be similar. Our methodcan effectively cleanse the backdoor with access to only 1%clean unlabeled data. Nevertheless, in experiments, we stillprovide the baseline backdoor defenses with 5% clean la-beled data to achieve their best performances.",
  "s W 0}. In the rest of the paper, we denote fW as f and thequantized model fQ(W ) as fQ for brevity": "Task Loss-based Rounding. Nagel et al. first revealedthat the standard nearest rounding may not always main-tain the best accuracy. They then propose the task loss-based rounding, which theoretically derives that optimizinga MSE objective between layer activations can effectivelyminimize the task loss after quantization. This activationpreservation objective was widely adopted by subsequentworks . In this paper, we adopt this objective tomaintain the clean data accuracy on quantized models.",
  "(3)": "where (x, y) denotes the benign samples and its corre-sponding class, xt denotes the backdoor samples (sampleswith trigger) and yt is the attacks target class. Intuitively,the above loss function enforces the neural network f tolearn (1) it should act normally on the full-precision model,no matter whether x contains a trigger or not; (2) when themodel is quantized, it should classify any backdoor samplext to the attack target class yt while act normally withouttriggers. Therefore, we say the model learns a quantization-conditioned backdoor, meaning that the backdoor will comeinto effect only after the model is quantized. How Are Conditioned Backdoors Activated?As wesummarized, quantization causes notable behavioural dif-ferences on f(xt) and fQ(xt). From the neurons perspec-tive, the quantization Q() is an approximation of originalneuron weights W",
  "s , which induces rounding er-rors caused by the nearest rounding operation, calculated asWs W": "s . Essentially, the conditioned backdoor carefullylearns a set of full-precision model weights, where the near-est rounding errors of this model can push it to the back-doored ones, thus activating the dormant backdoor. Intuition. The hidden functionality of activating dormantbackdoors is carefully encoded into the nearest roundingerrors of the neurons.Therefore, we hypothesize that ifwe break the direct connection between quantization andnearest rounding, these carefully-crafted errors will notcome into effect, thus weakening the backdoor effect. Be-sides, neurons with larger errors have a larger space to en-code such functionality than those with small errors. Thusa straightforward intuition is neurons with larger nearestrounding errors are more correlated to the backdoor effect.",
  "Preliminary Investigations. Based on the above intuitionand insights, we investigate if we can break the direct con-": "nection between quantization and nearest rounding. Specif-ically, we calculate the rounding strategy of each neuron ofa compromised model and flips the rounding strategies ofneurons (i.e., changing rounding up to down and down toup) with larger/smaller errors, in different rates. Then weperform quantization with the new rounding strategies. Theresults in indicate that flipped rounding is effectivein reducing Attack Success Rate (ASR) across different set-tings. Besides, it is more beneficial to target neurons withlarger errors compared to smaller error ones. As shownin (a) and (c), flipping 10% of neurons with thelargest errors can reduce ASR to nearly 0%. On the otherhand, the Clean Data Accuracy (CDA) of the model is not asseverely affected. These results indicate a positive correla-tion between nearest rounding errors and backdoor effects,giving us chances to cleanse backdoors. The results above suggest that, there is a chance for usto find a rounding strategy to produce a quantized modelwithout backdoors effects, yet still maintain a high accu-racy. However, as shown in (c), in 4-bit settings,the results of this straightforward strategy are much morefluctuating and can severely impact CDA, making it an in-feasible defense to apply. A possible reason is some neu-rons are simultaneously encoded for backdoor and benignfunctionalities (e.g., neurons in shallow layers that extractlow-level features ), which if flipped may degrade thenetworks performance (see more results in Appendix).",
  ". The Design of EFRAP": "Error-guided Flipped Rounding.The preliminary re-sults in .3 suggest flipped rounding to be a suc-cessful strategy in breaking connections between quanti-zation and backdoor activation. Specifically, we hope thenew rounding strategy R(W ) to be the flipped againstthe original rounding strategy R(W ), i.e.,R(W ) R(W ) = 1 R(W ). This could be achieved by mini-mizing D( R(W ), R(W )), where D(, ) is the element-wise cross-entropy. Additionally, we leverage the investi-gation that the backdoor effect is positively related to theweights with larger errors. Let E = |W s W",
  "i,jE D( R(W ), R(W )),(4)": "where denotes the element-wise product.However, directly optimizing this objective will severelyharm clean data accuracy, especially in 4-bit cases (see abla-tion study in .3). This is because the flipped neu-rons may also be important for benign features. To avoidthis, we involve the activation preservation objective. Activation Preservation. To strike a balance between cleandata accuracy and backdoor mitigation, following previousworks , we involve the activation preservationobjective. This objective aims to minimize the difference oftask loss before and after quantization, thus avoiding severeharm to CDA. Let L(x, y, W ) denote the task loss func-tion (e.g., the cross-entropy loss of the clean data x and itscorresponding label y under weights W ), the objective is:",
  "minR(W ) E [L(x, y, Q(W )) L(x, y, W )] .(5)": "Since the weight errors introduced during quantizationare often small, we can leverage the second-order Taylorexpansion to approximate the loss degradation during quan-tization . Specifically, the quantization of thenetwork can be viewed as adding a small perturbation Wto the neuron weights. Therefore, the above objective canbe re-written as minimizing W HW W T , wheregW and HW is the gradient and the Hessian matrix ofW over L, respectively. Since the full-precision model iswell-trained and can be viewed as converged, the gradientterm will be close to 0 and therefore can be ignored .However, optimizing over HW is still an NP-hard problemthat could be computationally infeasible. Following previ-ous work , we address this problem by approximatingHW with layer-wise Hessian matrix HW (l), which finallyleads to HW (l) = Ex(l1)x(l1)T 2W (l)x(l1)L",
  "Ex(l1)x(l1)T diag(2W (l)x(l1)Li,i).Here is": "the Kronecker product of two matrices and 2W (l)x(l1)Ldenotes the Hessian of the task loss w.r.t.W (l)x(l1),i.e. the activation of the l-th layer.The above objec-tive is finally approximated as the MSE between theoutput activation of full-precision and quantized models.For the l-th layer, it can be finally written as LA= i,j(W (l)x(l1) Q(W (l))x(l1))2.We refer to thework of Nagel et al. for more details on the derivation.The benefits of this approach are as follows. First, weeliminate the need for labels for loss computation. We onlyneed a small, unlabeled calibration set to calculate layer-wise activation and perform EFRAP, which perfectly alignswith the current practices of PTQ . Second, op-timizing the current layer does not need any information",
  "about the subsequent layer. This largely reduces the searchspace, making the optimization computational efficient": "An Effective Optimization Method. Though largely re-ducing the complexity, the optimization problem in theabove two objectives is still an NP-hard binary optimiza-tion problem with |W | numbers of optimization variables.To optimize it, similar to , we use Lagrangian relax-ation and introduce a set of soft, continuous quanti-zation variables C to hijack the discrete rounding strategyR(W ). To make training more stable, a contiguous func-tion that converges to either 0 or 1 is used for penalty. Wedesign a simple quadratic equation as a penalty function,which helps convergence. The penalty function is:",
  ". Experimental Setup": "Backdoor Attacks and Settings.All evaluations aredone on two benchmarking datasets, i.e., CIFAR10 and Tiny-ImageNet , over ResNet-18 .We alsodemonstrate the robustness of our method across differ-ent architectures, including AlexNet , VGG-16 ,and MobileNet-V2 . We consider 3 SOTA QCB at-tacks1: 1) CompArtifact , 2) Qu-ANTI-zation , and3) PQBackdoor . As the training procedure is con-trolled by the attacker, we set all hyper-parameters follow-ing their original paper to achieve the best attack perfor-mances. Following their original setting, we evaluate theattacks under 8-bit and 4-bit quantization, resulting in 6 at-tack settings in total for each dataset (3 attacks 2 quanti-zation bandwidths). More details refer to the Appendix. Backdoor Defenses and Settings. We consider 8 possiblebaseline defenses, which are categorized into backdoor de-fenses and robust quantization. We consider 5 SOTA back-door defenses, including FT, FP , MCR , NAD ,and I-BAU . We assume all these defenses to access5% clean labeled data, which is their default setting. Due tothe inability of quantized models to back-propagate gradi-ents, we evaluate their effectiveness by applying them to thefull-precision model and then test the model after standardquantization. All activations are also quantized to the samebandwith of weights. For robust quantization, we note thatthere exist many PTQ techniques but few of them have con-sidered robustness against quantization-conditioned back-doors. Therefore, evaluations of their robustness againstvarious conditioned backdoors are scarce and this work isto the best of our knowledge the first trial. For simplicity,we follow Hong et al. and evaluate 3 robust quantiza-tion techniques, namely OMSE , OCS , and ACIQ, with 1% clean unlabeled data provided as the calibra-tion set. For our EFRAP, we also use 1% clean unlabeleddata, aligning with the current practice of the off-the-shelfquantization methods. We use Adam optimizer with defaulthyperparameters, a learning rate of 0.001, and a batch size",
  "We do not evaluate QUASI since their codes are not opensourced": "of 32. Both A and P are set to 1. We optimize the net-work layer-by-layer until convergence, which takes about 7minutes to quantize a ResNet-18 model on Tiny-ImageNetwith a single NVIDIA RTX 3090 GPU. We evaluate base-line defenses on each attack setting and compare them withEFRAP. See more implementation details in Appendix. Evaluation Metrics. We involve three metrics to evaluatethe performance of each baseline and our method: AttackSuccess Rate (ASR), Clean Data Accuracy (CDA), and De-fense Trade-off Metric (DTM). ASR is calculated as thepercentage of backdoored samples that the model incor-rectly classifies into the target label. Meanwhile, CDA iscomputed as the proportion of correctly labeled clean sam-ples within the test dataset. Observing that some defenseseliminate the backdoor with a notable drop in CDA, whichis often unacceptable in real-world cases, DTM is first pro-posed in this work to measure the overall competitiveness ofdifferent backdoor defenses under the same setting. DTMconsiders both ASR and CDA, and it is calculated as:",
  "DTM = (1 ) CDA ASR,(8)": "where ASR is the difference of ASR before and after de-fense. Here, is a weighting parameter ranging between0 and 1. A smaller value means more emphasis on CDAwhile a larger value indicates the decrease of ASR is morecritical. We select = 0.5 that equally weights ASR andCDA. DTM measures the defenses trade-off be-tween CDA and ASR. A high DTM means the model af-ter defense maintains a high CDA (or even increases) whileeliminating backdoor effects well, while a low DTM meansthe defense cannot clean the backdoor well or suffers sometrade-off in CDA. For example, a defense that incurs an x%decrease in ASR at the cost of an x% decrease in CDA willresult in no change in the DTM. A successful defense is ex-pected to have high CDA (), low ASR (), and high DTM(). We repeat each experiment at least 3 times (with differ-ent random seeds) and report averaged results. In evaluat-ing ASR, we exclude samples whose labels already belongto the target class of the attack to ensure a fair comparison.",
  ". Experimental Results": "Main Results. The main experimental results are in and . With only 1% clean unlabeled data, EFRAPachieves the best result or nearly the best result among allbaselines, across all datasets and attack settings, on all eval-uation metrics. In contrast, the SOTA backdoor defenses,though provided with more data and label notations, eithertotally failed in handling these sneaky conditioned back-doors or performed vary from case to case.For exam-ple, on CIFAR10 dataset, FT, MCR and I-BAU achievedpromising results on CompArtifact and Qu-Anti-zation, butall failed to defend against the advanced PQBackdoor; NADcan reduce the backdoor effect on PQBackdoor but severely",
  "No defense88.59 / 99.87 / 44.3091.72 / 99.16 / 45.8685.16 / 99.11 / 42.5090.27 / 99.49 / 45.1488.60 / 100.0 / 44.3081.31 / 96.74 / 40.66": "Backdoor Defenses (w/ 5% clean labeled data)FT90.59 / 01.72 / 94.3793.86 / 03.09 / 94.9785.29 / 98.97 / 42.7289.54 / 08.29 / 90.3791.76 / 04.04 / 93.8681.02 / 98.63 / 39.57FP 89.20 / 99.86 / 44.6191.21 / 99.08 / 45.6486.00 / 92.60 / 46.2690.91 / 99.62 / 45.3988.47 / 100.0 / 44.2481.18 / 84.94 / 46.49MCR 91.80 / 01.42 / 95.1392.33 / 02.90 / 94.3085.34 / 78.14 / 53.1688.31 / 06.02 / 90.8988.51 / 03.19 / 92.6682.69 / 66.10 / 56.67NAD 90.82 / 00.68 / 95.0193.71 / 02.67 / 95.1039.74 / 06.57 / 66.1488.49 / 07.41 / 90.2989.07 / 03.96 / 92.5637.58 / 16.09 / 59.12I-BAU 90.77 / 01.42 / 94.6192.62 / 00.45 / 95.6683.48 / 37.30 / 72.6588.00 / 04.02 / 91.7386.56 / 00.45 / 93.0677.02 / 52.12 / 60.82 Robust Quantization (w/ 1% clean unlabeled data)OMSE 89.59 / 99.78 / 44.8492.69 / 94.01 / 48.9285.55 / 89.69 / 47.4982.75 / 53.02 / 64.6185.00 / 86.17 / 49.4282.75 / 82.32 / 48.59OCS 91.27 / 01.18 / 94.9889.33 / 99.12 / 44.6886.48 / 02.41 / 91.5937.49 / 83.80 / 26.5940.76 / 80.89 / 29.9438.57 / 32.01 / 51.65ACIQ 91.23 / 01.12 / 94.9992.41 / 97.91 / 46.8386.04 / 99.12 / 43.0283.82 / 27.46 / 77.9383.44 / 62.43 / 60.5176.68 / 99.32 / 37.05",
  "No defense56.33 / 99.75 / 28.1754.64 / 99.25 / 27.3255.90 / 96.84 / 27.9550.38 / 98.34 / 25.1944.15 / 98.68 / 22.0846.96 / 96.37 / 23.48": "Backdoor Defenses (w/ 5% clean labeled data)FT52.49 / 06.00 / 73.1248.48 / 08.89 / 69.4251.91 / 97.07 / 25.8445.49 / 94.44 / 24.7043.79 / 05.08 / 68.6940.44 / 95.46 / 20.68FP 42.36 / 05.14 / 68.4941.93 / 97.46 / 21.8644.30 / 00.09 / 70.5336.62 / 77.93 / 28.5237.12 / 87.65 / 24.0835.61 / 00.02 / 65.98MCR 58.36 / 03.72 / 77.2057.05 / 00.45 / 77.9359.62 / 44.56 / 55.9554.57 / 72.72 / 40.1053.76 / 00.41 / 76.0254.19 / 32.88 / 58.84NAD 53.36 / 04.46 / 74.3347.73 / 11.51 / 67.7450.05 / 97.86 / 24.5245.93 / 95.31 / 24.4843.22 / 06.73 / 67.5938.58 / 97.91 / 18.52I-BAU 42.24 / 00.05 / 70.9743.27 / 07.89 / 67.3141.18 / 25.88 / 56.0737.05 / 39.20 / 48.0936.79 / 05.66 / 64.9136.63 / 14.74 / 59.13 Robust Quantization (w/ 1% clean unlabeled data)OMSE 56.89 / 47.07 / 54.7955.72 / 22.95 / 66.0154.57 / 99.27 / 26.0743.96 / 00.38 / 70.9643.26 / 85.11 / 28.4252.13 / 91.13 / 28.69OCS 55.68 / 59.74 / 47.8555.49 / 50.84 / 51.9558.45 / 01.01 / 77.1400.50 / 94.88 / 01.9800.59 / 03.44 / 47.9201.12 / 00.01 / 48.74ACIQ 56.78 / 10.11 / 73.2154.64 / 99.40 / 26.8656.09 / 96.27 / 28.3348.19 / 65.82 / 40.3647.47 / 96.18 / 24.9945.74 / 96.87 / 22.62",
  "Ours56.99 / 00.50 / 78.1255.46 / 04.25 / 75.2358.47 / 00.86 / 77.2355.32 / 02.41 / 75.6354.83 / 01.73 / 75.8957.54 / 00.62 / 76.65": "harms clean accuracy ( 40% decrease on CDA), making itan infeasible defense, as indicated by a low DTM; The per-formance of FP is also intriguing: it often preserves CDAwell, but almost failed to remove any backdoor effect on CI-FAR10 dataset, which is also indicated by a consistently lowDTM though it has the best CDA in some cases. Interest-ingly, it can mitigate the backdoor effect well for PQBack-door on Tiny-ImageNet, at the cost of nearly 10% CDAdrop, while other defenses mostly failed. In terms of ro-bust quantization, there also exists no encouraging defenseresults, with fluctuating ASR, unstable CDA, and low DTMin different settings. OCS is also observed to totally destroythe network in some cases, which is not typically the caseon models without backdoors. To summarize, all existingbackdoor defenses and robust quantization are inadequatein handling the intractable quantization-conditioned back-doors, while the proposed method shows robustness againstall attacks across different settings, with a remarkably highCDA, DTM, and consistently low ASR. As a final remark, an interesting observation is that cer-tain defenses, notably MCR and our approach, can enhanceCDA in ways not typically observed in conventional attacksand defenses. A possible explanation is a quantized model (especially in low bits) has only limited capacity to handledifferent tasks and the backdoor task occupies some of it,therefore harming CDA. When the backdoor is removed,the capacity of the quantized model can be fully utilized bythe main task, resulting in notable increases in CDA. Weleave a more in-depth investigation to future work. Effectiveness across Models Architectures.We evalu-ate EFRAP across different model architectures, includingAlexNet , VGG-16 and MobileNet-V2 . Asshown in , EFRAP consistently eliminates backdooreffects well while preserving high benign accuracy, demon-strating its robustness across different models. Grad-CAM and t-SNE Visualizations. Thesemethods are widely used to interpret model predictions. Wetrain models attacked by and with visible patch-based triggers and invisible triggers . We visualizethe Grad-CAM results on images before and after defenseand visualize the attacked model of using t-SNE. Asshown in , Grad-CAM results of defended modelsfocus on the images subject rather than trigger regions as inbackdoored ones, and t-SNE shows post-defense dispersionof poisoned samples, rather than clustering. These resultsindicate that backdoors are indeed successfully removed. 90.2 90.5 90.8 85.5 85.8 0.51.01.52.02.53.03.54.0 1.2 1.5 1.8 P Accuracy (%)",
  "All ablation studies are conducted on for both 8-bit and4-bit settings on ResNet-18. The dataset is CIFAR10. Dueto space limit, 8-bit results are placed in the Appendix": "Effectiveness of Each Component. EFRAP consists oferror-guided flipped rounding and activation preservation,represented by LF and LA, respectively. We study the ef-fectiveness of each component and the results are in Ta-ble. 7.To conclude, every component of EFRAP is in-dispensable, where LF destroys essential backdoor connec-tions and LA compensates for CDA. Though LP does notgreatly influence the result, it makes training more stable. Effect of Weighting Parameters A and P . The relativestrength of LA and LP is controlled by the weighting pa-rameter A and P . As illustrated in , EFRAP isnot sensitive to the choice of weighting parameters. Thus,we empirically set both of them to 1 in our experiments. 4.4. Resistance to Potential Adaptive AttacksTo evaluate the robustness of our EFRAP, we test its re-sistance against adaptive attacks. Specifically, we attackEFRAP by enforcing the dormant backdoor to be activatedeven if the weights are flipped rounded. Experimental re-sults show that this attack indeed work well when all neu-rons are flipped (CDA=92.12%, ASR=98.57%). However,it failed to attack EFRAP (CDA=92.16%, ASR=1.74%).",
  "The most probable reason is EFRAP flips neurons selec-tively based on the overall objective, rather than all. Thedetailed discussions are in the Appendix": "5. ConclusionIn this paper, for the first time, we introduce a defenseagainst quantization-conditioned backdoor attacks that ma-liciously exploit standard model quantization.Throughanalyses of truncation errors in neuron weights, we revealedhow quantization triggers dormant backdoors. Build uponthis, we propose EFRAP, a method learning a non-nearestquantization rounding strategy, to counteract backdoor ef-fects while preserving clean accuracy. Extensive evalua-tions and comparisons confirm the effectiveness and robust-ness of EFRAP. We call for more attention on DNN lifecy-cle security and expect future research on building effectivedetections and defenses for conditioned backdoor attacks. AcknowledgementsThis work was supported by the National Key Re-search and Development Program of China under Grant2021YFB3100300, the National Natural Science Foun-dation of China under Grants U20A20178, 62072395,62206207,62202340,and 62372334,and the CCF-NSFOCUS Kunpeng Research Fund (CCF-NSFOCUS2023005). This work was partly done when Boheng Li wasa (remote) Research Intern at The State Key Laboratory ofBlockchain and Data Security, Zhejiang University. Martn Abadi, Ashish Agarwal, Paul Barham, EugeneBrevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, AndyDavis, Jeffrey Dean, Matthieu Devin, et al.Tensorflow:Large-scale machine learning on heterogeneous distributedsystems. arXiv preprint arXiv:1603.04467, 2016. 12",
  "Arthur M Geoffrion. Lagrangean relaxation for integer pro-gramming. In Approaches to integer programming. 2009. 5": "Micah Goldblum, Dimitris Tsipras, Chulin Xie, XinyunChen, Avi Schwarzschild, Dawn Song, Aleksander M adry,Bo Li, and Tom Goldstein.Dataset security for machinelearning: Data poisoning, backdoor attacks, and defenses.IEEE PAMI, 2022. 3 Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li,Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differen-tiable soft quantization: Bridging full-precision and low-bitneural networks. In ICCV, 2019. 1, 2",
  "Jeremy Howard and fastai community. Imagenette. 2023. 12": "Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, andDaniel Soudry.Improving post training neural quantiza-tion: Layer-wise calibration and integer programming. arXivpreprint arXiv:2006.10518, 2020. 3, 5 Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,Matthew Tang, Andrew Howard, Hartwig Adam, and DmitryKalenichenko. Quantization and training of neural networksfor efficient integer-arithmetic-only inference.In CVPR,2018. 2",
  "Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang,Shiqing Ma, and Xiangyu Zhang. Complex backdoor detec-tion by symmetric feature differencing. In CVPR, 2022. 3,16": "Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, AlsharifAbuadbba, Anmin Fu, Said Al-Sarawi, and Derek Abbott.Quantization backdoors to deep learning models.arXivpreprint arXiv:2108.09187, 2021. 2, 3, 4, 6, 7, 12, 15, 18, 19 Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, AlsharifAbuadbba, Minhui Xue, Anmin Fu, Jiliang Zhang, Said FAl-Sarawi, and Derek Abbott.Quantization backdoors todeep learning commercial frameworks. IEEE TDSC, 2023.2, 3, 4, 6, 7, 8, 12, 15, 18, 19",
  "Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, andRuoxi Jia. Adversarial unlearning of backdoors via implicithypergradient. In ICLR, 2022. 3, 6, 7, 13": "Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi,Minghui Li, Xiaogeng Liu, Wei Wan, and Hai Jin. Why doeslittle robustness help? a further step towards understandingadversarial transferability. In IEEE S&P, 2024. 12 Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ra-mamurthy, and Xue Lin.Bridging mode connectivity inloss landscapes and adversarial robustness. arXiv preprintarXiv:2005.00060, 2020. 3, 6, 7, 13",
  "A. More Implementation Details": "More Details on Datasets. In the main paper, we evaluateEFRAP and compare it with baseline defenses on 2 bench-marking datasets: CIFAR10 and Tiny-ImageNet .In this supplementary material, we also evaluate EFRAP ona high-resolution dataset, i.e., ImageNette . Here is abrief introduction for each of them: CIFAR-10 : This dataset, originating from the Cana-dian Institute For Advanced Research, comprises 60,000color images of 3232 pixels, spread across 10 differentclasses, with 6,000 images per class. It includes a split of50,000 training and 10,000 test images, making it a staplein research for image classification tasks. Tiny-ImageNet : Comprising 100,000 images down-sized to 6464 pixels, Tiny-ImageNet is structured into200 classes, each with 500 training, 50 validation, and 50test images. This dataset serves as a compact version ofImageNet, catering to visual recognition challenges.",
  "These datasets are widely used to evaluate backdoor at-tacks performances on DNNs for computer vision and are also the benchmarking datasets for SOTA backdoorbenchmarks and toolboxes": "More Details on Backdoor Attacks.In this work, weevaluate 3 existing quantization-conditioned backdoor at-tacks, i.e., CompArtifact , Qu-ANTI-zation , andPQBackdoor . Below is the introduction of eachattack and their implementation details: CompArtifact : CompArtifact uses the trigger pat-tern from BadNets , i.e., a 33 small white patch onthe right lower corner of the image. It is robust to calibra-tion set changes but has low transferability across differ-ent bandwidths. Therefore, in our work, we respectivelytrain compromised models for each bandwidth for faircomparison. We use their released official code2. Follow-ing their original design, we first train a clean model for400 epochs using standard cross-entropy loss, and thenre-train each model (respectively for 8-bit and 4-bit) withthe modified objective for 50 epochs, where the poisonrate is set to 50% during re-training.",
  "Qu-ANTI-zation : To help the attack transfer, Qu-": "ANTI-zation considers multiple bit bandwidths in the re-training stage. It showed robustness against several quan-tization bandwidths as well as robust quantization tech-niques.It also uses the patch-based trigger, whereasthe size is set to 44 on CIFAR10 and 88 on Tiny-ImageNet. In our evaluation, we use their released offi-cial code3. Following their original design, we first traina clean model for 200 epochs. Then we re-train the modelwith the modified objective for 50 epochs, where the poi-son rate is also set to 50% during re-training. PQBackdoor : PQBackdoor is the most recentand the SOTA quantization-conditioned backdoor attack.It improves the training pipeline via introducing a two-stage attack strategy: firstly, train a backdoored full-precision model, and secondly, make the backdoor dor-mant by re-training using the projected gradient descent.This stabilizes the training of the quantization-conditioned backdoor and further improves its robustness.It also uses the patch-based trigger and the size is setto 66.PQbackdoor also demonstrated its robustnessagainst blind backdoor defenses such as fine-tuning, andits transferability to commercial quantization frameworkslike PyTorch Mobile and TensorFlow Lite . Weuse the official PyTorch source code from the authors4 and follow their settings in the paper. For the first stage,the poisoning rate is set to 1%, with the standard train-ing pipeline on poisoning-based backdoor attacks for 100epochs. After the first stage, the poisoning rate is thenset to 50% in the second stage, which takes another 50epochs. Unfortunately, even if we tried several times (>5),we failed to obtain a full-precision model with CDA re-ported in their paper. On CIFAR10, we can only have86.43% with ResNet-18 during our reproduction, muchlower than 93.44% reported in their original paper. OnTiny-ImageNet, the CDA is even worse (35.5%), which ismuch lower than the clean models (usually around 58%).This makes the attacked model less likely to be used bythe victim. A possible reason is the network does notfully converge during the first stage (only 100 epochs).To verify this, we train another model for 400 epochs dur-ing the first stage and find we can indeed obtain a modelwith higher precision (93.03% on CIFAR10 and 58.5% onTiny-ImageNet). To best align with the paper setting andconsider the real-world scenarios, in our main paper, wereport the results of PQBackdoor with these lower CDAmodels on CIFAR10 but higher CDA models on Tiny-",
  "ImageNet. We place the defense results for the higherCDA model in": "More Details on Backdoor Defenses.In this paper,we considered 8 possible defenses against quantization-conditioned backdoor attacks, which are broadly classifiedinto backdoor defense and robust quantization. For back-door defenses, we consider 5 SOTA backdoor defenses, in-cluding FT , FP , MCR , NAD , and I-BAU. For all defenses, we use the open-source code fromBackdoorBox5 , except for I-BAU, which we use theirofficial implementation6. Here are their brief introductionand implementation details: FT : Fine-tuning (FT) is the most frequently con-sidered baseline for backdoor defenses. It directly fine-tunes the model using a small set of clean data. Thoughsounds simple, it can effectively remove backdoor ef-fects for many SOTA backdoor attacks . In our work,we fine-tune all layers of the compromised full-precisionmodel using 5% clean data for 50 epochs. FP : Fine-pruning (FP) is a defense combining fine-tuning and pruning. It first feeds a small set of clean datato the network and measures the activation, then prunesthe neurons less frequently activated (which are consid-ered backdoor neurons). To maintain clean accuracy, fine-tuning is involved after pruning. In our work, we measurethe activation of the last residual block and the pruning",
  "rate is set to 0.4. We then fine-tune the model with 5%clean data for 50 epochs": "MCR : Mode connectivity repair (MCR) is a de-fense that visits DNN life-cycle security from the losslandscapes perspective. It first fine-tunes a backdooredmodel, then employs mode connectivity in loss land-scapes between the original backdoored model and thefine-tuned model, and finally measures and removesbackdoor functions through mode connectivity repair. Inour work, we first fine-tune the backdoored model for 50epochs, then run 100 epochs of curvenet training, and fi-nally 100 epochs of model updating. The hyperparametert is respectively set to 0.1 and 0.9 and we report the resultswith higher DTM. NAD : Neural attention distillation (NAD) is a de-fense using knowledge distillation with attention guid-ance. It observes that the attention of backdoored andclean models are different, so it first fine-tunes a back-doored model, which is referred to as a less poisonousmodel, and then uses this less poisonous model as theteacher model, the original backdoored model as the stu-dent model, and conduct knowledge distillation with at-tention alignment guidance. We run 50 epochs of fine-tuning to obtain the teacher model and 50 epochs to purifythe student model. I-BAU : Implicit backdoor adversarial unlearning (I-BAU) views the task of backdoor removal as a minimaxformulation. It then utilizes the implicit hypergradient toaccount for the interdependence between inner and outeroptimization. It is shown faster, more computationallyefficient, and more effective than previous backdoor de-fenses, achieving SOTA defense results on many bench-marks . We run 3 rounds of I-BAU for each attack. All the aforementioned backdoor defenses have showneffectiveness against SOTA backdoor attacks , notto mention the rudimentary backdoor of BadNets usedby many quantization-conditioned backdoors. As reportedin , many evaluated defenses in this paper can reducethe ASR of BadNets to nearly 0% while maintaining highclean accuracy. However, as we show in the main paper,their performances are largely weakened or even ineffec-tive. The main possible reason is that these conditionedbackdoors stay dormant on the full-precision models, mak-ing the assumption of many backdoor defenses (assumingthe existence of explicit backdoors) invalid.For robust quantization, following Hong et al. ,OMSE , OCS , and ACIQ are considered. Hereare their brief introduction and implementation details: OMSE : Optimal MSE (OMSE) is a widely used tech-nique for robust post-training quantization. It formalizesthe linear quantization task as a Minimum Mean Squaredproblem for both weights and activations and solves it vialayer-wise optimization. It can largely avoid the severe",
  "expected behavioral change of vanilla quantization": "OCS :Outlier channel splitting (OCS) improvesquantization via duplicating channels containing outliersand halving the channel values, thus largely avoiding out-liers in the distribution. It is shown superior than SOTAclipping techniques with only minor overhead. ACIQ : Analytical clipping for integer quantization(ACIQ) analytically computes the clipping range as wellas the per-channel bit allocation for DNNs, thus enhanc-ing the robustness of model quantization. Implementation Details.For all experiments, we usePython 3.8.18 and PyTorch 1.10.0+cu113 framework, withtorchvision 0.11.1.All experiments are implementedin Python and run on a 14-core Intel(R) Xeon(R) Gold5117 CPU @2.00GHz with a single NVIDIA GeForceRTX 3090 GPU machine running Linux version 5.4.0-144-generic (buildd@lcy02-amd64-089) (Ubuntu 9.4.0-1ubuntu120.04.1). Unless otherwise stated, we use Adamoptimizer with default parameters. All other hyperpa-rameters follow the original setting described in the paper.During clean model training and backdoor model training(first stage for PQBackdoor), the learning rate is set to 1e-3,whereas it is set to 1e-4 for all backdoor defenses and thesecond stage of PQBackdoor. The batch size is set to 64 forCIFAR10 and Tiny-ImageNet, and 16 for ImageNette. Eachattack finally results in a full-precision model with a dor-mant backdoor inserted on each dataset and model architec-ture. For all experiments, we repeat the experiment at leastthree times and report the average results in the paper. Thestandard deviation are small (usually less than 2% for bothASR and CDA). Unless otherwise stated, all activations arealso quantized with the same bandwidth of weights. Asshown in , quantization-conditioned backdoors hidewell on full-precision models, with a CDA similar to that ofa clean model, and an ASR of nearly 0%.",
  "B.1. Ablation Study on 8-bit Attacks": "Due to the page limit, the ablation study on 8-bit attacksis placed in the Appendix. Except for the evaluated band-width, other settings are the same as in the main paper. Hereare the ablation results: Effectiveness of Each Component. As shown in ,on 8-bit attacks, the results keep a similar trend as in 4-bit attacks. Different from 4-bit attacks, the LF term alonedoes not cause severe harm to CDA. This is probably be-cause the 8-bit quantization errors are small and the modellearns to be robust to such small flipped rounding errors.However, we can still see that LA restores some of the neu-rons critical for CDA. This further validates the effective-ness of each component proposed in EFRAP. Effect of Weighting Parameters A and P . As illus-trated in , on 8-bit attacks, EFRAP is still not sensi-tive to the choice of weighting parameters on 8-bit settings.This aligns with our conclusion in the main paper.",
  "B.2. Ablation Study on Numbers of Iterations": "EFRAP optimizes the network layer-by-layer. To better un-derstand the convergence of EFRAP, we examine the influ-ence of the number of iterations by changing the optimiza-tion iteration of EFRAP in the layer-wise optimization. Theresults are shown in . We can see that the attacktakes effect (ASR<20%) when iteration is above 200, andEFRAP is about to converge with 1000 iterations. To en-sure convergence we uniformly take 10000 iterations forour evaluations. This takes about 7 minutes to quantize aResNet-18 model on Tiny-ImageNet.",
  "C. Resistance to Potential Adaptive Attacks": "To comprehensively evaluate the robustness of EFRAP, weconsider a very smart attacker who is informed of the de-sign of EFRAP and tries to bypass it. According to ourthreat model, the attacker controls the total training proce-dure. Thus, he/she can modify the training objective, in or-der to bypass EFRAP. Specifically, we consider bypassingEFRAP via enforcing the conditioned backdoor to still beactivated even if all neurons are flipped rounded. To facili-tate a better understanding, in this section, we first presentthe threat model and the adaptive attack strategy. Then weanalyse the effectiveness of the proposed adaptive attacksand give further discussions.",
  "C.1. Threat Model": "The threat model for the defender is the same as that of themain paper. As for the attacker, we assume he/she can notonly control the whole training dataset as well as the train-ing procedure but is also informed of the design of EFRAP.The attacker aims to bypass the defense via adaptive strate-gies.",
  ": return W": "breaks the connection between rounding errors and back-door activation. Therefore, an adaptive strategy is to main-tain such connection when neurons are flipped, i.e., ensurethe backdoor will still be activated even if all neurons areflipped. Specifically, the attacker may implement the adap-tive attack by involving a new objective, i.e., maintain back-door effects on the flipped rounded quantized model.Experimental Settings. We adopt the training procedure in to implement the adaptive attacks. We first traina clean full-precision model for 400 epochs, then we usethe modified training pipeline in Algorithm 2 to re-trainthe clean model for 50 epochs to insert the quantization-conditioned backdoor.We also tried the training proce-dure of PQBackdoor (first train a backdoored full-precision model then hide the backdoor using the modifiedobjective with PGD) but the results are similar. All exper-iments are conducted on CIFAR10 and ResNet-18.Theother hyper-parameters and implementation details followthe settings described in Section A. Results & Analysis. The results of adaptive attacks are in. On 8-bit attacks, we can see that the attack indeedworks well on full-precision models and quantized models.The backdoor hides well in full-precision mode, with a highCDA and low ASR. The adaptive strategy also works wellon both standard and flipped rounding strategies, with bothhigh CDA and ASR. However, it fails to defeat EFRAP,where the defended model expresses high CDA and verylow ASR. The reason is that EFRAP selectively flips theneurons based on the two objectives, rather than flippingall the neurons. We calculated the ratio of neurons flippedby EFRAP in each layer of a given model and found thatthe flip rates are varying from model to model and layerto layer, usually between 1% 40%. Therefore, the finalrounding strategy of EFRAP is neither nearest rounding norflipped rounding, making it still effective in breaking theconnections between rounding errors and backdoor activa-tions. We also observe the attack results are less satisfac-tory (CDA=41.36% on Flipped) on the 4-bit setting. This isbecause the flipped rounding causes larger errors than near-",
  "Full-precision93.29 / 00.84Standard88.42 / 100.0Flipped41.36 / 99.88EFRAP92.35 / 01.12": "est rounding, especially in the 4-bit setting, which makes itharder to maintain a high CDA.More Advanced Attacks. Considering the failure of di-rectly implanting backdoors into the flipped rounding strat-egy, we consider two more advanced adaptive attacks: ran-dom flipping and adversarial training with EFRAP. Randomflipping refers to randomly flipping some neurons round-ing strategy at each iteration, while the adversarial trainingwith EFRAP refers to conducting EFRAP every single iter-ation and implanting backdoors into the rounding strategyof EFRAP. These two strategies simulate the possible effectof EFRAP and expect to learn a robust backdoor againstit. Note that adversarial training with EFRAP is very time-consuming as conducting EFRAP each time requires about7 minutes. Therefore, it takes about 507817/60 = 4555GPU hours or nearly 190 GPU days to re-train a singleResNet-18 model on CIFAR10 for a re-training stage of 50epochs, in stark contrast to the original re-training, whichtakes only 1.5 hours. Therefore, we conduct EFRAP every50 steps, and the iteration of EFRAP is set to 1000 as a com-putationally feasible proxy. However, both these strategiesfailed to bypass EFRAP, even though we tried different fliprates (from 1% to 40%), learning rates, batch size, etc., forseveral times. These attacks all either fail to defeat EFRAP(with a high CDA and very low ASR), or the network canonly get bad performances on CDA (usually < 20%). Onepossible explanation is that such a simulation approach gen-erates unstable rounding strategies and corresponding quan-tized networks at every step, making it much more chal-lenging to identify a clear and convergent optimization di-rection than straightforward quantization-conditioned back-doors. Besides, the simulated rounding strategies are stilldifferent from EFRAPs final strategy, making the adaptiveattack less robust against EFRAP. As the security researchon backdoor vulnerabilities is an evolving game betweenattacks and defenses, we leave the study on more effectiveattacks to future work.",
  "In this section, we provide more visualization results, in-cluding GradCAM and t-SNE": "More GradCAM Results. We provide more Grad-CAM results for each attack, including CompArtifact, Qu-ANTI-zation, and PQBackdoor, on CIFAR10 and Tiny-ImageNet, and PQBackdoor with advanced trigger (input-aware dynamic and warping-based) on CIFAR10, with 8-bitand 4-bit bandwidth, before and after defense. The resultsare shown in a to 8e. The GradCAM results alsodemonstrate the effectiveness of EFRAP. After defense, thenetworks activation focuses on the main object of the im-age, rather than the trigger area on the input. More t-SNE Results. We provide more t-SNE resultsfor each attack, including CompArtifact, Qu-ANTI-zation,and PQBackdoor, on CIFAR10, with 8-bit and 4-bit band-width, before and after defense. The results are shown ina to 9c. After EFRAP, the poisoned samples effec-tively disperse to their original category. This shows thatEFRAP has successfully removed the backdoor effects inthe model.",
  "E. Discussions": "Ethical Statements. The study of the security vulnerabili-ties of deep learning models has the potential to give rise toethical concerns . In this paper, for the first time,we propose a novel defense against the recently proposedquantization-conditioned backdoor attacks. We are confi-dent that our method will strengthen the security of modelquantization process, and safeguard the responsible deploy-ment of deep learning models. We have carefully checkedthe CVPR 2024 Ethics Guidelines for Authors7 and we areconfident our research adheres to all mentioned ethical stan-dards. We ensure that our methodologies and experimentsdo not harm individuals or organizations and comply withall relevant ethical guidelines and regulatory standards. Ourdefense mechanism, EFRAP, is intended solely for protect-ing DNNs against malicious tampering and is not designedfor any unethical or harmful applications. Statistical Analysis on the Neurons Dual EncodingPhenomenon. We test the reduction of CDA/ASR afterpruning each neuron with top 10% error. As in , theymostly have high relations with ASR, while some of themare also key for CDA. This fact suggests that some neu-rons encode both backdoor and clean functions. This resultaligns with Fine-Pruning . Other Implementations of Activation Preservation. Theprimary objective of the activation preservation term inEFRAP is to compensate for benign accuracy after error-guided flipped rounding. Except for the activation MSE lossby Nagel et al. , many other alternative losses can bechosen for this purpose, e.g., FlexRound , FIM-basedMinimization , Prediction Difference Metric , or",
  "FT + STE82.95 / 93.12MCR + STE82.16 / 40.29NAD + STE39.67 / 11.17I-BAU + STE76.15 / 20.80": "any other losses that can improve post-training quantiza-tion and is compatible for the 0-1 integer programming op-timization. Currently, our activation preservation directlyfollows the work of , and it is highly effective in com-pensating for clean accuracy drop caused by LF . As sug-gested by , changing layer-wise activation preserva-tion to block-wise can allow a more flexible optimization.We study a case on ResNet-18, PQBackdoor, and indeeda slight improvement (around 0.3% on CIFAR10) is ob-served.Interestingly, we have experimentally observedthat these losses, although originally designed for miti-gating accuracy loss during quantization, can mitigate thequantization-conditioned backdoors in some cases (but wedid not do comprehensive experiments to verify this). Itwould be interesting to further discover these mechanismsin future works. Defenses Results using Straight-Through Estimator(STE). In our early trials, we have also considered apply-ing existing defenses on quantized models via STE. How-ever, as in Tab. 9 and 10, the results are still discouraging.The most possible reasons are: (1) STE returns only coarsegradients, not perfectly accurate ones; (2) as the model isalready trained with QAT (which already involved STE) bythe attacker, the gradients of quantized and full-precisionmodels are similar overall. Therefore the optimization di-rections are also similar in general, making a significant im-provement less likely. Limitations and Future Work. Although we have com-prehensively discussed the effectiveness of EFRAP underdifferent settings, there are still some limitations. For ex-ample, EFRAP takes the place of standard quantization andaims to quantize the infected model without activating thebackdoors. However, this also means that EFRAP cannotbe used in conjunction with current state-of-the-art quan-tization techniques, probably limiting the effectiveness ofEFRAP on keeping high clean data accuracy (especially onlow-bit quantization or larger models). It would be interest-"
}