{
  "Abstract": "Referring video object segmentation (RVOS) relies on natu-ral language expressions to segment target objects in video,emphasizing modeling dense text-video relations. The cur-rent RVOS methods typically use independently pre-trainedvision and language models as backbones, resulting in asignificant domain gap between video and text. In cross-modal feature interaction, text features are only used asquery initialization and do not fully utilize important infor-mation in the text. In this work, we propose using frozenpre-trained vision-language models (VLM) as backbones,with a specific emphasis on enhancing cross-modal fea-ture interaction. Firstly, we use frozen convolutional CLIPbackbone to generate feature-aligned vision and text fea-tures, alleviating the issue of domain gap and reducingtraining costs. Secondly, we add more cross-modal featurefusion in the pipeline to enhance the utilization of multi-modal information. Furthermore, we propose a novel videoquery initialization method to generate higher quality videoqueries. Without bells and whistles, our method achieved51.5 J &F on the MeViS test set and ranked 3rd place forMeViS Track in CVPR 2024 PVUW workshop: Motion Ex-pression guided Video Segmentation.",
  ". Introduction": "Referring video object segmentation (RVOS) is a contin-ually evolving task that aims to segment target objects invideo, referred to by linguistic expressions.To investi-gate the feasibility of using motion expressions to groundand segment objects in videos, a large-scale dataset calledMeViS was proposed, which contains a large numberof motion expressions to indicate target objects in complexenvironments. Therefore, Motion Expression guided VideoSegmentation requires segmenting objects in video contentbased on sentences describing object motion, which is more",
  ". Comparisons of LMPM and our Framework": "challenging compared to traditional RVOS datasets.The MeViS dataset further emphasizes the importanceof language understanding and modeling text-video rela-tions.The current RVOS methods typically use independently pre-trained vision and languagemodels as backbones. As shown in (a), LMPM uses Tiny Swin Transformer as the image encoder andRoBERTa as the text encoder. This leads to a signifi-cant domain gap between video and text, making text-videorelation modeling more difficult and requiring more train-ing costs to finetune the backbone. In addition, previousmethods do not place enough emphasis on cross-modal fea-ture interaction. For example, LMPM only use text em-bedding as query initialization and do not fully utilize keyinformation in the text.In this work, we propose using frozen pre-trained vision-language models (VLM) as backbones, with a specificemphasis on enhancing cross-modal feature interaction.Firstly, we use frozen convolutional CLIP back-bone to generate feature-aligned vision and text features.As shown in (b), we do not fine tune the CLIP back-",
  "Video Feature": ". The overview architecture of the proposed method. This model inputs multiple images and motion expression from video clips,and outputs multi-scale visual features and sentence-level text features through a frozen CLIP backbone. Cross-Modal Encoder fuses textand image features, Frame Query decoder independently generates frame queries for each frame. Then, Video Query Decoder reorder andfuses all frame queries to adaptively initialize video queries. Finally, Video Query Decoder refines video queries for final mask prediction. bone to preserve pre-trained knowledge of vision-languageassociation. This not only alleviates the issue of domaingap, but also greatly reduces training costs. Secondly, weadd more cross-modal feature fusion in the pipeline to en-hance the utilization of multi-modal information. We designthree cross-modal feature interaction module in the model,including cross-modal encoder, frame query decoder andvideo query decoder. These modules enhance video andtext features through simple cross-attention. Furthermore,to fully utilize the prior knowledge of frame queries, wepropose a novel video query initialization method to gener-ate higher quality video queries. Specifically, we performbipartite matching and reorder frame queries, then aggre-gate them in a weighted manner to initialize video queries.In this year, Pixel-level Video Understanding in the WildChallenge (PVUW) challenge adds two new tracks, Com-plex Video Object Segmentation Track based on MOSE and Motion Expression guided Video Segmentation trackbased on MeViS .In the two new tracks, additionalvideos and annotations that feature challenging elementsare provided, such as the disappearance and reappearanceof objects, inconspicuous small objects, heavy occlusions,and crowded environments in MOSE .Moreover, anew motion expression guided video segmentation datasetMeViS is provided to study the natural language-guidedvideo understanding in complex environments. These newvideos, sentences, and annotations enable us to foster thedevelopment of a more comprehensive and robust pixel-level understanding of video scenes in complex environ-ments and realistic scenarios. We make evaluations of theproposed method on the MeViS dataset. Without us-ing any additional training data, our method achieved 46.9",
  ". Method": "We propose a novel framework for referring video objectsegmentation. It contains a frozen convolutional CLIP im-age backbone for video feature extraction, a CLIP text back-bone for text feature extraction, a cross-modal encoder forimage and text feature fusion (Sec. 2.1), a frame query de-coder for generating frame queries (Sec. 2.2), a video queryinitializer for video query initialization (Sec. 2.3), and avideo query decoder for mask refinement (Sec. 2.4). Theoverall framework is available in .",
  ". Cross-modal Encoder": "Given an (Video, Text) pair, we extract multi-framemulti-scale image features Fv with CLIP image encoder,and text features Ft with CLIP test encoder. Due to the useof convolutional CLIP image encoder , we can extractmulti-scale features from the outputs of different blocks.After extracting vanilla video and text features, we fed theminto a cross-modal encoder for cross-modal feature fusion.As shown in (a), the cross-modal encoder is built ontop of the pixel decoder of Mask2Former , which lever-ages the Deformable self-attention to enhance imagefeatures. Inspired by Grounding DINO and GLIP ,we add an image-to-text cross-attention and a text-to-imagecross-attention for feature fusion. These modules help alignfeatures of different modalities, ultimately obtaining en-",
  ". Frame Query Decoder": "We develop a frame query decoder to independently gen-erate frame queries Qf RT Nf C for each frame, asshown in (b). Frame queries are directly initializedby text features, then are fed into a text cross-attention layerto combine text features, an image cross-attention layer tocombine image features, a self-attention layer, and an FFNlayer in each frame query decoder layer. Each decoder layerhas an extra text cross-attention layer compared with thetransformer decoder layer of Mask2Former , as we needto inject text information into queries for better modalityalignment.",
  ". Video Query Initializer": "After generating frame-level representation, the next step isto generate video queries Qv RNvC to represent theentire video clip.Inspired by LBVQ , video querieshave great similarity to frame queries per frame, and theiressence is the fusion of frame queries. Instead of the simpletext feature initialization strategy , we aggregate framequeries to achieve video query initialization.Firstly, as shown in (c), the Hungarian match-ing algorithm is utilized to match the Qf of adjacentframes, as is done in . Qtf = Hungarian(Qt1f, Qtf),t [2, T]",
  ". Video Query Decoder": "After obtaining the initialized video queries, they are fedinto the video query decoder for layer by layer refinement.As shown in (d), video queries are fed into a textcross-attention layer to combine text features, an querycross-attention layer to combine frame queries features, aself-attention layer, and an FFN layer in each video querydecoder layer. The video queries of the last layer will be dotmultiplied with image features to generate the final mask.",
  ". Datasets and Metrics": "Datasets. MeViS is a newly established dataset thatis targeted at motion information analysis and contains2,006 video clips and 443k high-quality object segmenta-tion masks, with 28,570 sentences indicating 8,171 objectsin complex environments. All videos are divided into 1,662training videos, 190 validation videos and 154 test videos.Evaluation Metrics. we employ region similarity J (aver-age IoU), contour accuracy F (mean boundary similarity),and their average J &F as our evaluation metrics.",
  ". Implementation Details": "We use ConvNeXt-Large CLIP backbones fromOpenCLIP pretrained on LAION-2B dataset. Ontop of the frozen CLIP backbone, we build our model fol-lowing Mask2Former . By default, Cross-modal Encoderis composed of six layers, Frame Query Decoder employsnine layers with Nf = 20 frame queries, and Video QueryDecoder employs six layers with Nv = 20 video queries.The coefficients for similarity loss is set as sim = 0.5. Wetrain 100,000 iterations using AdamW optimizer with alearning rate of 0.00005. Our model is trained on 4 NVIDIA3090 GPUs, each with a video clip containing 8 randomlyselected frames. All frames are cropped to have the longestside of 640 pixels and the shortest side of 360 pixels duringtraining and evaluation.",
  ". Conclusion": "In this work, we propose using frozen pre-trained vision-language models as backbones, with a specific emphasis onenhancing cross-modal feature interaction. We use frozenconvolutional CLIP backbone to generate feature-alignedvision and text features, alleviating the issue of domaingap and reducing training costs. We add more cross-modalfeature fusion in the pipeline to enhance the utilizationof multi-modal information.We propose a novel videoquery initialization method to generate higher quality videoqueries. Evaluations are made on the MeViS dataset andour method ranked 3rd place for MeViS Track in CVPR2024 PVUW workshop: Motion Expression guided VideoSegmentation. Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.End-to-end referring video object segmentation with multi-modal transformers. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages49854995, 2022. 1 Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-der Kirillov, and Rohit Girdhar.Masked-attention masktransformer for universal image segmentation. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 12901299, 2022. 2, 3, 4 Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, andChen Change Loy.Mevis: A large-scale benchmark forvideo segmentation with motion expressions. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 26942703, 2023. 1, 2, 3, 4 Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,Philip HS Torr, and Song Bai. Mose: A new dataset for videoobject segmentation in complex scenes. In Proceedings ofthe IEEE/CVF International Conference on Computer Vi-sion, pages 2022420234, 2023. 2",
  "Lee, and Seon Joo Kim. Vita: Video instance segmentationvia object token association. Advances in Neural InformationProcessing Systems, 35:2310923120, 2022. 3": "De-An Huang, Zhiding Yu, and Anima Anandkumar. Min-vis:A minimal video instance segmentation frameworkwithout video-based training. Advances in Neural Informa-tion Processing Systems, 35:3126531277, 2022. 3 Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, CadeGordon,Nicholas Carlini,Rohan Taori,Achal Dave,Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-clip, 2021. If you use this software, please cite it as below.4",
  "Harold W Kuhn. The hungarian method for the assignmentproblem. Naval research logistics quarterly, 2(1-2):8397,1955. 3": "Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, LuYuan, Lei Zhang, Jenq-Neng Hwang, et al.Groundedlanguage-image pre-training.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1096510975, 2022. 2 Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj,and Yan Lu. Robust referring video object segmentation withcyclic structural consensus. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 2223622245, 2023. 1 Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, et al. Grounding dino: Marrying dino with groundedpre-training for open-set object detection.arXiv preprintarXiv:2303.05499, 2023. 2 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, MandarJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-moyer, and Veselin Stoyanov. Roberta: A robustly optimizedbert pretraining approach. arXiv preprint arXiv:1907.11692,2019. 1 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 1 Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-enhofer, Trevor Darrell, and Saining Xie. A convnet for the2020s. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1197611986,2022. 1, 4",
  "Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon,Ross Wightman,Mehdi Cherti,Theo": "Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, et al. Laion-5b: An open large-scale dataset for trainingnext generation image-text models. Advances in Neural In-formation Processing Systems, 35:2527825294, 2022. 4 Dongming Wu, Tiancai Wang, Yuang Zhang, XiangyuZhang, and Jianbing Shen.Onlinerefer: A simple onlinebaseline for referring video object segmentation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 27612770, 2023. 1 Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and PingLuo.Language as queries for referring video object seg-mentation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 49744984, 2022. 1 Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-vocabulary seg-mentation with single frozen convolutional clip. Advances inNeural Information Processing Systems, 36, 2024. 2"
}