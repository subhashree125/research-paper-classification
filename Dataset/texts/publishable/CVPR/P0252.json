{
  "Abstract": "The Segment Anything Model (SAM), originally de-signed for general-purpose segmentation tasks, has beenused recently for polyp segmentation. Nonetheless, fine-tuning SAM with data from new imaging centers or clin-ics poses significant challenges. This is because this ne-cessitates the creation of an expensive and time-intensiveannotated dataset, along with the potential for variabilityin user prompts during inference. To address these issues,we propose a robust fine-tuning technique, PP-SAM, thatallows SAM to adapt to the polyp segmentation task withlimited images. To this end, we utilize variable perturbedbounding box prompts (BBP) to enrich the learning contextand enhance the models robustness to BBP perturbationsduring inference. Rigorous experiments on polyp segmen-tation benchmarks reveal that our variable BBP perturba-tion significantly improves model resilience. Notably, onKvasir, 1-shot fine-tuning boosts the DICE score by 20%and 37% with 50 and 100-pixel BBP perturbations duringinference, respectively.Moreover, our experiments showthat 1-shot, 5-shot, and 10-shot PP-SAM with 50-pixel per-turbations during inference outperform a recent state-of-the-art (SOTA) polyp segmentation method by 26%, 7%,and 5% DICE scores, respectively. Our results motivatethe broader applicability of our PP-SAM for other medicalimaging tasks with limited samples. Our implementation isavailable at",
  ". Introduction": "Deep learning-based algorithms haveemerged as a promising tool for detecting precancerous le-sions during colonoscopy procedures. Recently, a founda-tional model, namely the Segment Anything Model (SAM),has been introduced for general-purpose semantic segmen-tation. Several studies explore its zero-shot inference [2, 11, 12] or fine-tuning potential for polyp segmen-tation. However, when SAM is fine-tuned using data ex-clusively from one imaging center/clinic, fine-tuning it fordifferent centers/clinics with potentially out-of-distributiondata is crucial due to its limited generalizability. Yet, anno-tating datasets for new centers poses challenges in terms oftime, resources, and cost. Additionally, the complexity is compounded by the pos-sibility of user prompts being imprecise during inference.Since the prompts used by endoscopists are subjective, thereis a chance of variability from the human factors such as fa-tigue, experience, and number of cases examined in the day.SAM performs poorly when the endoscopists use a larger(inaccurate) bounding box prompt than the region of inter-est of the polyp. Therefore, it is critical to develop a robustadaptation method that is resilient to inaccurate (perturbed)bounding box prompts. To address these issues, we investigate fine-tuning ofSAM, namely PP-SAM, for polyp segmentation with vari-able bounding box prompt perturbations. By fine-tuningSAM on colonoscopy images, we demonstrate its superiorperformance for polyp segmentation, showcasing its po-tential to enhance colorectal cancer screening and diagno-sis across diverse clinical settings. Our approach stream-lines the time, cost, and resources required for data annota-tion during fine-tuning, making it effective for multi-centerpolyp segmentation. Our main contributions are as follows: 1. PP-SAM framework: We introduce PP-SAM, a newSAM-based robust adaptation framework with limiteddata for polyp segmentation. We also explore the trans-fer learning capabilities of different modules of SAM. 2. Variable prompt perturbation: We propose to use avery simple but effective strategy, variable perturbationof the bounding box prompt, during fine-tuning to makethe model more robust to prompt perturbation.",
  ". Segment anything model": "In , the authors of SAM propose a foundation model byintroducing a promptable segmentation task, a segmentationmodel to allow for zero-shot transfer to a variety of tasksand a new dataset for image segmentation. The idea forSAM comes from the natural language processing (NLP)domain, where large language models (LLMs) pre-trainedon large datasets have shown strong zero-shot performance. These types of large models have shown the abilityto generalize to tasks and datasets they have not seen dur-ing training . The work of SAM shows that thissame type of training on extremely large-scale datasets canbe translated to the computer vision domain to segment avariety of different image types, including medical images.",
  ". SAM in medical image segmentation": "Despite the strong performance of SAM, it fails to per-form optimally on out-of-distribution domains, such asmedical imaging due to its pre-training on natural images. By evaluating SAM on a collection of 19 med-ical imaging datasets from different anatomies and modali-ties , the authors of perform an experimental studyto determine the potential of SAM to be applied in medi-cal imaging. The authors find that the performance of SAMbased on single prompts is highly dependent on the datasetand the task, thus concluding that SAM shows impres-sive zero-shot segmentation performance for some medicalimaging datasets while performing poorly on others .Auto-SAM replaces SAMs conditioning on either amask or a set of points with an encoder that operates on thesame input image. The introduction of this encoder allowsfor Auto-SAM to obtain state-of-the-art (SOTA) re-sults on multiple medical image segmentation benchmarks.MedSAM goes in another direction by designing afoundation model for medical image segmentation throughusing a curated dataset with over one million images .",
  ". Polyp segmentation": "Colon polyps are an important precursor to colon cancer,thus the correct segmentation of colon polyps can reducemisdiagnosis of colon cancer . Many methods have beenproposed for polyp segmentation, but the limited amount ofcolonoscopy images remains a major challenge. A methodlike SAM that can perform segmentation without a largeamount of data looks thus very appealing.In , the authors propose Polyp-SAM, which is a fine-tuned version of the SAM model for polyp segmentation.Polyp-SAM achieves SOTA or near SOTA performance onfive datasets, thus showing the effectiveness of SAM inmedical image segmentation tasks. In , the authors usea text prompt-aided SAM called Polyp-SAM++, which isshown to be more robust and precise compared to the un-prompted SAM .The prior work on SAM mod-els (SAM models in medical imaging and SAM models inpolyp segmentation), builds upon the original SAM modeland uses different methods to achieve better segmentationresults than the original SAM for more specialized taskswhere pre-training on natural images may not be enoughto achieve SOTA results. However, none of these methodsconsiders the real-life inherent inaccuracy (perturbation) inprompts while adapting SAM for polyp segmentation.",
  ". Methodology": "shows our proposed PP-SAM framework. First,we take a small labeled dataset of images as input. Then, weextract the bounding box from the corresponding ground-truth (GT) segmentation mask. Then, we perturb the bound-ing box using our variable bounding box prompt perturba-tion method. Finally, we use this dataset with GT masksand perturbed bounding box prompts to fine-tune SAM .The main components are described next.",
  "Variable perturbed prompts for fine-tuning": "While SAM can utilize various prompts, we advocate foradapting the bounding box prompt due to its simplicity. Ourapproach involves fine-tuning SAM for polyp segmentationusing a variable (perturbed) bounding box prompt.Ourvariable perturbation extends the bounding box randomlyfrom 0 to n pixels in all four directions, as in box (2). When we fine-tune SAM with this strategy, the . Few-shot fine-tuning pipeline. Here, no perturbation represents the bounding box extracted from the original ground truth (GT)masks; variable perturbations means extending the bounding box on each side separately.",
  "Prompts during inference": "To assess the robustness of our method, we evaluate the per-formance of the models with different levels (0, 5, 10, ..., 95,and 100 pixels) of fixed perturbations in the bounding box(on all sides) during inference. For instance, the 10-pixelperturbation during inference means extending the bound-ing box by 10 pixels on all sides.",
  ". Datasets": "We use the Kvasir dataset to fine-tune SAM for few-shotpolyp segmentation. This dataset contains 1,000 polyp im-ages. Following , we adopt the same 900 images fromKvasir as the training set and the remaining 100 imagesas the testset.To assess the generalizability of our pro-posed decoder, we use three unseen test datasets, namelyClinicDB , EndoScene , and ColonDB . Clin-icDB consists of 612 images extracted from 31 colonoscopyvideos. EndoScene and ColonDB consist of 60 and 379 im-ages, respectively. The images of our three unseen testsetssignificantly differ from trainset as they are collected fromseparate hospitals/clinics/centers with different acquisitiondevices and procedures.",
  ". Evaluation metrics": "We use DICE similarity scores as evaluation metrics in allour experiments. The DICE similarity score measures theoverlap accuracy which is suitable for binary segmenta-tion with imbalanced data. As polyp segmentation is a bi-nary segmentation task with imbalanced polyp (lesion) andbackground regions, we favor the DICE score for assess-ing the performance of our PP-SAM on polyp segmenta-tion, where the precise delineation of anatomical structuresis crucial for diagnosis and treatment planning. The DICEscore DICE(Y, P) of a ground truth mask Y and a pre-dicted mask P is defined in Equation 1:",
  ". Implementation details": "We implement and fine-tune our PP-SAM using Pytorch1.11.0, operating on a single NVIDIA RTX A6000 GPUwith 48GB of memory. In our experiments, we set the max-imum length for variable bounding box prompt perturba-tion, n = 50. We resize the images to 1024 1024 andre-scale the bounding box to match the new image resolu-tion. We use the AdamW optimizer with both a learn-ing rate and weight decay rate of 0.0001. We do not use anydata augmentations and learning rate schedulers.During fine-tuning, we optimize the combined weightedCrossEntropy and mean intersection over union (mIoU) lossfunction. SAM with ViT-B (SAM-B) is fine-tuned for 100epochs with a batch size of 1 unless otherwise mentioned;we save the best model based on the DICE score with an in-ference bounding box perturbation of 30 pixels on all sides.We report the average DICE similarity scores over five runsto evaluate our fine-tuning performance. We calculate theDICE scores using the original resolution of the test images.",
  "Transfer learning capabilities of different mod-ules of SAM": "We empirically explore SAMs transfer learning capabili-ties across four distinct experimental configurations. Theoutcomes of these investigations, as it pertains to the impactof bounding box prompt perturbations during inference, areillustrated in . In our analysis, the strategic freez-ing of the mask decoder (i.e., exclusively fine-tuning theimage and prompt encoders) emerges as the most effectiveapproach, yielding the highest DICE scores.This supe-rior performance likely stems from the avoidance of over-fitting, which can occur when fine-tuning the mask decoder",
  ". Comparison of different levels of bounding box per-turbations during training, on the Kvasir testset. As shown, ourvariable prompt perturbation produces the overall best results": "with a limited dataset. Conversely, keeping the image en-coder frozen exposes the model to increased vulnerabilityto prompt perturbation. An even more significant decline inperformance is observed when both the image and mask de-coders are frozen, underscoring their collective importancein model adaptability. From these insights, we firmly advo-cate for freezing only the mask decoder and selectively fine-tuning the image and prompt encoders to optimize transferlearning efficiency and model robustness.",
  "Effectiveness of variable bounding box promptperturbations during fine-tuning": "In , we illustrate the impact of different bound-ing box prompt perturbations during fine-tuning. We assessDICE scores for perturbations of 0 (no perturbation), 10,20, 30, 40, 50, and random perturbations within 0-50 pixels . Experimental results on Kvasir testset. All models aretrained using the randomly sampled images from the Kvasir train-set. We use our variable perturbed bounding box (in the range of0 to 50) during training. Also, we keep the mask decoder frozenduring these experiments. on all sides of the original bounding box prompt. The re-sults reveal that the models without prompt perturbationsduring training are susceptible to larger inference pertur-bations. Resilience to these perturbations improves withlarger training perturbations. However, models fine-tunedwith variable perturbations (0-50 pixels) demonstrate betteroverall performance for both small and large inference per-turbations. We believe that variable perturbations on differ-ent sides during training enhance model robustness againstvarious levels of bounding box prompt perturbations.",
  "Learning ability of PP-SAM for polyp segmenta-tion on Kvasir dataset": "In , we present the outcomes of applying bothzero-shot and few-shot fine-tuning techniques to the Kvasirdataset during testing. As depicted in this figure, there isa discernible decrease in the DICE scores as the magni-tude of bounding box perturbations increases during infer-ence, a trend that aligns with our expectations. Notably,our fine-tuned models demonstrate enhanced durability inthe face of these prompt perturbations throughout the infer-ence process. The implementation of our random 1-shot and50-shot fine-tuning with variable perturbed bounding boxprompts significantly enhances model robustness, boostingthe DICE scores by 37% and 60%, respectively, over zero-shot with 100-pixel perturbations on all sides. We can alsoconclude that the DICE scores improve from 1-shot to 50-shot, with minimal difference beyond 50-shot.",
  "Generalizability of PP-SAM for unseen polypsegmentation": "displays evaluation results of unseen polypsegmentation on ClinicDB testset, where our fine-tuningmethod shows a significant performance improvement, i.e.,24% and 43.5% DICE score increase over zero-shot for 1-shot and 50-shot, respectively. In , we can seea similar improvement in the DICE scores for the unseenpolyp segmentation on EndoScene dataset. More specifi-cally, our 1-shot and 50-shot fine-tuning improves the DICEscores by 19% and 50%, respectively, over zero-shot infer-ence with a 100-pixel perturbation. shows the re-sults on the unseen ColonDB dataset, where our 1-shot and50-shot fine-tuning achieve 27% and 45% improvements,respectively, over zero-shot inference with 100-pixel per-turbations during inference. We observe minimal improve-",
  "Scalability of SAM with ViT encoders": "reports the zero-shot and fine-tuning results ofSAM models with ViT-B (Base) and ViT-L (Large) imageencoders. From this figure, we can conclude that SAM withViT-L surpasses ViT-B in zero-shot polyp segmentation.However, the fine-tuned models with both ViT encodersshow similar performance. Hence, these results could beused to argue the need for careful consideration when de-ploying models in real-world scenarios, where perturbationsin prompts may occur, and to stress the importance of modelrobustness to such changes. . Performance comparison with a SOTA method on theKvasir testset.We utilize the models trained using randomlyselected images from the Kvasir trainset for these experiments.SAM-B 25-pixel and SAM-B 50-pixel are the results of 25 and 50-pixel bounding box perturbations on all sides, respectively, duringinference.",
  "Performance comparison with SOTA": "We report the results of our fine-tuned SAM and a SOTAmethod, PVT-CASCADE in .We can seefrom the bar plot that our PP-SAM with variable bound-ing box perturbations during fine-tuning significantly out-performs PVT-CASCADE for up to 50 shots (i.e., PP-SAMrequires less labeled data to achieve closer to optimal per-formance). More precisely, 1-shot (74.5%), 5-shot (77%),and 10-shot (81.6%) fine-tuning of SAM using our methodachieves 26%, 7%, and 5% better DICE scores than PVT-CASCADE with 50-pixel bounding box perturbations dur-ing inference. Our method achieves further better perfor-mance (i.e., 32%, 11%, and 9% improvement for 1-shot, 5-shot, and 10-shot, respectively) with 25-pixel perturbations.Therefore, our PP-SAM is preferable to non-prompt-basedapproaches for polyp segmentation involving limited pre-cisely labeled ground truth segmentation masks.",
  ". Conclusions": "In this paper, we have presented PP-SAM, an innovativefine-tuning approach for SAM in polyp segmentation. Weintroduce a novel concept of variable perturbations inbounding box prompts during the training phase, which isaimed at improving the models robustness against varia-tions and inconsistencies in real-world prompt scenarios.The capabilities of PP-SAM have been demonstrated tobe substantially superior, both in terms of performance en-hancements and in maintaining resilience to prompt per-turbations, especially when compared to the conventionalzero-shot SAM inference methods on publicly available polyp datasets. Our experiments indicate that fine-tuningsolely the image and prompt encoder (while freezing themask decoder) yields superior results.While PP-SAM currently focuses on binary segmenta-tion and a single bounding box, future work aims to addressthese limitations. Nonetheless, even in its current state, PP-SAM simplifies SAM adoption for new centers/hospitals/-clinics, by requiring minimal annotation effort.",
  "Risab Biswas. Polyp-SAM++: Can A Text Guided SAMPerform Better for Polyp Segmentation?arXiv preprintarXiv:2308.06623, 2023. 1, 2": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-man, Simran Arora, Sydney von Arx, Michael S Bernstein,Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.On the opportunities and risks of foundation models. arXivpreprint arXiv:2108.07258, 2021. 2 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners. Advances in neural in-formation processing systems, 33:18771901, 2020. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 3 Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, HuazhuFu, Jianbing Shen, and Ling Shao.Pranet: Parallel re-verse attention network for polyp segmentation. In Int. Conf.Med. Image Comput. Comput. Assist. Interv., pages 263273.Springer, 2020. 1, 3 Debesh Jha, Pia H Smedsrud, Michael A Riegler, PalHalvorsen, Thomas de Lange, Dag Johansen, and Havard DJohansen. Kvasir-seg: A segmented polyp dataset. In Int.Conf. Multimedia Model., pages 451462. Springer, 2020.3, 4 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 2, 3",
  "Md Mostafijur Rahman and Radu Marculescu. G-cascade:Efficient cascaded graph convolutional decoding for 2d med-ical image segmentation. arXiv preprint arXiv:2310.16175,2023": "Md Mostafijur Rahman and Radu Marculescu. Multi-scalehierarchical vision transformer with cascaded attention de-coding for medical image segmentation. In Medical Imagingwith Deep Learning, pages 15261544. PMLR, 2024. Md Mostafijur Rahman, Mustafa Munir, and Radu Mar-culescu. Emcad: Efficient multi-scale convolutional atten-tion decoding for medical image segmentation.In IEEEConf. Comput. Vis. Pattern Recog., 2024."
}