{
  "Efficient MedSAMs: Segment Anything inMedical Images on Laptop": "Jun Ma, Feifei Li, Sumin Kim, Reza Asakereh, Bao-Hiep Le, Dang-Khoa Nguyen-Vu, Alexander Pfefferle,Muxin Wei, Ruochen Gao, Donghang Lyu, Songxiao Yang, Lennart Purucker, Zdravko Marinov, MariusStaring, Haisheng Lu, Thuy Thanh Dao, Xincheng Ye, Zhi Li, Gianluca Brugnara, Philipp Vollmuth, MarthaFoltyn-Dumitru, Jaeyoung Cho, Mustafa Ahmed Mahmutoglu, Martin Bendszus, Irada Pfluger, AdityaRastogi, Dong Ni, Xin Yang, Guang-Quan Zhou, Kaini Wang, Nicholas Heller, Nikolaos Papanikolopoulos,Christopher Weight, Yubing Tong, Jayaram K Udupa, Cahill J. Patrick, Yaqi Wang, Yifan Zhang, FranciscoContijoch, Elliot McVeigh, Xin Ye, Shucheng He, Robert Haase, Thomas Pinetz, Alexander Radbruch,Inga Krause, Erich Kobler, Jian He, Yucheng Tang, Haichun Yang, Yuankai Huo, Gongning Luo, KaisarKushibar, Jandos Amankulov, Dias Toleshbayev, Amangeldi Mukhamejan, Jan Egger, Antonio Pepe,Christina Gsaxner, Gijs Luijten, Shohei Fujita, Tomohiro Kikuchi, Benedikt Wiestler, Jan S. Kirschke,Ezequiel de la Rosa, Federico Bolelli, Luca Lumetti, Costantino Grana, Kunpeng Xie, Guomin Wu, BehrusPuladi, Carlos Martn-Isla, Karim Lekadir, Victor M. Campello, Wei Shao, Wayne Brisbane, Hongxu Jiang,Hao Wei, Wu Yuan, Shuangle Li, Yuyin Zhou, and Bo Wang",
  "Abstract": "Promptable segmentation foundation models have emerged as a transformative approach to addressing the diverseneeds in medical images, but most existing models require expensive computing, posing a big barrier to their adoptionin clinical practice. In this work, we organized the first international competition dedicated to promptable medical imagesegmentation, featuring a large-scale dataset spanning nine common imaging modalities from over 20 different institutions.The top teams developed lightweight segmentation foundation models and implemented an efficient inference pipeline thatsubstantially reduced computational requirements while maintaining state-of-the-art segmentation accuracy. Moreover, thepost-challenge phase advanced the algorithms through the design of performance booster and reproducibility tasks, resultingin improved algorithms and validated reproducibility of the winning solution. Furthermore, the best-performing algorithmshave been incorporated into the open-source software with a user-friendly interface to facilitate clinical adoption. The dataand code are publicly available to foster the further development of medical image segmentation foundation models and pavethe way for impactful real-world applications.",
  "INTRODUCTION": "Segmentation is a fundamental task in medical image analysis, aiming to provide accurate boundaries for anatomiesand pathologies, which plays an important role in many clinical tasks, such as disease detection and diagnosis, surgicalplanning, and treatment monitoring , . Over the past decade, deep learning-based models have revolutionizedmedical image segmentation by providing state-of-the-art accuracy and unprecedented automation . More recently,the emergence of foundation models, such as Segment Anything Model (SAM) , , has introduced a new paradigmfor segmentation tasks , . These models are trained on large-scale annotated images and capable of being generalizedacross multiple domains and transferred to unseen domains . For example, MedSAM and BiomedParse havedemonstrated strong zero-shot segmentation ability across common medical imaging modalities and a wide range ofanatomies by fine-tuning SAM on over one million medical image-mask pairs.However, despite these advances, adopting foundation models for segmentation in clinical practice remains an ongoingchallenge. One key bottleneck preventing the widespread use of these foundation models is the high demand for computingresources. Many of these segmentation foundation models are resource-intensive because of large model weights, makingthem difficult to deploy on commonly available hardware, such as laptops, creating a barrier to democratizing the useof advanced segmentation tools, especially in resource-constrained clinical environments. Additionally, as segmentationfoundation models have been developed and validated under different settings, it is difficult to assess their performancerigorously. This calls for standardized benchmarks, yet their absence hinders the identification of optimal models .International competitions have proven to be effective in driving advancements in various domains, such as ImageNetcompetition for deep convolutional neural networks and the critical assessment of protein structure predictioncompetition for AlphaFold . By providing a standard platform for researchers to compare their models, these challengesfoster collaborative efforts of researchers all over the world to identify novel approaches while ensuring transparent eval-uation and reproducible results . Thus, an international benchmark for promptable medical image segmentationcan greatly boost the progress in creating lightweight, efficient models that are also suitable for clinical deployment.",
  "arXiv:2412.16085v1 [eess.IV] 20 Dec 2024": "In this work, we present the competition results for promptable medical image segmentation, specifically designed toencourage the development of more efficient segmentation foundation models for diverse modalities and segmentationtargets. Importantly, we collaborate with 24 institutions worldwide to curate a new testing set with over 4000 cases, allowingfor fair model evaluation while minimizing the risk of data leakage. Based on this comprehensive benchmark dataset,the top-performing algorithms demonstrate substantial improvements in both segmentation accuracy and efficiency, withmodels achieving segmentation results over ten times faster than existing SAM-based foundation models , . Moreover,we work with the two best-performing algorithm developers and integrated the models into the open-source imagecomputing platform 3D Slicer with a user-friendly interface, making state-of-the-art segmentation models more accessibleand practical for a wider range of clinical applications. . Competition design. a, The task is to develop universal segmentation foundation models that can accept various medical image inputswith target bounding box prompts and generate the corresponding segmentation masks. The model should be lightweight and deployable onlaptops without reliance on graphics processing unit (GPU). b, Three phases in the competition. During the development phase, participants traintheir models on the training set and obtain performance metrics on the online validation set on Codabench. The top 20 teams on the validationleaderboard are invited to submit their algorithm dockers and we manually evaluate them on the hidden testing set. After that, we release all thetechnical details and code of the top ten teams and launch a post-challenge phase to invite participants to further boost their model performanceand reproduce the winning algorithms.",
  "Competition Design: Universal and Lightweight Medical Image Segmentation Foundation Models": "The competition task is designed for promptable medical image segmentation. Specifically, the input contains variousmedical images and bounding boxes for segmentation targets, and the output is the corresponding segmentation mask.The bounding box is selected as the prompt because it can precisely specify the target, which has lower ambiguity thanpoint prompts. The challenge consists of three distinct phases: a development phase lasting 122 days, a testing phasespanning 35 days, and a post-challenge phase of 35 days. These phases focus respectively on model training, evaluation ona hidden testing set, and subsequent improvements and reproducibility analysis.During the development phase, participants were provided with a large-scale and diverse training dataset, servingas the foundation for participants to design, develop, and train their models. Moreover, an online leaderboard wasmade available on Codabench , allowing for automatic evaluation of participants results. Participants could leveragethis feature to test their models against a tuning set, obtain performance feedback, and iteratively tune and refinetheir approaches. This phase was instrumental in enabling participants to optimize their models and prepare them forsubsequent testing.During the testing phase, the top 20 teams from the validation leaderboard were invited to make testing submissions.However, submissions were not limited to these top teams and other participants were also welcome to submit theirsolutions for evaluation. Participants were required to encapsulate their algorithms in Docker containers, ensuring astandardized and portable format for evaluation. Each Docker container was manually executed on a hidden testingset, with all evaluations conducted on the same workstation to ensure fairness and consistency. The teams were rankedbased on both the accuracy and efficiency of their models. Specifically, the evaluation criteria included the Dice SimilarityCoefficient (DSC), Normalized Surface Distance (NSD), and runtime performance. This rigorous and equitable processdetermined the testing leaderboard rankings.The post-challenge was designed with two primary objectives: further improving model performance by integratingsuccessful strategies from the top-performing algorithms (performance booster) and evaluating the reproducibility of thewinning solutions (reproducibility analysis). This phase aimed to push the boundaries of segmentation performance usingcollective knowledge and assess whether the winning solution could be easily implemented by others. In the performance-booster subtask, participants were encouraged to enhance their models by incorporating two new datasets, fast inferencemethods, and other optimization strategies contributed by participants. In the reproducibility subtask, all participants wereinvited to reproduce the results of the top-performing teams. We made the code and corresponding methodologies of topsolutions publicly available, allowing participants to validate them on the testing set. These collaborative efforts not onlyfostered the development of a refined solution capable of outperforming the original winning algorithms but also verifiedthe reproducibility of the top solutions.",
  "Competition Dataset: A Global Collaboration": "We provided participants with a diverse set of training and testing images to facilitate robust model development and eval-uation. a illustrates several visual examples from the training and testing sets across various imaging modalities,accompanied by their corresponding bounding box prompts and reference masks. The training dataset comprises 1,809,644image-mask pairs spanning 10 imaging modalities, including Computed Tomography (CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography (PET), ultrasound, X-ray, Optical Coherence Tomography (OCT), endoscopy, fundus,and microscopy images. b visualizes the distribution of image-mask pairs across these modalities, with CT andMRI images dominating the dataset due to their widespread use in clinical diagnostics. All images were sourced frompublicly available datasets with proper licensing permissions and were preprocessed to ensure standardization in intensityrange and format. However, some public datasets prohibit redistribution. To address this limitation, we compiled acomprehensive list of over 100 medical image segmentation datasets, detailing information such as modality, segmentationtargets, number of cases, and download links. Participants were also encouraged to contribute by adding new publicdatasets to the list. To ensure a fair comparison, participants were restricted to using only the officially preprocessed dataand datasets included in the curated list for model development.Since the majority of the public medical image segmentation datasets have been included in this competition, weopted to create a completely new testing set rather than annotating existing datasets to mitigate the potential risk of dataleakage. To ensure the dataset was both diverse and representative, we launched a Call for Dataset initiative, invitingresearchers from around the globe to contribute de-identified medical images in accordance with predefined criteria andlicensing requirements (Methods). As a result, we successfully collected 124,004 image-mask pairs from 4,414 cases (c,Supplementary ). While the testing set spans a wide range of imaging modalities, CT and MRI images continue todominate, reflecting their prevalence in clinical diagnostics.d illustrates the geographical distribution of participants and testing data sources in the competition. The bluecircles represent the locations of participant teams, with their size corresponding to the relative number of teams in eachregion. The challenge attracted participation from over 200 individuals and 61 data contributors across 24 institutions,which highlights the international interest in advancing medical image segmentation and demonstrates the competitionssuccess in engaging a wide array of contributors from different regions around the world. . Competition dataset. a, Example images and the annotations in the training and testing set. b, The number of image-mask pairs in thepublic training set. c, The number of image-mask pairs in the hidden testing set. All testing data has been newly collected specifically for thischallenge and was not publicly available beforehand. d, Geographical distribution of participants and data contributors in the testing set.",
  "Algorithm Overview": "We first developed a baseline model, LiteMedSAM, by distilling the large vision transformer encoder in MedSAMto a TinyViT (Methods). The code and model were publicly available at the beginning of the competition to reducethe entrance barrier for participants. In addition to the baseline model, we received 22 algorithm submissions during thetesting phase where 20 algorithms were from the top 20 teams on the validation leaderboard.Most of the top-performing teams employed a SAM-like promptable segmentation model, including an image encoderto extract features from input images, a prompt encoder to process the bounding box coordinates, and a mask decoderto generate segmentation masks. The key to standing out is to design lightweight image encoders and implement fastinference strategies. We have summarized the key components of the top five algorithms, focusing on their networkarchitectures and efficient inference strategies, in Supplementary . In this section, we specifically highlight the topthree best-performing algorithms.Best-performing algorithm. Le et al. (T1-seno ) proposed MedficientSAM, which used the EfficientViT-L1 modelas the image encoder and conducted knowledge distillation from the MedSAM encoder on the training set. Then, thewhole model was fine-tuned in an end-to-end way where the prompt encoder and the mask decoder were initializedwith MedSAMs pretrained weights. For faster inference, the Python-based pipeline was ported to C++, and the trainedmodel was exported to OpenVINO format. Moreover, an embedding caching mechanism was implemented for 3D images,which only needed to compute image embeddings once for multiple box prompts. Furthermore, all the modules werecompiled from source code to take advantage of runtime optimizations such as Advanced Vector Extensions and LinkTime Optimization.Second-best-performing algorithm. Pfefferle et al. (T2-automlfreiburg ) introduced a data-aware fine-tuning frame-work to efficiently produce tailored models for specific data or modality types. The image encoder was the lightweightEfficientViT-L0 model with a three-stage fine-tuning pipeline: knowledge distillation, general fine-tuning on the wholetraining set, and data-aware fine-tuning on a subset. In addition to the above OpenVINO and embedding caching mechanism for inference optimization, model caching and slim packages (e.g., Python 3.11 and OpenCV headless version)were used to speed up loading models and dockers, respectively.Third-best-performing algorithm. Wei et al. (T3-skippinglegday ) proposed RepMedSAM, which replaced Med-SAMs image encoder with a pure CNN and lightweight RepVit-M model . The prompt encoder and mask decoderwere from MedSAM, while the image encoder was trained by knowledge distillation. No specific runtime optimization wasimplemented during inference because the designed model already had relatively better latency compared to the baselinemodel.",
  "Performance analysis on the hidden testing set": "All the submitted algorithms were independently executed on the same computing platform (Methods) for a fair compari-son. Fig 3a and b show the average DSC and NSD scores across the 2D and 3D datasets, respectively. The 2D segmentationresults reveal generally high average DSC and NSD scores for most algorithms, with several teams, such as seno andhmi306, achieving median scores over 0.9. Those teams also have relatively narrow interquartile ranges (IQRs), suggestingthat their algorithms demonstrate both high accuracy and consistent performance across different cases.The overall range of scores for 3D segmentation appears broader than in the 2D case, reflecting the added complexityof volumetric segmentation tasks. 3D segmentation results also exhibit more variability with substantially lower medianscores compared to 2D results. While teams such as skippinglegday and waterlooviplab maintain strong performance withhigh average DSC and NSD scores, many teams display wider IQRs and longer whiskers, indicating greater inconsistencyin their segmentation results.In addition to segmentation accuracy, the efficiency of the segmentation algorithm is another crucial aspect, as itsubstantially impacts the overall user experience. The bubble plots in Fig 3c and d present the trade-off between segmen-tation accuracy and efficiency, measured as runtime in seconds, for the 2D and 3D datasets, respectively. The size of thecircles is proportional to the NSD score. For 2D segmentation, most algorithms achieve high DSC scores (above 0.8) whilemaintaining a runtime of less than 10 seconds per image. Notably, teams seno and automlfreiburg achieve exceptionalaccuracy and efficiency, with a NSD near 0.9 and a runtime less than 2 seconds. In contrast, 3D segmentation results showa wider spread in runtimes, ranging from approximately 10 seconds to over 70 seconds per case, reflecting the greatercomputational complexity of volumetric data processing. Nevertheless, teams seno and automlfreiburg still obtain thefastest inference with competitive DSC and NSD scores, demonstrating a great trade-off between segmentation accuracyand efficiency.Next, we conducted a fine-grained analysis of the three overall best-performing teams and baseline across the ninemodalities (Fig 3e, Supplementary -9). For the 3D modalities CT, MRI, and PET, T2-automlfreiburg and T3-skippinglegday achieve comparable performances, which are significantly better than T1 (p < 0.01). Across most 2Dmodalities, T1-seno consistently outperforms the others, achieving the highest accuracy with relatively narrow interquar-tile ranges, reflecting its robustness and consistency. T2-automlfreiburg, while competitive on endoscopy, fundus, andmicroscopy, shows inferior performance on the other modalities, particularly in ultrasound and X-ray. T3-skippinglegdayapproaches the performance of T1-seno for most modalities, especially excelling in microscopy. The LiteMedSAM baseline,while competitive for some modalities, such as PET and X-ray, lags substantially behind for most modalities, with lowerscores and wider performance ranges.Furthermore, we compare the runtime of the four algorithms across 3D and 2D modalities. For 3D modalities (Fig 3f),T1-seno and T2-automlfreiburg demonstrate consistently lower runtimes with minimal variance than other algorithms,indicating their efficiency and stability across all 3D images. In contrast, the baseline exhibits higher runtimes andsubstantial variability, particularly for CT, with runtimes exceeding 300 seconds in large CT scans. Similar trends areobserved across 2D modalities (Fig 3g). Notably, T1-seno consistently consumes around one to two seconds for all 2Dimages, which is five to ten times faster than the baseline model.Finally, we visualized some segmentation examples of the three best-performing algorithms for each modality (Fig 3h,Supplementary ). The top algorithms demonstrate superior versatility, achieving accurate results for most of theimages even if the object boundaries are not clear. However, certain challenging cases highlight areas where further refine-ment is needed. For instance, in 3D modalities like CT and MR, the segmentation of heterogeneous lesions, characterized byirregular shapes and varying intensities, leads to some over- or under-segmentations. Additionally, unseen or less commontargets, such as teeth segmentation in X-ray images, pose unique challenges where the algorithms struggle to generalizeeffectively, resulting in suboptimal predictions. These challenges could be addressed by enriching the training datasetthrough community-driven efforts to include more diverse and rare cases. Moreover, integrating interactive refinementmechanisms, which allow users to provide real-time corrections, could also help improve segmentation results.",
  "Post-challenge analysis": "We found that none of the algorithms consistently achieved the best performance across all modalities, indicating thatthese algorithms could be complementary. This motivated us to design a post-challenge to invite participants to enhancetheir algorithms by using the strategies from top solutions. Although all the top-three algorithms only used the providedtraining set, using external public datasets from the predefined dataset list was allowed and encouraged. To mitigate theeffects of training datasets, we reached out to the teams with the highest DSC and NSD scores in each modality and",
  "T2: 0.782": ". Evaluation results of 23 algorithms on the hidden testing set. Dot and box plot of the Average DSC and NSD scores on the a, 2D(n = 2,309 images) and b, 3D (n = 2,105 scans) testing set. The box plots display descriptive statistics across all testing cases, with the medianvalue represented by the horizontal line within the box, the lower and upper quartiles delineating the borders of the box and the vertical black linesindicating 1.5 IQR. The algorithms are organized on the x-axis based on their corresponding ranks. The bubble plots show the trade-off betweensegmentation accuracy (DSC and NSD) and efficiency on the c, 2D and d, 3D testing set. The circle size is proportional to the NSD score. e,Modality-wise segmentation accuracy performance (Average DSC and NSD) of three best-performing teams and LiteMedSAM baseline. Modality-wise segmentation efficiency (runtime) of three best-performing teams and LiteMedSAM baseline on 3D f, and 2D g, modalities. h, Visualizedsegmentation results of nine modalities. The blue bounding box and green overlay denote prompt and reference standard, respectively. For eachimage, the best DSC score from the three algorithms and the corresponding contour are presented. . Results of the post-challenge. a, The new best-performing algorithm (T2-automlfreiburg-post) and comparison to its predecessor (T2-automlfreiburg) and the previous best-performing algorithm (T1-seno) across all modalities. b, Runtime comparison. c, T1-reproduce successfullyreplicated the performance of T1-seno across all modalities identified the employed external datasets. Only one X-Ray and one PET dataset were used as additional datasetsby the modality-wise winning teams. Thus, we also added those two datasets to the training set.We received six submissions for the performance-booster task. One algorithm (T2-automlfreiburg-post) surpassedthe previous best-performing algorithm (T1-seno) and its previous algorithm (T2-automlfreiburg) in terms of the overallrank. The main technical improvements include using a shared EfficientViT model for all 3D modalities, early stoppingto avoid over-fitting, and speeding up the inference pipeline with C++ implementations from T1-seno . Fig 4ashows the modality-wise performance of the three algorithms. T2-automlfreiburg-post substantially improves the previousalgorithm on PET, ultrasound, X-ray, and microscopy modalities while preserving the performance for the other modalities.Importantly, the runtime is reduced by 2x, matching the efficiency of the fastest algorithm T1-seno (Fig 4b).In addition, we received one submission (T1-reproduce) for the reproducibility analysis task, aiming to achieve similarperformance to T1-seno using the released code . The results, as shown in Fig 4c, indicate that T1-reproduce successfullyreplicated the performance of T1-seno across all modalities, achieving nearly identical average DSC and NSD scores,although the training batch size was smaller than the original setting because of the computing resource limitation.",
  "DISCUSSION": "This work presents the first international competition dedicated to promptable medical image segmentation foundationmodels. One of the key achievements of the competition was the curation of a large-scale, unpublished dataset forbenchmarking, consisting of diverse imaging modalities and newly annotated cases. This dataset not only mitigated therisk of data leakage from existing public sources but also provided a robust and comprehensive foundation for evaluatingsegmentation algorithms. The challenge attracted substantial global participation, with thousands of submissions on theonline leaderboard from over 200 participants, showcasing the high level of interest in the community.This competition also introduced a novel task focused on efficient segmentation, aiming to develop deployable seg-mentation models that balance high accuracy with practical runtime performance without reliance on expensive GPUs.Although all the top teams employed SAM-like architectures, two key designs made their algorithms stand out: efficientnetwork architecture and fast inference strategies. In particular, all the three best-performing algorithms replaced the heavy-weight ViT image encoder in MedSAM with a lightweight image encoder, such as EfficientViT and RepViT ,which reduced the parameters by 50%. Moreover, knowledge distillation was an effective way to transfer knowledge fromlarger, more accurate models to compact models. Notably, using lightweight image encoders with knowledge distillationpreserved segmentation accuracy, which achieved comparable to or even better accuracy than MedSAM while substantiallyreducing computational overhead (Supplementary ).In addition to lightweight image encoders, we also identified several practical strategies for fast inference on CPU,including embedding caching, the use of a C++ inference pipeline, and integration with OpenVINO combined withoptimized Docker deployment. Embedding caching minimizes redundant computations by storing pre-computed 2Dslice embeddings for 3D image segmentation. The C++ inference pipeline improves execution speed by leveraging acompiled languages low-level optimizations, enabling faster runtimes compared to Python. Additionally, OpenVINO andoptimized Docker deployment streamline inference by utilizing a highly efficient runtime environment and lightweightcontainerization, reducing resource consumption and startup latency. Together, these strategies enable faster and moreresource-efficient deployments on edge devices without sacrificing segmentation accuracy.Furthermore, our post-challenge phase also introduced two innovative tasks: the performance booster and the repro-ducibility analysis, aimed at advancing algorithm performance and ensuring scientific rigor. One of the new submissionsset a new state-of-the-art by combining elements of existing successful solutions, demonstrating the power of collaborativeadvancements. The reproducibility analysis further validated the robustness of the competitions outcomes, as the winningsolution during the testing phase was successfully reproduced using the released code and documentation. This highlightsthe transparency and reliability of the methods developed during the challenge, ensuring that the results can serve as asolid foundation for future research.While the competition showcased substantial algorithm advancements for universal medical image segmentationmodels, there remains a substantial gap between these cutting-edge algorithms and their integration into clinical practice.The complexity of deploying advanced models, combined with the need for user-friendly interfaces, often hinders theiradoption in real-world settings. To bridge this gap, we collaborated with the two best-performing teams to incorporatetheir algorithms into 3D Slicer , a widely-used open-source platform for medical image analysis. This integrationallows clinicians and researchers to access state-of-the-art segmentation algorithms without requiring any coding expertise,making these powerful tools more accessible and practical for routine use.This work also has several limitations. First, while we provided one of the largest and most diverse datasets todate, the majority of curated testing images originated from North America, Europe, and Asia, with a noticeable lackof representation from South America, Africa, and Oceania. Second, all the top-performing algorithms utilized 2D modelsfor 3D data segmentation, which overlooked the slice-wise or 3D contextual information inherent in volumetric medicalimages . This limitation may impact segmentation performance, particularly for tasks requiring spatial consistency acrossslices. Finally, the prompts were based on the bounding box because of its effectiveness and less ambiguity. However, thissetting did not consider more flexible interactive segmentation approaches, allowing for iterative user feedback refinement. Future iterations of the competition will aim to address current limitations and push the boundaries further. Inparticular, we will expand the dataset by incorporating more images from recent public datasets and under-representedregions to enhance the global generalizability and fairness of the algorithms . Additionally, we will design a newtask to benchmark interactive segmentation and text-based segmentation methods, reflecting the evolving researchdirections , . This benchmark will assess how effectively algorithms can integrate user feedback through interactiveprocesses and respond to natural language descriptions for segmentation tasks.In conclusion, this competition marked a pivotal step forward in advancing promptable medical image segmentation,introducing a novel task that emphasized efficiency alongside accuracy and a large and diverse testing set from 24 differentinstitutions. The competition attracted over 200 participants worldwide, and the two best-performing algorithms, featuringa transformer-based lightweight image encoder and fast inference implementations, achieved over 10 times faster inferencespeed than the previous foundation model while maintaining comparable or even superior accuracy. The innovative post-challenge phase further validated the reproducibility of top-performing solutions and set a new state-of-the-art throughintegrated strategies, highlighting the importance of transparency and collaboration in driving progress. The two best-performing algorithms have been incorporated into 3D Slicer to bridge the gap between algorithmic innovation andclinical deployment. We believe these efforts will pave the way for more robust, accessible, and impactful medical imagesegmentation technologies, ultimately benefiting global healthcare practices.",
  "Dataset curation and pre-processing": "All the training images were from publicly available datasets with a license for re-distribution (Supplementary -2).The original images have a wide range of format, such as nifti, dicom, mhd, nrrd, jpg, and png. We normalized them tothe same npz format with image and reference standard inside it, allowing participants to get rid of tedious data cleaningand focus on model development.The pre-processing followed common practice , . Specifically, for CT images, we first adjusted the intensity tothe proper window level and width followed by rescaling to . For MRI and PET images, we applied intensitycut-off with the lower-bound and upper-bound of 0.5% and 99.5% percentile of foreground intensity and then rescaled theintensity to . For the 2D modalities, we applied the same intensity preprocessing as MRI if their intensity rangesare not in . Otherwise, no preprocessing was applied. For the reference standard, we converted the lesion semanticmask to instance mask, allowing the generation of lesion-wise bounding box prompts.",
  "Baseline: LiteMedSAM": "We provided an out-of-the-box baseline model, LiteMedSAM, with a detailed tutorial to reduce the entry barriers. Thedevelopment of LiteMedSAM involves a two-stage process: distillation and fine-tuning. In the first stage, we distillthe extensive medical imaging knowledge from MedSAMs heavyweight ViT-B image encoder into Tiny-ViT ,a compact hybrid architecture combining Transformer and convolution layers. In the second stage, we fine-tune the entireLiteMedSAM model.To prepare the input images, for MedSAMs ViT-b encoder, the images are first resized to 1024 x 1024 using bi-cubicinterpolation, while for Tiny-ViT, the longest edge of the input images is first resized to 256 using bilinear interpolation, andthen the images are padded to 256 x 256 with zeros on the right and bottom to maintain a consistent size. After resizing,min-max normalization is applied to scale the intensity values from the range of to . Data augmentation isthen performed, which includes random horizontal and vertical flips with a probability of 0.5. These preprocessing andaugmentation steps are consistent across both the distillation and fine-tuning stages.During the distillation process, MedSAMs image encoder serves as the teacher model, while Tiny-ViT acts as thestudent model. The teacher model remains frozen throughout the distillation. The output feature embeddings of bothimage encoders have a dimensionality of 64 x 64 x 256. We adopt Mean Squared Error (MSE) loss to encourage the featureembeddings of the student to match those of the teacher. The distillation is performed using the AdamW optimizer with a learning rate of 5e-5, a weight decay of 0.01, and a batch size of 8. The process continues until either 1000 epochsare reached or the loss no longer decreases.In the second stage, we plug in the distilled Tiny-ViT as the image encoder and fine-tune the entire LiteMedSAMmodel, including the prompt encoder and mask decoder. During the fine-tuning process, a loss function comprised bythe unweighted sum of the Dice loss and the binary cross-entropy (BCE) loss is adopted. The same AdamW optimizersettings as in the distillation stage are used. For fine-tuning, the input images are resized to 256 x 256 using the sameimage processing steps as in the distillation stage for Tiny-ViT. The fine-tuning process stops after 1000 epochs or when thetraining loss falls below 0.005, whichever occurs first. Both the distillation and fine-tuning of LiteMedSAM are performedon the same dataset used to train MedSAM.",
  "Deployment: 3D Slicer plugin": "Despite the open sourcing of these advanced algorithms, a significant challenge remains in integrating them seamlesslyinto clinical workflows, as this often demands basic coding skills. To address this, we incorporated the two best-performingalgorithms as a plugin for 3D Slicer, an open-source software platform for medical image analysis and three-dimensionalvisualization. The plugin was implemented using the loadable script module type, allowing for the easy integration ofPython scripts and seamless interaction with the Slicer framework.The module provides a user-friendly graphical interface where users can specify preprocessing options and define theregion of interest (ROI) for segmentation. Its architecture is based on a generic abstract class to facilitate the easy integrationof new segmentation models. Importantly, all these models can be executed on laptops without reliance on GPUs.",
  "Evaluation metrics and platform": "The evaluation metrics contained two segmentation accuracy metrics and one efficiency metric. We followed the recommen-dations in Metrics Reloaded to evaluate the segmentation accuracy. Specifically, we used Dice Similarity Coefficient(DSC) and Normalized Surface Distance (NSD) to quantitatively evaluate the region overlap and boundary similarity,respectively. The efficiency was measured by runtime (seconds) and we recorded the Docker container execution timefor each case. The submitted Docker algorithms were evaluated on the same platform with Ubuntu 20.04 system, whichcontains one Intel CPU (Xeon W-2133 3.60GHz) and an upperbound of 8GB RAM. Docker version 20.10.13 The evaluationcode and platform information were released at the beginning of the competition for a transparent evaluation.",
  "Ranking scheme and statistical analysis": "We employed the commonly used rank-then-aggregate scheme to obtain the final rank , , which contains threesteps. First, we computed the three metrics (DSC, NSD, and runtime) for each testing case and each algorithm. Then, weranked the 23 algorithms teams for each modality and each metric. Finally, we averaged all the rankings to obtain thefinal rank. Wilcoxon signed-rank test was used to statistically compare the performance of the algorithms. We also used thebootstrapping (N=1000) to analyze the ranking stability. The analysis was based on the widely used toolkit ChallengeR .",
  "Acknowledgements": "This work was supported by the Natural Sciences and Engineering Research Council of Canada (RGPIN-2020-06189 andDGECR-2020-00294) and CIFAR AI Chair programs. This research was enabled, in part, by computing resources providedby the Digital Research Alliance of Canada. B.L. and D.N. were supported by Vingroup Innovation Foundation (VINIF) inproject code VINIF.2019.DA19. S.Y. was supported by JST SPRING, Japan Grant Number JPMJSP2106. L.P. was supportedby the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) Project-ID 499552394 SFB 1597. L.Z.was supported by Zhejiang Provincial Natural Science Foundation of China under Grant No. LDT23F01015F01. M.A.M.was supported by Else Kroner Research College for young physicians (ref. number: 2023 EKFK.02) N.H. was supportedby DoD HT9425-23-1-0918. Y.T., J.K.U., and C.J.P. were supported by NIH R01HL150147. F.C. was supported by NIHK01HL143113. E.M. was supported by NIH R01HL144678. E.K. was supported by DFG KO 7162/1-1 543939932 and FWF10.55776/COE12. H.Y. was supported by DOD HT9425-23-1-0003. K.K. was supported by Juan de la Cierva fellowshipby the Ministry of Science and Innovation of Spain with reference number FJC2021-047659-I. F.B., L.L., and C.G. weresupported by the University of Modena and Reggio Emilia and Fondazione di Modena, through the FAR 2024 and FARD-2024 funds (Fondo di Ateneo per la Ricerca). We also appreciate all the data owners for providing public medical imagesto the community. B. He, A. C. Kwan, J. H. Cho, N. Yuan, C. Pollick, T. Shiota, J. Ebinger, N. A. Bello, J. Wei, K. Josan, G. Duffy, M. Jujjavarapu, R. Siegel,S. Cheng, J. Y. Zou, and D. Ouyang, Blinded, randomized trial of sonographer versus AI cardiac function assessment, Nature, vol. 616, no.7957, pp. 520524, 2023.K. Cao, Y. Xia, J. Yao, X. Han, L. Lambert, T. Zhang, W. Tang, G. Jin, H. Jiang, X. Fang et al., Large-scale pancreatic cancer detection vianon-contrast ct and deep learning, Nature medicine, vol. 29, no. 12, pp. 30333043, 2023.S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos, Image segmentation using deep learning: A survey, IEEETransactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp. 35233542, 2021.A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, arXivpreprint arXiv:2304.02643, 2023.N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Radle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion,C.-Y. Wu, R. Girshick, P. Dollar, and C. Feichtenhofer, Sam 2: Segment anything in images and videos, arXiv preprint arXiv:2408.00714,2024. [Online]. Available: A. Mazurowski, H. Dong, H. Gu, J. Yang, N. Konz, and Y. Zhang, Segment anything model for medical image analysis: an experimentalstudy, Medical Image Analysis, vol. 89, p. 102918, 2023.Y. Huang, X. Yang, L. Liu, H. Zhou, A. Chang, X. Zhou, R. Chen, J. Yu, J. Chen, C. Chen et al., Segment anything model for medical images?Medical Image Analysis, vol. 92, p. 103061, 2024.X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Gao, and Y. J. Lee, Segment everything everywhere all at once, arXiv preprint arXiv:2304.06718,2023.J. Ma, Y. He, F. Li, L. Han, C. You, and B. Wang, Segment anything in medical images, Nature Communications, vol. 15, p. 654, 2024. T. Zhao, Y. Gu, J. Yang, N. Usuyama, H. H. Lee, S. Kiblawi, T. Naumann, J. Gao, A. Crabtree, J. Abel, C. Moung-Wen, B. Piening, C. Bifulco,M. Wei, H. Poon, and S. Wang, A foundation model for joint segmentation, detection and recognition of biomedical objects across ninemodalities, Nature Methods, 2024. Z. Marinov, P. F. Jager, J. Egger, J. Kleesiek, and R. Stiefelhagen, Deep interactive segmentation of medical images: A systematic review andtaxonomy, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 10 99811 018, 2024. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in Computer Vision andPattern Recognition, 2009, pp. 248255. A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, Advances in NeuralInformation Processing Systems, vol. 25, 2012. J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Zdek, A. Potapenko et al., Highlyaccurate protein structure prediction with alphafold, nature, vol. 596, no. 7873, pp. 583589, 2021. W. Bulten, K. Kartasalo, P.-H. C. Chen, P. Strom, H. Pinckaers, K. Nagpal, Y. Cai, D. F. Steiner, H. van Boven, R. Vink et al., Artificialintelligence for diagnosis and gleason grading of prostate cancer: the panda challenge, Nature Medicine, vol. 28, no. 1, pp. 154163, 2022. J. Ma, Y. Zhang, S. Gu, C. Ge, S. Mae, A. Young, C. Zhu, X. Yang, K. Meng, Z. Huang, F. Zhang, Y. Pan, S. Huang, J. Wang, M. Sun, R. Zhang,D. Jia, J. W. Choi, N. Alves, B. de Wilde, G. Koehler, H. Lai, E. Wang, M. Wiesenfarth, Q. Zhu, G. Dong, J. He, J. He, H. Yang, B. Huang,M. Lyu, Y. Ma, H. Guo, W. Xu, K. Maier-Hein, Y. Wu, and B. Wang, Unleashing the strengths of unlabelled data in deep learning-assistedpan-cancer abdominal organ quantification: the flare22 challenge, The Lancet Digital Health, vol. 6, no. 11, p. e815e826, 2024. P. R. A. S. Bassi, W. Li, Y. Tang, F. Isensee, Z. Wang, J. Chen, Y.-C. Chou, Y. Kirchhoff, M. R. Rokuss, Z. Huang, J. Ye, J. He, T. Wald, C. Ulrich,M. Baumgartner, S. Roy, K. Maier-Hein, P. F. Jaeger, Y. Ye, Y. Xie, J. Zhang, Z. Chen, Y. Xia, Z. Xing, L. Zhu, Y. Sadegheih, A. Bozorgpour,P. Kumari, R. Azad, D. Merhof, P. Shi, T. Ma, Y. Du, F. BAI, T. Huang, B. Zhao, H. Wang, X. Li, H. Gu, H. Dong, J. Yang, M. A. Mazurowski,S. Gupta, L. Wu, J.-X. Zhuang, H. Chen, H. R. Roth, D. Xu, M. B. Blaschko, S. Decherchi, A. Cavalli, A. Yuille, and Z. Zhou, Touchstonebenchmark: Are we on the right way for evaluating AI algorithms for medical segmentation? in The Thirty-eight Conference on NeuralInformation Processing Systems Datasets and Benchmarks Track, 2024. Z. Xu, S. Escalera, A. Pavao, M. Richard, W.-W. Tu, Q. Yao, H. Zhao, and I. Guyon, Codabench: Flexible, easy-to-use, and reproduciblemeta-benchmark platform, Patterns, vol. 3, no. 7, p. 100543, 2022. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., Animage is worth 16x16 words: Transformers for image recognition at scale, in International Conference on Learning Representations, 2020. K. Wu, J. Zhang, H. Peng, M. Liu, B. Xiao, J. Fu, and L. Yuan, Tinyvit: Fast pretraining distillation for small vision transformers, in EuropeanConference on Computer Vision, 2022, pp. 6885. B.-H. Le, D.-K. Nguyen-Vu, T.-H. Nguyen-Mau, H.-D. Nguyen, and M.-T. Tran, Medficientsam: A robust medical segmentation model withoptimized inference pipeline for limited clinical settings, in CVPR 2024: Segment Anything In Medical Images On Laptop, 2024. A. Pfefferle, L. Purucker, and F. Hutter, Daft: Data-aware fine-tuning of foundation models for efficient and effective medical imagesegmentation, in CVPR 2024: Segment Anything In Medical Images On Laptop, 2024. M. Wei, S. Chen, S. Wu, and D. Xu, Rep-medsam: Towards real-time and universal medical image segmentation, in CVPR 2024: SegmentAnything In Medical Images On Laptop, 2024. A. Wang, H. Chen, Z. Lin, J. Han, and G. Ding, Repvit: Revisiting mobile cnn from vit perspective, in Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, 2024, pp. 15 90915 920. Y. Zhang, F. Ye, L. Chen, F. Xu, X. Chen, H. Wu, M. Cao, Y. Li, Y. Wang, and X. Huang, Childrens dental panoramic radiographs dataset forcaries segmentation and dental disease detection, Scientific Data, vol. 10, no. 1, p. 380, 2023. S. Gatidis, M. Fruh, M. P. Fabritius, S. Gu, K. Nikolaou, C. L. Foug`ere, J. Ye, J. He, Y. Peng, L. Bi, J. Ma, B. Wang, J. Zhang, Y. Huang, L. Heiliger,Z. Marinov, R. Stiefelhagen, J. Egger, J. Kleesiek, L. Sibille, L. Xiang, S. Bendazzoli, M. Astaraki, M. Ingrisch, C. C. Cyran, and T. Kustner,Results from the autopet challenge on fully automated lesion segmentation in oncologic pet/ct imaging, Nature Machine Intelligence, vol. 6,no. 11, p. 13961405, 2024. Z. Li, Y. WANG, and S. Wang, Reproducibility analysis: Reproduce the top one team results, in CVPR 2024: Segment Anything In MedicalImages On Laptop, 2024. H. Cai, J. Li, M. Hu, C. Gan, and S. Han, Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction, in Proceedingsof the IEEE/CVF International Conference on Computer Vision, 2023, pp. 17 30217 313. R. Kikinis, S. D. Pieper, and K. G. Vosburgh, 3d slicer: a platform for subject-specific image analysis, visualization, and clinical support, inIntraoperative imaging and image-guided therapy.Springer, 2013, pp. 277289. W. Li, C. Qu, X. Chen, P. R. Bassi, Y. Shi, Y. Lai, Q. Yu, H. Xue, Y. Chen, X. Lin, Y. Tang, Y. Cao, H. Han, Z. Zhang, J. Liu, T. Zhang, Y. Ma,J. Wang, G. Zhang, A. Yuille, and Z. Zhou, Abdomenatlas: A large-scale, detailed-annotated, & multi-center dataset for efficient transferlearning and open algorithmic benchmarking, Medical Image Analysis, vol. 97, p. 103285, 2024. C. Shi, R. Rezai, J. Yang, Q. Dou, and X. Li, A survey on trustworthiness in foundation models for medical image analysis, arXiv preprintarXiv:2407.15851, 2024. Z. Zhao, Y. Zhang, C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, One model to rule them all: Towards universal segmentation formedical images with text prompts, arXiv preprint arXiv:2312.17183, 2023. F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, nnu-net: a self-configuring method for deep learning-based biomedicalimage segmentation, Nature Methods, vol. 18, no. 2, pp. 203211, 2021. I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in International Conference on Learning Representations, 2019. L. Maier-Hein, A. Reinke, P. Godau, M. D. Tizabi, F. Buettner, E. Christodoulou, B. Glocker, F. Isensee, J. Kleesiek, M. Kozubek, M. Reyes, M. A.Riegler, M. Wiesenfarth, A. E. Kavur, C. H. Sudre, M. Baumgartner, M. Eisenmann, D. Heckmann-Notzel, T. Radsch, L. Acion, M. Antonelli,T. Arbel, S. Bakas, A. Benis, M. B. Blaschko, M. J. Cardoso, V. Cheplygina, B. A. Cimini, G. S. Collins, K. Farahani, L. Ferrer, A. Galdran,B. van Ginneken, R. Haase, D. A. Hashimoto, M. M. Hoffman, M. Huisman, P. Jannin, C. E. Kahn, D. Kainmueller, B. Kainz, A. Karargyris,A. Karthikesalingam, F. Kofler, A. Kopp-Schneider, A. Kreshuk, T. Kurc, B. A. Landman, G. Litjens, A. Madani, K. Maier-Hein, A. L. Martel,P. Mattson, E. Meijering, B. Menze, K. G. M. Moons, H. Muller, B. Nichyporuk, F. Nickel, J. Petersen, N. Rajpoot, N. Rieke, J. Saez-Rodriguez,C. I. Sanchez, S. Shetty, M. van Smeden, R. M. Summers, A. A. Taha, A. Tiulpin, S. A. Tsaftaris, B. Van Calster, G. Varoquaux, and P. F. Jager,Metrics reloaded: recommendations for image analysis validation, Nature Methods, vol. 21, no. 2, p. 195212, 2024. S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi et al., Identifying the best machine learning algorithms for brain tumorsegmentation, progression assessment, and overall survival prediction in the brats challenge, arXiv preprint arXiv:1811.02629, 2018. M. Wiesenfarth, A. Reinke, B. A. Landman, M. Eisenmann, L. A. Saiz, M. J. Cardoso et al., Methods and open-source toolkit for analyzingand visualizing challenge results, Scientific Reports, vol. 11, no. 1, pp. 115, 2021."
}