{
  "Abstract": "Generating dances that are both lifelike and well-alignedwith music continues to be a challenging task in the cross-modal domain. This paper introduces PopDanceSet, thefirst dataset tailored to the preferences of young audiences,enabling the generation of aesthetically oriented dances.And it surpasses the AIST++ dataset in music genre di-versity and the intricacy and depth of dance movements.Moreover, the proposed POPDG model within the iD-DPM framework enhances dance diversity and, through theSpace Augmentation Algorithm, strengthens spatial physi-cal connections between human body joints, ensuring thatincreased diversity does not compromise generation qual-ity. A streamlined Alignment Module is also designed toimprove the temporal alignment between dance and mu-sic.Extensive experiments show that POPDG achievesSOTA results on two datasets. Furthermore, the paper alsoexpands on current evaluation metrics. The dataset andcode are available at",
  "*These authors contributed equally.Corresponding author": "task of music-driven dance generation not only helps chore-ographers improve the efficiency of creating innovativedances but also facilitates performances by virtual charac-ters. It even extends to the field of neuroscience, assist-ing researchers in exploring the relationship between humanmovement and music . This task has long been hampered by the scarcity ofpublicly available datasets and the limitations in generativemodel capabilities. As of now, the AIST++ dataset isamong the few with a significant volume of data that is pub-licly accessible. Despite significant advancements in dancegeneration models in recent years, issues such as the com-plexity of training steps, instability in generation, and lackof diversity still exist. This paper introduces the PopDance-Set and the POPDG, aimed at enhancing both the datasetand the model aspects of dance generation. The AIST++ datasets limitations include a lack of aes-thetically oriented dances, a narrow range of dance and mu-sic genres, among others. The dances in this dataset areconfined to 10 subcategories of street dance, which hardlyencompass the vast array of dance styles in reality. The Pop-DanceSet, created through a popularity function designed inthis paper, filters dance videos that align with popular aes-thetics. It represents a significant breakthrough in terms ofaesthetically oriented content, diversity in dance types, mu-sic genres, and dance movements.",
  "arXiv:2405.03178v2 [cs.SD] 27 Dec 2024": "into the physical interconnections between specific joints.Instead, they approached the task holistically or attemptedto learn specific movement patterns . The alignmentbetween dance and music is also crucial. Prior methods ei-ther underestimated this issue or complicated the trainingprocess . Undoubtedly, these issues impact theoverall quality and diversity of generated dances.This paper specifically proposes a space augmentationalgorithm based on Attention Mechanism, forming a dancedecoder block to strengthen the spatial connections amongjoints in dance movements.Furthermore, a streamlinedalignment module is designed to encode the spatiotempo-ral features of music alongside dance, thereby significantlyenhancing their rhythmical synchronization.Finally, in the task of music-driven dance generation, ex-isting evaluation metrics have certain limitations. This pa-per also proposes evaluation metrics that are suited to thistask, thereby enabling a more reasonable assessment of thegenerated dances. In summary, the contributions of this pa-per can be enumerated as follows: We build the PopDanceSet, reflecting contemporary aes-thetic preferences. It significantly enriches the diversityand quantity of dances and music, increases the complex-ity of dance movements, and offers excellent extensibilityfor continuous supplementation. We introduce POPDG(Popular 3D Dance Generation),which is based on iDDPM and achieves a balance be-tween generation quality and diversity. The model paysparticular attention to the spatial features of the dancersbody joints, especially proposing the Space Augmenta-tion Algorithm. In addition, our newly designed Align-ment Module integrates the spatiotemporal features ofmusic and dance, strengthening the alignment betweendance and music. Extensive experiments were conducted in this study. Itwas observed that the POPDG produced the exciting re-sults, both on AIST++ and PopDanceSet. And we alsomake a reasonable extension to the evaluation metrics,making the assessment of dance generation more com-prehensive and objective.",
  ". Music-Dance Dataset": "High-quality dance generation relies on comprehensive anddiverse music-dance datasets.Earlier research primarilyutilized motion capture technology for limited dataset col-lection, as seen in , or leveraged pose es-timation models to derive 2D/3D poses fromonline dance videos. However, due to the complexities indance motion capture and the constraints of earlier poseestimation models, these datasets were limited in dancevariety, duration, and motion capture quality.A signifi- cant advancement was made with AIST++ , an exten-sion of AIST , offering longer durations, precise 3Djoint annotations, and high-quality dance movements, set-ting a new standard in the field. Despite its wide usage,later databases like PMSD, PhantomDance, andMMD provide only incremental advancements, mainlyproviding additional data for specific research tasks withoutmuch broader impact due to limited public availability.",
  ". Human dance generation": "Initially, music-driven dance generation, an autoregressivetask, explored the music-dance relationship using tradi-tional machine learning algorithms , but thesemethods produced dances with limited duration, diversity,and poor adaptability to various melodies and rhythms.The advent of deep learning saw researchers employing CNNs,LSTMs, MLPs, and GCNs to better capture deep features.Despite improved feature extraction and generalizability,generating dances with high diversity remains challenging.With FACTs introduction , Transformers have gainedprominence for their superior temporal feature modeling. Further advancements by Bailandoand EDGE using VQ-VAE, GPT, reinforcement learn-ing, and DDPM have enhanced dance quality and diversitybut at the cost of increased training complexity. The stabil-ity and overall quality of long-sequence dance generationcontinue to need enhancement.",
  ". Diffusion Models": "Diffusion models , a novel class of deep generativemodels, learn data distributions through reverse denois-ing processes.They have recently shown superior gen-erative capabilities in image generation, outperformingbenchmarks in general tasks . Additionally, theiradaptability in conditional generation tasks makes themhighly versatile.Dhariwal and Ho demonstratedtheir effectiveness with guided image generation, optimiz-ing the diversity-fidelity trade-off. Their impressive perfor-mance extends to various fields, including 3D monocularpose estimation and text-driven motion generation.While closely related to human pose and motion generationwith emerging applications in music-driven dance genera-tion , the high standards for quality and diversity in thisdomain mean diffusion models still necessitate further ex-ploration.",
  "PopDanceSet(Ours)1913212819": ". 3D Dance Datasets Comparison. PopDanceSet stands out for its aesthetically oriented content and inclusion of music withcorresponding lyrics. Encompassing a broad range of 19 genres and 132 subjects, it offers high diversity over 12,819 seconds of data,establishing itself as a valuable dataset for dance generation research. oped a popularity function to filter suitable dance videos.We selected BiliBili, the video platform most popularamong young people in China, as our data source. Usingmultiple linear regression and Students t test , we iden-tified the variables that influence video popularity, formu-lated the popularity function, and detailed the verificationprocess in supplementary Sec. 7.",
  "Pop = WN T + b,(1)": "In Eq. (1), we define N as [nfavorites, ndanmucounts,nviews, nlikes, nshares], where each term represents thenumber of favorites, danmu(live comments that scroll overthe video, offering an interactive and communal view-ing experience) counts, views, likes, and shares respec-tively. These are weighted by the coefficient vector W =[0.0251, 0.0095, 0.8033, 0.0967, 0.0243]. Additionally, thebias term b is set to 0.0443. We establish a Pop thresholdof 0.85 for selection criteria. Recognizing the inherent ad-vantage of authors with a larger following, we consider onlythose videos where the view count exceeds the number offollowers of the creator, denoted as nviews > nfollowers.Moreover, we opt to exclude videos with frequent changesin camera angles or excessive shaking, to ensure data con-sistency and quality.",
  ". Dataset Description": "We collected a total of 263 dance videos, containing 180pieces of music. In recent years, monocular 3D joint detec-tion technology based on SMPL has made signifi-cant progress, providing high-quality detection results. Weemployed the HybrIK model to extract the 3D jointfeatures of the dancers in all videos. Each frame of data hasthe following parameters: 24 SMPL pose parameters along with the global scaling,translation and pred scores;",
  "COCO-format human joint locations in 3D;": "The comparison between PoPDanceSet and other datasetsis in Tab. 1. It is readily apparent that PopDanceSet, whilesecond only to the AIST++ in terms of dance duration, hascomprehensively surpassed it in aspects such as dance andmusic genres. The collected dance genres encompass 19categories, including CPOP, KPOP, house dance, amongothers, and we have endeavored to maintain an even dis-tribution of the number of each dance type. Details of thedataset can be refer to supplementary Sec. 7. The musicin this dataset spans a wide range of rhythms and styles,from classical to rock, and most retain lyrics, maintainingconsistency with real-world dance environments. The com-plexity of movements in this dataset also exceeds that of theAIST++, meriting further research in the future.It is particularly noteworthy that the dance data collectedin the PopDanceSet consists of the pose data of humanbody joints for each frame and the position data in three-dimensional space, without any other parameters such asfacial features or body shape. Furthermore, the accompany-ing music for the dances is all publicly available. Therefore,PopDanceSet does not involve any issues of privacy.",
  ". Method": "Our model framework, as illustrated in , is basedon iDDPM(improved-Denoising Diffusion ProbabilisticModels) for sampling with denoising performed byDDIM(Denoising Diffusion Implicit Models).Dur-ing training, the model is fed a sequence of dance posesx RN156spanning a certain number of frames. In linewith most methods, the initial three dimensions representthe single root translation, followed by the 6-DOF (Degreesof Freedom) rotation representation of the 24 joints in theSMPL human body model. The final dimensions are binarycontact labels for the feet, hands, and neck. Therefore, thepose representation is x R156=3+246+9 per frame. Themodel gives equal importance to music and motion. In addi-tion to extracting 4800-dimensional music features throughJukebox like EDGE, it also features a music encoder witha structure symmetrical to that of the dance decoder. . POPDG Pipeline Overview. POPDG, utilizing the iDDPM framework, learns to denoise dance sequences from time t = Tto t = 0. The audio feature sequence serves as the input to the Music Encoder Block, while the noisy sequence is input to the DanceDecoder Block, with the output being the generated dance sequence. And N refers to the stack number. Beginning with a noisy sequencezT N(0, I), POPDG generates the estimated frame of the dance sequence. It then progressively noises the sequence back to zT 1,repeating the process until t = 0.",
  ". Improved-DDPM and DDIM": "Currently, the DDPM framework is employed in both theaction domain and dance generation domain, while iDDPM,in comparison to DDPM, learns not only the mean fromthe data distribution but also takes variance into account,thereby increasing the diversity of generation while ensur-ing quality. In iDDPM, the forward process also adheresto a Markov chain q(zt|x), and we calculate the mean andvariance using the following Eq. (2) and Eq. (3):",
  ". Music and Dance Spatiotemporal block": "As shown in , POPDG comprises two blocks: the mu-sic encoder and the dance decoder. These two blocks, basedon the principle of symmetry and empirical validation,have similar spatiotemporal Transformer modules.Thecore of two blocks lies in four attention mechanisms: MF-Attention (Music Feature-Attention), MT-Attention (Mu-sic Temporal-Attention), DS-Attention (Dance Spatial-Attention) and DT-Attention (Dance Temporal-Attention).",
  "Dance Decoder Block": "The details of the dance decoder block can be found in, which is composed of Transformer based on DS-Attention and DT-Attention. Previous methods primarilyused DT-Attention. The input dance sequence is xmotion.We take the positional encoded xmotion as Q, K andthe original xmotion as V , and pass through the classicattention:",
  "(7)where QKT results in attention map with [N N]. M rep-resents Mask operation. We randomly mask some frames in": ". Analysis of Joint Error Distribution in SMPL Hu-man Body Model. (a) SMPL Joint Labeling: Marks human bodyjoints from the hip (level 0 joint) outward, color-coded by differentlevels. (b) Joint Error Proportions: Shows that upper body jointsexperience increasing error the further they are from the hip. (c)Upper Body Joint Error Levels: Displays average errors across up-per body joint levels. the dance sequence to enhance robustness. It mainly paysattention to the temporal relationship within the input se-quence.In POPDG, we place a spatial attention, DS-Attention, tocapture the spatial connections between human body joints,as illustrated in . In the model, xmotion RbNh,where h represents the hidden feature dimension of thedance posture. We transpose xmotion before feeding it intoDS-Attention, thereby obtaining an attention map focusedon the spatial dimension. The additional Space Augmenta-tion Algorithm within DS-Attention is capable of capturingthe actual spatial connections between the joints.SMPL designates the hip as the root joint, with otherbody joints categorized into levels based on their distancefrom the hip. As shown in (a), joints with the samebackground color are at the same level. Comparing gener-ated dance movements with ground truth data, we observein (b) that joint errors increase with distance from theroot joint. The average error spans from 4% at the root toapproximately 6.5% at the upper body parts like the ribs andneck. (c) displays the average error across differentjoint levels.In the traditional multi-head attention model, attentionweights are calculated based on the similarity betweenqueries and keys. To better capture the spatial relationship . The Overview of Dance Spatial Attention. The keydistinction between dance spatial attention and standard multi-head attention is the incorporation of the Space Augmentation Al-gorithm when calculating the Attention Map between Query andKey. This algorithm is tailored to emphasize the upper body jointsin relation to the hip, enhancing their spatial inter connectivity. between specific joints, we introduced a Space Augmenta-tion Algorithm. This algorithm enhances weights based onthe distance of the joints from the root joint.The physical meaning of the algorithm is to strengthenthe relationship between each joint in the upper body andits parent joint. Specifically, assuming the (i + 1)th joint mis above the ith joint n. It is known that in the calculation ofthe attentionmap R in DS-Attention, we alreadyhave (0, m), (m, 0) and (m, n), (n, m) which represent theconnection weights of joint m with the root joint and jointn, respectively. If we add the value of (0, n) to (0, m) and(m, 0), essentially we enhance the connection between jointm and the root joint. Similarly, we can continuously passinformation from parent joints to downstream-level joints.The specific algorithm implementation can be summarizedby Algorithm 1. From an experimental perspective, it isviable with or without dividing by 2.",
  "Music Encoder Block": "Following the principle of symmetry, we adopted a designfor the music encoder block that mirrors that of the dancedecoder block, and the ablation experiments are displayedin .4. Building on the existing MT-Attention, wetranspose the music data. Unlike dance motions with cleartemporal and spatial definitions, after passing through MF-Attention, the musical feature xmusic will obtain relation-ships between mathematical features such as MFCC andchroma.",
  ". Alignment Module": "The quality of generated dance is also contingent on itscompatibility with the music.Therefore, building uponthe work in .2, we designed a concise alignmentmodule that can enhance the adaptability of dance to musicwhile ensuring the quality of dance generation.Before feeding the dance and music data into our mod-ule, we apply temporal processing to both. Unlike previ-ous methods generally applied spatial position encoding todance sequences, our work equally emphasizes both tempo-ral and spatial characteristics of dance. This involves per-forming a one-dimensional convolution operation on bothsets of features and adding the resultant values to the timestep t in the diffusion model.These combined featuresare then fed through MLP, consistent with DenseFiLM.The detailed model structure is depicted in .",
  "i=1(FK(x(i+1))FK(x(i)))b(i)22": "(9)We adopt the same FK loss function as used in the EDGE.While Lbody upgrades Lcontact, extending its focus fromsolely the feet to include the hands and the neck. FK()denotes the forward kinematic function that converts jointangles into joint positions. b(i) is the models own predic-tion of the binary body contact labels portion of the poseat each frame i.",
  ". Implement Details": "In this study, the generative capabilities of the POPDGmodel are demonstrated on PopDanceSet and AIST++.Initially, to ensure the intrinsic generation quality of thedataset, the construction process included manual checksto confirm the reliability of the extracted dance generationquality. For the PopDanceSet training, there were 736 videosegments utilized as the training set and 24 video segmentsused for testing, ensuring that the dances and music in thetest set had not appeared in the training set. The experimen-tal procedure on the AIST++ mirrored the previous method-ology, with the training and test sets comprising 952 and 40videos, respectively, and generating dance sequences last-ing 25 seconds in duration.The entire experimental process took around 100 hourson two A800 GPUs for the AIST++ and 66 hours for thePopDanceSet. The dance decoder blocks parameter set-tings were similar to those used for 3D pose estimation,with the hidden layer dimension uniformly at 512, and MLPlayers dimension at 1024. These two parameters were con-sistently applied in the music encoder block. DT-Attention,MF-Attention, and MT-Attention all employed the conven-tional 8-head attention mechanism. The optimizer chosenfor the model was Adan, with a learning rate set at 0.001and betas of 0.02, 0.08, and 0.01, with an eps of 1e-8.",
  "Motion Quality": "Researchers commonly employ the FID (Frechet InceptionDistance) metric to assess the motion quality of gen-erated dances. However, experiments often reveal that, de-spite some dances scoring well on FID, they exhibit poorvisual quality. In response, EDGE introduced the PhysicalFoot Contact (PFC) score, denoted by Eq. (11), which as-sesses the plausibility of dance movements directly throughthe acceleration of the hips and the velocity of the feet. Butsince PFC only considers the lower body and dance is a full-body movement, it is also important to consider the upperbody. Therefore, this paper builds upon PFC by includ-ing the neck and hands, extending the evaluation to the fullbody to create the PBC (Physical Body Contact) score.",
  "+f(rchest, rhand, null) + f(neck, head, null)] (12)": "In Eq. (12), the variables ||ajroot||, ||ajlchest||, ||ajrchest||and ||ajneck|| each represent the average acceleration of theroot joint, the left and right chest joints, and the neck jointof the SMPL model, respectively, projected onto the XYZplane for each frame i. Compared to PFC, PBC incorpo-rates a broader consideration of the plausibility of dancemovements. For detailed elaboration, please refer to sup-plementary Sec. 9.",
  ". Comparing to Existing Methods": "As illustrated in Tab. 2, a comparison with existing meth-ods reveals that our experiment demonstrates the superior-ity of the POPDG over previous models on the AIST++and PopDanceSet.Specifically, in the PopDanceSet ex-periment, POPDG outperformed all other methods, achiev-ing the most optimal results.In terms of the PFC andPBC metrics, POPDG surpassed EDGE by 1.6004(26.8%)and 0.4672(7.98%) in motion quality, respectively. More-over, POPDG also excelled in dance generation diversity,as evidenced by its superior performance on the Divk andDivg metrics, where it improved by 1.2576(34.9%) and0.2878(5.02%) compared to EDGE. The enhancement in di-versity can be attributed to the iDDPM, as discussed in Sec-tion 5.4, which augments dance diversity by predicting themean and variance of motion data. Furthermore, in the BASmetric, POPDG also surpassed Bailando, which specificallyemployed reinforcement learning to enhance this aspect.Also demonstrated in .4, the AM strengthens thealignment between music and dance. On the AIST++, al-though POPDG did not exhibit as significant an impact as inthe PopDanceSet, it still surpassed previous works in mostevaluation metrics.",
  ". Ablation Studies": "Modules in POPDG Tab. 3 shows the impact of in-corporating MF-Attention, DS-Attention, and AM onthe quality and music alignment of generated dances.By strengthening the connections between the dancersbody joints, weve improved the quality of the generateddances. Adding MF-Attention has also enhanced the out-comes, considering the models symmetry. While the ex-act relationships between various mathematical featuresof music are still unclear, we believe that POPDG hascaptured their deeper correlations. The newly designedAM has achieved positive results in both dance qualityand alignment, likely due to our modulation of dance us-ing the temporal features of both music and dance. iDDPM The fundamental difference between the DDPMand iDDPM generative frameworks lies in the fact thatwhile DDPM predicts the mean of the generated data, iD-DPM takes into account the variance as well. Theoret-ically, this gives iDDPM a stronger generative capacitycompared to DDPM. For the task of music-driven dancegeneration, there has traditionally been a trade-off be-tween generation quality and diversity, where improve-ments in the quality of generated dances tend to reducediversity. However, the use of iDDPM has allowed usto achieve a favorable balance between generation qual-ity and diversity. This is substantiated by the results pre-sented in Tab. 4.",
  "POPDG0.80146.24197.53743.67070.469": ". Dance Quality Comparison on the PopDanceSet and AIST++ Test Sets. For PopDanceSet, we repused the Bailando and EDGEmodels, and specifically developed a PyTorch version of FACT to generate dances matching POPDG in length. On AIST++, we continuedusing the top three previous models for comparison. POPDG mostly outperformed others in motion quality, diversity, and music-dancealignment on both datasets. Notably, the high Divg scores of FACT are skewed by excessive, meaningless swaying in both datasets. Inour metrics, signifies that higher values indicate better performance, implies the opposite, and represents that values closer to theGround Truth are better.",
  ". User Study": "Our user study involved twenty participants. All partici-pants in the user study were between the ages of 23 and25. We first trained POPDG on PopDanceSet and AIST++,then selected ten segments of wild music as input, and pro-vided the model-generated dance pairings for participantsto evaluate. Participants chose which dance segment theyfound more appealing.As Tab. 5 indicates, the major-ity(70%) preferred the dances from PopDanceSet, demon-",
  ". Conclusion and Discussion": "In this study, we introduce PopDanceSet to enrich data inthe field, increase the complexity of dance movements, andreflect contemporary aesthetics. The POPDG model intro-duced in this paper enhances the connectivity among thedancers body parts through DS-Attention and improves thealignment between the generated dance and music usingAM. Its iDDPM framework maintains a balance betweendance quality and diversity, achieving great results on bothPopDanceSet and AIST++. While the model is trained end-to-end, the training cost remains relatively high. Future re-search should explore strategies to balance the diversity andquality of generated dances using more lightweight models.Additionally, developing metrics that can objectively eval-uate dance quality is also crucial for the task. And we alsobelieve that a deeper study of music deserves our continuedeffort.",
  "Steven Brown and Lawrence M Parsons. The neuroscienceof dance. Scientific American, 299(1):7883, 2008. 1": "Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.Realtime multi-person 2d pose estimation using part affinityfields. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 72917299, 2017. 2 Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-Chen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster:choreography-oriented music-driven dance synthesis. ACMTransactions on Graphics (TOG), 40(4):113, 2021. 2",
  "Rukun Fan, Songhua Xu, and Weidong Geng.Example-based automatic music-driven conventional dance motionsynthesis. IEEE transactions on visualization and computergraphics, 18(3):501515, 2011. 2": "Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, HaoyiZhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu.Alpha-pose: Whole-body regional multi-person pose estimationand tracking in real-time.IEEE Transactions on PatternAnalysis and Machine Intelligence, 2022. 2 Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hos-sein Rahmani, and Jun Liu. Diffpose: Toward more reliable3d pose estimation. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1304113051, 2023. 2",
  "Minho Lee, Kyogu Lee, and Jaeheung Park.Musicsimilarity-based approach to generating dance motion se-quence.Multimedia tools and applications, 62:895912,2013. 2": "Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Dance-former: Music conditioned 3d dance generation with para-metric motion transformer. In Proceedings of the AAAI Con-ference on Artificial Intelligence, pages 12721279, 2022. 2 Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,and Cewu Lu. Hybrik: A hybrid analytical-neural inversekinematics solution for 3d human pose and shape estimation.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 33833393, 2021. 3",
  "Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang,and Cewu Lu. Hybrik-x: Hybrid analytical-neural inversekinematics for whole-body mesh recovery. arXiv preprintarXiv:2304.05690, 2023. 3": "Ruilong Li, Shan Yang, David A Ross, and AngjooKanazawa. Ai choreographer: Music conditioned 3d dancegeneration with aist++. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1340113412, 2021. 1, 2, 3 Weipeng Li, Boyuan Ren, Haoyue Xu, Shiyuan Cao, andYuyangsong Xie. Autodance: Music driven dance genera-tion. In 2021 International Symposium on Artificial Intelli-gence and its Application on Media (ISAIAM), pages 5559.IEEE, 2021. 2 Matthew Loper, Naureen Mahmood, Javier Romero, GerardPons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. In Seminal Graphics Papers: Pushingthe Boundaries, Volume 2, pages 851866. 2023. 3 Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh,Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversitymetrics for generative models. In International Conferenceon Machine Learning, pages 71767185. PMLR, 2020. 7",
  "Ferda Ofli, Engin Erzin, Yucel Yemez, and A Murat Tekalp.Learn2dance: Learning statistical music-to-dance mappingsfor choreography synthesis. IEEE Transactions on Multime-dia, 14(3):747759, 2011. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 6 Xuanchi Ren, Haoran Li, Zijian Huang, and Qifeng Chen.Self-supervised dance video synthesis conditioned on music.In Proceedings of the 28th ACM International Conference onMultimedia, pages 4654, 2020. 2 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 2 Matteo Ruggero Ronchi and Pietro Perona. Benchmarkingand error diagnosis in multi-instance pose estimation. In Pro-ceedings of the IEEE international conference on computervision, pages 369378, 2017. 3",
  "Arindam Sengupta, Feng Jin, Renyuan Zhang, and SiyangCao. mm-pose: Real-time human skeletal posture estimationusing mmwave radars and cnns. IEEE Sensors Journal, 20(17):1003210044, 2020. 2": "NaoShikanai,WorawatChoensawat,andKozaburoHachimura.Movement characteristics of entire bodies indancers interaction. In 2014 14th International Conferenceon Control, Automation and Systems (ICCAS 2014), pages13571361. IEEE, 2014. 7 Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando:3d dance generation by actor-critic gpt with choreographicmemory. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1105011059, 2022. 2 Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando++:3d dance gpt with choreographic memory. IEEE Transac-tions on Pattern Analysis and Machine Intelligence, 2023. 2",
  "JiamingSong,ChenlinMeng,andStefanoErmon.Denoising diffusion implicit models.arXiv preprintarXiv:2010.02502, 2020. 3": "Guofei Sun, Yongkang Wong, Zhiyong Cheng, Mohan SKankanhalli, Weidong Geng, and Xiangdong Li. Deepdance:music-to-dance motion choreography with adversarial learn-ing. IEEE Transactions on Multimedia, 23:497509, 2020.2, 3 Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody:An lstm-autoencoder approach to music-oriented dance syn-thesis. In Proceedings of the 26th ACM international confer-ence on Multimedia, pages 15981606, 2018. 2, 3",
  "Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:Editable dance generation from music. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 448458, 2023. 2": "Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki,and Masataka Goto. Aist dance video database: Multi-genre,multi-dancer, and multi-camera database for dance informa-tion processing. In ISMIR, page 6, 2019. 2 Guillermo Valle-Perez, Gustav Eje Henter, Jonas Beskow,Andre Holzapfel, Pierre-Yves Oudeyer, and Simon Alexan-derson. Transflower: probabilistic autoregressive dance gen-eration with multimodal attention.ACM Transactions onGraphics (TOG), 40(6):114, 2021. 2 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 4 Shuang Wu, Zhenguang Liu, Shijian Lu, and Li Cheng. Duallearning music composition and dance choreography. In Pro-ceedings of the 29th ACM International Conference on Mul-timedia, pages 37463754, 2021. 2",
  "Shuang Wu,Shijian Lu,and Li Cheng.Music-to-dance generation with optimal transport.arXiv preprintarXiv:2112.01806, 2021. 2": "Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, FanboMeng, and Yanfeng Wang. Choreonet: Towards music todance synthesis with choreographic action unit. In Proceed-ings of the 28th ACM International Conference on Multime-dia, pages 744752, 2020. 2 Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Jun-song Yuan. Mixste: Seq2seq mixed spatio-temporal encoderfor 3d human pose estimation in video. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1323213242, 2022. 3 Mingyuan Zhang, Zhongang Cai, Liang Pan, FangzhouHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-fuse: Text-driven human motion generation with diffusionmodel. arXiv preprint arXiv:2208.15001, 2022. 2",
  ". PopDanceSet Construction Details": "The POPDataset was established in September 2022, withthe dance videos primarily spanning from September 15,2021, to September 15, 2022. In the data preprocessingphase, we initially randomly selected 100 dance videosfrom BiliBilis dance section and collected data on the num-ber of coins, favorites, danmus, comments, views, likes, andshares. We visualized the view count data as in (a).From (a), it is evident that top popular videoshave view counts several orders of magnitude higher thanaverage popular videos, and the same happens to other fac-tors. Therefore, direct linear normalization is not suitable inthis case. Instead, we employ non-linear normalization (lognormalization) for preprocessing the data of the videos, asshown in (b).The core of this experiment in selecting popular dancevideos lies in constructing a popularity function. BiliBilisrecommendation algorithm for dance videos is given byEq. (14).",
  "nviews(14)": "where W= [1.2, 0.9, 1.2, 1.2, 0.75, 1.2, 1.8] and N=[ncoins, nfavorites, ndanmucounts, ncomments, nviews,nlikes, nshares]T . This formula indicates that the recom-mendation function considers multiple factors of a video,not just its view count. The function calculates the growthof these facotrs within a specific time frame, with aRecommendation value greater than 1 significantly in-creasing the likelihood of the video being recommended onthe homepage. Our popularity function was built upon thisbasis. By omitting the denominator in the formula, we ob-",
  "bias0.0420.046ncoins-0.0020.008nfavorites0.0190.030ndanmucounts0.0040.011ncomments-0.0020.008nviews0.7980.814nlikes0.0860.098nshares0.0190.027": "Note: This table presents the lower and upper bounds of variableestimates resulting from a multiple linear regression analysis fol-lowed by a T-test. The bounds signify the expected range of valuesfor each variable. Thus, we eliminated the number of coins and commentsfrom the model and, after another round of multiple lin-ear regression and t-test, obtain the formula presented asEq. (1).Following the selection measures described, we ulti-mately filtered out 263 (around 10% of all collected data)dance videos with a POP greater than 0.85 from a yearsspan of videos. We also edited these videos into 760 clipsfeaturing relatively high-quality dance content, distributedas Tab. 7:",
  "The whole loss is shown as Eq. (10). And in Sec. 4.4 wehave already shown velocity and acceleration loss and body": ". Comparison of visual effects between PopDanceSet and AIST++. (a) shows the dance generation results from PopDanceSet,and (b) shows those from AIST++. Both are comparisons of dance postures at the same frame every second under the same backgroundmusic. Compared to dances generated based on AIST++, PopDanceSet undoubtedly exhibits richer and more captivating movements.",
  ". PBC and PFC": "The EDGE constructs the PFC (Physical Foot Contact)evaluation metric based on the following two assumptions: On the horizontal (xy) plane, any center of mass (COM)acceleration must be due to static contact between the feetand the ground. Therefore, either at least one foot is sta-tionary on the ground or the COM is not accelerating.",
  "In the SMPL human body model, the COM (Center ofMass) is represented by the 0th joint at the hip, which isalso the root joint in Eq. (12). The essence of these two": "assumptions is that if the bodys root joint has accelerationin any direction on the XYZ plane, it means at least onefoot must be firmly planted on the ground, as it requiresforce to initiate movement. Since at least one foot is on theground, the velocity of that foot should be zero. Thus, thecore of PFC is to measure the extent of implausible move-ments where the bodys root joint is accelerating while bothfeet are moving (i.e., both have velocity). However, this cal-culation only considers the plausibility of lower body dancemovements and overlooks the analysis of upper body move-ments plausibility, such as the arms, head and neck. For in-stance, if the generated dance involves minimal lower bodymovement but excessive upper body swaying, it would bedeemed highly plausible under the PFC metric. Therefore,theres a significant need to also take the upper body intoconsideration. In dance, although the upper body movements are rela-tively independent, we can still observe constraints similarto those between the root joint and the feet within the up-per body joints. As illustrated in Eq. (12), whether the leftand right chest (referred to as the left and right inshoulderin the SMPL model) and neck joints (i.e., joints 12, 13, and14 in (a)) accelerate during a dance largely dependson whether the hands and head are moving, that is, whetherthey have velocity. Specifically, the movements of the handsand head do not necessarily cause movements in the left andright chest and neck joints, but if the latter do move, it gen-erally indicates that the hands and head have also changedposition, thus possessing velocity. Unlike Eq. (17), whichcalculates the irrationality of movements, Eq. (12) adds acalculation for the rationality of movements. Therefore, inPBC, the value of the original PFC needs to be negated,",
  ". Visual Effects Comparison": "showcases a comparison of typical dance clips fromPopDanceSet and AIST++. As outlined in Sec. 5.5, com-paring dances generated from the same model trained ondifferent datasets under the background of the same wildmusic allows for a clearer distinction of which datasetsdances are more appealing. From , its evident thatdances generated from PopDanceSet are noticeably moreengaging.In contrast, dances from AIST++ tend to bemore rigid, with several seconds of movement being merelyslight adjustments of a single pose. Clearly, the diversity ofmovements from PopDanceSet, especially in the arm parts,makes these dances more captivating. The only drawbackis that the AIST++, with its collection of human keypointsdata from nine camera angles, offers somewhat greater sta-bility in the dancers center of mass compared to PopDance-Set."
}