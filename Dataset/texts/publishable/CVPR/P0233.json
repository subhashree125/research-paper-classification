{
  "Abstract": "Label noise, commonly found in real-world datasets, hasa detrimental impact on a models generalization. To effec-tively detect incorrectly labeled instances, previous workshave mostly relied on distinguishable training signals, suchas training loss, as indicators to differentiate between cleanand noisy labels. However, they have limitations in that thetraining signals incompletely reveal the models behaviorand are not effectively generalized to various noise types,resulting in limited detection accuracy. In this paper, wepropose DynaCor framework that distinguishes incorrectlylabeled instances from correctly labeled ones based on thedynamics of the training signals. To cope with the absenceof supervision for clean and noisy labels, DynaCor first in-troduces a label corruption strategy that augments the orig-inal dataset with intentionally corrupted labels, enablingindirect simulation of the models behavior on noisy labels.Then, DynaCor learns to identify clean and noisy instancesby inducing two clearly distinguishable clusters from thelatent representations of training dynamics. Our compre-hensive experiments show that DynaCor outperforms thestate-of-the-art competitors and shows strong robustness tovarious noise types and noise rates.",
  ". Introduction": "The remarkable success of deep neural networks (DNNs)is largely attributed to massive and accurately labeleddatasets. However, creating such datasets is not only ex-pensive but also time-consuming. As a cost-effective alter-native, various methods have been employed for label col-lection, such as crowdsourcing and extracting imagelabels from accompanying text on the web . Un-fortunately, these approaches have led to the emergence ofnoise in real-world datasets, with reported noise rates rang-ing from 8.0% to 38.5% , which severely de-grades the models performance .",
  "*Corresponding authors": "To cope with the detrimental effect of such noisy la-bels, a variety of approaches have been proposed, includ-ing noise robust learning that minimizes the impact of in-accurate information from noisy labels during the trainingprocess and data re-annotation through al-gorithmic methods . Among them, the task ofnoisy label detection, which our work mainly focuses on,aims to identify incorrectly labeled instances in a trainingdataset . This task has gained much attention inthat it can be further utilized for improving the quality of theoriginal dataset via cleansing or rectifying such instances.Motivated by the memorization effect, which refers to thephenomenon where DNNs initially grasp simple and gen-eralized patterns in correctly labeled data and then grad-ually overfit to incorrectly labeled data , most existingstudies have utilized distinguishable training signals as in-dicators of label quality to differentiate between clean andnoisy labels. To elaborate, these training signals are derivedfrom the models behavior on individual instances duringthe training , involving factors such as training lossor confidence scores. Note that it is impractical to acquireannotations explicitly indicating whether each instance iscorrectly labeled or not.Hence, numerous studies havecrafted various heuristic training signals , de-signed based on human prior knowledge of the models dis-tinctive behaviors when faced with clean and noisy labels.Despite their effectiveness, the training signal-based de-tection methods still exhibit several limitations: (1) Theyonly focus on a scalar signal at a single epoch (or a repre-sentative one across the entire training trajectory), whichleads to limited detection accuracy (See Appendix B.2).Since the models distinct behaviors on clean and noisy la-bels draw different temporal trajectories of training signals,a single scalar is insufficient to distinguish them by cap-turing temporal patterns within training dynamics. (2) Ex-isting detection approaches based on heuristics are not ef-fectively generalized to various types of label noise. Noisylabels can originate from diverse sources, including humanannotator errors , systematic biases , and un-reliable annotations from web crawling , resulting in",
  "arXiv:2405.19902v1 [cs.LG] 30 May 2024": "different noise types and rates for each dataset; this even-tually requires considerable efforts to tune hyperparametersfor training recipes of DNNs .To tackle these challenges, our goal is to propose a fullydata-driven approach that directly learns to distinguish thetraining dynamics of noisy labels from those of clean la-bels using a given dataset without solely relying on heuris-tics. The primary technical challenge in this data-drivenapproach arises from the absence of supervision for cleanand noisy labels. As a solution, we introduce a label cor-ruption strategyimage augmentation attaching intention-ally corrupted labels via random label replacement. Sincethe augmented instances are highly likely to have incorrectlabels, we can utilize them to capture the training dynamicsof noisy labels. In other words, this allows us to simulatethe models behavior on noisy labels by leveraging the aug-mented instances with corrupted labels.In this work, we present a novel framework, namedDynaCor, that learns discriminative Dynamics with labelCorruption for noisy label detection. To be specific, Dyna-Cor identifies clean and noisy labels via clustering of latentrepresentations of training dynamics. To this end, it firstgenerates training dynamics of original instances and cor-rupted instances. Then, it computes the dynamics represen-tations that encode discriminative patterns within the train-ing trajectories by using a parametric dynamics encoder.The dynamics encoder is optimized to induce two clearlydistinguishable clusters (i.e., each for clean and noisy in-stances) based on two different types of losses for (1) highcluster cohesion and (2) cluster alignment between originaland corrupted instances. Furthermore, DynaCor adopts asimple validation metric for the dynamics encoder based onthe clustering quality so as to indirectly estimate its detec-tion performance where ground-truth annotations of cleanand noisy labels are not available for validation as well.The contribution of this work is threefold as follows: We introduce a label corruption strategy that augmentsthe original data with corrupted labels, which are highlylikely to be noisy, enabling indirect simulation of themodels behavior on noisy labels during the training.",
  ". Related Work": "We provide a brief overview of the two primary researchdirections for addressing incorrectly labeled instances in anoisy dataset: (1) Noisy label detection focuses on identi-fying instances that are incorrectly labeled within a dataset, aiming to enhance data quality. (2) Noise robust learningis centered on developing learning algorithms and modelsthat are resilient to the impact of noisy labels, ensuring ro-bust performance even in the presence of labeling errors. Noisy label detection.The main challenge in detect-ing noisy labels lies in defining a surrogate metric for la-bel quality, essentially indicating how likely an instance iscorrectly labeled. The widely adopted option is the train-ing loss, assessing the disparity between the model predic-tion and given labels , with higher loss oftenindicating incorrect labels.Various proxy measures, in-cluding gradient-based values and prediction-basedmetrics have been developed to differenti-ate between clean and noisy labels, utilizing methods likeGaussian mixture models or manually de-signed thresholds . However, these ap-proaches may overlook the potential benefits of adopting adata-driven (or learning-centric) detection model , whichcan be easily generalized to various noise types and lev-els. As a training-free alternative, a recent study intro-duces a non-parametric KNN-based approach based on theassumption that instances situated closely in the input fea-ture spaces derived from a pre-trained model are more likelyto share the same clean label. However, its efficacy in detec-tion heavily depends on the quality of the pre-trained modeland may not be universally applicable across domains withspecific fine-grained visual features. Noise robust learning.Extensive research have focusedon creating noise robust methods: loss functions ,regularization , model architectures , and training strategies . Re-cent studies have endeavored to integrate the process ofdetecting noisy labels and appropriately addressing theminto the training pipeline in various ways: re-weightinglosses or re-annotation . Besides,several studies treat detected noisy labelsas unlabeled and make use of established semi-supervisedtechniques . Current robust learning typi-cally relies on clean data, i.e., test data, for validation, whilenoisy detection methods can function without it, making di-rect comparisons difficult . In this sense, we will dis-cuss how these noise robust learning approaches can be ef-fectively combined with noisy detection methods (Sec. 5.5).",
  ". Problem Formulation": "For multi-class classification, let X be an input featurespace and Y = {1, 2, .., C} be a label space. Consider adataset D = {(xn, yn)}Nn=1, where each sample is inde-pendently drawn from an unknown joint distribution overX Y. In real-world scenarios, we can only access a noisilylabeled training set D = {(xn, yn)}Nn=1, where y denotes anoisy annotation, and there may exist n {1, ..., N} such . The proposed DynaCor framework consists of three steps: (1) Corrupted dataset construction generates the augmented imageswith corrupted labels, likely resulting in noisy labels, in order to provide guidance for discrimination between clean and noisy labels. (2)Training dynamics generation collects the trajectory of training signals for both the original and corrupted datasets by training a classifier.(3) Noisy label detection is performed by discovering two distinguishable clusters of dynamics representations, and for this, the dynamicsencoder is optimized to enhance both cluster cohesion and alignment between the original and the corrupted datasets. that yn = yn. In this work, we focus on the task of noisy la-bel detection, which aims to identify the incorrectly labeledinstances, i.e., {(xn, yn) D | yn = yn}. As an evaluationmetric, we use F1 score , treating the incorrectly labeledinstances as positive and the remainings as negative.",
  ". Overview": "DynaCor (Dynamics learning with label Corruption fornoisy label detection) framework learns discriminative pat-terns inherent in training dynamics, thereby distinguishingincorrectly labeled instances from clean ones. As illustratedin , DynaCor consists of three major steps. Corrupted dataset construction (Sec. 4.2): To address thechallenge arising from the lack of supervision for incor-rectly labeled instances, we introduce a corrupted datasetthat intentionally corrupts labels, providing guidance toidentify incorrectly labeled instances. Training dynamics generation (Sec. 4.3): We generatetraining dynamics, which denote a models behavior onindividual instances during training, by training a classi-fier using both the original and the corrupted dataset. Noisy label detection via dynamics clustering (Sec. 4.4):We seek to discover underlying patterns in the trainingdynamics by learning representations that reflect the in-trinsic similarities among data points, leveraging the char-acteristics of the corrupted dataset. For this, we encodethe training dynamics via a dynamics encoder that learnsdiscriminative representation using clustering and align-ment losses. Then we find clusters using a robust valida-tion metric designed for dynamics-based clustering.",
  ". Corrupted dataset construction": "Given the original dataset D, we construct a corrupteddataset D by intentionally corrupting labels for a randomlysampled subset of D with a corruption rate (0, 1].Specifically, to obtain a corrupted instance (x, y) from anoriginal data instance (x, y), we transform an input imageusing weak augmentation such as horizontal flip or centercrop, i.e., x = Aug(x). Then, we randomly flip the class la-bel to one of the other classes, i.e., y {1, ..., C}\\{y}. Thecorrupted dataset, guaranteed to exhibit symmetric noise ata higher rate than the original, provides additional signalsfor discerning incorrectly labeled instances in the clusteringprocess, as detailed in the following analysis. Analysis: the noise rate of the corrupted dataset.Weanalyze the lower bound on the noise rate of the cor-rupted dataset D.Let denote the noise rateof the original dataset D.1Following the previous liter-ature , we presume the diagonally dominantcondition, i.e., Pr(y = i|y = i) > Pr(y = j|y =i), i = j, which indicates that correct labels should notbe overwhelmed by the false ones. With this condition of < 1 1",
  "Training dynamics": "The training dynamics indicates a models behavior on indi-vidual instances during the training, quantitatively describ-ing the training process . Concretely, the trainingdynamics is defined as the trajectory of training signals de-rived from a models output across the training epochs. Inthe literature, various types of training signals have been employed for analyzing the models behavior.Given a classifier f, let f(x) RC denote the outputlogits of an instance x for C classes. Let t be a transfor-mation function that maps C logits to a scalar training sig-nal. In this paper, we use quantized logit difference as thetraining signal.2 It quantizes the difference between a logit of a given label and the largest logit among the remain-ing classes, i.e., t(f(x), y) = sign(fy(x)maxc=y fc(x)),where fc(x) denotes the logit for class c, and sign(x) = 1or -1 if x >= 0 or < 0, respectively. The training dynamicsfor an instance x is defined as",
  "(x,y) Dce (f(x), y), (2)": "where ce is the softmax cross-entropy loss. For each in-stance x, we obtain a training dynamics tx RE as spec-ified in Eq. (1) by tracking t(e)xover the course of trainingepochs E. Training dynamics of the original and the cor-rupted datasets are denoted by T := {tx|(x, y) D} andT := {tx|(x, y) D}, respectively.",
  "We provide a detailed analysis of various training signals for identify-ing incorrectly labeled instances in Appendix B.3": "in the representation space. The dynamics clustering iter-ates two key processes: (1) identifications of incorrectly la-beled instances (Sec. 4.4.1), and (2) learning distinct repre-sentations for each cluster (Sec. 4.4.2). The clustering qual-ity is assessed by a newly introduced validation metric byleveraging the corrupted dataset without a clean validationdataset (Sec. 4.4.3).",
  "Identification of incorrectly labeled instances": "Cluster initialization.Given a training dynamics tx, adynamics encoder generates its representation, i.e., zx =Enc(tx) Rdz. Let Z and Z denote the set of dynamicsrepresentations of the original and the corrupted datasets,respectively.We first introduce trainable parameters forcentroids of noisy and clean clusters, i.e., noisy, clean Rdz. We initialize noisy as the average representation ofthe corrupted instances Z, while clean is initialized as theaverage representation of the original instances Z. Note thatthis initialization is conducted only once at the beginning ofthe dynamics clustering step. Noisy label identification.We determine whether eachinstance x has been incorrectly labeled based on its assign-ment probability to the noisy cluster. The assignment prob-ability is computed based on the similarity between zx andthe noisy clusters centroid noisy. We employ a kernelfunction based on the Students t-distribution with onedegree of freedom as follows:",
  "Learning discriminative patterns in dynamics": "We introduce the strategy of inducing two distinguish-able clusters (each for correctly and incorrectly labeled in-stances) in the dynamics representation space. We proposetwo types of losses for (1) high cluster cohesion and (2)cluster alignment between original and corrupted instances. Clustering loss.We introduce a clustering loss to makethe clusters more distinguishable. We enhance cluster cohe-sion by adjusting each instances representation to be closerto a centroid through a self-enhancing target distribution.",
  "zx Z ZKL(p(zx)||q(zx)).(6)": "Alignment loss.We introduce an alignment loss thataligns the representation from each clusters original andcorrupted datasets. We hypothesize3 that symmetric noiseis relatively easy to identify among various noise types withdiverse difficulty levels. Consequently, incorrectly labeledinstances in the corrupted dataset exhibit more distinctivedynamics patterns than those in the original data, i.e., a reddashed line is farther away from blue lines than a red line inthe 3rd step of (left). From this perspective, the mis-matched noise types between the original and the corrupteddatasets positively impact the clustering process by adopt-ing alignment loss, which forces a red line to be alignedwith a red dashed line in the 3rd step of (right).Instances in the original dataset predicted as noisy andclean are denoted by Znoisy = {zx Z|v(zx) = 1}and Zclean = {zx Z|v(zx) = 0}, respectively. Analo-gously, for the corrupted dataset, we obtain Znoisy = {zx Z|v(zx) = 1} and Zclean = {zx Z|v(zx) = 0}. Then,we employ the alignment loss to reduce the discrepancy be-tween the representations of the original dataset and the cor-rupted dataset as follows:",
  "Validation metric": "One practical challenge in training the dynamics encoder isdetermining an appropriate stopping point in the absence ofground-truth annotations of clean and noisy labels for vali-dation. As a solution, we introduce a new validation metricfor the dynamics encoder to estimate its detection perfor-mance indirectly. For noisy label detection, we aim to max-imize (a) the assignment of incorrectly labeled instances tothe noisy cluster while minimizing (b) the assignment ofcorrectly labeled instances to the noisy cluster. Intuitively,in an ideally clustered space, the difference between (a) and(b) needs to be maximized.Since we cannot access the ground-truth annotations tocompute (a) and (b), we use the most representative in-stances as a workaround. Considering the corrupted datasethas a higher noise rate than the original dataset, we emulate(a) using instances predicted as noisy among the corrupteddataset, i.e., Znoisy. Similarly, (b) is emulated using in-stances predicted as clean among the original dataset witha lower noise rate, i.e., Zclean. Our validation metric is de-fined as the difference between two emulated values as",
  ". Experiment setup": "Datasets.We evaluate the performance of DynaCor onbenchmark datasets with different types of label noise, orig-inating from diverse sources: (1) synthetic noise on CIFAR-10 and CIFAR-100 , (2) real-world human noise onCIFAR-10N and CIFAR-100N , and (3) systematicnoise4 on Clothing1M . In the case of synthetic noise,following the previous experimental setup , we artifi-cially introduce the noise by using different strategies withspecific noise rates as outlined below. Symmetric Noise (Sym., = 0.6) randomly replaces thelabel with one of the other classes.",
  "Noise typeSym.Asym.Inst.Agg.WorstSym.Asym.Inst.HumanAvg.Noise rate ()0.60.30.40.090.40.60.30.40.4": "Avg.Encoder98.0 0.0389.7 0.1422.4 33.567.3 0.4292.8 0.1196.7 0.0774.9 0.1776.8 0.5179.5 0.3177.6AUM95.7 0.0786.5 0.1881.9 0.7274.0 0.1688.7 0.1996.4 0.1074.7 0.2181.2 0.2574.6 1.2583.7CL96.6 0.0494.0 0.1082.0 0.2168.6 0.3388.3 0.1188.0 0.0868.6 0.1675.9 0.1271.9 0.1081.5CORES97.7 0.035.00 0.3319.2 0.1080.5 0.0977.5 0.0983.9 0.2021.9 0.3236.7 0.4136.0 0.1250.9SIMIFEAT-V95.1 0.0689.4 0.0888.1 0.1179.6 0.1391.6 0.0686.0 0.0973.8 0.0780.5 0.0977.1 0.1284.6SIMIFEAT-R96.1 1.4188.9 0.1491.2 0.0779.6 0.4091.7 0.3590.3 0.0768.0 0.1077.3 0.0979.3 0.1184.7",
  "DynaCor98.0 0.0494.0 0.1592.3 0.3879.6 0.3792.3 0.1994.3 0.3476.3 0.2381.7 0.2180.4 0.1787.7": ". Average F1 score (%) along with standard deviation across ten independent runs of DynaCor and baseline methods on CIFAR-10and CIFAR-100. All methods except SIMIFEAT utilize the identical fixed image encoder from CLIP and train only a subsequentMLP, while SIMIFEAT uses pre-trained CLIP as a feature extractor. The rightmost column averages the F1 scores across nine differentsettings. Agg., Worst, and Human correspond to the real-world human label noises . The best results are in bold.",
  "Noise typeSym.Asym.Inst.Agg.WorstSym.Asym.Inst.HumanAvg": "Avg.Encoder94.1 0.1485.4 0.1988.5 0.2063.6 0.7287.6 0.1892.5 0.3475.2 0.3676.0 0.4978.8 0.1882.4AUM75.4 0.2246.4 0.3057.7 0.0316.7 0.0157.8 0.0475.8 0.2146.7 0.3257.8 0.1058.0 0.2154.7CL88.7 0.5691.9 0.1282.5 0.3757.0 0.3180.0 0.3277.9 0.3962.4 0.2467.3 0.2865.2 0.1974.8CORES92.9 0.1726.7 0.4449.2 1.1563.6 0.5874.7 0.3666.3 0.3533.8 0.4639.2 0.4531.9 0.4853.2SIMIFEAT-V94.6 0.0684.7 0.1783.7 0.0869.4 0.1788.3 0.0888.0 0.0970.3 0.1477.8 0.1076.2 0.1481.4SIMIFEAT-R92.9 1.8484.0 0.1386.9 0.0868.8 0.3288.5 0.3689.7 0.0766.2 0.1175.5 0.0877.8 0.1381.2",
  "DynaCor93.6 0.1894.2 0.4591.5 0.3172.6 2.4687.8 0.3791.3 0.4679.2 0.5979.5 1.1477.3 0.5485.2": ". Average F1 score (%) under identical settings to those in except for the backbone model. All methods except SIMIFEATutilize a randomly initialized Renset34 , while SIMIFEAT uses a pre-trained ResNet34 on ImageNet as a feature extractor. In the case of human noise, we choose two noise subtypesfor CIFAR-10N (denoted by Agg. and Worst) and a singlenoise subtype for CIFAR-100N (denoted by Human). Moredetails of the datasets are presented in Appendix A.1.",
  ". Noisy label detection performance": "We first evaluate DynaCor and the baseline methods fornoisy label detection. and present theirdetection F1 scores for two classifiers, CLIP w/ MLP andResNet34, across various noise types and rates. Notably,DynaCor achieves the best performance on average, i.e.,+3.0% in and +2.8% in , demonstratingits robustness to various types of noisy conditions. On theother hand, the baseline methods relying on training signals(i.e., Avg.Encoder, AUM, CL, and CORES) show consider-able variations in performance across different noise types.For example, in the case of CIFAR-10, Avg.Encoder andCORES perform well for symmetric noises, whereas theystruggle with identifying asymmetric or instance noises. Itis worth noting that asymmetric and instance noise are morecomplex than symmetric noise in that they can have a moredetrimental impact on model performance . These re-sults strongly support the superiority of our DynaCor frame-work in handling a wide range of label noise variations.",
  ". Effectiveness of validation metric": "To demonstrate the effectiveness of the proposed validationmetric (Sec.4.4.3), we compare the detection performanceof our dynamics encoder by employing our proposed met-ric and alternative criteria as stopping conditions during thetraining. Max epoch signifies the training over the maxi-mum number of epochs. Davies-Bouldin Index (DBI) assesses the quality of clustering results by calculating theratio of intra-cluster distances to inter-cluster separations. Alower DBI value implies more compact and well-separatedclusters, i.e., better clustering quality.In addition, Optepoch selects the optimal training epoch that achieves thebest detection results, providing the upper bound of detec-tion performance.In , our performance is close to the optimal caseacross various noise types and datasets, whereas Max epochand DBI fail to stop the training process at a proper epochon CIFAR-100. In conclusion, using the proper validationmetric is critical for achieving competitive detection per-formance, particularly in the scenario where ground-truthannotations are not available for validation.",
  ". Quantitative analyses": "The effect of corruption rate.We analyze the effect ofincreasing the corruption rate, which in turn amplifies theoverall noise level.5 For thorough analyses, we conduct acontrolled experiment within a supervised framework usingclassification,6 assuming the availability of ground-truth an-notations that indicate each instance as being correctly orincorrectly labeled. We then compare these results, gen-erally regarded as the performance upper bound for unsu-pervised methods, with those obtained by an unsupervisedapproach. We focus on assessing the ability of our proposedunsupervised learning model, i.e., DynaCor, to discriminatetraining dynamics and how this discrimination is affectedby increasing the overall noise level through corruption.As shown in , the detection F1 scores achievedby DynaCor (b) approaches those of supervisedlearning (a), demonstrating the effectiveness oftraining dynamics.This proximity is especially notablewhen utilizing a powerful image encoder, i.e., CLIP, whichmakes the training dynamics less susceptible to changes inthe corruption rate. In contrast, the training dynamics fromResNet34 are more affected by increased corruption rate.Surprisingly, in the case of Inst.type label noise, thetraining dynamics from the CLIP w/ MLP classifier becomeeven more distinguishable as the corruption rate increases to0.5. It shows that a higher noise rate in the training datasetcan enhance the discernibility of the training dynamics. Wehypothesize that the symmetric noise introduced throughour label corruption process may reduce the overall diffi-culty of the detection task. This is consistent with the as-sertion in Sec. 4.4.2 that the symmetric noise is relativelystraightforward to identify and, in turn, contributes to im-proving the performance of noisy label detection. The effect of two losses.We examine the effect of theclustering and alignment losses within our DynaCor frame-work.In , both losses enhance detection perfor-mance. We also observe that the alignment loss effectivelyaddresses the high imbalance between clean and noisy in-stances, particularly in scenarios with a low noise rate (e.g.,",
  ". Compatibility analysis of Dividemix with DynaCor onCIFAR100 over Asym. and Inst. with respect to noise rate": "Agg. on CIFAR-10). Given that DynaCor intentionallyincreases the noise rate by augmenting instances with cor-rupted labels, its benefits become more pronounced whendealing with datasets featuring a small original noise rate.In such cases, the alignment loss is crucial in stabilizing theclustering process by aligning the distinct distributions oforiginal and corrupted instances.",
  ". Compatibility analyses with robust learning": "We investigate the compatibility and synergistic effectsof integrating our framework with various robust learningtechniques: a semi-supervised approach (Dividemix ),loss functions (GCE and SCE ), and a regulariza-tion method (ELR ). Detailed analyses of incorporatingthe loss functions and regularization technique on the Cloth-ing1M dataset are provided in Appendix D.For the semi-supervised approach, we select Dividemix that iteratively detects incorrectly labeled instances andtreats them as unlabeled instances. We construct integratedmodels of Dividemix and DynaCor through two distinctapproaches: (1) DDyna-L is leveraging Dividemix to ob-tain the training dynamics of both original and corrupteddatasets within our framework, and (2) DDyna-S is sub-stituting the original detection method in Dividemix, i.e.,GMM, with DynaCor. For the base architecture, we em-ploy an 18-layer PreAct ResNet , adhering to its defaultoptimization settings and hyperparameters, as specified inthe original paper . Classification accuracy.We explore the impact of ourframework on the classifiers accuracy, specifically intro-ducing a corrupted dataset (DDyna-L) and supplantingthe existing noise detection method (DDyna-S). a demonstrates that both enhance classification performance.In essence, results obtained with DDyna-L demonstrate thatinstances with symmetric label noise introduced throughour corruption process prove beneficial for noise robustlearning, especially in scenarios featuring a low noise ratein the original dataset, pointed out as a challenging settingfor Dividemix . Detection F1 score.To report the noisy label detectionperformance within robust learning framework, i.e., Di-videmix and DDyna-S, we measure F1 score at every epochand report the value when test classification accuracy is atits highest. Note that they leverage a clean test dataset toidentify the optimal detection point; on the contrary, thenoisy detection method (DDyna-L) operates without accessto clean data, instead employing the procedure for modelvalidation on the noisy dataset itself (Sec. 4.4.3), presentinga more challenging task. b indicates that DDyna-Sand DDyna-L further improves the detection F1 score of Di-videmix, indicating the great compatibility of DynaCor withexisting semi-supervised noise robust learning. In scenariosinvolving Inst. label noise, DDyna-L exhibits compellingsynergistic effects across a wide range of noise rates.",
  ". Conclusion": "This paper proposes a new DynaCor framework that dis-tinguishes incorrectly labeled instances from correctly la-beled ones via clustering of their training dynamics. Dy-naCor first introduces a label corruption strategy that aug-ments the original dataset with intentionally corrupted la-bels, enabling indirect simulation of the models behavioron noisy labels. Subsequently, DynaCor learns to inducetwo clearly distinguishable clusters for clean and noisy in-stances by enhancing the cluster cohesion and alignmentbetween the original and corrupted dataset. Furthermore,DynaCor adopts a simple yet effective validation metric toindirectly estimate its detection performance in the absenceof annotations of clean and noisy labels. Our comprehen-sive experiments on real-world datasets demonstrate the de-tection efficacy of DynaCor, its remarkable robustness tovarious noise types and noise rates, and great compatibilitywith existing approaches to noise robust learning.",
  ". Acknowledgements": "This work was supported by the IITP grant fundedbytheMSIT(No.2018-0-00584,2019-0-01906,2020-0-01361), the NRF grant funded by the MSIT(No.2020R1A2B5B03097210, RS-2023-00217286), andthe Digital Innovation Hub project supervised by the DaeguDigital Innovation Promotion Agency (DIP) grant fundedby the Korea government (MSIT and Daegu MetropolitanCity) in 2024 (No. DBSD1-07). Devansh Arpit, Stanisaw Jastrzebski, Nicolas Ballas, DavidKrueger, Emmanuel Bengio, Maxinder S Kanwal, TeganMaharaj, Asja Fischer, Aaron Courville, Yoshua Bengio,et al. A closer look at memorization in deep networks. InInternational conference on machine learning, pages 233242. PMLR, 2017. 1, 4 Alan Joseph Bekker and Jacob Goldberger. Training deepneural-networks based on unreliable labels. In 2016 IEEEInternational Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 26822686. IEEE, 2016. 2 David Berthelot, Nicholas Carlini, Ian Goodfellow, NicolasPapernot, Avital Oliver, and Colin A Raffel. Mixmatch: Aholistic approach to semi-supervised learning. Advances inneural information processing systems, 32, 2019. 2 Wenkai Chen, Chuang Zhu, and Mengting Li.Sampleprior guided robust model learning to suppress noisy labels.In Joint European Conference on Machine Learning andKnowledge Discovery in Databases, pages 319. Springer,2023. 2, 13",
  "Jiangfan Han, Ping Luo, and Xiaogang Wang. Deep self-learning from noisy labels. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 51385147, 2019. 1, 2": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 6, 12 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Identity mappings in deep residual networks. In ComputerVisionECCV 2016: 14th European Conference, Amster-dam, The Netherlands, October 1114, 2016, Proceedings,Part IV 14, pages 630645. Springer, 2016. 8, 12 Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao. O2u-net: A simple noisy label detection approach for deep neu-ral networks. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 33263334, 2019. 1,2 Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, andLi Fei-Fei. Mentornet: Learning data-driven curriculum forvery deep neural networks on corrupted labels. In Interna-tional conference on machine learning, pages 23042313.PMLR, 2018. 2, 13 Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learningdeep networks from noisy labels with dropout regularization.In 2016 IEEE 16th International Conference on Data Mining(ICDM), pages 967972. IEEE, 2016. 2",
  "Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and LucVan Gool. Webvision database: Visual learning and under-standing from web data. arXiv preprint arXiv:1708.02862,2017. 1": "Zachary C Lipton,Charles Elkan,and BalakrishnanNaryanaswamy. Optimal thresholding of classifiers to maxi-mize f1 measure. In Machine Learning and Knowledge Dis-covery in Databases: European Conference, ECML PKDD2014, Nancy, France, September 15-19, 2014. Proceedings,Part II 14, pages 225239. Springer, 2014. 3 Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Car-los Fernandez-Granda.Early-learning regularization pre-vents memorization of noisy labels. Advances in neural in-formation processing systems, 33:2033120342, 2020. 1, 2,8, 14",
  "Curtis Northcutt, Lu Jiang, and Isaac Chuang.Confidentlearning: Estimating uncertainty in dataset labels. Journalof Artificial Intelligence Research, 70:13731411, 2021. 2,6": "Diane Oyen,Michal Kucer,Nicolas Hengartner,andHar Simrat Singh. Robustness to label noise depends on theshape of the noise distribution. Advances in Neural Informa-tion Processing Systems, 35:3564535656, 2022. 5, 6 Joshua C Peterson, Ruairidh M Battleday, Thomas L Grif-fiths, and Olga Russakovsky. Human uncertainty makes clas-sification more robust.In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 96179626, 2019. 1 Geoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kilian QWeinberger. Identifying mislabeled data using the area underthe margin ranking. Advances in Neural Information Pro-cessing Systems, 33:1704417056, 2020. 1, 2, 4, 6, 13 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 6, 12",
  "SainbayarSukhbaatar,JoanBruna,ManoharPaluri,Lubomir Bourdev, and Rob Fergus. Training convolutionalnetworks with noisy labels. arXiv preprint arXiv:1406.2080,2014. 3": "Zeren Sun, Xian-Sheng Hua, Yazhou Yao, Xiu-Shen Wei,Guosheng Hu, and Jian Zhang. Crssc: salvage reusable sam-ples from noisy data for robust learning.In Proceedingsof the 28th ACM International Conference on Multimedia,pages 92101, 2020. 2 Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie,Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, andYejin Choi. Dataset cartography: Mapping and diagnosingdatasets with training dynamics. In Proceedings of the 2020Conference on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 92759293, 2020. 1, 4",
  "Jingkang Wang, Hongyi Guo, Zhaowei Zhu, and Yang Liu.Policy learning using weak supervision. Advances in NeuralInformation Processing Systems, 34:1996019973, 2021. 1": "Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi,and James Bailey. Symmetric cross entropy for robust learn-ing with noisy labels. In Proceedings of the IEEE/CVF in-ternational conference on computer vision, pages 322330,2019. 2, 8, 14 Zhiguang Wang, Weizhong Yan, and Tim Oates. Time se-ries classification from scratch with deep neural networks: Astrong baseline. In 2017 International joint conference onneural networks (IJCNN), pages 15781585. IEEE, 2017. 6",
  "Hongxin Wei, Lue Tao, Renchunzi Xie, and Bo An. Open-set label noise can improve robustness against inherent labelnoise. Advances in Neural Information Processing Systems,34:79787992, 2021. 1": "Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu,Gang Niu, and Yang Liu.Learning with noisy labels re-visited: A study using real-world human annotations. arXivpreprint arXiv:2110.12088, 2021. 1, 5, 6, 8, 12 Qi Wei, Haoliang Sun, Xiankai Lu, and Yilong Yin. Self-filtering: A noise-aware sample selection for label noise withconfidence penalization. In European Conference on Com-puter Vision, pages 516532. Springer, 2022. 2 Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, NannanWang, Zongyuan Ge, and Yi Chang. Robust early-learning:Hindering the memorization of noisy labels. In Internationalconference on learning representations, 2020. 2 Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Ming-ming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, andMasashi Sugiyama.Part-dependent label noise: Towardsinstance-dependent label noise. Advances in Neural Infor-mation Processing Systems, 33:75977610, 2020. 5, 12 Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and XiaogangWang.Learning from massive noisy labeled data for im-age classification. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 26912699,2015. 1, 2, 5, 12",
  "Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsuperviseddeep embedding for clustering analysis.In Internationalconference on machine learning, pages 478487. PMLR,2016. 5": "Jiangchao Yao, Jiajie Wang, Ivor W Tsang, Ya Zhang, JunSun, Chengqi Zhang, and Rui Zhang. Deep learning fromnoisy image labels with quality embedding. IEEE Transac-tions on Image Processing, 28(4):19091922, 2018. 2 Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang,and Masashi Sugiyama. How does disagreement help gener-alization against label corruption? In International Confer-ence on Machine Learning, pages 71647173. PMLR, 2019.2",
  "A.1. Datasets": "Synthetic noise: instance-dependent label noise.Wedetail the process of generating instance-dependent labelnoise , which is the synthetic type label noise utilizedin our experiments. The key idea is that the probability ofan instance being incorrectly labeled to other classes is cal-culated based on both the input feature and its label, usingrandomly generated feature projection matrices with respectto each class. The procedure is provided in Algorithm 1.",
  ": end for": "Clothing1M .To assess DynaCors performance withsystematic type label noise, we use a real-world datasetClothing1M, which consists of clothing images across 14classes7 collected from online shopping websites. It com-prises one million images with inherent noisy labels in-duced by automated annotations derived from keywords inthe text surrounding each image. It also provides 50K, 14K,and 10K instances verified as clean for training, validation,and testing purposes. Adhering to the previous experimen-tal setup , for training, we utilize randomly sampled120K instances from the 1M noisy dataset while ensuringeach class is balanced. To evaluate classification perfor-mance, we use the 10K clean test set.",
  "A.2. Reproducibility": "For reproducibility, we provide detailed hyperparametersfor (1) classifiers used to generate training dynamics or tolearn robust models and (2) dynamics encoder to learn dis-criminative representations of the training dynamics. Classifier. shows details of the datasets, models,and training parameters used to generate training dynam-ics or to learn robust models in each section of this paper.Optimizer and momentum are fixed as SGD and 0.9, re-spectively. In the case of CLIP with MLP, we obtain inputfeatures using a fixed image encoder from CLIP and trainonly MLP, which consists of two fully connected layers of512 units with ReLUs . Resnet50 is pre-trained on Ima-geNet and is fine-tuned on Clothing1M. We follow theexperimental setups described in the reference papers.",
  ". Detailed hyperparameters used in the experiments for theclassifiers": "Dynamics encoder.For the dynamics encoder in Dy-naCor, we use a 1D Convolutional Neural Network (1D-CNN). It consists of three convolutional layers, each incor-porating rectified linear units (ReLUs) , followed by alinear layer with 512 output units. For optimization, weuse Adam with a learning rate 1 105 and a weightdecay 5104 without implementing a learning rate sched-uler. The model is trained for 10 epochs with a batch sizeof 1024.",
  "B. Analyses of Training Dynamics": "To assess the distinguishability of the inherent patternsmanifested in the training dynamics, we conduct a con-trolled experiment using classification within a supervisedlearning framework. This is predicated on the assumptionthat ground-truth annotations are available, explicitly speci-fying each instance as being correctly or incorrectly labeled.We first provide preliminaries for analyses (Sec. B.1).Then, we demonstrate the efficacy of capturing temporalpatterns in training dynamics versus summarizing these dy-namics into a single scalar value (Sec. B.2) on various train-ing signals. Lastly, we evaluate which training signals ex-hibit more distinctive patterns (Sec. B.3).",
  "B.1. Preliminaries": "Training signals. summarizes various trainingsignals introduced in the literature. Given an instance (x, y)and a classifier f, let f(x) RC and fy(x) denote theoutput logits of an instance x for C classes and its valuefor class y, respectively.(, ) is a loss function, andpy(x) =exp fy(x)Cc=1 exp fc(x) is a predicted probability of class y.vx indicates penultimate layer representation vectors of aninstance x, and uy is a representative vector for class y, de-rived through performing eigen decomposition on the grammatrix of data representations. , denotes inner product.",
  ". Dataset construction for supervised learning": "Supervised experimental setting. As illustrated in , we generate training dynamics by employing a classifierthat predicts the class probabilities for each input instanceacross the set of classes. Subsequently, we construct a newdataset comprising these extracted training dynamics andthe corresponding ground-truth labels that are assumed toexist. This new dataset is then utilized to train a 1D con-volutional neural network (1D-CNN) classifier (henceforthreferred to as a binary classifier) that distinguishes betweencorrectly and incorrectly labeled instances based on the pat-terns in their training dynamics. We train the binary classi-fier (whose encoder is the same as our dynamics encoder)for 20 epochs using the Adadelta optimizer with an ini-tial learning rate of 1 and a StepLR scheduler that reducesit by 1% for every epoch. The batch size is set to 128. Dur- ing training, we monitor the models performance on a val-idation set and report the F1 score for detecting incorrectlylabeled instances on the test set, corresponding to the pointwhere the validation F1 score achieves its maximum value.",
  "e=1t(e)x ,(11)": "To evaluate the relative efficacy of these approaches, we usetwo distinct types of training signals: probability and logitdifference in . For the binary classifier of the sum-marized one, we adopt a multi-layer perceptron (MLP) oftwo hidden layers. To ensure the models sufficient capacityto learn patterns in the data, we increase the model parame-ters until performance does not improve further. .Comparison of detection F1 score (%) achieved bythe binary classifiers trained using the training dynamics (comb-pattern bar and star marker in legend) versus those trained with thesummarized one for various noise types on CIFAR-100. Prob. andLogit diff. indicate the types of training signals in . Noiserates of Sym., Asym., and Instance are 0.6, 0.4, and 0.3, respec-tively. The human-induced noise has noise rates of 0.4. CLIP w/MLP (Left) and Resnet34 (Right) are used for training dynamicsgeneration. shows that the models trained with the train-ing dynamics consistently outperform those with the sum-marized training dynamics. The results demonstrate thattemporal patterns within training dynamics help distinguishbetween correctly and incorrectly labeled instances.",
  "We compare the detection F1 score of the binary classi-fier trained with the training dynamics derived from varioustraining signals in the supervised setting": ". Comparison of detection F1 score (%) of the raw train-ing dynamics from various training signals on CIFAR-100. Noiserates of Sym., Asym., and Instance are 0.6, 0.4, and 0.3, respec-tively. The human-induced noise type has noise rates of 0.4. TheAvg. indicates an averaged F1 score (%) over all noise types. CLIPw/ MLP (Upper) and Resnet34 (Lower) are used for training dy-namics generation. shows that, on average, more processed train-ing signals, such as probability differences and alignment ofpre-logits, exhibit superior performance compared to sim-pler ones. In this study, we select logit difference as the baseproxy measure due to its consistent performance across var-ious experimental settings. Moreover, we observe that de-tection performance for different types of noises is highlycorrelated with model architecture. We leave the study ofthe influence of model architectures in future work.",
  "D. Compatibility analysis with robust learningon Clothing 1M dataset": "We also investigate the compatibility of DynaCor with vari-ous loss functions (GCE , and SCE ) and regulariza-tion technique (ELR ), specifically designed for noiserobust learning. To this end, we measure the test accuracyof such noise robust classifiers trained using the originalClothing1M dataset and the cleansed dataset (i.e., the onewith only correctly labeled instances identified by Dyna-Cor), respectively."
}