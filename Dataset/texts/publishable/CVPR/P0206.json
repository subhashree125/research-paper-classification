{
  "Abstract": "We introduce Situation Monitor, a novel zero-shot Out-of-Distribution (OOD) detection approach for transformer-based object detection models to enhance reliability in safety-critical machine learning applications such as autonomousdriving. The Situation Monitor utilizes the Diversity-basedBudding Ensemble Architecture (DBEA) and increases theOOD performance by integrating a diversity loss into thetraining process on top of the budding ensemble architecture,detecting Far-OOD samples and minimizing false positiveson Near-OOD samples. Moreover, utilizing the resultingDBEA increases the models OOD performance and im-proves the calibration of confidence scores, particularly con-cerning the intersection over union of the detected objects.The DBEA model achieves these advancements with a 14%reduction in trainable parameters compared to the vanillamodel. This signifies a substantial improvement in efficiencywithout compromising the models ability to detect OODinstances and calibrate the confidence scores accurately.",
  ". Introduction": "In machine learning, models must exhibit effective gener-alization capabilities that require adaptability beyond theirtraining data. This adaptability is crucial for ensuring theeffective performance of models in real-life situations popu-lated with diverse objects. In real-world applications, partic-ularly in safety-critical scenarios such as autonomous drivingor medical diagnosis, the capability to detect OOD instancesis decisive for ensuring the robust performance of machinelearning models. OOD instances refer to situations wherethe model encounters data patterns or objects that differsignificantly from what it was exposed to during training.Detecting OOD instances becomes particularly challengingwhen models are expected to generalize effectively across",
  ". Out-of-Distribution definition, Dotted line represents the decisionboundary of an OOD detection model that generalizes effectively": "diverse and unpredictable situations . This adaptabilityis essential for ensuring the reliability and safety of machinelearning models in dynamic and complex environments.To address this classification challenge, this study ex-plores the two types of OOD conditions, as depicted in : Near-OOD and Far-OOD . In the context of a Near-OOD dataset, there is a notable resemblance in features andcharacteristics to the training dataset, also referred to asin-distribution (IN) data. The Near-OOD dataset may resem-ble datasets obtained from diverse acquisition sensors, e.g.,when evaluating a model trained with the autonomous driv-ing dataset KITTI , other autonomous driving datasetslike BDD100K , Cityscapes (2D bounding boxesfrom CityPersons ) or Lyft (2D bounding boxescomputed from 3D bounding boxes) can be categorizedas Near-OOD. The Far-OOD dataset introduces a differ-ent paradigm, where the dataset is entirely dissimilar to theIn-Distribution (IN) dataset, surpassing the characteristics",
  "arXiv:2406.03188v1 [cs.CV] 5 Jun 2024": "of Near-OOD conditions. Given the previous example, theCoCo dataset would suffice for this Far-OOD condition.The OOD module integrated within the Deep Neural Net-works (DNNs) for a specific application should avoid miss-classifying Near-OOD instances as OOD while correctlyflagging instances significantly different from the trainingdataset, i.e., Far-OOD, as the OOD cases.With the transition from traditional Convolution NeuralNetworks (CNNs) to transformer-based models due to theirremarkable ability to capture long-range dependencies andcontextual information , a shift and redesign of OODmethods designed initially for CNNs is in progress.Therefore, we showcase the OOD detection performanceof our proposed Situation Monitor derived by leveragingthe deviations of the ensemble predictions of the DBEAmodel integrated into the DINO-DETR transformer-based vision model. DBEA is derived from the buddingensemble architecture proposed by . The tandem lossfunction is hereby extended to incorporate a diversity lossfunction. Given that vision-based transformers commonlyfeature encoder-decoder structures. As a final stage of fullyconnected layers, it is generally applicable to integrate theSituation Monitor into various other vision transformer mod-els without loss of generality.In this work, we propose to:",
  ". Related Work": "Researchers are continuously exploring innovative method-ologies and refining existing techniques to bolster the capa-bilities of object detectors, particularly in handling out-of-distribution situations. Classifying Deep Neural Networks(DNNs) into deterministic and probabilistic networks pro-vides a foundational understanding.Deterministic Networks vs Probabilistic Networks: De-terministic networks in DNNs generate consistent outputsfor a given input in a deterministic manner . However,these networks cannot model prediction uncertainty. As aresult, confidence scores associated with their predictions be-come crucial for measuring uncertainty, serving as valuableindicators for OOD detection . In contrast, probabilis-tic DNNs explicitly model uncertainty in their predictions by outputting probability distributions over possible out-comes . This explicit modelling benefits OOD detectionacross a wide range of applications . While OOD de-tection using probabilistic networks can be computationallydemanding, researchers have devised various techniques fordeterministic networks, primarily focusing on post-hoc meth-ods .Unified Frameworks for OOD Detection: In additionto post-hoc methods, a few unified frameworks incorporateOOD detection seamlessly into the primary task of the DNN.These frameworks utilize zero-shot or few-shot learningstrategies to improve OOD detection capabilities .The findings from , indicating that wider networkswith similar architecture learn more similar features, areintegrated into the subsequent work of the sample-free uncer-tainty estimation method BEA . BEA notably exhibitedsignificant improvements in OOD detection, primarily focus-ing on anchor-based CNN models for object detection. It isworth noting that most of the related research is connectedto CNN-based DNNs, and there is limited exploration intoadapting and extending these techniques for transformer-based object detection models.Sample-Free Uncertainty Estimation Method: Notably,BEA , a sample-free uncertainty estimation method,demonstrated considerable performance enhancements inOOD detection. This method primarily focused on anchor-based CNN models for object detection. Similarly, Gaussian-Yolov3 introduced Gaussian parameters to exploit vari-ances and reduce false positives by calibrating them to theIoU.In summary, researchers are continually exploring newmethodologies and refining existing techniques to en-hance the capability of object detectors in handling out-of-distribution situations. This includes methods focused onboth deterministic and probabilistic networks, as well asunified frameworks that seamlessly incorporate OOD detec-tion into the primary task of the DNN. Additionally, thereis potential for adapting and extending these techniques fortransformer-based object detection models. However, mostof this research is associated with CNN-based DNNs, andthere is limited exploration into adapting and extending thesetechniques for transformer-based object detection models.",
  ". Problem Statement": "In the closed-world assumption, the training dataset (D)and testing dataset (T ) is from in-distribution dataset I, i.e.,D I. Therefore, the samples from the testing datasetis S(T ) = S(T I). However, in open-world settings andpractical, real-world scenarios, samples are also drawn fromOOD data. Therefore, the OOD samples O are composedof both Near-OOD (Onear) and Far-OOD (Ofar), i.e. O =Onear + Ofar. Similarly, in the open world setting, the testingdata T consists of known situations and classes originating . The primary aim of the Situation Monitor is to distinguish betweenknown and unknown situations. For instance, a model trained on datasetssuch as KITTI for automotive scenarios is categorized as a Near-Out-of-Distribution (Near-OOD) situation. Consequently, encountering an indoorscenario would be labelled as Far-Out-of-Distribution (Far-OOD) situationby the model. from Near-OOD Onear and unknown situation and unknownclasses to the model originating from Far-OOD Ofar i.e.,T = T I + T Onear + T Ofar and accordingly the samplesfrom the testing data is S(T ) = S(T I) + S(T Onear) +S(T Ofar). A model trained with D is not only required to detectobjects from S(T I) but it is also required to generalizewell to S(T Onear). Therefore, a OOD detection moduleshould not flag S(T Onear) as an OOD, rather only classifythe samples from S(T Ofar) as an OOD sample. As shown in , the primary purpose of the SituationMonitor is to distinguish between known and unknown situ-ations. Leveraging the remarkable generalization capabilityof deep learning models , the monitor adeptly classifiesNear-OOD situations as known situations. This classificationis grounded in the understanding that Near-OOD instancesshare significant similarities with the In-Distribution (IN)dataset, aligning with the inherent generalization prowess ofdeep learning models. Consequently, the Situation Monitoris pivotal in identifying situations based on their familiaritywith the models learned context. By accurately discerningknown and unknown situations, the monitor empowers themodel to make informed decisions in real-world applications,even when encountering instances beyond its training data.This enhances the models reliability and performance indiverse and dynamic environments. Overall, the SituationMonitor plays a crucial role in ensuring the effectiveness ofdeep learning models across various situations. In summary, our goal is to train a transformer-based objectdetection model on a training set D and without introducingthe samples from O, the Situation Monitor (OOD detectionmodule) of the model should have ability to classify the sam-ples from S(T Ofar) as an OOD situation and on contraryshould not flag the S(T Onear) samples as an OOD situation.",
  ". DBEA: Diversity based Budding Ensemble Ar-chitecture": "The recently introduced Budding Ensemble Architecture(BEA) represents a sample-free methodology for de-tecting OOD instances, showcasing notable performanceadvancements. Consequently, we employ the Budding En-semble Architecture approach in the design of our SituationMonitor.Within the Budding Ensemble Architecture (BEA) frame-work, a unified backbone and duplicated detectors replacethe conventional ensemble setup. This alteration enhancesconfidence score calibration, diminishes uncertainty errors,and introduces an overlooked advantage: superior OOD de-tection compared to other state-of-the-art sample-free meth-ods. The Tandem loss function (Ltandem) introduced inBEA, devised initially for YOLOV3 and SSD wasprimarily tailored for anchor-based object detection mod-els. Therefore, the Ltandem function to be applied to thetransformer model requires a series of modifications andadaptations to integrate with this novel ensemble approachseamlessly.We base our Situation Monitor design on the ResNet-based DINO-DETR transformer model. It typically com-prises a CNN-based backbone (ResNet), self-attention layer-based encoders, and decoders, followed by regression layers . Adapted architecture diagram from DINO-DETR for DBEA-DINO-DETR, illustrating the replication of final layers. For a comprehensiveunderstanding of DINO-DETR, please refer to in . In BEA, it is proposed to duplicate the final layers as and to create tandem detectors,which are subsequently utilized in the situation monitoring model. for classification and bounding box detection. We leveragethe insights from BEA to tailor the DINO-DETR object de-tection model for our Situation Monitor. Specifically, weintroduce diversity-based tandem detection layers, as de-picted in (b). In the architectural integration of BEAinto DINO-DETR, the 3 layer feed-forward neural networklayers (FFN) forming the final regression layers immedi-ately following the decoder are duplicated, resulting in twodetectors, and collectively called as tandem detectors.During inference, confidence scores and bounding boxes arecomputed from the mean of the tandem detectors.Despite the duplication of layers, as indicated in the BEApaper , no advantages were observed for the SituationMonitor during the training of the DINO-DETR model. Thislack of advantage stems from the tandem layers learningsimilar representations. Therefore, the incorporation of thetandem loss function, Ltandem, alongside the original baseloss function, Lbase, becomes crucial. The adaptation ofBEA with the diversity-based Ltandem function is calledDiversity-based Budding Ensemble Architecture (DBEA).This serves a dual purpose: creating an effective SituationMonitor and enhancing the calibration of confidence scores.The adapted tandem loss is now defined as follows:",
  "Ltandem = taLta + tqLtq(3)": "where 1i denotes whether the object prediction overlapswith the ground-truth.The variables x and y signify the pre-dicted centre points of the bounding box, while w and hrepresent the predicted height and width of the object, re-spectively. The Ltandem operates on positive predictionsand negative predictions, which is possible due to accessto ground truth during training. The Tandem-Aiding lossfunction Lta diminishes the errors associated with the pos-itive predictions between and detector. Similarly, theTandem-Quelling loss function Ltq amplifies the errorsrelated to negative predictions between and detectors.Therefore promoting agreement and disagreement concern-ing positive and negative predictions.Within the BEA paper, the Lta and Ltq loss functionsare applied independently to specific components of bothclassification and bounding box regression outputs. In thecontext of the transformer model, the Ltandem is exclusivelyapplied to the bounding box regression layers as shown inEq. (1) and (2). This selective application is adopted becauseintroducing this loss to the classification layer decreases theperformance of the Situation Monitor. To support Ltandemloss function for better calibration of the confidence scores,we introduce diversity at classification regression layer out-put between the tandem layers as shown in . To thisend, we use similarity score as a diversity measure to ensurethat the tandem detectors capture distinct representationsfrom the decoders. Specifically, we use the cosine similarityscore to introduce the diversity as depicted in Eq. (4).",
  "i=1Cosine Similarity(i, i)(4)": "where n represents total number of predictions and Z repre-sents the classification logits denoted as Z = (z1, z2, ..zk),zi is the unnormalized score for class i.Incorporating diversity introduces a dynamic range ofclassification outputs at the tandem layers. This diversityis instrumental in fostering a broader spectrum of values,thereby providing the tandem loss function with a largerspace to operate upon. Specifically, the tandem loss functionleverages this diversity to mitigate errors associated withpositive predictions effectively. By encouraging divergencein classification values, the model can better discern and re-fine its understanding of positive instances. Simultaneously,this diversity amplifies errors in the context of negative pre-dictions. To this end, the diversity and tandem-based lossfunction is constructed by incorporating both Ltandem andLdiversity into the base loss function of the DINO-DETRmodel, denoted as Lbase. This integration is illustrated inEquation (5).",
  ". Situation Monitor": "The Situation Monitor is a component of the BEA-DINO-DETR that serves as an Out-of-Distribution (OOD) detectionmodule. It identifies Far-OOD situations by analyzing dispar-ities in the predictions of tandem layer bounding boxes. Thetransformer model undergoes end-to-end training through azero-shot approach, which, in this case, means an explicitOOD dataset is not shown during the training process. Thismethodology distinguishes explicit situations by highlightingthe errors in variance between the tandem layer predictions,specifically in Far-OOD situations. The Ldbea loss function,incorporating cosine diversity during training, compels tan-dem detection layers to consider objects from diverse featuremaps and perspectives.The introduction of diversity loss (Ldiversity) duringtraining prompts a change in perspective, compelling tan-dem detection layers to examine objects from diverse featuremaps and viewpoints. Driven by the Ltandem loss function,tandem layers generate distinct predictions for bounding boxcentre points (). In instances where the set T is a sub-set of I within the specific category Onear, it is observed thatthe predicted widths and heights tend to be closely alignedwith no large variances. Similarly, we should expect largevariances when Ofar is encountered. Therefore, to captureFar-OOD situations effectively at the image level, a method-ology is applied to heighten the variances of predicted centrepoints (x and y) and heights and widths (w and h). Thesebounding box-related attributes are referred to as b in thefollowing equation. . Situation Monitor: The variance between DBEA and DBEAdetector predictions is interpreted as the prediction uncertainty USM. Ahigh uncertainty means Far-OOD, whereas low uncertainty means Near-OOD or in-distribution.",
  "(6)": "To heighten sensitivity to deviations and pinpoint errors atan image-specific level, these variances undergo additionalprocessing as shown in Eq. (6). This involves centringvariances by multiplication with their respective means. Thefinal OOD value, denoted as USM, is computed as the meanof these variances at the image level, as depicted in Eq. (6).This approach effectively addresses global trends across theentire dataset while localizing errors to individual images.By doing so, anomalies unique to each image are identified,enhancing performance in capturing Far-OOD situations.",
  ". Evaluation Metrics for Situation Monitor": "The impact of adding Ltandem and Ldiversity to the Lbaseloss function is evaluated using mean average precision(mAP) metrics. The calibration is evaluated using Pear-son correlation (PCorr) between confidence scores andthe intersection over union of objects and the ground truths.Pearson correlation is evaluated on two sets PCorr-all andPCorr-tp which is correlation on all the predictions andseparately on true positive samples. The Situation Moni-tor in a DBEA model uses USM values and it is assessedusing four standard metrics to evaluate its performance indetecting OOD situations. Whereas, the vanilla baselinemodels OOD detection is evaluated using their confidencescores. In this study, we do not assess the object detection performance (mAP/AP) on Near-OOD datasets. Instead,we simply demonstrate that the Situation Monitor does notidentify Near-OOD samples as OOD. AUROC calculates the area under the receiver operatingcharacteristic curve, a metric utilized to assess the perfor-mance of OOD detection. The OOD (T (T Ofar)) samplesare considered as positive samples. A higher AUROCvalue indicates superior performance. AUPR(In/Out) is the area under the receiver operatingcharacteristic curve and is a key metric for evaluatingOOD detection performance. It assesses how well a modeldistinguishes positive instances, with T (T Ofar) samplesconsidered as positives. AUPR comprises of AUPR-In andAUPR-Out. It considers the in-distribution (T (T I)) sam-ples as either positive or negative respectively. A higherAUPR value indicates superior model performance. FPR@95 expressed as FPR (false positive rate) at a fixedTPR (true positive rate) point represents the rate of falselyidentified positive instances among all negative instanceswhen the true positive rate is held at a specific percentagewhich in this case at 95%. The lower value indicatessuperior performance.",
  ". Experiment Setup": "The Situation Monitor (SM) is integrated into the DBEA-DINO-DETR object detection model, allowing end-to-endtraining without freezing the backbone or any particular layer.This ensures tandem detectors learn based on the Ldbealoss function. Evaluations are conducted on the KITTI and BDD100K datasets, widely used computer visiondatasets for autonomous driving situations. These datasetsare divided into training, validation, and testing sets withratios of 7.5:1:1.5 and 8.5:0.75:0.75, respectively. Eight outof nine usable classes are evaluated for the KITTI dataset,while all ten classes are considered for the BDD100K dataset.Additionally, CoCos evaluation dataset is utilized. A consis-tent 416 416 input image size is used for training acrossall models. For YoloV3 and SSD models, SGDoptimizer is employed with a learning rate of 0.001, mo-mentum of 0.95, and weight decay of 0.001, trained for 300epochs with batch-size of 28. DINO-DETR-based modelsutilize 900 queries, generating 900 predictions per image, fpr_at_95_tpr detection_error auroc aupr_in aupr_outAP AP50 PCorr (all) PCorr (tp) fpr_at_95_tpr Ablation study on div with ta=1 and q=10 Cosine diversity coeff optimalVanilla div=0, div=1 div=40 div=80",
  ". Ablation Study on div, ta and tq on KITTItrained DBEA-DINO-DETR": "In this ablation study section, multiple DBEA-DINO-DETRmodels are trained on the KITTI dataset wherein the param-eters outlined in equations 5 and 3 are varied. The primaryobjective is systematically showcasing and analysing eachcomponents distinctive impact. In all ablation experiments,the parameter ta is consistently maintained at either onewhen necessary or at zero. This is because Lta is designedto minimize errors in positive predictions, and an increase inthe ta factor beyond tq adversely affects calibration andthe fundamental object detection capability of the model. illustrates the impact of introducing Ldiversitywith varied div parameters on the Situation Monitor perfor-mance and its OOD detection. The radar plots presented inthis study will include the outcomes of a baseline model (thevanilla model) and reference optimal values against whichthe metrics will be compared. Without enabling diversity, theperformance on the Far-OOD CoCo dataset experiences asignificant decline compared to its enabled counterpart. Theoptimal configuration is identified when the div parameteris set to 40, resulting in elevated Average Precision metrics,as well as improved AUROC and AUPR OOD metrics, whileconcurrently maintaining lower FPR@95 and detection error",
  "DBEA-DINO-DETR (ours)47.984.678.948.399.699.7/99.61.22.2": ". Comparison of the performance of our sample-free method with that of previous state-of-the-art sample-free method. Our method is trained on theKITTI and BDD100K datasets, and the out-of-distribution detection evaluation is conducted on the COCO dataset instances. fpr_at_95_tpr detection_error auroc aupr_in aupr_outAP AP50 PCorr (all) PCorr (tp) fpr_at_95_tpr Ablation study on tq with ta=1 and div=40 tandem quelling coeff optimalVanilla tq=1 tq=10 tq=100",
  ". Analysis of the impact of varying parameter Ltandem in theablation study": "(DE@95) values. A subsequent increase of div from 40 to80 brings about enhancements across all metrics, with onlya minor impact on detection error and a slight decrease inFPR@95. Hence, choosing 40 as the parameter proves tobe the most effective, achieving a balance across all metrics.The selection of div, incremented by a factor of 2, is influ-enced by the consideration that both the classification andGIOU loss components of Lbase are scaled by a factorof 2, making factors of 2 more effective to the effectivenessof div.Similarly, the influence of tq is illustrated in , withta and div held at 1 and 40, respectively. This specificradar plot highlights that setting tq to 10 produces the most fpr_at_95_tpr detection_error auroc aupr_in aupr_outAP AP50 PCorr (all) PCorr (tp) fpr_at_95_tpr Ablation study on div, ta and tq tandem supporting error functions optimalVanilla div=40, ta=0, tq=10 div=40, ta=1, tq=0 div=40, ta=1, tq=1 div=40, ta=0, tq=0 div=0, ta=0, tq=0 div=40, ta=1, tq=10 div=40, ta=1, tq=100",
  ". Analysis of the impact of varying parameter Ltq in the ablationstudy": "well-balanced results compared to other factors. While thetransformer model exhibits commendable calibration whentrained with a tq factor of 100, there is a slight decline inthe Situation Monitor OOD performance. illustrates the results of comprehensive ablation ex-periments encompassing all three factors: div, ta, and tq.In the absence of diversity and tandem loss, the model ex-hibits comparable Average Precision (AP) performance butexperiences a notable decline in the FPR@95 and DE@95metrics. Notably, when the model is trained with specificparameters, namely div = 40, ta = 1, and tq = 10, itachieves optimal performance, manifesting in heightenedaccuracy and superior OOD detection capabilities. The con-",
  ". Benchmark Results": "Tab. 1 comprehensively compares baseline models (CNNand transformer-based) with state-of-the-art sample-freeOOD detection methods. Using various metrics discussed in, we assess the Situation Monitors overall objectdetection and OOD capabilities integrated into the DBEA-DINO-DETR model. Yolov3, SSD and DINO baseline mod-els are trained using original hyperparameters, and the re-ported results in Tab. 1 reflect their peak performance. Ap-plying our approach to the DINO-DETR model resultedin the DBEA-DINO-DETR model. Training it on datasetsKITTI and BDD100K showcases enhanced detection ac-curacy and improved correlation between predicted confi-dence scores and intersection over union. On the BDD100Kdataset, our DBEA-based DINO-DETR model demonstratesimproved correlation in overall and true positive predic-tions. Additionally, there is a substantial enhancement inthe Out-of-Distribution (OOD) performance for detectingCoCo images. Specifically, the OOD detection performancefor CoCo images is higher when the model is trained withBDD100K than the KITTI model. This highlights our ap-proachs effectiveness in advancing object detection andOOD performance, especially on larger datasets. Illustratedin (a), the Situation Monitor adeptly identifies Ofar",
  ". Overhead Analysis": "Given the computational intensity of transformers relative tothe CNNs, the DBEA adaptation of transformers incurs addi-tional costs due to the duplication of final regression layers.To mitigate this overhead, we limit the feed-forward chan-nels in both the encoder and decoder layers (N) of DBEA-DINO-DETR from 2048 to 1024, offsetting the overhead ofduplicating the final regression layers to create the tandemlayers. The Vanilla model trained on the KITTI dataset withN=2048 has a size of 48M, while our DBEA-DINO-DETRis only 42M, representing a 14% reduction compared tothe Vanilla model. We also observed that there is little butnegligible advantage in maintaining the same number offeed-forward channels of 2048 as the vanilla model. Theresults of the Situation Monitor presented in this section arebased on the DBEA model trained with 1024 feed-forwardchannels.",
  ". Conclusion": "In summary, this paper introduces a Situation Monitor drivenby zero-shot learning, which integrates a novel Diversity-based Budding Ensemble Architecture (DBEA) loss function.The incorporation of the DBEA loss function empowers theobject detection model to not only identify Far-OOD sam-ples but also to generalize over similar Near-OOD instanceseffectively. This prevents miss-classification as OOD, en-hancing the models adaptability and robustness. Throughan extensive ablation study with the parameters of DBEAsloss functions, we show significant improvement in detectionaccuracy and superior OOD detection outcomes comparedto baseline models and other existing methods. Additionally,our research demonstrates the scalability of the DBEA-basedmodel, validated through successful training with KITTI andBDD100K datasets. The Situation Monitor is suitable forsafety-critical applications, being 14% less computationallyintensive than the baseline DINO-DETR model.",
  "Alexandre B. Araujo, Allan G. Oliveira, and Claudia R. Jung.Giou loss for object detection. In Proceedings of the IEEEInternational Conference on Computer Vision (ICCV), pages48944902, 2019. 7": "Christoph Berger, Magdalini Paschali, Ben Glocker, and Kon-stantinos Kamnitsas. Confidence-based out-of-distributiondetection: a comparative study and analysis. In Uncertaintyfor Safe Utilization of Machine Learning in Medical Imaging,and Perinatal Imaging, Placental and Preterm Image Anal-ysis: 3rd International Workshop, UNSURE 2021, and 6thInternational Workshop, PIPPI 2021, Held in Conjunctionwith MICCAI 2021, Strasbourg, France, October 1, 2021,Proceedings 3, pages 122132. Springer, 2021. 2 Xingyu Chen, Xuguang Lan, Fuchun Sun, and NanningZheng. A boundary based out-of-distribution classifier forgeneralized zero-shot learning. In European conference oncomputer vision, pages 572588. Springer, 2020. 2 Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-JaeLee. Gaussian yolov3: An accurate and fast object detectorusing localization uncertainty for autonomous driving. InProceedings of the IEEE/CVF International conference oncomputer vision, pages 502511, 2019. 2 Marius Cordts, Mohamed Omran, Sebastian Ramos, TimoRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,Stefan Roth, and Bernt Schiele. The cityscapes dataset forsemantic urban scene understanding. In Proceedings of theIEEE conference on computer vision and pattern recognition,pages 32133223, 2016. 1 Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and LeiShu. Zero-shot out-of-distribution detection based on the pre-trained model clip. In Proceedings of the AAAI conferenceon artificial intelligence, pages 65686576, 2022. 2 Di Feng, Ali Harakeh, Steven L Waslander, and Klaus Di-etmayer. A review and comparative study on probabilisticobject detection in autonomous driving. IEEE Transactions onIntelligent Transportation Systems, 23(8):99619980, 2021.2",
  "Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Ex-ploring the limits of out-of-distribution detection. Advancesin Neural Information Processing Systems, 34:70687081,2021. 1": "Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the kitti vision benchmarksuite.In 2012 IEEE conference on computer vision andpattern recognition, pages 33543361. IEEE, 2012. 1, 6 R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nad-hamuni, A. Ferreira, M. Yuan, B. Low, A. Jain, P. On-druska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C.Tao, L. Platinsky, W. Jiang, and V. Shet.Woven planetperception dataset 2020. 2019. 1 Simon Kornblith, Mohammad Norouzi, Honglak Lee, andGeoffrey Hinton. Similarity of neural network representationsrevisited. In International conference on machine learning,pages 35193529. PMLR, 2019. 2 Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon.Accurate uncertainties for deep learning using calibrated re-gression. In International conference on machine learning,pages 27962804. PMLR, 2018. 2",
  "Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancingthe reliability of out-of-distribution image detection in neuralnetworks. arXiv preprint arXiv:1706.02690, 2017. 2": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings, PartV 13, pages 740755. Springer, 2014. 2 Wei Liu, Dragomir Anguelov, Dumitru Erhan, ChristianSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg.Ssd: Single shot multibox detector. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, TheNetherlands, October 1114, 2016, Proceedings, Part I 14,pages 2137. Springer, 2016. 3, 6",
  "Francesco Pinto, Philip HS Torr, and Puneet K. Dokania. Animpartial take to the cnn vs transformer robustness contest. InEuropean Conference on Computer Vision, pages 466480.Springer, 2022. 3": "Syed Sha Qutub, Neslihan Kose, Rafael Rosales, MichaelPaulitsch, Korbinian Hagn, Florian Geissler, Yang Peng,Gereon Hinz, and Alois Knoll. Bea: Revisiting anchor-basedobject detection dnn using budding ensemble architecture.2023. 2, 3, 4, 7, 8 Maithra Raghu, Thomas Unterthiner, Simon Kornblith,Chiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-formers see like convolutional neural networks? Advancesin Neural Information Processing Systems, 34:1211612128,2021. 2",
  "Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu.Generalized out-of-distribution detection: A survey. arXivpreprint arXiv:2110.11334, 2021. 1": "Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, YingyingChen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell.Bdd100k: A diverse driving dataset for heterogeneous multi-task learning. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 26362645,2020. 1, 6 Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, JunZhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detrwith improved denoising anchor boxes for end-to-end objectdetection. arXiv preprint arXiv:2203.03605, 2022. 2, 3, 4 Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele.Citypersons: A diverse dataset for pedestrian detection. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 32133221, 2017. 1"
}