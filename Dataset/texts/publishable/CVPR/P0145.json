{
  "Abstract": "Low-rank adaption (LoRA) is a prominent method thatadds a small number of learnable parameters to the frozenpre-trained weights for parameter-efficient fine-tuning.Prompted by the question, Can we make its representa-tion enough with LoRA weights solely at the final phaseof finetuning without the pre-trained weights?In thiswork, we introduce Progressive Compression LoRA (PC-LoRA), which utilizes low-rank adaptation (LoRA) to si-multaneously perform model compression and fine-tuning.The PC-LoRA method gradually removes the pre-trainedweights during the training process, eventually leaving onlythe low-rank adapters in the end.Thus, these low-rankadapters replace the whole pre-trained weights, achiev-ing the goals of compression and fine-tuning at the sametime.Empirical analysis across various models demon-strates that PC-LoRA achieves parameter and FLOPs com-pression rates of 94.36%/89.1% for vision models, e.g.,ViT-B, and 93.42%/84.2% parameters and FLOPs compres-sions for language models, e.g., BERT.",
  ". Introduction": "Ever since pre-trained Transformer models were intro-duced, they have shown outstanding effectiveness in a rangeof tasks within Natural Language Processing (NLP) and Computer Vision (CV) tasks. However, theirsubstantial size and the high computational demands posedifficulties in both deployment and fine-tuning.To address these challenges, several parameter-efficientfine-tuning methods have been introduced, including Pre-fix Tuning , Prompt Tuning , P-Tuning ,adapters , and Low-Rank Adaptation (LoRA) .Specifically, LoRA employs trainable low-rank matriceswithin transformer layers, which drastically cuts down thenumber of trainable parameters for fine-tuning. However,LoRA is only memory efficient during fine-tuning; when amodel fine-tuned with LoRA is used for inference, it offersno advantages over the original pre-trained model.Based upon the foundation of LoRA, we questionCan we achieve sufficient representation using only LoRA",
  "*These authors contributed equally to this work": ". The overall diagram of the PC-LoRA method. At eachtraining step, the pre-trained weights and bias gradually decay ac-cording to a decay factor , and eventually disappear and only theLow-Rank Adapter corresponding weights A, B and bias C re-main. weights in the final phase of fine-tuning, without relyingon pre-trained weights? To find the answer to this ques-tion, we propose a Progressive Compression-LoRA (PC-LoRA) method, which gradually reduces the pre-trainedweights (i.e., the base weights) during fine-tuning until theyare completely removed as shown in .In order to learn the representation of the pre-trainedmodel within the low-rank adapter, the PC-LoRA methodprogressively attenuates the output of the pre-trainedweights during the fine-tuning phase. This decay will re-sult in the loss of information, which prompts the low-rankadapter to increasingly attempt to compensate.Throughthis process, the representations of the pre-trained modelwill effectively flow into the low-rank adapter. Therefore,by the end of the training, the LoRA weights may containboth the representations of the original weights and the up-dates made for fine-tuning. This capability demonstrateshow PC-LoRA methodically achieves the dual objectives oflow-rank compression and parameter-efficient fine-tuning.Our PC-LoRA drastically reduces model parameters andcomputes significantly with a slight accuracy drop, achiev-ing up to 94.1%p parameter reduction and 89.1%p FLOPsdecrease for vision models e.g., ViT-B , and up to",
  "arXiv:2406.09117v1 [cs.CV] 13 Jun 2024": "93.5%p parameter and 84.2%p FLOPs reduction for lan-guage models, e.g., BERT . shows several com-pressed models by simply adjusting low rank (r) in theLoRA weights, demonstrating that our PC-LoRA exhibitsflexibility and scalability. In addition to reducing the modelbudget, PC-LoRA provides further advantages. Its layer-wise approach allows the application to any model with lin-ear layers, enhancing scalability. Moreover, PC-LoRA iscompatible with other methods like quantization , en-abling combined use to improve model efficiency.",
  ". PC-LoRA Method": "Our method, termed Progressive Compression with Low-Rank Adaptation (PC-LoRA), is designed to incrementallycompress a model by diminishing and eventually removingthe influence of pre-trained weights throughout the trainingprocess. In this approach, both pre-trained model weightsand low-rank adapter weights are initially used for the out-put computation. As the training progresses, the pre-trainedmodel weights gradually fade away according to a decayfactor and are eventually removed at the end of the training,leaving only the low-rank adapters. The overall concept isillustrated in .PC-LoRA Layer: Similar to the LoRA method, ourPC-LoRA method also attaches low-rank adapters to lin-ear layers. In PC-LoRA, the layers consist of the weightand bias from the pre-trained model, complemented by twolow-rank adapter weights, A and B with a rank (r), whichreplace the pre-trained models weight W. Additionally,weight C is used to substitute for the bias.",
  ". Training Configuration": "The models output at each layer is calculated by adding thedecayed pre-trained model output (Di) with the low-rankadapter output (Li), where i denotes the index of the spe-cific LoRA layer. A decay factor is used to graduallylessen the impact of the pre-trained model output, decreas-ing from 1 to 0. The details of the decay factor schedulingwill be described in Sec. 2.2. While training, the final out-put (Fi) is computed as:",
  "Fi = Di + Li.(3)": "During training, Ai, Bi and Ci are trainable, whilethe pre-trained model weights (Wi) and biases remain un-changed. The bias is not included in Equation 1 for simplic-ity. Initially, Ai is initialized with a random Gaussian dis-tribution, and both Bi and Ci start at zero, meaning BiAistarts at zero as well.After the completion of training, the output from the pre-trained model is completely eliminated by the decay factor, Model Size (Million Parameters) Accuracy % ViT_tiny ViT_small ViT_Base ViT_Base r=32 ViT_Base r=64 ViT_Base r=128 ViT_Base r=256 Full-fintuned ViTPC-LoRA",
  ". The performance comparisons based on different com-pression ratios of PC-LoRA using ViT-B compared to the fullyfinetuned ViT-B on CIFAR-10": "resulting in the forward pass being represented by the fol-lowing equation: Fi = Li, indicating that only the low-rankadapter is used. Note that when we trained exclusively withthe low-rank adapter, we observed no improvement in themodels performance; specifically, it achieved only 10% ac-curacy on CIFAR-10 and 50% accuracy on IMDb, whichdemonstrates that our method is more effective than the sim-ple layer-pruning method.The PC-LoRA method is optimized according to the lossterm as follows:",
  "Ltotal = Ltask(yi, yi) + (1 ) LfeatKD(FS, FT ) (4)": "The total loss Ltotal is defined by combining the task loss,Ltask, with the feature-based knoweldge distillation loss,LfeatKD. Ltask represents the loss for fine-tuning on a down-stream task, such as cross-entropy loss for classificationtasks with labels. LfeatKD is computed as the Mean SquaredError (MSE) between the intermediate features of the stu-dent model FS and those of the teacher model FT as below:",
  "m=1MSE(FSm, FTm),(5)": "where M is the number of PC-LoRA applied layers. Thestudent S is the model with PC-LoRA applied, whichincludes the decayed pre-trained model and low-rankadapters, while the teacher T is the original pre-trainedmodel. For the layers where PC-LoRA is applied, incorpo-rating the difference between the intermediate feature mapsof S and T into the loss term acts as a regularization. Sincethe teacher model T remains in its original, un-finetunedstate, adding this term helps to prevent the S, which willeventually retain only the low-rank adapters, from overfit-ting to the downstream task while training. The ablationstudy for the effect of LfeatKD is conducted in Tab. 6. Please . The three types of Decay Factor Scheduler: Sine, 1-Cosine, and Linear. As iterations progress, the decay factor de-creases from 1 to 0, affecting the rate at which the original weightbecomes less influential. Initially, a factor of 1 means the pre-trained models weights are entirely preserved, while a factor of 0indicates the complete transition to the new weights.",
  "Selected Decay function(n, q)if n < q,0if n q": "Here, q represents the endpoint of the decay phase, set asa proportion of the total iterations N. The selected decayfunction dictates how (n) is reduced from 1, influencingthe rate at which the influence of the original weights di-minishes. Initially, (n) = 1 indicates that the pre-trainedmodel is fully intact, whereas (n) = 0 signifies that theoriginal weights have been completely phased out.Thevalue q is ideally set between 40% and 80% of total iter-ations, within which range significant performance differ-ences have not been observed. From an ablation study ofthe decay function in Tab. 7, we set sine as a default forall experiments.",
  ". Implementation Details": "In this paper, we investigated model performances in twobenchmark tasks: image classification with the CIFAR-10 dataset and text classification using the IMDb dataset. Training details are provided in Appendix B and E.We compared PC-LoRA with two other methodsfor fine-tuning pre-trained models.The first method,Full Fine-Tuning (Full-FT), updates all param-eters of the model. The second, LoRA Fine-Tuning (LoRA-FT), incorporates LoRA but does not alter theembedding layers and uses a fixed rank of 32. The PC-LoRA approach uses the same configuration as the LoRA-FT method, focusing training only on layers modified forlow-rank adjustments.Furthermore, we evaluated how different ranks influencethe compressed model size and performance, comparingwith various sizes of ViT models, as illustrated in . Similarly, we extended our analysis to BERT models, asdetailed in Appendix D.In our research, we conducted ablation studies on thePC-LoRA method, detailed in Appendix C. These studiesincluded exploring different types of Decay Factor Sched-ulers to determine their impact on performance.Addi-tionally, we investigated the optimal ratio for feature-basedknowledge distillation loss, denoted as the value in Equa-tion 4, to enhance the accuracy of the compressed model.",
  ". Main Results": "Tab. 1 & Tab. 2 presents a comparative analysis of the per-formance of various vision and language models on the CI-FAR10 and IMDb benchmarks, utilizing Full-FT, LoRA-FT, and the proposed PC-LoRA method. The performanceof Full-FT and LoRA-FT is similar. When compared to theLoRA-FT method, which uses the same number of param-eters for training, the PC-LoRA method shows an averageperformance degradation of about -3.56%p. Despite suchperformance degradation, the final outcome of PC-LoRA isa compressed model, resulting in a reduction of 94.36%pexcluding the embedding layers, and an average 89.1%pdecrease in total GFLOPs in vision models. For NLP mod-els, also excluding the embedding layer, the reductions areabout 93.42%p, and 84.2%p in total GFLOPs. As the re-sults indicate, the PC-LoRA method demonstrates a favor-able trade-off on both CIFAR10 and IMDb benchmarks, bysignificantly reducing the GFLOPs and model parameterswhile only modestly compromising accuracy.In , the performance of various ViT models,including compressed ViT Base models with PC-LoRAmethod at different ranks, along with ViT Base, Tiny, andSmall models, is displayed. The x-axis represents the modelsize, and the y-axis shows the test accuracy on CIFAR10.The points marked with stars indicate the performance ofthe PC-LoRA compressed models, which are comparableto Vit Small and Tiny. Two key observations can be madefrom the results in . First, the models compressedusing the PC-LoRA method outperform the ViT tiny andViT small models despite having a similar model size. Sec-ond, the PC-LoRA method allows for compression by ad-justing a factor rank, enabling the preservation of the orig-inal model structure while adjusting its size to the desiredlevel. Therefore our method is capable of generating mod-els not only at the size levels of ViT Tiny, Small, and Base",
  "RoBERTa Full-FT94.6748.37124.65LoRA-FT94.4048.37124.65PC-LoRA92.597.6044.50 (93.42%)": ". Comparison of PC-LoRA (r=32), Full-FT, and LoRA-FT methods on different pre-trained models with IMDb, mea-suring GFLOPs and parameters during inference. Parenthesesindicate the percentage reduction in parameters with LoRA-FTcompared to PC-LoRA, excluding embedding layers. but also at any desired model size.Similarly, displays the performance of vari-ous BERT models, including compressed Bert Base modelswith PC-LoRA method at different ranks, along with BERTBase, Medium, and Small. shows that similar tothe results in , PC-LoRA compressed models out-perform similar-sized models. This consistency across dif-ferent models demonstrates the robustness and versatility ofapplying the our method for both CV and NLP tasks.Attention Visualization. shows a comparison ofattention maps across different models and inputs. The leftcolumn displays input images of a cat, a parrot, and a flower.The middle column shows the top three attention maps froma ViT Base model fine-tuned on CIFAR-10, highlighting ar-eas of highest activation. The right column features similarattention maps from the model compressed with PC-LoRA,also fine-tuned. Both columns show that the quality of theattention maps is similarly high, indicating that compres-sion does not significantly degrade performance. However,as detailed in Appendix F, it is important to note that thenumber of heads that effectively contribute to these high-quality attention maps is fewer in the PC-LoRA compressedmodel. This reduction in effective heads suggests that thecompression has been successful at reducing dimensionalitywithout substantially affecting the models ability to focuson relevant features in the input images.",
  ". Conclusion": "In this work, we have explored the ability to combine modelcompression and parameter-efficient fine-tuning throughlow-rank adaptation. By progressively attenuating the out-put of the pre-trained weights and allowing the LoRAweights to bridge the gap of the decayed representationof these weights, our PC-LoRA achieves significant modelcompression with only a slight drop in performance.Future work will explore the following potential ad-vancements: We will improve the Decay Factor Sched- .Attentionmapvisualizationwith[CLS]to-ken:Full-finetuned ViT-B (85.8M) vs.PC-LoRA ViT-B w/rank=32 (5.94M). Even with a much smaller model size, our com-pressed ViT shows comparable attention map quality compared tothe full-finetuned ViT-B. uler to enhance compression performance.Additionally,we plan to refine the initialization of low-rank adapters.Currently, it is initialized with a random Gaussian distri-bution for A and zeros for B, essentially starting the com-pression from a basic setup. To enhance the effectivenessof the PC-LoRA method, we will employ a more sophisti-cated approach by using the results of Singular Value De-composition (SVD) of pre-trained weights. This will serveas the initial information for A and B, facilitating a pro-gressive compression process. Moreover, we plan to applyPC-LoRA method to large-scale models and datasets andevaluate our method with other compression strategies, in-cluding pruning and low-rank factorization . Acknowledgement.This research was partly supported by Insti-tute of Information & communications Technology Planning & Evalua-tion (IITP) grant funded by the Korea government (MSIT) (No. RS2022-00187238, Development of Large Korean Language Model Technologyfor Efficient Pre-training) and Brian Impact Foundation, a non-profit or-ganization dedicated to the advancement of science and technology forall. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners, 2020. 1",
  "A. Related WorksA.1. Low-Rank Adaptation (LoRA)": "LoRA fine-tuning technique involves adding a few trainable parameterswhile keeping the original model parameters fixed. This is done by mod-ifying the pre-trained weight matrix W0 with a small, low-rank updateW = BA, where B and A are matrices of smaller dimensions. Duringtraining, only A and B are updated, with W0 remaining unchanged. Themethod calculates the new output h by adding W0x and BAx together.Our approach also includes adding a bias term and scheduling the reduc-tion of certain parameters to improve model efficiency and restoration.",
  "A.2. Feature-based Knowledge Distillation": "Knowledge distillation (KD) encompasses three principal methodologies:Response-based , Feature-based , and Relation-based KD .Each specializes in different aspects of knowledge transfer from a teacherto a student model. Given that PC-LoRA method is layer-wise, we haveutilized Feature-based KD to leverage the intermediate layers semanticinformation in the training process. Feature-based KD focuses on the useof feature maps from both the student and teacher models, enhancing thelearning process with a strategic focus on the intermediate representations.A distance function, such as Mean Squared Error (MSE) loss or Kullback-Leibler divergence loss is used to quantify the similar-ity of the matched features. Our adapter modules generate feature mapsthat match the size of the original models, eliminating the need for alter-ations required by other feature-based knowledge distillation methods andthereby avoiding associated losses.",
  "B.2. Settings for Training": "For the optimization of the PC-LoRA method, we used AdamW op-timizer, exploring learning rates within 1e-2, 1e-5. In terms of learningrate scheduling, we adopted CosineAnnealingLR , which is set with aminimum value of 0. We used the batch size to be 64 for CIFAR-10 imageclassification task and 20 for IMDb text classification task. The image clas-sification was trained for 62,500 iterations, and the text classification task,100,000 iterations. Additionally, all experiments were conducted on RTX5000 GPUs using PyTorch version 2.1, Python 3.10, and CUDA 11.8.0.",
  "C.1. Loss Scale factor,": "We differed Decay Factor Scheduler with sine: Tab. 3, linear: Tab. 4, and1-cosine: Tab. 5. For each table, we conducted an ablation study for theloss scale factor value in Equation 4. Its important to note that an al-pha value of 1 implies training solely with the Ltask task loss and settingthe value to 0 results in no fine-tuning, which explains our decision tocompare a values ranging from 0.2 to 1.0. Our results indicate that, gen-erally, performances are better with values lower than 1. Specifically, inthe CIFAR-10 task, there was an average performance increase of 1.25%p,while in the IMDb task, there was an increase of 2.64%p. This tendencywas observed regardless of the type of Decay Factor Scheduler, demon-strating that lower values enhance performance more effectively than an of 1.",
  "D. Compression comparison": "Weve conducted a comparison of model compression using PC-LoRAon both ViT Base and BERT Base architectures. Compression was per-formed across a broad spectrum of rank values, including 16, 32, 64, 128,and 256, to achieve models of various sizes. Upon comparing the perfor-mance of the compressed models with their corresponding baseline modelsof similar size, both ViT and BERT architectures demonstrated an overallimprovement in performance.",
  "ViT Base r=325.5995.04ViT Base r=649.8796.44ViT Base r=12819.7597.08ViT Base r=25638.6297.56": ". The performance of models based on different size ofViT vs PC-LoRA diversly compressed ViTBase : Fine-tuned withCIFAR-10. The table represents specific model sizes and perfor-mance as shown in . ViT Tiny, Small, and Base representthe baseline models, while ViT Base r=32, 64, 128, 256 indicatemodels that have been compressed to various sizes through PC-LoRA, applied to the ViT Base model.",
  "BERT Base r=1626.6592.40BERT Base r=3229.3492.67BERT Base r=12845.4993.12BERT Base r=25667.0293.36": ". The performance of models based on different sizes ofBERT vs PC-LoRA diversly compressed BERT Base: Fine-tunedwith IMDb. The table represents specific model sizes and perfor-mance as shown in . BERT Small, Medium, and Baserepresent the baseline models, while BERT Base r=16, 32, 128,256 indicate models that have been compressed to various sizesthrough PC-LoRA, applied to the BERT Base model."
}