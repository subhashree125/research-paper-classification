{
  "Customization": ". An example illustrating the use of our proposed CustomText method. It is a simulation of an advertisement designing workflow foran Ad campaign Adopt a Pet, where initial image (A) is generated using text-prompt, P by stable diffusion base model . Subsequently,the user can customize the font attributes of the text by using a user-interface (B). User can also perform incremental editing i.e., appendor remove extra text lines, by using space character ( ) on top of visible texts (C). The process can repeat until the end-user is satisfiedwith the final generated results (D).",
  "Abstract": "Textual image generation spans diverse fields like adver-tising, education, product packaging, social media, infor-mation visualization, and branding. Despite recent stridesin language-guided image synthesis using diffusion mod-els, current models excel in image generation but strugglewith accurate text rendering and offer limited control overfont attributes. In this paper, we aim to enhance the syn-thesis of high-quality images with precise text customiza-tion, thereby contributing to the advancement of image gen-eration models.We call our proposed method Custom-Text. Our implementation leverages a pre-trained TextD-iffuser model to enable control over font color, background,and types. Additionally, to address the challenge of accu-rately rendering small sized fonts, we train the ControlNetmodel for a consistency decoder, significantly enhancingtext-generation performance. We assess the performanceof CustomText in comparison to previous methods of textualimage generation on publicly available CTW-1500 dataset",
  ". Introduction": "The domain of text-to-image synthesis has witnessed re-markable advancements, with diffusion models emerging asa key paradigm in this domain. These models provide qual-ity and diversity of generated content through the diffusionprocess. Diffusion models such as DALLE-2 , Ima-gen , and Stable Diffusion , leverage the semanticrichness inherent in textual prompts. Recent breakthroughsin diffusion models , , , , , offer dis-tinct editing control advantages.Despite the significant progress in diffusion-based meth-ods, generating textual content within the image with com-plex font attributes is still a challenge. The generation oftextual-images (or images with textual information) has di-verse applications in industries such as entertainment, ad-vertising (see Figure. 1), education, and product packaging.Creating high-quality text-images in diverse formats such",
  ". Example demonstrating the control of our proposed method CustomText over fonts color, fonts types and fonts background onthe base image": "as posters, book covers, etc, conventionally requires pro-fessional skills and iterative design processes. Traditionaldigital methods involving manual labor may often yieldunnatural appearances or digital artifacts due to complexbackground textures and lighting variations. Moreover, thedirect rendering of text in a proper font on top of the gener-ated image lacks harmonization of text with the backgroundand does not produce a visually appealing image. Current efforts to enhance text rendering quality haveturned to diffusion models, exemplified by pioneeringframeworks like Imagen , eDiff-I , and Deep-Floyd . These models have demonstrated improved textgeneration capabilities by using better text encoders, e.g.,leveraging T5 series text encoders over the CLIP text encoder. Additionally, better trained language modelsdemonstrate increasing capability for text-rendering whenthey increase the parametric strength, e.g. Parti showsa dramatic improvement in text-rendering when the modelparameters increase from 350M to 200B. However, thesemodels lack comprehensive control over the generation pro-cess. Some concurrent works, such as GlyphDraw andTextDiffuser , aim to enhance control by conditioning onthe location and structures of Chinese characters and En-glish characters, respectively. However, the limitation of notsupporting multiple text bounding-box generation restrictsthe applicability of GlyphDraw to various text-imagescenarios, such as posters and book covers. On the otherhand, TextDiffuser addresses the challenges in creatingmultiple text boxes within images, but still fails in the gen-eration of dense and small text (see (a)). Addition-ally, both of these models cannot control font attributes dur-ing generation. In a very recent work by , authors haveproposed an architecture where a text encoder is trainedover the synthetic dataset to generate small and dense texts.Though the results are very impressive, the model is veryresource-intensive. To address these challenges, we present a novel approachnamed CustomText for generating images with customizedfont attributes including font type, color, and background(refer to ). Our key contributions in this paper areas follows:",
  "We supplement consistency model decoder witha ControlNet-based architecture to provide better ren-dering of small-sized fonts": "We assess the reconstruction performance of the traineddecoder over the test-split of CTW-1500 dataset andshow that it has improved performance over methods suchas ControlNet-canny and TextDiffuser . We alsodemonstrate the efficacy of our proposed method for small-font text generation over the self-created SmallFontSizedataset containing 200 examples of textual prompts withcorresponding character maps.",
  ". Proposed Method: CustomText": "We aim to generate images or perform image in-paintingwith controlled text generations. This control extends tofont attributes such as type, size, color, and background, allof which need to seamlessly integrate into a given referenceimage layout (in the case of image in-painting). Our ob-jective is to ensure a resulting image with a high degree ofharmonization and photo-realism. To achieve this objective,we identify two main areas of improvement over currentmethods such as , : (1) Provide explicit control oftext attributes during textual image generation. (2) For lay-outs that consist of small-sized characters, generate visuallyclear characters.",
  "params": ". Stage 1 pipeline for the generation of character mask (Mchar) and conditional mask (Mcond) using input textual promptand control parameters defining font-attributes. The transformer encoder-decoder architecture takes input prompt and for each word,unique non-overlapping bounding box is extracted. The input control parameters define the different font-attributes such as color, type,background, which enable the renderer to generate the desired conditional mask.",
  ". Text Customization with Attribute Control": "Our task is to generate image x, given a textual promptP with text-content g and a mask m representing a regionof interest for g. This generated image should incorporatethe formatted text F(g) within the designated region, whereF represents a set of formatting functions containing con-trol parameters that govern the manipulation of font color,type, and background. We propose a two-stage pipeline forcustom text generation.",
  "Pipeline: First stage": "In the first stage, as shown in , we obtain twomasks, namely, character mask Mchar and conditionalmask Mcond. The character mask Mchar defines the spatialposition for the generated text g, where we allocate a rect-angular box for each character generation. The conditionalmask Mcond specifies the necessary attributes of g based onthe functions in F. This stage takes textual prompt P asinput, with generated text g specified through single quotes. We utilize a Layout Transformer based architecture topredict a bounding box B for g in the mask image. Subse-quently, the character mask is created by using this bound-ing box information Mchar (B, g). This informationis also utilized by the rendering module for each characterand combined with the control parameters defined by F, toobtain the conditional mask, Mcond (B, F(g)).",
  "Pipeline: Second Stage": "In the second stage, as shown in , we utilize thepre-trained model of TextDiffuser , which provides spa-tial control (only) using the character mask. We modifythe inference pipeline to provide additional attribute con-trol in the text generation process through conditional maskMcond. For the purpose of generation, the diffusion modelis initialized with random Gaussian noise, xT , and charactermask, Mchar. We utilize DDPM scheduler to computeimage inversion of conditional mask Mcond. Our aim is toinfuse the effect (essence) of conditional mask features in",
  "(c) CustomText decoder": ". An example use-case of textual-image generation such as an advertisement with text Adapt a Shelter Pet today. The images 5a,5b and 5c are generated using TextDiffuser decoder, DALLE-3 Consistency decoder and our proposed CustomText decoder, respectively.The CustomText shows superior control over accurate text-generation as it is evident that it writes Shelter accurately in comparison toother methods. the denoising steps of xT . Hence, we obtain the noisy sam-ples Mcond for different timesteps t in [0, T] to obtain a listof latents, Qcond.To ensure the effects of Mcond in the generation process,the self-attention map of xt must encapsulate the effects ofqcond,t, where qcond,t represents Qcond sample at timestept. To affect this, a progressively reducing time-step depen-dent weighted qcond,t is added to xt for different t. Thisweighted addition diminishes the impact of Mcond on thefinal stages of the generation and helps avoid sharp bound-aries between the rendered image and the conditional im-age.",
  ". Generation of accurate small-sized text": "Many latent diffusion models employ Variational Au-toencoder (VAE) network to transform images intolow-dimensional latent spaces, to enhance computationalefficiency during the training and inference of diffusionmodels. However, when images are compressed into lower-dimensional latent spaces, the fine details might be lost.This can result in the generated images not accurately repro-ducing the original data, which could be problematic whereprecise details, such as the rendering of small-sized fonts,are crucial. To address this issue, we propose two alternateapproaches, where one approach uses a VAE-based latentdecoder with split and merge on latents, and another ap-",
  "patches 1-9": ". VAE based Decoder Enhance model. The latent comingfrom the diffusion model is splitted into 9 overlapping fragmentsin the Splitting Module. The resized fragments are upscaled anda trainable model learns to enhance the quality of the generation.Finally, the enhanced fragments are assembled to create final im-age in the original resolution by the merging module.",
  ".Modified Consistency Decoder architecture.Theweights of original decoder D are locked and a trainable Con-trolNet block C is appended": "treated as large-sized characters if they are scaled individu-ally. The cropped and scaled image patch may have upscaleblurs because of low resolution. However, if we can fix theblurring of the upscaled patch and consequently downscaleand append it back to the original resolution, then recon-struction quality should improve. Taking cues from this in-tuition, we propose an architecture, as shown in ,consisting of three parts: Splitting Module, Upscale En-hancer, and Merging module, which takes the latent featuremap as an input and produces the reconstructed image.Splitting Module: In this module, there are no learn-able parameters. It takes the input of feature map D(xg) R(hw)d, where D is the VAE decoder from a trainedstable diffusion model and h w is the size of the inputfeature map. The splitting module crops the feature mapinto 9 overlapping patches, such that each patchs dimen-sion is h/2 w/2. These patches are then upscaled usingbilinear interpolation back to their original image resolu-tion.Upscale Enhancer Module: Here, the objective is to re-fine the upscaled image. To achieve this, we use the se-quence of zero convolution operations. The zero convolu-tion refers to a 11 convolution with both weight and biasinitialized as zeros. The choice of 0-convolution is deliber-ate because the initialization of weights and biases with 0will not alter the input feature map in the forward pass and,hence, it does not distort the original pre-trained VAEs de-coder output. However, after one iteration update with non-zero reconstruction loss, weight and bias terms are updatedto non-zero, and the normal training procedure continues. Thus, zero-convolution provides a safer way to enhance re-construction without modifying the original weight of de-coders. As inspired from , the objective function usedto train the model consists of reconstruction loss (L2 loss),along with character-aware loss.Merging Module:Finally, the upscaled enhancedpatches should be merged to the final resolution.Sincethe patch resolution matches the final resolution, mergingthe patches to their original resolution (one used duringsplitting) will degrade the enhancements. Thus the patchesare merged, and weighted-addition aggregates the enhance-ments with the decoders final output. To merge the over-lapping patches, we use weighted merging to carefully allotmore weight to the patches that are at the center of the at-tending pixels in the final image.While this method is effective in enhancing the recon-struction quality and performs better than the vanilla de-coder, it still requires a large training dataset to learn. Fur-thermore, another drawback of the method is that the split-ting and merging dimensions of patches are pre-defined, andtherefore they might not always overlap with actual smallcharacter regions.",
  "Consistency Decoder with Character Map Guid-ance": "We propose an alternative approach to address the small-sized text generation, where we use a decoder that can gen-erate high-quality small characters from the latent represen-tations. Our approach, as shown in , involves in-troducing a Character Mask to a pre-trained Consistencydecoder Model (CM) that utilizes the semantic in-formation of the characters. The intuition is that by incor-porating the character guidance map (initially utilized fortext generation) into the ControlNet architecture, thedecoder gains additional guidance for effective character re-construction. The CM decoder then proves effective in pre-serving the identity and style of input characters, generatingrealistic and diverse small characters within the latent space.The consistency diffusion model consists of a decodernetwork f that takes as input a noise tensor zt sampledfrom a Gaussian distribution N(0, I), and outputs an im-age x0 that corresponds to the starting point of the diffusionpath trajectory. The model can generate images in one stepby sampling a noise tensor zT from the final timestep of thediffusion process and passing it through the decoder net-work f.Giventhepre-trainedconsistencymodeldecoderD(., .), we freeze the parameters and introduce Control-Net model C, with trainable parameters , as shown in the. The architecture takes latent vector, lt as inputfor D and character mask M (as defined in .1.1)for C. By adding the ControlNet architecture, we define",
  ". Implementation": "We use the pre-trained TextDiffuser model and stable-diffusion-v1-5 model for our pipeline.For the decoder,we use the pre-trained DALLE-3 consistency decoder avail-able publicly. Since, the original consistency decoder oftendistorts the small-sized characters in the decoded image,we tried to provide character map assistance for its cor-rection (Refer ). We have followed original Con-trolNet architecture to modify the consistency model.We have trained the model for 3500 steps over CTW1500dataset, with effective batch size of 96, using gradient accu-mulation. During inference, the generated image resolutionof 512512 is used, which is computed over CFG scale 7.5.",
  ". Results": "To evaluate the reconstruction performance of the trainedmodel, we have used MSE , PSNR and SSIM asmetrics and evaluated over CTW-1500 test set. We presentthe comparison results of the image generation with theControlnet-canny model, Textdiffuser and VAE based De-coder Enhance in . It is evident that our CustomTextdecoder performs best in all the three metrics. Furthermore,to evaluate readability, we have used EasyOCR toverify the quality of the reconstructed image in .Here, exact match of individual words is considered to com-pute the results. Additionally, we present the comparisonresults of CustomText for generating small-sized texts in theimages in over SmallFontSize dataset using OCRperformance and ClipScore . Although our ControlNetbased Consistency decoder model outperforms other exist-ing methods in the OCR results, however, original TextDif- fuser model performs better in terms of Clipscore by a smallmargin of 0.0015. If we compare between the CustomTextdecoder and Decoder enhance model, its important to notethat the CustomText decoder model has substantially fewerparameters. Thus, we expect to see a boost in performanceof CustomText when trained on larger dataset with compu-tationally heavier decoder enhance having more parameters.Due to the limited dataset constraint, we leave this experi-ments for future work. Next, please note that we only havecharacter map in the SmallFontSize dataset which guidesthe model to generate images, and thus, we could not com-pare on FID metric , as it requires ground truth imagesto compute/compare gaussian distributions. Furthermore,we qualitatively compare the performance of our proposedCustomText decoder against TextDiffuser decoder andDALLE-3 Consistency decoder for small text genera-tion. compares the three generated images con-taining texts with varying font-sizes. When dealing withsmaller characters, digital artifacts become more apparent,highlighting any spelling errors. However, not such largeobservation is visible in case of large size characters.Additionally, we demonstrate the controlling propertiesof the CustomText over generated texts in . Wealso provide qualitative comparison results of CustomTextwith original TextDiffuser in . Few more examplesof image generations using our CustomText are presentedin . Moreover, we demonstrate the application ofour proposed CustomText method for automatically gener-ating advertisements. In fields of digital marketing, the de-mand for visually compelling advertisements has increased.Thus, to meet this demand efficiently, its important to auto-mate and simplify the creative design process. One exampleworkflow for generating ads automatically is shown in Fig-ure 1.",
  ". Conclusion": "In digital marketing, there is an increased demand forcompelling advertisements.Text messages are an inte-gral part of most advertisements and marketing campaigns.While the generic diffusion-based generative models are be-coming proficient in better text rendering, there is no inher-ent control over text placement and attributes. In this paper,we presented CustomText as a method for generating imageswith high-quality customized fonts. Our approach providesnovel architectures to solve this problem. Even as we feelthat more research is required for fuller control through dif-fusion models, our experimental results highlight the supe-rior attribute control of our method in the generation of textcompared to the previous methods. We also demonstratedthat our method supports incremental editing, as shown in, to obtain customized and better-quality textualimages in content creation tasks such as posters, advertise-ments, and marketing materials. Going forward, we aim toprovide support for multi-lingual characters as the currentsystem is only trained for Latin alphabets.",
  "AdobeInc.Adobephotoshop. 2": "Youngmin Baek, Bado Lee, Dongyoon Han, SangdooYun, and Hwalsuk Lee. Character region awarenessfor text detection. In Proceedings of the IEEE/CVFconference on computer vision and pattern recogni-tion, pages 93659374, 2019. 8 Yogesh Balaji, Seungjun Nah, Xun Huang, ArashVahdat, Jiaming Song, Karsten Kreis, Miika Aittala,Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ed-iffi: Text-to-image diffusion models with an ensembleof expert denoisers. arXiv preprint arXiv:2211.01324,2022. 2 James Betker, Gabriel Goh, Li Jing, Tim Brooks,Jianfeng Wang, Linjie Li, Long Ouyang, JuntangZhuang, Joyce Lee, Yufei Guo, et al. Improving im-age generation with better captions.Computer Sci-ence. openai. com/papers/dall-e-3. pdf,2:3, 2023. 2, 5",
  "MeanSquaredError.Meansquarederror. squared error. 8": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-nik, Amit H Bermano, Gal Chechik, and DanielCohen-Or. An image is worth one word: Personal-izing text-to-image generation using textual inversion.arXiv preprint arXiv:2208.01618, 2022. 1 Kamal Gupta, Justin Lazarow, Alessandro Achille,Larry S Davis, Vijay Mahadevan, and Abhinav Shri-vastava. Layouttransformer: Layout generation andcompletion with self-attention. In Proceedings of theIEEE/CVF International Conference on Computer Vi-sion, pages 10041014, 2021. 3",
  "StructuralSimilarityIndexMeasure.Structuralsimilarityindexmeasure. similarity. 8": "Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,Zhongang Qi, Ying Shan, and Xiaohu Qie.T2i-adapter: Learning adapters to dig out more control-lable ability for text-to-image diffusion models. arXivpreprint arXiv:2302.08453, 2023. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. Learning transferable visual models from naturallanguage supervision. In International conference onmachine learning, pages 87488763. PMLR, 2021. 2 Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the limits of trans-fer learning with a unified text-to-text transformer. TheJournal of Machine Learning Research, 21(1):54855551, 2020. 2",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol,Casey Chu, and Mark Chen.Hierarchical text-conditional image generation with clip latents. arXivpreprint arXiv:2204.06125, 1(2):3, 2022. 1": "RobinRombach,AndreasBlattmann,DominikLorenz, Patrick Esser, and Bjorn Ommer.High-resolution image synthesis with latent diffusion mod-els.In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages1068410695, 2022. 1, 8 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, YaelPritch,Michael Rubinstein,and Kfir Aberman.Dreambooth:Fine tuning text-to-image diffusionmodels for subject-driven generation. In Proceedingsof the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 2250022510, 2023. 1 Chitwan Saharia, William Chan, Saurabh Saxena,Lala Li,Jay Whang,Emily L Denton,Kam-yar Ghasemipour, Raphael Gontijo Lopes, BurcuKaragol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language under-standing. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022. 1, 2 Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based se-quence recognition and its application to scene textrecognition.IEEE transactions on pattern analysisand machine intelligence, 39(11):22982304, 2016. 8"
}