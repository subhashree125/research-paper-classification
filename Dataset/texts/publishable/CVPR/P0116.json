{
  "Abstract": "Multimodal Foundation Models (MMFMs) have shown remarkable performance on various computervision and natural language processing tasks. However, their performance on particular tasks such asdocument understanding is still limited. They also require more compute, time, and engineering resourcesto finetune and deploy compared to traditional, unimodal models. In this report, we present MultimodalStructured Generation, a general framework which constrains the output logits of frozen MMFMs to forcethem to reason before responding with structured outputs that downstream APIs can parse and use. Weprovide a detailed account of our approach, including the technical details, theoretical discussions, andfinal evaluation results in the 2nd Multimodal Foundation Models Challenge hosted by the ComputerVision and Pattern Recognition (CVPR) conference. Our approach achieved the second highest score inthe hidden test set for Phase 2 and third highest overall. This shows the methods ability to generalizeto unseen tasks. And that simple engineering can beat expensive & complicated modelling steps as wefirst discussed in our paper, Retrieval Augmented Structured Generation: Business Document InformationExtraction as Tool Use.1",
  "Introduction": "Multimodal Foundation Models (MMFMs) have been made possible by recent advances on grafting togetherparts from different foundation models pretrained on specific modalities . The resulting \"Frankenstein\"models show remarkable results in diverse & multimodal tasks. However, their performance on documentunderstanding is still lacking.In this report, we present Multimodal Structured Generation, a general framework for controlling the outputformat of multimodal models. In this challenge, in particular, we use it to force frozen multimodal models toreason before responding with a structured output that downstream APIs can parse and use.Our team actually only learned about the challenge roughly two days before the submissions deadlinethefirst 24 hours of which was wasted working with a commercially-available model which, after clarifying withthe organizers, we were not allowed to use for the challenge. We neither had the time, compute resources,nor the budget to implement complicated modelling steps. Fortunately, we have already shown in our previouswork on Retrieval Augmented Structured Generation (RASG) that simple engineering could supplant morecomplicated modelling stepsat least on document information extraction tasks . RASG has four compo-nents: (1) Structured Generation (2) Retrieval Augmented Generation , (3) Supervised Finetuning, &(4) Structured Prompting . But for this challenge, we only managed to implement Structured Generationbut with multimodal models instead of just an LLM as in our previous work.Our team placed 2nd in Phase 2 of CVPRs 2nd MMFM Challenge and 3rd in the overall rankingsbeating multiple teams that have finetuned their own multimodal models. Phase 2 contains never-before-seenevaluation datasets. This shows the generality of the approach to unseen tasks. And it being finetuning-freemakes it easy and cheap for other teams to replicate. All of our scripts and evaluation results can be accessedin",
  ": Multimodal Structured Generation": "numbers, the LLMs output may seem runnable but in fact produce errors when ran on an IDE. There is aspectrum of approaches that guarantee the generative models outputs can indeed be usable by downstreamsystems. On one end are what we call \"soft\" constraints which simply ask or instruct the models to follow aspecified schema and retry until they succeed . On the other end are \"hard\" constraints which zero-out thelogits of invalid tokens altogether .Structured Generation can then easily be applied to generative Multimodal Models as the latter alsoproduce logits which we can then zero-out if they correspond to invalid tokens. See .",
  "Implementation Details": "We used Huggingfaces Inference Endpoints API to deploy the multimodal models and Huggingfaces TextGeneration Interface (TGI) API to call inference requests on the models .For Phase 1, since the test dataset is already publicly-available, we decided not to focus on it as theresults would not show the generality of our approach. Thus, we only used an unaugmented Llava v1.5 forPhase 1 . For Phase 2, we used Llava-Next (v1.6) augmented with Structured Generation to generatethe results for the mychart and myinfographic datasets . But to maximize results, we used aversion of Nous Hermes 2 Pro - Mistral 7B model augmented with Structured Generation for the mydocdataset .We finetuned this version of Hermes 2 Pro on the DocILE dataset not in this chal-lenge, but in our previous work . This model can be downloaded from used Structured Generation to force the multimodal models to reason before answering. We usedslightly different json output formats for each of the evaluation datasets (which can be seen in our repository),but they generally follow the following template: {\"name\" :\"<t o o l namee . g . i n f o g ra p h i c _e x p l a i r_ t o o l >\" ,\" d e s c r i p t i o n \" :\"<t o o l d e s c r i p t i o n e . g . I n f o g r a p h i c E x p l a i n e r Tool>\" ,\" parameters \" :{\" type \" :\" o b j e c t \" ,\" p r o p e r t i e s \" :{\"1 _reasoning \" :{\" type \" :\" s t r i n g \" } ,\"2_answer\" :{\" type \" :\" s t r i n g \" ,\" d e s c r i p t i o n \" :\" Concise answer to the user question . \"} ,} ,\" r e q u i r e d \" :[ \"1 _reasoning \" , \"2_answer\" ] ,} ,} For mydoc, we used the entity being requested as the \"key\" following the json format we used in ourprevious work . This makes it easy to request for multiple entities in the document (e.g. requesting for theBilling Name and Total Amount at the same time).",
  "{\"name\" :\" doc_extraction_tool \" ,\" d e s c r i p t i o n \" :\" Extract in f o rm ati on fromadocument\" ,\" parameters \" :{": "\" type \" :\" o b j e c t \" ,\" p r o p e r t i e s \" :{\"1 _reasoning \" :{\" type \" :\" s t r i n g \" } ,f \"2_{ key }\" :{\" type \" :\" i n t e g e r \"i fkey == \"page\" else \" s t r i n g \" ,\" d e s c r i p t i o n \" :\"Theanswer , e x a c t l y as i t appears in the document . \" ,\"maxLength\" :max_length ,}} ,\" r e q u i r e d \" :[ \"1 _reasoning \" ,f \"2_{ key }\" ] ,} ,} Note that we prepended indices to the keys in the json format. That is because the version of TGI weused still uses an older version of Outlines (version < 0.40.0) which implicitly reorders the keys by alphabeticalorder. Prepending the indices solves this issue. But they can be removed on later versions of TGI and Outlines.Also note that we asked the models to output the exact answers for mydoc while we only asked the modelsto be concise for mychart and myinfographic. This is because we suspect that the challenge organizers usedthe MMMU evaluation metric for mydoc and Mistral 7B to judge the outputs for mychart and myinfographic . The former requires exact outputs while implementation details on the evaluation script for the latterindicate that concise answers would be better. Although, these claims are unconfirmed.",
  "Discussion": "Interestingly, we managed to beat multiple teams who finetuned multimodal (vision + text) models using justan LLM and & structured generation on the Key-Information Extraction dataset, mydoc. This supports onefinding in our paper that visual information isnt really important for KIEand in this particular case, usingvision encoders was even harmful .We have four hypotheses on why:",
  "Hermes 2 Pro - Mistral 7B13.55%4.69%+ 1-Shot Retrieval+ 36.87%+ 40.55%+ Supervised Finetuning+ 17.71%+ 13.53%+ Structured Prompting+ 0.63%+ 10.30%": "* Benchmarks results ablating three components of Retrieval Augmented Structured Gener-ation on Key-Information Extraction (KIE) & Line Items Recognition (LIR) tasks on theDocILE dataset : (1) Retrieval Augmented Generation , (2) Supervised Finetuning,& (3) Structured Prompting . Structured Generation was not included in the ablationbenchmarks as it is a necessary component of RASG to ensure that the outputs are parse-able by downstream APIs . Results show that adding Structured Prompting, i.e. infusinglayout information to the text prompt, only adds a marginal increase in performance.",
  "First, perhaps the visual and layout information are not important for Key-Information Extraction": "The team behind DocLLM had this idea of removing the vision component and treating the layoutinformation as its own modality. And their Text + Layout only model worked just as well or even betterthan the Text + Vision and Text + Vision + Layout models they benchmarked . Our previous work,on the other hand, completely gets rid of the other modalities and replaces them with other augmen-tations (i.e. retrieval augmented generation, structured generation, infusing the layout information tothe prompt, & finetuning) instead . There, we found that infusing the layout information to the textprompt does not actually help for the KIE task either. See . We also do not expect randomly permuting the order of the words in the text prompt would help either.So perhaps what is actually important for the KIE task is locality. That is, that nearby words in theimage has to be nearby in the text prompt. But this is already guaranteed by good OCRs.",
  "Second, perhaps the LLMs can already infer the location of the words in the image from theposition (index) of the words in the text prompts": "Previous work show that LLMs trained without positional encodings can still learn position information. The possible reason is that they infer this from the (implicit) causal graph. Perhaps this is alsotrue in the 2D case. That is, we do not need to feed information on where the input words are becausethe LLMs can already infer this from the order of the words as they are fed to the LLM.",
  "Third, perhaps it has to do with the information capacity of these models": "The base LLM of the LLaVA-NeXT (v1.6) model we tested is Mistral-7B . And from a quick napkinmath with the neural scaling laws, this indicates that the language model can only hold around 7*20 =140B tokens of information . This is not a lot. And then we further pushed it to overcapacity byjamming in the embeddings from the visual encoder and projector.",
  "Fourth, which we find most compelling, is that we are simply not using enough image tokens fordocument information understanding": "Unlike pictures of pets (which only require a couple of tokens to describe), document images are packedwith information. And from empirical observations, business documents usually have around 3, 000 texttokens. And we would have more tokens if we also consider the layout information. And yet, we usuallypack them to image encoders that compresses the document to just 1024 tokens. Thats a lot ofinformation lost. Previous work which aims to reduce the number of image tokens shows empirical evidence for theseclaims . In the table in , see that more image tokens are required to achieve maximumperformance on document understanding datasets. : Comparison of approaches with the SS baseline and Matryoshka Multimodal Models (M3) acrossvarious benchmarks under LLaVA-NeXT . Here # Tokens denotes the number of visual tokens per imagegrid in LLaVA-NeXT. SS denotes the baseline model trained with a Specific Scale of visual tokens. M3 is atleast as good as SS, while performing better on tasks such as TextVQA, ChartQA, and MMBench. Oracledenotes the case where the best tradeoff between visual tokens and performance is picked.",
  "Conclusion": "Our results and final standings in CVPRs 2nd MMFM Challenge show our approachs ability to generalize tounseen tasks. While our implementation for this challenge is finetuning-free (which also makes it easy andcheap to implement for other teams), we could also apply it on finetuned models. However, as we noted in ourprevious work, naively combining finetuning and Structured Generation can lead to worse results . Futurework on the topic is necessary. But for now, ensuring that the alignment of output formats in the finetuningdataset and in the inference requests helps.Our results on the mydoc dataset also reinforce our claim that neither the visual nor precise layout infor-mation is necessary for solving key-information extraction. Although more work is required to verify the fourhypotheses we raised in the discussion section above.",
  "Mistral Team, Mistral 7B, 2024-06-09], 2024": "P. Lu, L. Qiu, J. Chen, et al., Iconqa: A new benchmark for abstract diagram understanding and visuallanguage reasoning, in The 35th Conference on Neural Information Processing Systems (NeurIPS 2021)Track on Datasets and Benchmarks, 2021. G. Jaume, H. Kemal Ekenel, and J.-P. Thiran, FUNSD: A dataset for form understanding in noisyscanned documents, in 2019 International Conference on Document Analysis and Recognition Work-shops (ICDARW), IEEE, Sep. 2019. doi: 10.1109/icdarw.2019.10029.",
  "W. Chen, H. Wang, J. Chen, et al., Tabfact: A large-scale dataset for table-based fact verification, inInternational Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia, Apr. 2020": "M. Mathew, D. Karatzas, and C. V. Jawahar, DocVQA: A dataset for VQA on document images,in 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), IEEE, Jan. 2021. doi:10.1109/wacv48630.2021.00225. M. Mathew, V. Bagal, R. Tito, et al., Infographicvqa, in 2022 IEEE/CVF Winter Conference onApplications of Computer Vision (WACV), IEEE, Jan. 2022. doi: 10.1109/wacv51458.2022.00264.[Online]. Available: X. Chen, Z. Zhao, L. Chen, et al., Websrc: A dataset for web-based structural reading comprehension, inProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Associationfor Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.343. [Online]. Available:"
}