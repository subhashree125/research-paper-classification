{
  "Abstract": "Animal Re-ID is crucial for wildlife conservation, yetit faces unique challenges compared to person Re-ID.First, the scarcity and lack of diversity in datasets leadto background-biased models. Second, animal Re-ID de-pends on subtle, species-specific cues, further complicatedby variations in pose, background, and lighting. This studyaddresses background biases by proposing a method to sys-tematically remove backgrounds in both training and eval-uation phases. And unlike prior works that depend on poseannotations, our approach utilizes an unsupervised tech-nique for feature alignment across body parts and posevariations, enhancing practicality. Our method achieves su-perior results on three key animal Re-ID datasets: ATRW,YakReID-103, and ELPephants.",
  ". Introduction": "While Person Re-identification (Re-ID) has seen consider-able advancements ,animal Re-ID remains underexplored despiteits significance in both industrial and conservation con-texts. Traditional animal identification methods are labor-intensive and impractical for large populations, highlightingthe need for improved automated Re-ID techniques .Animal Re-ID faces unique challenges, including smallnon-diverse datasets leading to background bias, greatervariations within an individual and smaller variations be-tween different individuals compared to humans.Thesechallenges necessitate animal-specific Re-ID methods.This study focuses on Re-ID for elephants, yaks, andtigers, emphasizing the importance of feature alignmentacross pose variations due to their four-legged nature andthe variability in their appearances.Unlike prior works that depend on pose annotations, our ap-proach employs an unsupervised learning method toidentify semantically similar animal parts, enhancing Re-IDperformance. . Proposed Animal Re-ID Approach: Addressing back-ground bias in Re-ID models, our method masks out backgroundsto focus on the animal. It learns part-aware representations, ensur-ing consistency across subjects. Part-aware features are mergedand a final Re-ID score is computed via cosine similarity. Our contributions illustrated in include: (i) Reduc-ing background bias by creating and sharing background-free images, (ii) Utilizing an unsupervised method for learn-ing part-based descriptors, thereby improving Re-ID accu-racy without needing pose annotations and (iii) assessingthe transferability of our method between different species,demonstrating the feasibility of transferring pose alignmentacross various species.",
  "arXiv:2405.13781v1 [cs.CV] 22 May 2024": ". Bias Towards Background: On the left, we display samples from the YakReID-103 dataset. Utilizing features from thePGCFL model, we identify the nearest neighbors. Each images entity label is presented in the bottom-left corner. The retrievedimages showcase four distinct entities, all sharing a remarkably similar background. This indicates that distances in the feature spaceare significantly influenced by background similarities. On the right, we exhibit the outcomes of our proposed background segmentationprotocol. The top-left image is the original, the bottom-left depicts results from SAM, the bottom-right from ISNet, and the top-rightcombines outputs from both SAM and ISNet. grounds. It is particularly problematic in real-world scenar-ios where the same entity can be seen with lots of differentbackgrounds. We illustrate this problem on the left side of.To counter this, we diverge from previous strategies thatadapt models to ignore backgrounds. Instead, we proposealtering the data by removing backgrounds from all im-ages using segmentation models ISNet and SAM .We combine both models to produce refined segmentationmasks, balancing ISNets completeness with SAMs preci-sion, particularly for distinguishing animal parts like tusksand horns. To get the best of both worlds, we combine bothapproaches to obtain foreground mask Mx from a given im-age x as :",
  "Mx ={m|m SSAM(x) h(m; SISNet(x)) < )} ,": "(1)where SSAM(x) are processed object masks from SAM,and SISNet(x) is the output mask from ISNet. h is a cri-terion (such as IoU) that filters SSAM(x) given SISNet(x)with threshold . We provide additional details about thecriterion in the supplementary material.The combination of both models yields better segmenta-tion masks as shown in . In the following sections, weuse this segmentation protocol to extract foreground masksfor the different benchmarks and mask out the backgroundboth during training and evaluation. Code and processedimages will be made available to facilitate replication andfurther research.",
  "In a standard Re-ID scenario, given a query image q of di-mensions w h 3, the goal is to rank a gallery set of": "N images G = {gj}N1 to find those matching the querysidentity. Images are ranked based on the cosine similar-ity between the query representation f(q) Rd and eachgallery image representation f(gj) Rd, calculated asc(f(q), f(gj)) =f(q),f(gj) f(q) f(gj). Here, f is the embeddingfunction into a d-dimensional Re-ID space, parametrizedby . The training process focuses on optimizing . Thegallery encompasses one or multiple images of each uniqueanimal identity, with a total of Cid identities (Cid <= N).",
  "Architecture": "Consistent with prior Animal Re-ID studies , ourmodel employs a modified SE-ResNet50 , followingadaptations in . We utilize the initial five convolutionalblocks of SE-ResNet50. The output from the fifth block isprocessed through a Global Average Pooling (GAP) layer, condensing spatial dimensions into 1D vectors. Thesevectors are then passed through a sequence comprising alinear layer, batch normalization , and dropout ,producing the Re-ID vector representation.For identityand orientation classification, the Re-ID features are furtherpassed through two linear layers to produce the final logits.Formally, for an input image x (0, 255)W H3, ourRe-ID model operates as:",
  "is the Re-ID representation, and ylr R1 and yid RCid": ". Overall architecture: Left - proposed models architecture. Input is a single image, processed through the first 3 layers ofbackbone and a convolutional block to extract DVE features (x). Features produced after the fifth backbone layer continue to globalaverage pooling and a linear layer for Re-ID features f(x), then pass through two classification heads for ID class and orientation predictionvia linear layers and softmax operations. Right - LDV E for part-aware representation.",
  ". Part-aware Feature Learning": "Identifying individuals within the same species depends onfine body part details, such as tigers stripes , yaks hornsand fur, or elephants ears and tusks.Part-based Re-IDmethods, which are effective , usually need poseannotations. Our unsupervised method leverages the De-scriptor Vector Exchange (DVE) for learning part-specificfeatures without posture labels, using DVEs technique forunsupervised dense landmark prediction .DVEs local image descriptors are designed to be equiv-ariant to transformations and invariant to variations withina category. The DVE objective function is:",
  "v gup(v|u; , x, x, x)dudv ,": "(3)where is a projection from the image domain to the localdescriptors domain, g is a random warping function, x is animage, and x = gx its deformation. p(v|u; , x, x, x) isthe probability of pixel u in image x matching with pixel vin image x. The probability computation uses an auxiliaryimage x to make the local descriptor invariant to intra-category variations. For more information about the DVEobjective function please refer to . DVE for Re-IDDVE descriptors are invariant to intra-category variations, meaning similar parts across differentsubjects (e.g. left leg of a tiger) will have comparable de-scriptors. Leveraging this, we incorporate the DVE propertyinto our Re-ID model by using the DVE loss LDV E to re-fine our models features. Specifically, the activation of thethird convolutional layer of the SE-ResNet is fed into an ex-tra convolution layer to get the descriptors (x) on whichLDV E is applied.Formally, those local descriptors are the results of thefollowing steps:",
  "xBR xSEc1:3 FCONV (x) ,(4)": "F in Rwhdse correspond to the activation of the thirdlayer. (x) in Rwhddve is the DVE feature.The output resolution of our models backbone is re-duced by a factor of 8 compared to the input image. Giventhat the resolution post-5th layer is too low for effective lo-cal descriptor learning via DVE, we opt to apply the DVEobjective at a higher resolution stage, specifically after thethird layer of the SE-ResNet, where the output is onlydownscaled by a factor of 4. illustrates how LDV E isobtained.",
  "LLR = (ylr log(ylr) + (1 ylr) log(1 ylr)) ,(6)": "where Cid is the number of entity classes, yid and ylr is theground truth label for entity and orientation, respectively.For learning the Re-ID representation, we diverge fromthe common use of triplet loss in previous studies and in-stead utilize circle loss . Given a sample x, lets con-sider K within-class and L between-class similarity scores,denoted by sip(i = 1, 2, ..., K) and snj (j = 1, 2, ..., L), re-spectively. The formulation of the circle loss is as follows:",
  ". Experiments": "In the following, we first describe the datasets, metrics andtraining procedures. Then, we investigate the effect of back-ground removal on Re-ID performance. Finally, we showthe benefit of the proposed part-aware features through aseries of quantitative and qualitative evaluations. DatasetsThe ATRW dataset is the largest wildlifeRe-ID dataset, featuring 182 entities across 92 tigers, with atraining set of 1,887 images from 107 entities and a test setof 1,762 images from 75 entities, without utilizing providedpose annotations. YakReID-103 includes 1,404 train-ing images of 121 entities, and we only use the hard-testingsubset of 433 images, where similar poses and backgroundsare excluded. ELPephants contains 2,078 images of276 elephants. We manually completed the datasets par-tial heading direction annotations and included only enti-ties with multiple side-view images. Its split into 1,380training and 380 testing images without predefined bounds.Each dataset ensures no entity overlap between training andtesting, considering each side of an individual as a distinctentity.",
  "MetricsThe evaluation employs mean average precision(mAP) and recall at 1 (R@1), with AP calculated per query": ". Visualization of the feature learned with LDV E Thefirst two rows show intra-species part alignment, the next two rowsdemonstrate that a model trained solely on tiger can generalize toother species and maintain alignment even in inter-species sce-nario. The final row is the results from the PGCFL baseline. Ineach row, the green dot in the left image is the local query, whilethe red dot in the center image indicates its matching point. Theright-most image provides a heatmap overlaid on the target image,showcasing the similarities between the local query and the centerimage. from ranked gallery lists.For ATRW, metrics are sep-arately computed for single-camera (R@1(s)) and cross-camera (R@1(c)) settings, with mmAP as their average.YakReID-103 and ELPephants lack camera data, consider-ing any same-entity gallery image as positive. ImplementationdetailsUtilizingSE-ResNet50pre-trained on ImageNet as the backbone, similar to PGCFL andPPGNet, our model is developed in Pytorch and operates onan Nvidia A100 GPU. Images are resized to 224x224, withdata augmentation including random cropping, patch eras-ing, and flipping, adjusting orientation labels accordingly.Training ceases after 80 epochs for ATRW and YakReID-103, and 100 for ELPephants, starting with a fixed back-bone for the initial three epochs. The learning rate is set at0.001 for the backbone and 0.01 for other layers, reducedby tenfold after two-thirds of the epochs. SGD optimizeris used with momentum 0.9, weight decay 5e 4, labelsmoothing 0.1, and a batch size of 30. Parameters dDV E,reID, and DV E are set to 64, 2, and 0.2 respectively, em-ploying Circle Loss with of 64 and m of 0.25. For testing, features from the original and flipped im-ages are concatenated for the Re-ID score, incorporating are-ranking strategy for ATRW results. Despite not beingdiscussed in the original works, we experimented with avanilla triplet sampling strategy for batch processing, en-suring adequate negative samples for Circle Loss.",
  "MethodsOrg. Re-ID TaskPose GTmmAPR@1(s)R@1(c)mAPR@1mAPR@1": "CLIP-Re-ID ViT Person56.586.372.049.882.212.725.3PPGNet R-50 Animal68.381.281.1----ResNet50 Animal65.991.183.460.986.020.033.9ViT Animal65.590.379.461.388.420.636.8PGCFL Animal66.990.886.355.882.718.533.4OursAnimal68.692.084.661.089.424.338.7 . Comparison to State-of-the-Art: Our method is evaluated on three datasets, each representing a different species: ATRW(Tiger), YakReID-103 (Yak), and ELPephants (Elephants), originally proposed for various Re-ID tasks. Results from images with maskedbackgrounds, detailed in Sec. 2.1, are highlighted. Our model achieves top performance, surpassing existing baselines in mAP on ATRWand ELPephants, even outperforming PPGNet, which utilizes extra pose labels. For results on original images see supplementary material.",
  "Effect of Background RemovalAs can be seen in Tab. 1": "background removal significantly impacts performance,with a mmAP decrease of 10.2% when training and test-ing without backgrounds, versus a 19.2% drop when test-ing a full-image trained model on a masked background.However, models trained on masked backgrounds can stillleverage backgrounds in testing, with an observed 9.7%mmAP increase, suggesting the importance of background-independent Re-ID models. Subsequent results are reportedusing masked backgrounds to emphasize identity recogni-tion regardless of the background. Comparison to State-of-the-ArtOur method surpassesexisting models like PGCFL , PPGNet (ATRWchallenge winner), CLIP-Re-ID (a recent person Re-IDmethod), and baselines with ResNet50 and ViT backbonesusing Lbase = LID + LLR + reID LreID. Specifically,PPGNet, which depends on ground-truth poses, is evaluatedsolely on ATRW. Results can be found in Tab. A.4. Our ap-proach leads in mmAP and R@1(s) on ATRW, equals ViTin mAP, and excels in R@1 on YakReID-103, while sig-nificantly outperforming all on ELPephants by a 5.8 mAPmargin against PGCFL. CLIP-Re-IDs adaptation to animaldatasets is hindered by greater intra-identity variations inanimals, a challenge not as prevalent in human subjects.Although PCN-RERP shows promising mAP on YakReID-103, its limited to datasets featuring animals in standingpostures. Results without background masking are providedin supplementary material. Qualitative Analysis of DVE.DVE helps the model em-bed body part information in its activation. Features for agiven body part will share similarity across different enti-ties. This helps align body parts and thus facilitates Re-ID.As shown in the two top rows of , when visualizingthe descriptors (x) learn through the loss LDV E we cansuccessfully match body parts. Towards transfer between species.We also investigatethe possibility of transfer between species. Using a modeltrained on tigers, we visualize third-layer features of the Re-ID backbone on yaks and elephants. While tigers, yaks,and elephants have widely different shapes and being onlytrained on tigers, our model was able to match body partsacross species as seen in rows 3 and 4 of .",
  ". Conclusion": "We have introduced a novel method to advance animalre-identification. Our approach diverges from prior workby learning part-aware features in an unsupervised man-ner.Furthermore, by automatically masking the back-ground, we not only address a recurrent challenge in an-imal Re-ID benchmarks but also effectively reduce themodels inclination to overly focus on the background.Lastly, we demonstrated that our approach represents aninitial step towards the inter-species transferability of Re-ID models.In future studies, we plan to further re-fine our methods inter-species generalization capabilitiesby leveraging additional multi-species unsupervised train-ing. Tsai-Shien Chen, Chih-Ting Liu, Chih-Wei Wu, and Shao-Yi Chien. Orientation-aware vehicle re-identification withsemantics-guided part attention network.In ComputerVisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part II 16, pages330346. Springer, 2020. 1",
  "Gong Cheng, Guangxing Wang, and Junwei Han.Isnet:Towards improving separability for remote sensing imagechange detection. IEEE Transactions on Geoscience and Re-mote Sensing, 60:111, 2022. 2": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 5, 1, 2 Ju He, Jie-Neng Chen, Shuai Liu, Adam Kortylewski, ChengYang, Yutong Bai, and Changhu Wang. Transfg: A trans-former architecture for fine-grained recognition. In Proceed-ings of the AAAI Conference on Artificial Intelligence, pages852860, 2022. 1 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 5, 2 Lingxiao He, Jian Liang, Haiqing Li, and Zhenan Sun.Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 70737082, 2018. 1 Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,and Wei Jiang.Transreid: Transformer-based object re-identification. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 1501315022, 2021. 1 Lex Hiby, Phil Lovell, Narendra Patil, N Samba Kumar, Ar-jun M Gopalaswamy, and K Ullas Karanth. A tiger cannotchange its stripes: using a three-dimensional model to matchimages of living tigers and tiger skins. Biology letters, 5(3):383386, 2009. 3, 1",
  "Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-works. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 71327141, 2018. 2": "Sergey Ioffe and Christian Szegedy. Batch normalization:Accelerating deep network training by reducing internal co-variate shift. In International conference on machine learn-ing, pages 448456. pmlr, 2015. 2 Pirazh Khorramshahi, Neehar Peri, Jun-cheng Chen, andRama Chellappa. The devil is in the details: Self-supervisedattention for vehicle re-identification. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, Au-gust 2328, 2020, Proceedings, Part XIV 16, pages 369386.Springer, 2020. 1 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 2 Matthias Korschens and Joachim Denzler. Elpephants: Afine-grained dataset for elephant re-identification. In Pro-ceedings of the IEEE/CVF International Conference onComputer Vision Workshops, pages 00, 2019. 1, 4 Lei Li, Tingting Zhang, Da Cuo, Qijun Zhao, Liyuan Zhou,and Suonan Jiancuo. Automatic identification of individualyaks in in-the-wild images using part-based convolutionalnetworks with self-supervised learning. Expert Systems withApplications, 216:119431, 2023. 1, 2",
  "Shuang Liu, Wenmin Huang, and Zhong Zhang. Person re-identification using hybrid task convolutional neural networkin camera sensor networks. Ad Hoc Networks, 97:102018,2020. 1": "Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and WeiJiang. Bag of tricks and a strong baseline for deep personre-identification. 2019 IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops (CVPRW), pages14871495, 2019. 2 Julien Martin, Wiley M. Kitchens, and James E Hines. Im-portance of well-designed monitoring programs for the con-servation of endangered species: Case study of the snail kite.Conservation Biology, 21, 2007. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 1 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, IlyaSutskever, and Ruslan Salakhutdinov. Dropout: a simple wayto prevent neural networks from overfitting. The journal ofmachine learning research, 15(1):19291958, 2014. 2 Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and ShengjinWang. Beyond part models: Person retrieval with refinedpart pooling (and a strong convolutional baseline). In Pro-ceedings of the European conference on computer vision(ECCV), pages 480496, 2018. 1",
  "A unified perspective of pair similarity optimization. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 63986407, 2020. 3": "James Thewlis, Samuel Albanie, Hakan Bilen, and AndreaVedaldi.Unsupervised learning of landmarks by descrip-tor vector exchange. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 63616371,2019. 1, 3 Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and XiZhou. Learning discriminative features with multiple gran-ularities for person re-identification. In Proceedings of the26th ACM international conference on Multimedia, pages274282, 2018. 1 Guangcong Wang, Jian-Huang Lai, Wenqi Liang, andGuangrun Wang.Smoothing adversarial domain attackand p-memory reconsolidation for cross-domain person re-identification. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1056810577, 2020. Di Wu, Si-Jia Zheng, Xiao-Ping Zhang, Chang-An Yuan, FeiCheng, Yang Zhao, Yong-Jun Lin, Zhong-Qiu Zhao, Yong-Li Jiang, and De-Shuang Huang. Deep learning-based meth-ods for person re-identification: A comprehensive review.Neurocomputing, 337:354371, 2019. Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, LingShao, and Steven CH Hoi.Deep learning for person re-identification: A survey and outlook. IEEE transactions onpattern analysis and machine intelligence, 44(6):28722893,2021. 1 Tingting Zhang, Qijun Zhao, Cuo Da, Liyuan Zhou, Lei Li,and Suonan Jiancuo. Yakreid-103: A benchmark for yak re-identification. In 2021 IEEE International Joint Conferenceon Biometrics (IJCB), pages 18. IEEE, 2021. 1, 2, 3, 4 Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.Shufflenet: An extremely efficient convolutional neural net-work for mobile devices. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages68486856, 2018. 1",
  "Supplementary Material": "In the supplementary material accompanying this paper,we offer enhanced visualizations that highlight the com-plexities involved in animal re-identification. Additionally,we conduct a comprehensive ablation study to assess theimpact of each component within our proposed model. Wealso present qualitative results to demonstrate the modelsperformance, provide an in-depth analysis of the modelscapability to transfer knowledge across different species,and furnish supplementary details regarding the implemen-tation. Our aim is to provide a thorough understanding ofour approach and its underlying mechanisms.",
  "A.5. Challenge of Animal Re-ID": "Telling apart different entities of the same animal species isa subtle task, as shown in Fig. A.5, one really needs to lookat specific details of the distinguishing body part. For tigers,identification is usually based on body stripe patterns .For yaks, it is mostly their horns, sometimes fur and texturecan help. For elephants, ears and tusks are important foridentification. Matching the same body part between differ-ent entities is key to improving the Re-ID performance.",
  "Table A.3. Ablation study. We study the influence of 5 differ-ent components of our approach: The impact of our 4 objectivefunctions, and the batch sampling strategy (B.S.)": "batch sampling strategy. The metric-based loss LreID per-forms better than the classification loss LID as the formerlearns more generalized feature representation. The use oforientation loss LLR in conjunction with LreID and LIDconsistently boosts the performance, suggesting the impor-tance of the pose information. LDV E loss and batch sam-pling, together, further improve the mAP score by 2.4%.This makes it evident that DVE loss helps to leverage an-imal part-specific information for better Re-ID. All resultsreported in this ablation study are without re-ranking.",
  "A.7. Detailed Comparison to State-of-the-Art": "Baselines.We compare our approach to PGCFL ,PPGNet the winner of the ATRW challenge, CLIP-Re-ID a recent person-reid method, and two other base-lines: First a simple baseline using a ResNet50 backbonetrained with Lbase = LID + LLR + reID LreID. Then,a baseline using a more modern ViT backbone is trainedwith Lbase as well. PPGNet is run for 160 epochs and wefollowed the experimental settings from the original work.For the two training stages of CLIP-Re-ID, the ResNet-based model run for 120 and 60 epochs, and the ViT-basedmodel run for 60 and 80 epochs. The rest of the exper-imental settings adhere to the CLIP-Re-ID original work.For all other methods, we use the same experimental settingpreviously described. PPGNet relies on groundtruth poses",
  "PPGNet R-50 Animal77.999.490.8----PGCFL Animal74.595.790.364.391.810.724.7PCN-RERP Animal---68.691.8--OursAnimal76.295.788.066.192.314.130.3": "Table A.4. Comparison to State-of-the-Art: Our method is evaluated on three datasets, each representing a different species: ATRW(Tiger), YakReID-103 (Yak), and ELPephants (Elephants), originally proposed for various Re-ID tasks. Results from images with maskedbackgrounds are highlighted. Our model achieves top performance, surpassing existing baselines in mAP on ATRW and ELPephants, evenoutperforming PPGNet, which utilizes extra pose labels. When considering original images, our model outperforms PGCFL across alldatasets and matches PPGNet on ATRW. CLIP-Re-ID, designed for person Re-ID, fails to generalize to animals due to high intra-classvariations. PCN-RERP performs well on Yak dataset but lacks generalization to non-standing animal postures.",
  "and therefore the evaluation is only shown on the ATRWdataset. Additionally, we compared with PCN-RERP who report their method on YakReID-103 dataset": "Results.In Tab. A.4, our approach outperforms priorstate-of-the-art models on the ATRW dataset for mmAP andR@1(s). On the YakReID-103, it matches the ViT base-line in mAP, and outperforms all in terms of R@1. On theELPephants dataset, it significantly outperforms all base-lines with a margin of 5.8 mAP points w.r.t. PGCFL. CLIP-Re-ID baseline originally proposed for person re-id taskfails to generalize on the animal dataset. This can be at-tributed to the fact of higher intra-identity variations occur-ring in animals than in persons, which CLIP-based modelsfail to capture. CLIP-based models are better known fortheir zero-shot inter-class/identity classification. For com-pleteness, we also provide results on the original benchmarkwithout masking backgrounds. Here, overall metrics arehigher but the ranking of the different baselines is similar.PCN-RERP has better overall mAP on Yak dataset butthis approach cannot be generalized to dataset where ani-mals are in the non-standing posture.",
  "We provide further results for transfer between species. InFig. A.6 we show that the local descriptors learned by ourapproach also work for models trained on elephants or yaks,": "Figure A.6. Intra-Species visualization of SEResNet trainedwith DVE loss. In each row, the red point from the left-most im-age is queried in the second image, and the red point in the middleimage is its matching point, the right-most image is a heatmapof cosine similarities between the middle image and the red pointin the left-most image. Interestingly, the last row show that evenwhen images are of bad quality and of different views, the match-ing can be good. Figure A.7. Inter-Species visualization of SEResNet trainedwith DVE loss. In each row, the red point from the left-most im-age is queried in the second image, and the red point in the middleimage is its matching point, the right-most image is a heatmap ofcosine similarities between the middle image and the red point inthe left-most image. even in challenging scenarios with low-quality images (andfailed background removal). In Fig. A.7 we provide addi-tional results for the matching of local DVE features acrossspecies. Finally in Fig. A.8 we compare this transfer capa-bility with two baselines: PGCFL and ResNet50. In bothcases, they fail to match body parts, confirming that the Figure A.8. Visualization of yak-elephant body part matchingusing features from the third layer of different models trainedon tigers. In each row, the red point from the left-most imageis queried in the second image, and the red point in the middleimage is its matching point, the right-most image is a heatmap ofcosine similarities between the middle image and the red point inthe left-most image. From top to bottom, each row correspond toour model, PGCFL and Resnet50 respectively.",
  "A.10. Transfer Quantitative Evaluation": "We provide a quantitative evaluation of transfer betweenspecies.Results can be found in Tab. A.5.While per-formance on transfer is lower than the supervised model,the model still manages to transfer some knowledge acrossspecies. Note that those performances are obtained on themasked background so the model can only rely on the ani-mals appearance.",
  "ATRW68.692.084.635.973.16.717.6YakReID-10349.580.970.961.089.46.516.3ElPephants47.280.966.334.575.024.338.7": "Table A.5. Evaluation of inter-species transferability We propose to evaluate the transferability of the proposed approach between threespecies: Tiger, Yak and Elephant. Each row of the table correspond to our model trained on a single species (Training data) and evaluatedon the test set of the three species. While the performance on transfer are below than the fully supervised one, the proposed model is ableto transfer meaningful representation between species."
}