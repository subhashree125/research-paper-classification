{
  "Abstract": "Labels are the cornerstone of supervised machine learn-ing algorithms. Most visual recognition methods are fullysupervised, using bounding boxes or pixel-wise segmen-tations for object localization. Traditional labeling meth-ods, such as crowd-sourcing, are prohibitive due to cost,data privacy, amount of time, and potential errors on largedatasets.To address these issues, we propose a novelannotation framework, Advanced Line Identification andNotation Algorithm (ALINA), which can be used for la-beling taxiway datasets that consist of different cameraperspectives and variable weather attributes (sunny andcloudy). Additionally, the CIRCular threshoLd pixEl Dis-covery And Traversal (CIRCLEDAT) algorithm has beenproposed, which is an integral step in determining the pix-els corresponding to taxiway line markings. Once the pixelsare identified, ALINA generates corresponding pixel coordi-nate annotations on the frame. Using this approach, 60,249frames from the taxiway dataset, AssistTaxi have been la- beled. To evaluate the performance, a context-based edgemap (CBEM) set was generated manually based on edgefeatures and connectivity. The detection rate after testingthe annotated labels with the CBEM set was recorded as98.45%, attesting its dependability and effectiveness.",
  ". Introduction": "With the advancement of sensors, hardware and intelli-gent software, autonomous vehicles have become the cen-ter of attention for computer vision and robotics research.Among the several components of autonomous vehicles, thecamera-based lane detection plays a crucial role in perceiv-ing the lines and then assisting autonomous vehicular sys-tems in making decisions to follow the correct path/lanes.It is envisioned, as we gain confidence in the correctnessof the design of autonomous systems, it will pave the wayfor the integration of learning based technologies in assist-ing safety critical systems such as, autonomous aircraft for",
  "arXiv:2406.08775v1 [cs.CV] 13 Jun 2024": "urban air mobility or aerial delivery . As a result, thishas increased the need for an advanced perception system,equipped with precise line identification capability. In fu-ture, it can assist pilots in the aircraft, especially when navi-gating the airport taxiway. As stated by the Aviation SafetyNetwork , approximately, 33.33% of the aircraft acci-dents, between the years 2015 and 2022, happened duringthe taxiing phase. The common causes of taxiway accidentsinclude poor weather conditions and high traffic on the taxi-way. In order to address these issues, some researchers haveformulated solutions for assisting pilots during the taxiingphase, including: light-based guidance systems, which havebeen designed by the Federal Aviation Administration andHoneywell , computer vision based taxiway guid-ance techniques , Airport Moving Maps , and sen-sors, such as LIDAR and cameras mounted on an aircraft. This research builds on the work of Ganeriwala et.al , particularly with the contour-based detection andline extraction method (CDLEM). They introduced Assist-Taxi dataset, which incorporates more than 300,000 framesof taxiway and runway instances. The data was collectedfrom Melbourne (MLB) and Grant-Valkaria (X59) generalaviation airports.In this research, we introduce an Advanced Line Identifi-cation and Notation Algorithm (ALINA), which is an anno-tation framework developed to detect and label taxiway linemarkings from video frames (). ALINA establishesa uniform trapezoidal region of interest (ROI) by utilizingthe initial frame, that is consistently applied across all sub-sequent frames of the video. The ROI is then geometricallymodified and the color space is transformed to produce a bi-nary pixel map. ALINA pinpoints pixels representing taxi-way markings through the novel CIRCular threshoLd pixElDiscovery And Traversal (CIRCLEDAT) algorithm, leadingto frame annotations and coordinate data files. A context-based edge map (CBEM) set was generated for comparisonto ensure accuracy in marking detection. ALINA was testedon a subset of AssistTaxi dataset - 60,249 frames extractedfrom three distinct videos with unique camera angles. Thefocus of this research had been on 60,249 frames out of avast 300,000-frames AssistTaxi dataset, as the motivationwas to lay down a rigorous yet tractable foundation for theempirical validation of ALINAs efficacy, paving the wayfor its scalability to larger datasets in future. Through thisresearch, we primarily aim to reduce the intensive, expen-sive and error-prone manual labeling , and provide la-beled data to enhance taxiway navigation safety.Our contribution include the following four main as-pects: To reduce the intensive, expensive, and error-prone man-ual labeling process, thus contributing to the efficientcombination of automated and manual creation of la-belled datasets. Development of the Advanced Line Identification andNotation Algorithm (ALINA) providing a robust frame-work for precise detection and labelling of continuousvideo datasets, particularly focusing on taxiway linemarkings.",
  "Introduction of a novel algorithm, CIRCLEDAT, tailoredto pinpoint pixels representing taxiway/road markingswith high accuracy, ensuring precise labeling across con-tinuous video frames": "Establishing a systematic approach based on change inperspective of scenario to justify the sample size ofground truth which is a subset from the AssistTaxi dataset.The remainder of the paper is organized as follows: InSec. 2, related research works have been discussed. Sec. 3entails the detailed methodology of ALINA, Sec. 4 presentsthe experimental results. Finally, Sec. 5 provides the con-clusion and outlines the directions for future work.",
  ". Literature Review": "While the research in classification of taxiway line mark-ings is still in its preliminary stages, the domain of car lanedetection has seen substantial advancements in road label-ing techniques. The knowledge derived from detecting carlanes not only provides fundamental perspectives that guideour comprehension of detecting taxiway markings but alsohighlights research gaps in car lane labeling methods whichcan be avoided and guide innovative approaches for taxiwaymarking detection. Therefore, this section begins by pre-senting notable works from the car lane detection domainand highlights their research gaps.Some researchers have used classification techniques forlane detection, where images are segmented into grids forrow-based lane location identification . How-ever, these methods lack precision and might miss somelanes. To ensure consistent lane detection, techniques suchas parametric curve modeling and key-point association have been used. Even thoughthese methods acquire high results, they can struggle in ad-verse weather conditions. Andrei et al. have utilizedprobabilistic Hough transform and dynamic parallelogramregion of interest (ROI) to develop a lane detection system.It was implemented on video sequences with manual ROIdefinition but encountered challenges with capturing curvedline endpoints, which are crucial for complete lane bound-ary delineation in real-world scenarios. Chen et al. de-tected road markings using machine learning with binarizednormed gradient detection and principal component analy-sis (PCA) classification, achieving an accuracy of 96.8%.However, the dependency on PCA limited the adaptabilityfor dynamic scenarios and challenging environments. Sim-ilarly, the method proposed by Ding et al. , which com-bined PCA and support vector machine (SVM), achieved a",
  "detection accuracy of 94.77%. However, it struggled to de-tect road markings due to the failure of the ROI extraction,in the case of reflection, occlusion and shadow": "Gupta et al. introduced a real-time framework forcamera-based road and lane markings, using techniquessuch as spatio-temporal incremental clustering, curve fit-ting, and Grassmann manifold learning. The real-time na-ture of this approach brings challenges in processing effi-ciency and speed, especially with increasing dataset com-plexities. Jiao et al. proposed an adaptive lane iden-tification system using the scan line method, lane-votedvanishing point, and multi-lane tracking Kalman filtering,achieving a 93.4% F1-Score.This approach lacked theadaptability to diverse conditions across different datasetsand real-world scenarios. Kiske developed an autonomous labeling system foridentifying highway lane markings using Velodyne lidar,HDR video streams, and high-precision GPS. However, thereliance on multiple data sources makes it less cost-effectiveand more complex for wide-scale implementations. Mutha-lagu et al. proposed a vision-based algorithm for lanedetection in self-driving cars, which used polynomial re-gression, histogram analysis, and a sliding window mecha-nism for detecting both straight and curved lines. The algo-rithm achieved notable accuracy, but its high computationaldemand poses scalability issues, and its limitation in detect-ing steep foreground curves presents challenges in dynamicterrains. Contour-based detection and line extraction method(CDLEM) was initially used for labeling the taxiway linemarkings from AssistTaxi dataset . The Canny edgedetection identified the line markings edges. Subsequently,the Hough transform, Ramer-Douglas-Peucker, and Bresen-hams algorithms were utilized to identify and label bothstraight and curved taxiway line markings. The limitationthis approach posed was the requirement to outline an ROIaround taxiway line marking for every scenario shift. While labeling car lanes has provided a guiding examplefor an end to end lane detection system , a directcomparison of lane detection to labeling taxiway line mark-ings is not being made in this research. The distinct seman-tics and attributes associated with the car lane datasets andairport taxiways dataset pose different set of challenges. Forexample, the airport taxiway dataset has different layouts,diverse markings, and presence of aircraft on airport taxi-ways. Transfer learning can be one of the approaches to ad-dress the differences and identify the commonalities withinthe two datasets , however it requires the taxiways tobe extensively labeled. Therefore, our research emphasizescreating and evaluating algorithms for labeling specific toairport taxiways, rather than contrasting them with car lanedatasets.",
  ". Frame Representation": "While reading a frame, ALINA stores each x, y coordi-nate and its red, green, blue (RGB) channel values fromthe frame into a multi-dimensional array, as represented inEq. (1).A[i, j, k] = I[i, j, k](1) where, i, j, and k denote the row, column, and channel in-dices of the frame, respectively. This formula copies thepixel values of the frame at location (i, j) in the kth colorchannel into the corresponding location in the array.",
  ". Interactive ROI Definition on the Initial Frame": "The user provides source points to ALINA for creating atrapezoidal region of interest (ROI) on the initial frame ofthe video, as shown in (a), (e), (f). It is drawn inthe vicinity of where the camera expects to see the taxi-ways line markings. ALINA treats the ROI as a constraintto limit the search area for taxiways line markings, re-sulting in reduced computational complexity and improveddetection accuracy .The source points, which cor-respond to the vertices of ROI, are denoted as follows:[x1, y1], [x2, y2], [x3, y3], [x4, y4].",
  ". Perspective Transformation of the ROI": "The next step in ALINA is warping the perspective. Thetrapezoidal ROI is warped into a birds eye view. (b), (f), (j) illustrates this transformation.The destina-tion points, which correspond to the vertices of a rect-angle defining the birds eye view, are represented as:[x1, y1], [x2, y2], [x3, y3], [x4, y4].It enables a more comprehensive and consistent view ofthe taxiway and its features, allowing for improved detec-tion of line markings irrespective of their orientation or cur-vature .Given the pairs of source and destination points, a ma-trix M is derived by solving a system of linear equationsformed from the pixel correspondences. The matrix essen-tially maps any pixel in the trapezoidal ROI to its corre-sponding pixel position in the birds eye view.The matrix M is represented in Eq. (2):",
  ". Color Feature Normalization": "To accurately distinguish line markings within the ROI,ALINA performs normalization of color characteristics.The normalization phase consists of the following key steps:1. RGB to HSV Conversion: The RGB color space ofROI is transformed to the hue, saturation, and value(HSV ) color space. Given a pixels RGB values, de-noted by (R, G, B), the transformation into the HSVspace is depicted as (R, G, B) (H, S, V ). This trans-formation is performed because HSV factors in varia-tions induced by lighting conditions and intrinsic colorproperties, providing a robust representation of color inan image .",
  ". HSV-based Color Thresholding": "Color thresholding is an essential technique in image seg-mentation, leveraging color information to partition im-age pixels into meaningful regions . In the context ofALINA, this technique finds its application in isolating thetaxiway line markings from the rest of the regions within theROI, using the HSV color space. Hue captures the wave-length of color, while saturation measures the intensity, andvalue quantifies the brightness.Determining the precise range for HSV that corre-sponds to the taxiway line markings necessitated a series ofempirical tests. Multiple frame samples were analyzed anda frequency distribution of HSV values for taxiway linemarking regions were plotted. Peaks in this distribution,indicative of dominant HSV values for taxiway line mark-ings, helped in ascertaining the lower and upper bounds ofH, S, V , i.e. (0, 70, 170) and (255, 255, 255), respectively.For a pixel p with HSV values denoted by H(p), S(p),V (p), the color thresholding function (p)is defined inEq. (5):",
  ". Histogram-based Analysis of the ThresholdedROI": "The histogram analysis focuses on the vertical projection ofwhite pixels in the thresholded ROI, analyzing the spatialdistribution and density of the taxiway line markings.For a thresholded ROI with dimensions W H, whereW represents the width (number of columns) and H repre-sents the height (number of rows), the binary representationof a pixel at position i, j is defined in the Eq. (6), where i isthe column index and j is the row index:",
  "j=0B(i, j)(7)": "For each column i, this equation aggregates the presenceof white pixels across all rows j, offering a count of whitepixels for that column. When plotted against the columnindex i, the histogram produced accentuates the density ofwhite pixels along the y-axis. Peaks in this histogram, de-note the presence and spatial positioning of taxiway linemarkings within the ROI.",
  ". Identifying Line Markings and Mitigating FalseDetections": "The histogram peak value, represented by Hp, indicates thehighest density of white pixels in a particular column of thehistogram. It was important to identify whether a peak inthe histogram truly represents a taxiway line marking or incontrast, is a result of noise or other disturbances. We iden-tified the presence of a taxiway line marking in the ROIusing a threshold value.",
  ". CIRCLEDAT: Circular Threshold Pixel Dis-covery and Traversal Algorithm": "The CIRCLEDAT algorithm assists in isolating the pixelsthat correspond to the taxiway line markings, irrespective oftheir dimension or curvature, and eliminate all other pixelsfrom the ROI. For an ROI I with dimensions W H, aninitial coordinate pair (x, y), a radius , a set V for recordingvisited pixels, and an array L for collecting taxiway linemarking pixels, the algorithm executes as follows:1. Initialization: The starting point (x, y) is pushed onto astack S and recorded in V. A set R is created containingall pixel coordinates within the circular distance. 2. Exploration: While S is not empty, the top coordinate(x, y) is popped. If I[y][x] = 255, which indicates awhite pixel, (x, y) is added to L. For each offset (i, j) inR: Calculate new coordinates (xnew, ynew) = (x+i, y+j). If (xnew, ynew) V or (xnew, ynew) is outside 0 xnew < W and 0 ynew < H, continue to the nextoffset.",
  ". Frame Unwarping and Annotating the TaxiwayLine Marking Pixels": "The final stage of labeling the frame involves an inverse per-spective transformation that returns the warped ROI to itsoriginal view, as shown in (b) ,(e), (h). After obtain-ing the unwarped ROI, white pixels are located and mappedonto the original frame using color red to clearly identifythe taxiway line markings, as illustrated in (c), (f), (i).This representation provides a precise and clear depiction ofthe taxiway line markings in the original frame. In addition,a text file is generated containing the x, y coordinates of allpixels corresponding to the taxiway line markings.Having completed the labeling of the videos initial",
  ". ALINA Performance: Specifications and Sce-narios": "The detailed processing time breakdown for ALINA whenlabeling a frame is presented in Tab. 1. On average, ALINArequires 50.9 milliseconds (ms) to label a single frame,yielding a rate of approximately 19.65 frames per second(fps).In the course of our systematic labeling process,ALINA effectively labeled 60,249 frames on a Linux sys-tem equipped with an Intel Core i7-9700K CPU clockedat 3.60GHz and 15GB RAM, operating on Ubuntu 22.04.1LTS. The algorithm was developed in the PyCharm IDEwith Python 3.8.15, harnessing essential libraries such asOpenCV, NumPy, MatPlotLib, and statistics.",
  "Total50.09": "As mentioned earlier, we applied ALINA on a subsetof the AssistTaxi dataset, consisting of 60,249 frames ex-tracted from three distinct videos, each with unique cameraangles. and demonstrates ALINAs consis-tent labeling performance across these frames, unaffectedby differing camera angles or weather conditions. Specifi-cally, in (a) from the first video, ALINA labeled thetaxiway line marking between two aircrafts. In (e)from the second video, it labeled three directional taxiwayline markings: left, straight, and right. Lastly, in (i) from the third video, ALINA labeled the taxiway linemarking in front of the aircraft, which is situated betweenstationary aircraft on the left and grassy area on the right.",
  "1if Hp > 00otherwise(8)": "Here (p) indicates the presence (1) or absence (0) ofthe taxiway line marking.Through empirical testing and histogram analyses, weidentified that true taxiway line markings consistentlyyielded peak values significantly above sporadic noise. Asshown in Tab. 2, setting a threshold at 75 led to a substan-tial fall in the percentage of false positives, but by raisingthe threshold to 150, the false positives were dropped to 0.Hence, the Toptimal was established at 150, using the equa-tions 9 and 10. This was primarily because true taxiway linemarkings consistently resulted in columns extending wellover 150 white pixels, while disturbances never reached thislevel.",
  "1if Hp >= Toptimal0otherwise(10)": "Once the taxiway line marking is detected using150(p), ALINA extracts its centroid coordinate, initializ-ing the subsequent CIRCLEDAT algorithm. In contrast, ifthere is no taxiway line marking in the ROI, ALINA simplystores the frame along with an empty text file, signifyingthat there is no taxiway line marking present in the frame,as shown in .",
  ". Generating a Context-based Edge Map Set": "To assess ALINA and CDLEMs effectiveness, we man-ually created a set of context-based edge maps (CBEM),which emphasize the edge pixel presence, edge corner lo-calization, thick edge occurrence, and edge connectivity. We prioritized the detection of the edges of a taxiwayline marking as our primary validation metric, because thisalone provides precise information into the line markingsposition within the frame. Our approach for developing theCBEM was as follows:1. Outlining the Contour Region: The taxiway line mark-ings in the frames were outlined manually to create anaccurate reference without including any other edge de-tails from the frame.",
  ". (a) Manually Outlined Contour Region (b) CBEM": "2. Pre-processing: The contour region was transformed tograyscale and then subjected to Gaussian blur, whichhelped in reducing the visual noise and improved theclarity of edges. Consequently, the gradient amplitudeand direction were calculated and non-maximum sup-pression was applied to eliminate the non-edge pixels. 3. Edge Detection with the Canny Algorithm: We usedthe Canny algorithm for precise edge extraction of taxi-way line markings.We also employed an automatedmethod for selecting the upper and lower thresholds .This method computes the median pixel intensity v of animage and subsequently determines the thresholds usingequations 11 and 12:",
  "upper = min(255, (1.0 + ) v)(12)": "where is a coefficient, defaulting to 0.33, for refiningthe thresholds.The process of manually outlining the contour regionaround the taxiway line marking and generating CBEM fora frame is illustrated in (a) and (b) respectively. The (e) shows the output of ALINA on the same frame.From the 60,249 frames, we selected a set of 120 framesto construct the CBEMs. The 60,249 frames spanned fromthree videos with durations of 11.47, 1.34, and 3.23 min-utes. Our objective was to identify the frames representingscenario shifts. Instead of conducting a granular frame-by-frame analysis, we reviewed the videos comprehensivelyand marked the specific frames that captured the scenarioshifts, amounting to a total of 120 images.Our selection is underpinned by the Law of Large Num-bers (LLN), as illustrated in Eq. (13):",
  "n N(0, 1)(14)": "where Sn = X1 + X2 + . . . + Xn and Xi are indepen-dent, identically distributed random variables, n is the sam-ple size, is the population mean, and is the standarddeviation. The CLTs cornerstone assertion, relevant in ourcontext, is that with a sufficiently extensive sample size, thesample means distribution gravitates towards a normal dis-tribution. This holds irrespective of the originating popu-lations distribution. Therefore, the mean distribution ex-trapolated from all possible 120-frame subsets is poised toachieve normality. Leveraging the Central Limit Theorem,our diverse 120-frame sample is statistically representativeof the entire 60,249-frame dataset. With the backing of bothLLN and CLT, our method ensures a robust evaluation ofALINA and CDLEM using CBEM set.",
  ". Sliding Window Vs CIRCLEDAT": "In the study by Muthalagu et al. , a sliding window(SW) search algorithm detected lane line marking pixelswith a time complexity of O(m n), where m and nrepresent the height and width of the frame, respectively.In contrast, our work introduces the innovative CIRCLE-DAT algorithm, which pinpoints line marking pixels witha significantly reduced time complexity of O(k), where kis number of pixels corresponding to the line marking in agiven frame. Prior to introducing CIRCLEDAT, we used theSW search algorithm to detect taxiway line marking pixels.Tab. 3 compares the performance of both algorithms whentested on the taxiway dataset frames.",
  ". Performance Evaluations": "We evaluated ALINAs performance against CDLEM usingCBEM set. By comparing x and y coordinates from boththe CBEM and ALINA or CDLEM, we calculated true pos-itives (TP) and false negatives (FN). TP indicates accurateidentification of taxiway line marking pixels, while FN de-notes missed pixels that should have been identified as partof the taxiway line marking.The recall or detection rate, essential for evaluating ob-ject detection algorithms, gauges an algorithms accuracyin identifying the particular objects in a frame . Inairport taxiway line marking detection, missing a markingcan pose safety risks, underscoring the importance of com-prehensive identification. The detection rate is calculated asthe ratio of TP values to the sum of TP and FN values, asshown in Eq. (15).",
  "TP + FN(15)": "ALINA and CDLEM achieved detection rates of 98.45%and 91.14%, respectively, as shown in Tab. 4. Addition-ally, in terms of processing time, ALINA processed a framein 50.09 ms, corresponding to approximately 19.65 fps,whereas CDLEM took 120.35 ms per frame, translating toroughly 8.30 fps.ALINAs superior performance is attributed to severalkey features.Its perspective transformation provides abirds-eye view of taxiway, eliminating distortions and of-fering clarity in distinguishing line markings from anoma-liesa challenge for CDLEM due to varying distances andangles. Unlike CDLEM, which may miss essential pixelsusing the Hough Transform and curve fitting, ALINAs shiftto the HSV color space, prioritizing H, S, and V compo-nents, enhances taxiway line marking detection even un-der varying weather. The CIRCLEDAT algorithm withinALINA swiftly captures all crucial pixels of taxiway mark-ings, regardless of their shape or fragmentation. A notablelimitation of CDLEM is its need to define a new ROI foreach scenario shift, resulting in 120 scenarios for 60,249frames, demanding extensive video pre-viewing. In con-trast, ALINA only requires an ROI for the initial frame of avideo, applied consistently to all following frames, leadingto just 3 ROIs for the same number of frames.",
  ". Conclusion": "In this work, we propose ALINA, a novel annotation frame-work, primarily designed for labeling pixel coordinates intaxiway datasets.This approach streamlines the annota-tion process, significantly reducing cost and manual laborneeded for precise labeling in these contexts.We alsopropose a traversal algorithm CIRCLEDAT, which deter-mines the pixels corresponding to the taxiway line mark-ings. We provide a comparative analysis with the slidingwindow search algorithm and evaluate the performance ofthe framework on a subset of the AssistTaxi dataset. Wehave tested ALINA with labels generated for 60,249 frames,and evaluated it with a context-based edge map (CBEM) setwhich was generated manually. We also provide theoreticalanalysis and a comparative study for ALINA to Contour-Based Detection and Line Extraction Method (CDLEM).In the future, we aim to evaluate ALINA for annotatingcar lane datasets, with the CIRCLEDAT algorithm beingutilized to identify pixel coordinates of road lane mark-ings. Md Abdullah Al Noman, Li Zhai, Firas Husham Almukhtar,Md Faishal Rahaman, Batyrkhan Omarov, Samrat Ray, Sha-hajan Miah, and Chengping Wang. A computer vision-basedlane detection technique using gradient threshold and hue-lightness-saturation value for an autonomous vehicle. Inter-national Journal of Electrical and Computer Engineering,13(1):347, 2023. 3 Heba Aly,Anas Basalamah,and Moustafa Youssef.Lanequest: An accurate and energy-efficient lane detectionsystem. In 2015 IEEE International Conference on PervasiveComputing and Communications (PerCom), pages 163171,2015. 3",
  "Heng-Da Cheng, X H Jiang, Ying Sun, and Jingli Wang.Color image segmentation: advances and prospects. Patternrecognition, 34(12):22592281, 2001. 4": "Shriyash Chougule, Nora Koznek, Asad Ismail, GaneshAdam, Vikram Narayan, and Matthias Schulze.Reliablemultilane detection and classification by utilizing cnn as aregression network. In Proceedings of the European con-ference on computer vision (ECCV) workshops, pages 00,2018. 2 Luca Cultrera, Lorenzo Seidenari, Federico Becattini, PietroPala, and Alberto Del Bimbo. Explaining autonomous driv-ing by learning end-to-end visual attention. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR) Workshops, 2020. 1",
  "Flight Safety Foundation.Asn aviation safety database: 2023. 2": "Teodor Fredriksson, David Issa Mattos, Jan Bosch, and He-lena Holmstrom Olsson. Data labeling: An empirical investi-gation into industrial challenges and mitigation strategies. InInternational Conference on Product-Focused Software Pro-cess Improvement, pages 202216. Springer, 2020. 2 Parth Ganeriwala, Siddhartha Bhattacharyya, Sean Gun-ther, Brian Kish, Mohammed Abdul Hafeez Khan, AnkurDhadoti, and Natasha Neogi. Assisttaxi: A comprehensivedataset for taxiway analysis and autonomous operations. In2023 International Conference on Machine Learning andApplications (ICMLA), pages 10941099, 2023. 2, 3 Parth Ganeriwala, Siddhartha Bhattacharyya, and RajaMuthalagu. Cross dataset analysis and network architecturerepair for autonomous car lane detection*. In 2023 IEEEIntelligent Vehicles Symposium (IV), pages 16, 2023. 3",
  "Jeffrey Kiske. Automatic labeling of lane markings for au-tonomous vehicles. 3": "Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. End-to-end lane shape prediction with transformers. In Proceed-ings of the IEEE/CVF winter conference on applications ofcomputer vision, pages 36943702, 2021. 2 Claire Meymandi-Nejad, Esteban Perrotin, Ariane Herbulot,and Michel Devy. Aircraft navigation on taxiways: Evalua-tion of line detection algorithms proposed for automotive ap-plications. In Proceedings of the 2020 4th International Sym-posium on Computer Science and Intelligent Control, NewYork, NY, USA, 2021. Association for Computing Machin-ery. 2 RajaMuthalagu,AnudeepsekharBolimera,andVKalaichelvi. Lane detection technique based on perspectivetransformation and histogram analysis for self-driving cars.Computers & Electrical Engineering, 85:106653, 2020. 3, 8 Nandith Narayan, Parth Ganeriwala, Randolph M. Jones,Michael Matessa, Siddhartha Bhattacharyya, Jennifer Davis,Hemant Purohit, and Simone Fulvio Rollini.Assuringlearning-enabled increasingly autonomous systems*.In2023 IEEE International Systems Conference (SysCon),pages 17, 2023. 2 Davy Neven, Bert De Brabandere, Stamatios Georgoulis,Marc Proesmans, and Luc Van Gool. Towards end-to-endlane detection: an instance segmentation approach. In 2018IEEE Intelligent Vehicles Symposium (IV), pages 286291,2018. 3 Rafael Padilla, Wesley L Passos, Thadeu LB Dias, Sergio LNetto, and Eduardo AB Da Silva. A comparative analysisof object detection metrics with a companion open-sourcetoolkit. Electronics, 10(3):279, 2021. 8 Marc Proesmans Proesmans, Bert De Brabandere Braban-dere, Davy Neven Neven, Luc Van Gool Gool, and Stama-tios Georgoulis Georgoulis. Towards end-to-end lane detec-tion: an instance segmentation approach. 2018. 2 Zequn Qin, Huanyu Wang, and Xi Li. Ultra fast structure-aware deep lane detection. In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part XXIV 16, pages 276291. Springer,2020. 2 Zhan Qu, Huan Jin, Yang Zhou, Zhen Yang, and Wei Zhang.Focus on local: Detecting lane marker from bottom up viakey point.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1412214130, 2021. 2",
  "Kevin Theuma, David Zammit-Mangion, Jason Gauci, Ken-neth Chircop, and Nicolas Riviere.A particle filter forground obstacle tracking in the airfield. In AIAA Scitech 2019Forum, page 1265, 2019. 2": "Bingke Wang, Zilei Wang, and Yixin Zhang. Polynomialregression network for variable-number lane detection. InComputer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XVIII16, pages 719734. Springer, 2020. 2 Jinsheng Wang, Yinchao Ma, Shaofei Huang, Tianrui Hui,Fei Wang, Chen Qian, and Tianzhu Zhang. A keypoint-basedglobal association network for lane detection. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 13921401, 2022. 2 Shenghua Xu, Xinyue Cai, Bin Zhao, Li Zhang, Hang Xu,Yanwei Fu, and Xiangyang Xue. Rclane: Relay chain pre-diction for lane detection. In European Conference on Com-puter Vision, pages 461477. Springer, 2022. 2 Seungwoo Yoo, Hee Seok Lee, Heesoo Myeong, SungrackYun, Hyoungwoo Park, Janghoon Cho, and Duck Hoon Kim.End-to-end lane marker detection via row-wise classifica-tion. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition workshops, pages 10061007, 2020. 2"
}