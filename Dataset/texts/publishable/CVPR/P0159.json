{
  "Abstract": "Diffusion models have demonstrated remarkable perfor-mance in image and video synthesis. However, scaling themto high-resolution inputs is challenging and requires re-structuring the diffusion pipeline into multiple independentcomponents, limiting scalability and complicating down-stream applications. In this work, we study patch diffusionmodels (PDMs) a diffusion paradigm which models thedistribution of patches, rather than whole inputs, keepingup to 0.7% of the original pixels. This makes it very ef-ficient during training and unlocks end-to-end optimizationon high-resolution videos.We improve PDMs in two principled ways. First, to en-force consistency between patches, we develop deep con-text fusion an architectural technique that propagates thecontext information from low-scale to high-scale patchesin a hierarchical manner. Second, to accelerate trainingand inference, we propose adaptive computation, which al-locates more network capacity and computation towardscoarse image details. The resulting model sets a new state-of-the-art FVD score of 66.32 and Inception Score of 87.68in class-conditional video generation on UCF-101 2562,surpassing recent methods by more than 100%. Then, weshow that it can be rapidly fine-tuned from a base 36 64low-resolution generator for high-resolution 64288512text-to-video synthesis.To the best of our knowledge,our model is the first diffusion-based architecture whichis trained on such high resolutions entirely end-to-end.Project webpage:",
  ". Introduction": "Recently, diffusion models (DMs) have achieved remark-able performance in image and video synthesis, greatlysurpassing previous dominant generative paradigms, suchas GANs , VAEs and autoregressive models .However, scaling them to high-resolution inputs broke theirend-to-end nature, since training the full-scale monolithicfoundational generator led to infeasible computational de-mands . Splitting the architecture into several stages",
  "tiled inference": ". Comparing existing diffusion paradigms: Latent Dif-fusion Model (LDM) (upper left), Cascaded DiffusionModel (CDM) (bottom left), and Patch Diffusion Model (thiswork) during training (upper right) and inference (bottom right).In our work, we develop hierarchical patch diffusion, which neveroperates on full-resolution inputs, but instead optimizes the lowerstages of the hierarchy to produce spatially aligned context infor-mation for the later pyramid levels to enforce global consistencybetween patches. satisfied the immediate practical needs, but having multiplecomponents in the pipeline makes it harder to tune and com-plicates downstream tasks like editing or distillation.For example, LDM trains a diffusion model in thelatent space of an autoencoder, which requires an additionalextensive hyperparameters search. The original work hasdedicated more than a dozen experiments to it (see Tab. 8of ), and the search for its optimal design is still on-going . Moreover, retraining an auto-encoder re-quires retraining the latent generator, resulting in extra com-putational costs. Also, having multiple components com-plicates downstream applications: for example, SnapFu-sion had to come with two unrelated sets of techniquesto distill the generator and the auto-encoder separately.",
  "Full-resolution DM65.31.24OOMOOMHPDM (32 1282 patch size)29.02.6441.31.55+ adaptive computation23.43.5829.92.49HPDM (16 642 patch size)18.14.2522.12.78+ adaptive computation14.26.7116.34.96": "Cascaded DM (CDM) sequentially trains severaldiffusion models of increasing resolution, where each nextDM is conditioned on the outputs of the previous one. Thisframework enjoys a more independent nature of its compo-nents, where each generator is trained independently fromthe rest, but it has more modules in the pipeline (e.g., Im-agenVideo consists of 7 video generators) and moreexpensive inference. An end-to-end design is a highly desir-able property of a diffusion generator, from the perspectivesof both practical importance and conceptual elegance.The main obstacle to moving a standard high-resolutionDM onto end-to-end rails is an increased computationalburden. In the past, patch-wise training proved success-ful for GAN training for high-resolution image , video(e.g., ) and 3D (e.g., ) synthesis, but, how-ever, has not picked up much momentum in the diffu-sion space.To our knowledge, PatchDiffusion andMaskDIT are the only works that explore it, but noneof them considers the required level of input sparsity toscale to high-resolution videos: PatchDiffusion still relieson full-resolution training for 50% of its optimization (so itis not purely patch-wise), while MaskDIT preserves 50%of the original input. In our work, we explore patch diffu-sion models while keeping just up to 0.7% of the originalpixels. The comparison of patch-wise training and conven-tional paradigms is depicted in , and in , weshow that it can achieve 5 larger throughput and is train-able on high-resolution videos. We focus on video synthesissince, for videos, the computational burden of high reso-lutions is considerably more pronounced than for images:there now exist end-to-end image diffusion models that areable to train even on 10242 resolution (e.g., ).For our patch-wise training, we consider a hierarchy ofpatches instead of treating them independently , whichmeans that the synthesis of high-resolution patches is con-ditioned on the previuosly generated low-resolution ones.It is a similar idea to cascaded DMs and helps to im-prove the consistency between patches and simplifies noisescheduling for high resolutions . To improve boththe qualitative performance and computational efficiency of patch diffusion, we develop two principled techniques:deep context fusion and adaptive computation.Deep context fusion considers conditions the generationof higher-resolution patches on subsampled, positionallyaligned features from the lower levels of the pyramid. Itserves as an elegant way to incorporate global context in-formation into synthesis of higher-frequency textural de-tails and to facilitate knowledge sharing between the stages.Adaptive computation restructures the model architecturein such a way that only a subset of layers operate on high-resolution patches, while more difficult low-resolution onesgo through the whole pipeline.We apply the designed techniques to the recent attention-based RIN generator , and benchmark our approachon two video generation datasets: UCF-101 in the64 2562 resolution, and our internal dataset of text/videopairs for 64 288 512 (and 16 576 1024) text-to-video generation. Our model achieves state-of-the-art per-formance on UCF-101 and demonstrates strong scalabilityperformance for large-scale text-to-video synthesis.",
  ". Related work": "High-level diffusion paradigms. To the best of our knowl-edge, one can identify two main conceptual paradigms onhow to structure a high-resolution diffusion-based genera-tor: latent diffusion models (LDM) and cascaded dif-fusion models (CDM) . For CDMs, it was shown thatthe cascade can be trained jointly , but scaling for highresolutions or videos still requires progressive training fromlow-resolution models to obtain competitive results .Video diffusion models. The rise of diffusion models asfoundational image generators motivated the com-munity to explore them for video synthesis as well .VDM is one of the first works to demonstrate theirscalability for conditional and unconditional video gener-ation using the cascaded diffusion approach .Ima-genVideo further pushes their results, achieving pho-torealistic quality. VIDM designs a separate moduleto implicitly model motion.PVDM trains a diffu-sion model in a spatially decomposed latent space. Make-A-Video uses a vast unsupervised video collection intraining a text-to-video generator by fine-tuning a text-to-image generator. PYoCo and VideoFusion designspecialized noise structures for video generation. Numer-ous works explore training of a foundational video genera-tor on limited resources by fine-tuning a publicly availableStableDiffusion model for video synthesis (e.g.,).Another important line of researchis the adaptation of the foundational image or video gen-erators for downstream tasks, such as video editing (e.g.) or 4D generation . None of thesemodels is end-to-end and all follow cascaded or la-tent diffusion paradigms. Patch Diffusion Models. Patch-wise generation has a longhistory in GANs and has enjoyed applications in im-age , video and 3D synthesis . In the contextof diffusion models, there are several works that explorepatch-wise inference to extend foundational text-to-imagegenerators to higher resolutions than what they had beentrained on (e.g., ). Also, a regular video diffu-sion model can be inferred in an autoregressive manner atthe test time because it can be easily conditioned on its pre-vious generations via classifier guidance or noise initializa-tion , and this kind of synthesis can also be seen as apatch-wise generation. Later stages of CDMs can also op-erate in a patch-wise fashion , even though they havenot been explicitly trained for this. These works have rele-vance to ours, since they design patch-wise sampling strate-gies with better global consistency in the resulting samplesand thus could be employed for our generator as well.The primary focus of our work is patch-wise training ofdiffusion models, which has been explored in several priorworks. Several works (e.g., ) train a diffusionmodel on a single image to produce its variations .The closest work to ours is PatchDiffusion , which ex-plores direct patch-wise diffusion training.However, tolearn the consistent global image structure, their developedmodel operates on full-size inputs in 50% of the optimiza-tion steps, which is computationally infeasible for high-resolution videos. Our generator design, in contrast, neveroperates on full-resolution videos and instead relies on con-text fusion to enforce the consistency between the patches.Apart from expensive training, diffusion models also suf-fer from slow inference , and some works explored alter-native denoising paradigms (e.g., ) to mitigate this,which is a close but orthogonal line of research.",
  ". Diffusion Models": "Given a dataset X = {x(n)}Nn=1, consisting of N samplesx(n) Rd (most commonly images or videos), we seek torecover the underlying data-generating distribution x(n) p(x). We follow the general design of time-continuous dif-fusion models , for which a neural network D(x; ) istrained to predict ground-truth dataset samples x from theirnoised versions x = x + , N(0, I):",
  "& add noise": ". Architecture overview of Hierarchical Patch DiffusionModel (HPDM) for a 3-level pyramid. The model is trained to de-noise all the patches jointly. During training, we use only a singlepatch from each pyramid level and restrict information propaga-tion in the coarse-to-fine manner. This allows one to synthesizethe whole image (or video) at a given resolution patch-by-patchusing tiled inference (see ). For large enough , the corrupted sample x is indistinguish-able from pure Gaussian noise, and this allows to employthe score predictor for sampling at test-time using Langevindynamics (with 0 and T ):",
  ". Recurrent Interface Networks": "For our base architecture, we chose to follow Recurrent In-terface Networks (RINs) for their simplicity and ex-pressivity. A typical RIN network has a uniform structureand consists of a ViT-like linear image tokenizer, fol-lowed by a sequence of identical attention-only blocks anda linear detokenizer to transform the image tokens back tothe RGB pixel values. RIN blocks do not employ an ex-pensive self-attention mechanism and instead rely onlinear cross-attention layers with a set of learnable latent to-kens. This allows to scale gracefully with input resolutionwithout sacrificing communication between far-away inputlocations. We refer the reader to the original work for",
  ". Method": "Our high-level patch diffusion design is different fromPatchDiffusion in that our model never operates onfull-resolution inputs.Instead, we consider a hierarchi-cal cascade-like structure consisting of L stages and patchscales s decrease exponentially:s = 1/2 for {0, 1, ..., L}. Patches are always of the same resolution r =(rf, rh, rw), which leads to substantial memory and compu-tational savings compared to full-resolution training. Dur-ing training, we randomly sample a video from the datasetand extract a hierarchy of patch coordinates c0, ..., cL insuch a way that the -th patch is always located inside theprevious < patches so that they provide the necessarycontext information. Hierarchical patch diffusion is trainedto jointly denoise a combination of these patches, denotedas P = (p) = 0L, and their corresponding noise levels = ()L=0:",
  "D( P0; ) P 22 min ,(4)": "where each patch is corrupted independently: P = (p +)L=0, N(0, , I) Restricting the information flowin the coarse-to-fine manner (see ) allows to do infer-ence at test-time in the cascaded diffusion fashion .Below, we elaborate on three fundamental componentsof our method that allow a patch-wise paradigm to achievestate-of-the-art results in video generation: deep context fu-sion, adaptive computation and overlapped sampling.",
  "p = downsample(crop(x; ); r),(6)": "where the crop function slices the input signal given thepixel offsets , and downsample resizes it to the specifiedresolution rf rh rw.Since we consider a hierarchical structure, during train-ing, we use fixed scales for each -th level s()= r/R, butrandomly sample offsets () U[0, 1 s() ]. For a level",
  "LinearContext Fusion": ". Deep Context Fusion. At each pyramid level, we grid-sample the features of a lower-resolution patch and concatenatethem to the activations tensor of the current level. In this way,the information propagates in the coarse-to-fine manner and pro-vides richer context than pixel-space concatenation of cascadedDMs (see Tab. 3). > 1, we sample its corresponding offset ()in each -thdimension in such a way, that the resulting patch is alwayslocated inside the patch from the previous pyramid level, asvisualized in . For brevity, we will omit the level su-perscript in the subsequent exposition for patch parameters.Setting patch resolutions rf, rh, rw lower than originalones Rf, Rh, Rw leads to drastic improvements in compu-tational efficiency, but worsens the global consistency ofthe generated samples. In , the authors use variable-resolution training, including 50% of optimization stepsperformed on full-size inputs to improve the consistency.The downside of such a strategy is that it undermines com-putational efficiency: for a large enough video, the modelcannot fit into GPU memory even for a batch size of 1. In-stead, in our work, we demonstrate that consistent genera-tion can be achieved with deep context fusion: conditioninghigher resolution generation on the activations from previ-ously generated stages.",
  ". Deep Context Fusion": "The main struggle of patch-wise models is preserving theconsistency between the patches, since each patch is mod-eled independently from the rest, conditioned on the previ-ous pyramid stage. Cascaded DMs provide the condi-tioning to later stages by simply concatenating an upsam-pled low-resolution video channel-wise to the currentlatent. While it can provide the global context informa-tion when the model operates on a full-resolution input,for patch-wise models, this leads to drastic context cut-outs, which, as we demonstrate in our experiments, severelyworsens the performance.Also, it limits the knowledgesharing between lower and higher stages of the cascade. Toaddress this issue, we introduce deep context fusion (DCF),a context fusion strategy that conditions the higher stages ofthe pyramid on spatially aligned, globally pooled featuresfrom the previous stages.For this, before each RIN block of our model, we pool the global context information from previous stages into itsinputs. For this, we use the patch coordinates to grid-samplethe activations with trilinear interpolation from all previ-ous pyramid stages, average them, and concatenate to thecurrent-stage features.More precisely,for a given patch b-th block in-puts ab1Rdrf rhrw with coordinates c=(s, f, h, w) R4 at the -th pyramid level; 1 contextpatches activations (ab1k)1k=1 with respective coordinates(ck)1k=1, we compute the context ctx Rdrf rhrw as:",
  "(9)": "Deep context fusion is illustrated in .To keep the dimensionalities the same across the net-work, we then project the resulted tensor fuse[]R(2d+3)rf rhrw with a learnable linear transformation.We considered other aggregation strategies, like concatenat-ing all the levelss features or averaging, but the former oneblows up the dimensionalities, making the training expen-sive, while the latter one was leading to poor performancein our preliminary experiments.An additional advantage of DCF compared to shallowcontext fusion of regular cascaded DMs is that the gradientcan flow from the small-scale patch denoising loss to thelower levels of the hierarchy, pushing the earlier cascadestages to learn such features that are more useful to the laterones. We found that this is indeed helpful in practice andimproves the overall performance additionally by 5%.",
  ". Adaptive Computation": "Naturally, generating high-resolution details is consideredto be easier than synthesizing the low-resolution struc-ture . In this way, allocating the same amount of net-work capacity on high-resolution patches can be excessive,that is why we propose to use only some of the computa-tional blocks when running the last stages of the pyramid. We name this strategy adaptive computation1 and demon-strate that it improves our models efficiency by 60%without compromising the performance (see Tab. 3). Theuniform RINs structure (i.e., all the blocks are iden-tical and have the same input/output resolutions) allows usto implement this easily: one simply skips some of the ear-lier blocks when processing the high-resolution activations.The high-level pseudo-code is provided in Listing 1.",
  "Listing 1. Pseudo-code for adaptive computation (Sec. 4.3)": "Adaptive computation involves two design choices: 1)whether to skip earlier or later blocks in the networks forhigher resolutions, and 2) how to distribute the computationassignments among the blocks per each pyramid stage. Wechose to allocate the later blocks to perform full computa-tion to make the low-level context information go throughmore processing before being propagated to the higherstages. For the block allocations, we observed that sim-ply increasing the computation assignments linearly withthe block index worked well in practice.",
  ". Tiled Inference": "Sampling from HPDM is different from regular diffusionsampling, since it is patch-wise and we never operate onfull-resolution inputs. During inference, we generate pyra-mid levels one-by-one, starting from rtrhrw video (cor-responding to a patch of scale s = 1), then using to generatethe video of resolution 2rt 2rh 2rw (corresponding topatch scale s = 1/2), and so on until we produce the finalvideo of full resolution Rf Rh Rw. We visualize thishierarchical tiled inference process in (bottom right).Each next stage of the pyramid uses the generated videofrom the previous stage through the deep context fusiontechnique described in Sec. 4.2. DCF provides strong globalcontext conditioning, but it is sometimes not enough to en-force local consistency between two neighboring patches.To mitigate this, we employ the MultiDiffusion strategyand simply average-overlap the score predictions s(p, )during the denoising process. More concretely, to gener-ate a complete video x RRf RhRw, we first generate(2Rf 1)(2Rh1)(2Rw1) patches with 50% of thecoordinates overlapping between two neighboring patches.Then, we run the reverse diffusion process for each patch 1Our notion of adaptive computation is different from the originalRINs one, where it is used to describe the models ability to distribute itscomputational capacity differently between different parts of an input .",
  ". Miscellaneous techniques": "The core ideas that enable our work have been describedabove, but from the implementation and engineering stand-points, there are several other techniques that played an im-portant role in bolstering the performance and would be ofinterest to a practitioner aiming to reproduce our results.Additional details and failed experiments can be found inAppendix A and D, respectively.Integer patch coordinates. We noticed that sampling apatch on the L-th cascade level at integer coordinates allowsto prevent blurry artifacts in generated videos: they appeardue to oversmoothness effects of trilinear interpolation.Noise Schedule Each stage of the pyramid operates on dif-ferent frequency signals, and higher levels of the pyramidhave stronger correlations between patch pixels. Inspiredby , we found it helpful to use exponentially smaller in-put noise scaling with each increase in pyramid level.Cached inference During inference, we do not need to re-compute all the activations for the previous pyramid stages,which makes it possible to cache them, which works evenmore gracefully. Caching block features allowed to speedup the inference by 40%. However, for the large model,caching needs to be implemented with CPU offloading toprevent GPU out-of-memory errors.",
  ". Implementation details": "We use RINs instead of U-Nets as the back-bone since its uniform structure is conceptually simpler andaligns well with adaptive computation. We use v-predictionparametrization with extra input scaling . FollowingRINs, we train our model with the LAMB optimizer ,with the cosine learning rate schedule and the maximum LRof 0.005. Our model has 6 RIN blocks, and we distribute theload for adaptive computation as : e.g., the 1-st and 2-nd blocks only compute the first pyramid level, the3-rd and 4-rd ones first two levels of the pyramid, and soon. Not using adaptive computation is equivalent to having a load of , which is almost twice as expen-sive. We use 768 latent tokens of 1024/3072 dimensionalitywith 1 4 4 pixel tokenization for class-conditional/text-conditional experiments, respectively. To encode the tex-tual information, we rely on T5 language model anduse its T5-11B variant. Further implementation details canbe found in Appx C.",
  ". Experiments": "Datasets.In our work, we consider two datasets:1)UCF101 (for exploration and ablations) and 2) ourinternal video dataset to train a large-scale text-to-videomodel. UCF101 is a popular academic benchmark for un-conditional and class-conditional video generation consist-ing of videos of the 240 320 resolution with 25 FPS andhas an average video length of 7 seconds. Our internaldataset consists of 25M high-quality text/video pairs inthe style of stock footage with manual human annotationsand 70M of low-quality in-the-wild videos with automat-ically generated captions. Additionally, for text-to-videoexperiments, we used an internal dataset of 150M high-quality text/image pairs for extra supervision .Evaluation. Following prior work , we eval-uate the model with two main video quality metrics: FrechetVideo Distance (FVD) , and Inception Score (IS) .For FVD and IS, we report their values based on 10,000generated videos. But for ablations, we use FVD@512 in-stead for efficiency purposes: an FVD variant computed onjust 512 generated videos. We noticed that it correlates wellwith the traditional FVD, but with just a fixed offset. Apartfrom that, we report the training throughput for various de-signs of our network and also provide the samples from ourmodel for qualitative assessment.",
  "els are trained for the final video resolution of 64 2562": "with the pyramid 16 642 32 1282 64 2562.Main results. Our patch-wise model is trained on UCF-101 for 64 2562 generation entirely end-to-endwith the hierarchical patch sampling procedure describedin Sec. 4. In Tab. 5, we compare these results with recentstate-of-the-art methods: MoCoGAN-HD , StyleGAN-V , TATS , VIDM , DIGAN , PVDM .While our model is trained to synthesize 64 frames, we re-port quantitative results for 16 generated frames, since it is amuch more popular benchmark in the literature (for this, wesimply subsample 16 frames out of the generated 64). Ourmodel substantially outperforms all previously reported re-sults for this benchmark (i.e., for the 16 2562 resolutionand without pretraining) by a striking margin of more than100%. To our knowledge, these are the best reported FVDand IS scores for the 16 2562 resolution on UCF. Make-A-Video reports FVD of 81.25 and IS of 82.55 whenfine-tuned from a large-scale text-to-video generator.Ablations. We consider two lines of ablations: ablatingcore architectural decisions and benchmakring various in-ference strategies, since the latter also crucially influencesthe final performance. For the training components, we firstanalyze the influence of deep context fusion. For this, welaunch an experiment with shallow context fusion, wherewe concatenate only the RGB pixels (non-averaged, onlyfrom the patch of the previous pyramid level) as the con-text information. As one can see from the results in Tab. 3(first row), this strategy produces considerably worse results(though the training becomes 10% faster).The next ablation is whether the low-level pyramidstages indeed learn such features that are more useful forlater pyramid stages, when they are directly supervised withthe denoising loss of small-scale patches through the con-text aggregation procedure. For this ablation, we detach thecontext variable ctx from the autograd graph. The resultsare presented in Tab. 3 (second row). One can observe thatthe performance can be better for earlier pyramid stages,but the late stage suffers: this demonstrates that the loweststage indeed learns to encode the global context in a waythat is more accessible for later levels of the cascade, but by . Comparison with the recent state-of-the-art methods onUCF-101 16 2562 class-conditional video generation (notethat our model is trained 64 2562 videos). Note that Make-A-Video was pretrained on a large-scale text-to-video dataset.",
  "HPDM-S344.573.73HPDM-M143.184.29CVPR24HPDM-L66.3287.68": "sacrificing a part of its capacity due to this.One of the key techniques we used in our model is adap-tive computation, and in Tab. 3 (third row), we demonstratehow the model performs without it. While it allows to ob-tain slightly better results, it decreases the training speedby almost twice. The cost of the later pyramid stages be-comes even more critical during inference time, when sam-pling high-resolution videos.Finally, we verify the existing observation of the commu-nity that positional encoding in patch-wise models help inproducing more spatially consistent samples . Thiscan be seen from the worse FVD@512 scores in Tab. 3 (4throw) when no coordinates information is input to the modelin context fusion (Eq. (9)).",
  ". Text-to-video generation": "Training setup. To explore the scalability of the patch-wiseparadigm, we launched a large-scale experiment for HPDMwith 4B parameters on a text/video dataset consisting of95M samples. Since training a foundational model incursextreme financial costs, we instead found it financially lessrisky to fine-tune it from a low-resolution generator. Forthis, we used the base SnapVideo model, which oper-",
  "full overlappingspatial overlappingno overlapping": ". Effect of the overlapped inference on the consistencybetween the patches. Surprisingly, even without the full-resolutiontraining and patch overlapping, our deep context fusion strat-egy manages to preserve strong consistency in the generated sam-ple. See Tab. 4 for quantitative analysis. ates on 36 64 resolution videos. Our patch-wise vari-ant, HPDM-T2V, was trained for the final output resolu-tion of 64 288 512 with the pyramid 8 36 64 16 72 128 32 144 256 64 288 512(4 pyramid levels in total). This 4-level pyramid structureresults in just 4 (1/8)3 0.7% of the original video pixelsseen in each optimization step. The base 36 64 gener-ator was trained for 500,000 iterations, and we fine-tunedHPDM-T2V for 15,000 more steps (3% of the base genera-tor training steps) with a batch size of 4096. We also fine-tune another model, HPDM-T2V-1K, a 165761024 text-to-video generator with a patch resolution of 1672128.It is initialized from the base 36 64 SnapVideo diffu-sion model, but fine-tuned for 100,000 iterations. Longerfine-tuning was required for it since its input resolution waschosen to be larger than that of the base generator to make it",
  "HPDM-T2V72 128299.320.53HPDM-T2V144 256383.321.15HPDM-T2V288 512481.923.77HPDM-T2V-1K576 1024447.524.51": "have 4 levels in the pyramid instead of 5. Apart from videos,following prior works (e.g., ), we utilize joint im-age/video training. For image training with RINs, followingSnapVideo , we simply repeat the image along the timeaxis to convert it into a still video.Results. We test the results quantitatively by reporting zero-shot performance on UCF-101 in terms of FVD and ISin Tab. 5, and also qualitatively by providing visual compar-isons with existing foundational generators in . Al-though trained for just 15,000 steps, HPDM-T2V yieldspromising results and has a comparable generation qual-ity to modern foundational text-to-video models (ImageV-ideo , Make-A-Video , and PYoCo ) on sometext prompts (see ).We provide more qualitative results on the project web-page:",
  ". Conclusion": "In this work, we developed the hierarchical patch diffusionmodel for high-resolution video synthesis, which effi-ciently trains in the end-to-end manner directly in the pixelspace, and is amenable to swift fine-tuning from a baselow-resolution diffusion model. We showed state-of-the-artvideo generation performance on UCF-101, outperformingthe recent methods by 100% in terms of FVD, andpromising scalability results for text-to-video generation.The techniques we developed hold significant potentialforapplicationacrossvariouspatch-wisegenerativeparadigms, including GANs, VAEs, autoregressive models,and beyond. In future work, we intend to investigate bettercontext conditioning, sampling strategies with strongerdependence enforcement, and also other tokenization/deto-kenization transformations to mitigate dead pixels artifacts.",
  "Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.Multidiffusion: Fusing diffusion paths for controlled imagegeneration. arXiv preprint arXiv:2302.08113, 2023. 3, 5, 8": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, JianfengWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,Yufei Guo, Wesam Manassra, Prafulla Dhariwal, CaseyChu, Yunxin Jiao, and Aditya Ramesh. Improving imagegeneration with better captions. 2023. Accessed: 2023-11-14. 1 Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.Align your latents: High-resolution video synthesis with la-tent diffusion models. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages2256322575, 2023. 2, 8 Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, YipingDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, andWen Gao. Pre-trained image processing transformer. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1229912310, 2021. 1",
  "Ting Chen. On the importance of noise scheduling for diffu-sion models. arXiv preprint arXiv:2301.10972, 2023. 2, 3,6": "Katherine Crowson, Stefan Andreas Baumann, Alex Birch,Tanishq Mathew Abraham, Daniel Z Kaplan, and EnricoShippole. Scalable high-resolution pixel-space image syn-thesis with hourglass diffusion transformers. arXiv preprintarXiv:2401.11605, 2024. 2 Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, JialiangWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-aofang Wang, Abhimanyu Dubey, et al.Emu: Enhanc-ing image generation models using photogenic needles in ahaystack. arXiv preprint arXiv:2309.15807, 2023. 1",
  "Prafulla Dhariwal and Alexander Nichol. Diffusion modelsbeat gans on image synthesis. Advances in Neural Informa-tion Processing Systems, 34:87808794, 2021. 2, 3, 6, 1": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 3, 1 Patrick Esser,Johnathan Chiu,Parmida Atighehchian,Jonathan Granskog, and Anastasis Germanidis.Structureand content-guided video synthesis with diffusion models.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 73467356, 2023. 2",
  "Rinon Gal, Dana Cohen Hochberg, Amit Bermano, andDaniel Cohen-Or.Swagan: A style-based wavelet-drivengenerative model. ACM Transactions on Graphics (TOG),40(4):111, 2021. 5": "Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, GuanPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.Long video generation with time-agnostic vqgan and time-sensitive transformer. In European Conference on ComputerVision, pages 102118. Springer, 2022. 7, 2 Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, AndrewTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation:A noise prior for video diffusion models. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 2293022941, 2023. 2, 8",
  "Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.Tokenflow: Consistent diffusion features for consistent videoediting. arXiv preprint arxiv:2307.10373, 2023. 2": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. In Adv. NeuralInform. Process. Syst., 2014. 1, 3 Niv Granot, Ben Feinstein, Assaf Shocher, Shai Bagon, andMichal Irani. Drop the gan: In defense of patches nearestneighbors as single image generative models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1346013469, 2022. 3",
  "Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, andQifeng Chen. Latent video diffusion models for high-fidelityvideo generation with arbitrary lengths.arXiv preprintarXiv:2211.13221, 2022. 8, 2": "Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, BenPoole, Mohammad Norouzi, David J Fleet, et al. Imagenvideo: High definition video generation with diffusion mod-els. arXiv preprint arXiv:2210.02303, 2022. 2, 6, 8 Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,Mohammad Norouzi, and Tim Salimans. Cascaded diffusionmodels for high fidelity image generation. The Journal ofMachine Learning Research, 23(1):22492281, 2022. 1, 2,4",
  "Allan Jabri, David Fleet, and Ting Chen.Scalable adap-tive computation for iterative generation.arXiv preprintarXiv:2212.11972, 2022. 2, 3, 5, 6, 1": "Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,Eli Shechtman, Sylvain Paris, and Taesung Park.Scal-ing up gans for text-to-image synthesis. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1012410134, 2023. 1 Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.Elucidating the design space of diffusion-based generativemodels.Advances in Neural Information Processing Sys-tems, 35:2656526577, 2022. 3, 1, 2 Levon Khachatryan, Andranik Movsisyan, Vahram Tade-vosyan,RobertoHenschel,ZhangyangWang,ShantNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXivpreprint arXiv:2303.13439, 2023. 2",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,and Mark Chen. Hierarchical text-conditional image gen-eration with clip latents. arXiv preprint arXiv:2204.06125,2022. 2, 3, 1": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1068410695, 2022. 1, 2 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III18, pages 234241. Springer, 2015. 2, 6 Masaki Saito, Shunta Saito, Masanori Koyama, and So-suke Kobayashi. Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporalgan. International Journal of Computer Vision, 2020. 6, 2",
  "Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Sin-gan: Learning a generative model from a single natural im-age. In ICCV, 2019. 3": "Liao Shen, Xingyi Li, Huiqiang Sun, Juewen Peng, Ke Xian,Zhiguo Cao, and Guosheng Lin. Make-it-4d: Synthesizinga consistent long-term dynamic scene video from a singleimage. In Proceedings of the 31st ACM International Con-ference on Multimedia, pages 81678175, 2023. 2 Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,Oran Gafni, et al. Make-a-video: Text-to-video generationwithout text-video data. arXiv preprint arXiv:2209.14792,2022. 2, 6, 7, 8",
  "Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-basedgenerative modeling in latent space. 2021. arXiv preprintarXiv:2106.05931, 2021. 1": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 3 Weilun Wang, Jianmin Bao, Wengang Zhou, DongdongChen, Dong Chen, Lu Yuan, and Houqiang Li.Sindiffu-sion: Learning a diffusion model from a single natural im-age. arXiv preprint arXiv:2211.12445, 2022. 3 Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, JunchenZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-tention in spatiotemporal diffusions for text-to-video gener-ation. arXiv preprint arXiv:2305.10874, 2023. 2, 8 Zhendong Wang, Yifan Jiang, Huangjie Zheng, PeihaoWang, Pengcheng He, Zhangyang Wang, Weizhu Chen,and Mingyuan Zhou.Patch diffusion: Faster and moredata-efficient training of diffusion models.arXiv preprintarXiv:2304.12526, 2023. 2, 3, 4, 8 Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan WeixianLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, XiaohuQie, and Mike Zheng Shou. Tune-a-video: One-shot tuningof image diffusion models for text-to-video generation. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 76237633, 2023. 2",
  "Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen ChangeLoy. Rerender a video: Zero-shot text-guided video-to-videotranslation. arXiv preprint arXiv:2306.07954, 2023. 2": "Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, SanjivKumar, Srinadh Bhojanapalli, Xiaodan Song, James Dem-mel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimiza-tion for deep learning: Training bert in 76 minutes. arXivpreprint arXiv:1904.00962, 2019. 6 Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, HanZhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al.Magvit:Masked generative video transformer.In Proceedings of",
  "Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, JunhoKim, Jung-Woo Ha, and Jinwoo Shin. Generating videoswith dynamics-aware implicit generative adversarial net-works. In ICLR, 2022. 2, 7": "Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.Video probabilistic diffusion models in projected latentspace. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, 2023. 2, 6, 7, 1 Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri,Myle Ott, Sam Shleifer, et al.Pytorch fsdp:experi-ences on scaling fully sharded data parallel. arXiv preprintarXiv:2304.11277, 2023. 1",
  "A. Limitations": "Although our model provides considerable improvementsin video generation quality and enjoys a convenient end-to-end design, it still suffers from some limitations.Stitching artifacts.Despite using overlapped inference,our model occasionally exhibits stitching artifacts. We il-lustrate these issues in (left). Inference strategies withstronger spatial communication, like classifier guidance ,should be employed to mitigate them.Error propagation. Since our model generally follows thecascaded pipeline (with the difference thatwe train jointly and more efficiently), it suffers from thetypical cascade drawback: the errors made in earlier stagesof the pyramid are propagated to the next. The error propa-gation artifacts are illustrated in (left).Dead pixels. By dead pixels artifacts we imply failuresof the ViT -like pixel tokenization/detokenization pro-cedure, where the model sometimes produces broken 4 4patches. They are illustrated in . These artifacts areunique to RINs and we have not experienced them inour preliminary experiments with UNets . However,since they do not appear catastrophically often, we chose tocontinue to experiment with RINs.Slow inference. Patch-wise inference requires more func-tion evaluations at test time, which slows down the in-ference process. For our exponentially growing pyramidstarting at 8 36 64 and ending at 64 288 512,with full (i.e., maximal) overlapping, we need to produce(2 64",
  "B. Additional results": "There are multiple incosistencies in quantitative evaluationof video generators that are inconsistent between previousprojects . For FVD on UCF101 (the most pop-ular metric for it), there are differences in the amounts of Training iteration FVD value FVD@128FVD@256FVD@512FVD@2048FVD@10k .Using different amounts of fake videos to computeFVD gives very correlated, but offset values with the maintrend being the more - the better. We hypothesize that usingmore synthetic samples yields better coverage of different modesof the data distribution and decreases the influence of outliers.These FVD scores are computed for different training steps ofHPDM-S. Using too few videos leads to undiscriminative resultsonly closer to convergence. fake/real videos used to compute the statistics, FPS values,resolutions, and real data subsets (train or train + test).To account for these differences, in Tab. 6, we release acomprehensive set of metrics for easier assessment of ourmodels performance in comparison with the prior work.Apart from that, it also includes additional models, HPDM-S and HPDM-M, and also the results for the fixed version ofour text-to-video HPDM model (after the main deadline, wenoticed that our FSDP-based training was not updatingsome of the EMA parameters properly, which was the causeof gaussian jitter artifacts in ).To compute real data FVD statistics, we always use thetrain set of UCF-101 (around 9.5k videos in total). We trainthe models with the default 25FPS resolution. Our modelsare trained for 64 frames, and to compute the results for16 frames, we simply take the first 16 frames out of thesequence.Additional results are also provided on the project web-page:",
  "C. Implementation details": "In this section, we provide additional implementation de-tails for our model. We train our model in a patch-wisefashion with the patch resolution of 16 64 64 for UCF-101 and 8 36 64 for text-to-video generation. Af-ter the main deadline, we continued training our model onUCF for several more training steps, and also trained twosmaller versions for fewer steps. We denote the smaller ver-sions as HPDM-S and HPDM-M, while the larger one isdenoted as HPDM-L. They differ in the amount of train- . Additional FVD evaluation results for class-conditional UCF-101 video generation. Pre-trained denotes whether the model waspre-trained on an external dataset. #samples is the amount of fake videos used to compute the fake data statistics. In , we alsodemonstrated that FVD scores computed for different amount of samples are well-correlated with one another. For IS, we cannot computeit for 64-frames-long videos due to the design of C3D model .",
  "ing steps performed and also the latent dimensionality of": "RINs : 256, 512 and 1024, respectively. Our text-to-video model HPDM-T2V was fine-tuned for 15k steps andHPDM-T2V-1K for 100k steps. We provide the hyperpa-rameters for our models in Tab. 8. For sampling, we usespatial 50% patch overlapping to compute the metrics (forperformance purposes), and full overlapping for visualiza-tions. We use stochastic sampling with second-order cor-rection for the first pyramid level. For later stages, weuse Also, we disabled stochasticity for text-to-video syn-thesis since we have not observed it to be improving theresults. We use 128 steps for the first pyramid stage, andthen decrease them exponentially for later stages, dividingthe number of steps by 2 with each pyramid level increase.",
  "D. Failed experiments": "In this section, we provide a list of ideas which lookedpromising inutitively, but didnt work out at the end ei-ther because of some fundamental fallacies related to them,or the lack of experimentation and limited amount of time toexplore them, or because of some potential implementationbugs which we have not been aware of.",
  ". Random samples from HPDM-L on UCF-101 64 2562 without classifier-free guidance. We display 16 frames from a64-frame-long video with 4 subsampling": "1. Cached inference has not sped up inference as much aswe expected. As described in Sec. 4.5 and Appendix C,we cache the activations from previous pyramid levelswhen sampling its higher stages. However, the speed-upwas just 40%, which was not decisive. One issue is thatwe do not cache some activations (tokenizer activationsand contexts). But the other reason is that grid-samplingis expensive. Grid sampling could be avoided by upsam-pling and then slicing, but this would lead to additionalmemory usage and will complicate the inference code.",
  "former blocks, but that led to inferior results": "5. Cheap high-res + expensive low-res U-Net backbone. U-Nets were also not converging well for us in their regu-lar design and were not giving substantial performanceyields when combined with adaptive computation (only10% during training versus 50% in RINs) due to theirregular amounts of blocks per resolution in their de-sign. 6. Random pyramid cuts.Another strategy to make thelater pyramid stages cheaper during training was to com-pute them only once in a while. For this, we would ran-domly sample the amount of pyramid stages for eachmini batch per GPU. When parallelizing across many",
  ". Full architecture illustration of HPDMwith depiction of the blocks": ". Hyperparameters for different variations of HPDM. For all the models, we used almost the same amount hyperparameters. ForHPDM-T2V, we used joint video + image training which is reflected by its batch size. For HPDM-T2Vand HPDM-T2V-1K, we also usedlow-res pre-training by first training the lowest pyramid stage on 36 64-resolution videos for 500k steps.",
  "HyperparameterHPDM-SHPDM-MHPDM-LHPDM-T2VHPDM-T2V-1K": "Conditioning informationclass labelsclass labelsclass labelsT5-11B embeddingsT5-11B embeddingsConditioning dropout probability0.10.10.10.10.1Tokenization dim10241024102410241024Tokenizer resolution1 4 41 4 41 4 41 3 41 3 4Latent dim256512102430723072Number of latents768768768768768Batch size7687687684096 + 40961024 + 1024Target LR0.0050.0050.0050.0050.005Weight decay0.010.010.010.010.01Number of warm-up steps10k10k10k5k5kParallelization strategyDDPDDPDDPFSDPFSDPStarting resolution16 64 6416 64 6416 64 648 36 6416 72 128Target resolution64 256 25664 256 25664 256 25664 288 51216 576 1024Patch resolution16 64 6416 64 6416 64 648 36 6416 72 128Number of RIN blocks 66666Number of pyramid levels33344Number of pyramid levels per block1/1/2/2/3/31/1/2/2/3/31/1/2/2/3/31/2/2/3/3/44/4/4/4/4/4",
  "step forward in the field. While our model exhibits promis-ing capabilities, its essential to consider its potential nega-tive societal impacts:": "Misinformation and Deepfakes. While our text-to-videomodel underperforms compared to the largest existingones (.e.g, ), it demonstrates a promising directionon how to improve the existing generators further, whichcreates a risk of generative AI misuse in creating mislead-ing videos or deepfakes. This can contribute to the spreadof misinformation or be used for malicious purposes. Intellectual Property Concerns. The ability to generatevideos can lead to challenges in copyright and intellec-tual property rights, especially if the technology is usedto replicate or modify existing copyrighted content with-out permission.",
  "Continuously work on improving the model to reduce bi-ases and ensure fair representation": "Collaborate with legal and ethical experts to understandand navigate the implications of video synthesis technol-ogy in terms of intellectual property rights. Engage withstakeholders from various sectors to assess and mitigateany economic impacts, particularly concerning job dis-placement.In conclusion, while our model represents a notable ad-vancement in video generation technology, it is imperativeto approach its deployment and application with a balancedperspective, considering both its benefits and potential soci-etal implications."
}