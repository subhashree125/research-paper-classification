{
  "Abstract": "The rising demand for computational photography onmobile devices drives the development of advanced imagesensors and algorithms for camera systems. However, thelack of opportunities for in-depth exchange between indus-try and academia is constraining the development of MobileIntelligent Photography and Imaging (MIPI). Building onthe successes of the prior MIPI Workshops at ECCV 2022and CVPR 2023, we are pleased to introduce our third MIPIchallenge in conjunction with CVPR 2024, which includesthree tracks focusing on novel image sensors and imagingalgorithms. In this paper, we summarize and review theDemosaic for the HybridEVS Camera track on MIPI 2024.A total of 110 participants from both industrial and aca-demic backgrounds contributed many valuable solutions toaddress the difficulty of the restoration of HybridEVSs rawdata, thus raising the reconstructed performance to a newheight. This paper gives a comprehensive description andanalysis of all solutions developed during this challenge.More detailed information about this challenge is availableat",
  ". Introduction": "Quad Bayer Color Filter Array (CFA) is a popular CFA pat-tern widely used in smartphone cameras such as the GalaxyS20 FE and Redmi Note8 Pro ( (a)). Quad BayerCFA differs from the traditional Bayer CFA by using 22cells of identical color filters. By utilizing demosaic tech-nics, it can acquire high-resolution images with good imagequality. Moreover, this design ensures exceptional low-light (a)(b) Event pixel RGB",
  ". (a) Quad Bayer pattern, (b) HybridEVS pattern": "performance through a 22 binning operation. But in exist-ing ISP-related research, exploration of Quad Bayer CFA isvery limited, with most pipelines concentrating on BayerCFA . Event Vision Sensors (EVS) determine,at the pixel level, whether a temporal contrast change be-yond a predefined threshold is detected . Comparedto CMOS image sensors (CIS), this new modality inherentlyprovides data-compression functionality and hence, enableshigh-speed, low-latency data capture while operating at lowpower. EVS has tremendous application potential in objecttracking, 3D detection, and slow-motion.Hybrid Event-based Vision Sensor (HybridEVS) isa novel hybrid sensor formed by combining Quad BayerCFA with Event-based Vision techniques. As shown in Fig-ure 1 (b), within the 4x4 block, two event pixels are usedto capture event signals, while the remaining pixels are uti-lized to obtain color information. Owing to the inability ofevent pixels to capture color and texture information, de-mosaicing tasks become more challenging for HybridEVS.Moreover, due to pixel flaws caused by the sensors man-",
  "Demosaic": ". The Demosaic for the HybridEVS Camera aims to reconstruct HybridEVS data into a high-quality RGB result with the sameresolution. This process involves passing the data through a demosaic module, which corrects defects and event pixels and reconstructs athree-channel RGB image of matching resolution. ufacturing process, defect pixels may occasionally arise,characterized by significantly divergent pixel values fromthose of unaffected pixels.Given the presence of event pixels and defect pixels, theDemosaic for HybridEVS Camera has become increasinglychallenging, with very limited related academic researchavailable. Therefore, we are organizing this competitionwith the overarching aim of cultivating innovative solutionsto elevate the related research level of this task to a newheight. We hold this challenge in conjunction with the thirdMIPI Challenge which will be held on CVPR@2024. Simi-lar to the previous MIPI challenge , we areseeking an efficient and high-performance image restora-tion algorithm to handle the HybridEVS camera demosaictask. MIPI 2024 mainly consists of three competition tracksfocusing on the following tasks: Few-shot RAW Image Denoising is geared towardstraining neural networks for raw image denoising in thescenarios where paired data is limited.",
  "As illustrated in , the Demosaic for HybridEVSCamera is dedicated to reconstructing the HybridEVS inputdata to a promising RGB result. Due to manufacturing de-": "fects, HybridEVS data may contain defect pixels whose ac-tual values deviate significantly from the ideal values. Ad-ditionally, the presence of event pixels, essential for captur-ing motion information, poses challenges to the reconstruc-tion task. Given a HybridEVS input Iin RHW , a de-mosaic method F aims to reconstruct Iin into a RGB resultIout RHW 3. We define the reconstruction task usingthe following formula:",
  ". Datasets": "As shown in , the training dataset consists of 800pairs of HybridEVSs input data and label results with a res-olution of 2K. Both the input and label have the same spatialresolution. The input is of 10 bits in the .bin format, whilethe output is of 8 bits in the .png format. The validation andtesting set has 50 scenes each.During the testing phase, in order to achieve a more accu-rate and comprehensive evaluation, the challenge employeda testing dataset comprising both simulated and real-world",
  ". Challenge Results": "Among the 108 registered participants, 7 teams successfullysubmitted their results, code, and factsheets in the final testphase. In order to ensure fairness in the competition, wehave decided to exclude public datasets such as div2k fromthe final testing rankings and instead utilize remaining non-public datasets for the final ranking. reports the finaltest results and rankings of the teams.Finally, the USTC604 team clinched the first place inthis challenge, followed by the lolers team in second place,and the Lumos Demosaicker team in third place. The over-all performance of all participating teams solutions con-sistently exceeds 40 dB on the testing dataset, indicatingthat all participating teams can achieve relatively good re-construction results.The PSNR of the first-place model . Results of MIPI 2024 challenge on the Demosaic forHybridEVS Camera. PSNR and SSIM are computed between thetest results and ground truth. To ensure fairness in the competi-tion, publicly available datasets such as DIV2K are excluded. Therunning time of input of 1080 1920 was measured. The mea-surement was taken on an NVIDIA Geforce GTX 1660Ti.",
  ". Methods": "USTC604This team proposes a coarse-to-fine frameworknamed DemosaicFormer which comprises a coarse demo-saicing network and a pixel correction network (see Fig-ure 5).For the coarse demosaicing stage, to produce a preliminary high-quality estimate of the RGB image fromthe HybridEVS raw data, this team introduces RecursiveResidual Group (RRG) which employs multiple DualAttention Blocks (DABs) to refine the feature representa-tion progressively.For the pixel correction stage, aim-ing to enhance the performance of image restoration andmitigate the impact of defective pixels, this team intro-duces the Transformer Block which consists of Multi-Dconv Head Transposed Attention (MDTA) and Gated-Dconv Feed-Forward Network (GDFN). The key innova-tion is the design of a novel Multi-Scale Gating Module(MSGM) applying the integration of cross-scale features in-spired by , which allows feature information to flow be-tween different scales. Due to the inability to accuratelymodel defective pixels, inspired by , this team extractedthe defect pixels map from the training data of the chal-lenge to generate more diverse and realistic inputs for dataaugmentation.During the training phase, this team ran-domly rotated and flipped ground-truth images of trainingsplit, then sampled them according to the HybridEVS pat-tern, and randomly covered the sampled images with a de-fect pixels map. The augmentation technology is appliedat the initial training of the proposed approach for improv-ing the models generalization and robustness. The trainingphase of the proposed method is divided into two stages:(1) Initial training of DemosaicFormer. This team useda progressive training strategy at first. Start training withpatch size 80 80 and batch size 84 for 58K itera-tions. The patch size and batch size pairs are updated to[(1282, 30), (1602, 18), (1922, 12)] at iterations [ 36K, 24K,24K]. The initial learning rate is 5 104 and remains un-changed when the patch size is 80. Later the learning ratechanges with the Cosine Annealing scheme to 1 107.The best model at this stage is used as the initialization ofthe second stage.(2) Fine-tuning DemosaicFormer. This team starts train-ing with patch size 192 192 and batch size 12. The initiallearning rate is 1104 and changes with Cosine Annealingscheme to 1 107, including 20K iterations in total. Notethat use the entire training data from the challenge withoutany data augmentation technologies at this stage. Exponen-tial Moving Average (EMA) is applied for the dynamic ad-justment of model parameters. lolersThis team proposed a SwinIR-based multi-branchnetwork for HybridEVS demosaicing (see ). Thebackbone uses an enhanced SwinIR to extract rich fea-tures with long-range dependencies and pass them to theUnPack branch and Pack branch, which are used to indicatethe spatial position of full resolution and provide uniformsampling, respectively. In addition, the authors propose alightweight Cascaded Maxout Block (CMBlk), which con-sists of a depthwise convolution and multiple consecutive",
  ". The multi-branch network architecture proposed by lol-ers team": "Maxouts, to give the model a powerful representation witha small number of parameters.The UnPack and Packbranches are composed of the same number of CMBlk.For a defected QCFA raw, the authors first separate theminto 4 single-channel inputs I, and use a defect pixel mask(DP mask) to perform defect pixel correction (DPC).",
  "I = CMBlk(I) DP mask + I": "After passing the DPC module, the 4-channel input I ispacked and unpacked respectively, restored to QCFA rawand 16-channel sub-images, and then input to the backboneand two branches.For the backbone network, it uses aSwinIR with a depth of 6 and follows 3 CMBlk after eachRSTB to improve its feature extraction capability. Accord-ingly, the features of each layer are passed separately intotwo lightweight branches for feature enhancement. Finally,the three branches are fused to infer the final result.In the training stage, the authors use the Adam optimizer,and the initial learning rate is set to 2e-4. First, they trainedthe DPC module and backbone by L1 loss. When the net-work is fitted, they fix the backbones parameters and addtwo branches to continue training. After 50w iterations, theparameters of all three branches are updated in an end-to-end manner.",
  "Lumos DemosaickerThis team introduces a novel two-stage network, termed the Two-Stage joint Inpainting andDemosaicing Network (TSIDN), as depicted in": "Initially, the network addresses the influence of event pointsby employing an inpainting process, which replaces themwith the average values of neighboring pixels.Subse-quently, the primary task is segmented into two stages, facil-itating independent and joint training for each sub-network.The first stage features a Quad-to-Quad (Q2Q) network,which takes inpainted Quad Bayer data and event pixelmaps as input. It utilizes a NAFNet to effectively re-store both event and defect pixels, integrating positional in-formation to enhance restoration accuracy. Building uponthis foundation, the second stage employs a Quad-to-RGB(Q2R) network based on SCUNet to focus on demo-saicing.This network benefits from Swin-Conv and U-Net structures, ensuring efficient demosaicing performance.During the training phases, strategies such as phase-basedtraining and progressive learning are incorporated to en-hance network performance. In the joint training stage, aprogressive learning strategy is employed, starting with apatch size of 256 pixels, which progressively increases to382 and then to 500 pixels. l1 loss is utilized during thepretraining stage, while PSNR loss is applied during jointtraining. The initial learning rate is set at 1 104 and isgradually decreased to 1 107, contributing to the robust-ness and efficiency of the proposed network. Quad bayer Input DemosaicSR Model (2X) Post-processingOutput L1 LossL1 LossL1 LossL1 Loss",
  ".Step-by-step Demosaic model for Hybrid Evs Cam-era(SBSDM)": "High-speed MachinesThe Demosaic for HybridEVSCamera requires a 4 scale expansion, which is quite chal-lenging for the model (see ). Therefore, a largenumber of parameters are needed for the model to learn,which in turn requires a substantial amount of data for train-ing. However, the amount of training data provided by thecompetition is quite low. To address this difficulty, this teamproposes the Step-by-step Demosaic model for HybridEVSCamera (SBSDM).This teams model, SBSDM, is a two-stage model. Thefirst stage involves transforming the Quad Bayer input into a2 RGB output image. In the data preprocessing part, sincethe PD point locations do not contain valid pixel informa-tion, this team has removed this information from the inputand decomposed the original image into 14 channels. Inthe model, this team adopted the SPAN model and ex-panded the channels to 96. The second stage involves a 2super-resolution. Since there are many public models forsuper-resolution tasks, this team can conveniently use mod-",
  "Split Windows": ". Overview of the proposed demosaicing method for eventcameras: The process begins with preprocessing the RAW image,followed by feature extraction via an encoder using Swin Trans-former blocks and a Shifted Window mechanism. The decoder,mirroring the encoder and including skip connections, reconstructsspatial details. The final stage is image reconstruction to producethe RGB output. Illustrated components include (a) the encoderarchitecture, (b) the Shifted Window mechanism for enhanced in-teraction, and (c) the decoder architecture. els pre-trained on large datasets as the second-stage model.Here, this team used EDSR , HAT , and SwinIR to construct different models. Finally, this team trained theentire model end-to-end using the L1 loss function. YunfanThis team has devised a comprehensive methodfor demosaicing images from event cameras by leveraginga sophisticated framework that combines the Swin Trans-former and U-Net architectures, as shown in Fig-ure 9. They have methodically outlined a multi-faceted ap-proach consisting of preprocessing, encoding, decoding, re-construction, and a novel loss function, each contributinguniquely to the image reconstruction process.In the preprocessing phase, the team effectively trans-forms the input RAW image and reduces computationalcomplexity using space-to-depth operations and 1 1convolutions. The encoding stage, utilizing Swin Trans-former blocks, extracts multi-scale features and captureslong-range dependencies, while the decoding phase mir-rors the encoding structure, progressively recovering theimages spatial resolution. The teams reconstruction mod-ule is adept at generating the final RGB image from upsam-pled features by reversing the preprocessing operations andrefining the high-dimensional features into a standard colorspace.Notably, the teams strategic innovation lies in their two-stage training methodology that employs a Charbonnier lossfor initial training and a Pixel Focus Loss for fine-tuning.They have meticulously engineered the Pixel Focus Lossto address long-tail distribution issues in training loss, fo-",
  ". (b) The architecture of the adopted GRL layer. (c) Thekey anchored stripe attention mechanism": "cusing on edge detail enhancement. This loss function isinstrumental in the models ability to distinguish betweenhigh-frequency edge information and low-frequency colorblock differences, enhancing fine details restoration.Through this systematic approach, the team has strength-ened the networks capability to learn global color distri-butions and local edge details, culminating in high-qualityRGB image reconstruction. Their experiments demonstratethe efficacy of their method. HIT-CVLABThis team designs a UNet structure net-work based on the GRL layer (see ), whichexplicitly models image hierarchies at global, regional, andlocal scales through anchored strip self-attention, windowself-attention, and channel attention-augmented convolu-tions.For training, the authors adopt mini-batch stochastic gra-dient descent (SGD) with a batch size of 64 for 600 epochs.The initial value of the learning rate is 3e-4 and graduallydecreases to 1e-6 with a CosineAnnealing schedule. Thislearning rate decreasing strategy helps the model adjust pa-rameters more carefully when it is close to convergence,thereby improving the generalization performance of themodel and being able to better cope with Noise and uncer-tainty in training data. Training loss functions include L1loss, L2 loss, and Sobel loss. CougerAIThe demosaicing model begins with the initialstep of converting the input image from its raw Bayer pat-tern (RGB) to the standard RGB color space using a Bay-erRG2BGR method (see ). The resulting RGBimage is then fused with the raw input image and jointlyprocessed through a novel architecture inspired by recentadvancements in computer vision.This joint processing involves passing the concatenatedimages through a Self-Calibrated convolution with a pixelattention block (SCPA), which enhances the spa-tial and spectral coherence of the input. Simultaneously,the raw input image is fed into a sophisticated denoisingblock to produce a three-channel denoised image. Thedenoising block is composed of four inverse convolutionallayers followed by an attention mechanism that effectively",
  ". Multi-Stage Fusion Demosaicing architecture proposedby CougerAI team": "suppresses noise while preserving important image details.After denoising, the denoised image and the output ofthe SCPA block are combined with the converted RGB im-age and fed through a downsampling layer to reduce com-putational complexity and enhance feature extraction. Thedownsampled features are then input to a residual learn-ing block, which consists of a series of residual groupblocks followed by a residual channel dense attentionblock. This architecture allows the model to effectively cap-ture both local and global contextual information, improv-ing its ability to reconstruct the image.Finally, the output of the residual learning block is addedback to the input image and upsampled to produce the finaldemosaiced image.",
  "Marcos V Conde, Florin Vasluianu, Sabari Nathan, and RaduTimofte. Real-time under-display cameras image restorationand hdr on mobile devices. In Computer VisionECCV 2022Workshops. Springer, 2022. 6": "Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng,Qingpeng Zhu, Qianhui Sun, Wenxiu Sun, Chen ChangeLoy, Jinwei Gu, Shuai Liu, et al. Mipi 2023 challenge onnighttime flare removal: Methods and results. In IEEE Con-ference on Computer Vision and Pattern Recognition, pages28522862, 2023. 2 Guillermo Gallego, Tobi Delbruck, Garrick Orchard, ChiaraBartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger,Andrew J Davison, Jorg Conradt, Kostas Daniilidis, et al.Event-based vision: A survey. IEEE Transactions on PatternAnalysis and Machine Intelligence, 44(1):154180, 2020. 1 Biay-Cheng Hseih,Hasib Siddiqui,Jiafu Luo,TodorGeorgiev, Kalin Atanassov, Sergio Goma, HY Cheng, JJ Sze,RJ Lin, KY Chou, et al. New color filter patterns and de-mosaic for sub-micron pixel arrays. In Proceedings of theInternational Image Sensor Workshop, pages 811, 2015. 1 Pei-Hsiang Hsu, Pei-Jun Lee, Trong-An Bui, and Yi-ShauChou. Yolo-spd: Tiny objects localization on remote sensingbased on you only look once and space-to-depth convolu-tion. In 2024 IEEE International Conference on ConsumerElectronics (ICCE), pages 13. IEEE, 2024. 5",
  "Yongnam Kim and Yunkyung Kim. High-sensitivity pixelswith a quad-wrgb color filter and spatial deep-trench isola-tion. Sensors, 19(21):4653, 2019. 1": "Kazutoshi Kodama, Yusuke Sato, Yuhi Yorikado, RaphaelBerner, Kyoji Mizoguchi, Takahiro Miyazaki, MasahiroTsukamoto, Yoshihisa Matoba, Hirotaka Shinozaki, AtsumiNiwa, et al. 1.22 m 35.6 mpixel rgb hybrid event-basedvision sensor with 4.88 m-pitch event pixels and up to10k event frame rate by adaptive control on event sparsity.In 2023 IEEE International Solid-State Circuits Conference(ISSCC), pages 9294. IEEE, 2023. 1 Yawei Li, Yuchen Fan, Xiaoyu Xiang, Denis Demandolx,Rakesh Ranjan, Radu Timofte, and Luc Van Gool.Effi-cient and explicit modelling of image hierarchies for imagerestoration. In IEEE Conference on Computer Vision andPattern Recognition, pages 1827818289, 2023. 6 Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, LucVan Gool, and Radu Timofte.SwinIR: Image restorationusing swin transformer. In IEEE International Conferenceon Computer Vision, pages 18331844, 2021. 4, 5 Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, andKyoung Mu Lee. Enhanced deep residual networks for singleimage super-resolution. In The IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR) Workshops,2017. 5",
  "Nahian Siddique, Sidike Paheding, Colin P Elkin, and Vi-jay Devabhaktuni. U-net and its variants for medical imagesegmentation: A review of theory and applications. IEEEACCESS, 9:8203182057, 2021. 5": "B Son, Y Suh, S Kim, H Jung, JS Kim, C Shin, K Park,K Lee, J Park, J Woo, et al. A 640 480 dynamic visionsensor with a 9 um pixel and 300 meps address-event repre-sentation. In IEEE International Conference on Solid-StateCircuits, San Francisco, CA, USA, pages 59, 2017. 1 Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou,Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu,Chen Change Loy, Jinwei Gu, et al. Mipi 2023 challenge onrgbw remosaic: Methods and results. In IEEE Conferenceon Computer Vision and Pattern Recognition, pages 28772884, 2023. 2 Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou,Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu,Chen Change Loy, Jinwei Gu, et al. Mipi 2023 challengeon rgbw fusion: Methods and results. In IEEE Conferenceon Computer Vision and Pattern Recognition, pages 28702876, 2023. 2 Cheng Wan, Hongyuan Yu, Zhiqi Li, Yihang Chen, Ya-jun Zou, Yuqing Liu, Xuanwu Yin, and Kunlong Zuo.Swift parameter-free attention network for efficient super-resolution. arXiv preprint arXiv:2311.12770, 2023. 5",
  "K Yonemoto. Principles and applications of ccd/cmos imagesensors, 2003. 1": "Syed Waqas Zamir, Aditya Arora, Salman Khan, MunawarHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and LingShao. Cycleisp: Real image restoration via improved datasynthesis. In IEEE Conference on Computer Vision and Pat-tern Recognition, pages 26962705, 2020. 4 Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.Restormer: Efficient transformer for high-resolution imagerestoration. In IEEE Conference on Computer Vision andPattern Recognition, pages 57285739, 2022. 4 Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yu-lun Zhang, Hao Tang, Deng-Ping Fan, Radu Timofte, andLuc Van Gool. Practical blind image denoising via swin-conv-unet and data synthesis.Machine Intelligence Re-search, 20(6):822836, 2023. 5 QingpengZhu,WenxiuSun,YuekunDai,ChongyiLi,Shangchen Zhou,Ruicheng Feng,Qianhui Sun,Chen Change Loy, Jinwei Gu, Yi Yu, et al. Mipi 2023 chal-lenge on rgb+ tof depth completion: Methods and results. InIEEE Conference on Computer Vision and Pattern Recogni-tion, pages 28632869, 2023. 2"
}