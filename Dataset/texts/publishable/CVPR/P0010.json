{
  "Yulin Wang, Haoji Zhang, Yang Yue, Shiji Song, Senior Member, IEEE,Chao Deng, Junlan Feng, Fellow, IEEE, and Gao Huang , Member, IEEE": "AbstractThis paper presents a comprehensive exploration of the phenomenon of data redundancy in video understanding, with theaim to improve computational efficiency. Our investigation commences with an examination of spatial redundancy, which refers to theobservation that the most informative region in each video frame usually corresponds to a small image patch, whose shape, size andlocation shift smoothly across frames. Motivated by this phenomenon, we formulate the patch localization problem as a dynamic decisiontask, and introduce a spatially adaptive video recognition approach, termed AdaFocus. In specific, a lightweight encoder is first employedto quickly process the full video sequence, whose features are then utilized by a policy network to identify the most task-relevant regions.Subsequently, the selected patches are inferred by a high-capacity deep network for the final prediction. The complete model can betrained conveniently in an end-to-end manner. During inference, once the informative patch sequence has been generated, the bulk ofcomputation can be executed in parallel, rendering it efficient on modern GPU devices. Furthermore, we demonstrate that AdaFocus canbe easily extended by further considering the temporal and sample-wise redundancies, i.e., allocating the majority of computation to themost task-relevant video frames, and minimizing the computation spent on relatively easier videos. Our resulting algorithm,Uni-AdaFocus, establishes a comprehensive framework that seamlessly integrates spatial, temporal, and sample-wise dynamiccomputation, while it preserves the merits of AdaFocus in terms of efficient end-to-end training and hardware friendliness. In addition,Uni-AdaFocus is general and flexible as it is compatible with off-the-shelf backbone models (e.g., TSM and X3D), which can be readilydeployed as our feature extractor, yielding a significantly improved computational efficiency. Empirically, extensive experiments based onseven widely-used benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, Jester, and Kinetics-400)and three real-world application scenarios (i.e., fine-grained diving action classification, Alzheimers and Parkinsons diseases diagnosiswith brain magnetic resonance images (MRI), and violence recognition for online videos) substantiate that Uni-AdaFocus is considerablymore efficient than the competitive baselines. Code & pre-trained models are available at and",
  "Index TermsDynamic Neural Networks, Efficient Deep Learning, Video Recognition": "1INTRODUCTIONThe proliferation of online videos, exemplified by the plat-forms such as YouTube and TikTok, has necessitated thedevelopment of automated methods for identifying humanactions, events, and other elements within them. This is cru-cial for facilitating the applications such as recommendation, , , surveillance , , and content-based searching. In recent years, remarkable success in accurate videorecognition has been achieved by leveraging deep networks, , , , , . However, the noteworthy perfor-mance of these models usually comes at the price of highcomputational costs. In real-world scenarios, computationdirectly translates into power consumption, carbon emissionand practical latency, which should be minimized undereconomic, environmental or safety considerations.To address this issue, a number of recent works propose toreduce the inherent temporal redundancy in video recognition, , , , , , . As illustrated in Y. Wang, Y. Yue, S. Song, and G. Huang are with the Department ofAutomation, BNRist, Tsinghua University, Beijing, China. G. Huang isalso with Beijing Academy of Artificial Intelligence, Beijing, China. H.Zhang is with the Shenzhen International Graduate School, TsinghuaUniversity, Guangdong, China. C. Deng and J. Feng are with the ChinaMobile Research Institute, Beijing, China.",
  "Equal contribution.Corresponding author": "(b), it is efficient to concentrate on the most task-relevantvideo frames, and allocate the majority of computation tothem rather than all frames. Nevertheless, another importantsource of redundant computation in image-based data,specifically spatial redundancy, has rarely been explored inthe realm of efficient video recognition. In fact, it has beenshown in 2D-image classification that deep networks (e.g.,ConvNets or vision Transformers) are able to produce correctpredictions by examining only a few discriminative regionsinstead of the entire images , , , , , .By performing inference on these relatively small regions,one can dramatically reduce the computational cost of visualbackbones (e.g., processing a 96x96 patch requires 18%computation of inferring a 224x224 image).In this paper, we are interested in whether this spatialredundancy can be effectively leveraged to facilitate efficientvideo recognition. We start by introducing a novel adaptivefocus (AdaFocus) approach to dynamically localize andattend to the task-relevant regions of each frame. In specific,our method first takes a quick glance at each frame with alightweight deep model to acquire cheap and coarse globalinformation. Then we train a policy network on its basisto select the most valuable region for recognition. Thisprocedure leverages the reinforcement learning algorithmdue to the non-differentiability of localizing task-relevantregions. Finally, we activate a high-capacity and accurate 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, includingreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse ofanycopyrighted component of this work in other works.",
  "Frame Sampling": ". Comparisons between existing temporal-based methodsand our proposed approaches. Most existing works aim to reducecomputational costs by selecting a few informative frames to process.Orthogonal to them, AdaFocus reveals that a superior computationalefficiency can be achieved by reducing the spatial redundancy. Built uponthis finding, we further demonstrate that it is feasible to formulate flexibleand highly efficient spatial-temporal dynamic computation simultaneouslyin a unified framework (Uni-AdaFocus), i.e., attending to the most valuablespatial regions of the most task-relevant video frames. local encoder to process only the selected regions. Since theproposed regions are usually small patches with a reducedsize, considerable computational costs can be saved. Anillustration of AdaFocus can be found in (c). Ourmethod allocates computation unevenly across the spatialdimension of video frames according to the contributions tothe recognition task, leading to significant improvements inefficiency with a preserved accuracy.On top of the vanilla AdaFocus framework, we delvedeep into the optimal design of efficient spatial dynamiccomputation algorithms, and further improve AdaFocus inseveral important aspects. Firstly, we simplify the trainingof AdaFocus by reformulating it as an end-to-end algorithm,eliminating the need for the complicated three-stage trainingprocedure with reinforcement learning. This yields reducedtraining cost, improved test accuracy, and greater accessibilityfor practitioners. Secondly, we present a discussion on howto introduce appropriate supervision signals for learningto select task-relevant regions, and propose a deep-feature-based approach for training more effective region selectionpolicies. Lastly, we propose a deformable patch mechanismthat enables AdaFocus to adapt flexibly to the task-relevantregions in various scales, shapes, and locations.It is worth noting that the basic formulation of AdaFocusdoes not account for the temporal-wise and sample-wiseredundancies, meaning that the computation is uniformlyallocated along the temporal dimension and across differentvideos. Therefore, our method is compatible with the ideas oftemporal-adaptive and sample-adaptive dynamic inference.For instance, it can be extended by concentrating computa-tional resources on the most informative video frames andby decreasing the computation spent on relatively easiersamples. In this paper, we demonstrate that these goalscan be attained by introducing a dynamic frame samplingalgorithm as well as a conditional-exit mechanism.Incorporating the aforementioned methodology inno- vations, we present unified AdaFocus (Uni-AdaFocus, see (d)), a holistic framework that seamlessly integratesspatial, temporal, and sample-wise dynamic computation.Importantly, it is compatible with a wide range of off-the-shelf backbone models (e.g., TSM and X3D ), whichcan be conveniently deployed as the feature extractor inUni-AdaFocus for improving their computational efficiency.Furthermore, the inference cost of Uni-AdaFocus can beadjusted online without additional training (by modifyingthe criterion for sample-conditional computation). This adapt-ability enables it to fully utilize fluctuating computationalresources or achieve the desired level of performance flexiblywith minimal power consumption, both of which are thepractical demands of numerous real-world applications, suchas search engines and mobile applications.Empirically, the effectiveness of Uni-AdaFocus is vali-dated based on seven widely-used benchmark datasets andthree real-world application scenarios. Extensive experimentsdemonstrate that Uni-AdaFocus consistently outperformsthe competitive baselines by significant margins, attaining anew state-of-the-art performance in terms of both theoreticalcomputational efficiency and practical inference speed.This paper extends previous conference papers that intro-duced the basic AdaFocus framework and preliminarilydiscussed its end-to-end training . Moreover, our deep-feature-based approach for training the patch selection policy(.1.1) is conceptually relevant to, and improvedupon (see for a detailed comparison). Wehave improved these earlier works substantially in severalimportant aspects, which are summarized in Appendix A.",
  "RELATED WORKS": "Video recognition. Convolutional networks (ConvNets) havemade a noteworthy impact on the field of automatic videorecognition, demonstrating exceptional accuracy on large-scale benchmarks , , , , . The methodsemployed within this field can be broadly categorized intoseveral distinct approaches. One such method entails theconcurrent capture of spatial and temporal informationthrough the use of 3D convolution, as demonstrated bythe works such as C3D , I3D , ResNet3D ,X3D , etc. An alternative technique involves the initialextraction of frame-wise features, followed by temporal-wiseaggregation using specialized architectures. This approachcan be seen in studies utilizing temporal averaging ,recurrent networks , , , and temporal channel shift, , . A third category of work employs two-streamarchitectures to model short-term and long-term temporalrelationships, as seen in , , , . More recently,driven by the success of vision Transformers (ViTs) , aconsiderable number of works focus on facilitating effectivevideo understanding with self-attention-based models ,, , . Notwithstanding the accomplishments of theaforementioned studies works, the expensive computationalcost of deep networks, particularly 3D-ConvNets, usuallyconstrain their practical application. Recent research effortshave been made towards improving the efficiency of videorecognition , , , , , .Temporal redundancy. A prominent strategy for facilitat-ing efficient video recognition entails the minimization of the",
  "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION3": "temporal redundancy within videos , , , , ,, , , , , , . Since not all frames areequally important for a given task, the model should ideallyallocate fewer computational resources towards less infor-mative frames . Several effective algorithms have beenproposed along this direction. For example, LiteEval adaptively selects an LSTM model with appropriate size ateach time step in a recurrent recognition procedure. Adaptiveresolution network (AR-Net) processes different frameswith adaptive resolutions to save unnecessary computationon less important frames. VideoIQ processes videoframes using different precision according to their relativeimportance. FrameExit learns to conclude the inferenceprocess after seeing a few sufficiently informative frames.Compared to these approaches, the contributions of thispaper lie in that 1) we develop the methodologies of reducingspatial redundancy (AdaFocus), that is, to concentrate majorcomputation on the task-relevant regions of video frames; 2)we demonstrate that AdaFocus is compatible with the spiritof reducing temporal redundancy by proposing a dynamicframe sampling algorithm tailored for our method; 3) weintegrate the spatial, temporal, and sample-wise dynamiccomputation into a Uni-AdaFocus framework, yielding state-of-the-art computational efficiency.In particular, OCSampler proposes a novel andeffective framework that learns to select task-relevant frameswith reinforcement learning. Our work is related to in thebasic paradigm of formulating frame selection as a sequentialweighted sampling problem without replacement, wherethe distribution is dynamically parameterized conditionedon each video utilizing a policy network. However, wedevelop novel theoretical analyses, which directly considerthe expected loss of this problem as an optimization objective,and reveal that it can be decomposed into a differentiableform solved by the Monte Carlo method, yielding an efficientend-to-end trainable algorithm (.2). Compared to, our method does not rely on reinforcement learningor multi-stage training, considerably reduces both the theo-retical complexity and the practical training wall-time, yetsignificantly improves the performance ().Spatial-wise dynamic networks perform computationadaptively on top of different spatial locations of the inputs, . The AdaFocus network studied in this papercan be classified into this category as well. Many of thespatially adaptive networks are designed from the lens ofinference efficiency , , , , . For example,recent investigations have revealed that 2D images can beefficiently processed via attending to the task-relevant ormore information-rich image regions , , , .In the realm of video understanding, the exploitation ofspatial redundancy as a means to reduce computational costremains a relatively unexplored area. It has been shown bythe attention-based methods , that the contributionsof different frame regions to the recognition task are notequivalent. Some preliminary studies , have begunto underscore the potential benefits of this approach.The spatial transformer networks are trained basedon an interpolation-based mechanism, which is similar to thedifferentiable patch selection technique in AdaFocus. How-ever, they focus on actively transforming the feature mapsfor learning spatially invariant representations, whereas our objective is to localize and attend to the task-relevantregions of the video frames for improving the computationalefficiency. Moreover, we demonstrate that a straightforwardimplementation of this mechanism fails to yield compet-itive results in our problem. To address the optimizationchallenges, our algorithm necessitates the introduction ofimproved designs, as discussed in this paper.",
  "Hidden StateCConcatenation": ". Overview of AdaFocus. It first takes a quick glance at eachframe vt using a lightweight global encoder fG. Then a policy network is built on top of fG to select the most important image region vt in termsof recognition. A high-capacity local encoder fL is adopted to extractfeatures from vt. Finally, a classifier aggregates the features acrossframes to obtain the prediction pt.",
  "ADAPTIVE FOCUS NETWORK (ADAFOCUS)": "Different from most existing works that facilitate efficientvideo recognition by leveraging the temporal redundancy, weseek to save the computation spent on the task-irrelevantregions of video frames, and thus improve the efficiencyby reducing the spatial redundancy. To attain this goal, wepropose an AdaFocus framework to adaptively identify andattend to the most informative regions of each frame, suchthat the computational cost can be significantly reducedwithout sacrificing accuracy. In this section, we first introducethe basic formulation of AdaFocus and its network architec-ture (.1). Then we show that the straightforwardoptimization problem derived from this basic formulationcan be solved by a reinforcement-learning-based three-stagealgorithm (AdaFocusV1, .2). Built upon these discus-sions, we further establish the feasibility of reformulating thetraining of AdaFocus into an end-to-end algorithm, whichconsistently improves the accuracy with a simpler and moreefficient training process (AdaFocusV2, .3).",
  "Network Architecture": "Overview. We start by giving an overview of AdaFocus(). Without loss of generality, we consider an onlinevideo recognition scenario, where a stream of frames comein sequentially while a prediction may be retrieved after pro-cessing any number of frames. At each time step, AdaFocusfirst takes a quick glance at the full frame with a lightweightdeep network fG, obtaining cheap and coarse global features.Then the features are fed into a policy network to aggregatethe information across frames and accordingly determine",
  "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION4": "the location of an image patch to be focused on, under thegoal of maximizing its contribution to video recognition. Ahigh-capacity local encoder fL is then adopted to processthe selected patch for more accurate but computationallyexpensive representations. Finally, a classifier fC integratesthe features of all previous frames to produce a prediction.In the following, we describe the four components of ourmethod in details.Global encoder fG and local encoder fL are both back-bone networks that extract deep features from the inputs, butwith distinct aims. The former is designed to quickly catch aglimpse of each frame, providing necessary information fordetermining which region the local encoder fL should attendto. Therefore, a lightweight network is adopted for fG. On thecontrary, fL is leveraged to take full advantage of the selectedimage regions for learning discriminative representations,and hence we deploy large and accurate models. Since fLonly needs to process a series of relatively small regionsinstead of the full images, this stage enjoys high efficiencyas well. Importantly, the formulation of fG and fL is generaland flexible, i.e., most state-of-the-art deep learning modelscan be conveniently deployed in AdaFocus to improvetheir computational efficiency for inference. Representativeexamples are given in .Formally, given video frames {v1, v2, . . .} with size HW, fG directly takes them as inputs and produces the coarseglobal feature maps eGt :",
  "eLt = fL(vt),t = 1, 2, . . . ,(2)": "where eLt denotes the fine local feature maps. Importantly,the patch vt is localized to capture the most informativeregions for the given task, and this procedure is fulfilled bythe policy network , which is introduced in the following.Policy network specifies which region the local encoderfL should attend to for each frame, i.e., the locations of{v1, v2, . . .}. This goal is attained by leveraging the coarseglobal features eGt from the global encoder fG. Note that thefeatures of both previous and current frames can be used,and hence should be designed as the architecture capableof encoding temporal information (e.g., via incorporating re-current networks, 3D convolution or self-attention modules).The detailed formulations and training algorithms related to will be further discussed in Sections 3.2 and 3.3.Classifier fC is a prediction network aiming to aggregatethe information from all the frames that have been processedby the model, and output the current recognition result ateach time step. To be specific, we perform global averagepooling on the feature maps eGt , eLt from the two aforemen-tioned encoders to get feature vectors eGt , eLt , and concatenatethem as the inputs of fC, namely",
  "t )": "qW/qh1m1dIAPzrwKBXh70hcWEP2HmI3AeJS0uGOZIQrskWfwGpKROzXyFjS/lnW86PMzvNpyfyxFe+Q+7AD7XAyjOtJ4+nN8+XLycBVSTt5ni+ecS+z1DPcaQ/wvNTpFW3TS0hKzqNqjl9P/HLehUZzPidxgSwjWQ+uVuWT9VNz5Q8pAuJo3MEm+7odhZ5aE/OTi3nDE9vHnYUO+/hcy42ht5CXRx8JzJ1eiLjDnFL8/cbeJK74/lnKObYTEyRI93RXsqEwQs7s769E05KxsV+uvqzsfdiqNRnaLrtMzek4v0LNvqEHvaA8nhI1MfKVv9L3wo3BV+Fn4lUJXVzKbpzTxFP7+A/yJk/4=</latexit>vt . Illustration of the policy network in AdaFocusV1. The outputsof parameterize a categorical distribution (|eG1, . . . , eGt ) on multiplepatch candidates (here we take 25 as an example). During training, wesample vt from (|eG1, . . . , eGt ), while at test time, we directly select thepatch with the largest softmax probability. facilitating efficient feature reuse. This design is natural sinceit has been observed that deep networks (e.g., ConvNets andVision Transformers) excel at learning representations forboth recognition and localization simultaneously , ,. Many existing methods also adopt similar reusing mech-anisms , , , , . In addition, the architectureof fC may have different choices, such as recurrent networks, , averaging the frame-wise predictions , ,, and accumulated feature pooling .",
  "AdaFocusV1: Three-stage Reinforcement Learning": "Patch localization as a sequential discrete decision task.As aforementioned, the policy network localizes the task-relevant patches {v1, v2, . . .}, which are cropped from videoframes and processed by the local encoder fL. However,the cropping operation is inherently non-differentiable. Toaddress this issue, a straightforward approach is to formulate as an agent that makes a series of discrete decisions, suchthat can be trained with reinforcement learning.As a basic assumption, we suppose that the location of thepatch vt is drawn from an action distribution parameterizedby the outputs of :",
  "vt (|eG1 , . . . , eGt ).(4)": "Note that we do not perform any pooling on the featuresmaps eGt since pooling typically corrupts the useful spatialinformation for localizing vt. In our implementation, weconsider multiple candidates (e.g., 36 or 49) uniformlydistributed across the images, and establish a categoricaldistribution on them. At test time, we simply adopt thecandidate with maximum probability as vt for a deterministicinference procedure. An illustration is shown in .Three-stage training. Since the formulation above in-cludes both continuous (i.e., video recognition) and discrete(i.e., patch localization) optimization, the standard end-to-end training paradigm cannot be directly applied. Therefore,we introduce a three-stage training algorithm to solve thecontinuous and discrete optimization problems alternatively,which can be found in Appendix B, due to spatial limitations.",
  "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION5": "However, this procedure is unfriendly for practitioners. First,effectively deploying the reinforcement learning algorithmis nontrivial. It requires considerable efforts for properlydesigning the key components (e.g., the action space andthe reward function), and implementing specialized opti-mization techniques (e.g., deep Q-Network or proximalpolicy optimization ). Second, the three-stage alternativealgorithm is an indirect formulation for optimizing therecognition objective, which tends to be time-consuming,and may result in sub-optimal solutions. Third, the per-formance of AdaFocusV1 largely depends on a number ofimplementation configurations (e.g., performing pre-training,freezing some components in different stages, and stage-wisehyper-parameter searching) that need to be carefully tunedon a per-dataset or per-backbone basis.In the following, we present an end-to-end trainableformulation for AdaFocus to address the issue of ineffi-cient training. The proposed network, AdaFocusV2, canbe conveniently implemented to achieve consistently betterperformance than AdaFocusV1 with reduced training cost.A comparison of AdaFocusV1 and V2 is given in .",
  "],(5)": "where eG1 , . . . , eGt are the global features of 1th tth framesextracted by the global encoder fG. Notably, we refer to thecoordinates of the top-left corner of the frame as (0, 0), andEq. (5) ensures that vt will never go outside of vt. Our aimis to calculate the values of all pixels in vt, while allowingthe gradients to be back-propagated through (xtc, ytc).Feed-forward. We first introduce the feed-forward pro-cess of our method. Formally, the coordinates of a pixel inthe patch vt can be expressed as the addition of (xtc, ytc) anda fixed offset:",
  "original video frame": ". Illustration of interpolation-based patch selection. Thisoperation is differentiable, i.e., the gradients can be directly back-propagated into the policy network through the selected image patchvt. Consequently, integrating the learning of into a unified end-to-endtraining paradigm turns out to be feasible. the ith row and jth column of vt, while oij represents thevector from the patch centre (xtc, ytc) to (xtij, ytij). Given afixed patch size, oij is a constant conditioned only on i, j,regardless of t or the inputs of .Since the values of (xtc, ytc) are continuous, there does notexist a pixel of vt exactly located at (xtij, ytij) to directly getthe pixel value. Alternatively, as illustrated in , wecan always find that the location (xtij, ytij) is surrounded byfour adjacent pixels of vt, forming a grid. The coordinates are(xtij, ytij), (xtij+1, ytij), (xtij, ytij+1) and (xtij+1, ytij+1), respectively, where denotes the rounding-down operation. By assuming that the corresponding pixelvalues of these four pixels are (mtij)00, (mtij)01, (mtij)10, and(mtij)11, the pixel value at (xtij, ytij) (referred to as mtij) canbe obtained via interpolation algorithms. For example, wemay simply adopt the differentiable bilinear interpolation:",
  "t=1 LCE(pt, y),(10)": "where T and y denote the length and the label of the video{v1, v2, . . .}, and pt is the softmax prediction at tth frame.However, importantly, such a straightforward implemen-tation leads to the severely degraded performance (see for experimental evidence). We attribute this issue to theabsence of some appealing optimization properties intro-duced by the three-stage training procedure, namely the lackof supervision, input diversity and training stability. To solvethese problems, we develop simple but effective trainingtechniques, with which end-to-end training can significantlyoutperform the three-stage counterpart. These techniques donot introduce additional tunable hyper-parameters, whileachieving consistent improvements across varying datasets,backbone architectures, and model configurations.Lack of supervision: auxiliary supervision. The effective-ness of three-stage training is largely ensured by a properinitialization. For example, AdaFocusV1 pre-trains the twoencoders (i.e., fG, fL) separately using a direct frame-wiserecognition loss (by simply appending a fully-connectedlayer) . However, when solving problem (10), we do notintroduce such pre-training mechanisms, which hurts theoverall training efficiency of our method. In other words, fGand fL are trained without specialized initialization, whilethey are only indirectly supervised by the gradients from theclassifier fC. To this end, we find that explicitly introducingauxiliary supervision on fG and fL effectively facilitates theefficient end-to-end training of AdaFocus. In specific, weattach two linear classifiers, FCG() and FCL(), to the outputsof fG and fL, and replace the loss function L in (10) by L:",
  "(11)": "where eGt and eLt are the feature vectors after performingglobal average pooling on the feature maps eGt and eLtoutput by fG and fL. Intuitively, minimizing L enforcesthe two encoders to learn linearized deep representations,which has been widely verified as an efficient approach fortraining deep networks , , . This paradigm benefitsthe learning of fC as well, since its inputs are explicitlyregularized to be linearly separable.Lack of input diversity: diversity augmentation. In thestage I for training AdaFocusV1, image patches are randomlycropped, yielding highly diversified inputs for learning well-generalized local encoder fL. However, the patch selectionprocess presented in .3.1 is deterministic. In Eq.(11), given a video frame, the local encoder fL only hasaccess to the patch specified by the policy network .This procedure leads to the limited diversity of trainingdata for the inputs of fL. Empirically, we observe that itresults in the inferior performance of fL. We address thisissue by proposing a straightforward diversity augmentationapproach. For each video, we first compute L by activating as aforementioned. Then we infer fL and the classifier fCfor a second time using randomly cropped patches, obtainingan additional loss Lrandom, which follows Eq. (11) as well.Our final optimization objective combines L and Lrandom:",
  "(L + Lrandom).(12)": "Lack of training stability: stop-gradient. In AdaFocusV1,the policy network is learned on top of the fixed andcompletely trained global encoder fG. When it comes to end-to-end training, and fG are simultaneously updated. In thiscase, we observe that the gradients back-propagated from interfere with the learning of fG, leading to an unstabletraining process with slow convergence speed. We find thatthis problem can be solved by simply stopping the gradientsbefore the inputs of . In other words, we propose to trainfG using the pure classification objective without any effectfrom , as done in AdaFocusV1. This design is rationalsince previous works have revealed that the representationsextracted by deep recognition networks can naturally beleveraged for localizing task-relevant regions , , .",
  "UNIFIED ADAFOCUS (UNI-ADAFOCUS)": "In this section, we further introduce an enhanced UnifiedAdaFocus (Uni-AdaFocus) framework, which improvesAdaFocusV2 in several important aspects. First, the spatialdynamic computation algorithm is refined and advanced(.1): 1) we introduce a deep-feature-interpolation-based technique for the effective training of the policynetwork , which incurs minimal additional training cost, yetfacilitates a more stable learning process for and improvesthe final performance; and 2) we develop a deformablepatch mechanism incorporating resizable rectangular imagepatches, enabling the capture of task-relevant regions withdiverse shapes and sizes within each video frame.Additionally, we extend our method by simultaneouslymodeling the spatial-temporal dynamic computation. Thisis accomplished by proposing a dynamic frame samplingalgorithm tailored for AdaFocus (.2), which enablesour approach to allocate the majority of computation tothe task-relevant spatial regions of the most informativeframes. Lastly, we demonstrate that AdaFocus can be furtheroptimized by reducing the sample-wise redundancy (.3), i.e., unevenly distributing computation across relativelyeasier and harder videos, leading to a considerableimprovement of the overall efficiency.By leveraging these technical innovations, Uni-AdaFocusestablishes a holistic framework that seamlessly combinesspatial, temporal, and sample-wise dynamic computation.It retains the merits of AdaFocusV2 in efficient end-to-endtraining and being compatible with a wide range of backbonenetworks, while substantially improving both theoretical andpractical computational efficiency during inference.",
  "Training the Policy Network with Feature Interpolation": "Inappropriate supervision signals for . AdaFocusV2 hasenabled the end-to-end training of the policy network .However, the gradients received by may be ambiguousand indirect with respect to learning a proper spatial dynamiccomputation policy. Ideally, during training, the modelshould be aware of how the semantic contents of the patchvt will change with different outputs of , as our goal isto localize the most informative parts of each video frame.Nevertheless, as shown in Eqs. (7 9), the gradients of are acquired by varying the pixel values of vt. Each pixelvalue is determined solely by four neighboring pixels in theoriginal video frame vt. Such a limited source of informationmight be too local to accurately represent the semantic-levelchanges when the location of vt varies.",
  "Back-propagation": ". Guiding the training of with deep features. The gradients for are obtained by minimizing the deep-feature-based loss Lspatial, insteadof being back propagated from the pixel space. With this design, thesupervision signals for contain much more semantic-level information,which contributes to learning a more effective patch selection policy. Gradients derived from interpolating deep features.To alleviate the problem arising from the pixel-space-basedgradient computation for , we propose to guide the trainingof by utilizing deep features. Our primary insight isto allow the outputs of to have a direct influence oncertain deep representations associated with vt. The featuresextracted by deep networks are known to excel at capturingthe task-relevant semantics of the inputs , , , ,. Once the gradients can explicitly inform about how itsoutputs will affect vt at the deep feature level, will receivedirect supervision signals regarding whether vt containsvaluable contents for the recognition task.Driven by this motivation, we propose an elegant im-plementation by reusing the coarse global feature maps eGt .Assume that we want to obtain the P P task-relevant patchvt centred at (xtc, ytc) from the H W video frame vt. Weapproximate this process on top of eGt . Specifically, supposethe size of eGt is HGW G. We take the HG",
  "= E{v1,v2,...}LCE(SoftMax(FCAux(eGt)), y),(14)": "where eGt is derived by pooling eGt, y is the label andFCAux() denotes a linear classifier. The gradients Lspatial/can be solved by leveraging Eqs. (7 9). It should be notedthat only is updated following Eq. (14), while the training ofall the other components remains unchanged. Furthermore,the process of obtaining vt from vt and feeding vt into thelocal encoder fL is not altered. The only difference is thatthe gradients of are calculated using (14) rather than beingback-propagated from vt. See for an illustration.The underlying logic behind Eq. (14) is straightforward.In the context of training , we employ the change of eGt to estimate the change of the deep features correspondingto vt. This design is reasonable since the relative position ofeGt on eGt is identical to that of vt on vt. That is to say, thelocation-preserving nature of deep features ensures that eGt represents the contents at the location of vt.Notably, Eq. (14) is conceptually relevant to AdaFocusV3 in training using deep features. However, comparedto , our method provides more effective guidance for based on the global information within eGt , introducesnegligible computational overhead during training (i.e., onlyan additional time of linear classification: FCAux()), and doesnot leads to a discrepancy between training/test inputs forthe local encoder fL. See Appendix E for more comparisons.",
  "Deformable Patches": "Limitations of fixed-size patches. In both AdaFocusV1 andV2, the informative image patches {v1, v2, . . .} are assumedto have a fixed size (i.e., P P). The model is only trainedto dynamically determine their locations to capture themost task-relevant spatial regions of each video frame. Thisdesign is straightforward, and it simplifies the model trainingprocess. However, in contrast, it compromises the flexibilityof AdaFocus. As a matter of fact, the shapes and sizes ofdiscriminative regions may vary across different videos anddifferent frames. By design, the patches with a fixed size areinherently unable to accommodate the informative regionsadaptively conditioned on each individual video frame.In other words, {v1, v2, . . .} may either omit crucial task-relevant details or include undesirable redundant contents.Incorporating deformable patches. To address this issue,we propose to allow the shape and size of the patches{v1, v2, . . .} to be dynamically configured according to thecharacteristics and semantics of each frame. To be specific, welet the policy network output a quadruple: (xtc, ytc, Htp, W tp),where xtc, ytc represent the centre coordinates of the patchvt, while Htp, W tp denote its height and width. Then wedirectly take the HtpW tp patch at (xtc, ytc) to obtain vt. It isworth noting that before feeding vt into the local encoderfL, we resize all the patches to P P, such that they can beconveniently processed in parallel on hardwares. Althoughthis implementation technique may result in slight geometricdistortion, we observe that the flexibility of adapting toarbitrary task-relevant regions significantly outweighs this",
  "(b) Deformable Patches (Uni-AdaFocus)": ". Deformable patches, which enable the patch selection policyto adapt flexibly to the task-relevant regions in various shapes, scales,and locations. The selected patches are resized to a common size P 2to be processed efficiently by fL on hardwares (e.g., GPUs). Notably,the flexibility of adapting to arbitrary task-relevant regions significantlyoutweighs the weakness of geometric distortion, yielding considerableaccuracy improvements across diverse scenarios weakness, leading to considerable accuracy improvements.An illustration of this procedure is depicted in .Training algorithm. Given the aforementioned formula-tion, an important problem is how to learn a proper thatcan assign a suitable patch height/width to each frame. Asa straightforward approach, one may replace the P in Eq.(13) with Htp or W tp, and subsequently obtain the gradientsfollowing Eq. (14). Nevertheless, Eq. (14) is not specificallydesigned for learning to determine Htp and W tp, and simplyutilizing it will lead to a trivial optimization solution, i.e.,encouraging Htp, W tp 0 for all video frames. This outcomecan be attributed to the characteristics of deep features: wecan always find a point in any feature map, at which locationthe feature minimizes the recognition loss. As a consequence,Eq. (14) will push the resizable rectangular patches to fit thissingle point. We find this problem can be circumvented byslightly modifying Eq. (14), specifically,",
  "(15)": "where HW is the size of the original video frames. In (15),we introduce a regularization term [(HHtp)2+(WW tp)2]to explicitly promote larger Htp, W tp, and is a pre-definedcoefficient to control the strength of regularization. Despiteits simplicity, Eq. (15) is capable of training flexible andadaptable patch selection policies that effectively capturetask-relevant regions in various shapes, sizes, scales, andlocations, resulting in significant accuracy improvementsacross diverse scenarios.",
  "Reducing Temporal Redundancy Dynamic FrameSampling": "Up to this point, we have presented considerable discus-sions on learning to establish a flexible spatial dynamiccomputation mechanism. However, our methodology thusfar follows a basic assumption, that is, the computationis uniformly distributed along the temporal dimension. Infact, AdaFocus can be further improved by reducing thetemporal-wise redundancy, namely adaptively identifyingand attending to the most task-relevant video frames. In this subsection, we will demonstrate that this objective can beachieved with a dynamic frame sampling algorithm tailoredfor our approach.A unified framework. We seek to develop a unifiedframework that considers both spatial and temporal redun-dancies. Hence, instead of processing videos on a frame-by-frame basis, here we assume the lightweight global encoderfG first takes a quick glance at the whole video. To bespecific, suppose that a video comprises T0 frames in total.We uniformly sample TG (TG < T0) frames from the video,and feed them into fG at the same time, directly obtainingthe inexpensive and coarse global features correspondingto the entire video. Then we activate the policy network to concurrently determine: 1) the TL (TL <T0) task-relevantframes out of all the T0 frames that the high-capacity localencoder fL should process; and 2) the height, width, andlocation of the informative patch to be fed into fL for eachframe identified in 1). In the following, we will first introducehow to acquire 1), and then demonstrate that our previouslydiscussed spatial dynamic computation techniques can beeffortlessly adapted for obtaining 2). An illustration of ourunified framework is shown in .Mathematical formulation for dynamic frame sampling.To select TL task-relevant frames from the original T0 frames,we start by formulating a weighted sampling problem. Weassume that generates a weight for each of the T0 frames:{w1, . . . , wT0}, with T0j=1wj = 1. Based on these weights,we execute weighted sampling without replacement over T0frames, resulting in TL selected frames. Formally, we refer totheir indices as n1, . . . , nTL, which yields",
  ". (17)": "Herein, Lframe(vni) represents the loss corresponding to thenthi frame vni, which measures the quantity of the valuabletask-relevant information encompassed by vni or a shortvideo clip centred at vni. The two expectations in Ltemporalare taken over the single-video frame-sampling distribution,and the different videos within training data, respectively.Importantly, it is difficult to solve problem (17) directlywith gradient-based methods. Calculating the exact valueof E{n1,n2,...}[] will lead to a complexity of O(P T0TL ), whereP T0TL denotes the number of TL-permutations of T0. This inef-ficiency will make Ltemporalintractable when TL is not verysmall (e.g., with TL >8). On the other hand, a more efficientapproach is to estimate E{n1,n2,...}[] utilizing Monte Carlosampling. However, this will eliminate the differentiabilityof Ltemporalwith respect to . As a consequence, cannotreceive gradients from Ltemporalas supervision signals.",
  "Concatenation": ". Overview of Uni-AdaFocus, a unified framework that models spatial-temporal dynamic computation concurrently, leading to a considerablyimproved computational efficiency for inference. Compared with , here we let simultaneously determine which frames are more task-relevant,as well as which spatial regions within these frames contain more valuable information for the task. The high-capacity and computationally intensivelocal encoder fL is only activated on the most informative regions of the most important frames. Notably, similar to AdaFocusV2, Uni-AdaFocus canbe trained efficiently in an end-to-end fashion, and is compatible with off-the-shelf advanced backbone networks (e.g., TSM and X3D ).",
  ". (21)": "As the complexity of generating a single Monte Carlo sampleis O(T0 + TL log T0), solving Eq. (21) has a total complexityof O(TL(T0 + TL log T0) M), which is dramatically moreefficient than solving Eq. (17). Additionally, Eq. (21) isdifferentiable with respect to {w1, . . . , wT0}, allowing forthe end-to-end training of . In this paper, we fix M =128,which works reasonably well in various scenarios.Similar to us, OCSampler also formulates frameselection as a video-conditional sequential weighted sam-pling problem. Our contribution over lies in developing a novel solving algorithm for this basic formulation. Wepropose to consider the expected loss over dynamic framesampling as the training objective, and reveal that it can bedecomposed into a differentiable form solved with the MonteCarlo method, yielding an efficient end-to-end optimizationprocedure. See Appendix E for more comparisons.Implementation. Our implementation of dynamic framesampling minimizes Eq. (21) without notable additional costs.See Appendix C for more details (due to spatial limitations).Unified spatial-temporal dynamic computation. No-tably, dynamic frame sampling is fully compatible withthe previously described spatial-wise dynamic computationmechanism. For example, one may train to identify theinformative frame patches for the TG frames processed by fGin accordance with (15), and interpolate between the outputsof of adjacent frames to acquire a deformable task-relevantpatch for each of the TL selected frames. In summary, theoverall training objective of our method can be written as",
  "Reducing Sample-wise Redundancy Conditional-exit": "Apart from the aforementioned spatial and temporal redun-dancies, an additional factor contributing to a significantamount of redundant computation can be attributed to theequivalent treatment of diverse samples concerning theircomputational cost. In fact, numerous studies , , ,, have reported the existence of a significant numberof easier samples within datasets, which can be accuratelyrecognized with considerably less computation compared",
  ". Illustration of the conditional-exit algorithm": "to other samples. We propose to model this sample-wiseredundancy through a straightforward early-exit algorithm.This approach can be seamlessly integrated into the modelstrained using the previously outlined paradigm, obviatingthe need for an additional training phase.In the context of videos, we assume that activating thehigh-capacity local encoder fL to process a subset of framesrather than all frames may be sufficient for the easiersamples. To implement this idea, at test time, we propose tocompare the entropy of the softmax prediction pt at tth frame(i.e., j ptj log ptj) with a pre-defined threshold t. Theinference will be terminated with pt if j ptj log ptj t.We always adopt an infinite-threshold at the final frame. Theconcept is illustrated in . The values of {1, 2, . . .}are solved on the validation set. Suppose that the modelneeds to classify a set of samples Dval within a givencomputational budget B > 0 , . One can obtainthe thresholds through",
  "subject toFLOPs(1, 2, . . . |Dval) B.(23)": "Here Acc(1, 2, . . . |Dval) and FLOPs(1, 2, . . . |Dval) re-fer to the accuracy and computational cost on Dval usingthe thresholds {1, 2, . . .}. Notably, by changing B, one canobtain varying values of {1, 2, . . .}. The computational costof our method can be flexibly adjusted without additionaltraining by simply adjusting these thresholds. In our im-plementation, we solve problem (23) following the methodproposed in on the training set, which we find performson par with using cross-validation.",
  "EXPERIMENTS": "Overview. This section presents extensive empirical resultsto validate the effectiveness of Uni-AdaFocus. In .1, we compare Uni-AdaFocus with state-of-the-art efficientvideo understanding frameworks under the standard ex-perimental settings in most of the literature. In .2,we deploy Uni-AdaFocus on top of representative recentlyproposed lightweight deep networks (i.e., TSM and X3D),and demonstrate that our method is able to effectivelyimprove the computational efficiency of these cutting-edgemodels. In .3, we further evaluate Uni-AdaFocusin the context of three representative real-world applicationscenarios. In .4, comprehensive analytical resultsand visualizations are provided to give additional insightsinto our method. Besides, in both Sections 5.1 & 5.2, wepresent the comparisons of Uni-AdaFocus v.s. AdaFocusV1/V2/V3.Datasets. Seven large-scale video understanding bench-mark datasets are considered, i.e., ActivityNet , FCVID, Mini-Kinetics , , Something-Something (Sth-Sth)V1&V2 , Jester and Kinetics-400 . We also con-sider three real-world application scenarios i.e., fine-graineddiving action classification on Diving48 , Alzheimersand Parkinsons diseases diagnosis with brain magneticresonance images (MRI) on ADNI , OASIS , and PPMI, and violence recognition for online videos on RLVS .For data pre-processing, we adopt the same pipeline as ,, . More details are deferred to Appendix G.",
  ".8x": ". Comparisons of Uni-AdaFocus and state-of-the-art efficient video understanding approaches on ActivityNet in terms of inferenceefficiency. Our method adopts P 2 {962, 1282, 1602}, corresponding to the three black curves. Notably, the inference cost of Uni-AdaFocus canswitch within each black curve without additional training (by modifying the conditional-exit criterion, see: .3). , AR-Net , AdaFrame , AdaFuse , VideoIQ ,Dynamic-STE , AdaMML , SMART , FrameExit, OCSampler , NSNet , TSQNet , AFNet ,and the preliminary versions of AdaFocus , , .Following the common practice of these baselines, we adoptMobileNet-V2 and ResNet-50 as the global encoderfG and local encoder fL in Uni-AdaFocus. We uniformlysample T0 =48 frames from each video, and set TG =TL =16.In addition, note that Uni-AdaFocus resizes the selectedinformative patches with varying scales and shapes to acommon size P 2, under the goal of processing these patchesefficiently on hardwares like GPUs (see .1.2 and for details). We consider P 2 {962, 1282, 1602}.More implementation details can be found in Appendix H.Comparisons with state-of-the-art baselines on Activ-ityNet, FCVID and mini-Kinetics are summarized in . It is clear that Uni-AdaFocus (1282) outperforms all thecompetitive efficient video understanding methods by largemargins. For example, on ActivityNet, it achieves 3.5%higher mean average precision (mAP) (80.4% v.s. 76.9%)than the strongest baseline, OCSampler , with smallerGFLOPs (18.1 v.s. 21.7). On FCVID and Mini-Kinetics, similarobservations can be consistently obtained, i.e., Uni-AdaFocusoutperforms the recently proposed NSNet by 2.3%and 2.1% with similar or less computation. In ,we further present the variants of the baselines and Uni-AdaFocus with different computational costs for a morecomprehensive comparison. It can be observed that ourmethod leads to a considerably better efficiency-accuracytrade-off. When achieving the same state-of-the-art levelmAP, the number of the required GFLOPs/video for Uni- TABLE 3Comparisons of Uni-AdaFocus and AdaFocusV1/V2 in terms oftraining efficiency. The mAP and wall-clock training time on ActivityNetare reported. The latter is obtained using 4 NVIDIA 3090 GPUs. Thebest results are bold-faced. E2E refers to End-to-End. For faircomparison, here we do not perform sample-wise dynamic computation.",
  "AdaFocusV16.4h7.2h8.6hAdaFocusV23.4h3.7h4.3hUni-AdaFocus3.6h3.8h4.1h": "AdaFocus is approximately 4.8 less than OCSampler .Effectiveness of sample-wise dynamic computation. Ta-ble 2 and evaluate the performance of Uni-AdaFocusboth with and without the conditional-exit mechanism in.3. When this mechanism is activated, we vary theaverage computational budget, solve early-exit thresholds,and evaluate the corresponding mAP or Top-1 accuracy,as stated in .3. One can observe that sample-wiseadaptive computation effectively improves the inferenceefficiency. Moreover, this mechanism enables Uni-AdaFocusto adjust its inference cost online without additional training(by simply changing the early-exit thresholds).Comparisons with the preliminary versions of AdaFo-cus. The comparisons of Uni-AdaFocus and AdaFocusV1/V2in terms of mAP v.s. training/inference cost are presented in and , respectively. One can observe that bothUni-AdaFocus and AdaFocusV2 can be trained efficiently inan end-to-end fashion, which reduces the time consumptionfor training by 2 compared to the three-stage trainingalgorithm of AdaFocusV1, while Uni-AdaFocus improves",
  "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION12": "TABLE 4Uni-AdaFocus-TSM v.s. representative efficient video understanding models on Sth-Sth V1&V2 and Jester. MN2, R18/R34/R50 and BN-Inc.denote MobileNet-V2, ResNet-18/34/50 and BN-Inception. TSM+ refers to the augmented TSM baseline with the same network architecture as ourmethod except for the policy network . The throughput is benchmarked on an NVIDIA 3090 GPU. The best results are bold-faced. The blue textshighlight the comparisons with the underlined baselines. 120-epoch training, which is 2.4x longer than our method and the baselines.",
  "Building on Top of Existing Efficient Backbones": "Setups. In this subsection, we implement Uni-AdaFocus ontop of the recently proposed efficient network architectures,ConvNets with temporal shift module (TSM) and X3Dnetworks , to demonstrate that our method can furtherimprove the efficiency of such state-of-the-art lightweightmodels. For TSM, we still use MobileNet-V2 and ResNet-50as fG and fL, but add TSM to them. Following the originaldesign of TSM , a fully-connected layer is deployed as theclassifier fC, and we average the frame-wise predictions asthe outputs. For a fair comparison, we augment the vanillaTSM by introducing the same two backbone networks asours (named as TSM+), where their output features arealso concatenated to be fed into a linear classifier. In otherwords, TSM+ differentiates itself from our method only inthat it feeds the uniformly-sampled whole video framesinto ResNet-50, while we feed the dynamically-selectedinformative image patches from task-relative frames. ForX3D, we simply replace MobileNet-V2/ResNet-50 by X3D-S/X3D-L on top of the settings of TSM. Here we directly compare the performance of our method with X3D, sinceboth our two backbones come from the family of X3Dnetworks. Following the experimental settings in the originalpapers of TSM and X3D , the video recognitiontask on Sth-Sth V1&V2, Jester and Kinetics-400 is consideredhere. We uniformly sample T0 = 24/36/48/96 frames fromeach video, and set TL =8/12/16/32 correspondingly, withTG =8/16. See Appendix H for more implementation details.Results on Sth-Sth V1&V2 and Jester are reported in. Uni-AdaFocus enables TSM to concentrate the ma-jority of computation on the most task-relevant video framesand image regions, while it allows allocating computationunevenly across easier and more difficult videos. Asa consequence, the overall computational efficiency duringinference is dramatically improved. For example, our methodreduces GFLOPs by 3.99x (8.8 v.s. 35.1) on top of TSM+, butoutperforms it by 2.9% (62.5% v.s. 59.6%) on Sth-Sth V2in terms of Top-1 accuracy. Compared with the recentlyproposed S2DNet framework, Uni-AdaFocus is able toachieve considerably better performance (e.g., enhancingthe accuracy by 1.3-1.7% using less inference cost) with 2.4xfewer training epochs. In , we also report the actualinference speed of our method on an NVIDIA 3090 GPU,which is benchmarked using a batch size of 128. At eachmini-batch, when the inference procedure reaches each exit,",
  "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION13": "the samples that meet the early-termination criterion will beoutput, with the remaining videos continuing to be processed.It can be observed that our practical speedup is significant aswell, with a slight drop compared with the theoretical results.We tentatively attribute this to the inadequate hardware-oriented optimization in our implementation.Comparisons with the preliminary versions of AdaFo-cus on Sth-Sth are shown in . When achievingthe same accuracy, Uni-AdaFocus reduces the computationalcost by 2.7-3.5x. Furthermore, when leveraging the sameamount of computation at test time, Uni-AdaFocus improvesthe test accuracy by 2-3.5% compared with AdaFocusV2.Results with X3D on Kinetics-400. In , weimplement our method on top of the lightweight X3D backbones, and report its performance on the large scaleKinetics-400 benchmark. For a comprehensive comparison,we also present the results of representative state-of-the-art efficient video understanding models on Kinetics-400.Note that we mainly compare Uni-AdaFocus-X3D with thesebaselines under similar validation accuracies or inferencecosts. One can clearly observe that our method contributesto a significantly improved accuracy-efficiency trade-off. Forexample, compared to X3D-L, Uni-AdaFocus-X3D reducesthe computational cost by 3.76x (4.9 v.s. 18.4), while improv-ing the accuracy by 0.5% (76.2% v.s. 75.7%) in the meantime.Furthermore, compared to the best baseline, MoViNet-A2,Uni-AdaFocus-X3D has 2.10x less GLOPs/video (4.9 v.s. 10.3)when achieving the same accuracy of 75.0%.",
  "Real-world Application Scenarios": "We also evaluate Uni-AdaFocus in three representativerealistic application scenarios: 1) fine-grained diving actionclassification; 2) Alzheimers and Parkinsons diseases di-agnosis with brain magnetic resonance images (MRI); and3) violence recognition for online videos. Due to spatiallimitations, these results can be found in Appendix D. Inall scenarios, Uni-AdaFocus outperforms existing works bylarge margins, yielding a new state-of-the-art performance.",
  "Ablation Studies": "Effectiveness of the techniques for end-to-end training(introduced in .3.2) is validated in . Onecan observe that all of the three techniques significantlyimprove the performance across different experimentalsettings. The combination of them allows our method tobe trained effectively in an end-to-end fashion. Importantly, TABLE 5Performance of Uni-AdaFocus-X3D and representative efficientvideo understanding models on Kinetics-400. MN2/R18/R34/R50/ENdenotes MobileNet-V2/ResNet-18/34/50/EfficientNet. The blue textshighlight the comparisons with the underlined baselines.",
  "+3276.2% (0.5%) 4.93 (3.76x)": "these techniques do not introduce additional tunable hyper-parameters. In all our experiments, we simply implementthem as fixed components of the training algorithm.Effectiveness of the improved techniques for spatial-temporal dynamic computation (introduced in Sections 4.1and 4.2). The ablation study results are provided in .It is clear that guiding the training of with deep featuresconsistently improves the performance by 1% via obtaininga better patch selection policy. Built upon it, introducingdeformable patches further yields significant gains by flexiblyadapting to the task-relevant regions with diverse scales andshapes. Notably, this mechanism is particularly effectivewhen the input size of the local encoder fL is relatively small(e.g., 962). In other words, it is important to properly con-figure the actual contents of the inputs when the input sizeis small. Moreover, dynamic frame sampling is compatiblewith the spatial dynamic computation, and is able to furtherenhance the accuracy.Design of the sample-wise dynamic computation mech-anism (introduced in .3) is studied in . Wecompare our design with two baselines, i.e., 1) early-exit withfixed frame length; and 2) random early-exit with the sameexit proportion as Uni-AdaFocus. It can be clearly observedthat our adaptive conditional-exit algorithm outperformsboth of them. When achieving the same mAP, our method isable to effectively save the required amount of computation. TABLE 6Ablation studies of the training techniques proposed in.3.2. These techniques enable our method to betrained efficiently in an end-to-end fashion. The results ofUni-AdaFocus (w/o sample-wise dynamic) are reported. OnSth-Sth, we adopt #Frames=8+12.",
  "% +3.9% 47.0% +0.9%": "TABLE 7Ablation studies of the techniques proposed in Sections 4.1 and 4.2.Representative conditions with different backbone networks, datasets and varyingP 2 (input size for fL) are considered. The results of Uni-AdaFocus (w/osample-wise dynamic) are reported. On Sth-Sth, we adopt #Frames=8+12. The firstline of the table corresponds to the last line of .",
  "Adaptive Early-exit (ours)Fixed Early-exitRandom Early-exit": ". Ablation studies of the conditional-exit algorithm. The resultson ActivityNet are provided. For a fair comparison, different variantsare deployed on top of the same base model. Our method significantlyreduces the computational cost when achieving the same mAP. TABLE 8Effects of Reusing eGt for Recognition. Representative conditions withdifferent backbone networks, datasets and varying P 2 (input size for fL)are considered. The results of Uni-AdaFocus (w/o sample-wise dynamic)are reported. On Sth-Sth, we adopt #Frames=8+12. The last line of thetable corresponds to the last line of .",
  "Dynamic (ours)45.7% 58.3% 67.4% 74.8% 79.6%": "Effectiveness of reusing eGt for recognition. As afore-mentioned, our method leverages the coarse global featureeGt for both activating the policy network and recognition,aiming to facilitate efficient feature reusing (see .1for details). The effectiveness of this design is studied in . One can observe that this mechanism is able to improvethe accuracy by 0.8-2.8%. It is worth noting that this featurereusing introduces negligible computational cost, and hencethe improvement of performance is almost a free lunch.Effectiveness of the learned spatial-temporal dynamiccomputation policy is investigated in . The threemajor components of our policy are one by one replacedwith several corresponding alternatives, aiming to validatewhether our design is proper. In specific, our baselinesinclude 1) patch localization policy: 1-a) randomly samplingpatches, 1-b) cropping patches from the centres of the frames,1-c) sampling patches from a gaussian distribution centredat the frame centre; 2) policy to determine the sizes/shapesof patches: 2-a) fixed patch size, 2-b) randomly determiningpatch sizes, 2-c) down-sampling the frames as patches, 2-d)",
  "Easy VideosHard Videos": ". Examples of the task-relevant patches and informativevideos frames selected by Uni-AdaFocus (zoom in for details). Wepresent a variety of representative input videos, where easy/hard videosrefer to the samples to which Uni-AdaFocus allocates a relatively smalleror larger number of computation resources. removing the regularization term in Eq. (15) when trainingour policy; and 3) frame sampling policy: 3-a) uniformlysampling frames from each video, 3-b) randomly samplingframes, 3-c) directly taking the central clips of videos, 3-d)sampling frames from a gaussian distribution centred at thevideo centre, 3-e) adopting MG Sampler . For an isolatedand focused reflection of the effects of our policy with clearcomparisons, here we do not reuse the global feature eGtfor recognition (its effects have been studied in ).We assume that Uni-AdaFocus processes a fixed number offrames for all videos, and report the corresponding mAPon ActivityNet. One can observe that our learned policieshave considerably better performance compared with allthe baselines. For 1), an interesting phenomenon is thatrandom policy appears strong and outperforms the centralpolicy, which may be attributed to the spatial similaritybetween frames. That is to say, adjacent central patchesmight have repetitive contents, while randomly sampling islikely to collect more comprehensive information. For 2), weobserve that introducing the regularization term in Eq. (15) isan important technique to learn effective deformable-patchpolicies. For 3), although the recently proposed MG Sampler is also able to improve the accuracy, our dynamic framesampling algorithm outperforms it by large margins. 5.4.2Visualization ResultsRepresentative visualization results are shown in (blue boxes indicate the patches selected by Uni-AdaFocus).Our method can adaptively attend to the informative regionsof some task-relevant video frames, such as the dancer, thedog, the skateboard, and the actions of human hands. Due tospatial limitations, more visualizations (including an analysisof failure cases) can be found in Appendix F.",
  "%74.3%77.0%78.5%79.0%79.5%": "TABLE 11Analysis of the design of our spatial dynamic computation mechanism. We investigate eliminating its three major components (two of which areintroduced by the deformable patch mechanism proposed in .1.2), or further incorporating more flexible dynamic transformations. Characteristics of the Patch Selection Mechanism within Uni-AdaFocus (P 2=1282)ActivityNet mAP after Processing t Frames(i.e., corresponding to pt)(basic spatial dynamic)(deformable-patch)(introducing more dynamic transformations)",
  "% (4.6%)53.4% (4.9%)62.8% (4.6%)70.8% (4.0%)76.6% (3.0%)": "of those studies, here we further design a series of more in-depth analyses, aiming to highlight the important findings ofthis paper, and clarify our novel contributions over existingworks. The performance of Uni-AdaFocus in Tables 10-16 isbuilt upon the experimental setups of the last line of . Importance of different dynamic computation strategies. examines the impact of eliminating certain aspects ofspatial/temporal/sample-wise dynamic computation. Specif-ically, when removing each respective component, our modelprocesses full video frames, uniformly samples frames, orrandomly performs early-exit, yielding an equivalent com-putation allocation across different spatial regions, temporallocations, or diverse samples. It can be observed that model-ing any one within the three types of dynamic computationsignificantly improves the computational efficiency of Uni-AdaFocus, while they are compatible with each other inan arbitrary form of combination. In general, incorporatingspatial adaptiveness results in the most significant gains. Design of the spatial dynamic computation mechanism.Diverse variations in locations, scales, and shapes are threefundamental characteristics intrinsic to visual elements. Asshown in , the performance of Uni-AdaFocus canbe markedly improved by explicitly considering adaptingto every type of these variations when selecting the task-relevant regions of different video frames. Intriguingly,introducing more flexible dynamic transformations (e.g.,affine, homography, and thin-plate spline) slightly degradesthe accuracy. This phenomenon may be caused by the lackof sufficiently general demands for utilizing these imagewarping/rectification techniques to process the task-relevantregions in our problem. On the contrary, they may distort theinputs of the local encoder fL from the natural distribution,and increase the complexity of training the policy network .",
  "CONCLUSION": "This paper presented Uni-AdaFocus, an approach that en-ables deep networks to allocate computation dynamicallyacross three dimensions: spatial, temporal, and differentsamples. The major motivation behind it is to concentratethe computational resources on the most task-relevant imageregions, informative video frames, and relatively difficulttest data, and thus attain a superior overall accuracy with aminimal total computational cost. We delved deep into themodel design, training, and efficient implementation of Uni-AdaFocus. With our proposed techniques, Uni-AdaFocus canbe easily deployed on top of popular lightweight backbones(e.g., TSM and X3D) and considerably improves their infer-ence efficiency through adaptive computation. Moreover, itcan be trained efficiently in an end-to-end fashion. Empiricalresults based on seven large-scale benchmark datasets andthree real-world application scenarios demonstrate that Uni-AdaFocus achieves state-of-the-art performance in termsof generalization performance, theoretical computationalefficiency, and practical inference speed.In the future, it would be interesting to further explorehow our proposed dynamic computation techniques can beemployed to improve the inference efficiency of multi-modallarge language models. For example, one may consideradaptively identifying task-relevant video frames and imageregions conditioned on the input text or audio prompts.Given the expensive cost of training large models, it wouldalso be important to investigate how pre-trained multi-modalmodels can be fine-tuned to acquire such capabilities ofdynamic computation.",
  "Grant 62321005, in part by the BNRist project under GrantBNR2024TD03003, and in part by Tsinghua University-China Mobile Communications Group Company, Ltd. JointInstitute": "J. Davidson, B. Liebald, J. Liu, P. Nandy, T. Van Vleet, U. Gargi,S. Gupta, Y. He, M. Lambert, B. Livingston et al., The youtubevideo recommendation system, in Proceedings of the fourth ACMconference on Recommender systems, 2010, pp. 293296. Y. Deldjoo, M. Elahi, P. Cremonesi, F. Garzotto, P. Piazzolla, andM. Quadrana, Content-based video recommendation systembased on stylistic visual features, Journal on Data Semantics, vol. 5,no. 2, pp. 99113, 2016.",
  "Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang,Dynamic neural networks: A survey, IEEE Transactions on PatternAnalysis and Machine Intelligence, vol. 44, no. 11, pp. 74367456,2022": "G. Huang, Y. Wang, K. Lv, H. Jiang, W. Huang, P. Qi, and S. Song,Glance and focus networks for dynamic visual recognition, IEEETransactions on Pattern Analysis and Machine Intelligence, vol. 45,no. 4, pp. 46054621, 2023. Y. Rao, Z. Liu, W. Zhao, J. Zhou, and J. Lu, Dynamic spatialsparsification for efficient vision transformers and convolutionalneural networks, IEEE Transactions on Pattern Analysis andMachine Intelligence, pp. 114, 2023.",
  "Y. Wang, X. Pan, S. Song, H. Zhang, G. Huang, and C. Wu,Implicit semantic data augmentation for deep networks, inNeurIPS, 2019": "S. Li, C. H. Liu, Q. Lin, Q. Wen, L. Su, G. Huang, and Z. Ding,Deep residual correction network for partial domain adaptation,IEEE transactions on pattern analysis and machine intelligence, vol. 43,no. 7, pp. 23292344, 2020. B. Xie, S. Li, M. Li, C. H. Liu, G. Huang, and G. Wang, Sepico:Semantic-guided pixel contrast for domain adaptive semanticsegmentation, IEEE Transactions on Pattern Analysis and MachineIntelligence, vol. 45, no. 7, pp. 90049021, 2023. M. Xie, S. Li, K. Gong, Y. Wang, and G. Huang, Adapting acrossdomains via target-oriented transferable semantic augmentationunder prototype constraint, International Journal of ComputerVision, vol. 132, no. 4, pp. 14171441, 2024.",
  "M. Kim, H. Kwon, C. Wang, S. Kwak, and M. Cho, Relational self-attention: Whats missing in attention for video understanding,in NeurIPS, 2021, pp. 80468059": "C. Lian, M. Liu, J. Zhang, and D. Shen, Hierarchical fully con-volutional network for joint atrophy localization and alzheimersdisease diagnosis using structural mri, IEEE transactions on patternanalysis and machine intelligence, vol. 42, no. 4, pp. 880893, 2018. G. Cao, M. Zhang, Y. Wang, J. Zhang, Y. Han, X. Xu, J. Huang,and G. Kang, End-to-end automatic pathology localization foralzheimers disease diagnosis using structural mri, Computers inBiology and Medicine, p. 107110, 2023. X. Zhang, Y. Yang, H. Wang, S. Ning, and H. Wang, Deep neuralnetworks with broad views for parkinsons disease screening, in2019 IEEE International Conference on Bioinformatics and Biomedicine(BIBM), 2019, pp. 10181022.",
  "J. Jang and D. Hwang, M3t: three-dimensional medical imageclassifier using multi-plane and multi-slice transformer, in CVPR,2022, pp. 20 71820 729": "K. Oh, J. S. Yoon, and H.-I. Suk, Learn-explain-reinforce: Coun-terfactual reasoning and its guidance to reinforce an alzheimersdisease diagnosis model, IEEE Transactions on Pattern Analysisand Machine Intelligence, vol. 45, no. 4, pp. 48434857, 2023. A. Traor and M. A. Akhloufi, 2d bidirectional gated recurrentunit convolutional neural networks for end-to-end violence detec-tion in videos, in Image Analysis and Recognition: 17th InternationalConference, ICIAR 2020, Pvoa de Varzim, Portugal, June 2426, 2020,Proceedings, Part I, 2020, pp. 152160. J. P. de Oliveira Lima and C. M. S. Figueiredo, A temporalfusion approach for video classification with convolutional andlstm neural networks applied to violence detection, InteligenciaArtificial, vol. 24, no. 67, pp. 4050, 2021.",
  "ASUMMARY OF CHANGES": "This paper extends two previous conference papers that in-troduced the basic AdaFocus network and preliminarilydiscussed its end-to-end training . Moreover, our deep-feature-based approach for training the patch selection policy(.1.1) is conceptually relevant to, and improvedupon (see for a detailed comparison). Wehave improved these earlier works substantially in severalimportant aspects, as summarized in the following. The spatial dynamic computation algorithm has beenimproved. First, we introduce a deep-feature-basedapproach for training the patch selection policy effec-tively, which incurs negligible additional cost whileconsiderably improving the final accuracy (.1.1). Second, we develop a deformable patch mecha-nism, enabling AdaFocus to flexibly capture the task-relevant spatial regions within each video frame withdiverse shapes, sizes, and scales (.1.2). We propose a dynamic frame sampling algorithmtailored for AdaFocus (.2), and extend ourmethod by simultaneously modeling the spatial,temporal, and sample-wise dynamic computation.The resulting Uni-AdaFocus framework ()achieves a remarkable performance across varioussettings in terms of computational efficiency. The experimental results on six benchmark datasets(i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, and Jester) have been updated. Wereport new state-of-the-art results with Uni-AdaFocus(Tables 2 and 4, ), and comprehensivelycompare Uni-AdaFocus with the works in , , (Figures 10 and 11, ). Moreover, weconduct more experiments to evaluate our method ontop of the efficient X3D backbone networks onthe large-scale Kinetics-400 benchmark (). We further apply our approach to three real-worldapplication scenarios (i.e., fine-grained diving actionclassification, Alzheimers and Parkinsons diseasesdiagnosis using brain magnetic resonance images(MRI), and violence recognition for online videos),and report encouraging results (.3). Additional analytical results are provided, includ-ing the comprehensive ablation studies of the newcomponents (Tables 7, 8, and 9, ), in-depthdiscussions of our important findings (Tables 10and 11), thorough comparisons with existing works(Tables 15 and 16), and new visualization results(Figures 13, 14, and 15).",
  "BTHREE STAGE TRAINING OF ADAFOCUSV1": "Since the formulation of AdaFocusV1 includes both con-tinuous (i.e., video recognition) and discrete (i.e., patchlocalization) optimization, the standard end-to-end trainingparadigm cannot be directly applied. Therefore, we introducea three-stage training algorithm to solve continuous anddiscrete optimization problems alternatively. Stage I: Warm-up. We first initialize fG, fL and fC, butleave the policy network out at this stage. Then werandomly sample the image patches vt to minimize thecross-entropy loss LCE() over the training set Dtrain:",
  "vt RandomCrop(vt).(24)": "Here T and y refer to the length and the label correspondingto the video {v1, v2, . . .}, respectively. In this stage, themodel learns to extract task-relevant information from anarbitrary sequence of frame patches, laying the basis fortraining the policy network .Stage II: Learning to select informative patches. Atthis stage, we fix the two encoders (fG and fL) and theclassifier fC obtained from stage I, and evoke a randomlyinitialized policy network to be trained with reinforcementlearning. Specifically, after sampling a location of vt from(|eG1 , . . . , eGt ) for the frame vt (see Eq. (4)), will receivea reward rt indicating whether this action is beneficial. Wetrain to maximize the sum of discounted rewards:",
  ",(25)": "where (0, 1) is a discount factor for long-term rewards.In our implementation, we fix = 0.7 and solve Eq. (25)using the off-the-shelf proximal policy optimization (PPO)algorithm . Notably, here we directly train on the basisof the features extracted by fG, since previous works , have demonstrated that the vision backbones learned forrecognition generally excel at localizing task-relevant regionswith their deep representations.Ideally, the reward rt is expected to measure the value ofselecting vt in terms of video recognition. With this aim, wedefine rt as:",
  "rt(vt|v1, . . . , vt1)= pty(vt|v1, . . . , vt1) EvtRandomCrop(vt) [pty(vt|v1, . . . , vt1)] ,(26)": "where pty refers to the softmax prediction on y (i.e., con-fidence on the ground truth label). When computing rt,we assume all previous patches {v1, . . . , vt1} have beendetermined, while only vt can be changed. The second termin Eq. (26) refers to the expected confidence achieved bythe randomly sampled vt. By introducing it we ensuresEvt[rt] = 0, which is empirically found to yield a more stabletraining procedure. In our experiments, we estimate this termwith a single time of Monte Carlo sampling. Intuitively, Eq.(26) encourages the model to select the patches that arecapable of producing confident predictions on the correctlabels with as fewer frames as possible.Stage III: Fine-tuning. Finally, we fine-tune fL and fC(or only fC) with the learned policy network from stage II,namely minimizing Eq. (24) with vt (|eG1 , . . . , eGt ). Thisstage further improves the performance of our method.",
  "utilizing the features of the TG uniformly sampled frames pro-cessed by the global encoder fG. We approximate the T0 TLsampling process with TG TL": "T0 TG weighted sampling,where the weights are able to be acquired by down-sampling{w1, . . . , wT0}. Subsequently, Lframe(vni) can be effortlesslyobtained with the off-the-shelf outputs of the auxiliary linearclassifier, i.e., LCE(SoftMax(FCG(eGni)), y) (see: Eq. (11)). Notethat when fG is implemented as video backbones with tem-poral fusion operations (e.g., 3D convolution or temporal self-attention), Lframe(vni) actually represents the loss of a shortvideo clip centred at vni. Moreover, at test time, to establisha deterministic inference procedure, we simply compute thecumulative distribution function of a single-time weightedsampling distribution parameterized by {w1, . . . , wT0}, andtake TL uniform quantile points on the vertical axis to findthe corresponding positions on the horizontal axis.",
  "IOS": "Setups. This subsection evaluates Uni-AdaFocus in threerepresentative realistic application scenarios. The details ofour experimental setups can be found in Appendix H.2.Fine-grained diving action classification on Diving48. The results are reported in . Our method iscompared with both the basic baselines (i.e., TSM/TSM+)and representative recently proposed methods that achievecurrent state-of-the-art performance. Uni-AdaFocus attainsthe best test accuracy with a minimal computational cost. Inparticular, it improves the accuracy by 7.7% (88.1% v.s. 80.4%)with 1.87x less computation on top of TSM+, which has astatic computational graph. This may be explained by thatUni-AdaFocus is more effective for less biased scenarios likeDiving48, where the task-relevant information is intensive,and it is lossless or even beneficial to filter out the lessinformative image regions and redundant video frames.Alzheimers and Parkinsons diseases diagnosis withbrain magnetic resonance images (MRI). Alzheimersand Parkinsons diseases are two of the most commonneurodegenerative disorders among the elderly , ,, characterized by the irreversible loss of neurons aswell as the impairment of cognitive and motor functions.The accurate diagnosis of them and early intervention atthe prodromal stage is of great importance, with whichthe onset of the diseases can be significantly delayed. Toaddress this issue, analyzing the MRI of potential patients isa widely used technique, where MRI depict the 3D structureof human brains in a non-invasive fashion. Our proposedmethod can be deployed as an effective approach for thedata-driven automatic analysis of 3D MRI data, under thegoal of improving the diagnosis precision and reducing thereliance on experienced clinicians, which are typically scarce.As shown in , Uni-AdaFocus is able to outperformthe state-of-the-art medical imaging analysis frameworkson ADNI , OASIS , and PPMI in terms of theaccuracy of detecting Alzheimers and Parkinsons diseases.Notably, many of the baselines leverage generative models,Transformer networks, or 3D convolution, which are morecomplicated and computationally intensive than our method.",
  "Uni-AdaFocus (1282)w/o sample-wise dynamicMN2+R5098.5% (2.2%) 27.2 (23.0x)": "Violence recognition for online videos. Realizing highlyaccurate automatic recognition of violent behaviors is apractically important task, e.g., to detect harmful visualcontents on the Internet in real-time or to build surveillancesystems in the real world to increase safety. The gains of Uni-AdaFocus are also significant for such realistic applications.As shown in , our method achieves a test accuracy of98.5% on RLVS , which outperforms all the competitivebaselines, and attains a new state-of-the-art performance.Moreover, the computational cost of our method is 23.0x lessthan the previous best baseline in , which is beneficialfor developing real-time video processing applications.",
  "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION21": "TABLE 15Uni-AdaFocus v.s. AdaFocusV3 in terms of the algorithm for learning spatial dynamic computation strategies. The three majoradvantages of Uni-AdaFocus over AdaFocusV3 are systematically removed to evaluate the performance of the ablated versions of our method.Notably, the training cost can be affected by either varying the number of training epochs or adjusting the input size of fL during training.",
  "[P 2": "(P +32)2 E epochs] [supervision from eGt ] [(P + 32)2 P 2]44.9%57.4%65.9%73.3%78.6% [E epochs] [supervision from eGt ] [(P + 32)2 P 2]45.1%57.8%66.5%73.5%79.1%Uni-AdaFocus [E epochs] [supervision from eGt ] [P 2 P 2]45.7%58.3%67.4%74.8%79.6% TABLE 16Uni-AdaFocus v.s. OCSampler in terms of the algorithm for learning temporal dynamic computation strategies. We replace our dynamicframe sampling algorithm in Uni-AdaFocus with OCSampler (implemented with the official code and configurations provided by ), andcompare its performance with our method. All other experimental setups remain unchanged for a fair comparison.",
  "Uni-AdaFocusOTL(T0 + TL log T0) M3.8h (2.5x)77.4%": "critical limitations of , as discussed in . Themethod in increases the input size of fL during trainingto probe the context information of the patch vt, andleverages the resulting enlarged local feature eLt to estimatethe gradients of . In contrast, the size of training/test inputsof fL is consistent in Uni-AdaFocus, which not only enhancesthe performance by eliminating the training/test discrepancyfor fL, but also avoids additional training cost (with thesame epochs). Besides, the cleverness of Uni-AdaFocus isalso manifested in employing the off-the-shelf global featureeGt from fG to produce gradients. It can be obtained with-out extra cost, while the global-level semantic informationencompassed by eGt demonstrates superior efficacy than thelocal feature eLt in guiding the training of .Comparisons with OCSampler . Both Uni-AdaFocusand formulate frame selection as a sequential weightedsampling problem without replacement, where the distribu-tion is dynamically parameterized conditioned on each video.Compared to , our contribution lies in developing a novelsolving algorithm for this basic formulation. Our theoreticalanalyses (.2) propose to consider the expected lossover dynamic frame sampling as the training objective, andreveal that it can be decomposed into a differentiable formsolved with the Monte Carlo method, leading to an efficientend-to-end optimization procedure. As shown in ,our method works reasonably well without the multi-stagetraining and reinforcement learning techniques utilized in, while it eliminates the factorial time complexity of thealgorithm. Empirically, it reduces the real training time by2.5x, yet improves the mAP considerably (77.4% v.s. 74.9%).",
  "FADDITIONAL VISUALIZATION RESULTS": "In , we present an analysis of the representativefailure cases of Uni-AdaFocus. We find that our currentmodel exhibits reduced efficacy in understanding the non-typical actions that are rare and may lie within the tail ofthe data distribution. For instance, videos such as a horsedrinking coffee or riding horses in the sea pose challenges",
  ".Additional visualization results focusing primarily onvideos that incorporate multi-person/object interactions (zoom infor details). Blue boxes indicate the patches selected by Uni-AdaFocus": "for the model. Moreover, it appears to be more challengingfor the model to accomplish video understanding tasksincorporating relatively higher demands of reasoning. Forexample, in the lower left video of , Uni-AdaFocussucceeds in identifying the interactions between humanhands and raw potatoes, yet fails to infer that these actionsare part of cookie-baking preparation. Similarly, in thelower right video of , Uni-AdaFocus succeeds inlocalizing task-relevant regions, yet misinterprets the role ofthe dog in the action. Future works may focus on addressingthese limitations of our current model. Notably, we do notobserve particular differences in the patterns of failure casesacross different datasets. Hence, we select the general andinteresting cases of ActivityNet video recognition (whichincorporates a broader range of various interactions) as",
  "Jester dataset consists of 148,092 videos in 27action categories. The average duration is 3 seconds": "Kinetics-400 is a large-scale human action videodataset collected from YouTube, which contains 400human action classes. It includes 240k videos fortraining and 20k videos for validation. The averageduration is around 10 seconds . Data pre-processing. Following , , , the train-ing data is augmented via random scaling followed by224x224 random cropping, after which random flipping isperformed on all datasets except for Sth-Sth V1&V2 andJester. At test time, since we consider improving the inferenceefficiency of video recognition, we resize the short sideof video frames to 256 and perform 224x224 centre-crop,obtaining a single clip per video for evaluation. On Kinetics-400, we also report the results of 3-crop prediction , ,where we take three 224x224 crops to cover the spatialdimensions, and average the SoftMax scores for prediction.Notably, these experimental settings are adopted under theconsideration of ensuring efficiency. Since our aim is toimprove the computational efficiency of the models, wedo not consider the settings of more inference views (e.g., 30views), which dramatically increase the inference cost.",
  "ActivityNet, FCVID and Mini-Kinetics. Following thecommon practice of our baselines, we adopt MobileNet-V2": "and ResNet-50 as the global encoder fG and localencoder fL in Uni-AdaFocus. The temporal accumulated max-pooling module proposed in is deployed as the classifierfC. The policy network adopts an efficient two-brancharchitecture, corresponding to producing the temporal andspatial dynamic computation policies. Given the input coarseglobal features with the size C T H W, the temporalbranch pools them to CT11 (since here we do not needto preserve the spatial information), and processes themwith two 311 convolutional layers followed by a fully-connected layer. In contrast, the spatial branch compressesthe channel number to 128 T H W with 1 1 1convolution to reduce the computational cost, and thenprocesses the features with two 333 convolutional layersfollowed by a fully-connected layer. Compared to directlyprocessing C T H W features, this design eliminatesthe redundant information in the corresponding branch,introducing minimal computational overheads, which aregenerally negligible. The training hyper-parameters of ourapproach can be found in Appendix H.3.Sth-Sth V1&V2, Jester and Kinetics-400. We implementUni-AdaFocus on top of the recently proposed efficientnetwork architectures, ConvNets with temporal shift module(TSM) and X3D networks , to demonstrate that ourmethod can further improve the efficiency of such state-of-the-art lightweight models. For TSM, we still use MobileNet-V2 and ResNet-50 as fG and fL, but add TSM to them.Following the original design of TSM , a fully-connectedlayer is deployed as the classifier fC, and we average theframe-wise predictions as the output. For a fair comparison,we augment the vanilla TSM by introducing the same twobackbone networks as ours (named as TSM+), where theiroutput features are also concatenated to be fed into a linearclassifier. In other words, TSM+ differentiates itself from ourmethod only in that it feeds the uniformly-sampled wholevideo frames into ResNet-50, while we feed the dynamically-selected informative image patches of task-relative frames.For X3D, we simply replace MobileNet-V2/ResNet-50 byX3D-S/X3D-L on top of the settings of TSM. Here we directlycompare the performance of our method with X3D, sinceboth our two backbones come from the family of X3Dnetworks.Following the experimental settings in the original papersof TSM and X3D , the video recognition task on Sth-Sth V1&V2, Jester and Kinetics-400 is considered here. Weuniformly sample T0 =24/36/48/96 frames from each video,and set TL = 8/12/16/32 correspondingly, with TG = 8/16.Notably, as the videos in these datasets are very short(average duration 3-4s for Sth-Sth V1&V2 and Jester, 10s for Kinetics-400), we find the networks require the visualangle of adjacent inputs (frames/patches) to be similar forhigh generalization performance. We also observe that the lo-cations and sizes of task-relevant regions do not significantlychange across the frames in the same video (due to the shortduration). Therefore, here we let Uni-AdaFocus generate acommon patch location/size for the whole video. In addition,as the backbone networks usually necessitate processing allthe inputs simultaneously, here our sample-wise conditional-exit is performed by adaptively activating/deactivating fL.Importantly, such simplification does not affect the main ideaof our method since different videos have varying patch",
  "H.2Real-world Application Scenarios": "In our paper, we evaluate the effectiveness of Uni-AdaFocusin three representative realistic application scenarios, includ-ing 1) fine-grained diving action classification , and 2)Alzheimers and Parkinsons diseases diagnosis with brainmagnetic resonance images (MRI) , , , and 3)violence recognition for online videos . For 1), we adoptthe Diving48 dataset , which consists of 18k trimmedvideo clips of 48 unambiguous dive sequences. All the videoshave similar backgrounds, while the networks need to havea strong ability to model temporal dynamics and distinguishbetween diving actions with subtle visual differences. Herewe implement Uni-AdaFocus-TSM following .2.For 2), we adopt the ADNI and OASIS datasetsfor Alzheimers disease, and the PPMI dataset forParkinsons disease. Since the amount of data in each datasetis relatively small, here we report the accuracy of 5-foldcross validation. The pre-processing of the MRI data followsfrom and . For 3), we adopt the RLVS dataset ,which contains 1000 violence and 1000 non-violence videoscollected from YouTube. For both 2) and 3), we simply utilizethe implementation of Uni-AdaFocus in .1, whichis sufficient to achieve superior performance. Besides, forthese applications, our major focus is to achieve relativelyhigh accuracy, and hence we do not perform sample-wisedynamic computation.",
  "H.3Training Hyper-parameters": "In this subsection, we present the configurations and hyper-parameters for training Uni-AdaFocus. The values of hyper-parameters are determined with the following procedure: wehold out 25% of the training samples as a mini-validation setto search for the hyper-parameters, and then we put thesesamples back to train our model on the whole training setwith our selected optimal hyper-parameters, with the finalperformance reported on the official validation or test set.ActivityNet, FCVID and Mini-Kinetics. As stated inthe paper, all the components (i.e., fG, fL, fC and ) of Uni-AdaFocus are trained simultaneously in a standard end-to-end fashion. An SGD optimizer with cosine learningrate annealing and a momentum of 0.9 is adopted. OnActivityNet, the size of the mini-batch is set to 32, and theL2 regularization co-efficient is set to 2e-4. The number oftraining epochs is set to 50. The two encoders fG and fL areinitialized using the official ImageNet pre-trained modelsprovided by PyTorch , while fC and are trained fromrandom initialization. The initial learning rates of fG, fL, fCand are set to 0.001, 0.002, 0.04 and 0.0004, respectively.The value of is selected within {0.25, 0.5, 0.75, 1} on aper-dataset basis. On FCVID and Mini-Kinetics, due to therelatively larger size of training data, we reduce the L2 regu-larization co-efficient to 1e-4, and increase the batch size to 64(the initial learning rates are linearly scaled correspondingly).The experiments on ActivityNet, FCVID and Mini-Kinetics with all patch sizes use the same aforementioned trainingconfigurations.Sth-Sth V1&V2 and Jester. When TSM is imple-mented as the backbones in Uni-AdaFocus, the initial learn-ing rates of fG, fL, fC and are set to 0.01, 0.02, 0.02 and 0.004,respectively. The L2 regularization co-efficient is set to 1e-3 onSth-Sth V1 and Jester, and 5e-4 on Sth-Sth V2. These changesmainly follow the official implementation of TSM . Allother training settings are the same as the experiments onFCVID and Mini-Kinetics. All the experiments on Sth-SthV1&V2 and Jester adopt the same aforementioned trainingconfigurations.Kinetics-400. When X3D is implemented as ourbackbones, we initialize fG and fL with the official pre-trained models provided by . The initial learning ratesof fG, fL, fC and are set to 0.0025, 0.005, 0.005 and 0.002,respectively. The L2 regularization co-efficient is set to 5e-5.All other training settings are the same as the experiments onFCVID and Mini-Kinetics. All the experiments on Kinetics-400 adopt the same aforementioned training configurations."
}