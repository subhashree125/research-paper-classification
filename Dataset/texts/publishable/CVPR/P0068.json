{
  "Fabio TosiLuca BartolomeiMatteo Poggi": "AbstractStereo matching is close to hitting a half-century of history, yet witnessed a rapid evolution in the last decade thanks todeep learning. While previous surveys in the late 2010s covered the first stage of this revolution, the last five years of research broughtfurther ground-breaking advancements to the field. This paper aims to fill this gap in a two-fold manner: first, we offer an in-depthexamination of the latest developments in deep stereo matching, focusing on the pioneering architectural designs and groundbreakingparadigms that have redefined the field in the 2020s; second, we present a thorough analysis of the critical challenges that haveemerged alongside these advances, providing a comprehensive taxonomy of these issues and exploring the state-of-the-arttechniques proposed to address them. By reviewing both the architectural innovations and the key challenges, we offer a holistic viewof deep stereo matching and highlight the specific areas that require further investigation. To accompany this survey, we maintain aregularly updated project page that catalogs papers on deep stereo matching in our Awesome-Deep-Stereo-Matching repository.",
  "INTRODUCTION": "Stereo matching the task of estimating dense disparitymaps from a pair of rectified images has been a funda-mental problem in computer vision for nearly half a century,playing a crucial role in a wide range of applications, such asautonomous driving, robotics, and augmented reality. Afterabout twenty-five years of hand-designed stereo algorithms, the use of end-to-end deep neural networks has becomethe dominant paradigm in the late 2010s.Existing surveys , have offered valuable insightson this rapid revolution, categorizing end-to-end architec-tures into 2D and 3D classes according to their cost-volumecomputation and optimization strategies, while also empha-sizing the challenges remaining open.Since then, however, the domain has progressed rapidly,with the emergence of novel methods and paradigms in-spired by breakthroughs in other areas of deep learning.Transformer-based and iterative refinement architec-tures are prime examples of how the field has evolved,demonstrating the potential for further improvements inaccuracy and efficiency.Despite the remarkable achievements, multiple chal-lenges have emerged as deep stereo matching has advanced.One of the most critical issues, already highlighted in pre-vious surveys, is the lack of generalization, particularlywhen facing domain shifts between synthetic and real data.Although synthetic datasets have been crucial for pre-training deep stereo networks, these models often performpoorly when applied to real scenes without fine-tuning.Recognizing the importance of this issue, several techniqueshave been proposed in recent years to improve zero-shotgeneralization and to develop domain adaptation methodsfor seamless adaptation to unknown target domains.",
  "F. Tosi, Luca Bartolomei and M. Poggi are with the Department ofComputer Science and Enegineering, University of Bologna, Italy, 40136": "Alongside the domain-shift problem, deep stereo match-ing has overcome several other limitations. The tendencyof deep networks to over-smooth depth at object bound-aries, leading to inaccurate 3D reconstructions, has beena persistent challenge. The increasing diversity of camerasetups and the need to process images with different resolu-tions has underscored the importance of developing flexiblealgorithms. The demand for high-resolution and highlydetailed disparity estimation, combined with the necessityfor real-time performance on resource-constrained devices,has further complicated the task. These challenges have ledto the development of more robust, versatile, and efficientdeep stereo models that effectively address these limitationsand improve their practical application.Furthermore, the integration of complementary imagingmodalities, including depth sensors, non-visible spectrumcameras, event cameras, and gated cameras, has openednew avenues for enhancing the robustness and accuracy ofstereo matching in challenging environments. By leveragingthe strengths and complementary nature of these modalities,multimodal stereo matching techniques aim to extend theapplicability and reliability of stereo vision to a broaderrange of real-world scenarios, overcoming the limitationsof relying solely on visible-spectrum cameras.Driven by these significant developments, this surveyprovides a thorough and up-to-date review of the latestadvancements in deep stereo matching that have emergedduring the 2020s. We cover a wide range of topics exploredsince the last surveys , , including novel architectures,multi-modal approaches, and cutting-edge techniques de-signed to address the aforementioned critical challenges.Our overview comprehensively analyzes over 100 distinctcontributions to deep stereo matching, all of which havebeen presented at top conferences and published in pres-tigious journals. Our goal is to offer a thorough overviewof the state-of-the-art, discussing the progress made sinceprevious surveys and highlighting the current trends and",
  "Background": "To better understand the latest trends in the field, weintroduce the fundamentals of deep stereo matching thathave been driving advancements until the late 2010s. In theinterest of space, we recommend referring to the supple-mentary material for a detailed overview of the backgroundinformation. This additional resource also provides detaileddescriptions of the datasets commonly used in this context,as well as the primary evaluation metrics employed in thisdomain. Finally, for a more comprehensive overview anddetailed descriptions of the body of research that arosebefore 2020, readers can refer to existing surveys related todeep stereo matching, such as , , , .",
  "Foundational Deep Stereo Architectures": "This section categorizes and analyzes classical deep stereomodels that emerged in recent years. These are grouped intofive categories: CNN-based cost volume aggregation (Sec.2.1.1), Neural Architecture Search (NAS) for stereo matching(Sec. 2.1.2), iterative optimization-based (Sec. 2.1.3), VisionTransformer-based (Sec. 2.1.4), and Markov random field-based architectures (Sec. 2.1.5). These represent the primarytrends and techniques that have significantly advanced thestate-of-the-art of stereo depth estimation. The taxonomydepicted in summarizes these categories.",
  "CNN-based Cost Volume Aggregation": "Here, we cover recent deep stereo methods using estab-lished CNN-based cost volume aggregation techniques ,, which broadly fall into two categories: 2D and 3Darchitectures, distinguished by their strategy for encodingfeatures and geometry. Both construct a cost volume fromthe left and right input images - 2D architectures typicallyemploy a correlation layer on the extracted features, while3D architectures concatenate or compute the feature dif-ference over the full disparity range. This cost volume isthen processed via plain 2D convolutions in 2D encoder-decoder architectures, or through 3D convolutions in 3Darchitectures that explicitly encode geometry.Among them, AANet [Code] aims to replace com-putationally expensive 3D convolutions while maintaining",
  ": A taxonomy of deep learning-based stereo matchingarchitectures in the 2020s. We categorize the reviewedmethods based on their key designs and paradigms": "high accuracy by introducing a sparse points based intra-scale cost aggregation method using deformable convolu-tions to address the edge-fattening issue at disparity dis-continuities, and an approximation of traditional cross-scalecost aggregation using neural network layers to handle largetextureless regions. In contrast, Bi3D [Code] formulatesstereo matching as a collection of binary classification tasks,training a neural network to classify whether an objectis closer or farther than a given depth plane, enablingselective depth estimation within a specific range of inter-est and offering a flexible trade-off between accuracy andcomputational efficiency. WaveletStereo takes a differentapproach by learning wavelet coefficients of the disparitymap instead of directly estimating disparity. It compriseslow-frequency and high-frequency sub-modules to handlesmooth and detailed regions, respectively, and reconstructsthe disparity map iteratively by mapping multi-resolutioncost volumes to wavelet coefficients through convolutionalnetworks and inverse wavelet transforms. CFNet [Code], on the other hand, combines a fused cost volume repre-sentation, which fuses multiple low-resolution cost volumesto capture global and structural information, and a cascadecost volume representation, which adaptively adjusts thedisparity search range at each stage using variance-baseduncertainty estimation. UASNet constructs cascade 3D cost volumes with improved disparity range predic-tion and effective sampling by introducing an uncertaintydistribution-guided range prediction (URP) module to han-dle ambiguities and an uncertainty-based disparity sam-pler (UDS) module that discretizes the per-pixel predicteddisparity range based on the matching uncertainty. PCW-Net [Code] employs a multi-scale cost volume fusionmodule to construct combination volumes on the upperlevels of the pyramid and integrate them for initial disparityestimation, covering multi-scale receptive fields and ex-tracting domain-invariant structural cues. It also introducesa warping volume-based disparity refinement module tonarrow down the residue searching range from the initialdisparity searching range to a fine-grained one. SEDNet[Code] focuses on joint disparity and uncertainty esti-mation by extending the stereo backbone GWCNet witha lightweight uncertainty estimation subnetwork. It matchesthe distribution of disparity errors with the distributionof uncertainty estimates using a KL divergence-based lossfunction enabled by a differentiable histogramming scheme,ensuring that the uncertainty estimates follow the samedistribution as the true errors of the disparity estimator.",
  "Neural Architecture Search for Stereo Matching": "Neural architecture search (NAS) aims to automate the de-sign of neural architectures, alleviating the manual effort re-quired by human experts. While NAS has achieved successin high-level vision tasks like classification and detection,applying it to the dense prediction problem of stereo match-ing is challenging. State-of-the-art deep stereo networks arecomputationally intensive, making the naive application oftraditional NAS methods prohibitively expensive due to thevast search space. To overcome this limitation, recent workshave proposed tailored NAS frameworks that incorporatetask-specific priors from the stereo matching pipeline. Thekey motivation is to enable efficient architecture searchfor this domain while maintaining high accuracy, withoutsolely relying on manually designed architectures. LEASt-ereo [Code] is the first end-to-end hierarchical NASframework for stereo that incorporates task-specific humanknowledge. It consists of a 2D feature net for local imagefeature extraction and a 3D matching net for computing andaggregating matching costs from a 4D feature volume. Thecell-level structure of these sub-networks is searched using anovel residual cell design and tailored candidate operations.At the network level, the search explores the arrangementof cells and the size of intermediate feature maps and vol-umes. This hierarchical approach, which embeds geometricpriors, leads to accurate stereo matching models withoutrelying on human-designed architectures. Building uponLEAStereo, EASNet [Code] addresses its limitations,such as high computational costs and limited scalability,by introducing a stereo network that can be deployed ondevices with varying computing resources. The architecturesearch space covers depth, width, kernel size, and scale,allowing the network to adapt to different resource con-straints. A progressive shrinking training strategy enablesefficient optimization, allowing sub-networks with diverseconfigurations to be extracted without additional trainingwhile maintaining high accuracy. 2.1.3Iterative Optimization-based ArchitecturesIterative optimization-based approaches have emerged asa promising paradigm for stereo matching, offering signifi-cant advantages over existing deep cost aggregation meth-ods. Inspired by the success of iterative refinement in opticalflow estimation, particularly the RAFT architecture ,these methods have been adapted to stereo matching withimpressive results. By bypassing explicit cost aggregationand instead iteratively updating a disparity estimate usinga high-resolution cost volume, these methods address keylimitations of conventional approaches. Specifically, itera-tive stereo enables the direct use of high-resolution costvolumes without the computational burden of 3D convolu-tions, making it applicable to high-resolution images. More-over, by avoiding a predefined disparity range, it can handlea wide range of disparities without sacrificing accuracy orefficiency. In fact, the recurrent nature of the update operatorallows for a flexible trade-off between accuracy and speedthrough a variable number of iterations.RAFT-Stereo [Code] , a deep stereo architecture in-spired by the optical flow network RAFT , has been agame-changer for recent stereo solutions. By introducingnovel principles, illustrated in , such as lightweightcost volumes, and iterative refinement, RAFT-Stereo haspaved the way for more efficient and effective processingof high-resolution stereo pairs. The architecture consistsof three main components: a feature extractor, a correla-tion pyramid, and a Convolutional Gated Recurrent Units(ConvGRU)-based update operator. The feature extractorcomprises a feature encoder, applied to both images togenerate dense feature maps, and a context encoder, appliedonly to the left image to initialize the update operatorand inject context features. The correlation pyramid is con-structed without predefining the range of disparities bycomputing the visual similarity between feature vectors,restricting the computation to pixels sharing the same y-coordinate, resulting in a 3D cost volume:",
  "hfijh gikh,C RHW W(1)": "Here, C represents the cost volume, f and g are the densefeature maps extracted from the left and right images, re-spectively. This lightweight cost volume is computed usinga single matrix multiplication, making it computationallyefficient. The update operator iteratively retrieves localcost values from All-Pairs Correlations (APC) and updatesthe disparity field using multi-resolution ConvGRUs withcross-connections. The multi-level GRUs efficiently prop-agate information across the image, improving the globalconsistency of the disparity field. This iterative refinementprocess allows for a trade-off between accuracy and effi-ciency through early stopping. The final disparity is upsam-pled to the original resolution using a convex upsamplingmethod. Building upon the principles introduced by RAFT-Stereo, several methods have proposed enhancements andadaptations to address specific challenges. Among them,ORStereo addresses the challenge of training stereomatching models on high-resolution images with largedisparity ranges by proposing a two-phase approach. Itestimates a down-sampled disparity map and occlusionmask in the first phase, followed by patch-wise refinement : RAFT-Stereo architecture. It constructs a correlation pyramid from correlation features (blue) extracted from eachimage. A context encoder extracts context image features (white) and an initial hidden state. The disparity field startsat zero. In each iteration, the GRU(s) (green) sample from the correlation pyramid using the current disparity estimate.Sampled correlation features, initial image features, and current hidden state(s) are processed by the GRU(s) to update thehidden state and disparity estimate. Picture from . using a recurrent residual updater (RRU) and a normalizedlocal refinement (NLR) module in the second phase. TheRRU recurrently updates the disparity and occlusion pre-dictions, while the NLR employs normalization techniquesto generalize to unseen disparity ranges. Building uponthe principles of iterative refinement, CREStereo [Code] introduces a hierarchical network with recurrent re-finement and an adaptive group correlation layer (AGCL)for robust matching. The AGCL computes correlations inlocal search windows, reducing memory and computationrequirements, and incorporates deformable search windowsand group-wise correlations. In contrast, EAI-Stereo [Code] focuses on error correction by incorporating an error-aware refinement module that combines left-right warpingwith learning-based upsampling, allowing the network tolearn error correction capabilities. IGEV-Stereo [Code] takes a different approach by building a Combined Ge-ometry Encoding Volume (CGEV) that encodes geometry,context information, and local matching details. It combinesa Geometry Encoding Volume (GEV), obtained throughlightweight 3D regularization of the cost volume, with All-Pairs Correlations to provide both non-local geometry andlocal matching details. DLNR [Code] , on the otherhand, focuses on preserving high-frequency informationduring the iterative process by incorporating a DecoupleLSTM module, a Normalization Refinement module, anda multi-scale, multi-stage Channel-Attention Transformerfeature extractor. CREStereo++ extends CREStereo byintroducing an uncertainty guided adaptive correlation(UGAC) module to enhance robustness and accuracy. It em-ploys a content-aware warping layer with learnable offsetsguided by variance-based uncertainty estimation, adjustingthe sampling range when building the cost volume to cap-ture more potential correspondences in ill-posed regions.Selective-Stereo [Code] addresses the fixed receptivefield limitation by proposing a Selective Recurrent Unit(SRU) with multiple GRU branches to adaptively captureand fuse multi-frequency information, and a ContextualSpatial Attention (CSA) module to guide the adaptive fusion based on image regions. Any-Stereo [Code] focuses onmaintaining the regularized disparity at a higher resolutionby modeling the disparity as a continuous representationover 2D spatial coordinates using an Implicit NeighborMask Function (INMF). It proposes Intra-scale SimilarityUnfolding (ISU) and Cross-scale Feature Alignment (CFA)strategies to complement missing information and details inthe latent code. XR-Stereo [Code] , instead, presents anarchitecture that leverages temporal information in videostreams. It employs a RAFT-style scheme, where the keyidea is to warp the previous frames disparity and hiddenstate to the current frame using camera poses, serving as awarm-start for the GRU.MC-Stereo [Code] addresses the limitations of ex-isting approaches in handling multi-peak distribution andfixed search range by introducing a multi-peak lookupstrategy and a cascade search range during the iterative pro-cess. MoCha-Stereo[Code] tackles the loss of geomet-ric edge details by capturing repeated geometric contoursacross channels as motif channels that preserve commongeometric structures. It employs a Motif Channel Attention(MCA) mechanism, a Motif Channel Correlation Volume(MCCV), and a Reconstruction Error Motif Penalty (REMP)module to recover lost geometric edge details through aniterative process. Finally, ICGNet[Code] builds overexisting architectures such as IGEV-Stereo and introducesadditional intra-view and cross-view geometry constraints,by extracting keypoints with an off-the-shelf detector andenforcing the image features extractor to predict the verysame keypoints (intra-view), as well as by forcing the verycorrespondences between left and right image features asthose occurring between left and right keypoints processedby an off-the-shelf matcher (cross-view).",
  "Vision Transformer-based Architectures": "In recent years, Vision Transformers (ViT) have emergedas a promising alternative to CNNs for various computervision tasks, including stereo matching. Transformers, orig-inally developed for natural language processing , have demonstrated remarkable performance in capturing long-range dependencies and global context information. Thesemethods move away from traditional cost-volume construc-tion and instead formulate stereo matching as a sequence-to-sequence problem, utilizing attention mechanisms to es-tablish correspondences between pixels in the left and rightimages. Key components of vision transformer-based stereomatching include self-attention, cross-attention, and posi-tional encoding schemes that provide spatial cues.One of the pioneering works in this area is the STereoTRansformer (STTR) [Code] , which introduces a Trans-former architecture that, unlike traditional cost-volume con-struction, relaxes the fixed disparity range constraint andalternates between self-attention along epipolar lines andcross-attention between left-right pairs. This approach al-lows the network to capture long-range pixel associationsand resolve ambiguities in matching regions. Moreover,the architecture incorporates a relative positional encodingscheme to provide discriminative spatial cues and enforcesa uniqueness constraint during matching, ensuring one-to-one correspondence between pixels across the stereo pair.Building upon STTR, subsequent works have focusedon enhancing the contextual information and refining thedisparity estimation process. CEST [Code] introducesthe Context Enhanced Path (CEP), a plug-in module thatextracts long-range context information from low-resolutionfeatures. By following a coarse-to-fine approach and em-ploying attention masking, optimal transport for uniquenessconstraints, and post-processing modules like upsamplingand context adjustment, CEST achieves improved dispar-ity map refinement. Similarly, ChiTransformer [Code] proposes a self-supervised method with an encoder-decodertransformer architecture, utilizing parallel ViT streams anddepth cue rectification blocks to fuse binocular cues and pro-duce depth estimates. While these methods focus on staticstereo matching, Dynamic-Stereo [Code] tackles thechallenge of temporally consistent disparity estimation fromstereo videos. By introducing a hybrid encoder-decodernetwork that combines transformer-based attention acrossspace, view, and time with an iterative refinement approachsimilar to RAFT, Dynamic-Stereo achieves good results incapturing dynamic scenarios.Some works have also explored transformers for uni-fying various dense correspondence tasks. For instance,GMStereo [Code] presents a unified model for opticalflow, rectified and unrectified stereo matching tasks, formu-lating them as a dense correspondence matching problem.By exploiting cross-attention in a Transformer-based modeland employing parameter-free task-specific matching layers(2D, 1D, or another form) to obtain dense correspondences,GMStereo demonstrates the versatility and effectiveness oftransformers in handling multiple related tasks.Pre-training has also emerged as a promising approachto further improve the performance of ViT-based stereomatching models. CroCo-Stereo [Code] introduces alarge-scale pre-training approach that leverages a cross-viewcompletion pretext task to learn robust representations. Thepre-trained model, CroCo v2, utilizes a ViT encoder anddecoder architecture with self-attention and cross-attentionmechanisms, and is then finetuned for stereo matchingusing a Dense Prediction Transformer (DPT) head, which di- rectly processes the encoder and decoder features to predictthe final disparity, in contrast to existing methods relying oncost volumes or iterative refinement.Recognizingthecomplementarystrengthsofcost-volume-based and transformer-based approaches, ELFNet[Code] proposes a novel framework that fuses the twoparadigms. By employing heads in each branch to estimatealeatoric and epistemic uncertainties using deep eviden-tial learning, and introducing a two-stage fusion strategy,ELFNet effectively integrates local and global informationbased on the estimated uncertainties.The progressive refinement of disparity estimates inoccluded regions is another crucial aspect addressed byViT-based models. GOAT [Code] introduces a paralleldisparity and occlusion estimation module (PDO) and an it-erative occlusion-aware global aggregation module (OGA).By exploiting restricted global spatial correlation within afocus scope guided by the occlusion mask, GOAT achievesrobust disparity refinement in occluded regions. 2.1.5Markov Random Field-based ArchitecturesMarkov Random Fields (MRFs) have been widely used intraditional stereo matching methods before the advent ofdeep learning, thanks to their ability to reduce matchingambiguities in challenging regions by enforcing spatial co-herence and smoothness constraints on the disparity map.However, traditional MRF models rely on hand-crafted po-tential functions and message passing procedures, whichoften lead to suboptimal results. Recently, a new line ofresearch has emerged that aims to leverage the power ofdeep learning to create fully data-driven MRF models forstereo. The first attempt in this direction is the NeuralMarkov Random Field (NMRF) model [Code] . Specifi-cally, NMRF introduces a novel fully data-driven approachthat consists of a local feature CNN, a Disparity ProposalNetwork (DPN), a Neural MRF inference stage, and adisparity refinement stage. The local feature CNN extractsmulti-level features from the stereo pair, while the DPNprunes the search space by identifying the top k disparitymodals for each pixel through neural message passing. Theinference stage performs coarse-level disparity estimationusing a probabilistic graphical model with self and neighboredges, where the potential functions and message passingare learned through neural networks. Finally, the disparityrefinement stage further improves the coarse estimates. Themodel is trained using a combination of proposal loss,initialization loss, and disparity loss, which measures thediscrepancy between candidate labels, cost volume, andthe ground truth disparity. By learning complex pixel re-lationships and potential functions directly from data whileretaining the graph inductive bias of MRFs, NMRF opensup new possibilities for combining the strengths of bothtraditional MRFs and deep learning in stereo matching.",
  "Efficiency-Oriented Architectures": "While deep learning has enabled dramatic accuracy im-provements over traditional stereo methodologies by train-ing convolutional neural networks to learn powerful featurerepresentations and cost volume filtering for better match-ing, many deep stereo networks are too computationally intensive for real-time operation, especially on embeddedor mobile devices with strict power, memory, and compu-tational constraints. In such cases, there is an importantneed to develop efficient deep neural architectures that canprovide the accuracy benefits of learned approaches whileoperating at real-time speeds with low latency on low-power edge devices. 2.2.1Compact Cost Volume RepresentationsEfficient stereo matching often requires compact cost vol-ume representations to reduce memory and computationaldemands. Various approaches have been proposed to extractessential matching information while suppressing redun-dant data, enabling faster processing without significantaccuracy degradation.Among these frameworks, Fast DS-CS [Code] , un-like most existing stereo networks using expensive learnedmatching costs and 3D convolutions, uses traditional effi-cient matching costs to construct initial cost volumes andlearns a mapping to convert the cost information at eachpixel into a low-dimensional cost signature feature vector,which is then processed using an encoder-decoder networkwith 2D convolutions. In contrast, DecNet [Code] pro-poses a decomposition model that reduces the computa-tional cost growth as resolution increases by decomposingstereo matching into a dense matching at very low resolu-tion and a series of sparse matchings at higher resolutions,enabling logarithmic complexity growth. ACVNet [Code] introduces an attention-based filtering approach, lever-aging correlation clues and a multi-level adaptive patchmatching mechanism to generate the Attention Concatena-tion Volume (ACV). PCVNet [Code] , instead, proposesa parameterized cost volume representation that encodesthe entire disparity space using a multi-Gaussian distri-bution, allowing for a global view of the disparity spaceand progressively focusing on local regions for fine-grainedmatching. IINet aims to address the redundancy inexplicit 3D cost volumes by proposing a compact 2D im-plicit network, incorporating intra-image context informa-tion, Fast Multi-scale Score Volume (FMSV), ConfidenceBased Filtering (CBF), and Intra-Inter Fusing (IIF) networkwith Residual Context-aware Upsamplers (RCUs) for en-hanced information transmission. These methods employdifferent strategies, such as cost signatures, dense-sparsedecomposition, attention-based filtering, parameterized rep-resentations, and implicit networks, to achieve compact yetinformative cost volume representations. 2.2.2Efficient Cost Volume ProcessingProcessing high-dimensional cost volumes can be a majorbottleneck in real-time stereo matching. To address this, var-ious techniques have been developed to efficiently processcost volumes, such as cascaded architectures, edge-awareupsampling, and multi-scale feature extraction.Accordingly, CasStereo [Code] addresses this byproposing a cascade cost volume formulation, construct-ing a cost volume using a feature pyramid and iterativelynarrowing down the depth range at each stage, recoveringthe output in a coarse-to-fine manner. In contrast, BGNet[Code] focuses on edge-preserving cost volume up-sampling using a learned bilateral grid, enabling efficient upsampling while preserving sharp edges and allowingcomputationally expensive operations to be performed atlower resolutions without compromising accuracy. MAB-Net [Code] introduces the Multibranch Adjustable Bot-tleneck (MAB) module for multi-level feature extraction,with the 2D MAB module having three branches with dif-ferent dilation rates and the 3D MAB module factorizing 3Dconvolutions into disparity-wise and spatial convolutions,aiming for high accuracy with a small model size. Tem-poralStereo [Code] , instead, employs a coarse-to-finearchitecture with sparse cost volumes, enriching them withmulti-level context, statistical fusion for robust, global costaggregation, and an adaptive shifting strategy that adjuststhe disparity candidates based on the aggregated costs.Moreover, it can seamlessly operate in both single stereopair and temporal modes, leveraging past information toboost matching accuracy with a negligible runtime increase. 2.2.3Compact ArchitecturesComputing the cost volume is the primary contributor tocomputational complexity in end-to-end networks, but notthe only one. Thus, designing an architecture that is compactin any part can further decrease complexity.For instance, StereoVAE proposes a computationallyefficient and lightweight framework combining a traditionalmatching algorithm and a variational autoencoder (VAE)based neural network. Initially, the former generates acoarse disparity map from a low-resolution stereo imagepair, leveraging its low computational complexity. Subse-quently, a tiny VAE-based network upscales and refines thecoarse disparity map, taking the coarse map and the leftimage as inputs.MobileStereoNet [Code] proposes a 3D and a 2Dlightweight stereo models to pursue efficiency. The for-mer builds upon GwcNet-g , while the latter uses 2Dencoder-decoder to process a 3D cost volume, which isconstructed efficiently using a new Interlacing Cost Vol-ume module. Both models replace various componentswith 2D and 3D versions of MobileNet-V1 and MobileNet-V2 convolution blocks to further reduce complexity.Bringing lightweight design to the extreme, PBCStereo is a fully binarized deep stereo network. To mitigateaccuracy degradation due to quantization, PBCStereo intro-duces the Interpolation+Binary Convolution (IBC) moduleto replace binary deconvolutions and fuse shallow/deepfeatures, and proposes the Binary Input Layer (BIL) codingmethod to binarize inputs while preserving pixel preci-sion. PBCStereo incorporates IBC modules for any of fea-ture extraction, cost aggregation, and disparity refinementstages. FADNet [Code] focuses on efficient 2D-basedcorrelation layers with stacked residual blocks. It employs aDispNetC-based backbone extensively reformed with resid-ual blocks, point-wise correlation layers, and multi-scaleresidual learning, adopting a coarse-to-fine training strategywith a loss weight scheduling technique. HITNet [Code] avoids building a full 3D cost volume and uses fastmulti-resolution initialization. The disparity map is rep-resented as planar tiles with learnable feature descriptorsat multiple resolutions, and the network iteratively refinesdisparity hypotheses via differentiable 2D geometric propa-gation and warping, reasoning about slanted plane hypothe- ses for accurate warping and upsampling in coarse-to-finemanner. CoEX [Code] , instead, pursues efficiency andaccuracy through the Guided Cost volume Excitation (GCE)module, which utilizes extracted image features from theMobileNetV2 backbone to excite relevant channels inthe cost volume, improving feature extraction without sig-nificant computational overhead. CoEX also introduces top-k selection before soft-argmin disparity regression, comput-ing the final disparity estimate using only the top-k match-ing cost values. AAFS [Code] combines an efficientbackbone made of depthwise separable convolutions, anattention-aware feature aggregation (ACCV) module, anda cascaded 3D CNN architecture to achieve real-time stereoon edge devices. The ACCV module adaptively aggregatesinformation from different scales of the feature maps, en-hancing their representational capacity, while the cascaded3D CNN architecture regularizes cost volumes at multiple incoarse-to-fine manner. Lastly, MADNet 2 [Code] revisesMADNet with the all-pairs correlation module fromRAFT-Stereo, while removing its original context network.Combined with refined augmentation techniques, thesemodifications enable MADNet 2 to significantly outperformits predecessor while preserving computational efficiency.",
  "In this section, we focus on models combining stereo match-ing with other dense tasks within a multi-task framework": "2.3.1Semantic Stereo MatchingIntegrating semantic information into stereo matching hasthe potential to significantly improve the performance ofdepth estimation algorithms. By exploiting high-level se-mantic cues, these methods can better handle challengingscenarios such as textureless regions, occlusions, and depthdiscontinuities. Despite the many attempts , , , reported in previous surveys , only few furtherworks have emerged in the 20s, specifically targeting real-time stereo matching with semantic guidance. Among them,RTS2Net proposes a lightweight, real-time architec-ture for joint semantic segmentation and disparity esti-mation. Key aspects include a multi-stage, coarse-to-finepyramidal decoder design for trading accuracy and speed,a shared encoder extracting common features for both tasks,separate semantic and disparity decoders operating on theshared features, and a synergy refinement module thatexploits the complementarity between semantics and depth.SGNet , instead, built upon PSMNet , introducesthree modules to embed semantic constraints: a confidencemodule computing the consistency between disparity andsemantic features, a residual module optimizing the initialdisparity map based on semantic categories, and a loss mod-ule supervising disparity smoothness based on semanticboundaries and regions. 2.3.2Normal-Assisted Stereo MatchingThese methods aim to exploit the geometric constraintsbetween depth and surface normals to regularize the depthestimation process and enhance the overall performance.One notable example of this approach is HITNet , whichachieves high accuracy by geometrically reasoning about disparities and inferring slanted plane hypotheses. Buildingupon a similar concept, NA-Stereo was proposed forgeneric multi-view stereo that can also be applied to theclassic binocular stereo matching task. The key idea is toincorporate a normal estimation network (NNet) into thedepth estimation framework, jointly optimizing for bothdepth and normals through a consistency loss. The methodconstructs a 3D cost volume by plane sweeping and ac-cumulates multi-view image information, which is thenregularized by NNet that predicts surface normals from theaccumulated features.",
  "Joint Stereo And Optical Flow": "This family of frameworks processes stereo videos by jointlyestimating three elements for each pair of consecutive leftimages: the disparity map for that image pair, the opti-cal flow between the two left images, and optionally, thechange in disparity between them. Predicting these threecues together allows for estimating the 3D scene flow ofthe observed scene . It is worth mentioning that, amongthe literature, several works directly estimate 3D scene flowstarting from point clouds or pre-computed disparity maps, . Here, we survey only those approaches estimatingdisparity maps as well. Among these, DWARF [Code] proposed a compact coarse-to-fine architecture estimatingdisparity, flow, and disparity change at multiple scales, bybuilding compact correlation volumes exploiting featureswarping across the scales. Specifically, a 3D correlation layeris deployed across the correlation scores computed betweentwo consecutive stereo pairs. On the contrary, Effiscene tackles scene flow estimation in an unsupervised manner, bydecomposing the task into predicting stereo depth, opticalflow, camera poses, and motion segmentation. The fouringredients combined together allow for estimating the 3Dscene flow of the scene in the absence of any annotation.Finally, Feature-Level Collaboration follows the sametrack and proposes an architecture for joint estimation ofstereo depth and optical flow, followed by a pose decoderestimating the camera ego-motion from the previous twopredictions.",
  "Depth-Guided Sensor Stereo Matching": "Guided stereo matching aims to enhance the accuracy androbustness of stereo networks by leveraging sparse depthcues from external sensors, such as LiDAR. These depthhints, independent of visual appearance, help to overcomelimitations in challenging scenarios like textureless regionsand mitigate the impact of domain shift.Pseudo-LiDAR++ adapts a stereo network to di-rectly estimate depth by constructing a depth cost volume and refines the estimates using a graph-based depth cor-rection algorithm that propagates sparse LiDAR measure-ments. In contrast, LiStereo employs a two-brancharchitecture, with a color image branch for stereo featureextraction and a LiDAR branch for processing sparse depthmaps, which are then fused to output dense depth maps.The Sparse Signal Superdensity (S3) framework,instead, addresses the challenges of low density and imbal-anced distribution of sparse depth cues by expanding thedepth estimates around sparse signals based on the RGBimage and controlling their influence through confidenceweighting. LSMD-Net fuses LiDAR and stereo infor-mation using a dual-branch disparity predictor, where theoutputs from these branches are fused at each pixel using amixture density module that models the estimated disparityat each pixel as a probability distribution using Laplaciandistributions. The final disparity map is then determined bythe expectation of the more confident branch.Differently from the previous strategies, VPP-Stereo[Code] uses depth measurements from an active sensorto hallucinate patterns onto the stereo images, thus simplify-ing visual correspondence. Various patterning strategies areproposed to improve performance. The hallucinated stereopair is processed by any stereo matching algorithm (eitherlearned or hand-crafted) to obtain dense depth estimations.Lastly, SDG-Depth [Code] incorporates a sparse Li-DAR deformable propagation module to generate a Semi-Dense hint Guidance map and a confidence map, whichguide cost aggregation in stereo matching. Additionally,SDG-Depth introduces a learned disparity-depth conversionmodule to reduce triangulation errors.",
  "Event-Camera-Based Stereo Matching": "Event cameras , also known as neuromorphic cameras,are peculiar sensors designed to mitigate the shortcomingsof conventional imaging devices, such as limited dynamicrange, sensor noise, and motion blur caused by rapid move-ments. In contrast to color cameras, which capture framesat fixed intervals, event cameras operate asynchronously.As will be discussed, the majority of these methods adaptsuccessful models from traditional stereo literature tothis novel paradigm, primarily by developing appropriatetechniques for handling asynchronous event streams.Specifically, given a timestamp td at which disparity mapestimation is desired, events are usually sampled backwardin time from the stream, either based on a time interval (SBT)or a maximum number of events (SBN), and processed toobtain tensors ready to be processed by standard CNNs.The pioneering effort dates back to the 2010s: DDES[Code] ,inspired by the previous successes in theclassical stereo literature . Events are organized inqueues and processed by temporal fully connected layers,implemented as MLPs to aggregate the information encodedalong time by the events, and used to build a 4D costvolume, optimized with a 3D UNet inspired by GC-Net.More recent works delved into strategies for codifyingthe events streams: SE-CFF [Code] aims at solving theloss of meaningful events due to overriding, by proposinga ConcentrationNet for aggregating the events streams into a compact representation preserving fine details. Further-more, SE-CFF distills knowledge from future events duringtraining to compensate for the missing information in pastevents. SE-CFF builds a correlation-based cost volume andprocesses it with dilated 2D convolutions and a 2DUNet to get the final prediction. Given the many eventssampled from streams, SCSNet implements a differ-entiable mechanism for retaining only those relevant forestimating disparity at the desired timeframe. Furthermore,a Neighbor Cross Similarity Feature (NCSF) extraction mod-ule is designed to encode the similarity across the differentmodalities images and events. SCSNet uses group-wisecorrelation layers and 3D convolutions in its backbone.In contrast, DTC-SPADE samples events according toSBT and stores them in voxel grids, that are processed byDiscrete-Time Convolution (DTC) modules recurrent oper-ators designed to embed data in a variety of temporal scales.Furthermore, spatially adaptive denormalization modules(SPADE ) are used to complement the sparse voxel gridembeddings with edge information from the original rawevents. Following DDES , DTC-SPADE deploys a GC-Net-like backbone to predict a dense disparity map.Peculiarly, ADES focuses on domain adaptation of anevent-stereo model, either based on AANet or PSM-Net , trained on the pseudo-events frame generated byEventGAN from color images. During adaptation onreal events streams, these are used to generate pseudo-colorimages and compute a smudge-aware self-supervisedloss. Furthermore, a Motion-invariant Consistency moduleis designed to perturb events and simulate slower or fastermotion and then impose a consistency loss between the dis-parity maps obtained from perturbed and original inputs.Eventually, some frameworks emerged to process bothcolor and events. EI-Stereo [Code] has been proposedto match pairs of event and color images to exploit thebest of the two worlds. An event-intensity recycling net-work iteratively processes each image with a single channelof the corresponding event stack from the most recentto the oldest while keeping a hidden state propagatedthrough the channels. Then, an AANet-like architecture is deployed for estimating the disparity map. On the sametrack, EFS deploys distinct feature extractors for imagesand events. The features are fused across the modalities andused to build a correlation-based, multi-scale cost volume,that passes through a 3D aggregation network to estimate adense disparity map. An additional cost volume betweenevent features is built at training time, and processed toobtain a sparse disparity map.Lastly, SAFE estimates disparity from asymmetricinputs a color image and an event stream by divid-ing the task into three sub-problems: i) asymmetric color-event matching, ii) color Structure-from-Motion (SfM), andiii) event SfM. Two feature extractors process images andevents, and then three cost volumes are built, i) betweenimage and events, ii) time-adjacent images, and iii) time-adjacent events, and combined in a single volume, to beprocessed by a fusion module deploying 3D ConvLSTMs.",
  "Gated Stereo MatchingAdverse weather conditions such as fog and snow cansignificantly affect the performance of conventional color": "cameras. Gated cameras are designed to be robustagainst these conditions. During acquisition, an illuminatoremits light to flood a specific range of the scene in front ofit. Multiple acquisitions are performed by illuminating pre-determined distances (slices) each one not interfering withthe others and then accumulated in a single frame. GatedStereo utilizes a synchronized wide-baseline gatedstereo camera and deploys a network made of monocularand stereo branches, exploiting both time-of-flight intensitycues and multi-view cues, respectively. These branches arecombined through a fusion network to produce the finaldepth map. The framework is trained in a semi-supervisedmanner, with a novel ambient-aware and illuminator-awareself-supervised loss on gated images. 2.4.4Pattern Projection-BasedPattern projection-based approaches for active stereo vi-sion have gained significant attention in recent years dueto their ability to enhance depth estimation accuracy androbustness in challenging scenarios. These methods employa projector to cast a structured light pattern onto the scene,providing additional cues for stereo matching. By exploit-ing the known pattern geometry and the correspondencebetween the projected pattern and the captured images,pattern projection-based techniques can effectively handletextureless regions, and other ambiguities that often posedifficulties for traditional passive matching algorithms. Theprojected patterns are typically designed to have high spa-tial frequency and good uniqueness properties to facilitateaccurate and efficient stereo matching.In the 10s, ActiveStereoNet appeared as the pi-oneering work in end-to-end learning for active stereosystems, introducing a novel reconstruction loss based onlocal contrast normalization (LCN) to address the challengesof illumination variations and textureless regions. Further-more, ActiveStereoNet proposes an invalidation networkto explicitly handle occlusions, which is trained end-to-end without ground-truth data. Building upon this foun-dation, In the 20s, Polka Lines takes a step furtherby jointly learning the structured illumination pattern andthe reconstruction algorithm, parameterized by a diffractiveoptical element (DOE) and a neural network, respectively.This joint optimization approach enables the learned PolkaLines patterns to be tailored to the reconstruction network,achieving high-quality depth estimates across various con-ditions. Activezero addresses the lack of real-worlddepth annotations by proposing a mixed-domain learningframework that combines supervised learning on syntheticdata with self-supervised learning on real data. It introducesa novel temporal IR reprojection loss that is more robustto noise and textureless patches and invariant to illumi-nation changes. Despite the similarities with Polka Lines,it focuses on improving generalization. MonoStereoFusion takes a different path by integrating a monocular struc-tured light subsystem and a binocular stereo subsystem toleverage their complementary advantages. The monocularsubsystem handles textureless regions, while the binocularsubsystem is used for distant objects and outdoor scenes.In contrast to other methods, MonoStereoFusion introducesan IR camera without a narrow-band filter, allowing it toreceive both visible and IR light simultaneously. Lastly, ActiveZero++ extends the ActiveZero framework byincorporating an illumination-invariant feature matchingmodule and a confidence-based depth completion module.The illumination-invariant feature matching module uses aself-attention mechanism to learn robust features that areinsensitive to illumination variations. The confidence-baseddepth completion module uses the confidence from thestereo network to identify and improve erroneous areas indepth prediction through depth-normal consistency. 2.4.5Cross-Spectral Stereo NetworksCross-spectral stereo matching has emerged as a promisingapproach for estimating depth by finding correspondencesbetween images captured in different spectral bands, such asvisible (RGB) and near-infrared (NIR), short-wave infrared(SWIR), or thermal infrared (TIR). Indeed, in many real-world scenarios, RGB stereo matching often struggles to findreliable correspondences e.g., in low-light conditions, fog,etc. By leveraging the distinct appearance characteristics ofmaterials and objects across different spectral bands, cross-spectral stereo aims to overcome these limitations. Thiscomes with unique challenges, such as the significant ap-pearance differences between images from different spectralbands or the scarcity of annotated, cross-spectral datasets.Before the 20s, CS-Stereo processed RGB and NIRimages without depth supervision. It simultaneously esti-mates disparity and translates the RGB image to a pseudo-NIR image using a disparity prediction network and aspectral translation network, incorporating a material recog-nition network to handle unreliable matching regions. Sim-ilarly, UCSS employed a spectral translation networkbased on F-cycleGAN to minimize appearance variationsbetween cross-spectral images and a stereo matching net-work to estimate disparity, enabling iterative end-to-endunsupervised learning. SS-MCE takes a different ap-proach, estimating dense flow fields between images ofdifferent spectra without requiring ground truth. It employsa dual-spectrum siamese-like structure with two flow esti-mation modules, each estimating the flow field to align oneimage with the other and then warp it back, demonstratingits versatility across different spectral combinations. RGB-MS focuses on registering RGB and MS images withdifferent resolutions using a self-supervised deep archi-tecture consisting of coarse and fine-grained sub-modules,employing a proxy label distillation strategy for supervision.DPS-Net , instead, is an end-to-end network for polari-metric stereo depth estimation that leverages multi-domainsimilarity and geometric constraints, constructing RGB andpolarization correlation volumes, introducing an iso-depthcost to handle polarization ambiguities, and employing acascaded dual-GRU architecture to recurrently update thedisparity. Finally, Gated-RCCB is an approach thatfuses high-resolution RCCB and gated NIR cameras to es-timate dense, accurate depth maps. It exploits active andpassive signals across visible and NIR spectra, proposinga cross-spectral stereo network with a fusion module toeffectively integrate features from both modalities.",
  "Differences in stereo camera setups, such as baselinedistance, focal length, and sensor properties": "Variations in the disparity range due to different depthsof the scenes (e.g., indoor vs. outdoor environments).These differences can make stereo networks learn domain-specific features or shortcuts that do not transfer well tonovel scenarios. To address this challenge, several tech-niques have been explored lately. 3.1.1Zero-Shot GeneralizationZero-shot generalization refers to the capability of a stereonetwork to generalize from one domain (e.g., syntheticdata) to a completely different domain (e.g., real-worldscenes) without the need for fine-tuning or adaptation. Thisis particularly desirable when collecting stereo images orground truth data for the target domain is expensive orinfeasible. It is worth mentioning that the recent iterativemodels reviewed in Sec. 2.1.3 excel at this , despite notimplementing any strategy specific for this purpose.",
  "a)Domain-Agnostic Feature Modeling": "These methods focus on various aspects, such as featureregularization, feature consistency, shortcut avoidance, andrepresentation learning, each tackling the problem froma unique perspective while sharing the common goal oflearning robust, domain-invariant representations. DSMNet[Code] and FCStereo [Code] both aim to learndomain-invariant features, but they differ in their approach.DSMNet introduces a Domain Normalization (DN) layerto regularize the distribution of learned features acrossspatial and channel dimensions, reducing sensitivity toimage-level style variations and local contrast differences between domains. In contrast, FCStereo focuses on explicitlyencouraging feature consistency between matching pixelsfrom left and right views through two loss functions: thestereo contrastive feature (SCF) loss and the stereo selec-tive whitening (SSW) loss. While DSMNet emphasizes fea-ture regularization, FCStereo prioritizes feature consistencyacross domains. GraftNet [Code] takes a different pathby leveraging broad-spectrum features from a model pre-trained on large-scale datasets. It grafts these features tothe cost aggregation module of an existing stereo networkand uses a shallow network to restore task-related informa-tion. In comparison to DSMNet and FCStereo, which learndomain-invariant features from scratch, GraftNet exploitsexisting knowledge from pre-trained models to improvegeneralization. ITSA [Code] and HVT [Code] both address the issue of shortcut learning, where net-works exploit spurious correlations or superficial cues insynthetic training data rather than learning transferablerepresentations. On the one hand, ITSA uses information-theoretic losses to automatically restrict the encoding ofshortcut-related information into feature representations bycombining the task loss with an approximation of the Fisherinformation loss. On the other hand, HVT emphasizes dataaugmentation, by transforming synthetic training imageshierarchically at global, local, and pixel levels to diversifythe training domain and prevent the model from learningdataset-dependent shortcuts. MRL-Stereo , instead, in-troduces an approach to learning domain-invariant repre-sentations using masked representation learning. By feedinga masked left image and a complete right image into themodel and reconstructing the original left image, MRL-Stereo encourages the learning of structural and domain-invariant features.",
  "b)Non-parametric Cost Volumes": "Non-parametric cost volume construction methods buildcost volumes using conventional, domain-agnostic match-ing functions rather than relying on learned featuresthat may be sensitive to domain-specific characteristics.Matching-Space Networks (MS-Nets) [Code] , for ex-ample, move the learning process from the color spaceto the Matching Space by replacing learning-based featureextraction with four conventional matching functions (NCC,ZSAD, CENSUS, and SOBEL) and their associated confi-dence scores. These functions and scores are combined togenerate a 4D matching volume, which is then regularizedusing adapted versions of popular architectures like GCNet and PSMNet . Similarly, ARStereo [Code] leverages Census Transform binary patterns as the matchingfeatures for building cost volumes, together with a backboneapplied to extract high-level semantic contextual featuresfrom the reference image alone, avoiding the vulnerabilitiesof matching at features level. Then, this hybrid cost volumeis regularized through the remaining layers.",
  "image Graft-PSMNet ITSA-PSMNet LSSI-PSMNet NS-PSMNet": ": Qualitative comparison PSMNet variants. From left to right: reference image, disparity maps predicted bynetworks trained on synthetic data with ground-truth (Graft-PSMNet, ITSA-PSMNet) or on real data without any ground-truth (MfS-PSMNet, NS-PSMNet). [Code] , and Expansion of Visual Hints for Stereo(EVHS) . On the one hand, NDR combines traditionalstereo algorithms with deep learning to obtain refined,high-resolution disparity maps with sharp edges. Trainedsolely on synthetic data, the network can refine and super-resolve disparity maps from any source, including classicalstereo methods or deep stereo networks, generalizing wellto real images in a zero-shot manner. On the other hand,EVHS exploits visual hints from visual-inertial odometry.The method expands sparse and unevenly distributed 3Dcues using a 3D random geometric graph, connecting hintsthat are close in the 3D world to improve the learning andinference process, filtered by confidence. Expanded hintsare integrated within DeepPruner , guiding the dif-ferentiable patchmatch algorithm within a narrow disparityrange.",
  "d)Real-World Monocular to Synthetic Stereo Data": "Generating diverse, realistic training data is crucial forimproving generalization. To address this, approaches likerendering synthetic data or using real-world single/sparseimages have been proposed. Methods like Learning Stereofrom Single Images (LSSI) (LSSI) [Code] and NeRF-Supervised Deep Stereo (NS-Stereo) (NS-Stereo) [Code] create diverse stereo training data directly from eas-ily acquired real-world monocular images. This exposesstereo networks to natural textures/appearances, promotingzero-shot generalization, while enabling training on numer-ous scenes leveraging intrinsic real-world visual properties.Specifically, LSSI leverages pre-trained monocular depthnetworks (e.g., MiDaS ) to predict depth maps for singleimages, which are then converted to disparity maps andused both to synthesize stereo pairs, as well as pseudo-labelsfor training a stereo network. Similarly, NS-Stereo generatesstereo training data from sparse real-world image sequencescaptured with a single handheld camera. However, insteadof using monocular depth estimation, NS-Stereo fits a NeRFmodel to each scene and uses it to render arbitrarystereo pairs by synthesizing a reference view and a targetview displaced by a virtual baseline. Interestingly, NS-Stereotakes a step further by generating stereo triplets to handleocclusions in the photometric loss and exploiting the depth",
  "e)Knowledge Transfer": "Fine-tuning a pre-trained network often harms generaliza-tion itself. This happens, in particular, when the originaltraining dataset is much larger than the few samples usedfor fine-tuning, however, few works focused on overcomingthis issue. On this path, DKT-Stereo [Code] aimsat preserving the original, Dark Knowledge of a modelduring fine-tuning, with a frozen teacher network, an ex-ponential moving average (EMA) teacher network, and astudent network, all initialized with the same pre-trainedweights. A further Filter and Ensemble (F&E) module dis-cards the region being inconsistent across the pseudo-labelsby the frozen and the EMA teachers to avoid insufficientregularization, and ensembles the labels in the consistentregion to prevent overfitting ground-truth details at theexpense of generalization.",
  "Offline Adaptation": "When a set of stereo pairs is available from an unseen do-main, the domain shift can be compensated for by carryingout offline domain adaptation. In the absence of ground-truth annotations, a common approach consists of usingself-supervised learning techniques, such as photometriclosses, to either train from scratch a network specialized forthe target domain or to fine-tune a pre-trained one.Flow2Stereo [Code] pursues the former strategy byjointly learning optical flow and stereo matching by exploit-ing the 3D geometry of stereoscopic videos in two stages.In the first stage, a teacher network predicts confidentoptical flow using photometric consistency and geometricconstraints. In the second stage, the student network is re-fined using a self-supervised loss and proxy learning tasks,with the teacher networks confident predictions serving aspseudo-labels. On the same track, Reversing-Stereo [Code] reverses the typical relationship between monocularand stereo depth estimation . At the core, a monocu-lar completion network (MCN) leverages sparse disparitypoints computed by any traditional stereo algorithm andsingle-image cues to predict dense and accurate disparitymaps, by aggregating multiple MCN predictions with a randomized subset of the input points. These high-qualityproxy labels are then used to supervise the training of deepstereo networks. Similarly, Revealing-Stereo proposesa framework to improve both stereo and monocular depthestimation by leveraging their reciprocal relations, introduc-ing an occlusion-aware distillation strategy to train a monoc-ular depth network with reliable predictions from a stereonetwork, and an occlusion-aware fusion module to combinetheir advantages. TiO-Depth [Code] , instead, is atwo-in-one self-supervised model handling monocular andbinocular tasks via a Siamese architecture with monocularfeature matching and multi-stage joint training. PASMNet[Code] deploys a parallax-attention mechanism (PAM)integrating epipolar constraints with an attention mecha-nism to calculate feature similarities along the epipolar line.PASMnet, instead, employs this mechanism within a featureextractor, a cascaded parallax-attention module for coarse-to-fine matching cost regression, and a disparity refinementmodule, and is trained with a combination of photometricloss, smoothness loss, and PAM-specific losses that enforceleft-right consistency, smoothness, and cycle consistency ofthe parallax-attention maps at multiple scales.MultiscopicVision proposes a self-supervisedframework for stereo matching utilizing multiple imagescaptured at aligned camera positions, introducing lossesto optimize the network without ground-truth. StereoGAN[Code] tackles the domain gap between synthetic andreal data by jointly optimizing a domain translation networkand a stereo matching network. The domain translationcomponent utilizes a GAN-based adversarial loss to gener-ate realistic images from synthetic data while maintainingstereo consistency and epipolar geometry through novelconstraints. By leveraging knowledge of the target domain,StereoGAN effectively adapts synthetic data to more closelyresemble the characteristics of the real-world domain.Assuming a pre-trained model, AdaStereo alignsinput representations and improves cross-domain adapta-tion ability, by deploying 1) a non-adversarial progressivecolor transfer algorithm for input image-level alignment,2) an efficient, parameter-free cost normalization layer forchannel-level feature alignment, to regulate the norm distri-bution of pixel-wise feature vectors; and 3) a self-supervisedauxiliary task that reconstructs target-domain images usingpredicted disparities and occlusion masks.In contrast, UCFNet [Code] extends CFNet and adapts a pre-trained model to the target domain usingthe pseudo-labels predicted by the model itself, and filteredaccording to uncertainty. Lastly, RAG [Code] exploitsNAS for discovering novel cells and adapting a pre-trainednetwork to a specific domain, by expanding its architecture.Accordingly, RAG discovers a different structure for eachdomain: at test time, the proper one is selected dependingon the domain faced, by a Scene Router network ensemblinga set of auto-encoders trained on the single domains duringadaptation.",
  "When stereo images from the target domain are not avail-able beforehand, a stereo network can be adapted in anonline manner during deployment . In this case, stereo": "image pairs from the target domain are collected continu-ously, and the network adapts by relying on self-supervisedlearning objectives, such as photometric consistency loss.The main challenges of this setting consist of avoidingdegradation of the model and preserving efficiency.The first works extending are AoHNet andMAD++ [Code] . Both enhance the effectiveness andspeed of the adaptation procedure by replacing photomet-ric losses with pseudo-labels from traditional stereo algo-rithms. In addition, AoHNet deploys an Adapt-or-Hold(AoH) mechanism to figure out whether to adapt or not,thus reducing the overall computational overhead. In con-trast, PointFix exploits Model Agnostic Meta-Learning(MAML) to improve online adaptation. In the inner loop,wrong pixels are selected from the prediction by the basenetwork and fixed by a PointFixNet, which predicts residu-als used to adapt the base network itself. In the outer loop,both the base network and the PointFixNet are optimizedbased on the performance of the former after adaptation. Attest time, only the inner loop is performed. Lastly, FedStereo[Code] casts online adaptation as a distributed process,in which a server receives and aggregates updated weightsfrom a set of actively adapting clients. These weights arethen sent to a listening client not carrying out adaptation.To minimize communication overhead, a strategy inspiredby MAD is used to transfer only a subset of the weights.",
  "Over-Smoothing": "A common limitation of most stereo networks is their ten-dency to over-smooth depth discontinuities. During infer-ence, 3D stereo architectures typically employ a soft argmaxoperation to obtain the final disparity estimate by calculat-ing the mean of the often multimodal predicted distribu-tion. This leads to disparity estimates falling between theforeground and background modes, resulting in erroneouspredictions and over-smoothed depth discontinuities. Thisapproach leads to disparity estimates that fall betweenthe foreground and background modes, resulting in er-roneous predictions and over-smoothed depth discontinu-ities. 2D stereo architectures, despite not explicitly usingsoft argmax, are still affected by this issue. This yieldsinconsistent bleeding artifacts in the reconstructed 3Dgeometry around object boundaries, highly undesirable forapplications requiring accurate reconstruction with precisecontours. 3.2.1Unimodal Distribution ModelingUnimodal distribution modeling aims to alleviate the over-smoothing issue by constraining the disparity estimation toa single dominant mode. SM-CDE introduces a single-modal weighted average operation during inference. It con-siders the locality of the estimated disparity distribution andapplies weighted averaging only to the dominant mode.Additionally, SM-CDE analyzes different loss functions anddemonstrates that using cross-entropy loss with Gaussiandistribution during training provides more stable and fine-grained supervision. Similarly, AcfNet [Code] aims toimprove the sharpness of disparity maps by directly su-pervising the cost volume with adaptive unimodal groundtruth distributions. It introduces a confidence estimation",
  "(d) Ground Truth": ": Bleeding Artifacts. The smooth disparities predicted between foreground and background objects project into flyingpoints in 3D space (a,c), whereas precise 3D reconstructions demand sharp discontinuities (b,d). network to modulate the variance of the unimodal distribu-tion based on the confidence of finding a unique match. Thisensures that pixels with high confidence have sharp peaks,while those with low confidence have flatter peaks. AcfNetalso proposes a stereo focal loss to address the sampleimbalance problem in the cost volume. Another approach,CDN [Code] , addresses the issue by introducing anarchitecture that outputs a continuous distribution overarbitrary disparity values. It predicts probabilities and real-valued offsets for each disparity value in a pre-defineddiscrete set and takes the mode of this distribution as thefinal prediction. CDN also uses a novel loss function basedon the Wasserstein distance between the true and predicteddistributions. While the above methods focus on modelingunimodal distributions, LaC [Code] introduces a LocalSimilarity Pattern (LSP) to capture local structural informa-tion by explicitly revealing relationships between a pointand its neighbors. To address the over-smoothing problemcaused by static convolutional filters, LaC proposes CostSelf-Reassembling (CSR) and Disparity Self-Reassembling(DSR) strategies to adaptively propagate reliable disparityvalues based on image content. 3.2.2Multi-Modal Distribution ModelingMoving beyond unimodal distributions, multi-modal dis-tribution modeling has emerged as a promising approachto achieve sharp depth discontinuities. SMD-Nets [Code] is a pioneering work in this direction, utilizing bimodalmixture densities as output representation. It encodes the in-put into a feature map, from which a multi-layer perceptronestimates the parameters of a bimodal Laplacian mixturedistribution at any continuous 2D location. The bimodal dis-tribution effectively models both the foreground and back-ground disparities, allowing for precise depth estimationnear object boundaries. The final disparity is obtained byselecting the mode with the highest density value. Buildingupon this concept, ADL [Code] proposes an adaptivemulti-modal cross-entropy loss that models the ground-truth disparity distribution as a mixture of Laplacians. Thenumber of modes and their weights are determined by localclustering and statistics within a local window. ADL alsointroduces a dominant-modal disparity estimator (DME) torobustly locate the dominant mode.",
  "Transparent and Reflective Objects": "Non-Lambertian materials have always posed significantchallenges to matching algorithms, as they introduce mis-leading visual information about scene geometry. When atransparent object is present, a matching algorithm com-putes correspondences between points behind it, failing to perceive the objects distance from the camera. Alternatively,when dealing with a reflective surface, the algorithm mightfail to triangulate depth properly. However, driven by data,deep learning has the potential to address these difficultchallenges as well. In this regard, DDF proposes aframework for fusing the disparity predicted from stereoimages with the depth acquired by a structured-light cam-era. The fusion is performed as a per-pixel weighted averageof the two, with the weights predicted by a 2D UNet. Anadditional UNet is deployed for refining the fused depthmap. An alternative strategy consists of segmenting thenon-Lambertian materials and use the semantic masks toassist the stereo network. TA-Stereo and Depth4ToM[Code] implement this approach from two differentperspectives. The former applies the segmentation masksto the stereo images to enforce similar appearance acrossthe two and directly ease matching at testing time. Incontrast, Depth4Tom in-paints non-Lambertian objects ac-cording to the segmentation masks and processes themto obtain pseudo-labels for fine-tuning the stereo model.Pseudo-labels are predicted by a pre-trained monoculardepth model and are fused with the predictions by thestereo network itself, replacing these latter in correspon-dence with non-Lambertian materials.",
  "Asymmetric Stereo": "Most stereo frameworks assume the stereo image pair iscaptured by cameras with identical properties. However,asymmetries between the images are common, due to dif-ferences in the cameras intrinsic parameters, resolutions, ornoise levels. These asymmetries can affect the matching pro-cess between the two images. Consequently, a new researchdirection has emerged to design stereo solutions that arerobust to such asymmetries between the stereo pairs.Visually-Imbalanced Stereo (VI-Stereo) is the firstwork in this direction. Given an HR left and LR right imagepair, it deploys a UNet to upsample the latter and restorea balanced stereo pair, which is processed by a DispNet to predict an HR disparity map. On the contrary,NDR [Code] , downsamples the left image tothe resolution of the right one, runs a traditional stereoalgorithm to obtain an LR and then upsample it, guidedby the HR features, according to a continuous formulation.Differently from the previous works, DA-AS studiesa self-supervised setting and proposes feature-metric con-sistency to replace the photometric loss, which yields sub-optimal results with asymmetric images, yet can be usedan initial stereo model to learn good features for matching.Then, these features can be used to compute feature-metric",
  "EXPERIMENTS AND ANALYSIS": "In this section, we compare the stereo frameworks surveyedso far on the standard benchmarks in the field. Specifically,we report the leaderboards of the KITTI 2015 and theMiddlebury v3 online benchmarks the former being themost popular dataset used in the literature, while the latterstill represented a challenging benchmark in the late 2010s.Additionally, we include results from the Robust VisionChallenge (ROB), which demonstrates the recent efforts toachieve robustness to domain shifts. Finally, we present theleaderboard of the Booster online benchmark, representingthe challenges that remain open in the 2020s. In each table,we highlight the first , second , and third best results insingle categories, with the absolute best being reported inpurple.",
  "TABLE 2: Middlebury-v3 Online Benchmark": "The table reports three main sub-categories: 1) foundationalstereo networks published in the 2020s, 2) efficient architec-tures published in 2020-2024, and 3) representative modelsfrom the 2010s. Although the KITTI benchmark was alreadysaturated before the 2020s, the most recent advances inthe field allowed for further improvements, with the latestproposals from 2023-2024 establishing consistently at thevery top of the leaderboard, outsitting LEAStereo afternearly four years. Concerning efficient architectures, we canappreciate how the gap with state-of-the-art models hasbeen significantly reduced, making them a viable alternativefor practical applications.",
  "Middlebury v3": "reports the results sourced from the Middlebury v3online benchmark, with methods being ranked according tothe bad2.0 metric. We divide it into two sub-tables, groupingon top the models among those covered in this survey,and at the bottom those from previous years . At firstglance, we can appreciate a large improvement over the pre-vious state-of-the-art, which got better and better throughthe five years that passed from the previous surveys. Bylooking at the top 6 positions of the leaderboard, we findRAFT-Stereo and five more architectures derived fromit, suggesting how the former has been a game changerfor a benchmark challenging as Middlebury v3. Indeed, itis worth noticing how RAFT-Stereo has been one of thevery first architectures, after HSMNet, capable of processingMiddlebury images at full resolution, unleashing the possi-bility of maintaining many more details in the final disparitymaps, as well as its outstanding generalization performancecontributed to its success on this benchmark. Selective-IGEVcurrently represents the state-of-the-art: on the one hand, itachieves an average error on non-occluded pixels below thepixel for the first time. On the other hand, the average errorraises over 1.5 on all pixels, in particular those large onescaused by very close objects, suggesting that the occlusionsmay still represent an open challenge.",
  "TABLE 4: Booster Dataset": "2020, and 2022, reported respectively from the bottom to thetop. All methods are ranked on the individual benchmarksrespectively according to D1-all, bad1, and avg error met-rics on KITTI, Middlebury, and ETH3D, while the overallrank is computed according to the Schulze ProportionalRanking (PR) . We highlight that the challenge allowedfor training on any existing dataset both synthetic andreal ones; for a synthetic-to-real benchmark, we refer thereaders to . The trend highlighted through the threeeditions is consistent with the evolution of the field wehave reported in this survey. Indeed, end-to-end deep stereowas still young in 2018, as we can notice from the bottom-most section of the table, where iResNet and PSMNet are the only frameworks belonging to this category.Then, the field developed quickly in the following two yearsand CFNet ranked first in 2020, surpassing any of thenetworks from the 10s, with HSM-Net being the best amongthese latter models. Lastly, in 2022 the first three positionswere taken by models extending the optimization paradigmintroduced by RAFT-Stereo , confirming how the designstrategy emerged from this latter has been a game-changerin the field. The rapid improvements observed through thethree editions can be appreciated, in particular, by focusingon the error rates achieved on the Middlebury dataset, morespecifically on the bad1 metric: indeed, this latter was higherthan 45% for the winner of the first edition in 2018, droppingto about 26% for the winner of the 2020 edition and, finally,getting down to 16.5% in the latest edition.",
  "reports entries from the Booster online benchmark.Specifically, we show error metrics computed at full resolu-": "tion (i.e., approximately 12 Megapixels) across different cat-egories: All, which covers the whole regions of the images,as well as classes 0 to 3, which represents pixels belonging tomaterials from opaque to highly transparent/reflective. Allmethods are ranked, for each category, according to the bad2score. We can appreciate how the whole benchmark remainsvery difficult for current state-of-the-art stereo networks,with the very high resolution and the presence of non-Lambertian objects being the two main challenges. PCVNet is the absolute winner, outperforming any other frame-work on classes 2 and 3, while remaining competitive on 0and 1. By looking at RAFT-Stereo and CREStereo, for whichresults are reported by using both the original weights aswell as those obtained after running a fine-tuning on theBooster training set, we highlight how the error rates on themost challenges classes 2 and 3 are largely reduced afterfine-tuning, proving that state-of-the-art models have thepotential to learn how to deal with non-Lambertian objectsfrom context. Nonetheless, we point out how significantefforts are still necessary to properly deal with the two,aforementioned challenges.",
  "DISCUSSION": "In this section, we highlight the key messages from oursurvey, highlighting the key advances made in the 2020sand suggesting potential avenues for future developmentsin deep stereo matching.Architecture Design. As the benchmark results show,the new design strategy introduced by RAFT-Stereo hasbeen a game changer, bringing much higher robustness todomain shifts. Most of the latest frameworks released a fewmonths before this survey followed this new paradigm, and we expect more to come. Nonetheless, the quest for findingnovel and effective architectures is not over, as witnessed bythe very latest proposals achieving increasingly betterresults.Stereo Beyond RGB. A trend that has been consolidatingover the past five years is the use of other modalities, suchas images from thermal, multispectral, or event cameras,as input to stereo matching networks. This brings freshperspectives to a longstanding yet vivid field. However,online benchmarks for these new tasks are still rare, andmore would help to consolidate this track.Open Challenges. Despite the many successes in ad-dressing some of the challenges anticipated by previoussurveys , some persist. Indeed, the Booster dataset , highlighted how images at very high resolution remaindifficult to deal with, as well as non-Lambertian objectsare critical, mainly due to the lack of training data or sub-optimal approaches to handle them. Similarly, challengingweather conditions still represent potential obstacles.Foundational Models. Finally, in the wake of the emer-gence of visual foundational models for various computervision tasks, we argue that a foundational model for stereomatching is still missing. While some attempts have beenmade recently for single-image depth estimation , aneffort in this direction has not yet been made for stereo.",
  "CONCLUSION": "In this paper, we have surveyed the advances in deepstereo matching that have emerged in the 2020s. Buildingupon existing surveys , whose time horizon waslimited to the end of 2019, we have deeply investigatedthe new architectures and design trends that have appearedrecently and have become the standard implementationpatterns today. We also looked at more advanced appli-cations of stereo matching, ranging from sensor fusion tocross-spectral matching across different modalities. Finally,we have analyzed the performance of various methods onpopular online benchmarks. This analysis not only identifiesthe current leading approaches but also sheds light on theremaining challenges and potential future research direc-tions. We believe that this survey can serve as a practicalguide for both novices and seasoned experts, providinginspiration for their work.",
  "D. Scharstein and R. Szeliski, A taxonomy and evaluationof dense two-frame stereo correspondence algorithms, Interna-tional journal of computer vision, vol. 47, pp. 742, 2002": "M. Poggi, F. Tosi, K. Batsos, P. Mordohai, and S. Mattoccia, Onthe synergies between machine learning and binocular stereo fordepth estimation from images: a survey, IEEE Transactions onPattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 53145334, 2021. H. Laga, L. V. Jospin, F. Boussaid, and M. Bennamoun, A sur-vey on deep learning techniques for stereo-based depth estima-tion, IEEE transactions on pattern analysis and machine intelligence,vol. 44, no. 4, pp. 17381764, 2020. Z. Li, X. Liu, N. Drenkow, A. Ding, F. X. Creighton, R. H.Taylor, and M. Unberath, Revisiting stereo depth estimationfrom a sequence-to-sequence perspective with transformers, inProceedings of the IEEE/CVF International Conference on ComputerVision (ICCV), October 2021, pp. 61976206.",
  "L. Lipson, Z. Teed, and J. Deng, Raft-stereo: Multilevel recurrentfield transforms for stereo matching, in International Conferenceon 3D Vision (3DV), 2021": "M. Poggi, S. Kim, F. Tosi, S. Kim, F. Aleotti, D. Min, K. Sohn,and S. Mattoccia, On the confidence of stereo matching in adeep-learning era: a quantitative evaluation, IEEE transactionson pattern analysis and machine intelligence, vol. 44, no. 9, pp. 52935313, 2021. M. Poggi, F. Tosi, and S. Mattoccia, Quantitative evaluation ofconfidence measures in a machine learning world, in Proceedingsof the IEEE International Conference on Computer Vision, 2017, pp.52285237.",
  "M. Yang, F. Wu, and W. Li, Waveletstereo: Learning waveletcoefficients of disparity map in stereo matching, in IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR),June 2020": "A. Badki, A. Troccoli, K. Kim, J. Kautz, P. Sen, and O. Gallo, Bi3d:Stereo depth estimation via binary classifications, in IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR),June 2020. Z. Shen, Y. Dai, and Z. Rao, Cfnet: Cascade and fused cost vol-ume for robust stereo matching, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2021, pp.13 90613 915. Y. Mao, Z. Liu, W. Li, Y. Dai, Q. Wang, Y.-T. Kim, and H.-S. Lee,Uasnet: Uncertainty adaptive sampling network for deep stereomatching, in Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), October 2021, pp. 63116319.",
  "Z. Shen, Y. Dai, X. Song, Z. Rao, D. Zhou, and L. Zhang, Pcw-net: Pyramid combination and warping cost volume for stereomatching, in European Conference on Computer Vision.Springer,2022, pp. 280297": "L. Chen, W. Wang, and P. Mordohai, Learning the distributionof errors in stereo matching for joint disparity and uncertaintyestimation, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2023, pp. 17 23517 244. X. Cheng, Y. Zhong, M. Harandi, Y. Dai, X. Chang, H. Li, T. Drum-mond, and Z. Ge, Hierarchical neural architecture search fordeep stereo matching, Advances in Neural Information ProcessingSystems, vol. 33, 2020.",
  "Q. Wang, S. Shi, K. Zhao, and X. Chu, Easnet: searching elasticand accurate network architecture for stereo matching, in Euro-pean Conference on Computer Vision.Springer, 2022, pp. 437453": "Y. Hu, W. Wang, H. Yu, W. Zhen, and S. Scherer, Orstereo:Occlusion-aware recurrent stereo matching for 4k-resolution im-ages, in 2021 IEEE/RSJ International Conference on IntelligentRobots and Systems (IROS).IEEE, 2021, pp. 56715678. J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan,and S. Liu, Practical stereo matching via cascaded recurrent net-work with adaptive correlation, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2022, pp.16 26316 272.",
  "H. Zhao, H. Zhou, Y. Zhang, Y. Zhao, Y. Yang, and T. Ouyang,Eai-stereo: Error aware iterative network for stereo matching,in Proceedings of the Asian Conference on Computer Vision, 2022, pp.315332": "G. Xu, X. Wang, X. Ding, and X. Yang, Iterative geometry encod-ing volume for stereo matching, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2023, pp.21 91921 928. H. Zhao, H. Zhou, Y. Zhang, J. Chen, Y. Yang, and Y. Zhao,High-frequency stereo matching network, in Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition,2023, pp. 13271336. J. Jing, J. Li, P. Xiong, J. Liu, S. Liu, Y. Guo, X. Deng, M. Xu,L. Jiang, and L. Sigal, Uncertainty guided adaptive warpingfor robust and efficient stereo matching, in Proceedings of theIEEE/CVF International Conference on Computer Vision (ICCV),October 2023, pp. 33183327. X. Wang, G. Xu, H. Jia, and X. Yang, Selective-stereo: Adaptivefrequency information selection for stereo matching, in Proceed-ings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2024.",
  "M. Feng, J. Cheng, H. Jia, L. Liu, G. Xu, and X. Yang, Mc-stereo: Multi-peak lookup and cascade search range for stereomatching, 2024": "Z. Cheng, J. Yang, and H. Li, Stereo matching in time: 100+fps video stereo matching for extended reality, in Proceedings ofthe IEEE/CVF Winter Conference on Applications of Computer Vision,2024, pp. 87198728. Z. Chen, W. Long, H. Yao, Y. Zhang, B. Wang, Y. Qin, andJ. Wu, Mocha-stereo: Motif channel attention network for stereomatching, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2024. R. Gong, W. Liu, Z. Gu, X. Yang, and J. Cheng, Learning intra-view and cross-view geometric knowledge for stereo matching,in IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), June 2020.",
  "H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao, andA. Geiger, Unifying flow, stereo and depth estimation, IEEETransactions on Pattern Analysis and Machine Intelligence, 2023": "P. Weinzaepfel, T. Lucas, V. Leroy, Y. Cabon, V. Arora, R. Bregier,G. Csurka, L. Antsfeld, B. Chidlovskii, and J. Revaud, CroCov2: Improved Cross-view Completion Pre-training for StereoMatching and Optical Flow, in ICCV, 2023. J. Lou, W. Liu, Z. Chen, F. Liu, and J. Cheng, Elfnet: Evidentiallocal-global fusion for stereo matching, in Proceedings of theIEEE/CVF International Conference on Computer Vision, 2023, pp.17 78417 793.",
  "J. Zeng, C. Yao, L. Yu, Y. Wu, and Y. Jia, Parameterized costvolume for stereo matching, in Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, 2023, pp. 18 34718 357": "X.Li,C.Zhang,W.Su,andW.Tao,Iinet:Implicitintra-inter information fusion for real-time stereo matching,Proceedings of the AAAI Conference on Artificial Intelligence,vol. 38, no. 4, pp. 32253233, Mar. 2024. [Online]. Available: J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, Cascade residuallearning: A two-stage convolutional neural network for stereomatching, in Proceedings of the IEEE international conference oncomputer vision workshops, 2017, pp. 887895. B. Xu, Y. Xu, X. Yang, W. Jia, and Y. Guo, Bilateral grid learningfor stereo matching networks, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2021, pp.12 49712 506. J. Xing, Z. Qi, J. Dong, J. Cai, and H. Liu, Mabnet: a lightweightstereo network based on multibranch adjustable bottleneck mod-ule, in Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XXVIII 16.Springer, 2020, pp. 340356.",
  "M. Poggi and F. Tosi, Federated online adaptation for deepstereo, in CVPR, 2024": "A. Bangunharcana, J. W. Cho, S. Lee, I. S. Kweon, K.-S. Kim,and S. Kim, Correlate-and-excite: Real-time stereo matchingvia guided cost volume excitation, in IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS), 2021. Q. Wang, S. Shi, S. Zheng, K. Zhao, and X. Chu, Fadnet: Afast and accurate network for disparity estimation, in 2020 IEEEinternational conference on robotics and automation (ICRA).IEEE,2020, pp. 101107. V. Tankovich, C. Hane, Y. Zhang, A. Kowdle, S. Fanello, andS. Bouaziz, Hitnet: Hierarchical iterative tile refinement networkfor real-time stereo matching, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR),June 2021, pp. 14 36214 372. J. Cai, Z. QI, K. Fu, X. Shi, Z. Li, X. Liu, and H. Liu, Pbcstereo:A compressed stereo network with pure binary convolutionaloperations, in Proceedings of the Asian Conference on ComputerVision (ACCV), December 2022, pp. 43784394. J.-R. Chang, P.-C. Chang, and Y.-S. Chen, Attention-aware fea-ture aggregation for real-time stereo matching on edge devices,in Proceedings of the Asian Conference on Computer Vision (ACCV),November 2020. F. Shamsafar, S. Woerz, R. Rahim, and A. Zell, Mobilestereonet:Towards lightweight deep networks for stereo matching, inProceedings of the IEEE/CVF Winter Conference on Applications ofComputer Vision (WACV), January 2022, pp. 24172426. P. L. Dovesi, M. Poggi, L. Andraghetti, M. Mart, H. Kjell-strom, A. Pieropan, and S. Mattoccia, Real-time semantic stereomatching, in 2020 IEEE International Conference on Robotics andAutomation (ICRA).IEEE, 2020, pp. 10 78010 787.",
  "U. Kusupati, S. Cheng, R. Chen, and H. Su, Normal assistedstereo depth estimation, in IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), June 2020": "F. Aleotti, M. Poggi, F. Tosi, and S. Mattoccia, Learning end-to-end scene flow by distilling single tasks knowledge, in Proceed-ings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07,2020, pp. 10 43510 442. Y. Jiao, T. D. Tran, and G. Shi, Effiscene: Efficient per-pixelrigidity inference for unsupervised joint learning of optical flow,depth, camera pose and motion segmentation, in Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recogni-tion, 2021, pp. 55385547. C. Chi, Q. Wang, T. Hao, P. Guo, and X. Yang, Feature-levelcollaboration: Joint unsupervised learning of optical flow, stereodepth and camera motion, in Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR), June 2021,pp. 24632473.",
  "Y. You, Y. Wang, W.-L. Chao, D. Garg, G. Pleiss, B. Hariharan,M. Campbell, and K. Q. Weinberger, Pseudo-lidar++: Accuratedepth for 3d object detection in autonomous driving, in ICLR,2020": "J. Zhang, M. S. Ramanagopal, R. Vasudevan, and M. Johnson-Roberson, Listereo: Generate dense depth maps from lidar andstereo imagery, in 2020 IEEE International Conference on Roboticsand Automation (ICRA).IEEE, 2020, pp. 78297836. Y.-K. Huang, Y.-C. Liu, T.-H. Wu, H.-T. Su, Y.-C. Chang, T.-L.Tsou, Y.-A. Wang, and W. H. Hsu, S3: Learnable sparse signalsuperdensity for guided depth estimation, in Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition,2021, pp. 16 70616 716. H. Yin, L. Deng, Z. Chen, B. Chen, T. Sun, X. Yusen, J. Xiao, Y. Fu,S. Deng, and X. Li, Lsmd-net: Lidar-stereo fusion with mixturedensity network for depth sensing, in Proceedings of the AsianConference on Computer Vision (ACCV), December 2022, pp. 552568.",
  "H. Cho and K.-J. Yoon, Selection and cross similarity for event-image deep stereo, in European Conference on Computer Vision.Springer, 2022, pp. 470486": "K. Zhang, K. Che, J. Zhang, J. Cheng, Z. Zhang, Q. Guo, andL. Leng, Discrete time convolution for fast event-based stereo,in Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), June 2022, pp. 86768686. H. Cho, J. Cho, and K.-J. Yoon, Learning adaptive dense eventstereo from the image domain, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR),June 2023, pp. 17 79717 807. M. Mostafavi, K.-J. Yoon, and J. Choi, Event-intensity stereo:Estimating depth by the best of both worlds, in Proceedings ofthe IEEE/CVF International Conference on Computer Vision, 2021,pp. 42584267.",
  "H. Cho and K.-J. Yoon, Event-image fusion stereo using cross-modality feature propagation, in Proceedings of the AAAI Confer-ence on Artificial Intelligence, vol. 36, no. 1, 2022, pp. 454462": "X. Chen, W. Weng, Y. Zhang, and Z. Xiong, Depth from asym-metric frame-event stereo: A divide-and-conquer approach, inProceedings of the IEEE/CVF Winter Conference on Applications ofComputer Vision (WACV), January 2024, pp. 30453054. S. Walz, M. Bijelic, A. Ramazzina, A. Walia, F. Mannan, andF. Heide, Gated stereo: Joint depth estimation from gated andwide-baseline active stereo cues, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2023, pp.13 25213 262. Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle,V. Tankovich, M. Schoenberg, S. Izadi, T. Funkhouser, andS. Fanello, Activestereonet: End-to-end self-supervised learningfor active stereo systems, in Proceedings of the European Conferenceon Computer Vision (ECCV), September 2018. S.-H. Baek and F. Heide, Polka lines: Learning structured illumi-nation and reconstruction for active stereo, in Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), June 2021, pp. 57575767. I. Liu, E. Yang, J. Tao, R. Chen, X. Zhang, Q. Ran, Z. Liu, andH. Su, Activezero: Mixed domain learning for active stere-ovision with zero annotation, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2022, pp.13 03313 042. Y. Xu, X. Yang, Y. Yu, W. Jia, Z. Chu, and Y. Guo, Depth estima-tion by combining binocular stereo and monocular structured-light, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), June 2022, pp. 17461755. R. Chen, I. Liu, E. Yang, J. Tao, X. Zhang, Q. Ran, Z. Liu, J. Xu,and H. Su, Activezero++: Mixed domain learning stereo andconfidence-based depth completion with zero annotation, IEEETransactions on Pattern Analysis and Machine Intelligence, 2023. T. Zhi, B. R. Pires, M. Hebert, and S. G. Narasimhan, Deepmaterial-aware cross-spectral stereo matching, in Proceedings ofthe IEEE conference on computer vision and pattern recognition, 2018,pp. 19161925. M. Liang, X. Guo, H. Li, X. Wang, and Y. Song, Unsupervisedcross-spectral stereo matching by learning to synthesize, inProceedings of the AAAI Conference on Artificial Intelligence, vol. 33,no. 01, 2019, pp. 87068713. C. Walters, O. Mendez, M. Johnson, and R. Bowden, Thereand back again: Self-supervised multispectral correspondenceestimation, in 2021 IEEE International Conference on Robotics andAutomation (ICRA).IEEE, 2021, pp. 51475154. F. Tosi, P. Z. Ramirez, M. Poggi, S. Salti, S. Mattoccia, andL. Di Stefano, Rgb-multispectral matching: Dataset, learningmethodology, evaluation, in Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, 2022, pp. 15 95815 968. C. Tian, W. Pan, Z. Wang, M. Mao, G. Zhang, H. Bao, P. Tan, andZ. Cui, Dps-net: Deep polarimetric stereo depth estimation, inProceedings of the IEEE/CVF International Conference on ComputerVision (ICCV), October 2023, pp. 35693579.",
  "X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, Group-wisecorrelation stereo network, in CVPR, 2019": "Z. Teed and J. Deng, Raft: Recurrent all-pairs field transformsfor optical flow, in Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceedings, Part II16.Springer, 2020, pp. 402419. Z. Liang and C. Li, Any-stereo: Arbitrary scale disparityestimation for iterative stereo matching, Proceedings of the AAAIConference on Artificial Intelligence, vol. 38, no. 4, pp. 33333341,Mar. 2024. [Online]. Available: A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gellyet al., An image is worth 16x16 words: Transformers for imagerecognition at scale, arXiv preprint arXiv:2010.11929, 2020.",
  "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.Gomez, . Kaiser, and I. Polosukhin, Attention is all you need,Advances in neural information processing systems, vol. 30, 2017": "N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, andC. Rupprecht, Dynamicstereo: Consistent dynamic depth fromstereo videos, in Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), June 2023, pp. 13 22913 239. X. Gu, Z. Fan, S. Zhu, Z. Dai, F. Tan, and P. Tan, Cascadecost volume for high-resolution multi-view stereo and stereomatching, in IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), June 2020. M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.Chen, Mobilenetv2: Inverted residuals and linear bottlenecks,in Proceedings of the IEEE conference on computer vision and patternrecognition, 2018, pp. 45104520. A. Tonioni, F. Tosi, M. Poggi, S. Mattoccia, and L. D. Ste-fano, Real-time self-adaptive deep stereo, in Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), June 2019. W. Zhan, X. Ou, Y. Yang, and L. Chen, Dsnet: Joint learning forscene segmentation and disparity estimation, in 2019 Interna-tional Conference on Robotics and Automation (ICRA).IEEE, 2019,pp. 29462952. G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, Segstereo: Exploitingsemantic information for disparity estimation, in Proceedings ofthe European conference on computer vision (ECCV), 2018, pp. 636651. H. Jiang, D. Sun, V. Jampani, Z. Lv, E. Learned-Miller, andJ. Kautz, Sense: A shared encoder network for scene-flow es-timation, in Proceedings of the IEEE/CVF international conferenceon computer vision, 2019, pp. 31953204.",
  "A. Li, A. Hu, W. Xi, W. Yu, and D. Zou, Stereo-lidar depthestimation with deformable propagation and learned disparity-depth conversion, arXiv preprint arXiv:2404.07545, 2024": "G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba,A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis,and D. Scaramuzza, Event-based vision: A survey, IEEE Trans-actions on Pattern Analysis and Machine Intelligence, vol. 44, no. 1,pp. 154180, 2022. A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy,A. Bachrach, and A. Bry, End-to-end learning of geometry andcontext for deep stereo regression, in Proceedings of the IEEEinternational conference on computer vision, 2017, pp. 6675. T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, Semantic imagesynthesis with spatially-adaptive normalization, in Proceedingsof the IEEE/CVF conference on computer vision and pattern recogni-tion, 2019, pp. 23372346. A. Zhu, Z. Wang, K. Khant, and K. Daniilidis, Eventgan: Lever-aging large scale image datasets for event cameras, in 2021 IEEEInternational Conference on Computational Photography (ICCP), 2021,pp. 111.",
  "F. Zhang, X. Qi, R. Yang, V. Prisacariu, B. Wah, and P. Torr,Domain-invariant stereo matching networks, in Europe Confer-ence on Computer Vision (ECCV), 2020": "J. Zhang, X. Wang, X. Bai, C. Wang, L. Huang, Y. Chen, L. Gu,J. Zhou, T. Harada, and E. R. Hancock, Revisiting domaingeneralized stereo matching networks from a feature consistencyperspective, in Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), June 2022, pp. 13 00113 011. B. Liu, H. Yu, and G. Qi, Graftnet: Towards domain generalizedstereo matching with a broad-spectrum and task-oriented fea-ture, in Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), June 2022, pp. 13 01213 021. W. Chuah, R. Tennakoon, R. Hoseinnezhad, A. Bab-Hadiashar,and D. Suter, Itsa: An information-theoretic approach to auto-matic shortcut avoidance and domain generalization in stereomatching networks, in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), June 2022, pp.13 02213 032. T. Chang, X. Yang, T. Zhang, and M. Wang, Domain generalizedstereo matching via hierarchical visual transformation, in Pro-ceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), June 2023, pp. 95599568. Z. Rao, B. Xiong, M. He, Y. Dai, R. He, Z. Shen, and X. Li,Masked representation learning for domain generalized stereomatching, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), June 2023, pp. 54355444.",
  "C. Cai, M. Poggi, S. Mattoccia, and P. Mordohai, Matching-space stereo networks for cross-domain generalization, in 2020International Conference on 3D Vision (3DV), 2020, pp. 364373": "K. Cheng, T. Wu, and C. Healey, Revisiting non-parametricmatching cost volumes for robust and generalizable stereomatching, Advances in Neural Information Processing Systems,vol. 35, pp. 16 30516 318, 2022. F. Aleotti, F. Tosi, P. Z. Ramirez, M. Poggi, S. Salti, S. Mattoccia,and L. Di Stefano, Neural disparity refinement for arbitraryresolution stereo, in 2021 International Conference on 3D Vision(3DV).IEEE, 2021, pp. 207217.",
  "F. Tosi, F. Aleotti, P. Z. Ramirez, M. Poggi, S. Salti, S. Mattoccia,and L. Di Stefano, Neural disparity refinement, IEEE Transac-tions on Pattern Analysis and Machine Intelligence, 2024": "A. Pilzer, Y. Hou, N. Loppi, A. Solin, and J. Kannala, Expansionof visual hints for improved generalization in stereo matching,in Proceedings of the IEEE/CVF Winter Conference on Applications ofComputer Vision (WACV), January 2023, pp. 58405849. J. Watson, O. M. Aodha, D. Turmukhambetov, G. J. Brostow, andM. Firman, Learning stereo from single images, in ComputerVisionECCV 2020: 16th European Conference, Glasgow, UK, August2328, 2020, Proceedings, Part I 16.Springer, 2020, pp. 722740.",
  "IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), June 2020": "F. Aleotti, F. Tosi, L. Zhang, M. Poggi, and S. Mattoccia, Re-versing the cycle: self-supervised deep stereo through enhancedmonocular distillation, in Computer VisionECCV 2020: 16th Eu-ropean Conference, Glasgow, UK, August 2328, 2020, Proceedings,Part XI 16.Springer, 2020, pp. 614632. Z. Chen, X. Ye, W. Yang, Z. Xu, X. Tan, Z. Zou, E. Ding, X. Zhang,and L. Huang, Revealing the reciprocal relations between self-supervised stereo and monocular depth estimation, in Proceed-ings of the IEEE/CVF International Conference on Computer Vision(ICCV), October 2021, pp. 15 52915 538. W. Yuan, Y. Zhang, B. Wu, S. Zhu, P. Tan, M. Y. Wang, andQ. Chen, Stereo matching by self-supervision of multiscopicvision, in 2021 IEEE/RSJ International Conference on IntelligentRobots and Systems (IROS).IEEE, 2021, pp. 57025709.",
  "Z. Shen, X. Song, Y. Dai, D. Zhou, Z. Rao, and L. Zhang, Digginginto uncertainty-based pseudo-label for robust stereo matching,IEEE Transactions on Pattern Analysis and Machine Intelligence,2023": "R. Liu, C. Yang, W. Sun, X. Wang, and H. Li, Stereogan: Bridgingsynthetic-to-real domain gap by joint optimization of domaintranslation and stereo matching, in IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), June 2020. X. Song, G. Yang, X. Zhu, H. Zhou, Z. Wang, and J. Shi,Adastereo: A simple and efficient approach for adaptive stereomatching, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), June 2021, pp. 10 32810 337. C. Zhang, K. Tian, B. Fan, G. Meng, Z. Zhang, and C. Pan,Continual stereo matching of continuous driving scenes withgrowing architecture, in Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), June 2022, pp.18 90118 910.",
  "C. Chen, X. Chen, and H. Cheng, On the over-smoothingproblem of cnn based disparity estimation, in Proceedings of theIEEE/CVF International Conference on Computer Vision, 2019, pp.89979005": "Y. Zhang, Y. Chen, X. Bai, S. Yu, K. Yu, Z. Li, and K. Yang, Adap-tive unimodal cost volume filtering for deep stereo matching, inProceedings of the AAAI Conference on Artificial Intelligence, vol. 34,no. 07, 2020, pp. 12 92612 934. D. Garg, Y. Wang, B. Hariharan, M. Campbell, K. Q. Weinberger,and W.-L. Chao, Wasserstein distances for stereo disparityestimation, Advances in Neural Information Processing Systems,vol. 33, pp. 22 51722 529, 2020. B. Liu, H. Yu, and Y. Long, Local similarity pattern and cost self-reassembling for deep stereo matching networks, in Proceedingsof the AAAI Conference on Artificial Intelligence, vol. 36, no. 2, 2022,pp. 16471655.",
  "Y. Liu, J. Ren, J. Zhang, J. Liu, and M. Lin, Visually imbalancedstereo matching, in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2020, pp. 20292038": "X. Chen, Z. Xiong, Z. Cheng, J. Peng, Y. Zhang, and Z.-J. Zha, Degradation-agnostic correspondence from resolution-asymmetric stereo, in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), June 2022, pp.12 96212 971. T. Song, S. Kim, and K. Sohn, Unsupervised deep asymmetricstereo matching with spatially-adaptive self-similarity, in Pro-ceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023, pp. 13 67213 680. D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, A naturalisticopen source movie for optical flow evaluation, in ComputerVisionECCV 2012: 12th European Conference on Computer Vision,Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12. Springer,2012, pp. 611625. D. Scharstein, H. Hirschmuller, Y. Kitajima, G. Krathwohl,N. Nesic, X. Wang, and P. Westling, High-resolution stereodatasets with subpixel-accurate ground truth, in Pattern Recog-nition: 36th German Conference, GCPR 2014, Munster, Germany,September 2-5, 2014, Proceedings 36.Springer, 2014, pp. 3142. S. Duggal, S. Wang, W.-C. Ma, R. Hu, and R. Urtasun, Deep-pruner: Learning efficient stereo matching via differentiablepatchmatch, in Proceedings of the IEEE/CVF international confer-ence on computer vision, 2019, pp. 43844393. R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun,Towards robust monocular depth estimation: Mixing datasetsfor zero-shot cross-dataset transfer, IEEE Transactions on PatternAnalysis and Machine Intelligence, vol. 44, no. 3, 2022.",
  "C. Godard, O. Mac Aodha, and G. J. Brostow, Unsupervisedmonocular depth estimation with left-right consistency, inCVPR, 2017": "Z. Zhou and Q. Dong, Two-in-one depth: Bridging the gapbetween monocular and binocular self-supervised depth estima-tion, in Proceedings of the IEEE/CVF International Conference onComputer Vision, 2023, pp. 94119421. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Doso-vitskiy, and T. Brox, A large dataset to train convolutionalnetworks for disparity, optical flow, and scene flow estimation,in Proceedings of the IEEE conference on computer vision and patternrecognition, 2016, pp. 40404048.",
  "X. Cheng, P. Wang, and R. Yang, Learning depth with convolu-tional spatial propagation network, IEEE transactions on patternanalysis and machine intelligence, vol. 42, no. 10, pp. 23612379,2019": "F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, Ga-net: Guidedaggregation net for end-to-end stereo matching, in Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition,2019, pp. 185194. G. Yang, J. Manela, M. Happold, and D. Ramanan, Hierarchicaldeep stereo matching on high-resolution images, in Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition, 2019, pp. 55155524.",
  "the IEEE/CVF international conference on computer vision workshops,2019, pp. 00": "E. Ilg, T. Saikia, M. Keuper, and T. Brox, Occlusions, motion anddepth boundaries with a generic network for disparity, opticalflow or scene flow estimation, in Proceedings of the Europeanconference on computer vision (ECCV), 2018, pp. 614630. K. Batsos, C. Cai, and P. Mordohai, Cbmv: A coalesced bidirec-tional matching volume for disparity estimation, in Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition,2018, pp. 20602069.",
  "M. Schulze, A new monotonic, clone-independent, rever-sal symmetric, and condorcet-consistent single-winner electionmethod, Social choice and Welfare, vol. 36, no. 2, pp. 267303,2011": "P. Z. Ramirez, F. Tosi, M. Poggi, S. Salti, S. Mattoccia, andL. Di Stefano, Open challenges in deep stereo: The boosterdataset, in Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), June 2022, pp. 21 16821 178. P. Z. Ramirez, A. Costanzino, F. Tosi, M. Poggi, S. Salti, S. Mat-toccia, and L. Di Stefano, Booster: a benchmark for depth fromimages of specular and transparent surfaces, IEEE Transactionson Pattern Analysis and Machine Intelligence, 2023.",
  "Theory": "1.1.1Learning for the Stereo PipelineIn the early days, deep learning was studied for improvingindividual components of the traditional stereo pipeline. Specifically, CNN-based approaches were explored tolearn more robust and discriminative matching cost func-tions, optimize the cost volume, and refine the final dispar-ity map.One of the primary areas of interest was the develop-ment of learned matching cost functions to replace hand-crafted ones, such as the sum of absolute differences (SAD)or the census transform (CT) . These learned cost func-tions, typically implemented using Siamese CNN architec-tures, were trained to predict the similarity between imagepatches extracted from stereo pairs, resulting in more accu-rate and robust matching costs , , . The resultingcost volumes were then processed using conventional opti-mization techniques, such as SGM , and refined usingtraditional post-processing steps including bilateral and/ormedian filtering.Some efforts were also made to improve the cost volumeoptimization and refinement stages using learning-basedapproaches. Several methods have been proposed to learnhow to modulate the cost volume based on the reliabilityof matching costs , select highly confident pixels asconstraints for optimization , and adapt the aggregationstep in SGM to reduce streaking artifacts . CNNs werealso employed to refine the final disparity map , ,, , replacing conventional filtering techniques.These early learning-based approaches demonstratedthat deep learning could improve stereo matching by replac-ing certain hand-crafted components with learned methods,while still relying on the traditional pipeline structure.The success of these methods in improving individualcomponents of the stereo pipeline paved the way for thedevelopment of end-to-end deep stereo networks, whichwould come to dominate the field in the following years. By showing that learned components could outperform theirhand-crafted counterparts, these early approaches laid thegroundwork for the paradigm shift towards fully learnablemodels that directly estimate disparity maps from stereoimage pairs.",
  "End-to-End Deep Stereo": "The advent of deep learning has revolutionized the field ofstereo matching, enabling the development of end-to-endmodels that directly estimate disparity maps from stereoimage pairs. These models have largely replaced traditionalstereo matching pipelines, which typically consist of mul-tiple hand-crafted steps . According to , , end-to-end deep stereo networks directly estimating disparity mapsfrom stereo image pairs can be broadly categorized into twomain classes, based on their architecture: 2D networks and3D networks.Both 2D and 3D stereo networks begin by extractingdeep features from the left and right input images usingshared-weight CNNs. The key difference lies in how theyconstruct and process the cost volume, which encodes thesimilarity between features at different disparity levels.2D networks, such as DispNet , usually build a 3Dcost volume by computing the correlation between featuresat corresponding pixels across a range of disparities, encod-ing the similarity between patches centered on each pixel:",
  "o[k,k][k,k]f1(x1 + o), f2(x2 + o)(1)": "where f1 and f2 are the features from the left andright images, and k determines the neighborhood size. Theresulting 3D cost volume is processed using 2D convo-lutions using an encoder-decoder design, inspired by theU-Net model , to directly regress disparity values foreach pixel. The use of plain 2D convolutions allows thesenetworks to achieve real-time performance.In contrast, 3D networks, first introduced by GC-Net and later followed by more advanced architectures such asPSMNet and GA-Net , construct a 4D cost volumeby concatenating or computing the difference between fea-tures at all possible disparities. This 4D cost volume is thenprocessed using 3D convolutions, which explicitly encodethe geometry of the scene and capture the relationships",
  "d=0d (cd)(2)": "where is the softmax operator applied along the dis-parity dimension D, and cd are the cost values at eachdisparity level d. First, the predicted costs cd from the costvolume are converted to a probability volume by takingthe negative of each value. The probability volume is thennormalized across the disparity dimension using the soft-max operation, (). Finally, the disparity is computed asthe sum of each disparity d weighted by its normalizedprobability. While 3D networks generally achieve higheraccuracy than their 2D counterparts, they have significantlyhigher computational and memory requirements due to theuse of 3D convolutions.While 3D networks generally achieve higher accuracythan their 2D counterparts, they have significantly highercomputational and memory requirements due to the addi-tional dimension. Various techniques have been proposedto mitigate this computational burden, such as coarse-to-fine strategies , , , , adaptive search spacepruning , and hierarchical architectures , and multi-scale feature extraction.Additionally, multi-task learning approaches have beenexplored to leverage the complementary nature of tasks likesemantic segmentation and edge detection . For amore comprehensive overview and detailed descriptions ofthese methods, readers can refer to existing surveys, such as, .",
  "Passive Stereo Datasets": "Middlebury 2021 1. This data collection consists of 24indoor stereo datasets captured using a mobile device (Ap-ple iPod touch 6G) mounted on a robotic arm, enablingacquisition of ground truth disparities via structured light-ing. Spanning 11 distinct scenes imaged from 1-3 viewpointsunder varying illuminations and exposures, including flash,ambient light, and device torch lighting, each dataset pro-vides two 19201080 resolution views organized into direc-tories containing multiple illumination/exposure variantsalongside calibration data and ground truth disparity maps.Booster 2. This work introduces a novel high-resolution stereo dataset targeting the open challenges ofnon-Lambertian surfaces and high-resolution stereo match-ing. The dataset consists of 419 high-resolution (12 Mpx)stereo image pairs, as well as 419 unbalanced pairs with a12 Mpx left image and 1.1 Mpx right image, collected across64 different indoor scenes. The scenes contain a varietyof specular and transparent surfaces, which are carefullyannotated with dense ground-truth disparities using a noveldeep space-time stereo framework. This framework lever-ages a pre-trained deep stereo network to accumulate costvolumes from multiple textured stereo pairs, enabling accu-rate sub-pixel disparity labels even for the challenging non-Lambertian regions. In addition to the ground-truth dispar-ities, it provides manually annotated material segmentationmasks to facilitate analysis of network performance on dif-ferent surface types. The dataset is divided into 228 trainingand 191 test samples, where the test ground truth dis-parities are withheld, providing a challenging benchmarkto encourage further research on these open problems instereo matching. The dataset is accompanied by an onlineevaluation benchmark for assessing stereo methods.Holopix50k 3. Holopix50k is an in-the-wild stereodataset comprising 49,368 image pairs captured by users ofthe Holopix mobile social platform, covering a wide varietyof realistic scenarios in mobile photography. The imageshave an average resolution of 0.74 (0.30) Mpx and were col-lected from the first Lightfield-enabled social media applica-tion. The dataset underwent post-processing steps to ensure",
  ": New stereo datasets in the 20s (Synthetic)": "high-quality stereo pairs, including removing vertical dis-parity and disparity-based filtering. However, Holopix50kcontains only stereo RGB images without any ground truthdisparity, making it suitable for self-supervised training ofstereo networks. Moreover, due to the lack of ground truthdisparity, Holopix50k is not typically used as a benchmarkfor evaluating stereo vision algorithms.InStereo2K 4. The InStereo2K dataset is a large-scalestereo dataset designed for indoor scenes, containing 2050pairs of stereo images with highly accurate disparity mapsobtained using a structured light system. The dataset issplit into a training set of 2000 image pairs and a testset of 50 image pairs, with a resolution of 1080 860pixels. InStereo2K covers a wide range of indoor scenes,including offices, classrooms, bedrooms, living rooms, anddormitories. The structured light system used to generatethe dataset consists of two color cameras with a resolutionof 1280 960 pixels and a projector with a resolution of1024 768 pixels.",
  "Multimodal Stereo Datasets": "CATS 5. The Color and Thermal Stereo (CATS) datasetis a large-scale benchmark consisting of approximately 1400images captured by a sensor platform with two visible-bandcameras at a resolution of 1280 960 pixels, two long waveinfrared (LWIR) thermal cameras at a resolution of 640480pixels, and a LiDAR for high-accuracy (2mm) ground truth.The dataset features 100 indoor and 80 outdoor clutteredscenes with diverse objects under various lighting and envi-ronmental conditions. CATS provides rectified stereo pairsin color, thermal, and cross-modality configurations, alongwith corresponding ground truth disparity maps generatedby projecting the LiDAR data onto the images using a novelsemi-automatic calibration method.MVSEC 6. The Multi Vehicle Stereo Event Cam-era dataset (MVSEC) collects data from both indoor andoutdoor environments with event cameras. Specifically, a10cm-baseline stereo camera made with DAVIS 346B sensors",
  "Synthetic": "We now introduce more datasets obtained through graphicengines, allowing for generating vast amounts of stereoimages and dense ground truth disparities with little effort. shows an overview of same samples taken from thesedatasets.HR-VS 13. The High-res Virtual Stereo (HR-VS)dataset, presented in along with the HSMNet architec-ture, is a synthetic high-resolution stereo dataset generatedusing the Carla simulator . It contains 780 pairs ofstereo images at 20562464 pixel resolution collected under4 weather conditions in Town01, with a camera baselineof 0.54m. The dataset mimics real-world driving scenariosby limiting the disparity range to [9.66, 768] pixels andthe depth range to [2.52, 200] meters, making it suitablefor training high-resolution stereo matching algorithms forautonomous driving and urban scene understanding.Virtual KITTI 2 14 Building upon its predecessor,Virtual KITTI , this dataset offers photo-realistic render-ings of driving scenarios generated using the Unity gameengine. The dataset comprises 5 image sequences, each aclone of a real-world sequence from the KITTI trackingbenchmark, with a resolution of 1242 375 pixels. Whatsets Virtual KITTI 2 apart is its diverse set of renditions foreach sequence, including variations in weather conditionslike fog and rain, as well as modifications to the cameraconfiguration, such as rotations of 15 and 30. In additionto RGB images, the dataset provides ground truth data fordepth, optical flow, scene flow, instance segmentation, andsemantic segmentation.TartanAir 15. The TartanAir dataset is a large-scale,challenging dataset designed for robot navigation tasks,with a special focus on stereo vision and SLAM-relatedproblems. It provides synchronized stereo RGB imagesacross 1037 long motion sequences in 30 diverse environ-ments, covering structured urban and indoor scenes as wellas unstructured natural environments, resulting in over 1million frames. Collected in photo-realistic simulation usingthe Unreal Engine and AirSim plugin, the dataset alsoincludes depth images, segmentation labels, camera poses,occupancy grid maps, optical flow, stereo disparity, andsimulated LiDAR measurements. TartanAir covers a widerange of motion patterns and includes challenging sceneswith dynamic lighting, low illumination, adverse weather,and dynamic objects, aiming to bridge the gap betweensimulation and real-world performance.UnrealStereo4K 16. The UnrealStereo4K dataset is alarge-scale synthetic stereo dataset created using the UnrealEngine and the open-source plugin UnrealCV. It consists of8 scenes, including both indoor and outdoor environments,with a total of 8,000 stereo pairs at 38402160px resolution.The dataset is divided into 7,720 training pairs, 80 validationpairs, and 200 in-domain test pairs. An additional 200 out-of-domain test pairs from an unseen scene are providedto evaluate the generalization ability of stereo matching",
  "p|Dp Dgtp |(3)": "where Dp and Dgtpare the predicted and ground truthdisparity values for pixel p, respectively, and N is the totalnumber of pixels. The EPE calculates the average absolutedifference between the predicted and ground truth disparityvalues across all pixels. Optionally, the Root Mean SquaredError (RMSE) is also taken into account in some bench-marks, such as Middlebury v3 and Booster:",
  "W. Bao, W. Wang, Y. Xu, Y. Guo, S. Hong, and X. Zhang, In-stereo2k: a large real dataset for stereo matching in indoor scenes,Science China Information Sciences, vol. 63, pp. 111, 2020": "P. Z. Ramirez, A. Costanzino, F. Tosi, M. Poggi, S. Salti, S. Mat-toccia, and L. Di Stefano, Booster: a benchmark for depth fromimages of specular and transparent surfaces, IEEE Transactions onPattern Analysis and Machine Intelligence, 2023. K. Chaney, F. Cladera, Z. Wang, A. Bisulco, M. A. Hsieh, C. Kor-pela, V. Kumar, C. J. Taylor, and K. Daniilidis, M3ed: Multi-robot,multi-sensor, multi-environment event dataset, in Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR) Workshops, June 2023, pp. 40154022. C. Tian, W. Pan, Z. Wang, M. Mao, G. Zhang, H. Bao, P. Tan, andZ. Cui, Dps-net: Deep polarimetric stereo depth estimation, inProceedings of the IEEE/CVF International Conference on ComputerVision (ICCV), October 2023, pp. 35693579.",
  "M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, Dsec: Astereo event camera dataset for driving scenarios, IEEE Roboticsand Automation Letters, vol. 6, no. 3, pp. 49474954, 2021": "S. Walz, M. Bijelic, A. Ramazzina, A. Walia, F. Mannan, andF. Heide, Gated stereo: Joint depth estimation from gated andwide-baseline active stereo cues, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2023, pp.13 25213 262. M. Poggi, F. Tosi, K. Batsos, P. Mordohai, and S. Mattoccia, Onthe synergies between machine learning and binocular stereo fordepth estimation from images: a survey, IEEE Transactions onPattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 53145334, 2021. H. Laga, L. V. Jospin, F. Boussaid, and M. Bennamoun, A surveyon deep learning techniques for stereo-based depth estimation,IEEE transactions on pattern analysis and machine intelligence, vol. 44,no. 4, pp. 17381764, 2020. M. Poggi, F. Tosi, and S. Mattoccia, Quantitative evaluation ofconfidence measures in a machine learning world, in Proceedingsof the IEEE International Conference on Computer Vision, 2017, pp.52285237.",
  "M.-G. Park and K.-J. Yoon, Leveraging stereo matching withlearning-based confidence measures, in Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, 2015, pp.101109": "A. Spyropoulos, N. Komodakis, and P. Mordohai, Learning todetect ground control points for improving the accuracy of stereomatching, in Proceedings of the IEEE conference on computer visionand pattern recognition, 2014, pp. 16211628. M. Poggi and S. Mattoccia, Learning a general-purpose confi-dence measure based on o (1) features and a smarter aggregationstrategy for semi global matching, in 2016 Fourth internationalconference on 3D vision (3DV).IEEE, 2016, pp. 509518. A. Shaked and L. Wolf, Improved stereo matching with constanthighway networks and reflective confidence learning, in Proceed-ings of the IEEE conference on computer vision and pattern recognition,2017, pp. 46414650. S. Gidaris and N. Komodakis, Detect, replace, refine: Deep struc-tured prediction for pixel wise labeling, in Proceedings of the IEEEconference on computer vision and pattern recognition, 2017, pp. 52485257.",
  "K. Batsos and P. Mordohai, Recresnet: A recurrent residual cnnarchitecture for disparity map enhancement, in 2018 InternationalConference on 3D Vision (3DV).IEEE, 2018, pp. 238247": "F. Aleotti, F. Tosi, P. Z. Ramirez, M. Poggi, S. Salti, S. Mattoccia, andL. Di Stefano, Neural disparity refinement for arbitrary resolutionstereo, in 2021 International Conference on 3D Vision (3DV).IEEE,2021, pp. 207217. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,and T. Brox, A large dataset to train convolutional networks fordisparity, optical flow, and scene flow estimation, in Proceedings ofthe IEEE conference on computer vision and pattern recognition, 2016,pp. 40404048. O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutionalnetworks for biomedical image segmentation, in Medical imagecomputing and computer-assisted interventionMICCAI 2015: 18thinternational conference, Munich, Germany, October 5-9, 2015, proceed-ings, part III 18.Springer, 2015, pp. 234241. A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy,A. Bachrach, and A. Bry, End-to-end learning of geometry andcontext for deep stereo regression, in Proceedings of the IEEEinternational conference on computer vision, 2017, pp. 6675.",
  "J.-R. Chang and Y.-S. Chen, Pyramid stereo matching network,in Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, 2018, pp. 54105418": "F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, Ga-net: Guidedaggregation net for end-to-end stereo matching, in Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition,2019, pp. 185194. S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, andS. Izadi, Stereonet: Guided hierarchical refinement for real-timeedge-aware depth prediction, in Proceedings of the European con-ference on computer vision (ECCV), 2018, pp. 573590. Y. Wang, Z. Lai, G. Huang, B. H. Wang, L. Van Der Maaten,M. Campbell, and K. Q. Weinberger, Anytime stereo image depthestimation on mobile devices, in 2019 international conference onrobotics and automation (ICRA).IEEE, 2019, pp. 58935900. Z. Yin, T. Darrell, and F. Yu, Hierarchical discrete distributiondecomposition for match density estimation, in Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, 2019,pp. 60446053. S. Duggal, S. Wang, W.-C. Ma, R. Hu, and R. Urtasun, Deep-pruner: Learning efficient stereo matching via differentiable patch-match, in Proceedings of the IEEE/CVF international conference oncomputer vision, 2019, pp. 43844393. G. Yang, J. Manela, M. Happold, and D. Ramanan, Hierarchicaldeep stereo matching on high-resolution images, in Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,2019, pp. 55155524. G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, Segstereo: Exploitingsemantic information for disparity estimation, in Proceedings of theEuropean conference on computer vision (ECCV), 2018, pp. 636651. X. Song, X. Zhao, H. Hu, and L. Fang, Edgestereo: A contextintegrated residual pyramid network for stereo matching, inComputer VisionACCV 2018: 14th Asian Conference on ComputerVision, Perth, Australia, December 26, 2018, Revised Selected Papers,Part V 14.Springer, 2019, pp. 2035.",
  "mixture density networks, in Conference on Computer Vision andPattern Recognition (CVPR), 2021": "Q. Wang, S. Zheng, Q. Yan, F. Deng, K. Zhao, and X. Chu, Irs:A large naturalistic indoor robotics stereo dataset to train deepmodels for disparity and surface normal estimation, in 2021 IEEEInternational Conference on Multimedia and Expo (ICME).IEEE,2021, pp. 16. J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan,and S. Liu, Practical stereo matching via cascaded recurrentnetwork with adaptive correlation, in Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2022, pp.16 26316 272. L. Mehl, J. Schmalfuss, A. Jahedi, Y. Nalivayko, and A. Bruhn,Spring: A high-resolution high-detail dataset and benchmark forscene flow, optical flow and stereo, in Proc. IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), 2023. P. Z. Ramirez, F. Tosi, M. Poggi, S. Salti, S. Mattoccia, and L. Di Ste-fano, Open challenges in deep stereo: The booster dataset, inProceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), June 2022, pp. 21 16821 178.",
  "Y. Hua, P. Kohli, P. Uplavikar, A. Ravi, S. Gunaseelan, J. Orozco,and E. Li, Holopix50k: A large-scale in-the-wild stereo imagedataset, arXiv preprint arXiv:2003.11172, 2020": "W. Treible, P. Saponaro, S. Sorensen, A. Kolagunda, M. ONeal,B. Phelan, K. Sherbondy, and C. Kambhamettu, Cats: A color andthermal stereo benchmark, in Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR), July 2017. A. Z. Zhu, D. Thakur, T. Ozaslan, B. Pfrommer, V. Kumar, andK. Daniilidis, The multivehicle stereo event camera dataset: Anevent camera dataset for 3d perception, IEEE Robotics and Au-tomation Letters, vol. 3, no. 3, pp. 20322039, 2018.",
  "J. Zhang and S. Singh, Loam: Lidar odometry and mapping inreal-time. in Robotics: Science and systems, vol. 2, no. 9.Berkeley,CA, 2014, pp. 19": "C. Bai, T. Xiao, Y. Chen, H. Wang, F. Zhang, and X. Gao, Faster-lio:Lightweight tightly coupled lidar-inertial odometry using parallelsparse incremental voxels, IEEE Robotics and Automation Letters,vol. 7, no. 2, pp. 48614868, 2022. F. Tosi, P. Z. Ramirez, M. Poggi, S. Salti, S. Mattoccia, and L. Di Ste-fano, Rgb-multispectral matching: Dataset, learning methodol-ogy, evaluation, in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2022, pp. 15 95815 968. G. Yang, J. Manela, M. Happold, and D. Ramanan, Hierarchicaldeep stereo matching on high-resolution images, in Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), June 2019.",
  "A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, Virtual worlds asproxy for multi-object tracking analysis, in CVPR, 2016": "W. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu,A. Kapoor, and S. Scherer, Tartanair: A dataset to push thelimits of visual slam, in 2020 IEEE/RSJ International Conference onIntelligent Robots and Systems (IROS).IEEE, 2020, pp. 49094916. L. Jospin, A. Antony, L. Xu, H. Laga, F. Boussaid, and M. Ben-namoun,Active-passivesimstereo-benchmarkingthecross-generalization capabilities of deep learning-based stereo meth-ods, Advances in Neural Information Processing Systems, vol. 35,pp. 29 23529 247, 2022."
}