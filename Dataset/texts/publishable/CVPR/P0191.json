{
  "Abstract": "Tracking and segmenting multiple objects in complexscenes has always been a challenge in the field of videoobject segmentation, especially in scenarios where objectsare occluded and split into parts. In such cases, the def-inition of objects becomes very ambiguous. The motiva-tion behind the MOSE dataset is how to clearly recognizeand distinguish objects in complex scenes.In this chal-lenge, we propose a semantic embedding video object seg-mentation model and use the salient features of objects asquery representations. The semantic understanding helpsthe model to recognize parts of the objects and the salientfeature captures the more discriminative features of the ob-jects. Trained on a large-scale video object segmentationdataset, our model achieves first place (84.45%) in the testset of PVUW Challenge 2024: Complex Video Object Seg-mentation Track.",
  ". Introduction": "Video Object Segmentation (VOS) focuses on trackingand segmenting target objects within a video sequence, be-ginning with mask annotations in the initial frame . This technique has significant potential across vari-ous applications, especially given the increasing volume ofvideo content in areas such as autonomous driving, aug-mented reality, and interactive video editing . The cur-rent VOS task faces several major challenges, including sig-nificant changes in target appearance, occlusions, and iden-tity confusion due to similar objects and background clutter.These challenges are particularly pronounced in long-termvideos, making the task even more complex.VOS methods generally achieve video objectsegmentation by comparing the test frame with previousframes.They begin by employing an association model",
  ". Visual examples of the MOSE test set . The left is thefirst frame and the right is the corresponding targets": "to generate pixel-wise correlated features between the testframe and target templates. These correlated features arethen used to predict the target masks accurately . Tohandle variations in target appearances over time, certainmethods incorporates a memory module that storessamples of the targets as their appearances change. Addi-tionally, recent approaches introduce object queriesto differentiate between various target objects, thus mitigat-",
  "ing identity confusion": "To evaluate the performance of current methods in real-istic environments, The MOSE (coMplex video ObjectSEgmentation) dataset is a comprehensive collection de-signed to provide complex and challenging scenarios withoccluded and crowded objects. Unlike existing datasets thatfeature relatively isolated and salient objects, MOSE fo-cuses on realistic environments, highlighting the limitationsof current VOS methods and encouraging the developmentof more robust algorithms. This dataset is crucial for ad-vancing VOS technologies to perform effectively in real-world applications where object interactions and occlusionsare common. Current methods struggle with the MOSE dataset due toseveral challenges. First, when targets have multiple com-plex or separate parts because of occlusion, backgroundclutter, and shape complexity, existing methods tend to pro-duce incomplete prediction masks. This issue arises be-cause these methods rely heavily on pixel-level correla-tion, which emphasizes pixel-wise similarity while over-looking semantic information.Second, although objectqueries improve ID association accuracy, they per-form poorly in sequences with significant target appearancechanges and very small targets. The current query propa-gation strategy, which updates using the entire online pre-dicted target sample, is susceptible to introducing noise andaccumulating errors. This can lead to tracking failures, suchas missing targets and ID switches, particularly when track-ing small targets. To address the aforementioned issues, we propose a VOSframework to learn both the semantic prior and discrimina-tive query representation. Specifically, we design a blockthat efficiently learns both semantic and detailed informa-tion, which can extract rich semantic features from a pre-trained Vision Transformer (ViT) without the need to trainall feature extraction parameters. To better model the queryrepresentation of the target, we design a more discrimina-tive filtering mechanism to generate discriminative queries.",
  "The main designs in this project include:": "we propose a fusion block to incorporate the seman-tic information from the ViT feature for Video ObjectSegmentation. We use the cls token from a pretrainedViT backbone to extract the semantic prior and globalaverage polling of the image patches to extract theglobal information of the current frames. Furthermore,we design a local fusion to leverage the spatial infor-mation from the ViT features. The proposed frame-work shows robustness when facing multiple complexsequences.",
  ". Our solution": "To solve the problems of VOS, we propose a robustsemantic-aware and query-enhanced video object segmen-tation method. In this solution, we first introduce the pro-posed fusion block, which utilizes the semantic and detailedinformation of the pretrained ViT models. This helps usdeal with complex target appearance variance and ID con-fusion between targets with similar appearances. In detail,we fuse the information of the cls token from the ViT tomulti-scale features and conduct local fusion between framepatches and multi-scale features for detailed fusion. In addi-tion, to ensure the target representation of the target queries,we develop a discriminative query representation module inthe query transformer to capture the local representation ofthe targets. Details of the proposed solution are describedas follows.",
  ". Discriminative Query Generation": "We note that updating the target query memory directlywith entire object patches generated based on online pre-dicted masks is ineffective as the predicted masks oftencover background noise, reducing target distinctiveness andleading to accumulating errors over time. To propagate tar-get queries effectively across frames, we update the targetqueries with the most distinctive feature of the target object. In detail, we select the discriminative feature of a targetobject by comparing the target query with every channel ac-tivation in the correlated feature map of the target and takingthe most similar one. Based on the discriminative target fea-ture generated from a new target sample, we can update tar-get queries by dynamically calculating the relationship be-tween the salient query and salient pixel features in an addi-tive manner. The proposed discriminative query generationscheme adaptively refines target queries with the most rep-resentative features, which helps deal with the challenges ofdramatic appearance variations in long-term videos.",
  ". Challenge Description": "The Pixel-level Video Understanding in the Wild(PVUW) Challenge features four tracks.This year, twonew tracks: the Complex Video Object Segmentation Track,based on MOSE , and the Motion Expression GuidedVideo Segmentation Track, based on MeViS are in-cluded.These new tracks include additional videos andannotations that present challenging elements such as ob-ject disappearance and reappearance, small inconspicuousobjects, heavy occlusions, and crowded environments inMOSE. Furthermore, the MeViS dataset is provided to ex-plore natural language-guided video understanding in com-plex settings. These enhancements aim to promote the de-velopment of more comprehensive and robust pixel-levelvideo scene understanding in complex and realistic scenar-ios.",
  ". Implementation Details": "Training. Our training settings are similar to Cutie .To enhance the performance of our model, we utilize theMEGA dataset constructed by Cutie, which includes theYouTubeVOS , DAVIS , OVIS , MOSE , andBURST datasets. We sample eight frames to train themodel, and three are randomly selected to train the match- ing process. For each sequence, we randomly choose atmost three targets for training. The point supervision in lossis adopted to reduce the memory requirements. We trainthe model for 195k on the MEGA dataset. All our modelsare trained on 8 x NVIDIA V100 GPUs and tested on anNVIDIA V100 GPU.",
  "Inference. Our feature and query memory is updated every3rd frame during the testing phase. For longer sequences,": "we employ a long-term fusion strategy for updating. Toenhance storage quality, we skip frames without targets anddo not store them. The test input size contains two scales:720 for general size and 1080 for small targets. The finalscore is a version of multi-scale fusion .Evaluation Metrics. We use mean Jaccard J index andmean boundary F score, along with mean J &F to evaluatesegmentation accuracy.",
  ". Results": "The proposed solution achieves 1st place on the com-plex video object segmentation track of the PVUW Chal-lenge 2024, as listed in 1. In addition, we also show someof our quantitative results in and . It can be seenthat the proposed solution can accurately segment small tar-gets and distinguish similar targets in some difficult scenar-ios which have severe changes in object appearance, andconfusion of multiple similar objects and small objects. Inthe five submissions, we find that some inference parame-ters influence the performance, which are the test size, thememory interval, memory or not, the flip augmentation, andmulti-scale fusion.",
  ". Conclusion": "In this paper, we propose a robust solution for the taskof video object segmentation, which helps the model under-stand the semantic information of the targets and generatediscriminative queries of the target. In the end, we achieve1st place on the complex video object segmentation trackof the PVUW Challenge 2024 with 84.45% J &F. Thedetailed version is under peer review. The code and fullversion will be released as soon as possible. Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khu-rana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst:A benchmark for unifying object recognition, segmentationand tracking in video. In WACV, pages 16741683, 2023. 4"
}