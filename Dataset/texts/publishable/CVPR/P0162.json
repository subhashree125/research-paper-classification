{
  "Abstract": "Egocentric vision aims to capture and analyse theworld from the first-person perspective.We explore thepossibilities for egocentric wearable devices to improveand enhance industrial use cases w.r.t.data collection,annotation,labellinganddownstreamapplications.This would contribute to easier data collection andallow users to provide additional context.We envi-sion that this approach could serve as a supplementto the traditional industrial Machine Vision workflow.Code, Dataset and related resources will be available at:",
  ". Introduction": "The field of Egocentric Computer Vision has seen in-creased attention in recent years . This has beencatalysed due to the increased mainstream focus on wear-able Augmented Reality (AR) and Virtual Reality (VR) de-vices . The Computer Vision community has intro-duced several novel datasets in recent years, with an aim tounlock and explore new challenges and innovations in thisarea . These large and diverse datasets capture hu-mans in varying everyday scenarios.In contrast, we focus on industrial production scenar-ios. Industry 4.0, or smart manufacturing, focuses on digitaltransformation of product development, including manufac-turing, use, maintenance, and recycling . There is a sig-nificant gap between the current state-of-the-art in ArtificialIntelligence (AI) and Computer Vision Research, and its in-tegration into traditional production systems . The bot-tleneck often tends to be the digitisation of workflows andthe inability to capture the expertise of the Subject-MatterExperts (SMEs) proficiently.The conventional exocentric/allocentric data collectionin the industry is summarised in a, which requirescareful labelling, annotation and documentation for trainingAI models or knowledge transfer. In this ongoing research",
  ". Related Work": "Egocentric Computer Vision. Understanding the worldfrom the first-person perspective is intuitive for humans, butposes several challenges for conventional Machine Visionand AI methods . Several iterations and configurationsof wearable devices have been proposed . Suchdevices enable additional user specific data to be capturedalongside visual data, such as eye-gaze, hand pose, voice in-teraction. Several novel datasets and benchmarks have beenreleased in recent years, which introduce new challengesand research directions for the community.Industrial Machine Vision.Traditional image pro-cessing has led to several important advancements in the in-dustry, which is further accelerated by deep learning based",
  "arXiv:2406.07738v1 [cs.CV] 11 Jun 2024": ". A summary of the proposed pipeline. The User/SME wearing the egocentric device interacts with the object/machinery anddocuments their observation in natural language. The multimodal dataset is then processed to obtain image/video data, and the transcription,eye-gaze, hand interaction provides the labels and annotations, along with metadata. Top: Point cloud reconstruction example from a usecase. Bottom: A conceptualisation of the data processing.",
  ". Methods": "Proposed Pipeline. shows the planned imple-mentation in an industrial setting. We use the Meta Ariaglasses as the data capturing device. The multimodaldata captured by the user is then processed to extract themost meaningful information about the process, or the ma-chinery.The user guidance via voice serves as the leadindicator for understanding which portion of the continu-ous stream of the data should be processed. The audio datais processed via a custom language model setup, to obtainstructured metadata and labels about the given portion ofthe stream. The camera stream data, augmented by user interaction (eye-gaze or hands), is processed and synchro-nised with the audio description data to add annotation andcontext (e.g. object labels, defects, miscellaneous observa-tions). Additional processed data, such as user trajectory,location and other modalities would also be valuable foradding more context to the captured data.Current egocentric datasets primarily encompass every-day and outdoor activities, with sparse representation ofindustry-specific scenarios, necessitating domain-specificdata collection. Fine-grained activities like screwing andunscrewing bolts require high-resolution classification dueto their visually similar but functionally distinct nature. Weplan to open-source our data to encourage the broader re-search community to address such problems.Challenges. Industrial Machine Vision often requirescontrolled settings and high precision image processing.Egocentric data capture cannot fully replace standard digi- tisation stations and setups. In such cases, egocentric datawould augment and assist the user in understanding theworkflows and operations.Capturing user guidance viavoice may be challenging due to noise, presence of otherloud voices or perceived discomfort. In such cases, con-trolling parts of the user input via hand gestures or othermeans may be valuable. Additionally, capturing user eyegaze, hand gestures and other personal data poses inher-ent challenges in such cases. Egocentric AI systems pro-duce large volumes of data, challenging the processing ca-pabilities of on-device hardware. This necessitates adap-tive, privacy-centric continual learning strategies and opti-misation of data transfer to mitigate compute and bandwidthbottlenecks.Downstream Applications.We explore diverse ap-plications of egocentric computer vision in industrial set-tings, aimed at enhancing operational efficiency and accu-racy . Key applications include improving data collec-tion and annotation through automated processes, enablingprecise part recognition and analysis, and facilitating defectand anomaly labelling. Additionally, we investigate sceneunderstanding and action recognition within these environ-ments, offering substantial support for operators throughreal-time assistance and guidance. The system would alsoplay a crucial role in training and knowledge transfer, en-suring that new and existing employees quickly adapt toevolving industrial demands.Each of these applicationsunderscores the potential of egocentric computer vision toaugment traditional industrial operations, making processesmore intuitive and interactive.Continual Learning. NN models often suffer due tocatastrophic forgetting when they are retrained on newerdata . It is necessary to develop systems that learncontinuously and perform increasingly better on the mostimportant tasks, while still retaining the knowledge fromprevious broad scale training. We believe such adaptive andintelligent systems would provide users with a more mean-ingful and helpful experience over time. For our work, wefocus on a limited number of use cases and working en-vironments. The participants perform similar tasks on dif-ferent industrial objects multiple times over several weeks.These include tasks involving similarities w.r.t. the objects,working environments, and the actions being performed.One of the key challenges with training and continuallyimproving ML models based on egocentric inputs is han-dling the large amount of multimodal data available to thesensors every second. Moreover, personal data (such as theuser eye-gaze and hand pose) or sensitive data (such as con-fidential office work, conversations at home or work) maynot be suitable to be saved and sent for training. Hence,a federated or distributed learning paradigm would be re-quired for handling the data and continually training themodels , as shown in . . A summary of a distributed Continual Learning frame-work for egocentric applications. The three layers of applicationinclude personal (top), organisational (middle) and global (bot-tom). The most sensitive information is stored and processed bythe personal computing setup with limited compute. The organi-sational layer trains the local models incrementally, which receiveuser feedback and related data from the egocentric device. Theglobal foundational models require large amounts of data, whichcould be periodically shared by the organisation (after anonymiza-tion and review).",
  ". Summary": "In this extended abstract, we propose an approach forautomated data collection and labelling for industrial usecases. The methods and challenges were briefly discussed.This undertaking brings several eccentric benchmarks andtasks, including scene understanding, object detection andtracking, diarisation, action recognition, hand, and eyetracking, among others. We believe such workflows couldsignificantly reduce the efforts required for digitisation andautomation, and would improve knowledge transfer be-tween SMEs and trainees, and aid the development of con-text aware models. We thank the Meta AI team and Reality Labs for theProject Aria initiative, including the research kit, the opensource tools and related services. The data collection forthis study is carried out at the IWF research labs of TUBerlin. Apple. Apple vision pro, 2024. Accessed: 2024-05-10. 1 Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Sanja Fidler, Antonino Furnari, Evangelos Kazakos, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, andMichael Wray. Scaling egocentric vision: The epic-kitchensdataset. In Proceedings of the European Conference on Com-puter Vision (ECCV), 2018. 1 Jakob Engel, Kiran Somasundaram, Michael Goesele, Al-bert Sun, Alexander Gamino, Andrew Turner, Arjang Talat-tof, Arnie Yuan, Bilal Souti, Brighid Meredith, Cheng Peng,Chris Sweeney, Cole Wilson, Dan Barnes, Daniel DeTone, David Caruso, Derek Valleroy, Dinesh Ginjupalli, DuncanFrost, Edward Miller, Elias Mueggler, Evgeniy Oleinik, FanZhang, Guruprasad Somasundaram, Gustavo Solaira, HarryLanaras, Henry Howard-Jenkins, Huixuan Tang, Hyo JinKim, Jaime Rivera, Ji Luo, Jing Dong, Julian Straub,Kevin Bailey, Kevin Eckenhoff, Lingni Ma, Luis Pesqueira,Mark Schwesinger, Maurizio Monge, Nan Yang, Nick Char-ron, Nikhil Raina, Omkar Parkhi, Peter Borschowa, PierreMoulon, Prince Gupta, Raul Mur-Artal, Robbie Pennington,Sachin Kulkarni, Sagar Miglani, Santosh Gondi, SaranshSolanki, Sean Diener, Shangyi Cheng, Simon Green, SteveSaarinen, Suvam Patra, Tassos Mourikis, Thomas Whe-lan, Tripti Singh, Vasileios Balntas, Vijay Baiyya, WilsonDreewes, Xiaqing Pan, Yang Lou, Yipu Zhao, Yusuf Man-sour, Yuyang Zou, Zhaoyang Lv, Zijian Wang, Mingfei Yan,Carl Ren, Renzo De Nardi, and Richard Newcombe. Projectaria: A new tool for egocentric multi-modal ai research,2023. 1, 2",
  "German Federal Ministry of Economics and Climate Action(BMWK). Industry 4.0, 2024. Accessed: 2024-05-10. 1": "Ian J. Goodfellow, Mehdi Mirza, Xia Da, Aaron C.Courville, and Yoshua Bengio. An empirical investigation ofcatastrophic forgeting in gradient-based neural networks. In2nd International Conference on Learning Representations,ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Confer-ence Track Proceedings, 2014. 3 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Mar-tin, Tushar Nagarajan, Ilija Radosavovic, Santhosh KumarRamakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray,Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, SiddhantBansal, Dhruv Batra, Vincent Cartillier, Sean Crane, TienDo, Morrie Doulaty, Akshay Erapalli, Christoph Feichten-hofer, Adriano Fragomeni, Qichen Fu, Abrham Gebrese-lasie, Cristina Gonzalez, James Hillis, Xuhua Huang, YifeiHuang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kot-tur, Anurag Kumar, Federico Landini, Chao Li, YanghaoLi, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu,Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, WillPrice, Paola Ruiz, Merey Ramazanova, Leda Sari, KiranSomasundaram, Audrey Southerland, Yusuke Sugano, Rui-jie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi,Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall,Dima Damen, Giovanni Maria Farinella, Christian Fuegen,Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Han-byul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, AudeOliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, JianboShi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torre- sani, Mingfei Yan, and Jitendra Malik. Ego4d: Around theworld in 3,000 hours of egocentric video. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1899519012, 2022. 1 Kristen Grauman, Andrew Westbury, Lorenzo Torresani,Kris Kitani, Jitendra Malik, Triantafyllos Afouras, KumarAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote,Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, MariaEscobar, Cristhian Forigua, Abrham Gebreselasie, SanjayHaresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain,Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin,Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa,Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun So-mayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang,Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu,Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, JiaboHu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Ku-mar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo,Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumin-inu Oguntola, Xiaqing Pan, Penny Peng, Shraman Praman-ick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran So-masundaram, Chenan Song, Audrey Southerland, MasatoshiTateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, MingfeiYan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, ChenZhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez,Gedas Bertasius, David Crandall, Dima Damen, Jakob En-gel, Giovanni Maria Farinella, Antonino Furnari, BernardGhanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe,Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva,Jianbo Shi, Mike Zheng Shou, and Michael Wray.Ego-exo4d: Understanding skilled human activity from first- andthird-person perspectives, 2024. 1 Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman. Dis-covering important people and objects for egocentric videosummarization. In 2012 IEEE Conference on Computer Vi-sion and Pattern Recognition, pages 13461353, 2012. 1",
  "Daniele Mazzei and Reshawn Ramjattan. Machine learningfor industry 4.0: A systematic review using deep learning-based topic modelling. Sensors, 22(22), 2022. 1, 2": "Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Sid-dhant Bansal, Francesco Ragusa, Giovanni Maria Farinella,Dima Damen, and Tatiana Tommasi. An outlook into thefuture of egocentric vision, 2024. 1, 3 Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari,Holger R Roth, Shadi Albarqouni, Spyridon Bakas, Math-ieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al.The future of digital health with federated learning. NPJ dig-ital medicine, 3(1):17, 2020. 3"
}