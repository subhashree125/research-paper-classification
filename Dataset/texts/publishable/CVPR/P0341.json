{
  "Continual Learning in the Presence of Repetition": "Kulis, B., 2013. Metric learning: A survey. Foundations and Trends in Machine Learning 5, 287364. Le, Y., Yang, X., 2015. Tiny ImageNet Visual Recognition Challenge. Technical Report. Stanford University. Lesort, T., Lomonaco, V., Stoian, A., Maltoni, D., Filliat, D., Daz-Rodrguez, N., 2020. Continual learning for robotics: Definition, framework,learning strategies, opportunities and challenges. Information Fusion 58, 5268. URL:",
  "A B S T R A C T": "Continual learning (CL) provides a framework for training models in ever-evolving environ-ments. Although re-occurrence of previously seen objects or tasks is common in real-worldproblems, the concept of repetition in the data stream is not often considered in standardbenchmarks for CL. Unlike with the rehearsal mechanism in buffer-based strategies, wheresample repetition is controlled by the strategy, repetition in the data stream naturally stemsfrom the environment. This report provides a summary of the CLVision challenge at CVPR2023, which focused on the topic of repetition in class-incremental learning. The report initiallyoutlines the challenge objective and then describes three solutions proposed by finalist teamsthat aim to effectively exploit the repetition in the stream to learn continually. The experimentalresults from the challenge highlight the effectiveness of ensemble-based solutions that employmultiple versions of similar modules, each trained on different but overlapping subsets of classes.This report underscores the transformative potential of taking a different perspective in CL byemploying repetition in the data stream to foster innovative strategy design.",
  ". Introduction": "Designing systems that can learn and accumulate knowledge over time in non-stationary environments is animportant objective in artificial intelligence research . In traditional machine learning, however, the commonpractice is to build and train models based on statistical learning assumptions. Under these assumptions, a modelhas access to a static dataset with samples that are independent and identically distributed (IID). This implies thatboth the training set and test set come from the same distribution and remain unchanged throughout the training andevaluation processes. In reality, these assumptions are often violated, and the model is exposed to different forms ofshift in the data. To address such shifts, continual learning (CL) offers a framework to simulate the never-endinglearning setting . In CL, a model is exposed to a data stream consisting of a potentially unbounded sequence ofexperiences . Unlike under the statistical learning assumptions, in CL, the model does not have full accessto a static dataset. Instead, it gets non-IID access to the data distribution in the form of a data stream, as data becomepartially available in an incremental manner. In a specific variant of CL, referred to as batch continual learning ortask-based continual learning, the model gets locally IID access to part of the data distribution. These locally IID partsof the data stream have been referred to as tasks, contexts, or experiences.A central objective in CL is to design models that mitigate catastrophic forgetting . Catastrophic forgettingrefers to the phenomenon where a models performance on a previously learned task rapidly and drastically declines asthe model continues to train on data from other tasks. To tackle the issue of forgetting in neural networks, researchershave proposed various strategies. Some strategies add regularization terms to the loss to penalize changes to parts of",
  ". Previously encountered classes can re-occur with varying repetition patterns": "Since many existing strategies in the CL literature have only been tested in repetition-free settings, their efficacy andperformance in the presence of repetition remain unclear. To investigate the influence of repetition in the data streamon the relative effectiveness of different strategies, a set of CIR benchmarks was generated using a sampling-baseddata stream generator controlled by four interpretable parameters (see subsection 2.1). Participants were tasked withdeveloping strategies that, upon completing training on the entire stream, could achieve high average accuracy on a testset containing unseen samples from the same classes present in the stream. To allow participants flexibility in strategydesign without the need for significant computational resources, we used CIFAR-100 , a relatively small-scaledataset of natural images (32 32 RGB images, 100 classes), as the base dataset for the stream generator.The competition consisted of two phases: Pre-selection Phase The pre-selection phase took place between the 20th of March 2023 and the 20th of May 2023.For this phase, there were three challenge data streams, which were released on the GitHub repository1 of the challenge.The participants were asked to submit their solutions in the form of model predictions on the test set after training oneach stream. Appendix B provides details on the participation over time in the pre-selection phase. Final Evaluation Phase The final phase started from the 20th of May 2023. In this phase, the five highest-rankingparticipants from the pre-selection phase were asked to send the source code of their strategies. Their strategies weretested on three new data streams, and the average accuracy on the test set after training on each of these three newstreams was used as the metric for the final ranking. Participants were allowed to make changes to their strategiesbetween the pre-selection phase and the final evaluation phase.",
  "Class-IL with Repetition": ": Illustration of transitioning from standard Class-IL to cumulative Class-IL by increasing the probabilityof repetition for seen classes, which is set by control param-eter . Class-IL: class-incremental learning. By default, an equal number of samples is assigned toall classes present in each experience. , which controlsthe first occurrence\" property of the stream, determineswhen dataset classes make their initial appearance. Forinstance, in one stream, all classes might appear forthe first time at the outset, whereas in another stream,new classes might appear gradually. Once a class hasappeared for the first time, it re-appears based on aper-class repetition probability, which is controlled by. illustrates how increasing the repetitionprobability from 0 to 1 corresponds to a transition fromstandard class-incremental learning, where each classis present in only one experience, to cumulative class-incremental learning, where each class re-appears in eachexperience after its first occurrence. While the repetitionprobability can be dynamically set based on a time-varying probability mass function, in this challenge, the repetition probability for each class remains fixed over time.This means that, in this challenge, is a list of probability values, with one value assigned to each class in the dataset.",
  ": Examples of generated streams with a Geometric distribution over the first occurrences of the datasets classes,and a Zipfian distribution for the repetition of classes along the stream": "Selected Challenge Stream Parameters The settings used for and in each of the challenge data streams areprovided in Appendix C. The number of experiences per stream () was fixed as 50, and the number of samples perexperience () was set to 2000. Following the default setting, the number of samples in each experience was equallydistributed across the classes present in the experience. 2.2. ParticipationParticipation was in the form of teams. Team registrations and solution submissions were handled using the Co-daLab platform . Each team was allowed to create only one account on the platform to prevent parallel submissions.Participants were provided with a DevKit based on the Avalanche library : The DevKit contains all necessary scripts and codes to load the benchmarks for the challengeconfigurations. Researchers who are interested in evaluating the performance of their own strategies can use the DevKitfor both pre-selection and final phase streams. 2.3. Rules and RestrictionsSubmission: Each submission in the pre-selection phase consisted of three sets of predictions. Each set ofpredictions was made by testing the model against the challenges test set, after training the designed strategy onone of the three challenge streams.Strategy Design: Within each experience in a data stream, users had full access to the training data from thatexperience, but not to the data from other experiences. In the default settings of the DevKit, the number of epochs perexperience was set to 20. Participants had the flexibility to tweak and tailor the epoch iterations and dataset loading.For instance, a possible strategy would be to iterate for more epochs in the initial experiences and fewer in the finalones.Model Architecture: All participants were required to utilize the (Slim-)ResNet-18 provided in the DevKit asthe base architecture for their models. However, they were allowed to use multiple copies and to add additionalmodules, e.g. gating modules, provided they adhered to the maximum GPU memory usage (4000 MB) allowed forthe competition during training. The GPU memory limit is set to almost eight times the memory size needed to trainthe Slim-ResNet-18 backbone with a batch size of 64. This allows for considerable flexibility in adding additionalmodules to the backbone.Replay Buffer: Using a replay buffer to store dataset samples was not allowed. However, buffers could be used tostore any form of data representation, such as the models internal representations of data samples. Irrespective of thebuffer type, the buffer size (i.e., the total number of stored exemplars) was not allowed to exceed 200.Hardware Limitations: To encourage a comparison between strategies that use similar computational resources,participants were limited to using one GPU for training, and the maximum training time for each run was capped at500 minutes. No computing limitations were specified for the inference time.",
  ". Strategy 1: HAT-CIR": "The strategy proposed by team xduan7 is called HAT-CIR. This strategy combines the strengths of networkreplicas and test-time decision-making, along with other elements, such as Hard Attention to the Task (HAT) and Supervised Contrastive Learning (SupCon) . 3.1. Motivation and Related WorkIn CIR, as more generally with class-incremental learning, comparing model output logits across different classes isdifficult because images from different classes are not jointly used for training. One potential solution is using a memorybuffer and replaying samples to calibrate the logits, but due to restrictions on the storage of data, this approach is notwell suited for this challenge. Another strategy involves parameter isolation. Parameter-isolation methods divide theneural network into segments, each responsible for a specific task or experience, preventing the network from forgettingknowledge acquired from prior experience. For example, HAT employs trainable binary masks to partition the networkbased on the experience identifier. This has proven effective, particularly in task-incremental learning problems wherethe experience identifier is provided during both training and test phases. However, the parameter-isolation strategyencounters challenges when applied to the CIR setting, where experience identifiers are unavailable during testing.To address the experience identifier limitation in parameter-isolation methods within the CIR setting, OODdetection techniques can be used to enable task-agnostic inference. Such techniques can detect samples that arenot from the current experience, as for example demonstrated in and . Another option is to use representationlearning methods like SupCon to make the output logits more comparable across different classes. This has beenexplored in previous works (e.g., ) and has been shown to be effective when applied to the class-incrementallearning setting.In light of these existing works, and considering the specific challenges posed by the competition, the subsequentsections will elaborate on how to adapt and integrate these strategies to propose a novel solution for the CIR setting. 3.2. Method DescriptionThe proposed method comprises three core components: (i) structural design, featuring HAT-based partitioningand network replicas; (ii) a two-phase training strategy, including supervised contrastive learning and classification;and (iii) a momentum-based inference mechanism for test-time decision making. A schematic of the method is shownin . 3.2.1. Structural DesignHAT-based Partitioning To mitigate catastrophic forgetting, HAT isolates network parameters based on experienceIDs. However, the original HAT method suffers from slow training and hyperparameter sensitivity when handling alarge number of experiences. To overcome this, HAT-CL is used, which initializes masks to ones and uses a cosinemask scaling curve to facilitate better alignment with network weights. By using the cosine mask scaling curve, eachtraining epoch is divided into three phases:",
  ". Train weights while masks are mostly ones2. Train masks and weights together to make masks more sparse3. Fine-tune weights while masks are mostly ones again": "These changes significantly improve the training speed and stability, and performance of HAT. Furthermore, the effectof the regularization term for the masks is gradually reduced. This step ensures full network capacity utilizationand provides an accounting for the number of classes in each experience through variable regularization terms. Itis important to note that the HAT-based partitioning was only used in the pre-selection phase; for the final phase, onlynetwork replicas were used, which led to higher performance. Network Replicas Two types of network replicas, fragments and ensembles, are used to increase the model capacityand improve performance. Fragments refer to network replicas trained on different experiences. For example, if thereare ten experiences, five fragments might be trained, each on two experiences. Ensembles are replicas trained on thesame experiences. For each experience, multiple ensemble replicas will be trained on the same samples with differentinitializations and data augmentation. Thus, their predictions are averaged for final decision-making. These two replicatypes are complementary and can be combined with or without HAT-based partitioning. Importantly, adding fragmentreplicas does not increase the computational costs during training, while adding ensemble replicas does. Due to the",
  "=1max (0, (( ), ( )) (( ), ( )) + )": "In this equation, () produces the embedded feature vector for input , (, ) denotes a distance function, , ,and represent the anchor, positive, and negative samples, respectively, and is a margin parameter. The numberof samples in a batch is denoted by . The training of hard attention masks occurs exclusively in this supervisedcontrastive learning phase, due to their sensitivity to learning rates and epoch numbers.In the second training phase of each experience, the parameters of the network are trained further using a standardcross-entropy classification loss.",
  ". DiscussionIn this section, the limitations and possible improvements for the method are critically assessed, as well as theimplications of the findings for future research": "Limitations A significant drawback of the approach arises when the number of classes is small in the initialexperiences. In such cases, the network fails to learn effective class representations due to the limited diversity. Thisnegatively impacts the performance of the momentum-based test-time decision-making strategy. Another inherentlimitation is related to the use of HAT. The rigidity of the network structure requires careful hyperparameter tuning tomatch the expected total number of experiences, which is not always known in advance in CIR settings. As a result,suboptimal parameter allocation might be achieved, leading to compromised performance. Future Directions The experiments show that the methods performance scales positively with the number of net-work replicas, implying that network capacity plays a significant role. Utilizing larger networks, possibly incorporatingdifferent structures with pre-trained weights, might offer avenues for further improvement in performance.",
  ". Strategy 2: Horde": "The strategy proposed by team mmasana is called Horde. This strategy learns an ensemble of feature extractors(FEs) on selected experiences, which should provide robust features useful for discriminating between seen and unseendownstream classes. After training, the learned FEs are frozen to obtain a zero-forgetting ensemble. However, since notall classes are present in each experience, the outputs need alignment to balance them effectively. To do this, Horde usespseudo-feature projection on the outputs to retain as much discrimination between classes as possible, while learning aunified head capable of discriminating between all classes seen so far. To further facilitate the pseudo-feature projection,FEs are trained with both the usual cross-entropy loss and an additional metric learning loss, which promotes alignmentamong the learned classes within each feature space. 4.1. Motivation and Related WorkAn important motivation for the proposed approach is balancing the stability-plasticity dilemma . This balanceinvolves retaining useful knowledge from previous experiences while learning new ones from the sequence. Forplasticity, robust and discriminative representations are important, as they help adapt to new classes and promotegeneralization (or forward transfer). Horde encourages the learning of such representations by combining a metriclearning loss and the usual cross-entropy classification loss. Support for stability can be provided by zero-forgettingmethods , which freeze the feature extractor part of model after training or apply masks to the parametersor the intermediate representations . The set up of the challenge excludes the use of the experience ID at testtime and the use of pretrained models, both of which limit the direct application of existing zero-forgetting methods.However, the enforcement of the stability component via freezing is preserved for Horde and represented with the useof an ensemble of FEs that retain all learned knowledge.Initial inspiration for Horde is provided by the existing methods Feature Translation for Exemplar-Free Class-Incremental Learning (FeTrIL) and Ternary Feature Masks (TFM) . FeTrIL uses a pretrained frozenbackbone to benefit from the stability of having zero-forgetting for that part of the model, while the plasticity isintroduced via the learning of a unified head. This method has a pseudo-feature generator that uses the representations",
  "New Classes Seen": ": The panels indicate for stream 1 (left) and stream 2 (right) in which experiences Horde decided to add a newFE (indicated by bright colors), while comparing the experiences based on three factors: the total number of classes in theexperience (blue), the number of classes in the experience on which no FE had been trained yet (yellow), and the numberof new, unseen classes in the experience (red). The black line represents the total amount of classes seen so far. from a single FE and applies a geometric translation to align both past and new classes. The pretrained backbone couldbe replaced by learning on the first experience, although, the method heavily relies on the initial training covering amajority of classes such that the resulting feature extractor is expressive and discriminative enough. TFM freezesthe existing model and extends it at each experience while reusing the representations from previous experiences. Thismethod applies masking on the outputs of each layer to control the flow of gradients so that previous knowledge canbe used but its corresponding weights are not modified. However, the dynamic architecture requires the experience IDat test time. Both methods break the challenge rules in multiple ways when used as proposed in their original works.Horde is inspired by these two methods by having a dynamic zero-forgetting architecture for the ensemble andmaking use of a pseudo-feature generation approach for learning the unified head. The approach is adapted to thechallenge by extending the pseudo-feature generation with the usage of the standard deviation, and using the contrastiveloss to improve the learned shape of the representations, and avoiding the issues for the pseudo-featuregeneration. Furthermore, the proposed pseudo-feature generation method is adapted to the challenge scenarios thatcontain class repetition. 4.2. Method DescriptionThe proposed strategy Horde combines the feature representations of individual FEs into a single unified headcapable of predicting all classes seen so far. This is achieved through a two-step training process: first, the learning ofan FE (on selected experiences only), and second, the pseudo-feature alignment for adapting the unified head (on eachexperience). Each individual FE is an expert model trained on a single experience, after which it is frozen and addedto the ensemble. In the second training step, data are passed through all the ensembles models, and the unified head isfine-tuned. Training the unified head involves using representations directly from the expert FEs familiar with a classand the pseudo-feature projections from the representations of FEs not trained on that class.When to add a new FE to the ensemble: For the specific settings of the competition, two constraints (heuristics)were designed to determine when to add an FE to the ensemble. First, to constrain the presence of overly overfittedFEs and to limit the size of the ensemble, experiences with fewer than five classes are not considered. Second, theaddition of FEs to the ensemble is stopped after 85% of the classes have been seen, because once robust features formost classes have been learned, good performance on the remaining ones is expected. Additionally, an FE is alwaystrained on the first experience. With these constraints, the proposed approach learns feature extractors based on thehighlighted experiences shown in .",
  ",": "where , is the estimated projection from class to class . This projection is applied to each sample in the trainingbatch while learning the unified head, and not used during evaluation. The target class is chosen at random frompreviously learned classes, and both the original representation and the projected one are added onto the loss. Theclass prototypes (i.e., the mean and standard deviation ) are always updated before the unified head is trained bycalculating the statistics over the available class data. However, since multiple FEs exist, access might not be availableto the mean and standard deviation for each class, depending on when they were learned, or if those classes haveappeared since that time. Therefore, if the class statistics are unknown for a feature extractor , estimates are neededfor , and ,. We fix the estimation of the standard deviation to = 1, as we do not have any information about theclass shape for this feature extractor. For the estimation of the unknown mean statistic of a specific FE output, we resortto the simple heuristic of using the original representation of the current sample without modification: , = ,.",
  ". Strategy 3: DWGRNet": "The strategy proposed by team pddbend is called Dynamic Weighted Gated Representation Network (DWGRNet).This strategy creates independent branches for each experience, and uses gating units to control which branches areactive. During training, the branch corresponding to the current experience is activated by its gating unit, facilitatinglearning, while branches from previous experiences remain inactive. To improve the models generalization androbustness, data augmentation techniques, such as AugMix , are used to enhance the data samples. In the testingphase, the gating units control the sequence of predictions. Importantly, using predictions from all branches, as donein , may not always yield optimal results. This is because many branches may not have been exposed to a particularclass, leading to overconfident and potentially inaccurate predictions. This issue resembles an open-set recognitionproblem. To mitigate this, DWGRNet assigns weights based on entropy, feature norm, and the number of classesexperienced by each branch. Specifically, the entropy of each branchs predicted probability distribution is assessed.High entropy indicates the possibility of the sample being an open-set item for that branch. Similarly, the feature normis computed. A higher feature norm suggests that the sample is likely to be an open-set sample. Finally, it is posited thatexperiences with a larger number of classes will render the models prediction more reliable. As a result, weightingcan also adjusted based on the number of classes in each experience. 5.1. Motivation and Related WorkTo achieve a good balance between stability and plasticity in class-incremental learning, reference introducedthe method Dynamically Expandable Representation (DER). DER decouples the adaptivity of feature representationfrom the classification procedure. When faced with a new task that contains novel categories, the DER method firstfreezes the previously learned feature extractor. Then, it incorporates a new feature extractor to expand the main featureextractor network. Ultimately, the features extracted by all the extractors are combined and forwarded to the classifiermodule for predicting the category. This strategy exhibits notable efficiency in preserving existing knowledge whileproviding sufficient flexibility to capture new information.However, the DER architecture cannot be directly employed in the context of this competition. This is becauseDER constructs a model ensemble that exploits replay buffers for raw sample storage. Another drawback of DER isthe simultaneous execution of predictions across all branches, which has high computational complexity. In practice,DER applies predictions from each branch without distinction. A problem with this is that certain branches produceoverconfident predictions for unseen classes, leading to sub-optimal performance.To fix the limitations of DER, DWGRNet implements an open-set approach . Within this framework,each model is restricted to be exposed to a limited number of classes. This approach is inspired by recent advancementsin open-set recognition literature that have elucidated the relationships between logit entropy, feature norm, and out-of-distribution (OOD) samples. Capitalizing on these findings, a scoring system is designed for OOD samples. Thisscoring system requires the evaluation of each model to determine whether a test sample falls within its OOD category,prior to the ensembling process. This evaluation allows two possible options: (1) to either suppress the OOD outputsor (2) to harness the OOD scores to assign weight to each sample. Therefore, it serves to alleviate the constraints ofthe DER method, with the overarching aim of augmenting both its performance and applicability in the realm of CIR. 5.2. Method DescriptionDWGRNet uses gating units to control the activation of each branch. A new branch is added with each newexperience and then activated, while the model parameters in the old branches are kept frozen. As illustrated ina, during the training phase, no special loss functions or replay buffers are used. Instead, the standard cross-entropy loss is used to train the model, while using AugMix to enhance the models generalization and robustness.AugMix combines different data augmentation techniques. During the testing phase, the gating units could activateeach branch one by one to avoid the need for a large GPU memory. Their outputs are collected and then used to makethe final prediction. This process is illustrated in b.This method is similar to a model ensemble. It is designed in a way that both complies with the competition rulesand also speeds up training time and reduces memory space during training and inference. On the other hand, thenaive model ensembling approach may not work properly as each model only learns limited knowledge. Therefore,",
  ". Results": "6.1. Pre-selection PhaseIn the pre-selection phase of the challenge, the teams were ranked based on their average accuracy. The averageaccuracy for each team was computed as the mean accuracy on the test set after training on each of the three streamsreleased for the first phase, namely 1, 2, and 3. The top ten teams in the pre-selection phase are shown in .The top five teams proceeded to the final phase. 6.2. Final PhaseIn the final phase of the challenge, three streams, namely 4, 5, and 6, with configurations different from thosein the pre-selection phase streams, were used to evaluate the solutions submitted by the finalist teams. The details ofthe challenge stream configurations are given in Table C.1 in the Appendix C. The final phase demonstrated substantial",
  "Results from the pre-selection phase for the top ten teams. Shown is the accuracy (as %) on the test set of CIFAR-100after training on each of the three streams from the pre-selection phase": "variations in performance among the solutions of the finalist teams, as presented in . Team xduan7 ()was selected as the challenge winner with an average accuracy of 62.75%, thereby substantially outperforming theother finalists. Team linzz earned the second spot with an average accuracy of 45.02%. However, they decided not tocontribute their solution to this report. The other two teams, mmasana () and pddbend (), achievedaverage accuracies of 41.11% and 40.91%, respectively. Recorded video presentation of the submitted solutions by thefinalist teams can be accessed through Baselines To put the performance of the solutions of the finalist teams in perspective, we also report in the performance of several popular CL baselines strategies after training on each of the three data streams from thefinal phase. The approach Naive refers to sequential fine-tuning the Slim-ResNet-18 backbone without any continuallearning mechanism. For Elastic Weight Consolidation (EWC) , a popular parameter regularization method, thelambda hyperparameter is set to 1; using higher or lower lambda values results in slightly lower average accuracy.For Learning without Forgetting (LwF) , a popular functional regularization method, the alpha and temperaturehyperparameters are set to their default values of 1 and 2, respectively. For Experience Replay (ER), two versions arerun: one with a memory buffer with total capacity of 200 samples and another with total capacity of 2000 samples.For all baselines the implementation of the Avalanche library (version 0.3.1) is used. For Joint, the backbone modelis trained in an iid fashion with access to samples from all classes at the same time.The comparison with the CL baseline strategies shows that there is a significant gap between the performance ofthe finalist solutions and these baselines. In particular, even ER with a moderately-sized buffer (i.e., 2000 samples)performs substantially worse than all of the finalist solutions. Notable is that the solution of team xduan7 approachesthe performance of jointly training the backbone model on all training data at the same time. 6.3. Additional Experiments6.3.1. No RepetitionTo test whether the repetition in the data stream plays an important role in the effectiveness of the finalist solutions,they are also evaluated after training on a data stream without repetition. For this, the standard CIFAR-100 class-incremental learning benchmark is used, with 20 experiences (or tasks) that are encountered one after the other,whereby each experience contains five distinct classes. The results in indicate that the performance of thefinalist solutions is significantly reduced when there is no repetition in the data stream. Importantly, while ER withbuffer size of 2000 was clearly outperformed by each of the finalist solutions on the data streams with repetition, whenthere is no repetition, this version of ER performs better than all the finalist solutions. This shows that repetition in thedata stream can change the relative effectiveness of different CL strategies. 6.3.2. Tiny ImageNetTo probe the generalizability of our results, in this section the finalist solutions are evaluated on three CIR datastreams generated using the Tiny ImageNet dataset . This dataset contains natural images of 6464 pixels, divided",
  "pddbend32.6836.6639.1222.28": "Results for the finalist solutions after training them on three CIR data streams constructed from the Tiny ImageNet dataset.Team linzz is not included in these additional experiments since they decided not to participate in the writing of this report. over 200 classes, of which we only use a random subset of 100 classes. The configurations to generate the data streamsare the same as those used for the data streams 4, 5 and 6 in the final phase of the challenge. The evaluation resultsare shown in . These results indicate consistency in the performance of the three finalist solutions relative tothe dataset used to generate the data streams.",
  "CRediT authorship contribution statement": "Hamed Hemati: Conceptualization, Software, Investigation, Visualization, Writing original draft. LorenzoPellegrini: Software. Xiaotian Duan: Methodology, Writing original draft. Zixuan Zhao: Methodology, Writing original draft. Fangfang Xia: Methodology, Writing original draft. Marc Masana: Methodology, Writing original draft. Benedikt Tscheschner: Methodology, Writing original draft. Eduardo Veas: Methodology, Writing original draft. Yuxiang Zheng: Methodology, Writing original draft. Shiji Zhao: Methodology, Writing originaldraft. Shao-Yuan Li: Methodology, Writing original draft. Sheng-Jun Huang: Methodology, Writing originaldraft. Vincenzo Lomonaco: Conceptualization. Gido M. van de Ven: Supervision, Conceptualization, Visualization,Writing original draft, Writing review & editing.",
  "Acknowledgements": "This work was supported by the Exascale Computing Project, a collaborative effort of the U.S. Department ofEnergy Office of Science and the National Nuclear Security Administration [grant number 17-SC-20-SC]; by TU-GrazSAL DES Lab, part of the University SAL Labs initiative of Silicon Austria Labs (SAL) and its Austrian partneruniversities for applied fundamental research for electronic based systems; by the National Science and TechnologyMajor Project [grant number 2022ZD0114801]; by a senior postdoctoral fellowship from the Research Foundation Flanders (FWO) [grant number 1266823N]; and by a Marie Skodowska-Curie fellowship under the European UnionsHorizon Europe programme [grant number 101067759]. Aljundi, R., Chakravarty, P., Tuytelaars, T., 2017. Expert gate: Lifelong learning with a network of experts, in: Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, pp. 33663375.",
  "Chen, Z., Liu, B., 2018. Lifelong Machine Learning, Second Edition. Synthesis Lectures on Artificial Intelligence and Machine Learning,Springer Cham": "Cho, S., Lee, H., Baek, S., Paik, S.B., 2024. Neuromimetic metaplasticity for adaptive continual learning. arXiv preprint arXiv:2407.07133 . De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., Tuytelaars, T., 2022. A continual learning survey:Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 33663385.",
  "Doan, T., Mirzadeh, S.I., Farajtabar, M., 2023. Continual learning beyond a single model, in: Proceedings of The 2nd Conference on LifelongLearning Agents, PMLR. pp. 961991. URL:": "Duan, X., 2023. HAT-CL: A hard-attention-to-the-task pytorch library for continual learning. arXiv preprint arXiv:2307.09653 . Feng, K., Zhao, X., Liu, J., Cai, Y., Ye, Z., Chen, C., Xue, G., 2019. Spaced learning enhances episodic memory by increasing neural patternsimilarity across repetitions. Journal of Neuroscience 39, 53515360. Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A.A., Pritzel, A., Wierstra, D., 2017. PathNet: Evolution channels gradientdescent in super neural networks. arXiv preprint arXiv:1701.08734 .",
  "Hsu, Y.C., Liu, Y.C., Ramasamy, A., Kira, Z., 2018. Re-evaluating continual learning scenarios: A categorization and case for strong baselines.arXiv preprint arXiv:1810.12488": "Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., Krishnan, D., 2020. Supervised contrastive learning.Advances in Neural Information Processing Systems 33, 1866118673. Kim, G., Esmaeilpour, S., Xiao, C., Liu, B., 2022a. Continual learning based on OOD detection and task masking, in: Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 38563866. Kim, G., Liu, B., Ke, Z., 2022b. A multi-head model for continual learning via out-of-distribution replay, in: Chandar, S., Pascanu, R., Precup,D. (Eds.), Proceedings of The 1st Conference on Lifelong Learning Agents, PMLR. pp. 548563. URL: Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska,A., et al., 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences 114, 35213526. Koh, H., Kim, D., Ha, J.W., Choi, J., 2022. Online continual learning on class incremental blurry task configuration with anytime inference,in: International Conference on Learning Representations. URL:",
  "Lesort, T., Ostapenko, O., Misra, D., Arefin, M.R., Rodrguez, P., Charlin, L., Rish, I., 2022. Scaling the number of tasks in continual learning.arXiv preprint arXiv:2207.04543": "Li, Z., Hoiem, D., 2017. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 29352947. Lomonaco, V., Pellegrini, L., Rodriguez, P., Caccia, M., She, Q., Chen, Y., Jodelet, Q., Wang, R., Mai, Z., Vazquez, D., et al., 2022. Cvpr2020 continual learning in computer vision competition: Approaches, results, current challenges and future directions. Artificial Intelligence303, 103635. Mallya, A., Lazebnik, S., 2018. PackNet: Adding multiple tasks to a single network by iterative pruning, in: Proceedings of the IEEE conferenceon Computer Vision and Pattern Recognition, pp. 77657773. Maltoni, D., Lomonaco, V., 2019. Continuous learning in single-incremental-task scenarios. Neural Networks 116, 5673. Masana, M., Liu, X., Twardowski, B., Menta, M., Bagdanov, A.D., Van De Weijer, J., 2023.Class-incremental learning: survey andperformance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 55135533. Masana, M., Tuytelaars, T., van de Weijer, J., 2021. Ternary feature masks: Zero-forgetting for task-incremental learning, in: Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 35703579.",
  "Mermillod, M., Bugaiska, A., Bonin, P., 2013. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting toage-limited learning effects. Frontiers in Psychology": "Mitchell, T., Cohen, W., Hruschka, E., Talukdar, P., Yang, B., Betteridge, J., Carlson, A., Dalvi, B., Gardner, M., Kisiel, B., et al., 2018.Never-ending learning. Communications of the ACM 61, 103115. Moon, J.Y., Park, K.H., Kim, J.U., Park, G.M., 2023. Online class incremental learning on stochastic blurry task boundary via mask and visualprompt tuning, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1173111741. Normandin, F., Ostapenko, A., Caccia, M., Riemer, M., Khetarpal, K., Golemo, F., Hurtado, J., Lesort, T., She, Q., Jia, X., Liu, Y., 2021.Continual Learning Challenge, CVPR 2021. Accessed: 2024-03-29. Oquab, M., Bottou, L., Laptev, I., Sivic, J., 2014. Learning and transferring mid-level image representations using convolutional neuralnetworks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 17171724.",
  "Parisi, G.I., Kemker, R., Part, J.L., Kanan, C., Wermter, S., 2019. Continual lifelong learning with neural networks: A review. Neural Networks113, 5471. URL:": "Pavao, A., Guyon, I., Letournel, A.C., Tran, D.T., Baro, X., Escalante, H.J., Escalera, S., Thomas, T., Xu, Z., 2023. Codalab competitions: Anopen source platform to organize scientific challenges. Journal of Machine Learning Research 24, 16. URL: Pellegrini, L., Zhu, C., Xiao, F., Yan, Z., Carta, A., De Lange, M., Lomonaco, V., Sumbaly, R., Rodriguez, P., Vazquez, D., 2022. 3rd continuallearning workshop challenge on egocentric category and instance level object understanding. arXiv preprint arXiv:2212.06833 . Petit, G., Popescu, A., Schindler, H., Picard, D., Delezoide, B., 2023. Fetril: Feature translation for exemplar-free class-incremental learning,in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 39113920.",
  "van de Ven, G.M., Siegelmann, H.T., Tolias, A.S., 2020. Brain-inspired replay for continual learning with artificial neural networks. NatureCommunications 11, 4069": "van de Ven, G.M., Tolias, A.S., 2018. Three continual learning scenarios, in: NeurIPS Continual Learning Workshop. van de Ven, G.M., Tuytelaars, T., Tolias, A.S., 2022. Three types of incremental learning. Nature Machine Intelligence 4, 11851197. Verwimp, E., Aljundi, R., Ben-David, S., Bethge, M., Cossu, A., Gepperth, A., Hayes, T.L., Hllermeier, E., Kanan, C., Kudithipudi, D.,Lampert, C.H., et al., 2024. Continual learning: Applications and the road forward. Transactions on Machine Learning Research URL: Verwimp, E., Yang, K., Parisot, S., Hong, L., McDonagh, S., Prez-Pellitero, E., De Lange, M., Tuytelaars, T., 2023. CLAD: A realisticcontinual learning benchmark for autonomous driving. Neural Networks 161, 659669. Vogelstein, J.T., Dey, J., Helm, H.S., LeVine, W., Mehta, R.D., Tomita, T.M., Xu, H., Geisa, A., Wang, Q., et al., 2020. Representationensembling for synergistic lifelong learning with quasilinear complexity. arXiv preprint arxiv:2004.12908 . Wang, L., Zhang, X., Li, Q., Zhang, M., Su, H., Zhu, J., Zhong, Y., 2023. Incorporating neuro-inspired adaptability for continual learning inartificial intelligence. Nature Machine Intelligence 5, 13561368.",
  "Zenke, F., Poole, B., Ganguli, S., 2017. Continual learning through synaptic intelligence, in: International Conference on Machine Learning,PMLR. pp. 39873995": "Zhan, L., Guo, D., Chen, G., Yang, J., 2018. Effects of repetition learning on associative recognition over time: Role of the hippocampus andprefrontal cortex. Frontiers in Human Neuroscience 12, 277. Zhu, C., Xiao, F., Alvarado, A., Babaei, Y., Hu, J., El-Mohri, H., Chang, S., Sumbaly, R., Yan, Z., 2023. EgoObjects: A large-scale egocentricdataset for fine-grained object understanding, in: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).",
  "B. Participation Over Time": "The graphs in Figure B.1 show the number of submissions per day by all teams, and the maximum average accuracy achieved on each day. Insummary, participants began with a low average accuracy of 10%. Over time, the highest average accuracy showed gradual improvement, peakingand stabilizing at 40% towards the end. The challenge saw a steady increase in engagement, with daily submissions starting from as low as oneto three in the beginning, and reaching above 30 towards the end. 1611 16 21 26 31 36 41 46 51 56",
  "D. Appendix HAT-CIR": "D.1. Ablation Study for Single-model SettingsThe ablation study, presented in Table D.1, indicates that various components of the proposed method were important for achieving robustperformance in the pre-selection phase, when network replicas were not yet used. Notably, the modified HAT-CL technique significantly outperformsthe traditional HAT method in the context of CIR, thus confirming its efficacy. The supervised contrastive learning phase also plays a crucial rolein learning better feature representations, as its absence led to a marked decline in performance.",
  "Baseline39.08%42.89%34.59%-With Original HAT23.89%26.05%19.47%15.71%Without SupCon35.39%28.93%22.16%10.02%Without Momentum & TTA30.67%35.16%26.61%8.04%": "Table D.1Ablation study results for HAT-CIR when using a single model without using network replicas. Shown is the testaccuracy (as %) after training on each data stream from the final phase of the challenge. The Avg. Decrease fromBaseline column indicates how much performance is lost relative to the baseline (i.e., the version of HAT-CIR that wasused in the pre-selection phase of the challenge). The term Original HAT refers to the original hard attention method,SupCon stands for supervised contrastive learning, and Momentum & TTA are abbreviations for momentum-basedtest-time decision-making and test-time augmentation, respectively.",
  ".%.%.%": "Table D.2Effect of the number and types of network replicas on the performance of HAT-CIR on the data streams from the finalphase of the challenge. The table shows the test accuracy (as %) achieved for each data stream with different numbers offragments and ensembles. D.2. Influence of Number of Network ReplicasThe results presented in Table D.2 demonstrate that both increases in ensemble replicas and increases in fragment replicas enhance performance.Yet, by adding more replicas, the added benefit starts to diminish. This suggests there is a sweet spot in balancing the number of fragments andensembles for optimal performance without excessive training and/or test time. Importantly, adding ensemble replicas comes with the trade-offof both longer training and test time, while adding fragment replicas only comes with the trade-off of longer test time. Because the rules of thechallenge predominantly put restrictions on computational resources during training, the final version of the model that was selected predominantlyused fragment replicas.",
  "E. Appendix Horde": "E.1. AlgorithmInspired by zero-forgetting methods, Horde promotes stability on each task-specific Feature Extractor (FE) while leveraging class repetitionfor balancing plasticity by learning an aligned unified representation. Each extractor provides a discriminative representation for the seen classes ofthe task where it is learned. Then, pseudo-feature projection is used to achieve alignment on the unified head. A description of the pseudo-code ofHorde is provided in Algorithm 1. Additionally, the feature extractor training steps are shown in Algorithm 2.",
  ": Freeze FE": "E.2. Additional Experimental AnalysisTo further study the effects of the ensemble size, the Horde strategy is evaluated with different number of maximum ensemble size. The resultsare presented in Figure E.1. Stream 1 seems to be invariant to the ensemble size, probably due to the first experience having 51 classes and, therefore,providing a robust representation from the beginning. This can be exploited by heavily enforcing stability and limited number of FEs, thus becomingunnecessary to extend the ensemble after enough classes are seen. Streams 2 and 3 seem to show a benefit of extending the ensemble over time,which accommodates the incorporation of new seen classes without the need of replacing older FEs.",
  "Test Accuracy": "Stream 3 zerosrandomfeatures Figure E.2: Impact of estimating unknown class statistics under different heuristics. Zeros and random heuristics hadalmost the same performance. Results are shown in the form of the test accuracy (as a proportion) after training on eachof the three data streams from the initial phase of the challenge.",
  "= (,1, , ,)": "For the pseudo-feature projection step, given a class and a feature extractor , estimates are needed for , and ,. In cases we do not haveany information about the class shape for this feature extractor, we fix the estimation of the standard deviation to = 1. For the estimation of ,we propose three heuristics: 1. zeros: setting all , estimations to 0.2. random: randomly sample , from a multivariate normal distribution (0, 1).3. original features: estimate , with the original representation of the current sample without modification. Results are presented in Figure E.2, showing that estimating with the original representations provides a clear advantage. Both zeros and randomheuristics seem to perform similarly, and clearly underperform compared to the original features one.",
  "F. Appendix DWGRNet": "F.1. Additional Experimental AnalysisAn ablation study was conducted to investigate the role of each module. The results are presented in Table F.1. It can be seen that entropy playsthe most significant role in the final accuracy. Adjusting each models logits using the number of classes and the number of features also leads toimproved accuracy. This verifies the effectiveness of each module."
}