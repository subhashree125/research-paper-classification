{
  "Abstract": "Part-aware panoptic segmentation (PPS) requires (a)that each foreground object and background region in animage is segmented and classified, and (b) that all partswithin foreground objects are segmented, classified andlinked to their parent object. Existing methods approachPPS by separately conducting object-level and part-levelsegmentation. However, their part-level predictions are notlinked to individual parent objects. Therefore, their learn-ing objective is not aligned with the PPS task objective,which harms the PPS performance. To solve this, and makemore accurate PPS predictions, we propose Task-AlignedPart-aware Panoptic Segmentation (TAPPS). This methoduses a set of shared queries to jointly predict (a) object-level segments, and (b) the part-level segments within thosesame objects. As a result, TAPPS learns to predict part-level segments that are linked to individual parent objects,aligning the learning objective with the task objective, andallowing TAPPS to leverage joint object-part representa-tions. With experiments, we show that TAPPS considerablyoutperforms methods that predict objects and parts sepa-rately, and achieves new state-of-the-art PPS results.",
  ". Introduction": "To fully understand what is depicted in an image, it is im-portant to consider concepts at different levels of abstrac-tion.For rich scene understanding, we should not onlyrecognize foreground objects (e.g., car, human) and back-ground regions (e.g., sky, ocean), but also simultaneouslyidentify the parts that constitute the objects (e.g., car-wheel,human-arm). In a step towards such comprehensive sceneunderstanding, De Geus et al. introduced part-aware panop-tic segmentation (PPS) . The objective of this computervision task is (1) to output a segmentation mask and classlabel for all thing objects and stuff regions in an image likefor panoptic segmentation we call these object-levelsegments and (2) to simultaneously segment and classify Per object query Object class Object mask Object queries Part queries",
  "Car": "Part-level segments within object Car-body, car-wheel, etc. .Task-aligned part-aware panoptic segmentation.(a) Existing works separately predict object-level segments andobject-instance-unaware part-level segments. (b) In this work, wepredict objects and parts jointly, using a set of shared queries. Thisallows our method to predict parts within individual object seg-ments, aligning its learning objective with the PPS task objective. all parts within each identified object. These are called part-level segments and should be explicitly linked to an object-level segment, establishing a part-whole relation.Current state-of-the-art methods address PPS byusing two different sets of learnable queries to separatelypredict object-level and part-level segments, see a.While these methods outperform earlier baselines , theyhave one main limitation: their learning objective is notaligned with the task objective. Where the PPS task ob-jective is to predict parts within each individual object-levelsegment, these existing works conduct part-level semanticsegmentation, predicting part-level masks that cover mul-tiple objects (see a). In other words, these networks",
  "arXiv:2406.10114v1 [cs.CV] 14 Jun 2024": "are not optimized for the PPS task, but instead solve thesurrogate subtasks of object-level panoptic segmentationand part-level semantic segmentation. As a result, to as-sign parts to individual objects, these methods require post-processing. Additionally, we hypothesize that there are sev-eral other negative consequences: (1) These networks learna conflicting feature representation. They learn that object-level thing instances should be separated, but also that theparts of these object-level instances should be grouped to-gether. We expect that this harms the ability of the net-works to separate instances.(2) Predicting objects andparts separately may cause incompatible predictions (e.g.,a car-window part with a bicycle object), which makes itunclear which prediction to trust, and requires further post-processing. (3) The networks encode information about ob-jects and their parts separately, whereas this information ispotentially complementary. See Sec. 3.2.2 for more detailson these limitations.In this work, we aim to design a simple network forPPS that overcomes these limitations and thereby makesmore accurate PPS predictions. To achieve this, we pro-pose the Task-Aligned Part-aware Panoptic Segmentation(TAPPS) method. Instead of using separate queries for ob-jects and parts, TAPPS uses one set of shared queries tojointly represent objects and the parts they contain. Specif-ically, each of these queries learns to represent at most oneobject-level segment, for which it predicts (1) a segmenta-tion mask and object-level class, and (2) the segmentationmasks and classes for all part-level segments within this ob-ject. This is visualized in b and explained in Sec. 3.With this approach, TAPPS explicitly predicts part-levelsegments per individual object. Therefore, the networkslearning objective is now aligned with the PPS task objec-tive, and TAPPS is directly optimized for the PPS task. As aresult, both object- and part-level segmentation are learnedin an object-instance-aware manner.This removes theconflict in the learned feature representations, allowing forbetter instance separability. Additionally, we hypothesizethat predicting objects and parts from the same query re-duces incompatibilities between object and part predictions,as they are made using the same information. Moreover, weuse TAPPS to go even further, and explicitly only predictpart segments compatible with predicted object segments.This enforces full object-part compatibility and simplifiesthe part segmentation task. Finally, the network can nowencode complementary object-level and part-level informa-tion in a shared query, giving it a richer representation formaking object- and part-level segmentation predictions.With experiments, detailed in Sec. 4, we show thatTAPPS outperforms the baseline with separate object andpart queries in multiple aspects: (1) The part segmenta-tion quality within identified objects is significantly better,which shows the positive impact of using joint object-part representations and simplifying the part segmentation task.(2) The object instance segmentation performance is con-siderably improved, demonstrating the benefits of learningparts in an object-instance-aware manner.Together, thisyields a large overall improvement with respect to the base-line, and causes TAPPS to considerably outperform existingworks across different benchmarks, achieving new state-of-the-art results. See Sec. 5 for more extensive results.To summarize, we make the following contributions: We propose TAPPS, a simple approach for PPS that alignsthe learning objective with the task objective, facilitatingobject instance separability and enabling joint object-partrepresentations for more accurate PPS predictions. We use the shared object-part queries to constrain TAPPSto only predict part segments that are compatible with theobject class, enforcing object-part compatibility and sim-plifying the part segmentation task.",
  ". Related work": "Part-aware panoptic segmentation.Part-aware panop-tic segmentation is an image segmentation task intro-duced for scene understanding at multiple abstraction lev-els.It extends panoptic segmentation by requiring(a) object-level panoptic segmentation, and (b) part-levelsegmentation within object-level segments. Most existingworks do not predict parts per object-level segment, but in-stead make separate predictions for panoptic segmentationand the surrogate task of part-level semantic segmentation or part segmentation . JPPF proposesa single-network approach with a shared encoder followedby separate heads for semantic, instance, and part segmen-tation. To merge the predictions by these heads to the PPSformat, JPPF proposes a new rule-based fusion strategy thatoutperforms the originally introduced merging strategy forPPS . Panoptic-PartFormer and its extension Panoptic-PartFormer++ tackle PPS with a Transformer-based approach that independently makes panoptic segmen-tation and part segmentation predictions using separate setsof learnable queries for thing, stuff and part segments. Al-ternatively, ViRReq is a paradigm in which PPS is pre-dicted in a cascading fashion, by first segmenting objectsand then segmenting the parts within these objects by re-quest, but it requires multiple networks to achieve this.In contrast to these existing approaches, we propose amethod that jointly predicts object-level segments and thepart-level segments within these objects. This aligns thelearning objective with the task objective, and yields im-proved PPS performance. For more details, see Sec. 3. Part-level image segmentation.Part-level segmentationis also widely studied beyond PPS. Most works address partsegmentation, i.e., semantic segmentation for parts .Formore complete scene understanding, other works also takeinto account object instances, like PPS does, with instance-aware part segmentation .However,these works do not consider background stuff classes, whichPPS does consider. Alternatively, UPerNet separatelypredicts part-level and object-level semantic segmentation,including background classes, but without any instanceawareness. Another work has extended PPS by also pre-dicting the relations between objects , training a modelon annotations from different datasets. In this work, we ad-dress the PPS task as introduced by De Geus et al. , andwe compare our approach to state-of-the-art alternatives.",
  ". PPS task definition": "Part-aware panoptic segmentation (PPS) requires con-sistent image segmentation across two abstraction levels:the object level and part level. For object-level segmen-tation, following panoptic segmentation , an imageshould be divided into N object-level segments, where Nvaries per image. Each segment si consists of a binary maskMobjiand an object class label cobji . Thing classes require asegment per object instance, and stuff classes require a sin-gle segment per class. Next, per segment si, PPS requires aset of Ki part-level segments. Each part-level segment spti,jconsists of a binary part-level mask Mpti,j and a part-levelclass cpti,j that are compatible with the object-level mask andclass. Specifically, the part-level mask must be a subset ofthe object-level mask, and the part-level class should be oneof the part-level classes defined for the object-level class.For instance, if cobjiis car, then cpti,j cannot be human-headbut it can be car-window. For all parts, a single part-levelsegment is required for each part class within an object.To summarize, each image should be divided into a setof segments S = {si}Ni=1 = {(Mobji , cobji , Pi)}Ni=1, wherePi = {spti,j}Kij=1 = {(Mpti,j, cpti,j)}Kij=1 is a set of part-levelsegments compatible with the object-level segment in si. Inpractice, part-level classes are only defined for some object-level classes. When segment si has an object class that doesnot have parts, then Ki = 0 and si = (Mobji , cobji ).",
  "Mask classification framework": "The concept of mask classification is to predict a set ofN q object-level segments, i.e., a set of N q pixel-levelmasks Mobj and corresponding class labels cobj. To makethese predictions, mask classification networks output twocomponents: high-resolution features F REHW andqueries Q RN qE, where H and W are the heightand width of the features, and E is the feature and querydimensionality.Each of the N q queries is used to pre-dict the class label and segmentation mask of at most oneobject-level segment. The class predictions are a functionof only the queries, and the mask predictions are a func-tion of both the queries Q and the high-resolution featuresF. To output these high-resolution features, the input imageis first fed into a backbone to extract multi-scale features,e.g., ResNet or Swin .Subsequently, these fea-tures are further refined and upsampled to high-resolutionfeatures by a pixel decoder, e.g., Semantic FPN .In the other part of the network, the queries Q are gen-erated by processing learnable queries Q0 using a trans-former decoder, which applies self-attention across queriesand cross-attention with image features. Through bipartitematching between N q predicted and N ground-truth seg-ments, each query is assigned to at most one ground-truthsegment. As N q is not always equal to N, some queriesmay not be assigned to a ground-truth segment. If so, theydo not receive any supervision for segmentation, and willlearn a no object class. In this work, we build upon theMask2Former instantiation of this meta-architecture.",
  "Mask classification for PPS": "With the aforementioned meta-architecture, we can conductobject-level segmentation, and thereby solve one aspect ofthe PPS task definition. To solve the full task, the object-level segments should also be further segmented into parts.Current state-of-the-art approaches achieve thisby introducing an additional set of queries, the part-levelqueries depicted in a. Each of these part-level querieslearns to represent a part-level segment. However, in con-trast to the PPS task definition, these part-level segmentsare not explicitly linked to an object-level segment.In-stead, these part-level segments represent an entire part-level class, i.e., their masks contain all pixels belonging toone part-level class across multiple object segments.This means that the learning objective of these networksis not aligned with the PPS task objective. As a result, theyare not directly optimized for PPS, but instead for two sur-rogate tasks: object-level panoptic segmentation and part-level semantic segmentation. In addition to necessitating JOPS head (per query) Object-level class Object-level mask Nc part masks within object MLP Npc FC parts ipt Npc part queriesper object MLP Nc compatiblequeries BackbonePixel decoder Learnable queries",
  "Qi": "FC Adaptation layers Ndyn part-level classes FC MLP .Dynamic part segmentation.When conductingdynamic part segmentation, the JOPS head uses N dyn fully-connected (FC) layers to generate N dyn per-object part queries.Each per-object part query dynamically learns to represent at mostone part-level segment within an object. For each per-object partquery, we predict (a) a part-level class and (b) a part-level mask. fied the authors of this bug, and they confirmed it.Toassess whether this problem also occurred for Panoptic-PartFormer++, for which the code is not available, we re-quested the authors to send us the predictions by Panoptic-PartFormer++, so we could re-evaluate them on Pascal-PP.We are thankful that the authors have sent us these predic-tions. In Tab. 11, we provide both the originally reportedscores, and the scores after our re-evaluation given the of-ficial PPS evaluation repository . For all methods, theoverall PartPQ scores are higher when using the correctevaluation. Therefore, we compare against these higher val-ues in Tab. 2 of the main manuscript.",
  ". Task-aligned PPS": "To overcome the limitations of state-of-the-art methods andachieve a better PPS performance, the objective of this workis to design a simple PPS approach that uses the powerfulmask classification framework, but that aligns its learningobjective with the PPS task objective, and explicitly predictspart-level segments within individual objects. To achievethis, we should (a) generate a unique query for each object-part combination, i.e., for each part-level segment within anobject-level segment, and (b) explicitly link these part-levelsegments to their parent objects, as PPS requires. To ac-complish this, we propose Task-Aligned Part-aware Panop-tic Segmentation (TAPPS), a method that jointly represents object-level segments and their parts with a set of sharedqueries, and generates per-object part queries from eachshared query. Each of these queries can then be used to pre-dict one part-level segment within an object-level segment,and is automatically linked to its parent object segment.",
  "Overall architecture": "In , we visualize the network architecture of TAPPS.First, we initialize a set of N q learnable queries, Q0 RN qE.Each of these queries learns to represent oneobject-level segment, but also the part-level segments be-longing to this object (e.g., car3 and also car3-wheels, car3-windows, etc.). Following the mask classification frame-work described in Sec. 3.2, these learnable queries Q0 andthe features from a backbone are fed into a decoder, whichgenerates a set of processed queries Q and high-resolutionfeatures F. Subsequently, these processed queries and fea-tures enter the Joint Object and Part Segmentation (JOPS)head. For each query Qi RE, this head predicts (a) anobject-level class, (b) an object-level segmentation mask,and (c) a set of part-level segmentation masks and classesfor the parts within this object-level segment.With this approach, we solve the limitations of existingworks. Following the PPS task definition, we predict part-level segments per individual object-level segment. (1) Asa result, we learn object-instance-aware representations forboth parts and objects, improving the ability of the networkto separate instances. (2) By predicting objects and partsjointly, the compatibility between both sets of predictionsgreatly improves. Moreover, in the JOPS head, we ensurethat we only predict compatible parts, ensuring full object-part compatibility (see Sec. 3.3.2). (3) The joint object-partrepresentations allow the network to fully leverage the com-plementary information from both abstraction levels.",
  "JOPS head": "Within the JOPS head, we predict an object-level seg-ment and compatible part-level segments for each sharedquery.For the object-level predictions,we followMask2Former : (1) For the object-level class, we feedeach processed query Qi through a single fully connectedlayer to predict a score for each possible class, and obtainthe predicted class cobjiby picking the highest-scoring class.(2) For the object-level mask, we feed the query Qi througha 3-layer MLP and generate a mask by taking the productof the resulting mask queries Qmi and the features F, andapplying a sigmoid activation, yielding Mobji HW .To predict part-level segments using the same featuresF, we need to generate queries for each part class withinan object-level segment. To do so, we first apply an MLPto each query Qi to adapt it for part-level segmentation,and then apply N pc different fully-connected layers, whereN pc is the total number of part-level classes in the dataset.This results in Qpti RN pcE, a set of per-object part-level queries, where each query always corresponds to afixed, predetermined part class.We could then take theproduct of Qpti and the features F to generate a segmen-tation mask for all part-level classes. However, we alreadyhave an object-level class prediction for each query, and weknow that only a subset of all N pc part classes is compatiblewith a certain object class. Therefore, we propose to onlypredict and supervise part-level masks for those compati-ble part-level classes. This simplifies the part segmentationtask that the network needs to learn, as it no longer has tolearn to predict empty segmentation masks for incompati-ble part classes. Concretely, as visualized in , we iden-tify the object-level class cobjifor each query Qi, and keeponly the part-level queries Qpt,ci RN cE that correspondto the N c part classes that are compatible with the object-level class. Then, we compute the product of these remain-ing queries Qpt,ciand the features F, and apply a sigmoidactivation to generate compatible part segmentation masksMpti N cHW . As each per-object part query corre-sponds to a fixed part class, we also know the part classescpti . Note that the number N c depends on the object-levelclass and will therefore vary per query Qi.",
  "Training": "To assign each query to at most one ground-truth object-level segment with corresponding part-level segments dur-ing training, we apply bipartite matching based on the pre-dicted and ground-truth object-level classes and masks.To supervise the object-level segments, we use the cross-entropy loss for the classes, and both the Dice andcross-entropy loss for the segmentation masks. Together,these losses form the object-level loss Lobj. For the part-level segmentation masks, we also use the Dice and cross-",
  ". Experimental setup": "Datasets.We use the two PPS benchmarks for evaluation.Cityscapes Panoptic Parts (Cityscapes-PP) extendsthe original Cityscapes dataset with part-level labels. Itconsists of street scene images from several cities. It haslabels for 19 object classes (11 stuff; 8 thing). Part classesare defined and labeled for 5 object-level thing classes; thereare 23 part classes in total. We train on the train split (2975images), and evaluate on the val split (500 images).Pascal Panoptic Parts (Pascal-PP) , which combinesexisting labels for Pascal VOC , consists of awide range of scenes and classes. There are 59 object-levelclasses (39 stuff; 20 thing), and part-level classes are de-fined for 15 thing classes. Unless otherwise indicated, weuse the default part class definition with 57 part classes intotal . To evaluate a more challenging setting, we ad-ditionally evaluate on Pascal-PP-107, which uses the 107non-background classes from the Pascal-Part-108 definitionintroduced by Michieli et al. for part segmentation. Forboth class definitions, we train on the training split (4998images), and evaluate on the validation split (5105 images). Baseline.Our baseline is a version of TAPPS that usesthe same network architecture, but uses a separate set of100 additional queries for part-level segmentation (see alsoa). Comparing the results of the baseline in Tab. 1 toexisting methods in Tab. 2, we find that our baseline out-performs state-of-the-art approaches that also use separatepart-level queries , indicating that it is a strong base-line. See the supplementary material for more details. Evaluation metrics.The part-aware panoptic segmenta-tion performance is evaluated using the default Part-awarePanoptic Quality (PartPQ) metric . It captures both theability to recognize and segment object-level segments (i.e.,stuff regions and thing instances), and the ability to furthersegment the identified object-level segments into part-levelmasks. The PartPQ per object-level class c is given by",
  "(b) Cityscapes-PP val": ". Main results. We compare TAPPS to a strong baseline that uses separate sets of queries to predict object- and part-level segments,instead of predicting object- and part-level segments jointly like TAPPS (see Sec. 4). class c is part of TPc if the Intersection-over-Union (IoU)of their object-level masks is larger than 0.5. If a ground-truth segment is not identified, it is part of FNc; if a predic-tion is incorrect, it is part of FPc. The IoUp term capturesthe segmentation performance within identified object-levelsegments. For object-level classes for which part classesare defined, it calculates the part-level mIoU. Otherwise, ituses the object-level IoU. We report the mean PartPQ overall classes, but also separately for object-level classes withparts (PartPQPt) and without parts (PartPQNoPt).To individually assess the ability of methods to conductpart-level segmentation within identified objects, we reportthe Part Segmentation Quality for object-level classes thathave parts, PartSQPt. Following De Geus et al. , perobject-level class c, the PartSQPt calculates the average partsegmentation mIoU within object-level segments:",
  "|TPc|.(3)": "To evaluate the ability of networks to conduct object-level panoptic segmentation, we report the Panoptic Qual-ity (PQ) metric . Similarly to the PartPQ, we report theaverage PQ over all classes, but also over all thing classes(PQTh) and stuff classes (PQSt) separately. Implementation details.TAPPS is built on top of thepublicly available code of state-of-the-art panoptic segmen-tation network Mask2Former . For all datasets, we use abatch size of 16 images, and train on 4 Nvidia A100 GPUs.To optimize TAPPS, we use AdamW , a weight decayof 0.05, and a polynomial learning rate decay schedule withan initial learning rate of 104 and a power of 0.9. ForCityscapes-PP, we train for 90k iterations and apply con-ventional data augmentation steps : random flip, ran-dom resize with a factor between 0.5 and 2.0, and finally arandom crop of 5121024 pixels. For Pascal-PP, we trainfor 60k iterations in case of ImageNet pre-training, but foronly 10k iterations in case of COCO pre-training, to preventoverfitting. Following state-of-the-art panoptic segmenta- tion implementations on COCO , we apply large-scale jittering with a scale between 0.1 and 2.0 followedby a random crop of 10241024 pixels. During inference,we resize the image such that the shortest side is 800 pix-els. Note that we use exactly the same training and testingsettings for both TAPPS and the baseline. For more imple-mentation details, see the supplementary material.",
  ". Main results": "First, we compare TAPPS to the strong baseline that usesseparate sets of queries for object-level panoptic segmenta-tion and part-level semantic segmentation (see Sec. 4). Theresults in Tab. 1a demonstrate that TAPPS significantly out-performs the baseline on Pascal-PP in several aspects. Mostimportantly, the PartPQ for object-level classes with parts(PartPQPt) is +4.4 or +2.4 higher, depending on the pre-training strategy. Looking in more detail, we find that thisincrease is caused by two individual improvements: (1) Thepart-level segmentation quality (PartSQPt) is considerablyhigher than for the baseline. This shows the positive im-pact of having a joint representation for objects and parts,and simplifying the part segmentation task by only allowingcompatible predictions (see also Sec. 5.3). (2) The panopticquality for thing classes (PQTh) also sees a substantial in-crease. Here, it should be noted that (a) all the object-levelclasses with parts are thing classes, and (b) thing classes re-quire instance separation. Thus, this improvement does notonly show the benefit of learning objects and parts jointly,but also indicates that learning parts in an object-instance-aware manner indeed improves the ability of the network toseparate object instances (see also Sec. 5.4).For Cityscapes-PP, we see similar results in Tab. 1b.Again, we observe improvements on the PartPQPt, PartSQPt and PQTh metrics. In this case, the absolute improvementsare slightly smaller. This is expected because this datasetcontains significantly fewer classes and very similar images,making it easier to learn the PPS task and limiting the po-tential gains that can still be obtained by TAPPS.",
  "Panoptic-PartFormer RN-50I,C43.962.457.560.161.6Panoptic-PartFormer++ RN-50I,C42.565.159.2-63.6TAPPS (ours)RN-50I,C48.965.761.366.964.4": "Panoptic-PartFormer Swin-BI,C45.667.862.059.066.6Panoptic-PartFormer++ Swin-BI,C46.068.262.3-68.0Panoptic-PartFormer++ ConvNeXt-BI,C46.469.163.1-68.2SegFormer-B5 + CondInst + BPR MiT-B5 + RN-50I,C*48.667.562.5--TAPPS (ours)Swin-BI,C53.069.064.868.068.0 . Comparison with state of the art. Evaluation on the Cityscapes-PP and Pascal-PP benchmarks . RN-50 is ResNet-50 .Other backbones are EfficientNet-B5 , MiT-B5 , Swin-B and ConvNeXt-B . I = ImageNet , C = COCO panoptic ,C* = COCO pre-training for instance segmentation.PartPQ scores for these existing methods have been re-evaluated using officialcode and are higher than originally reported on Pascal-PP , see supplementary material for more details.",
  ". Comparison with state of the art": "In Tab. 2, we compare TAPPS to existing state-of-the-art methods across different datasets, backbones, and pre-training settings.On both Pascal-PP and Cityscapes-PP,TAPPS significantly outperforms existing work.Mostimportantly, it consistently scores higher on the PartPQ,PartPQPt, and PartSQPt metrics. TAPPS is only slightly out-performed by JPPF on Cityscapes-PP with ImageNetpre-training, but we note that JPPF uses the EfficientNet-B5 backbone, which is much more powerful than theResNet-50 used by TAPPS. Moreover, we do outper-form JPPF by a large margin on the Pascal-PP dataset,showing the strength of TAPPS on more complex datasets.Overall, we achieve new state-of-the-art results on bothdatasets, obtaining PartPQ scores of 60.4 and 64.8 onPascal-PP and Cityscapes-PP, improvements of +6.3 and+1.7, respectively. In the supplementary material, we com-pare qualitative examples of TAPPS, our baseline, and ex-isting approaches, and we also show typical failure cases.",
  ". Predicting only compatible part classes during train-ing and testing. Evaluated on Pascal-PP": "are compatible with the querys object-level class (seeSec. 3.3.2). In Tab. 3, we evaluate the effect of predict-ing only compatible parts during both training and testing.In addition to the main metrics, we also assess the per-centage of predicted objects for which there are no con-flicting part-level predictions. The results show that, evenwhen allowing incompatible predictions, TAPPS has signif-icantly fewer object-part conflicts than the baseline. Natu-rally, when predicting only compatible parts during testing,all object-part conflicts are removed, eliminating the need",
  ". Performance for things. ImageNet pre-training": "for post-processing. However, this does not yield a big im-provement in terms of the segmentation quality. When wealso apply this during training, we do observe a part seg-mentation quality (PartSQPt) improvement. This shows thatsimplifying the part segmentation task during training leadsto improved performance, as we hypothesized in Sec. 3.3.2.JOPS head architecture.In Tab. 4, we evaluate theimpact of using different numbers of adaptation layers in theJOPS head before applying the N pc fully-connected (FC)layers to generate the part queries (see ). We findthat one or two adaptation layers are necessary to generateobject- and part-level representations that are sufficientlydistinct to perform their respective tasks accurately; addingany more layers does not yield further improvements.",
  ". Additional analyses": "Instance separability.Tab. 1 showed that TAPPS im-proves the PQTh, i.e., the ability to recognize, segment andclassify object instances. To assess if this is due to better ob-ject recognition or improved instance separability, we groupall thing instances of the same object-level class togetherand evaluate the instance-agnostic semantic segmentationperformance with the mIoUTh. If the PQTh improvementwere due to improved recognition, we expect the mIoUTh",
  "to improve too. However, the results in Tab. 5 show only aminor mIoUTh improvement. This indicates that the PQTh": "gain is not due to better recognition, but mainly results froma better ability to separate instances, showing the benefit oflearning parts in an object-instance-aware manner.Performance on Pascal-PP-107.To assess the per-formance of TAPPS in a more complex setting, we evaluateit on Pascal-PP-107, which has 107 part-level classes in-stead of 57. The results on this dataset, reported in Tab. 6,show once more that TAPPS consistently improves the partsegmentation and thing segmentation performance with re-spect to the baseline, in this case leading to a PartPQPt im-provement of +4.1 or +2.2, depending on the pre-trainingstrategy. This demonstrates that TAPPS is also effective",
  ". Fixed or dynamic part segmentation. Evaluated onPascal-PP , with pre-training on COCO panoptic": "when the PPS task becomes more complex.Fixed vs. dynamic part segmentation.As explainedin Sec. 3.3.2, TAPPS uses a fixed fully connected layer foreach part class to generate the corresponding per-objectpart query. Alternatively, it is possible to predict part masksand classes dynamically within each object segment, usinga set of dynamic queries like we do for object-level seg-mentation. In Tab. 7, we compare our fixed part segmenta-tion setting with this dynamic approach, which is explainedin more detail in the supplementary material. We find thatour fixed approach results in a better PartSQPt and thereforePartPQPt performance. We hypothesize that the dynamicapproach performs worse because it makes the part segmen-tation task unnecessarily complex, as the network needs tolearn to assign segments to queries dynamically, and addi-tionally predict a class label.",
  ". Conclusion": "With experiments, we have shown that TAPPS considerablyoutperforms methods that predict objects and parts sepa-rately, by improving the object instance separability, partsegmentation quality, and object-part compatibility. Impor-tantly, these improvements can be attributed to the fact thatTAPPS is directly optimized for the PPS task, using a setof shared queries to jointly predict objects and correspond-ing parts. With our promising findings, we hope to inspirefuture research towards even more complete scene under-standing, e.g., image segmentation at even more abstractionlevels, potentially with more flexible class hierarchies.Acknowledgements.This work is supported by EindhovenEngine, NXP Semiconductors, and Brainport Eindhoven.Thiswork made use of the Dutch national e-infrastructure with the sup-port of the SURF Cooperative using grant no. EINF-5302, whichis financed by the Dutch Research Council (NWO). In the appendix, we provide the following additional mate-rial: In Appendix A, we present the results of additional exper-iments, in which we evaluate the effect of different lossweights and data augmentation techniques, and assess theefficiency of TAPPS and other approaches.",
  "A. Additional experiments": "Loss weights.In Tab. 8, we show the impact of using dif-ferent loss weights to balance the losses for object-level seg-mentation and part-level segmentation, using the weightsobj and pt (see Eq. 1 of the main manuscript). We findthat balancing the losses with obj = pt = 1.0 yields thebest performance. As expected, the object-level segmenta-tion performance, reflected in the PQ metric, drops whenobj is decreased. Conversely, the part-level segmentationperformance, reflected in the PartSQPt metric, drops whenpt is decreased. Data augmentation techniques.As explained in Ap-pendix B.1 of this document, we use large-scale jitter-ing data augmentation for our experiments on Pascal-PP.However, the existing works Panoptic-PartFormer andPanoptic-PartFormer++ use less aggressive data aug-mentation techniques during training. To show that the dif-ference in data augmentation techniques is not the mainreason that TAPPS outperforms these methods, we trainTAPPS with the same data augmentation techniques thatare used by these methods, according to the official coderepository of Panoptic-PartFormer. Specifically, we applya random horizontal flip, and then directly resize the im-age such that the smallest side is 800 pixels. The resultsin Tab. 9 show that TAPPS still significantly outperformsboth existing methods when using these data augmentationtechniques. This shows that the improvement by TAPPS ismainly caused by the methodology and network architec-ture, and not by the data augmentation differences. Finally,we note that there are no differences in data augmentationtechniques between our method and existing works on theCityscapes-PP dataset, so these results in Tab. 2 of the mainmanuscript are directly comparable.",
  ". Efficiency. We evaluate the average inference speed inframes per second (fps) and the maximum required GPU memoryon the Pascal-PP val set , using an Nvidia A100 GPU": "portantly, we observe that the default version of TAPPS,which only predicts the masks for the N c compatible parts,is much more efficient than the version that predicts masksfor all N pc part classes, in terms of both inference speedand memory. Moreover, by only considering compatibleparts, TAPPS is also more efficient than the baseline thatuses separate object-level and part-level queries. Anotherreason that TAPPS is more efficient than the baseline is thatits part-level queries do not participate in the self-attentionand cross-attention operations in the decoder, unlike thoseof the baseline. This shows the strength of the simplicity ofTAPPS.In Tab. 10b, we compare TAPPS to existing method",
  "B.1.1General": "This subsection describes the implementation details thatapply to both TAPPS and the baseline. For completeness,we repeat some of the details already mentioned in the mainmanuscript.Both TAPPS and the baseline are implemented on topof the publicly available code of Mask2Former , whichuses Detectron2 . All experiments are conducted witha batch size of 16, using 4 Nvidia A100 GPUs in total.Following Mask2Former, we optimize all networks usingAdamW , using a polynomial learning rate schedulewith an initial learning rate of 104, a power of 0.9, anda weight decay of 0.05. When we apply ImageNet pre-training, we initialize the backbone with weights pre-trainedon ImageNet-1K . In case of COCO pre-training, weinitialize both the backbone and the compatible decoder lay-ers with weights pre-trained on COCO panoptic segmenta-tion ; we use the weights provided in the officialrepository of Mask2Former. Like Mask2Former, TAPPSapplies deep supervision . This means that the segmenta-tion masks and classes are predicted after each transformerlayer in the decoder, and that a loss is calculated for thesepredictions at each of these layers. The overall loss is thesum of the total losses at all transformer layers.For experiments on Pascal-PP , we train for60k iterations in case of ImageNet pre-training. In case ofCOCO pre-training, we train for 10k iterations, to avoidoverfitting. Following state-of-the-art panoptic segmenta-tion implementations on COCO , during training, weapply a random horizontal flip, followed by large-scale jit-tering with a scale between 0.1 and 2.0 and a random crop of 10241024 pixels. During inference, we resize the im-age such that the shortest side is 800 pixels.For experiments on Cityscapes-PP , we train for90k iterations for both ImageNet and COCO pre-training.We follow the conventional data augmentation steps forCityscapes during training : random horizontal flipwith a probability of 0.5, scaling the image with a ran-dom factor between 0.5 and 2.0, and finally a random cropof 5121024 pixels. During inference, we feed the full-resolution images of 10242048 pixels.",
  "B.1.2TAPPS": "For TAPPS, we use N q = 100 shared queries.This isequal to the default number of object-level queries used byMask2Former, because each shared query still representsonly one object-level segment. Following Mask2Former,query embedding dimension E = 256.By default, theadaptation layer in the JOPS head is an MLP with twofully connected layers with 256 input and output channelsand a ReLU activation in between; see Tab. 4 of the mainmanuscript for ablations.During inference, TAPPS outputs PPS predictions with-out requiring rule-based post-processing, as it does not haveto assign object-instance-unaware parts to individual ob-jects, or resolve conflicts between object- and part-levelpredictions.Specifically, for each query, TAPPS simplyoutputs (a) an object-level class and mask, and (b) a part-level mask for each part-level class that is compatible withthe predicted object-level class. For each pixel within theobject-level mask, TAPPS keeps only the highest-scoringpart mask prediction, applying argmax. This results in aset of compatible part-level segments that belong to an in-dividual object-level segment. Applying this to all sharedqueries, we output a set of predictions that comply with thePPS task definition.",
  "B.1.3Baseline": "Like Mask2Former, our baseline uses 100 queries forobject-level segmentation.Additionally, to also conductpart-level semantic segmentation, it uses 100 additionalqueries. In , we depict the network architecture forthis strong baseline. As seen in this figure, both sets ofqueries are concatenated when entering the Transformerdecoder, so there can be interaction between object-leveland part-level queries through self-attention. Note that, al-though they are concatenated, these queries are not mixedor shared. The first 100 queries are still object-level queries,which learn to represent object-level segments, and the fi-nal 100 queries are part-level queries, which learn to rep-resent object-instance-unaware part-level segments. At theend of the decoder, the queries are again split into two",
  "B.1.4Dynamic part segmentation": "In Tab. 7 of the main manuscript, we compare our defaultversion of TAPPS to a version that applies dynamic partsegmentation. By default, as explained in Sec. 3.3.2 of themain manuscript, we generate a set of fixed per-object partqueries in the JOPS head. That means that each per-objectpart query corresponds to a fixed, pre-determined part-levelclass, and that this query predicts a mask for this class. Al-ternatively, we can use a set of dynamic queries, which donot correspond to a fixed class, like we do for object-levelsegmentation. As depicted in , for each query Qi, weapply N dyn fully-connected (FC) layers to generate a set ofdynamic per-object part queries Qdyni RN dynE. As these queries are dynamic, they do not correspond to a fixed class,so for each of these queries we predict (a) a part-level classwith a single fully-connected layer, and (b) a part-level seg-mentation mask by first applying a 3-layer MLP and thentaking the product of the resulting mask queries with thefeatures F. We use N dyn = 50.To supervise these part-level predictions during train-ing, we assign each per-object part query to at most onepart-level ground-truth segment using the same Hungar-ian matching algorithm we use for object-level segmenta-tion . This matching is applied separately within eachobject-level segment. If there is no matching ground-truthsegment for a part query, we do not supervise the segmen-tation mask and supervise a no-part class label. The partclass prediction by this dynamic version of TAPPS is super-vised with a cross-entropy loss. The other losses remain thesame.During inference, the procedure is the same as for the de-fault TAPPS. The only difference is the source of the part-level class prediction. This dynamic version explicitly pre-dicts it, whereas, for the default fixed version, it is knownbecause each per-object part query is associated with a pre-determined part class.",
  "B.2. Evaluation of existing work": "As mentioned in Tab. 2 of the main manuscript, thePartPQ scores of Panoptic-PartFormer and its exten-sion Panoptic-PartFormer++ on Pascal-PP reported inour work are higher than the scores that these works origi-nally reported . This is due to an evaluation bug thatwe discovered in the official code repository of Panoptic-PartFormer , which caused the resulting PartPQ scoresto be lower than they actually are. Note that this bug onlyapplies to Pascal-PP and not to Cityscapes-PP. We noti-",
  "Panoptic-PartFormer Swin-B I,C47.464.350.654.1Panoptic-PartFormer++ Swin-B I,C49.348.952.151.3": ". Re-evaluation of existing work on Pascal-PP . After discovering an evaluation bug in the official code ofPanoptic-PartFormer which caused the PartPQ scores to be lower than they actually are, we re-evaluate the predictions by Panoptic-PartFormer and Panoptic-PartFormer++ using the official PPS evaluation repository . We use these higher correct numbers inour comparisons in the main manuscript. I = ImageNet , C = COCO panoptic pre-training. JOPS head with dynamic part segmentation (per query) Object-level class Object-level mask Ndyn part-level masks MLP Ndyn FC Ndyn part queriesper object MLP",
  "C. Qualitative results": "In and , we compare TAPPS against the strongbaseline that we describe in Appendix B.1.3. These exam-ples show some of the advantages of TAPPS over the base-line. Specifically, we observe that TAPPS (1) makes moreaccurate part segmentation predictions within identified ob-jects, and (2) is better able to separate different object in-stances. Comparing TAPPS to state-of-the-art existing modelPanoptic-PartFormer in and , we observeeven more significant differences. In addition to the object-level segmentation quality, the part segmentation qualitywithin objects is considerably better for TAPPS. This ap-plies to large and small objects across different classes.In and , we show examples of predic-tions by TAPPS with a Swin-B backbone, whichachieves new state-of-the-art PPS performance. These ex-amples show the high segmentation quality that TAPPS canachieve, across different types of objects and classes.Finally, shows examples of typical errors madeby TAPPS. Notably, TAPPS struggles with images in whichobjects are seen from uncommon perspectives, and imageswith many objects and complex occlusions. Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler,Raquel Urtasun, and Alan Yuille.Detect What You Can:Detecting and Representing Objects using Holistic Modelsand Body Parts. In CVPR, 2014. 5, 6, 7, 8, 9, 10, 12, 15, 17,19, 21",
  "Qizhu Li, Anurag Arnab, and Philip HS Torr.Holistic,instance-level human parsing. In BMVC, 2017. 3": "Xiangtai Li, Shilin Xu, Yibo Yang, Guangliang Cheng, Yun-hai Tong, and Dacheng Tao. Panoptic-PartFormer: Learninga Unified Model for Panoptic Part Segmentation. In ECCV,2022. 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 17, 18 Xiangtai Li, Shilin Xu, Yibo Yang, Haobo Yuan, Guan-gliang Cheng, Yunhai Tong, Zhouchen Lin, and DachengTao.PanopticPartFormer++:A Unified and DecoupledView for Panoptic Part Segmentation.arXiv preprintarXiv:2301.00954, 2023. 1, 2, 3, 5, 7, 9, 10, 11, 12",
  "(a) Input image(b) Ground truth(c) Baseline(d) TAPPS (ours)": ". Qualitative examples of TAPPS and our strong baseline on Cityscapes-PP . Both networks use ResNet-50 withCOCO pre-training . White borders separate different object-level instances; color shades indicate different categories. Note that thecolors of part-level categories are not identical across instances; there are different shades of the same color. In these examples, we cansee how TAPPS improves both the instance separability and part segmentation quality with respect to the strong baseline. The red boxesindicate regions in which these differences are best visible. Best viewed digitally.",
  "(a) Input image(b) Ground truth(c) Panoptic-PartFormer (d) TAPPS (ours)": ". Qualitative examples of TAPPS and Panoptic-PartFormer on Cityscapes-PP . Both networks use ResNet-50 with COCO pre-training . White borders separate different object-level instances; color shades indicate different categories. Note thatthe colors of part-level categories are not identical across instances; there are different shades of the same color. Best viewed digitally.",
  "(a) Input image(b) Ground truth(c) TAPPS (ours)": ". Examples of errors in TAPPS predictions. The predictions are made by TAPPS that uses a ResNet-50 backbone pre-trained on COCO panoptic . Top three images are from Pascal-PP validation , bottom three images are from Cityscapes-PPval . White borders separate different object-level instances; color shades indicate different categories. Note that the colors of part-level categories are not identical across instances; there are different shades of the same color. Best viewed digitally."
}