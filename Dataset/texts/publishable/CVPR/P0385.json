{
  "Abstract": "The popularity of large-scale pre-training has promotedthe development of medical foundation models. However,some studies have shown that although foundation modelsexhibit strong general feature extraction capabilities, theirperformance on specific tasks is still inferior to task-specificmethods. In this paper, we explore a new perspective calledKnowledge Decomposition to improve the performanceon specific medical tasks, which deconstruct the founda-tion model into multiple lightweight expert models, eachdedicated to a particular task, with the goal of improv-ing specialization while concurrently mitigating resourceexpenditure.To accomplish the above objective, we de-sign a novel framework named Low-Rank Knowledge De-composition (LoRKD), which explicitly separates graidentsby incorporating low-rank expert modules and the efficientknowledge separation convolution. Extensive experimen-tal results demonstrate that the decomposed models per-form well in terms of performance and transferability, evensurpassing the original foundation models. Source code isavailable at:",
  ". Introduction": "Foundation models pre-trained on large-scale and diversedatasets, have been proven to possess powerful general fea-ture extraction capabilities and can handle various tasks .However, some studies have indicatedthat the performance of foundation models is still inferiorto task-specific methods, suggesting that current foundationmodels are unable to simultaneously guarantee both gener-ality and specialization. Moreover, with the gradual expan-sion of data scale and model capacity, the deployment costsof future foundation models may become exorbitant. Toaddress them, we proposes a new perspective called knowl-edge decomposition, aiming to offer potential solutions forthe practical application of medical foundation models.",
  "Expert1Expert2Expert3Expert4": ". Knowledge decomposition is employed to break downthe foundation model into multiple lightweight expert models,each dedicated to a specific domain. The goal of this paradigm isto improve the specialization of deployment models within a spe-cific domain, while simultaneously reducing deployment costs. The purpose of knowledge decomposition is to breakdown the foundation model into multiple lightweight ex-pert models, where each expert model focuses solely on aspecific domain, such as a department within a hospital (asshown in ). The resulting expert models, comparedto the original foundation model, gain stronger specializa-tion and lower deployment costs. In order to preserve boththe performance and transferability of decomposed mod-els, we need to inject task-specific knowledge and com-mon knowledge into each expert model correspondingly,which is highly challenging. To the best of our knowledge,there has been no research conducted in the medical fieldon how to decompose a foundation model into multiple ex-pert models. However, recently in the field of natural im-ages, KF has made preliminary explorations into thisissue. KF decomposes the pre-trained model into a commonknowledge network (CKN) and multiple task-specific net-works (TSNs) by manipulating the mutual information be-tween models. After decomposition, the CKN can be com-bined with each TSN to form task-specific expert models.However, in our experiments, we find that the effectiveness",
  "arXiv:2404.17184v1 [cs.CV] 26 Apr 2024": "of this method in medical scenarios is not significant.In this paper, we propose a novel method for knowledgedecomposition of medical foundation models called Low-Rank Knowledge Decomposition (LoRKD). LoRKD con-sists of two main components: low-rank expert modulesand the efficient knowledge separation convolution. Theformer provides multiple parameter-efficient task-specificknowledge carriers for each convolution, which effectivelycontrols the introduction of parameters while ensuring suf-ficient feature representation capability. The latter providesan efficient implementation method for expert knowledgeseparation at the convolutional level, allowing gradients tobe separated into the corresponding expert modules in a sin-gle forward propagation, while accumulating them in theshared backbone.This ensures that each expert modulelearns task-specific knowledge while the shared backbonelearns common knowledge. After decomposition, we canintegrate the task-specific expert modules and the sharedbackbone through parameter fusion, ensuring model perfor-mance and transferability without increasing additional pa-rameters. Furthermore, benefiting from the training patternand parameter fusion mechanism, our decomposed modelcan easily switch task knowledge across different domains.The performance comparison on three pre-training datasetsand seven downstream datasets demonstrates the effective-ness of LoRKD. A large number of analytical experimentsfurther showcase the advantages of LoRKD from differentperspectives. In a nutshell, our contributions can be sum-marized as the following: We introduce knowledge decomposition to broaden ap-plication of medical foundation models, which decom-poses models into multiple lightweight experts to reducecosts and enhance specialization. The incorporation ofthis novel perspective offers potential solutions for thepractical implementation of medical foundation models. We design a new method LoRKD, which consists of twocomponents: low-rank expert modules and the efficientknowledge separation convolution. LoRKD injects task-specific knowledge into the corresponding expert mod-ules through efficient explicit gradient separation.",
  ". Related work": "Knowledge Distillation. Knowledge distillation (KD) is an effective knowledge transfer method, which can be cat-egorized into two groups: logits-based distillation and feature-based distillation . The former encourages students to mimic the softmaxoutputs of teacher models, while the latter encourages stu-dents to mimic the intermediate-level features from the hid-den layers of teacher models. Different from these methods that focus on transferring complete knowledge, our goal isto decompose knowledge into different expert models.Multi-Task Learning. Multi-task learning (MTL) aims to train a unified model to solve multiple distinctbut related tasks . Therefore, themain focus of MTL is to train better general feature extrac-tor. For example, MoCo addresses the convergenceissue of Multi-Gradient Descent Algorithm (MGDA) to ensure convergence to Pareto optima. Aligned-MTL stabilizes the training process by aligning the principal com-ponents of the gradient matrix. However, tasks within thecontext of foundation models often display significant di-versity, and solely pursuing common knowledge may not beappropriate. In summary, MTL focuses on extracting sharedknowledge from relevant tasks, while knowledge decompo-sition emphasizes separating task-specific knowledge fromthe foundation model trained on diverse tasks.Knowledge Decomposition. Different from the previousdisentangled representation learning that are usually donethrough adversarial learning or variationalauto-encoder , the goal of knowledge decompo-sition is to break down the pre-trained foundation modelinto multiple task-specific experts. Recently, in the field ofnatural images, KF has explored knowledge decompo-sition by promoting modularization of knowledge throughoptimizing mutual information loss . It decom-poses a pre-trained model into a common knowledge net-work and multiple task-specific networks. In this paper, weconduct the first exploration of knowledge decomposition inthe medical field and propose a novel approach that not onlybetter controls the number of parameters but also attains amore advanced level of performance and transferability.",
  ". Proposed Method": "Given a medical foundation model Fp pre-trained on abroad range of data, our goal is to decompose Fp into Tlightweight expert models F1, ..., FT that can be deployedto T different medical departments instead of using Fp. Ourlightweight decomposition model comprises a shared back-bone Fs and T expert modules {E1, ..., ET } during train-ing. To achieve efficient knowledge decomposition, we pro-pose low-rank expert modules and efficient knowledge sep-aration convolution which will be described in detail below.An overview of our method can be seen in .",
  ". Low Rank Expert Modules": "Considering the limited computational resources and thescalability of the number of tasks, expert modules, as car-riers of task-specific knowledge, need to strike a balancebetween the number of parameters and the feature repre-sentation capability. LoRA , as a commonly used fine-tune method in foundation models, has been proven to beparameter-efficient . Inspired by this, we propose to",
  "Aggregated Weight": ". The overview of LoRKD. We introduce low-rank expert modules to control the number of parameters and efficient knowledgeseparation convolution to achieve computationally efficient explicit gradient separation. The decomposed models can replace the originalfoundation model in specific domains, and can switch task knowledge conveniently between different departments. use a similar low-rank structure as the carriers for knowl-edge decomposition, named low rank expert modules.Given a shared convolution W0 RCoutCinkk in Fs,where Cout, Cin, k represent the number of output channels,the number of input channels, and the kernel size respec-tively, we configure two low-rank factors Bt RCoutkrk and At RrkCink for t-th expert, where r represents therank. As a result, for the features belonging to the t-th task,original convolution operation gt = W0ht can be trans-formed into gt = (W0 + BtAt)ht, where, for brevity, weomit the reshape operation, and ht, gt represent the inputfeatures and output features respectively.It is worth noting that, unlike previous scenarios whereW0 remains fixed in LoRA, in our knowledge decompo-sition scenario, W0, as a carrier of common knowledge,requires updating along with the low-rank factors.",
  ". Efficient Knowledge Separation Convolution": "To achieve knowledge decomposition, we propose explicitgradient separation as our solution. This approach requireseach expert module to compute gradients solely for its cor-responding task, enabling the acquisition of task-specificknowledge. Simultaneously, the shared backbone collectsgradients from all tasks to facilitate the acquisition of com-mon knowledge among tasks. However, when a mini-batchof data contains T tasks, the convolution operation becomesT times gt = (W0 + BtAt)ht, where t {1, ..., T}.The T times forward propagation significantly increasesthe training time, especially when a large number of tasksneeded to be decomposed. To address this, we propose Ef- ficient Knowledge Separation Convolution (EKS Conv.).In order to clarify our improvements in convolution, wefirst review the standard convolution operation. In a deepconvolutional neural network, the input features of eachconvolution can be represented as h RBCinHW ,where B, H, W represent the sample number of a mini-batch, the height and width of the feature maps, respec-tively. If the kernel size of the convolution is k and thestride is 1, each output feature unit oij RBCout in outputfeatures g RBCoutHW can be represented as",
  "n=0h(i+m)(j+n) mn,": "where i {1, ..., H} , j {1, ..., W} , and h(i+m)(j+n) RBCin represents the units of the input feature map h, andmn RCinCout represents the convolution weights.Our EKS Convolution improves upon the traditionalconvolution operation by enabling gradient separation to beachieved in a single forward propagation, regardless of thenumber of tasks. EKS Convolution avoids the computa-tional overhead of duplicating data input for each convo-lution, greatly enhancing training efficiency. Specifically,for each EKS Convolution, in addition to the input featuremap h, the task label M RBT , corresponding to themini-batch is also simultaneously inputted as a referencefor subsequent parameter aggregation, and M is a one-hotvector. Then, the output features can be computed by",
  "g = g1 gt gTgt = (W0 + BtAt)ht = (W0 + BtAt)Mth,(1)": "where means the concatenation operation, ht representsthe set of Bt features in h that correspond to the t-th task,and Mt is an index matrix that indicates which features inh belong to the t-th task. To avoid redundant convolutionaloperations, we propose parameter aggregation, where theparameters used in the current iteration are aggregated intoW according to M. This ensures that the number of for-ward propagation is always equal to 1, and the operationg = Wh is equivalent to the Eqn. (1). Specifically, theoperation of the Eqn. (1) can be converted by",
  "BA = B1A1 ... BtAt ... BTAT": "represents the Hadamard product, and BA M RBT CoutCinkk represents the configuration of low-rank expert modules for the each input feature and i cor-responds to the second dimension of (BA M).Theweight of shared convolution W0 is applied to all tasks.In this way, we obtain the aggregated weight WRBCoutCinkk, which is equivalent to Eqn. (1) but re-quires only a single forward propagation.Another challenge associated with it is that W has 5 di-mensions, unlike traditional convolutions which typicallyhave 4 dimensions.To ensure compatibility with exist-ing deep learning libraries, we have borrowed the con-cept of group convolution (GConv) . Specifically, weset the group number to B and {1, ..., B}.Then,we reshape h to h R1BCinHW and reshape W toW RBCoutCinkk. Consequently, each output featureunit oij in g can be computed by",
  ". Loss Function": "In order to transfer the knowledge from the medical foun-dation model into the lightweight decompostion model,we introduce a task knowledge transfer loss denoted asLtransfer.Specifically, for a mini-batch of training data{(xi, yi, yti)}Bi=1, where xi represents the i-th input imagein the current mini-batch, yi represents the class label acrossall tasks, and yti represents the class label within its cor- responding task t. We denote the feature extracted fromthe foundation model as f bi= F(xi; Fp), and the fea-tures extracted from the lightweight decompostion modelas f di = F(xi; Fs; Et). Then, the Ltransfer for sample xican be written as LKL(f bi , f di ), where LKL represents theKullback-Leibler divergence.Moreover, we can also leverage class label information{yti} to enhance task-level supervision. Specifically, duringtraining, we integrate T classification heads {h1, ..., hT }into the lightweight decompostion model (classifier in Fig-ure 2).These classification heads can individually pre-dict {Y1, ..., YT } classes where Yt represents the numberof classes for the t-th task, Y is the total number of allclasses and Ti=1 Yi = Y . The logits extracted from thelightweight decompostion model can be denoted as gdi =ht(f di ) and the prediction probability can be calculated by",
  ". Task Knowledge Switch": "After decomposition, the lightweight decomposition modelenables easy switching between different task knowledgethrough task knowledge switch, allowing for conversion tothe corresponding expert model based on the requirementsof various medical departments. Specifically, when deploy-ing the model on the t-th task, the original parameters W0can be replaced with Wt = W0 + BtAt. Similarly, whenswitching knowledge to another task t, expert knowledgecan be conveniently switched using W0 = Wt BtAtand Wt = W0 + BtAt. The parameter fusion mecha-nism of low-rank expert modules ensures that the deployedexpert models consistently maintain a size equal to Fs.",
  ". Experimental Setup": "Dataset. To evaluate the decomposition performance, weuse three medical multi-task datasets with different datascales: Radimagenet (1.35 million images), MedM-nist (705,689 images) and Med-MT (119,655 images).We decompose the foundation models pre-trained on thesedatasets into 11/10/8 lightweight expert models, respec-tively, with each decomposed expert model focusing ona specific anatomical region. Detailed information aboutthese datasets can be found in the supplementary materials. In addation, to determine the extent to which the de-composed expert models can fully replace pre-trained mod-els in specific domains, we evaluated the transferability ofthese expert models on seven different downstream datasets,including COVID , BTC , AD , Mura ,AUITD , HAM10000 , and DET10 . Detailedinformation can be found in the supplementary materials.Competitive methods. (1) Baseline refers to training fromscratch on downstream tasks. (2) Single-Task Learning(STL) refers to training multiple single-task networks inde-pendently. (3) Multi-Task Learning (MTL) refers to train-ing a single model to predict all tasks. (4) STL-KD and (5)MTL-KD correspond to the KD version of STL and MTL,respectively, which utilize knowledge distillation to trans-fer knowledge from the pre-trained models.(6) MoCo-MTL and (7) Aligned-MTL are the advancedMTL algorithms.(8) KF represents the advancedknowledge decomposition method, which is the closest toour goal and serves as our primary comparison object. Weexplain the purpose of using these methods for comparisonin the supplementary materials.Implementation details. For the decomposition training,we use the SGD optimizer with a learning rate of 0.05and CosineAnnealingLR as the scheduler for training 100epochs. For the downstream fine-tuning, we use AdamWoptimizer with a learning rate of 5e-5 and train the modelfor 240 epochs. The default values for the hyperparametersare set as follows: =10, =1 and r=8. The pre-trainedmodel structure is ResNet50 , and the structure of thelightweight decomposition model is ShuffleNetV2 . Inour experiments, we apply EKS convolution to all convolu-tions in ShuffleNetV2 except for GConv.",
  ". Decomposition performance": "The performance comparison of different methods on threepre-training datasets is shown in . Each column cor-responds to a specific task. Avg represents the task-levelaverage accuracy. Parmas represents the total number ofparameters during training. Only KF and our method fo-cus on the knowledge decomposition of pre-trained models.Considering the generalization requirement of the founda-tion model, it is common for the foundation model to use aunified classification head during training, instead of config-uring a specific classification head for each task . Thisis also why the performance of the foundation model in Ta-ble 1 is relatively poor. Note that other methods can onlysolve problems within specific tasks.The foundation model vs. STL. The results of the foun-dation model are superior to STL on Med-MT, but signifi-cantly inferior to STL on Radimagenet and MedMnist, par-ticularly for MedMnist, which suggest that as the increaseof scale and diversity of the pre-training dataset, the special-ization of the pre-trained model gradually diminishes due to conflicts between different domain knowledge. In contrast,training models independently for each task (STL) can pre-vent interference between different tasks and thus achievebetter performance than foundation models on Radimagenetand MedMnist. However, STL is unable to learn commonknowledge among tasks, so it often requires more data toensure its generalization ability. Besides, training T indi-vidual models is not only time-consuming but also resultsin a linear increase of parameters.MTL-based methods vs. STL-based methods. We canalso observe that MTL outperforms STL in Radimagenetand Med-MT, while underperforms STL in MedMnist. Itmay be related to the degree of correlation between tasksincluded in the pre-training dataset, where MedMnist hasthe most diverse modalities (refer to supplementary mate-rials).In contrast to the standard MTL, other advancedMTL methods, namely MoCo-MTL and Aligned-MTL, donot yield improvements and may even exhibit worse perfor-mance. This observation suggests that balancing multipleoptimization objectives to obtain a better shared encoder isnot an effective solution when there are significant differ-ences among tasks. The knowledge distillation variants ofSTL and MTL (STL-KD and MTL-KD) do not show sig-nificant performance improvement, which suggests that thegeneral features extracted by foundation models have lim-ited benefits for specific tasks and indirectly reflects the im-portance of both specialized and general features. It alignswith the design philosophy of our LoRKD.LoRKD vs. KF and other methods. Compared to theknowledge decomposition method KF, our method showssignificant performance advantages and introduces fewerparameters. Specifically, even including 11/10/8 experts,our method has less than half the number of parameters ofKF. This result validates the effectiveness of our low-rankexpert modules and the efficient knowledge separation con-volution. Furthermore, our method also achieves the bestaverage performance compared to other non-knowledge de-composition baselines, highlighting the potential of knowl-edge decomposition in extracting task-specific knowledge.",
  ". Transferability": "In order for the decomposed lightweight expert model tofully replace the foundation model within a specific domain,it is necessary not only for the expert models to performwell on the same distribution of data (pre-training dataset),but also to evaluate its generalization ability on downstreamtasks with close distributions. The performance comparisonof the expert models decomposed from three pre-trainingdatasets on seven downstream datasets is shown in .For KF and our method, we fine-tune the correspondingexpert models on downstream datasets, such as using thelung expert model for the COVID dataset. If there was nocorresponding expert model, similar to , we fine-tune on . The decomposition performance on pre-training datasets. Each column represents the performance of different methods forspecific tasks. It is worth noting that except for KF and ours, the concept of knowledge decomposition does not exist in other methods.The presence of homonymous experts implies different modalities. For more details, please refer to the supplementary materials.",
  "KF3.9965.3074.6752.1961.1277.8879.8160.2133.5063.09LoRKD1.9579.3785.0679.0488.6372.5783.6565.0752.4275.73": "the shared backbone (with ). As for other non-knowledgedecomposition methods, we use the models trained on thepre-training dataset for fine-tuning to demonstrate the ad-vantages of knowledge decomposition in terms of transfer-ability. For details, please refer to supplementary materials. The performance of fine-tuning foundation models is ob-served to be inferior to Baseline, providing evidence againthat the foundation model cannot replace task-specific mod-els in terms of performance, due to lack of specialization.Compared to baseline, both STL-based and MTL-basedmethods show little improvement, indicating that solely fo-cusing on task-specific knowledge or common knowledgedoes not contribute to transferability. Conversely, our ex-pert models incorporate both common knowledge and task-specific knowledge, which exhibit strong transferability andeven significantly outperform KF. Another benefit whencompared to KF is that our method is compatible with the parameter fusion and does not require the simultaneous de-ployment of two networks (CKN and the correspondingTSN need to be deployed simultaneously in KF).Furthermore, we discover an interesting phenomenon. Incomparison to MTL-KD, our method outperforms it moresignificantly on downstream datasets. This shows the ad-vantage of knowledge decomposition in transferability, andthe transferability can not be directly reflected through thedecomposition performance. And as the scale of the pre-training dataset increases, the transferability of our decom-posed expert models also improves, indicating that increas-ing the scale of pre-training datasets benefits the transfer-ability of the decomposed model.",
  "The impact of Rank r. We evaluate the impact of r for de-composition performance on pre-training dataset and trans-": ". The performance of the decomposed expert models on seven downstream datasets. Params represents the number of modelparameters during deployment. Comp. Ratio denotes the compression ratio, defined as the ratio of the deployed model parameters to theparameters of the foundation model. - indicates the absence of data corresponding to the downstream tasks in the pre-training dataset.",
  "r=455.0885.7179.9575.6177.9898.0583.46r=856.2685.7181.4775.9278.5198.3384.93r=1656.1986.2182.4978.8178.5198.3384.87": "ferability on downstream datasets in . The resultsshow that for Radimagenet, increasing r from 4 to 8 leadsto a significant performance improvement, while increasingr from 8 to 16 does not provide further enhancement. Thissuggests that selecting a larger r is not necessarily better andmay be related to the scale of datasets. Moreover, we findthat the improvement on pre-training dataset is positivelycorrelated with the improvement in transferability. Lower Costs and Higher Efficiency.We compare thenumber of parameters and the FLOPs among differentmethods on Radimagenet, as shown in .It canbe observed that, compared to the foundation model, ourssignificantly reduce the number of parameters and FLOPs,demonstrating that our method can effectively reduce de-",
  "Params (Deployment)1.251.601.251.251.2523.51FLOPs (Deployment)0.150.160.150.150.154.13": "ployment costs while maintaining high computational effi-ciency. In comparison to KF, even at r=16, our method stillincurs significantly lower costs. As r increases, our costs donot increase significantly and remains at a lower level. It isworth mentioning that if parameter fusion is used, our costswill be the same as baseline.Furthermore, we comparethe efficiency of different methods in following thesetting in , where b, r, l and d represent the batch size,the rank, the maximum sequence length and the feature di-mension respectively. c2 is the computational coefficient ofmatrix multiplication. If the following condition is satis-fied,rbld2",
  "EKS conv (ours) Y = X(W0 + Ti=1(BA M)i) Tc2(rd2) + c2(bld2)": "cases, as bl > Tr, r > 2 are common settings. A detailedexplanation is provided in the supplementary materials.Stronger Specialization. Taking the DET10 dataset withdetection annotations as an example, we evaluate the dif-ferences in the activated regions between the decomposedexpert model and the foundation model during the predic-tion process from the perspective of Grad-CAM . Thecorresponding visualization results are shown in .It can be observed that although the foundation modelcan focus on the correct regions, the range of regions it at-tends to is usually larger compared to the detection boxes ofGround-Truth. This may be due to the fact that the featureextractor of the foundation model retains a certain degree ofgeneral feature extraction ability, tending to focus on moreinformation regardless of the specific task. Conversely, ourexpert models focus on more precise abnormal regions anddemonstrate stronger specialization compared to the foun-dation model. Compared to KF, our approach also achieveshigher recognition accuracy, further indicating that our de-composed model has better transferability.Knowledge Disentanglement.To verify whether ourmethod can indeed achieve knowledge decoupling betweendifferent tasks, we measure the mutual information gap(MIG) scores for different methods. MIG is a widelyused metric for quantifying disentanglement. The resultsare shown in , where higher MIG scores indicatea higher level of disentanglement. It can be observed thatour method exhibits a higher level of disentanglement com-pared to the previous KF and other baselines, which maybenefit from our explicit gradient separation.",
  ". The comparison of MIG scores on different methods": "In addition, we find that the degree of disentanglementin MTL is lower compared to STL. This suggests that whenMTL employs a shared encoder to acquire common knowl-edge for across tasks, the entanglement of gradients fromdifferent tasks also results in knowledge entanglement. Ad-ditionally, compared to STL, the degree of disentanglementin STL-KD is also lower, which can be attributed to thetransfer of common knowledge from the foundation model.More analysis, such as using larger foundation mod-els and comparing CKA feature similarity across differenttasks, can be found in the supplementary materials.",
  ". Conclusion": "This paper proposes a new perspective called knowledgedecomposition, which focuses on reducing the deploymentcosts and enhancing specialization for medical foundationmodels. We design a low-rank expert module and an ef-ficient gradient separation convolution to successfully de-compose the foundation model into multiple lightweight ex-pert models, and validate their transferability and disentan-glement. We hope that this research will provide new in-sights for the development of medical foundation models.Acknowledgment.This work is supported by the Na-tional Key R&D Program of China (No. 2022ZD0160703),STCSM (No.22511106101, No.22511105700, No.21DZ1100100), 111 plan (No. BP0719010) and NationalNatural Science Foundation of China (No. 62306178).",
  "Algerian ultrasound images thyroid dataset: Auitd,Kaggledataset. / algeria - ultrasound - images -thyroid-dataset-auitd. 5": "Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil DLawrence, and Zhenwen Dai.Variational informationdistillation for knowledge transfer.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 91639171, 2019. 2 Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-man, Simran Arora, Sydney von Arx, Michael S Bernstein,Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.On the opportunities and risks of foundation models. arXivpreprint arXiv:2108.07258, 2021. 1 Christopher P Burgess, Irina Higgins, Arka Pal, LoicMatthey, Nick Watters, Guillaume Desjardins, and Alexan-der Lerchner.Understanding disentangling in backslashbeta-vae. arXiv preprint arXiv:1804.03599, 2018. 2",
  "Ben Glocker, Charles Jones, Melanie Roschewitz, and StefanWinzeck. Risk of bias in chest radiography deep learningfoundation models. Radiology: Artificial Intelligence, 5(6):e230060, 2023. 1": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 5 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,Xavier Glorot, Matthew Botvinick, Shakir Mohamed, andAlexander Lerchner. beta-vae: Learning basic visual con-cepts with a constrained variational framework. In Interna-tional conference on learning representations, 2016. 2",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.Distill-ing the knowledge in a neural network.arXiv preprintarXiv:1503.02531, 2015. 2": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,Karan Grewal, Phil Bachman, Adam Trischler, and YoshuaBengio.Learning deep representations by mutual in-formation estimation and maximization.arXiv preprintarXiv:1808.06670, 2018. 2 Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.Lora: Low-rank adaptation of large language models. arXivpreprint arXiv:2106.09685, 2021. 2 Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang,Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen,Chaoyu Chen, et al. Segment anything model for medicalimages? arXiv preprint arXiv:2304.14660, 2023. 1",
  "Jingyu Liu, Jie Lian, and Yizhou Yu. Chestx-det10: Chestx-ray dataset on detection of thoracic abnormalities, 2020. 5": "Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 18711880, 2019. 2 Yang Liu, Zhaowen Wang, Hailin Jin, and Ian Was-sell. Multi-task adversarial network for disentangled featurelearning. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, pages 37433751,2018. 2",
  "sarial training. Advances in neural information processingsystems, 29, 2016. 2": "Xueyan Mei, Zelong Liu, Philip M Robson, Brett Marinelli,Mingqian Huang, Amish Doshi, Adam Jacobi, Chendi Cao,Katherine E Link, Thomas Yang, et al. Radimagenet: anopen radiologic deep learning research dataset for effectivetransfer learning.Radiology: Artificial Intelligence, 4(5):e210315, 2022. 4, 5 Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, NirLevine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Im-proved knowledge distillation via teacher assistant. In Pro-ceedings of the AAAI conference on artificial intelligence,pages 51915198, 2020. 2",
  "Sebastian Ruder. An overview of multi-task learning in deepneural networks. arXiv preprint arXiv:1706.05098, 2017. 2": "Ahmad Saleh, Rozana Sukaik, and Samy S. Abu-Naser.Brain tumor classification using deep learning. In 2020 In-ternational Conference on Assistive and Rehabilitation Tech-nologies (iCareTech), pages 131136, 2020. 5 Andrew B Sellergren,Christina Chen,Zaid Nabulsi,Yuanzhen Li, Aaron Maschinot, Aaron Sarna, Jenny Huang,Charles Lau, Sreenivasa Raju Kalidindi, Mozziyar Etemadi,et al. Simplified transfer learning for chest radiography mod-els using less data. Radiology, 305(2):454465, 2022. 1 Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.Grad-cam:Visual explanations from deep networks viagradient-based localization. In Proceedings of the IEEE in-ternational conference on computer vision, pages 618626,2017. 8",
  "YonglongTian,DilipKrishnan,andPhillipIsola.Contrastive representation distillation.arXiv preprintarXiv:1910.10699, 2019. 2": "Hugo Touvron, Matthieu Cord, Matthijs Douze, FranciscoMassa, Alexandre Sablayrolles, and Herve Jegou. Trainingdata-efficient image transformers & distillation through at-tention. In International conference on machine learning,pages 1034710357. PMLR, 2021. 2 Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre-sentation learning gan for pose-invariant face recognition. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 14151424, 2017. 2 Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. Theham10000 dataset, a large collection of multi-source der-matoscopic images of common pigmented skin lesions. Sci-entific data, 5(1):19, 2018. 5"
}