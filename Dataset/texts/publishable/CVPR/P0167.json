{
  "Abstract": "Motion Expression guided Video Segmentation (MeViS),as an emerging task, poses many new challenges to the fieldof referring video object segmentation (RVOS). In this tech-nical report, we investigated and validated the effectivenessof static-dominant data and frame sampling on this chal-lenging setting.Our solution achieves a J &F score of0.5447 in the competition phase and ranks 1st in the MeViStrack of the PVUW Challenge. The code is available at: Track Solution 2024.",
  ". Introduction": "Pixel-level Video Understanding in the Wild (PVUW) isa workshop providing large-scale datasets, competitions,and discussions for video scene parsing, one of the funda-mental problems in computer vision. Since 2021, PVUWhas encouraged much improvement in video semantic seg-mentation and video panoptic segmentation.This year,two new subjects join PVUW: 1) Complex Video Ob-ject Segmentation (MOSE) and 2) Motion Expressionguided Video Segmentation (MeViS) , supplementingPVUW with object-centric pixel-level video understanding,vital to many real-world applications such as video editingand human-computer interactive systems. With large-scalevideos, diverse/realistic challenges, and high-quality anno-tations, MOSE and MeViS build a competitive platform andencourage comprehensive and robust solutions.This technical report focuses on the MeViS subject: Mo-tion Expression guided Video Segmentation, which aims tosegment target objects in videos, guided by natural languageexpressions. Before MeViS, several benchmarks have been proposed to encourage explorations in this field.",
  "Equal contributions.*Corresponding authors": "Despite fostering surging research works, these benchmarksfocus more on short videos with less same-category objectsand static attributes (e.g., location and appearance). As aresult, frame-level segmentation also predicts high-qualitymasks. This would weaken the investigation of temporalproperties, vital to understanding real-world videos. Recently, MeViS (Motion expressions Video Segmenta-tion) has been proposed to emphasise temporal proper-ties in RVOS. Compared with previous benchmarks, MeViSbrings several unique challenges: 1) Motion-dominant lan-guage expressions, 2) Complex scenes with multiple same-category instances, 3) One-to-more text-object pairs, and 4)Long videos. These challenges encourage RVOS to focuson dynamic attributes, comprehensive multi-modal interac-tions, and efficient long-term video understanding. The success of MTTR and ReferFormer moti-vates the community to consider transformer-based end-to-end architecture as the dominant paradigm. Given aninput video and text, the paradigm encodes object queriesfrom all frames and decodes text-relevant ones into masks.The difference between MTTR and ReferFormer lies inquery encoding: The former encodes general object queries,and the latter considers language-guided queries, enlight-ening most following works. These could be roughly di-vided into two categories: 1) Robust vision-language align-ments , which focus on the alignments be-tween visual/textual properties; and 2) Temporal-aware in-teractions , where improvements leverage spa-tial and temporal properties to segment the targets. Themore recent ones, SOC and MUTR , achieveSoTA performance on previous benchmarks due to theirefficient interactions between object sequences and texts.This idea comes from VITA , a video instance segmen-tation method, which also inspired the latest RVOS SoTA:DsHMP .Despite good results, they struggle withMeViS since they are trained on static-dominant data. In",
  "arXiv:2406.07043v1 [cs.CV] 11 Jun 2024": "addition, MeViS consists of long videos, challenging RVOSin comprehensive video understanding and efficiency.With these new and realistic challenges, this report im-proves existing RVOS methods in training and inferenceschemes. Specifically, we consider MUTR as the base-line. With pre-trained weights on Ref-COCO series and Ref-YouTube-VOS , we fine-tune them on MeViS.Masks with one-to-more text-object pairs are considered asa whole to encourage adaptive object perception based ontexts. To balance comprehensive understanding and effi-ciency, we split long input videos into sub-videos via framesampling. With these improvements, our solution ranks 1stin the MeViS Track.Experiments on the MeViS valid set indicate that pre-vious RVOS data still contribute to this challenging set-ting due to their sufficient and well-aligned object masksand texts. In addition, ablations on sampling schemes re-veal that there is much room for improvement in temporalmodelling over long videos. Specifically, limited by com-putational resources, the temporal modules are trained withpseudo videos with less frames. During inference, however,videos have much more temporal contexts. This inconsis-tency leads to considering fewer frames (sampled) in tem-poral modules outperform the one with all frames. We hopethese findings are helpful for future research.",
  ". Referring Video Object Segmentation": "Recent RVOS methods build their end-to-end architecturesupon transformers.Specifically, given input videos andtexts, the methods initialise fixed-number object queries tointegrate vision-language contexts. Queries from differentframes are considered a trajectory if they have the same in-dex. The trajectory best matching with texts is decoded intomasks on each frame. As pioneer works, MTTR andReferFormer build foundation architectures with vi-sual/textual encoders, multi-modal transformers, and maskdecoders, motivating many following works. They improveRVOS in mainly two aspects: 1) Robust multi-modal align-ments and 2) Temporal-aware interactions.With cyclic structural consensus, R2VOS showsbetter results when text-referred objects are absent fromframes. In SgMg , a spectrum-based multi-modal atten-tion is proposed to improve query-guided mask predictions.FS-RVOS improves RVOS to adapt new visual/textualconcepts via the cross-modal affinity. As a versatile model,UNINEXT unifies different object perception tasks anddata, generalising well on previous RVOS benchmarks.Previous methods rarely consider temporal properties orachieve this implicitly, e.g., with video-swin-transformer as",
  "Encoder": ". Overview of our solution. Given an input video, we di-vide all frames into N subsets via non-continuous sampling. Herewe take two subsets as an example. They are marked with Blueand Green boxes. In particular, each subset is segmented individ-ually, guided by the input text, and combined for the final results. backbone. In HTML , vision and language informa-tion are interacted over hierarchical temporal contexts. Mo-tivated by VITA , which validates video understand-ing can be achieved via associating frame-level objects, re-cent methods encode temporal properties only from objectqueries, leading to end-to-end and efficient architectures:SOC , MUTR , and DsHMP . The former twoemphasise mutual multi-modal fusion and achieve SoTAperformance on previous benchmarks, while the latter per-form hierarchical multi-modal interactions and show high-quality results on ALL RVOS benchmarks.",
  ". Semi-supervised Video Object Segmentation": "Unlike RVOS, which specifies the target objects with texts,semi-supervised video object segmentation (SVOS) consid-ers manually annotated masks (usually on the first frame) astargets . Therefore, SVOS methods focus on dense corre-spondence between frames and can propagate high-qualitymasks from one or several frames to the whole video.With this feature, most winner solutions fromprevious RVOS competitions use SVOS methods to refinetheir results. In brief, they first select high-confident masksfrom overall predictions. Then, the masks are propagatedto remaining frames to refine their corresponding results.The intuition behind the idea is that the offline RVOS meth-ods struggle to generate spatial-temporal consistent objectmasks. This could be mitigated significantly via powerfulSVOS methods (once the selected masks are high-quality).Memory-based paradigm (since STM ) has domi-nated SVOS due to its efficient, robust, and dense corre-spondence between frames. In particular, the paradigm con-siders not only the first frame annotations but also predic-tions from intermediate frames as references.This way,SVOS could better adapt to object changes. Earlier SoTAs improve STM with robust cross-frame correspondence. The recent focus has been shiftedto more challenging and realistic settings: long videos andcomplex scenes, motivating high-quality benchmarks and solutions. Specifically, XMem is proposed tosegment long videos with dynamic memory management.AOT series consider object representations to en-hance the robustness against complex scenes. By integrat-ing object queries into dense correspondence, Cutie sig-nificantly reduces the matching noise between frames andachieves the SoTA SVOS performance.",
  ". Method": "shows our solution, where we consider MUTR as the base model, with Swin-Transformer-Large as visionencoder and RoBERTa-base as text encoder. Given an in-put video with T frames (V = {vt RHW 3}Tt=1) andreferring text E = {ei}Li=1 with L words, we first sampleV into N subsets: {Vn}Nn=1. Then, we segment each sub-set individually under the guidance from E, achieving masksubsets: {Mn}Nn=1. Finally, the masks are combined forthe final predictions: M = {mt RHW }Tt=1. Training details.With MUTRs weights jointly trainedon Ref-COCO , Ref-COCO+ , Ref-COCOg ,and Ref-YouTube-VOS , we perform fine-tuning onMeViS training videos. For the expressions specifying mul-tiple objects, we consider all masks as a whole to encouragethe model to perceive and segment all objects from videos.To better leverage pre-trained parameters, we follow MUTRto sample five frames as a pseudo video and use the samelosses. The fine-tuning is performed for two epochs, wherethe learning rate is reduced to 10% during the last one. Inference details.Given an input video, we resize eachframe to keep its shorter size at 360. Unlike previous RVOSbenchmarks, MeViS videos consist of much frames andthus cannot be inferred with one feed-forward pass. As di-agrammed in , we sample the video into N = T | Tcsubsets and perform referring segmentation individually.Tc = 30 is the length of each subset.",
  "(c)f1f2f3f4f5f6f7f8f9": ". Difference between (a) Non-continuous sampling, (b)Continuous sampling, and (c) No sampling. Each box here de-notes one frame, and the same colour boxes are sampled as pseudovideos for referring segmentation. Note that the best object trajec-tory selection is performed individually in each sampled video.For No sampling, we still divide videos into subsets and predictmasks upon those. During the selection, we feed all object queriesinto temporal modules and consider the resulting probabilities toselect the best mask trajectory. Training method.Tab. 2 compares J &F on differenttraining data. To generalise the conclusion, we take an-other RVOS method with temporal properties (SOC )into account. MUTR and SOC share the same training andinference procedure. It is observed that the training datain previous benchmarks still contribute to this challengingsetting, due to their sufficient and well-aligned object-text",
  "Sampling method.Tab. 3 ablates methods and hyper-parameters for sampling frames. The difference betweenthese methods is diagrammed in . Although tempo-": "ral modules in MUTR enable us to collect and infer long-term object queries from videos, they are only trained withpseudo videos with five frames. The gap between trainingand inference temporal contexts struggles with temporal in-teractions over long videos. Results in Tab. 3 and show that temporal modules works better than frame-levelpredictions (sub-video length=1) but the performance can-not be improved further with more temporal contexts.",
  ". Conclusion": "This technical report explores the value of training data andtemporal contexts for the challenging MeViS benchmark.The competitive results and ablations demonstrate that thewell-aligned object-text data (even with primarily the staticattributes) are helpful in motion expression-guided referringvideo segmentation. In addition, we investigate the effec-tiveness of temporal contexts and reveal room for improve-ment in the temporal multi-modal analysis of long videos."
}