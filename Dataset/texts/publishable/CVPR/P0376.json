{
  "Abstract": "Object detection in radar imagery with neural networksshows great potential for improving autonomous driving.However, obtaining annotated datasets from real radar im-ages, crucial for training these networks, is challenging,especially in scenarios with long-range detection and ad-verse weather and lighting conditions where radar perfor-mance excels. To address this challenge, we present Rad-SimReal, an innovative physical radar simulation capableof generating synthetic radar images with accompanyingannotations for various radar types and environmental con-ditions, all without the need for real data collection. Re-markably, our ndings demonstrate that training object de-tection models on RadSimReal data and subsequently eval-uating them on real-world data produce performance lev-els comparable to models trained and tested on real datafrom the same dataset, and even achieves better perfor-mance when testing across different real datasets.Rad-SimReal offers advantages over other physical radar sim-ulations that it does not necessitate knowledge of the radardesign details, which are often not disclosed by radar sup-pliers, and has faster run-time. This innovative tool has thepotential to advance the development of computer vision al-gorithms for radar-based autonomous driving applications.Our GitHub:",
  "(a)(b)": ". Comparison between synthetic and real radar imagesfrom four different scenarios. Each scenario shows the cameraimage and the corresponding radar image. (a) and (b) simulationscenarios. (c) and (d) real scenarios. corresponding to range and angle coordinates, providing avisual representation of the scene. Afterwards, computer vi-sion algorithms are employed to identify objects within thisvisual image.Numerous Deep Neural Network (DNN) methods haveemerged for detecting objects in radar images . These techniques involve training the DNNusing annotated real data. Several datasets containing realannotated radar images have been introduced . These datasets vary in terms of the radar type andenvironmental conditions. However, the primary challengewith object detection DNNs trained on real data lies in theconsiderable effort required to collect and annotate the data.This challenge is particularly hard in the case of radar sinceit is used to detect objects at long range, adverse weather and lightening conditions in which annotations are difcultto obtain.In an effort to address the challenge posed by data anno-tations, an alternative approach that generates training datathrough generative methods has been proposed.Severalstudies have explored the training of a Generative Adver-sarial Network (GAN) using unlabeled real radar data toproduce synthetic radar data that closely mimics actual realradar data . These studies have demonstratedthat when training a detection DNN with synthetic data gen-erated by the GAN and testing on real data, the performancegap compared to training with real data is small.Despite the advantage of not requiring data annotation,generative data generation still presents the hurdle of col-lecting a large volume of unlabeled real data. This posesa signicant limitation in system development, as it neces-sitates collecting a substantial domain-specic dataset foreach unique radar sensor, distinct sensor mounting condi-tions, and environmental conditions in order to effectivelytrain the GAN for generating radar images that match thespecic distribution of the data. This problem is resolvedwhen using physical radar simulation instead of generativeradar simulation.In physical simulation, synthetic radar images are cre-ated through the physical modeling of the environment andthe radar sensor . Consequently, for each distinctradar sensor, mounting setup, scenario distribution, and en-vironmental conditions a simulated dataset can be generatedwithout the necessity of collecting any real radar data. Phys-ical radar simulations have been extensively explored in theradar domain . Their process involvesseveral steps. It begins with the creation of 3D automotivescenarios, followed by the calculation of radar reectionsachieved through signal ray tracing from the radar to objectsand back. Subsequently, the radars received signal is gen-erated based on these reections and the specic radar hard-ware conguration. Finally, the radar image is produced byapplying radar-specic signal processing algorithms to thereceived signal.The specic hardware and signal processing design of aradar signicantly inuence the output radar image. Thus,the domain shift from one radar type to another is signi-cant, possibly more pronounced than in other sensors likecameras. This underscores a major limitation of currentphysical radar simulations, as they demand a comprehen-sive understanding of the radars hardware parameters andsignal processing algorithms to produce a radar image thataccurately replicates the real-world image. These detailsare not always disclosed by radar suppliers, and even whenavailable, their implementation within the simulation neces-sitates radar expertise. Another issue with physical radarsimulation is its high computational demand, resulting inlong processing times when generating large datasets. The similarity between real radar images and syntheticimages generated by physical radar simulation, when thehardware and signal processing details of the radar are avail-able, has been evaluated both qualitatively andquantitatively. The quantitative evaluation involves calcu-lating correlation coefcients between synthetic and realimages , and measuring distances between correspond-ing reection points or objects in synthetic and real images. However, there has been a lack of evaluation concern-ing the performance gap between an object detection DNNtrained with physical radar data and tested on real data, incomparison to a DNN solely trained and tested on real data.In this paper, we present RadSimReal an innovativephysical radar simulation method that does not require priorknowledge of the radar hardware specications or its signalprocessing algorithms. Consequently, it can be applied to awide range of radar sensors without necessitating expertisein radar-specic details. Additionally, this novel simulationapproach offers considerably faster processing time in com-parison to conventional physical radar simulations. illustrates the similarity between radar images generated byRadSimReal and real radar images, which raises the ques-tion of whether synthetic images can replace real ones fortraining object detection neural networks. To answer this,we conduct a novel analysis comparing the performance ofDNN object detection models trained with RadSimReal dataand tested on real data versus models trained and tested ex-clusively with real data.Our ndings reveal that object detection DNNs trainedon RadSimReal data and tested on real data exhibit perfor-mance levels comparable to those trained on real data whenboth the training and testing datasets are sourced from thesame real dataset, and outperform them when the trainingand testing are from different real datasets. RadSimRealis a powerful tool for efciently generating extensive an-notated training data with exibility in radar sensor type,mounting congurations and environmental conditions, allwithout the overhead effort of collecting real data for eachspecic setting.The main contributions of the paper are as follows:1. A pioneer analysis revealing that object detection DNNs,trained with physical radar simulation data and tested onreal data, perform comparably to models trained on realdata when the train and test sets of the real data are fromthe same dataset, and outperforms them when they arefrom different datasets. This highlights the benets ofphysical simulation, efciently generating training datawith annotations for diverse radar congurations withoutthe challenges of collecting real data, overcoming a mainlimitation in generative approaches.2. An innovative physical radar simulation technique thatoffers an important advantage over other reference phys-ical simulation methods by not necessitating in-depth knowledge of specic radar implementation details,which are often undisclosed, nor radar expertise, andalso has a signicantly faster run-time.3. A simulation tool offering efcient data generation withground truth information that conveniently supports var-ious radar types. This tool will contribute to advancingradar-based computer vision research.",
  ". Radar Object Detection": "Various studies in the literature extensively investigate DNNmethods for object detection in radar images. Kim et al. applied YOLO to radar images, surpassing the per-formance of conventional radar detection methods. Xu et al. used a ResNet-18 encoder with a CNN decoder to esti-mate 3D bounding box properties. RADDet integratedresidual blocks and YOLO detection heads , while Zhanget al. employed a CNN-based version of U-Net forradar object detection. Meyer et al. utilized graph con-volution networks for radar object detection. A two-stageobject detection approach was introduced in . An al-ternative approach for object detection involves using radarpoint clouds , which are generatedfrom the reection points detected in radar images using theConstant False Alarm Rate (CFAR) algorithm . CFARintroduces signicant information loss during subsequentDNN processing , leading to a considerable degrada-tion in object detection performance when compared to us-ing radar images. Therefore, this paper focuses on DNNprocessing of radar images.",
  ". Radar Datasets": "Numerous publicly accessible automotive radar datasets of-fer real radar images along with object annotations. TheRADIATE and Oxford RobotCar datasets use a360 mechanical scanning antenna, differing from conven-tional radar images generated by antennas arrays.TheCRUW dataset provides radar images with a limitedrange of up to 25 meters, and the annotations consist of ob-ject center points without bounding box information. RA-DIal and K-Radar provide high-resolution radarreection intensity images, but details about their radarhardware are undisclosed, making them unsuitable for con-ventional physical radar simulation.The RADDet dataset features radar images from 15 diverse automotivescenarios captured using a Texas Instruments (TI) automo-tive radar prototype . The CARRADA dataset comprises radar images from 30 controlled scenarios, uti-lizing the same TI radar as RADDet, allowing for cross-dataset performance evaluation. Furthermore, the availabil-ity of hardware specications for the TI prototype radarin these datasets facilitates the generation of synthetic data",
  ". Generative Radar Data Generation": "Weston et al. introduced a GAN method that generatessynthetic radar images conditioned on a 3D representationfrom CARLA simulation , which was trained on a rel-atively large dataset comprising 222,420 images from theOxford RobotCar . Their study showed that a segmenta-tion model, trained on synthetic data and tested on real data,achieves performance similar to a model trained and testedexclusively on real data. L2R GAN used the same ex-tensive dataset to train a GAN for converting LIDAR pointclouds into radar images. Synthetic image quality was eval-uated against real images using PSNR and SSIM metrics,along with qualitative assessment through a human subjec-tive study. Oliveira and Bekooij employed a GAN togenerate synthetic radar images conditioned on boundingbox layouts. They showed a small performance gap be-tween an object detection DNN trained on synthetic dataand tested on real data versus one trained solely on real data.Fidelis used a GAN to generate radar received signalsand derived radar images through signal processing. Theirstudy showed a close distribution resemblance between syn-thetic and real images, assessed by the FID metric .Wheeler et al. used a Variational Autoencoder con-ditioned on an object list and raster grid of the ground road-ways. They validated synthetic image similarity to real im-ages using K-L divergence for clutter and average squareddeviation for objects. While generative methods bridge thesynthetic-real data gap, they require extensive training datafor each radar type, mounting conguration, and environ-mental condition, posing a signicant overhead.",
  ". Physical Radar Simulation Data Generation": "In physical simulation, synthetic radar images are gener-ated by simulating the environment, the radar sensor, andits installation on the vehicle. Numerous studies have pro-posed methods for physically modeling both the environ-ment and radar systems. MaxRay created realistic sce-narios in Blender , simulated reections through raytracing with RF propagation properties, and tested radar de-tection and clutter removal on synthetic data of an OFDMradar simulation. ViRa used the Unity game engine to generate environments and simulated an FMCW radar.They demonstrated the similarity of their simulated radardata to real-world radar measurements in a laboratory set-ting using correlation metrics. Thieling et al. pro-posed a radar simulation with environmental inuences likerain.Their study assessed the simulations performancein qualitative terms. In another approach , a ray trac-ing technique for a MIMO radar simulation was presented,with the realism of the generated radar images evaluatedthrough qualitative comparisons to real radar images for the same scenarios. Several other ray tracing radar simulationsfor realistic automotive scenarios have also been published, which veried the simulations accuracy bycomparing them to real measurements.All these radar simulation studies havent explored theuse of DNN detection methods with physical radar simu-lation data. The performance gap between training objectdetection DNNs with radar simulation synthetic data versusreal data and testing on real data remains unexplored. Fur-thermore, implementing these simulations requires detailedknowledge of radar hardware design and signal processingalgorithms, factors often undisclosed by radar suppliers.",
  ". Proposed Physical Simulation Method": "presents a block diagram comparing our proposedphysical simulation approach, RadSimReal, with the con-ventional physical simulation method. The diagram con-sists of three primary components. RadSimReal encom-passes (a) and (c), whereas the conventional simula-tion method involves (a) and (b). (a) pertainsto the environmental simulation, a shared element in bothapproaches, independent of the specic radar sensor. In thispart, a 3D scene is generated using a graphics engine; in ourimplementation, we employed CARLA . Then, densereection points from objects within the scene are acquiredby ray-tracing of RF propagation paths, starting from theradar to objects in the environment and then returning tothe radar . Subsequently, the RF reectivity of thesepoints is determined using physical formulas , account-ing for surface material, orientation, and distance from theradar. Following the environmental simulation, the subse-quent step involves the radar simulation part, responsiblefor translating the intensities of reection points into theradars output image. (c) illustrates the radar simula-tion block diagram of RadSimReal, which deviates from theconventional radar simulation method shown in (b).In the conventional radar simulation presented in(b), the radars received signal, stemming from all re-ection points within the scene, is acquired based on thespecic hardware design details of the radar. Additionally,noise is introduced into the received signal. For detailedinformation regarding this part, we refer readers to the sup-plementary material. Following this, radar signal process-ing algorithms are applied to the received signal, yieldingthe radar 3D tensor. This tensor encapsulates the radar re-ection intensity across range, azimuth angle, and Dopplercells. Notably, we consider a radar with only azimuth angleand without elevation, although the simulation is not limitedto such radar and could be expanded to include elevation. Inthe nal step of (b), the 3D tensor is transformed intoan image format with dimensions of range and azimuth an-gle by selecting the maximum value along the Doppler di-mension. Its worth mentioning that the simulation provides exibility in outputting the entire radar tensor or convertingit into an image using various techniques, depending on theimplementation of the object detection DNN.The conventional radar simulation method depicted in(b), necessitates an in-depth comprehension of theradar hardware design and the associated software pro-cessing to faithfully mimic a specic radar. This entailsa comprehensive understanding of specic details such asthe transmit signal waveform, antenna array conguration,sampling rate, beamforming algorithms, and more. How-ever, this presents a major challenge, as these details areoften proprietary and not publicly disclosed by radar man-ufacturers. This limitation is resolved in (c) by Rad-SimReal, as elaborated in the following.RadSimReal relies on the radars point spread function(PSF). Every reection point is apparent in the radar tensorby a multi-dimensional PSF, spanning dimensions of range,angle, and Doppler, and centered on the coordinates of thereection point. (a) shows an example of a 2D slice ofa radars PSF in range and angle centered at a range of 25mand an azimuth angle of 0. The PSF exhibits a wide spreadin the angular domain but a narrow spread in both rangeand Doppler, although the Doppler aspect is not depicted inthe gure. This characteristic arises from the radars coarseangular resolution and high range and Doppler resolution.It is essential to note that the shape of the PSF dependsupon the specic radar hardware design and signal process-ing algorithms . Fortunately, it can be acquired from aradar tensor of a narrow object, such as a pole or a dedi-cated corner reector . Importantly, obtaining the PSFdoes not require knowledge of any details about the radardesign, thereby circumventing a key limitation of conven-tional radar simulation. The straightforward procedure forobtaining the radars PSF from a radar tensor is detailed inthe supplementary material.In (b), we present a radar image generated throughthe conventional simulation outlined in (b). This im-age illustrates a scenario with three reection points andis free from noise. In (c), we depict the outcome ofthe conventional simulation for the same scenario, this timewith the addition of noise. The white points in the guresdenote the locations of these reection points. Notably, theimage displays a superposition of the radars PSF centeredon each point. Therefore, the same image can be obtainedby convolution between the reection points and the PSF.As a result, RadSimReal generates the radar tensor by 3Dconvolution of the reection points with the PSF, followedby the addition of noise, as presented in (c). The ten-sor is then converted to an image similarly as in (b).Despite the differences in the simulation implementa-tions in (c) and (b), they lead to the generationof an identical radar image when the full PSF is applied.For a detailed mathematical explanation of this equivalence,",
  "(c) Proposed Radar Simulation": ". Block diagram illustrating the processing steps for conventional simulation (a)+(b) and RadSimReal (a)+(c). (a) Simulates theenvironment to generate reection points with RF reectivity of an automotive scene, while (b) and (c) represent the conventional approachand RadSimReals approach, respectively, for transforming the reection points into a radar image. please refer to the supplementary material. Notably, the en-ergy of the PSF diminishes rapidly from its central point inboth the range and Doppler domains. To reduce the compu-tational complexity of the simulation, we truncate the PSFto encompass up to 99% of its energy. This truncation leadsto a substantial reduction in the PSF size by a factor exceed-ing 1000, thereby signicantly enhancing the run-time ofthe convolution operation in the simulation. In (d), thetruncated PSF is displayed, containing 99% of the energy ofthe original PSF from (a). The radar image, obtainedthrough convolution of the truncated PSF with the samethree reection points as depicted in (b), is showcasedin (e) without noise and in (f) with the additionof noise. The truncated portion of the PSF has very low in-tensity value (about 80 dB lower than PSF peak), which issignicantly lower than the noise level. Consequently, thedifference between the radar image from the conventionalsimulation ((c)) and our proposed simulation utilizingthe truncated PSF ((f)) is practically indistinguishable. While RadSimReal ((a)+(c)) produces radar im-ages similar to the conventional simulation ((a)+(b)),it possesses two signicant advantages over the conven-tional simulation. Firstly, it eliminates the necessity of pos-sessing in-depth radar design information by relying on asimple radar measurement of its PSF and noise variance, asdetailed in the supplementary material. Secondly, it exhibitsa signicantly faster run-time, as evaluated and demon-",
  ". Simulation Fidelity Evaluation": "In this section, we assess the resemblance between realradar images and synthetic images generated by RadSim-Real.Our evaluation initiates with a comparison be-tween synthetic and real radar images generated from thesame scenario.Creating a simulation scene that faith-fully replicates a real-world scenario for which an actualradar measurement was taken poses a considerable chal-lenge. To overcome this challenge, we substituted the re-ection points derived from the CARLA simulation enginein (a) with points obtained from a high-resolution LI-DAR sensor. This LIDAR was positioned in close proxim-ity to the radar and captured measurements from the samescene at the same time as the real radar measurement. Forthe assignment of the RF reection intensity to the LIDARpoints the surface material and orientation of the LIDARpoints were obtained by the following three steps: (1) Man-ual segmentation of LIDAR points into object types suchas vehicles, poles, signs, roads, and buildings. (2) Surfacematerial allocation of each point based on its object type.(3) Computation of the angle of the surface normal vectorusing a polygon mesh generated from the LIDAR points.The remaining stages of the simulation processing were asoutlined in (c).An example of a real radar image is shown in (c)",
  "(f)": ". Radar image generated with conventional simulation vs.RadSimReal. (a) Radars PSF 2D slice in range and angle dimen-sions. (b) Radar image without noise generated by conventionalsimulation for a scenario with 3 reection points. (c) Radar imageof (b) with noise. (d) Truncated PSF with 99% of its energy. (e)Radar image obtained for the same scenario as in (b) by RadSim-Real without noise, (f) The radar image of (e) with noise. and a synthetic radar image in (d) generated from thesame scene (as explained above). The prototype radar usedin (c) had 3.9 azimuth resolution, and 0.28m rangeresolution, and the same radar was simulated in (d).The camera image of the scene is presented in (a),and the LIDAR points segmented to different object typesare shown in (b). It is observed that the synthetic andreal reection intensity images have close resembles, whichshows that the simulation models well the real radar.Next, we present a comparison between real radar im-ages extracted from the RADDet dataset , and syn-thetic images generated by simulating the same radar asin RADDet with RadSimReal.As the RADDet datasetlacks LIDAR measurements, simulating the precise scenar-ios of real radar images, as depicted in , is unfea-sible. Hence, we present synthetic and real radar imagesfrom different scenarios and assess their characteristic re-semblance. In , the upper two rows illustrate syn-thetic radar images along with their corresponding cameraimages, produced by simulating the radar used in the RAD-Det dataset. Meanwhile, the lower two rows exhibit real",
  "(d) Simulated radar image": ". Comparison between RadSimReal image and a real radarimages for the same scenario. (a) Camera image of the scenario.(b) High-resolution LIDAR points segmented by object type. (c)Real radar image. (d) RadSimReal image. The black points in (c)and (d) represent the LIDAR points. radar images and their respective camera images from theRADDet dataset. Although the synthetic and real imagesstem from different scenarios and cannot be compared oneto one, they exhibit analogous characteristics. Both real andsynthetic radar images display reections with varying in-tensities and have a similar spreading functions. The closeresemblance between the synthetic and real images makesit hard to distinguish between them, which is another indi-cation that RadSimReal models well the real radar.We also assessed the statistical similarity between theRadSimReal data and real data using the Frechet InceptionDistance (FID) . The FID score is commonly employedin the literature to quantify the statistical resemblance be-tween synthetic and real datasets. The FID score betweenthe RADDet training set (comprising 8196 images) and theRADDet test set (comprising 1962 images) is 6.76. Onthe other hand, the FID score between the RADDet train-ing set and a synthetic dataset of 10,000 images generatedby RadSimReal is 6.54. The similarity between these twoscores indicates that the statistical characteristics of the syn-thetic dataset produced by RadSimReal closely resemblewith those of the RADDet data.",
  ". Computation Efciency": "The complexity ratio between conventional simulation andRadSimReal corresponds to the ratio between the entireradar tensor volume and the PSF volume. As explained in, our simulation truncates the PSF to preserve 99%of its energy, drastically reducing its volume. This resultsin a substantial complexity reduction, approximately by afactor of 1000, with RadSimReal compared to conventionalradar simulation. Further details on the computational com-plexity calculation and run time measurements are available",
  ". Simulation to Real Domain Gap Analysis": "In this section, we analyze the object detection performancegap between models trained with RadSimReal data andthose trained with real data, both tested on real data. For thisanalysis we use three different object detection methods:U-Net, RADDet, and Probabilistic. U-Net employs aU-Net as proposed in with an additional input channelof the input image Cartesian coordinates. RADDet refersto the object detection network introduced in the RADDetpaper , while Probabilistic is the network from .We assess performance using the RADDet , CAR-RADA , and CRUW datasets. These datasets fea-ture automotive radar reection images captured with a TIradar prototype in diverse scenes, providing ex-tensive testing coverage. While RADDet and CARRADAfeature 2D bounding box annotations for objects, CRUWprovides annotations indicating the center points of objects,to which weve subsequently added bounding box exten-sions using the CFAR algorithm . RADDet includesimages from 15 densely populated automotive scenarioswith favorable weather conditions, while CARRADA com-prises 30 staged scenarios with varying object densities andweather conditions, including challenging conditions likesnow. In both datasets the radar is mounted on a stationaryplatform. In the CRUW dataset the radar is mounted on avehicle, capturing radar images from scenarios where theego-vehicle was in motion (highway and city streets) andscenarios where it was stationary (campus roads and park-ing lots).We employed a dataset split for RADDet that ensuresdifferent scenarios in the train and test sets thereby prevent-ing potential overtting in the split proposed in the datasetpaper . Our RADDet split consists of a training set with8196 images and a test set with 1962 images. For CAR-RADA, the training set comprises 2208 images, and thetest set includes 276 images. As for CRUW, our trainingset comprises 9623 images, with a test set of 2226 images.The synthetic dataset generated by RadSimReal comprised10000 training images, comparable in size to that of RAD-Det and CRUW. The simulated scenarios involved a radarmounted on a vehicle driving in city streets, experiencingboth stationary and moving phases.We conducted performance tests on the three object de-tection models mentioned above using three separate testdatasets: RADDet, CARRADA and CRUW. These mod-els were individually trained with four distinct datasets:the RADDet training set, the CARRADA training set, theCRUW training set, or the RadSimReal dataset.Perfor-mance was assessed using Average Precision (AP) for classcar at IOU thresholds 0.1, 0.3 and 0.5. The AP was deter-mined by the area under the precision-recall curve. presents AP results, comparing performance between train-ing and testing on individual datasets versus training withRadSimReal and testing on different datasets. It addition-ally includes cross-dataset evaluation between RADDet andCARRADA, both characterized by a stationary radar setup(unlike CRUW), with the primary difference lying in theirscenes.The results reveal several important insights. All threemodels trained using RadSimReal exhibit performance onboth the RADDet and CRUW test sets that closely resem-bles their performance when trained on the correspond-ing RADDet or CRUW training sets. In evaluations withthe CARRADA test set, models trained with RadSimRealconsistently outperform those trained with the CARRADAtraining set, likely due to the small size of the latter. No-tably, models trained with RADDet experience a signicantperformance decline on the CARRADA test set comparedto their performance when trained with RadSimReal. Theseresults demonstrate that object detection DNNs trained withRadSimReal perform comparable to those trained on realdata and even outperform DNNs trained on real data whensubjected to cross-dataset evaluation or when dealing withlimited training data.Subsequently, we assess the performance of object de-tection models on the RADDet test set when trained using acombination of data from RadSimReal and the training setof RADDet. The outcomes of this evaluation are presentedin . The ndings reveal that augmenting the Rad-SimReal dataset with real datasets from RADDet does notyield a signicant performance enhancement. This suggeststhat the domain shift from RadSimReal data to real data isinsignicant.Next, we provide qualitative examples comparing be-tween an object detection model trained with RadSimRealand one trained with real data. In , we showcase thedetection scores at the output of the U-Net model for twoexamples taken from the RADDet test set, comparing theperformance of the model trained with RadSimReal againstthe same model trained with the RADDet training set. Eachexample is displayed in a separate row, featuring the origi-nal image alongside the output detection scores and bound-ing boxes of both models. Notably, the detection scores andboxes of both models resemble each other, indicating thatthe model trained with synthetic data delivers similar per-formance to the one trained exclusively on real data.The analysis in this section shows RadSimReals suc-cess in bridging the object detection performance gap be-tween synthetic and real data. It is important to note thatthe images generated by RadSimReal are similar to thoseproduced by other existing physical radar simulations. Con-sequently, training object detection models with other phys-ical radar simulations could achieve a similar performanceas with RadSimReal. The signicance of our study lies in",
  "(b)": ". Qualitative comparison of object detection DNN trained on RadSimReal vs. real data from the RADDet dataset. Rows correspondto different scenarios from RADDet test set. (a) Input radar image, (b) U-Net models detection score and bounding boxes trained withRADDet. (c) U-Net trained with RadSimReal data. Detected and ground truth bounding boxes marked in pink and white, respectively.",
  "CARRADASim70.7762.4743.96Sim + Real71.1162.6344.52": "unveiling this key discovery for the rst time and introduc-ing RadSimReal, which holds advantages over existing sim-ulations. It eliminates the need for in-depth knowledge ofthe radar design, typically undisclosed, and has faster run-time. Additionally, we acknowledge that the synthetic toreal performance gap can also be closed by generating datawith generative methods such as GAN. However, these ap-proaches have a major drawback compared to the physi-cal simulation model; they require the collection of a largeamount of real data for each distinct variation in radar type,its mounting conguration and environmental conditions.",
  ". Conclusion": "This paper introduces RadSimReal, a novel physical radarsimulation that generates synthetic radar images for train-ing object detection DNNs. We have shown that the Rad-SimReal images closely resemble real radar images bothqualitatively and statistically. Most importantly, our resultsreveal that training object detection DNNs with these syn-thetic images and testing them on real data yield results sim-ilar to those obtained when training exclusively with realdata. Moreover, it attains superior performance in cross-dataset evaluations with different real datasets.RadSimReal offers distinct advantages over alternativemethods of synthetic data generation. It can efciently sim-ulate diverse radar types without the need for extensive realdata collection, a process demanding substantial resources,or in-depth knowledge of proprietary radar implementationdetails, which are often condential. Instead, it only re-quires a measurement of the radars PSF. This work high-lights the great potential of radar simulation in radar-basedcomputer vision applications, paving the way for its in-creased adoption and further exploration in this eld. Maximilian Arnold, M Bauhofer, Silvio Mandelli, MarcusHenninger, Frank Schaich, Thorsten Wild, and Stephan tenBrink. Maxray: A raytracing-based integrated sensing andcommunication framework. In 2022 2nd IEEE InternationalSymposium on Joint Communications & Sensing (JC&S),pages 17. IEEE, 2022. 2, 3 Dan Barnes, Matthew Gadd, Paul Murcutt, Paul Newman,and Ingmar Posner. The oxford radar robotcar dataset: Aradar extension to the oxford robotcar dataset.In 2020IEEE International Conference on Robotics and Automation(ICRA), pages 64336438. IEEE, 2020. 3",
  "Charles Cook. Radar signals: An introduction to theory andapplication. Elsevier, 2012. 11, 13": "Andreas Danzer, Thomas Griebel, Martin Bach, and KlausDietmayer.2d car detection in radar data with pointnets.In 2019 IEEE Intelligent Transportation Systems Conference(ITSC), pages 6166. IEEE, 2019. 3 Marcio L Lima de Oliveira and Marco JG Bekooij. Gen-erating synthetic short-range fmcw range-doppler maps us-ing generative adversarial networks and deep convolutionalautoencoders.In 2020 IEEE Radar Conference (Radar-Conf20), pages 16. IEEE, 2020. 2, 3 Xu Dong, Pengluo Wang, Pengyue Zhang, and LangechuanLiu.Probabilistic oriented object detection in automotiveradar. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition Workshops, pages 102103, 2020. 1, 3, 7",
  "ulator. In 2010 Asia-Pacic Microwave Conference, pages16651668. IEEE, 2010. 4": "Zhaofei Feng, Shuo Zhang, Martin Kunert, and WernerWiesbeck. Point cloud segmentation with a high-resolutionautomotive radar. In AmE 2019-Automotive meets Electron-ics; 10th GMM-Symposium, pages 15. VDE, 2019. 3 Eduardo C Fidelis, Fabio Reway, Herick Ribeiro, Pietro LCampos, Werner Huber, Christian Icking, Lester A Faria,and Torsten Schon.Generation of realistic synthetic rawradar data for automated driving applications using genera-tive adversarial networks. arXiv preprint arXiv:2308.02632,2023. 2, 3 Yuval Haitman and Oded Bialer. Boostrad: Enhancing objectdetection by boosting radar reections. In Proceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision, pages 16381647, 2024. 3",
  "Roland Hess. Blender foundations: The essential guide tolearning blender 2.5. Taylor & Francis, 2013. 3": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. Gans trained by atwo time-scale update rule converge to a local nash equilib-rium. Advances in neural information processing systems,30, 2017. 3, 6 Nils Hirsenkorn, Paul Subkowski, Timo Hanke, AlexanderSchaermann, Andreas Rauch, Ralph Rasshofer, and ErwinBiebl. A ray launching approach for modeling an fmcw radarsystem. In 2017 18th International Radar Symposium (IRS),pages 110. IEEE, 2017. 2, 4 Martin Holder, Clemens Linnhoff, Philipp Rosenberger, andHermann Winner. The fourier tracing approach for modelingautomotive radar sensors. In 2019 20th International RadarSymposium (IRS), pages 18. IEEE, 2019. 2, 4",
  "Eugene F Knott, John F Schaeffer, and Michael T Tulley.Radar cross section. SciTech Publishing, 2004. 4, 11": "Florian Kraus, Nicolas Scheiner, Werner Ritter, and KlausDietmayer. Using machine learning to detect ghost imagesin automotive radar. In 2020 IEEE 23rd International Con-ference on Intelligent Transportation Systems (ITSC), pages17. IEEE, 2020. 3 Michael Meyer, Georg Kuschk, and Sven Tomforde. Graphconvolutional networks for 3d object detection on radar data.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 30603069, 2021. 1, 3 Anthony Ngo, Max Paul Bauer, and Michael Resch. A multi-layered approach for measuring the simulation-to-reality gapof radar perception for autonomous driving. In 2021 IEEEInternational Intelligent Transportation Systems Conference(ITSC), pages 40084014. IEEE, 2021. 2 Arthur Ouaknine, Alasdair Newson, Julien Rebut, FlorenceTupin, and Patrick Perez.Carrada dataset: Camera andautomotive radar with range-angle-doppler annotations. In2020 25th International Conference on Pattern Recognition(ICPR), pages 50685075. IEEE, 2021. 1, 3, 7 Dong-Hee Paek, Seung-Hyun Kong, and Kevin Tirta Wijaya.K-radar: 4d radar object detection dataset and benchmarkfor autonomous driving in various weather conditions. arXivpreprint arXiv:2206.08171, 2022. 3 Julien Rebut, Arthur Ouaknine, Waqas Malik, and PatrickPerez. Raw high-denition radar for multi-task learning. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1702117030, 2022. 1,3 Joseph Redmon, Santosh Divvala, Ross Girshick, and AliFarhadi. You only look once: Unied, real-time object de-tection. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 779788, 2016. 1, 3",
  "Hermann Rohling.Radar cfar thresholding in clutter andmultiple target situations. IEEE transactions on aerospaceand electronic systems, pages 608621, 1983. 3, 7": "Nicolas Scheiner, Nils Appenrodt, Jurgen Dickmann, andBernhard Sick. Radar-based road user classication and nov-elty detection with recurrent neural network ensembles. In2019 IEEE Intelligent Vehicles Symposium (IV), pages 722729. IEEE, 2019. 3 Christian Schoffmann, Barnaba Ubezio, Christoph Bohm,Stephan Muhlbacher-Karrer, and Hubert Zangl.Virtualradar: Real-time millimeter-wave radar sensor simulation forperception-driven robotics. IEEE Robotics and AutomationLetters, 6(3):47044711, 2021. 2, 3 Ole Schumann, Markus Hahn, Jurgen Dickmann, and Chris-tian Wohler. Semantic segmentation on radar point clouds.In 2018 21st International Conference on Information Fu-sion (FUSION), pages 21792186. IEEE, 2018. 3 Christian Schuler, Marcel Hoffmann, Johanna Braunig, In-grid Ullmann, Randolf Ebelt, and Martin Vossiek. A realisticradar ray tracing simulator for large mimo-arrays in automo-tive environments. IEEE Journal of Microwaves, 1(4):962974, 2021. 2, 3, 4 Marcel Sheeny, Emanuele De Pellegrin, Saptarshi Mukher-jee, Alireza Ahrabian, Sen Wang, and Andrew Wallace.Radiate: A radar dataset for automotive perception in badweather. In 2021 IEEE International Conference on Roboticsand Automation (ICRA), pages 17. IEEE, 2021. 1, 3",
  "radar object detection network cross-supervised by camera-radar fused object 3d localization. IEEE Journal of SelectedTopics in Signal Processing, 15(4):954967, 2021. 1": "Yizhou Wang, Zhongyu Jiang, Yudong Li, Jenq-NengHwang, Guanbin Xing, and Hui Liu. Rodnet: A real-timeradar object detection network cross-supervised by camera-radar fused object 3d localization. IEEE Journal of SelectedTopics in Signal Processing, 15(4):954967, 2021. 3, 7 Rob Weston, Oiwi Parker Jones, and Ingmar Posner. Thereand back again: Learning to simulate radar data for real-world applications. In 2021 IEEE International Conferenceon Robotics and Automation (ICRA), pages 1280912816.IEEE, 2021. 2, 3",
  "Tim A Wheeler, Martin Holder, Hermann Winner, andMykel J Kochenderfer. Deep stochastic radar models. In2017 IEEE Intelligent Vehicles Symposium (IV), pages 4753. IEEE, 2017. 2, 3": "Ao Zhang, Farzan Erlik Nowruzi, and Robert Laganiere.Raddet: Range-azimuth-doppler based radar object detectionfor dynamic road users. In 2021 18th Conference on Robotsand Vision (CRV), pages 95102. IEEE, 2021. 1, 3, 6, 7, 14,15 Guoqiang Zhang, Haopeng Li, and Fabian Wenger. Objectdetection and 3d estimation via an fmcw radar using a fullyconvolutional network. In ICASSP 2020-2020 IEEE Interna-tional Conference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 44874491. IEEE, 2020. 1, 3, 7",
  "A. PSF and Noise Variance Measurement": "This section explains the process of measuring the radarsPoint Spread Function (PSF) and noise variance, essen-tial components for RadSimReal as elaborated in . To obtain the PSF, a radar measurement can be con-ducted in a scenario featuring a narrow and stationary ob-ject, like a pole, positioned in an isolated area where thereare no prominent reecting objects nearby. The pole, char-acterized by narrow spread in distance, azimuth angle, andDoppler frequency, can be treated as an approximation ofa point reector. If possible, using a radar corner reectoras the isolated target is preferred. A radar corner reectoris specically designed to exhibit an exceptionally narrowspread in all dimensions .To measure the PSF, multiple radar tensors that capturethe same scenario as described above are collected at var-ious time instances when both the radar and the observedobject remain stationary. These tensors are then averaged toreduce the noise in the PSF measurement. Subsequently, thetruncated PSF utilized in RadSimReal is derived by extract-ing a 3D segment from the averaged radar tensor, centeredaround the reection point of the narrow object. The in-tensity of the PSF diminishes rapidly from its central point.Each dimension of the PSF is truncated at a point where itsintensity signicantly falls below the radars noise variance(the noise variance in the tensor without averaging). provides a demonstration of the PSF measure-ment for the radar utilized in the RADDet dataset. (a)presents a camera image of a scene from RADDet featuringa pole that was used for the PSF measurement. Figs. 6(b),(c), and (d) display the measured PSF slices in range,Doppler, and azimuth angle, respectively, alongside cor-responding slices of a PSF obtained through conventionalsimulation of the radar in the RADDet dataset (as detailedin (a)+(b)). The gure illustrates that the measuredPSF closely resembles the simulated radars PSF.It is worth noting that the simulation method employedto derive the reference PSF in Figs. 6 necessitates an in-depth understanding of the specic radar hardware designand processing algorithms. This information is not alwaysdisclosed by radar suppliers. In contrast, the measurementprocedure outlined above enables the acquisition of the PSFthrough a straightforward measurement that does not re-quire detailed knowledge of the radar design.Next, we proceed to elucidate how to measure the radarsnoise variance, a prerequisite for RadSimReal as detailed in. The noise variance can be determined by iden- tifying a region within the radar tensor that lacks any ob-jects. This specic portion of the radar tensor comprisesonly noise, allowing the calculation of noise variance byassessing the variance of the tensor cells within this region. illustrates an instance of a radar image from the RAD-Det dataset, with red rectangles indicating sections withoutreections that can be utilized for measuring the noise vari-ance.",
  "B. Equivalency Between Outputs of Conven-tional Simulation and RadSimReal": "In , we claimed that the identical output tensorfrom the conventional simulation ((a)+(b)) couldbe achieved by convolving the radars Point Spread Func-tion (PSF) with the 3D reection points in the scene ((a)+(c)). This section provides an explanation for the va-lidity of this equivalence. We rst provide an intuitive un-derstanding of this equivalence through a straightforwardexample in Section B.1. Subsequently, in Section B.2, wepresent a formal mathematical derivation of this equiva-lence.",
  "B.1. Intuitive Explanation of Simulations Equiva-lence": "We explain the equivalence between the conventional sim-ulation and RadSimReal through a simplied example of aradar signal. The conventional radar simulation of this ex-ample is depicted in . (a) illustrates a transmit-ted radar pulse signal at time zero. In (b), the receivedsignal is illustrated for a scenario involving two reectionpoints. It is evident that the received signal is essentiallythe transmitted signal but delayed by 1 and 2. These de-lays represent the times taken for the signal to travel fromthe radar to the rst and second reection points and back.Importantly, these time delays are proportional to the dis-tances of the rst and second reection points denoted byd1 and d2, respectively. To obtain the received energy foreach delay (distance) hypothesis, the radar employs a matchlter on the received signal . The match lter oper-ation is a correlation between the received signal and thetransmitted signal. (c) displays the result of the matchlter in this example. The signal propagation times, 1 and2, are proportional to the reection points distances. Hencethe time scale in (c) can be converted to distance. Thecorresponding match lter output as a function of distanceis depicted in (d).",
  ". Measurement of radar noise variance from radar imagein the RADDet dataset. Red rectangles in the radar image markreection-free regions utilized for noise variance measurement": "illustrates the simulation conducted by RadSim-Real for the same example depicted in . In (a),Kronecker delta functions at the distances d1 and d2 of thetwo reection points are presented.(b) illustratesthe radars PSF, which is the auto-correlation of the trans-mitted signal.(c) displays the output of the con-volution between the delta functions in (a) and thePSF in (b). It is evident that the match lter outputin (d) is identical to the result in (c). Thus,the output of the conventional simulation can be achieved through convolution between the radars PSF and the reec-tion points, represented as delta functions at the distancesof the reection points. This approach is the simulationmethodology employed by RadSimReal.While the representation in and 9 show a one-dimensional match lter applied to a simplied single trans-mitted radar pulse, practical automotive radars involve ex-tending the transmitted signal and match lter across a se-quence of pulses and multiple antennas. Consequently, theresult is a multidimensional output tensor with dimensionsof range (distance), Doppler, and angle, rather than a single-dimensional output. This tensor is the one discussed in Sec-tion3. Nevertheless, the processes in each dimension canbe separable. Therefore, the fundamental principle remainsunchanged: the output radar tensor can be calculated byconvolving a 3D PSF with dimensions for range, Doppler,and angle with delta functions in the 3D space having thesame dimensions. The delta functions are positioned in thisspace at the reection points range, Doppler, and angle. Amathematical derivation of this equivalence is detailed inSection B.2.",
  "B.2. Mathematical Derivation of the SimulationsEquivalence": "The radar emits a periodic sequence of short signals throughmultiple antennas, and this signal can be characterized inthree dimensions as s(n, m, q), where the indices n, m, andq correspond to distinct time scales. These time scales rep-resent the time samples of the short signal, the time samplesof the short signal periods within the repetition sequence,",
  "Distance": ".Illustration of the RadSimReal processes achievingan identical output as the conventional simulation in the simpli-ed example shown in . (a) Representation of two reec-tion points using Kronecker delta functions centered at reectionpoints distances d1 and d2. (b) The radars PSF as a functionof distance. (c) The result of convolution between the reectionpoints in (a) and the PSF in (b). The output in (c) is identical tothe output of the conventional simulation shown in (d). and the signal duration along the antenna array, respectively.The transmitted signal reects off objects in the environ-ment and returns to the radar with delays in each of the timescales, which are proportional to the reection position andspeed. Let id, if, and i denote the ith reection pointsdelays in the short signal duration, the delay in the periodbetween the short signals, and the delay between the an-tennas. These delays correspond to the reection pointsdistance (range), Doppler frequency (radial velocity), andangle, respectively. The received signal is an aggregation ofthe received signals from individual reection points, eachwith its corresponding reection intensity. The received sig-nal samples along each of the three time dimensions can beexpressed as follows:",
  "iis(n id, m if, q i),(1)": "where i represents the intensity of the ith reection point.The radar tensor, which represents the received energyin each distance, Doppler, and angle, is obtained by apply-ing a match lter to the received signal . The matchlter is a 3D correlation between the received signal andthe transmitted signal s(n, m, q), in all three delays dimen-sions, which are proportional to the distance, Doppler, andangle. This correlation is expressed by:",
  "ii(n id, m if, q i),": "(5)where the symbol denotes a convolution operation, and(n id, m if, q i) is a 3D Kronecker delta function.This function takes a value of 1 when n = id, m = if,and q = i, and is zero elsewhere. Therefore, rather thanobtaining the radar tensor through match ltering, as shownin (2), it can equivalently be obtained by convolving theradars PSF, x(n, m, q), with reection points that are rep-resented by 3D Kronecker delta functions that are shifted bythe 3D delays (id, if, i) of the reection points and scaledby their intensities (i), as expressed in (5). The 3D delaysare directly proportional to the range, Doppler frequency,and angle of the reections. Consequently, the dimensionsof the radar tensor are eventually transformed from delaysto range, Doppler, and angle. This equivalent approach isthe methodology utilized in deriving the tensor in RadSim-Real.",
  "U-Net94.6889.5971.0084.7683.0155.53": "Real. Both simulations initiate with the shared step of gen-erating reection points in the environment (as depicted in(a)). The runtime of this phase depends on the graph-ics simulation engine and can be very fast, even in real-time.The signicant run-time difference lies in transforming re-ection points into radar images (parts (b) and (c) of ), which are evaluated next.The initial phase of the conventional simulation involvesthe generation of received samples per radar image frame.These samples result from aggregating the received signalsfrom individual reection points, leading to a computationalcomplexity of O(NpNr), where Nr represents the numberof received samples per radar image, and Np denotes thenumber of reection points in the scenario. The numberof received samples per frame, Nr, is directly proportionalto the number of cells in the radar tensor, denoted as Ns.Consequently, the complexity of the rst part of the con-ventional simulation can be expressed as O(NpNs).In the subsequent stage of the conventional radar simu-lation, signal processing algorithms are applied to the re-ceived signal to generate the radar tensor. These algorithmscoherently combine received signal samples for each range,angle, and Doppler cell in the radar tensor. This process,called match ltering, is efciently executed through a se-ries of Fast Fourier Transform (FFT) operations in range,Doppler, and angle, resulting in an overall complexity thatis lower bounded by O(Ns log(Ns)).The total complexity of the conventional simulation isderived by summing the complexities of the two afore-mentioned parts. This yields a complexity of O(Ns(Np +log(Ns))). Since Np is signicantly larger than log(Ns),the complexity of the conventional simulation approachsimplies to O(NsNp).Moving on, we proceed to compute the computationalcomplexity of RadSimReal, which involves performing con-volution between the reection points and the radars PSF.The reection points are sparsely distributed within theradar tensor, i.e., Ns Np. Consequently, the convolutionbetween the reection points and the PSF can be carried outas a sparse convolution. This operation entails aggregatingthe PSFs of reection points, resulting in a complexity ofO(NpNf), where Nf denotes the number of cells in theradars PSF.",
  "Scene IDFrame Numbers": "00 439, 559 724, 1549 19711440 555, 731 1548, 1972 257122572 303833039 343743438 365353654 407364074 433174332 5018, 5623 624385019 5622, 6244 660896609 8046108047 8634118635 9158129159 9437139438 9745, 10175 10292149746 10174 Therefore,thecomplexityratiobetweenconven-tionalsimulationsandRadSimReal isexpressedasO((NsNp)/(NpNf)) = O(Ns/Nf), which represents theproportion of the entire radar tensor volume to the PSF vol-ume. As detailed in , our simulation truncates thePSF to preserve 99% of its energy, signicantly reducing itsvolume. Consequently, this leads to a substantial reductionin complexity, exemplied by a ratio of 1250 for the radarutilized in the RADDet, CARRADA, and CRUW datasets.We tested the average run time for generating radar im-ages using the conventional physical radar simulation andour simulation. These tests were conducted for the TI radaremployed in the RADDet . We implemented thesimulations in Matlab 2020a, making use of the parallelprocessing toolbox. The computations were executed on acomputer equipped with an Intel(R) Xeon(R) W-2235 CPUoperating at 3.80GHz, alongside an Nvidia Quadro RTX5000 GPU with 16GB of memory. The results reveal thatthe average run time for generating a radar images from re-ection points with our simulation is 0.0105 second, andwith the conventional physical simulation it takes 5.296 sec-onds. RadSimReal generates images about 500 times fasterthan the conventional simulation, which is on the order ofthe factor 1250 that was obtained from the analysis above.",
  "D. RADDet Train-Test Set Split": "The performance evaluation conducted in uti-lized a train-test set split of the RADDet dataset that dif-fers from the split proposed in . displays theAverage Precision (AP) results at IOU 0.1, 0.3, and 0.5 forthe three object detection methods employed in the paper (RADDet, Probabilistic, and U-Net). The comparisonis made between the original RADDet train-test split andour suggested split. The AP results are assessed on the testset within each respective split. The results reveal that allmethods achieved signicantly higher AP results with theoriginal RADDet split compared to our split. This discrep-ancy in results can be attributed to the fact that the test andtraining images in the original split were derived fromthe same scenarios, often with small temporal gaps. Conse-quently, a strong correlation is established between the testand training samples, leading to overtting of all methodson the test set.To address the issue of overtting, we implemented atrain-test set partitioning strategy that ensures distinct sce-narios between the training and testing sets. The RADDetdataset comprises 15 unique scenes, each detailed in . In our partitioning scheme, scenes 9 and 11 were desig-nated for the test set, while the remaining scenes were usedfor the training set. The adjusted training set comprises a to-tal of 17,021 cars compared to 16,755 in the original split.For the test set, we have 4,094 cars compared to 4,135 inthe original split."
}