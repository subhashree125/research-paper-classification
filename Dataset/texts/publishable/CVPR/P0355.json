{
  "Abstract": "Recent works in dataset distillation seek to minimizetraining expenses by generating a condensed syntheticdataset that encapsulates the information present in alarger real dataset.These approaches ultimately aim toattain test accuracy levels akin to those achieved by mod-els trained on the entirety of the original dataset. Previousstudies in feature and distribution matching have achievedsignificant results without incurring the costs of bi-level op-timization in the distillation process. Despite their convinc-ing efficiency, many of these methods suffer from marginaldownstream performance improvements, limited distillationof contextual information, and subpar cross-architecturegeneralization. To address these challenges in dataset dis-tillation, we propose the ATtentiOn Mixer (ATOM) moduleto efficiently distill large datasets using a mixture of chan-nel and spatial-wise attention in the feature matching pro-cess. Spatial-wise attention helps guide the learning pro-cess based on consistent localization of classes in their re-spective images, allowing for distillation from a broaderreceptive field.Meanwhile, channel-wise attention cap-tures the contextual information associated with the classitself, thus making the synthetic image more informative fortraining. By integrating both types of attention, our ATOMmodule demonstrates superior performance across variouscomputer vision datasets, including CIFAR10/100 and Tiny-Imagenet. Notably, our method significantly improves per-formance in scenarios with a low number of images perclass, thereby enhancing its potential.Furthermore, wemaintain the improvement on cross-architectures and ap-plications such as neural architecture search.",
  "*Equal contribution": ". The ATOM Framework utilizes inherent information tocapture both context and location, resulting in significantly im-proved performance in dataset distillation. We display the perfor-mance of various components within the ATOM framework, show-casing a 5.8% enhancement from the base distribution matchingperformance on CIFAR10 at IPC50. Complete numerical detailscan be found in . and inferencing pipelines .This growth can be attributed to the escalating complex-ity of model architectures and the ever-expanding scale ofdatasets. Despite the increasing computational burden, twodistinct approaches have emerged as potential avenues foraddressing this issue: the model-centric and data-centricapproaches. The model-centric approach is primarily con-cerned with mitigating computational costs by refining thearchitecture of deep learning models. Techniques such aspruning, quantization, knowledge distillation, and architec-tural simplification are key strategies employed within thisparadigm . In contrast, thedata-centric approach adopts a different perspective, focus-ing on exploring and leveraging the inherent redundancywithin datasets. Rather than modifying model architectures,",
  "arXiv:2405.01373v1 [cs.CV] 2 May 2024": "this approach seeks to identify or construct a smaller datasetthat retains the essential information necessary for main-taining performance levels. Coreset selection was a fairlyadopted method for addressing this gap .In particular works such as Herding and K-Center offered a heuristic-based approach to intelligently select aninformative subset of data. However, as a heuristic-basedmethod, the downstream performance is limited by the in-formation contained solely in the subset. More recently,shapely data selection found the optimal subset of databy measuring the downstream performance for every subsetcombination achievable in the dataset. However inefficientthis may be, the downstream performance is still limited bythe diversity of samples selected. therefore, Dataset Distil-lation (DD) has emerged as a front-runner wherein asynthetic dataset can be learned. Dataset distillation aims to distill large-scale datasetsinto a smaller representation, such that downstream mod-els trained on this condensed dataset will retain competi-tive performance with those trained on the larger originalone . Recently, many techniques have been intro-duced to address this challenge, including gradient match-ing , feature/distribution matching ,and trajectory matching .However, many ofthese methods suffer from complex and computationallyheavy distillation pipelines or inferior perfor-mance . A promising approach, DataDAM ,effectively tackled the computational challenges present inprior distillation techniques by employing untrained neu-ral networks, in contrast to bi-level optimization methods.However, despite its potential, DataDAM faced several sig-nificant limitations: (1) it obscured relevant class-content-based information existing channel-wise in intermediatelayers; (2) it only achieved marginal enhancements on pre-vious dataset distillation algorithms; and (3) it exhibited in-ferior cross-architecture generalization. In this work, we introduce ATtentiOn Mixer, dubbedATOM as an efficient dataset distillation pipeline that strikesan impressive balance between computational efficiencyand superior performance. Drawing upon spatial attentionmatching techniques from prior studies like DataDAM ,we expand our receptive field of information in the match-ing process. Our key contribution lies in mixing spatial in-formation with channel-wise contextual information. Intu-itively, different convolutional filters focus on different lo-calizations of the input feature; thus, channel-wise atten-tion aids in the distillation matching process by compress-ing and aggregating information from multiple regions asevident by the performance improvmenets displayed in Fig-ure 1. ATOM not only combines localization and context,but it also produces distilled images that are more gener-alizable to various downstream architectures, implying thatthe distilled features are true representations of the original dataset. Moreover, our approach demonstrates consistentimprovements across all settings on a comprehensive distil-lation test suite. In summary, the key contributions of thisstudy can be outlined as follows:[C1]: We provide further insight into the intricaciesof attention matching, ultimately introducing the use ofchannel-wise attention matching for capturing a higher levelof information in the feature-matching process. Our mixingmodule combines both spatial localization awareness of aparticular class, with distinctive contextual information de-rived channel-wise.[C2]:Empirically we show superior performanceagainst previous dataset distillation methods including fea-ture matching and attention matching works, without bi-level optimization on common computer vision datasets.[C3]: We extend our findings by demonstrating supe-rior performance in cross-architecture and neural architec-ture search. In particular, we provide a channel-only settingthat maintains the majority of the performance while incur-ring a lower computational cost.",
  ". Related Works": "Coreset Selection. Coreset selection, an early data-centricapproach, aimed to efficiently choose a representative sub-set from a full dataset to enhance downstream training per-formance and efficiency. Various methods have been pro-posed in the past, including geometry-based approaches , loss-based techniques as mentioned in , decision-boundary-focused methods , bileveloptimization strategies , and gradient-matching al-gorithms outlined in .Notable among them areRandom, which randomly selects samples as the coreset;Herding, which picks samples closest to the cluster center;K-Center, which selects multiple center points to minimizethe maximum distance between data points and their nearestcenter; and Forgetting, which identifies informative trainingsamples based on learning difficulties . Whilethese selection-based methods have shown moderate suc-cess in efficient training, they inherently possess limitationsin capturing rich information. Since each image in the se-lected subset is treated independently, they lack the rich fea-tures that could have been captured if the diversity withinclasses had been considered. These limitations have moti-vated the emergence of dataset distillation within the field. Dataset Distillation. Dataset distillation has emerged asa learnable method of synthesizing a smaller, information-rich dataset from a large-scale real dataset. This approachoffers a more efficient training paradigm, commonly ap-plied in various downstream applications such as contin-ual learning , neural architecture search, and federated learning . The semi-nal work, initially proposed by Wang et al. , introduced . (a) An overview of the proposed ATOM framework. By mixing attention, ATOM is able to capture both spatial localization andclass context. (b) Demonstration of the internal architecture for spatial- and channel-wise attention in the ATOM Module. The spatial-wiseattention computes attention at specific locales through different filters, resulting in a matrix output, whereas the channel-wise attentioncalculates attention between each filter, naturally producing a vectorized output. bilevel optimization, comprising an outer loop for learningthe pixel-level synthetic dataset and an inner loop for train-ing the matching network. Following this, several studiesadopted surrogate objectives to tackle unrolled optimizationproblems in meta-learning. For example, gradient match-ing methods learn images by aligningnetwork gradients derived from real and synthetic datasets.Trajectory matching improves performanceby minimizing differences in model training trajectories be-tween original and synthetic samples. Meanwhile, featurematching strategies aim to align fea-ture distributions between real and synthetic data within di-verse latent spaces. Despite significant advancements in thisfield, methods still struggle to find a trade-off between thecomputational costs associated with the distillation pipelineand the models performance. A recent work, DataDAM, used spatial attention to improve the performance offeature-matching-based methods by selectively matchingfeatures based on their spatial attention scores. However,although this method operates without bilevel optimiza-tion, it only marginally improves performance on larger testsuites. In this study, we delve deeper into the potential ofattention-based methods and demonstrate superior perfor-mance compared to DataDAM and previous benchmarksacross various computer vision datasets. Additionally, weachieve a lower computational cost compared to conven-tional attention-matching approaches by leveraging infor-mation in a channel-wise manner. Attention Mechanism.Attention mechanisms havebeen widely adopted in deep learning to enhance perfor-mance across various tasks . Initially appliedin natural language processing , it has extended to com- puter vision, with global attention models improvingimage classification and convolutional block attention mod-ules enhancing feature map selection. Additionally, at-tention aids model compression in knowledge distillation. They are lauded for their ability to efficiently incor-porate global contextual information into feature represen-tations. When applied to feature maps, attention can takethe form of either spatial or channel-based methods. Spa-tial methods focus on identifying the informative regions(where), while channel-based methods complementarilyemphasize the informative features (what). Both spatiallocalization and channel information are crucial for identi-fying class characteristics. Recently, Sajedi et al. proposedDataDAM to concentrate only on spatial attention, cap-turing class correlations within image localities for efficienttraining purposes. However, inspired by the inherent ob-fuscation of the content in the attention maps, we proposean Attention Mixer module that uses a unique combinationof spatial and channel-wise attention to capture localizationand information content.",
  ". Methodology": "Given the larger source dataset T= {(xi, yi)}|T |i=1 con-taining |T | real image-label pairs, we generate a smallerlearnable synthetic dataset S = {(sj, yj)}|S|j=1 with |S| syn-thetic image and label pairs.Following previous works, we use random sampling to initial-ize our synthetic dataset. For every class k, we obtain abatch of real and synthetic data (BTk and BSk , respectively)and use a neural network () with randomly initializedweights to extract intermediate and output features. We illustrate our method in where an L-layer neu-ral network () is used to extract features from the realand synthetic sets.The collection of feature maps fromthe real and synthetic sets can be expressed as (Tk) =[f Tk,1, , f Tk,L] and (Sk) = [f Sk,1, , f Sk,L], respec-tively. The feature f Tk,l comprises a multi-dimensional array within R|BTk |ClWlHl, obtained from the real dataset atthe lth layer, where Cl denotes the number of channels andHl Wl represents the spatial dimensions. Correspond-ingly, a feature f Sk,l is derived for the synthetic dataset.We now introduce the Attention Mixer Module (ATOM)which generates attention maps for the intermediate fea-tures derived from both the real and synthetic datasets.Leveraging a feature-based mapping function A(), ATOMtakes the intermediate feature maps as input and producesa corresponding attention map for each feature. Formally,we express this as: A(Tk)= [aTk,1, , aTk,L1] andA((Sk)) = [aSk,1, , aSk,L1] for the real and syntheticsets, respectively. Previous works have shown thatspatial attention, which aggregates the absolute values offeature maps across the channel dimension, can emphasizecommon spatial locations associated with high neuron acti-vation. The implication of this is retaining the most infor-mative regions, thus generating an efficient feature descrip-tor. In this work, we also consider the effect of channel-wiseattention, which emphasizes the most significant informa-tion captured by each channel based on the magnitude of itsactivation. Since different filters explore different regions orlocations of the input feature, channel-wise activation yieldsthe best aggregation of the global information. Ultimately,we convert the feature map f Tk,l of the lth layer into an at-tention map aTk,l representing spatial or channel-wise atten-tion using the corresponding mapping functions As() orAc() respectively. Formally, we can denote the spatial andchannel-wise attention maps as:",
  "(f Tk,l)(:, :, i). By leveraging both types of attention, wecan better encapsulate the relevant information in the inter-mediate features, as investigated in .3. Further, the": "effect of power parameters for spatial and channel-wise at-tention, i.e. ps and pc is studied in the .3.Given our generated spatial and channel attention mapsfor the intermediate features, we apply standard normaliza-tion such that we can formulate a matching loss between thesynthetic and real datasets. We denote our generalized lossLATOM as:",
  "vec(aTk,l) R|BTk |(WlHl) and zSk,l = vec(aSk,l)": "R|BSk |(WlHl) to represent the vectorized spatial atten-tion map pairs at the lth layer for the real and syntheticdatasets, respectively.Meanwhile, for channel-based at-tention, we have zTk,l = vec(aTk,l) R|BTk |(Cl) and zSk,l = vec(aSk,l) R|BSk |(Cl) to represent the flattenedchannel attention map pairs at the lth layer for the real andsynthetic datasets, respectively.The parameter K is thenumber of categories in a dataset, and P denotes the distri-bution of network parameters. We estimate the expectationterms in Equation (3) empirically if ground-truth data dis-tributions are not available.Following previous works , we lever-age the features in the final layer to regularize our matchingprocess. In particular, the features of the penultimate layerrepresent a high-level abstraction of information from theinput images in an embedded representation and can thusbe used to inject semantic information in the matching pro-cess . Thus, we employ LMMD as describedin out-of-the-box.Finally, we learn the synthetic dataset by minimizing thefollowing optimization problem using SGD optimizer:",
  "LATOM + LMMD,(4)": "where is the task balance parameter inherited from .In particular, we highlight that LMMD brings semantic infor-mation from the final layer, while LATOM mixes the spatialand channel-wise attention information from the interme-diate layers. Note that our approach assigns a fixed labelto each synthetic sample and keeps it constant during train-ing. A summary of the learning algorithm can be found inAlgorithm 1.",
  ": end forOutput: Synthetic dataset S = {(si, yi)}|S|i=1": "64 for additional experimentation. The supplementarymaterials provide more detailed dataset information.Network Architectures. We employ a ConvNet archi-tecture for distillation, following prior studies. The de-fault ConvNet comprises three convolutional blocks, eachconsisting of a 128-kernel 3 3 convolutional layer, in-stance normalization, ReLU activation, and 3 3 averagepooling with a stride of 2. To accommodate the increasedresolutions in Tiny ImageNet, we append a fourth convolu-tional block. Network parameters are initialized using nor-mal initialization in all experiments.Evaluation Protocol. We evaluate the methods usingstandard measures from previous studies .Five sets of synthetic images are generated from a real train-ing dataset with 1, 10, and 50 images per class. Then, 20neural network models are trained on each synthetic set us-ing an SGD optimizer with a fixed learning rate of 0.01.Each experiment reports the mean and standard deviationvalues for 100 models to assess the efficacy of distilleddatasets. Furthermore, computational costs are assessed bycalculating run-time per step over 100 iterations, as well aspeak GPU memory usage during 100 iterations of training.Implementation Details. We use the SGD optimizerwith a fixed learning rate of 1 to learn synthetic datasetscontaining 1, 10, and 50 IPCs over 8000 iterations with taskbalances () set at 0.01. Previous works have shown thatps = 4 is sufficient for spatial attention matching . Assuch we set our default case as: pc = ps = 4. This isfurther ablated in .3. We adopt differentiable aug-mentation for both training and evaluating the synthetic set,following . For dataset reprocessing, we utilizedthe Kornia implementation of Zero Component Analysis(ZCA) with default parameters, following previous works . All experiments are performed on a single A100GPU with 80 GB of memory. Further hyperparameter de-tails can be found in the supplementary materials.Competitive Methods.In this paper, we comparethe empirical results of ATOM on three computer visiondatasets: CIFAR10/100 and TinyImageNet. We evaluateATOM against four corset selection approaches and thir-teen distillation methods for training set synthesis.Thecorset selection methods include Random selection ,Herding , K-Center , and Forgetting .Wealso compare our approach with state-of-the-art distillationmethods, including Dataset Distillation (DD), Flexi-ble Dataset Distillation (LD), Dataset Condensation (DC), Dataset Condensation with Contrastive (DCC) ,Dataset Condensation with Differentiable Siamese Aug-mentation (DSA), Distribution Matching (DM),Deep Generative Priors (GLaD), Aligning Features (CAFE), VIG , Kernel Inducing Points (KIP),Matching Training Trajectories (MTT), and AttentionMatching (DAM).",
  ". Comparison with State-of-the-art Methods": "Performance Comparison.In this section, we presenta comparative analysis of our method against coreset anddataset distillation approaches. ATOM consistently outper-forms these studies, especially at smaller distillation ratios,as shown in . Since the goal of dataset distillationis to generate a more compact synthetic set, we emphasizeour significant performance improvements at low IPCs. Weachieve almost 4% improvement over the previous attentionmatching framework , DataDAM when evaluated onCIFAR-100 at IPC1. Notably, our performance on CIFAR-100 at IPC50 is 50.2% that is nearly 90% of the baselineaccuracy at a mere 10% of the original dataset. These exam-ples motivate the development of dataset distillation worksas downstream models can achieve relatively competitiveperformance with their baselines at a fraction of the train-ing costs. Our primary objective in this study is to investi-gate the impact of channel-wise attention within the feature-matching process. Compared to prior attention-based andfeature-based methodologies, our findings underscore thesignificance of channel-wise attention and the ATOM mod-ule, as validated also in the ablation studies in .3.Cross-architecture Generalization. In this section, weassess the generalization capacity of our refined dataset bytraining various unseen deep neural networks on it and thenevaluating their performance on downstream classificationtasks. Following established benchmarks ,we examine classic CNN architectures such as AlexNet, VGG-11 , ResNet-18 , and additionally, astandard Vision Transformer (ViT) . Specifically, weutilize synthetic images learned from CIFAR-10 with IPC50using ConvNet as the reference model and subsequently",
  "use AlexNet for CIFAR-10 dataset. All other methods use ConvNet for training and evaluation. Bold entries are the best results": "train the aforementioned networks on the refined datasetto assess their performance on downstream tasks. The re-sults, as depicted in , indicate that ATOM demon-strates superior generalization across a spectrum of archi-tectures.Notably, it achieves a significant performanceboost of over 4% compared to the prior state-of-the-art onResNet-18 . This implies that the channel-wise atten-tion mechanism effectively identifies features not only rel-evant to ConvNet but also to a wider range of deep neuralnetworks, thereby enhancing the refined dataset with thisdiscerned information.",
  ". Cross-architecture testing performance (%) on CIFAR-10with 50 images per class. The ConvNet architecture is employedfor distillation. Bold entries are the best results": "Distillation Cost Analysis. In this section, we delve intoan examination of the training costs required for the distilla-tion process. Although the main goal of dataset distillationis to reduce training costs across different applications suchas neural architecture search and continual learning, the dis-tillation technique itself must be efficient, enabling smoothoperation on consumer-grade hardware. Approaches such",
  "IPC1IPC10IPC50IPC1IPC10IPC50": "DC 0.160.013.310.0215.740.10351536214527DSA 0.220.024.470.1220.130.58351336394539DM 0.080.020.080.020.080.02332334553605MTT 0.360.230.400.20OOM27118049OOMDAM 0.090.010.080.010.160.04345235613724ATOM (Ours)0.080.020.080.020.130.03315232634151ATOM (Ours)0.100.020.100.010.170.02360143145134 . Comparisons of training time and GPU memory usagefor prior dataset distillation methods. Run time is averaged perstep over 100 iterations, while GPU memory usage is reportedas peak memory during the same 100 iterations of training on anA100 GPU for CIFAR-10. Methods that surpass the GPU memorythreshold and fail to run are denoted as OOM (out-of-memory).ATOM represents our method with on-channel attention, henceoffering a better tradeoff in computational complexity. as DC, DSA, and MTT introduce additional computationaloverhead due to bi-level optimization and training an ex-pert model. In contrast, our method, akin to DM and DAM,capitalizes on randomly initialized networks, obviating theneed for training and thereby reducing the computationalcost per step involved in the matching stage. As illustratedin utilizing solely the channel-based ATOM de-creases the computational burden of matching compared tothe default ATOM configuration. This efficiency is crucial,as channel-wise attention offers a more effective distillationprocess while maintaining superior performance (refer to.3).Convergence Speed Analysis. In , we plot the",
  ". Test accuracy evolution of synthetic image learning onCIFAR10 with IPC50 for ATOM (ours), DM and DataDAM": "downstream testing accuracy evolution for the synthetic im-ages on CIFAR10 IPC50. Comparing with previous meth-ods, DM and DataDAM , we can explicitly seean improvement in convergence speed and a significantlyhigher steady state achieved with the ATOM framework.Our included convergence analysis supports the practicalityof our method and the consistency to which we outperformprevious baselines.",
  ". Ablation Studies and Analysis": "Evaluation of loss components in ATOM. In , weevaluate the effect of different attention-matching mecha-nisms with respect to pure feature matching in interme-diate layers and distribution matching in the final layer(LMMD).The results clearly demonstrate that attention-matching improves the performance of the distillation pro-cess. In particular, the attention-matching process improvesfeature matching by 8.0%. Further, it seems that channelattention is able to capture the majority of relevant infor-mation from the intermediate features, as evidenced by animprovement of over 1.5% from spatial attention matching.Ultimately, this provides an incentive to favor channel at-tention in the distillation process.",
  ". Evaluation of loss components and attention componentsin ATOM using CIFAR-10 with IPC50": "Evaluating attention balance in ATOM. In this sec-tion, we evaluate the balance between spatial and channel-wise attention through the power value p.ReferencingEquation (1) and Equation (2), modulating the values of psand pc ultimately affects the balance of spatial and channel-wise attention in LATOM. In , we examine the im- pact of different exponentiation powers p in the attention-matching mechanisms.Specifically, we conduct a grid-based search to investigate how varying the exponentiationof spatial (ps) and channel (pc) attention influences subse-quent performance. Our findings reveal that optimal per-formance (nearly 1% improvement over our default) oc-curs when the exponentiation for channel attention signif-icantly exceeds that of spatial attention. This suggests thatassigning a higher exponential value places greater empha-sis on channel-attention matching over spatial-wise match-ing. This aligns with our observations from the loss compo-nent ablation, where channel-wise matching was found toencapsulate the majority of information within the featuremap. Consequently, we deduce that prioritizing channel-wise matching will enhance downstream performance out-comes.",
  ". Evaluation of power values in the spatial and channelattention computations for LATOM using CIFAR-10 with IPC10": "Visualization of Synthetic Images. We include samplesof our distilled images in . The images appear tobe interleaved with artifacts that assimilate the backgroundand object information into a mixed collage-like appear-ance. The synthetic images effectively capture the corre-lation between background and object elements, suggestingtheir potential for generalizability across various architec-tures, as empirically verified in . Additional visual-izations are available in the supplementary material.",
  ". Applications": "Neural Architecture Search. In Table we lever-age our distilled synthetic datasets as proxy sets to accel-erate Neural Architecture Search.In line with previousstate-of-the-art, , we outline our architecturalsearch space, comprising 720 ConvNets on the CIFAR-10 dataset. We commence with a foundational ConvNetand devise a consistent grid, varying in depth D {1, 2,3, 4}, width W {32, 64, 128, 256}, activation func-tion A {Sigmoid, ReLU, LeakyReLU}, normalizationtechnique N {None, BatchNorm, LayerNorm, Instan-ceNorm, GroupNorm}, and pooling operation P {None,MaxPooling, AvgPooling}.Additionally, we benchmarkour approach against several state-of-the-art methods, in-cluding Random, DSA , DM , CAFE , DAM, and Early-Stopping.Our method demonstrates su-perior performance, accompanied by a heightened Spear-mans correlation (0.75), thereby reinforcing the robustness",
  ". Limitations": "Many studies in dataset distillation encounter a constraintknown as re-distillation costs . This limitationbecomes apparent when adjusting the number of images perclass (IPC) or the distillation ratios. Like most other dis-tillation methods, our approach requires re-distillation onthe updated setting configuration, which limits flexibility re-garding configuration changes and storage allocation. Ad-ditionally, we observed in that dataset distillationmethods often struggle with generalizing to transformer ar-chitectures. Despite ATOM outperforming other methods,there is still a noticeable performance drop compared toconvolutional neural networks. This suggests that the ef-fectiveness of transformers for downstream training mightbe constrained by the distilled data.",
  ". Conclusion": "In this work, we introduced an Attention Mixer (ATOM)for efficient dataset distillation. Previous approaches havestruggled with marginal performance gains, obfuscatingchannel-wise information, and high computational over-heads. ATOM addresses these issues by effectively combin-ing information from different attention mechanisms, facili-tating a more informative distillation process with untrainedneural networks. Our approach utilizes a broader receptivefield to capture spatial information while preserving dis-tinct content information at the channel level, thus better aligning synthetic and real datasets. By capturing informa-tion across intermediate layers, ATOM facilitates multi-scaledistillation. We demonstrated the superior performance ofATOM on standard distillation benchmarks and its favorableperformance across multiple architectures. We conductedseveral ablative studies to justify the design choices behindATOM. Furthermore, we applied our distilled data to Neu-ral Architecture Search, showing a superior correlation withthe real large-scale dataset. In the future, we aim to extendattention mixing to various downstream tasks, including im-age segmentation and localizations. We also hope to ad-dress limitations of ATOM, such as re-distillation costs andcross-architecture generalizations on transformers. Sharat Agarwal, Himanshu Arora, Saket Anand, and ChetanArora. Contextual diversity for active learning. In ComputerVisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part XVI 16, pages137153. Springer, 2020. 2 Hossam Amer, Ahmed H Salamah, Ahmad Sajedi, and En-hui Yang. High performance convolution using sparsity andpatterns for inference in deep convolutional neural networks.arXiv preprint arXiv:2104.08314, 2021. 1",
  "mental learning. In Proceedings of the European conferenceon computer vision (ECCV), pages 233248, 2018. 2, 5": "George Cazenavette, Tongzhou Wang, Antonio Torralba,Alexei A Efros, and Jun-Yan Zhu.Dataset distillationby matching training trajectories.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 47504759, 2022. 2, 3, 5, 6, 1 George Cazenavette, Tongzhou Wang, Antonio Torralba,Alexei A Efros, and Jun-Yan Zhu.Generalizing datasetdistillation via deep generative prior.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 37393748, 2023. 6, 1 Xuxi Chen, Yu Yang, Zhangyang Wang, and Baharan Mirza-soleiman. Data distillation can be like vodka: Distilling moretimes for better quality. In The Twelfth International Confer-ence on Learning Representations, 2024. 2",
  "Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scalingup dataset distillation to imagenet-1k with constant memory.In International Conference on Machine Learning, pages65656590. PMLR, 2023. 3": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 1 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In International Con-ference on Learning Representations, 2021. 5 Jiawei Du, Yidi Jiang, Vincent YF Tan, Joey Tianyi Zhou,and Haizhou Li.Minimizing the accumulated trajectoryerror to improve dataset distillation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 37493758, 2023. 2, 3",
  "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bern-hard Scholkopf, and Alexander Smola. A kernel two-sampletest. The Journal of Machine Learning Research, 13(1):723773, 2012. 4": "Jianyang Gu, Kai Wang, Wei Jiang, and Yang You. Sum-marizing stream data for memory-restricted online continuallearning. In Proceedings of the AAAI Conference on Artifi-cial Intelligence (AAAI), 2024. 2 Ziyao Guo, Kai Wang, George Cazenavette, HUI LI,Kaipeng Zhang, and Yang You. Towards lossless dataset dis-tillation via difficulty-aligned trajectory matching. In TheTwelfth International Conference on Learning Representa-tions, 2024. 2, 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Delving deep into rectifiers: Surpassing human-level perfor-mance on imagenet classification.In Proceedings of theIEEE international conference on computer vision, pages10261034, 2015. 3, 5 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 5, 6",
  "Samir Khaki and Konstantinos N Plataniotis. The need forspeed: Pruning transformers with one recipe. arXiv preprintarXiv:2403.17921, 2024. 1": "Krishnateja Killamsetty, Sivasubramanian Durga, GaneshRamakrishnan, Abir De, and Rishabh Iyer.Grad-match:Gradient matching based data subset selection for efficientdeep model training. In International Conference on Ma-chine Learning, pages 54645474. PMLR, 2021. 2 Krishnateja Killamsetty, Durga Sivasubramanian, GaneshRamakrishnan, and Rishabh Iyer.Glister: Generalizationbased data subset selection for efficient and robust learning.In Proceedings of the AAAI Conference on Artificial Intelli-gence, pages 81108118, 2021. 2 Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, andRishabh Iyer. Retrieve: Coreset selection for efficient androbust semi-supervised learning. Advances in Neural Infor-mation Processing Systems, 34:1448814501, 2021. 2 Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, SangdooYun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, andHyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In International Conference on Ma-chine Learning, pages 1110211118. PMLR, 2022. 3",
  "Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Datasetmeta-learning from kernel-ridge regression. In InternationalConference on Learning Representations, 2021. 5": "Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziu-gaite. Deep learning on a data diet: Finding important ex-amples early in training. Advances in Neural InformationProcessing Systems, 34:2059620607, 2021. 2 Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, GeorgSperl, and Christoph H Lampert. icarl: Incremental classifierand representation learning. In Proceedings of the IEEE con-ference on Computer Vision and Pattern Recognition, pages20012010, 2017. 2, 5 Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat-suya Harada. Maximum classifier discrepancy for unsuper-vised domain adaptation. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages37233732, 2018. 4",
  "Ahmad Sajedi and Konstantinos N Plataniotis. On the ef-ficiency of subclass knowledge distillation in classificationtasks. arXiv preprint arXiv:2109.05587, 2021. 1": "Ahmad Sajedi, Yuri A Lawryshyn, and Konstantinos N Pla-taniotis. Subclass knowledge distillation with known sub-class labels. In 2022 IEEE 14th Image, Video, and Multidi-mensional Signal Processing Workshop (IVMSP), pages 15.IEEE, 2022. 1 Ahmad Sajedi, Samir Khaki, Ehsan Amjadian, Lucy ZLiu, Yuri A Lawryshyn, and Konstantinos N Plataniotis.Datadam: Efficient dataset distillation with attention match-ing. In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision, pages 1709717107, 2023. 1, 2,3, 4, 5, 6, 7",
  "Ahmad Sajedi, Samir Khaki, Konstantinos N. Plataniotis,and Mahdi S. Hosseini. End-to-end supervised multilabelcontrastive learning, 2023": "Ahmad Sajedi, Yuri A Lawryshyn, and Konstantinos N Pla-taniotis. A new probabilistic distance metric with applicationin gaussian mixture reduction. In ICASSP 2023-2023 IEEEInternational Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 15. IEEE, 2023. Ahmad Sajedi, Samir Khaki, Yuri A Lawryshyn, and Kon-stantinos N Plataniotis. Probmcl: Simple probabilistic con-trastive learning for multi-label visual classification. arXivpreprint arXiv:2401.01448, 2024. 1",
  "Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. arXivpreprint arXiv:1409.1556, 2014. 5": "Samarth Sinha, Han Zhang, Anirudh Goyal, Yoshua Bengio,Hugo Larochelle, and Augustus Odena. Small-gan: Speed-ing up gan training using core-sets. In International Confer-ence on Machine Learning, pages 90059015. PMLR, 2020.2 Felipe Petroski Such, Aditya Rawal, Joel Lehman, KennethStanley, and Jeffrey Clune. Generative teaching networks:Accelerating neural architecture search by learning to gener-ate synthetic training data. In International Conference onMachine Learning, pages 92069216. PMLR, 2020. 2 Mariya Toneva, Alessandro Sordoni, Remi Tachet desCombes, Adam Trischler, Yoshua Bengio, and Geoffrey JGordon. An empirical study of example forgetting duringdeep neural network learning. In International Conferenceon Learning Representations, 2018. 2, 6 Mariya Toneva, Alessandro Sordoni, Remi Tachet desCombes, Adam Trischler, Yoshua Bengio, and Geoffrey JGordon. An empirical study of example forgetting duringdeep neural network learning. In International Conferenceon Learning Representations, 2019. 2, 5 Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, andYang You.Cafe: Learning to condense dataset by align-ing features. In Proceedings of the IEEE/CVF Conference",
  "Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In SoKweon. Cbam: Convolutional block attention module. InProceedings of the European conference on computer vision(ECCV), pages 319, 2018. 3": "Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, andJian Cheng.Quantized convolutional neural networks formobile devices. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 48204828,2016. 1 Yuanhao Xiong, Ruochen Wang, Minhao Cheng, Felix Yu,and Cho-Jui Hsieh. Feddm: Iterative distribution matchingfor communication-efficient federated learning. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1632316332, 2023. 2 Enneng Yang, Li Shen, Zhenyi Wang, Tongliang Liu, andGuibing Guo. An efficient dataset condensation plugin andits application to continual learning. Advances in Neural In-formation Processing Systems, 36, 2024. 2 Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao.On compressing deep models by low rank and sparse decom-position. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 73707379, 2017. 1 Sergey Zagoruyko and Nikos Komodakis. Paying more at-tention to attention: Improving the performance of convolu-tional neural networks via attention transfer. arXiv preprintarXiv:1612.03928, 2016. 3, 4 Hansong Zhang, Shikun Li, Pengju Wang, and ShimingZeng, Dan Ge. M3D: Dataset condensation by minimizingmaximum mean discrepancy. In Proceedings of the AAAIConference on Artificial Intelligence (AAAI), 2024. 3, 4",
  "Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Datasetcondensation with gradient matching. In International Con-ference on Learning Representations, 2021. 1, 2, 3, 5, 6,7": "Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Im-proved distribution matching for dataset condensation.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 78567865, 2023. 2, 3,4 Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng,Dongze Lian, Yifan Zhang, Yang You, and Jiashi Feng.Dataset quantization. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1720517216, 2023. 1",
  ". Datasets": "We conducted experiments on three main datasets: CI-FAR10/100 and TinyImageNet . These datasetsare considered single-label multi-class; hence, each imagehas exactly one class label. The CIFAR10/100 are conven-tional computer vision benchmarking datasets comprising3232 colored natural images. They consist of 10 coarse-grained labels (CIFAR10) and 100 fine-grained labels (CI-FAR100), each with 50,000 training samples and 10,000test samples.The CIFAR10 classes include Airplane,Car, Bird, Cat, Deer, Dog, Frog, Horse,Ship, and Truck. The TinyImageNet dataset, a subset ofImageNet-1K with 200 classes, contains 100,000 high-resolution training images and 10,000 test images resizedto 6464. The experiments on these datasets make up thebenchmarking for many previous dataset distillation works.",
  ". Dataset Pre-processing": "We applied the standardized preprocessing techniques to alldatasets, following the guidelines provided in DM andDataDAM . Following previous works, we apply thedefault Differentiable Siamese Augmentation (DSA) scheme during distillation and evaluation. Specifically forthe CIFAR10/100 datasets, we integrated Kornia zero-phasecomponent analysis (ZCA) whitening, following the param-eters outlined in . Similar to DataDAM , weopted against ZCA for TinyImagenet due to the computa-tional bottlenecks associated with full-scale ZCA transfor-mation on a larger dataset with double the resolution. Notethat we visualized the distilled images by directly applyingthe inverse transformation based on the corresponding datapre-processing, without any additional modifications.",
  ". Hyperparameters": "Our method conveniently introduces only one additional hy-perparameter: the power term in channel attention, i.e. pc.All the other hyperparameters used in our method are di-rectly inherited from the published work, DataDAM .Therefore, we include an updated hyperparameter table in aggregating our power term with the remaining pre-set hyperparameters. In the main paper, we discussed theeffect of power terms on both channel- and spatial-wise at-tention and ultimately found that higher channel attentionpaired with lower spatial attention works best. However,our default, as stated in the main draft, is pc = ps = 4.Regarding the distillation and train-val settings, we use the SGD optimizer with a learning rate of 1.0 for learning thesynthetic images and a learning rate of 0.01 for trainingneural network models (for downstream evaluation). ForCIFAR10/100 (low-resolution), we use a 3-layer ConvNet;meanwhile, for TinyImagenet (medium-resolution), we usea 4-layer ConvNet, following previous works in the field. Our batch size for learning the synthetic imageswas set to 128 due to the computational overhead of a largermatching set.",
  ". Neural Architecture Search Details": "Following previous works , we define a searchspace consisting of 720 ConvNets on the CIFAR10 dataset.Models are evaluated on CIFAR10 using our IPC 50 dis-tilled set as a proxy under the neural architecture search(NAS) framework. The architecture search space is con-structed as a uniform grid that varies in depth D {1,2, 3, 4}, width W {32, 64, 128, 256}, activationfunction A {Sigmoid, ReLu, LeakyReLu}, normaliza-tion technique N {None, BatchNorm, LayerNorm, In-stanceNorm, GroupNorm}, and pooling operation P{None, MaxPooling, AvgPooling} to create varying ver-sions of the standard ConvNet. These candidate architec-tures are then evaluated based on their validation perfor-mance and ranked accordingly. In the main paper, measures various costs and performance metrics associatedwith each distillation method. Overall distillation improvesthe computational cost; however, ATOM achieves the high-est correlation, which is by far the most important metricin this NAS search, as it indicates that our proxy set bestestimates the original dataset."
}