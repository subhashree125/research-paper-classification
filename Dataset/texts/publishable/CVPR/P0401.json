{
  "University of Illinois Urbana-Champaign, IL, USA{samyakr2,sb56,n-ahuja}@illinois.edu2 Vizzhy, USA": "Abstract. Multi-label Recognition (MLR) involves the identificationof multiple objects within an image. To address the additional com-plexity of this problem, recent works have leveraged information fromvision-language models (VLMs) trained on large text-image datasets forthe task. These methods learn an independent classifier for each object(class), overlooking correlations in their occurrences. Such co-occurrencescan be captured from the training data as conditional probabilities be-tween a pair of classes. We propose a framework to extend the indepen-dent classifiers by incorporating the co-occurrence information for objectpairs to improve the performance of independent classifiers. We use aGraph Convolutional Network (GCN) to enforce the conditional prob-abilities between classes, by refining the initial estimates derived fromimage and text sources obtained using VLMs. We validate our methodon four MLR datasets, where our approach outperforms all state-of-the-art methods.",
  "Introduction": "Multi-label recognition (MLR) involves identifying each of the multiple classesfrom which objects are present in an image. It has many applications such asidentifying all different: diseases evident in a chest x-ray , products in a queryimage for e-commerce , and food items in a plate for diet monitoring systems. MLR is more challenging than classifying images having a single object because an image may contain combinatorially large mix of classes, forwhich learning would require exponentially larger number of images (O(2N) im-ages for N classes) than for Single label recognition (SLR). The objects may alsooccur in different layouts so recognition either requires object segmentation andrecognition of each segmented object independently, or recognizing the objectmix from the features measured over the entire image.",
  "Rawlekar et al": "46. Xu, M., Zhang, Z., Wei, F., Lin, Y., Cao, Y., Hu, H., Bai, X.: A simple baseline foropen-vocabulary semantic segmentation with pre-trained vision-language model.In: European Conference on Computer Vision. pp. 736753. Springer (2022) 47. Yang, J., Price, B., Cohen, S., Yang, M.H.: Context driven scene parsing withattention to rare classes. In: Proceedings of the IEEE conference on computervision and pattern recognition. pp. 32943301 (2014)",
  "Improving MLR using Class Co-Occurrence Probabilities3": "improve upon the outputs of the independently obtained class estimates usingthe vision-language model. As commonly done, we compensate for the differ-ences in the frequencies of different classes occurring in the training images byreweighing the estimated probabilities of different classes to remove the classbias in the training data.We test our approach on four benchmarks: MS-COCO-small (5% of the train-ing set), PASCAL VOC, FoodSeg103, and UNIMIB-2016. The first two are com-monly used for MLR, whereas we have two additional datasets that have beenused in other contexts . The number of images in each of these datasetsis small compared to even many SLR datasets, although MLR calls for largerdatasets. This makes MLR here even harder. Our experiments show that theuse of inter-class influence in the second stage of our framework significantlyimproves performance over the state-of-the-art methods, which detect each classindependently. As expected, the advantage is higher for classes for which VLMyields low accuracy but which frequently co-occur with other classes for whichVLM accuracy is higher. We also show that our loss re-weighing greatly improvesthe performance on datasets where there is a significant class bias.Our contributions: We propose a two-stage framework to adapt VLMs for MLR with limitedannotated data, by enhancing the VLM-based independent class estimatesobtained in the first stage, with conditional probability priors extracted fromthe dataset, using a GCN. We validate our algorithm quantitatively using mean average precision (mAP)on four MLR datasets. Our method surpasses the previous SOTA MLRapproaches by more than 2% (COCO-14-small), 0.4% (VOC-2007), 3.9%(FoodSeg103) and 11% (UNIMIB2016).",
  "Multi-Label Recognition": "Multi-label recognition is an important, well-studied problem in computer vision,with a wide range of approaches being proposed to tackle it . An importantline of work has focused on learning binary classifiers for identifying each class ofobjects in an image . These approaches require large labeled datasets fortraining. They make no use of information that can be derived from modelingthe co-occurrence of different classes, which is especially important when only asmall amount of annotated data is available for training.Other works attempt to model label dependencies in an image. mod-els label dependencies implicitly by using a deep embedding network sharedby classifiers for all different classes of objects. Other works have proposed touse recurrent neural networks (RNNs) to model label dependencies in an image. Specifically, they cast MLR into a sequence prediction problem, us-ing beam search to find a sequence of objects having the highest likelihood ofbeing present in the image. Like our method, these methods also model label",
  "Vision-Language Models for MLR": "VLMs learn representations that are transferable to a wide range of downstreamtasks such as recognition , retrieval , and segmentationby aligning hundreds of millions of image-text(prompts) pairs. Such approachescommonly focus on learning prompts suitable for these downstream tasks . adapts VLMs for MLR, proposing to learn only a pair of prompts associatedwith the presence/ absence of each class while keeping the VLM itself frozen. Textembeddings extracted from the learned prompts are used to gather local evidencefrom the image features extracted by the VLM, which is then aggregated andcombined. SCPNet is another VLM based approach for MLR, differing from in its use of a GCN to help learn text embeddings for the presence/absenceprompts. The GCN models priors derived from class name similarities in CLIPsembedding space, enforcing these priors during prompt learning. also aug-ments training with a self supervised contrastive loss. Both these methods learnindependent prompt-based classifiers (as VLM is frozen), not making any useof co-occurrence information during inference. Our method instead learns inter-dependent classifiers by using a GCN to enhance predictions made by the VLM.The inter-dependent classifiers model and make use of conditional probabilitiesduring both training and inference and use co-occurrence information from theactual training dataset.",
  "Long tailed Learning": "The distribution of frequency with which objects belong to a class in real-worldimages often follows a long-tailed distribution. Networks trained formulti-class classification on such data tend to perform poorly on the tail classeswhich have less data available. Several approaches have been proposed to miti-gate this issue including data augmentation (augment the tail classes) , datare-sampling (sample images to obtain balanced distribution) , adjust-ing classifier margins (classification thresholds vary for every class) and lossre-weighing being popular. We use loss re-weighing to mitigate the effectof label imbalance on our method when it is trained to model label conditionalprobabilities.",
  "Text Encoder": "(Frozen) 23... N-1 N 12... N-1 N 1 2. . .N-1 N 123... N-1 N ... N-1 N : Method Overview: Given an image with multiple objects, we extract im-age features and text features from the subimages using a vision-language model(CLIP). An image-text feature aggregation module (Sec. 3.1) combines these fea-tures to identify all classes present in the image as a union of the classes presentin the subimages, giving an initial set of image level class logits. These logits arepassed to a GCN, that uses conditional probabilities between classes to refinethese initial predictions (Sec. 3.2). We train this framework while reweightingthe loss generated by classes to address any class imbalance in the training datausing a Reweighted Asymmetric Loss (RASL), a weighted version of ASL.",
  "Method": "Suppose in a given set of images, D = {xi}, i {1 . . . |D|}, every image xi maycontain objects from up to N classes. The image is thus associated with N labelsyi {0, 1}N where yji denotes the presence or absence (1 or 0) of the j-th class inthe image. Then the MLR problem requires identification of all labels associatedwith any input image.Our approach in this paper uses a VLM g, parameterized using weights (apair of encoders g,img, g,text). VLMs are pretrained to align image and textualfeatures over large datasets to learn features suited for various tasks/domains.As mentioned in Sec. 1, these models associate a pair of positive and negativetext prompts {tj,+, tj,} with each class j (complete set denoted by ). A textencoder g,text extracts text embeddings from each of these prompts, gives themto an image-text feature aggregation head p, which matches visual features ex-tracted from different subimages with the text embeddings, and combines themto obtain an initial set of logits for each class in the image. We then use a GCNf with weights to refine the logits output by p, by leveraging the statisticalco-occurrence of classes observed in the training dataset. An overview of ourproposed method is given in .The following subsections present the various parts of our method.",
  "Initial Logits Estimation": "We use a VLM as a feature extractor and initial classifier for our method. TheVLMs image encoder does spatial pooling of windows in the final layer to obtaina single d dimensional feature vector for a single-label image xi. This is notsuitable for MLR case as spatial pooling operation combines the features ofmultiple objects in different regions of the image, with overall features beingdominated by those extracted from a single object. We remove the pooling layerof the image encoder, using it to get features g,img(xi) = zi (of shape dHW)for a given image xi, hence preserving information from the individual windows.Our image-text feature aggregation head p is similar to . For each class j,it learns a pair of text prompts {tj,+, tj,}, which are projected to d-dimensionalembeddings rj,+, rj, using g,text. Cosine similarity of the d-dimensional imagefeatures at a particular point (h, w) with rj,+ indicates the presence of the class,while similarity with rj, indicates its absence. These similarities are aggregatedand used by p to give logits p(zi) for the image.",
  "Improving MLR using Class Co-Occurrence Probabilities7": "using information from only those nodes used for computing the logits, reducingthe number of parameters learned while also taking advantage of the conditionalprobability estimates. After passing through multiple layers of the GCN, we getthe updated predictions f(p(zi)). We add the initial logits to the updatedlogits to obtain our refined logits prediction p(zi) + f(p(zi))",
  "Training": "We train the image-text feature aggregation module p and the GCN f, whilefreezing the VLM g. We adopt the widely used Asymmetric Loss (ASL) , amodified version of the focal loss, to train our network for MLR.ASL addresses the inherent imbalance in MLR caused by the prevalenceof negative examples compared to positive ones in training images. Similar to fo-cal loss , ASL underweighs the loss term due to negative examples. However,it does so using two focusing parameters (+ and ) instead of one () used byfocal loss. However, ASL does not address the issue of sample imbalance, causedby some classes having fewer examples in the dataset. Towards this, we add aloss re-weighting term () to ASL. Our re-weighed ASL (RASL) is defined as:",
  "Ours66.8 65.8 64.2 80.9 86.1 83.4 72.2": ": Comparison of results obtained by our method and the state-of-the-artbaselines, on four MLR datasets in the low data regime: FoodSeg103, UNIMIB2016, COCO-small (5% of COCOs training data) and VOC-2007. Our approachachieves the best performance on all metrics: per-class and overall average preci-sions (CP and OP), recalls (CR and OR), F1 scores (CF1 and OF1), and meanaverage precision (mAP). * indicates methods that fine-tune the complete back-bone network. as FoodSeg103 and UNIMIB 2016, which are smaller MLR datasets suitablefor testing in the low data regime. Details of these datasets are given below:MS-COCO 2014-small: MS-COCO is another popular MLR dataset andconsists of 82,081 training images and 40,504 validation images with objectsbelonging to 80 classes. To evaluate our methods performance in the low dataregime, we use MS-COCO 2014-small, which is a small, randomly selected sub-set comprising 5% of MS-COCO 2014 which amounts to 4014 images. Duringtesting, we use the complete validation set.PASCAL VOC 2007: VOC is a widely used outdoor scene MLR datasetconsisting of 9,963 images from 20 classes. We follow the standard trainval setfor training and use the test set for testing.FoodSeg103: FoodSeg103 serves as a benchmark dataset for food segmen-tation and multi-label food recognition. It consists of 4983 training images and2135 test images, with a total of 32,097 food instances belonging to 103 differ-ent food classes. The number of images per class follows a long-tail distributiontypical of real-world datasets. We use the standard train-test data split.UNIMIB 2016: UNIMIB is another multi-label food recognition dataset.It consists of 1027 images with 3616 food instances spanning 73 classes. Similar",
  "Implementation Details": "In our experiments, we use CLIP (Contrastive Language-Image Pre-Training) as the VLM. Consistent with recent works that use VLMs for MLR ,we select ResNet-101 as the visual encoder and standard transformer withinCLIP as the text encoder. Both encoders are kept frozen during our experiments,and we train the GCN and learnable prompts. Following , we resize theimages to 448 448 for COCO and VOC datasets and to 224 224 for UNIMIBand FoodSeg103. Similar to previous works we apply Cutout andRandAugment to augment training images. We use a 3-layer GCN networkfor all our experiments. We use SGD for optimizing parameters with an initiallearning rate of 0.002, which is reduced by cosine annealing. We train for 50epochs and use a batch size of 32. We set the loss hyperparameters in Eq. 4 as = 3, + = 1 and = 0.05 . We conduct all experiments on a single RTXA4000 GPU.",
  "Evaluation Metrics": "To evaluate the performance of our approach on the four MLR datasets, we usestandard metrics, also used by previous MLR approaches . The metricsinclude the commonly used mean average precision (mAP) as well as class andoverall precisions (CP and OP), recalls (CR and OR), and F1 scores (CF1 andOF1). mAP is obtained by calculating the mean of individual average precision(AP) values over all classes. For each class, AP is computed as the area underthe Precision-Recall curve.",
  "Results": "We primarily compare our approach with DualCoOp and SCPNet , asthey are the only other MLR baselines that use VLMs, making them SOTAmethods in limited data settings across all four standard benchmarks discussedearlier. As seen in , our method outperforms DualCoOp by 0.4% andSCPNet by 7.0% mAP on the VOC-2007. On COCO-small, our method out-performs DualCoOp by 2.4% and SCPNet by 3.3% mAP. On the FoodSeg103dataset, our approach significantly improves upon DualCoOp by 3.9% and SCP-Net by 4.1% mAP. In the UNIMIB dataset, our method achieves substantialperformance gains of 14.1% over DualCoOp and 12.2% mAP over SCPNet.Furthermore, for VOC, we extend our comparison to approaches that do notuse VLMs and instead rely on complete fine-tuning. These approachesalso use a ResNet-101 backbone similar to our visual encoder, but the backboneis initialized with weights pre-trained on ImageNet instead . Our method alsooutperforms these methods. More detailed comparison can be found in Table. 1.",
  "Impact of the Strength of Conditional Probability onPerformance": "In this section, we determine the impact of the strength of conditional probabilityof a pair of classes on MLR performance on the COCO-small dataset. Specifically,we observe how the improvement in average precision of a class of objects (AP)brought by our method varies with the average conditional probability of theclass paired with the top three other classes it co-occurs the most with. Notethat we choose to average the top three values of conditional probabilities of theclass because the COCO dataset typically contains an average of around threeobjects per image. The improvement in (AP) for a class = AP achieved bylogits after refinement - AP achieved by the raw VLM logits before refinement.We visualize the variation in AP with the avg. conditional probability in, where we observe an increasing trend of AP with increase in theaverage of the top-3 conditional probabilities for a given class. Note that forease of visualization of the scatterplot, we use bins of size 0.02 to group togetherclasses having similar average conditional probabilities. The points represent theaverage AP value of all classes within the respective bin.This implies that classes having stronger conditional probabilities with otherclasses benefit more from our approach of refining logits using conditional prob-abilities, as is intuitively expected.",
  "Performance on Classes that are Difficult to Recognize": "In this section, we empirically explore the impact of our approach on classes thatare difficult to recognize when using image features exclusively. For concreteness,we focus our analysis on the 10 classes in FoodSeg103 and UNIMIB datasets onwhich the previous state-of-the-art approach (DualCoop) performs (in termsof CF1) the worst. compares the performance of our method on these classes with theprevious SOTA DualCoOp and SCPNet. We see that our method sig-nificantly improves the performance of these methods, which relies solely onVLMs without modeling any conditional probabilities. Specifically, for Dual-CoOp, we observe a growth of 22.3% in CP, 33.9% in CR and 34.8% for CF1on UNIMIB2016, and 15.6% in CP, 7.2% in CR and 11.89% in CF on Food-Seg103. For SCPNet, we see gains of 27.1% CP, 25.2% CR, and 26.5% CF1 onUNIMIB2016, and 15.8% CP, 5.8% CR, and 12.4% CF1 on FoodSeg103.This underscores the importance of the information obtained by modelingjoint class probabilities in recognizing classes of objects that are difficult torecognize from image features alone.",
  "Ours57.660.059.128.726.928.4": ": A comparison of the average performance of our approach with theprevious state-of-the-art VLM-based method DualCoOp and SCPNet onclasses that are difficult to recognize using only visual features (having 10 lowestCF1 values on the FoodSeg103 and UNIMIB). Our approach significantlyimproves MLR performance on such classes due to its use of information derivedfrom class conditional probabilities.",
  "Effect of Loss Reweighing": "In this subsection, we investigate the effects of our loss reweighing strategy forUNIMIB2016 and FoodSeg103, which exhibit significant class imbalance. Weanalyze its impact on performance on (1) All classes as a whole and (2) Classesthat are difficult to recognize using only visual features, and hence have a greaterreliance on information obtained from conditional probabilities.(1) All classes as a whole: As observed in , loss re-weighing im-proves the performance of our method by 1.6% and 7.8% in mAP on FoodSeg103and UNIMIB2016, respectively.",
  "Conclusion": "In this paper, we present a novel two-stage framework for multi-label recogni-tion when only a small number of annotated images are available. Our frameworkbuilds on recent methods that make use of VLMs to counter this paucity of la-beled data but overlook information derived from co-occurence of object pairs.Our framework refines the logit predictions made by VLMs adapted for multi-label recognition by leveraging known conditional probabilities of class pairsderived from the training data distribution. Specifically, we use a graph convolu-tional network to enrich the logits predicted by the VLM with information fromconditional probabilities of classes. Our method outperforms all state-of-the-artapproaches on 4 MLR benchmarks: COCO-14-small, VOC 2007, FoodSeg103and UNIMIB2016 in a low data regime, demonstrating the utility of modelingclass co-occurrence in such cases.",
  "Limitations": "(1) If the independent classifiers learned by state-of-the-art approaches (relyingon only visual information, not modeling the conditional probability of classpairs) are strong, and characterized by a high average precision (AP) of eachclass, our method would yield lower improvements. However, in practice, manyMLR datasets are not very large, with independent classifiers learned from thembeing relatively weak. MLR on such datasets is likely to benefit significantlyfrom our method.(2) As shown in Figure. 2, the advantage provided by our method is higher whenthe conditional probability of pairs of classes co-occurring in an image is higher.For images that consist of objects which are rarely found together, our methodprovides very little added benefit over independent classifiers.",
  "National Institute of Food and Agriculture under grant 2020-67021-32799/1024178and Vizzhy.com are gratefully acknowledged": "1. Abdelfattah, R., Guo, Q., Li, X., Wang, X., Wang, S.: Cdul: Clip-driven un-supervised learning for multi-label image classification. In: Proceedings of theIEEE/CVF International Conference on Computer Vision. pp. 13481357 (2023) 2. Anthimopoulos, M.M., Gianola, L., Scarnato, L., Diem, P., Mougiakakou, S.G.: Afood recognition system for diabetic patients based on an optimized bag-of-featuresmodel. IEEE journal of biomedical and health informatics 18(4), 12611271 (2014)",
  ". Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: Smote: synthetic mi-nority over-sampling technique. Journal of artificial intelligence research 16, 321357 (2002)": "9. Chen, T., Lin, L., Chen, R., Hui, X., Wu, H.: Knowledge-guided multi-label few-shot learning for general image recognition. IEEE Transactions on Pattern Analysisand Machine Intelligence 44(3), 13711384 (2020) 10. Chen, T., Xu, M., Hui, X., Wu, H., Lin, L.: Learning semantic-specific graph rep-resentation for multi-label image recognition. In: Proceedings of the IEEE/CVFinternational conference on computer vision. pp. 522531 (2019) 11. Chen, Z.M., Wei, X.S., Wang, P., Guo, Y.: Multi-label image recognition withgraph convolutional networks. In: Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition. pp. 51775186 (2019)",
  ". Ciocca, G., Napoletano, P., Schettini, R.: Food recognition: a new dataset, ex-periments, and results. IEEE journal of biomedical and health informatics 21(3),588598 (2016)": "13. Cole, E., Mac Aodha, O., Lorieul, T., Perona, P., Morris, D., Jojic, N.: Multi-labellearning from single positive labels. In: Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. pp. 933942 (2021) 14. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automateddata augmentation with a reduced search space. In: Proceedings of the IEEE/CVFconference on computer vision and pattern recognition workshops. pp. 702703(2020)",
  ". DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net-works with cutout. arXiv preprint arXiv:1708.04552 (2017)": "19. Ding, Z., Wang, A., Chen, H., Zhang, Q., Liu, P., Bao, Y., Yan, W., Han, J.:Exploring structured semantic prior for multi label recognition with incompletelabels. In: Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. pp. 33983407 (2023) 20. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image isworth 16x16 words: Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929 (2020)",
  ". Huang, H., Rawlekar, S., Chopra, S., Deniz, C.M.: Radiology reports improve visualrepresentations learned from radiographs. In: Medical Imaging with Deep Learning.pp. 13851405. PMLR (2024)": "25. Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave,A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., Schmidt, L.:Openclip (Jul 2021). if you use this software, please cite it as below. 26. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,Li, Z., Duerig, T.: Scaling up visual and vision-language representation learningwith noisy text supervision. In: International conference on machine learning. pp.49044916. PMLR (2021)",
  "Improving MLR using Class Co-Occurrence Probabilities15": "31. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P.,Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,2014, Proceedings, Part V 13. pp. 740755. Springer (2014) 32. Liu, F., Xiang, T., Hospedales, T.M., Yang, W., Sun, C.: Semantic regularisa-tion for recurrent image annotation. In: Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition. pp. 28722880 (2017)",
  ". Menon, A.K., Jayasumana, S., Rawat, A.S., Jain, H., Veit, A., Kumar, S.: Long-taillearning via logit adjustment. arXiv preprint arXiv:2007.07314 (2020)": "37. Meyers, A., Johnston, N., Rathod, V., Korattikara, A., Gorban, A., Silberman, N.,Guadarrama, S., Papandreou, G., Huang, J., Murphy, K.P.: Im2calories: towardsan automated mobile vision food diary. In: Proceedings of the IEEE internationalconference on computer vision. pp. 12331241 (2015) 38. Misra, I., Lawrence Zitnick, C., Mitchell, M., Girshick, R.: Seeing through thehuman reporting bias: Visual classifiers from noisy human-centric labels. In: Pro-ceedings of the IEEE conference on computer vision and pattern recognition. pp.29302939 (2016)",
  ". Park, S., Lim, J., Jeon, Y., Choi, J.Y.: Influence-balanced loss for imbalancedvisual classification. In: Proceedings of the IEEE/CVF international conference oncomputer vision. pp. 735744 (2021)": "40. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models fromnatural language supervision. In: International conference on machine learning. pp.87488763. PMLR (2021) 41. Ridnik, T., Ben-Baruch, E., Zamir, N., Noy, A., Friedman, I., Protter, M., Zelnik-Manor, L.: Asymmetric loss for multi-label classification. In: Proceedings of theIEEE/CVF International Conference on Computer Vision. pp. 8291 (2021)",
  ". Sun, X., Hu, P., Saenko, K.: Dualcoop: Fast adaptation to multi-label recognitionwith limited annotations. Advances in Neural Information Processing Systems 35,3056930582 (2022)": "43. Wang, J., Yang, Y., Mao, J., Huang, Z., Huang, C., Xu, W.: Cnn-rnn: A uni-fied framework for multi-label image classification. In: Proceedings of the IEEEconference on computer vision and pattern recognition. pp. 22852294 (2016) 44. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S., Roelofs, R., Lopes,R.G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.: Robust fine-tuning of zero-shot models. In: Proceedings of the IEEE/CVF conference on computer vision andpattern recognition. pp. 79597971 (2022) 45. Wu, X., Fu, X., Liu, Y., Lim, E.P., Hoi, S.C., Sun, Q.: A large-scale benchmarkfor food image segmentation. In: Proceedings of the 29th ACM international con-ference on multimedia. pp. 506515 (2021)"
}