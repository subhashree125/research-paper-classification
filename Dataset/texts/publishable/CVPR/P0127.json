{
  "Abstract": "We present a method for automatically modifying aNeRF representation based on a single observation of anon-rigid transformed version of the original scene. Ourmethod defines the transformation as a 3D flow, specificallyas a weighted linear blending of rigid transformations of 3Danchor points that are defined on the surface of the scene.In order to identify anchor points, we introduce a novel cor-respondence algorithm that first matches RGB-based pairs,then leverages multi-view information and 3D reprojectionto robustly filter false positives in two steps. We also intro-duce a new dataset for exploring the problem of modifyinga NeRF scene through a single observation. Our dataset1 contains 113 synthetic scenes leveraging 47 3D assets. Weshow that our proposed method outperforms NeRF editingmethods as well as diffusion-based methods, and we alsoexplore different methods for filtering correspondences.",
  ". Introduction": "Transforming a neural radiance field (NeRF) based on a sin-gle RGBD image is an important problem. Consider thefield of robotics as an example, where NeRFs are often usedto represent complicated 3D scenes . No-tably, whenever the scene is modified, the robot has to re-capture multiple views to re-train a new NeRF. This processdiscards important information from the original scene andis time consuming. We are hence interested in developingtools that allow a given NeRF scene to be transformed into anew scene observed via a single RGBD image (see ).Concretely, we are interested in retrieving the transformedscene geometry and rendering the new scene from differentperspectives.NeRF editing is a natural approach for solving this prob-lem, and current works have shown tremendous success atmodifying NeRF appearance or geometry from userinputs. However, most NeRF editing methods",
  "Output": "Exported mesh of the transformed scene . Problem definition. Given a NeRF of the original scene,and a single RGBD image of the transformed scene, we are in-terested in producing novel views and exporting a mesh of thistransformed scene. Here we visualize the NeRF (top left) and atransformation of the scene (bottom left). We then show how thescene is re-rendered given a new camera pose in the transformedscene (top right) and its scene mesh (bottom right). do not offer an automatic mechanism to match the trans-formed scene and thus require to manually define the trans-form (which can be non-trivial for non-rigid transforma-tions). In our problem setting, user input is not available.Other successful works have looked at NeRF transforma-tion through time , where the time component isdensely sampled.In contrast, we only assume a singleRGBD view of the transformed scene. Although this sin-gle observation alone (without access to the original NeRFscene) can be used to directly retrieve the transformed scenevia pretrained methods such as DreamGaussian , ourexperiments show that this approach struggles to recoverthe real geometry of the object. Transforming a NeRF using a single RGBD introduceschallenges: what is the non-rigid transformation being ob-served? what object parts correspond to each other? howdid the unseen part (not visible in the RGBD image) de-form? Inspired by mesh shape manipulation , we pro-",
  "arXiv:2406.10543v1 [cs.CV] 15 Jun 2024": "pose NeRFDeformer which addresses this problem by mod-eling the transformation as a 3D scene flow. Concretely,the flow is a weighted linear blending of rigid transforma-tions through 3D anchor points on the surface of the scene.This definition is more flexible than the MLP-based flowused by prior work as we can express an ap-proximate inverse flow. As the flow definition leveragesanchor points from the original scene to the transformedscene, we design a novel robust NeRF-based correspon-dence matching between the NeRF scene and the RGBDobservation. The method first fuses pixel correspondencesfrom the pixel matching approach ASpanFormer , thenapplies two steps of filtering in pixel and 3D space.We demonstrate efficacy of our method on a challeng-ing curated dataset. This dataset was specifically designedfor this problem:113 scenes are created from 47 dy-namic Objaverse assets . We also propose different base-lines for the problem of single-view NeRF transformation.More specifically we show that adding depth information toSINE is not enough to retrieve more complicated sceneswith non-rigid transformations. Our method achieves thebest results for both geometric reconstruction and novelview synthesis.Our contributions are summarized as follows: 1) We ex-plore how a 3D scene flow can be built from 3D corre-spondences to transform a given NeRF to a novel scene,for which there is only single RGBD image observation.2) We present a novel robust NeRF-based correspondencematching procedure between the original NeRF scene andthe transformed observation. 3) We introduce a comprehen-sive new dataset for evaluating this problem setting.",
  ". Related Work": "Neural editing and transformation. Many works have ad-dressed neural 3D scene editing and transformation. Scene-level editing works can changethe global appearance of a scene like the global palette,style or lighting . This differs from object-level edit-ing works which often learn decompositionsof the scene. They can add or remove objects, or apply arigid transformation. In general, these approaches focus ona single global rigid transformation and when present onlyadjust one global attribute.Some prior works consider geometric editing.Seal-3D defines the scene flow directly from a users 2Dedits, while others use a mesh as a proxyto define local coordinates for ray bending. Importantly, theformer work is only suitable for simple geometric edits likescaling or translation, while the latter works need laborioususer edits in the form of 3D vertex displacements. In con-trast, our method performs a non-rigid transformation givena single RGBD image and does not need laborious 3D edits.Some conditional generative approaches learn a distribution over NeRF parameters from a large 3Dasset dataset. Editing is then formulated as mapping a giventarget image to a NeRF parameter . Such formulationrestricts edits to the distribution of objects captured in thedataset, which is often not flexible enough to honor desireduser requests (which we demonstrate in the experiment sec-tion). SINE , the closest work to ours, achieves greatresults on the problem of geometric editing through a sin-gle observation. Different from our formulation, SINE represents flow via an MLP, which struggles to model accu-rate cyclic flows (forward and backward). They use Flow-Former to find 2D correspondences between the trans-formed view and a single original view captured from thesame camera pose as the transformed view. This approachlimits the number of high quality correspondences.Forthese reasons, SINE struggles with complicated non-rigidtransformations.Notably, plenty of dynamic NeRF approaches also ad-dress the problem of deforming NeRF scenes . These methods focus on deforming scenes throughtime, where the time component is well sampled. In con-trast, we assume multiple views for one point in time andone single transformed view at a second discrete trans-formed time. Dynamic NeRF approaches struggle to cap-ture the non-rigid transformation in such a setting becausethe amount of regularization is limited and correspondencescan be hard to obtain implicitly.Pixel correspondence matching.Optical flow methodslike RAFT or FlowFormer predict correspon-dences for all pixels from an image pair. However, bothare trained on image pairs with small camera movementin between which does not suit our setting. DINOv2 uses self-supervised learning to learn a per-pixel embeddingwhich can be used for correspondence matching. However,the matching is coarse and possibly less accurate to guideour 3D scene flow formulation. SuperGlue matcheskeypoints detected from SuperPoint , and LoFTR aswell as ASpanFormer match pixels using a downsam-pled image pair. We show that using such an approach iseffective when combined with proper filtering.Novel view synthesis from a single view.Early workson this topic conducted regression-based training on largedatasets, e.g., PixelNeRF .Motivated by diffusion-based generative models, recent works explore how pre-trained diffusion models can aid novel view synthesis givena single view. Specifically, prior approaches exploit text-conditioned diffusion models or image-conditioned ones . Differently, we develop anapproach tailored to NeRF non-rigid transformation and ob-tain superior results given a pretrained NeRF.3D scene flow representations.Prior works have stud-ied various representations for modeling 3D scene flows.As mentioned above, many dynamic NeRF and NeRF edit-",
  ".Overview of our method: we use two linked flows,F AB for transformed geometry reconstruction (bottom) andF BA for rendering the transformed scene (top)": "ing works apply an MLP-based flow, whichworks well when images are plenty. Notably, often a cyclicloss is required to connect two directions, which struggleswhen the transformations are complicated. Online non-rigidtracking methods explore linear blending of an-chor points as a flow design, but they do not apply their flowon NeRF-based new view synthesis. In the field of avatarmodeling, many works rely on domain-specific templates,e.g., SMPL , to model the 3D scene flow . Ourproblem differs since we work on adapting NeRFs for gen-eral scenes and do not assume that a domain-specific objecttemplate is available.",
  ". NeRFDeformer": "Consider an original scene A that has been transformed intoa scene B, see Figure . Our goal is two-fold: renderthe transformed scene B from novel viewpoints, and extractthe geometry M B of the transformed scene. To addressthese goals we assume the availability of 1) a pre-trainedNeRF that can be used to render the original scene Afrom arbitrary camera poses, and 2) a single RGBD im-age (IB, DB) that captures the transformed scene B from acamera pose CB SE(3).At its core, our method recovers both a forward F AB",
  "to obtain the corresponding points (pA). The direction dA": "for each point is computed from the transformed differencebetween neighboring points along the ray, to preserve localgeometry. These transformed points and directions are thenfed to the original NeRF-based rendering given in Eq. (3).Similarly, the mesh M A = (V, E) consisting of ver-tices V and triangle faces E is obtained from the NeRF of the original scene via the classic marching cubes algo-rithm . The transformed mesh M B is then obtained byapplying the forward flow to all the original vertices:",
  "M B = ({F AB(v) : v V}, E),(4)": "where we preserve topology by reusing the triangle faces E.Thus, it is apparent that the two 3D scene flows play anintegral role in the process of novel view rendering, as wellas in supporting recovering geometry of the transformedscene. As a result, the core of our method is aimed at recov-ering these scene flows. In the following we first detail the3D scene flow is defined with local linear transformationsand its trainable parameters (see Sec. 3.1). We then dis-cuss how to optimize the trainable parameters (see Sec. 3.2)which is based on 3D corresponding points. Finally we dis-cuss how corresponding points are extracted from the avail-able information (see Sec. 3.3).",
  "in the transformed space (B). The mapping is formulated": "as a weighted linear blending of rigid transformations i SE(3), which are anchored at distinct 3D points. In ourcase the anchor points are the vertices vi V of the trianglemesh M A = (V, E) extracted via marching cubes from theoriginal NeRF as illustrated in .Each vertex vi has an associated 6D rigid transform ithat contains a rotation Ri, a rotation origin vi, and a trans-lation ti; so that the rigid transformation and its inverse aregiven by",
  "V {k(vk),k = 1, . . . , |V |}.(10)": "Note from Eq. (5) that k(vk) = vk + tk.As mentioned in Sec. 1, our forward flow and backwardflow definition has two advantages over MLP-based flowswith a cyclic loss, used in prior work : 1) the backwardflow can be extracted from the forward flow without anytraining, and 2) forward and backward flows are cyclic onlynear the surface area where all linear transformations aresimilar. Thus there is no need to encourage them to be cyclicin empty space. In addition, our flow definition permits ad-ditional flexibility far from surface areas while encouragingcyclic behavior near surface areas, which is necessary foraccurate geometric reconstruction and novel view synthe-sis.",
  "LDG = LARAP + LCon(11)": "to learn the transformation components Ri and ti.Theas-rigid-as-possible (ARAP) loss LARAP regularizes bothtransformation components, while the consistency termLCon focuses on learning the translation terms through 3Dcorrespondences.The ARAP loss is applied on a decimated mesh for ef-ficient computation. In practice when the transformationis invoked, the parametric functions Ri and ti are com-puted via a weighted combination of learnable rotation ma-trices and translation vectors defined on the vertices of thedecimated mesh.The computation is differentiable andhence end-to-end trainable. The ARAP loss regularizes thesquared distances between each anchored vertex transfor-mation applied to its neighbors and the actual transformedneighbor position. We refer the reader to the supplementalmaterial for more details about this loss term.The consistency loss LCon constrains the translations ofthe vertices for which corresponding points exist. In or-der to ground the transformation, we first identify a set ofcorresponding points between scenes A and B. Let set Idenote the vertex indices for which correspondences ex-ist. Thus we have the following set of corresponding points{(vAi , vBi ) : i {1, . . . , |I|}}. The process of selectingthese points is described in the following section, with theconsistency loss defined as follows:",
  ". Robust NeRF-based Correspondence Matching": "We seek to produce reliable correspondences between theoriginal NeRF scene and our transformed scene which isillustrated in a single RGBD ( (a)). Inspired by thework of ASpanFormer , we first propose to find RGB-based correspondences between the transformed RGB com-ponents and original NeRF produced renders which are fil-tered first in pixel space. Finally we lift the pixel correspon-dences to 3D and filter the false positives in 3D space.",
  "...1": "(b) Rendering original images(a) Transformed view (c) Pixel-space filtering (d) 3D-space filtering . The transformed space image (a) is matched with theinput NeRF scene first via 2D dense matching between the trans-formed image and original images IA1 , ..., IAN rendered from theNeRF (b).Pixel-space filtering (c) is applied where we onlyshow selected matches (red and blue lines represent bad and goodmatches respectively). We show how any given pixel in IB can bematched to multiple views (see green, yellow, and red small cir-cles). Out of the multiple matches, we keep the one with largestcontinuous patch of matched pixels, e.g., in IA1 the green circlehas 2 matched neighbors whereas in IA2 there are 8. Thus we keepthe latter. The points are then unprojected into 3D (d) and keeppairs that are physically close in the original space while behavingsimilarly in the transformed space. 2D pair correspondences and filtering. A set of images isfirst rendered from the provided NeRF that fully covers thehemisphere defined around the object ( (b)). UsingASpanFormer, dense RGB-based matching is performedbetween the transformed image and our RGB NeRF ren-ders, where low confident correspondences are filtered out.To handle multiple matches between the transformed imageand different renders (a given pixel might be matched tomultiple locations on different images), the most confidentpair of the lot is selected. This confidence is determined bythe pixel neighbor density size, e.g., the more the adjacentpixels have matches the more likely the matches are valid( (c)). See supplemental for greater details.3D-space filtering.Using the previous pixel correspon- dence, their positions in 3D are lifted using the provideddepth information. In order to determine which pairs arevalid, points in the original scene are first clustered, and wesubsequently compare how the clusters behave in the trans-formed scene ( (d)). If a cluster does not maintain itstight structure we filter the points that diverged. The intu-ition is as follows: point pairs that have adjacent points inthe original scene should stay adjacent in the transformedscene. See supplemental for greater details.In order to define the anchor points, I, for any validpairs point in the original space, we find the closest vertexextracted on the mesh. This anchor point is then linked to itscorrespondence in 3D. Finally, these anchor point matchesare used to optimize our 3D flow, as previously presented.",
  ". Implementation Details": "We use K = 20 in the KNN employed in Eqs. (7)(9). Weonly calculate the flow near the surface (surface distance< 7e5) and regard other space as empty since the flowis only invertible near surface areas. We set = 0.1 inEq. (11). The marching cubes resolution for M A and meshdecimation hyperparameters are set to obtain |V| 500kvertices. Mesh decimation is used to reduce the numberof vertices to 2,000. We use Adam optimizer with alearning rate of 0.001 to minimize LDG for 3k iterations.For correspondence matching, original NeRF rendersare from a hemisphere which has the same distance to theobject as CB. Specifically, we sample 200 camera posi-tions on the hemisphere, render images, and finally aug-ment images by rotating the yaw to one of the 7 angles:. More hyperparam-eter details are given in the appendix.",
  ". Experiments": "Dataset.We demonstrate efficacy of baselines and ourmethod on 113 scenes, which originate from 47 dynamicobject models from the Objaverse dataset . These scenescover a wide variety of complex non-rigid transformations.For each of the 47 dynamic object models, we manually se-lect one animation frame as the original reference and trainour NeRF via Instant-NGP with default settings for 100k iterations using 400 images with a resolution of2880 2880, uniformly sampled on a hemisphere abovethe object. Then we select one to three transformed anima-tion frames (depending on the difficulty), different from theoriginal animation frame(s). For each transformed time, werender one image and its corresponding depth map as thetransformed view.",
  "Baselines.We compare with generative models such asZero123-XL , which finetunes a 2D diffusion model togenerate new views given relative camera poses and a tar-": "get image; as well as DreamGaussian , which is a 3D-aware Gaussian splatting based diffusion model. We alsoinclude two naive baselines: and finetuned. The for-mer keeps the original NeRF without any change while thelater finetunes the NeRF for an extra 2k iterations on thegiven transformed view using the default 2D reconstruc-tion loss. Furthermore, we compare our method with a re-implementation (details in the supplemental) of SINE .Note that for a fair comparison, for the methods that do notrequire depth as input like DreamGaussian, we still use theground truth depth to solve the scale ambiguity; and SINEand our method use depth. Metrics.For novel view synthesis, we use Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure(SSIM) and Learned Perceptual Image Patch Similar-ity (LPIPS) as the metrics. We render 30 new viewsdifferent from the training view poses and calculate the av-erage of these metrics on 30 views, and then average across113 scenes. For geometric reconstruction evaluation, wecompute chamfer distance (CD) and Volume IoU (VmIoU).Since the process of marching cube on a collapsed NeRFscene can lead to bad reconstructions, we define a suc-cessful reconstruction when the chamfer distance is below0.004. As such we report metrics for both all scenes (CD),for scenes that are below that threshold (CD (success)), andwe also report the success rate for each method. Results.Tab. 1 shows quantitative results for our methodand baselines on our proposed dataset. See for qual-itative results.Tab. 1 is separated in two parts, wherewe first present results on visual fidelity reconstruction(left most columns) and on geometric fidelity (right mostcolumns).The original NeRF (), trained on the orig-inal scene without any further changing, performs betterthan other diffusion-based methods such as Zero123-XL orDreamGaussian. Diffusion-based models do not perform aswell as their prior knowledge might not cover the contentof the proposed scenes, e.g., it has a knowledge about catsbut less about doors (see ), and it ignores the infor-mation from the prior NeRF. When fine-tuning the origi-nal NeRF scene to the transformed observation, the NeRFcollapses as it does not have multiple views for constrain-ing its behaviour. Overall our method is the best suited toboth visually reconstruct the scene and extract a meaning-ful mesh with a success rate of 90%. We also observe thatDreamGaussian can capture the coarse shape of theobject but lacks fine-grained texture and geometry, whileSINE produces inconsistent transformations as it tendsto pick wrong correspondences. A real world experiment isincluded in , showing the potential of our method forhandling imperfect settings (camera pose noise).",
  "Ablations": "The quantitative results of our ablation study are provided inTab. 2. Our goal is to motivate some key design decisions inour method, especially compared with SINE. Note that thesettings for the results in row 3-1 of Tab. 2 correspond to thedesign choices for our implementation of SINE . Furthernote, row 4 shows the design choices of our final method.Correspondence matching.We first analyze the effec-tiveness of ASpanFormer as a pixel correspondencematching method and compare to FlowFormer used forSINE . Comparing row 1-1 and 1-2 in Tab. 2, we findASpanFormer correspondences lead to better performancein all metrics. This is expected as ASpanFormer is trainedon a dataset with large displacements while FlowFormeris trained on image pairs from adjacent frames in videos,i.e., the displacements are smaller. Although the correspon-dences are stronger, it is still important to exploit correspon-dences from multiple views and filter the false positives toimprove further.Single/multiple views for correspondence. In Sec. 3.3 wepresented a method that leverages NeRF to render multipleviews, and as such here we evaluate the impact of objectcoverage (single or multiple views). Our method (row 4in Tab. 2) is compared with the baseline in row 1-2, whichonly uses correspondences between the transformed imageand a single original image whereas both are rendered fromthe same camera pose. Using multiple original images im-proves all metrics significantly. (c,d,e) visualizes thecorrespondences obtained for a specific scene when using asingle original image and when using multiple original im-ages. Trivially multiple images outperforms using a singleimage.Correspondence Filtering. We also compare using cor-respondence from multiple images obtained via ASpan-Former and only filter based on method confidence scores(row 2 in Tab. 2). Results indicate that filtering of corre-spondences is a non-trivial problem and our pixel-level fil-tering with specially designed scores and our 3D filteringare adequate for our problem ( (c,f)).Scene Flow. Our scene flow is compared with the MLPcyclic flow for new view synthesis used in SINE . Com-paring Tab. 2 row 1-1 with row 3-1, or Tab. 2 row 4-1 withrow 3-2, we observe that the MLP design hampers the per-formance in all metrics. We believe that the cyclic con-straint is too strong and limits the expressiveness of theMLP. Replacing our flow representation method (row 4)with a MLP (row 3-2) leads to a decrease in performance.Please note that we only run experiments for visual metricsas we observed that using an MLP in spaces that do not havecoverage extremely degrades the quality of the output. See for qualitative results.Depth quality. We test injecting noise to the transformedview depth via SimKinect . We explore 3 different levels",
  "Training & transformed viewsView 1View 2Mesh": ". Qualitative results comparing our method to prior work. We first show in the left-most columns the original scene and thetransformed view. The other columns show different renderings of the transformed scene: ground truth in blue, DreamGaussian ingreen, SINE in yellow, and our method in red (lexicographic order within each 2 2 block).",
  "ASpFmultiple2D + 3D25.94.20.9240.0340.0610.0401.462.90.9030.6660.20": ". Real world results. Left: the original scene of a half-opened box, where the first image is a training view (take from 364images), second and third images are NeRF renders, and the lastimage is the mesh reconstruction. Right: the transformed sceneof the same box fully opened, where the first image is the uniquetraining view, second and third image are NeRF renders, and thelast image the mesh reconstruction.",
  ". Conclusion": "NeRFDeformer successfully transforms a NeRF scene us-ing only a single RGBD observation of the transformedscene. The method uses local linear transformations on thesurface to map the original configuration to the transformedone. In order to learn these linear transformations we intro-duce a new method to find dense correspondences betweena NeRF scene and a single RGBD observation.Future work should include exploring relaxing the needof depth input, such as through leveraging prior knowledgeabout shape or scene compositions. We are also interestedin grounding diffusion models through scene flow to helpdetermine where generation should be focused on.Acknowledgments.Work supported in part by NSF grants2008387, 2045586, 2106825, MRI 1725729, and NIFA award2020-67021-32799. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,Zesong Yang, Hujun Bao, Guofeng Zhang, and ZhaopengCui. Sine: Semantic-driven image-based nerf editing withprior-guided editing field. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 2091920929, 2023. 1, 2, 3, 4, 6, 7, 8",
  "Aljaz Bozic, Pablo Palafox, Michael Zollhofer, Angela Dai,Justus Thies, and Matthias Niener. Neural non-rigid track-ing. Advances in Neural Information Processing Systems,33:1872718737, 2020. 3": "Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Ming-min Zhen, Tian Fang, David Mckinnon, Yanghai Tsin, andLong Quan.Aspanformer: Detector-free image matchingwith adaptive span transformer. In European Conference onComputer Vision, pages 2036. Springer, 2022. 2, 4, 6, 8 Matt Deitke, Ruoshi Liu, Matthew Wallingford, HuongNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprintarXiv:2307.05663, 2023. 2, 5, 8 Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. In Proceedings of the IEEE conference oncomputer vision and pattern recognition workshops, pages224236, 2018. 2",
  "Michelle Guo, Alireza Fathi, Jiajun Wu, and ThomasFunkhouser. Object-centric neural scene rendering. arXivpreprint arXiv:2012.08503, 2020. 2": "Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.Shape, light, and material decomposition from images usingmonte carlo rendering and denoising. Advances in NeuralInformation Processing Systems, 35:2285622869, 2022. Jinkai Hu, Chengzhong Yu, Hongli Liu, Lingqi Yan, YiqianWu, and Xiaogang Jin. Deep real-time volumetric renderingusing multi-feature fusion. In ACM SIGGRAPH 2023 Con-ference Proceedings, pages 110, 2023. 2 Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang,Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and HongshengLi. Flowformer: A transformer architecture for optical flow.In European Conference on Computer Vision, pages 668685. Springer, 2022. 2, 6, 8 Matthias Innmann, Michael Zollhofer, Matthias Niener,Christian Theobalt, and Marc Stamminger. Volumedeform:Real-time volumetric non-rigid reconstruction. In ComputerVisionECCV 2016: 14th European Conference, Amster-dam, The Netherlands, October 11-14, 2016, Proceedings,Part VIII 14, pages 362379. Springer, 2016. 3 Clement Jambon,Bernhard Kerbl,Georgios Kopanas,StavrosDiolatzis,GeorgeDrettakis,andThomasLeimkuhler.Nerfshop:Interactive editing of neuralradiance fields.Proceedings of the ACM on ComputerGraphics and Interactive Techniques, 6(1), 2023. 1, 2 Wonbong Jang and Lourdes Agapito. Codenerf: Disentan-gled neural radiance fields for object categories. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 1294912958, 2021. 2 Kacper Kania, Kwang Moo Yi, Marek Kowalski, TomaszTrzcinski, and Andrea Tagliasacchi. Conerf: Controllableneural radiance fields. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1862318632, 2022. 2",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization.arXiv preprint arXiv:1412.6980,2014. 5": "Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, GordonWetzstein, and Kalyan Sunkavalli. Palettenerf: Palette-basedappearance editing of neural radiance fields. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 2069120700, 2023. 2 Yunzhi Lin, Thomas Muller, Jonathan Tremblay, BowenWen, Stephen Tyree, Alex Evans, Patricio A Vela, and StanBirchfield. Parallel inversion of neural radiance fields forrobust pose estimation. In 2023 IEEE International Confer-ence on Robotics and Automation (ICRA), pages 93779384.IEEE, 2023. 1",
  "Minghua Liu,Chao Xu,Haian Jin,Linghao Chen,Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization. NeurIPS, 2023. 2": "Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:Zero-shot one image to 3d object.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 92989309, 2023. 2 Steven Liu, Xiuming Zhang, Zhoutong Zhang, RichardZhang, Jun-Yan Zhu, and Bryan Russell.Editing condi-tional radiance fields. In Proceedings of the IEEE/CVF inter-national conference on computer vision, pages 57735783,2021. 2",
  "from images. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 82808290, 2022. 2": "Richard A Newcombe, Dieter Fox, and Steven M Seitz.Dynamicfusion: Reconstruction and tracking of non-rigidscenes in real-time. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 343352,2015. 3 Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.Dinov2: Learning robust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023. 2 Keunhong Park, Utkarsh Sinha, Jonathan T Barron, SofienBouaziz, Dan B Goldman, Steven M Seitz, and RicardoMartin-Brualla. Nerfies: Deformable neural radiance fields.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 58655874, 2021. 1, 2, 3 Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,Qing Shuai, Hujun Bao, and Xiaowei Zhou.NeuralBody: Implicit Neural Representations with Structured La-tent Codes for Novel View Synthesis of Dynamic Humans.In CVPR, 2021. 3 Yicong Peng, Yichao Yan, Shengqi Liu, Yuhao Cheng,Shanyan Guan, Bowen Pan, Guangtao Zhai, and XiaokangYang. Cagenerf: Cage-based neural radiance field for gen-eralized 3d deformation and animation. Advances in NeuralInformation Processing Systems, 35:3140231415, 2022. 1,2 Albert Pumarola, Enric Corona, Gerard Pons-Moll, andFrancesc Moreno-Noguer.D-nerf: Neural radiance fieldsfor dynamic scenes. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1031810327, 2021. 1, 2, 3 Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-rokhodov, Peter Wonka, S. Tulyakov, and Bernard Ghanem.Magic123: One Image to High-Quality 3D Object Genera-tion Using Both 2D and 3D Diffusion Priors. ArXiv, 2023.2 Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,and Andrew Rabinovich.Superglue:Learning featurematching with graph neural networks.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 49384947, 2020. 2 William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie PackKaelbling, and Phillip Isola. Distilled feature fields enablefew-shot language-guided manipulation. In 7th Annual Con-ference on Robot Learning, 2023. 1",
  "Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,Lizhuang Ma, and Dong Chen. Make-It-3D: High-Fidelity3D Creation from A Single Image with Diffusion Prior.ArXiv, 2023. 2": "Zhenggang Tang,Balakumar Sundaralingam,JonathanTremblay, Bowen Wen, Ye Yuan, Stephen Tyree, CharlesLoop, Alexander Schwing, and Stan Birchfield. Rgb-onlyreconstruction of tabletop scenes for collision-free manip-ulator control. In 2023 IEEE International Conference onRobotics and Automation (ICRA), pages 17781785. IEEE,2023. 1 Zachary Teed and Jia Deng. Raft: Recurrent all-pairs fieldtransforms for optical flow.In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer,2020. 2",
  "Bing Wang, Lu Chen, and Bo Yang.Dm-nerf: 3d scenegeometry decomposition and manipulation from 2d images.arXiv preprint arXiv:2208.07227, 2022. 2": "Can Wang, Menglei Chai, Mingming He, Dongdong Chen,and Jing Liao.Clip-nerf: Text-and-image driven manip-ulation of neural radiance fields.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 38353844, 2022. 2 Xiangyu Wang, Jingsen Zhu, Qi Ye, Yuchi Huo, YunlongRan, Zhihua Zhong, and Jiming Chen. Seal-3d: Interactivepixel-level editing for neural radiance fields. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1768317693, 2023. 2",
  "Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,and Zhangyang Wang. NeuralLift-360: Lifting an in-the-Wild 2D Photo to A 3D Object with 360 Views. CVPR,2022. 2": "Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, HanZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.Learning object-compositional neural radiance field for ed-itable scene rendering. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1377913788, 2021. 2 Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, YindaZhang, Zhaopeng Cui, and Guofeng Zhang.Neumesh:Learning disentangled neural mesh-based implicit field forgeometry and texture editing. In European Conference onComputer Vision, pages 597614. Springer, 2022. 2 Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Polle-feys, Zhaopeng Cui, and Guofeng Zhang.Intrinsicnerf:Learning intrinsic neural radiance fields for editable novelview synthesis.In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 339351,2023. 2 Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Tsung-Yi Lin, Alberto Rodriguez, and Phillip Isola.NeRF-Supervision: Learning dense object descriptors from neuralradiance fields. In IEEE Conference on Robotics and Au-tomation (ICRA), 2022. 1",
  "Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.pixelNeRF: Neural radiance fields from one or few images.In CVPR, 2021. 2": "Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,Rongfei Jia, and Lin Gao. Nerf-editing: geometry editing ofneural radiance fields. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1835318364, 2022. 2 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 586595, 2018. 6"
}