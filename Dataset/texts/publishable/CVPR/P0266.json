{
  "University of Maryland College Park1, Google DeepMind2, Google Research3": ". We present a method for multi-concept customization of text-to-video models. Our method only relies on a pre-trained text tovideo model and concept specific data in the form of image(s) or video(s). In the illustration above, our model uses an image of a teddybear and ocean background (top row) and an image of a dog and video of playing violin (bottom row) to generate the customized videos.",
  "Abstract": "We present a method for multi-concept customizationof pretrained text-to-video (T2V) models.Intuitively,the multi-concept customized video can be derived fromthe (non-linear) intersection of the video manifolds ofthe individual concepts, which is not straightforwardto find.We hypothesize that sequential and controlledwalking towards the intersection of the video manifolds,directed by text prompting, leads to the solution.To doso, we generate the various concepts and their corre-sponding interactions, sequentially, in an autoregressivemanner.Our method can generate videos of multiplecustom concepts (subjects, action and background) suchas a teddy bear running towards a brown teapot, a dogplaying violin and a teddy bear swimming in the ocean.We quantitatively evaluate our method using videoCLIPand DINO scores, in addition to human evaluation.Videos for results presented in this paper can be found at",
  ". Introduction": "Text-to-video generation models such as Video Diffu-sion Models , Make-A-Video , Imagen Video ,Phenaki CogVideo , VideoFusion , Gen-1 are capable of generating visually appealing videos. How-ever, their application is constrained due to the inabilityto generate long videos (beyond a few seconds) and limited control . These limitations are problematicmainly because when generating a long video that includesmultiple subjects, we expect the same subjects to appearlater in the video.For instance, if we generate a videoof a specific Teapot boiling under a specific Tree, later inthe video we expect to see the same Teapot and the sameTree and not another randomly generated one. One way toachieve such results with the current text to video modelsis via providing extreme level of details in the captions, sothat the model generates the same Tree and the same Teapoteverytime. However, this is usually impractical be-cause the models may not be sensitive to all the details andthe concepts may be personal and unseen by the model dur-ing large-scale pre-training .Another way of getting consistent subjects in consecu-tive video frames is that video generation should have the",
  "arXiv:2405.13951v1 [cs.CV] 22 May 2024": "capability to generate clips conditioned on one or moregiven subjects that may or may not be a part of the train-ing data. Model customization methods such asDreamBooth and DreaMix work well for singleconcept video customization such as generating a randomvideo of a customized subject or making a random subjectperform a custom action. However, they are not as effectivein the multi-concept customization regime . We hypoth-esise that this is due to the inability of the model in un-derstanding the interactions between different custom con-cepts, that it may have not encountered during training .Also, learning characteristics of concepts from limited dataper concept results in severe overfitting . Hence, thevariance of the model w.r.t. the custom concept is low andit is predominant to use the variance of the model to over-come bias issues w.r.t. the custom concept. Additionally,when multiple concepts are customized in a video, there isa mixup of their corresponding attributes Prior work on multi-concept customization of text to im-age models and concurrent work on multi-conceptvideo customization are able to put together multiplesubjects in the scene in a realistic manner. However, theyoften train individual adapter models for each conceptbefore projecting to a common space, making it memoryinefficient for video models, and also rely on expensivedata augmentation from the original training dataset. More-over, the text-to-image customization methods suffer fromattribute binding problems and artifacts when extendedto the video space. Main contributions.In this paper, we present a simple,yet effective solution for multi-concept video customizationbased on teaching the model interactions between variousconcepts one-by-one in order to generate the custom video.We show that sequential and controlled traversal through themanifolds of each subject (each understood by the model)towards the intersection space of the manifolds, while gen-erating video frames sequentially, leads to the generation ofthe custom video. The main intuition behind our proposedmethod is that while sequentially generating the frames ofa video, the model needs to remember what it previouslygenerated and synthesize the interactions accordingly whilestaying causal in time. Our solution for multi-concept videocustomization exploits the inherent strong causality in au-toregressive T2V models to generate the interactions be-tween the different concepts in the video. Moreover, at eachstage of generation after the first step, the availability ofprior knowledge through previously generated frames helpsin increasing the variance of the model, which is useful forgenerating the interactions between various concepts.We qualitatively evaluate our method on diverse scenar-ios of generating multi-concept customized videos such asa dog playing violin, a dog and a teddy bear eating togetherand a teddy bear walking in a living room; and report quan- titative metrics such as videoCLIP and DINO, along withhuman evaluation. We show improved results over base-lines such as DreamBooth style finetuning. We also showthe usefulness of our method for single concept customiza-tion in cases where compositionality is desired.",
  ". Text to Video Models": "Recent advances in generative AI has led to the develop-ment of text-to-video models such as Video Diffusion Mod-els , Make-A-Video , Phenaki , CogVideo ,ImagenVideo , PyoCo , PVDM , etc. Thesepretrained text-to-video models, along with pretrained text-to-image models, have been used for video generation,video editing and personalization (including motion/ ac-tion personalization) by a number of approaches such asDreamix , Tune-A-Video , Pix2Video , AlignYour Latents , CCEdit , Fatezero , video-p2p , Wang et.al. ,ConditionVideo, infu-sion ,Zhang et. al. . However, these methods areusually limited to performing style transfer, concept editingand single subject personalization in videos where a sin-gle subject is involved. Methods such as Esser et. al. are able to control the structure and content individually,but require a large amount of data to train on. The goal ofour paper is to generate a multi-concept customized video,consistent with the input text description, using a limitednumber of data samples per image/ video concept (1 in thispaper) and a pretrained text-to-video model.",
  ". Personalization of generative models": "Image and video personalization using generative AImodels has seen tremendous progress in the recent pastfor single concept personalization with the development ofmethods such as DreamBooth , ControlNet , In-structionpix2pix , Imagic , Null text inversion ,Video ControlNet , Attend-and-excite , paint by ex-ample , Free-bloom .Recent times have alsoseen multi-concept customization methods for text-to-image models such as Custom Diffusion , FastCom-poser , SVDiff , Subject Diffusion . However,multi-concept customization methods developed for text-to-image models are not very effective for video cus-tomization and cause artifacts, attribute binding problems,memory and data issues. In contrast, our method trains asingle lightweight adapter jointly on all concepts and doesnot require any data regularization for inducing variance.VideoDreamer is a multi-subject video customizationmethod concurrent to our work. VideoDreamer leveragesa pretrained text-to-image stable diffusion model to extendto custom video generation.Its use of a text-to-imagemodel limits its application to cases where there is minimalsubject-subject interaction or limited motion. Moreover, itcannot transfer or customize motion information. In con- . Proposed method for multi-concept video customization. First, we add adapter layers to the transformer architecture in anautoregressive T2V model and finetune these additional layers on the given images or videos associated with the N custom concepts andtheir corresponding text prompts. The goal is to find the solution at the intersection of the video manifolds corresponding to various customconcepts. Then, we condition on m (= 5) prior frames and sequentially generate the custom concepts and their interactions in a controlledmanner using a set of prompts p0...pN. The prompts p0...pN are designed to represent the scene in a top-down manner, each promptadding a custom concept and the associated interaction.",
  ". Problem Formulation": "We are given a pretrained text-to-video model M andimage(s) or video(s) ci, i = 1...N corresponding to N con-cepts with text descriptions tci, i = 1...N, where N is thenumber of concepts. Our goal is to generate a video Vgenthat contains the custom concepts, as described by a querytext tgen. We assume that the model is capable of auto-regressively generating future frames.",
  ". Proposed Method": "We present an overview of our method in . Asour first step, we incorporate the knowledge of the customconcepts ci, i = 1...N within the text-to-video model byfinetuning the diffusion model. Our next step is to use thefinetuned model to generate the custom video, correspond-ing to tgen, by adding subjects one-by-one, in an autore-gressive manner. Each step of the generation progressivelyadds various custom concepts and their corresponding in-teractions, as dictated by the query text tgen. We now turnto describe our method in detail.",
  "Multi-concept finetuning": "Given ci, i = 1...N, we fine-tune M. Following , wedo parameter efficient fine-tuning and specifically, adaptertuning, as is shown effective for generative image trans-formers . Note that we jointly train a single adapterfor all concepts instead of training an individual adapter foreach concept as in .Our next step is to use the finetuned text-to-video modelto generate personalized videos with custom concepts, dic-tated by a query text tgen. Consider a Teapot and a Tree.The video manifold M1 of Teapots contains videos of theTeapot pouring tea, floating in the ocean, being washed,boiling tea and in a kitchen, dictated by text. Similarly,the video manifold M2 of Trees contains videos of the treeswaying in the wind, the relational notion of under the tree,in a park, squirrels playing around the tree, cat climbingthe tree and the tree being located in a dense forest, dic-tated by text. Thus, each of the manifolds contains what themodel is familiar with in terms of generation. The videocorresponding to \"a Teapot boiling tea under a Tree\" is con-tained in the (non-linear) intersection of the two manifoldsM1 M2, generation of which is not straightforward. Inthe next section, we present a method to generate the multi-concept customized video.",
  "Causal generation, one subject at a time": "We hypothesize that sequential and controlled traversalthrough the various manifolds (each understood by themodel) towards the intersection space of the manifolds,while generating video frames sequentially, leads to the so-lution. Our solution is based on multi-step generation, eachstep incrementally generating what the model understands,while remembering what it has already generated. Thus,we begin at the manifold corresponding to one of the cus-tom concepts to generate a few frames corresponding to thefirst concept by using an appropriate text description. Next,to generate video frames with multiple concepts, we condi-tion on the generated video frames of the first concept anduse text descriptions describing the other concepts to incre-mentally add them and their interactions.For instance, in the case of \"a Teapot boiling tea undera Tree\", we begin at the manifold of Trees to generate afew frames of the Tree using the text prompt \"a Tree\". Next,move towards the manifold of Teapots to generate a video ofthe Teapot boiling tea under a Tree. Consequently, to gen-erate the next video frames of \"a Teapot boiling tea undera Tree\", we use the text prompt \"a Teapot boiling tea undera Tree\", conditioned on the previously generated frames ofthe Tree. Thus, the first few frames of the video depict theTree, and the subsequent frames show the Teapot boiling teaunder the Tree.At each step of the sequential generation, the existence ofa strong prior (in terms of video frames generated upto thatstage), along with the variance arising from the pretrainedmodel, alleviates bias related issues. Consequentially, ateach step of the sequential generation, the model needs tobe conditioned on an appropriate number of prior frames itpreviously generated and generate the subsequent frames inan autoregressive manner. If too few frames are used for theconditioning, the model cannot remember prior knowledge.On the other hand, if too many frames are used, the modelmay not be able to bring in new concepts due to bias issues.Moreover, it is important to keep the sequential generationcontrolled, in order to make sure that the generated videostays within the intersection manifold i.e. the number offrames generated for each concept is a key hyperparameter.In summary, causality is key to maintain prior context.Algorithmically, let F be the total number of frames thatare being generated and pi, i = 1...N be the prompts de-scribing the various concepts and their interactions. Tokenscorresponding to all F frames are initialized to be empty.",
  "for k in range (0, F) : zk M(zk)|(zk1:km, p).(1)": "As per this equation, the generation of each frame (exceptthe first m frames) is conditioned on the past m frames. Theprompt p is sequentially set to p1...pN. Since the model cangenerate 11 frames in one go, we set p = p1 for k <= 11.In the 2-concept customization case, we set p = p2 for the",
  "rest of the frames": "Structuring the concepts.Sequential generation requiresthe various concepts to be structured in an appropriate man-ner it is important to carefully design pi, i = 1...N. Wepropose to structure the concepts in a top-down manner -consistent with the natural representation of a scene. Forexample, Custom subject and custom background - first generatethe background. Next, condition on the frames con-taining the custom background to add the custom sub-ject performing action. For eg. given concepts a B1*futuristic restaurant and a C2@ cat, and tgen = aC2@ cat eating noodles in a B1* futuristic restaurant,we set",
  "p1 = a B1* futuristic restaurant p2 = a C2@ cat eating noodles in the futuristicrestaurant": "Custom subject and custom action (from a video) -first generate a random (related subject) performingthe custom action. Next, steer the model towards themanifold of the custom subject performing action. Foreg. given concepts a person playing tennis in a tenniscourt and a C2@ cute cat, and tgen = a C2@ cutecat playing tennis in a tennis court, we set",
  ". Setup": "Our proposed method for multi-concept video cus-tomization is built on autoregressive generation.Conse-quentially, we use Phenaki , a text-to-video model withautoregressive capabilities, as our backbone architecture.Phenaki learns video representations in a causal manner,compressing the video into a small representation of dis-crete tokens using a spatial transformer and a causal trans-former. A MaskGiT transformer model is then trained toreconstruct the video tokens, conditioned on a text prompt.A bidirectional text transformer is used to compute videotokens from text. At inference, starting from empty tokensand conditioned on the input text, video tokens are gener-ated, which are then detokenized to generate a video. Ateach step of the inference, Phenaki can freeze video tokens . Two subject customization. While Phenaki containssome prior knowledge of the interactions between different kindsof subjects, the finetuned model is unable to generate the inter-actions between the custom subjects. Through controlled and se-quential autoregressive generation of concepts and their interac-tions, our method is able to generate customized videos with twosubjects interacting with each other. corresponding to past frames and generate video tokenscorresponding to future frames in an autoregressive man-ner.Classifier-free guidance controls the alignmentbetween the text and the generated video. Phenaki is pre-trained on a large-scale text-image and text-video dataset,which we use for multi-concept video customization. Weuse the low-resolution Phenaki model in all experiments.We finetune the adapter weights jointly on all image con-cepts, followed by video concepts, for 1000 steps with alearning rate of 1e4 using the Masked Visual Token Mod-eling (MVTM) loss , along with classifier-free guidance.The adapter has a hidden size of 2. Our generated videoshave a resolution of 160 96. At inference time, to con-dition on prior frames, we set m = 5. Conditioning on 5frames, determined empirically, is consistent with the factthat Phenaki is trained to generate 11 frames at a time andconditioning on half the frames achieves the right trade-offbetween the amount of prior knowledge required for thegeneration of future frames. For each example, we present . Subject Action customization. Phenaki, due to lack ofprior knowledge, is unable to generate certain actions. The fine-tuned model is able to generate the custom subject, but not theprecise action, even after finetuning. Generating the motion cor-responding to the action first, followed by generating the customsubject performing action (conditioned on the motion) enables thegeneration of the custom subject performing the custom action.Thus, we are not only able to teach the action to the model, butare also able to customize the subject performing the action.",
  "Phenaki: This baseline corresponding to Phenaki as itwas trained with no additional fine-tuning": "We use Imagen to generate the image for each customconcept, concept-specific videos are from UCF-101 .Custom concepts can take multiple forms, e.g. background,visuals of a character or subject and a custom performedaction. We experiment with various types of customizationsuch as subject-subject, subject-action, subject-background.The design of the prompts was a purely creative task, we . Subject BG customization. While subject-backgroundcustomization is relatively easy for the Phenaki model, the fine-tuned model is unable to emphasize on the background due to biasissues. This is resolved by our model, which first generates thebackground and then generates video frames of the custom sub-ject performing action, conditioned on the background.",
  "designed close to 100 prompts and attempted to keep it asdiverse as possible. Our data set-up is as follows:": "Subject-subject customization: We use 14 examplescorresponding to the 3 examples shown in the paper, 9in the supplementary and a cute dog and a blue teddybear in a running race, a cute dog washing a blueplate. Subject-action customization: We use 4 subjects: asmall cute fox, a small cute dog, a small cute cat anda blue teddy bear and 8 actions: playing violin, play-ing hula hoop, playing drums, playing tennis, crawl-ing, doing pushups, riding bicycle, doing TaiChi. Thisresults in a total of 32 videos. Subject-background customization: We use 4 subjects:a small cute fox, a small cute dog, a small cute catand a blue teddy bear, 4 backgrounds: living room, fu-turistic restaurant, forest themed cinema theater, oceanwith seaplants and fishes. For each background, we",
  "Two-concept customization": "Two-subject customization ():In the first case,while Phenaki is able to generate the video of a genericteddy bear running towards a generic teapot, the teddy bearis much smaller than the teapot. The finetuned model gen-erates both subject in the video but is unable to generate thecorrect interaction (of the teddy bear running towards theteapot). Our method structures the generation as p0 = aB1* brown teapot and p1 = a C2@ blue teddy bear run-ning towards the B1* brown teapot to generate a video ofthe C2@ blue teddy bear running towards the B1* brownteapot; the static object (teapot) is generated first followedby the moving teddy bear. In the second example, Phenakiis able to generate the video with generic subjects. The fine-tuned model has a bias towards the teapot and fails to gener-ate the tree. Our method structures the generation as p0 = aC2@ tree and p1 = a B1* brown teapot boiling tea under aC2@ tree to generate a customized video of the teapot boil-ing tea under the tree; the background, static subject (tree)is generated first followed by the teapot boiling tea. In thethird example, Phenaki generates a toy teddy bear, that isnot eating. The finetuned model mixes the attributes of theconcepts. Our method structures the generation as p0 =a cute B1* dog eating together with the C2@ blue teddybear and p1 = a C2@ blue teddy bear eating togetherwith a cute B1* dog to generate the dog and teddy beareating together; the emphasis is on the dog first, followedby the teddy bear. In summary, controlled and sequentialgeneration of the custom concepts and their interactions iseffective in generating a multi-subject customized video. Subject-action customization ():We first gen-erate the motion depicting the action, followed by thesubject performing the action conditioned on the motionframes. Hence, p0, p1 take the general structure of p0 =arandom subject performing the action, p1 =custom sub-ject performing the action. In the first case, the pretrainedmodel Phenaki does not understand what a teddy bear play-ing tennis looks like and is unable to generate the video.In the result of the fine-tuned model, there is some move-ment in terms of the teddy bear holding the tennis bat butthe video does not depict a teddy bear playing tennis. Ourmethod is able to generate a video of the custom teddy bearplaying tennis. Thus, our method is not only able to teachthe model the action playing tennis, but is also able to ef-fectively condition on the motion to customize the subjectperforming action. Similarly, in the third example, neitherPhenaki nor the finetuned model are able to generate a cat playing tennis. Our method is able to generate a video ofa custom cat playing tennis, thus teaching the model theaction, as well as customizing the subject. In the secondexample, again, the Phenaki model does not know what ateapot riding a bicycle is. The finetuned model is able togenerate some movement of the teapot but the generatedvideo isnt really of a teapot riding a bicycle. Subject-background customization ():Ournext task is to customize the subject and the background(and generate a random action).We first generate thecustom background, followed by the subject perform-ing a generic action in the background.Hence, p0, p1take the general structure of p0 =custom background,p1 =custom subject performing action in the background.In the first example, while Phenaki understands what ateddy bear walking in a living room is, when finetuned,due to bias issues, the finetuned model is unable to gener-ate a custom teddy bear walking in the custom living room.Our method first generates the custom living room, condi-tioned on which, the custom teddy bear walking in the livingroom is generated. Similarly, in the second and third exam-ples, while Phenaki understands the interactions betweenthe subject and the background, after finetuning, due to biasissues, the model is unable to generate the background well.Our method is able to generate videos of custom cats attend-ing meetings in futuristic restaurants, and a custom teddybear dancing in a forest themed cinema theater.",
  "Three-concept customization": "We show generated videos with 3 custom concepts in Fig-ure 6. In the first example, we have three concepts: a teddybear, a fox and a teapot and we want to generate a videoof the two animals drinking tea together from the teapot.Phenaki is unable to generate the video (even without cus-tomization) - we see that the attributes of the three conceptsare mixed up. The model generates a blue teapot insteadof a brown teapot and a hybrid of the teddy bear and thefox. In the result of the finetuned model, a teddy bear in thecolor of the fox is generated, and the teapot is in the colorof the teddy bear. There is no fox in the scene. By bring-ing in the concepts and the interactions systematically, ourmethod is able to generate the custom blue teddy bear drink-ing tea from the custom brown teapot with the custom fox.In the second example, we want to generate a dog playingdrums under a tree. The dog, the tree and the action of play-ing drums are customized. Phenaki understands the sceneand is able to generate a generic video. The finetuned modelbrings in all concepts into the scene but is unable to generatethe action of playing drums. Our method, by conditioningon the motion tokens (of playing drums), is able to generatea video of the custom dog playing drums under the customtree. . Three-concept customization. In the first case, thebaseline models mix the attributes of the various concepts. Ourmethod, by sequential generation of concepts and interactions, isable to generate the complex video with all desired concepts andinteractions in the scene. Similarly, in the second example, whilethe finetuned model is able to bring in all concepts into the video,it is unable to generate the interaction of playing drums. Condi-tioning on motion information helps our method do so.",
  "Compositional single-concept customization": "While the focus of our method is on multi-subject cus-tomization, it is also useful for single-concept customiza-tion where the model does not understand compositionality.In such cases naive finetuning may not work due to biasissues and leveraging causality to generate compositionalscenes can lead to better results. We show two cases in Fig-ure 7. These results provide a direction for future work onusing causality to generate compositional videos.",
  ". Quantitative results": "We quantitatively evaluate our method using the Video-CLIP score and the self-supervised similarity score,DINO . The former computes the alignment with text,signifying if the concepts and interaction described by thetext are present in the generated video. To compute theDINO score, we compute the average over the individualDINO score w.r.t. each concept in the video, also aver-aged over all frames of the video. Blau et. al. state thatgenerative models face difficulty in trading off perception-distortion. Similarly, we argue that videoCLIP-DINO (ortext describing concept-concept interaction vs fidelity w.r.t.each concept) is a difficult trade-off to solve as it relates tothe bias-variance trade-off.We also analyze our results with a human evaluationstudy. For each video, we ask participants three questions:(i) Is concept 1 present in the video? (ii) Is concept 2 presentin the video? (iii) Is concept 1 interacting with concept 2 asdescribed by the text? We report the percentage of affirma-tive responses for each question and for each method.We show the results in .We use 14 exam-ples for subject-subject customization, 32 examples forsubject-motion customization and 32 examples for subject-background customization.To compute the DINO andvideoCLIP scores, for each example, we generate 8 resultsand choose the generated video with the best videoCLIPscore as the final result. For the human evaluation, we showall 8 results (for each example) to human raters and computethe average. Subject-subjectcustomization.WeshowimprovedDINO score, there is a small improvement in the video-CLIP score as well. Human raters also rate our methodmuch higher than the baseline in terms of both conceptinteraction, as well as alignment of the characteristics ofthe concept with the input images.This indicates ourmethods ability to better generate multiple custom subjectsinteracting with each other. Subject-actioncustomization.WeshowimprovedvideoCLIP score indicating our methods ability to gen-erate the action better. The DINO score of our method islower than that of the baseline. The baseline method is un-able to generate good actions and in many cases reproducesthe input concept image with mild movement, hence, itsresemblance to the input concept image is better leading toa higher DINO score. Human raters rate our method betterin terms of both interaction as well as concept fidelity. Subject-background customization.We show improvedDINO score, indicating our methods ability to generate thesubject as well as the background with high fidelity. Thevideo-CLIP score of our method is lower than that of thebaseline.This is because the baseline method generatesa video that focuses on the subject performing the action.This also implies higher resemblance to the input subjectimage or a higher individual DINO score w.r.t. the sub-ject. We believe this also leads to human raters preferringthe baseline method more than our method. However, thebackground is not very well depicted. The individual DINOscore for background for our method is 0.3496, while thatof the baseline is 0.1896. Our method focuses on gener-ating the entire background depicted in the input conceptimage, along with the subject performing action; thus in-dicating higher consistency with both input concepts whilegenerating the interaction.",
  ". Limitations and Future Work": "Our method has a few limitations: (i) extension beyondthree concepts is non-trivial because the model may nothave the ability of remember what it generated in the firststep, and structuring the concepts/ interactions is complex.(ii) Controlling the interactions through text is not easy,it may be useful to have more control signals that enablethe model to generate interactions respecting the 3D world(such as contact points between subject and ground, rel-ative size between objects). (iii) We use the low resolu-tion model of Phenaki to generate the results. This was theonly autoregressive model we had access to at the time ofsubmission of the paper. The development of better videofoundation models and superresolution can enable the gen-eration of better results. (iv) The quality of the generatedvideos is highly dependent on the base model, with the de-velopment of richer models that understand temporal co-herence and 3D relations in the world, the artifacts in thegenerated videos can be removed. Furthermore, the recentdevelopment of autoregressive diffusion transformer mod-els for video generation can enable the application of ourmulti-concept video customization method. (v) Our methodtakes in a single image per subject to generate personalizedvideos. This leads to low variance, and the model hallu-cinates a lot of details corresponding to the subject. Thiscauses low subject fidelity in some generated videos, a di-rection for future work. Future work can focus on overcom-ing these limitations, in addition to further improving thetext alignment and fidelity w.r.t. input concepts. Anotherdirection for future work is to automate the structuring ofthe prompts using LLMs and simplify hyperparameter tun-ing related to finding the number of future frames ( andconditioning on prior frames) to be generated using eachprompt.Also, incorporating causality within non-causaltext-to-video models such as video diffusion models to usethem for multi-concept video customization is an avenue toexplore. This paper provides a direction for the importanceof causality in multi-concept customization and can also beextended to long video generation. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.Align your latents: High-resolution video synthesis with la-tent diffusion models. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages2256322575, 2023. 2",
  "vision and pattern recognition, pages 62286237, 2018. 8": "Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-structpix2pix: Learning to follow image editing instructions.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1839218402, 2023.2 Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 96509660, 2021. 8 Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.Pix2video: Video editing using image diffusion. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 2320623217, 2023. 2 Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William TFreeman. Maskgit: Masked generative image transformer.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1131511325, 2022.4, 5 Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, andDaniel Cohen-Or.Attend-and-excite: Attention-based se-mantic guidance for text-to-image diffusion models. ACMTransactions on Graphics (TOG), 42(4):110, 2023. 2 Hong Chen, Xin Wang, Guanning Zeng, Yipeng Zhang,Yuwei Zhou, Feilin Han, and Wenwu Zhu.Video-dreamer: Customized multi-subject text-to-video generationwith disen-mix finetuning. arXiv preprint arXiv:2311.00990,2023. 1, 2 Patrick Esser,Johnathan Chiu,Parmida Atighehchian,Jonathan Granskog, and Anastasis Germanidis.Structureand content-guided video synthesis with diffusion models.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 73467356, 2023. 1, 2 Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan,Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo.Ccedit: Creative and controllable video editing via diffusionmodels. arXiv preprint arXiv:2309.16496, 2023. 2 Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.An image is worth one word: Personalizing text-to-image generation using textual inversion.arXiv preprintarXiv:2208.01618, 2022. 2 Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, AndrewTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation:A noise prior for video diffusion models. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 2293022941, 2023. 2",
  "Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, JingyiYu, and Sibei Yang. Free-bloom: Zero-shot text-to-videogenerator with llm director and ldm animator. arXiv preprintarXiv:2309.14494, 2023. 2": "Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, HuiwenChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 60076017, 2023. 2 Anant Khandelwal. Infusion: Inject and attention fusion formulti concept zero-shot text-based video editing. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 30173026, 2023. 2 Nupur Kumari, Bingliang Zhang, Richard Zhang, EliShechtman, and Jun-Yan Zhu. Multi-concept customizationof text-to-image diffusion. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 19311941, 2023. 2, 3, 8 Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, andJoshua B Tenenbaum. Compositional visual generation withcomposable diffusion models. In European Conference onComputer Vision, pages 423439. Springer, 2022. 2",
  "Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, andYu Qiao.Conditionvideo: Training-free condition-guidedtext-to-video generation. arXiv preprint arXiv:2310.07697,2023. 2": "Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-ing attentions for zero-shot text-based video editing. arXivpreprint arXiv:2303.09535, 2023. 2 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2250022510, 2023. 1, 2, 5 Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,Oran Gafni, et al. Make-a-video: Text-to-video generationwithout text-video data. arXiv preprint arXiv:2209.14792,2022. 1, 2"
}