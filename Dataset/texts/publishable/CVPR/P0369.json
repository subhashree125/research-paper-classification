{
  "Abstract": "We introduce a lightweight and accurate architecturefor resource-efficient visual correspondence. Our method,dubbed XFeat (Accelerated Features), revisits fundamen-tal design choices in convolutional neural networks for de-tecting, extracting, and matching local features. Our newmodel satisfies a critical need for fast and robust algorithmssuitable to resource-limited devices. In particular, accu-rate image matching requires sufficiently large image res-olutions for this reason, we keep the resolution as large aspossible while limiting the number of channels in the net-work. Besides, our model is designed to offer the choice ofmatching at the sparse or semi-dense levels, each of whichmay be more suitable for different downstream applica-tions, such as visual navigation and augmented reality. Ourmodel is the first to offer semi-dense matching efficiently,leveraging a novel match refinement module that relies oncoarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based localfeatures in speed (up to 5x faster) with comparable or betteraccuracy, proven in pose estimation and visual localization.We showcase it running in real-time on an inexpensive lap-top CPU without specialized hardware optimizations. Codeand weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24.",
  ". Introduction": "As a crucial step for many higher-level vision tasks, lo-cal image feature extraction remains a highly active topicof research.Despite the recent advancements, the largeimprovements achieved from recent image matching meth-ods mostly come at the cost of high compu-tational requirements and increased implementation com-plexity. Since image feature extraction is critical for a myr-iad of tasks , efficient solutionsare highly desirable, especially on resource-constrained Frames per second Pose accuracy @ 10 XFeat* XFeat SuperPoint DISK",
  "ALIKE": ". Additional qualitative results on ScanNet-1500 indoor dataset. Our proposed approaches consistently outperformstate-of-the-art methods such as DISK and ALIKE in indoor imagery, both in terms of camera pose and inlier ratio. Notice that SuperPointalso often outperforms DISK and ALIKE. Appendix E provides a detailed discussion on the reasons behind our methods superiority. dense solutions as shown in Tab. 6 and surpasses coarse-to-fine approaches such as Patch2Pix in accuracy, while be-ing faster and delivering many more matches than sparselearned matchers as LightGlue. Naturally, XFeat, as a local descriptor, offers limited robustness to aggressive viewpointchanges and highly ambiguous image pairs compared totransformer-based feature matchers. Coupling a lightweighttransformer such as LightGlue or LoFTRs linear trans-",
  "XFeat*": ".Sparse (top) and semi-dense (bottom) matching.XFeat stands out with its dual ability to perform both sparse andsemi-dense matching, providing fast features for a wide range ofapplications from visual localization with sparse matches to poseestimation and 3D reconstruction where denser correspondencesdeliver additional constraints and a more complete representation. this does not preclude the potential for optimizing XFeat onspecific hardware configurations. Moreover, XFeat is suit-able to perform both sparse feature matching based on key-points and dense matching of the coarse feature map. Thisversatility brings the best of both worlds: keypoint-basedmethods are more suitable to efficient visual localizationbased on Structure-from-Motion (SfM) maps , whiledense feature matching can be more effective for relativecamera pose estimation in poorly textured scenes .Compared with current methods available for image cor-respondence, our method significantly improves the trade-off ratio between matching accuracy and computational ef-ficiency as shown in , outperforming all lightweightdeep learning local feature alternatives by up to 5 in speedwhile being comparable to much larger models as Super-Point and DISK in accuracy. To mitigate compu-tational costs while maintaining competitive accuracy, ourwork brings three main contributions: A novel lightweight CNN architecture that can be de-ployed on resource-constrained platforms and down-stream tasks that require high throughput or compu-tational efficiency, without the requirement of time-consuming hardware-specific optimizations. Our methodcan readily replace existing lightweight handcrafted solu-tions , expensive deep models and lightweightdeep models in several downstream tasks such as vi-sual localization and camera pose estimation; We design a minimalist, learnable keypoint detectionbranch that is fast and suitable for small extractor back-bones, showing its effectiveness in visual localization,camera pose estimation, and homography registration; Lastly, a novel match refinement module for obtainingpixel-level offsets from coarse semi-dense matches is pro-posed. Our new strategy does not require high resolu-tion features besides the local descriptors themselves asopposed to existing techniques , greatly reducingcompute and achieving high accuracy and matching den-sity shown in , and respectively.",
  ". Related Work": "Image matching.Modern image matching techniquesrange from employing classic keypoint detection coupled with deep-learning based description of localpatches , to performing joint key-point detection and description in the sameCNN backbone.More recently, middle-end approaches,known as learned matchers , and also end-to-end semi-dense and dense methods, demon-strated remarkable improvements in robustness and accu-racy for matching wide-baseline image pairs, especiallywith the recent advances introduced by the transformer ar-chitecture . However, recent methods largely empha-size image matching accuracy and robustness, thereby in-flating computational demands to undesired levels, evenfor systems with moderate GPU resources. They requiresignificant adaptations to work efficiently in large-scaledownstream tasks such as visual localization , simul-taneous localization & mapping , and structure-from-motion . In contrast, in this paper we show that it is pos-sible to drastically reduce compute utilization in both sparsekeypoint extraction and pixel-level semi-dense matching,while attaining similar, or even better performances com-pared to more computationally expensive methods. Efficient description & matching.Recent works high-light the growing emphasis on computational efficiency fordescription and matching. SuperPoint proposed a self-supervised CNN for both keypoint detection and descrip-tion.However, one major disadvantage of using Super-Point is that it can still incur significant computational costswhen applied to image sizes that are common for imagematching. SiLK reevaluates elements of learned fea-ture extraction, proposing an effective yet simple strategyfor keypoint and descriptor learning that achieves perfor-mance comparable to existing methods.The key aspectthat underscores SiLKs competitiveness its dependenceon the original image size for descriptor extraction is alsoits main drawback in terms of computational cost, as it sub-stantially slows down inference. ALIKE introduced alightweight network balancing robustness and speed, withdifferentiable keypoint detection and a neural reprojectionloss. Yet, its reliance on the original image resolution inthe final feature map considerably increases memory andcompute footprints. ZippyPoint incorporates quantiza- tion and binarization in a CNN. Although it achieved no-table speed improvements, it requires custom compilationand specific low-level processor arithmetic operations, re-stricting its applicability across diverse hardware.Works considering minimalist CNN architectures mayemploy both fixed handcrafted and learned filters in con-volutional blocks .Beyond feature extraction, recentadvancements in feature matching also highlight the ne-cessity for quick inference speeds. LightGlue speedsup learnable feature matching and maintains high accuracycompared to SuperGlue .Nevertheless, LightGluestransformer-based architecture is still costly for tasks wherecomputational efficiency is critical.In contrast to exist-ing methods, we focus on highly-efficient and robust imagematching for ubiquitous deployment: from resource-limiteddevices such as low-budget boards and embedded systemsto smartphones and cloud applications.",
  ". XFeat: Accelerated Features": "Local feature extraction accuracy heavily depends on inputimage resolution. For instance, in camera pose, visual lo-calization, and SfM tasks, the correspondences should befine-grained enough to allow pixel-level matches.How-ever, feeding high-resolution images into network back-bones increases computational requirements to undesiredlevels even for simple, small network backbones such asSuperPoint VGG-like architecture . In this section,we describe how to reduce significantly the computationalcost using strategies to minimize the computational bud-get while mitigating robustness loss due to a considerablysmaller CNN backbone.",
  ". Featherweight Network Backbone": "Let I RHW C be a gray-scale image, where H is theheight, W the width in pixels, and C = 1 denotes thenumber of channels. To decrease a CNN processing cost,a common approach is to start with shallow convolutionsand then incrementally halve spatial dimensions (Hi, Wi)while doubling the channel count Ci in the i-th convolu-tional block . Assuming a convolutional layer with unitstride, padding, no bias term and square kernel size k k,the cost of convolution in terms of floating point operations(Fops) for the i-th layer can be expressed as:",
  "Fops = Hi Wi Ci Ci+1 k2.(1)": "Naively pruning channels C across the entire network com-promises its capability of handling challenges like varyingillumination and viewpoint as demonstrated in the ablationexperiments (Sec. 4.4).Efficient networks use depthwise separable con-volutions to cut down Fops by up to 9 times (with 3 3kernel size) with fewer parameters than standard convolu-tions. However, in local feature extraction, where shallower networks handle larger image resolutions ,this approach is less effective compared to their original usein low-resolution input scenarios like classification and ob-ject detection . This leads to limited representa-tional capacity and minor speed gains in shallow networksfor local feature extraction.In Eq. (1), the Hi Wi terms emerge as the primarycomputational bottleneck impacting Fops in CNNs. Super-Point and ALIKE reduce channel depth and layercount uniformly to alleviate the problem. We delve intothe core of the issue, formulating a strategy to minimizeearly-layer depth and reconfigure channel distribution, sig-nificantly improving the accuracy-compute trade-off. Ourproposed strategy involves reducing the channel count ininitial convolution layers as much as possible due to thehigh spatial resolution. To counterbalance the parameterreduction, rather than adhering to the traditional VGG-likeapproach of doubling channels, we propose tripling thechannel count as the spatial resolution decreases, until a suf-ficient number of channels is reached (usually 128 for localfeature backbones ). This strategy, marked by atriple rate increase in convolutional depth as spatial resolu-tion halves, effectively redistributes the networks convolu-tional depth. It ensures minimal depth in early layers whilecompensating for the reduced parameter count accross thebackbone. This approach not only significantly reduces thecomputational load in the early stages, particularly for high-resolution images, but also optimizes the networks overallcapacity through more effective management of convolu-tional depth. We found a good trade-off between spatialaccuracy and speedup gains by starting with C = 4 chan-nels and concluding at C = 128 in the final encoder block,achieving a spatial resolution of H/32 W/32.Our networks simplicity is anchored in blocks calledbasic layers, a 2D convolution with kernel sizes from 1to 3, ReLU + BatchNorm, and a stride of 2 for res-olution reduction, forming convolutional blocks, each acomposite of basic layers.The backbone features sixblocks, halving resolution and increasing depth in se-quence:{4, 8, 24, 64, 64, 128}, plus a fusion block formulti-resolution features. More details on architecture arein the supplementary material.",
  "Convolutional block": ". Accelerated feature extraction network architecture. XFeat extracts a keypoint heatmap K, a compact 64-D dense descriptormap F, and a reliability heatmap R. It achieves unparalleled speed via early downsampling and shallow convolutions, followed by deeperconvolutions in later encoders for robustness. Contrary to typical methods, it separates keypoint detection into a distinct branch, using1 1 convolutions on an 8 8 tensor-block-transformed image for fast processing. has demonstrated success in local feature extraction to in-crease robustness to viewpoint changes and akey ingredient for small network backbones to work wellin practice. We merge the intermediate representation atthree different scale levels:{1/8, 1/16, 1/32} by bilinearly up-sampling and projecting all intermediate representations toH/8 W/8 64 followed by element-wise summation. Fi-nally, a convolutional fusion block composed of three basiclayers is used to combine the representations into the finalfeature representation F. An additional convolutional blockis used to regress a reliability map R RH/8W/8, whichmodels the unconditional probability Ri,j that a given localfeature Fi,j can be matched confidently. An overview ofour method is shown in . Keypoint head.In general, backbones for local featureextracion rely on UNets , VGG , and ResNets .The strategy used in SuperPoint offers the fastest ap-proach to extract pixel-level keypoints. It uses features inthe final encoder with 1/8 of the original image resolution,and extracts pixel-level keypoints by classifying the coordi-nate of the keypoint in a flattened 88 grid from the featureembeddings. We adopt a strategy similar to SuperPoint, butwith a major difference. We introduce a novel approachthat employs a dedicated parallel branch for keypoint de-tection focused on low-level image structures. As shownin the ablation experiments (Sec. 4.4), by jointly training adescriptor and a keypoint regressor within a single neuralnetwork backbone significantly degrades the performanceof semi-dense matching for compact CNN architectures.Our key insight lies in the efficient utilization of the low-level features through a minimalist convolutional branch.To maintain spatial resolution without sacrificing speed, werepresent the input image as a 2D grid comprised of 8 8pixels on each grid cell, and we reshape each cell into 64- dimensional features. This representation preserves spatialgranularity within individual cells, while exploiting rapid1 1 convolutions for regressing keypoint coordinates. Af-ter four convolutional layers, we obtain a keypoint embed-ding K RH/8W/8(64+1) encoding the logits of keypointdistribution inside a cell ki,j K, and classify the keypointas one of the 64 possible positions inside ki,j R65 plus adustbin to consider the case where no keypoint is found .During inference, the dustbin is discarded and the heatmapis re-interpreted as an 8 8 cell. depicts the entireprocess of the Keypoint Head. Dense matching.Recent research has demon-strated the benefits of dense image region matching, im-proving coverage and robustness.Our work proposes alightweight module for dense feature matching, differingfrom other detector-free methods in two ways.Firstly,we can control memory and compute footprint by select-ing top-K image regions according to their reliability scoreRi,j and caching them for future matching. Secondly, wepropose a simple and lightweight Multi-Layer Perceptron(MLP) to perform coarse-to-fine matching without high-resolution feature maps , enabling us to performsemi-dense matching in resource-constrained settings.Given the dense local feature map F, which is at 1/8 ofinput spatial resolution, or a subset Fs F, we proposea simple refinement strategy to recover pixel-level offsets.Let fa F1 and fb F2 be two matching features obtainedby traditional nearest neighbor matching from an image pair(I1, I2). We predict offsets o = MLP(concat(fa, fb)), clas-sifying the offset (x, y) that leads to the correct pixel-levelmatch at original image resolution:",
  "x 8 MLP": ". Match refinement module for dense matching set-ting. This module learns to predict pixel-level offsets by onlyconsidering as input pairs of nearest neighbors from the originalcoarse-level features at 1/8 of original spatial resolution, signifi-cantly saving memory and compute. where o R88 has the logits of a probability distributionover the possible offsets.The match refinement module is trained in an end-to-endmanner alongside the backbone network, ensuring that theintermediate feature representation retains fine-grained spa-tial details within a compact embedding space. The offsetprediction is conditioned on the coarsely matched featurepair (fa, fb), reducing the search space. illustrates thelightweight match refinement module.",
  ". Network Training": "We train XFeat in a supervised manner with pixel-levelground truth correspondences.We assume image pairs(I1, I2) with N matching pixels MI1I2 RN4, wherethe first two columns of MI1I2 encode the (x, y) coordi-nates of the points in I1, and the last two columns for I2. Learning local descriptors.To supervise the local fea-ture embeddings F, we employ the negative log-likelihood(NLL) loss. Descriptor sets F1 and F2 are sampled fromthe dense maps F(,), and each is represented in RN64,comprising N 64-dimensional descriptors. The i-th rowsF1(i, ) and F2(i, ) correspond to two descriptors of thesame point from I1 and I2 respectively. Then, the similar-ity matrix S RNN is obtained by: S = F1FT2 . Giventhe symmetry of matching, we take both matching direc-tions , resulting in the dual-softmax loss Lds, where thesimilarity measure of corresponding features lie in the maindiagonal Sii of S and softmaxr is performed row-wise:",
  "Learning reliability.We supervise the reliability mapduring training by interpreting the dual-softmax probabil-": "ity as a confidence measure, denoted as R RN.R1and R2 are obtained by matching F1 and F2 with thedual-softmax strategy:R1 = maxr(softmaxr(S)), andR2 = maxr(softmaxr(ST)), similarly to Eq. (3). As thetraining progresses, intuitively, distinct features will havehigh confidence matching probability. Thus, we supervisethe reliability map directly with the L1 loss given the dualsoftmax scores R1 and R2:",
  "where is the sigmoid activation function and theHadamard product. Note that for the reliability loss Lrel,we only backpropagate the gradients through R": "Learning pixel offsets.The match refinement moduleis supervised with pixel-level offsets obtained from theground-truth correspondences MI1I2 at the original inputimage resolution. We also employ the NLL loss over thelogits o described in Sec. 3.2. During training, correspond-ing descriptors F1(i, ) and F2(i, ), together with theirground-truth offset (x, y) are obtained using MI1I2(i, ),and the fine matching loss Lfine becomes:",
  "ilog(softmax (oi))yi,xi.(5)": "Learning keypoints.Our keypoint detection branch isminimalist by design. Whilst it is possible to supervise thekeypoint head with existing keypoint losses ,we chose to employ knowledge distillation from a largerteacher network to facilitate its learning.We opted forALIKE keypoints obtained from its tiny backbone tosupervise our model. This choice is strategic, as the smallerbackbone tends to concentrate on lower-level image fea-tures like corners, lines, and blobs, aligning well with ourdesigned detector branch, given its limited receptive fieldsize of 8 8 pixels. Given the keypoint raw logit mapK RH/8W/8(64+1), we map keypoint coordinates fromthe teacher network (tx, ty) inside each cell ki,j R65 tolinear index tidx = (tx + ty 8),tidx {0, 1, ..., 63}. Tosupervise the dustbin, when no keypoint is detected inside acell ki,j, we set tidx = 64. During training, we set an upperlimit of samples for the no keypoint case to avoid class im-balance. Finally, the NLL loss is employed to compute thekeypoint loss Lkp:",
  ". Experiments": "We evaluate XFeat on relative camera pose estimation, vi-sual localization, and homography estimation.We alsopresent ablations to justify our design decisions, and a com-prehensive runtime analysis in a GPU-free setting.Training. XFeat was implemented on PyTorch andtrained on a blend of Megadepth and syntheticallywarped COCO images, using a 6:4 ratio, with im-ages resized to (W = 800, H = 600). Hybrid trainingwas found to enhance generalization in our experiments(Sec. 4.4), aligning with recent findings . The traininginvolved batches of 10 image pairs using the Adam opti-mizer , leading to convergence within 36 hours on anNVIDIA RTX 4090 GPU. Further details on computationalresource utilization and hyperparameter specifics are pro-vided in the supplementary material.XFeat inference.We considered two settings: Sparse(XFeat) and semi-dense matching (XFeat), both utilizingthe same pretrained backbone. In XFeat, we extracted upto 4,096 keypoints from the keypoint heatmap K, usingtheir scores derived from the keypoint and reliability con-fidences: score = Ki,j Ri,j. Local features were thenbicubically interpolated from F at these keypoint locationsand matched with Mutual Nearest Neighbor (MNN) search.For XFeat, we enhanced features by processing images at 2different scales (0.65 and 1.3, resizing the image internallyafter receiving the input), retaining the top 10,000 featuresaccording to their reliability. We used MNN search and off-set refinement to match the features, retaining only thosewith offset prediction confidence above 0.2.Baselines. Among the selected baselines, DISK sets ahigh benchmark in accuracy at the cost of increased com-putational demand. This is followed by SiLK , Super-Point , ZippyPoint , and ALIKE . For SiLK andALIKE, we opted for their smallest available backbones ALIKE-Tiny and VGGnp- aligning with our focus onmodels emphasizing compute efficiency. Finally, ORB represents the upper limit in terms of speed. Thus, we eval-uate XFeat against the current state-of-the-art through a di-verse set of baselines covering the spectrum of computa-tional expense and accuracy. We use the top 4, 096 detectedkeypoints for all baselines, except for those marked with, where the top 10, 000 keypoints are used. For matching,MNN search is employed. ZippyPoint model was used in itsform as provided by the authors without hardware-specificcompilation, due to the lack of clear instructions.",
  ". Relative pose estimation": "Setup. Megadepth and ScanNet test sets are usedas in previous works , providing camera poses onscenes that do not overlap with our training set. The scenescontain significant viewpoint and illumination changes si-multaneously and present repetitive structures, posing a sig- nificant challenge. LO-RANSAC is used to estimatethe essential matrix. We search for the optimal thresholdfor each method, and resize the images such that the max-imum dimension becomes 1,200 pixels for Megadepth anduse the default (VGA) resolution for ScanNet.Metrics. We use the area under the curve (AUC) at thresh-olds of {5, 10, 20} . Additionally, we report theAcc@10, which is the proportion of poses where the max-imum angular error is below 10 degrees, the mean inlier ra-tio (MIR), which is the ratio of matching points that complywith the estimated model after RANSAC, and the numberof inlier points (#inliers). Finally, we measure the framesper second (FPS) of each method on a budget-friendly lap-top without GPU and an Intel(R) i5-1135G7 @ 2.40GHzCPU. We also indicate whether the descriptor is floating-point (denoted by f) or binary-based (denoted by b) and re-port the descriptor dimensionality (dim).Results.Tab. 1 shows the metrics on the relative camerapose estimation task on Megadepth-1500. Our method ismuch faster (5) than the fastest available learning-basedsolution (ALIKE) and achieves competitive results in thesparse setting on several metrics.Moreover, it can de-liver state-of-the-art results for the dense matching config-uration using 10, 000 descriptors on AUC@20, Acc@10 and MIR in a fair comparison with DISK, a much heaviermodel, considering the same number of descriptors. shows examples where XFeat stands out over existing solu-tions. Our method also allows more efficient matching withlow-dimensional descriptors (64-f) compared to DISK andSuperPoint. Detailed timing analysis is provided in the sup-plementary material alongside additional quantitative com-parison with recent popular learned matchers .It is worth mentioning that we obtain state-of-the-art resultsin more loose thresholds due to the requirement of interpo-lating the descriptors and predicting offsets at coarser reso-lution. Tab. 2 shows AUC values for the most competitivemethods in ScanNet-1500 indoor imagery. Notice that noneof the methods were retrained. DISK and ALIKE showsigns of bias towards landmark datasets, while our approachdemonstrates superior generalization. A more detailed dis-cussion and qualitative results for ScanNet and Megadepthare available in the supplementary material.",
  "Metrics.We followed ALIKE protocol and esti-mated Mean Homography Accuracy (MHA). We used pre-": ". Megadepth-1500 relative camera pose estimation. Our method achieves superior performance compared to other lightweightmethods, while also outperforming SuperPoint at 9 speedup, and with comparable results to DISK at 16 speedup. denotes 10kkeypoints. FPS is the average of 30 frames standard deviation computed in VGA resolution. Best in bold, second best underlined,separated by method class (standard/fast). + indicates code used as provided by authors without hardware optimization.",
  "@512.59.6 / 11.39.08.016.7 / / 22.318.516.432.6 / / 33.929.925.947.8 / 50.3": "defined thresholds of {3, 5, 7} pixels. The accuracy wascomputed considering the average corner error in pixels bywarping the four reference image corners to target imagesusing the ground-truth homography and estimated one. Results. Tab. 3 shows that our method is on par with themost accurate descriptors, reinforcing the robustness of ourproposed keypoint and descriptor heads. In contrast, theperformance of other lightweight solutions as ORB andSiLK are heavily compromised on the illumination and viewpoint splits, due to their limited capacity in handlingagressive viewpoint and illumination changes present in thehardest image pairs. Our method also stands out for lessstrict thresholds, as discussed in Sec. 4.1 Results.",
  ". Visual localization": "Setup. The hierarchical localization pipeline HLoc isused to localize images of day and night scenes from theAachen dataset . Given the provided keypoint corre-spondences, HLoc triangulates an SfM map using the avail-able ground-truth camera poses. A separate set of queryimages are then localized within the 3D map using the key-point matches. For a fair comparison, we resize the imagessuch that maximum dimension is held at 1, 024 pixels, andextract the top 4, 096 keypoints for all approaches.",
  "within thresholds of position errors {0.25m, 0.5m, 5m}and rotation errors {2, 5, 10} respectively": "Results. presents the results of the visual local-ization experiment. Our method demonstrates similar per-formance to leading approaches as SuperPoint and DISK,while achieving a significant speed advantage, being at least9 times faster and with a more compact descriptor. Thesefindings challenge the prevailing trend in the literature toemploy large and more intricate models for downstreamtasks. Contrarily, they underscore the efficacy of simplermodels that not only match accuracy but also offer the ben-efits of efficient operation on resource-constrained systems.",
  "Default42.650.2(i) No synthetic data41.533.9(ii) Smaller model37.440.7(iii) Joint keypoint extraction42.939.7(iv) No match refinement-38.6": "both real and synthetic warps is beneficial, especially forthe dense matching setting. Second, we evaluate if we canfurther reduce channel count in the network (ii). We halvethe channels of the last three convolutional blocks to 32 in-stead of 64, but performance significantly degrades for bothsparse and dense settings. We also demonstrate the ratio-nale behind devising a parallel branch for keypoint detec-tion. Without the proposed keypoint head (iii), an additionalconvolutional block is used on top of the output descriptorembeddings akin to SuperPoint. As shown in Tab. 5, XFeat experiences degradation in performance when trained underthis specific setup, since the limited network size constrainsthe capacity of intermediate embeddings, rendering themless effective for semi-dense matching in non-repeatable re-gions, adversely affecting the match refinement task. Thus,we opted to design a parallel branch which offers greattrade-off between sparse and dense matching as shown inTab. 5 Default and (iii). Lastly, we evaluate the benefitsof our proposed match refinement module. For XFeat, thematch refinement step is critical for enhancing accuracy. Inour benchmarks, this module incurs only an additional 11%inference cost compared to MNN matching for an averageof 10,000 descriptors. Please check the supplementary ma-terial for a thorough timing analysis.",
  ". Conclusion": "This work introduced XFeat, a lightweight CNN architec-ture for accelerated feature extraction, applicable to bothsparse and semi-dense image matching. With experimentson three different tasks and ablation analyses, we showedthat it is possible to achieve fast and accurate image match-ing without resorting to advanced low-level hardware op-timizations. This stands in contrast to the prevalent trendof deploying increasingly large and convoluted models. Webelieve XFeat paves the way for next-generation applica-tions in augmented reality and mobile robotics, where ef-ficient and general data-driven solutions remain crucial forreal-world deployment, particularly in mobile applications.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition.In CVPR,pages 770778, 2016. 3": "Andrew G Howard, Menglong Zhu, Bo Chen, DmitryKalenichenko, Weijun Wang, Tobias Weyand, Marco An-dreetto, and Hartwig Adam.Mobilenets: Efficient con-volutional neural networks for mobile vision applications.arXiv:1704.04861, 2017. 3 Menelaos Kanakis, Simon Maurer, Matteo Spallanzani, AjadChhatkuli, and Luc Van Gool. Zippypoint: Fast interest pointdetection, description, and matching through mixed preci-sion discretization. In CVPRW, pages 61136122, 2023. 1,2, 7",
  "Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,and Bohyung Han. Large-scale image retrieval with attentivedeep local features. In ICCV, pages 34563465, 2017. 1": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, Zem-ing Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:An imperative style, high-performance deep learning library.NeurIPS, 32, 2019. 6 Guilherme Potje, Gabriel Resende, Mario Campos, and Er-ickson R Nascimento. Towards an efficient 3d model estima-tion methodology for aerial and ground images. Mach. Vis.and Applications., 28:937952, 2017. 1",
  "[Supplementary Material]XFeat: Accelerated Features for Lightweight Image Matching": "In this supplementary material accompanying the mainpaper, we present a more detailed overview of the archi-tecture of our proposed CNN backbone and the practicesemployed in the training process. Moreover, we providean expanded set of qualitative results and extended discus-sion, providing additional contextualization with the currentstate-of-the-art methods. Code and weights are available atverlab.dcc.ufmg.br/descriptors/xfeat cvpr24.",
  "A. Backbone details": "To maintain the backbones structural simplicity, we employa primary unit termed the basic layer. This unit is structuredwith a 2D convolution with square kernel sizes k = 1 ork = 3, complemented by ReLU activation and Batch Nor-malization. A stride of 2 in the convolution is applied forhalving the spatial resolution as needed. The networks ar-chitecture is modular, comprising several basic layers as abasic block, as depicted in . Each block consists oftwo or three basic layers. The backbone of our networkcomprises six of these basic blocks, designed to halve thespatial resolution in each step while progressively augment-ing the depth using the approach detailed in Sec. 3.1 of themain paper. The first basic layer on each block performs thespatial downsampling. Two additional basic blocks, in the",
  "Skip Connection(AvgPool 4x4 + 1x1 conv)": ". Detailed descriptor backbone. Our backbone is com-prised of 23 convolutional layers, following the downsamplingstrategy described in Sec. 3.1 of the main paper. Our networkis deeper compared to ALIKE and SuperPoint backbonesin terms of layers, but due to the efficient downsampling strategyadopted, our networks inference is much faster. end, are employed to perform the fusion of multi-resolutionfeatures and reliability map prediction, respectively. Pre-liminary experiments revealed that adding a single skip con-nection to the model as shown in slightly increasedperformance, which has led to its incorporation in the finalbackbone design.",
  "B. Training description": "We trained the network on a mix of Megadepth scenesusing the training split provided by and syntheti-cally warped pairs using raw images (without labels) fromCOCO in the proportion of 6 : 4 respectively. Allimage pairs were resized to (W= 800, H= 600),and ground-truth correspondences were scaled accordingly.Our ablations show that hybrid training significantly im-proves generalization for small CNNs, as observed in high-capacity models . The network was trained on batchesof 10 image pairs using the Adam optimizer with aninitial learning rate of 3 104, applying an exponentialdecay of 0.5 at every 30,000 gradient updates.Conver-gence is attained after 160,000 iterations, within 36 hourson a single NVIDIA RTX 4090 GPU, consuming 6.5 GBof VRAM in total, considering both training and syntheticwarps done on the fly on GPU. Disk I/O is the predomi-nant speed bottleneck due to the overhead of loading im-ages and depth maps from the Megadepth dataset in theiroriginal resolution, which can be easily solved with a morecareful data preparation scheme. The low memory usageof our method enables training on entry-level hardware, fa-cilitating the fine-tuning or full training of our network forspecific tasks and scene types.",
  "C. Detailed timing analysis": "This section reports a detailed timing analysis of our pro-posed solutions in sparse and semi-dense matching settings.Regarding XFeats match refinement step, we show in that the match refinement cost is negligible. Morenotably, even with the refinement step included, XFeat achieves a similar matching time compared to XFeat withthe same number of keypoints because refinement is per-formed after the nearest neighbor search. Additionally, wepresent the extraction running times for the most efficientmethods available on an Orange Pi Zero 3 equipped with aCortex-A53 ARM processor. This device stands out as oneof the smallest and most affordable consumer-grade em-bedded computers ($28). Considering its limited process-ing power, we adjusted the input resolution to 480 360 XFeat-jointXFeat*-jointXFeatXFeat*",
  ". Detailed timing analysis on i7-6700K CPU. Required time by each step of our ablated methods": "for all methods and used their standard PyTorch imple-mentation without any deployment optimization. Our find-ings show that XFeat operates at an average of 1.8 FPS,SuperPoint at 0.16 FPS, and ALIKE at 0.58 FPS, respec-tively. This experiment shows that XFeat is the only learnedmethod capable of running over one FPS on a highly con-strained embedded device that is not optimized for neuralnetwork inference.",
  "D. Megadepth-1500 qualitative results": "shows more qualitative results of our two proposedapproaches compared to the baseline methods used in themain paper.For more challenging cases such as strongviewpoint and illumination changes, XFeat and XFeat ex-hibit exceptional robustness even compared to DISK the largest CNN architecture regarding floating pointoperations.We hypothesize that this robustness is at-tributed to our networks large receptive field and depthcompared to shallower models such as SuperPoint, ALIKE,and SiLK , demonstrating the effectiveness of our feath-erweight backbone in the compute-accuracy trade-off.",
  "E. ScanNet-1500 extended discussion": "Recalling the results obtained in Tab. 2 of the main pa-per, XFeat and XFeat surpass both fast and standard lo-cal feature extractors in pose accuracy while being signif-icantly faster for indoor relative pose estimation.DISKand ALIKE, which were trained in the same Megadepthscenes as XFeat, display signs of overfitting in landmarkimagery: they perform exceptionally well in strict thresh-olds (AUC@5) on Megadepth-1500 test set, but their rel-ative performance are similar or worse in tasks such as ho-mography estimation and visual localization compared toXFeat and SuperPoint, as one can observe in Tab. 3 andTab. 4 of the main paper.We conjecture that XFeat produces less biased local de-scriptors due to our hybrid training with synthetic warpson COCO. SuperPoint also demonstrate increased gener- alization accross different downstream tasks and datasetsdue to its inherent self-supervised training strategy on syn-thetic warps. Hybrid training can encourage local featurerepresentations to focus less on distinctive textures oftenpresent in landmark outdoor imagery that could bias theCNN training. In addition, the large receptive field of ournetwork, as well as its increased layer depth compared tothe other approaches, helps XFeat in indoor imagery (whichoften lacks distinctiveness at the local level), resulting inmore consistent matches compared to DISK and ALIKEin ScanNet-1500, even though XFeat and the competitorswere not trained on ScanNet data.",
  "Since XFeat uses paired inputs when performing the re-finement step, we provide additional comparisons of XFeat": "(semi-dense matching) with popular learned matchers suchas LoFTR and LightGlue , and coarse-to-finestrategies as Patch2Pix , to elucidate the key differ-ences.The results for these new approaches are shownin Tab. 6.Although XFeat needs paired inputs for re-finement, it fundamentally differs in its methodology fromlearned matchers, being only comparable to Patch2Pix, aswe rely on traditional nearest neighbor search for matching,followed by a lightweight refinement of matches, incurringa negligible computational load (see ). The require-ment for paired inputs does not change the usual pipelinefor SfM and visual localization tasks because XFeats fea-tures can be stored for each image independently, as usu-ally done for sparse settings. For instance, high-resolutionfeature maps are not required, unlike LoFTR, to producerefined matches.Our techniques are, in fact, complementary to learnedmatchers; for example, LightGlue can be trained using bothXFeat and XFeat features.Learned matchers are moredata hungry and much more expensive to train, e.g., LoFTRuses 64 GPUs for 24 hours to be trained.XFeat, forits turn, can be trained on a single 8 GB GPU. Further-more, XFeat offers up to 22 speedup over existing semi-",
  "(a) sparse matching with XFeat(b) semi-dense matching with XFeat*": ". Additional qualitative results on Megadepth-1500 landmark dataset. XFeat and XFeat are robust in demandingscenarios with significant viewpoint and illumination variations, outperforming even the more computationally intensive DISK model insemi-dense matching with 10,000 local features at a striking 16 speedup. In a sparse setting with 4,096 keypoints, our method, whichis many times faster than ALIKE (5) and SuperPoint (9), demonstrates more robustness to wide baseline transformations due to theeffective re-formulation of XFeats backbone CNN."
}