{
  "Abstract": "Document image restoration is a crucial aspect of Doc-ument AI systems, as the quality of document images sig-nificantly influences the overall performance. Prevailingmethods address distinct restoration tasks independently,leading to intricate systems and the incapability to harnessthe potential synergies of multi-task learning. To overcomethis challenge, we propose DocRes, a generalist modelthat unifies five document image restoration tasks includ-ing dewarping, deshadowing, appearance enhancement,deblurring, and binarization. To instruct DocRes to per-form various restoration tasks, we propose a novel visualprompt approach called Dynamic Task-Specific Prompt(DTSPrompt).The DTSPrompt for different tasks com-prises distinct prior features, which are additional char-acteristics extracted from the input image. Beyond its roleas a cue for task-specific execution, DTSPrompt can alsoserve as supplementary information to enhance the modelsperformance. Moreover, DTSPrompt is more flexible thanprior visual prompt approaches as it can be seamlessly ap-plied and adapted to inputs with high and variable res-olutions.Experimental results demonstrate that DocResachieves competitive or superior performance compared toexisting state-of-the-art task-specific models. This under-scores the potential of DocRes across a broader spectrum ofdocument image restoration tasks. The source code is pub-licly available at",
  ". DocRes is a generalist model that unifies five documentimage restoration tasks, including tasks of dewarping, deshadow-ing, appearance enhancement, deblurring, and binarization": "ments, as underscored in recent studies .The restoration of document images, addressing ob-jectives like flattening documents, shadow removal, cleanappearance restoration, deblurring, or text segmentation,holds both academic and practical significance. Existing ap-proaches typically treat these restorationtasks separately, relying on models specifically designed foreach task. While effective in achieving commendable per-formance, this paradigm results in a system that requiresthe design of multiple models and extensive maintenance.Moreover, it fails to leverage the potential synergies ofmulti-task learning.To tackle this problem, motivated by recent pioneeringworks that unify various vision tasks,we seek to explore a generalist model for document imagerestoration. As illustrated in , our proposed DocResseeks to unify five document image restoration tasks: de-warping, deshadowing, appearance enhancement, deblur-ring, and binarization. To empower such a generalist modelto perform specific tasks and generate desired outputs, itis often crucial to convey instructional information to themodel. Existing visual generalist models accomplish thisby drawing inspiration from advancements in Natural Lan-guage Processing (NLP) and leveraging prompt learning",
  "arXiv:2405.04408v1 [cs.CV] 7 May 2024": "techniques. Notably, studies like transform thevision task into an NLP one by discretizing continuous vi-sion output and using discrete tokens as prompts. However,the autoregressive decoding paradigm they employ is inher-ently less effective for low-level tasks.In the realm of unifying image-to-image tasks, recentmethods have suggested vision-centricprompts, known as visual prompts, which exhibit promis-ing potential across various vision tasks, including imagerestoration. Among them, approaches like pro-pose using a pair of input/output samples as a visual promptto guide the model to perform the corresponding task.Nevertheless, these methods exhibit reduced efficiency asthey necessitate an additional pair of samples during in-ference, making them less suitable for low-level tasks in-volving high-resolution images. ProRes introduces amore straightforward visual prompt method, wherein a ma-trix composed of learnable parameters, matching the inputsshape, is assigned for each task and added to the input as avisual prompt. However, the training process of ProRes isintricate, which requires initializing each visual prompt bypre-training on task-specific models. Additionally, some ofthe above visual prompt methods rely on Mask Image Mod-eling (MIM) and require the ViT framework. However,ViT typically demands that input images during testing andtraining have the same resolution, and due to memory con-straints, ViT struggles with high-resolution images. Thismakes it challenging for these methods to adapt to low-leveltasks, which typically involve patch training, whole-imagetesting, as well as variable and high-resolution inputs.Recognizing the limitations of the aforementionedprompt methods, we introduce a new visual prompt forDocRes called the Dynamic Task-Specific Prompt (DT-SPrompt).The inspiration behind DTSPrompt is rootedin prevalent practices observed in existing document im-age restoration endeavors, where certain prior features ex-tracted from the input image are typically employed to en-hance models performance, such as background images forshadow removal and text content masks for dewarp-ing . Specifically, the DTSPrompt for differenttasks comprises distinct prior features based on the charac-teristics of each task, where prior features we adopted in-clude document segmentation masks, binarization results,gradient maps, and so on. DTSPrompt can not only serveas an effective cue for the model to determine which taskto perform but also function as supplementary informa-tion to enhance the performance for the corresponding task.DTSPrompt can be seamlessly applied to various existingrestoration networks, rather than limited to ViT. This en-ables DocRes to handle high and variable resolutions en-countered in document restoration.Experiments demonstrate that, without additional com-plex network designs, DocRes, as a generalist model, achieves competitive or even superior performance on vari-ous benchmarks compared to existing well-established andcarefully designed task-specific methods.In summary, our contributions are as follows:",
  ". Document Image Restoration": "Dewarping addresses the elimination ofgeometric distortions such as curves and crumples, whichnot only hinder OCR engine performance butalso degrade document readability. Deshadowing focuses on removing shadows, a common occurrencein photographed document images, to produce shadow-freedocuments. Appearance enhancement, also known as illu-mination correction , goes beyond specific ap-pearance degradations, striving to restore a clean appear-ance akin to digital-born PDFs. This is valuable as it signif-icantly enhances document readability and aesthetics. De-blurring aims to eliminate blurriness and restorea clear image. Binarization involves segmenting fore-ground text from document images, a critical task for appli-cations primarily focused on text content, usually obscuredby stains, artifacts, black margins, or weak contrast.Existing approaches typicallytreat these restoration tasks independently, resulting in acomplex document image restoration system and the inabil-ity to harness the potential synergies among tasks. Whilerecent efforts aim to tackle several tasks witha unified network architecture, they still require individualtraining and separate models for each task.",
  "whether the task is explicitly specified for the model duringinference: task-agnostic and task-oriented.Task-agnostic. Task-agnostic methods [21, 22, 40, 55,": "69] do not require users to specify the task type, but theyare less flexible and cannot handle a broader range of tasks,typically being confined to specific domains like weathereffect removal. This arises because some tasks share simi-lar inputs but demand distinct outputs, leading to ambiguityin the learning process. The task-agnostic setup proves in-appropriate for unifying document image restoration sincetasks like dewarping, deshadowing, and appearance en-hancement share similar inputs but demand distinct outputs.Task-oriented.To achieve a task-oriented generalistmodel, explicit task information needs to be introduced.Some approaches discretize the con-tinuous vision output, use discrete tokens as task prompts,and use the Transformer decoder for autoregressive predic-tion. These methods are more suited for visual understand-ing tasks, such as detection, captioning, and visual questionanswering but are inappropriate for low-level tasks involv-ing high-resolution outputs. More recently, there have beenmethods proposing vision-centric prompts for unification purposes, known as visual prompts, show-ing promising potential in various vision tasks, includingimage restoration.Among them, use an in-put/output sample of a specific task as a prompt, resemblingthe paradigm used in inpainting tasks where surroundingpixel information is learned to fill in missing pixel positions.Nevertheless, the additional input leads to inefficiency andlimitation of image resolution. ProRes employs learn-able parameters with the same shape of the input image as avisual prompt and adds it pixel-wise to the input. While ef-fectively guiding the model, the prompt for each task needsto be trained on task-specific models for initialization, re-sulting in a complex training pipeline.Moreover, all these visual prompt methods mentionedabove are limited to the ViT framework, making themunable to adapt to the variable resolutions in restorationtasks.The quadratic computational complexity also re-stricts the input resolution. For example, the input resolu-tion for ProRes and Painter is limited to 448448,which is insufficient for document images with resolu-tions commonly exceeding 1K. While block-wise process-ing could be employed, many document image restorationtasks heavily depend on global information, such as de-shadowing and binarization, making this strategy unfeasi-ble. In contrast, our DTSPrompt method is not confinedto the ViT framework and can be applied to various moreflexible restoration networks to form our DocRes model.",
  "As depicted in , the input document image undergoesan initial processing step by the DTSPrompt generator to": "generate task-specific DTSPrompts, which are composed ofvarious prior features extracted from the input image. SuchDTSPrompts are instrumental in guiding the restoration net-work to execute distinct tasks while simultaneously enhanc-ing overall performance. In the following sections, we firstprovide a detailed exploration of the process involved in ob-taining various prior features to construct the DTSPromptfor each task. Then we introduce our prompt fusion ap-proach and the selection of the restoration network.",
  "DTSPrompt = G (Is, task ) Rhw3.(2)": "Here, G represents our DTSPrompt generator (depicted in), which is responsible for extracting prior featuresfrom Is based on the specified task. In the following, weprovide a detailed explanation of how the DTSPrompt foreach task is constructed. Visual results of DTSPrompts foreach task are presented in .Dewarping. Existing document dewarping methods [11, 17, 23] often use text line masks or text block masks to as-sist the dewarping model, making it more attentive to thedewarping of regions with meaningful content. Documentmask is commonly employed to enhance themodels understanding of document boundaries and reducethe learning difficulty by decoupling the margin removaland content rectification processes. Here, we choose thesimplest document mask as our prior feature for the dewarp-ing task. This mask, denoted as Pm(Is) Rhw, is ob-tained by directly using an existing document segmentationmodel proposed in (noted that any other existing docu-ment segmentation models can also be employed).Furthermore, considering that predicting the backwardmap in dewarping task is inherently a problem related tocoordinate positions, inspired by , we introduce thex-coordinate and y-coordinate as additional prior features,denoted as Pcx Rhw and Pcy Rhw, respectively.They represent the coordinate values of the pixel at (i, j),i.e., Pcx(i, j) = i and Pcy(i, j) = j, which enable bet-ter perception of positional information. The DTSPromptfor dewarping task is obtained by concatenating these priorfeatures along the channel dimension:",
  "DTSPrompt": ". The overall pipeline for DocRes. The document image to be restored, denoted as Is, is initially fed into the DTSPrompt generator,which extracts specific prior features based on the task to form the DTSPrompt. Alongside Is, DTSPrompt is input into the restorationnetwork. It serves not only as a guidance for the restoration network on the particular task to be performed but also functions as auxiliaryinformation derived from Is to improve performance.",
  ". The DTSPrompt for different tasks is composed of dis-tinct prior features. Most of these prior features are extracted fromthe input image, making them dynamic. Zoom in for the best view": "the document background with shadows as our prior fea-ture. To obtain this background from the document image,we initially employ dilation operations to eliminate the tex-tual content within the document. Subsequently, we usea median filter to smooth out artifacts introduced due toincomplete removal. We represent this whole process asPbg(Is) Rhw3. The DTSPrompt for the deshadowingtask is thus formulated as",
  "G (Is, deshadow) = Pbg (Is) .(4)": "Appearance enhancement. Current methods commonly adopt background light, shadow map, or white-balance kernel as prior features to facilitate appearance en-hancement based on the concept of intrinsic images. How-ever, accurately obtaining these prior features is challengingand usually requires an extra model for training and predict-ing. For simplicity, we leverage the discrepancy betweenthe original image and the document background Pbg(Is)as our prior feature for this task, which can serve as initialenhancement guidance for the model:",
  "G (Is, appearance) = Pdiff (Is) .(6)": "Deblurring.The gradient distribution is a prior fea-ture widely used in traditional optimization-based deblur-ring methods , which is typically employed as regular-ization information to constrain the solution space of theoptimization function. Here, we use the gradient map of theinput image Pg (Is) Rhw as an additional input, aimingfor the model to implicitly learn gradient prior information,rather than utilizing a gradient distribution prior to constrainthe solution space of the output. The DTSPrompt for the de-blurring task is expressed as",
  "G (Is, deblur) = [Pg (Is) , Pg (Is) , Pg (Is)] .(7)": "Binarization. Integrating prior features as supplemen-tary for performance enhancement is widely adopted in bi-narization task , and the effectiveness has beenextensively demonstrated.For this task, we employ theSauvola binarization algorithm to yield the initial bi-narization outcome and threshold map as our prior features,which is denoted as Pb(Is) Rhw and Pt(Is) Rhw,",
  ". Prompt fusion and restoration network": "Exploring how to integrate the acquired prompt informationinto the network holds significant merit. A well-conceivedfusion method has the potential to substantially enhanceoverall performance. However, the primary focus of thispaper lies in evaluating the efficacy and potentials associ-ated with DTSPrompt, rather than delving into the intrica-cies of designing a complex network structure. In line withthis objective, we adopt a straightforward fusion approachto seamlessly incorporate DTSPrompt into the restorationnetwork. Specifically, we opt for concatenating DTSPromptand Is along the channel dimension to construct a new input Rhw6 for the restoration network.Due to the simplicity of DTSPrompt, we have the flexi-bility to choose from various restoration networks. In thiscase, we opt for the off-the-shelf Restormer withoutmodifications to form our DocRes. With such a restorationnetwork, DocRes can support inputs of up to 1600 1600and adapt to inputs with variable resolutions.Its note-worthy that other restoration networks can be employed in-terchangeably since DTSPrompt does not require specificmodules within the network.",
  ". Datasets": "Dewarping.We adopt the Doc3D dataset and theDIR300 benchmark for the training and testing, respec-tively. Doc3D is a synthetic dataset comprising 100K sam-ples, which includes geometrically distorted document im-ages and corresponding backward maps. DIR300 is a real-world benchmark with 300 geometrically distorted imagesand corresponding flat ground-truths.Deshadowing. The training set for this task consists of14,200 synthetic images from FSDSRD and 4,371 realimages from the training set of RDD . We use Jungsdataset (87 images), Kliglers dataset (300 images)and OSR (237 images) to form our testing set.Appearance enhancement. The training set for this taskcontains 90K synthetic images from the Doc3DShade dataset and 450 real-world images from the RealDAE training set. 150 images from the testing set of RealDAEand 130 images from DocUNet are used for evalua-tion. As introduced in , the degraded images in Do-cUNet should be aligned to the flat ground truths before theevaluation of this task. Following , we achieve align-ment by using the document alignment model rather than some dewarping models , which can resultin better alignment and thus provide a more accurate evalu-ation. We denote the aligned dataset as DocUNet*.Deblurring. The Text Deblur Dataset (TDD) con-sists of 66K training samples, from which we randomly se-lect 40K samples to train our model. The 1.6K testing sam-ples from TDD form the testing set of this task.Binarization.We use DIBCO18 as our testingset. Following , the remaining years of (H)-DIBCOdatasets are used as the training data,and images from Noisy Office dataset , SynchromediaMultispectral dataset , Persian Heritage Image Bina-rization dataset and Bickley Diary dataset are alsoused for training.",
  ". Evaluation metrics": "Deshadowing, appearance enhancement, and deblurringtasks adopt the commonly used PSNR and SSIM as eval-uation metrics. The evaluation of dewarping incorporatesmulti-scale structural similarity (MS-SSIM) , local dis-tortion (LD) and align distortion (AD) . MS-SSIMbuilds upon the traditional SSIM by considering multiplescales. LD evaluates dewarping performance by utilizingthe offset between the dewarped result and the flat groundtruth. AD, an enhancement of LD, refines the evaluation byexcluding offset noise in low-textured regions and mitigat-ing the impact of global transformations. the parameters forMS-SSIM, LD, and AD align with those established in priorworks . For the binarization task, we employPSNR, F-measure (FM), and pseudo F-measure (pFM) asour evaluation metrics.",
  ". Implementation details": "We train our model on 8 NVIDIA A6000 GPUs for 100,000steps with a global batch size of 80. AdamW with a weightdecay of 5 104 is adopted. We use the cosine learningrate scheduler with 2104 as the maximum learning rate.Before commencing such unified training, the model un-dergoes a pre-training phase exclusively on the dewarpingtask for 50,000 steps to initialize the model. This is fora more stable training purpose, as dewraping significantlydiffers from other tasks: while it involves coordinates re-gression, other tasks entail regression of image content.During the unified training process, the sampling weightfor dewarping, deshadowing, appearance enhancement, de-blurring, and binarization are all set to 0.2. Apart from thebinarization task, which employs the standard cross-entropyloss for its output, all other tasks are supervised using theL1 loss. Images of deshadowing, appearance enhancement,deblurring, and binarization tasks are randomly cropped aspatches with a size of 256256, while images of dewarpingtask are resized to 256 256 during training. . Quantitative comparison results between our all-in-one generalist DocRes model and existing task-specific state-of-the-art modelson 5 tasks. From top to bottom, the tasks include dewarping, deshadowing, appearance enhancement, deblurring, and binarization. Bestresults are shown in bold.",
  ". Results": "Comparisons with SOTA task-specific models. We con-duct a comprehensive comparison between our proposedDocRes and existing meticulously designed task-specificmethods. Specifically, for the dewarping task, we bench-mark DocRes against the current state-of-the-art method,DocGeo , and the recent model by Li et al. . In thedeshadowing domain, we compare DocRes with the latestSOTA models, namely BGSNet (utilizing the modeltrained on the RDD dataset provided by the authors) andDocShadow (based on the model trained on the SD7Kdataset provided by the authors). For appearance enhance-ment, we assess DocRes against UDoc-GAN and GC-DRNet . The binarization task involves a comparisonwith the current SOTA method, GDB . Additionally, wecontrast our approach with DocDiff and DE-GAN , both of which aim to unify multiple tasks within a singlenetwork structure but still necessitate separate training foreach task, resulting in the need for multiple models.Results across multiple benchmark datasets for the fivetasks are presented in . It can be seen that DocResnot only competes with existing task-specific SOTA mod-els but also surpasses them in several instances. DocResachieves new records in certain metrics for benchmarkdatasets related to dewarping, deshadowing, deblurring, andappearance enhancement tasks. Even for binarization tasks,where the dedicated SOTA model GDB still holds the topposition, DocRes exhibits performance closely trailing be-hind it. In contrast to existing unified-structure methods likeDE-GAN and DocDiff, which still require separate trainingfor each task, DocRes demonstrates significant advantages.Visualized results on these benchmarks from DocRes areshowcased in .While there remains room for improvement compared tosome of these well-designed specialized models, its impor-tant to underscore that our primary objective in this paperis not to achieve SOTA performance on every task. Instead,the focus is on evaluating the efficacy and potential of theunified DocRes approach. Further research, for example,can explore more sophisticated prompt fusion mechanismsto achieve SOTA performance in each task.Ablation studies. In this subsection, we conduct abla-tion studies to evaluate the effectiveness of our DTSPrompt.Restormer is treated as our baseline model. We firstindividually train it on each task, obtaining task-specific re-sults shown in the third column of . As a state-of-the-art image restoration network, Restormer demonstratesproficiency across various tasks when trained in isolation.However, when training it in a unified model setting (thefourth column of ), we see a significant performancedecline across almost all benchmarks. This decline is at-",
  "InputDeshadowingAppearanceenhancementDewarping Final results": ". For the same input, we employ different DTSPrompts tovalidate the controllability of DocRes. The results in the last col-umn indicate that using DocRes enables the complete restorationprocess for photographed documents, achieving the desired finalresults for users. Zoom in for the best view. tributed to the similarity in input across tasks, coupled withdistinct output requirements, leading to confusion in themodels learning process.Additionally, we explore the effectiveness of fixedprompts. A fixed prompt for each task is a h w 3 ma-trix, with constant values determined solely by the task andremaining constant regardless of the input image. For exam-ple, the fixed prompt for the dewarping task is a h w 3matrix filled with zeros, while for the deshadowing task,it is filled with ones. We adopt the same fusion approach, where the concatenated result of the fixed prompt and theinput image are fed into the restoration network.Such a simple fixed prompt approach achieves competi-tive performance compared to task-specific settings in ap-pearance enhancement and deshadowing tasks.There iseven a noticeable improvement in the binarization task,which could be attributed to multi-task learning helpingmitigate the generalization issue caused by the scarcity oftraining data for the binarization task. However, for taskslike dewarping and deblurring, the fixed prompt method stillexperiences significant performance declines, highlightingthe challenges of creating an excellent generalist model.Our DTSPrompt excels in dewarping, deshadowing,deblurring, and binarization tasks compared to the fixedprompt approach.Particularly, there are significant im-provements in the dewarping and binarization tasks. Froman overall perspective across all tasks, DTSPrompt achievessuperior results compared to the task-specific setting, byusing only a single model without the need for additionaltraining parameters or structural modifications.Control ability. In this subsection, we explore the con-trol ability of DTSPrompt. Our focus is on observing howwell DocRes can perform the correct restoration task whendifferent DTSPrompts are employed for the same input im-age.Specifically, we consider three tasks related to thephotographed scene: dewarping, deshadowing, and appear-ance enhancement. The visualization results in the first four",
  "InputDE-GANDocRes": ".Visualization results when applying models (DE-GAN and our DocRes) to perform binarization on pho-tographed documents, which were merely trained on scanned an-cient document binarization data. Zoom in for the best view. columns of show that DocRes can accurately performthe corresponding tasks. In addition, we also demonstratein the last column that a single DocRes model can completethe entire enhancement process of photographed documentimages and obtain the final results desired by the user.Generalization. An essential trait of a generalist modellies in its ability to harness synergies among multi-task datato improve overall generalization. In this context, we delveinto the generalization capability of DocRes through visu-alizations.As outlined in .1, the training datafor the binarization task in DocRes primarily consists ofscanned documents, particularly ancient ones, presentinga considerable gap from the photographed modern docu-ments . Here, we aim to apply DocRes to binarize pho-tographed documents, introducing unseen noises like blur-riness, shadows, low-light conditions, and reflections thatare not present in the binarization training set. As shownin , DocRes consistently demonstrates excellent per-formance on such out-of-domain data.In contrast, DE-GANs performance significantly deteriorates in thepresence of shadow and low-light interference.Moreover, we extend the evaluation to the deblurringtask for photographed document images. Notably, the train-ing data for the deblurring task in DocRes consists exclu-sively of clean document images. As illustrated in ,DocRes exhibits superior deblurring performance on pho-tographed document images compared to DE-GAN and DocDiff , both of which were also trained exclu-sively on the TDD dataset for the deblurring task.We attribute DocRess robust out-of-domain generaliza-tion capability to its learning of patterns associated withphotographed noise through tasks like dewarping, deshad-owing, and appearance enhancement.",
  ". Discussions and conclusions": "This paper presents DocRes, a generalist model designedfor unifying document image restoration tasks, includingdewarping, deshadowing, appearance enhancement, deblur-ring, and binarization. The key innovation of DocRes isthe incorporation of Dynamic Task-Specific Prompt (DT-SPrompt), which leverages prior features to construct vi-sual prompts, acting not only as a guiding cue for spe-cific tasks but also supplying additional information to en-hance restoration performance. It can be seamlessly ap-plied to existing restoration networks, resulting in a gen-eralist model that can accommodate input with high or vari-able resolutions. With the support of DTSPrompt, DocResachieves performance levels matching or surpassing SOTAtask-specific models, without the need for extra training pa-rameters or complex architectural designs. We also illus-trate DocRess controllability in performing different taskswhen presented with the same input image and its capacityto generalize to out-of-domain data. Notably, the DTSPrompt is not confined to the specifictasks explored in this paper. It can be potentially extendedto incorporate more diverse prior features ,such as DCT coefficients, SIFT, JPEG noise, and resam-pling artifacts, to accommodate a broader range of imagerestoration tasks.Additionally, it is worth investigatingprompt fusion mechanisms for better integrating the DT-SPrompt.In summary, this paper successfully attemptsto develop a unified multi-task model for document imagerestoration, inspiring future research of generalist or foun-dation models for pixel-level image processing tasks.",
  "Ioannis Pratikakis, Konstantinos Zagori, Panagiotis Kaddas,and Basilis Gatos. ICFHR 2018 competition on handwrittendocument image binarization (H-DIBCO 2018). In ICFHR,2018. 5, 6": "Ioannis Pratikakis, Konstantinos Zagoris, Xenofon Kara-giannis, Lazaros Tsochatzidis, Tanmoy Mondal, and IsabelleMarthot-Santaniello. ICDAR 2019 competition on documentimage binarization (DIBCO 2019). In ICDAR, 2019. 5 Chenfan Qu, Chongyu Liu, Yuliang Liu, Xinhong Chen,Dezhi Peng, Fengjun Guo, and Lianwen Jin. Towards robusttampered text detection in document image: New dataset andnew solution. In CVPR, pages 59375946, 2023. 8",
  "Mohamed Ali Souibgui and Yousri Kessentini. DE-GAN:A conditional generative adversarial network for documentenhancement. IEEE TPAMI, 44(3):11801191, 2020. 2, 6,8, 1": "Mohamed Ali Souibgui, Sanket Biswas, Andres Mafla,Ali Furkan Biten, Alicia Fornes, Yousri Kessentini, JosepLlados, Lluis Gomez, and Dimosthenis Karatzas.Text-DIAE: a self-supervised degradation invariant autoencoderfor text recognition and document enhancement. In AAAI,pages 23302338, 2023. 2 Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, YangLiu, Chenguang Zhu, Michael Zeng, Cha Zhang, and MohitBansal. Unifying vision, text, and layout for universal doc-ument processing. In CVPR, pages 1925419264, 2023. 2,3",
  "Francisco Zamora-Martnez, Salvador Espaa Boquera, andMara Jose Castro Bleda. Behaviour-based clustering of neu-ral networks applied to document enhancement. In IWANN,2007. 5": "Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yan-ning Zhang. All-in-one multi-degradation image restorationnetwork via hierarchical degradation representation. In ACMMM, pages 22852293, 2023. 2, 3 Jiaxin Zhang, Canjie Luo, Lianwen Jin, Fengjun Guo, andKai Ding. Marior: Margin removal and iterative content rec-tification for document dewarping in the wild. In ACM MM,pages 28052815, 2022. 2, 3, 5 Jiaxin Zhang, Bangdong Chen, Hiuyi Cheng, Lianwen Jin,Fengjun Guo, and Kai Ding. DocAligner: Annotating real-world photographic document images by simply taking pic-tures. arXiv preprint arXiv:2306.05749, 2023. 5 Jiaxin Zhang, Lingyu Liang, Kai Ding, Fengjun Guo, andLianwen Jin. Appearance enhancement for camera-captureddocument images in the wild. IEEE Transactions on Artifi-cial Intelligence, 2023. 1, 2, 4, 5, 6",
  ". Efficiency": "As shown in , we compared the number of param-eters and computational complexities of our method withthose of other methods. It can be observed that even whencompared to certain task-specific models, our method main-tains an advantage in both the number of parameters andcomputational complexity. This advantage is particularlysignificant when considering that the number of parametersof these task-specific models would multiply several timesover if they were to support multitasks. In contrast, ourmethod does not require additional parameter increments.",
  ". Additional ablation study": "To quantify the individual contributions of task synergiesand DTSPrompt, we conducted an additional ablation ex-periment and presented the results in . From the ta-ble, it can be observed that both DTSPrompt and multitasksynergy lead to improvements. The improvement broughtby multitask synergy may be attributed to the relativelysmall amount of binarization training data. The incorpora-tion of other task data aids the model in avoiding overfittingto the limited binarization training data.",
  ". More visualized results from DocRes. Zoom in for bestview": "illustrate the potential issue of error accumulation when ap-plying DocRes for end-to-end camera-captured documentimage enhancement. Due to the iterative nature of DocResfor end-to-end tasks, where each forward pass utilizes theoutput of the previous pass as input, errors in one task canaccumulate and affect the final result. Exploring methodsthat can accomplish multiple document restoration tasks ina single forward pass would be a meaningful avenue for fu-ture research.",
  ". Further discussions about DTSPrompt": "In fact, from the ablation experiments in of themain text, it is evident that compared to fixed prompts ortask-specific models, DTPrompt does not demonstrate sig-nificant advantages and may even perform worse in certaintasks, such as shadow removal and deblurring tasks. Thisis mainly because document image restoration tasks likedeblurring lack recognized prior features that definitively",
  ". Failure case from DocRes when applying it for end-to-end camera-captured document image enhancement. Zoom in forbest view": "enhance performance.In such tasks, DTPrompt primar-ily serves as a discriminative cue rather than significantlyboosting model performance. As for shadow removal tasks,while shadow maps have been widely proven to be useful,our extraction operations are very simple, involving basicimage processing techniques rather than the conventionalapproach of using deep models for prediction.Based on these observations and analyses, future im-provements for DTSPrompt could focus on two directions:1. Exploring more effective prior features for specific tasks:This entails delving deeper into identifying prior featuresthat are more conducive to enhancing performance for par-ticular tasks, such as deblurring. 2. Employing a train-able prediction module for the DTSPrompt generator: Thiswould enhance the prior feature extraction capabilities. Im-portantly, theres still no necessity to designate a separateDTSPrompt generator for each task. Instead, a single DT-SPrompt generator with shared parameters could simulta-neously output multiple prior features. During input to therestoration network, different prior features can be chosenfor task guidance based on the specific task at hand."
}