{
  "Abstract": "Inverse imaging problems (IIPs) arise in various appli-cations, with the main objective of reconstructing an im-age from its compressed measurements. This problem is of-ten ill-posed for being under-determined with multiple in-terchangeably consistent solutions. The best solution in-herently depends on prior knowledge or assumptions, suchas the sparsity of the image. Furthermore, the reconstruc-tion process for most IIPs relies significantly on the imaging(i.e. forward model) parameters, which might not be fullyknown, or the measurement device may undergo calibrationdrifts. These uncertainties in the forward model create sub-stantial challenges, where inaccurate reconstructions usu-ally happen when the postulated parameters of the forwardmodel do not fully match the actual ones. In this work, wedevoted to tackling accurate reconstruction under the con-text of a set of possible forward model parameters that ex-ist. Here, we propose a novel Moment-Aggregation (MA)framework that is compatible with the popular IIP solutionby using a neural network prior. Specifically, our methodcan reconstruct the signal by considering all candidate pa-rameters of the forward model simultaneously during theupdate of the neural network. We theoretically demonstratethe convergence of the MA framework, which has a simi-lar complexity with reconstruction under the known forwardmodel parameters. Proof-of-concept experiments demon-strate that the proposed MA achieves performance compa-rable to the forward model with the known precise param-eter in reconstruction across both compressive sensing andphase retrieval applications, with a PSNR gap of 0.17 to1.94 over various datasets, including MNIST, X-ray, Glas,and MoNuseg. This highlights our methods significant po-tential in reconstruction under an uncertain forward model.",
  "*These authors contributed equally to this paper.Corresponding Author": ".The illustration of the Moment-Aggregation (MA)framework for IIPs with a neural network that considers the effectof all possible candidate parameters of the forward model simul-taneously. MA loss is constructed after every forward propagation(we call this time point a moment) and then is used to update pa-rameters in backward propagation. Left: The losses by candidateforward model parameters, and one of them is the precise param-eter. Their labels are unknown (i.e. the precise or not precise)during training. Right: The loss at different moments by MA.The loss is moment-wise convex/smooth, and the overall trainingcan achieve the global minima as reconstruction using the preciseparameter.",
  "y = A (x0; ) + ,(1)": "where A() denotes the forward imaging model, which istypically governed by different mathematical and physi-cal principles and often parameterized by . Some real-world examples of this paradigm include magnetic reso-nance imaging (MRI) , tomographic imaging, lensless photography , microscopic imaging, and even image processing , eachof which with its own forward modeling stemmed from theunderlying physics and utilized technology.IIPs are typically ill-posed (underdetermined for m <",
  "arXiv:2405.02944v1 [cs.CV] 5 May 2024": ". The typical workflow of IIPs. First, a forward model isapplied to a signal to obtain the measurement. Then the measure-ment is used to reconstruct the original signal via machine learning(ML) or deep learning (DL) algorithms. n), which means they have multiple interchangeably con-sistent solutions.The core idea to solve these problemsis incorporating prior information about the original signal(e.g., prior distribution, smoothness, sparsity, etc.) into thereconstruction algorithm. This enhances the reconstructionquality by reducing the search space and steering the algo-rithm toward the most probable and reality-compliant so-lution . Mathematically, an IIP is typically given in avariational formulation:",
  "arg minx12y A(x; )22 + 0R(x),(2)": "where x denotes the reconstructed image, R(x) denotes theregularization term governed by prior knowledge, and 0controls the regularization strength. The typical workflowis shown in .It is worth mentioning that one key issue of IIPs is thatthe quality of signal reconstruction can be severely declinedif the designed and implemented parameters of forwardmodels do not match. (Left) shows reconstruction us-ing a forward model with the known precise parameter cansuccessfully recover the signal while with a wrong parame-ter fails. This issue is general when recording microscopicimages with low-cost equipment. The small scale and pre-cision limitation of such equipment makes it challengingto accurately depict the forward model. Furthermore, an-other application scenario involves employing diverse setupparameters to capture various samples, wherein, due to aninadvertent mix-up or loss of the setup records, the for-ward model aligning with corresponding samples is neces-sary for accurate reconstruction. This process of rematchingdifferent setup configurations for distinct samples is recog-nized as a laborious and time-consuming endeavor, whichis widely neglected by existing methods.To address this issue, we consider several possible can-didate parameters of forward models, and we formulatethe recovery task under uncertain parameters of a forwardmodel as a two-variable optimization problem.We pro- pose a general optimization framework named Moment-Aggregation (MA) that is compatible with the state-of-the-art method for IIPs based on untrained neural network pri-ors. Here, the moment is defined as the time point afterforward propagation and before backward propagation. Ag-gregation means considering the effects of all possible can-didates simultaneously (shown in (Right)). By usingthe gradient-stopping trick, we construct aggregation func-tions that are able to adjust according to the training pro-cess automatically.Subsequently, leveraging the advan-tages of neural network-based back-propagation for opti-mization, our framework can achieve a recovery accuracycomparable to the signal recovered by using the known pre-cise parameter. An exemplary loss surface is shown in .In summary, our contribution is two-fold: i) We proposeMoment-Aggregation, a general framework to solve IIPsunder uncertainty parameters of the forward model. ii) Weprovide a theoretical analysis of MA. The experiments con-ducted on two applications, including compressive sensingand phase retrieval, confirm the feasibility of our method.",
  ". Related Work": "IIPs by Neural Network Priors. The conventional meth-ods to solve IIPs rely on handcrafted prior domain knowl-edge; however, these methods are often sensitive to the hy-perparameters (e.g., 0 in Eq.2.Note this is differentfrom the parameter of the forward model) and often yielda poor recovery performance . Recent deep-learningmethods, such as supervised learning and unsupervisedlearning , demonstrate an outstanding ability to solveseveral image tasks. Due to this powerful tool, authors in show that inverse problems can be solved byusing the prior from pre-trained generative models, whichis known as learned network prior. Along with the priorthat is learned by massive training data, recently, the com-munity has observed that even without training onany dataset, the randomly initialized convolutional neuralnetworks (CNNs) already hold the prior for image signals.This prior, often known as deep image prior (DIP), statesthat CNNs are able to capture a significant amount of low-level image statistics before any training on a specific imagedataset. Hence, DIP becomes the natural choice to serve asthe prior in IIPs (i.e. R(x) in Eq. 2) and is employed bynumerous works . Theseworks often involve a randomly initialized CNN-based gen-erative model and solve the inverse problem via training thenetwork parameters. As these works often assume knowingthe precise or near-precise parameter of the forward model,our work is orthogonal but complementary to them and aimsto recover signals under a set of candidate parameters.Convergence Guarantee. There are numerous works provide the convergence and error guarantee for IIPs with . Using CS-DIP to reconstruct x0 with measurement y. Left: The signal is successfully reconstructed under the forward modelwith the precise parameter while failing under a wrong parameter. Right: Our method can successfully reconstruct x0 by optimizing undera set of candidate parameters (we assume one of them is close to the precise parameter). neural network prior. For example, authors in provea near-linear convergence rate for a LLipschitz contin-ues generative network. Afterward, authors in inves-tigate the convergence rate by projected gradient descentwith generative network prior, while authors in studyan algorithm based on Langevin dynamics with learnednetwork prior.Likewise, with untrained network prior,authors in prove the convergence rate for under-parameterized networks and authors in prove it for theover-parameterized networks. It is observed that in order to ensure the derivation istractable, these works often employ multiple assumptions,such as Lipschitz continues, the range of neural network,and the network only has linear layers and Relu activationfunctions . We admit these assumptions simplify thereal optimization process of the reconstruction, but theirtheoretical results offer enough insights for the communityto develop further works. Again, their works often assumethe forward model parameter is known, and in this work,we build theoretical analysis in our scenarios based on theirconclusion. Recovery with Uncertainty in CS. There are several works studying the problem of mismatch measure-ment in CS. However, they often assume the error of theforward model is white additive noise and relatively smallto the precise parameter. Besides, their theoretical guar-antee is often designed for CS problems and relies on theGaussianity assumption, which is difficult to generalize tobroader scenarios. More importantly, authors in show using neural network prior is relatively robust to sucha noisy forward model. Contrastingly, we consider recon-struction with a discrete set of parameter candidates, andthe distance among different measurements resulting from",
  "2y A(x; i)22,(4)": "where i . We omit the term 0R(x) as the prior isincluded in the neural network. The straightforward solu-tion to this problem is performing reconstruction multipletimes by traversing all possible candidates. However, thissolution is extremely inefficient, which is not friendly forapplications with computation resource constraints, espe-cially when the number of candidates is large. To addressthis issue, we present our framework and provide the theo-retical insights from convex optimization.",
  ". Moment-Aggregation Training Framework": "To solve IIPs under a set of candidate parameters, the ideais to construct a new loss L by using such a neural networkG presented in assumption 1. If the loss L satisfies the sim-ilar properties with F(x; ), the loss has a high probabil-ity of converging to a similar optimal with F(x; ). It isnoteworthy that the neural network G can only optimize xthrough optimizing w since i can be viewed as an inde-pendent variable with w. Now, we define the new loss andname it aggregation loss,",
  "definition 1. Given a set of candidate parameters andneural network G, any aggregation loss should satisfies: i)L is -strong convexity, -strong smoothness w.r.t x, and ii)limxx0 L(x, ) 0": "Here, the first condition ensures its convergence rate istractable, while the second condition ensures the neural net-work can converge to the same optima as recovery by usingthe known precise parameter.Nevertheless, constructing a loss L is still challengingat this time because there is no prior knowledge about thequality of each candidate forward model parameter. Oursolution is calculating the temporary quality of each candi-date based on F(x; i) after every forward propagation ofG. We define this time point as,",
  "definition 2. The moment is the time point between the for-ward propagation and backward propagation of each itera-tion by the neural network G": "Note that the surrogate qualities may not be super reli-able at the beginning. i.e., F(x; ) F(x; i) is possiblewhen the neural network does not converge well. However,with this surrogate quality of candidates, we are able to con-struct the moment-aggregation loss (MA loss) that satisfiesthe conditions of aggregation loss presented in definition 1at each moment. We conjecture the loss in the entire life-time should also have similar properties with aggregationloss if each moment an MA loss has similar properties withaggregation loss",
  "Stop Gradient for i,": "Here, H(i; F(x; 1), , F(x; nc)) is the function to cal-culate the weight for each candidate based on the surrogatequalities at each moment (i.e. i will be updated at eachiteration). Stopping gradient means when the neural net-work performs backward propagation, we consider everyi as a constant. H should satisfy: i) nci=1 i = 1, and ii)limxx0 H(; F(x; 1), , F(x; nc)) 1.",
  ". Standard CS problem": "Setup. We primarily evaluate our algorithm in the stan-dard CS problem , where A (x0; ) = x0. The for-ward model parameter is a random Gaussian kernel Rmn.Each element of is Gaussian i.i.d and obeysi,j N0, 1 m. The set consists 1 precise parameterand 9 candidate parameters randomly generated by the samedistribution. We choose two datasets for our evaluation: i)a toy dataset MNIST , each image has 28 28 pixelsand ii) Shenzhen Chest X-Ray Dataset , we downsam-ple each image to 256256 pixels.Implementation. Our experiment is based on the unlearnedtraining pipeline CS-DIP provided by . The neural net-work is the generator of DCGAN and is randomly ini-tialized. We use Adam optimizer with a fixed learningrate 1e-3. The experiments are conducted on a cluster nodewith a V100 16G GPU.Baselines. We include: i) Upper-bound: The most impor-tant baseline is the reconstruction with the known preciseparameter, which is treated as the upper bound of our prob-lem. ii) Random Parameter: We random select a param-eter from the set . This is the blind reconstruction, andwe simply compute the expected value of reconstruction re-sults by using every candidate. We include Lasso-wavelet,lasso-DCT, BM3D-AMP , and CS-DIP . iii) Uni-form Aggregation: We consider every candidate parameterto have the same quality (i.e. wi = 1/nc). and iv) Alter-nating Optimization: In each epoch, this baseline involvesfirst updating the neural network, then finding a good i thathas the minimum loss and backpropagating this loss.Evaluation Metrics.We employ two widely used met-rics to measure the reconstruction performance with groundtruth: i) Peak Signal-to-Noise Ratio (PSNR) and ii) Struc-",
  "tural Similarity Index Measure (SSIM)": "Results.The numerical results are shown in .The first observation is that using Random Parameter toreconstruct the signal blindly is not feasible, which onlyachieves around 10dB PSNR, meaning almost nothing isreconstructed. This is consistent with the fundamental prin-ciple of IIPs. Then, we observe our methods can achievesimilar reconstruction results with the upper bound, whichis reconstructing using the known parameter. For example,in both MNIST and X-ray datasets, our method only has a0.04-0.07 SSIM reduction. Another interesting observationis that alternating optimization shows obvious superiorityover blind reconstruction, and it can reconstruct the signalsometimes, e.g. around 19 dB in PSNR for X-ray imagereconstruction. However, this method is not stable sinceit quickly switches different candidate parameters to opti-mize, which results in a high probability of failure to recon-struct. Some samples of reconstructed signals for MNISTand X-ray datasets are shown in (Left and Right), re-spectively. Both of them illustrate that reconstructed signalsby our method can achieve very similar performance withthe upper bound. We also demonstrate the convergence ratein (Left), which shows our method can converge tothe same level of reconstruction error with a lagging. This lagging is reasonable, because the upper bound using theknown precise parameter is easy to converge, while underan uncertain set of candidates, the error landscape for opti-mization is more complicated. (Right) shows the run-time for each epoch by using a set of candidate parame-ters and only one precise parameter. Our methods over-head is caused by the computation of the forward processfor each candidate parameter. Although the overhead exists,our method is still much faster than training different neu-ral networks separately with different candidate parameters.For example, if we only have one device that can train themodel, in the MNIST dataset, our method requires 0.007seconds to update for every epoch; however, training 10different neural networks for different candidates requiresapproximately 0.005 10 = 0.5 seconds.",
  ". Applications in Phase Retrieval": "Setup. We also show the feasibility of our methods in phaseretrieval. Here, we take holographic imaging as an exam-ple . Suppose O(d = 0) denotes a complex-valuedobject wave at location d = 0. We can use the angularspectrum method to describe the propagation of the wave tothe sensor plane d = z as O(d = z) = F1{P (, d =z) F{O(x, y; d = 0)}}, where denotes the wavelength,",
  ". Comparison of the upper bound and our proposed method (MA) for Left: convergence and Right: runtime each iteration.i.e.forward-propagation and backward-propagation": "x, y denotes the coordinate in the object space that is or-thogonal with z, and F and F1 denote Fourier transformand inverse Fourier transform, respectively. P (, d = z) iscalled the transfer function and is based on the experimentequipment and setups (Refer to ). Similarly, a referenceplane wave can propagate to the sensor plane. The sensorplane captures the superposition of the object wave and ref-erence wave as H = O2(d = z) + R2(d = z)2, knownas a hologram, and our goal is to retrieve O(d = 0) fromH. This problem is also an ill-posed IIP problem, and hereP (, d = z) can be considered as the forward model withuncertain parameters due to the low-quality equipment oran inaccurate precision optical rail. In our simulation, weset the known wavelength and distance to = 0.520mand 5000m to generate holograms, respectively. The setof uncertain parameters = {d1, , d10} is generatedby di U(z 500, z + 500). We choose samples fromthe Gland segmentation dataset (GlaS) and the Multi-Organ Nucleus Segmentation (MoNuSeg) dataset togenerate the simulated holograms. Baseline.i) Upper-bound:the reconstruction with aknown forward model parameter. ii) Random Parameter:We evaluate CS-DIP in this application, which presented in. iii) Uniform Aggregation, and iv) Alternating Opti-mization.Evaluation Metrics. PSNR, and SSIM.Results. The results are shown in and . In thisexperiment, our method consistently demonstrates a smallgap in the reconstruction with the known precise parame-ter. For example, there are 0.307 and 0.167 gaps in PNSRfor Glas and MoNuSeg, respectively. We also find that us-ing Random Parameter, Uniform Aggregation and Alter-nating Optimization can reconstruct the low-frequency in-formation of the object (e.g. outline and shape, as shownin the second column of for Alternating Optimiza-tion) while lacking the reconstruction of the detailed tex-ture. This may be because the reproduced measurement (i.e.y the prediction after the forward process) in this task is stilllike an image, which can be partially fitted by the neural net-work. However, the detailed texture represents depth infor-",
  ". Discussion and Conclusion": "This paper focuses on a scenario addressing inverse imag-ing problems (IIPs), where the main challenge arises fromuncertainties in the parameters of the forward model usedfor the imaging process. These uncertainties can stem fromvarious sources, such as calibration drifts in imaging de-vices, imprecise knowledge of the device parameters, orvariations in experimental setups, making the task of recon-structing the original image from its compressed measure-ments particularly difficult. In this work, we consider thereare a set of candidate parameters. Instead of testing differ-ent candidate parameters independently, our proposed MA framework marks a significant step forward under this pa-rameter uncertainty by effectively aggregating informationfrom all candidate parameters of the forward model. Ourtheoretical analysis is built on the aforementioned works,where they provide the convergence guarantee under the as-sumption that the forward model parameter is known. Wetake a step forward to show that we can construct a lossunder a set of candidate parameters with similar proper-ties to the loss with a known parameter, and hence, conver-gence by our method is ensured. Our experimental resultsdemonstrate that the MA framework achieves a close per-formance to that of reconstructions using known forwardmodel parameters(upper bound). Specifically, our methodonly has a 0.04-0.07 SSIM difference with the upper boundin MNIST and X-ray dataset, respectively. Additionally,there are only 0.307 and 0.167 reductions in PNSR for theGlas and MoNuSeg datasets, respectively.This proposed method demonstrates significant potentialin scenarios where accurate parameters remain unknown,particularly in medical imaging, including fundus cameraimaging, microscopic imaging, MRI, and CT. We admitperformance gaps and occasionally unstable reconstructionstill exist, and we conjecture this is because of the compli-cated error landscape in real optimization beyond our as-sumptions, which will be investigated in the future. Futurework will explore extending the MA framework to morecomplex imaging models and closer to real-world scenar-ios.",
  "Mingqin Chen, Peikang Lin, Yuhui Quan, Tongyao Pang,and Hui Ji.Unsupervised phase retrieval using deep ap-proximate mmse estimation. IEEE Transactions on SignalProcessing, 70:22392252, 2022. 2": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-offrey Hinton. A simple framework for contrastive learningof visual representations. In International conference on ma-chine learning, pages 15971607. PMLR, 2020. 2 Xiwen Chen, Hao Wang, Abolfazl Razi, Michael Kozicki,and Christopher Mann. Dh-gan: a physics-driven untrainedgenerative adversarial network for holographic imaging. Op-tics Express, 31(6):1011410135, 2023. 1, 2 Li Feng, Leon Axel, Hersh Chandarana, Kai Tobias Block,Daniel K Sodickson, and Ricardo Otazo. Xd-grasp: golden-angle radial mri with reconstruction of extra motion-state di-mensions using compressed sensing. Magnetic resonance inmedicine, 75(2):775788, 2016. 1 Yosef Gandelsman, Assaf Shocher, and Michal Irani.double-dip: unsupervised image decomposition via coupleddeep-image-priors. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1102611035, 2019. 2 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 2 Reinhard Heckel and Mahdi Soltanolkotabi. Compressivesensing with un-trained neural networks: Gradient descentfinds a smooth approximation. In International Conferenceon Machine Learning, pages 41494158. PMLR, 2020. 3 Chinmay Hegde. Algorithmic aspects of inverse problemsusing generative models. In 2018 56th Annual Allerton Con-ference on Communication, Control, and Computing (Aller-ton), pages 166172. IEEE, 2018. 3 Stefan Jaeger, Sema Candemir, Sameer Antani, Y`-Xiang JWang, Pu-Xuan Lu, and George Thoma. Two public chest x-ray datasets for computer-aided screening of pulmonary dis-eases. Quantitative imaging in medicine and surgery, 4(6):475, 2014. 5",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization.arXiv preprint arXiv:1412.6980,2014. 5": "N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane,and A. Sethi. A Dataset and a Technique for GeneralizedNuclear Segmentation for Computational Pathology. IEEETrans Med Imaging, 36(7):15501560, 2017. 7 Hengrong Lan, Juze Zhang, Changchun Yang, and Fei Gao.Compressed sensing for photoacoustic computed tomogra-phy based on an untrained neural network with a shape prior.Biomedical Optics Express, 12(12):78357848, 2021. 1, 2",
  "Farhad Niknam, Hamed Qazvini, and Hamid Latifi. Holo-graphic optical field recovery using a regularized untraineddeep decoder network. Scientific reports, 11(1):10903, 2021.7": "Giacomo Oliveri, Marco Salucci, Nicola Anselmi, and An-drea Massa. Compressive sensing as applied to inverse prob-lems for imaging: Theory, applications, current trends, andopen challenges. IEEE Antennas and Propagation Maga-zine, 59(5):3446, 2017. 1 Adnan Qayyum, Inaam Ilahi, Fahad Shamshad, Farid Bous-said, Mohammed Bennamoun, and Junaid Qadir. Untrainedneural network priors for inverse imaging problems: A sur-vey. IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022. 2",
  "Mathieu Rosenbaum and Alexandre B Tsybakov. Sparse re-covery under matrix uncertainty. 2010": "Mathieu Rosenbaum and Alexandre B Tsybakov. Improvedmatrix uncertainty selector. In From Probability to Statis-tics and Back: High-Dimensional Models and ProcessesAFestschrift in Honor of Jon A. Wellner, pages 276291. Insti-tute of Mathematical Statistics, 2013. 3 Jonathan Scarlett, Reinhard Heckel, Miguel RD Rodrigues,Paul Hand, and Yonina C Eldar. Theoretical perspectives ondeep learning methods in inverse problems. IEEE journal onselected areas in information theory, 3(3):433453, 2022. 2,3 Viraj Shah and Chinmay Hegde. Solving linear inverse prob-lems using gan priors: An algorithm with provable guaran-tees. In 2018 IEEE international conference on acoustics,speech and signal processing (ICASSP), pages 46094613.IEEE, 2018. 2 K. Sirinukunwattana, J. P. W. Pluim, H. Chen, X. Qi, P. A.Heng, Y. B. Guo, L. Y. Wang, B. J. Matuszewski, E. Bruni,U. Sanchez, A. hm, O. Ronneberger, B. B. Cheikh, D. Raco-ceanu, P. Kainz, M. Pfeiffer, M. Urschler, D. R. J. Snead,and N. M. Rajpoot. Gland segmentation in colon histologyimages: The glas challenge contest. Med Image Anal, 35:489502, 2017. 7",
  "Hengyong Yu and Ge Wang. Compressed sensing based in-terior tomography. Physics in medicine & biology, 54(9):2791, 2009. 1": "Wenhui Zhang, Liangcai Cao, David J Brady, Hua Zhang, JiCang, Hao Zhang, and Guofan Jin. Twin-image-free holog-raphy: a compressive sensing approach. Physical review let-ters, 121(9):093902, 2018. 1, 6, 7 Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei AEfros.Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEEinternational conference on computer vision, pages 22232232, 2017. 2"
}