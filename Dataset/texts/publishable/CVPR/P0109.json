{
  "Abstract": "In this report, we present our champion solution forEPIC-KITCHENS-100 Multi-Instance Retrieval Challengein CVPR 2024.Essentially, this challenge differs fromtraditional visual-text retrieval tasks by providing a cor-relation matrix that acts as a set of soft labels for video-text clip combinations.However, existing loss functionshave not fully exploited this information.Motivated bythis, we propose a novel loss function, Symmetric Multi-Similarity Loss, which offers a more precise learning ob-jective.Together with tricks and ensemble learning, themodel achieves 63.76% average mAP and 74.25% averagenDCG on the public leaderboard, demonstrating the effec-tiveness of our approach. Our code will be released at:",
  ". Introduction": "The goal of visual-text retrieval task is to accurately matchvisual content, such as images or videos, with correspond-ing natural language descriptions. This task is important forvarious applications, including content-based image searchand multimedia recommendation systems. Consequently,we have witnessed a rapid growth, with a numbers of inno-vative approaches being proposed to improve retrieval ac-curacy and efciency . The EPICKITCHENS-100(EK-100) Multi-Instance Retrieval Challenge is dis-tinguished by the inclusion of a relevancy matrix that de-nes the relationships between verbs and nouns . Thisfeature makes it more challenging for evaluating retrievalmethods.AVION provides us an ideal baseline model asthey leverage the vanilla CLIP-based model to achieveimpressive performance with minimal computational cost.However, upon exploring existing loss functions, we foundthat the learning objective of the current state-of-the-art lossfunction, the Adaptive Max-Margin Multi-Instance (Adap-tive MI-MM) Loss , is not always correct due to the hard mining strategy employed in their algorithm.Specically, the hard mining strategy deployed by theAdaptive MI-MM loss allows the dataloader to select hardpositive samples where the correlation value in the rele-vancy matrix is lower than 1. In this situation, there is apossibility that negative pairs have a stronger relation to thecorresponding textual descriptions, leading the model to op-timize in the wrong direction. However, directly removingthe hard mining strategy would result in a signicant dropin the performance of models.To address this issue, we propose a novel loss functioncalled Symmetric Multi-Similarity (SMS) Loss, which isadapted from Multi-Similarity Loss , and offers a moreprecise learning objective for the EK-100 Multi-InstanceRetrieval Challenge, by symmetrically optimizing the posi-tive and negative pairs. Concretely, we redened the corre-lation between positive and negative pairs, and a relaxationfactor is added to prevent the loss between similar pairsfrom becoming dominant. Meanwhile, to obtain better re-sult, we introduce a simple but useful trick of horizontallyipping video frames during the inference phase and em-ploy an ensemble of diverse models.In experiments, we achieved the 1st place in the EK-100Multi-Instance Challenge 2024 on the public leaderboard.Our method, which adopts the previous state-of-the-art so-lution as the baseline algorithm, signicantly improves itsperformance, i.e., an average mAP increase from 58.42%to 63.76% (+5.34%) and an average nDCG increase from70.76% to 74.25% (+3.49%).",
  "For a typical visual-text retrieval task, a given triplet set": "D = {V, T , C} is provided as input. Here, V = {vi}Nvi=1represents the video set and T = {tj}Ntj=1 represents narra-tion set, with Nv and Nt samples, respectively. The labelC = {cij {0, 1}|i = 1, 2, ..., Nv, j = 1, 2, ..., Nt} de-notes if a visual-text pair matches, where cij = 1 signies(vi, tj) is a corresponding visual-text pair, and vice versa.Meanwhile, in deep metric learning, it is challenging tooptimize every feature to its exact position, generally weleverage a margin to separate positive and negative pairs.Thus, for typical visual-to-text retrieval, the instinct learn-ing objective is:",
  "Ov2t := S(V, Tp) S(V, Tn) C ,(1)": "where S() denotes the similarity metric, Tp and Tn arethe matching pairs and mismatching pairs to the video set.Since C is the hard label, i.e., C can only be either 0 or 1.Therefore for each iteration, the margin between the posi-tive and negative pair becomes: (cp cn) = . Assumingthat we use the cosine similarity as the metric, where thematrix production between L2-normalized features repre-sents their similarity, thus the learning objective becomes:",
  ":= vTi tj + vTi tk 0.(2)": "Where j and k are samples in positive and negative sets,respectively. That is exactly the learning objective of MI-MM Loss , a commonly used loss function for text-image retrieval task, Since our task is bidirectional, i.e.,we need video-to-text and text-to-video retrieval simulta-neously, thus the loss function can be formulated as:",
  "+": "(3)Here, []+ denotes the ReLU function. However, the EK-100 Multi-Instance Retrieval Challenge introduces a cor-relation matrix C = {cij |i = 1, 2, ..., Nv, j =1, 2, ..., Nt}, meaning C is no longer a hard label, and cijcould be any value between 0 and 1. To leverage this priorinformation, the adaptive MI-MM Loss is proposed, for-mulated as:",
  "(i,j,k)N[cijvTi tj+vTi tk]++[cijtTi vj+tTi vk]+": "(4)While the learning objective of adaptive MI-MM Lossis similar to MI-MM Loss, introducing the relevancy ma-trix C to the learning objective also means that the corre-lation between negative pairs, cik, is not always 0. Thismakes the learning objective less precise for this challenge.Moreover, EgoVLP employs a hard mining strategy that dene the positive set as i+ = {j|cij 0.1}, i.e., the par-tially matched video-text pairs could be treated as the pos-itive samples. This can be problematic when cij < cik,leading the learning objective in the opposite direction tothe correct one.",
  "(i,j,k)N[ Sij]+ + [Sik ]+ .(6)": "This reveals that the learning objective for Multi-Similarity Loss is to push positive pairs closer to the marginwhile pulling negative pairs away from it. This inspires usto dene a symmetric loss function for positive and negativepairs. However, as previously illustrated, it is challengingto determine if tj and tk are relatively more positive to thevideo clip vi, Therefore, directly applying Multi-SimilarityLoss to this challenge is still far from satisfaction.",
  "(i,j,k)Ncij cik.(7)": "There are three distinct scenarios to consider.WhenR > 0, Sij is the relatively more positive pair comparedto Sik, and vice versa. Meanwhile, when R = 0, the dis-tance between Sij and Sik should be optimized to 0. How-ever, in practice, we nd that the loss at R = 0 tend tobe the dominant loss since the value of R is very small.To mitigate this, we introduce a relaxation factor, , suchthat when the Euclidean distance between Sij and Sik issmaller than , we cease optimizing this part. This adjust-ment allows us to maintain the major learning objective, i.e.,",
  "(i,j,k)N": "[R Sij + Sik]+R > 0[R + Sij Sik]+R < 0[Sij Sik1 ]+R = 0(8)Here, S denotes both the similarity of video-to-text andtext-to-video.Theoretically, the relaxation factor should be less thanthe minimum value of C for C > 0. This ensures that theoptimization process remains effective and balanced acrossdifferent correlation scenarios. However, in practice, wesometimes need a larger to prevent the model from focus-ing on the similar pairs.",
  ". Inference Augmentation": "Inspired by , we employ a ip function during the in-ference phase. Generally, we could directly calculate thesimilarity by the matrix production of features from videoand text encoders. Here we rst obtain the original fea-tures, then ip the video feature horizontally, and feed boththe features into the model, Finally, we add the obtainedfeatures together before calculating the similarity matrix.Pytorch-style pseudo code are shown in Alg. 1.",
  ". Implementation Details": "We directly utilized the framework, as well as the pretrainedmodels from AVION , which is a vanilla CLIP-basedmodel trained on the LLM-augmented Ego4D dataset . We then ne-tuned the model using our SMS loss onthe EK-100 dataset . During training, we conduct theexperiments on 4 RTX 6000 Ada GPU, and the batch sizeof our ViT-B-based model is 64 per GPU, resulting in thetotal batch size of 256. For our ViT-L-based model, wecould only t 60 video clips on every 48GB GPU, resultingin the total batch size of 240. The dimension of each video clip is 16 3 224 224, indicating that we sample 16frames per video clip, with each frame resized to a heightand width of 224 pixels. We use the AdamW optimizer with a learning rate of 2 105 and train the model forthe warmup and total epoch of 1 and 100, respectively. Thedimension of feature space is set to 256. For our SMS loss,the margin is set to 0.6 and relaxation factor is set to0.1.",
  ". Ablation Study and Competition Result": "To verify the effectiveness and robustness of our SMS loss,we conduct an ablation study on both our ViT-B-based andViT-L-based models. The experiment results are presentedin . Note that we report the rst three signicant dig-its without rounding, which may result in slight differencesfrom the public leaderboard.All experiments across different loss functions are con-ducted under the same learning rate and optimizer settings.We use the best-performing hyperparameters for each lossfunction. Specically, a margin of 0.2 for the MI-MM lossand 0.4 for the adaptive MI-MM loss.",
  ". Ablation study result of loss functions and tricks on EK-100 dataset": "For both the ViT-B-based and ViT-L-based models, ourSMS loss demonstrates superior performance compared toits counterparts. Specically, for the ViT-B-based model,our SMS loss improves the average mAP by 1.9% and theaverage nDCG by 2.4% compared to the adaptive MI-MMloss. Similarly, for the ViT-L-based model, our SMS lossalso improves the model on the average mAP by 2.3% andon the average nDCG by 2.4%. We can also observe a stableimprovement after the ip function is applied. Concretely,the performance of our model improves by 0.5% on bothaverage mAP and nDCG.Additionally, we conducted an experiment on the ViT-B-based model with = 0 to illustrate the necessity of the re-laxation factor. We observed that the performance droppedby 1.8% on average mAP and 0.8% on average nDCG com-pared to when = 0.1. This highlights the importance ofthe relaxation factor in achieving optimal performance.",
  ". Ensemble Strategy": "details the settings of the individual models usedin our ensemble learning approach. The parameter lr-endrepresents the minimum learning rate during the trainingphase, and all the adaptive MI-MM loss-based methods aretrained for 100 epochs. We employ a straightforward en-semble strategy by directly summing the similarity matricesobtained from all models. All the models are augmentedduring inference phase by the ip function.Due to time constraints, we trained only one SMS-basedmodel ( = 0.1) for 100 epochs. The remaining two mod-els are ne-tuned for 20 epochs based on the initial SMS-based model ( = 0.1). Compared to the single model, ourensemble model outperforms the best individual models by1.1% on average mAP and 0.6% on average nDCG.",
  ". Conclusion": "In this report, we explore the loss function in depth toachieve the precise learning objective of EK-100 Multi-Instance Retrieval Challenge. According to the obtainedlearning objective, we present a novel loss function calledSymmetric Multi-Similarity Loss, which addresses the lim-itations of existing loss functions. Furthermore, we imple-ment a simple trick on the inference phase to further en-hance our models performance. By combining a more ac-curate loss function, inference tricks, and ensemble learn-ing, our model achieved leading performance among theparticipants.Limitations: Due to the time limit, we are unable to con-duct experiments with a wider range of settings, but directlyusing the same settings as that for adaptive MI-MM loss.However, the gradient of the SMS loss is much smaller thanthat of the adaptive MI-MM loss since R cij. Therefore,using a higher learning rate could yield better results.Additionally, SMS loss requires an extra cik, meaningthat a B B relevancy matrix for every batch size B isneeded, while it is not readily available from the dataloader.Our current approach is to collect it from le during the losscalculation phase, which increases the training time. Weplan to address this issue in future work. Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Sanja Fidler, Antonino Furnari, Evangelos Kazakos, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, andMichael Wray. Scaling egocentric vision: The epic-kitchensdataset. In ECCV, 2018. 1, 3 Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Antonino Furnari, Jian Ma, Evangelos Kazakos, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, andMichael Wray.Rescaling egocentric vision: Collection,pipeline and challenges for epic-kitchens-100. IJCV, 130:3355, 2022. 1, 3 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video.InCVPR, pages 1899519012, 2022. 3 Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, MichaelWray, Rui Yan, Eric Z. XU, Difei Gao, Rong-Cheng Tu,Wenzhe Zhao, Weijie Kong, Chengfei Cai, WANG HongFa,Dima Damen, Bernard Ghanem, Wei Liu, and Mike ZhengShou. Egocentric video-language pretraining. In NeurIPS,pages 75757586, 2022. 1, 2"
}