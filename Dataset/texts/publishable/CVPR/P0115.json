{
  "Abstract": "Neural Radiance Fields (NeRFs) have unmatched fidelityon large, real-world scenes. A common approach for scal-ing NeRFs is to partition the scene into regions, each ofwhich is assigned its own parameters. When implementednaively, such an approach is limited by poor test-time scal-ing and inconsistent appearance and geometry. We insteadpropose InterNeRF, a novel architecture for rendering a tar-get view using a subset of the models parameters. Ourapproach enables out-of-core training and rendering, in-creasing total model capacity with only a modest increaseto training time. We demonstrate significant improvementsin multi-room scenes while remaining competitive on stan-dard benchmarks.",
  ". Introduction": "Neural Radiance Fields (NeRFs) are a class of powerful,high-fidelity representations for 3D reconstruction achiev-ing unparalleled reconstruction accuracy. Unlike many ap-proaches, NeRF models are straightforward to train and re-silient to local minima while delivering impressive qualityacross a wide variety of scenes. Nevertheless, reconstruc-tion quality is inherently limited by model capacity, andnew methods are needed to scale beyond the memory andcompute limitations of present-day hardware. A natural way to increase capacity is to spatially partitionnetwork parameters based on geometry orcamera location. Geometry-based partitioning dividesthe scene into multiple regions, which each have their ownset of parameters. While a fraction of model parameters arerequired to render a target ray, the number and choice of pa-",
  "arXiv:2406.11737v1 [cs.CV] 17 Jun 2024": "rameters varies depending on ray origin and direction andocclusions within the scene itself. Camera-based partition-ing, on the other hand, assigns parameters to possible querycameras. While rendering is typically simpler and more ef-ficient, inconsistencies arise when multiple parameter setsredundantly represent the same scene content.We introduce InterNeRF, a high-capacity NeRF archi-tecture tailored to large, multi-room scenes. Key to our ap-proach is the concept of camera-centric parameter inter-polation: the efficient interpolation of network parametersbased on camera origin. In particular, we designate a subsetof model parameters as spatially-partitioned, loading andunloading parameters relevant to the active camera regionduring training. At rendering time, the appropriate modelparameters are loaded based on camera origin. Given suffi-cient training time, our method outperforms a state-of-the-art baseline by a wide margin on the Zip-NeRF dataset.",
  ". Related Work": "Large-Scale 3D Reconstruction: There is a large body ofwork tackling large scene reconstruction. This includes anumber of classic works that apply structure from motion toextremely large photo collections . Ever since neu-ral radiance fields emerged as the dominant paradigmfor novel view synthesis, a number of works have sought toalso scale NeRF to large scenes. BungeeNeRF com-bines satellite and ground level images in a single NeRF byappending residual blocks to represent progressively finerscales. Each block has its own output head that predictsresidual color and density, but earlier blocks are only trainedon coarser scale (aerial) images. Similar ideas for renderingat high resolution in a progressive manner are explored invariable bitrate neural fields and PyNeRF .Grid-Guided NeRFs initially trains instant-NGPwith a small MLP and then later incorporates a large view-dependent MLP, an approach that helps the model performbetter on large urban scenes. F2-NeRF proposes adap-tive space warping by finding local perspective warps thatshrink regions far from any camera (a generalization of theNDC transform to multiple cameras), which helps to allo-cate model capacity more efficiently in scenes with longcamera trajectories. VR-NeRF uses a customized cam-era rig to capture high resolution HDR footage that can beused to train a large-scale NeRF. Submodel NeRFs: Several works train independent NeRFsubmodels that are each responsible for representing a sub-set of the scene, and aggregate their predictions at inferencetime. Some works were motivated by efficiency or compo-sition . In Block-NeRF , street view data is rep-resented by submodels centered at street intersections, eachtrained on cameras within a fixed radius. Target views arerendered by compositing whichever submodels have visi- bility of the region, as estimated by an auxiliary visibilitynetwork. Mega-NeRF partitions a scene using an oc-tree, and trains each cell only on rays that pass through it.Scalable Urban Dynamic Scenes builds on this workby using LIDAR data to prune rays that do not terminatewithin the cell.ScaNeRF uses a similar tile-basedsubmodel partition, and additionally performs bundle ad-justment within a distributed parallel training framework.Streamable MERF distills a large NeRF into several sub-models that use a memory-efficient triplane representation.The work also features a deferred MLP whose parametersare interpolated based on the distance of the camera originfrom the submodel centers, similar to the way parametersare interpolated in our work.",
  ". Preliminaries": "NeRF: A NeRF is a neural network that represents a 3Dscene as a continuous mapping from a 3D spatial locationxi to a volumetric density i and color ci. Novel images ofthe scene are rendered by tracing rays through this neuralrepresentation, where each ray r(t) = o + t d is renderedaccording to the Gaussian quadrature approximation to thevolume rendering equation,",
  "where i = ti+1 ti.(2)": "Ray intervals are partitioned into non-overlapping intervals{[ti, ti+1]}, and a multilayer perceptron (MLP) is trained toestimate i and ci of these intervals from xi (taken to be thecenter of each interval) . This process is repeated, wherea proposal network learns to map a uniform sampling ofdistances {ti} to a coarse set of densities, which are thenresampled to produce new distances that are concentratedaround scene content . Instant-NGP and Zip-NeRF: Instant-NGP introduceda learned, multi-resolution datastructure for efficiently fea-turizing spatial coordinates throughout the scene. At coarsescales in the datastructure, spatial coordinate xi is trilin-early interpolated into a dense, multi-channel 3D grid toproduce feature vectors (which we refer to as grid features),and at fine scales xi is trilinearly interpolated into a 3D gridbacked by a hash table to produce a feature vector (whichwe call hash features). Instant-NGP works by querying aspatial coordinate across all scales and concatenating theinterpolated outputs to construct a feature vector. We writethis interpolation and concatenation as: zi = NGP(xi).A small MLP called the geometry MLP then takes thesefeature vectors and predicts a scalar density value i, and a . Our framework. (1) We partition the scene into a parameter grid and assign training cameras to each cell based on its origin.(2) For a given (training or target) view, we obtain mixing weights as the bilinear interpolation coefficients based on camera origin. (3)Each query point along the ray is used to index into a multi-resolution set of grid features per parameter set, with either explicit assignmentor a hash table. The mixing weights are applied here to yield a single set of features. (4) Each parameter set also has its own set of MLPweights, which are combined using the same mixing weights to form a new MLP. larger MLP takes both zi and the viewing direction of theray d as input to predict radiance ci = MLP(zi, d).Building on mip-NeRF and Instant-NGP, Zip-NeRF uses multisampling and reweighting to parameterize theconical sub-frusta along each ray using NGPs. This strategyachieves the anti-aliasing and scale-reasoning provided bymip-NeRF while being significantly faster to optimize andevaluate. Zip-NeRF still attains the highest quality in novelview synthesis, outperforming 3D Gaussian Splatting onphotos acquired with rectilinear lenses and additionally ableto accommodate distorted lenses (which 3DGS cannot).",
  ". InterNeRF": "We propose Interpolated NeRF (InterNeRF), a scalable,out-of-core approach for increasing model capacity with-out a corresponding increase in memory usage. We adoptthe concept of spatially-partitioned model parameters: eachparameter is broadly categorized as either INTERPOLATEDor SHARED, with INTERPOLATED parameters varying withcamera origin and SHARED remaining unchanged. To ren-der a particular camera, we interpolate INTERPOLATED pa-rameters within a local neighborhood and perform a for-ward pass similar to typical NeRF model.While interpolated parameters provide the model thefreedom to specialize to relevant camera views, they alsopermit origin-dependent geometry, and geometric coher-ence can suffer when all parameters are interpolated. Thusthe role of the SHARED parameters is to maintain a stablecoarse geometry, and we choose to share proposal networkparameters and all NGP grid features.",
  "Parameter grid: To determine the anchor locations for pa-rameter sets, we begin by establishing a 2D axis-aligned": "bounding box containing all training cameras. The bound-ing box is partitioned into Nx Ny non-isotropic grid cells.We assume that scenes are largely planar and thus omit apartitioning along the vertical axis. Cameras are then as-signed to cells based on their origin (see ). Note thatonly cells with a sufficient number of training cameras areinstantiated; if a cameras origin lies outside of an instan-tiated cell, cameras are reassigned to the nearest active cellinstead. Parameter mixing: Similar to Block-NeRF, we assigneach vertex in the parameter grid its own parameter set forinterpolation. However, Block-NeRF applies nearest neigh-bor interpolation, which introduces discontinuities at theboundaries between parameter sets and necessitates post-processing heuristics like visibility maps and image-spaceinterpolation to avoid popping artifacts. We instead ex-plore bilinear interpolation, which naturally results in acontinuous field of model parameters.We implement bilinear interpolation layer-by-layer, per-forming a forward pass of each layer with four param-eter sets followed by interpolation of their outputs.Inthe NGP component, four sets of hash features are re-trieved and interpolated at each resolution; in the MLP,each fully connected layer performs four multiply-adds fol-lowed by interpolation. This approach avoids constructingper-sample model parameters, which would significantly in-crease memory usage. Cell-by-cell training: To decouple memory usage from pa-rameter count, we optimize four parameter sets at a timecorresponding to a single grid cell. We optimize cells se-rially in a round robin fashion, loading and unloading par-titioned model parameters as needed. When optimizing acell, we by default use training cameras that lie within the",
  ".Images from BERLIN and NYC in the Zip-NeRFdataset, rendered by Zip-NeRF and InterNeRF with a 54 grid": "same cell. Rather than optimizing cells for a fixed numberof iterations, we vary the number of iterations proportion-ally to the number of cameras assigned to it. We find thisimproves quality and reduces floating artifacts. Ray reassignment: To increase the amount of training sig-nal each cell receives, we incorporate cameras from othercells during optimization. In particular, each training batchis constructed such that a fixed percentage pr of camera raysare sourced from neighbors at most Kr cells away. The in-terpolation weights are determined by projecting the cameraorigin to the closest point in the cell. We find that this strat-egy strongly reduces floating geometry immediately outsideof the training camera frustum.",
  ". Experiments": "Datasets: We evaluate our approach on two datasets: fourlarge multi-room scenes introduced by Zip-NeRF andthe four indoor and five outdoor scenes of the mip-NeRF360 dataset . All models were trained from scratch usingthe original, distorted photos with every 8th image held outfor test. To illustrate the higher fidelity of our approach, weuse images at double their typical resolution: 1752 1168for Zip-NeRF and 2456 1632 or 3120 2080 for mip-NeRF 360. All camera parameters have been estimated withCOLMAP at full resolution . We note that this datasetis not amenable rasterization methods such as 3D Gaussian",
  "Splatting, which assume a pinhole camera model": "Baseline: We compare our method to Zip-NeRF, a state-of-the-art NeRF method for reconstruction of large indoorspaces. Our implementation uses three networks: two forproposing ray intervals and one for color and density pre-diction. All three networks consist of a multiresolution hashgrid with up to 221 entries followed by a per-interval geom-etry MLP with 1 layer and 64 units. The last network addi-tionally employs an appearance MLP with 4 layers and 256units for predicting view-dependent color. Training Details: Outside of the parameter partitioningscheme proposed in this work, our model architecturematches that of the Zip-NeRF baseline. In experiments onthe Zip-NeRF dataset, we partition all parameters in the sec-ond proposal network and final density and appearance net-work and set pr = 0.3, Kr = 1. In experiments on themip-NeRF 360 dataset, only the final networks parametersare partitioned and we set pr = 0.8, Kr = 4.To maintain a similar memory footprint with the base-line, we reduce the number of multiresolution hash grid en-tries per partitioned variable by 4. This ensures that ourmethod uses roughly as much device memory as the base-line, even if the total number of parameters is larger. Comparison to Baseline: On the Zip-NeRF dataset, ourmethod achieves significantly higher quality than the Zip-NeRF baseline at 400,000 steps; see Tab. 1. Qualitatively,this translates to higher geometric and texture detail; see. We further observe a clear, positive relationship be-tween model quality, training time, and number of cells.In contrast, baseline quality does not improve with train-ing time. This strongly suggests model capacity, rather thanoptimization time, to be Zip-NeRFs limiting factor.We perform a similar study on a subset of the mip-NeRF360 dataset, where a similar relationship is lacking; see Ta-bles 2 and 3. While our method modestly outperforms thebaseline at 200,000 steps in terms of PSNR and SSIM, anegative relationship between LPIPS and number of cellsis evident, as each parameter set receives fewer trainingiterations as the number of parameter sets grows under afixed total training budget. This study further suggests thatZip-NeRFs model capacity is sufficient for medium-sizedscenes. Runtime Analysis: Although our method achieves higherreconstruction quality, the current implementation trainsmore slowly than its Zip-NeRF counterpart. The reason forthis is twofold: first, our method applies each partitionedlayer four times to the baselines one; second, partitionedparameters are swapped in and out of device memory duringtraining. Based on back-of-the-envelope calculations, webelieve the time required for the latter to be largely avoid-able with appropriate optimizations. We further believe thenumber of training steps can be significantly reduced by tak-",
  ". Conclusion": "In this work, we have introduced InterNeRF, a scalable, out-of-core NeRF model architecture for reconstructing large,multi-room scenes. We demonstrated that parameter inter-polation is an effective approach for increasing model ca-pacity without a corresponding increase to memory or com-pute requirements. While our method demonstrates impres-sive quality, additional work is needed to reduce trainingtime, compare our approach to other submodel-based ap-proaches, test other interpolation schemes, and validate ourmethod on even larger (e.g. city-scale) scenes.",
  "Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu,Taku Komura, Christian Theobalt, and Wenping Wang. F2-nerf: Fast neural radiance field training with free camera tra-jectories, 2023. 2": "Xiuchao Wu, Jiamin Xu, Xin Zhang, Hujun Bao, Qix-ing Huang, Yujun Shen, James Tompkin, and Weiwei Xu.ScaNeRF: Scalable bundle-adjusting neural radiance fieldsfor large-scale scene rendering.ACM Transactions onGraphics (TOG), 2023. 1, 2 Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.Bungeenerf: Progressive neural radiance field for extrememulti-scale scene rendering.In European conference oncomputer vision, pages 106122. Springer, 2022. 2 Linning Xu, Vasu Agrawal, William Laney, Tony Garcia,Aayush Bansal, Changil Kim, Samuel Rota Bul`o, LorenzoPorzi, Peter Kontschieder, Aljaz Bozic, Dahua Lin, MichaelZollhofer, and Christian Richardt. VR-NeRF: High-fidelityvirtualized walkable spaces. SIGGRAPH Asia, 2023. 2",
  ". Training Details": "Training: We implement our method and the Zip-NeRFbaseline by building on the camp zipnerf codebase.Models are trained using sixteen A100s for up to 24 hours.We optimize all models with a batch size of 216 rays us-ing the Adam optimizer. Unless otherwise stated, hyperpa-rameters match those described in the Zip-NeRF paper .We employ an exponentially decaying learning rate sched-ule with an initial learning rate of 1e-2 and a final learningrate of 1e-3 or 1e-4 on the Zip-NeRF and mip-NeRF 360datasets, respectively. We further introduce a linear learn-ing rate warm-up schedule of 2,500 steps starting with aninitial learning rate of 1e-8. We apply L2 regularization toall hash grid parameters with a weight of 0.001 for proposalnetworks and 0.1 for density and appearance networks onthe Zip-NeRF dataset and 0.1 for all networks on the mip-NeRF 360 dataset. We further force volumetric renderingweights along each camera ray to sum to unity. Model Architecture: All models, including the Zip-NeRFbaseline and InterNeRF, are composed of three networks:two proposal networks and one appearance and density net-work. Each network, in turn, is composed of up to threecomponents: a multiresolution hash grid, a Geometry MLP,and potentially an Appearance MLP. All three networkscontain the first two components; only the last contains anAppearance MLP. The shape of these MLPs is identical forall methods as described in the main text.The majority of each methods parameters lie in theirmultiresolution hash grid. We describe the number of hashresolution levels, number of entries per level, number of fea-tures per entry, and spatial resolution of these multiresolu-tion grids in Tables 4 and 5. We further indicate which mul-tiresolution grids are spatially-partitioned and interpolatedfor our method.",
  ". Multiresolution hash grid sizes on the Zip-NeRF dataset": "timized one at a time. We optimize each cell k for 2 Nkiterations, where Nk is the number of training cameras allo-cated to this cell. When a new grid cell is chosen, SHAREDparameters are left in memory while the appropriate INTER-POLATED are swapped in. To order cells, we assign eachcell a linearized integer identifier based on its position in the2D scene grid and loop over cells according to this identi-fier. When loading a new cell for training, INTERPOLATEDvariables along with associated Adam statistics are loadedinto memory.Dataset: Unlike the majority of prior work, we use high-resolution versions of the mip-NeRF 360 and Zip-NeRFdatasets with the original lens distortion. This enables adeeper exploration of model capacity a key goal of thiswork but prevents metrics from being directly compara-ble to prior publications. The use of distorted photos furtherlimits our ability to compare to rasterization-based methodssuch as 3D Gaussian Splatting . Concretely, we use pho-tos at double the resolution used in prior work. For themip-NeRF 360 dataset, this means full-resolution photosfor indoor scenes and 2 downsampled photos for outdoorscenes. For the Zip-NeRF dataset, we use 2 downsampledphotos for all scenes except for BERLIN, where we use 4downsampling.",
  ". Additional Results": "Overall, we find a strong positive correlation betweenmodel capacity and texture detail on multi-room scenes asdemonstrated in . Our method consistently providescrisp detail on textured surfaces including painting, carpets,and curtains. The same cannot reliably be said, however,for the mip-NeRF 360 dataset. In the majority of scenes,our method is indistinguishable from the Zip-NeRF base-line as in ."
}