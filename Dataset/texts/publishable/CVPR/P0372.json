{
  "Abstract": "Determining the location of an image anywhere on Earthis a complex visual task, which makes it particularly rel-evant for evaluating computer vision algorithms. Yet, theabsence of standard, large-scale, open-access datasets withreliably localizable images has limited its potential. To ad-dress this issue, we introduce OpenStreetView-5M, a large-scale, open-access dataset comprising over 5.1 million geo-referenced street view images, covering 225 countries andterritories. In contrast to existing benchmarks, we enforcea strict train/test separation, allowing us to evaluate therelevance of learned geographical features beyond merememorization. To demonstrate the utility of our dataset,we conduct an extensive benchmark of various state-of-the-art image encoders, spatial representations, and trainingstrategies. All associated codes and models can be found at",
  ". Introduction": "While natural image classification is the standard for evaluat-ing computer vision methods , global geolocationoffers a compelling alternative task. In contrast to classifi-cation, where the focus is often a single object, geolocationinvolves detecting and combining various visual clues, likeroad signage, architectural patterns, climate, and vegeta-tion. Predicting a single GPS coordinate or location labelfrom these observations necessitates a rich representationof both the Earths culture and geography; see forsome examples. Furthermore, the abundance of geo-taggedstreet-view images depicting complex scenes with a clearand consistent point of view makes this task appropriate fortraining and evaluating modern vision models.Despite this potential, few supervised approaches are",
  "climate/vegetationtraffic markersarchitectureculture/script": ". Global Visual Geolocation. Predicting the location ofan image taken anywhere in the world from just pixels requiresdetecting a combination of clues of various abstraction levels .Can you guess where these images were taken?1 trained and evaluated for the task of geolocation.Weattribute this to the limitations of existing geolocationdatasets: (i) Large and open geolocation datasets containa significant portion of noisy and non-localizable images; (ii) Street view datasets are better suited forthe task but are both proprietary and expensive to down-load . To address these issues, weintroduce OpenStreetView-5M (OSV-5M), an open-access",
  "non-localizablelandmarkslocalizable": ". Localizable vs Non-Localizable. Images from ourdataset (green) occupy the space between weakly localizable im-ages (red) like the ones from the test set of Im2GPS3k andlandmark images used to advertise CV conferences (blue). dataset of 5.1 million high-quality and crowd-sourced streetview images. Our ambition is to make both street view im-ages and global geolocation new standards for measuringprogress in deep learning.Automating visual geolocation has significant potentialbenefits, with direct applications in fields such as journalism,forensics, as well as historical and cultural studies. Learningrobust geographical representations may also be valuable forvarious deep learning challenges, including self-supervisedlearning and generative modeling, or the development ofmore interpretable AI systems. Thanks to its size and scope,and its strict train/test split, OSV-5M serves as a robust andreliable benchmark for computer vision models. To demon-strate this, we design an extensive evaluation experiment tomeasure the impacts of various factors such as pretrainingstrategies, model scale, spatial representations, fine-tuningapproaches, contrastive losses, and auxiliary tasks.",
  ". Localizability": "As noted by Izbicki et al. , images exhibit a range oflocalizability, an inherently perceptual concept, see .Non-localizable images lack information that connects themto a specific location or are of too low quality to properlyanalyze. Weakly localizable images only contain vague orindirect hints, such as people, animals, and objects in indoorscenes. Localizable images should contain enough informa-tion to allow for an informed guess relative to their location.For example, street view images are generally localizableas they typically contain salient features indicative of thelocal environment such as climate, nature, architecture, orutility and regulatory infrastructure. At the far end of thespectrum, landmark images showcase emblematic monu-ments or iconic landscapes, making their location instantlyidentifiable to most viewers.According to this criteria, a visual inspection suggests that35% of the images in Im2GPS3k, a dataset commonly used to benchmark geolocation methods , are non-localizable.When used for evaluation, this may lead to unreliable errorsor promote methods that have memorized biases of the train-ing distribution. When used for training, non-localizableimages can lead to sub-optimal representations or encouragespurious correlations. OSV-5M predominantly compriseslocalizable street view images whose accurate geolocationrequires robust geographical representations.",
  "We motivate the need for OSV-5M by reviewing existinggeolocation datasets from the two main sources of geotaggedimages: web-scraped and street view images, see": "Web-Scraped.Image hosting platforms like Flickr pro-vide a near-endless source of geotagged images, which hasbeen used to create large open datasets, like YFC100M .Most images correspond to personal or amateur photographsrepresenting food, art, and images of pets and friends, and areeither weakly or non-localizable. Even strongly localizableimages are typically taken in tourist spots, injecting an oftenWestern cultural bias towards recognizable landmarks .The provided location metadata can be occasionally missingor inaccurate, and the online nature of these images impliesthey can be easily removed, hindering reproducibility2. Forevaluation purposes, cleaner subsets have been proposed thatimprove both the image distribution coverage and annotationquality , but remain still heavily biased and predomi-nantly non-localizable. Despite their small scope and size,these datasets are currently the primary means of evaluatinggeolocation models. Street View.Conversely, street view images tend to bestrongly localizable. Captured through panoramic camerasor dash-cams, they depict in high quality a vehicles sur-roundings, which corresponds mostly to outdoor scenes withrich geographical cues. Google famously provides a globalstreet view coverage, which is, however, expensive to ac-quire for academic purposes ($1000 for 150k images) andcannot be shared. Existing open datasets from this sourceeither only consist of dense samples from 3 US cities meantfor navigation , or are inaccessible .Luckily, crowd-sourced platforms such as Mapillary offer a global and diverse source of open-access street viewimages for various environments, from dense cities and sub-urbs to remote and inhabited landscapes. These imageshave been used to construct several benchmarks for multipletasks other than geolocation, including depth estimation ,semantic segmentation , traffic sign detection and classi-fication , place recognition and visual localization. With 5.1M Mappilary images taken across the globe,OSV-5M is the largest open-access street-view image dataset",
  ". Geolocation Methods": "Place recognition and visual localization are popular tasks that consist in finding the pose ofimages in a known scene. In contrast, visual geolocationpredicts 2D coordinates or discrete locations (e.g., countries),and aims for lower accuracy and the ability to generalize tounseen areas . Existing geolocation approaches can becategorized by whether they treat geolocation as an imageretrieval problem, a classification problem, or both. Image Retrieval-Based Approaches.A straightforwardmethod for image localization is to find the most similarimage in a large image database and predict its location .The first successful approaches involved retrieving the near-est image in a space of handcrafted features such as colorhistograms , gist features , or textons . It waslater improved with SIFT features and support vector ma-chines . Deep features further boosted the performanceof these approaches . While such models typically ex-hibit high performance given a large and dense enough im-age database, they do not involve representation learning.Consequently, unless provided with robust features, theymay perform poorly in sparsely represented or dynamicallychanging environments. Classification-Based Approaches.Geolocation can alsobe approached as a classification problem by discretizinglatitude and longitude coordinates. The choice of partition iscritical, ranging from regular , adaptive , semantic- driven , combinatorial , administrative , andhierarchical partitions. Classification-based meth-ods must strike a delicate balance between the quantity andsize of cells; if the discretization is too coarse, the perfor-mance will be limited, while too many small cells may nothave enough samples for learning-based methods. Further-more, a typical classification loss such as cross-entropy doesnot incorporate the distance between regions: confusing twoadjacent cells is equivalent to mistaking the continent. Hybrid Approaches.Retrieval and classification ap-proaches can be combined to overcome the limitations ofdiscretization. This can be achieved using ranking losses or contrastive objectives . Haas et al. followa classification-then-regression approach based on proto-type networks. Finally, Izbicki et al. go beyond single-location prediction by estimating probability distributionsbased on spherical Gaussians.",
  ". OpenStreetView-5M": "OpenStreetView-5M establishes a new open benchmark forgeolocation by providing a large, open, and clean dataset.The Appendix details the construction of the dataset. Asdetailed below, OpenStreetView-5M improves upon severallimitations of current geolocation datasets. Scale. Deep neural networks have historically been selectedover other machine learning methods because they benefitfrom larger amounts of data. OSV-5M consists of 4,894,685training and 210,122 test images, with a height of 512 pixelsand an average width of 792 127 pixels. Scope. Many geolocation datasets are restricted to a fewcities or are significantly biased towards the West-ern world . In contrast, OpenStreetView-5M images areuniformly sampled on the globe, covering 70k cities and 225countries and territories, as shown in . The distribu-tion of test images across countries has a normalized entropyof 0.78 [73, Eq. 19], suggesting high diversity. Our train sethas a normalized entropy of 0.67, which is comparable tothe entropy of the distribution of the countries area (0.71).",
  "Access. OpenStreetView-5M is based on the crowd-sourcedstreet view images of Mapillary which follow the CC-BY-SA license: free of use with attribution": "Quality Evaluation. We estimate through manual inspec-tion of 4500 images that 96.1% (0.57%) of the images inthe OpenStreetView-5M dataset are localizable, with a 95%confidence level [31, Chap. 8]. Among the weakly or non-localizable images, 70% (2.7% total) are low-quality: under-or over-exposed, blurry, or rotated; 30% (1.2% total) arepoorly framed, indoor, or in tunnels.",
  "(d) Continent and country distributions of the training set": ". OpenStreetView-5M. Image density and proportions per country and continent for the train and test sets. To ensure an unbiasedevaluation, we prioritize the uniformity of the test sets distribution across the globe over the training set distribution. Spatial Separation. Without carefully enforcing the spatialseparation between train and test images, geolocation canreduce to place-recognition. As our goal is to assess thecapacity of models to learn robust geographical representa-tions, we ensure that no image in the OSV-5M training setlies within a 1km radius of any image in the test set. Sequence Separation. Street-view images are typicallyacquired by a limited number of camera sensors mountedon the top or front of a small fleet of vehicles assigned to agiven region. This correlation between location, cars, andsensors can be exploited to simplify the geolocation task.Notoriously, players of the web-based geolocation gameGeoGuessr can locate images from Ghana by spottinga piece of duct tape placed on the corner of the roof rackof the Google Street View car . OpenStreetView-5Mtries to avoid this pitfall by ensuring that no image sequence(a continuous series of images acquired by the same user)appears in both training and test sets. While this might notprevent images taken with the same vehicle on different daysfrom being in both sets, it limits such occurrences. Metadata. Rich metadata beyond geographical coordinatescan improve the robustness and versatility of geolocationmodels. Each image in our dataset is associated with fourtiers of administrative data: country, region (e.g., state), area(e.g., county), and the nearest city . Note that areas arenot defined for one-third of the dataset. We also associateeach image with a set of additional information: land cover,climate, soil type, the driving side, and distance to the seawhere the image was taken. See the Appendix for more",
  ". Benchmark": "We use OSV-5M to benchmark supervised deep learningapproaches in the context of visual geolocation. We firstpresent our evaluation metrics (.1) and framework(.2). We then explore several design choices, start-ing with the image encoder backbone (.3), theprediction objective (.4), the fine-tuning strategy(.5), and the choices of contrastive losses (Sec-tion 4.6). In each experiment, we select the top-performingdesigns and integrate them into a combined model, whichwe evaluate and analyze in .7.",
  ". Evaluation Metrics": "We denote the space of images by I and the span of longitudeand latitude coordinates by C = .Our objective is to design a model that maps an image from Ito its corresponding location in C. We measure the accuracyof predicted location across geolocation models with threecomplementary sets of metrics:- Haversine distance , between predicted and groundtruth image locations;- Geoscore, based on the famous GeoGuessr game , de-fined as 5000 exp(/1492.7) ;- Accuracy of predicted locations across administrativeboundaries: country, region, area, and city.While the average distance between predictions andground truth is sensitive to outliers (i.e., a few poor pre-dictions can significantly undermine an otherwise high-performing algorithm), the accuracy metric based on admin-istrative borders can avoid this issue. However, this metric",
  "Auxiliary Tasks": ". Visual Geolocation Model. We propose a simple and versatile framework for visual geolocation and explore the impact of variouscomponents of this approach in train-test performance on OpenStreetView-5M. Starting from the left, the input image is converted to avector representation by an image encoder f img (red). Then a geolocation head f loc maps this vector to a set of geographical predictions(mint). Then a contrastive objective is potentially added (cyan), as well as auxiliary targets to learn better representations for geolocation(lila). We also consider various parameter fine-tuning strategies for training our image encoder, by freezing all or part of f img (yellow). . Impact of Image Encoder. Several pretrained backbones are evaluated in OpenStreetView-5M. We outline the influence of variousarchitectures, pretraining strategies, and datasets. Best scores are highlighted in bold. We denote closed datasets with .",
  ". Framework": "The models evaluated in this benchmark follow a consis-tent architecture, represented in . All considerednetworks contain the two following modules:- the image encoder f img : I Rd, which maps an imageto a d-dimensional vector;- the geolocation head f loc : Rd C, which maps thisvector to geographic coordinates. Implementation details.Unless stated otherwise, f img isalways a pretrained and frozen CLIP ViT-B/32 model with d = 768 and f loc is a Multilayer Perceptron (MLP)with GroupNorms . This base model directly regressesgeographical coordinates and uses the L1 norm as loss func-tion. The model is trained with a batch size of 512 imagesfor 30 epochs (260k iterations) with a fixed learning rate of2 104. Throughout the paper we will denote in blue thefrozen base model, in orange its fine-tuned version, and ingreen the model combining all top-performing designs.",
  ". Image Encoder": "We first benchmark various architectures for the image en-coder module f img, with varying backnones, and pretrainingstrategies and datasets:- Architecture. We test a standard ResNet50 , and mod-ern ViTs of multiple sizes (B-32, L-14, and bigG-14).- Pretraining.We consider different types of pretrain-ing objectives, including classification on ImageNet, self-supervized pretraining DINOv2 , text supervision CLIP, as well as StreetCLIP , which is finetuned specifi-cally for geolocation.- Dataset. We consider several pretraining datasets, includ-ing LAION-2B , DATA COMP , Meta-CLIP ,and the proprietary datasets of DINOv2, OpenAI, and Street-CLIP . Analysis.Our experimental results are presented in Ta-ble 2. Here, we summarize several key takeaways:- Model Size. As shown in Rows 1, 2, 4, and 9 of ,there is a direct correlation between the size of the imageencoder and its geolocation performance. The large ViT,bigG-14 model with 1.8 billion parameters (Row 9) improvessignificantly on the performance of its smaller versions. As . Prediction Modules. We report the performance of var-ious prediction models and objectives. QuadTrees, hierarchicalsupervision, and hybrid models all significantly improve on di-rect regression or classification with administrative borders. Weunderline the accuracy for divisions that the method is specificallytrained to categorize.",
  "Hybrid11.0k3036251860.836.39.55.7": "the size of models correlates with their training time, weselect ViT-L-14 as the best compromise.- Pretraining. As seen in rows 3, 7, and 8, CLIP pretrainingleads to better results than DINO or image classification. Wethus focus on the latter for further comparisons.- Dataset. Rows 4 to 8 show the significant impact of thechoice of pretraining datasets. The geolocation-orientedStreetCLIP (row 8) leads to the best results, followed byOpenAIs CLIP (row 7). However, both datasets are not openaccess. We choose DATA COMP (row 5) as the best open-source dataset for its slightly better country classificationrate compared to Meta-CLIP (row 6).",
  "We examine three different possible supervision schemes forthe geolocation head f loc: regression, classification (includ-ing hierarchical classification), and a hybrid approach": "Regression.We start with the most straightforward ap-proach: f loc directly regresses coordinates in C. We train anMLP supervised with the L1 loss between true and predictedcoordinates. To account for the periodicity of the latitude,we also test an approach where we regress instead the cosineand sine of the longitude and latitude and then recover thereal coordinates with trigonometry . Classification.We divide the train set into a set K of Kdivisions, such as countries, regions, areas, and cities, whichamount to K = 222 , 2.8k, 9.3k, and 69.8k, respectively. Assome administrative borders can have vastly different sizes,we also consider an adaptive partition with a QuadTree ofdepth 10 and maximum leaf size of 1000, correspondingto 11k cells. We then train a classifier f classif : Rd RK",
  "Fine-tuning88.01322893208554.919.11.60.8": "each division with the average location of its training im-ages: f lookup : K C. The predicted geolocation can besummarized as: f loc = f lookup arg max f classif.In our implementation, f classif is an MLP trained withcross-entropy, while f lookup is a look-up table obtained di-rectly from the training set. Hierarchical Supervision.We can exploit the nested na-ture of the administrative divisions and QuadTree cells tosupervise all levels simultaneously . More precisely,we predict a probability vector at the finest resolution (eithercity or maximum depth of the QuadTree), which we aggre-gate recursively to obtain predictions at all levels. We cannow supervise with a cross-entropy term for each level. Hybrid Approach.Inspired by approaches that combineboth classification and retrieval , we perform regres-sion and classification in a two-step approach. Given theoutput of our QuadTree classifier f classif : Rd RK, wedefine f relative : Rd 2K that outputs the relativecoordinates of the predicted location inside each cell k. Wescale these values such that (0, 0) points to the centroid ofthe training images in the cell and 2 spans the entirebounding box. Using the cell prediction of the classifierf classif and the relative position from f relative, we can predictthe location of the image with sub-cell precision.We train f classif with the cross-entropy, and f relative withthe L2 loss between the predicted and true relative coordi-nates on the division that contains the true location. Analysis.We report the performance of different predic-tion heads in , and make the following observations:- Regression. Predicting sines and cosines does not improvethe regression models performance. We hypothesize thatthis is due to the non-linearity of the trigonometric formula.- Classification. Classification methods generally performwell in Geoscore and starkly improve their respective clas-sification rates, e.g. +23.2% region accuracy for the regionclassifier compared to the regression model. However, theirinfluence on the average error distance is smaller. Coarsepartitions, like countries, are limited by the low precision off lookup. Inversely, overly refined partitions such as cities leadto a more challenging classification setting where most labels",
  "text-based2812217166.013.00.70.2": "have only a few training examples. QuadTree-constructed la-bels achieve performance close to the administrative division-based classifier across all levels, e.g. 54.8% vs. 56.3% forcountries and 27.7% vs. 30.2% for regions. This compoundsinto an overall better performance, which shows that adapt-ing the granularity of the label distribution according to theimage density appears to be a successful heuristic.- Hierarchical & Hybrid. Supervising on all levels simulta-neously significantly improves the prediction. Hybrid meth-ods bridge the gap between classification and regression,yielding high precision without relying on very fine-grainedpartitions. These results validate the underlying spatial hi-erarchical nature of geographical data . We select bothhybrid and hierarchical designs for the combined model.",
  "We evaluate different fine-tuning strategies to quantify theimpact of learning dedicated features for geolocation. In allconfigurations, we learn f loc from random weight, and f img": "is fine-tuned as follows:- Frozen. f img is initialized with pretrained weights andremains frozen.- LoRA-32.We fine-tune f img using Low Rank Adap-tion and a rank of 32 (more values in supplementary).- Last block. We unfreeze the last transformer block of f img,responsible for producing the image embedding.- Fine-tuning. We fine-tune all parameters of f img. Analysis.In , we report the impact of differentfine-tuning strategies. Training only the last transformerblock instead of using LoRA leads to a ten times largerGeoscore improvement in only half the training time. Thissuggests that pretrained models can extract relevant patchembeddings, while image encoding must be significantlyadapted for geolocation. Fine-tuning the entire networkleads to an even larger improvement but a five-fold increasein training time. However, the resulting performance iscomparable to the frozen ViT-bigG-14 shown in andtrains 9 times faster. We select the fine-tuning configurationas the top-performing approach and denote it in orange.",
  ". Contrastive Objectives": "Contrastive learning builds positive and negative samplepairs from the training set and pushes representations ofpositive pairs close to each other while contrasting negativeones . Positive pairs can be formed within the samemodality, such as different views of an object, or acrossmodalities, such as images and captions. In the geolocationcontext, we propose two approaches to construct such pairs:- Geographic. We match images if they are within the sameadministrative division: countries, regions, areas, cities, orQuadTree cells. We modify the dataloader to ensure eachimage is part of at least one positive pair. Contrary to Haaset al. , we use the multi-positive MIL-NCE loss asour contrastive objective to account for images in severalpositive pairs, e.g. in the same country.- Text-Based. Similar to Haas et al. , we pair eachimage with a textual description of its location formed as thefollowing string: An image of the city of $CITY, in the areaof $AREA, in the region of $REGION, in $COUNTRY.. Analysis.In , we measure the impact on the fine-tuned model of different approaches for constructing con-trastive pairs. We observe a consistent improvement in termsof performance when building positive pairs with regions,which may be the division most likely to present unique andhomogeneous visual and cultural identities. In contrast, ar-eas appear to hurt the performance when used contrastively.Overall, contrastive learning yields a much higher countryand region classification rate compared to the classification-based approaches of , suggesting that encouraginggeographically consistent representations is advantageousfor geolocation. We also observe that using text as a proxywhen geographically consistent pairs exist is not beneficial.",
  "Combined model2734260854.924.513.69.4": "into a strong geolocation model, denoted in green: ViT-L-14backbone pretrained on DATA COMP, QuadTree partitionwith hybrid prediction and hierarchical supervision, fullyfine-tuned with a region-contrastive loss. As shown in Ta-ble 6, this model starkly improves on the base model, withan increase of +1309 in Geoscore, an average distance re-duced by 45%, and significantly better accuracy at all levelsof administrative divisions. Analysis.In , we compare the performance of ourcombined model to a random baseline (select the locationof a random image in the training set) and a human per-formance obtained by asking 80 annotators to guess thelocations of the same 50 images randomly sampled from thetest set .Despite the difficulty of the task, the averageannotators performance is significantly better than chance.Our baseline model, and more substantially our combinedmodel, far surpasses the accuracy of annotators. We alsoevaluate two state-of-the-art geolocation models: StreetCLIP evaluated in zero-shot using the text string given in Sec-tion 4.6, and the GeoEstimator model fine-tuned on ourtraining set. As both models are designed for geolocation,they yield good performance. Owing to its bespoke geo-cells, GeoEstimator reaches the highest accuracy for areaclassification, illustrating the benefit of architectures withbuilt-in geographical priors. See the appendix for furtherexperiments, notably on the impact of auxiliary variables. Nearest Neighbor.We perform retrieval by matching eachimage from the test set with an image from the train setbased on the cosine distance between the features of eachimage encoder. We perform approximate matching with theFAISS algorithm through the AutoFAISS package ,without re-ranking . As reported in , retrievalmethods trained through contrastive learning exhibit highperformance. However, the supervision of our combinedmodel based on geographic coordinates and cells does notenhance its retrieval performance. In fact, its retrieval scoreis lower than that of its pretrained image encoder. Thesefindings are consistent with observations that fine-tuningself-supervised models decreases retrieval performance .",
  ". Spatial Distribution of Errors. We plot the averageprediction error of the combined model in km across the globe": "Error Distribution.We report in a heatmap ofthe average error distance. Areas sparsely populated withtraining images, such as South America, have a significantlyhigher error rate. We report a Pearson correlation coefficientof 0.25 between image density and error, suggesting thatimage density is not the only factor in the mistakes of ourproposed model. See for a visualization of the errordistribution. Over half of the combined models predictionsare within 250km of the true image locations.",
  ". Conclusion": "We introduced a new open-access street view dataset of un-precedented size and quality, enabling the consistent trainingand evaluation of global geolocation models for the firsttime. Through an extensive experimental framework, wedemonstrate that our dataset is a competitive benchmark fordeveloping and evaluating general and bespoke state-of-the-art computer vision approaches for geolocation. Through itsscale and quality, we expect OSV-5M to also be useful forself-supervised learning and generative modeling, valuabletasks beyond the scope of visual geolocation. Acknowledgements.OSV-5M was made possible throughthe generous support of the Mapillary team, which helped usnavigate their vast street view image database. Our work wassupported by the ANR project READY3D ANR-19-CE23-0007, and the HPC resources of IDRIS under the allocationAD011014719 made by GENCI. We thank Valerie Gouetfor her valuable feedback.",
  "Manuel Lopez Antequera, Pau Gargallo, Markus Hofinger,Samuel Rota Bul`o, Yubin Kuang, and Peter Kontschieder.Mapillary planet-scale depth dataset. In ECCV, 2020. 2": "Hylke E Beck, Niklaus E Zimmermann, Tim R McVicar,Noemi Vergopolan, Alexis Berg, and Eric F Wood. Presentand future Koppen-Geiger climate classification maps at 1-kmresolution. Scientific data, 2018. 15, 19 David M Chen, Georges Baatz, Kevin Koser, Sam S Tsai, Ra-makrishna Vedantham, Timo Pylvanainen, Kimmo Roimela,Xin Chen, Jeff Bach, Marc Pollefeys, et al. City-scale land-mark identification on mobile devices. In CVPR, 2011. 1",
  "Muller-Budack etal. Geolocation estimation of photos usinga hierarchical model and scene classification. In ECCV. 14": "Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, JonathanHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad,Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanu-jan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann,Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh,Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Ha-jishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, AlexDimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, andLudwig Schmidt. DataComp: In search of the next generationof multimodal datasets. NeurIPS Dataset and Benchmark,2023. 5",
  "Oisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-only geographical priors for fine-grained image classification.In CVPR, 2019. 6": "David Martin, Charless Fowlkes, Doron Tal, and JitendraMalik. A database of human segmented natural images andits application to evaluating segmentation algorithms andmeasuring ecological statistics. In ICCV, 2001. 3 Sneha Mehta, Chris North, and Kurt Luther. An exploratorystudy of human performance in image geolocation tasks. InHCOMP 2016 GroupSight Workshop on Human Computationfor Image and Video Analysis, volume 308, 2016. 1, 8",
  "Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, IvanLaptev, Josef Sivic, and Andrew Zisserman.End-to-endlearning of visual representations from uncurated instructionalvideos. In CVPR, 2020. 7, 17": "Piotr Mirowski, Andras Banki-Horvath, Keith Anderson,Denis Teplyashin, Karl Moritz Hermann, Mateusz Mali-nowski, Matthew Koichi Grimes, Karen Simonyan, KorayKavukcuoglu, Andrew Zisserman, et al. The StreetLearnenvironment and dataset. arXiv preprint arXiv:1903.01292,2019. 2, 3 Hatem Mousselly-Sergieh, Daniel Watzinger, Bastian Hu-ber, Mario Doller, Elod Egyed-Zsigmond, and Harald Kosch.World-wide scale geotagged image dataset for automatic im-age annotation and reverse geotagging. In ACM multimediasystems, 2014. 2, 3, 13",
  "sentation learning with contrastive predictive coding. arXivpreprint arXiv:1807.03748, 2018. 17, 18": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo,Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, DanielHaziza, Francisco Massa, Alaaeldin El-Nouby, et al. DinoV2:Learning robust visual features without supervision. TMLR,2023. 5 Nathan Piasco, Desire Sidibe, Valerie Gouet-Brunet, andCedric Demonceaux. Improving image description with auxil-iary modality for visual localization in challenging conditions.International Journal of Computer Vision, 2021. 3",
  "Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. In NeurIPSDatasets and Benchmarks Track, 2021. 1": "Pedro A Sanchez, Sonya Ahamed, Florence Carre, Al-fred E Hartemink, Jonathan Hempel, Jeroen Huising, PhilippeLagacherie, Alex B McBratney, Neil J McKenzie, MariaDe Lourdes Mendonca-Santos, et al. Digital soil map of theworld. Science, 2009. 15, 19 Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, LarsHammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Oku-tomi, Marc Pollefeys, Josef Sivic, et al. Benchmarking 6-DoFoutdoor visual localization in changing conditions. In CVPR,2018. 3 Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,Aarush Katta, Clayton Mullis, Mitchell Wortsman, PatrickSchramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generationimage-text models. 2022. 5",
  "Xiwu Zhang, Lei Wang, and Yan Su. Visual place recognition:A survey from deep learning perspective. Pattern Recognition,2021. 3": "This supplementary material starts by providing fur-ther details on the construction and analysis of our datasetOpenStreetView-5M in Section A, showcasing indicativesamples in Figure A. Then, we provide additional ex-periments in Section B and qualitative results in Fig-ure B. Finally, Section C further implementation detailscan be found and Section D outlines our Datasheet forOpenStreetView-5M.A. OpenStreetView-5M Dataset OpenStreetView-5M is designed to achieve an open, large-scale, balanced, and global geographical coverage. Throughthe Mapillary API and the support of the Mapillary team,we gained access to the locations of all 1.8B images . Toprovide a more manageable and better distributed dataset,we design a specific construction approach, presented in thissection. The code to reproduce the treatment can be foundat",
  "A.1. Construction Approach": "Sampling.We start by ensuring that regions with highimage density are not disproportionately represented. Wedefine a 100 100m grid across the entire world and ran-domly choose one image per cell. Then, both the trainingand test sets are sampled with a weight proportional to thelocal image density raised to the power of 0.75. Such astrategy balances density-based sampling (which tends tobe biased towards urban centers) and area-based sampling(which might favor larger countries). We eliminate imagesfrom the test set that are either located within a 1km radiusof any train image or share a sequence ID. Handcrafted Filters.We apply a series of handcraftedfilters to remove low-quality images- Blurriness. Blurry images indicate low quality and poten-tially low localizability. We remove images whose averagelogarithmic magnitude spectrum is below 120dB.- Radiometry.Certain images hosted on Mapillary aretoo dark to be meaningfully analyzed, while other havea distinct encoding errors giving them a purple tint. Toremove those, we first filter out images whose averagebrightness (average value over pixels and RGB channels)is below 50. To handle purple images, we remove imagesfor which over 50% of pixels meet the following criteria:R > 60 & G > 60 & B < 50.- Exposition. The exposure of Dash-cam images can bebadly exposed, for example, when they face the sun. Tofilter them, we remove images for which 70% of pixels havea brightness over 250 (overexposed) or under 5 (underex-posed). Rotation-Based Filtering.We perform a learning-basedfiltering based on a pretrained and frozen RotNet network. This model learns self-supervised image representa-tions by training for the pretext task of predicting a randomrotation applied to an input image. Although it it used as apretext task in the original paper, it becomes useful for filter-ing out images downloaded from Mapillarys website thatare incorrectly rotated. We use the pretrained network to in-fer the rotation of various images and then use the followingfiltering strategy depending on RotNets prediction: - 0 (96% of images) For normal street view images thecues that signify an absence of rotation are multiple: the skyis up, and cars and pedestrians are upward. We keep theseimages unchanged.- 180(4%) Over 90% images predicted to be rotatedby 180 are, in fact, actually upside down. We rotate allthese images by a half-turn. For the images in the test, weperform an additional visual inspection to remove the smallproportion of non-localizable images not removed by theprevious filters.- 90 or 270 (0.2%) Images predicted as rotated by aquarter-turn are in the vast majority taken indoors or intunnels. We remove all such images from both the trainand test set.",
  "A.2. Discussion": "Why Not Just Subsample YFCC100M?The wide adop-tion of YFCC100M, with its nearly 50 million geotaggedimages,, might question the need for creating yet another geo-tagged image dataset. However, several compelling reasonsjustify creating OpenStreetView-5M instead of subsamplingYFCC100M:- Data Distribution. The images shared on Flickr do no aimto capture our world in an objective way, but instead focuson aesthetic and cultural value. For example, recognizablelandmarks like the Eiffel Tower or the Louvre, are a culturalsymbol of the city of Paris, yet they lack any information thatis useful in identifying other cities as French or even otherstreets as Parisian. Additionally, many images are renders orinfographics. In contrast, OSV-5M only features dashcampictures, that offer a consistent front-view perspective, that ismore objective as it doesnt focus on something specific, andthus may be more beneficial for learning visual geographicalrepresentations.- Localizability. From a manual inspection of 1000 imageswe find that fewer than 10% (1.3%, 95% confidence) ofYFCC100Ms images are perceptually localizable. In starkcontrast, OSV-5M boosts this perceptual localizability to arate of 96.1% (0.57, 95% confidence), making it a more",
  "Figure A. Images from OSV-5M. The true locations can be found on the next page. The Mapillary users are credited in the subcaptions": "suitable candidate for a standard evaluation benchmark forglobal geolocation.- Geographical Bias. Images in the YFCC100M datasetexhibit a high cultural bias towards the Western world, withover 35% of images from the US and nearly 70% from NorthAmerica and Europe . OSV-5M offers a more equitableglobal representation, as detailed in of the mainpaper.- Selection Challenges. Subsampling YFCC100M based onmetadata alone is ambiguous: 30% of images lack titles, 68%lack descriptions, 30% lack tags, and 50% lack geotagging.The tags travel and nature cover fewer than 2 millionimages. Using instead automated selection methods mayinadvertently propagate existing biases, such as filteringstreet views of non-Western countries.- Persistence.As happens with a lot of large researchdataset, YFCC comes only as a collection of image URLsthat need to be downloaded directly from Flickr.Sucha dataset construction approach, even if the only feasiblechoice for very large datasets, is very volatile and can preventfuture reproducibility. For example, 60% of the 2014 YFCC-split was deleted by 2020 . While YFCC100M used to be hosted on Yahoos Webscope, this option is nolonger available . Instead users need to create an AWSaccount, that requires a credit card to acquire API credentialsfor downloading the data through a designated S3 bucket. Even if no charge is applied, this setting may be pro-hibitive for academics or residents of certain countries. Also,due to the sensitive nature of the Flickr data, users need tomake a formal request to download the dataset, somethingthat isnt needed for our dataset. Instead, OSV-5M ensurespersistence, open and easy access for long-term and broadusage.To summarize, YFCC100M is a vast and unstructuredset of images, a subset of which may be well suited forlocalization and place recognition. However, the ambiguouslocalizability, geographical content, metadata, persistence,and access to its images highlight the need for a dedicateddataset like OSV-5M, specifically designed for the task ofglobal visual geolocation. Visible GeoTags.Due to the diversity in user input data,we found that a small percentage of images (< 5%) havea visible overlayed text on the bottom part that tags theirlocation. This should be taken into consideration when con- Figure B. True Locations. Location of the images of Figure A. With blue we visualize errors of the combined model that are superior to500 km. Most of the images (9 out of 16) are predicted within 500km of where they were taken. We observe that two difficult images (9 and14) are erroneously mapped to the US, which could be explained by the geographical bias of the training set. structing a benchmark for a future dataset. However, dueto the standard ViTs resampling of images to 224 224,these coordinates become indecipherable, as demonstratedin Figure C. We implement for our data loader the option toadd a Gaussian blur with a width of 2 to the bottom 14 rows.When training and/or testing a baseline model with this blur,we observe only small and inconclusive differences in score:training without blurring but testing with it yielded slightlybetter results than both training and testing without the blur,yet training and testing with the blur produced inferior out-comes. This indicates that (i) the network is not able toread the coordinates, and (ii) the bottom rows do not containcritical geographic information. However, we recommendusing the blur for methods that use higher-resolution modelsto obscure any potential location-specific details in the text. Limitations.We list three main limitations of our OSV-5M dataset:(i) Geographical Bias in Training. Due to our reliancecrowd-sourced from Mapillary users, the distribution of loca-tions is biased towards Western countries. We designed ourtest set to explicitly balance this distribution, but the trainingset remains affected by the number of selected images.(ii) User separation. We successfully separated imagesfrom the same sequence between training and test sets. How- ever, we could not separate images uploaded by the sameuser on different days, as the required metadata was notavailable at the time of the dataset construction.(iii) Resolution. The dataset provides images with a verticalresolution of 512 pixels. This restricts the ability to zoom inand read distant texts, for example in street signs, potentiallyobscuring valuable visual cues. However, through our meta-data users can access higher-resolution versions of all ourimages on the mapillary website. Training SOTA methods on OSV-5M.Many state-of-the-art geolocation methods either rely onprivate datasets or lack publicly available code, that preventstheir evaluation. In our main paper we evaluated the perfor-mance of the pretrained StreetCLIP model both for zero-shotretrieval (Tab 6 and Fig 6) and as a pretrained image en-coder (Tab 2), yet the implementation required to fine-tunethe model is not publicly available. Similarly, the completetraining code of Translocator is also not available. Wemanaged to train the publicly available ISN model onOSV-5M, achieving good performance which we attributeto its bespoke geocell module. The aforementioned diffi-culty in training and evaluating SOTA models show the clearneed for open-source datasets and implementations of visualgeolocation approaches, that our paper directly addresses.",
  "(2) Image rescaled in dataloader": "Figure C. Visible Geotagging. A small minority of images (< 5%) have visually overlayed geotags at their bottom left corner (1). Forthose images as resized by our data loader to 224 224 and as optionally blurred, we empirically measure to not provide any importantinformation that the network can use to improve its performance .",
  "Figure D. Geoscore. From a point centered in Paris, red contourshighlight level sets of the score along the earths spherical geometry": "Geoscore.In our paper geoscore is introduced as a betterevaluation method as it strikes a balance between rewardingprecision and not being oversensitive to outlier predictions.Let us consider, for example, a model which produces nineaccurate predictions but fails on the tenth image, choosingNew Zealand instead of Ireland, a 20 000km mistake. Con-trast this with another model which consistently mispredictsby 2 000km. Solely examining the mean error might mis-leadingly favor the latter model, when the first one has ahigher geographic proficiency. In terms of geoscore, themodel with one major error would achieve an average scoreclose to 4500, while the one that is consistently off wouldscore 1300. In that way, geoscore provides a more intuitiveway to compare the performance of models on our dataset.See Figure D for an illustration of Geoscore.",
  "This section presents further results and analysis of our pro-posed framework": "Auxiliary Supervision.We start by evaluating the perfor-mance gained by learning to predict various auxiliary infor-mation. Based on their coordinates, we associate to eachimage of our dataset the following meta-data, according toits latitude and longtitude coordinates:- Land Cover. Relying on the Global Land Cover Share Table A. Auxiliary Variables. We report the impact on geolocationperformance of learning to predict various auxiliary variables. Wealso report the performance on the test set for each variable as theoverall accuracy or the average error.",
  "all--2910198754.019.81.60.8": "Database , we classify each image of our dataset intoone of 11 land cover types, such as artificial, forest, or crops.- Climate. We use recent Koppen-Geiger climate classi-fication maps to associate each image with a climatetype among 31, such as tropical rainforest, arid steppe, ortemperate with dry winter.- Soil Type. Thanks to the Digital World Soil Map , wecharacterize the local soil with a 15 class nomenclature, suchas acrisols, fluvisols, or ferralsols.- Driving Side. We also add a binary indicator for whethera country uses left or right-hand traffic.- Distance to the Sea. For all locations we compute thedistance to their nearest sea.The maps we used to extract land cover, climate, and soiltypes come in a resolution of 1 km (or 30 arc-seconds).We use an MLP f aux to predict the images metadata inaddition to its coordinates. All categorical variables are su-pervised with the unweighted sum of cross-entropy terms,while the distance to the sea is supervised with the L1 loss.Adding auxiliary tasks encourages the model to focus onrelevant geographical cues. As seen in Table A, we onlyobserve a modest impact, indicating that the large train setof OSV-5M allows our model to already learn good latentvariables for geolocation. It should be noted that our model",
  "can perform accurate predictions for complex geographicvariables in the test set, which may have some useful appli-cations in itself": "Spatial Separation.We study the impact of the radiusof spatial separation between the train and the test set. Wedo this by creating test sets along different radii of sep-aration from the training set: 0m (488k images), 500m(294k), 1km (210k), 2km (166k), 3km (136k), 4km (117k)and 5km (107k). As observed in Figure E, all methods, in-cluding retrieval-based approaches, are equally affected bythis phenomenon, indicating that, as expected, the problemof global geolocation becomes harder as the separation ra-dius increases. This allows us to define different versionsof our test set tiered by difficulty. In particular, if we re-move the separation between train and test makes the taskbecomes significantly easier: 3952 geoscore for StreetCLIPin retrieval mode and 3852 for our best model, correspondingto an average distance error of 1191km.",
  "(3) Selected head": "Figure H. Attention Maps. We visualize the self-attention mapsof the [CLS] token of the last layer of the image encoder of thecombined model. We show the mean across all heads in (2), andmanually selected an interesting layer in (3). LoRA.Fig F shows the results with different widths of theLoRA bottleneck, ranging from 2 to 64. We share similarobservations with the LoRA paper [30, 7.2]: higher ranksdo not increase or even slightly decrease performance. Un-freezing the last transformer block remains more efficientin terms of training time, and fine-tuning the entire modelleads to even better performance. Erroneous Predictions.In Fig G we illustrate somesources of geolocation errors not related to the density oftraining images. These include landscapes that are: (i) simi-lar between very distant countries (Fig G (a,b)), or (ii) anykey information is far away from the camera (Fig G (b,c)),or are (iii) monotonous and nearly featureless (Fig G (c)). Humans and Baselines.We compare in Table B our mod-els against two random baselines: selecting randomly a lo-cation on the map or the location of a random image fromthe training set. We also construct an Annotator EnsembleOracle by selecting the most accurate prediction for eachimage from all annotators. Our baseline model, and moresubstantially our combined model, far surpasses the accu-",
  "racy of individual annotators, but is still outmatched by theAnnotator Ensemble Oracle": "Attention Maps.We represent in Figure H the self-attention maps of the [CLS] token of the last layer of thecombined model of images from the teaser. We observethat the network focuses on regions of interest containinguseful geographical cues, such as the double yellow roadlinea specific trait to certain countriesor vegetation andbuildings.",
  "In this section, we detail our architecture, loss, metrics, andthe retrieval algorithm": "Architecture.All considered networks have a base imageencoder I Rd, with a d which depends on each architec-ture (d = 768 for the model ViT-B-32, and d = 1024 forall the other encoders). We then add one or several heads tomap the image representation to geographical information:- Regression f loc. This network directly predicts the lon-gitude and latitude of an image with a MLP of size d d 64 2 with group norms of 4 groups and withoutnormalizing the last layer.- Regression f loc sin/cos. For this variation, we predict thecosine and sine of both coordinates with an MLP: d d 64 4 with a normalization that ensures that the squaredsum between coordinate 0, 1 and 2, 3 is 1. We then use theatan2 function to recover the corresponding coordinates.- Classification f classif. To predict in which of the K ge-ographic divisions an image was taken, we use an MLP:d d 512 K.- Hybrid f relative. In the hybrid model, we predict boththe division and the position of the image within this cell.The relative position is predicted for all cells with an MLPrelative : d d 512 R2K with a specialized normal-ization for the last layer, explained below. During inference,we select the relative prediction of the cell with the highestprediction score for f classif. During training, we only su-pervise the relative prediction that corresponds to the truecell.",
  "w, h": "Figure I. Hybrid Model. The normalization of the hybrid modelrequires special considerations to ensure that the output (x, y) ofaux is such (0, 0) maps to the cells centroid w, h, and that2 maps the entire cell. For this network, a key implementation detail is the nor-malization of the last layer of relative. We require that foreach cell a prediction of (0, 0) should correspond to the cen-troid h, w C2 of the training set images in the cell, andthat a range of prediction of 2 covers the entire bound-ing box of size h, w. As illustrated in Figure I, we denoteby x, y 2 the relative position of the centroid in thecell and by x, y the prediction of the MLP aux. The outputof f relative is defined as follows:",
  "yyif y 0y(1 y)else.(2)": "This normalization allows the network relative to easily pre-dict the centroid of the cell, which facilitates learning thedistribution of images of that cell. This is particularly cru-cial for cells with an off-centered centroid, as it providesincreased precision in high density areas. In practice, re-moving this normalization decreases the performance of thehybrid model by 59 points of geoscore, or 22% from the ben-efit brought by using a hybrid model over pure classification.- Auxiliary f aux. Finally, the auxiliary network is an MLPd d 64 A, where A corresponds to the number ofauxiliary task predictions: 11 for land cover, 31 for climate,15 for soil type, 1 for the driving side, and 1 for the distanceto the nearest sea. For all classification tasks (i.e. everythingexcept the distance to the sea), we softmax the output logits.",
  "with Pi B the set of image positively paired with i andT a temperature parameter set as 0.1. If an image has onlyone positive match, this equation becomes the InfoNCE loss": "Nearest Neighbors RetrievalTo perform nearest neigh-bor retrieval, we create a HNSW32 indexe using the FAISSlibrary through the autofaiss package ( This approachachieves fewer than 200 self-consistency errors per millionwith over 90% compression rate.During retrieval, our training set is divided into five parts,each requiring 15 minutes for index computation and collec-tively consuming 15.6GB of storage for StreetCLIP embed-dings, our most resource-intensive model. This setup enablesus to predict locations for 12,000 to 32,000 test images persecond, depending on the model size.Although retrieval methods demonstrate high perfor-mance and have been made efficient with approximate meth-ods, it is important to note that they are not a learning tech-nique, as they rely on already geographically relevant repre-sentations that are already learned.",
  "Q1 For what purpose was the dataset created? Was therea specific task in mind? Was there a particular gap thatneeded to be filled? Please provide a description": "OpenStreetView-5M (OSV-5M) is the first globalscale, open-access, large dataset of street view im-ages. Its goal is to enable the training and evalu-ation of modern computer vision approaches forglobal visual geolocation, which would dependuntil now on proprietary or expensive APIs suchas Google Street View. More broadly, OSV-5Mcan be used to evaluate and improve representationlearning.",
  "Q9 Is there a label or target associated with each in-stance?": "Yes. Each image is associated with the followingtargets: longitude and latitude, administrative divi-sion (country, region, sub-region, closest city), andlabels corresponding to the local land cover, soil,and climate type at a resolution of 30 arc seconds(1km). We also add the distance to the nearest seaand the driving side of the country.",
  "Q13 Are there any errors, sources of noise, or redundan-cies in the dataset?": "Yes. We have heavily filtered the dataset usingsemi-automatic methods to discard low-quality im-ages and wrong localization, as presented in Sec-tion A. We have estimated through the manualinspection of 4500 images that 96.1% (0.57%with a 95% confidence level) of the images inOpenStreetView-5M are perceptually localizable,i.e. provide a clear enough overview of their sur-roundings.",
  "See": "No. The license plates and faces of pedestrianshave been privacy blurred by Mapillary using anautomatic algorithm with over 99% recall for facesand 99.9% recall for license plates.1 Furthermore,users can signal images that violate privacy.We also manually inspected 4500 images and ob-served no confidentiality leak. With a confidenceof 95% we can assume that fewer than 0.067% ofthe dataset contains leaks. Q20 Does the dataset contain data that might be consid-ered sensitive in any way (e.g., data that reveals racialor ethnic origins, sexual orientations, religious be-liefs, political opinions or union memberships, orlocations; financial or health data; biometric or ge-netic data; forms of government identification, suchas social security numbers; criminal history)?",
  "D.3. Collection Process": "Q22 How was the data associated with each instance ac-quired?The images of Mapillary are taken and uploaded byusers of the Mapillary platform. We downloadedthe images directly from Mapillarys API. Addi-tional metadata was collected from the followingopen-access sources: (i) land cover: Global LandCover Share Database (ii) climate: Koppen-Geiger climate classification maps , (iii) soiltype: Digital World Soil Map (iv) administra-tive division: reverse geocoder .",
  "Q24 If the dataset is a sample from a larger set, what wasthe sampling strategy (e.g., deterministic, probabilis-tic with specific sampling probabilities)?": "We first defined a 100100m grid across the entireworld and sampled one image per cell among the1.8B images of Mapillary. We then sample thetrain and test sets with a weight proportional to thelocal image density raised to the power of 0.75.We then filter the images based on both learnedand handcrafted filters, as described in Section A.",
  "Cultural and Ethical Sensitivity. The datasetspans a wide range of cultures and countries, each": "with its own set of ethical norms and cultural sensi-tivities. We strongly advise against using OSV-5Min a way that might propagate stereotypes, misrep-resent cultures, or otherwise harm the dignity andrepresentation of the featured communities. Manipulation and Misrepresentation.Thedataset should not be used to create misleadingrepresentations of locations or to manipulate im-ages in a way that distorts or misrepresents thereality of the places and the depicted people.",
  "The dataset will be distributed upon the publicationof the preprint on arXiv, which should be in Q2 of2024": "Q47 Will the dataset be distributed under a copyright orother intellectual property (IP) license, and/or underapplicable terms of use (ToU)? If so, please describethis license and/or ToU, and provide a link or otheraccess point to, or otherwise reproduce, any relevantlicensing terms or ToU, as well as any fees associatedwith these restrictions."
}