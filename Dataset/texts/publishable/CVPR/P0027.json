{
  "Abstract": "We address the issue of the exploding computational re-quirements of recent State-of-the-art (SOTA) open setmultimodel 3D mapping (dense 3D mapping) algorithmsand present Voxel-Aggregated Feature Synthesis (VAFS), anovel approach to dense 3D mapping in simulation. Dense3D mapping involves segmenting and embedding sequentialRGBD frames which are then fused into 3D. This leads toredundant computation as the differences between framesare small but all are individually segmented and embedded.This makes dense 3D mapping impractical for research in-volving embodied agents in which the environment, and thusthe mapping, must be modified with regularity. VAFS drasti-cally reduces this computation by using the segmented pointcloud computed by a simulators physics engine and synthe-sizing views of each region. This reduces the number of fea-tures to embed from the number of captured RGBD framesto the number of objects in the scene, effectively allowinga ground truth semantic map to be computed an orderof magnitude faster than traditional methods. We test theresulting representation by assessing the IoU scores of se-mantic queries for different objects in the simulated scene,and find that VAFS exceeds the accuracy and speed of priordense 3D mapping techniques.",
  ". Introduction": "Rapid advancements in transformer based models havebrought us closer to realizing the long standing goal ofbuilding embodied agents capable of perceiving, planning,and acting autonomously . While limited success hasalready been achieved in some domain-specific scenarios, and Large Language Models have demonstrated sur-prising zero-shot reasoning capabilities , open-set 3Dperception remains impractical for most such systems.A truly autonomous system must be able to perceive bothsemantic and spatial environment information in a joint rep- resentation , and must be able to do so efficiently .Recent research building off of 2D vision language models(VLMs) has taken a sizeable step towards that goal with thecreation of open-set 3D mapping methods (dense 3D map-ping) which align semantic features with point clouds. Theresulting clouds can serve as inputs to 3D VLMs or beused as tools by LLM-based agents .Approaches to dense 3D mapping typically involve seg-menting and embedding a sequence of images which arethen fused into a 3D representation.The large numberof similar images which must be processed in this way toachieve successful 3D fusion results in a computational loadwhich makes these approaches unsuitable for most real timesystems, especially those tasked with exploring unseen en-vironments. Even with modern GPUs, these methods ofopen-set multimodal 3D mapping (dense 3D mapping) cantake upwards of 15 seconds per frame .The computational complexity of dense 3D mappingrenders it impractical in many situations it would other-wise prove valuable. In particular, research in agentic co-operation frequently takes place in simulation, where high-fidelity perception and efficient computation are crucial forenabling realistic interaction between agents and their en-vironments.Instead, agents are typically provided withtext-based observations that leave them blind to their envi-ronment, confounding the results of such studies especiallywhen the agents are in close proximity .To address these shortcomings, we propose Voxel-Aggregated Feature Synthesis (VAFS), a novel approach toefficient dense 3D mapping. Instead of embedding eachframe received, we create and embed synthetic views ofthe different point cloud segments in isolation, and then usevoxel aggregation to ensure uniform point distribution. Byreducing the computation required to implement dense 3Dmapping, VAFS makes this technique viable in a broaderset of domains, including research which requires real timeupdates. In particular, we focus on simulator-based applica-tions as such environments readily provide segmented pointclouds, further increasing the efficiency gain over fusion-",
  ". Multi-Modality and Perception": "Multi-modal perception has seen rapid advancement in re-cent years. In 2D, models such as CLIP and ALIGN emerged to encode text and images into a shared latentspace, while segmentation models such as SAM andMASKFORMER emerged to perform class-agnosticimage segmentation. Semantic segmentation models suchas OpenSeg and LSeg merge those tasks, creat-ing embeddings for objects within images, while caption-ing models like Blip go from image to text. Anothertrend has been integrating image understanding into pre-existing models, with nearly all major LLM providers suchas OpenAI and Google offering native image sup-port and open source methods coming along not long after. More complex tasks like motion and object track-ing and 3D scene representations have begun to be considered as well as the frontier expands.However, the considerable cost associated with these tech-niques, attentions N 2 runtime in particular, has motivateda number of papers to explore alternatives to representingthese modalities in full or ways to isolate and ignore regionsof input lacking in information .",
  "Dense 3D mapping algorithms1 generally follow thesesteps:1. Capture multi-view depth images covering the wholescene": "2. Compute pixel-wise features for each depth image3. Fuse the depth images into a 3D environment represen-tation, using some method to combine the features oftwo points of the fusion operation decides to merge theirassociated points.In , 2D semantic relevancy maps are computed andprojected to 3D using the RGB-D depth data, and subse-quently used to compute the volume of the relevant region.Other approaches extract 2D pixel alignedfeatures with off-the-shelf models and perform 3D fusionwith a neural field (NeRF) . NeRF-based methods typ-ically do not create a standalone 3D representation, result-ing in slower semantic queries, though Gaussian splatting",
  "Rigidly defined in 3.1": "has been demonstrated to be a viable means of overcom-ing this limitation . The most common setup, detailedin ConceptFusion , uses simultaneous localization andmapping (SLAM) to fuse the 2D embeddings .OV3D takes this a step further, using text descrip-tions of image segments created by vision-language mod-els instead of image embeddings to incorporate additionalcontext.In all such setups, large overlap is needed be-tween input images to ensure the fusion step (whether thatbe direct reconstruction, SLAM , or a NeRF ) canachieve a well-aligned representation. This overlap makespre-processing the image set prohibitively expensive, andlimits the usefulness of these methods for systems requiringfrequently updated mappings.",
  "(1)": "We take inspiration from ConceptFusions definition ofdense 3D mapping problems, but relax the requirement thatv must be a depth image and that M must include normalsand confidence counts to arrive at the above. These con-straints are unnecessary in this context because the simula-tor is capable of giving us a ground truth point cloud, re-moving the need for us to calculate camera poses or keeptrack of an estimate of position error. Unfortunately, thisdoes mean adding new constraints on the simulation envi-ronment, which are detailed in 3.2.",
  "The information encoded in a pixel is dependent onlyupon the object of which that pixel is a part": "We have a model that can transform a view into its cor-responding 2D feature defined as fe : vm em, m {G, o}.In our specific implementation, we use the Mujoco sim-ulator following after RoCo , and define cameras in thesimulators XML file to follow each object in the environ-ment at a fixed distance and angle.",
  "Pt = {fp(S, t, k)|k Sp}(3)": "View Synthesis: We then group the points by their ob-ject reference, and render a synthetic view vo of the objectcorresponding to those points by aligning a camera with theaverage estimated normal of the points. We repeat the pro-cess for the entire set of points to get the global view vG.This process is detailed in Algorithm 1.",
  "eGt = fe(vG)(4)": "In order to contextualize the object features, we followthe method of ConceptFusion to compute the impor-tance of an object to the scene by assessing its differencefrom the global feature.We then use this to compute aweighted sum aggregating information from that object fea-ture and the global feature and normalize it to get the pointcloud feature cot. Then, we assign these features to theircorresponding points in Pt to get the update concept cloudas follows:",
  "Voxel aggregation: We use voxel pooling to both main-tain a consistent density of points in our concept cloud and": "ensure that relative positioning relationships at the bordersbetween objects are represented explicitly. Using as thevoxel size and as the increment (all outcomes in 4 used0.1 for the starting size and increment), we create a voxelgrid V starting from the origin and for each voxel Vijk addand normalize the features of all the points within to createthe combined feature and take their centroid to be the newposition.",
  ". Experimental Setup and Results": "One of the most common use cases for 3D dense mappingmodels is creating 3D semantic representations of an envi-ronment which can be queried in various modalities. To val-idate the performance of VAFS, We compared it to Concept-Fusion and LeRF on the task of identifying regionsin a simulated scene (we used a scene from the RoCoBenchdataset ) that were most strongly related to the query.We found that VAFS computed the map far faster than eitherbaseline (), while maintaining a higher IoU scoreacross the board even on indirect semantic queries ().We also found that our synthetic view generation improvedthe localization abilities of VAFS compared to the fusion-based baselines. Both baselines gave additional relevance tothe regions around the target region, while VAFS preventedthis blurring of features ().",
  ". Conclusion": "We present VACC as a computationally efficient wayto implement dense 3D mapping algorithms in simula-tion, increasing the accessibility of these algorithms tosimulation-based agentic research. We demonstrate an or-der of magnitude decrease in runtime while achieving bet-ter performance than fusion-based approaches. Future linesof work will include extending VAFS to include pointcloud segmentation and testing the method on real-worldfootage. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-otar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu,Keerthana Gopalakrishnan, Karol Hausman, et al. Do as ican, not as i say: Grounding language in robotic affordances.arXiv preprint arXiv:2204.01691, 2022. 1",
  "C3": ". The high-level workflow of VAFS. At each time step, we associate points P with segments C and render views of the regionsof interest. We then align embeddings of those views with the point cloud and run voxel aggregation to ensure the distribution of pointsremains uniform. Subsequent time steps represent updates to the point cloud, and the process runs again with new views generated forsegments of the point cloud that have changed.",
  "Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-ing open-vocabulary image segmentation with image-levellabels, 2022. 2": "Qiao Gu,Alihusein Kuwajerwala,Sacha Morin,Kr-ishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal,Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa,Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum,Antonio Torralba, Florian Shkurti, and Liam Paull. Concept-graphs: Open-vocabulary 3d scene graphs for perception andplanning, 2023. 2 Qiao Gu,Alihusein Kuwajerwala,Sacha Morin,Kr-ishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal,Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa,Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum,Antonio Torralba, Florian Shkurti, and Liam Paull. Concept-graphs: Open-vocabulary 3d scene graphs for perception andplanning, 2023. 1",
  "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Inject-ing the 3d world into large language models, 2023. 1, 2": "Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, JayPatrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, TianyiZhang, Shibo Zhao, Yu Quan Chong, Chen Wang, KatiaSycara, Matthew Johnson-Roberson, Dhruv Batra, XiaolongWang, Sebastian Scherer, Zsolt Kira, Fei Xia, and YonatanBisk. Toward general-purpose robots via foundation mod-els: A survey and meta-analysis, 2023. 1 Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, WenhaiWang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, YuQiao, and Hongyang Li. Planning-oriented autonomous driv-ing, 2023. 1",
  "Chen Huang, Oier Mees, Andy Zeng, and Wolfram Burgard.Visual language maps for robot navigation. 2023 IEEE In-ternational Conference on Robotics and Automation (ICRA),pages 1060810615, 2022. 2": "Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala,Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, ShuangLi, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, et al.Conceptfusion: Open-set multimodal 3d mapping.arXivpreprint arXiv:2302.07241, 2023. 1, 2, 3 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representationlearning with noisy text supervision, 2021. 2 Xiaosong Jia, Liting Sun, Hang Zhao, Masayoshi Tomizuka,and Wei Zhan. Multi-agent trajectory prediction by combin-ing egocentric and allocentric views. In 5th Annual Confer-ence on Robot Learning, 2021. 2",
  "Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, AnimaAnandkumar, Jose M. Alvarez, Ping Luo, and Tong Lu.Panoptic segformer: Delving deeper into panoptic segmen-tation with transformers, 2022. 2": "Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, andQing Li. Ov-nerf: Open-vocabulary neural radiance fieldswith vision and language foundation models for 3d semanticunderstanding. ArXiv, abs/2402.04648, 2024. 2 Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu,Yingchen Yu, Abdulmotaleb El-Saddik, Christian Theobalt,Eric P. Xing, and Shijian Lu. 3d open-vocabulary segmenta-tion with foundation models. ArXiv, abs/2305.14093, 2023.2",
  "Takafumi Taketomi, Hideaki Uchiyama, and Sei Ikeda. Vi-sual slam algorithms: A survey from 2010 to 2016. IPSJtransactions on computer vision and applications, 9:111,2017. 2": "Gemini Team, Rohan Anil, Sebastian Borgeaud, YonghuiWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: afamily of highly capable multimodal models. arXiv preprintarXiv:2312.11805, 2023. 2 Vadim Tschernezki, Iro Laina, Diane Larlus, and AndreaVedaldi. Neural feature fusion fields: 3d distillation of self-supervised 2d image representations.2022 InternationalConference on 3D Vision (3DV), pages 443453, 2022. 2"
}