{
  "Hao Zhu, Zhen Liu, Qi Zhang, Jingde Fu, Weibing Deng, Zhan Ma, Yanwen Guo, Xun Cao": "AbstractImplicit Neural Representation (INR), which utilizes a neural network to map coordinate inputs to corresponding attributes,is causing a revolution in the field of signal processing. However, current INR techniques suffer from the frequency-specified spectralbias and capacity-convergence gap, resulting in imperfect performance when representing complex signals with multiple frequencies.We have identified that both of these two characteristics could be handled by increasing the utilization of definition domain in currentactivation functions, for which we propose the FINER++ framework by extending existing periodic/non-periodic activation functions tovariable-periodic ones. By initializing the bias of the neural network with different ranges, sub-functions with various frequencies in thevariable-periodic function are selected for activation. Consequently, the supported frequency set can be flexibly tuned, leading toimproved performance in signal representation. We demonstrate the generalization and capabilities of FINER++ with differentactivation function backbones (Sine, Gauss. and Wavelet) and various tasks (2D image fitting, 3D signed distance field representation,5D neural radiance fields optimization and streamable INR transmission), and we show that it improves existing INRs. Project page:",
  "INTRODUCTIONT": "HE way a signal is represented is the foundation of allthe following problems and determines the paradigmfor solving them. Traditional representations, such as theimage matrices, point cloud or volumes , focus on record-ing the elements individually and have offered significantcontributions in history. However, this representation is in-creasingly inadequate for addressing the numerous inverseproblems arising in modern times, such as neural render-ing , inverse imaging and simulations , . On thecontrary, the implicit neural representation (INR) , whichcharacterizes a signal by preserving the mapping func-tion from the coordinates to corresponding attributes usingneural networks, is gaining increasing attentions thanksto the advantages of querying attributes continuously andincorporating differentiable physical process seamlessly. Asa result, INR has found widespread application in solv-ing domain-specific inverse problems , , particularlyin cases where large-scale paired datasets are unavailable,and only measurements and forward physical process areprovided.However, existing INR techniques suffer the well-knownspectral-bias , , i.e., the low-frequency components ofthe signal are more easily to be learned. To address this bias,the positional encoding , which aims at embeddingmultiple orthogonal Fourier or Wavelet bases into the",
  "Manuscript received April 19, 2005; revised August 26, 2015": "subsequent network is proposed. However, a significantchallenge arises from the fact that the frequency distributionof a signal to be inversely solved is often unknown. Thiscan potentially lead to a mismatch between the pre-definedbases frequency set and the characteristics of the signalitself, resulting in an imperfect representation . Apartfrom the pre-defined frequency set, there is a growing focuswithin the research community on automatic frequencytuning, achieved through the use of periodic or non-periodic activation , functions. Nevertheless, thesespecified-designed activation functions merely alleviate thespectral-bias to a frequency-specified one and inappro-priate parameters initialization often results in a capacity-convergence gap.These problems are closely related to a same phe-nomenon, i.e., the under-utilization of definition domainin the used activation functions. While the widespreadused activation functions have an infinite domain, the non-linear components are often concentrated on a small regioncentered around the origin and the input values are mostlydropped into these areas in practical applications. By in-terspersing narrow activation functions with different fre-quencies along the full definition domain and then selectingthe ideal one by controlling the range of input values, thesupported frequency set could be significantly expanded,resulting in improved expressive power of current INRs.Following this idea, we propose the FINER++, which is auniversal framework to extend current activation functionsto their corresponding variable-periodic versions, where thelocal non-linear components are repeated many times alongthe x-axis and the scale parameters of each local componentsare changed.Different from previous explorations , , whichfocus on optimizing the weight matrix for manipulating fre-quency candidates with better matching degree, FINER++",
  "(b) Comparisons of training curves": ". FINER++ framework for INR. We observe that the supported frequency set in classical INRs is limited due to the under-utilization of activationfunctions definition domain, i.e., they mainly employ the central region near the origin point. To overcome this limitation, we propose the FINER++framework by extending the activation functions from periodic/non-periodic functions to variable-periodic ones. This innovation allows for tuning thesupported frequency set by adjusting the initialization range of the bias vector in the neural network. (a) visualizes the selected narrow activationfunctions in classical activation Sine, Gauss. and Wavelet alongside our proposed variable-periodic ones with different bias settings (purple areas).(b) plots the training curves of previous INRs and FINER++, demonstrating the impact of different initializations applied to the bias vector b (seeSec. 5.1.1 for more details). opens a novel way to achieve frequency tuning by modu-lating the bias vector, or in other words, the phase of thevariable-periodic activation functions. We demonstrate thatno matter which backbone activation function is used, boththe shift-invariance and eigenvalues distribution of neuraltangent kernel (NTK) in its FINER++s version can be en-hanced (see Figs 1a, 5) by increasing the initialization rangeof bias vector, thus the spectral bias could be flexibly tunedand the capacity-convergence gap could be significantlyalleviated. To verify the performance, extensive experimentsare conducted on 2D image fitting, 3D signed distance fieldrepresentation, 5D neural radiance field optimization andstreamable implicit neural representation.This work extends our preliminary exploration as pre-sented in CVPR24 . In comparison with the conferenceversion, we first derive and summarize three key char-acteristics of INRs, i.e., under-utilized definition domainin activation function, frequency-specified spectral biasand capacity-convergence gap. Subsequently, we introducea universal framework that extends existing periodic andnon-periodic activation functions to their variable-periodicversions, moving beyond the Sine function. This enhance-ment improves the utilization rate of the definition domainand thus handling the latter two characteristics. To validatethis framework, we re-conducted all experiments from theconference version using a wider range of backbone activa-tion functions, achieving superior performance. Moreover,we demonstrate the enhanced capabilities of FINER++ in anovel task: streamable INR transmission.The main contributions of the work include,",
  "Implicit Neural Representation": "INRs , serve as the foundational building blocksfor neural scene representations. These representations aredesigned to learn continuous functions using a multi-layerperception (MLP) that maps coordinates to visual signals,such as images , , , videos , , and 3Dscenes , . Neural Radiance Fields or NeRF , anotable breakthrough in this domain, learns a 5D INR toreconstruct the radiance fields (density and view-dependentcolor) of a scene. With the widespread application of NeRFand its variants , on realistic view synthesis, INRshave rapidly expanded into various fields of vision andsignal processing, such as cross-model media representa-tion/compression , , neural camera representations, , microscopy imaging and partial differentialequations solver , . Despite the interest and successof these implicit representations, current approaches oftensuffer from the well-known spectral-bias problem. As aresult, the INRs may struggle to capture high-fidelity detailsof complex signals, leading to suboptimal performance infitting functions and ineffectiveness in various applications.",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20153": "for learning low-frequency components of a signal morereadily than high-frequency components. To address thespectral-bias problem, several innovative strategies havebeen proposed for INR-based methods. In particular, spatialencoding is applied to the input data, such as frequencyor polynomial decomposition , , , high-pass fil-tering , , to emphasize high-frequency componentsbefore feeding into the model. uses the Fourier featuresmapped from spatial coordinates as the input of MLPs toimprove the performance of INRs in adequately expressinghigh-frequency information of signals. Additionally, variousarchitectural modifications have been integrated into INRs,including multi-scale or pyramid representations , ,, which can aid in capturing both low-frequency andhigh-frequency components of a scene. implementsa multi-scale network architecture with a band-limitedFourier spectrum to minimize artifacts during the down-sampling or upsampling process. However, the frequencydistribution of a signal requiring inverse solving is oftenunknown, making it difficult to design an appropriate rep-resentation or model for the signal without prior knowledgeof its frequency content.In addition to the positional encoding with pre-definedfrequency bases, there has been a growing interest in the re-search community for automatic frequency tuning throughthe use of nonlinearity activation functions , , ,. Sitzmann et al. propose the Sinusoidal Represen-tation Network (SIREN), a method designed to representcomplex signals and functions using periodic activationfunctions, especially sine functions. SIREN has demon-strated its effectiveness in representing intricate details andhigh-frequency components when compared to traditionalactivation functions like ReLU or sigmoid. Ramasinghe etal. propose the Gaussian-based activation function toimprove SIRENs performance with random initializations.Saragadam et al. draw inspiration from signal pro-cessing literature and propose the Wavelet-based activationfunctions to improve INRs performance for visual signals.However, INRs activated with different functions could beviewed as different function expansion process such as theFourier expansion , Gaussian mixture models or Waveletexpansion, and the first layer of these INRs could be viewedas bases encoding layers , as a result, the problem ofrequiring prior frequency knowledge also exists.Apart from INRs with specified-designed encoding lay-ers or activation functions, the spectral bias could also bealleviated by introducing hash-tables between the inputcoordinates and the neural networks. Muller et al. propose the learnable multi-resolution hash-tables, whereeach input coordinate are used to index hash-keys whichare fed into the subsequent network, then these hash-keysand network parameters are jointly optimized during thetraining process. Kang et al. propose the multi-shiftwindows hash-tables and achieve better performance forsolving partial differential equations. Abou et al. extendthe multi-resolution hash-tables to dynamic scenes wherethe default uniform hash grid is changed according to themotions. Zhu et al. visualize the learned hash keysand highlight the role of the hash table in reorganizingcomplex signals into simpler low-frequency ones, therebysignificantly enhancing the performance of hash-table-based INRs using a compact network. Their subsequent work regularizes the noisy interpolation in hash-table-based INRby introducing a continuous analytical function betweenthe input and the network. Although the hash-based INRsoutperform function-expansion-based ones, the significantlyincreased storage overhead for saving hash-tables limits itsapplications in storage-sensitive tasks such as the compres-sion and transmission.",
  "Pipeline of INRs": "Given a signal sequence {xi, yi}Ni=1, where xi and yi re-spectively represent the coordinate and the correspondingattributes of the i-th element, N is the number of elementsin the signal. INR focuses on pursuing a neural networkf(x; ) to characterize the attributes as accurate as possible.Mostly, the multi-layer perceptron (MLP) network is used tomodel the f(x; ) which could be formulated as follows:",
  "f(x; ) = W Lz L1 +b L(1)": "where z l denotes the output of layer l, = {W l,b l | l =1, 2, ..., L} are the network parameters to be optimized, Lis the number of layers. p(x) is the activation function,such as the periodic function sin(0x) , the non-periodicGaussian function e(s0x)2 and the Wavalet functionej0xe|s0x|2 , p {0, s0, (0, s0), ...} refers to theempirical parameter for controlling the scale of these func-tions1.",
  "Under-utilizedDefinitionDomaininActivationfunctions": "In classical deep learning theory , the input of neuralnetwork is often normalized to or other ranges andthe network parameters are also initialized in a fixed rangefollowing various strategies (e.g., Xavier initialization and Kaiming initialization ). Because of these fixedranges, the value range of each neuron before activationis often distributed in a small region centered around theorigin point. As a result, existing works always focus ondesigning activation functions with non-linearity at a smallregion centered around the origin point and ignore a widerange of areas which are far away from the origin point. Forexample, most of the non-linearity in both Gaussian and Wavelet functions gathers at the [31s0 , 31s0 ] ac-cording to the Statistics theory , where1s0 refers to thestandard deviation of the Gaussian function or the Gaussian",
  "Frequency-specified Spectral Bias": "Although the universal approximation theorem has provedthe capability of MLP with infinite width and depth forrepresenting any functions, only a series of functions couldbe represented especially considering the specifically de-signed activation functions mentioned above. To derive theproperties of this series of functions, or in other words, theexpressive power of INR2, we follow the idea of viewingINRs as a function expansion process , where the firstlayer including the activation is modelled as encodingbases with different parameters. By extending the non-linear activation function (x) as polynomial functions andassuming the signal attribute is a 1-dimensional scalar, thefunctions which could be represented by the INR followsthe form,",
  "pFpcpp(x, p),cp, p R(2)": "where Fp is the supported frequency set defined by theempirical scale parameter p and p represents the phase ofa trigonometric function or the mean of a Gaussian function.Note that the connotation of frequency changes accordingto different activation functions, e.g., frequency refers tothe frequency and variance when the Sine and Gaussianfunctions are used, respectively.For example, when the periodic function sin(0x), Gaus-sian function e(s0x)2 and Wavelet function ej0xe|s0x|2",
  "F0,sFs0c,sej(x+)e(s(xs))2Wavelet": "(3)where c, , cs, s, c,s R (Please refer the supple-mental material for details of derivation). From Eqn. 3, itis observed that MLPs with specifically designed activationfunctions may not be able to represent arbitrary functions.As a result, the second characteristic of INR could be sum-marized as, Characteristic 2. In INRs, there exists a frequency-specifiedspectral bias, where the type of frequency is determined by thechosen activation function, and the supported set of frequency isdetermined by the empirical scale parameter within the activationfunction.",
  "Capacity-Convergence Gap": "According to the Eqn. 3, the expressive power of INRsis determined by empirical scale parameters 0 and s0.Considering the fact that these empirical parameters playthe role of scaling the input values, it seems that they couldbe removed or set as 1 by default since the scaling role canalso be played by changing the initialization ranges of theweight and bias matrixes {W l,bl}l=1,...,L.However, previous studies indicate that specialized em-pirical scale values and specially designed initializationstrategies are crucial. Otherwise, the performance maybe significantly reduced. For example, the weight matrix{W l}l=1,...,L in SIREN are initialized following a uni-form distribution with the range [",
  "/n,": "6/n], where nis the number of inputs for each neuron. Although other pa-pers , do not require specified designed initializationschemes, it is found that this performance reduction alsoexists when setting the scale parameters as 1 and changingthe initialization ranges of network parameters. compares the performance of various INRs withdifferent initialization strategies for fitting a high-resolutionimage. The blue line are the results following the standardform of authors, i.e., default initialization strategies (Gauss.and Wavelet) and W U(",
  "/n, p": "6/n) in Sine). Although they areequivalent in the mathematical form, there is a performancegap in practice (the red dotted boxes in ).As a result, the initialization range of network parame-ters {W l,bl}l=1,...,L could not be scaled unboundedly con-sidering the convergence. In summary, there is a capacity-convergence gap in existing INRs that, Characteristic 3. The function set that INRs could be repre-sented can be enlarged by increasing the initialization range ofnetwork parameters, which violates the rule of guaranteeing theconvergence, resulting a performance gap between theory andpractice.",
  "FINER++:VARIABLE-PERIODICACTIVATIONFUNCTIONS FOR INRS": "To address the characteristics mentioned above, we proposethe FINER++ which utilizes variable-periodic functions foractivating INRs. In this section, we will first introduce aunified framework to extend existing periodic/non-periodicfunctions to their variable-periodic versions, followed by theproposed initialization scheme. Subsequently, we analysethe behaviors of FINER++s supported frequency set andtraining dynamics under different initialization ranges fromboth geometric and neural tangent kernel perspectives.",
  "SineGauss.Wavelet": ". Visualization of capacity-convergence gap in various INRs forfitting a 2K image. The performance of various INRs drops significantlywhen set the empirical scale parameters as 1 and changing the initial-ization range of network parameters. the other two characteristics. We have found that all three ofthese inherent characteristics or limitations can be mitigatedby introducing variable-periodic activation functions. Toachieve this goal, we propose the FINER++ framework,which extends any continuous periodic or non-periodicfunctions to their corresponding variable-periodic forms.",
  "(5)": "a compares activation functions in previous INRsand their corresponding variable-periodic versions. It isobserved that the central non-linear components in originalfunctions are repeated multiple times in FINER++. Becausethe |x|+1 increases with the input variable, the frequencyincreases along the x-axis in the variable-periodic versions.Note that, Eqn. 4 focuses on continuous functions and is notavailable for piecewise functions (e.g., ReLU).",
  "Handling INRs Characteristics": "To fully utilize the definition domain of the activationfunctions, a straight-forward method is changing the ini-tialization ranges of the bias vector {b l}l[1,L] of the MLPnetwork, which is equivalent to shift different areas of theactivation function to the original point. However, it ismeaningless in previous activation functions since the areasfar away from the original point are all linear components(e.g., Gaussian and Wavelet) or non-linear components but",
  ". Visualizations of FINER++ (Wavelet) with different f. Inappro-priate f setting results in the problems of degeneration (left) andoverlap (right)": "sharing same properties with the central component (e.g.,Sine).Different from previous activation functions, because thenon-linear components exist everywhere along the x-axisin the proposed variable-periodic functions (a), shiftdifferent areas to the original point becomes meaningfuland could address two limitations mentioned in the abovesections. For example, since the frequency varies fromsub-function to sub-function in the FINER++, different biasvector b leads to different supported frequency set Fp,resulting in different spectral bias. Furthermore, given fixedscale parameters, the selected non-linear components withlarger frequency means enlarging the scale parameters,thus the capacity-convergence gap could also be mitigated. 4.1.2Selection of fAs shown in Eqn. 4, the key to extending a function to itsvariable-periodic version is by composing it with a variable-periodic function, specifically x = sin(f(|x| + 1)x). How-ever, due to changes in the operational range of the originalfunctions non-linear components with different empiricalscale parameters parameters p, it is crucial to control therate of frequency increase parameter f to avoid issuessuch as degeneration and overlap. visualizes thecurves of FINER++ (Wavelet) with different f values. It isobserved that when f is small (e.g., f = 1 or smaller),FINER++ (Wavelet) gradually degenerates into a standardWavelet function because the distance between neighboringnon-linear components also increases gradually. Conversely,when f is large (e.g., f = 15 or larger), the sidelobesof each non-linear component overlap with neighboringcomponents. We applied Wavelet- and Gaussian-based INRsto represent 2D images and counted the outputs of eachneuron across 16 images. It was found that the valuesbefore activation mostly fell within an area slightly largerthan [31s0 , 31s0 ] (1s0 is the standard derivation of theGaussian function). As a result, f was empirically set to2.5 for all subsequent experiments.",
  "Initialization scheme for bias vector": "As demonstrated in the previous section, the supportedfrequency set of FINER++ can be manipulated by adjust-ing the ranges of the bias vector b. However, due to thenon-convex nature of variable-periodic activation functions,Eqn. 4 exhibits many local minima, and gradient-basedoptimizations (e.g., SGD or Adam) cannot guarantee mov-ing b to the global optimum without proper initialization.",
  ". Comparisons of used activation function sin((|x| + 1)x) underdifferent bias b. More sub-functions with high-frequency are includedwhen b is set with a larger value": "Traditional initialization strategies, which typically involveuniform or Gaussian sampling within a narrow region cen-tered around 0, restrict the supported frequency set ofFINER++ to the frequency of the first non-linear com-ponents in the variable-periodic function (i.e., the orangearea in (a)). This limitation results in underutilizationof other non-linear components ((b)), which possessdifferent frequencies.To get rid of the limited supported frequency set usingtraditional initialization methods, we derive a novel initial-ization scheme for b for tuning the supported frequencyset flexibly, meanwhile the initialization for W follows thedefault mechanisms of different INRs , , . Wepropose to initialize b following a uniform distributionU(k, k) with a larger range k than the default one intraditional methods,",
  "Supposing the supported frequency set of SIREN andFINER++ (Sine) are F0 and F0,k, respectively. To analysetheir relationship, let us start from the simplest case": "k is close to the origin point. Because the initialization forW follows , the term Wx has similar distribution with theone in SIREN, that |Wx| . As a result, the term |(|Wx +b| + 1)(Wx +b)| drops into the area of [2 , 2 + ].As shown in (a), the activation function sin((|Wx +b| + 1)(Wx + b)) mainly spans two narrow sub-functionswith different frequencies. For the points dropped into thefirst sub-function (i.e., |Wx| 4+11",
  "F0 F0,k.(11)": "k is far away from the origin point. For example, b is ini-tialized as 10. Because the frequencies of each sub-functionsare further increased forb = 10 (in (b), the frequency isincreased from the orange box to purple box), the supportedfrequency set F0,k is increased a lot compared with the onein Eqn. 10, thus the Eqn. 11 also holds when k is far awayfrom the origin point.Although the analysis is built upon the Sine function,similar relationship (Eqn. 11) could also be obtained whenother activation functions are used. In summary, Proposition 1. The supported frequency set Fp,k of FINER++increases with the increase of the initialization range of b, andthe supported frequency set Fp in previous INRs is a subset ofF0,k in FINER++.",
  "Neural Tangent Kernel Perspective": "Neural tangent kernel (NTK) theory views the trainingof neural network as kernel regression, where the con-vergence of the network on fitting signals could be de-rived by analysing the diagonal property of the NTK orthe distribution of NTKs eigenvalues. Generally speaking,stronger diagonal property results in better shift-invarianceand better convergence, more larger eigenvalues leads tofaster convergence for high-frequency components , .Without loss of generality, we focus on a simple case,i.e., the signal to be learned has 1D input and 1D output andFINER++ has 1 hidden layer with n neurons, such a networkcould be written as f(x; ) = nk=1 ck(wkx + bk), where(x) = sin((|x| + 1)x) is the activation function. According",
  "FINER++ (Wavelet)": ". Visualizations of NTKs and the corresponding eigenvalues in FINER++. From top to bottom, the NTKs and NTKs eigenvalues of FINER++with sine, Gaussian and Wavelet activation functions are visualized, respectively. From left to right, (a)-(d) visualize the NTKs when b is initializedfollowing U(1, 1), U(5, 5), U(10, 10) and U(20, 20), respectively. (e) plots the corresponding eigenvalues. Because the max eigenvalue ismuch larger than the smallest one, all eigenvalues are processed by a log function for visualization.",
  "(12)": "It is observed that, the scale term is approximately pro-portional to the absolution of bias bk for all nodes of thekernel, however the change rule of sign term for diagonalelements differs significantly from non-diagonal elements.Specifically, the sign term is always a positive value fordiagonal elements while could be either positive or negativefor non-diagonal elements.As a result, the diagonal elements K(xi, xi) are increasedwith the increase of |b|, while the non-diagonal elementsK(xi, xj) can be very small, appearing as a diagonal en-hanced kernel. According to , , NTK with a strongerdiagonal property provides better shift-invariance, i.e., thecoordinates in the training set are little coupled with eachother during the training process, thus the signal could bebetter learned.In Figs. 5, the left 4 sub-figures in the first row visualizethe NTKs of FINER++ with Sine activation for learning a1D signal with 1024 coordinates. It is observed that thediagonal property of NTK is enhanced with the increase ofinitialization range of b, verifying the analysis mentioned",
  "FINER++ (Sine)FINER++ (Gauss.)FINER++ (Wavelet)": ". Comparisons of FINER++s behaviors with different initializations applied to bias vector b. For each image, the right-bottom box visualizesits Fourier spectrum. With an increased initialization range, more high-frequency content can be represented, leading to a significant reduction inerror. of eigenvalues which are larger than 100 is significantlyincreased when larger initialization range is applied to bias.Although the analysis above is based on the Sine acti-vation, similar observations hold true when using Gaussianand Wavelet activation functions. In , the second andthird rows depict the Neural Tangent Kernels (NTKs) andthe distribution of corresponding eigenvalues for FINER++with Gaussian and Wavelet activation functions, respec-tively. It is observed that the diagonal property of theNTKs is strengthened, and the number of large eigenvalues(e.g., greater than 100) increases with the widening of theinitialization range of b. Its worth noting that, due to thenonlinear scaling of parameters in Gaussian and Waveletfunctions with respect to frequency, some off-diagonal el-ements in the NTKs of FINER++ (Gauss.) and FINER++(Wave) are also amplified.",
  "Discussion of parameters selection": "Compared with previous INRs, FINER++ introduces anadditional parameter k, which represents the initializationrange of the bias vector. This added parameter not onlyaddresses the spectral bias and capacity-convergence gapbut also enhances the robustness of parameter selection. compares image representations between FINER++and previous INRs across various parameters. All six INRsshare the same network configuration: 3 layers with 256neurons per layer. The comparison shows that FINER++versions (Sine, Gauss., and Wavelet) achieve higher PSNRvalues compared to their previous counterparts. Moreover,FINER++ consistently outperforms previous INRs over awide range of parameter settings, demonstrating enhancedrobustness in parameter selection.",
  "FINER++s behaviors under different initializations": "To better understand the behavior of FINER++ under dif-ferent initialization ranges, we set the scale parameters inprevious INRs to small values (0 = 1 for Sine, s0 = 2.5for Gauss., and s0, 0 = 2.5, 5 for Wavelet) and vary theinitialization ranges of bias vectors. According to the anal-ysis in Sec. 4, the supported frequency set of FINER++increases with the initialization range of b. Different curvesin b reflect this behavior. The error and the spectrummaps of the learned images using FINER++ in alsodemonstrate this behavior qualitatively. For example, in theresults of FINER++ with Sine activation (first row in ), most of the energies are gathered at the low-frequency areaswhen b U(1, 1), resulting in blurred boundaries inreconstructed leaves. By increasing the initialization range,more high-frequency contents appear. When the Gaussianor Wavelet activation functions are used, the effects of ini-tialization range for tuning FINER++s supported frequencysets still work. Although the changes could not be wellobserved in the spectrum maps since the scale parametershere are not linearly corrected with the frequency, the errormaps help recognizing the efficacy of changing initializationranges of b.According to , the first layer of INR plays the role offrequency encoding. We visualize 8 neurons output fromtotal 256 neurons in first layer from previous INRs and theirFINER++s versions, where 4 neurons in the 1st row havethe smallest frequencies and the last 4 neurons have thelargest frequencies (see ). It is observed that differentneurons in previous INRs encode similar frequencies, result-",
  "Comparisons with the State-of-the-arts": "We compare FINER++ with four classical INRs, i.e., theFourier feature embedding (PEMLP) , INRs with peri-odic activation functions (SIREN) , Gaussian activationfunctions (Gauss.) and wavelet activation functions(WIRE) . Note that the latter three methods are allcombined with the FINER++ framework to better evaluatethe performance. For a fair comparison, all INRs are set witha same network configuration (3 hidden layers with 256neurons per layer, which is also a common configurationin literature ) and are trained with the same Adam opti-mizer and L2 loss function between the network outputand the ground truth, other parameters are set accordingto the open-source codes released by authors. In FINER++,k is set as1 2, 1 and 1 for Sine, Gauss. and Waveletbackbones, respectively. Tab. 1 compares FINER++ withothers quantitatively. FINER++ outperforms other methodsin all three metrics and there are significantly improvementscompared with backbone INRs. demonstrates thedetails of different methods. By extending previous INRs totheir FINER++s versions, over-smoothed text boundariesbecomes clear (Sine backbone, e.g., the texts 67 and SBS inthe cyan and green boxes, respectively) and unwelcome seri-ous artefacts in the smooth background are removed (Gauss.and Wavelet backbones, e.g., white and red billboards in thecyan and green boxes, respectively, as well as the while walland the blue glass in the purple and red boxes, respectively.)",
  "D Shape Representation": "Signed distance field (SDF) is one of the most commonlyused implicit surface representations in the computer graph-ics . As the name implies, SDF characterizes the distancefrom the given 3D point to the closest surface using acontinuous function, and the sign of the distance is usedto denote whether the point is inside (negative) or outside(positive) the surface. Recently, representing the SDF usingINR is drawing more and more attentions , . Given a3D point x, INR learns a 3D mapping function f : R3 R1",
  "to output the signed distance field values s. We applythe FINER++ to this task and compare to four classicalINRs mentioned above. In this task, k is set as 1, 1 and": "2 for Sine, Gauss. and Wavelet backbones, respectively.In the experiment, 4 shapes from public dataset areused for evaluation. For a fair comparison, all methodsuse a same network configuration, i.e., 3 layers with 256neurons per layer and the same coarse-to-fine loss functionis used according to . In the training stage, 10k points arerandomly sampled in each iteration and is repeated 200kiterations. In the testing stage, a 5123 grid is extracted forevaluation and visualization.Tab. 2 provides quantitative comparisons between theproposed FINER++ and four baselines. Because FINER++provides more freedom for tuning the supported frequencyset, the performance of all three baseline methods (SIREN,Gaussian and Wavelet) are improved when the FINER++framework is applied, and the best results are achievedby the FINER++ with the Wavelet backbone. com-pares the reconstructed details visually on the Armadilloand Lucy rendered using Marching Cubes . In eachscene, two representative regions are zoomed-in for compar-isons, i.e., the low-frequency smooth pectoral and the high-frequency rough shank in Armadillo, as well as the mid-frequency wrinkles on clothes in underarm and hemlinein Lucy. For PEMLP, because the pre-defined frequencymay not match the frequency distribution in the SDF ofArmadillo and Lucy, all of the pectoral, shank and wrinkleson clothes are not well represented. SIREN represents thesmooth pectoral well but loses the details of the shank andover-smooths the wrinkles on clothes. Gaussian could notprovide stable representation for all shapes (Tab. 2) andthere are obvious artefacts in the reconstructed SDF such asthe noise outside the shape in . WIRE overcomes thelimitation of SIREN for representing high-frequency shankat the cost of rough pectoral and wrinkles on clothes. Byapplying the FINER++ framework to three baselines, allof the problems of high-frequency noise and over-smoothtextures could be well addressed and consistent perfor-mance are provided in all the low-, mid- and high-frequencycomponents.",
  ". Qualitative comparisons on representing the signed distance fields of Armadillo and Lucy": "realism and scalability for embedding different renderingprocesses . Given a 3D point x and a 2D observed di-rection d, NeRF focuses on learning a 5D mapping functionf : R5 R4 with 5D coordinate (x, d) to its 3D color cand 1D opacity . For any pixel p in novel view images,its ray function in 3D space is firstly calculated using thein/extrinsic matrices of camera , then several points aresampled along the ray within a predefined depth range,furthermore the direction and position coordinates of thesepoints are fed into the INR for querying the radiance value(c, ), finally the color C(p) of the pixel is calculated usingthe differentiable volume rendering technique , ,",
  ",1": "3 and 1 for Sine, Gauss. and Wavelet backbones,respectively). To better verify the advantages of FINER++for representing high-frequency components, we followthe experimental setting of WIRE that only 25 images areused for training instead of commonly used 100 images.Tab. 3 and provide quantitative and qualitativecomparisons of FINER++ against different methods on theBlender dataset . FINER++ (Wavelet) achieves the bestperformance in almost all 8 scenes and all results of back-bones are improved when the FINER++ is applied. demonstrates the advantage of FINER++ for representinghigh-frequency components visually. For example, the holes(red boxes) and the highlights (green boxes) in the frameof the truck are over-smoothed in the reconstructions of all",
  "( 3 , 2 )(2 ,3 )bUkkUkk": ". Pipeline of applying FINER++ to streamable INR transmission.From left to right, the initialization range of bias vector in widenednetwork are increased to represent high-frequency components better. baselines in Lego, however, these areas are all well recon-structed in the corresponding FINER++s versions. Thesephenomenon also appears in the reconstructed results ofFicus, such as leaves (red boxes) and branches (green boxes).",
  "Streamable INR Transmission": "With the increasing use of INR in inverse optimization andsignal representation, there is a growing demand to transferreconstructed or represented INRs between different users.However, due to the strong coupling of INR parameters,challenges arise when distributing INRs to devices withvarying resolutions or providing streaming services thatrequire progressively decoding different signal componentswhile ensuring consistency with transferred network pa-rameters. To address these challenges, Cho et al. pro-pose a progressive training strategy. As depicted in ,this strategy involves initially training a small, narrow net-work to model low-frequency components. Subsequently,the network width is increased, and the additional networkparameters (represented by the dotted lines in ) aretrained to capture mid-frequency components, while theparameters from the initial step remain fixed (solid lines in). This process is repeated to model higher-frequencycomponents. In their implementation, Cho et al. use a SIRENnetwork with a default scale parameter (0 = 30) for boththe initial narrow network and the subsequently addednetworks. However, as analysed in Sec. 3, SIREN exhibitsa frequency-specified spectral bias, which limits its expres- sive power for modeling high-resolution signals under thistraining configuration.FINER++ is inherently suitable for building streamableINR, especially in cases where the signal has high resolu-tion. As analysed in Sec. 4, the supported frequency setcould be significantly manipulated by scaling the initial-ization range [k, k] of the bias vector. Consequently, theperformance of FINER++-based streamable INR could besignificantly enhanced by changing the initialization rangeof the bias vector in the widened network parameters. visualizes the pipeline of applying FINER++ instreamable INR transmission. Note that, the rule of settinginitialization range (e.g.,b U(3k, 2k)U(2k, 3k)) differsfrom the analysis in Sec. 4 to reduce the frequency overlapbetween neurons in different widths. compares thestreamable INR between the SIREN and FINER++ (Sine) indifferent stages qualitatively. It is observed that FINER++(Sine) provides more clear details than SIREN not onlyin areas with different frequencies but also in all stages,demonstrating the advantages and potential of applyingFINER++ in streamable INR.",
  "CONCLUSION": "We have proposed and verified the FINER++ which ex-tends existing periodic/non-periodic functions to variable-periodic ones for activating INR. We have pointed out 3characteristics of existing INRs, i.e., under-utilized defini-tion domain in activation functions, frequency-specifiedspectral bias and capacity-convergence gap. The proposedFINER++ addresses these characteristics by building a fam-ily of variable-periodic activation functions and initializingthe bias vector to different ranges, where different sub-functions with different frequencies along the definitiondomain will be selected for activation. As a result, the lattertwo characteristics of existing INRs could be well handled.We have demonstrated the advantages of FINER++ overother INRs in image fitting, 3D shape representation, neuralrendering and streamable INR transmission. In the future,we will focus on designing INRs without any frequency-specified spectral bias.",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 201514": "A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, W. Yi-fan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi et al.,Advances in neural rendering, in Computer Graphics Forum,vol. 41, no. 2.Wiley Online Library, 2022, pp. 703735. Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari,J. Tompkin, V. sitzmann, and S. Sridhar, Neural fields in visualcomputing and beyond, Computer Graphics Forum, vol. 41,no. 2, pp. 641676, 2022.",
  "Z. Hao, S. Liu, Y. Zhang, C. Ying, Y. Feng, H. Su, and J. Zhu,Physics-informed machine learning: A survey on problems,methods and applications, arXiv preprint arXiv:2211.08064, 2022": "N. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. Hamprecht,Y. Bengio, and A. Courville, On the spectral bias of neuralnetworks, in International Conference on Machine Learning.PMLR, 2019, pp. 53015310. G. Yuce, G. Ortiz-Jimenez, B. Besbinar, and P. Frossard, A struc-tured dictionary perspective on implicit neural representations,in Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2022, pp. 19 22819 238. B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra-mamoorthi, and R. Ng, Nerf: Representing scenes as neuralradiance fields for view synthesis, in European conference oncomputer vision.Springer, 2020, pp. 405421. M.Tancik,P.Srinivasan,B.Mildenhall,S.Fridovich-Keil,N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng,Fourier features let networks learn high frequency functionsin low dimensional domains, Advances in Neural InformationProcessing Systems, vol. 33, pp. 75377547, 2020.",
  "S. Ramasinghe and S. Lucey, Beyond periodicity: Towards a uni-fying framework for activations in coordinate-mlps, in EuropeanConference on Computer Vision.Springer, 2022, pp. 142158": "V. Saragadam, D. LeJeune, J. Tan, G. Balakrishnan, A. Veer-araghavan, and R. G. Baraniuk, Wire: Wavelet implicit neuralrepresentations, in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2023, pp. 18 50718 516. Z. Liu, H. Zhu, Q. Zhang, J. Fu, W. Deng, Z. Ma, Y. Guo, andX. Cao, FINER: Flexible spectral-bias tuning in implicit neu-ral representation by variable-periodic activation functions, inProceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2024, pp. 110.",
  "T. Muller, A. Evans, C. Schied, and A. Keller, Instant neuralgraphics primitives with a multiresolution hash encoding, ACMTransactions on Graphics (ToG), vol. 41, no. 4, pp. 115, 2022": "J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan, Mip-nerf: A multiscale representa-tion for anti-aliasing neural radiance fields, in Proceedings of theIEEE/CVF International Conference on Computer Vision, 2021,pp. 58555864. R. Gao, Z. Si, Y.-Y. Chang, S. Clarke, J. Bohg, L. Fei-Fei, W. Yuan,and J. Wu, Objectfolder 2.0: A multisensory object dataset forsim2real transfer, in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2022, pp. 10 59810 608.",
  "Y. Strumpler, J. Postels, R. Yang, L. V. Gool, and F. Tombari, Im-plicit neural representations for image compression, in EuropeanConference on Computer Vision.Springer, 2022, pp. 7491": "X. Huang, Q. Zhang, Y. Feng, H. Li, X. Wang, and Q. Wang, Hdr-nerf: High dynamic range neural radiance fields, in Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2022, pp. 18 39818 408. X. Huang, Q. Zhang, Y. Feng, H. Li, and Q. Wang, Invertingthe imaging process by learning an implicit camera model, inProceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2023, pp. 21 45621 465.",
  "H. Zhu, Z. Liu, Y. Zhou, Z. Ma, and X. Cao, DNF: Diffractiveneural field for lensless microscopic imaging, Optics Express,vol. 30, no. 11, pp. 18 16818 178, 2022": "M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informedneural networks: A deep learning framework for solving for-ward and inverse problems involving nonlinear partial differentialequations, Journal of Computational physics, vol. 378, pp. 686707, 2019. R. Singh, A. Shukla, and P. Turaga, Polynomial implicit neu-ral representations for large diverse datasets, in Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023, pp. 20412051. N. Raghavan, Y. Xiao, K.-E. Lin, T. Sun, S. Bi, Z. Xu, T.-M. Li,and R. Ramamoorthi, Neural free-viewpoint relighting for glossyindirect illumination, in Computer Graphics Forum, vol. 42, no. 4.Wiley Online Library, 2023, p. e14885.",
  "N. Benbarka, T. Hofer, A. Zell et al., Seeing implicit neuralrepresentations as fourier series, in Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vision, 2022, pp.20412050": "N. Kang, B. Lee, Y. Hong, S.-B. Yun, and E. Park, Pixel: Physics-informed cell representations for fast and accurate pde solvers,in Proceedings of the AAAI Conference on Artificial Intelligence,vol. 37, no. 7, 2023, pp. 81868194. J. Abou-Chakra, F. Dayoub, and N. Sunderhauf, Particlenerf:A particle-based encoding for online neural radiance fields, inProceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision, 2024, pp. 59755984.",
  "J. Cho, S. Nam, D. Rho, J. H. Ko, and E. Park, Streamable neuralfields, in European Conference on Computer Vision.Springer,2022, pp. 595612": "Hao Zhu is an Associate Researcher in theSchool of Electronic Science and Engineering,Nanjing University. He received the B.S. andPh.D. degrees from Northwestern PolytechnicalUniversity in 2014 and 2020, respectively. Hewas a visiting scholar at the Australian NationalUniversity. His research interests include compu-tational photography and optimization for inverseproblems. Zhen Liu is a graduate student in the De-partment of Computer Science and Technology,Nanjing University. He is co-supervised by Prof.Xun Cao and Prof. Yang Yu. He received hisB.S. degree from the Beijing Institute of Tech-nology in 2021. His research interests includecomputational photography and implicit neuralrepresentation. Qi Zhang is currently a lead researcher withVivo. Before that, he was a researcher with Ten-cent AI Lab. He received his Ph.D. from theSchool of Computer Science at NorthwesternPolytechnical University in 2021. He receivedCCF Doctorial Dissertation Award Nominee in2021. His research interests include 3D vision,neural rendering, Gaussian Splatting, and AIGC. Jingde Fu received the B.S. degree in 2024 fromthe Department of Mathematics, Nanjing Uni-versity, Nanjing, China. He is currently a grad-uate student for Ph.D degree in the Depart-ment of Mathematics, Nanjing University, Nan-jing, China. His current research interests in-clude numerical methods for partial differentialequations and modeling methods driven by dataand mechanism. Weibing Deng received the B.S. degree in 1992,and the Ph.D. degree in 2002 from the Depart-ment of Mathematics, Nanjing University, Nan-jing, China. He worked as a postdoctoral fellowin the Institute of Computational Mathematics,Chinese Academy of Sciences, form 2003 to2004. He was a Visiting Scholar with the Cali-fornia Institute of Technology, USA, from 2007 to2008. He is a Professor at the school of Mathe-matics, Nanjing University. His current researchinterests include analysis and computation ofmulti-scale problems, numerical homogenization methods, and model-ing methods driven by data and mechanism. Zhan Ma (SM19) is a Professor in ElectronicScience and Engineering School, Nanjing Uni-versity, Nanjing, Jiangsu, 210093, China. Hereceived the B.S. and M.S. degrees from theHuazhong University of Science and Technol-ogy, Wuhan, China, in 2004 and 2006, respec-tively, and the Ph.D. degree from the New YorkUniversity, New York, in 2011. From 2011 to2014, he has been with Samsung ResearchAmerica, Dallas, TX, and Futurewei Technolo-gies, Inc., Santa Clara, CA, respectively. Hisresearch focuses on learning-based video communication and com-putational imaging. He is a co-recipient of the 2019 IEEE BroadcastTechnology Society Best Paper Award, the 2020 IEEE MMSP ImageCompression Grand Challenge Best Performing Solution, the 2023 IEEEWACV Best Algorithm Paper Award, and the 2023 IEEE Circuits andSystems Society Outstanding Young Author Award. Yanwen Guo received the PhD degree in ap-plied mathematics from the State Key Lab ofCAD&CG, Zhejiang University in 2006. He is aprofessor with the National Key Lab for NovelSoftware Technology, Nanjing University. He wasa visiting scholar with the Department of Electri-cal and Computer Engineering, University of Illi-nois at Urbana-Champaign, from 2013 to 2015.His research interests include image and videoprocessing, vision, and computer graphics.",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 201516": "Xun Cao received the B.S. degree from Nan-jing University, Nanjing, China, in 2006, andthe Ph.D. degree from the Department of Au-tomation, Tsinghua University, Beijing, China,in 2012. He held visiting positions with PhilipsResearch, Aachen, Germany, in 2008 and Mi-crosoft Research Asia, Beijing, from 2009 to2010. He was a Visiting Scholar with the Uni-versity of Texas at Austin, Austin, TX, USA, from2010 to 2011. He is a Professor at the Schoolof Electronic Science and Engineering, NanjingUniversity. His current research interests include computational photog-raphy and image-based modeling and rendering."
}