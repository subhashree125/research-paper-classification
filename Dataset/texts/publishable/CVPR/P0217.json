{
  "Abstract": "Advancements in egocentric video datasets like Ego4D,EPIC-Kitchens, and Ego-Exo4D have enriched the study offirst-person human interactions, which is crucial for appli-cations in augmented reality and assisted living. Despitethese advancements, current Online Action Detection meth-ods, which efficiently detect actions in streaming videos, arepredominantly designed for exocentric views and thus fail tocapitalize on the unique perspectives inherent to egocentricvideos. To address this gap, we introduce an Object-AwareModule that integrates egocentric-specific priors into exist-ing OAD frameworks, enhancing first-person footage inter-pretation. Utilizing object-specific details and temporal dy-namics, our module improves scene understanding in de-tecting actions. Validated extensively on the Epic-Kitchens100 dataset, our work can be seamlessly integrated intoexisting models with minimal overhead and bring consis-tent performance enhancements, marking an important stepforward in adapting action detection systems to egocentricvideo analysis.",
  ". Introduction": "Recent advancements in large-scale egocentric videodatasets, such as EPIC-Kitchens , Ego4D , andEgo-Exo4D , have revolutionized the study of humaninteractions and behaviors from a first-person perspective.These datasets offer a wealth of varied examples of daily ac-tivities, fostering developments in fields like activity recog-nition, social interaction analysis, and personal assistanttechnologies. Such progress promises to enhance user ex-periences across diverse applications.A primary challenge in this domain is the understandingof continuous, long, fluid video streams, central for applica-tions such as augmented reality and robotics. These appli-cations require technologies that can efficiently parse andinterpret continuous video feeds, ensuring context-awareand timely interactions. Consequently, there is an urge foradvanced methodologies to meet the real-time demands ofdynamic environments in understanding first-person view",
  ". Overview of our methodology": "streaming videos.There has been ongoing work in Online Action Detection(OAD) , which involves identifying actions in a stream-ing video without access to future frames. However, ex-isting works mainly focus on third-personview exo-centric videos, as its development has progressedusing third-person view datasets such as THUMOS ,TVSeries , and FineAction . With the recent devel-opments in egocentric datasets, recent works havereleased benchmarks for understanding key steps in stream-ing egocentric videos by employing existing OAD models.While these benchmarks have provided valuable insights,employing existing models specialized in exocentric viewfails to fully exploit the unique attributes of egocentric videothat set it apart from traditional third-person footage.The main limitation stems from the inherent differencesin perspective and context between exocentric and egocen-tric videos. Egocentric videos, characterized by their first-person viewpoint, necessitate a nuanced, object-centeredunderstanding of the scene. This is due to the lack of poseinformation, a consequence of the cameras positioning inegocentric setups. For example, in a cooking scenario, anegocentric video might capture close-up actions such as theusers hands slicing vegetables or stirring a pot. In contrast,an exocentric video could provide a broader view of theusers full body and the kitchen layout but fail to detail thefine-grained movements and object interactions typical ofthe egocentric perspective. Therefore, models designed foregocentric videos must prioritize hand-object interactionsto more accurately identify specific actions like chopping,stirring, or pouring.",
  "arXiv:2406.01079v1 [cs.CV] 3 Jun 2024": "Recognizing this need, we instantiate an effort to inte-grate egocentric-specific priors into existing methodologies,thereby boosting its effectiveness in interpreting the first-person footage. We specifically focus on the fact that theperceived objects present in the scene offer a rich contextualbackground for understanding fine-grained activity. For in-stance, an apple placed on top of the cutting board with aknife immediately signals the impending action of CuttingApple.Similarly, the interaction patterns observed withobjects commonly found in certain environmentssuch asutensils, appliances, and ingredientscan provide impor-tant cues for recognizing specific activities.As such, we aim to enhance scene object awareness inprior models by introducing an Object-Aware Module. Thismodule begins by integrating an off-the-shelf detector with the existing OAD framework. It utilizes learnable vec-tors to attend to the detected objects outputs, extracting ob-ject scene information. These vectors are further refined byincorporating temporal cues from the OAD model, creat-ing enriched, object and temporally aware representations.These can then be processed to determine the impendingaction. The overview of our methodology is shown in .We validate our methodology through extensive experi-ments on the Epic-Kitchens-100 dataset using the most re-cent state-of-the-art OAD models. Due to its lightweightand versatile design, our Object-Aware Module can beseamlessly integrated into any existing OAD works withminimal overhead, notably enhancing the interpretation ofegocentric data. Our results demonstrate consistent perfor-mance improvements across various models, confirming themodules broad applicability and effectiveness.",
  ". Related Works": "Online Action Detection has seen considerable advance-ments since its introduction . Recent developments pri-marily fall into two categories: traditional RNN variants and transformer-based models . No-tably, RNN-based approaches have recently been revis-ited , demonstrating state-of-the-art performance withhigh efficiency. This resurgence is attributed to innovationsin minimal RNN architectures and the adoption of special-ized loss functions tailored for streaming video processing.However, the majority of advancements have predom-inantly focused on traditional third-person view datasets.Our work diverges by extending OAD systems beyond theirstandard third-person constraints. We introduce a plug-and-play module that enhances object awareness, a crucial ele-ment in interpreting first-person footage. This initiative notonly advances the application of OAD to egocentric videosbut also lays the groundwork for future research in egocen-tric video analysis.",
  ". Methodology": "We present a method to augment existing OAD models ininterpreting the first-person footage by making them awareof the objects in the scene. The key to this lies in extractingobject information in the scene (Sec. 3.1), adequately in-tegrating object information and temporal cues (Sec. 3.2),and refining the updated pieces of information to detect ac-tions more accurately (Sec. 3.3).",
  ". Extracting Object Information": "Recognizing that scene awareness of objects provides in-formative contextual information for understanding actions,we initiate our process by extracting object data from thescene.Drawing inspiration from notable works in thefield , we employ Faster-RCNN . We apply thisdetector to the last frame of every video snippet inputtedinto our model. From the output of K detected boundingboxes, we aggregate their confidence scores correspondingto their category. This results in a vector f R1C whereC is the number of classes. Each value in C will indicate thelikeliness at which the object is present in the scene. Thisobject representation effectively encodes the objects presentin the scene and as we validate in the experimental section,enriches scene comprehension.",
  ". Object-Aware Module": "We introduce an Object-Aware Module designed to effec-tively integrate object information with temporal cues. Asillustrated in , our module is composed of two trans-former layers. In the first layer, learnable query vectors andobject informationextracted as detailed in Sec. 3.1in-teract to update the query vectors. These vectors are thenprocessed through the second transformer layer, where theyundergo cross-attention with temporal cues captured bythe OAD model. Subsequently, the updated queries passthrough a feed-forward layer, preparing them for actionclassification. The first layer enhances the query vectorssensitivity to scene-specific object information, fostering object awareness. The second layer employs cross-attentionto merge this object information with temporal cues, crucialfor accurately identifying ongoing actions.Despite its straightforward design and implementation,the module is effective in enhancing action identificationfrom the first-person footage as validated in .3.This efficacy is attributable to the query-propagating de-sign, which has demonstrated considerable success acrossvarious domains, including object detection , instancesegmentation , and action localization .We haveadapted and refined this design for our module, enablingit to serve as an efficient information bottleneck that effec-tively encapsulates object information.",
  ". Classifying Actions": "Finally, the updated information must be aggregated effec-tively to enhance action detection accuracy. The Object-Aware Module generates N learnable query vectors, cap-turing both object and temporal data essential for actionclassification. A global max pooling operation then aggre-gates these vectors to consolidate the N vectors into a sin-gle vector. This consolidated vector is subsequently classi-fied using standard classifiers. Given that the Epic-Kitchensdataset requires the classification of Verbs, Nouns, and Ac-tions separately, we utilize three distinct classifiers tailoredto each category.",
  ". Experimental Setup": "OAD Models. We employ the open-source implementa-tions of TeSTra , MAT , and MiniROAD in theirdefault configurations. Notably, since TeSTra and MAT im-plement the MixCLIP data augmentation technique, we alsoapply MixCLIP in our use of MiniROAD.Setting. We follow the standard OAD protocol: videos areprocessed at 24 fps and frames are fed into a TSN model pretrained on the Kinetics-400 dataset , using a win-dow size and stride of 6. Regarding the module configura-tion, we utilize a single block of the Object-Aware Modulewith 16 learnable queries and an embedding dimension of1024 within the module. We project the object informationwith a single feed-forward layer to match the dimension ofquery vectors.",
  ". Results": "Main ResultsThe results of applying our methodology tothree recent OAD models are presented in . We ob-serve consistent improvements in Verb, Noun, and Actionaccuracy across all models tested. These findings confirmthe versatility of our Object-Aware Module, demonstratingits capability to be seamlessly integrated into any existingOAD framework.Importantly, as validated by improve-ments in all Verb, Noun, and Action accuracies, it enhancesobject awareness, which is crucial for accurately interpret-ing egocentric scenes. We observe the most significant im-provements in Verb accuracy, as identifying the object ofinteraction provides important clues about the impendingaction, such as opening, squeezing, cutting, etc. However,gains in Action accuracy are less pronounced, which we at-tribute to the complex nature of the datasets action labels,encompassing over 3,800 distinct classes. How to utilize object information shows the out-comes from various methods of integrating object informa-tion using the state-of-the-art OAD model . The base-line, presented in the first row, does not include object in-formation. The second row illustrates results when objectinformation is concatenated with RGB at the input level. Incontrast, the last row highlights our approach using the OA-Module. These results indicate that navely adding objectinformation does not yield optimal outcomes, emphasizingthe critical role of the OA-Module in effectively leveragingthis data.",
  ". Limitations": "As this represents the preliminary stage of an ongoingproject, our experimental results are currently somewhatlimited. We acknowledge that our incorporation of objectscores for providing object information, as described in Sec-tion 3, could be further improved. We are currently work-ing on using only the active objects that the user is actu-ally interacting with, instead of all the objects present in thescene. Also, instead of a nave objectness score, we are ex-perimenting to incorporate the active objects spatial infor-mation. We plan to extend this framework to more diversedatasets, including Ego4D and Ego-Exo4D.",
  ". Conclusion": "This work aims to enhance existing Online Action Detec-tion frameworks for more effective application in egocen-tric videos by leveraging egocentric priors. We introducethe Object-Aware Module, a flexible module that can beseamlessly integrated into any current OAD method. Thismodule utilizes object information as important contextualcues to predict impending actions better. Through this, weimprove the adaptability and accuracy of OAD systems ininterpreting first-person video data, paving the way for ad-vancements in applications that require precise real-timeanalysis.",
  "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-der Kirillov, and Rohit Girdhar.Masked-attention masktransformer for universal image segmentation.In CVPR,pages 12901299, 2022. 3": "Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Sanja Fidler, Antonino Furnari, Evangelos Kazakos, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.Scaling egocentric vision: The epic-kitchens dataset.InECCV, pages 720736, 2018. 1 Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Antonino Furnari, Evangelos Kazakos, Jian Ma, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.Rescaling egocentric vision: Collection, pipeline and chal-lenges for epic-kitchens-100. IJCV, pages 123, 2022. 1,3",
  "Ross Girshick. Fast r-cnn. In ICCV, pages 14401448, 2015": "KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video.InCVPR, pages 1899519012, 2022. 1 Kristen Grauman, Andrew Westbury, Lorenzo Torresani,Kris Kitani, Jitendra Malik, Triantafyllos Afouras, KumarAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote,et al.Ego-exo4d:Understanding skilled human activ-ity from first-and third-person perspectives. arXiv preprintarXiv:2311.18259, 2023. 1"
}