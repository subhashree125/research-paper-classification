{
  "Abstract": "Instance segmentation is data-hungry, and as model ca-pacity increases, data scale becomes crucial for improvingthe accuracy. Most instance segmentation datasets todayrequire costly manual annotation, limiting their data scale.Models trained on such data are prone to overfitting on thetraining set, especially for those rare categories. While re-cent works have delved into exploiting generative modelsto create synthetic datasets for data augmentation, theseapproaches do not efficiently harness the full potential ofgenerative models.To address these issues, we introduce a more efficientstrategy to construct generative datasets for data augmen-tation, termed DiverGen. Firstly, we provide an explana-tion of the role of generative data from the perspective ofdistribution discrepancy. We investigate the impact of dif-ferent data on the distribution learned by the model. Weargue that generative data can expand the data distribu-tion that the model can learn, thus mitigating overfitting.Additionally, we find that the diversity of generative datais crucial for improving model performance and enhanceit through various strategies, including category diversity,prompt diversity, and generative model diversity. With thesestrategies, we can scale the data to millions while main-taining the trend of model performance improvement. Onthe LVIS dataset, DiverGen significantly outperforms thestrong model X-Paste, achieving +1.1 box AP and +1.1mask AP across all categories, and +1.9 box AP and +2.5mask AP for rare categories. Our codes are available at",
  "*Equal contribution.Correspondence should be addressed to HC and CS": "els learning capabilities improve, the demand for trainingdata increases. However, current datasets for instance seg-mentation heavily rely on manual annotation, which is time-consuming and costly, and the dataset scale cannot meetthe training needs of models. Despite the recent emergenceof the automatically annotated dataset SA-1B , it lackscategory annotations, failing to meet the requirements of in-stance segmentation. Meanwhile, the ongoing developmentof the generative model has largely improved the controlla-bility and realism of generated samples. For example, therecent text2image diffusion model can generatehigh-quality images corresponding to input prompts. There-fore, current methods use generative models fordata augmentation by generating datasets to supplement thetraining of models on real datasets and improve model per-formance. Although current methods have proposed variousstrategies to enable generative data to boost model perfor-mance, there are still some limitations: 1) Existing methodshave not fully exploited the potential of generative models.First, some methods not only use generative data butalso need to crawl images from the internet, which is signifi-cantly challenging to obtain large-scale data. Meanwhile, thecontent of data crawled from the internet is uncontrollableand needs extra checking. Second, existing methods do notfully use the controllability of generative models. Currentmethods often adopt manually designed templates to con-struct prompts, limiting the potential output of generativemodels. 2) Existing methods often explain the roleof generative data from the perspective of class imbalance ordata scarcity, without considering the discrepancy betweenreal-world data and generative data. Moreover, these meth-ods typically show improved model performance only inscenarios with a limited number of real samples, and theeffectiveness of generative data on existing large-scale realdatasets, like LVIS , is not thoroughly investigated. In this paper, we first explore the role of generative datafrom the perspective of distribution discrepancy, address-ing two main questions: 1) Why does generative data aug-mentation enhance model performance? 2) What types ofgenerative data are beneficial for improving model perfor-",
  "arXiv:2405.10185v1 [cs.CV] 16 May 2024": "mance? First, we find that there exist discrepancies betweenthe model learned distribution of the limited real trainingdata and the distribution of real-world data. We visualize thedata and find that compared to the real-world data, generativedata can expand the data distribution that the model can learn.Furthermore, we find that the role of adding generative datais to alleviate the bias of the real training data, effectivelymitigating overfitting the training data. Second, we find thatthere are also discrepancies between the distribution of thegenerative data and the real-world data distribution. If thesediscrepancies are not handled properly, the full potentialof the generative model cannot be utilized. By conductingseveral experiments, we find that using diverse generativedata enables models to better adapt to these discrepancies,improving model performance. Based on the above analysis, we propose an efficientstrategy for enhancing data diversity, namely, GenerativeData Diversity Enhancement. We design various diversityenhancement strategies to increase data diversity from theperspectives of category diversity, prompt diversity, and gen-erative model diversity. For category diversity, we observethat models trained with generative data covering all cate-gories adapt better to distribution discrepancy than modelstrained with partial categories. Therefore, we introduce notonly categories from LVIS but also extra categories fromImageNet-1K to enhance category diversity in data gen-eration, thereby reinforcing the models adaptability to distri-bution discrepancy. For prompt diversity, we find that as thescale of the generative dataset increases, manually designedprompts cannot scale up to the corresponding level, limitingthe diversity of output images from the generative model.Thus, we design a set of diverse prompt generation strate-gies to use large language models, like ChatGPT, for promptgeneration, requiring the large language models to outputmaximally diverse prompts under constraints. By combiningmanually designed prompts and ChatGPT designed prompts,we effectively enrich prompt diversity and further improvegenerative data diversity. For generative model diversity,we find that data from different generative models also ex-hibit distribution discrepancies. Exposing models to datafrom different generative models during training can enhanceadaptability to different distributions. Therefore, we employStable Diffusion and DeepFloyd-IF to generateimages for all categories separately and mix the two types ofdata during training to increase data diversity. At the same time, we optimize the data generation work-flow and propose a four-stage generative pipeline consistingof instance generation, instance annotation, instance filtra-tion, and instance augmentation. In the instance generationstage, we employ our proposed Generative Data DiversityEnhancement to enhance data diversity, producing diverseraw data. In the instance annotation stage, we introduce anannotation strategy called SAM-background. This strategy obtains high-quality annotations by using background pointsas input prompts for SAM , obtaining the annotationsof raw data. In the instance filtration stage, we introduce ametric called CLIP inter-similarity. Utilizing the CLIP image encoder, we extract embeddings from generative andreal data, and then compute their similarity. A lower simi-larity indicates lower data quality. After filtration, we obtainthe final generative dataset. In the instance augmentationstage, we use the instance paste strategy to increasemodel learning efficiency on generative data.Experiments demonstrate that our designed data diver-sity strategies can effectively improve model performanceand maintain the trend of performance gains as the datascale increases to the million level, which enables large-scale generative data for data augmentation. On the LVISdataset, DiverGen significantly outperforms the strong modelX-Paste , achieving +1.1 box AP and +1.1 mask APacross all categories, and +1.9 box AP and +2.5 mask APfor rare categories.In summary, our main contributions are as follows: We explain the role of generative data from the perspec-tive of distribution discrepancy. We find that generativedata can expand the data distribution that the model canlearn, mitigating overfitting the training set and the di-versity of generative data is crucial for improving modelperformance. We propose the Generative Data Diversity Enhancementstrategy to increase data diversity from the aspects of cat-egory diversity, prompt diversity, and generative modeldiversity. By enhancing data diversity, we can scale thedata to millions while maintaining the trend of model per-formance improvement. We optimize the data generation pipeline. We proposean annotation strategy SAM-background to obtain higher-quality annotations. We also introduce a filtration metriccalled CLIP inter-similarity to filter data and further im-prove the quality of the generative dataset.",
  ". Related Work": "Instance segmentation. Instance segmentation is an im-portant task in the field of computer vision and has beenextensively studied. Unlike semantic segmentation, instancesegmentation not only classifies the pixels at a pixel levelbut also distinguishes different instances of the same cat-egory. Previously, the focus of instance segmentation re-search has primarily been on the design of model structures.Mask-RCNN unifies the tasks of object detection andinstance segmentation. Subsequently, Mask2Former fur-ther unified the tasks of semantic segmentation and instancesegmentation by leveraging the structure of DETR .Orthogonal to these studies focusing on model architec-ture, our work primarily investigates how to better utilizegenerated data for this task. We focus on the challenging",
  "LVIS valDeepFloyd-IFLVIS trainLVIS valStable DiffusionDeepFloyd-IF": ". Visualization of data distributions on different sources. Compared to real-world data (LVIS train and LVIS val), generative data(Stable Diffusion and IF) can expand the data distribution that the model can learn. long-tail dataset LVIS because it is only the long-tailedcategories that face the issue of limited real data and re-quire generative images for augmentation, making it morepractically meaningful.Generative data augmentation. The use of generativemodels to synthesize training data for assisting percep-tion tasks such as classification , detection ,segmentation , etc. has received widespreadattention from researchers. In the field of segmentation,early works utilize generative adversarial networks(GANs) to synthesize additional training samples. Withthe rise of diffusion models, there have been numerous ef-forts to utilize text2image diffusion mod-els, such as Stable Diffusion , to boost the segmentationperformance. Li et al. combine the Stable Diffusionmodel with a novel grounding module and establish an auto-matic pipeline for constructing a segmentation dataset. Dif-fuMask exploits the potential of cross-attention mapsbetween text and images to synthesize accurate semanticlabels. More recently, FreeMask uses a mask-to-imagegeneration model to generate images conditioned on the pro-vided semantic masks. However, the aforementioned workis only applicable to semantic segmentation. The most rele-vant work to ours is X-Paste , which promotes instancesegmentation through copy-pasting the generative imagesand a filter strategy based on CLIP .In summary, most methods only demonstrate significantadvantages when training data is extremely limited. Theyconsider generating data as a means to compensate for datascarcity or class imbalance. However, in this work, we takea further step to examine and analyze this problem fromthe perspective of data distribution. We propose a pipelinethat enhances diversity from multiple levels to alleviate theimpact of data distribution discrepancies. This provides newinsights and inspirations for further advancements in thisfield.",
  ". Analysis of Data Distribution": "Existing methods often attribute the role of gen-erative data to addressing class imbalance or data scarcity. Inthis paper, we provide an explanation for two main questionsfrom the perspective of distribution discrepancy.Why does generative data augmentation enhance modelperformance? We argue that there exist discrepancies be-tween the model learned distribution of the limited real train-ing data and the distribution of real-world data. The roleof adding generative data is to alleviate the bias of the realtraining data, effectively mitigating overfitting the trainingdata.First, to intuitively understand the discrepancies betweendifferent data sources, we use CLIP image encoder toextract the embeddings of images from different data sources,and then use UMAP to reduce dimensions for visualiza-tion. Visualization of data distributions on different sourcesis shown in . Real-world data (LVIS train andLVIS val) cluster near the center, while generative data (Sta-ble Diffusion and IF ) are more dispersed, indicatingthat generative data can expand the data distribution that themodel can learn.Then, to characterize the distribution learned by themodel, we employ the free energy formulation used byJoseph et al. . This formulation transforms the logitsoutputted by the classification head into an energy function.The formulation is shown below:",
  "Here, q is the feature of instance, hc(q) is the cth logitoutputted by classification head h(.), n is the number ofcategories and is the temperature parameter. We train": "one model using only the LVIS train set (train), and an-other model using LVIS train with generative data (gen).Both models are evaluated on the LVIS val set and we useinstances that are successfully matched by both models to ob-tain energy values. Additionally, we train another model us-ing LVIS val (val), treating it as representative of real-worlddata distribution. Then, we further fit Gaussian distributionsto the histograms of energy values to obtain the mean andstandard deviation for each model and compute the KLdivergence between them. DKL(ptrainpval) is 0.063,and DKL(pgenpval) is 0.019. The latter is lower, indicatingthat using generative data mitigates the bias of limited realtraining data.Moreover, we also analyze the role of generative datafrom a metric perspective. We randomly select up to fiveimages per category to form a minitrain set and then conductinferences using train and gen. Then, we define a metric,termed train-val gap (TVG), which is formulated as follows:",
  "TVGkw = APkwminitrain APkwval.(2)": "Here, TVGkw is train-val gap of w category on task k,APkwd is AP of w category on k obtained on datasetd, w {f, c, r}, with f, c, r standing for frequent, common,rare respectively, and k {box, mask}, with box, maskreferring to the object detection and instance segmentation.The train-val gap serves as a measure of the disparity in themodels performance between the training and validationsets. A larger gap indicates a higher degree of overfittingthe training set. The results, as presented in , showthat the metrics for the rare categories consistently surpassthose of frequent and common. This observation suggeststhat the model tends to overfit more on the rare categoriesthat have fewer examples. With the augmentation of genera-tive data, all TVG of gen are lower than train, showing thatadding generative data can effectively alleviate overfittingthe training data.",
  "LVIS13.1610.7121.8016.8039.5931.68LVIS + Gen9.648.3815.6412.6929.3922.49": ". Results of train-val gap on different data sources.With the augmentation of generative data, all TVG of LVIS arelower than LVIS + Gen, showing that adding generative data caneffectively alleviate overfitting to the training data. What types of generative data are beneficial for improv-ing model performance? We argue that there are alsodiscrepancies between the distribution of the generative dataand the real-world data distribution. If these discrepanciesare not properly addressed, the full potential of the generativemodel cannot be attained.We divide the generative data into frequent, common,and rare groups, and train three models using each group of data as instance paste source. The inference resultsare shown in . We find that the metrics on the cor-responding category subset are lowest when training withonly one group of data. We consider model performanceto be primarily influenced by the quality and diversity ofdata. Given that the quality of generative data is relativelyconsistent, we contend insufficient diversity in the data canmislead the distribution that the model can learn and a morecomprehensive understanding is obtained by the model froma diverse set of data. Therefore, we believe that using di-verse generative data enables models to better adapt to thesediscrepancies, improving model performance.",
  ". Generative Data Diversity Enhancement": "Through the analysis above, we find that the diversity ofgenerative data is crucial for improving model performance.Therefore, we design a series of strategies to enhance datadiversity at three levels: category diversity, prompt diversity,and generative model diversity, which help the model to bet-ter adapt to the distribution discrepancy between generativedata and real data.Category diversity. The above experiments show that in-cluding data from partial categories results in lower per-formance than incorporating data from all categories. Webelieve that, akin to human learning, the model can learnfeatures beneficial to the current category from some othercategories. Therefore, we consider increasing the diversity ofdata by adding extra categories. First, we select some extracategories besides LVIS from ImageNet-1K categoriesbased on WordNet similarity. Then, the generative datafrom LVIS and extra categories are mixed for training, requir-ing the model to learn to distinguish all categories. Finally,we truncate the parameters in the classification head corre-sponding to the extra categories during inference, ensuringthat the inferred category range remains within LVIS.Prompt diversity. The output images of the text2image gen-erative model typically rely on the input prompts. Existingmethods usually generate prompts by manually design-ing templates, such as a photo of a single {category name}.When the data scale is small, designing prompts manually isconvenient and fast. However, when generating a large scale of data, it is challenging to scale the number of manuallydesigned prompts correspondingly. Intuitively, it is essentialto diversify the prompts to enhance data diversity. To easilygenerate a large number of prompts, we choose large lan-guage model, like ChatGPT, to enhance the prompt diversity.We have three requirements for the large language model:1) each prompt should be as different as possible; 2) eachprompt should ensure that there is only one object in the im-age; 3) prompts should describe different attributes of the cat-egory. For example, if the category is food, prompts shouldcover attributes like color, brand, size, freshness, packagingtype, packaging color, etc. Limited by the inference costof ChatGPT, we use the manually designed prompts as thebase and only use ChatGPT to enhance the prompt diversityfor a subset of categories. Moreover, we also leverage thecontrollability of the generative model, adding the constraintin a white background after each prompt to make the back-ground of output images simple and clear, which reduces thedifficulty of mask annotation. Generative model diversity. The quality and style of outputimages vary across generative models, and the data distri-bution learned solely from one generative models data islimited. Therefore, we introduce multiple generative mod-els to enhance the diversity of data, allowing the modelto learn from wider data distributions. We selected twocommonly used generative models, Stable Diffusion (SD) and DeepFloyd-IF (IF). We use Stable DiffusionV1.5, generating images with a resolution of 512 512, anduse images output from Stage II of IF with a resolution of256 256. For each category in LVIS, we generated 1k im-ages using two models separately. Examples from differentgenerative models are shown in .",
  ". Generative Pipeline": "The generative pipeline of DiverGen is built upon X-Paste . It can be divided into four stages: instance gen-eration, instance annotation, instance filtration and instanceaugmentation. The overview of DiverGen is illustrated in. Instance generation. Instance generation is a crucial stagefor enhancing data diversity. In this stage, we employ ourproposed Generative Data Diversity Enhancement (GDDE),as mentioned in Sec 3.2. In category diversity enhancement,we utilize the category information from LVIS categoriesand extra categories selected from ImageNet-1K . Inprompt diversity enhancement, we utilize manually designedprompts and ChatGPT designed prompts to enhance promptdiversity. In model diversity enhancement, we employ twogenerative models, SD and IF. Instance annotation. We employ SAM as our annota-tion model. SAM is a class-agnostic promptable segmenterthat outputs corresponding masks based on input prompts,such as points, boxes, etc. In instance generation, leveragingthe controllability of the generative model, the generative im-ages have two characteristics: 1) each image predominantlycontains only one foreground object; 2) the background ofthe images is relatively simple. Therefore, we introduce aSAM-background (SAM-bg) annotation strategy. SAM-bgtakes the four corner points of an image as input promptsfor SAM to obtain the background mask, then inverts thebackground mask as the mask of the foreground object. Dueto the conditional constraints during the instance genera-tion stage, this strategy is simple but effective in producinghigh-quality masks. Instance filtration. In the instance filtration stage, X-Pasteutilizes the CLIP score (similarity between images and text)as the metric for image filtering. However, we observe thatthe CLIP score is ineffective in filtering low-quality im-ages. In contrast to the similarity between images and text,we think the similarity between images can better filter outlow-quality images. Therefore, we propose a new metriccalled CLIP inter-similarity. We use the image encoder ofCLIP to extract image embeddings for objects in thetraining set and generative images, then calculate the similar-ity between them. If the similarity is too low, it indicates asignificant disparity between the generative and real images,suggesting that it is probably a poor-quality image and needsto be filtered. Instance augmentation. We use the augmentation strategyproposed by X-Paste but do not use the data retrievedfrom the network or the instances in LVIS training setas the paste data source, only use the generative data as thepaste data source.",
  ". Settings": "Datasets. We choose LVIS for our experiments. LVISis a large-scale instance segmentation dataset, containing164k images with approximately two million high-qualityannotations of instance segmentation and object detection.LVIS dataset uses images from COCO 2017 dataset, butredefines the train/val/test splits, with around 100k imagesin the training set and around 20k images in the validationset. The annotations in LVIS cover 1,203 categories, with atypical long-tailed distribution of categories, so LVIS furtherdivides the categories into frequent, common, and rare basedon the frequency of each category in the dataset. We use theofficial LVIS training split and the validation split.Evaluation metrics.The evaluation metrics are LVISbox average precision (APbox) and mask average precision(APmask). We also provide the average precision of rarecategories (APboxrand APmaskr). The maximum number ofdetections per image is 300.Implementation details. We use CenterNet2 as thebaseline and Swin-L as the backbone. In the trainingprocess, we initialize the parameters by the pre-trained Swin-L weights provided by Liu et al. . The training size is 896and the batch size is 16. The maximum training iterations is180,000 with an initial learning rate of 0.0001. We use theinstance paste strategy provided by Zhao et al. .",
  "Data diversity is more important than quantity. To inves-tigate the impact of different scales of generative data, we": "use generative data of varying scales as paste data sources.We construct three datasets using only DeepFloyd-IF with manually designed prompts, all containing originalLVIS 1,203 categories, but with per-category quantities of0.25k, 0.5k, and 1k, resulting in total dataset scales of 300k,600k, and 1,200k. As shown in , we find that usinggenerative data improves model performance compared tothe baseline. However, as the dataset scale increases, themodel performance initially improves but then declines. Themodel performance using 1,200k data is lower than thatusing 600k data. Due to the limited number of manuallydesigned prompts, the generative model produces similardata, as shown in a. Consequently, the model cannot gain benefits from more data. However, when using ourproposed Generative Data Diversity Enhancement (GDDE),due to the increased data diversity, the model trained with1,200k images achieves better results than using 600k im-ages, with an improvement of 1.21 box AP and 1.04 maskAP. Moreover, when using the same data scale of 600k, themask AP increased by 0.64 AP and the box AP increasedby 0.55 AP when using GDDE compared to not using it.The results demonstrate that data diversity is more importantthan quantity. When the scale of data is small, increasingthe quantity of data can improve model performance, whichwe consider is an indirect way of increasing data diversity.However, this simplistic approach of solely increasing quan-tity to increase diversity has an upper limit. When it reachesthis limit, explicit data diversity enhancement strategies be-come necessary to maintain the trend of model performanceimprovement.",
  ".83300k49.6544.0145.6841.11600k50.0344.4447.1541.961200k49.4443.7542.9637.91600k50.6744.9948.5243.631200k51.2445.4850.0745.85": ". Results of different scales of generative data. Whenusing the same data scale, models using our proposed GDDE canachieve higher performance than those without it, showing that datadiversity is more important than quantity. verGen with previous data-augmentation related methodsin . Compared to the baseline CenterNet2 , ourmethod significantly improves, increasing box AP by +3.7and mask AP by +3.2. Regarding rare categories, our methodsurpasses the baseline with +8.7 in box AP and +9.0 in maskAP. Compared to the previous strong model X-Paste ,we outperform it with +1.1 in box AP and +1.1 in maskAP of all categories, and +1.9 in box AP and +2.5 in maskAP of rare categories. It is worth mentioning that, X-Pasteutilizes both generative data and web-retrieved data as pastedata sources during training, while our method exclusivelyuses generative data as the paste data source. We achievethis by designing diversity enhancement strategies, furtherunlocking the potential of generative models.",
  ". Ablation Studies": "We analyze the effects of the proposed strategies in DiverGenthrough a series of ablation studies using the Swin-L backbone.Effect of category diversity. We select 50, 250, and 566extra categories from ImagNet-1K , and generate 0.5kimages for each category, which are added to the baseline.The baseline only uses 1,203 categories of LIVS to gener-ate data. We show the results in . Generally, increas-ing the number of extra categories initially improves thendeclines model performance, peaking at 250 extra categories.The trend suggests that using extra categories to enhancecategory diversity can improve the models generalizationcapabilities, but too many extra categories may mislead themodel, leading to a decrease in performance.",
  ". Ablation of different generative models. Increasingmodel diversity is beneficial for improving model performance": "Effect of annotation strategy.X-Paste uses fourmodels (U2Net , SelfReformer , UFO andCLIPseg ) to generate masks and selects the one with thehighest CLIP score. We compare our proposed annotationstrategy (SAM-bg) to that proposed by X-Paste (max CLIP).In , SAM-bg outperforms max CLIP strategy acrossall metrics, indicating that our proposed strategy can pro-duce better annotations, improving model performance. Asshown in , SAM-bg unlocks the potential capabilityof SAM, obtaining precise and refined masks.Effect of CLIP inter-similarity. We compare our proposedCLIP inter-similarity to CLIP score . The results areshown in . The performance of data filtered by CLIPinter-similarity is higher than that of CLIP score, demonstrat-ing that CLIP inter-similarity can filter low-quality imagesmore effectively.",
  ". Conclusions": "In this paper, we explain the role of generative data augmen-tation from the perspective of data distribution discrepanciesand find that generative data can expand the data distributionthat the model can learn, mitigating overfitting the trainingset. Furthermore, we find that data diversity of generativedata is crucial for improving model performance. Therefore,we design an efficient data diversity enhancement strategy,Generative Data Diversity Enhancement. We design vari-ous diversity enhancement strategies to increase data diver-sity from the aspects of category diversity, prompt diversity,and generative model diversity. Finally, we optimize thedata generative pipeline by designing the annotation strategySAM-background to obtain higher quality annotations andintroducing the metric CLIP inter-similarity to filter data,which further improves the quality of the generative dataset.Through these designed strategies, our proposed methodsignificantly outperforms the existing strong models. Wehope DiverGen can provide new insights and inspirationsfor future research on the effectiveness and efficiency ofgenerative data augmentation.",
  "David Arthur and Sergei Vassilvitskii. K-means++ the ad-vantages of careful seeding. In Proc. Annual ACM-SIAMSymposium on Discrete algorithms, pages 10271035, 2007.11": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proc. Eur. Conf.Comp. Vis. Springer, 2020. 1, 2 Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, ZhenguoLi, and Dit-Yan Yeung. Integrating geometric control intotext-to-image diffusion models for high-quality detection datageneration via text prompt. arXiv: Comp. Res. Repository,2023. 3 Bowen Cheng, Ishan Misra, Alexander G Schwing, AlexanderKirillov, and Rohit Girdhar. Masked-attention mask trans-former for universal image segmentation. In Proc. IEEE Conf.Comp. Vis. Patt. Recogn., pages 12901299, 2022. 1, 2",
  "Christiane Fellbaum. Wordnet. In Theory and applications ofontology: computer applications, pages 231243. Springer,2010. 4, 11": "Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wang-meng Zuo. Diverse data augmentation with diffusions foreffective test-time prompt tuning. In Proc. IEEE Int. Conf.Comp. Vis., pages 27042714, 2023. 3 Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, and Barret Zoph. Simplecopy-paste is a strong data augmentation method for instancesegmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,pages 29182928, 2021. 7",
  "James M Joyce. Kullback-leibler divergence. In InternationalEncyclopedia of Statistical Science, pages 720722. Springer,2011. 4": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander Berg, Wan-Yen Lo, et al. Segment anything.In Proc. IEEE Int. Conf. Comp. Vis., pages 40154026, 2023.1, 2, 5, 11 Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis,Sanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthe-sizing imagenet with pixel-wise annotations. In Proc. IEEEConf. Comp. Vis. Patt. Recogn., pages 2133021340, 2022. 3",
  "Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, YanfengWang, and Weidi Xie. Open-vocabulary object segmentationwith diffusion models. In Proc. IEEE Int. Conf. Comp. Vis.,pages 76677676, 2023. 3": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. In Proc.Eur. Conf. Comp. Vis., pages 740755. Springer, 2014. 6 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProc. IEEE Int. Conf. Comp. Vis., pages 1001210022, 2021.6, 7",
  "Leland McInnes, John Healy, and James Melville. Umap:Uniform manifold approximation and projection for dimen-sion reduction. arXiv: Comp. Res. Repository, 2018. 3, 11": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo,Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, DanielHaziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:Learning robust visual features without supervision. Trans.Mach. Learn. Research, 2023. 12 Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-hghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Go-ing deeper with nested u-structure for salient object detection.Pattern Recognition, 106:107404, 2020. 8 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervision.In Proc. Int. Conf. Mach. Learn., pages 87488763. PMLR,2021. 2, 3, 5, 11, 12 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution imagesynthesis with latent diffusion models. In Proc. IEEE Conf.Comp. Vis. Patt. Recogn., pages 1068410695, 2022. 1, 2, 3,5, 8, 11 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet large scalevisual recognition challenge. Int. J. Comput. Vision, 115:211252, 2015. 2, 4, 5, 7, 11",
  "Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva,Christoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova.Deepfloyd-if, 2023. 1, 2, 3, 5, 6, 8, 11": "Yukun Su, Jingliang Deng, Ruizhou Sun, Guosheng Lin, Han-jing Su, and Qingyao Wu. A unified transformer frameworkfor group-based segmentation: Co-segmentation, co-saliencydetection and video salient object detection. IEEE Trans.Multimedia, 2023. 8 Jingru Tan, Gang Zhang, Hanming Deng, Changbao Wang,Lewei Lu, Quanquan Li, and Jifeng Dai. 1st place solutionof LVIS challenge 2020: A good box is not a guarantee of agood mask. arXiv: Comp. Res. Repository, 2020. 7 Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao,Yefei He, Hong Zhou, Mike Zheng Shou, and Chunhua Shen.DatasetDM: Synthesizing data with perception annotationsusing diffusion models. Proc. Advances in Neural Inf. Process.Syst., 2023. 1, 3 Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou,and Chunhua Shen. Diffumask: Synthesizing images withpixel-level annotations for semantic segmentation using dif-fusion models. Proc. IEEE Int. Conf. Comp. Vis., 2023. 1,3 Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong,and Chen Change Loy. Mosaicfusion: Diffusion models asdata augmenters for large vocabulary instance segmentation.arXiv: Comp. Res. Repository, 2023. 3 Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, andHengshuang Zhao. FreeMask: Synthetic images with denseannotations make stronger segmentation models. Proc. Ad-vances in Neural Inf. Process. Syst., 2023. 3",
  "Yi Ke Yun and Weisi Lin. Selfreformer: Self-refined networkwith transformer for salient object detection. arXiv: Comp.Res. Repository, 2022. 8": "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-qiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt,generate, then cache: Cascade of foundation models makesstrong few-shot learners. In Proc. IEEE Conf. Comp. Vis. Patt.Recogn., pages 1521115222, 2023. 3 Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, andSanja Fidler. Datasetgan: Efficient labeled data factory withminimal human effort. In Proc. IEEE Conf. Comp. Vis. Patt.Recogn., pages 1014510155, 2021. 3 Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong Chen,Dong Chen, Fang Wen, Lu Yuan, Ce Liu, Wenbo Zhou, QiChu, Weiming Zhang, and Nenghai Yu. X-paste: Revisitingscalable copy-paste for instance segmentation using CLIP andstablediffusion. Proc. Int. Conf. Mach. Learn., 2023. 1, 2, 3,4, 5, 6, 7, 8, 12, 13",
  "A.1. Data Distribution Analysis": "We use the image encoder of CLIP ViT-L/14 to extractimage embeddings. For objects in the LVIS dataset, weextract embeddings from the object regions instead of thewhole images. First, we blur the regions outside the objectmasks using the normalized box filter, with the kernel sizeof (10, 10). Then, to prevent objects from being too small,we pad around the object boxes to ensure the minimumwidth of the padded boxes is 80 pixels, and crop the imagesaccording to the padded boxes. Finally, the cropped imagesare fed into the CLIP image encoder to extract embeddings.For generative images, the whole images are fed into theCLIP image encoder to extract embeddings. At last, we useUMAP to reduce dimensions for visualization. is setto 0.9 in the energy function.To investigate the potential impact of noise in the rareclasses to TVG metrics, we conduct additional experimentsto demonstrate the validity of TVG. We randomly take fivedifferent models each for the LVIS and LVIS + Gen datasources, compute the mean () and standard deviation ()of their TVG, and calculate the 3 sigma range ( + 3 and3), which we think represents the maximum fluctuationthat potential noise could induce. As shown in , wefind that: 1) The TVGs of LVIS all exceed the 3 sigma upperbound of LVIS + Gen, while the TVGs of LVIS + Gen areall below the 3 sigma lower bound of LVIS, and there is nooverlap between the 3 sigma ranges of LVIS and LVIS +Gen; 2) For both LVIS + Gen and LVIS, there is no overlapbetween the 3 sigma ranges of different groups, e.g. frequentand common, common and rare. These two findings suggestthat even in the presence of potential noise, the results cannot be attributed to those fluctuations. Therefore, we thinkour proposed TVG metrics are reasonable and can supportthe conclusions.",
  "A.2. Category Diversity": "We compute the path similarity of WordNet synsets be-tween 1,000 categories in ImageNet-1K and 1,203 cat-egories in LVIS . For each of the 1,000 categories inImageNet-1K, if the highest similarity for that category isbelow 0.4, we consider the category to be non-existent inLVIS and designate it as an extra category. Based on thismethod, 566 categories can serve as extra categories. Thenames of these 566 categories are presented in .",
  "A.3. Prompt Diversity": "Limited by the inference cost of ChatGPT, we use the man-ually designed prompts as the base and only use ChatGPTto enhance the prompt diversity for a subset of categories.For manually designed prompts, the template of promptsis a photo of a single {category name}, {category def}, ina white background. category name and category def arefrom LVIS category information. For ChatGPT designedprompts, we select a subset of categories and use ChatGPTto enhance prompt diversity for these categories. The namesof the 144 categories in this subset are shown in .We use GPT-3.5-turbo and have three requirements for theChatGPT: 1) each prompt should be as different as possible;2) each prompt should ensure that there is only one objectin the image; 3) prompts should describe different attributesof the category. Therefore, the input prompts to ChatGPTcontain these three requirements. Examples of input promptsand the corresponding responses from ChatGPT are illus-trated in . To conserve output token length, thereis no strict requirement for ChatGPT designed prompts toend with in a white background, and this constraint willbe added when generating images.",
  "A.4. Generative Model Diversity": "We select two commonly used generative models, StableDiffusion and DeepFloyd-IF . For Stable Diffusion,we use Stable Diffusion V1.5, with 50 inference steps and aguidance scale of 7.5. All other parameters are set to theirdefaults. For DeepFloyd-IF, we use the output images fromstage II, with stage I using the weight IF-I-XL-v1.0 and stageII using IF-II-L-v1.0. All parameters are set to their defaults.",
  "A.5. Instance Annotation": "We employ SAM ViT-H as the annotation model. Weexplore two annotation strategies, namely SAM-foregroundand SAM-background. SAM-foreground uses points sam-pled from foreground objects as input prompts. Specifically,we first obtain the approximate region of the foreground ob-ject based on the cross-attention map of the generative modelusing a threshold. Then, we use k-means++ clusteringto transform dense points within the foreground region intocluster centers. Next, we randomly select some points from the cluster centers as inputs to SAM. We use various metricsto evaluate the quality of the output mask and select the maskwith the highest score as the final mask. However, althoughSAM-foreground is intuitive, it also has some limitations.Firstly, cross-attention maps of different categories requiredifferent thresholds to obtain foreground regions, making itcumbersome to choose the optimal threshold for each cate-gory. Secondly, the number of points required for SAM tooutput mask varies for different foreground objects. Com-plex object needs more points than simple object, makingit challenging to control the number of points. Addition-ally, the position of points significantly influences the qualityof SAMs output mask. If the position of points is not ap-propriate, this strategy is prone to generating incompletemasks.Therefore, we discard SAM-foreground and proposea simpler and more effective annotation strategy, SAM-background. Due to our leveraging of the controllability ofthe generative model in instance generation, the generativeimages have two characteristics: 1) each image predomi-nantly contains only one foreground object; 2) the back-ground of the images is relatively simple. SAM-backgrounddirectly uses the four corner points of the image as inputprompts for SAM to obtain the background mask, then in-verts the background mask as the mask of the foreground ob-ject. The illustrations of point selection for SAM-foregroundand SAM-background are shown in . By usingSAM-background for annotation, more refined masks can beobtained. Examples of annotations from SAM-foregroundand SAM-background are shown in .",
  ". Results of SAM-foreground and SAM-background.SAM-background achieves better annotation quality": "consistent with Sec A.1. Then we calculate the cosine sim-ilarity between embeddings of objects in LVIS training setand embeddings of generative images. For each generativeimage, the final CLIP inter-similarity is the average simi-larity with all objects of the same category in the trainingset. Through experiments, we find that when the filteringthreshold is 0.6, the model achieves the best performanceand strikes a balance between data diversity and quality, sowe set the threshold to 0.6.Furthermore, we also explore other filtration strategies.From our experiments, using pure image-trained models likeDINOv2 as image encoder or combining CLIP scoreand CLIP inter-similarity is not as good as using just CLIPinter-similarity alone, as shown in . Therefore, weultimately opt to only use CLIP inter-similarity.",
  "B.1. Prompt Diversity": "We find that images generated from ChatGPT designedprompts have diverse textures, styles, patterns, etc., greatlyenhancing data diversity. The ChatGPT designed promptsand the corresponding generative images are shown in Fig-ure 9. Compared to manually designed prompts, the diversityof images generated from ChatGPT designed prompts can besignificantly improved. A visual comparison between gener-ative images from manually designed prompts and ChatGPTdesigned prompts is shown in .",
  "B.2. Generative Model Diversity": "The images generated by Stable Diffusion and DeepFloyd-IFare different, even within the same category, significantlyenhancing the data diversity. Both Stable Diffusion andDeepFloyd-IF are capable of producing images belongingto the target categories. However, the images generatedby DeepFloyd-IF appear more photorealistic and consis-tent with the prompt texts. This indicates DeepFloyd-IFssuperiority in image generation quality and controllabilitythrough text prompts. Examples from Stable Diffusion andDeepFloyd-IF are shown in and , respec-tively.",
  "B.4. Instance Augmentation": "The use of instance augmentation strategies helps alleviatethe limitation in relatively simple scenes of generative dataand improves the efficiency of model learning on the gen-erative data. Examples of augmented data are shown in. tenchgreat white sharktiger sharkelectric raystingraybramblinggoldfinchhouse finchjuncoindigo buntingAmerican robinbulbuljaymagpiechickadeeAmerican dipperkite (bird of prey)fire salamandersmooth newtnewtspotted salamanderaxolotlAmerican bullfrogloggerhead sea turtleleatherback sea turtlebanded geckogreen iguanaCarolina anoledesert grassland whiptail lizardagamafrilled-necked lizardalligator lizardGila monsterEuropean green lizardchameleonKomodo dragonNile crocodiletriceratopsworm snakering-necked snakeeastern hog-nosed snakesmooth green snakekingsnakegarter snakewater snakevine snakenight snakeboa constrictorAfrican rock pythonIndian cobragreen mambaSaharan horned vipereastern diamondback rattlesnakesidewinder rattlesnaketrilobiteharvestmanscorpiontickcentipedeblack grouseptarmiganruffed grouseprairie grousepeafowlquailpartridgesulphur-crested cockatoolorikeetcoucalbee eaterhornbilljacamartoucanred-breasted merganserblack swantuskerechidnaplatypuswallabywombatjellyfishsea anemonebrain coralflatwormnematodeconchsnailslugsea slugchitonchambered nautilusAmerican lobstercrayfishhermit crabisopodwhite storkblack storkspoonbillgreat egretcrane birdlimpkincommon gallinuleAmerican cootbustardruddy turnstonedunlincommon redshankdowitcheroystercatcheralbatrossgrey whaledugongsea lionChihuahuaJapanese ChinMaltesePekingeseShih TzuKing Charles SpanielPapillontoy terrierRhodesian RidgebackAfghan HoundBasset HoundBeagleBloodhoundBluetick CoonhoundBlack and Tan CoonhoundTreeing Walker CoonhoundEnglish foxhoundRedbone CoonhoundborzoiIrish WolfhoundItalian GreyhoundWhippetIbizan HoundNorwegian ElkhoundOtterhoundSalukiScottish DeerhoundWeimaranerStaffordshire Bull TerrierAmerican Staffordshire TerrierBedlington TerrierBorder TerrierKerry Blue TerrierIrish TerrierNorfolk TerrierNorwich TerrierYorkshire TerrierWire Fox TerrierLakeland TerrierSealyham TerrierAiredale TerrierCairn TerrierAustralian TerrierDandie Dinmont TerrierBoston TerrierMiniature SchnauzerGiant SchnauzerStandard SchnauzerScottish TerrierTibetan TerrierAustralian Silky TerrierSoft-coated Wheaten TerrierWest Highland White TerrierLhasa ApsoFlat-Coated RetrieverCurly-coated RetrieverGolden RetrieverLabrador RetrieverChesapeake Bay RetrieverGerman Shorthaired PointerVizslaEnglish SetterIrish SetterGordon SetterBrittany dogClumber SpanielEnglish Springer SpanielWelsh Springer SpanielCocker SpanielSussex SpanielIrish Water SpanielKuvaszSchipperkeGroenendael dogMalinoisDobermannMiniature PinscherGreater Swiss Mountain DogBernese Mountain DogAppenzeller SennenhundEntlebucher SennenhundBoxerBullmastiffTibetan MastiffGreat DaneSt. BernardhuskyAlaskan MalamuteSiberian HuskyAffenpinscherSamoyedPomeranianChow ChowKeeshondbrussels griffonPembroke Welsh CorgiCardigan Welsh CorgiToy PoodleMiniature PoodleStandard PoodledingodholeAfrican wild doghyenared foxkit foxArctic foxgrey foxtabby cattiger catPersian catSiamese catEgyptian Maulynxleopardsnow leopardjaguarcheetahmongoose meerkatdung beetlerhinoceros beetleflybeeantgrasshoppercricket insectstick insectpraying mantiscicadaleafhopperlacewingdamselflyred admiral butterflymonarch butterflysmall white butterflysea urchinsea cucumberharefox squirrelguinea pigwild boarwarthogoxwater buffalobisonbighorn sheepAlpine ibexhartebeestimpala (antelope)llamaweaselminkblack-footed ferretotterskunkbadgerarmadillothree-toed slothorangutanchimpanzeegibbonsiamangguenonpatas monkeymacaquelangurblack-and-white colobusproboscis monkeymarmosetwhite-headed capuchinhowler monkeytiti monkeyGeoffroys spider monkeycommon squirrel monkeyring-tailed lemurindrired pandasnoek fisheelrock beauty fishclownfishsturgeongar fishlionfishacademic gownaccordionaircraft carrieraltarapiaryassault riflebakerybalance beambaluster or handrailbarbershopbarnbarometerbassinetbassoonlighthousebell towerbaby bibboathousebookstorebreakwaterbreastplatebutcher shopcarouseltool kitautomated teller machinecassette playercastlecatamarancellochainchain-link fencechainsawchiffonierChristmas stockingchurchmovie theatercliff dwellingcloakclogsspiral or coilcandy storecradleconstruction cranecroquet ballcuirassdamdesktop computerdisc brakedockdomedrilling rigelectric locomotiveentertainment centerface powderfire screenflutefountainFrench horngas pumpgolf ballgonggreenhouseradiator grillegrocery storeguillotinehair sprayhalf-trackhand-held computerhard disk driveharmonicaharpcombine harvesterholsterhome theaterhoneycombhookgymnastic horizontal barjigsaw puzzleknotlens caplibrarylifeboatlighterlipsticklotionloupe magnifying glasssawmillmessenger bagmaracamarimbamaskmatchstickmaypolemazemegalithmilitary uniformmissilemobile homemodemmonasterymonitormopedmortar and pestlemosquemosquito nettentmousetrapmoving vanmuzzlemetal nailneck bracenotebook computerobeliskoboeocarinaodometeroil filterpipe organoscilloscopeoxygen maskpalacepan fluteparallel barspatiopedestalphotocopierplectrumPickelhaubepicket fencepierpirate shipblock planeplanetariumplastic bagplate rackplungerpolice vanprayer rugprisonhockey puckpunching bagpurseradioradio telescoperain barrelfishing casting reelrestaurantrugby ballsafescabbardschoonerCRT monitorseat beltshoe storeshoji screen or room dividerbalaclava ski maskslide rulesliding doorslot machinesnorkelkeyboard space barspatulamotorboatspider webspindlestage steam locomotivethrough arch bridgesteel drumstethoscopestone walltramstretcherstupasubmarinesundialsunglassessunscreensuspension bridgeswingtape playertelevisionthatched roofthreshing machinethronetile rooftobacco shoptoilet seattorchtotem poletoy storetrimarantriumphal archtromboneturnstiletypewriter keyboardvaulted or arched ceilingvelvet fabricvestmentviaductsinkwhiskey jugwhistlewindow screenwindow shadeairplane wingwoolsplit-rail fenceshipwrecksailboatyurtwebsitecrossworddust jacketmenuplateguacamoletriflebaguettecabbagebroccolispaghetti squashacorn squashbutternut squashcardoonmushroomGranny Smith applejackfruitcherimoya (custard apple)pomegranatehaycarbonarachocolate syrupdoughmeatloafpot piered wineespressotea cupeggnogmountainbubblecliffcoral reefgeyserlakeshorepromontorysandbarbeachvalleyvolcanobaseball playerbridegroomscuba diverrapeseeddaisyyellow ladys slippercornacornrose hiphorse chestnut seedcoral fungusgyromitrastinkhorn mushroomearth star fungushen of the woods mushroomboletecorn cob",
  ". Extra categories from ImageNet-1K": "Biblepirate flagbookmarkbow (weapon)bubble gumelevator carchocolate moussecompasscorkboardcougarcream pitchercylinderdollardolphineyepatchfruit juicegolf clubhandcuffhockey stickpopsiclepan (metal container)pew (church bench)piggy bankpistolroad mapsatchelsawhorseshawlsparkler (fireworks)spiderstring cheeseTabasco sauceturtleneck (clothing)violinwaffle ironwhistlewind chimeheadstall (for horses)fishing rodcoat hangerclaspcrab (animal)flamingostirrupmachine gunpin (non jewelry)speardrumstickcornetbottle openereaseldumbbellgarden hosemoneysaddle (on an animal)garbagewindshield wiperneedleliquorbambooarmorpretzeltongsski polefroghairpintripodflagpolehosebelt bucklestreetlightcoleslawantennahookLegothumbtackcoatrackplow (farm equipment)vinegarstrappoker (fire stirring tool)cufflinkchopsticksaladdragonflymusical instrumentsharpenerbat (animal)lanyardmat (gym equipment)gargoyleunderdrawerspaperback bookrazorbladeearringswordshovelturkey (food)ambulancepencilweathervanetrampolineapplesaucejamskitraytissue paperlamppostclipboardrouter (computer equipment)batterylollipopcrayonlatchfig (fruit)sunglassestoothpickbusiness cardpadlockasparagusshot glasssledkeyboltpipesteering wheeldeck chairgreen beanpouchtelephone polefire hoseladlepliershair curlerhandlescrewdriverdining tablecartoarwolfenvelopelegumeshopping carttrench coat",
  "ChatGPT Response:": "Please write 20 prompts for stable diffusion to generate images of cream pitcher. Each prompt needs to be different, the focus of the description should be the different attributes of the category, such as if the category is animal, the prompts should describe the animals color, size, age, species, etc., if it is food, the prompts should describe the color of food, brand, size, freshness, packaging type, packaging color, etc. Each prompt should ensure that there is only one entity in the generated image. The prompt should start with words like \"a photo of \", \"a real photo of\", \"an image of \", \"a photography of \", \"an illustration of \", etc. The output result should not contain anything else, just a prompt list in the format: 1. promptA 2. promptB 3. promptC ... The first sentence of the output is \"this is the list:\".",
  "Example of using ChatGPT to design prompts": "This is the list: 1. A photo of a small cream pitcher with a glossy white finish. 2. An image of a vintage cream pitcher with delicate floral patterns. 3. A real photo of a large cream pitcher made of fine bone china. 4. A photography of a modern cream pitcher with a sleek silver handle. 5. An illustration of a cream pitcher in a vibrant shade of turquoise. 6. A photo of a small cream pitcher with a rustic, weathered appearance. 7. An image of a cream pitcher adorned with intricate gold filigree. 8. A real photo of a tall cream pitcher with a matte black finish. 9. A photography of a cream pitcher in a classic white color with a scalloped edge. 10. An illustration of a cream pitcher shaped like a whimsical cow. 11. A photo of a vintage cream pitcher with a charming polka dot design. 12. An image of a cream pitcher made of clear glass with a frosted bottom. 13. A real photo of a cream pitcher in a unique hourglass shape. 14. A photography of a cream pitcher with a playful print of colorful flowers. 15. An illustration of a cream pitcher with a hand-painted blue and white pattern. 16. A photo of a small cream pitcher made of delicate porcelain. 17. An image of a cream pitcher with a textured surface resembling tree bark. 18. A real photo of a cream pitcher in a vibrant shade of coral pink. 19. A photography of a cream pitcher with a gold-trimmed spout and handle. 20. An illustration of a cream pitcher shaped like a graceful swan.",
  ". Example of using ChatGPT to design prompts": "A photo of a copper tray with a hammered texture, in a white background A photo of a large tray made of marble with white veins, in a white background A photo of a large tray with vibrant floral designs, in a white background An image of a wooden tray with intricate carvings, in a white background A photo of a small ceramic tray in a vibrant turquoise color, in a white background A photography of a ceramic tray with colorful geometric patterns, in a white background An illustration of a tray made of recycled materials, in a white background A photography of a small porcelain tray adorned with intricate blue and white designs, in a white background A photography of a tray made of bamboo with a natural brown color, in a white background A real photo of a crystal tray with sparkling facets, in a white background A real photo of a large tray made of clear acrylic material, in a white background An illustration of a gold tray with a mirrored bottom, in a white background",
  "SAM-bg": ". Examples of different annotation strategies. Masks generated by max CLIP tend to be incomplete, while our proposed SAM-bgis able to produce more refined and complete masks when processing images with multiple categories. . Examples of augmented data. The use of instance augmentation strategies helps alleviate the limitation in relatively simplescenes of generative data and improves the efficiency of model learning on the generative data."
}