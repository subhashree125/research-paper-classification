{
  "Abstract": "UAV tracking and pose estimation plays an imperativerole in various UAV-related missions, such as formationcontrol and anti-UAV measures. Accurately detecting andtracking UAVs in a 3D space remains a particularly chal-lenging problem, as it requires extracting sparse features ofmicro UAVs from different flight environments and contin-uously matching correspondences, especially during agileflight. Generally, cameras and LiDARs are the two maintypes of sensors used to capture UAV trajectories in flight.However, both sensors have limitations in UAV classifica-tion and pose estimation. This technical report briefly in-troduces the method proposed by our team NTU-ICG forthe CVPR 2024 UG2+ Challenge Track 5. This work de-velops a clustering-based learning detection approach, CL-Det, for UAV tracking and pose estimation using two typesof LiDARs, namely Livox Avia and LiDAR 360. We combinethe information from the two data sources to locate dronesin 3D. We first align the timestamps of Livox Avia data andLiDAR 360 data and then separate the point cloud of ob-jects of interest (OOIs) from the environment. The pointcloud of OOIs is clustered using the DBSCAN method, withthe midpoint of the largest cluster assumed to be the UAVposition. Furthermore, we utilize historical estimations tofill in missing data. The proposed method shows competi-tive pose estimation performance and ranks 5th on the finalleaderboard of the CVPR 2024 UG2+ Challenge.",
  ". Introduction": "Unmanned aerial vehicles (UAVs), commonly known asdrones, have become increasingly prevalent and have signif-icantly impacted various fields such as transportation, pho-tography, and search and rescue , providing immensebenefits to the general public. However, the widespread useand advanced capabilities of small commercial UAVs havealso introduced complex security challenges that go beyondtraditional concerns .",
  "Equally contributed co-first authors, *Corresponding author": "In recent years, there has been a notable surge in researchfocused on anti-UAV systems .Current anti-UAV solutions primarily rely on visual, radar, and radio fre-quency (RF) modalities. Despite these advancements, iden-tifying drones remains a significant challenge for sensorssuch as cameras, especially when drones operate at high al-titudes or under extreme visual conditions. These methodsoften struggle to detect small drones due to their compactsize, resulting in a reduced radar cross-section and a smallervisual presence. Additionally, contemporary anti-UAV re-search predominantly concentrates on object detection and2D tracking, neglecting the critical aspect of 3D trajectoryestimation. This oversight considerably limits the practicalapplications of anti-UAV systems in real-world scenarios. To address these challenges, our team NTU-ICG par-ticipated in the CVPR 2024 UG2+ Challenge Track 5 which aims to integrate features from di-verse modalities to achieve robust 3D UAV position estima-tion, even under challenging conditions where certain sen-sors may fail to provide valid information. This challengeinvolves the use of fisheye camera images, millimeter-waveradar data, and lidar data from a Livox Mid-360 (LiDAR360) and a Livox Avia for both drone-type classification and3D position estimation tasks, with ground truth provided bya Leica Nova MS60 Multi-Station . Our proposed method, a clustering-based learning de-tection approach (CL-Det), leverages the complementarystrengths of Livox Avia and LiDAR 360 to enhance UAVtracking and pose estimation. Initially, we align the times-tamps of Livox Avia data and LiDAR 360 data to ensuretemporal coherence. By utilizing the LiDAR data, whichprovides coordinates of objects in space at specific times-tamps, we compared these coordinates to the known groundtruth positions of the drone at corresponding timestamps.This comparison allowed us to effectively pinpoint the loca-tion of the drone within the cloud of LiDAR data points. Wethen separate the point cloud of objects of interest (OOIs)from the surrounding environment. The OOIs point cloudis clustered using the DBSCAN method, with the midpointof the largest cluster assumed to be the UAV position. The",
  "arXiv:2405.16867v1 [cs.RO] 27 May 2024": "information from the radar dataset provided also encoun-ters a significant challenge from missing data. To addresspotential data gaps, we utilize historical estimations to fillin missing information, ensuring continuity and accuracy inUAV tracking.The evaluation process is conducted on a hold-out setof multimodal datasets derived from the MMAUDdataset, the first dataset dedicated to predicting the 3D po-sitions of drones using multimodal data. The method isevaluated on the provided test data to infer both the posi-tion and type of the drone at each given timestamp. Theranking criteria for this challenge are based on two factors:i) Mean Square Loss (MSE Loss) relative to the groundtruth labels of the testing set (main metric) and ii) the clas-sification accuracy of the UAV types in the testing set.Our proposed method, CL-Det, demonstrated competitiveperformance, ranking 5th on the final leaderboard of theCVPR 2024 UG2+ Challenge, highlighting its effectivenessin real-world UAV tracking and pose estimation tasks.",
  ". K-Means Clustering Approach": "K-Means clustering works by partitioning the entire datasetinto K distinct clusters based on their proximity to the cen-troids of the clusters.For our application, the centroidsrepresent potential drone locations, and the clusters encap-sulate the distribution of LiDAR points around these cen-troids. We initially set the number of clusters to correspondclosely with the expected number of drones in the field ofview of the LiDAR.To determine the drones position, we analyzed the clus-ters at each timestamp, identifying which clusters centroidis nearest to the ground truth position of the drone. Thismethod assumes that the drone is likely to be at the cen-ter of a dense cluster of points, given its significant relativesize and distinct shape compared to the surrounding envi-ronment.",
  ". Optimization of Cluster Parameters": "To optimize our use of the K-Means algorithm, we focusedon the following key areas:Number of Clusters (K): We experimented with differentvalues of K to find the most suitable number that reflectsthe actual number of drones likely to be present in the Li-DAR data. This was critical because an incorrect number ofclusters could lead to inaccurate drone detection, either bymerging multiple drones into a single cluster or dividing asingle drone into multiple clusters.Initialization Method:We tested various initializationmethods to start the clustering process. The default methodis to select cluster centers randomly, but we found that us-ing the k-means++ initialization, which spreads out the initial centroids before proceeding with the standard algo-rithm, often led to better and more consistent results.Iteration and Convergence: The algorithm was allowed torun until the centroids did not change significantly betweeniterations, ensuring that a stable solution was found. Wemonitored the change in centroid positions as a function ofiteration to determine when the algorithm had effectivelyconverged.",
  ". Monitoring and Estimating Drone Position": "Cluster Density and Centroid Proximity: After formingclusters, we analyzed each clusters density and the proxim-ity of its centroid to the LiDAR data points. Clusters withhigher point densities were considered more likely to repre-sent the drone, as drones typically generate more reflectiveLiDAR returns than the surrounding air.Centroid Tracking: By tracking the movement of the cen-troid over time, we could further refine our estimate of thedrones trajectory and position.This tracking correlatedwith the drones known ground truth trajectory to validateour clustering approach.Each time sequence was analyzed separately, and droneswere classified into different classes (0, 1, 2, 3) based ontheir specific characteristics and trajectories. Each dronebelonged to a distinct class based on its flight pattern andoperational role. By assigning class labels to each drone,we could track and predict their positions more accuratelyacross various sequences, enhancing our models robustnessand reliability.",
  ". Elbow Method Application": "The elbow method was crucial in determining the opti-mal number of clusters. By plotting the sum of squareddistances from points to their respective cluster centroidsagainst the number of clusters, we identified a knee in thecurve. This point represents a balance between complex-ity (number of clusters) and effectiveness (compactness ofclusters), guiding us to choose the most appropriate K valuefor subsequent experiments.To validate the effectiveness of K-Means clustering, wealso implemented the DBSCAN (Density-Based SpatialClustering of Applications with Noise) algorithm as acomparative approach. DBSCAN excels in identifying clus-ters of varying shapes and sizes, which is advantageous in",
  ". Data Sources": "The dataset provided by the CVPR 2024 UG2+ ChallengeTrack 5 includes several modalities of data as follows: Double fisheye camera visual images Livox Mid-360 (LiDAR 360) 3D point cloud data Livox Avia 3D point cloud data Millimeter-wave radar 3D point cloud dataUpon investigation, it was found that only 14 out of 59test sequences have non-zero radar values; therefore, theradar dataset is excluded from this work due to data avail-ability issues. We utilized two primary types of sensors:LiDAR 360 and Livox Avia, which provide 3D point clouddata for identifying the drones position. The detailed datadescription are outlined as follows: LiDAR 360 provides 360-degree coverage with 3D pointcloud data. This data typically includes the environmentand other observable objects, as shown in .",
  ". Implementation Details": "The program retrieves LiDAR 360 or Livox Avia data fromthe nearest timestamp for each sequence with the giventimestamp in the test dataset. Clustering is performed us-ing the DBSCAN algorithm with appropriate parameters toensure robust clustering. Visual inspection is employed forinitial point separation, ensuring accurate categorization ofenvironment points.The implementation was carried out on a Lenovo Idea-pad Slim 5 Pro (16) running Windows 11 with AMDRyzen 7 5800H CPU and 16GB DDR4 RAM. The analy-sis was conducted in a Jupyter Notebook environment us-ing Python 3.10. For clustering, we utilized the DBSCANalgorithm from the Scikit-Learn library1. The DBSCAN al-gorithm was configured with an epsilon (eps) value of 2 anda minimum number of points (minPts) set to 1.",
  ". Conclusions": "This work proposes a clustering-based learning method CL-Det using advanced clustering techniques like K-Means andDBSCAN for UAV detection and pose estimation with Li-DAR data. Our method ensures reliable and accurate esti-mation of drone positions by leveraging multi-sensor dataand robust clustering techniques. The fallback mechanismsensure continuous position estimation even in the absenceof primary sensor data. Through rigorous parameter op-timization and comparative analysis, we demonstrate thecompetitive performance of our method in drone trackingand pose estimation (ranked 5th place in the CVPR 2024UG2+ Challenge Track 5).",
  "Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu,et al. A density-based algorithm for discovering clusters inlarge spatial databases with noise. In kdd, pages 226231,1996. 2": "Nan Jiang, Kuiran Wang, Xiaoke Peng, Xuehui Yu, QiangWang, Junliang Xing, Guorong Li, Guodong Guo, QixiangYe, Jianbin Jiao, et al. Anti-uav: a large-scale benchmark forvision-based uav tracking. IEEE Transactions on Multime-dia, 25:486500, 2021. 1 Yifan Li, Dian Yuan, Meng Sun, Hongyu Wang, Xiaotao Liu,and Jing Liu. A global-local tracking framework driven byboth motion and appearance for infrared anti-uav. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 30253034, 2023. 1"
}