{
  "Abstract": "Significant progress has been made in scene text detec-tion models since the rise of deep learning, but scene textlayout analysis, which aims to group detected text instancesas paragraphs, has not kept pace. Previous works eithertreated text detection and grouping using separate mod-els, or train a model from scratch while using a unifiedone. All of them have not yet made full use of the alreadywell-trained text detectors and easily obtainable detectiondatasets. In this paper, we present Text Grouping Adapter(TGA), a module that can enable the utilization of variouspre-trained text detectors to learn layout analysis, allowingus to adopt a well-trained text detector right off the shelfor just fine-tune it efficiently. Designed to be compatiblewith various text detector architectures, TGA takes detectedtext regions and image features as universal inputs to as-semble text instance features. To capture broader contex-tual information for layout analysis, we propose to predicttext group masks from text instance features by one-to-manyassignment. Our comprehensive experiments demonstratethat, even with frozen pre-trained models, incorporating ourTGA into various pre-trained text detectors and text spotterscan achieve superior layout analysis performance, simul-taneously inheriting generalized text detection ability frompre-training. In the case of full parameter fine-tuning, wecan further improve layout analysis performance.",
  "Text Detector": ". Overview of proposed Text Grouping Adapter. The dashed boxes denote the matched group and instance. The text detector canbe frozen or fine-tuned together with TGA when training.MIi is the predicted instance mask of Ii.MGi and MGi are predicted groupmask of Ii and assigned ground-truth one of Ii. To illustrate, a same group mask is duplicated as MGp and MGq and assigned to Ip and Iq.",
  "arXiv:2405.07481v1 [cs.CV] 13 May 2024": "fied Detector considers it from the perspective of the affini-ties between low-level text instances. This enables a di-rect prediction of the affinities between text instances. Themasks of high-level text entities can be derived by connect-ing these low-level instance masks. However, this UnifiedDetector requires a query-based network structure and onlyconsiders jointly training its text detection branch with thelayout branch from scratch. This implies a lack of flexibilityin the detection network structure. More importantly, it onlybenefits from the layout analysis dataset, given the signifi-cantly smaller size of the existing layout analysis dataset,e.g., 8,281 images in HierText compared to 30,000 inthe text detection dataset IC19-LSVT , which obviouslylimits the potential of text detection.Given these limitations, we pose the question: can weenable already pre-trained text detectors with a new mod-ule to learn layout analysis? Nonetheless, answering thisquestion presents nontrivial challenges. The first challengearises from the diversity of existing text detectors in termsof architecture and text region representations. Specifically,these text detectors employ a wide array of network struc-tures, ranging from query-based transformers to fully convolutional networks and dynamicconvolutional networks . Moreover, they also modelthe text region diversely, such as semantic mask ,instance mask and parameterized curve . An-other challenge is the lack of global features in pre-trainedtext detectors since it might overly focus on local featuresfor text instance detection when pre-training.To address these challenges, we introduce Text Group-ing Adapter (TGA), a novel module that adapts diverse pre-trained text detectors to learn layout analysis. Not only doesit provide flexibility and compatibility for network struc-tures, it also empowers the model to inherit a robust textdetection capability from pre-training on large-scale textdatasets. Specifically, the TGA takes text regions and im-age features as input, and outputs affinities between regionsto represent layouts following the approach of the UnifiedDetector. The TGA comprises two key components: TextInstance Feature Assembling (TIFA) and Group Mask Pre-diction (GMP). By seamlessly converting the text regionsinto text instance masks and embedding image feature intopixel embedding map, our TIFA ensures TGA can obtaintext instance representations from the two inputs. It be-comes the cornerstone of TGAs compatibility with varioustext detection architectures. GMP is designed as an addi-tional task during training, to boost the learning of cohesivefeatures for text instances within their respective groups,which helps the instances to understand the group regionand aggregate global features necessary for layout analysis.By doing so, TGA effectively bridges the gap between pre-trained text detectors and scene text layout analysis.Our extensive experiments reveal that even when freez- ing the pre-trained models, the integration of our TGAwith various pre-trained text detectors and text spotters cansignificantly enhance performance on the layout analysisdataset. Moreover, it also allows for the inheritance of ageneralized text detection ability from pre-training.Re-markably, when full parameter fine-tuning is applied, ourTGA can further improve layout analysis performance. Var-ied ablation studies on the components of TGA are furtherconducted, demonstrating their substantial contribution tothe improvement of layout analysis performance. We be-lieve such a Text Grouping Adapter will accelerate the de-velopment and application of layout analysis via leveragingbroader pre-trained models and datasets.2. Related Works",
  ". Text Detection and Spotting": "As an important and active topic in the computer visionfield with numerous practical applications, text detectionhas been studied extensively. However, to date, these worksexhibit a high degree of variance in terms of text regionrepresentations and network structures.For text repre-sentations, a series of works modelthe text regions as semantic masks.As alternative ap-proaches, propose to leverage param-eterized curves such as Bezier curves and Fourier contoursto adaptively fit highly-curved text regions. With regardsto network structures, some works use fully convolutional networks to predict semantic masks,while others incorporate diverse approaches like query-based transformers and region-based convolu-tional networks .",
  ". Text Layout Analysis": "While text detection has been extensively developed andstudied, layout analysis remains in its nascent stages, partic-ularly concerning scene text, due to the complex and chal-lenging task of distinguishing relationships between text in-stances. Some works propose to analyze thelayout of document images, where the task is defined asto localize semantically coherent text blocks. These worksonly focus on document images and neglect word or line-level text instances. Long et al. is the first work tostudy layout analysis in the scene text field and train a uni-fied model to detect text instances and recognize their re-lationships as layout. Given the fact that layout analysisannotations are expensive and rare, training a unified scenetext layout analysis model only based on the layout analysisdataset poses limitations in learning text instance detectioncapabilities. Our work aims to harness the advancementsmade in scene text detection to enhance layout analysis.2.3. Adapter",
  "Adapters have garnered extensive utilization in the Natu-ral Language Processing (NLP) field as an efficient tool": "to enable the adaptation of pre-trained models to down-stream NLP tasks. In the field of computer vision, earlierworks apply adapters with incremental learning anddomain adaptation . Recent works focus on lever-aging adapters to transfer pre-trained vision transformersinto downstream tasks, including dense prediction andvision-language domains . Similar to the previ-ous adapter works on other domains, our TGA transfers thepre-trained text detector into the layout analysis task to in-herit the knowledge learned from pre-training.",
  ". Methods": "illustrates the pipeline of our proposed Text Group-ing Adaptor (TGA), including two key components: TextInstance Feature Assembling (TIFA) and Group Mask Pre-diction (GMP). TGA takes image features and detected textregions as inputs, producing affinities between text regions.Within the TIFA component, text instance features are as-sembled, and GMP encourages these instance features tolearn cohesive group features during training. Then, thelearned text instance features are used to predict the finalaffinity matrix. We now discuss these components in detail.",
  ". Text Instance Feature Assembling": "Text Instance Feature Assembling (TIFA) is designed toprocess diverse representations of text regions and producetext instance features as output.This requires a unifiedtext region representation and a spatially correspondent ap-proach to assemble instance features. Inspired by instancesegmentation methods , we employ a pixel embed-ding map and instance masks to assemble text instance fea-tures. We sequentially introduce this module as follows.Unified text region representation.Various text detec-tors generate different representations of text regions, e.g.,semantic masks, instance mask, and kinds of parameter-ized curves. For the compatibility of TGA and instance-level understanding, we select the text instance mask asour representation, which can be converted seamlessly fromother representations. Specifically, for semantic masks, weemploy binarization on each mask and utilize the algo-rithm to identify text instances within them. In the caseof parameterized curves, we transform each curve instanceinto a close polygon, then draw and fill the polygon to createthe corresponding instance mask. We define I = {Ii}Ni=1as the extracted text instance set, including N instances.Empty masks are padded when less than N instances arefound in the output of text detectors. By this process, weunify disparate text region representations as text instancemasks MI for later feature assembling. Pixel embedding map.Grouping text instances necessi-tates both low-level features to discriminate small-size textsand high-level features to provide sufficient contextual in-formation.For this, we derive a pixel embedding map",
  "} of the input image size. Through a seriesof convolutional layers and upsampling, each Xi is trans-formed into a corresponding Pi, all standardized to 1": "8 of theinput image size. The final pixel embedding map P is ob-tained by summing up {Pl|i = 1, ..., n} and then feedingthe result to another convolution to refine the pixel features.With a simple fully convolutional network, we embed themulti-scale features into the pixel embedding map, ensur-ing the retention of distinct text features while capturing thenecessary contextual information for layout analysis. Werefer this simple fully convolutional network as Pixel Em-bedding Layers, shown in the . Feature assembling.As the converted instance mask pre-diction MI determines if a pixel of the image belongs to thetext instance, we can assemble the instance feature by inte-grating the pixel embedding map P and the text instancemask MI. This is accomplished through a multiplicationoperation between the two, which is equivalent to extract-ing the D-dimensional feature for each point correspondingto the pixel embedding map and sum pooling spatially.",
  ". Group Mask Prediction": "The text instance features assembled from individual in-stance masks in TIFA exclusively focus on their respectiveinstance regions, overlooking the contextual information.Hence, we introduce Group Mask Prediction (GMP) sub-sequent to TIFA. GMP encourages text instances to learncohesive features of their corresponding group, thereby re-alizing the implicit clustering, which is crucial for the accu-rate prediction of the final affinity matrix.To facilitate interaction among text instances and extractcontextual information in group mask prediction, we firstincorporate self-attention layers into the process. We fedtext instance features into the self-attention layers: FI =SA(FI, FI), where FI RND denotes the updated textinstance features and SA() denotes self-attention layers.Then, we set the group mask with a one-to-many assign-ment as the learning target. Specifically, for each text groupentity, e.g., text paragraph, we assign the groups mask to allinstances that belong to that group using Hungarian match-ing. To achieve a more accurate assignment, we avoid directmatching of the detected instance masks with the ground-truth group masks. Instead, we first match the detected in-stance masks with the ground-truth instance masks. Sub-sequently, we transform the matched ground-truth instancemasks into the corresponding ground-truth group masks ac-",
  "iLmatch(MIi , MI(i)),(2)": "where we match for a permutation of N text instances SN with the lowest cost. Under certain , MIi is theground-truth text instance mask, while MI(i) is its matchedpredicted value.Lmatch is the pair-wise matching cost,which is designed to be consistent with the original loss oftext detectors, detailed in our supplementary material.We then obtain the predicted group mask MG via a dot-product between the updated text instance features and thepixel embedding map:",
  ",(4)": "where the predicted instances without matched group-truthones are excluded from the loss calculation.This approach transforms DETR-style one-to-oneassignment to a one-to-many assignment by replacingmatched instance masks with corresponding group masks,resulting in the same group mask being assigned to all textinstances belonging to the group. It also differs from theone-to-many assignment concept referred to in other DETRvariants , which assign single instance supervisioninto many instance predictions for faster convergence ratherthan implicitly clustering instances for group entities.",
  "Following Unified Detector, we predict an affinity matrixbetween text instances to represent the layout. By multiply-ing the updated text instance features with their transpose,": "we can obtain the predicted affinity matrix A = FI (FI),where the element Ai,j denotes the predicted affinity scorebetween Ii and Ij, i.e., the possibility of the instance pairbelonging to the same group. We utilize the previously ob-tained instance matching result and annotations to con-struct ground truth binary affinity matrix A RNN andthe binary instance loss weight C RNN. The elementAi,j {0, 1} is set to 1 if the pair, Ii and Ij, belongs to thesame text group and 0 otherwise. Ci,j is used to excludethe instances without matched ground truth under .",
  ". Cascade TGA for Word-based Text Detector": "Different from directly grouping detected lines into para-graphs, grouping detected words into paragraphs is actuallya hierarchical grouping task. Its more challenging as thewords in a paragraph might exhibit greater diversity andbe more spatially distant from each other. Thanks to thecompatibility of the proposed TGA, we can simply cas-cade more than one TGA to address this problem, referredas Cascade TGA. As shown in , we cascade twoTGAs, named Word TGA and Line TGA. Detected textword regions and image features are first fed into WordTGA to predict line masks. The predicted line masks aresubsequently fed into Line TGA to predict paragraph masksand the affinity matrix. This design complements the mid-level supervision and showcases the compatibility of TGA.",
  ". Experiments": "In this section, we set up comprehensive experiments andanalysis on TGA. We incorporate TGA into diverse pre-trained text detectors and compare their performance witha series of competitive baseline methods. To explore thepossibility of parameter-effectively adapting the text detec-tors, we also compare the TGAs performance on diversedatasets under the condition that the parameters of the textdetectors are either frozen or not. Finally, we validate theeffectiveness of TGAs components by extensive ablationstudies and visualizations.",
  ". Experiment Settings": "Baselines.Since the layout analysis can build on word in-stances and text line instances, we compare methods sepa-rately on word-based layout analysis and line-based layoutanalysis. Since there is little literature specially studyingword-based layout analysis, we solely draw on the UnifiedDetector as our word-based baseline. As for line-based methods, we adapt baselines from , including GoogleCloud OCR API , Max-DeepLab-Cluster andthe Unified Detector .Google OCR (GCP) API isa public commercial OCR engine. Max-DeepLab-Clusterbuilds two separate Max-DeepLab models to detect textlines and paragraphs and reassign the affinities between in-stances by post-processing. The Unified Detector stands outas the first unified model to detect text instances and layoutsimultaneously, achieving the previous state-of-the-art per-formance. Note that in the following comparison, we referto the Unified Detector with 384 queries for its best perfor-mance. Pre-trained text detectors.We applied TGA to the fol-lowing four models pre-trained for text detection withdiverse network structures and text region representa-tions:DBNetpp , Deepsolo , KNet , andMaskDINO .In terms of the network architecture,DBNetpp utilizes a fully convolutional network, whileMaskDINO and DeepSolo employ query-based transform-ers. KNet introduces dynamic kernels with the convolu-tional network. Regarding the text region representations,DBNetpp produces semantic masks. KNet and MaskDINOmodel the text region as text instance masks. DeepSolostands out as the state-of-the-art text spotting model, out-putting Bezier Curve control points. Besides the differencesresulting from their methods, in our experiments, DBNetppis pre-trained as word-level text detectors, while DeepSoloserves as the word-level text spotting model.KNet andMaskDINO are pre-trained as line-level text detectors. Datasets and metrics.HierText is the scene text lay-out analysis dataset for our training and evaluation, which iswell annotated with not only paragraph masks but also textline and word masks. It consists of 8,281, 1,724, and 1,634images in training, validation, and test set, respectively. Fol-lowing , we use Precision (P), Recall (R), F1 score (F),and Panoptic Quality (PQ) as our metrics for layout anal-ysis and text detection on the HierText Dataset, where PQis computed as the product of the F1 score and the averageIntersection over Union of all True Positive pairs.For text detection, CTW1500 , MSRA-TD500 ,IC19-LSVT and HierText, totally 39,581 images, arethe datasets used for the pre-training of line-level text de-tector. As for word-level text detection, we proceed withthe continual pre-training of the DBNetpp model and di-rectly draw the off-the-shelf parameters from the DeepSolomodel. We use Average Precision (AP) and the HarmonicMean (H-mean) as the text detection metrics. We reportmore details of datasets, metrics and pre-training in our sup-plementary material.",
  "Line-based": "GCP API-------56.17-46.33Max-DeepLab-Cluster-------62.23-52.52Unified Detector88.2M / 88.2M78.4461.0467.7652.8479.9162.2368.5853.60TGA + KNet-R5043.4M / 43.4M77.0459.0871.9055.2777.8860.0371.3755.04TGA + MaskDINO-R5052.2M / 52.2M76.4158.9175.4658.2877.4359.8975.4158.33TGA + KNet-R505.9M / 43.4M76.6658.6871.3154.5877.3759.5470.3454.05TGA + MaskDINO-R505.9M / 52.2M84.1064.5174.6757.6284.9265.3574.2257.47TGA + KNet-Swin-B5.9M / 106.2M79.1060.6473.5856.4179.6761.4072.4955.90TGA + MaskDINO-Swin-B7.1M / 123.0M85.8466.5075.4558.7286.6567.2775.1158.65TGA + MaskDINO-R5012.0M / 58.2M85.0065.6078.0060.2886.1866.6377.6160.05 . Results of word and line detection on the HierText Dataset for all models. Word-based refers to detects and groups word regionsas paragraphs while Line-based does the same for text line regions. The fine-tune strategy and trainable parameter size is inapplicable forAPI or non-unified model. Deepsolo-ViTAE-S means the Deepsolo with ViTAE-S as its backbone. KNet-R50 means the KNet withResNet-50 as its backbone. Swin-B denotes Swin-Base . Other models naming follows the same approach. TGA means anenhanced TGA by adding more layers in Pixel Embedding Layers for more trainable parameters while other settings keep the same. 3, the number of attention heads is 4, and the dimension ofthe hidden layers is 512. During the inference phase, weset the group threshold t as 0.8 for all these models. As fortraining hyperparameters, we mainly refer to the originaltext detector training hyperparameters, which are providedin our supplementary material.",
  ". Main Comparison": "In this main comparison, we emphasize Paragraph F and PQas our main metrics since they represent the performance oflayout analysis. Instance F and PQ also are considered asbetter instance detection helps layout analysis. As shown in, we incorporate TGA into various pre-trained textdetectors, achieving impressive results on both word-basedand line-based layout analysis. On word-based compari-son with the Unified Detector, with much smaller trainableparameter size and total parameter size (around 10% train-able parameters and one-half total parameters), both frozenDBNetpp and DeepSolo combined with TGA achieve on-par, even superior performance on Paragraph PQ. Its alsonoteworthy that, due to the limitation of the text detectorsparameter size, our layout analysis results are producedgiven this worse instance-level performance, which furtherdemonstrates our advantages in grouping semantic entities.On line-based layout analysis, while applying TGA intodifferent text detectors, we also involve the settings ofwhether or not to freeze the text detector. As shown in thebottom section of , under all possible configurations,all our models consistently outperform all previous mod- els on the Paragraph PQ of the test set to varying degrees.Among them, even the lightest model, TGA with frozenKNet-R50 outperforms Unified Detector on Paragraph PQby 0.8% relatively. For the most powerful model, the TGA,adding more trainable parameters in Pixel Embedding Lay-ers to get the enhanced pixel embedding map, achieves60.05 PQ at the paragraph level with frozen MaskDINO-R-50. It is a 12.0% relative improvement compared to the Uni-fied Detector. For our models with ResNet-50, we noticethe fully fine-tuned models gain slightly better performancethan the models with frozen text detectors on paragraph-level metrics, which is further studied in the following sec-tion. We further show that TGA is robust to different anno-tation levels under the same model, detailed in supplemen-tary material. In summary, TGA is seamlessly compatiblewith various pre-trained text detectors and achieves supe-rior layout analysis performance, even given the limitationof parameter size and worse detected text instances.",
  ". Comparison of Fine-Tuning Strategies": "We study the effect of different fine-tuning strategies whenusing TGA. We fine-tune the same pre-trained text detec-tors on the HierText Dataset for layout analysis and eval-uate them on both text detection datasets and layout anal-ysis datasets. We adopt two different fine-tune strategies:(1) only fine-tune TGAs parameters and freeze the orig-inal text detector, referred to as frozen text detector. (2)fine-tune all parameters, including original text detector andTGA, referred to as full fine-tuning. Results are shown in",
  "TGA + MaskDINO-Swin-B123.0MOut of Memory7.1M27.3h61.2559.5882.6366.3758.56": ". Comparison between different fine-tuning strategies: frozen text detector and full fine-tuning. Out of Memory denotes this erroroccurs when fully fine-tuning the TGA + MaskDINO-SWin-B on a 8 * V100-32G workstation with minimal batch size. The values onHierText Val set slightly differ from ones reported in for slightly different annotations, detailed in supplementary material.",
  ". Ablation studies of category for GMP loss function": ". We can obverse full fine-tuning strategy consis-tently enhances the layout analysis performance of all mod-els. The most notable improvement is seen in MaskDINO-R50, where the full fine-tuning strategy surpasses the frozentext detector strategy by a significant 4.2 on Paragraph PQ.On the other hand, the benefit of frozen text detector isalso obvious in that it significantly saves the training timeand GPU memory for the less trainable parameters, espe-cially for the ones that have heavier backbones. Frozen de-tector detector strategy saves 34.7% training time for KNet-Swin-B and prevents out-of-memory error for MaskDINO-Swin-B fine-tuning. Another advantage of frozen text de-tector is that it can keep the generalized detection abilityobtained from pre-training and prevent overfit on the lay-out analysis dataset, especially given the fact that the cur-rent size of the layout analysis dataset is much smallerthan broad text detection datasets. As shown in the mid-dle columns of , on CTW1500, MSRA-TD500 andLSVT detection dataset, the full fine-tuning models perfor-mances rapidly drop on text detection metrics. Represen-tatively, full fine-tuning KNet-R50 relatively drops 23.5%, 17.1%, and 8.1%, respectively on the three text detectiondatasets compared with frozen one. It shows the trade-offchoice of full fine-tuning or freezing text detector, wherethe former helps boost better layout analysis performanceon the specific dataset, and the latter saves computing re-sources and benefits robustness on broader scenarios.4.4. Ablation Study To validate the efficacy of the key components in our pro-posed TGA, we conducted a series of experiments where werespectively replace the Pixel embedding map with a singlescale image feature and removed the Group Mask Predic-tion feature. These experiments were performed under twodifferent fine-tuning strategies. We compared single TGAwith Cascade TGA in the training process and analyze thesecomponents sequentially in the following paragraphs. Pixel embedding map.Pixel embedding map is designedto encode high-level features for layout contextual informa-tion while preserving low-level features for text instance de-tails. As shown in , the removal of the Pixel embed-ding map, regardless of whether the text detector is frozenor fully fine-tuned, negatively impacts the performance oflayout analysis. This suggests that our proposed compo-nents are generally effective across different fine-tuningstrategies. The results also indicate that components playdifferent roles under different fine-tuning strategies. Theremoval of the pixel embedding map leads to a 3.03 dropin the Paragraph PQ under a frozen text detector, as com-pared to a 0.48 drop under full fine-tuning. It indicates dis-tinct scale features are needed between text detection pre-training and layout analysis fine-tuning. Hence, the removalof the pixel embedding map under a frozen text detectorresults in the loss of global contextual information, whichsignificantly impairs the layout analysis performance.Group Mask Prediction.Group Mask Prediction (GMP)is a novel component in our Text Grouping Adapter, whichleverages a one-to-many assignment to encourage text in-stances belonging to the same group to predict the samegroup mask. This process enables text instances to learn",
  "Ground TruthUnified Detector(Line)Ours(Word)Ours(Line)": ". Visualization of results on the validation set of the Hier-Text Dataset: from left to right, the sequence includes the groundtruth, line-based Unified Detector, TGA + MaskDINO-Swin-Band TGA + DeepSolo-ViTAE-S. (Zoom in for the best view) group-level features that encompass more contextual infor-mation.As shown in , models with GMP con-sistently outperform those without it.Particularly underthe full fine-tuning setting, solely add Group Mask Predic-tion gains 4.56 on Paragraph PQ. When fine-tuned with thefrozen text detector, the improvement is slightly less pro-nounced due to the limitation of detection-biased features.With the assistance of the pixel embedding map, we still ob-serve a substantial improvement of 5.18 on Paragraph PQ. Group mask v.s. affinity matrix.The question arises asto why GMP helps the prediction of the affinity matrix givenboth group masks and affinity are derived from group an-notations? To answer this question, we initially ablate thesupervision used in GMP. As shown in the last row of Ta-ble 3, replacing the one-to-many assigned group mask withthe one-to-one assigned line mask in the prediction causesthe drop back to baseline performance levels. This confirmsthe advantages of GMP from the unique group masks andthe one-to-many assignment, not merely from simple maskprediction. Unlike the affinity matrix, which depicts layoutthrough pairwise relationships, the group mask representslayout through the collective representation of all instanceswithin the group, thereby optimizing the inter-instance dis- tances globally. Our investigation into mask prediction losscombinations for GMP, shown in , demonstrates thatrelying solely on binary cross-entropy loss, computed pixel-wise, leads to a drastic decline in layout analysis perfor-mance. Conversely, employing dice loss, which evaluatesthe holistic statistical resemblance between predicted groupmasks and their true counterparts, significantly elevates lay-out analysis outcomes. When using dice loss, which focuson the holistic statistic similarity between predicted groupmasks and ground-truth ones, it greatly boosts the perfor-mance of layout analysis. These findings validate the GMPdesigns capacity to capture a more global and holistic rep-resentation of group instance information. We further visu-alize the clustering effect of GMP and provide more abla-tions in supplementary material. Cascade TGA.In , we evaluate DBNetpp withsingle TGA and Cascade TGA, respectively, at varioustraining stages with the frozen text detector. The resultsindicate that introducing Cascade TGA not only acceleratesconvergence, but also produces superior results comparedto single TGA. This structural prior introduced by CascadeTGA reduces the problem of clustering words into para-graphs to a two-stage problem: first clustering words intolines and then clustering lines into paragraphs.",
  ". Qualitative Results": "We compare visualizations of generated layouts betweenUnified Detector, our line-based MaskDINO with TGA andour word-based DeepSolo with TGA in . We ob-serve that our line-based model performs better in the de-tails, with more complete text masks and accurate groupingresults. It is also noteworthy that our DeepSolo with TGAsimultaneously produces the result of text detection, recog-nition, and layout analysis as a unified model. Facing morechallenging in word-based layout analysis, it shows slightdefects like losing the capture of small-size texts.",
  ". Conclusion": "We present Text Grouping Adapter (TGA), a versatile mod-ule that enhances the capability of various pre-trained textdetectors to serve for layout analysis. TGA takes text masksand image features as inputs to predict text group masksfrom text instance features. It facilitates the full exploita-tion of well-trained text detectors and easily obtainable textdetection data. This work provides insights and a practicalsolution for aligning layout analysis with text detection andalso has the potential to model general object relations.",
  "Galal M Binmakhashen and Sabri A Mahmoud. Documentlayout analysis: a comprehensive survey. ACM ComputingSurveys (CSUR), 52(6):136, 2019. 1": "Alessandro Bissacco, Mark Cummins, Yuval Netzer, andHartmut Neven. Photoocr: Reading text in uncontrolled con-ditions. In Proceedings of the ieee international conferenceon computer vision, pages 785792, 2013. 1 Theodore Bluche and Ronaldo Messina.Gated convolu-tional recurrent neural networks for multilingual handwrit-ing recognition. In 2017 14th IAPR international conferenceon document analysis and recognition (ICDAR), pages 646651. IEEE, 2017. 1 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European confer-ence on computer vision, pages 213229. Springer, 2020. 4 Qiang Chen, Xiaokang Chen, Jian Wang, Shan Zhang, KunYao, Haocheng Feng, Junyu Han, Errui Ding, Gang Zeng,and Jingdong Wang.Group detr: Fast detr training withgroup-wise one-to-many assignment. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 66336642, 2023. 4 Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, TongLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter fordense predictions. In The Eleventh International Conferenceon Learning Representations, ICLR 2023, Kigali, Rwanda,May 1-5, 2023. OpenReview.net, 2023. 3",
  "Daniel Hernandez Diaz, Siyang Qin, Reeve Ingle, YasuhisaFujii, and Alessandro Bissacco. Rethinking text line recog-nition models. arXiv preprint arXiv:2104.07787, 2021. 1": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, ShijieGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-angyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-sual instruction model.arXiv preprint arXiv:2304.15010,2023. 3 Google. Google cloud platform text detection api, 2023. 5 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 6 Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu,Weihong Lin, Lei Sun, Chao Zhang, and Han Hu.Detrswith hybrid matching. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1970219712, 2023. 4 Joonho Lee, Hideaki Hayashi, Wataru Ohyama, and SeiichiUchida. Page segmentation using a convolutional neural net-work with trainable co-occurrence features. In 2019 Inter-national Conference on Document Analysis and Recognition(ICDAR), pages 10231028. IEEE, 2019. 1, 2",
  "and segmentation. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages30413050, 2023. 5": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.Hoi.BLIP-2: bootstrapping language-image pre-trainingwith frozen image encoders and large language models. InInternational Conference on Machine Learning, ICML 2023,23-29 July 2023, Honolulu, Hawaii, USA, pages 1973019742. PMLR, 2023. 3 Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, YijuanLu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei.Trocr: Transformer-based optical character recognition withpre-trained models. In Proceedings of the AAAI Conferenceon Artificial Intelligence, pages 1309413102, 2023. 1 Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and XiangBai. Real-time scene text detection with differentiable bina-rization. In Proceedings of the AAAI conference on artificialintelligence, pages 1147411481, 2020. 1, 2, 5 Minghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, andXiang Bai. Real-time scene text detection with differentiablebinarization and adaptive scale fusion. IEEE Transactions onPattern Analysis and Machine Intelligence, 45(1):919931,2022. 1, 2 Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, LianwenJin, and Liangwei Wang.ABCNet: Real-time scene textspotting with adaptive Bezier-curve network. In Proc. IEEEConf. Computer Vision and Pattern Recognition (CVPR),2020. 1, 2 Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, PengChen, Chongyu Liu, and Hao Chen. Abcnet v2: Adaptivebezier-curve network for real-time end-to-end text spotting.IEEE Transactions on Pattern Analysis and Machine Intelli-gence, 44(11):80488064, 2021. 1 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 6 Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He,Wenhao Wu, and Cong Yao. Textsnake: A flexible represen-tation for detecting text of arbitrary shapes. In Proceedings ofthe European conference on computer vision (ECCV), pages2036, 2018. 1, 2 Shangbang Long, Siyang Qin, Dmitry Panteleev, AlessandroBissacco, Yasuhisa Fujii, and Michalis Raptis. Towards end-to-end unified scene text detection and layout analysis. InCVPR 2023, pages 10491059, 2022. 1, 2, 5 Siyang Qin, Alessandro Bissacco, Michalis Raptis, YasuhisaFujii, and Ying Xiao. Towards unconstrained end-to-end textspotting. In Proceedings of the IEEE/CVF international con-ference on computer vision, pages 47044714, 2019. 2 Zobeir Raisi, Mohamed A Naiel, Georges Younes, StevenWardell, and John S Zelek. Transformer-based text detectionin the wild. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 31623171,2021. 2",
  "Amir Rosenfeld and John K Tsotsos. Incremental learningthrough deep adaptation. IEEE transactions on pattern anal-ysis and machine intelligence, 42(3):651663, 2018. 3": "Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Den-gel, and Sheraz Ahmed. Deepdesrt: Deep learning for de-tection and structure recognition of tables in document im-ages. In 2017 14th IAPR international conference on docu-ment analysis and recognition (ICDAR), pages 11621167.IEEE, 2017. 1, 2 Baoguang Shi, Xiang Bai, and Serge Belongie. Detectingoriented text in natural images by linking segments. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 25502558, 2017. 1 Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Can-jie Luo, Chun Chet Ng, Junyu Han, Errui Ding, JingtuoLiu, Dimosthenis Karatzas, et al. Icdar 2019 competitionon large-scale street view text with partial labeling-rrc-lsvt.In 2019 International Conference on Document Analysis andRecognition (ICDAR), pages 15571562. IEEE, 2019. 2, 5",
  "Satoshi Suzuki et al. Topological structural analysis of dig-itized binary images by border following. Computer vision,graphics, and image processing, 30(1):3246, 1985. 3": "Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, andLiang-Chieh Chen.Max-deeplab:End-to-end panopticsegmentation with mask transformers.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 54635474, 2021. 5 Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo, Xi-aoxue Chen, Yaqiang Wu, Qianying Wang, and Mingxi-ang Cai. Decoupled attention network for text recognition.In Proceedings of the AAAI conference on artificial intelli-gence, pages 1221612224, 2020. 1 Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu,Gang Yu, and Shuai Shao. Shape robust text detection withprogressive scale expansion network.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 93369345, 2019. 2",
  "Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao.Vitae: Vision transformer advanced by exploring intrinsicinductive bias. Advances in neural information processingsystems, 34:2852228535, 2021. 6": "Cong Yao, Xiang Bai, Wenyu Liu, Yi Ma, and Zhuowen Tu.Detecting texts of arbitrary orientations in natural images.In 2012 IEEE conference on computer vision and patternrecognition, pages 10831090. IEEE, 2012. 5 Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu,Tongliang Liu, Bo Du, and Dacheng Tao. Deepsolo: Lettransformer decoder with explicit points solo for text spot-ting. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1934819357,2023. 1, 2, 5"
}