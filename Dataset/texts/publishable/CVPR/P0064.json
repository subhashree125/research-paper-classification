{
  "Abstract": "This paper introduces a method for VizWiz-VQA usingLVLM with trainable cross-attention and LoRA finetuning.We train the model with the following conditions: 1) Train-ing with original images. 2) Training with enhanced imagesusing CLIPSeg to highlight or contrast the original image.3) Training with integrating the output features of VisionTransformer (ViT) and CLIPSeg features of the original im-ages. Then, we ensemble the results based on Levenshteindistance to enhance the prediction of the final answer. Inthe experiments, we demonstrate and analyze the proposedmethods effectiveness.",
  ". Introduction": "Visual Question Answering (VQA) tasks integrate vi-sion and language, requiring a model to understand and an-swer questions about a given image. The conventional ap-proaches in VQA have relied on direct mappings betweenvisual content and question-answer pairs, often skipping thenuanced interplay between image-specific details and thecontext of the questions . Enhancing the effectivenessof VQA models necessitates attention to global image fea-tures and localized, relevant visual cues that directly pertainto the questions asked. The VizWiz-VQA focus on theparts of the image that correspond to the questions intentis crucial. In this paper, we introduce a parameter-efficientmethod using the Low-rank Adaptation(LoRA) and a cross-attention mechanism between text and visual input based ona large vision language model (LVLM). In addition, we in-tegrate an image segmentation technique, called CLIPSeg to improve visual understanding. We show results usingvalidation/test-dev sets to demonstrate the effectiveness ofour method.",
  ". Method": "Trainable cross-attention and LoRA tuning. We use theQwen-VL model, which consists of ViT and QwenLM,as a backbone and set the cross-attention module to a learn-able state to effectively learn specific parts of the imageassociated with the keywords in the question. The LoRAadapter , which has high parameter efficiency, is appliedto fine-tune the QwenLM. In this configuration, only thecross-attention and LoRA application parts are set as train-able parameters, and the rest of the model remains withfixed parameters.Image segmentation for enhancement. We use a query-aware segmentation feature, using the CLIPSeg model ,which identifies and separates specific objects within an im-age based on text queries. CLIPSeg enhances image pro-cessing by dynamically isolating related objects and inte-",
  ". Experimental Results on test-dev set of VizWiz-VQA": "grating them into the model. We enhance this with seg-mentation techniques highlighting specific image areas andcontrast methods differentiating between masked and un-masked regions, providing more apparent visual distinc-tions.Ensemble using Levenshtein distance. In our proposedmethod, we employ an ensemble of various predictive mod-els trained to enhance accuracy. For the final answer gen-eration, we utilize a method based on the Levenshtein dis-tance to select the most accurate prediction. This approachleverages the edit distance metric to compare and evaluatethe outputs of different models, thereby optimizing the se-lection of the final answer.",
  ". Experiments": "Experimental results. Our experiment is conditioned un-der three conditions, as shown in . (1) Input theoriginal image into Qwen-VLs ViT(FT-L/CA). (2) Inputthe original image with two variants segmentation (high-light, contrast). The segment-highlight adds the segmenta-tion output to the original image, and segment-contrast ad-justs the brightness of the original image other than segmen-tation output. (3) Integrate ViT features and CLIPSeg fea-tures in the model. We show the results of applying segmen-tation as examples in the appendix A. In our experiments,we perform experiments on zero-shot and fine-tuning. Inthe zero-shot setting, we confirmed without instructions,performance deteriorates significantly, thus we add and use instructions. As shown in and , integrat-ing feature information directly in the model proves moreeffective than applying CLIPSeg output to the input imageand transforming it, and particularly, the ensemble modelyields the highest performance. The query-aware CLIPSegactively improves correct answer rates across multiple cate-gories.",
  "This work was supported by the National Research Foun-dation of Korea (NRF) grant funded by the KoreanGovernment (MSIT) (No.2021R1C1C1012590), (No.2022R1A4A1023248), and (No. 2022R1A5A7026673)": "Instructions: The correct answer type is one of [number,words, yes, no]. If it is impossible to answer an image-related ques-tion or there is no existing information, please reply as unanswerable.Question: Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.Qwen-vl: A frontier large vision-language model with ver-satile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, ChiLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.Vizwiz grand challenge: Answering visual questions fromblind people. In Proceedings of the IEEE conference on com-puter vision and pattern recognition, pages 36083617, 2018.1 Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA:Low-rank adaptation of large language models. In Interna-tional Conference on Learning Representations, 2022. 1 Siyu Lu, Yueming Ding, Mingzhe Liu, Zhengtong Yin, LirongYin, and Wenfeng Zheng. Multiscale feature extraction andfusion of image and text in vqa. International Journal of Com-putational Intelligence Systems, 16(1):54, 2023. 1"
}