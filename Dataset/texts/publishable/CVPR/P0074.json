{
  "Preserve input subject": ". We present Joint-Image Diffusion (JeDi), a finetuning-free image personalization model that can operate on any number ofreference images. JeDi is able to preserve the appearance of custom subjects while generating novel variations. As shown in the top row,JeDi does not suffer from the issues of overfitting and lack of diversity exhibited by the prior models. The examples in the bottom tworows demonstrate JeDis high-quality results on challenging personalization tasks.",
  "Abstract": "Personalized text-to-image generation models enableusers to create images that depict their individual posses-sions in diverse scenes, finding applications in various do-mains.To achieve the personalization capability, exist-ing methods rely on finetuning a text-to-image foundationmodel on a users custom dataset, which can be non-trivialfor general users, resource-intensive, and time-consuming.Despite attempts to develop finetuning-free methods, theirgeneration quality is much lower compared to their fine-tuning counterparts. In this paper, we propose Joint-Image Diffusion (JeDi), an effective technique for learning afinetuning-free personalization model. Our key idea is tolearn the joint distribution of multiple related text-imagepairs that share a common subject.To facilitate learn-ing, we propose a scalable synthetic dataset generationtechnique. Once trained, our model enables fast and easypersonalization at test time by simply using reference im-ages as input during the sampling process. Our approachdoes not require any expensive optimization process or ad-ditional modules and can faithfully preserve the identityrepresented by any number of reference images. Experi-mental results show that our model achieves state-of-the-",
  ". Introduction": "The state-of-the-art in text-to-image generation has ad-vanced significantly in the last two years, propelled bythe emergence of large-scale diffusion models and pairedimage-text datasets . Despite theirsuperior capability in generating high-quality images well-aligned to the input text prompts, existing models cannotgenerate novel images depicting specific custom objects orstyles that are only available as few reference images ex-ternal to the training datasets. To address this important usecase, various personalization methods have been developed.The key challenge of personalized image generation isto produce distinct variations of a custom subject while pre-serving its visual appearance.Most existing approachesachieve this goal by finetuning a pre-trained model on a ref-erence set of images to make it memorize the custom con-cept. Although these methods can yield good synthesis re-sults, they require substantial resources and a long trainingtime to fit the custom subject, and more than one referenceimage is needed to avoid overfitting. To overcome thesechallenges, there has been recent interest in developingfinetuning-free personalization methods .These methods typically encode the reference images into acompact feature space, and condition the diffusion model onthe encoded features. However, the encoding step results ininformation loss, leading to poor appearance preservation,especially for challenging unusual objects as seen in .Therefore, the performance of encoder-based personaliza-tion techniques is inferior to finetuning-based approaches.In this paper, we present JeDi, a novel approach forfinetuning-free personalized text-to-image generation thatexcels at preserving input reference content. Our core ideais to train a diffusion model to learn a joint distribution ofmultiple related text-image pairs that share a common sub-ject. As illustrated in , this goal is achieved usingtwo key ingredients: First, we construct a synthetic datasetof related images in which each sample contains a set oftext-image pairs that share a common subject. We presenta scalable approach for creating such a dataset using LLMsand pre-trained single-image diffusion models. Second, wemodify the architecture of existing text-to-image diffusionmodels to encode relationships between multiple images ina sample set. Specifically, we adapt the self-attention layersof the diffusion U-Net so that the attention blocks corre-sponding to different input images are coupled. That is, theself-attention layer corresponding to each image co-attendsto every other image in the sample set. The use of the cou-",
  "pled self-attentions at different levels of hierarchy in theU-Net provides a much stronger representation needed forgood input preservation": "At test time, JeDi can take multiple text prompts as in-put and generate images of the same subject in differentcontexts. By simply substituting reference images as ob-served variables in the sampling process, JeDi can gener-ate personalized images based on any number of referenceimages. We utilize guidance techniques on referenceimages to further improve the image alignment. JeDi canachieve high-fidelity personalization results even in chal-lenging cases involving unique subjects (, 7), usingas few as a single reference image.",
  ". Text-to-Image Generation": "Denoising diffusion models formulate the im-age generation task as a series of progressive denoisingsteps. The denoising network can be trained conditionedon text embeddings to generate images from an input cap-tion. DALL-E2 achieves high-resolution text-to-imagesynthesis using two diffusion models: the first model trans-forms a CLIP text embedding to a CLIP image embedding,while the second model transforms the image embeddingto an output image. Imagen trains a cascaded diffu-sion model conditioned on T5 language embeddings .eDiff-I uses an ensemble of expert denoisers to increasethe model capacity, with each expert specializing in a spe-cific noise range. Latent diffusion models trainthe diffusion model in a compact latent space of an autoen-coder for efficient training and sampling.",
  "Prompt 1Prompt 2": ". Overall framework. (a) We generate training data by using large language models and prompting pretrained single-imagediffusion models. (b) During training, the JeDi model learns to denoise multiple same-subject images together, where each image attendsto every image of the same subject set through coupled self-attention. (c) At inference, personalized generation is performed in an inpaintingfashion where the goal is to generate the missing images of the joint-image set.",
  ". Personalized Text-to-Image Generation": "Personalization as inpainting. While the joint-image dif-fusion model discussed in the previous section can generatesame-subject images, it does not input a reference imagethat needs to be personalized. In this section, we proposeto solve the input image personalization problem by cast-ing it as an inpainting task. That is, given a few text-imagepairs as reference, the task of generating a new personalizedsample can be viewed as inpainting the missing images of ajoint-image set containing reference images ( (c)).We design the inpainting model by modifying the inputlayer of the diffusion U-Net so that it can be conditioned onreference images. More specifically, the input to the diffu-sion model is a concatenated list of noisy images, referenceimages and a binary mask indicating whether the referenceimage is used or not. When the binary mask is all 0s, the reference image is used. On the other hand, when the bi-nary mask is all 1s, the reference image is an empty blackimage (indicating the missing images that need to be gen-erated). During training, for every image in the joint-imageset, we use the reference image with a probability of 0.5i.e., we assign the binary mask to 0 with probability 0.5.The training training loss can then be written as follows, L = EN (0,1),t[1,T ],m(N0.5) (xt, x, M)22,(2)where M is the spatially repeated tensor of a binomial vari-able m; x = xM denotes the reference images, with theunknown elements set to zero.During inference, we utilize the replacement trick [20, 32] in which the known part of the joint-image set is re-placed with the forward diffusion response of the clean ref-erence image. Let x = [x1, x2, ..., xn] be the reference im-ages, which are the known elements in the image set x =[x1, ..., xn, xn+1, ..., xN] to be generated. During sampling,at each diffusion step t, we only keep the backward diffu-sion output for the unknown elements xn+1, ..., xN whilereplacing the known part with the forward diffusion out-put, i.e. the noised real images x1t, ..., xnt .Image guidance.Classifier-free guidance is a populartechnique used in single-image diffusion models to makethe image generations more aligned to the input condition-ing . To improve the faithfulness of the generated sam-ples to the input reference images, we use image guidancein addition to the text guidance during sampling. The scorefunction with the use of image guidance is as follows:",
  ". Dataset Creation": "Training a model to produce a joint distribution of multi-ple same-subject images requires a dataset where each sam-ple is a set of images sharing a common subject. Whilethere exist some same-subject datasets such as CustomCon-cept101 and DreamBooth , they are small in scaleand lack sufficient variations desired for diffusion modeltraining. Therefore, we create a diverse large-scale datasetof image-text pairs containing the same subject, called theSynthetic Same-Subject (S3) dataset, using large languagemodels and single-image diffusion models . illustrates our data generation process. We firststart with a list of common objects and prompt ChatGPTto generate a text description for each object in the list.Then, we use the pre-trained SDXL model to generatea dataset of same-subject photo collages by appending thetext photos of the same to each of the text prompt gen-erated in the previous step. We observed that by promptingthe SDXL model this way, it can generate photo collages ofthe same subject with varying poses. However, the gener-ated images usually contain a close-up view of an object ina simple background. To increase the data diversity, we em-ploy a post-processing step that performs background aug-mentation on the generated objects.",
  ". Samples from the synthetic same-subject (S3) dataset.Each column denotes different images from one joint-data sample": "Given a generated photo collage, we first run object de-tection and segmentation to separate out objectinstances and extract foreground region. We discard pairsof instances with CLIP image scores lower than 0.95as they may not contain the same subject. We then paste theobject at a random location in a blank image, and use thestable diffusion inpainting model to inpaint the backgroundbased on a new prompt related to the same object name. Inaddition, we use InstructPix2Pix to stylize the generatedsamples with a probability of 0.5 to increase style variationusing randomly selected style prompts. shows someexamples of the text-image data generated through this pro-cess. The generated samples have consistent subjects withgood diversity and pose variations.",
  ". Joint-Image Diffusion": "The goal of training a joint-image diffusion model is togenerate multiple related images sharing the same subject.Conventional diffusion models , however, canonly generate individual images independently as the net-work architecture does not have any connections betweendifferent samples in a batch. We found that with a sim-ple modification, a single-image diffusion model can be adapted to a joint-image model which can generate imageshaving related content (such as the same custom subject).More specifially, given a set of same-subject noisy in-put images, we modify the attention layers of the U-Net tofuse together the self-attention features for different imagesin the same set. As illustrated in , a coupled self-attention layer has features at each spatial location attend-ing to every other location across all images in the set. Sincethe U-Net architecture has attention layers at various reso-lutions, the use of coupled self-attentions at multiple res-olutions makes the generated image set to have consistenthigh-level semantic features as well as low-level attributes. visualizes the pixel-wise correspondences and atten-tion heat maps of coupled self-attention layers. We observethat the co-attended regions across different images formthe right correspondences across all resolutions.After a coupled self-attention layer, the output is fed toa regular cross-attention layer, which aligns the visual fea-ture of each image to the corresponding text prompt. Thecoupled self-attention layer can be implemented by simplyadding two reshaping operations before and after a regu-lar self-attention, thus enabling simple and easy adaptationfrom pre-trained single-image diffusion models. (b) illustrates the training process. We start by cre-ating noisy same-subject data by adding isotropic Gaussiannoise, and train the joint-image diffusion model to denoisethe data. Ideally, there is no limit on the size of each imageset of the same subject. In our experiments, we randomlyset the size to 2, 3, or 4 during training. The training lossof JeDi is very similar to that of a regular diffusion model.We use -prediction and a simplified training objective in-troduced in . The loss function is as follows,",
  "Correspondence visualization Heat mapCorrespondence visualization Heat map": ". Visualization of the coupled self-attentions. For both scales (8x8 and 16x16), the correspondence map (Corr.) shows theconnections with the highest weights between elements in the two images. The heatmap visualizes the distribution of the attention weightsin an image for a specific element in another image (marked with a red box). We observe that similar regions in different images areco-attended in the coupled self-attention layers.",
  ". Experiments": "Dataset. We construct the Synthetic Same-Subject dataset(S3 dataset) for training as described in Sec. 3.1. After CLIPfiltering, we obtain 1.6M sets of images with each set con-taining 2-4 images. We also include the video frames fromWebVid10M and rendered multi-view images from Ob-javerse during training, as the frames from the video andthe rendered images from the asset usually have the samesubject. We use the original video caption or asset cap-tion as text prompts for all images obtained from the samevideo/asset. Additionally, we include the single-image datafrom LAION aesthetic dataset and use a set size of 1for these images. To evaluate our models, we use the testdataset proposed in DreamBooth . DreamBooth testset contains 30 real-world subjects with 4-6 images and 25prompts for each subject.Evaluation metrics. The two main evaluation criteria for personalized text-to-image generation include (1) align-ment between generated images and the input text prompts,and (2) faithfulness of the generations to the input referenceimages. We use the CLIP image-text similarity (CLIP-T)between the generated images and the input captions for (1).For (2), we follow prior works and use the co-sine similarity of CLIP image embedding (CLIP-I) andDINO image embedding (DINO) between the generatedimages and the reference images. DINO is considered tobe a preferred metric for measuring image similarity as it issensitive to the appearance variations of different images inthe same concept class. Additionally, we also report CLIP-Iand DINO scores only on the foreground masked images,i.e., images with foreground objects cutout using object de-tection and segmentation . This helps remove thebackground variations when computing the image similar-ity scores to better reflect the faithfulness to the referencesubject. We call these metrics MCLIP-I and MDINO.",
  "InputDBCDOursDBCDOurs": ". Visual comparison with finetuning methods. Finetuning methods memorize the input images (DB outputs) or result in poorinput preservation (CD outputs) when the number of reference images is small. On the other hand, JeDi can generate images that arefaithful both to the input images and the text prompt. 2048 and a learning rate 5e5. We initialize the weights us-ing the pre-trained StableDiffusion model. For each batch,we randomly sample image sets from S3, WebVid10M, Ob-javerse and LAION datasets with equal probability. We ran-domly choose the image set size between 2-4 except forLAION, where the set size is always 1. Our model takes36 hours to train on 32 A100 GPUs for 140K steps.",
  "(DB) and CustomDiffusion (CD) . For each sub-ject, we use one input reference image for finetuning-freeapproaches and three for finetuning-based approaches": "Comparison to finetuning-free methods. showsthe visual comparison of our results to BLIPD and ELITE.It can be seen that our method can faithfully capture the vi-sual features of the input reference image, including bothsemantic attributes and low-level details. However, the gen-erations from BLIPD and ELITE can only roughly resemblethe color patterns and semantic features of the input. Wealso observe that BLIPD and ELITE can produce reason-able results for common object classes such as dogs. This is . Quantitative comparisons. All finetuning-based methods use 3 input images, while the finetuning-free methods use 1 inputimage. JeDi model with 1 input image outperforms all finetuning-free baselines, while the JeDi model with 3 input images outper-forms all finetuning baselines. JeDi obtains a much higher masked DINO scores, which suggests that we achieve stronger input subjectpreservation compared to other baselines.",
  "DINO ()0.34110.75010.74320.4652MDINO ()0.43940.86390.86170.5922CLIP-T ()0.33250.30200.30410.3259": "because their encoder can easily recognize the common ob-ject categories (such as the dog breed) which makes the per-sonalized generation easier. However, for unique uncom-mon objects, their results tend to be much worse, e.g. thetoy in the second row and the image in the third row. Notethat even for the common classes such as the dog examplein the first row, the generations from BLIPD and ELITEcan miss some input features (different haircuts despite be-ing the same breed). In contrast, our method eliminates in-formation loss caused by the encoder and results in muchbetter preservation of the custom concept. This is also re-flected in the quantitative comparison , where ourmethod outperforms BLIPD and ELITE by a large margin.Comparison to finetuning-based methods. shows the visual comparison of our approach with Dream-Booth (DB) and CustomDiffusion (CD). When the num-ber of reference images is limited, it is challenging forfinetuning-based methods to avoid overfitting and gener-ate novel variations of the input subject. From , wesee that DB often directly copies the input image due tooverfitting, while the images generated by CD do not faith-fully preserve the features of the input subject. In contrast,our method creates proper variations of the reference sub-jects without changing its key visual features. Even withoutany expensive finetuning, our method outperforms DB andCD in quantitative comparisons when we provide the same",
  ". Ablation Study": "Size of S3 dataset. reports the results of training ourmodel using different numbers of synthetic images. Datasetsize 0 refers to training the model on only videos and multi-view images. This setting yields the best image alignment(DINO and MDINO) and the worst text alignment as themodel learns a shortcut to ignore the text prompts and copythe input images.The performance of columns 2-4 areroughly similar, which shows that we do not obtain muchgains by increasing the size of the synthetic dataset.Joint-image diffusion model.In Table. 4, we reportthe ablation study of different design choices in the train-ing of JeDi. The first column shows the results of a CLIPencoder baseline, which is a single-image diffusion modelconditioned on the CLIP image features of the reference im-age. Our JeDi model yields a much better image alignmentthan the CLIP encoder baseline, which demonstrates the ad-vantages of using the joint-image model over the image en-coders. We also find that adding CLIP image embedding asextra conditional input to JeDi (+ CLIP emb) does not im-prove the performance as shown in column 3. This impliesthat the joint-image model already captures the informationextracted in the image embedding. The last column reportsthe results without image guidance (w/o IG). By comparingthe second and the last columns, we can see that image guid-ance is crucial to obtaining good personalization results.",
  ". Conclusion": "This paper presents JeDi, a novel approach for finetuning-free personalized text-to-image generation using a joint-image diffusion model. We show how a single image diffu-sion U-Net can be adapted to learn a joint image distribu-tion using coupled self-attention layers. To train the joint-image diffusion model, we construct a synthetic datasetcalled S3, in which each sample contains a set of imagessharing the same subject. The experimental results showthat the proposed JeDi model outperforms the previous ap-proaches both quantitatively and qualitatively in benchmarkdatasets.",
  ". Implementation Details": "In this section, we describe the key modifications based onStableDiffusion v1.41 to implement the proposed method.We point to the original location in StableDiffusion codeand highlight the modified lines in each code snippet.Data loading.We store all images and captions thatbelong to an image set in a single image file and text file.Images are vertically concatenated and the captions corre-sponding to different images are separated by a special to-ken <|split|>, as illustrated in . This enablesus to easily reuse existing single-image dataloaders in Py-Torch.We only use square images in training and obtain",
  "Van.txt": ". Data format. An image file is the concatenation ofall images in the set. The text file contains corresponding captionsseparated by a special token.the image set size by dividing the image height with the im-age weight. Given a batch of data, we extract individualimages and text, and obtain the size of the image set ng asfollows,Joint-image diffusion models.The proposed joint-image diffusion model can be easily implemented based ona single-image diffusion model with a few simple modifica-tions as follows,Personalization as inpainting. As described in Sec. 3.3of the paper, we cast the personalized generation probleminto an inpainting task. In training, the inpainting masksare generated randomly.First, a binary random vectorng mask is sampled with every bit set to zero or one withequal probability. Then a mask of the same spatial sizeas the input image is constructed by replicating ng maskalone the height and width dimensions. The actual inputfed into the U-Net is the concatenation of the mask mask,noisy image x, and masked clean image x0*(1-mask).",
  "10h = x.type(self.dtype)": "References003At test time, x0 is the concatenation of all input images andan all-zero image. The corresponding elements in ng maskare set to 0 for the input images and 1 otherwise.Synthetic same-subject (S3) dataset. Algorithm 1 de-scribes the training data generation ( in the paper) indetails. Please note that we use the term instance segmen-tation for simplicity; however, in our implementation, wecombine an object detection model and a segmentationmodel to separate object instances rather than usingan actual instance segmentation model. The object-centricprompts (GPT(l, object) in Algorithm 1) are generated byinstructing ChatGPT to generate details of an object l, and",
  "BLIP Diffusion 0.28510.81070.82340.60910.6018ELITE 0.21930.60820.64300.18620.2156JeDi0.28560.86970.88380.79340.7926": "the scene-centric prompts (GPT(l, scene)) in Algorithm 1)are generated by instructing it to describe a scene involvingthe object l. The list of object names used in our implemen-tation and a random subset of the generated training sam-ples can be found in the attached file. We found that the ini-tial text prompts generated by ChatGPT lack variations andtherefore pair the images with the captions obtained fromBLIPv2 in training samples.",
  "Evaluation metric. We use the CLIP ViT-B/32 to com-pute CLIP-T, CLIP-I and MCLIP-I. We use DINO VIT-S/16": "to compute DINO and MDINO. We use one input imageper subject for comparison with finetuning-free methods,and average the pair-wise scores to all real images of thesame subject and over all possible choices of the input im-age. For comparison with finetuning-based methods, werandomly select three input images. In ablation studies weuse one randomly selected input image for each subject andonly compute the scores using input/output pairs by defaultunless stated otherwise.",
  ". Additional Results": "shows additional personalized generation results onreal-world human and object images. Our method can gen-erate high-quality images with diverse content while pre-serving the key visual features of the subjects in input im-ages. Although the model is not trained on human-specificdata, it can still generate reasonable results for human sub-jects, as shown in the second and third row of .",
  ". Comparison with State-of-the-Art Methods": "provides additional visual comparisons withfinetuning-based methods DreamBooth (DB) and Cus-tomDiffusion (CD). Finetuning-based methods suffer fromthe overfitting issue and might fail to preserve the subjectidentity.For common subjects, they tend to extensivelycopy from the reference images, adding only minor adjust-ments to match the given text, e.g. in the first example, forthe prompt a backpack in the snow, DreamBooth nearlyreplicate a reference image with slight snow patterns addedin the bottom. For unique subjects, the finetuning-basedmethods often fail to preserve the distinctive features, e.g.the cartoon character in the fourth row. This is becausethese methods use the loss on retrieved or generated im-ages of similar subjects as regularization during finetuning.For unique and rare objects, these images can be visuallydistinct from the reference images and interfere the modelfrom memorizing the custom concept.To further demonstrate the advantage of our methodsfor challenging cases, we collect a new test set containingunique subjects with only single input image for each sub-ject. Most the input images are from Reddit AI Art chan-nel2. visualize the input images. The first five rowsin compare the results of our method to state-of-the-art finetuning-free methods BLIP-Diffusion (BLIPD) andELITE on the unique subject test set. We also include the results on common subjects from DreamBooth test set in thelast two rows for comparison. It can be seen that BLIPD andELITE can produce reasonable results for common subjectssuch as the dog of a typical breed and a common stuffed an-imal (row 6-7). However, for unique subjects, their resultshardly resemble the subject from the reference image (row1-5). In contrast, our method can faithfully capture the keyvisual features of the subject. The advantage of our methodis also clearly reflected in the quantitative results in ,where our method outperforms ELITE and BLIP-Diffusionby a large margin.",
  ". Additional Analysis": "Image guidance. As we have discussed in the paper, theuse of image guidance can significantly improve the faith-fulness to the input images. This is also supported by thevisual comparison in (column 4-5).In our main experiments in the paper, we use a simplestrategy for image guidance where both the image and textinput are set to null for unconditional inference. Here wediscuss a more flexible guidance strategy to model trade-off between image alignment and text alignment. The scorefunction with flexible image guidance is as follows, (xt, x, M) = 0 + 1(1 0) + 2[(xt, x, M) 1],(4)where 0 = (xt, 0, M) represents the unconditionalscore when the text prompt and all reference images are setto null; 1 represents the partially conditional score wheneither the text prompt or reference images are kept. We cancompute the partial conditional score 1 using image con-ditioning to emphasize text alignment, or text conditioningto emphasize image alignment. We call these two optionstext first and image first strategies, respectively. re-ports the quantitative results based on different strategiesaveraged over a varying guidance scale in [1.5, 10]. It canbe seen that the text first strategy yields higher DINO andMDINO scores, indicating better image alignment. Imagefirst strategy yields a higher CLIP-T score, which indicatesbetter text alignment.",
  "DINO0.46520.72680.65580.7508MDINO0.59220.83840.78630.8527CLIP-T0.32590.30130.31560.2853": "We can also adjust the ratio between the guidance scaleof image condition and text condition for more flexible per-sonalized generation. visualize the change of DINOand CLIP-T scores with the varying ratio. We can see thatthe use of image guidance is important. Using only text 0.270.280.290.300.310.320.33 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80",
  "DINO0.34110.33790.70650.7501MDINO0.43940.42920.77400.8639CLIP-T0.33250.32470.30150.3020": "Comparison with baseline models. We have discusseda CLIP-encoder baseline in the paper (CLIP baseline). Herewe include the comparison to another two baseline models.Concate baseline: the input images are concatenated to thenoisy images to be fed into the UNet. Learned baseline:similar to SuTI where an learnable encoder is used toextract a feature vector from the input images. All modelsare trained on the same training data described in Sec. 4 ofthe paper and are based on the same StableDiffusion v1.4backbone for fair comparison. reports the quanti-tative comparison results and the visual comparison can befound in (column 1,2,3,5). The results indicate thatour method significantly outperforms all baseline modelsin terms of image alignment, as evidenced by considerablyhigher DINO and MDINO scores.Training data identity similarity. reports theaverage CLIP and DINO scores on training samples overthe 1,000 ImageNet categories, which indicates a high over-all identity similarity for a wide array of categories. Forcontext, we also show the scores on real images from theDreamBooth test set, which has a slightly higher identitysimilarity but covers much less categories than our dataset.",
  "Quantitative results with more input images. Although": "our method does not have an inherent constraint on the num-ber of inputs, for simplicity, we only use 1-3 input imagesin the current implementation. We find that our method stilloutperforms DB and CD, even when they are finetuned withthe maximum available reference images in the test set (4-6images), as shown in . We will add the experimentswith more reference images, e.g. 10, in the revised version.",
  "Ours (1 input)0.30400.78180.87640.61900.7510Ours (3 inputs)0.29320.81390.90110.67910.8037": "Inference cost. The inference cost is comparable to othermethods when N is small (reported in the table below).When N is substantially larger, e.g. a database, we can re-duce the inference cost by first finetuning the model on thedatabase, and then retrieving the few images closest to thetext prompt to be the actual test time input (please refer tothe future work section).",
  ". Limitations and Future Work": "A limitation of JeDi is that it needs to process all refer-ence images at inference time. This enables finetuning-freepersonalization but leads to efficiency drop when the num-ber of reference images increases. Therefore, JeDi is moresuitable for subject image generation given a few referenceimages, and are less efficient in adapting to a new domaingiven a large database of reference images. A potential so-lution is to combine JeDi with finetuning-based methods.When a large database of reference images are available,we can first finetune JeDi on the database. Then at infer-ence time, given a text prompt, we retrieve the most relaventimages from the database to use as the test-time inputs toJeDi.Another limitation is that the current implemen-tation cannot be directly applied for multi-subject imagegeneration. There are two possible ways to extend JeDifor multi-subject generation: (1) generate multiple subjectssequentially through inpainting, and (2) construct a multi-subject S3 dataset by combining multiple sets of subjects.We will explore these directions in future work.",
  "Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisser-man. Frozen in time: A joint video and image encoder forend-to-end retrieval. In International Conference on Com-puter Vision, 2021. 6": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-imagediffusion models with an ensemble of expert denoisers. arXivpreprint arXiv:2211.01324, 2022. 2, 4 James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-feng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, JoyceLee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, CaseyChu, Yunxin Jiao, and Aditya Ramesh.Improving im-age generation with better captions. 2",
  "Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-structpix2pix: Learning to follow image editing instructions.In IEEE Conference on Computer Vision and Pattern Recog-nition, 2023. 4": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In In-ternational Conference on Computer Vision, 2021. 6 Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz,Xuhui Jia, Ming-Wei Chang, and William W Cohen.Subject-driven text-to-image generation via apprenticeshiplearning. Conference on Neural Information Processing Sys-tems, 36, 2024. 4 Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, JialiangWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-aofang Wang, Abhimanyu Dubey, et al.Emu: Enhanc-ing image generation models using photogenic needles in ahaystack. arXiv preprint arXiv:2309.15807, 2023. 2 Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,Oscar Michel, Eli VanderBilt, Ludwig Schmidt, KianaEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:A universe of annotated 3d objects. In IEEE Conference onComputer Vision and Pattern Recognition, 2023. 6",
  "InputELITEBLIPDOurs1ELITEBLIPDOurs1": ". Visual comparison with finetuning-free methods on the unique subject test set (row 1-5) and on Dreambooth test set (row6-7). BLIP Diffusion and ELITE can generate reasonable results for common subjects but often fail in challenging cases involving uniquesubjects. In contrast, our method can handle challenging cases and generate personalized images with well-preserved details. Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao, SpencerWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar,and Ross Girshick.Segment anything.arXiv preprintarXiv:2304.02643, 2023. 6 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 4, 2 Nupur Kumari, Bingliang Zhang, Richard Zhang, EliShechtman, and Jun-Yan Zhu. Multi-concept customizationof text-to-image diffusion. In IEEE Conference on ComputerVision and Pattern Recognition, 2023. 2, 3, 7, 1 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models. In In-ternational conference on machine learning, pages 1973019742. PMLR, 2023. 2, 3, 6, 7, 1",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models.arXivpreprint arXiv:2301.12597, 2023. 3": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, et al. Grounding dino: Marrying dino with groundedpre-training for open-set object detection.arXiv preprintarXiv:2303.05499, 2023. 4, 6, 2 Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool.Repaint: Inpaint-ing using denoising diffusion probabilistic models. In IEEEConference on Computer Vision and Pattern Recognition,2022. 5 Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. Glide: Towards photorealistic image generationand editing with text-guided diffusion models. arXiv preprintarXiv:2112.10741, 2021. 4",
  "OpenAI.Chatgpt. Accessed: 2023. 3": "DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann, Tim Dockhorn, Jonas Muller, Joe Penna, andRobin Rombach.Sdxl: Improving latent diffusion mod-els for high-resolution image synthesis.arXiv preprintarXiv:2307.01952, 2023. 2, 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International Conference on Machine Learning,2021. 3, 4, 6 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J Liu. Exploring the limits of transfer learning witha unified text-to-text transformer. The Journal of MachineLearning Research, 21(1):54855551, 2020. 2",
  "eration with clip latents. arXiv preprint arXiv:2204.06125,2022. 2": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution image syn-thesis with latent diffusion models. In IEEE Conference onComputer Vision and Pattern Recognition, 2022. 2, 4 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration. In IEEE Conference on Computer Vision andPattern Recognition, 2023. 2, 3, 6, 7, 1 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding. In Conference on Neural Informa-tion Processing Systems, 2022. 2, 4 Christoph Schuhmann, Richard Vencu, Romain Beaumont,Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, TheoCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:Open dataset of clip-filtered 400 million image-text pairs.arXiv preprint arXiv:2111.02114, 2021. 6",
  "Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-booth: Personalized text-to-image generation without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023. 2,3": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-hishek Kumar, Stefano Ermon, and Ben Poole. Score-basedgenerative modeling through stochastic differential equa-tions. In Proc. ICLR, 2021. 2, 5 Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, LeiZhang, and Wangmeng Zuo. Elite: Encoding visual con-cepts into textual embeddings for customized text-to-imagegeneration. arXiv preprint arXiv:2302.13848, 2023. 2, 3, 6,7, 1"
}