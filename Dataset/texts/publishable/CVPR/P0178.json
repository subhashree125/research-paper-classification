{
  "Abstract": "We provide a two-way integration for the widely adoptedControlNet by integrating external condition generation al-gorithms into a single dense prediction method and in-corporating its individually trained image generation pro-cesses into a single model.Despite its tremendous suc-cess, the ControlNet of a two-stage pipeline bears limita-tions in being not self-contained (e.g.calls the externalcondition generation algorithms) with a large model redun-dancy (separately trained models for different types of con-ditioning inputs). Our proposed OmniControlNet consoli-dates 1) the condition generation (e.g., HED edges, depthmaps, user scribble, and animal pose) by a single multi-tasking dense prediction algorithm under the task embed-ding guidance and 2) the image generation process fordifferent conditioning types under the textual embeddingguidance. OmniControlNet achieves significantly reducedmodel complexity and redundancy while capable of produc-ing images of comparable quality for conditioned text-to-image generation.",
  "* equal contribution. Work done during the internship of Yilin Wang,Haiyang Xu, Zhizhou Sha, and Zirui Wang at UC San Diego": "to a recent wave of generative model progressing beyondtraditional models such as VAE and GAN .The ControlNet further promotes the popularityof text-to-image generation by introducing additional usercontrols as the conditioning input available in a myriad offorms including edges , line segments , humanpose , normal map , depth map , segmentationmap , and user scribble. With the additional image-level input beyond the text prompts, ControlNet can greatlyexpand the scope of application domains for text-to-imagegeneration to real-world workflows in various areas, includ-ing design, architecture, gaming, art, manufacturing, anima-tion, and human-computer interaction.ControlNet is a two-stage pipeline comprising 1)a condition generation stage and 2) a text-to-image gener-ation stage conditioned on the output from the first stage.Despite the great success ControlNet has achieved, it stillsuffers from the issue of large model redundancy in twomeans: 1) in stage 1, a specific external algorithm is exe-cuted to create each type of image-level condition, and 2) instage 2, a separate diffusion model is trained for each typeof conditional input. gives an schematic illustrationfor the ControlNet method .In this paper, we aim to alleviate the algorithm and modelredundancy problem in ControlNet by proposing Om-niControlNet, which provides a dual-stage integration. Thatis, in stage 1, instead of calling the external algorithms,",
  "Frozen Module": ". An overview of our conditioned text-to-image generation pipeline. Beginning with the original ControlNet structure ,we utilize the textual inversion to learn task embeddings. Subsequently, we append the prefix use <feature> as feature to the prompt andfeed the result into the trainable copy. The left side of the figure provides an overview of the conditioned text-to-image generation model,while the right side illustrates the process of learning the CLIP embedding for the new word with textual inversion . single model. Subsequently, the samples generated by theStage 1 model serve as the training data for the conditionedtext-to-image generation (stage 2) model. During inference,images are input into the Stage 1 model, whose output isthen forwarded to the Stage 2 model for further processing.By utilizing this stage 1 model, we can directly sample dif-ferent features from a single model without needing multi-ple expert models. Then, we can use these sampled featuresto generate images that share similar features with the origi-nal one but with specified semantic meanings. showsthe structure of the whole pipeline.",
  ". Text-to-Image Generation": "The task of text-to-image generation isto generate an image matching the provided text promptsusing deep learning models. Before the wide use of diffu-sion models, the task was primarily achieved by GAN based models . The work Generative Adver-sarial Text to Image Synthesis applied an encoder to en- code the texts and concatenated the encoded features to theimage features before inserting them into the GAN model,which was among the first works to tackle the task. After theintroduction of diffusion models , lots of diffusion-based models appeared , which mainly used cross attention tocombine the image and text features in the UNet back-bone. DALLE-2 and Stable Diffusion are amongthe outstanding literature in the field. Many works, includ-ing T2I-Adapter , ControlNet our OmniControl-Net model, are based on the Stable Diffusion model.2.2. Image-to-Image Generative Model Image-to-image generation involves transferring an im-age from one domain to another. For example, in Control-Net , additional features provided as images are fedinto the model to generate the required images. Before thewidespread use of diffusion models, GAN-based models such as and Transformer-based models were com-monly adopted.CycleGAN was one of the fore-most models for image-to-image transfer, utilizing a GAN-based approach for style transfer with cycle consistency.With the introduction of diffusion models , many have demonstrated the significant potentialof diffusion models in this task. Recently, several works have combined text and imageconditions within diffusion models, enabling the generationof high-quality images. ControlNet is a notable example,taking text prompts and additional features as constraints toguide image generation.",
  "ControlNet has demonstrated its performance in condi-tional image generation across various conditions, includ-": "ing Depth Map , Canny Edge , OpenPose ,Normal Map , User Scribble, and Segmentation ,etc. In this section, we delve into four representative tasks:Depth Map, HED Edge, User Scribble, and Animal Pose,along with the expert models associated with each.Generating depth maps to represent relative distances is afundamental challenge in computer vision and 3D scene un-derstanding tasks. Numerous methods have been proposed,ranging from traditional stereo matching algorithms to deep learning-based approaches .We use MiDAS as our expert model, which exhibitsexceptional performance and generalization capabilities.Image edge detection plays a major role in tasks suchas object segmentation and visual salience. Early methods relied on manual design for edge detec-tion. However, with the advent of deep learning, learning-based methods have demonstratedgreat potential in handling edge detection tasks. A classicbenchmark in this field is Holistically-Nested Edge Detec-tion (HED) , and we take it as our expert model.User scribbles serve as user-defined guidance for imagegeneration tasks, enabling users to convey their intentionsand preferences to the generative model. In ControlNet, thisinvolves a simple mapping of pixels with values greater than127 to 255, and the rest to 0 in an image.Generating pose maps, which encode spatial informationabout the arrangement of objects or characters in images, iscrucial for tasks like image-to-image translation, particu-larly in human or object pose manipulation. Human poseestimation models are designed todescribe human skeletons. Notable benchmarks in this do-main include PoseNet and OpenPose . In terms ofanimal pose estimation, which often presents more diversityand challenges than human pose estimation, datasets likeAP-10K and APT-36K are considered main-stream references.",
  ". ControlNet": "The ControlNet model presents an efficient frame-work for fine-tuning the Stable Diffusion model . Itintroduces an additional control feature (e.g. depth map oredge detection) to the generative process, ensuring that thegenerated images adhere to both the textual prompt and thecontrol condition. In our approach, the weights of the Sta-ble Diffusion model (SD-v1.5) are fixed, while a trainableduplicate of the weights from the 12-layer U-Net encoderand middle block is created. The additional features are in-tegrated into this trainable duplicate via a zero-convolutionlayer (a 11 convolution layer with all-zero initial weights).We denote the encoder in the frozen part as E, the en-coder of the trainable copy as E, the middle block and the",
  ". Original ControlNet model. For different fea-tures, we have to use different expert models for condition gener-ation, and we have to train ControlNet on each of the features": "decoder of the frozen part as M and D, respectively. Letthe CLIP-encoded additional feature be cf, the input of themodel as z, time as t, and the CLIP-encoded text promptas ct. With Z1, Z2 representing two trainable zero con-volution layers, the output of the trainable copy should beE(Z1(cf)+z, t, ct). Consequently, the output of the model,pred, which also estimates the noise in the denoising pro-cess, should be",
  "Task Embedding": ". An overview of our multi-task dense image predictionpipeline. First, we leverage a Swin Transformer to extract multi-scale features and propose a multi-head FPN to get full-resolutionfeature maps. Finally, we utilize task-specific embeddings to de-code dense predictions from the feature maps. As depicted in , our multi-task dense image predic-tion model is architecturally divided into three components:a backbone structure, a Multi-Head Feature Pyramid Net-work (FPN) , and a Decoder Head.Initially, we employ a pre-trained Swin Transformer to extract multi-scale image features. Considering the res-olution of the input image as 1, the extracted features ateach stage correspond to resolutions of 1",
  ",116, and132, with a uniform feature channel count of 256": "Subsequently, a Multi-Head FPN is employed to harnessrich semantic information from these multi-scale features.To foster feature diversity across various task types, theFPN is structured in a parallel configuration with m distinctheads, each representing a variant of the original FPN ar-chitecture. Specifically, each FPN head undergoes an addi-tional transposed convolution layer to upscale the resolutionto 1 while simultaneously reducing the channel dimen-sion to C. The concatenated outputs of all m heads yielda comprehensive, full-resolution multi-task output featurewith channel dimension mC.In the final stage, task-specific embedding is leveragedto decode the target condition from the aforementioned out-put. The flexibility in the type of task embedding is note-worthy; both one-hot and clip text embeddings derived fromthe task name are effective. We employ a Multilayer Per-ceptron (MLP) to project the task embedding into a latentspace with an embedding dimension of mC, subsequentlyunsqueezing the channel dimension to 1. A cross-productoperation is then executed between the output of the Multi-Head FPN and the encoded task embedding, culminating inthe decoder output, followed by a Sigmoid.",
  ". Stage 2: Conditioned T2I Generation": "provides an overview of our conditioned text-to-image generation (stage 2) pipeline.For different tasks, such as depth map or hed edge asan additional feature, we initially apply the textual inver-sion , using 16 random images for each feature to learnthe corresponding new words (represented by forms suchas <depth> or <hed>). Subsequently, we add these newwords into the CLIP embedding space so that whenthey are used in text prompts, the CLIP encoder can recog-nize their specific meanings.After acquiring these new embeddings, we adapt theprompts for each (prompt, feature, image) triplet. For in-stance, if the feature for a given triplet is the depth map ofthe image and the original prompt is a motorcycle in frontof a tree, the revised prompt would be Use <depth> asa feature, a motorcycle in front of a tree. The modifiedtriplets are fed into the trainable copy, while the correspond-ing original triplets are fed into the frozen part. Follow-ing this, the model is trained with a methodology similar toControlNet, where the triplets are fed into the model undif-ferentiated, without separating them by features.Tab. 1 provides the comparison of the model size as wellas the data scale when compared to other integrated models,including UniControl , and Uni-ControlNet , andour model demonstrates several advantages.When compared to UniControl, our model, followingthe structure of ControlNet, requires no additional param-eters. In contrast, UniControl incorporates an additionalmixture-of-experts (MoE) module, resulting in a substan-",
  "n refers to the number of datasets we combine. In our work, n = 2": ". Comparison of parameters and data scale between Om-niControlNet and competing works. Extra Parameters refers tothe number of extra parameters compared to the original Control-Net, while Extra Data refers to the increased amount of data dur-ing training. Uni-ControlNet needs to fill the blanks of the mixeddatasets with black images, which will double the scale of the data. tially larger model (20M more parameters than other mod-els, including ControlNet, Uni-ControlNet, and our model).During training, an increase of 1 in batch size leads to a 3Gigabytes increase in GPU memory usage.In contrast to Uni-ControlNet, our model does notneed to perform channel-wise concatenation of multiple ad-ditional features.In our configuration, different featuresoriginate from varying sets of images. Whereas for Uni-ControlNet, when an image provides a feature such as adepth map but lacks another (e.g. animal pose), the corre-sponding channels for the animal pose are filled with zeros,yielding a larger data scale.",
  ". Textual Inversion Module": "Textual Inversion is an approach for extracting anddefining new concepts from a few example images, whichis the inversion process of text-to-image generation. Thismethod creates new words or tokens in the embeddingspace of the text encoder within the text-to-image genera-tion pipeline, such as Stable Diffusion . Once estab-lished, these unique tokens can be integrated into textualprompts, allowing for precise control over the characteris-tics of the images produced.We leverage Stable Diffusion as our base model. For theset of images provided, the prompt is set to s = an im-age of <w>, while the embedded feature v of the word<w>is our target. For the frozen SD model, suppose c isthe encoded feature of s, then we can express c = c(v), as cis determined by v. Therefore, the optimization goal shouldbe",
  "Datasets": "Training. The dataset for both multi-task dense image pre-diction (stage 1) and conditioned text-to-image generation(stage 2) training consists of 2 different parts.Featuresdepth map, HED edge, and user scribble are from the firstpart, while the feature animal pose is from the second part.In the first part, we first use YOLOv5 model to de-tect all the humans in the images from the Laion-5B dataset and choose the first 50,000 images that consist atmost 1 human. We directly sample user scribbles from theimages, employ an HED boundary detection model to generate HED edges, and use the Midas depth detector to produce depth maps. The captions of the images aretaken from the origin Laion-5B dataset. In the second part,we utilize the AP-10K dataset and use the MMPose model to generate the animal poses of the animals. Thecaptions are generated by the BLIP2 model. In order tomake the 2 parts contain approximately the same number ofimages, we duplicate each image in the second part 5 times.Sampling and Testing. For the features depth map, HEDedge, and user scribble, we utilize the validation split of theCOCO2017 dataset and obtain the corresponding fea-ture in the same way as the training set. We use the first cap-tion for each image in the dataset. For the animal pose, weutilize the APT-36K dataset and choose the first im-age from each frame as the dataset. We sample the animalposes the same way as the training set and use the BLIP2 model to perform the image captioning.",
  "Ours34.8636.5736.6351.100.30240.29710.29710.3269": ". Quantitative results of our model, including single stage 2 (conditioned text-to-image generation) model and integrated stage 1(multi-task dense image prediction) + integrated stage 2 (conditioned text-to-image generation) models. Although methods that utilize dif-ferent models (T2I-Adapter and ControlNet) tend to perform better, our framework demonstrates competitive results among the integratedmodels. The numbers in bold indicate the best performance among the integrated methods. The bold numbers represent the best scoreamong integrated methods. employed for the other three scenarios. The assigned lossweights for depth, HED edge, user scribble, and pose are0.5, 1, 5, and 5, respectively. We resize all the images to512512 and take a batch size 16. The model employs anSGD Optimizer with an initial learning rate of 1e-6, whichsubsequently decreases to 9e-7 following a polynomial de-cay pattern after 120k iterations. The entire training processtakes about 20 hours on 8 NVIDIA RTX 3090 GPUs.For the textual inversion module, each of the new wordof a corresponding feature is trained on 8 NVIDIA RTX3090 GPUs for about 1 hour.For our conditioned text-to-image generation (stage 2)model, the number of DDIM diffusion steps is set to 50.We adopt the AdamW optimizer and set the learning rate to1e-5. We train the model on 8 NVIDIA RTX 3090 GPUswith batch size 2 for 50,000 iterations (4 epochs), whichtakes about 40 hours.",
  "Evaluation Metrics": "For our multi-task dense image prediction (stage 1)model, various metrics are adopted to evaluate different as-pects of the models performance. For depth estimation, theRoot Mean Square Error (RMSE) is utilized. For edge de-tection, three distinct metrics are adopted: the fixed contourthreshold (ODS), per-image best threshold (OIS), and av-erage precision (AP). The ODS is a metric that evaluatesedge detection performance by considering a fixed thresh-old value across all images, thereby providing a universalperformance measure. On the other hand, OIS varies thethreshold for each image to find the optimal threshold forthat particular image, offering a more adaptive measure ofperformance. Lastly, AP is a commonly used metric in edgedetection tasks. It computes the average precision value for recall values over the interval .For our conditioned text-to-image generation (stage 2)model and the integrated model, we adopt FID score and CLIPt similarity score as our metrics. For the FIDscore, we utilize a widely used inception model to measurethe similarity between synthesized and real images. For theCLIPt similarity score, for each pair of generated image andcorresponding caption, we use ViT-B/32 CLIP to en-code them, and calculate the inner product of them as theCLIPt similarity score. We report the average of the innerproducts of all the image-caption pairs.",
  ". Experiment Results": "and display the visual results for boththe multi-task dense image prediction (stage 1), the condi-tioned text-to-image generation (stage 2), and the combinedmodel. According to the figure, it is evident that the modelsfrom both stages and the combined one can generate high-quality results. Stage 1: Integrated Dense Prediction. To demonstratethe ability of our stage 1 model, we show the result on thedepth benchmark NYUDv2 and the HED benchmarkBSDS500 .For depth estimation, we compare our result withDPThybrids contemporary work, including DeepLabv3+, RelativeDepth , ACAN , ShapeNet andDPThybrid . As shown in Tab. 3, our result outperformsall the models except for DPThybrid.For edge detection, we compare with classic methods in-cluding . As illustrated in Tab. 4, our model surpasses all themodels except for HED .",
  "Ours0.472": ". Depth performance of our multi-task dense image predic-tion (stage 1) model. Our model utilizes the output of DPThybrid asthe training data; therefore, it is acceptable for surpassing all othermethods except for DPThybrid. ation.We compare the quantitative results on the met-rics FID score and CLIPt similarity score with other meth-ods, including ControlNet , T2I-Adapter , Uni-ControlNet and UniControl . The latter two meth-ods build an integrated pipeline that can use a single modelto generate images with different additional features, whilefor the first two methods, a new model must be trained foreach different additional feature.Tab. 2 presents the numerical results for the FID scoreand the CLIPt similarity score across various additional fea-tures and methods. Although methods that utilize differ-ent expert models for different features perform better, ourmethod ranks among the best-performing methods withinthe category of integrated models. Integrated Model Results. In the integrated model, similarto the stage 2 model, we once again compare the quantita-tive results using metrics such as FID score and CLIPt simi-larity score with methods including T2I-Adapter , Con-",
  "Ours0.7610.7820.811": ". HED performance of our multi-task dense image predic-tion (stage 1) model. For the three metrics, ODS, OIS, and AP, thelarger the number, the better the performance. We can see that ourmethod achieves competitive performance. trolNet , UniControl , and Uni-ControlNet .The quantitative results are presented in Tab. 2. It can beobserved that although the overall performance of the inte-grated model is slightly inferior to methods directly utiliz-ing features from multiple expert models, it still manages togenerate images of promising quality.",
  "To demonstrate the effectiveness of our model, Omni-ControlNet, and to reveal the impacts of certain structural": "designs, we conducted several ablation studies: 1) Inject-ing learned task prefix embedding into different parts of theconditioned text-to-image generation module; 2) Learningweights of the zero-convolution layers with an MLP whilethe model is trained with the learned task prefix embedding;and 3) Comparing different encoding methods and the num-ber of heads in the multi-head Feature Pyramid Network.For 1) and 2), we report the results based on our unifiedstage 2 setting. For 3), we report the results based on ourunified (stage 1 + stage 2) setting.",
  ". Prefix Injection": "In our original framework, only the text prompts fed intothe trainable copy of the SD model contain prefixes such asUse <depth> as feature. In this ablation study, we addedthe prefix to both parts of the model. The results are shownin Tab. 5. We observe that adding the prefix only to thetrainable part yields better results.",
  ". Quantitative results of generating zero-conv weights viatextual inversion embeddings.Learn weight by MLP refers tothe model using an MLP to learn the weight of the first zero-convolution": "In our original framework, the zero-conv layers are ini-tialed with zeros and updated during each training step bybackpropagation, where multiple tasks share the same zero-conv weights. In the ablation study, we use an MLP to gen- erate the weights of the first zero-conv layer from the textualinversion embedding of each task. The results are presentedin Tab. 6. We observe that directly training the first con-volution layer instead of using the MLP yields a better FIDscore, yet generating the weights dynamically via MLP pro-duces an overall higher CLIPt score.",
  ". Different Task Encoding and Number of Heads": "In our foundational framework, a multi-head FeaturePyramid Network (FPN) is employed to process multi-scalefeatures, while one-hot encoded task embeddings are uti-lized for extracting target conditions. Our ablation studyinvestigates the indispensability of the multi-head FPN andthe efficacy of one-hot encoding. We implement two varia-tions: one model with a single FPN head and another lever-aging complex text embeddings generated by the CLIP text encoder. The comparative results are detailed in Tab. 7.Results show that integrating one-hot encoding with multi-ple FPN heads yields superior performance, demonstratingthe effectiveness of our design.",
  ". Conclusion and Limitations": "In this paper, we propose OmniControlNet, a stream-lined approach that combines multiple external conditionimage generation processes into a cohesive one. This inte-gration addresses the limitations of ControlNets two-stagepipeline, which relies on external algorithms and has sepa-rate models for each input type. With OmniControlNet, wehave a multitasking algorithm for generating conditions likeedges, depth maps, and poses and an integrated image gen-eration process guided by textual embedding. This resultsin a simpler, less redundant model capable of generatinghigh-quality text-conditioned images. Limitations. 1) When adding an additional task condition,its required to train a new embedding for the task. 2) Withthe integrated stage 1 model, the training complexity willincrease, and image generation quality will decrease com-pared to using separate expert models as the stage 1 model.",
  "Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Onlya matter of style: Age transformation using a style-basedregression model. ACM Transactions on Graphics (TOG),40(4):112, 2021. 2": "Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-tendra Malik.Contour detection and hierarchical imagesegmentation. IEEE transactions on pattern analysis andmachine intelligence, 33(5):898916, 2010. 6, 7 Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-tendra Malik.Contour detection and hierarchical imagesegmentation. IEEE Transactions on Pattern Analysis andMachine Intelligence, 33(5):898916, 2011. 3, 6 Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,Samuli Laine, Bryan Catanzaro, et al.ediffi: Text-to-image diffusion models with an ensemble of expert denois-ers. arXiv preprint arXiv:2211.01324, 2022. 2 Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani.Deepedge: A multi-scale bifurcated deep network for top-down contour detection. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages43804389, 2015. 6, 7 Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. High-for-low and low-for-high:Efficient boundary detectionfrom deep object features and its applications to high-levelvision. In Proceedings of the IEEE international conferenceon computer vision, pages 504512, 2015. 6, 7",
  "Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.Realtime multi-person 2d pose estimation using part affin-ity fields. In CVPR, 2017. 3": "Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.Sheikh. Openpose: Realtime multi-person 2d pose estima-tion using part affinity fields. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2019. 1, 3 Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-ing Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,and Wen Gao. Pre-trained image processing transformer.In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1229912310, 2021.2",
  "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,and Hengshuang Zhao. Anydoor: Zero-shot object-levelimage customization.arXiv preprint arXiv:2307.09481,2023. 2": "Yuru Chen, Haitao Zhao, Zhengwei Hu, and Jingchao Peng.Attention-based context aggregation network for monoc-ular depth estimation.International Journal of MachineLearning and Cybernetics, 12:15831596, 2021. 6, 7 Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,Sunghun Kim, and Jaegul Choo. Stargan: Unified genera-tive adversarial networks for multi-domain image-to-imagetranslation.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 87898797,2018. 2",
  "Piotr Dollar and C Lawrence Zitnick. Fast edge detectionusing structured forests. IEEE transactions on pattern anal-ysis and machine intelligence, 37(8):15581570, 2014. 6,7": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold,Sylvain Gelly, et al.An image is worth 16x16 words:Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929, 2020. 6 Patrick Esser, Robin Rombach, and Bjorn Ommer. Tamingtransformers for high-resolution image synthesis. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1287312883, 2021. 2",
  "Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficientgraph-based image segmentation. International journal ofcomputer vision, 59:167181, 2004. 6, 7": "Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, ArjunAkula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang,and William Yang Wang. Training-free structured diffusionguidance for compositional text-to-image synthesis. arXivpreprint arXiv:2212.05032, 2022. 2 Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-manghelich, and Dacheng Tao. Deep Ordinal RegressionNetwork for Monocular Depth Estimation. In IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR),2018. 3",
  "Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik,and Daniel Cohen-Or.Stylegan-nada: Clip-guided do-main adaptation of image generators.arXiv preprintarXiv:2108.00946, 2021. 2": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprintarXiv:2208.01618, 2022. 4, 5 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,and Yoshua Bengio. Generative adversarial nets. Advancesin neural information processing systems, 2014. 1, 2 Geonmo Gu, Byungsoo Ko, SeoungHyun Go, Sung-HyunLee, Jingeun Lee, and Minchul Shin. Towards light-weightand real-time line segment detection. In Proceedings of theAAAI Conference on Artificial Intelligence, 2022. 1 Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, YuQiao, Dahua Lin, and Bo Dai. Animatediff: Animate yourpersonalized text-to-image diffusion models without spe-cific tuning. arXiv preprint arXiv:2307.04725, 2023. 2",
  "Jyh-Jing Hwang and Tyng-Luh Liu. Pixel-wise deep learn-ing for contour detection. arXiv preprint arXiv:1504.01989,2015. 6, 7": "Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward HAdelson. Crisp boundary detection using pointwise mutualinformation. In Computer VisionECCV 2014: 13th Eu-ropean Conference, Zurich, Switzerland, September 6-12,2014, Proceedings, Part III 13, pages 799814. Springer,2014. 6, 7 Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 44014410, 2019. 2",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2: Bootstrapping language-image pre-training withfrozen image encoders and large language models, 2023.5": "Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu,Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong JaeLee. Gligen: Open-set grounded text-to-image generation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 2251122521, 2023.2 Joseph J Lim, C Lawrence Zitnick, and Piotr Dollar. Sketchtokens: A learned mid-level representation for contour andobject detection. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 31583165,2013. 6, 7",
  "Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding,Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, XianyanJia, et al.M6: A chinese multimodal pretrainer.arXivpreprint arXiv:2103.00823, 2021. 2": "Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollar, andC Lawrence Zitnick. Microsoft coco: Common objects incontext. In Computer VisionECCV 2014: 13th EuropeanConference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part V 13, pages 740755. Springer, 2014. 5 Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,Bharath Hariharan, and Serge Belongie.Feature pyra-mid networks for object detection. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 21172125, 2017. 3 Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.Learning depth from single monocular images using deepconvolutional neural fields.IEEE transactions on pat-tern analysis and machine intelligence, 38(10):20242039,2015. 3 Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, andJoshua B Tenenbaum.Compositional visual generationwith composable diffusion models. In European Confer-ence on Computer Vision, pages 423439. Springer, 2022.2 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 3",
  "David Marr and Ellen Hildreth. Theory of edge detection.Proceedings of the Royal Society of London. Series B. Bio-logical Sciences, 207(1167):187217, 1980. 3": "D.R. Martin, C.C. Fowlkes, and J. Malik. Learning to detectnatural image boundaries using local brightness, color, andtexture cues. IEEE Transactions on Pattern Analysis andMachine Intelligence, 26(5):530549, 2004. 3 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guidedimage synthesis and editing with stochastic differentialequations. arXiv preprint arXiv:2108.01073, 2021. 2 Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter:Learning adapters to dig out more controllable abil-ity for text-to-image diffusion models.arXiv preprintarXiv:2302.08453, 2023. 2, 6, 7 Alejandro Newell, Kaiyu Yang, and Jia Deng.Stackedhourglass networks for human pose estimation. In Com-puter VisionECCV 2016: 14th European Conference, Am-sterdam, The Netherlands, October 11-14, 2016, Proceed-ings, Part VIII 14, pages 483499. Springer, 2016. 3 Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptivenormalization. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 23372346, 2019. 2, 3 Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipula-tion of stylegan imagery. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 20852094, 2021. 2, 8",
  "Vision and Pattern Recognition (CVPR), pages 14021412,2022. 3": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,Yingbo Zhou, Huan Wang, Juan Carlos Niebles, CaimingXiong, Silvio Savarese, et al. Unicontrol: A unified diffu-sion model for controllable visual generation in the wild.arXiv preprint arXiv:2305.11147, 2023. 2, 4, 6, 7 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International conference on machine learning,pages 87488763. PMLR, 2021. 6 Michael Ramamonjisoa and Vincent Lepetit.Sharpnet:Fast and accurate recovery of occluding contours in monoc-ular depth estimation.In Proceedings of the IEEE/CVFInternational Conference on Computer Vision Workshops,pages 00, 2019. 6, 7 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, ScottGray, Chelsea Voss, Alec Radford, Mark Chen, and IlyaSutskever. Zero-shot text-to-image generation. In Interna-tional Conference on Machine Learning, pages 88218831.PMLR, 2021. 1, 2",
  "Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-sion transformers for dense prediction. ICCV, 2021. 6, 7": "Rene Ranftl, Katrin Lasinger, David Hafner, KonradSchindler, and Vladlen Koltun. Towards robust monocu-lar depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis andMachine Intelligence, 44(3), 2022. 5 Rene Ranftl, Katrin Lasinger, David Hafner, KonradSchindler, and Vladlen Koltun. Towards robust monocu-lar depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis andMachine Intelligence, 44(3), 2022. 3 Joseph Redmon, Santosh Divvala, Ross Girshick, and AliFarhadi. You only look once: Unified, real-time object de-tection. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 779788, 2016. 5 Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-geswaran, Bernt Schiele, and Honglak Lee. Generative ad-versarial text-to-image synthesis.In Proceedings of The33rd International Conference on Machine Learning, 2016.2 Xiaofeng Ren and Liefeng Bo.Discriminatively trainedsparse code gradients for contour detection. In Proceedingsof the 25th International Conference on Neural InformationProcessing Systems-Volume 1, pages 584592, 2012. 6, 7",
  "Encoding in style: a stylegan encoder for image-to-imagetranslation. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 22872296,2021. 2": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution imagesynthesis with latent diffusion models. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1, 2, 3, 4 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image seg-mentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th InternationalConference, Munich, Germany, October 5-9, 2015, Pro-ceedings, Part III 18, pages 234241. Springer, 2015. 2 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, 2023. 1 Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,Jonathan Ho, Tim Salimans, David Fleet, and MohammadNorouzi.Palette: Image-to-image diffusion models.InACM SIGGRAPH 2022 Conference Proceedings, pages 110, 2022. 2 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-mans, et al. Photorealistic text-to-image diffusion modelswith deep language understanding. Advances in Neural In-formation Processing Systems, 35:3647936494, 2022. 1",
  "Ashutosh Saxena, Min Sun, and Andrew Y. Ng. Make3d:Learning 3d scene structure from a single still image. IEEETransactions on Pattern Analysis and Machine Intelligence,31(5):824840, 2009. 3": "Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon,Ross Wightman,Mehdi Cherti,TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, Patrick Schramowski, Srivatsa Kundurthy, KatherineCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Je-nia Jitsev. Laion-5b: An open large-scale dataset for train-ing next generation image-text models. In Advances in Neu-ral Information Processing Systems, pages 2527825294.Curran Associates, Inc., 2022. 5 Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, andZhijiang Zhang. Deepcontour: A deep convolutional fea-ture learned by positive-sharing loss for contour detection.In 2015 IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 39823991, 2015. 3, 6 Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, andZhijiang Zhang. Deepcontour: A deep convolutional fea-ture learned by positive-sharing loss for contour detection.In Proceedings of the IEEE conference on computer visionand pattern recognition, pages 39823991, 2015. 7",
  "without text-video data. arXiv preprint arXiv:2209.14792,2022. 2": "Amos Sironi, Vincent Lepetit, and Pascal Fua.Multi-scale centerline detection by learning a scale-space distancetransform. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, pages 26972704,2014. 6, 7 Amos Sironi, Vincent Lepetit, and Pascal Fua. Projectiononto the manifold of elongated structures for accurate ex-traction. In Proceedings of the IEEE International Confer-ence on Computer Vision, pages 316324, 2015. 6, 7",
  "Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Er-mon. Dual diffusion implicit bridges for image-to-imagetranslation. arXiv preprint arXiv:2203.08382, 2022. 2": "Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao,Qi Tian, Matti Pietikainen, and Li Liu.Pixel differencenetworks for efficient edge detection.In Proceedings ofthe IEEE/CVF international conference on computer vi-sion, pages 51175127, 2021. 3 Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deephigh-resolution representation learning for human pose es-timation. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 56935703,2019. 3",
  "Zhuowen Tu. Learning generative models via discrimina-tive approaches. In 2007 IEEE Conference on ComputerVision and Pattern Recognition, 2007. 1": "Narek Tumanyan, Michal Geyer, Shai Bagon, and TaliDekel.Plug-and-play diffusion features for text-drivenimage-to-image translation.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 19211930, 2023. 2 Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,Haochen Wang, Falcon Z. Dai, Andrea F. Daniele, Moham-madreza Mostajabi, Steven Basart, Matthew R. Walter, andGregory Shakhnarovich. Diode: A dense indoor and out-door depth dataset, 2019. 1, 3 Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advancesin neural information processing systems, 30, 2017. 2",
  "Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, andElisa Ricci. Structured attention guided convolutional neu-ral fields for monocular depth estimation. In CVPR, 2018.3": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional genera-tive adversarial networks. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages13161324, 2018. 2 Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, andXiaogang Wang.Learning feature pyramids for humanpose estimation. In proceedings of the IEEE internationalconference on computer vision, pages 12811290, 2017. 3 Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, LongLan, and Dacheng Tao. APT-36k: A large-scale bench-mark for animal pose estimation and tracking. In Thirty-sixth Conference on Neural Information Processing Sys-tems Datasets and Benchmarks Track, 2022. 3, 5 Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, andDacheng Tao. Ap-10k: A benchmark for animal pose es-timation in the wild. In Thirty-fifth Conference on NeuralInformation Processing Systems Datasets and BenchmarksTrack (Round 2), 2021. 3, 5 Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autore-gressive models for content-rich text-to-image generation.arXiv preprint arXiv:2206.10789, 2022. 2 Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xi-aogang Wang, Xiaolei Huang, and Dimitris N Metaxas.Stackgan:Text to photo-realistic image synthesis withstacked generative adversarial networks.In Proceedingsof the IEEE international conference on computer vision,pages 59075915, 2017. 2 Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Addingconditional control to text-to-image diffusion models. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 38363847, 2023. 1, 2, 3, 5, 6, 7 Shihao Zhao, Dongdong Chen, Yen-Chun Chen, JianminBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusionmodels. Advances in Neural Information Processing Sys-tems, 2023. 2, 4, 6, 7 Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-dler, Adela Barriuso, and Antonio Torralba. Semantic un-derstanding of scenes through the ade20k dataset. Interna-tional Journal of Computer Vision, 127(3):302321, 2019.1 Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei AEfros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEEinternational conference on computer vision, pages 22232232, 2017. 2 Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-ward multimodal image-to-image translation. Advances inneural information processing systems, 30, 2017. 2"
}