{
  ", amitraj93.github.io": ". Our method, ICE-G, allows for quick color or texture edits to a 3D scene given a single style image, or mask selection on a singleview. We show two rendered views of mask select editing for the Garden Scene where we apply stone texture to the table and fall colors tothe grass (left). We also show two renders of correspondance based editing where we can transfer the color of the blue car to the lego and thetexture of the grass to the table (right). AbstractRecently many techniques have emerged to create highquality 3D assets and scenes. When it comes to editingof these objects, however, existing approaches are eitherslow, compromise on quality, or do not provide enough cus-tomization. We introduce a novel approach to quickly edit a3D model from a single reference view. Our technique firstsegments the edit image, and then matches semantically cor-responding regions across chosen segmented dataset viewsusing DINO features. A color or texture change from aparticular region of the edit image can then be applied toother views automatically in a semantically sensible manner. These edited views act as an updated dataset to further trainand re-style the 3D scene. The end-result is therefore anedited 3D model. Our framework enables a wide varietyof editing tasks such as manual local edits, correspondencebased style transfer from any example image, and a combi-nation of different styles from multiple example images. Weuse Gaussian Splats as our primary 3D representation dueto their speed and ease of local editing, but our techniqueworks for other methods such as NeRFs as well. We showthrough multiple examples that our method produces higherquality results while offering fine grained control of editing.Project page: ice-gaussian.github.io",
  ". Introduction": "Editing of 3D scenes and models is an area of growingimportance as applications like robotics simulation, videogames, and virtual reality grow in popularity.Editable3D representations can lead to dynamic and customizableenvironments in these applications, allowing artists, devel-opers, and researchers alike to quickly iterate on projectsand produce valuable content.Recently, Gaussian Splats have emerged as apowerful method to represent 3D objects and scenes,allowing for fast training and preservation of high-qualitydetails. Prior to this, NeRFs (Neural Radiance Fields) have been used extensively to create scenes, and manytechniques have been introduced to edit the color and textureof NeRFs. However, such editing has thus far been slowand limited in the types of edits possible. Our work seeksto develop a general method that works on both Splats andNeRFs, supporting fast and high-quality style edits.NeRF editing works can be categorized by their editing in-terfaces into text-based or image-based methods. Text-basedapproaches use text-image models for guidance, deliveringresults faithful to prompts but limited by the ambiguity oftext descriptions for 3D scenes. This leads to uncertaintiesin conveying specific colors, styles, or textures, such as theexact shade of light blue or the precise pattern of a sandtexture.Our image-based editing approach addresses the ambi-guities of text-based methods, yet current techniques limitmodifications to a single style image for the entire scene andlack the ability to transfer color or texture between differentimage parts or specify them per region. Additionally, 3D edit-ing classifications based on changescolor, texture, shape,or their combinationsshow that shape modifications oftenreduce image quality by converting 2D guidance into 3Dusing methods like Score Distillation Sampling (SDS) or Iterative Dataset Update (IDU) , which generalizefeatures at the expense of detail.In this paper, we propose a method that aims to take thetexture and/or color of different segmented regions from anediting image, and transfer them to corresponding segmentedregions of a sampled set of 2D images from the original scenedataset in a 3D consistent manner. To generate high qualityresults, we restrict our method to editing color and texturewhile preserving shape. This editing image can either be atotally different object or an edited view from the originaldataset.To do this, given a 3D model (e.g. Splat or NeRF), wepropose to sample and edit a subset of the original data as apreprocessing step. Specifically, we use the Segment Any-thing Model (SAM) to find corresponding regions ofboth the editing image and the sampled views. For each re-gion of each sampled view from the original dataset, we haveto find the best corresponding region from the editing image to transfer style from. To find these matches, we utilizea custom heuristic which minimizes the distance betweenthese mask regions in an extracted DINO feature space.We then copy over colors by changing the hue and copy overtextures by refitting them with Texture Reformer . Toapply these updates onto the 3D model (e.g. Gaussian Splat),we then finetune it using L1 and SSIM losses for color, anda Nearest Neighbor Feature Matching (NNFM) loss fortexture.We generate results for objects and scenes in the NeRFSynthetic , MipNeRF-360 , and RefNeRF datasets and compare against color and texture editing base-lines to show qualitative improvement of our method. Over-all, our main contributions are: 1) We provide a flexible andexpressive mode of specifying edits, leveraging SAM andusing a DINO-based heuristic to match image regions to theediting image in a multiview consistent manner, and 2) Weprovide fine grained control of choosing colors and texturesfor each part of the segmented editing view.",
  ". Related Works": "There are a few works that aim to edit 3D models, and theyfall into a few categories. First are diffusion-based editingmethods, which broadly try to lift inconsistent 2D imageedits from text prompts into 3D via specialized losses. Thereare also local texture editing methods that are able to targetregions and apply textures from a source image. Finally,since our method is capable of color editing as well, thereare purely color edit methods we compare to that usuallyapply manually specified colors to images.",
  ". 2D Priors": "InstructNeRF2NeRF adapts the InstructPix2pix 2Dediting model to 3D, enabling edits in color, shape, andtexture based on text prompts. Initially applied to selectdataset images, these edits may lack multiview consistencybut achieve 3D uniformity through Iterative Dataset Update(IDU), progressively refining more dataset examples. Whilethis method supports extensive shape and color modifica-tions, it tends to fall short in result quality and detailedtexture rendering.Vox-E uses a voxel grid and diffusion model forupdates, focusing on large feature edits. It processes viewswith text-guided noise predictions to align edits, but struggleswith fine texture/color adjustments, often resulting in blockytextures or unintended area expansions.Blended-NeRF blends new objects or textures intoscenes, guided by CLIP losses to match text inputswithin a chosen 3D scene region. It modifies the scenesMLP with CLIP loss and blends colors and densities for theedits. While it achieves realistic textures, as a text-basedmethod, it faces challenges in accurately conveying complextextures or specific regions without image input.",
  ". Local Texture Editing": "S2RF introduces local texture editing for specific scenetypes, utilizing an object detection model along with SAMfor precise region masking. This method applies NNFMloss from ARF for style/texture transfer onto masked ar-eas, demonstrating the capability to apply varied textures todifferent scene parts.Semantic-driven Image-based NeRF Editing (SINE) offers a method for 3D texture editing, leveraging a prior-guided editing field combined with original views. It uses aViT extracted style features to adjust textures, enablinglocalized edits. While it supports seamless rendering bymerging template NeRF with the editing field, the processdemands 12 hours of training per scene and faces compati-bility issues with Gaussian Splats due to its unique renderingapproach.",
  ". Color Editing": "Decomposing NeRF for Editing via Feature Field Distilla-tion allows color editing of NeRFs using text prompts.It generates a feature field for selecting and altering colors in3D regions. Utilizing CLIP-LSeg and DINO as 2Dteacher networks, it learns an extra feature field integratedinto the original NeRF, applying updates through photomet-ric and feature loss functions. This approach enables soft 3Dsegmentation via a dot product between an encoded queryand the feature field, facilitating text-specified color edits in3D regions through modified rendering functions.CLIP-NeRF learns a conditional NeRF representa-tion that aims to separate appearance and shape information.CLIP embeddings are passed through appearance and shapemappers which extract the respective information and ad-ditively combine them with the conditional NeRF. Thesemapping layers are trained along with the NeRF via a CLIPsimilarity loss iterating over randomly sampled NeRF views.This method primarily edits color, but also shows minorshape changes on objects like cars and chairs.RecolorNeRF aims to decompose the scene into a setof pure-colored layers, and editing that pallet to change thecolor of the scene. This method achieves aesthetic results,but cannot distinguish between two different objects thathave the same color in a scene. ProteusNeRF is ableto rapidly edit the color of a NeRF by selecting a maskedregion and change its color, propogating the change into3D. ICE-NeRF finetunes the NeRF with the desiredcolor edits, introducing techniques to preserve multiviewconsistency and avoid unwanted color changes.",
  ". Concurrent Work in Gaussian Splat Editing": "Recently many methods have emerged that show editingcapbilities on Gaussian Splats. One such example is Gaus-sianEditor: Editing 3D Gaussians Delicately with Text In-structions . This pipeline feeds the user prompt and scene description to an LLM to select regions of interest, and thenapplies a 2D diffusion prior to edit various views. Anothersimilarly named paper GaussianEditor: Swift and Control-lable 3D Editing with Gaussian Splatting introduces Hi-erarchical Gaussian Splatting, a technique to allow more finegrained editing via 2D diffusion priors. The user must selectpoints on the screen, and change visual aspects manually in2D for the edit to be carried over to 3D.Instruct-GS2GSEditing: Editing 3D Gaussian SplattingScenes with Instructions is based off InstructNerf2Nerfand inherits the same advantages and disadvantages of thatmethod. The authors use the same IDU method, but tunesome hyperparameters to suit Gaussian Splatting. Anotherpaper TIP-Editor: An Accurate 3D Editor Following BothText-Prompts And Image-Prompts uses LoRA to person-alize a diffusion model with the style of a reference image,and then uses this along with a user prompt to generate2D edits. These edits are additionally bounded by a userspecified region to contain edits.Overall, these methods show some interesting results onediting using 2D Diffusion priors, but sometimes suffer thequality downgrade associated with diffusion models, andare not able to trasnfer style globally from a standalone 2Dimage.",
  ". Method": ". The user supplied style image is segmented and itsmasked regions are matched with masked regions of sampled datsetviews via DINO correspondences. The color/texture is then trans-ferred to those matching regions, and the splat is edited with thisupdated dataset. Our method supports different types of 3D models, andwe primarily demonstrate it on top of Gaussian Splatting due to its favorable speed. We also implement our method ona regular NeRF framework for time comparison. Thereare two main interfaces, one for manual texture/color editingand another for automatically transferring these attributes from an example image as shown in . The processdiffers only for creating the edit image, but uses the samesegmentation, part matching, and texture/color losses acrossboth. After making changes to the edit view, or choosing theconditional image, the algorithm is run on a number of sam-pled images where the style is transferred to these randomlysampled views. Color is naturally multiview consistent sinceonly the hue is changed, and the underlying grayscale is pre-served, so standard L1/SSIM loss is used to push the colorupdates. Since this is not the case for transferred textureupdates, we employ the Nearest Neighbor Feature Matching(NNFM), originally proposed in ARF , to make the tex-ture change 3D consistent. Texture changes are done withthis NNFM loss in a first round of iterations, and then coloris changed with L1/SSIM losses in a second round, sincewe find that more vivid color is transferred via standard lossfunctions than NNFM.",
  "Gaussian Splatting": "Gaussian Splats is a recent 3D scene representationtechnique that allow for faster training and rendering. Thescene is represented as a collection of 3D Gaussians, whichare defined by position, covariance, opacity, and color. Agiven view is rendered from a differentiable rasterizer, whichreturns any given 2D view of this set of gaussians givenstandard NeRF-style viewing parameters. Since the raster-izer is differentiable, edits to the returned 2D image arebackpropagated, and make the appropriate changes to theunderlying gaussian representation. This method has thebenefit of quick training time and producing more realistictextures than NeRFs in many cases. We base our method offof Gaussian Splats, but the technique works for NeRFs aswell.",
  "SAM": "The Segment Anything Model (SAM) is a zero shotimage segmentation model. It consists of a ViT encoder andmask decoder that produces the mask of each instance. Thisdecoder is conditioned on either specific points, a box, ortext to produce various masks. For the purpose of separatingall parts of an object or scene, prompting with a grid ofpoints is most effective. This produces distinct maskedregions, which can be used as editing regions to apply newcolors and textures.",
  "Self-Distillation with No Labels (DINO) is a self-supervised technique for training Vision Transformers(ViTs) where a single ViT acts as both student and": "teacher. The student model learns from input data using stan-dard methods, while the teacher model updates its weightsthrough an exponential moving average of the studentsweights, ensuring stable updates. This process encouragesthe student to learn generalizable and robust features by pre-dicting the more stable teacher outputs. DINO facilitateseffective ViT training without labeled data, leading to modelsthat better focus on relevant image parts. The methods abil-ity to identify pixel-wise image correspondences is furtherdemonstrated in .",
  "Texture Reformer": "Texture Reformer introduces View-Specific TextureReformation (VSTR) for transferring textures between im-age regions. By utilizing source and target semantic masksalong with VGG feature extraction , it overlays texturesfrom one area to another, adjusting to the new shapes con-tours. The technique employs patch grids and convolutionfor texture application, with statistical refinements ensuringrealistic integration within the targeted masked regions.",
  "NNFM Loss": "Artistic Radiance Fields (ARF) offers a method forinfusing 3D NeRF scenes with style elements from 2D im-ages. By processing 2D scene views alongside style imagesthrough a VGG-16 encoder, it applies a novel NNFM lossto match local features between the two, diverging fromtraditional Gram matrix losses that blend style details glob-ally. This local matching technique ensures the preservationof texture specifics, marking a notable advancement overprevious approaches.",
  ". 2D Editing": "There are two options for editing. Firstly, we can take adifferent conditional image(s), and copy styles from all partsonto the target objects views. In this approach, we use thetexture-reformer module to bring all source textures onto asquare array, so they can be cropped to the size of the targetmasks as necessary. We also store which colors correspondwith which mask ids. Secondly, there is manual editing,where we start with any arbitrary view of the target object,and assign different styles to different regions. In both cases,we seek to generate a mapping of mask id to color/texture tofind and copy these styles to the appropriate regions in thenext steps. When editing, we can specify whether we want tocopy only the color or the texture as well. This will determinewhether we use the Texture Reformer module to extract tex-tures, and whether we use NNFM loss or L1 loss alone in thedownstream style applying steps, as opposed to sequentially.",
  ". Segmentation": "The Segment Anything Model (SAM) is an encoder-decoder model that can be prompted with several grid pointsto make masks of most identifiable parts of an image. Weuse SAM to segment both the edit image and sampled viewsinto their component parts, since it is the state of the art atthis task and runs fairly quickly. Since we provide an optionto manually specify which masks to edit, in our mask pro-cessing step, we allow users to specify a limit of N masks forsimplicity. We choose the largest N-1 masks and group therest of the image into the Nth mask. This basically enablesthe user to separate the editing view into any number of partsto have control over fine grained features. We first segmentthe editing image and store those masks, and segment eachdataset view as we iterate over it.",
  ". DINO Mask Matching": "We use DINO features to find which is the best editing imageregion to copy style from for each region of each of thesampled dataset views. Extracted DINO features have beenshown to find corresponding pixels between two images .We use a similar feature extraction technique, but create acustom heuristic to measure the distance between two masksin the DINO feature space. First, we extract the DINOfeature vector for each of the masks in the editing image, andstore this information as it does not change. When iteratingover a given dataset image, we extract DINO features aftersegmentation, and find the best matching region with thefollowing heuristic:",
  ". Texture Reformer": "We use the texture reformer module to copy texturesfrom the editing image. Since we will be obtaining maskedregions (see next section), we can use that for our sourcesemantic map, and the editing image itself for our sourcetexture. For our target semantic mask, we can use the entireblank image of the same size. When applying textures tovarious different regions of the different dataset views, wecan simply crop this full sized texture to shape. The reasonwe do this, rather than mapping the texture to each individualsemantic mask for each view, is because we find empiricallyit does not matter, and time is saved by just doing this once.Running Texture Reformer per view does not lead to anymore naturally view consistent results without NNFM loss.This is done after the edit image is segmented in .",
  "Applying Color": "Applying a color change to a region of a view image is donein the HSV representation. In this representation the image issplit up into the three channels of Hue, Saturation, and Value,rather than the standard RGB. The hue controls what coloris expressed, the saturation controls how strong the color is,and the value controls how light or dark it is. The grayscaleof an image, which contains the texture of the original viewis the value. Therefore, to edit the color in a given targetregion, we copy over the average hue and saturation valuesof the source region, while leaving the value alone. If theuser wants to brighten or darken a view overall, that canalso be achieved by shifting the value field by a specifiedconstant.Once this edit is made on the views, we use this as thedata for training the edited 3D model. The loss functionused is standard Structural Similarity Index (SSIM) and L1interpolation used to train Gaussian Splatting:",
  "Applying Texture": "For texture, we either have manually specified a texture froma pattern image, or we automatically extracted texture froma matching region and expanded it as a pattern image withtexture reformer. In either case we can crop this imagesized texture to fit the mask region and add it. The texturewill have the same pattern cropped to different viewpoints,and so is not 3D consistent. However, we again train a 3Dmodel using this data and the NNFM loss, and over severaliterations this will blend the image to be so. We find thatusing NNFM alone causes degradation in image quality andartifacts, and so we regularize it with the original GaussianSplat training loss:",
  ". Experiment Details": "We discovered in our ablation study in , that sam-pling around 20% of the images in the dataset for editing issufficient for a good quality result. The color editing stage isrun for around 2000 iterations, and the texture editing takes3000 iterations to fully stylize the Gaussian Splat like the editing inputs. For the L1+SSIM portion of the loss, we usethe Gaussian Splat implementation default interpolation. Fortexture loss, we find that adding 50% of the original loss asa regularizer to the NNFM loss works best. . Comparing different dataset sampling rates for turningthe road to a river. Sampling 5% or 10% of images from a datasetto edit results in numerous artifacts and other degradations, andquality peaks at around 20% sampling.",
  ". Comparison of our method in local texture editing of oursand baselines": "In our method: For the ship, we selected the mask thatcorresponds to the water and indicate that the sand textureshould be applied there. For the chair, we select the backand armrests. For the mic we select the stand and for thehotdog we select the plate.In BlendedNeRF: To edit, a 3D box region and a corre-sponding prompt are required. For the ship, the box covers the xy plane, extending upwards to the ships start, with theprompt dunes of sand. The chairs box is above the seatcushion, including the armrests, using a wooden chair. Themics stand is boxed with a wooden stand. For the hotdog,the box spans the xy plane, extending vertically to the hot-dogs start.In Vox-E: We should specify our prompt as what we wantthe final image to be, as this is the input to the diffusionguidance. For the ship we use a ship in sand. For the chairwe use a chair with a wooden back. For the mic we usea microphone with a wooden stand, and for the hotdog weuse a hotdog on a blue granite plate.",
  "Analysis": "Vox-E allows users to use text prompts for editing objectvoxel grids but has notable limitations. The text prompt cantfocus edits on specific areas, leading to unwanted changes,such as unnecessary coloring in the chair and microphoneexamples as in . It also struggles with texture rep-resentation, producing rough, pixelated textures that dontmatch the intended edits, as seen in the plate and ship ex-amples. Additionally, while Vox-E can change shapes, thissometimes results in unintended alterations. BlendedNeRF can produce high-quality visuals but suf-fers from unintended artifacts and shape distortions due to itsedit region being box-shaped, making precise edits difficultin intertwined areas. This issue is evident in examples likethe ship, where sand spills out improperly, the chair withmisplaced wooden panels, a mic with fuzzy artifacts, anda hotdog plate turned square. Unlike box-based edits, ourmask-based approach allows for more precise region modifi-cations. Additionally, BlendedNeRF struggles with texturedefinition, failing to produce detailed contours in sand orrealistic wood grain, as highlighted in the ship and woodexamples.",
  ". Showing local and correspondance based color editingacross ours and baselines": "In our method: The purple color is applied to the waterregion of the image for the ship. For the chair, drums, andplant, appropriate color regions are automatically extractedwith mask matching and applied onto the dataset views.In D.F. Fields: This method uses a text phrase for the newcolor and a filter phrase for object selection, where sim-ply naming the object works best due to D.F. Fields dif-ficulty in selecting object subparts. Adding backgroundto the filter phrase improves performance. Color promptsare specific, like purple water for the ship, brown drums,golden plant, and blue and gold chair. Excessively de-tailed prompts tend to reduce effectiveness.In CLIP-NeRF: In CLIP-NeRF, we specify a prompt as asentence of what we want to see in the result sentence. Forthe ship, this is a ship in purple water. For the chair, it is achair with blue cushions and a gold frame. For the drums isbrown drumset with bronze cymbols, and for the ficus it isplant with fall-colored leaves.",
  "DF Fields effectively changes colors across broad areas butstruggles with precision in smaller regions. This model can-": "not finely detail or recolor small parts of images, exemplifiedby unchanged colors in the instruments 8 and difficulty indifferentiating the ship from its surrounding water. Addition-ally, trying to specify exact RGB colors through text oftenleads to discrepancies between intended and actual colors.CLIP-NeRF produces attractive results but can deviatefrom precise prompts. For instance, a request for a ship inpurple water resulted in both the ship and a bucket beingcolored purple instead of just the water. This indicates a chal-lenge with the precision of text-based editing compared todirect mask and color adjustments. Other examples includeunintended additions like a gold pattern on a chair meant tobe simple, minimal changes to drum colors, and an unex-pectedly colored pot on a plant, showcasing the limitationsin accurately reflecting user intentions through text/vectorembedding.",
  ". Inherited Limitations": "We inherit a few limitations from pretrained componentsutilized in our method. SAM, which performs segmentationon selected views can sometimes fail to generate fine grainedmasks from certain angles, lumping together two parts of anobject. When this happens, an edit that would have normallybeen constrained to one area in that particular view cansometimes bleed into other areas. This is rare in most scenes,but can occur in complicated scenes, or object sections withill-defined boundaries. Also, using SAM to select maskedregions means that our method cannot perform edits to the3D geometry of the object.The NNFMloss function is great at copying texture andoverall style from a source area to destination area, but makesthe result unreflective. This is seen in , where theoriginal surface of objects was reflective, and applying a newtexture unintentionally overwrote those effects. Likewise, ifthe edit images texture contains any such light scatteringeffects, these are not carried over onto the Gaussian Splat.",
  ". Computation Time": "We find that our method performs much faster when imple-mented on Gaussian Splats, showing that color and stylelosses can be applied faster on this representation. In ourexperiments, running our method on top of standard NeRFstook more iterations to transfer style, and each iteration alsoran slower, showing that it is easier to change color and tex-ture on a Gaussian Splat. We include timings for the otherbaselines we tested in , along with the timing forSINE from that paper.",
  ". User Study": "In the user study, we seek to understand how users perceiveour method as compared with leading baselines. Since thetext prompts we chose for each of these baselines detailedin Sections 4.2 and 4.3 are a faithful representation of theedit we intend to express with the conditional image weuse for our method, we can compare against these baselinesaccurately. We solicit feedback on the user preferences from38 people, and asked about their expertise with generativemodels. The ratings were requested via a Google Form. Tenwere familiar with generative computer vision and twenty-eight were not.",
  ". Percent of users who preferred each method for textureediting": "In all cases, our method was favored by most users. For theship example, it received high preference due to Blended-NeRFs sand spilling out of bounds and Vox-Es unrepresen-tative grainy texture. In the chair scenario, 63.2% preferredour method, noting it provided a reasonable texture, whereasBlendedNeRF was a close second. Vox-Es inaccuracies,such as miscoloring parts of the mic, were noted by attentive users. Our plate design also won majority preference, withBlendedNeRFs version turning square and Vox-E erasingcondiments. Similarly, our method was the top choice for themic, as BlendedNeRFs edits introduced unwanted artifacts.",
  "Color": "Here we test against Distilled Feature Fields and CLIP-NeRF,with three global style transfer examples from conditionalimages, and one local color editing example as in .For the global color transfer, we explain the concept of cor-respondence in simple English, by asking the user to selectthe result which takes on the color scheme of the edit imageapplied onto the original. For the local color transfer on theship example, we mention that the goal is to turn the waterin the image purple.",
  ". Percent of users who preferred each method for colorediting": "In color editing, our method again won over 60% of userpreference in each scenario. For the ship, our recolor wasfavored as Distilled Feature Fields partially recolored thetray border and CLIP-NeRF mistakenly colored the ship, notthe water. Our chair was preferred for its accurate gold frameand blue cushion, matching the throne, though CLIP-NeRFalso attracted 21.1% of users with its intriguing, albeit un-intended, result. Both DF Fields and CLIP-NeRF struggledwith coloring the drums correctly, leading to low preference.For the plant, DF Fields failed to alter its green color, whileCLIP-NeRFs reddish fall colors caught some interest, butoverall, our method was seen as most accurately reflectingthe intended edits.",
  ". Conclusion": "In this work, we have introduced a robust and flexible methodfor editing color and texture of 3D images and scenes. Weprovide interfaces to copy style from an edit image or manu-ally specify changes, enabling creative appearance editingfor a variety of applications. Our key innovation, DINO-based mask matching, runs quickly and contains edits todiscrete regions, leading to higher quality than other meth-ods. Future work could explore how to make 3D consis-tent shape changes to these discrete regions in addition tocolor and texture, without compromising on resulting 3Dscene quality like most other current methods do. Over-all, we showcase our methods unique input expressivityand resulting 3D model quality on a variety of objectsand scenes, proving it is well suited for creative applica-tions. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,Zesong Yang, Hujun Bao, Guofeng Zhang, and ZhaopengCui. Sine: Semantic-driven image-based nerf editing withprior-guided editing field. In The IEEE/CVF Computer Visionand Pattern Recognition Conference (CVPR), 2023. 3",
  "Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-structpix2pix: Learning to follow image editing instructions.In CVPR, 2023. 2": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the International Conference on Computer Vision(ICCV), 2021. 2, 3, 4, 5 Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, XiaofengYang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu,and Guosheng Lin. Gaussianeditor: Swift and controllable3d editing with gaussian splatting, 2023. 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. ICLR, 2021. 3, 4",
  "Ori Gordon, Omri Avrahami, and Dani Lischinski. Blended-nerf: Zero-shot object generation and blending in existingneural radiance fields. arXiv preprint arXiv:2306.12760, 2023.2": "Ayaan Haque, Matthew Tancik, Alexei Efros, AleksanderHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing3d scenes with instructions. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, 2023. 2 Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu,Vitor Guizilini, Thomas Kollar, Adrien Gaidon, Zsolt Kira,and Rares Ambrus. Neo 360: Neural fields for sparse viewsynthesis of outdoor scenes. 2023. 7"
}