{
  "Abstract": "In noisy label learning, estimating noisy class posteriorsplays a fundamental role for developing consistent classi-fiers, as it forms the basis for estimating clean class pos-teriors and the transition matrix. Existing methods typi-cally learn noisy class posteriors by training a classificationmodel with noisy labels. However, when labels are incor-rect, these models may be misled to overemphasize the fea-ture parts that do not reflect the instance characteristics, re-sulting in significant errors in estimating noisy class poste-riors. To address this issue, this paper proposes to augmentthe supervised information with part-level labels, encourag-ing the model to focus on and integrate richer informationfrom various parts. Specifically, our method first partitionsfeatures into distinct parts by cropping instances, yieldingpart-level labels associated with these various parts. Sub-sequently, we introduce a novel single-to-multiple transi-tion matrix to model the relationship between the noisy andpart-level labels, which incorporates part-level labels intoa classifier-consistent framework. Utilizing this frameworkwith part-level labels, we can learn the noisy class posteri-ors more precisely by guiding the model to integrate infor-mation from various parts, ultimately improving the classi-fication performance. Our method is theoretically sound,while experiments show that it is empirically effective insynthetic and real-world noisy benchmarks.",
  "*Corresponding author": "However, obtaining accurate labeling in real-world taskstypically involves manual labeling, which is often time-consuming and costly . In contrast, massive amountsof noisy labels are readily available through web crawlers,questionnaires and crowdsourcing .To reduce the negative impact of noisy labels, variousheuristic strategies are applied to Noisy Label Learning(NLL), such as selecting reliable samples and correcting labels . Although thesemethods can train classifiers that perform well empirically,they do not guarantee a consistent classifier, meaning thatthe classifiers learned from noise-labeled data will not nec-essarily converge to the optimal one learned from clean data.To overcome this problem, various classifier-consistentmethods have been proposed, with the most successfulmethods employing loss correction procedures . The basic idea of these methods isthat, the clean class posterior P(Y |x) can be inferred fromnoisy class posterior P( Y |x) and transition matrix T(x),from the equation P( Y |x) = T(x)P(Y |x) 1. The noisyclass posteriors estimation plays a fundamental role in thisprocess, directly affecting the calculation of clean class pos-teriors while also being critical for the accurate transitionmatrix estimation .Considering that the ground-truth noisy class posterioris typically unavailable, existing methods often learn itthrough the classification tasks supervised by noisy labels. In classification, labelswill encourage the model to focus on label-related featurepart to minimize the classification loss. However, when la-bels are incorrect, models will be misled into overempha-sizing the erroneous parts that do not reflect the instance 1We define P(Y |x) = [P(Y = 1|X = x), ..., P(Y = c|X =x)] and Tij(x) = P( Y = j|Y = i, X = x), where c denotes thenumber of classes, X and Y represents the variable for clean labels andinstances, respectively.",
  "(b)": ". Illustration of overemphasis that arises when learning noisy class posterior with classification loss (a), as well as theframework we proposed to alleviate this overemphasis (b). Class activation maps are used to visualize feature importance forestimation, where the highlighted areas (with stronger red intensity) represent the focus regions of the model. characteristics. As illustrated in a, for an instanceof a feathered monkey labeled as bird, the noisy la-bel bird will force the backbone network to focus onfeatures related to birds, such as feathers, while ignoringother important features, such as monkeys facial features,to achieve a smaller loss through backpropagation. In suchcases, the models output tends to reflect the probability ofbirds given feathers features rather than birds giventhis feathered monkey instance. This introduces signifi-cant errors in estimating the noisy class posteriors. To ad-dress this issue, a straightforward idea is to augment super-vised information with part-related labels, thus encouragingthe model to focus on disparate feature parts. This can en-hance the models ability to capture instance characteristicsand aid in estimating noisy class posteriors. This idea isconsistent with object recognition research, which revealsthat feature parts are used by humans and machines to iden-tify objects , drawing on evidence from psychologicaland physiological studies , as well as computa-tional theories and learning algorithms . Motivated by this idea, we propose a Part-Level Multi-labeling (PLM) method, which generates multiple part-levellabels to guide the models focus. As illustrated in Fig-ure 1b, by augmenting the supervised information, we at- tempt to capture distinct features of various parts, therebyrectifying the models excessive focus on specific mislead-ing parts.Specifically, to generate part-level labels, wedesign a multi-labeling approach based on instance crop-ping. As displayed in the second row of b, thisapproach partitions features into distinct parts by instancecropping, such that some of the parts do not contain exces-sively focused features. By means of this partitioning, wecan employ a network trained on noisy data to assign labelsto these parts, resulting in multiple part-level labels asso-ciated with various diverse focused regions. Subsequently,a single-to-multiple transition matrix is utilized to modelthe relationship between the single noisy label and the mul-tiple part-level labels for each instance. With this matrix,we proposed a label joint training framework that incorpo-rates both part-level and noisy supervised information intoa classifier-consistent framework. As shown in the first rowof b, this framework guides the model to focuson more diverse regions, thereby obtaining representationsthat reflect instance characteristics, ultimately enhancingthe learning of noisy class posteriors. Then, it can be usedto estimate other critical metrics such as transition matrix orserve as a component of loss correction for NLL. We exten-sively evaluated our proposed method on synthetic datasets with instances-independent and -dependent noise, as well ason real-world datasets, offering empirical evidence of its ef-ficacy in reducing errors in noisy class posterior estimationand improving performance on NLL tasks.Our main contributions are summarized as follows:",
  "We focus on a novel problem of estimating noisy classposteriors in noisy label learning, which forms the basisfor building classifier-consistent algorithms": "We are the first to note the misleading effect of incorrectlabels on noisy class posteriors, where incorrect labelsguide the model to overly focus on feature details thatdo not reflect instance characteristics. To counter this,we propose to incorporate part-related supervision by aconsistent classifier, which guides the model to integrateinformation from various parts. Extensive experiments on a variety of synthetic and real-world noisy datasets have confirmed the effectiveness ofproposed method. The method notably enhances the es-timation of noisy class posteriors and can be integratedwith various NLL methods that rely on noisy class poste-riors to boost their classification performance.",
  ". Related work": "The primary goal of NLL is to mitigate the impact of mis-labeled data in the training, fostering more robust models.In this context, various data-centric and loss-centric approaches havebeen proposed. The former involves data preprocessing toreduce the involvement of noisy data in training, while thelatter constructs robust losses to diminish the strength of thesupervised signals generated by noisy data.The first category of methods often uses heuristic algo-rithms to reduce the side effects of error data, such as se-lecting reliable samples , correcting la-bels , reweighting samples , smoothinglabels and noise reduction . However, the use ofheuristic algorithms makes these methods prone to over-fitting noisy data. To address this issue, the second cate-gory of methods aims to introduce robust loss functions toweaken the supervised signals from mislabeled data, avoid-ing the reliance on heuristic algorithms. The most promi-nent approach within this category is loss correction meth-ods . These methods typi-cally use a transition matrix to correct the loss to guaranteethat the trained classifier will converge to the optimal classi-fier learned from the clean data (i.e., statistically consistentclassifier). The critical task of this method is to estimatethe transition matrix and thus infer the clean class posteriorfrom the noisy data.In these methods, the noisy class posterior is often uti-lized to estimate important parameters, such as calculatinglosses to obtain low-loss samples and estimating the tran- sition probabilities to obtain a transition matrix, and eveninferring clean class posteriors with a consistent classifier.Unfortunately, the estimation of these important metrics areall affected by the estimation errors of noisy class posteri-ors, which is considered a unresolved bottleneck of classi-fication performance . While some label processingmethods implicitly reduce overemphasis on spe-cific parts by decreasing the models fitting to noisy labels,they usually apply the same treatment to all labels, lackingan effective guidance to help the network integrate infor-mation from other parts. Hence, these methods struggle toaddress the issue where incorrect labels mislead the networkto focus on features that do not reflect the instance charac-teristics.",
  ". Problem setting": "Let D denote the joint probability distribution of a pair ofrandom variables (X, Y ) X C, where X and Y rep-resent the random variables associated with instances andtheir corresponding clean labels, respectively. In this con-text, X denotes the instance space, while C = {1, . . . , c}represents the label space with c denoting the number ofclasses. Given a labeled training dataset D = {(xi, yi)}ni=1with size n, where each example (xi, yi) is drawn indepen-dently from the probability distribution D, the classificationtask aims to learn a classifier f : X C that maps each in-stance xi to its corresponding label yi based on the trainingdata D. However, obtaining samples from the distributionD in real-world tasks presents a significant challenge, sincethe observed labels are often corrupted by noise. Let Ybe the random variable of noisy labels and D be the dis-tribution of a pair of random variables (X, Y ). The goalof NLL is to learn a robust classifier from noisy sample setD = {(xi, yi)}ni=1 independently drawn from D, which canassign the clean labels for the instances.",
  ". Preliminary: NLL with consistent classifiers": "For building a consistent classifier, the mainstream meth-ods using a transition matrix, which can relate the ran-dom variables of Y and Y , to infer the cleanclass posteriors from noisy class posteriors. Specifically,the noisy class posterior vector P( Y |x)=[P( Y=1|x), . . . , P( Y = c|x)] can be transformed into the mul-tiplication of the clean class posterior vector P(Y |x) =[P(Y = 1|x), . . . , P(Y = c|x)] and the noise transi-tion matrix T(x) Rcc. Here, T(x) denotes the tran-sition matrix of instance x, and the element of the i-th rowand j-th column of T(x) is defined by Tij(x) = P( Y =j|Y = i, x). Specifically, according to Eq. (1), there isP( Y |x) = T(x)P(Y |x). It means that the NLL taskcan be resolved by estimating the transition matrix T(x)",
  "(1)": "Thus, in the context of constructing a consistent classifierwith a transition matrix, the accuracy of clean class posteri-ors directly relies on the estimation of noisy class posteriors.Besides, the estimation of the transition matrix, in manymainstream methods, also depends on noisy class posteri-ors . Therefore, the estimation of noisy class posteriorsis crucial in building classifier-consistent algorithms. To re-duce errors in estimating noisy class posteriors, this paperintroduces a training framework with a single-to-multipletransition matrix. It can serve as a component in loss correc-tion methods to build a classifier-consistent NLL algorithm..4 provides a detailed discussion on the label jointtraining framework.",
  ". Part-level multi-labeling": "This section provides a detailed explanation of the processof generating part-level labels through instance cropping, asillustrated in the second row of b. Specifically, foreach instance, we obtain a set of cropped parts by apply-ing K times crop, each capturing different features. Then, anoisy classifier trained on the raw noisy data is employed toassign labels to these parts, resulting in part-level labels thatreflect diverse features. Since many parts do not contain theoveremphasized features after cropping, the network can at-tend to other features when assigning labels to them, ratherthan being restricted by the features that are strongly corre-lated with the noisy labels. This leads to more informativelabels that better reflect the comprehensive information ofthe instance.Formally, for an instance xi, the goal of instancecropping is to generate a set of sub-instances Si={si1, . . . , siK}, where sij signifies the j-th part of xi.Given a cropping number K, the results of instance crop-ping are influenced by the chosen cropping operation,which can be guided by different criteria, such as user-specified location, saliency, or attention maps. In the ex-periments conducted in this paper, we use image data as anexample and crop five equally-sized parts uniformly fromthe four corners and the central position.After the in-stance cropping, we employ a labeling classifier networkf l trained from noisy data to label each part. Part-level la-bels are a form of multi-label, and thus can be decomposedinto c independent binary labels, each corresponding to oneof the possible labels in the label space. Thus, the part-level labels of instance xi can be represented as a vector",
  ". Label joint training framework": "To leverage the part-level labels for estimating the noiseclass posterior, we propose a label joint training frame-work. This framework uses noisy labels and part-level la-bels jointly to estimate the noise class posterior, based on asingle-to-multiple transition matrix that models how a sin-gle noisy label relates to multiple part-level labels.As discussed in the previous section, the part-level labelsof an instance can be interpreted as a multi-label. Therefore,the random variable associated with the part-level labels canbe represented by a joint distribution (Y 1, , Y c), whereY j {0, 1} represents the random variable associated withthe j-th label component of the part-level labels. Then, sim-ilar to the discussion in .2, the part-level labels canbe linked to the noisy labels as follows:",
  "i=1Uij(x)P( Y = i|X = x),(3)": "where Uij(x) = P(Y j = 1| Y = i, X = x) is the ij-thentry of matrix U(x) Rcc, and U(x) denotes the tran-sition matrix of instance x. In this way, we implementedthe joint integration of part-level and noisy labels within aclassifier-consistent framework.Joint training with single-to-multiple matrix. For esti-mating single-to-multiple transition matrix and noisy classposterior, we construct a single-to-multiple transition ma-trix estimation network gu : X Rcc and a noisy classposterior estimation network ge : X Rc. We perform ajoint training of gu and ge by minimizing the following lossfunction:",
  "(c(ge(x), y) + m(gp(x), y)),(4)": "where network gp(x) = gu(x)ge(x), c and m represent aclassification loss (e.g., the cross entropy loss) and a multi-label classification loss (e.g., the binary cross-entropy loss),respectively. By minimizing the loss function c, the out-put of network ge is adjusted to fit the noisy class posteriorP( Y |x). Furthermore, by minimizing m, gu(x)ge(x) willoutput part-level multi-labels. Therefore, based on Eq. (3), gu(x) is compelled to model the single-to-multiple tran-sition matrix. As the estimation of gu becomes more ac-curate, the noisy class posterior estimation of ge also im-proves by minimizing m. By employing the joint train-ing approach, we ensure that the noisy class posterior es-timation benefits from the supervision of noisy labels andpartial-level labels as Eq. (3). The richer part contextual su-pervision prevents the overemphasis by ge on specific part,thus avoiding overconfidence. Here, we introduce a novelmatrix, and its identifiability is discussed in Appendix B.Classification with consistent classifier. Beyond enhanc-ing the efficiency of noise class posterior estimation to im-prove the estimation precision of other crucial metrics, ourmethod can also function as a classifier-consistent compo-nent to be incorporated into a consistent classifier. From Eq.(1) and Eq. (3), we can formulate the relationship betweenthe part-level labels and the clean labels as follows:",
  ". Experiment setup": "Datasets. We evaluated the performance of our proposedmodel using both synthetic and real-world datasets. Thesynthetic datasets included MNIST , CIFAR-10 and CIFAR-100 , which we generated by applying threetypes of noise: symmetry flipping , pair flipping and instance-dependent label noise (IDN) . The MNISTdataset consists of 60,000 training images and 10,000 testimages, representing handwritten digits from 0 to 9. TheCIFAR-10 and CIFAR-100 datasets consist of 50,000 train-ing images and 10,000 test images, representing 10 and 100object classes, respectively. We also used Clothing1M ,a large-scale dataset of 1M real-world clothing images withnoisy labels for training and 10k images with clean labelsfor testing. The dataset also provides 50k and 14k cleanlabels for training and validation, but we did not use themfollowing the settings of VolMinNet . For model selec-tion, we randomly sampled 10% of the noisy training dataas validation sets from each dataset.Backbone and implementation details. For MNIST, weuse a Lenet as backbone network. The Stochastic Gra-dient Descent (SGD) optimizer is used to train the network,",
  "with an initial learning rate 5 102, weight decay 104": "and momentum 0.9. For CIFAR-10, we use a ResNet-18 as backbone network. The SGD optimizer is used totrain the network, with an initial learning rate 102, weightdecay 102, momentum 0.9. We use the cosine learningrate decay strategy for MNIST and CIFAR-10. For CIFAR-100, we use a ResNet-32 as backbone network. SGDoptimizer is used to train the network, with an initial learn-ing rate 5 102, weight decay 103, momentum 0.9.The learning rate is divided by 100 after the 40-th epoch.We apply random horizontal flips and random crops of size3232 pixels after adding a 4-pixel padding on each side forboth CIFAR-10 and CIFAR-100 datasets. For all syntheticdatasets, the labeling network and estimation network areboth trained for 50 epochs with batch size 128 with sameparameters. For clothing1M, we use a ResNet-50 pre-trained on ImageNet as backbone network. SGD optimizeris used to train the network, with an initial learning rate102, weight decay 103, momentum 0.9. The learningrate is divided by 10 after the 10-th epoch and the 20-thepoch. We resize the image to 256 256 pixels and applyrandom crops of size 224 224. The labeling network andestimation network are both trained 30 epochs with batchsize 128, and we undersample the noisy labels for eachclass in every epoch to balance them like VolMinNet.To generate sub-instances, we select five parts uniformly atthe four corners and one central position. The part sizes are22 22 pixels for MNIST, 25 25 pixels for CIFAR-10and CIFAR-100, and 179179 pixels for Clothing1M. Ad-ditionally, we discussed and conducted experiments on var-ious instance cropping approach in Appendix D. The codeare available at",
  ". Noisy class posterior estimation": "We compared PLM with the CE approach in terms of theestimation error of noisy class posteriors across diversedatasets and noise rates. This comparison is pertinent asthe previous loss correction methods predominantly utilize the CE loss to estimate the poste-rior probability of noisy labels.For parity in compari-son, we implemented both methods using the same back-bone network as .1. The estimation error is calcu-lated as the l2-distance between the ground-truth and theestimated noisy class posteriors of on the pseudo-anchordataset, where the ground-truth originates from the noisegeneration process. Specifically, we first train a model withclean labels to select high-confidence samples as pseudo-anchor dataset. Then we assume that a pseudo-anchor sam-ple (x, i) satisfies P(Y = i|X = x) = 1, thereby theground-truth noisy class posterior of (x) can be calculatedas P( Y |X = x) = P( Y |Y = i, X = x). Thus, on thesynthetic datasets, noise class posteriors can be obtainedthrough the defined noise flipping probability P( Y |Y =",
  ". Mean estimation errors of noise class posterior for 5 trials on Mnist, CIFAR-10 and CIFAR-100. The error bars of standarddeviation are shaded in each plot. The lower the better": "i, X = x). This setting are similar to the Dual-T .We conducted these experiments across noise rates of[0.1, 0.2, 0.3, 0.4, 0.5]. As depicted in , our methodand the comparison methods demonstrated similar trendsin the estimation error of noisy class posterior probabili-ties. However, the estimation error of PLM is consistentlysmaller than that of the CE at different noise rates, espe-cially at higher ones. The results indicate that our methodcan provide estimations that are closer to the true noisy classposterior probabilities. The visualization in Appendix E canalso serve as evidence for this result.",
  ". Comparison for classification": "To compare the accuracy of the NLL, we first estimate thenoise transition matrix using the same approach as Forward.Subsequently, we train a single-to-multiple transi-tion matrix estimation network using the method proposedin this paper. Afterward, we fix the transition matrix es-timation network and train the classification network witha method like Forward, following the label transition rela-tionship in Eq. (5) and the training objective in Eq. (4).Here, we use the average of the 10 estimated anchors pre- dicted probabilities as a row in the transition matrix, andtrain the classification network with the same parametersas the estimation network in .1. We compare theclassification accuracy of PLM with the following baselinemethods: (1) Decoupling , which trains two predictorsand updates them only on examples that they disagree on;(2) Co-teaching , which trains two deep networks andexchanges the examples with small loss for network up-dating; (3) Forward , which corrects the training lossby the class flipping transition matrix; (4) T-Revision ,which estimates transition matrix with a slack variable tocorrect training loss; (5) Dual-T , which estimates tran-sition matrix by factorizing the original transition matrixwith an intermediate class; (6) DMI , which handles la-bel noise by using information-theoretic loss based on thedeterminant of the joint distribution matrix; (7) VolMin-Net , which estimates the matrix that incorporates theminimum volume constraint into the label-noise learning;(8) Class2Simi , which transforms noisy class labelsinto noisy similarity labels to reduce the noise rate. For afair comparison, we implement all methods with the samebackbone network and default parameters on each dataset . The average classification accuracy and standard deviation (expressed in percentage) across five trials on the MNIST, CIFAR-10,and CIFAR-100 datasets under various synthetic noisy label settings. The best classification accuracy is indicated in bold.",
  "MNISTCIFAR-10CIFAR-100Sym-20%Sym-50%Sym-20%Sym-50%Sym-20%Sym-50%": "Decoupling97.58 0.1695.66 0.1787.11 0.2477.62 0.3069.71 0.3448.30 0.68Co-teaching96.35 0.1092.01 0.2478.09 0.9167.16 0.3244.89 0.6332.19 0.90T-Revision98.86 0.0498.41 0.0889.41 0.1883.68 1.0761.14 0.6940.01 0.90Dual-T98.82 0.1798.24 0.1489.79 0.4077.97 1.9165.61 0.3950.07 1.56DMI98.87 0.0996.96 0.6884.65 0.8370.62 3.7351.89 2.2432.02 1.36VolMinNet98.82 0.1298.13 0.1590.19 0.1784.09 0.6867.70 0.9957.99 0.40Class2Simi99.11 0.0698.33 0.2684.92 0.5070.02 3.2749.78 1.0532.02 1.33Forward98.47 0.1997.78 0.2085.49 1.1274.33 1.3158.70 0.7339.82 2.15PLM99.32 0.0298.96 0.0791.10 0.2485.08 0.1669.54 0.2360.44 0.28",
  "Pair-20%Pair-45%Pair-20%Pair-45%Pair-20%Pair-45%": "Decoupling97.37 0.1684.01 1.9088.93 0.3870.55 2.2966.65 0.2743.82 1.75Co-teaching95.38 0.2989.17 0.2779.18 0.4468.58 0.6845.78 1.0228.98 0.42T-Revision99.07 0.1096.42 4.4191.16 0.1681.88 13.3060.56 0.6649.44 1.64Dual-T98.86 0.0992.47 5.9189.59 1.3474.39 4.3071.08 0.2752.25 3.28DMI98.81 0.1396.45 0.5487.66 0.6980.30 3.3843.63 0.8632.22 3.42VolMinNet99.11 0.0899.10 0.0891.55 0.1386.12 1.2671.65 0.6261.21 2.98Class2Simi98.97 0.1298.33 0.2687.27 0.3175.47 6.0953.39 0.9536.04 2.31Forward98.82 0.1591.02 10.8189.33 1.0969.99 9.7960.12 0.3337.99 0.37PLM99.40 0.0399.39 0.0592.84 0.1791.80 0.6372.67 0.3261.90 1.82 . The classification accuracy (expressed in percentage) on the Clothing1M dataset. Comparative baseline method results are quotedfrom PTD and VolMinNet since we employed the same experimental setup. The best classification accuracy is indicated in bold.",
  "IDN-20%IDN-30%IDN-40%IDN-50%": "Co-teaching88.43 0.0886.40 0.4180.85 0.9762.63 1.51DMI89.99 0.1586.87 0.3480.74 0.4463.92 3.92PTD89.33 0.7085.33 1.8680.59 0.4164.58 2.86TMDNN88.14 0.6684.55 0.4879.71 0.9563.33 2.75MEIDTM91.38 0.3487.68 0.2682.63 0.2472.17 1.51Forward89.62 0.1486.93 0.1580.29 0.2765.91 1.22PLM91.41 0.1788.60 0.5383.98 2.5376.87 1.59 by the PyTorch and run all experiments on the NVIDIARTX 3090 GPUs. It should be noted that we did not com-pare our method with some state-of-the-art methods, suchas DivideMix and NCR.These methods incor-porate multiple advanced methods (e.g., semi-supervisedlearning, contrastive learning and complex data augmenta-tion, etc.) to improve their performance under noisy labels,while PLM is solely focused on reducing the error of es-",
  "timating noisy class posterior to benefit the NLL, negatingthe need for additional techniques. Thus, a direct compari-son would not present a fair assessment": "In , we report the classification accuracy of PLMand all baseline methods, under varying noise generationmethods and rates. Here, we use Sym- and Pair- to de-note the symmetry flipping and pair flipping methods withnoise rates of , respectively.Compared to other base-line methods, PLM shows superior classification accuracy.We executed five experiments on synthetic noisy datasetswith distinct random seeds, introducing a higher degree oflabel randomness. Despite varying data generations, ourmethod demonstrates robust performance. Our method out-performed the comparative methods in all metrics.It isworth noting that although we did not combine PLM withthe latest loss correction methods in this study, the PLMmethod still achieved competitive performance and signifi-cantly improved the classification performance of the modelunder high noise rates. At the same time, our method onlyexhibits a linear increase in computational time, and we pro- . The average classification accuracy and standard deviation (expressed in percentage) across five trials on the CIFAR-10 andCIFAR-100 datasets.The better classification accuracy is indicated in bold.",
  "VolMinNet90.19 0.1784.09 0.6891.55 0.1386.12 1.2667.70 0.9957.99 0.4071.65 0.6261.21 2.98PLM-V91.75 0.1984.10 0.5593.40 0.2386.91 1.0270.95 0.4962.37 0.2974.55 0.1264.05 0.53": "vide relevant discussions in Appendix C. The results indi-cate that PLM can serve as an effective auxiliary componentto improve the robustness of the model to label noise.Additionally, we report the results on the Clothing1Mdataset in , utilizing the same dataset settings asVolMinNet. Our proposed method demonstrates superiorperformance over the compared baseline methods on theClothing1M dataset, reflecting a 3.91% enhancement rel-ative to the Forward method. Additionally, we reported theperformance on the real-world Animal-10N dataset inAppendix H. These results of real-world datasets can be at-tributed to the PLM approach, which models instance in-formation at the part-level, thereby bolstering the modelsrobustness against instance-dependent noise.To furtherdemonstrate this advantage, we conducted experiments inIDN synthetic noise in . Here, we adopted Forwardsapproach to estimate the noise transition matrix to evalu-ate how PLM improves the robustness to IDN using thismethod, and employ a backbone network in alignment withthe experimental and network settings of PTD . We com-pare with the following baseline methods: (1) Co-teaching; (2) DMI ; (3) Forward ; (4) PTD , whichuses a part-based noise transition matrix estimation tech-nique; (5) TMDNN , which uses deep neural networksto estimate the transition matrix by exploiting Bayes opti-mal labels; (6) MEIDTM , which estimates IDN transi-tion matrix by formulating a manifold assumption. Amongthem, PTD, TMDNN, and MEIDTM are methods that aredesigned for the IDN modeling. The results demonstratesthat PLM achieves competitive performance without utiliz-ing IDN modeling techniques.Learning with different noise transition matrices.Toassess the efficacy of our proposed method in combina-tion with various noise transition matrix estimation tech-niques, we present classification experiment results on theCIFAR-10 and CIFAR-100 datasets in . Specifically,we merge PLM with the matrix estimation techniques out-lined in Forward , Dual-T , T-Revision , andVolMinNet (denoted as PLM-F, PLM-D, PLM-R, andPLM-V). As both original Forward and T-Revision methods filtered the highest-confidence portions of samples, theirmethods may not be universally applicable . Therefore,to assess PLMs performance in a more general contextwhen combined with various transition matrices, we utilizethe highest probability samples in each category as anchorsfor Forward, T-Revision, PLM-F, and PLM-R. For PLM-R,we also integrated the reweighting strategy outlined in T-Revision , detailed in Appendix A. illustratesthat our approach attains increased accuracy while signifi-cantly reducing biases. The results suggest that PLM canserve as a component to enhance the robustness of existingclassifier-consistent methods. Furthermore, experiments fortransition matrix in Appendix G indicate that PLM can helpreduce the matrix estimation error.",
  ". Conclusion and limitation": "Estimating the noisy class posterior accurately is criticalfor noisy label learning.In this paper, we introduces apart-level multi-labeling method aimed at augmenting su-pervised information, thereby reducing the estimation er-ror of estimating noisy class posterior.By introducinga single-to-multiple transition matrix, we incorporate thepart-level supervised information derived from cropped in-stances into a classifier-consistent framework, effectivelymitigating overemphasis. Extensive experiments validatethe robust performance of our method, both for estimatingthe noisy class posterior and for noisy label learning as acomponent of loss correction. One significant limitation ofthis study is that cropping original instances with a uniformcriterion for labeling may be too simplistic. Future workwill involve a detailed exploration of the theories and prac-tices associated with instance representation learning, withthe objective of formulating a more appropriate croppingcriterion for noisy label learning.Acknowledgments.This research was partially sup-ported by the National Key Research and DevelopmentProject of China No.2021ZD0110700,the Key Re-search and Development Project in Shaanxi ProvinceNo.2023GXLH-024,the National Science Foundationof China under Grant No.62037001 and No.62250009. Shivani Agarwal, Aatif Awan, and Dan Roth. Learning todetect objects in images via a sparse, part-based represen-tation. IEEE transactions on pattern analysis and machineintelligence, 26(11):14751490, 2004. 2",
  "Irving Biederman. Recognition-by-components: a theory ofhuman image understanding. Psychological review, 94(2):115, 1987. 2": "De Cheng, Tongliang Liu, Yixiong Ning, Nannan Wang,Bo Han, Gang Niu, Xinbo Gao, and Masashi Sugiyama.Instance-dependent label-noise learning with manifold-regularized transition matrix estimation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1663016639, 2022. 1, 3, 7, 8 De Cheng, Yixiong Ning, Nannan Wang, Xinbo Gao, HengYang, Yuxuan Du, Bo Han, and Tongliang Liu.Class-dependent label-noise learning with cycle-consistency regu-larization. Advances in Neural Information Processing Sys-tems, 35:1110411116, 2022. 1, 3, 4",
  "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.On calibration of modern neural networks. In Internationalconference on machine learning, pages 13211330. PMLR,2017. 3": "Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, IvorTsang, Ya Zhang, and Masashi Sugiyama. Masking: A newperspective of noisy supervision. Advances in neural infor-mation processing systems, 31, 2018. 3 Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, MiaoXu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with ex-tremely noisy labels. Advances in neural information pro-cessing systems, 31, 2018. 3, 5, 6, 8 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Identity mappings in deep residual networks. In ComputerVisionECCV 2016: 14th European Conference, Amster-dam, The Netherlands, October 1114, 2016, Proceedings,Part IV 14, pages 630645. Springer, 2016. 5 Dan Hendrycks, Mantas Mazeika, Duncan Wilson, andKevin Gimpel.Using trusted data to train deep networkson labels corrupted by severe noise. Advances in neural in-formation processing systems, 31, 2018. 1, 3 Ehsan Hosseini-Asl, Jacek M Zurada, and Olfa Nasraoui.Deep learning of part-based representation of data usingsparse autoencoders with nonnegativity constraints.IEEEtransactions on neural networks and learning systems, 27(12):24862498, 2015. 2 Yingsong Huang, Bing Bai, Shengwei Zhao, Kun Bai, andFei Wang. Uncertainty-aware learning against label noise onimbalanced datasets. In Proceedings of the AAAI Conferenceon Artificial Intelligence, pages 69606969, 2022. 1 Ahmet Iscen, Jack Valmadre, Anurag Arnab, and CordeliaSchmid. Learning with neighbor consistency for noisy la-bels. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 46724681,2022. 7 Nazmul Karim, Mamshad Nayeem Rizve, Nazanin Rah-navard, Ajmal Mian, and Mubarak Shah.Unicon: Com-bating label noise through uniform selection and contrastivelearning.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 96769686, 2022. 1, 3",
  "Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix:Learning with noisy labels as semi-supervised learning.In International Conference on Learning Representations,pages 113, 2019. 1, 3, 7": "Jichang Li, Guanbin Li, Feng Liu, and Yizhou Yu. Neigh-borhood collective estimation for noisy label identificationand correction. In Computer VisionECCV 2022: 17th Eu-ropean Conference, Tel Aviv, Israel, October 2327, 2022,Proceedings, Part XXIV, pages 128145. Springer, 2022. 1,3 Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.Gradient descent with early stopping is provably robust tolabel noise for overparameterized neural networks. In Inter-national conference on artificial intelligence and statistics,pages 43134324. PMLR, 2020. Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu.Selective-supervised contrastive learning with noisy labels.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 316325, 2022. 3",
  "E Wachsmuth, MW Oram, and DI Perrett. Recognition ofobjects and their component parts: responses of single unitsin the temporal cortex of the macaque. Cerebral Cortex, 4(5):509522, 1994. 2": "Yikai Wang, Xinwei Sun, and Yanwei Fu. Scalable penalizedregression for noise detection in learning with noisy labels.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 346355, 2022. 1, 3 Shijie Wu and Xun Gong. Boundaryface: A mining frame-work with noise label self-correction for face recognition.In Computer VisionECCV 2022: 17th European Confer-ence, Tel Aviv, Israel, October 2327, 2022, Proceedings,Part XIII, pages 91106. Springer, 2022. 1, 3 Songhua Wu, Xiaobo Xia, Tongliang Liu, Bo Han, Ming-ming Gong, Nannan Wang, Haifeng Liu, and Gang Niu.Class2simi: A noise reduction perspective on learning withnoisy labels. In International Conference on Machine Learn-ing, pages 1128511295. PMLR, 2021. 1, 3, 6 Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, ChenGong, Gang Niu, and Masashi Sugiyama. Are anchor pointsreally indispensable in label-noise learning?Advances inNeural Information Processing Systems, 32:112, 2019. 1,3, 4, 5, 6, 8 Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Ming-ming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, andMasashi Sugiyama.Part-dependent label noise: Towardsinstance-dependent label noise. Advances in Neural Infor-mation Processing Systems, 33:75977610, 2020. 1, 2, 3, 5,7, 8 Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, JunYu, Gang Niu, and Masashi Sugiyama.Sample selectionwith uncertainty of losses for learning with noisy labels.arXiv preprint arXiv:2106.00445, 2021. 1 Xiaobo Xia, Bo Han, Nannan Wang, Jiankang Deng, Jia-tong Li, Yinian Mao, and Tongliang Liu. Extended t: Learn-ing with mixed closed-set and open-set noisy labels. IEEETransactions on Pattern Analysis and Machine Intelligence,2022. 1, 3, 4, 5 Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and XiaogangWang.Learning from massive noisy labeled data for im-age classification. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 26912699,2015. 5 Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang.L dmi: A novel information-theoretic loss function for train-ing deep nets robust to label noise. Advances in neural infor-mation processing systems, 32, 2019. 3, 6, 8 Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, GangNiu, and Tongliang Liu.Estimating instance-dependentbayes-label transition matrix using a deep neural network.In International Conference on Machine Learning, pages2530225312. PMLR, 2022. 1, 3, 8 Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and JamesTin-Yau Kwok. Searching to exploit memorization effect inlearning with noisy labels. In International Conference onMachine Learning, pages 1078910798. PMLR, 2020. 1, 3"
}