{
  "Abstract": "The Long-form Video Question-Answering task requiresthe comprehension and analysis of extended video con-tent to respond accurately to questions by utilizing bothtemporal and contextual information.In this paper, wepresent MM-Screenplayer, an advanced video understand-ing system with multi-modal perception capabilities thatcan convert any video into textual screenplay representa-tions. Unlike previous storytelling methods, we organizevideo content into scenes as the basic unit, rather than justvisually continuous shots.Additionally, we developed aLook Back strategy to reassess and validate uncertaininformation, particularly targeting breakpoint mode. MM-Screenplayer achieved highest score in the CVPR2024LOng-form VidEo Understanding (LOVEU) Track 1 Chal-lenge, with a global accuracy of 87.5% and a breakpointaccuracy of 68.8%.",
  ". Introduction": "With the rapid development of video models, signifi-cant progress has been made in the domain of video un-derstanding. However, the length of videos that these mod-els can effectively handle remains limited. The long-formVideo Question-Answering (LVQA) task has been intro-duced to explore the potential of models in understandinglong-duration videos, specifically those videos longer thanfive minutes. The LVQA task demands a comprehensiveunderstanding of the entire video from the global perspec-tive, as well as the precise temporal capturing ability toanswer questions about specific moments. This presentsa substantially more challenging task in video understand-ing. Recently, the first long video understanding bench-mark, MovieChat has been proposed. It includes 1,000 high-quality video clips sourced from various movies andTV series, accompanied by 14,000 manual annotations.MovieChat enables quantitative evaluation of long-formvideo understanding capabilities in question-answering.Most previous video models focus on end-to-end train-ing to build a video question-answering system. Works likeMovieChat rely on the question input to constructvideo representations and answers. Due to the lack of high-quality large-scale annotated data, these models have verylimited capabilities in handling the LVQA task. Anotherline of work adopts a series of foundational models to con-vert video content into textual representations, which werefer to as storytelling methods . They leverage thepowerful language understanding capabilities of LLMs tocomprehend video content based on the generated scripts.However, these methods either perform captioning on eachindividual frame or use scene detection to segment visu-ally continuous shots as the basic unit. These approachesoverlook the temporal relationships between different seg-ments, thus limiting their understanding of the video con-tent. For example, multiple quick-cut shots in a movie oftenrepresent a single coherent event.To address the aforementioned issues, we introduceMM-Screenplayer, an agent system endowed with mul-timodal perception capabilities for tackling LVQA tasks.It comprises three essential components: the Multi-modalPerception module, which receives inputs from both visualand audio tracks of video; the Scene-Level Script Genera-tion Module, which prompts LLMs to reassemble and com-prehend shots temporally, generating high-level semanticscenes as the basic unit; and the Look back for determina-tion Module, which extracts, analyzes, and summarizes in-formation from video frames before and after the specifiedtimestamp for breakpoint mode video question answering.MM-Screenplayer achieved first place in the CVPR 2024",
  "ACT1INTRO Skate Rink Announcement [SPEAKER_03] {'gender': Male', 'hairColor': None', 'skinColor': 'Light', 'isWearingGlasses': No, Cloths: hat, brown coat}": "SCENE . Comic bookstore, festooned with banners reading...[18.697-22.064][Description] A group of Revelers in superhero costumes line up, eagerly awaiting the announcement[Speaker_02] (silence,): And the award for best group costume goes to Justice League of America. Number three![22.064-24.055] [Description] A group of Revelers in superhero costumes celebrating and hugging each other.",
  ". Method": "Given a video V, MM-Screenplayer generates a com-prehensive screenplay M to thoroughly represent the con-tent of the video.Unlike previous storytelling methods,we organize textual descriptions into higher-level seman-tic scenes rather than individual shots, thereby promotinga deeper comprehension of the videos narrative. Addition-ally, to address certain issues of the breakpoint mode, we in-troduce a Look-Back mechanism: if the model is uncertainto make judgments solely based on the given screenplay,it then extracts frames to utilize extra visual information toimprove the understanding of the given video, therefore pro-ducing more reliable answers. The overall architecture ofour model is illustrated in .",
  ". Multi-Modal Perception": "Our model comprehensively analyzes both visual and au-dio tracks of a video to extract rich multi-modal informa-tion. For the visual track, we start with scene detection todivide the video into distinct shots. For each shot, framesare sampled at regular intervals. A Vision-Language Modelsuch as GPT-4V is utilized to generate image captions foreach frame . For the audio track, an Automatic SpeechRecognition (ASR) model is applied to transcribe the di- alogue from the audio. Additionally, we utilized an Au-dio Event Localization model to detect and index signifi-cant audio events throughout the video. By combining theseprocesses, we captured a structured set of data encompass-ing visual shots, frame descriptions, dialogue transcripts, aswell as audio events. This multi-modal extraction providesa robust foundation for advanced video analysis and under-standing.",
  ". Scene-Level Scripts Generation": "The concept of scene is fundamental in the design ofscreenplays, as it provides a higher-level semantic deci-sion of the video. In contrast, simple storytelling methodsmerely rely on basic scene detection and treat individualshots as primary units . Such an approach hinders thecomprehensive understanding of the narrative.For example, in the film Titanic (1997), the sequence lead-ing up to the ships collision with the iceberg consists ofnumerous quickly shifting shots. If these shots are viewedin isolation rather than treated as part of a cohesive scene,the true narrative is lost. To tackle this issue, we propose aScene-Level Scripts Generation module that utilizes LLMsto merge shots into coherent scenes.Initially, we arranged ASR transcripts in chronologicalorder. For dialogues with pauses longer than 2 seconds, in-tervening visual content becomes significant. To addressthis observation, we insert a separator marker betweensentences with gaps of more than 2 seconds, guiding theLLM to perform an initial rough split of the transcript. In",
  "Ours87.54.1868.83.52": "the next phase, to integrate multi-modal information andgrasp the subtext, we insert captions of visual and au-dio events between these coarse split scenes. This createsa multi-modal text representation, allowing the LLM to re-process and refine the separation of the content. By ana-lyzing visual descriptions, dialogues and audio events, themodule is capable of identifying logical boundaries.",
  ". Look Back for Determination": "Our pipeline employs LLMs to understand long-formvideo content and answer questions across different modesusing tailored prompts and generated screenplays which arereusable, enhancing efficiency in LVQA by eliminating theneed for re-encoding video input for each question.A look-back mechanism is specifically designed for han-dling problems in breakpoint mode. We observed that inrare cases, such as the given problem requiring time local-ization with a higher precision, the screenplay alone mayfail to extract sufficient information for answer generation.Therefore, when the response produced by the AnswerGenerator was detected empty or invalid (such as: con-taining negative keywords such as cannot, dont know,etc.), the model will fall back to reproduce a new answerwith the incorporation of visual information.The visual information is obtained through the videotrack through frame extraction.As in breakpoint mode,the exact objective timestamp is given by the question, thepipeline will extract the frames slightly before and afterthat timestamp consecutively, as the retrieved informationis used for producing the new answer.By combining the extracted visual time series informa-tion with the produced screenplay, the model was able to notonly gather sufficient information near the provided videobreakpoint but also comprehend the video plot globally,therefore being more probable to produce valid and correctanswers to breakpoint problems. .Performance Metrics with Different Components.Acronyms:SSGM - Scene-level Scripts Generation Module,LBDM - Look Back for Determination Module, G-Acc - GlobalAccuracy, G-Score - Global Score, B-Acc - Breakpoint Accuracy,B-Score - Breakpoint Score.",
  ". Experimental Settings": "The LVQA Challenge offers 170 videos as the test set toevaluate the models performance. The evaluation processis processed by the competition organizer using unreleasedanswers. In our proposed screenplay generation pipeline,we employed GPT-4-turbo as the LLM driving all textscript processing tasks. For visual description generation,we cherry-picked GPT-4o as the corresponding VLM. Ad-ditionally, we integrated whisperX as the ASR model.Gemini-1.5 pro was chosen as the audio analyzer for au-dio event localization. The versions and parameters of theLLM and VLM are fixed to ensure reproducibility. All ex-periments are conducted on a single T4 GPU without anyextra training process.",
  ". Main Results": "As shown in , MM-Screenplayer achieved topperformance on the MovieChat-1K test set, with a globalaccuracy of 87.5% and a global score of 4.18. In break-point mode, our method attained an accuracy of 68.8% anda score of 3.52, ranking highest among all participants.The outstanding performance in both global and break-point modes demonstrates that our proposed screenplay for-mat plays a pivotal role in representing long video con-tent and providing rich contextual information. This en-ables LLM to understand long-form video content and ac-curately answer questions. Compared to the locally repro-duced MovieChat baseline, our pipeline generates an-swers that exhibit a comprehensive understanding of long-form video content and accurate temporal capture ability.",
  "Moviechat: He finally goes to a boat": ". Comparison of answers produced by MM-Screenplayer and other state-of-the-art methods for a question from MovieChat1K-testset. Our method produced significantly better answers while all other methods answers were incorrect. demonstrate the effectiveness of both modules. However,the results also indicate that solely relying on screenplaycontent is unreliable for addressing some detailed issues inbreakpoint mode. This is because our visual descriptionsmight not have adequately captured the necessary informa-tion.3.4. Qualititave Results presents one of our answers on MovieChat-1Ktest set. MM-Screenplayer produced significantly better an-swers which precisely captured the environmental transitionof this clip of Trumans World, while other methods ei-ther produced incorrect destinations or could not answer thequestion at all. Screenplay enables our method to accuratelycapture the high-level global semantics of this movie clipwhile the look-back mechanism could further guarantee theunderstanding of specific moments along with the contextprovided by the screenplay.4. Conclusion In this paper, we introduced MM-Screenplayer, a com-prehensive video understanding system with multi-modalperception capabilities that can convert videos of arbitrarylength into higher-level textual representations. Our innova-tive approach organizes video content into scenes as the ba-sic unit and employs a Look Back strategy to reassess andvalidate uncertain information. MM-Screenplayer achievedthe highest score in the CVPR2024 LOng-form VidEo Un-derstanding (LOVEU) Track 1 Challenge, demonstratingexceptional proficiency in long-form video understanding.This accomplishment underscores the effectiveness of ourmethod in comprehending and analyzing extended videocontent for accurate question-answering.References Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXiv preprint arXiv:2303.08774,2023. 3",
  "KunChang Li, Yinan He, Yi Wang, Yizhuo Li, WenhaiWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.Videochat: Chat-centric video understanding. arXiv preprintarXiv:2305.06355, 2023. 3": "Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin,Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, LinLiang, Zicheng Liu, Yumao Lu, et al. Mm-vid: Advanc-ing video understanding with gpt-4v (ision). arXiv preprintarXiv:2310.19773, 2023. 1, 2, 3 Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-had Shahbaz Khan. Video-chatgpt: Towards detailed videounderstanding via large vision and language models. arXivpreprint arXiv:2306.05424, 2023. 1, 3 Machel Reid, Nikolay Savinov, Denis Teplyashin, DmitryLepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, RaduSoricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit-twieser, et al.Gemini 1.5: Unlocking multimodal under-standing across millions of tokens of context. arXiv preprintarXiv:2403.05530, 2024. 3 Enxin Song, Wenhao Chai, Guanhong Wang, YuchengZhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, YanLu, Jenq-Neng Hwang, et al. Moviechat: From dense to-ken to sparse memory for long video understanding. arXivpreprint arXiv:2307.16449, 2023. 1, 3",
  "Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen,and Xin Geng. Exploring diverse in-context configurationsfor image captioning. Advances in Neural Information Pro-cessing Systems, 36, 2024. 2": "Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang,Shoubin Yu, Mohit Bansal, and Gedas Bertasius.A sim-ple llm framework for long-range video question-answering.arXiv preprint arXiv:2312.17235, 2023. 1, 3 Hang Zhang, Xin Li, and Lidong Bing. Video-llama: Aninstruction-tuned audio-visual language model for video un-derstanding. In Proceedings of the 2023 Conference on Em-pirical Methods in Natural Language Processing: SystemDemonstrations, pages 543553, 2023. 3"
}