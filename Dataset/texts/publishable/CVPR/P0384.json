{
  "Abstract": "Artificial neural networks often suffer from catastrophicforgetting, where learning new concepts leads to a com-plete loss of previously acquired knowledge. We observethat this issue is particularly magnified in vision transform-ers (ViTs), where post-pre-training and fine-tuning on newtasks can significantly degrade the models original generalabilities. For instance, a DINO ViT-Base/16 pre-trained onImageNet-1k loses over 70% accuracy on ImageNet-1k af-ter just 10 iterations of fine-tuning on CIFAR-100. Over-coming this stability-plasticity dilemma is crucial for en-abling ViTs to continuously learn and adapt to new domainswhile preserving their initial knowledge. In this work, westudy two new parameter-efficient fine-tuning strategies:(1) Block Expansion, and (2) Low-rank adaptation (LoRA).Our experiments reveal that using either Block Expansionor LoRA on self-supervised pre-trained ViTs surpass fullyfine-tuned ViTs in new domains while offering significantlygreater parameter efficiency. Notably, we find that BlockExpansion experiences only a minimal performance dropin the pre-training domain, thereby effectively mitigatingcatastrophic forgetting in pre-trained ViTs1.",
  ". Introduction": "Humans excel at continual learning, gradually acquiringnew information while retaining previously learned con-cepts, with a complete loss of prior knowledge being arare occurrence. In contrast, artificial neural networks, in-cluding vision image transformers (ViT) , often sufferfrom catastrophic forgetting of old concepts as new onesare learned . Catastrophic forgetting directly stems",
  "*Joint first-authorship.1We have made the source code available to the public at:": "from the stability-plasticity dilemma, a fundamental issuein neural networks that balances the integration of newknowledge (plasticity) against the preservation of existingknowledge (stability) . Addressing this dilemma couldenhance their applicability in dynamic, real-world scenar-ios. In these cases, the model must continually be fine-tuned with data from new class or domain distributions, while preserving its prior knowledge. We observe that fine-tuning a DINO ViT , initiallypre-trained on the ImageNet-1K , across various transferdatasets, results in decreased performance on the originalImageNet-1K. Notably, we find that even a short fine-tuningperiod of 10 iterations on CIFAR-100 leads to a signif-icant drop of 70% accuracy on the original ImageNet-1K,highlighting the pronounced effects of catastrophic forget-ting in pre-trained ViTs. This underscores the need for fine-tuning approaches that preserve the models general capa-bilities while integrating new, domain-specific knowledge.In response, we investigate Parameter-Efficient Fine-tuning(PEFT) strategies to focus on adjusting a minimal numberof model parameters, aiming to maintain the models foun-dational strengths while incorporating new insights. PEFT strategies have been extensively explored in Nat-ural Language Processing (NLP) and multimodalfoundation models , garnering significant at-tention for their ability to fine-tune models without exten-sively altering their structure.The exploration of thesestrategies has recently extended to vision models ,with research focusing on various aspects of parameter-efficient fine-tuning including efficiency , scal-ability , transferability , and robustness .On the other hand, efforts to tackle catastrophic forgettinghave been made independently , yet incorporat-ing cutting-edge PEFT techniques from NLP into the vi-sion domain to mitigate catastrophic forgetting in VisionTransformers (ViTs) remains an underexplored area. Our",
  "arXiv:2404.17245v2 [cs.CV] 5 Jul 2024": "research distinguishes itself from these broad explorationsof PEFT characteristics by specifically focusing on leverag-ing PEFT techniques to mitigate catastrophic forgetting dur-ing the fine-tuning of Vision Transformers (ViTs), therebypresenting a novel study in this domain. Drawing on therecent Low-Rank Adaptation (LoRA) and Block Ex-pansion techniques from Natural Language Process-ing, we adapt and apply these concepts to ViTs. Our workconducts a thorough comparison of these innovative strate-gies against traditional fine-tuning methods, focusing par-ticularly on their ability to retain performance on originaltasks while excelling in new domains.Our findings re-veal that both Block Expansion and LoRA exhibit a remark-able ability to adapt to diverse transfer domains. However,LoRAs performance may deteriorate in scenarios involvingsimpler datasets like CIFAR-10. In contrast, Block Expan-sion demonstrates a robust ability to safeguard the modelsperformance on the pre-training dataset.",
  "This section provides an overview of conventional fine-tuning approaches and delves into the application of LoRAand Block Expansion techniques to ViTs": "Standard Fine-tuning.Standard fine-tuning is a set ofcommonly adopted techniques to adapt pre-trained mod-els for new tasks or domains. It starts with a pre-trainedmodel, whose weights and biases are initialized from itsoriginal training phase. For full fine-tuning, every param-eter of the model, encompassing weights and biases acrossall layers, is updated via gradient changes based on the newtask or dataset as shown in . The model is con-sidered a differentiable function, allowing gradients derivedfrom the new tasks loss to adjust all the trainable parame-ters. Other variations in standard fine-tuning involve: Top-k (Fine-tuning only top-k layers) and linear probing (Fine-tuning only the linear classification layer). Block Expansion.We introduce the concept of Block Ex-pansion for fine-tuning pre-trained ViTs, building upon anidea that was recently proposed for language models but has yet to be explored in vision. This technique is usedto augment the capacity of a model without altering its ini-tial output. In a ViT model comprised of sequential trans-former blocks (0, 1, ..., N), Block Expansion adds anidentity block (id) after a set of transformer blocks suchthat id(x) = x, meaning it returns the input as its out-put, ensuring the models output remains unchanged imme-diately after expansion. To expand a model from N to N blocks, the original blocks are first grouped into sets con-taining M blocks each. Within each set, an identity copy ofthe topmost block is created and placed on top, effectivelyincreasing the models depth without initially changing itsbehavior. In each newly expanded block, two linear layers",
  "are zero-initialized to enable identity mapping, as shown in (c). These newly added blocks are only fine-tunedwith the new data while the remaining blocks are frozen": "Low Rank Adaptation (LoRA).LoRA has gained sig-nificant popularity for language tasks due to its effective-ness in fine-tuning large language models (LLMs) likeGPT . However, the adoption of LoRA in the visiondomain has been more limited. We investigate how thistechnique, previously celebrated in language models, can betailored and applied to fine-tune pretrained ViTs. We intro-duce LoRA to ViTs by introducing auxiliary low-rank ma-trices A and B to adapt the weight matrices of a pretrainedViT model, specifically targeting the queries (Q) and val-ues (V ) in the multi-headed self-attention mechanism. Inpractice, for a given weight matrix W, the adaptation isperformed by calculating W = W + AB, where W is theadapted weight matrix, and A and B are the low-rank matri-ces introduced for adaptation as shown in (d). Thismodification allows the original model to retain its learnedrepresentations while gaining the flexibility to adapt to newtasks with a relatively small increase in parameters.",
  "This section explores strategies for models to maintain per-formance on original tasks while excelling in new domains": "CatastrophicForgetting.Catastrophicforgettingiswhen a model loses the knowledge acquired during pre-training upon being fine-tuned for a different task.Toquantify this effect, we utilize the k-nearest neighbors(K-NN) method to evaluate the fine-tuned models perfor-mance on the pre-training data. Specifically, after addinga linear classification layer to the pre-trained backboneand fine-tuning it on a transfer dataset, we apply K-NN tomeasure backbones accuracy on the pre-training dataset.We compare this accuracy to the original backbone, alsoevaluated with K-NN on the pre-training dataset.Wechose the K-NN approach for evaluating performance onImageNet-1K (IN-1K) to reduce computation time.",
  ". Main results": "In this section, we evaluate the adaptability and catastrophicforgetting of a pre-trained DINO ViT/B-16 model ,which was originally trained on the ImageNet-1K dataset.Our fine-tuning experiments are conducted on five diversedatasets: DTD (Describable Textures Dataset) , Flow-ers102 , Food101 , CIFAR-10 , and CIFAR-100. To customize the model for each dataset, we introducean additional linear layer to the pre-trained architecture.During the fine-tuning process, we test learning rates of0.05, 0.01, and 0.005, and run the models for 10,000 steps,assessing accuracy every 500 steps. The checkpoint with . Comparison of different ViT fine-tuning approaches: (a) Linear-probing ViT model with all weights frozen (cyan blocks)and a trainable classifier (white block). (b) Fully fine-tuned ViT with all trainable weights (white blocks). (c) Block Expansion withadditional blocks containing trainable zero-initialized linear layers (red blocks) and other trainable parameters (white blocks). (d) Low-Rank Adaptation (LoRA) weights (white blocks) added in parallel to the frozen pre-trained weights of Queries and Values (cyan blocks). Accuracy on CIFAR-100 Accuracy on IN-1K 93 94 95 96 97 98 99 Accuracy on CIFAR-10 65 66 67 68 69 70 71 Accuracy on DTD Accuracy on Foods101 Accuracy on Flowers102 Block ExpansionLoRAFully Fine-tuningLinear Probes . Comparison of top-1 accuracy between fine-tuned DINO ViT/B-16 models on transfer datasets and ImageNet-1K: the figureillustrates that models fine-tuned with Block Expansion achieve high accuracy on target datasets (e.g., CIFAR-10) while also preservingknowledge of the pre-trained dataset (ImageNet-1K). the highest accuracy is chosen as the best model for eachstrategy.This models backbone is then used for K-NNevaluation on ImageNet-1K, enabling an analysis of adapt-ability and the extent of catastrophic forgetting by compar-ing performance on ImageNet-1K and the transfer datasets.Our findings, displayed in , reveal that mod-els fine-tuned with Block Expansion typically excel in boththe transfer and source domains (ImageNet-1K), effectivelyavoiding catastrophic forgetting. The LoRA method, al-though effective for CIFAR-100, shows inconsistent resultsand catastrophic forgetting, particularly on CIFAR-10. Inour experiments, we incorporate three blocks (p = 3) forBlock Expansion, and a rank of 8 (r = 8) for LoRA.We also compare two other standard fine-tuning strate- gies: full fine-tuning and linear probing (only linear layerfine-tuned). Full fine-tuning yields high performance ontransfer datasets but leads to significant accuracy losseson ImageNet-1K, indicating a loss of original representa-tions, aka catastrophic forgetting. Conversely, linear prob-ing maintains good ImageNet-1K accuracy but underper-forms on transfer datasets, highlighting the challenge ofachieving a balance between learning new tasks and pre-serving existing knowledge.",
  "In this section, we examine factors influencing the effective-ness of the above-discussed parameter-efficient fine-tuningtechniques. Our focus includes the impact of the number of": "Top-1 Accuracy lr=0.05 lr=0.005 Number of Added Blocks CIFAR-100IN-1K . Exploring learning rate effects on catastrophic forgettingin fine-tuning with Block Expansion shows that higher rates, de-spite similar transfer dataset performance, worsen source domainforgetting, especially with more blocks added. The green line rep-resents the accuracy of the unchanged backbone on IN-1K. . Comparing fine-tuning strategies for DINO ViT/B-16on CIFAR-100: This table presents top-1 accuracy on transferand source datasets, alongside the count of trainable parameters.Block Expansion stands out for maximizing transfer dataset accu-racy while preserving source dataset performance.",
  "p = 17.2 M82.7275.7579.24p = 214.3 M86.7075.5481.12p = 321.3 M88.5874.6181.60p = 428.4 M89.0972.2880.69": "blocks added in Block Expansion, the rank used in LoRA,and the extent of layer fine-tuning. CIFAR-100 serves asour chosen transfer dataset for these experiments.The results, presented in , indicate that Block Ex-pansion leads to the highest mean accuracy compared toother fine-tuning approaches, with the optimal configura-tion adding three blocks (p = 3), achieving 81.60% meanaccuracy. This performance across CIFAR-100 and IN-1K,reflects the models ability to adapt to new domains whileretaining knowledge from the source domain. However, wenote a trade-off in accuracy between CIFAR-100 and IN-1Kwhen additional blocks are added. Moreover, the data sug-gest that the rank parameter in LoRA does not significantlyimpact mean accuracy, indicating a plateau in effectiveness.Block Expansion with three blocks (p = 3) has the same 71 73 75 77 79 81 83 85 87 Accuracy on CIFAR-100 Accuracy on IN-1K Block ExpansionLoRAFully Fine-tuningLinear Probes",
  ". Comparing top-1 accuracy of fine-tuned DINO ViT/S-16 models shows LoRA and Block Expansion methods outperformtraditional fine-tuning on both transfer datasets and ImageNet-1K": "number of trainable parameters as fine-tuning the top threelayers of ViT (Top 3), yet it achieves better accuracy onboth CIFAR-100 and IN-1K, highlighting the effectivenessof Block Expansion. However, it is worth mentioning thatthe overall parameter count for Block Expansion is higherdue to the addition of new layers.Additionally, we fine-tuned the model by varying thenumber of added blocks and learning rates while using theBlock Expansion strategy. As depicted in , while ahigh learning rate does not significantly affect performanceon the transfer dataset, it can lead to catastrophic forget-ting, with accuracy decreasing by 10%, a phenomenon notencountered at lower learning rates. To further validate theeffectiveness of the methods across different models, we ap-plied them to a smaller DINO ViT variant. As illustratedin , fully fine-tuning ViT-S/16 led to a reductionin ImageNet-1K accuracy to 15.77%, whereas employingLoRA and Block Expansion maintained performance onboth CIFAR-100 and ImageNet-1K.",
  ". Conclusion": "In this work, we explored parameter-efficient fine-tuning(PEFT) strategies, namely Block Expansion and Low RankAdaptaion (LoRA), to mitigate catastrophic forgetting inViTs.We found that catastrophic forgetting, while al-ready present in neural networks, is particularly magnifiedin ViTs. Our experiments show that these PEFT strategiesoutperform standard fine-tuning methods, achieving betterperformance with greater parameter efficiency. AlthoughLoRA is generally effective for fine-tuning, it can performpoorly on simpler datasets like CIFAR-10, suggesting fur-ther investigation. Our adapted Block Expansion techniqueemerges as a robust method, not only enabling better fine-tuning but also retaining most of its performance on theoriginal pre-training dataset. This indicates that Block Ex-pansion can decrease the catastrophic forgetting faced byViTs, allowing them to adapt to new domains while pre-serving their previously acquired knowledge.",
  "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.Food-101 - mining discriminative components with randomforests. In European Conference on Computer Vision, 2014.2": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners. Advances in neural in-formation processing systems, 33:18771901, 2020. 2 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 96509660, 2021. 1, 2",
  "Hila Chefer, Idan Schwartz, and Lior Wolf.Optimizingrelevance maps of vision transformers improves robustness.Advances in Neural Information Processing Systems, 35:3361833632, 2022. 1": "Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,Yibing Song, Jue Wang, and Ping Luo.Adaptformer:Adapting vision transformers for scalable visual recogni-tion. Advances in Neural Information Processing Systems,35:1666416678, 2022. 1 Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, SammyMohamed, and Andrea Vedaldi. Describing textures in thewild. 2014 IEEE Conference on Computer Vision and Pat-tern Recognition, pages 36063613, 2013. 2 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 1, 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 1 Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville,and Yoshua Bengio.An empirical investigation of catas-trophic forgetting in gradient-based neural networks. arXivpreprint arXiv:1312.6211, 2013. 1",
  "Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Ku-mar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgicalfine-tuning improves adaptation to distribution shifts. ArXiv,abs/2210.11466, 2022. 1": "Weicong Liang, Yuhui Yuan, Henghui Ding, Xiao Luo, Wei-hong Lin, Ding Jia, Zheng Zhang, Chao Zhang, and Han Hu.Expediting large-scale vision transformer for dense predic-tion without fine-tuning. Advances in Neural InformationProcessing Systems, 35:3546235477, 2022. 1 Soroosh Safari Loaliyan and Greg Ver Steeg. Comparativeanalysis of generalization and harmonization methods for 3dbrain fmri images: A case study on openbhb dataset. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR) Workshops, pages 49154923, 2024. 1",
  "Michael McCloskey and Neal J Cohen. Catastrophic inter-ference in connectionist networks: The sequential learningproblem. In Psychology of learning and motivation, pages109165. Elsevier, 1989. 1": "Martial Mermillod, Aurelia Bugaiska, and Patrick Bonin.The stability-plasticity dilemma: Investigating the contin-uum from catastrophic forgetting to age-limited learning ef-fects. Frontiers in psychology, 4:54654, 2013. 1 Maria-Elena Nilsback and Andrew Zisserman. Automatedflower classification over a large number of classes. 2008Sixth Indian Conference on Computer Vision, Graphics &Image Processing, pages 722729, 2008. 2 Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hong-sheng Li.St-adapter: Parameter-efficient image-to-videotransfer learning. Advances in Neural Information Process-ing Systems, 35:2646226477, 2022. 1",
  "Bardia Safaei, VS Vibashan, Celso M de Melo, and Vishal MPatel. Entropic open-set active learning. In Proceedings ofthe AAAI Conference on Artificial Intelligence, pages 46864694, 2024. 1": "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter:Parameter-efficient transfer learning for vision-and-languagetasks. 2022 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 52175227, 2021. 1 Riccardo Volpi, Diane Larlus, and Gregory Rogez. Continualadaptation of visual representations via domain randomiza-tion and meta-learning.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 44434453, 2021. 1 Haixin Wang, Xinlong Yang, Jianlong Chang, Dian Jin, Ji-nan Sun, Shikun Zhang, Xiao Luo, and Qi Tian. Parameter-efficient tuning of large-scale multimodal foundation model.In Thirty-seventh Conference on Neural Information Pro-cessing Systems, 2023. 1"
}