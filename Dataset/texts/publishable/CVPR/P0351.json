{
  "Abstract": "Deep learning models have revolutionized the field ofmedical image analysis, due to their outstanding perfor-mances. However, they are sensitive to spurious correla-tions, often taking advantage of dataset bias to improveresults for in-domain data, but jeopardizing their general-ization capabilities. In this paper, we propose to limit theamount of information these models use to reach the finalclassification, by using a multiple instance learning (MIL)framework. MIL forces the model to use only a (small) sub-set of patches in the image, identifying discriminative re-gions. This mimics the clinical procedures, where medicaldecisions are based on localized findings. We evaluate ourframework on two medical applications: skin cancer diag-nosis using dermoscopy and breast cancer diagnosis usingmammography. Our results show that using only a subsetof the patches does not compromise diagnostic performancefor in-domain data, compared to the baseline approaches.However, our approach is more robust to shifts in patientdemographics, while also providing more detailed explana-tions about which regions contributed to the decision. Codeis available at:",
  ". Introduction": "Deep learning (DL) architectures revolutionized the field ofmedical image analysis, achieving performances that rivaleven those of more experienced clinicians. It is undeniablethat DL models can extract relevant and sometimes new in-formation from medical data. However, there is still a highdegree of uncertainty associated with the information thatis being used by these models and whether it maps to ac-tual (novel) concepts, or if the models are identifying spuri-ous correlations and taking advantage of dataset bias .Thus, in order to really leverage DL systems in healthcare, itis necessary to ensure that these models are simultaneously explainable and able to achieve good performances outsidethe datasets they were trained on.The evolution in the DL field has led to the proposal ofdifferent ways for extracting information from images. Inthis scope, convolutional neural networks (CNNs) are stillthe most common architectures in medical image analysis.However, in recent years, vision transformers (ViTs) havealso gained popularity . CNNs and ViTs adopt dif-ferent feature extraction paradigms: CNNs explore the lo-cal neighborhood, while ViTs are able to capture the imagecontext by using self-attention blocks that leverage spatialinformation and distant relationships. ViTs also adopt anexplicit patch-based strategy, as opposed to the traditionalfull-image analysis performed by CNNs. Nevertheless, botharchitectures end up learning patch-based representationsthat are then aggregated into a single representation vector(e.g., through the global average pooling operation in CNNsand the class token in ViTs).Patch or region-based analysis resembles clinical prac-tice for medical image inspection, where doctors search forlocalized findings and criteria to perform a diagnosis. How-ever, contrary to DL models, clinicians do not need to pro-cess all regions in a medical image, as they are able to au-tomatically identify the key regions that match a malignantdiagnosis. An example is the 7-point checklist method usedin skin image analysis . This approach focuses solelyon the presence or absence of certain dermoscopic featureswithin the lesion, regardless of their spatial arrangement.Another example is breast cancer, where radiologists iden-tify and classify findings in mammography.The development of DL models capable of identifyingregions of interest (ROIs) in medical images and using onlythose regions to perform a diagnosis is a promising line ofresearch. On one hand, these methods are more aligned withclinical practice.Additionally, showing the ROIs grantssome measure of explainability to the model. On the otherhand, by forcing the model to use only a part of the imageto perform a diagnosis, we can: i) improve its robustness tobias, as the information that the model can use is limited and",
  "arXiv:2405.01654v1 [cs.CV] 2 May 2024": "results across different Kr values for the ISIC 2019 valida-tion set and both test datasets: PH2 and Derm7pt. Theseresults indicate that a higher Kr does not necessarily leadto better performance, especially on the test datasets. It isclear that Kr values of 0.6, 0.7, and 0.8 outperform the rest.Therefore, in this work we used the Kr = 0.6 and Kr = 0.7EViT-S configurations.",
  ". Related Work": "Most state-of-the-art classification models for medical im-age analysis are either based on CNNs or ViTs . Whilethese architectures are conceptually different, both can beviewed as extractors of patch-level features. These featuresare then aggregated into a single vector that represents theentire image. Pooling operators, in particular global aver-age pooling, are usually used for CNNs, while ViTs inte-grate the information of all patches into the class token. Inthe end, the representation is fed to an MLP head that per-forms the binary or multi-class classification. This meansthat, when training a model, we are allowing it to exploreall the available information to define the decision bound-aries. The drawback is that the model can learn spuriouscorrelations and use them to achieve higher performances during training and in-domain validation .Attention blocks, in particular spatial ones, can be seenas a mechanism to reduce the amount of image informationused by the models , as they act as patch selectors. De-spite their popularity, spatial attention blocks are not sparse(apart from a few exceptions ), which means that allregions in the image end up contributing to the decision.Moreover, they consist of additional layers of parametersto be learned end-to-end, increasing the model complexity,and their placement in the architecture is not trivial.A particular type of attention is self-attention used byViTs . Here, multi-head self-attention (MSA) blocks are used to extract complex features by leveraging patchcorrelations.Self-attention also affects the class embed-ding that will be fed to the classifier. However, while thevisualization of the output of the MSA blocks allows theidentification of relevant patches, they are all used to buildthe class embedding. To overcome this issue and reduce theamount of information, Liang et al. introduced the Ex-pediting ViT (EViT), which progressively discards less rel-evant patches. The EViT model uses attention scores to de-termine the significance of each patch towards the modelsoutput.The k most relevant patches are categorized asattentive, while the remaining ones are deemed inatten-tive and are subsequently merged into a single embedding.EViT showcases a promising direction for information se-lection in ViTs. However, the ratio between attentive andinattentive patches must be empirically defined by the user.MIL-based frameworks have been explored in the fieldof medical image analysis to process high-resolution im-ages, such as those of histopathology . MILis particularly well suited in this context, as often we onlyhave access to image level labels (e.g, tumor staging), butthe relevant information is localized in a small portion ofthe image that we want to identify. To achieve this goal, theoriginal image is partitioned into big patches that are thenindependently processed by a CNN and aggregated in theend, using a variety of strategies such as max or attentionpooling and transformers .It is clear that MIL frameworks are explainable, as theyhighlight relevant patches in an image. Moreover, when cer-tain operators are used (max or top-k pooling) they can beseen as a proxy to a spatial attention module that does notrequire the learning of additional model parameters. How-ever, the application of MIL in the medical domain hastwo limitations: i) it is often applied to binary classifica-tion problems, while several medical problems are multi-class (e.g., skin image analysis); and ii) by dividing the im-age into patches and processing each one independently, wemay be losing relevant features. Regarding the latter issue,we propose to apply MIL only after the feature extractionprocesses of CNNs and ViTs. This allows us to select rele-vant patches and reduce the amount of information used by",
  "Embedding-level": ". Overview of the proposed approach. An encoder block (CNN or ViT-based) extracts patch representations from the input image.Each patch will be an instance of a bag. Then, a MIL block determines the bag label using an instance or embedding-level approach. the classifier, while still exploring the capabilities of thesetwo architectures to extract information from images. Forthe multi-class problem, there have been some attempts toextend MIL to this setting , without definitiveresults. In this work, we propose a generalized MIL for-mulation for a multi-class problem and show that the binaryproblem is a particular case of this formulation.",
  ". Proposed Approach": "Our approach, illustrated in , relies on MIL strate-gies to obtain the image classification using only a subsetof its patches. The input image is processed with an en-coder block that extracts patch features. Then, a MIL blockpredicts the classification of the image (bag), based on thosepatch (instance) representations. The following sections de-scribe each block in detail.",
  ". Patch Encoder Block": "The first component of our approach is a patch encoderblock. This block is responsible for generating the repre-sentation vector of each of the N N patches in the image.Two types of encoders may be adopted: CNNs, where patchrepresentations correspond to each pixel of the output fea-ture map; or ViTs, where patch representations correspondto the final representation of each patch token.",
  "CNN Encoder": "Most popular CNNs follow the same type of architecture,consisting of a sequence of convolutional blocks (with con-volutional, pooling, and normalization layers), followed bya classification head. The convolutional blocks process theimage using small kernels that extract low-level to high-level features from each region in the image. Their output isa NN feature map, X RNND, with N much smallerthan the size of the original image, illustrated in . Due to the convolution operations, each pixel in this fea-ture map can be interpreted as the representation of a patch(receptive field) in the image. Typically, the feature map isthen transformed using a global average pooling, resultingin a representation vector for the entire image that is the in-put to the classification head. The underlying premise ofthis step, however, is that the relevant information for theclassification task is spread across the entire image.To avoid the above premise, we discard the global aver-age pooling and treat each pixel in the feature map as aninstance for the MIL classifier. Concretely, we assume thatthe feature map X contains the representation of all N Npatches in the image.",
  "ViT Encoder": "ViT-based architectures use the multi-head self-attention(MSA) mechanism to extract complex features basedon patch correlations in images while taking into accountpositional information. The input image is first transformedinto a sequence of N 2 patches. Then, these patches are pro-cessed by several linear projections and MSA layers.Each MSA consists of running several self-attentionmechanisms in parallel on a sequence comprising the patchrepresentations and an additional class token. The result-ing attention maps hold information regarding the pairwisesimilarities between patches. Effectively, each MSA layermodifies the patches and the class token representations bycombining the information contained in the entire sequencethrough weighted averages.In the standard ViT, the final image classification is ob-tained by applying an MLP head to the class token, whichharnesses information from all patches. To avoid this, wediscard the class token and use the final patch represen-tations, denoted as X RNND, as input to the MILblock. This means that each patch representation captures",
  ". MIL Block": "The MIL block aims to apply a MIL classifier to the patchrepresentations obtained by the patch encoder block. Thetensor X is first flattened to a matrix Z RN 2D, which isour collection of instances, represented as a bag in .The j-th line vector in Z, zj R1D, is the representationof the j-th patch in the input image. The proposed MILclassifier consists of three key operations: h, a linear projection function that maps the input from anembedding of dimension D to the logits with dimensionC (the number of classes), given by h(Z) = WZ + b,where W RCD and b RC is a bias term;",
  ", a permutation-invariant pooling that aggregates thepatch-level information into image-level by applying atop-k average, where k = 1 leads to max pooling andk = N 2 leads to average pooling; and": ", a non-linear activation function.The order of these three functions, {h, , }, determines thespecific MIL approach used: instance-level or embedding-level, as detailed in the following sections.This formulation is a generalization of the classical MILapproach for binary problems. However, it should be em-phasized that the binary case represents a special case of ourformulation, where we set C = 1 and is the sigmoid func-tion. On the other hand, in a multi-class problem, C > 2and is the softmax function.",
  "Instance-level Approach": "The instance-level approach is characterized by performingclass predictions on each patch. It can be implemented intwo different modes, although in both cases the first step isto apply h to the patch representations. This projects thepatch features into a new embedding space of dimension C,corresponding to the patch logits.For the first instance-level mode (I-1), the second step isto apply , which converts the logits obtained in the previ-ous step into class probabilities for each patch. Then, in thefinal step, the pooling function is applied, resulting in thepredicted class probabilities y C.The second instance-level mode (I-2) reverses the orderof these two steps. It applies the pooling , followed by ,to convert the pooled logits into probabilities.Notice that in the special case of a binary problem, I-1and I-2 lead to the same classification result, even thoughthey estimate different probabilities for the two classes.Therefore, we only show results with I-1 for this setting.",
  "Embedding-level Approach": "The embedding-level approach starts by aggregating patchrepresentations with the pooling function, . This leads to anew vector, zI R1D, representing image-level features.Only then are these features transformed to logits with thelinear projection h. Finally, the operator converts the log-its to the predicted class probabilities, y C.When is the average pooling, this approach revertsback to the standard CNN strategy of applying a global av-erage pooling before the classification head.",
  ". Experimental Setup": "We evaluated the performance of the proposed approach intwo medical image classification problems: skin cancer di-agnosis in dermoscopy and breast cancer diagnosis in mam-mography. For each of these settings, we trained a set ofbaselines (standard CNNs and ViTs models) and our MILapproaches described in . In order to comparewith a recent approach that also performs patch selection,we trained various EViTs with different keep rate val-ues for the attentive patches. Our results are all evaluatedin terms of class recall (R) and balanced accuracy (BA -the average of the recalls). Below, we describe the adopteddatasets, as well as the training specifications.",
  ". Datasets": "Skin Cancer.For dermoscopy image analysis, we ad-dress two main challenges: binary and multi-class classi-fication. The ISIC 2019 dataset is our primarydataset, which we partition into a training (80%) and val-idation (20%) sets. For the binary problem, the trainingphase consisted of using only the melanoma (MEL) andnevi (NV) classes from the ISIC 2019. We also evaluatedthe generalization capabilities of the proposed approachin several out-of-domain datasets: HIBA , PH2 ,and Derm7pt . Each of the previous datasets containsimages collected from patients of different demographicgroups, allowing us to do a preliminary assessment of thefairness of the different models.For the multi-class classification task, we employed theISIC 2019 dataset for training and validationpurposes, while the HIBA dataset served as our test-ing ground.These datasets encompass eight diagnosticcategories: Actinic keratosis (AK), Basal cell carcinoma(BCC), Benign keratosis (BKL), Dermatofibroma (DF),Melanoma (MEL), Melanocytic nevus (NV), Squamous cellcarcinoma (SCC), and Vascular lesion (VASC). pro-vides a detailed overview of the class distributions for thetraining, validation, and test datasets for both binary andmulti-class scenarios.Breast Cancer. For mammography image analysis, weevaluated our proposal on the binary task of distinguishing",
  "Total20,2285,0661,270200827": "breasts with findings from those with no findings. We em-ployed the DDSM dataset for both training (90%)and validation (10%). Specifically, the training dataset com-prised 2, 428 cases identified with findings and 1, 342 caseswithout findings. For validation, we evaluated 260 caseswith findings against 137 cases without findings.Preprocessing. All input images were resized to a uni-form size of 224 224 3. Mammography images wereconverted from grayscale to RGB by replicating the colorchannel.To preserve the original aspect ratio of boththe dermoscopy and mammography images, we appliedpadding to ensure that all images had a square format.",
  ". Training Setup": "Encoder Block.We explored a variety of CNN-basedpre-trained backbones for the patch encoder block, encom-passing ResNet-18 (RN-18), ResNet-50 (RN-50), VGG-16, DenseNet-169 (DN-169), and EfficientNetB3 (EN-B3).Additionally, we explored ViT models DEiT-S and EViT-S with a keep rate (Kr) of 0.7. Every model was pre-trainedon the ImageNet1k dataset .For ViT-based encoders, images were partitioned into1414 patches. To match these resolutions in the CNN ex-periments, we collected the CNN feature maps with a spa-tial dimension of 1414. Among the evaluated CNN back-bones, EN-B3 emerged as the best model, leading to thecreation of the MIL-EN-B3 model with 2.2M parameters.From the transformer variants, DEiT-S was chosen, formingthe MIL-DEiT-S model with 22M parameters. Comparisonbetween the various backbones can be seen in Supplemen-tary Material.For the instance-level approach, we conducted experi-ments with three MIL pooling operators: the max operator,the average operator, and the top-k average operator, withthree different configurations: k 12.5%, k = 25%, andk = 50%. Our experiments showed that k = 25% wasgenerally the best representation of the top-k average pool- ing operator. In the case of the embedding-level approach,we used the following MIL pooling operators: the column-wise global max pooling operator, the column-wise globalaverage pooling operator, and the column-wise global top-kaverage pooling operator, with k = 25% (different k valueswere tested and can be seen in Supplementary Material).Baseline Models. The baseline models for our exper-iments used the same backbones as the MIL models de-scribed above. Here, however, we use the full architecture,replacing only the classification layer with one specific toour medical problems.EViT Baselines. We adopted the EViT-S configurationwith 22.1M parameters as the standard for comparing ourmethod. In all configurations, we placed the token reorga-nization block in three different layers: the 3rd, the 6th, andthe 9th layers. The keep rate (Kr) determines the num-ber of attentive tokens retained by the token reorganiza-tion block. We explored different settings and settled onKr = 0.6 and Kr = 0.7. With these choices, the EViTmodel with Kr = 0.6 preserves 43 patches, while the modelwith Kr = 0.7 retains 68 patches out of 196 patches. Theassessment of additional Kr values can be seen in the Sup-plementary Material.Training Configurations. All models were trained us-ing a class-weighted categorical Cross Entropy (CE) lossfunction since all datasets are highly unbalanced. Onlineaugmentation strategies tailored to each task were used inorder to enhance model robustness. Specifically, for der-moscopy image classification, we adopted the augmentationconfiguration outlined by Touvron et al. . In contrast,the mammography image classification task incorporatedrandom horizontal and vertical flips, along with random ro-tations. All tested models were implemented and trainedusing PyTorch on NVIDIA GeForce RTX 3090 and 4090.",
  ". Binary MIL": "Our experimental results for the binary classification of der-moscopy and mammography images are summarized in Ta-ble 2. These results show that there is a marginal differ-ence between the MIL models and their baseline counter-parts. For the ISIC 2019 set, the standard deviation for BAstands at a modest 1.60%, and for the DDSM dataset, aneven smaller standard deviation of 0.49% is observed. Interms of backbones, the one based on DEiT-S achieves bet-ter performances in the case of skin cancer, suggesting that",
  "EMax91.087.494.595.892.399.3Topk91.586.996.194.792.397.1Avg91.487.495.495.691.999.3": "in this context the patch correlation may contain discrimi-native information. In the case of breast cancer, it seemsthat the performances are fairly similar. When comparingthe DEiT-S and MIL-DEiT-S results with those of EViT, weconclude that: i) discarding several patches does not signifi-cantly affect the performance of the models; and ii) our MILframework is very competitive against more complex mod-els for information selection. Finally, regarding MIL withinstances against the embedding versions, we conclude thatperforming an analysis at the patch level seems to be betterin most settings.In summary, the binary results underscore the potentialof integrating a MIL into CNN and ViT pipelines to selectkey patches for diagnosis. This process effectively reducesthe information used by the classifiers without significantperformance loss, suggesting that the most discriminativeinformation is concentrated in a few regions of the images.",
  ". Multi-class MIL": "In this section, we discuss the results of our proposed multi-class MIL framework, as detailed in . The tablecompares the performance of our MIL methods with that ofbaseline models and EViT on the challenging task of multi-class classification of dermoscopy images. Here we showthe results for ISIC 2019 and HIBA , which was theonly test set where all classes matched the ones used fortraining. In this section, we will only discuss the results forISIC 2019, while the HIBA results will be discussed in thenext section.",
  "EMax82.433.8Topk82.235.6Avg82.636.2": "When comparing our multi-class MIL framework withthe baseline models, we find that the latter performs bet-ter in this task. There is a more noticeable difference inperformance when using a CNN as the backbone of ourmodel.This disparity might stem from how the EN-B3model and the MIL model handle feature extraction. Specif-ically, the EN-B3 model may use different types of fea-tures than the MIL model, which extracts feature mapsfrom a previous layer in the network. This leads to mod-els that have a significantly different number of parame-ters, which may also impact their ability to memorize in-domain features. Specifically, our MIL-EN-B3 model op-erates with only 2.2M parameters, as opposed to the moresubstantial 11M parameters of the EN-B3 model. This hy-pothesis is further supported by the similar performancebetween the instance-level and embedding-level MIL ap-proaches. The embedding-level approach mainly serves asa bridge between the MIL framework and traditional CNN-based architectures. The performance comparison betweenthe embedding-level MIL-EN-B3 and the EN-B3 baselinemirrors the gap observed with the instance-level MIL, sug-gesting that the disparities in the results could in fact beattributed to the different sizes of the model architectures.Regarding the two instance-level approaches for multi-",
  "EMax81.368.694.285.375.095.674.758.391.1Topk83.171.195.189.780.099.475.155.295.1Avg82.269.694.785.075.095.076.061.191.0": "class classification (I-1 and I-2), it seems that I-1 performsbetter across the two backbones. This leads us to recom-mend the I-1 formulation in future multi-class MIL appli-cations. Once more, the instance-level MIL seems to con-sistently outperform the embedding approach, reinforcingthe importance of performing a patch-based analysis ratherthan aggregating all or a subset of the image information.The multi-class classification task is inherently morechallenging than its binary counterpart. Nevertheless, theresults from the EViT model still prove that information se-lection is desirable and leads to improved performance com-pared to traditional models that classify over the entire im-age. Moreover, our MIL models with DEiT-S backbone stillhold their own against both the baselines and EViT. Thesefindings challenge the assumption that larger, more complexmodels are always better. In essence, our results argue for amore targeted, efficient approach to medical image analysis.",
  ". Generalization Across Diverse Demographies": "We evaluated the robustness of our MIL-based modelsacross varied dermoscopy image datasets, each represent-ing distinct patient demographics not seen in our training orvalidation sets. displays the binary results for skintests from the HIBA, PH2, and Derm7pt datasets, while Ta-ble 3 shows the results for HIBA.Our MIL models consistently outperformed their base-line counterparts on unseen data. For example, the instance-level MIL-EN-B3 model using max pooling outperformedthe EN-B3 baseline by a BA of 3.6% on the HIBA dataset.In addition, the multi-class results on the HIBA test set (see ) show that even if there is a performance drop whenthe MIL models are compared with the baselines, they stillgeneralize better to unseen data. Finally, contrary to whatis stated in the literature, the embedding-level approachdid not consistently outperform the instance-level models.In fact, the instance-level models often outperformed theirembedding-level counterparts. These results suggest thatthe key patches identified by the instance-level MIL mod-els may have significant medical relevance, contributing toimproved generalization to unseen data.In summary, the generalization results show that ourMIL models deal better with unseen data, being potentiallymore fair across different demographics, despite using lessinformation. This establishes MIL as a promising methodto improve fairness in medical image analysis.",
  ". Explainability of MIL Models": "Our results suggest that the key regions identified by theinstance-level MIL models are correlated with meaning-ful information within the image, thereby increasing themodels robustness to dataset bias compared to baselinecounterparts. Nevertheless, it is critical to assess whetherthese identified regions truly capture clinical findings or aresimply artifacts. To clarify this, we compared heatmapsgenerated by the EN-B3 baseline to those generated byour MIL models, focusing on the binary classification ofmelanoma versus nevus in the PH2 dataset. shows the Grad-Cam visualizations for thebaseline EN-B3 model, highlighting the areas that influenceits predictions for melanoma (MEL) and nevus (NV) clas- Instance-level MIL-EN-B3 with top-k average pool.",
  "Probability": "Heatmap MELGrad-CAM MELGrad-CAM NV Instance-level MIL-EN-B3 with max pool. InputProbability Heatmap MELGrad-CAM MELKey patch . Visualization of the key patches identified by two different MIL approaches. On the left, we have the instance-level MIL-EN-B3using max polling, and on the right, we have the instance-level MIL-EN-B3 using the top-k average pooling operator, with k 12.5%,The images used for visualization are taken from the PH2 test set and refer to the binary classification task of melanoma vs. nevus. InputGrad-CAM MELGrad-CAM NV",
  ". Grad-Cam heatmap visualizations generated by theEN-B3 baseline model for images from the PH2 test set": "sifications, while illustrates the key regions identi-fied by the instance-level MIL-EN-B3 model for the samelesions. In the MIL setting, we compare two pooling strate-gies: max pooling and top-k average pooling, which aver-ages the values of the 25 most relevant patches.Notably, our instance-level MIL approaches consistentlyhighlight key patches that, similar to ROIs in clinical di-agnosis, lie within or at the edges of lesions for melanomacases, or bordering healthy tissue for nevus cases. This pref-erence for clinically relevant areas confirms the ability ofour models to extract medically relevant features, validat-ing their performance on the validation set and their abilityto generalize to unseen data.When comparing the heatmaps, it is clear that the EN-B3 model produces coarse heatmaps, whereas the MILsinstance-level approaches produce a more detailed delin-eation of relevant regions, providing finer explanations.This clarity and specificity reinforces MILs position as amore clinically translatable tool that can potentially provide",
  ". Conclusions": "This work demonstrates the potential of integrating MILinto the pipeline of CNNs and ViTs to select relevantpatches and use less information in the classification stage.Our findings reveal that despite a significant reduction inthe amount of information, MIL models can achieve resultscomparable to more complex networks. By focusing on themost discriminative image patches, similar to clinical prac-tice, MIL models show a strong ability to generalize acrossdifferent datasets and demographic groups. This suggestsa promising direction towards creating more explainable,efficient, and fair medical image analysis systems. More-over, the assessment of selected regions underscores MILsalignment with clinical relevance, providing a more inter-pretable decision-making process that mirrors the diagnos-tic approach of medical experts. Future work will focus onvalidating the identified key regions against specific medi-cal concepts, as well as exploring these regions to improvemodel performance and further enhance clinical applicabil-ity and fairness.",
  "Alceu Bissoto, Catarina Barata, Eduardo Valle, and San-dra Avila. Even small correlation and diversity shifts posedataset-bias issues. Pattern Recognition Letters, 2024. 1": "Kaitao Chen, Shiliang Sun, and Jing Zhao. Camil: Causalmultiple instance learning for whole slide image classifica-tion. In Proceedings of the AAAI Conference on ArtificialIntelligence, pages 11201128, 2024. 2 Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung,Theresa C Thai, Kathleen Moore, Robert S Mannel, HongLiu, Bin Zheng, and Yuchen Qiu. Recent advances and clin-ical applications of deep learning in medical image analysis.Medical Image Analysis, 79:102444, 2022. 1, 2 Noel C. F. Codella, David A. Gutman, Emre Celebi, M. EmreCelebi, Brian Helba, Michael A. Marchetti, Stephen W.Dusza, Nabin Mishra, Aadi Kalloo, Aadi Kalloo, Konstanti-nos Liopyris, Nabin K. Mishra, Harald Kittler, and Allan C.Halpern. Skin lesion analysis toward melanoma detection:A challenge at the international symposium on biomedicalimaging (isbi) 2016, hosted by the international skin imag-ing collaboration (isic). arXiv: Computer Vision and PatternRecognition, 2016. 4 Marc Combalia, Noel C. F. Codella, Veronica Rotemberg,Brian Helba, Veronica Vilaplana, Ofer Reiter, Ofer Re-iter, Allan C. Halpern, Susana Puig, and Josep Malvehy.Bcn20000: Dermoscopic lesions in the wild. arXiv: Imageand Video Processing, 2019. 4 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 5 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An imageis worth 16x16 words: Transformers for image recognitionat scale. International Conference on Learning Representa-tions, 2020. 2 Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis,Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-lix A Wichmann. Shortcut learning in deep neural networks.Nature Machine Intelligence, 2(11):665673, 2020. 1, 2 Tiago Goncalves, Isabel Rio-Torto, Lus F Teixeira, andJaime S Cardoso.A survey on attention mechanisms formedical applications: are we moving toward better algo-rithms? IEEE Access, 10:9890998935, 2022. 2 Michael D. Heath, Michael D. Heath, Kevin W. Bowyer,Kevin W. Bowyer, Daniel B. Kopans, Daniel B. Kopans,W. Philip Kegelmeyer, W. Philip Kegelmeyer, Richard H.Moore, Richard H. Moore, Kai-Chun Chang, K.I. Chang, S.Munishkumaran, and S. Munishkumaran. Current status of",
  "Maximilian Ilse, Jakub M. Tomczak, and M. Welling.Attention-based deep multiple instance learning.Interna-tional Conference on Machine Learning, 2018. 2": "Jeremy Kawahara, Sara Daneshvar, Giuseppe Argenziano,and Ghassan Hamarneh. Seven-point checklist and skin le-sion classification using multitask multimodal neural nets.IEEE Journal of Biomedical and Health Informatics, 23(2):538546, 2019. 4 Mara Agustina Ricci Lara, Mara Victoria RodrguezKowalczuk, Maite Lisa Eliceche, Mara Guillermina Ferra-resso, Daniel Luna, Susana Carrera Bentez, and Luis DanielMazzuoccolo. A dataset of skin lesion images collected inargentina for the evaluation of ai tools in this population. Sci-entific Data, 2023. 4, 6 Giuseppe Di Leo, Consolatina Liguori, Antonio Pietrosanto,Gabriella Fabbrocini, and M. Sclavenzi. Elm image process-ing for melanocytic skin lesion diagnosis based on 7-pointchecklist: a preliminary discussion. 2004. 1 Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multipleinstance learning network for whole slide image classifica-tion with self-supervised contrastive learning. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1431814328, 2021. 2",
  "Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, JueWang, and P. Xie.Not all patches are what you need:Expediting vision transformers via token reorganizations.arXiv.org, 2022. 2, 4": "Pedro Henrique Martins, Vlad Niculae, Zita Marinho, andAndre FT Martins. Sparse and structured visual attention. In2021 IEEE International Conference on Image Processing(ICIP), pages 379383. IEEE, 2021. 2 Teresa Mendonca, Pedro M. Ferreira, Jorge S. Marques,Andre R. S. Marcal, and Jorge Rozeira. Ph 2 - a dermoscopicimage database for research and benchmarking. Annual In-ternational Conference of the IEEE Engineering in Medicineand Biology Society, 2013. 4",
  ". Top-k Search In The MIL Framework": "We conducted experiments with three different k values inthe instance-level approach: approximately 12.5%, 25%,and 50%.In the context of our experiments, which in-volved input images of 224 224 resolution and patchesof 16 16 resolution, we dealt with a total of N = 196patches. This implies that for the k 12.5% configuration,we considered 25 patches; for k = 25%, we worked with49 patches, and for k = 50%, we used 98 patches. To de-termine the optimal k value for the top-k average operatorin the instance-level approach, we conducted experimentsusing various backbones on the validation set of the ISIC2019 dataset. A summary of these experiments is shown in. RN-18RN-50VGG-16DN-169EN-B3DEiT-SEViT-SDEiT-cls EViT-fused MIL Backbones BA Top-k k=12.5%k=25%k=50% . Search for the optimal k hyperparameter in the instance-level top-k average MIL pooling operator. We explored three val-ues for the hyperparameter: k 12.5%, k = 25%, and k = 50%.Our experiments were conducted and evaluated on the validationset of the ISIC 2019 dataset, employing different MIL backbones.The backbones included RN-18, RN-50, VGG16, DN-169, EN-B3, DEiT-S, DEiT-cls (DEiT with the CLS token), EViT-S, andEViT-fused (EViT with the fused embedding). Notably, with EViTbackbones, k 12.5% resulted in only 9 patches, k = 25% re-tained 17 patches, and k = 50% maintained 34 patches. Theseresults indicate that using more patches in the bag evaluation doesnot necessarily lead to better performance.",
  "The plot in shows that the choice of k for thetop-k average operator in the instance-level approach doesnot significantly impact the performance of the different": "MIL models. This observation suggests that not all patcheswithin a dermoscopy image contribute equally to the clas-sification task, indicating that the discriminative informa-tion lies within a (small) subset of image patches. Inter-estingly, the k 12.5% and k = 25% scenarios con-sistently yield the highest BA results across different MILbackbones. Based on these results, we selected k = 25% asthe default configuration for the top-k average pooling op-erator. To ensure a fair comparison between the embedding-level and instance-level approaches, we have also adoptedk = 25% as the preferred setting for the column-wise globaltop-k average operator in the embedding-level approach.",
  "and Derm7pt, for the binary classification task of melanoma versusnevus. The x-axis represents the different Kr values, while the y-axis represents the corresponding BA results": "The combination of layers in which token reorganizationtakes place and the choice of Kr (keep rate) are the mostcritical hyperparameters in the EViT architecture. Since wedecided to fix the token reorganization block in the 3rd, 6th,and 9th layers, an extensive search for the best Kr config-urations in the EViT-S model was required. Since the Krhyperparameter plays a crucial role within the EViT archi-tecture, we conducted a series of experiments to examine itsimpact on the EViT-S configuration. shows the BA",
  ". Selection Of MIL Backbones And Baselines": "To facilitate the experiments conducted in this paper, wecarefully selected a representative model from each of theCNN and Transformer baselines. This selection process in-volved an extensive evaluation of each model on the valida-tion set of the ISIC 2019 dataset, focusing on the binaryclassification problem of melanoma (MEL) versus nevus(NV). The results of this evaluation are summarized in table1.",
  "ViT-S91.386.895.8ViT-B90.685.395.8DEiT-S91.786.796.7DEiT-B91.787.296.2": ". Evaluation results of a set of baseline models on thevalidation set of the ISIC 2019 dataset. The baseline models in-clude different architectures, including RN-18, RN-50, VGG-16,DN-169, EN-B3 from the CNN-based category, and ViT-S, ViT-B, DEiT-S, DEiT-B from the Transformer-based category. Theevaluation is performed for the binary classification problem ofmelanoma versus nevus.",
  ". Additional MIL Heatmaps": "provides a visual representation of the different vi-sualizations produced by the instance-level MIL model us-ing the top-k average pooling operator. In this case, the lastrow shows the gradients associated with the patches classi-fied as nevus. . Results of multi-class image classification on the ISIC 2019 validation set. I-1 and I-2 denote the first and second instance-levelapproaches, respectively, and E denotes the embedding-level approach.",
  "EMax82.480.987.875.183.376.489.074.692.2Topk82.273.483.674.393.875.692.269.196.1Avg82.670.588.779.889.675.191.565.999.9": ". Visualization of the heatmaps generated by the MIL classifier, specifically the instance-level MIL model using the top-k averagepooling operator. The backbone used for the MIL model is the RN-18. The images are taken from the validation set of the ISIC 2019dataset, and belong to the binary problem of melanoma vs. nevus. The Figure shows the input images in the first row, followed by the patchprobability heatmap for the melanoma class in the second row. The third row shows the gradient heatmap for each patch. In this case, thelast row shows the gradients with respect to the patches that the model predicted to be nevus."
}