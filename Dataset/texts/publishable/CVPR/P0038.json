{
  "Abstract": "There is a gap in the understanding of occluded objectsin existing large-scale visual language multi-modal mod-els. Current state-of-the-art multimodal models fail to pro-vide satisfactory results in describing occluded objects forvisual-language multimodal models through universal vi-sual encoders.Another challenge is the limited numberof datasets containing image-text pairs with a large num-ber of occluded objects. Therefore, we introduce a novelmultimodal model that applies a newly designed visual en-coder to understand occluded objects in RGB images. Wealso introduce a large-scale visual-language pair datasetfor training large-scale visual-language multimodal mod-els and understanding occluded objects. We start our ex-periments comparing with the state-of-the-art models.",
  ". Introduction": "The latest multimodal dialogue models , such as MiniGPT-4 and mPLUG-Owl showedthat despite significant progress, their description of large-scale language models for occluded objects remains unsat-isfactory.Therefore, we propose OCC-MLLM, a visual languagemodel (shown in ) designed to understand occludedobjects in image conversations. To achieve this goal, we de-veloped a visual encoder module consisting of the commonCLIP model and the proposed 3D model . Addition-ally, a dataset of 600, 000 image-text pairs was created andreleased.",
  ". Method": "First, we formulate the generative process of the proposedMLLM, named Occlusion-Aware Multimodal Large Lan-guage Model (OCC-MLLM), for occlusion-aware descrip-tions of objects at hand.Second, we introduce the for-mulation details of each proposed OCC-MLLM module. Third, the proposed occlusion loss is calculated, and anocclusion-aware training strategy for large multi-modal lan-guage models is introduced. We represent the generationprocess of the proposed OCC-MLLM into three parts: in-put formula, model forwarding, and decoding.",
  "Input Formulation": "The input of the proposed OCC-MLLM consists of imagesand text.Putting aside specific architectural differences,OCC-MLLM generally applies a visual encoder module toextract visual tokens from raw images and uses a cross-modal mapping module to map them to text space as theinput of LLM. The mapped visual tokens are used as part ofthe LLM input along with the text input. The visual tokensare represented as xv = {x0, x1, . . . , xN1}. N representsthe length of the visual token, which is a fixed number inmost cases. Similarly, the input text is segmented using a to-kenizer and expressed as xp = {xN, xN+1, . . . , xM+N1}.The image and text tokens are then concatenated as the finalinput {xi}T 1t=0 where T = N + M.",
  "Decoding": "After applying logits p (xt | x<t), several decoding strate-gies have been developed, including greedy decoding,Beam Search , DoLa, etc. The decoded tokens are con-catenated to the last one of the original input text for thenext generation round until the end of the generation. Theproposed OCC-MLLM applies a beam search strategy is a decoding strategy based on cumulative scores.",
  ". Dual Visual Encoder Module": "In forwarding the proposed OCC-MLLM, we designed anew visual encoder module, which consists of two visualencoders. The first visual encoder is the joint CLIP ,which is used to extract the visual embedding (token) xvfrom the RGB input xv1 without a specific occlusion rep-resentation.The second visual encoder is used to pro-vide a representation of the occluded object visual embed-ding(token) xv2. Then, the combined representation is cal-culated as follows:",
  ". Visual Embedding For Occluded Objects": "For the second visual encoder to provide the visual embed-ding (token) xv2 of the occluded object, we designed thesecond visual encoder f3D, which is composed as follows:In the first step, a representation of the signed distancefunction (SDF) of the occluded object in 3D space is cal-culated (shown in Figure. 2). This representation is merged",
  "where fs and fo are the subject SDF decoder and the objectSDF decoder, respectively, v represents the 3D point": "In the second step, we apply the calculated SDFs of bod-ies and objects for 3D mesh reconstruction (shown in Fig-ure 2). The computed object SDFobject(v) already containsthe visual representation of the object under occlusion. Wereconstruct the 3D mesh Mobj of the occluded object andthen project it into the 2D RGB space Iobj. Then, to makethe 2D visual representation Iobj easy to use with large lan-guage models, we use the visual embedding of xv2 as theextracted embedding of the CLIP model . The abovecalculation is expressed as follows:",
  ". Dataset Overview": "We released a custom dataset (OCC-HO) containing600,000 image-text pairs. This dataset was released to de-scribe occluded objects, and to the best of our knowledge,it is for text descriptions of occluded objects. Besides, wemanually calculate the occlusions that about a quarter of theobjects are occluded on average,It is important to note that the annotations of each sampleare manually checked. Furthermore, we apply the proposeddataset in the instruction tuning stage. All input images areresized to 224 224. (Shown in ).",
  ". Dataset Annotation": "We have provided 5 questions for each image in this dataset.These five questions are: Whats the object in the hand?;Is the object in the hand round?; Is the object in the handlong?; Is the object in the hand thin?; and Describe theobject in the hand. They are all based on the category,shape, and specific description of the objects in their hands.Firstly, we used GPT4V [? ] to provide preliminaryanswers to the five questions raised regarding the images.Then, manually check the answers to each image. Man- ual correction and completion of the answers to the imagequestions will be done for incorrect or unanswered images.Finally, all the image questions and answers are organizedinto image pairs to construct a complete dataset of imagesand texts for occluding objects.In addition, we also utilized a 3D reconstruction method to reconstruct these occluded objects and obtained2D images containing only objects, further improving ourdataset. In this way, the constructed dataset includes im-ages of occluded objects and two image text datasets thatonly contain images of unobstructed objects after 3D re-construction.",
  ". Experiments on GPT4v": "We first test the performance of GPT4v on the testingpart of the proposed dataset. Four instructions are appliedto test each sample in the testing dataset. And the accuracyis demonstrated in the . As shows, the ac-curacy of the GPT4v is low. In detail, the accuracy forthe instruction 1(Whats the object in the hand?) is 0.0361,the accuracy for the instruction 2(Is the object in the handround?) is 0.6705, the accuracy for the instruction 3(Is theobject in the hand long?) is 0.6290, the accuracy for theinstruction 4(Is the object in the hand thin?) is 0.5370. Itdemonstrates that GPT4V cannot achieve satisfactoryresults for the occluded objects.",
  ". Experiments on MiniGPT4-V2)": "To effectively evaluate the dataset proposed for occlu-sion object text description, we fine-tuned two epochs forMiniGPT4-V2.The hyperparameter settings for fine-tuning MiniGPT4-V2 are set as the following: The batchsize is 16; The learning rate is 0.00002; The weight at-tenuation coefficient is 0.In addition, to verify the ef-fectiveness of the constructed occluded dataset. As shows, in comparison with GPT4V, the accuracy ishigher for instruction 1, the accuracy is about the same forinstruction 2, instruction 3 and instruction 4. The visual en-coder of the proposed MiniGPT4-V2 is the common clipencoder. (Shown in ). It demonstrates that fine-tuning on a classical multi-modal large language modelwith a single joint clip encoder improves the accuracyof the instructions from 0.0361 to 0.3209. However, 0.3209is still not satisfactory.",
  ". Experiments on the Proposed SDF Encoder": "Then, we explore the ability of the SDF encoder forthe test description of the occluded objects. At the stage1, we pretrain the SDF encoder for the task of 3Dreconstruction from a single image. At stage 2, we fine-tune the SDF encoder, which loads the weights of thereconstruction and then fine-tune the encoder for the taskof object classification.In detail, we use each image of the occluded object inthe training dataset and the category of the correspondingobject for training. In the testing phase, we calculate theaccuracy of the occluded objects given a single image of theoccluded object. As demonstrates, the accuracy ofthe instruction 1 is further improved from 0.3209 to 0.5194.We will continue fine-tuning the proposed SDF encoderfor the tasks corresponding to the instruction 2-4.",
  ". Future Experiments": "As the above results demonstrated, the proposed SDFencoder is promising for understanding the occluded ob-jects. We will explore this encoders ability in subsequentexperiments.Firstly, the SDF encoder continues to be fine-tuned forthe task of the instruction 2, instruction 3 and instruction 4.Secondly, the SDF encoder is merged with a classical largelanguage model to provide the text description of theoccluded objects. Finally, the SDF encoder and the com-mon clip encoder are merged as the equation 3 shown,and the proposed dual visual encoder module is applied ina classical multi-modal large language model for thedescription of the occluded objects. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. Advances inNeural Information Processing Systems, 35:2371623736,2022. 1",
  "Zerui Chen, Shizhe Chen, Cordelia Schmid, and Ivan Laptev.gsdf:Geometry-driven signed distance functions for 3dhand-object reconstruction, 2023. 1, 2, 3, 4": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, ShijieGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-angyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-sual instruction model.arXiv preprint arXiv:2304.15010,2023. 1 Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, PingLuo, and Kai Chen.Multimodal-gpt: A vision and lan-guage model for dialogue with humans.arXiv preprintarXiv:2305.04790, 2023.",
  "Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,Zecheng Tang, and Nan Duan.Visual chatgpt: Talking,drawing and editing with visual foundation models. arXivpreprint arXiv:2303.04671, 2023. 1": "Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,Michael Zeng, and Lijuan Wang.Mm-react: Promptingchatgpt for multimodal reasoning and action. arXiv preprintarXiv:2303.11381, 2023. 1 Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Yaya Shi, et al.mplug-owl: Modularization empowerslarge language models with multimodality. arXiv preprintarXiv:2304.14178, 2023. 1"
}