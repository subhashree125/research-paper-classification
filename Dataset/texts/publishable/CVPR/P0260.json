{
  "Abstract": "Remarkable strides have been made in reconstructingstatic scenes or human bodies from monocular videos. Yet,the two problems have largely been approached indepen-dently, without much synergy. Most visual SLAM methodscan only reconstruct camera trajectories and scene struc-tures up to scale, while most HMR methods reconstruct hu-man meshes in metric scale but fall short in reasoning withcameras and scenes. This work introduces Synergistic Cam-era and Human Reconstruction (SynCHMR) to marry thebest of both worlds. Specifically, we design Human-awareMetric SLAM to reconstruct metric-scale camera poses andscene point clouds using camera-frame HMR as a strongprior, addressing depth, scale, and dynamic ambiguities.Conditioning on the dense scene recovered, we further learna Scene-aware SMPL Denoiser to enhance world-frameHMR by incorporating spatio-temporal coherency and dy-namic scene constraints. Together, they lead to consistentreconstructions of camera trajectories, human meshes, anddense scene point clouds in a common world frame. Projectpage:",
  "*Part of this work was done when interned at Adobe Research": "1. IntroductionPhysically plausible 3D human motion reconstruction frommonocular videos is a long-standing problem in computervision and graphics and has many applications in charac-ter animation, VFX, video games, sports, and healthcare.It requires estimating 3D humans across video frames in acommon coordinate even with a moving camera. While hu-man mesh recovery (HMR) has made significant progressrecently , most existing methods typically estimate 3Dhumans in the camera coordinate by one frame at a time andfail to disambiguate camera motion. It calls for methods tojointly reconstruct 3D human and camera motion in a con-sistent global coordinate system from monocular videos. Inother words, taking a video captured by a moving camera asinput, the method should recover both temporally and spa-tially coherent movements of human bodies and cameras.Intuitively, if the accurate camera motion is given, onecan transform the bodies from individual camera frames toa common world frame by multiplying the inverse of cam-era extrinsic matrices. In practice, with humans movingin the scene, estimating the camera motion of a video isstill an open challenge in monocular SLAM . It not onlyfalls short in capturing accurate depths on views with smallcamera translations but more crucially, only estimates scene",
  "? dynamic?": ". Illustration of three types of ambiguities in visualSLAM. We show SLAM reconstruction results from DROID-SLAM . (a) Depth ambiguity occurs when there are only mi-nor camera translations between different views. This can lead togeometric failures in reconstruction such as the folded back corri-dor in the side view. (b) Scale ambiguity is inherent in monocularSLAM systems and requires additional reference for disambigua-tion. (c) Dynamic ambiguity gets pronounced when moving fore-grounds dominate frames. Over-reliance on foreground key pointswill result in incorrect camera trajectories. structures and camera trajectories up to scale. The humanmotion also breaks the static key point assumption in thebundle adjustment. As a result, one needs additional refer-ence to disambiguate the depth, the scale, and the dynamicas illustrated in .To leverage SLAM results in HMR pipelines, current world-frame HMR methods often refine camera poses byintegrating either partial camera parameters, such as aglobal scale of the translation , or full extrinsic ma-trices in an optimization-based framework.However, their optimization-based nature leads to complexmulti-stage schemes, making the overall pipeline unneces-sarily slow and easy to break.In this work, we explore a fundamentally different wayto marry the best of HMR and SLAM. A 2D object can firstbe lifted from the image plane to the camera frame and thentransformed into a common 3D space. This two-step pro-cess coincides with the combination of camera-frame HMR,which brings imaged 2D humans to 3D camera frames, andSLAM, which estimates the camera-to-world transforma-tion. Noticing these correspondences, we leverage camera-frame HMR as a strong prior to bridge from the image planeto the camera frame for disambiguating SLAM, and utilizeSLAM reconstructions to constrain the transformation ofhuman meshes from individual camera frames to a common global space. The overall pipeline thus results in a bettersynergy of the two, which we dub Synergistic Camera andHuman Reconstruction (SynCHMR).We design SynCHMR based on several insights. First,despite camera-frame HMR methods cannot reconstruct hu-mans in a coherent global frame, the estimated body di-mension and location still provide cues to disambiguateSLAM. Unlike SLAHMR which applies SLAM outof the box and corrects the scale afterward, we endowthe SLAM process with human meshes from camera-frameHMR to address ambiguities. To this end, we capitalizeon estimated absolute depths to provide pseudo-RGB-D in-puts for SLAM and confine the bundle adjustment tostatic backgrounds. Since current depth estimation meth-ods predicts either relative depth maps or depths withdata biases, we propose to calibrate their outputs by align-ing with estimated human bodies in the camera frame .With these priors, SLAM knows the depth, scale, and dy-namic information from HMR and consequently estimatesless ambiguous scene structures and camera poses.Next, we place human meshes in the common coordi-nate recovered by SLAM. The gap between human trackstransformed from camera frames and their real plausibleworld-frame motions stems from two sources of error: noiseinduced by camera-frame HMR and by SLAM.Motionprior models can be used for denoising pur-poses as they contribute to the temporal coherence of world-frame human tracks. However, their exclusive focus on hu-man modeling either leaves them agnostic to the underly-ing scenes or assumes the scene is a simple groundplane . Our intuition is that when placing a human,static elements of the scene, such as the ground, and dy-namic components like moving objects are both possible tobe in contact with the human, thereby providing clues forplacing the body coherently and compatibly with the scene.We hence introduce a Scene-aware SMPL Denoiser thatlearns to denoise the transformed human tracks by consider-ing both temporal consistencies of moving humans and im-plicit constraints from dynamic scenes. This global aware-ness makes it more flexible for in-the-wild videos.Our contributions can be summed up as follows: We present a novel pipeline, SynCHMR, that takes amonocular video as input and reconstructs human mo-tions, camera trajectory and dense scene point clouds allin one global coordinate, as shown in , whereas cur-rent world-frame HMR methods can recoveronly an estimated or pre-defined ground plane. We propose a novel Human-aware Metric SLAM processto robustly calibrate estimated depth with estimated hu-man meshes, resulting in metric-scale camera pose esti-mation and metric-scale scene reconstruction.",
  "There is considerable prior arts of HMR. We briefly dis-cuss how they adopt different camera models and refer thereaders to for a more comprehensive review": "HMR from a single image. State-of-the-art (SOTA) meth-ods use parametric body models and esti-mate the parameters either by fitting to detected image fea-tures or by regressing directly from pixels withdeep neural networks . These approaches assume weak perspec-tive/orthographic projection or pre-define the focal length asa large constant for all images. Kissos et al. show thatreplacing focal length with a constant closer to ground truthalleviate the body tilting problem. SPEC and Zolly estimate focal length to account for perspective distor-tion. CLIFF takes into account the location of humansin images to regress better poses in the camera coordinates.Many of these camera-frame HMR methods assume zerocamera rotation, which entangles body rotation and camerarotation. When applied on video data, they fail to recon-struct humans in a coherent global space since they operatein a per-frame manner and hence cannot reason about howthe camera moves across frames.HMR from videos aims to regress a series of body pa-rameters from a temporal sequence. It opens up new prob-lems such as whether the reconstructed bodies are in a com-mon global coordinate or not. Some temporal methods con-sider a static camera , which makes the cameraspace a natural choice of the common coordinate. The chal-lenge of coherent global space emerges when the cameramoves. Early methods show promising resultson videos of dynamic cameras. Despite the reconstructedhuman meshes look great when overlaid on images, they donot share a common coordinate in 3D.Recent HMR methods capitalize on human motion priorto constrain the global trajectories in the world space,which in turn implicitly disentangles human movementfrom camera movement.GLAMR consider a data-driven prior models learned on large-scale MoCap databasee.g. AMASS , while D&D and Yu considerphysic-inspired prior. These world-frame HMR methodsoften struggle on noise in local poses caused by partial oc-clusions, which is very common in in-the-wild videos withclose-up shots and crowd scenes. Kaufmann et al. andBodySLAM++ circumvent this problem by employingIMU sensors to provide more robust body estimates but re- quire extra sensory devices. To fully disentangle human andcamera motion, another line of work lever-ages state-of-the-art SLAM techniques, e.g. , toexplicitly estimate camera motion from the input video andinfer the body parameters in the world coordinate of SLAM.Closest to us is SLAHMR which solves for a globalscale to connect the pre-computed SLAM results and bodytrajectories. To carefully guide the optimization process,these methods tend to have complex, multi-stage optimiza-tion schemes, making the overall pipeline easy to break andunnecessarily slow.Note that in stark contrast to the methods above, whicheither assume or estimate a simple ground plane as scenerepresentation, SynCHMR reconstructs dense scenes fromin-the-wild videos without pre-scanning with extra devicesa priori like in . We provide detailedcomparisons with these world-frame HMR in Supp. Mat.",
  ". Method": "Taking as input an RGB video {It RHW 3}Tt=1 withT frames and N people in the scene, we aim to recoverhuman meshes {Vwnt R36890}N,Tn=1,t=1, dynamic scenepoint clouds {Pwmt RHW 3}Tt=1, and correspondingcamera poses {Gmt SE(3)}Tt=1 in a common world coor-dinate system. The superscripts w, c, and m denote the worldframe, the camera frame, and the metric scale, respectively.To this aim, we propose a two-phase alternative condition-ing pipeline as depicted in . In the first phase, wecalibrate camera motion by injecting a camera-frame hu-man prior to SLAM. This resolves depth, scale, and dy-namic ambiguities, yielding metric-scale camera poses anddynamic point clouds. Subsequently, in the second phase,we transform the camera-frame human tracks into the worldframe and utilize the dynamic point clouds obtained in thefirst phase for conditional denoising.",
  "wm}": ". The architecture of SynCHMR. Our pipeline comprises two phases. The first phase, Human-aware Metric SLAM (Sec. 3.2), in-fers metric-scale camera poses and metric-scale point clouds by exploiting the camera-frame human prior. The second phase, Scene-awareSMPL Denoising (Sec. 3.3), involves the conditional denoising of world-frame noisy SMPL parameters. These parameters, initialized bytransforming from the camera frame, get refined through conditioning on the dynamic point clouds obtained in the first phase. The wholepipeline thus reconstructs humans, scene point clouds, and cameras harmoniously in a common world frame.",
  "ij,(2)": "where pij = rij+pij is the corrected correspondence, is the Mahalanobis distance which weighs the error termswith ij = diag wij, and G and d are updated poses andinverse depths. Upon this objective, DROID-SLAM con-siders an additional term that penalizes the squared distancebetween the measured and predicted depth if the input iswith an extra sensor depth channel {Dt}Tt=1.",
  "HMR": "We employ 4DHumans for reconstructing camera-framehuman meshes from an in-the-wild video. Specifically, itperforms per-frame human mesh recovery with an end-to-end transformer architecture and associates them to formhuman tracks. Each tracked human n in frame t is repre-sented by SMPL parameters as {nt, nt, nt, nt},including global orientation nt R33, body pose nt R2233, shape nt R10, and root translation nt R3.Then the parametric SMPL model can use these parametersto recover a human mesh with vertices Vnt R36890 inmetric scale: Vnt = SMPL(nt, nt, nt) + nt.",
  "To start off, we estimate per-frame depth maps {Dt}with an off-the-shelf depth estimator, ZoeDepth and": "predict per-frame human instance segmentation masks{Mnt} with an image instance segmentation network,Mask2Former . We adapt ZoeDepth for video-consistentdepth estimation by choosing a per-video metric head fromthe majority vote of per-frame routers, for which we dubZoeDepth+.While ZoeDepth claims to estimate metricdepths, we observe a domain gap when inference on newdatasets. Consequently, we only treat its output as up-to-affine depths that need to be further aligned with the met-ric scale. To aid our optimization with human awareness,we use camera-frame human meshes {Vcnt} recovered by4DHumans to introduce a metric prior.",
  "Calibrating Depth with Human Prior": "We calibrate the per-frame depths with human meshes inHuman-aware Depth Calibration. This involves optimizingtwo parameters, a world scale s and a world offset o, sharedacross all frames. During optimization, we linearly trans-form Dt to Dt = sDt+o and unproject these depth maps tocamera-frame point clouds {Pct} with Pct = 1(pt, Dt).Our intuition is to align the human point cloud Pcnt =MntPct with the camera-frame human mesh vertices Vcntin terms of absolute depth and size. To achieve pixel-wisealignment, we use a depth term to pull points on the hu-man point cloud toward their corresponding human meshvertices along the z-axis",
  "TPE": ". The architecture of Scene-aware SMPL Denoiser.World-frame noisy SMPL parameters {wnt, nt, nt, wnt}0 arefirst projected by a linear layer and summed with temporal po-sitional embeddings (TPE) to get initial latent humans {zSMPLnt,0 }.Per-frame point clouds are aggregated to xscene and encoded withthe point encoder E. Then we query the encoded scene E(xscene)with latent humans {zSMPLnt,0 } in the scene-conditioned denoiser Dand feed the result {zSMPLnt,1 } to prediction heads {P, P, P, P}to obtain denoised SMPL parameters {wnt, nt, nt, wnt}1. 0-norm indicating the number of non-zero pixels on a mask.As the recovered human meshes can be noisy in depthbut still have a stable body dimension, we also adopt a sizeterm to leverage the relative position of mesh vertices",
  "Disambiguating SLAM with Calibrated Depth": "While DROID-SLAM originally supports RGB-D in-put mode where the D channel stands for sensor depth,one cannot trivially access sensor depths from in-the-wildvideos. Our insight is that an estimated absolute depth canbe utilized as a depth prior, albeit noisy. So we combinethe original RGB video and the calibrated depth as pseudo-RGB-D inputs {It, Dmt } to disambiguate depth and scale.Furthermore, we modify the cost function Eq. (2) to re-solve the dynamic ambiguity by masking out dynamic fore-grounds in confidence maps",
  "wnt = Rtcnt,wnt = Rt(cnt + c) + tmt c,(11)": "where c = c(nt) is the pelvis location in the shape blendbody mesh. Note that we do not need to introduce an extracamera scale as SLAHMR since the camera poses havealready been in the metric scale. The root-relative poses ntand the shapes nt stay unchanged as in the camera frame.We denote the initialized and the denoised parameters witha suffix 0 and 1 respectively, i.e. {wnt, nt, nt, wnt}0,1.",
  "Constraining Humans with Dynamic Scenes": "Different from existing works that incor-porate energy terms in optimization to apply explicit sceneconstraints, we propose to learn implicit scene constraintswith a Scene-aware SMPL denoiser shown in . Thenoisy initial SMPL parameters {wnt, nt, nt, wnt}0 arefirst projected to a latent space, where it gets further up-dated by conditioning on implicit scene constraints",
  "{zSMPLnt,1 }Tt=1 = D{zSMPLnt,0 }Tt=1, E(xscene) + TPE,(13)": "where FC is a shared linear layer, TPE is shared tempo-ral positional embeddings, {zSMPLnt, }Tt=1 RT D is the D-dimensional latent for human n, and xscene RLC is theC-channel dynamic scene point clouds with a total numberof points L. E and D refer to the scene encoder and thescene-conditioned denoiser, respectively. We set C = 7which is the concatenation of point coordinates {Pwmt },colors {It}, and estimated human semantic segmentationmasks {Mt =",
  ". Experimental Setting": "Datasets. We assess the performance of SynCHMR primar-ily for global human motion estimation but also report theaccuracy of estimated camera trajectories. Traditional videodatasets in HMR literature are typically captured by staticcameras, e.g. , hence not suitable for ourpurpose. Standard SLAM benchmarks such as donot meet our needs either as there is often no human movingin the scene. We consider the following datasets.3DPW is an in-the-wild dataset captured with iPhones.The ground truth bodies are not in coherent world frames sowe use it to supervise root relative poses and for evaluation.EgoBody has ground-truth poses captured by multipleKinects and egocentric-view sequences recorded by a head-mounted device, whose trajectories are further registeredin the world space of Kinect array. We use it for trainingthe SMPL denoiser in Sec. 3.3 and for evaluation (on bothbody and camera estimation). For HMR evaluation, unlike considering only the validation set, we additionallyreport results on its completely withheld test set.EMDB is a new dataset providing SMPL poses fromIMU sensors and global camera trajectories. We include itfor training the SMPL denoiser to enrich the diversity anduse the camera trajectories to evaluate the quality of SLAM.Evaluation Metrics.For HMR evaluation, we reportcommon PA-MPJPE, which measures the quality of root-relative poses. For datasets that have ground-truth poses in aworld coordinate, we follow and consider WA-MPJPEand FA-MPJPE. The former measures the error after align-ing the entire trajectories of the prediction and ground truthwith Procrustes Alignment , while the latter aligns onlywith the first frame.We also report acceleration errors.For SLAM, we consider absolute trajectory error (ATE) forcamera trajectory evaluation as well as the threshold accu-racy (n), the absolute relative error (REL), and the rootmean squared error (RMSE) for scene depth evaluation .Implementation Details. In Human-aware Depth Calibra-tion, we use the L-BFGS algorithm with learning rate 1 to",
  ". Comparison results on 3DPW-Test. The row in gray isthe full pipeline of SynCHMR. We abbreviate PA-MPJPE as PA,with the same below for FA-MPJPE (FA) and WA-MPJPE (WA)": "optimize for a maximum of 30 iterations. As for the Scene-aware SMPL Denoiser, we train it on the union of 3DPW-Train, EgoBody-Train, and EMDB for 100k steps with anAdamW optimizer, a batch size of 16, and a learning rate of1e-5. For camera-frame SMPL ground truths like in 3DPW,we only incorporate body shapes and poses in train-ing. We train the denoising process by randomly samplinga temporal window size T spanning 64 to 128 and inferencewith T = 100. The scene-conditioned denoiser D is param-eterized with a 6-layer Transformer Decoder. For the sceneencoder E, we consider ViT and SPVCNN in Tab. 4 and re-port results for SPVCNN in Tabs. 1 and 2. Before inputtingthe world-frame noisy SMPL parameters to the denoiser,we first interpolate wnt,0 and nt,0 on SO(3), nt,0 on R10,and wnt,0 on R3 when there are missing observations.",
  ". Comparison Results": "We first evaluate the estimated local poses with PA-MPJPEon 3DPW, which is common in the literature. In Tab. 1,we show that placing the bodies from 4DHumans alreadyleads to lower error than SLAHMR. Passing them throughthe denoiser further reduces the error. We note that PA-MPJPE only measures local pose accuracy not the qualityof global trajectories. Since 3DPW does not support anyworld metrics, Tab. 1 only aims to show that SynCHMRproduces reasonable local poses on a common dataset.Next, we assess the quality of global motion estimation,which is essentially a more challenging task. Tab. 2 showsthe results on EgoBody. Note that current optimization-based methods report the error of the validationset. For fairness and completeness, we report results onboth validation and test sets and run state-of-the-art meth-ods on the test set when the code is available. In Tab. 2, wesee that the proposed SynCHMR has the overall lowest PA-MPJPE, FA-MPJPE, and WA-MPJPE (gray rows). Com-paring it with the row above (4DHumans) confirms the ben-efit of our scene-conditioned denoiser. For a fair compari-son, we also initialize the global optimization of SLAHMRwith 4DHumans, which is more accurate than PHALP+ inSLAHMR, but we do not observe improvement. Notably,despite the concurrent work PACE has a tightly in-tegrated SLAM and body fitting objective, it still uses na-tive DROID-SLAM to initialize the camera parameters likeSLAHMR does. This is arguably sub-optimal as the initial-",
  ". Ablation study for different scene encoders and fea-tures regarding world-frame HMR. Init. and Pred. refer to be-fore and after SMPL denoising, respectively": "ization is not aware of body information, which can leadto errors that cannot be corrected in the global optimizationstage. Consequently, it also has higher world-space errors.Optimization methods often employ a zero velocity term tosmooth out human motion, which explains the lower accel-eration error. However, we do not observe a big differencein jittery between our results. Please refer to Supp. Mat. formore details.",
  ". Ablation Study": "We ablate the design choices in SynCHMR.In Tab. 3,we evaluate SLAM-optimized camera trajectories and scenedepths with EgoBody and EMDB. We see that directly in-cluding un-calibrated monocular depths does not guaran-tee more accurate estimations (3rd vs. 1st and 4th vs. 2nd row).Precluding the dynamic foreground pixels withMask2Former generally improves performance. We em-pirically find that our depth calibration with human priorworks the best when using it with foreground masking,which has the lowest error in both datasets. More SLAMevaluation and discussion can be found in Supp. Mat. In Tab. 4, we verify the benefit of scene conditioning forthe SMPL denoiser. We train it with EgoBody-train in dif-ferent conditioning schemes and report the T = 32 resultson EgoBody-val. First, placing the predicted bodies from4DHuman in the global space directly with estimated cam-era extrinsics has the highest error (1st row). When condi-tioning on a constant zero tensor, the denoiser behaves likea motion prior and reduces the error (2nd row). To encodethe appearance and geometry information of the scene, weconsider ViT or SPVCNN as the encoder E and tryvaried combinations of appearance features (RGB), geome-try features (XYZ) and aggregated subject masks (Mask).When using ViT to encode the scene, adding XYZ fea-tures or masks does not reduce the error. In contrast, whenusing SPVCNN, adding RGB information or conditioningon masks does improve performance. Overall, SPVCNNyields lower errors than ViT and enabling all conditioningleads to the lowest world-space error measure.",
  ". Qualitative Analysis and Discussion": "In the first two rows of , we visualize the results of3DPW and EgoBody in a global space. Despite occlusions,our SynCHMR estimates human meshes reliably and placesthem in a dense scene point cloud, whereas the scenes inGLAMR and SLAHMR consist of only a sim-ple ground plane. Applying scene constraints with such anoverly simplified scene can result in erroneous estimation,e.g., incorrect human trajectories as shown in the top viewof the 1st row, and the vertically shortened human bodies inthe 4th row of (d). Note that since TRACE is scene ag-nostic, the ground plane in (c) is only for visualization, notnecessarily indicating scene penetration.We also test on more in-the-wild DAVIS videos con-taining human subjects. Since DAVIS provides no ground-truth human meshes nor camera trajectories, we show onlythe visual comparison. The 3rd row shows that we can han-dle multi-person cases as well as SLAHMR, while GLAMRoften fails when multiple humans and dynamic camerasboth occur. In a challenging scenario where the subject istaking selfies (the 4th row), both GLAMR and SLAHMR areconfused by the foreground human dominating the framesand reconstruct an almost static global trajectory, failing to",
  "Failed": ". Qualitative comparison among world-frame HMR approaches. We show (b) GLAMR and (c) TRACE results withtheir pre-defined ground planes, (d) SLAHMR outputs with its estimated ground plane, and (e) our SynCHMR outputs with densescenes. In the first row, we also demonstrate top-view human trajectories within circles. See supplementary for video results. disentangle the camera and the human motions due to thedynamic ambiguity. TRACE fails to produce results due tosevere frame truncation. In contrast, SynCHMR still suc-cessfully provides reasonable trajectories.",
  "As SynCHMR focuses on disentangling camera and humanmovements, we follow SLAHMR to approximate the fo-cal length as W +H": "2. When the subject has a shape that thebody model cannot explain well, e.g., children or obese peo-ple, calibrating depth with the estimated bodies is less ideal.As we develop and validate SynCHMR on real videos, itsaccuracy on composed or generated videos remains an openquestion. Finally, since SynCHMR handles dynamic sceneswith moving subjects, it does not require an a priori scannedstatic scene. This opens up new challenges, such as incor-porating dynamic point clouds as scene constraints.",
  "We present SynCHMR, a method that reconstructs cameratrajectories, human bodies, and dense scenes from in-the-": "wild videos all in one global coordinate. SynCHMR hastwo core innovations. First, it leverages monocular depthestimation and uses the dimension and location of humanmeshes to calibrate the range of depth. This allows SLAMto better resolve the inherent scale ambiguity problem asshown in the experiment. Second, we train a data-drivenmotion denoiser and condition it with the scene in the sameglobal coordinate, which is the first such scene-conditionedmotion prior.Combining the two, the full SynCHMR pipeline uses human bodies to improve SLAM, and the bet-ter estimated scene and camera trajectory, in turn, providebetter constraints for feed-forward human motion denois-ing. It achieves SOTA results on common benchmarks com-pared with existing optimization-based approaches. We appreciate constructive comments from Duygu Ceylan.This project was partially supported by the NIH under con-tracts R01GM134020 and P41GM103712, and by the NSFunder contracts DBI-1949629, DBI-2238093, IIS-2007595,IIS-2211597, and MCB-2205148.",
  "Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,and Matthias Muller. ZoeDepth: Zero-shot transfer by com-bining relative and metric depth. arXiv, 2023. 2, 4, 6, 7,1": "Federica Bogo, Angjoo Kanazawa, Christoph Lassner, PeterGehler, Javier Romero, and Michael J. Black. Keep it SMPL:Automatic estimation of 3D human pose and shape from asingle image. In European Conference on Computer Vision(ECCV), 2016. 3 Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-der Kirillov, and Rohit Girdhar.Masked-attention masktransformer for universal image segmentation. In ComputerVision and Pattern Recognition (CVPR), pages 12901299,2022. 4, 7, 2 Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Ky-oung Mu Lee. Beyond static features for temporally consis-tent 3d human pose and shape from a video. In ComputerVision and Pattern Recognition (CVPR), 2021. 3 Yudi Dai, Yitai Lin, Chenglu Wen, Siqi Shen, Lan Xu, JingyiYu, Yuexin Ma, and Cheng Wang. HSC4D: Human-centered4D scene capture in large-scale indoor-outdoor space usingwearable imus and LiDAR. In Computer Vision and PatternRecognition (CVPR), pages 67926802, 2022. 3 Yudi Dai, Yitai Lin, Xiping Lin, Chenglu Wen, Lan Xu,Hongwei Yi, Siqi Shen, Yuexin Ma, and Cheng Wang.Sloper4d: A scene-aware dataset for global 4d human poseestimation in urban environments. In Computer Vision andPattern Recognition (CVPR), pages 682692, 2023. 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In International Conference on Learning Representa-tions (ICLR), 2021. 7 Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,Angjoo Kanazawa, and Jitendra Malik. Humans in 4D: Re-constructing and tracking humans with transformers. In In-ternational Conference on Computer Vision (ICCV), 2023.2, 3, 4, 5, 6, 7, 1",
  "Chengan He, Jun Saito, James Zachary, Holly Rushmeier,and Yi Zhou. Nemf: Neural motion fields for kinematic an-imation. Advances in Neural Information Processing Sys-tems, 35:42444256, 2022. 2": "Dorian F Henning, Tristan Laidlow, and Stefan Leutenegger.BodySLAM: joint camera localisation, mapping, and humanmotion tracking. In European Conference on Computer Vi-sion (ECCV), pages 656673. Springer, 2022. 2, 3, 1 Dorian F Henning, Christopher Choi, Simon Schaefer, andStefan Leutenegger.BodySLAM++:Fast and tightly-coupled visual-inertial camera and human motion tracking.In International Conference on Intelligent Robots and Sys-tems (IROS), pages 37813788. IEEE, 2023. 3 Chun-Hao P. Huang, Hongwei Yi, Markus Hoschle, MatveySafroshkin, Tsvetelina Alexiadis, Senya Polikovsky, DanielScharstein, and Michael J. Black. Capturing and inferringdense full-body human-scene contact. In Computer Visionand Pattern Recognition (CVPR), pages 1327413285, 2022.3, 1 Catalin Ionescu, Dragos Papava, Vlad Olaru, and CristianSminchisescu. Human3.6M: Large scale datasets and predic-tive methods for 3D human sensing in natural environments.Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 36(7):13251339, 2014. 6 Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, XiaoweiZhou, and Kostas Daniilidis.Coherent reconstruction ofmultiple humans from a single image. In Computer Visionand Pattern Recognition (CVPR), 2020. 3 Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,Iain Matthews, Takeo Kanade, Shohei Nobuhara, and YaserSheikh. Panoptic studio: A massively multiview system forsocial motion capture. In International Conference on Com-puter Vision (ICCV), 2015. 6",
  "Muhammed Kocabas, Nikos Athanasiou, and Michael J.Black.Vibe: Video inference for human body pose andshape estimation. In Computer Vision and Pattern Recog-nition (CVPR), 2020. 3": "Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,and Michael J. Black.PARE: Part attention regressor for3D human body estimation. In International Conference onComputer Vision (ICCV). IEEE, 2021. 3 Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch,Lea Muller, Otmar Hilliges, and Michael J. Black. SPEC:Seeing people in the wild with an estimated camera. In In-ternational Conference on Computer Vision (ICCV), pages1103511045, 2021. 3 Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, YunrongGuo, Michael J. Black, Otmar Hilliges, Jan Kautz, and UmarIqbal. PACE: Human and motion estimation from in-the-wild videos.In International Conference on 3D Vision(3DV), 2024. 2, 3, 5, 6, 7, 1 Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, andKostas Daniilidis. Learning to reconstruct 3D human poseand shape via model-fitting in the loop. In International Con-ference on Computer Vision (ICCV), 2019. 3 Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,and Cewu Lu. Hybrik: A hybrid analytical-neural inversekinematics solution for 3d human pose and shape estimation.In Computer Vision and Pattern Recognition (CVPR), pages33833393, 2021. 3",
  "Jiefeng Li, Siyuan Bian, Chao Xu, Gang Liu, Gang Yu,and Cewu Lu. D&D: Learning human dynamics from dy-namic camera. In European Conference on Computer Vision(ECCV), 2022. 3, 1, 2": "Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,and Youliang Yan. CLIFF: Carrying location information infull frames into human pose and shape estimation. In Euro-pean Conference on Computer Vision (ECCV), pages 590606, 2022. 3 Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and YuLi. One-stage 3d whole-body mesh recovery with compo-nent aware transformer.In Computer Vision and PatternRecognition (CVPR), pages 2115921168, 2023. 3 Miao Liu, Dexin Yang, Yan Zhang, Zhaopeng Cui, James MRehg, and Siyu Tang. 4D human body capture from egocen-tric video via 3D scene grounding. In International Confer-ence on 3D Vision (3DV), pages 930939. IEEE, 2021. 3, 5,1, 2",
  "motion capture as surface shapes. In International Confer-ence on Computer Vision (ICCV), pages 54415450, 2019.3": "DushyantMehta,OleksandrSotnychenko,FranziskaMueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll,and Christian Theobalt.Single-shot multi-person 3Dpose estimation from monocular RGB.In InternationalConference on 3D Vision (3DV), 2018. 6 Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, andMichael J. Black. Expressive body capture: 3D hands, face,and body from a single image. In Computer Vision and Pat-tern Recognition (CVPR), 2019. 3 F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.Gross, and A. Sorkine-Hornung. A benchmark dataset andevaluation methodology for video object segmentation. InComputer Vision and Pattern Recognition (CVPR), 2016. 7,2 Rene Ranftl,Katrin Lasinger,David Hafner,KonradSchindler, and Vladlen Koltun. Towards robust monoculardepth estimation: Mixing datasets for zero-shot cross-datasettransfer. Transactions on Pattern Analysis and Machine In-telligence (TPAMI), 44(3):16231637, 2020. 2 Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,Srinath Sridhar, and Leonidas J. Guibas.HuMoR: 3Dhuman motion model for robust pose estimation.In In-ternational Conference on Computer Vision (ICCV), pages1146811479, 2021. 2, 3 Yu Rong, Takaaki Shiratori, and Hanbyul Joo. FrankMo-cap: Fast monocular 3d hand and body motion capture byregression and integration. In International Conference onComputer Vision Workshops (ICCVw), 2021. 3 Nitin Saini, Chun-Hao P Huang, Michael J Black, and AamirAhmad. SmartMocap: Joint estimation of human and cameramotion using uncalibrated rgb cameras. IEEE Robotics andAutomation Letters, 2023. 2, 3, 1",
  "Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang.Recovering 3d human mesh from monocular images: A sur-vey. arXiv preprint arXiv:2203.01923, 2022. 1": "Timo von Marcard, Roberto Henschel, Michael J. Black,Bodo Rosenhahn, and Gerard Pons-Moll.Recovering ac-curate 3D human pose in the wild using IMUs and a mov-ing camera. In European Conference on Computer Vision(ECCV), pages 614631, 2018. 6 Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qing-ping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and TakuKomura. Zolly: Zoom focal length correctly for perspective-distorted human mesh reconstruction. In International Con-ference on Computer Vision (ICCV), 2023. 3",
  "Yuanlu Xu, Song-Chun Zhu, and Tony Tung. DenseRaC:Joint 3d pose and shape estimation by dense render-and-compare. In International Conference on Computer Vision(ICCV), 2019. 3": "Ming Yan, Xin Wang, Yudi Dai, Siqi Shen, Chenglu Wen,Lan Xu, Yuexin Ma, and Cheng Wang. Cimi4d: A large mul-timodal climbing motion dataset under human-scene interac-tions. In Computer Vision and Pattern Recognition (CVPR),pages 1297712988, 2023. 3 Vickie Ye, Georgios Pavlakos, Jitendra Malik, and AngjooKanazawa.Decoupling human and camera motion fromvideos in the wild. In Computer Vision and Pattern Recog-nition (CVPR), pages 2122221232, 2023. 2, 3, 5, 6, 7, 8,1 Jae Shin Yoon, Zhixuan Yu, Jaesik Park, and Hyun Soo Park.HUMBI: A large multiview dataset of human body expres-sions and benchmark challenge.Transactions on PatternAnalysis and Machine Intelligence (TPAMI), 45(1):623640,2021. 6",
  "Ri Yu, Hwangpil Park, and Jehee Lee.Human dynamicsfrom monocular video with dynamic camera movements.Transactions on Graphics (TOG), 40(6), 2021. 3, 5, 1, 2": "Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, andJan Kautz. GLAMR: Global occlusion-aware human meshrecovery with dynamic cameras. In Computer Vision andPattern Recognition (CVPR), pages 1102811039, 2022. 2,3, 7, 8, 1 Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, BillFreeman, Rahul Sukthankar, and Cristian Sminchisescu.Weakly supervised 3d human pose and shape reconstructionwith normalizing flows. In European Conference on Com-puter Vision (ECCV), 2020. 3 Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d humanpose and shape regression with pyramidal mesh alignmentfeedback loop.In International Conference on ComputerVision (ICCV), 2021. 3",
  "Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys,and Siyu Tang. Learning motion priors for 4D human bodycapture in 3D scenes. In International Conference on Com-puter Vision (ICCV), 2021. 2, 3": "Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, TaeinKwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Ego-Body: Human body shape and motion of interacting peo-ple from head-mounted devices. In European Conference onComputer Vision (ECCV), 2022. 3, 6 Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Ru-binstein, Noah Snavely, and William T Freeman. Structureand motion from casual videos. In European Conference onComputer Vision (ECCV), pages 2037. Springer, 2022. 3",
  ". SynCHMR Setting vs. Prior Work": "We compare the setup of recent world-frame HMR methodsthat handle dynamic cameras in Tab. 5. Methods that esti-mate world-frame body parameters through learning-basedapproaches often ignore the camera at test time .On the other hand, optimization approaches need to esti-mate the camera at test time to fit to the detected 2D jointkey points , and we have discussedthe downsides of their camera estimation approaches inSec. 2 of the main paper. It is still worth noting that none ofthese methods reconstruct dense scene point clouds, exceptLiu et al. , who adopt COLMAP for this purpose.However, since COLMAP is not robust enough for in-the-wild videos, they demonstrate results only on sequences ac-quired in a controlled capture settings. In stark contrast,SynCHMR is designed to work on casual videos. It doesnot assume the scene is a ground plane as in or isscanned a priori as in . It has a light-weight setupbut it reconstructs the most information human meshes,camera trajectory, and dense scene, all in one coherentglobal space.",
  ". SLAM Evaluation": "Qualitative ablation study.In Tab. 3 of the main paperwe quantitatively analyze the contribution of each designchoice in our human-aware SLAM; here we provide visualexamples. In , we show the results where we grad-ually add each design choice as stronger priors to the na-tive visual SLAM. Merely using RGB inputs in (a),naive DROID-SLAM fails in capturing the geometrystructure of the scene. This results in a back-folded corri-dor, which is far from reasonable. The dynamic human alsoconfuses the SLAM model, leading to a messy human pointcloud in the center and everything else surrounding it in acircular shape. Masking out the human in (b) onlyremoves the messy human point cloud but still produces abroken geometry since the depth ambiguity remains. An ex-tra estimated depth channel in (c)(d) helps to resolvethe depth ambiguity and correct the scene geometry. How-ever, as we filter out points with epipolar inconsistency, theresulting point cloud is rather sparse. This indicates depthestimation with ZoeDepth does not guarantee each pointhas a consistent location across different frames, and SLAMfails to correct this error. Finally, our Human-aware MetricSLAM in (e) is able to output a dense point cloud.This reflects the success in finding more points with consis-tent 3D locations. As the scene reconstruction depends oncamera pose estimation in SLAM, our pipeline potentiallyproduces more accurate camera poses.Results on TUM-RGBD dataset. Tab. 3 of the main pa-per considers HMR datasets that provide ground truth cam-era trajectories. Here, we report the results on a commonSLAM benchmark TUM-RGBD .Since it does notcontain humans in the scene, we can only apply our adaptedvideo-consistent ZoeDepth , namely ZoeDepth+, with-out calibrating the scales. In Tab. 6, we see that this depth-augmented version yields an average lower error than theoriginal DROID-SLAM. This suggests that despite the un-known scale, estimated monocular depth still provides priorinformation to better reason about camera trajectories. Onecan see this as a byproduct of SynCHMR.",
  "Metric SLAM (Ours)": ". Qualitative comparisons of the parkour sequence from DAVIS . (a) naive DROID-SLAM reconstructed pointcloud with RGB input; (b) DROID-SLAM reconstructed point cloud with RGB input, where the foreground humans are masked out by aninstance segmentation method Mask2Former ; (c) DROID-SLAM reconstructed point cloud with RGB-D input, where the depth channelis from ZoeDepth estimations, the same below; (d) DROID-SLAM reconstructed point cloud with RGB-D and instance segmentationmask inputs (e) our proposed Human-aware Metric SLAM reconstructed point cloud. Please see the webpage for video results.",
  ". HMR Evaluation": "Qualitative comparison. In , we compare the esti-mated human body meshes and scene point clouds of (a)SLAHMR and (b) our SynCHMR. We observe incom-patible scales and structures in SLAHMR visualizations.This can be the reason why SLAHMR uses a ground planeinstead of point clouds in the global refinement stage. SMPL denoiser analysis.To better understand the im-pact of our scene-aware SMPL denoiser, we annotate thetest set of EgoBody with 5 attributes: frame trunca-tion, scene occlusion, subject reappearing, camera motion,and motion blur. In , we plot the amount of errorreduced by SMPL denoiser in these attributes. First, it con-firms that the denoiser always brings improvement as thereare no negative numbers. Second, we identify truncation,large camera motion, and motion blur as three primary sce-narios where the denoiser helps greatly, as we see noticeableupward trends for them. The underlying mechanism mightbe our SMPL denoiser captures more comprehensive sceneinformation with dynamic scene modeling, which is benefi-cial in these situations where single-frame observations arebad and one needs to rely on cross-frame clues. Runtime analysis. We report the runtime of our SynCHMRalong with state-of-the-art models in Tab. 2. Note that theruntime for PACE does not include camera-frame ini-tialization with HybrIK . To integrate per-frame hu-man bodies into a smooth motion, SLAHMR employsa HuMoR-like motion prior, which is slow due to its auto-regressive nature. PACE improves this by proposinga parallel motion prior. Similarly, while adding in sceneawareness, our feed-forward SMPL Denoiser also benefitsfrom the parallel inference of the Transformer architecture."
}