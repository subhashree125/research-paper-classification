{
  "Abstract": "Video quality assessment (VQA) is a challenging prob-lem due to the numerous factors that can affect the percep-tual quality of a video, e.g., content attractiveness, distor-tion type, motion pattern, and level. However, annotatingthe Mean opinion score (MOS) for videos is expensive andtime-consuming, which limits the scale of VQA datasets,and poses a significant obstacle for deep learning-basedmethods. In this paper, we propose a VQA method namedPTM-VQA, which leverages PreTrained Models to transferknowledge from models pretrained on various pre-tasks, en-abling benefits for VQA from different aspects.Specifically, we extract features of videos from differentpretrained models with frozen weights and integrate themto generate representation. Since these models possess var-ious fields of knowledge and are often trained with labelsirrelevant to quality, we propose an Intra-Consistency andInter-Divisibility (ICID) loss to impose constraints on fea-tures extracted by multiple pretrained models. The intra-consistency constraint ensures that features extracted bydifferent pretrained models are in the same unified quality-aware latent space, while the inter-divisibility introducespseudo clusters based on the annotation of samples andtries to separate features of samples from different clus-ters. Furthermore, with a constantly growing number ofpretrained models, it is crucial to determine which modelsto use and how to use them. To address this problem, wepropose an efficient scheme to select suitable candidates.Models with better clustering performance on VQA datasetsare chosen to be our candidates.Extensive experimentsdemonstrate the effectiveness of the proposed method.",
  "Equal contribution. Corresponding authors": ".The KoNViD-1k dataset provides video frames thatdemonstrate a correlation between content/motion patterns andvideo quality. To identify potential reasons for poor perceptualvideo quality, we have highlighted specific factors in italics thatcorrespond to the labeled MOS. Ciscos Visual Networking Index (VNI), global IP videotraffic is predicted to account for 82% of all IP traffic by2022, both in business and consumer sectors . The sig-nificant increase in video content consumption poses sig-nificant challenges for video providers to deliver better ser-vices. Since the perceptual quality of videos has a signifi-cant impact on the Quality of Experience (QoE), identifyingthe quality of videos has become one of the most impor-tant issues . Video quality assessment(VQA) aims to assess the perceptual quality of input videosautomatically, imitating the subjective feedback of humanswhen viewing a video. It has been extensively studied inthe context of assessing compression artifacts, transmissionerrors, and overall quality . Data-drivendeep learning-based methods have been drawing more andmore attention compared to conventional methods based onhand-crafted features, as they possess better performance.Compared with other high-level computer vision tasks,datasets for VQA are much smaller. One of the most popu-lar datasets for human action classification Kinetics has650,000 clips, while the popular VQA dataset KoNViD-1k has only 1,200 videos. One of the reasons is becauseVQA is a highly subjective task . To obtain an un-",
  "arXiv:2405.17765v1 [cs.CV] 28 May 2024": "biased label, it is recommended by annotation guidelines that the subjective quality of a single video should bemeasured in a laboratory test by calculating the arithmeticmean value of multiple subjective judgments, i.e., MeanOpinion Score (MOS). Take KoNViD-1k as an example, ithas 114 votes for each video on average. This significantlyraises the cost of labeling and limits the size of the VQAdataset. Such a small amount of data limits the power ofdata-driven VQA methods. To deal with the problem, mostexisting VQA methods choose to finetuneusing weights pretrained on common larger datasets (e.g.,ImageNet ). However, existing works showthat the perceptual quality of a video is related to many fac-tors, e.g., content attractiveness, aesthetic quality, distortiontype, motion pattern, and level. Only considering content-based pretrained models may not be sufficient for VQA.Thus, in this work, we focus on how to better utilize a largeamount of available pretrained models to benefit VQA.The present study initially observes a correlation be-tween VQA tasks and other computer vision tasks. To illus-trate, displays several examples from the KoNViD-1kdataset. It is reasonable to assume that models pretrainedon datasets for various pre-tasks have the ability to cap-ture distinct characteristics regarding video quality. Conse-quently, we conduct a simple clustering experiment utiliz-ing Large Margin Nearest Neighbor (LMNN) to inves-tigate the correlation between typical pretrained models andthe VQA task. Based on the findings, we propose a practi-cal approach, named PTM-VQA (PreTrained Models-VQA),which leverages pretrained models as feature extractors andpredicts the quality of input videos based on integrated fea-tures. As the parameters of pretrained models remain fixed,we can introduce more pretrained models without exhaust-ing computational resources.Moreover, we notice that labels in common datasets forpretraining are quite quality-irrelevant. For instance, a clearphoto of a puppy with high quality and a blurred photoof a puppy may have the same object-wise label, whereastheir quality-wise label may be significantly different. Thiswill confuse the learning process for the VQA task. There-fore, we propose an Intra-Consistency and Inter-Divisibility(ICID) loss, which applies constraints on features extractedby multiple pretrained models from different samples.Specifically, model-wise intra-consistency requires featuresextracted by different pretrained models to be in the sameunified quality-aware latent space.Meanwhile, sample-wise inter-divisibility introduces pseudo clusters based onthe MOS of samples and aims to separate features of sam-ples from different clusters.Furthermore, as the number of pretrained models contin-ues to grow (e.g., PyTorch image models library (Timm) supports over 700 pretrained models), finding mod-els suitable for the VQA task through trial-and-error be- comes unfeasible. Therefore, we propose to use the Davies-Bouldin Index (DBI) to evaluate the clustering resultsand adopt it as the basis for model selection and weightingfor feature integration. To summarize, the main contribu-tions are specified below: We explore and confirm the association between pre-trained models utilizing various pre-text tasks and theireffectiveness in performing VQA. Moreover, we present apractical non-reference VQA method named PTM-VQA,which exploits cutting-edge pretrained models with diver-sity to benefit VQA effectively. To constrain features with diversity into a unified quality-aware space and eliminate the mismatch between objec-tive and perceptual annotations, we propose an ICID loss.To avoid looking for a needle in a haystack, we proposean effective way to select candidate models based on DBI,which also determines the contributions of different pre-trained models. PTM-VQA achieves SOTA performance with a rathersmall amount of learnable weights on three NR-VQA datasets, including KoNViD-1k, Live-VQC, andYouTube-UGC. Extensive ablations also prove the effec-tiveness of our method.",
  ". Related Work": "VQA.Based on whether the pristine reference video isrequired, VQA methods can be classified as Full Refer-ence (FR), Reduced Reference (RR), and No Reference(NR). Our work will be focused on the NR-VQA method.Traditional NR-VQA methods either measure video qual-ity by rule-based metric , or predict MOS by an esti-mator (e.g., Multi-Layer Perceptron, Support Vector Ma-chine) based on hand-crafted features . In recent years,deep learning-based VQA methods have been studied andsurpassed traditional methods.STDAM introduceda graph convolution to extract features and a bidirectionallong short-term memory network to handle motion infor-mation. StarVQA proposed encode spatiotemporal in-formation of each patch on video frames and feed them intoa Transformer. RAPIQUE proposed to combine tex-ture features and deep convolutional features. These works,however, neglected the correlation between VQA and othertasks and did not explore other datasets. BVQA tookone step further and proposed to transfer knowledge fromIQA and action recognition to VQA. Our work further in-vestigates the possibility of using more kinds of tasks. Pretrained models.Pretrained models reveal the greatpotential in deep learning. In Natural Language Process-ing (NLP), BERT and GPT-3 demonstrated sub-stantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by finetuning ona specific task. The advent of ViT had migrated this t-SNE results of MAE MOS[1-2) MOS[2-2.5) MOS[2.5-3) MOS[3-3.5) MOS[3.5-4) MOS[4-5)",
  "(h) ViT-B, DBI=3.15": ". Visualization of clustering results of features extracted by different pretrained models using t-SNE . Videos in KoNViD-1k are used. The number of cluster centers is set to be 6 according to the range of MOS values. And DBI scores, which will be introducedin detail in Sec. 3.4, measure the divergence of clustering results (the smaller, the better). capability into the visual realm. Some subsequent litera-ture had shown that the same benefits can beachieved. For example, CLIP trained on the WebIm-ageText matched the performance of the original ResNet-50on ImageNet zero-shot, without using any of the original la-beled data. In the field of quality assessment (QA), there arealso efforts to introduce pretrained models toimprove performance. Among them, VSFA extractedfeatures from a pretrained image classification neural net-work for its inherent content-aware property. And BVQA proposed transferring knowledge from IQA and ac-tion recognition datasets with motion patterns. Recently,Ada-DQA utilized diverse pretrained models to distillquality-related knowledge. However, its training cost is rel-atively high. We hope to tap the potential of the pretrainedmodel itself and reduce the tuning process, in this work. Metric learning.Metric learning can learn distance met-rics from data to measure the difference between samples.It has been used in many research, including QA. RankIQA trained a siamese network to rank synthesized imageswith different levels of distortions constrained by pairwiseranking hinge loss and then finetune the model on the targetIQA dataset. UNIQUE sampled ranked image pairsfrom individual IQA datasets and used a fidelity loss and a hinge constraint to supervise the training process.FPR extracted distortion/reference feature from the in-put/reference, hallucinated pseudo reference feature fromthe input alone, and used a triplet loss to pull the pris-tine and hallucinated reference features closer while push-ing the distortion feature away.",
  ". Observations": "In recent years, there has been a surge of research attentiontowards pretraining, as evidenced by a number of notableworks , that demonstrate the effectivenessof applying pretrained models to downstream tasks. Thismeets the main obstacle of VQA tasks, where the cost of an-notation poses a significant challenge in scaling up datasets.In addition to such efforts, the field of VQA has also wit-nessed endeavors towards leveraging pretrain-ing to capture intrinsic content-aware or motion-related pat-terns, with a view to enhancing the representation of per-ceptual qualities. However, the impact of various factors in-herent to pretrained models (e.g., neural network architec-tures, pre-text tasks, and pretrained databases) on the per-formance of model transfer remains a subject of inquiry. Tothe best of our knowledge, there has been limited explo-ration and exploitation of these factors, as well as newly-appeared cutting-edge pretrained models, in VQA. There-fore, our objective is to investigate ways to fully leveragethese models in VQA applications. In order to examine the relationship between pretrainedmodels and VQA tasks, we designed a simple clusteringexperiment. Specifically, we selected a pretrained modeland utilized its frozen weights as a feature extractor to ob-tain corresponding video features. We then clustered thesefeatures into multiple centers using LMNN , based ontheir range of MOS values. To this end, we selected eightmodels, including MAE trained on ImageNet-1k , . The pipeline of the proposed PTM-VQA. Features of input videos are extracted by pretrained models with frozen weights,transformed to the same dimension, and integrated to generate the final representation. Expect for the ordinary smooth L1 loss forregression, we add an ICID loss to ensure model-wise consistency and sample-wise divisibility. Swin-Base trained on ImageNet-22k , X3D trained on Kinetics-400 , ir-CSN-152 trained onSports-1M , CLIP trained on WebImageText ,ConvNeXt trained on ImageNet-22k, TimeSformer trained on Kinetics-400, and ViT-Base trained onImageNet-22k. In , we show that some of these mod-els display surprisingly discriminatory results, despitenot having been exposed to quality-related labels duringpre-text task training. We hypothesize that some quality-aware representations were learned concurrently during thepre-text task training. For instance, CLIP, which learns vi-sual concepts through natural language supervision, mayinclude emotional descriptions relating to image quality insome texts. Similarly, other models trained on action tasks(e.g., ir-CSN-152) may be sensitive to motion-related dis-tortions (e.g., camera shaking or motion blurriness).As such, these broader pretrained models may be useful inimproving VQA task performance.",
  ". Pipeline of the proposed PTM-VQA": "Assuming the availability of multiple pre-trained models,the conventional approach for employing them in VQAtasks is through fine-tuning on target datasets while inte-grating extracted features for quality prediction. Nonethe-less, this approach is computationally resource-intensive,making it less feasible as the number of pre-trained mod-els increases and their sizes become larger. For instance,the ViT model requires a TPUv3 with eight cores and 30days of training , while the MAE model consumes 128TPUv3 cores and 800 epochs of training . This wouldbe unaffordable in a VQA task. However, the findings il-lustrated in suggest that pretrained models have the potential to be applied to VQA tasks with their weightsfrozen. In this paper, we propose an effective framework,named PTM-VQA, to utilize the knowledge from diversepretrained models efficiently.As shown in , given an input video x(i), N pre-trained models, whose weights are frozen, are utilized toextract features, resulting in representations from differ-ent perspectives. Specifically, for video clip-based mod-els, we uniformly sample frames in the temporal dimen-sion to form the input clip. Corresponding representationsare then generated by these models. For frame-based mod-els, they are fed with sampled frames and the output fea-tures are averaged to perform the spatiotemporal represen-tation. Features extracted by models can be noted as z(i)n ,where n {1, . . . , N}. To further distill quality-aware fea-tures and perform dimension alignment, we apply a learn-able transformation module following each feature extrac-tor. Structurally, the transformation module consists of twofully connected layers, each followed by a normalizationlayer and an activation layer of GELU. The transformedfeatures are defined as f (i)n RD, where D represents thealigned dimension. Then features are integrated to obtain aunified representation through:",
  "h(i) =Nn=1 nf (i)nNn=1 n,(1)": "where n is the coefficient for each model. When n equals1/N, it means calculating an average, with each model con-tributing equally to the final representation. Last, h(i) isused to get the quality prediction through a regression head,which is a single fully-connected layer.Drawing on the proposed design, the training methodol- . Illustration of ICID loss. The figure shows examples of several triplets of triplet loss; two sets of intra-consistency betweenfeatures extracted by four pretrained models; and one sample (with two triplets) for inter-divisibility. ogy exhibits remarkable efficiency, thereby circumventingthe computational overhead associated with the finetuningapproach aforementioned. As attested by the results pre-sented in Tab. 1, the entire training regimen can be accom-plished within a span of approximately two hours, lever-aging a single GPU. This retains the information of thepretrained models well, but it also increases the difficultyof obtaining preferable performance due to the reduction oflearnable parameters. Some concerns are as follows:1. Due to various pre-texts of pretrained models, featuresgenerated by different models are of large diversity,which may distribute over inconsistent feature spaces. How to constrain these abundant features into a uni-fied quality-aware space is important. 2. Different from the objective category in common classi-fication tasks, the perceptual quality of a video is moreimplicit and related to various factors (e.g., content at-tractiveness, distortion type, and level, motion pattern andlevel), whereas videos of the same quality often rendercompletely different content and vice versa. Therefore,it is difficult for the models trained based on objectiveannotations to distinguish these samples of the same cat-egory but with a large perceptual quality difference. Amore comprehensive contrast approach beyond sample-wise comparison needs to be proposed to deal with theseoutliers. 3. There exist hundreds of pretrained models available inpublic libraries. How to select the desired models effi-ciently and how to determine the contribution of thesemodels to represent the perceptual quality effectively isan urgent problem to be solved.",
  "Ltriplet(fa, fp, fn) = max(fa fp2 fa fn2 + , 0), (2)": "where fa,fp,fn are features of an anchor sample a, a positivesample p of the same class as a, and a negative sample nwhich has a different class of a. And is a margin betweenanchor-positive and anchor-negative pairs. Some previousstudies in QA also applied triplet loss to measurethe distance between the distorted feature and the referencefeature of the same sample. Since the MOS values are con-tinuous, the original triplet loss cannot be directly used toconstrain the distance between arbitrary samples. We makesome modifications to constrain features generated by dif-ferent pretrained models and samples, as given in . Intra-consistency constraint.To solve the first con-cern, and unify features generated by different pretrainedmodels into a unified quality-aware latent space, we pro-pose a model-wise intra-consistency constraint. Formally, itis defined to minimize the distance between arbitrary two ofthe transformed features through computing a cosine simi-larity, which is widely used in deep metric learning :",
  ".(3)": "Inter-divisibility constraint.To solve the second con-cern, we split videos into distinct pseudo clusters under dif-ferent numerical intervals, according to the annotated MOSvalues (on a scale of 1.0 to 5.0). For example, videos withMOS in the range of 1.0 to 2.0 are generally consideredto be of poor quality, and whose content cannot be nor-mally recognized due to the existence of various distor-tions. And videos with MOS in the range of 4.0 to 5.0are of high quality, whose content is unambiguous, with-out noise, shaking, and blurring. We identify the videoswithin the same range as the same category, thus divid-ing them into K clusters.Each cluster can be noted asSk = {x(i)|y(i) (pk, qk], qk > pk [1.0, 5.0]}, wherey(i) is the labeled MOS for the i-th input video, pk and qkare the endpoints of the interval. Through this pseudo clus-ter, triplet loss can be utilized for samples belonging to the .Details of PTM-VQA for different datasets.Time1 is the training time.Time2 represents the inference time using a1080P/30FPS/20s video. The overall computational cost is relatively small compared with existing SOTA methods.",
  "Ltriplet(h(i), h(j), h(l)), where x(i), x(j) Sk, x(l) / Sk. (4)": "Besides, the original feature f extracted by individual mod-els is replaced by the integrated feature h. As shown in, the original triplet loss performs a sample-to-sampleform, which is highly affected by the sampling of triples.When facing outliers that are of the same quality but ren-der different contents or vice versa, it may lead to bad localminima and prevent the model from achieving top perfor-mance. Thus we propose using the centroid of the cluster torepresent the positive and negative points as:",
  ". Selection scheme through DBI": "For the third concern, we observe an obvious differencein the clustering results of different pretrained models in. Since the weights of models are frozen both in theclustering test and subsequent training process, the diver-gence of clustering results can reflect the relevance of VQAtasks. We propose using the Davies-Bouldin Index (DBI) as a metric for model selection, which is commonlyemployed for evaluating clustering results. In our particularsetting, the DBI can be expressed as follows:",
  "where ck is the centroid of cluster Sk for the set of ex-tracted feature z(i), dk represents the average distance be-tween each sample and its corresponding centroid. For the": "n-th model, its DBI score can be noted as n. A lower DBIindicates better clustering performance, which means thatthe pretrained model (e.g., ConvNeXt, Swin-Base, ir-CSN-152, CLIP in ) is more relevant to downstream VQAtasks. During training, the DBI scores computed offline canbe used in the aggregation procedure as given in Equ. 1,where n can be replaced by 1/n. It means the modelsthat are more relevant to the VQA task contribute more tothe feature representation.",
  ". Datasets and evaluation criteria": "Datasets. Our method is evaluated on 4 public NR-VQAdatasets, including KoNViD-1k , LIVE-VQC ,YouTube-UGC and LSVQ . In detail, KoNViD-1k contains 1,200 videos that are fairly filtered from a largepublic video dataset YFCC100M. The videos are 8 secondslong with 24/25/30 FPS and a resolution of 960 540. TheMOS ranges from 1.22 to 4.64. Each video owns 114 anno-tations to get a reliable MOS. LIVE-VQC consists of 585videos with complex authentic distortions captured by 80different users using 101 different devices, with 240 annota-tions for each video. YouTube-UGC has 1,380 UGC videossampled from YouTube with a duration of 20 seconds andresolutions from 360P to 4K, with 123 annotations for eachvideo. And LSVQ is the largest VQA dataset currently (pro-posed in 2021) with 39,076 videos. All the datasets containno pristine videos, thus only NR methods can be evaluatedon them. Following , we split the dataset into a 80%training set and a 20% testing set randomly for the first threedatasets. For LSVQ, we follow the official split setting. Weperform 10 repeat runs in each dataset using different split-tings to get the mean values of PLCC and SRCC.Evaluation criteria. Pearsons Linear Correlation Co-efficient (PLCC) and Spearmans Rank-Order CorrelationCoefficient (SRCC) are selected as criteria to measure theaccuracy and monotonicity. They are in the range of .A larger PLCC means a more accurate numerical fit withMOS scores. A larger SRCC shows a more accurate rank-ing between samples. Besides, the mean average of PLCCand SRCC is also reported as a comprehensive criterion.",
  "PTM-VQA0.87180.85680.86430.81980.81100.81540.85700.85780.85740.85910.84540.8523": "V100 GPU by training for 60 epochs. For KoNViD-1k, weselect ConvNeXt, ir-CSN-152, and CLIP as feature extrac-tors. For LIVE-VQC, we use CLIP and TimeSformer. ForYouTube-UGC, an extra Video Swin-Base is used togetherwith those selected on KoNViD-1k. For KoNViD-1k, wesample 16 frames with a frame interval of 2. As videos inLIVE-VQC and YouTube-UGC has a longer time duration,we use larger intervals for these two datasets. Since mostaugmentations will introduce extra interference to the qual-ity of videos , we only choose the center crop to producean input with a size of 224 224. During training, we useAdamW optimizer with a weight decay of 0.02. Cosine an-nealing with a warmup of 2 epochs is adopted to control thelearning rate. The dimension D of transformed features isset to 128. The margin is set to be 0.05. is set to be0.2. By default, we select the checkpoint generated by thelast iteration for evaluation. During inference, we follow asimilar procedure as given in by using 4 5 views. Tobe specific, 4 clips are uniformly sampled from a video inthe temporal domain. For each clip, we take 5 crops in thefour corners and the center. The final score is computed asthe average score. More details are given in Tab. 1.",
  ". Comparison with SOTA methods": "We select existing VQA methods for comparison in threedatasets. As shown in Tab. 2, our method obtains com-petitive results on all three datasets. Compared with tra-ditional methods that rely on statistical regularities (e.g.,VIIDEO , NIQE , and BRISQUE ), PTM-VQA models outperform by large margins.Comparedwith some deep learning-based methods that apply well-designed networks (e.g., TLVQM , StarVQA ),PTM-VQA still obtains higher performances. Especially,VSFA and RIRNet also adopt pretrained mod-",
  "PTM-VQA-1k0.85360.85300.77840.7279PTM-VQA-VQC0.86370.85450.78170.7359PTM-VQA-UGC0.84430.84290.77690.7300": "els that contain content-dependency or motion informa-tion to finetune in VQA tasks. PTM-VQA demonstratesthat features extracted directly from pretrained models canalso achieve better results. As the best two SOTA meth-ods BVQA and STDAM who utilize extra IQAdatasets, PTM-VQA proves that transferring knowledgefrom pretrained models can achieve competitive resultscompared with a model trained with additional data.To assess the generalizability of the selected combina-tions, we evaluate on the largest LSVQ dataset using thethree combinations utilized in KoNViD-1k, LIVE-VQC,and YouTube-UGC. As given in Tab. 3, PTM-VQA modelsdemonstrate a significant performance advantage over ex-isting methods, indicating the benefits of leveraging pre-trained models with a larger amount of data.We also compare the cost of inference time with open-source methods on a 1080P video (100 repeat runs). Andthe inference time cost is 75s (BRISQUE), 248s (TLVQM),117s (VSFA), 0.12s (StarVQA), and 2.45s (BVQA) respec-tively.Thanks to the reduced dimensions (e.g., framesampling, center cropping) and model selection using",
  "S1=[1,2), S2=[2, 2.5), S3=[2.5, 3),0.87180.8568S4=[3, 3.5), S5=[3.5, 4), S6=": "DBI, PTM-VQA models do not significantly increase in-ference time over StarVQA and BVQA, as given in Tab. 1.Meanwhile, due to the different number and composition ofpretrained models, the calculation cost of PTM-VQA mod-els vary. Even so, the largest PTM-VQA can process a high-resolution video in about 1s, and the smaller models canprocess nearly 6/7 videos per second. Cross-database comparison.To emphasize the validityand generalizability, we perform the cross-database evalua-tion in Tab. 4. Models trained on LSVQ are tested on muchsmaller datasets of KoNViD-1k and LIVE-VQA directly. Itcan be seen that PTM-VQA transferred very well to bothdatasets, highlighting the general efficacy.",
  "/0.85700.8578": "When either or both constraints are absent, performance de-grades significantly. These prove the effectiveness of intra-consistency constraints in transferring knowledge from dif-ferent pretrained models and inter-divisibility constraints ingenerating stable predictions. Ablation on the clustering settings.Tab. 6 gives the re-sults with different numbers of clusters.When K is 2,videos are simply classified as low-quality and high-qualityones. When K is 4, videos are evenly divided into four partson a scale of 1.0 to 5.0. Due to the relatively small amountof data at both endpoints, a 6-split setting can be obtainedby using fine-grained division in the middle fraction seg-ment. Since the need to ensure the number of samples percluster within the batch, a larger number of clusters are notattempted. The best result can be acquired when K is 6. Ablation on the DBI strategy.The effectiveness of DBIcan be evaluated in two aspects: (1) Model selection strat-egy. We performed 10 experiments based on randomly se-lected pretrained models in KoNViD-1k, resulting in PLCC(0.79170.0578, SRCC (0.75830.0492). Compared withthe DBI-based strategy, the performance is poor and the ran-domness is high. (2) Feature integration. Tab. 7 shows theeffectiveness of DBI in guiding the integration of differentmodels, allowing more relevant models to contribute more.",
  ". Conclusion": "In this paper, we proposed PTM-VQA that utilizes in-the-wild pretrained models as feature extractors for NR-VQAtasks, transferring quality-related knowledge from diversepre-text domains. The DBI scores are used to select can-didates from a large amount of available pretrained mod-els. To constrain features with large diversity into a uni-fied latent space of quality and tackle outliers, we proposea new ICID loss. Under small computational cost, PTM-VQA models obtain SOTA results in widely-used bench-marks. Experiments in larger datasets and cross-databaseevaluation further prove generalizability.",
  "Gedas Bertasius, Heng Wang, and Lorenzo Torresani.Isspace-time attention all you need for video understanding?In ICML, pages 813824. PMLR, 2021. 4": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners. In NeurIPS, 2020. 2, 3",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In CVPR, pages 248255. IEEE Computer Society,2009. 2, 3, 4": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: pre-training of deep bidirectional trans-formers for language understanding.In NAACL-HLT (1),pages 41714186. Association for Computational Linguis-tics, 2019. 2, 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In ICLR. OpenReview.net, 2021. 2, 4",
  "Anish Mittal, Michele A. Saad, and Alan C. Bovik. A com-pletely blind video integrity oracle. IEEE Trans. Image Pro-cess., 25(1):289300, 2016. 1, 7": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, Zem-ing Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-son, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Mar-tin Raison, Alykhan Tejani, Sasank Chilamkurthy, BenoitSteiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:An imperative style, high-performance deep learning library.In NeurIPS, pages 80248035, 2019. 6 Lihui Qian, Tianxiang Pan, Yunfei Zheng, Jiajie Zhang,Mading Li, Bing Yu, and Bin Wang. No-reference nonuni-form distorted video quality assessment based on deep mul-tiple instance learning. IEEE Multim., 28(1):2837, 2021.1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision. In ICML, pages87488763. PMLR, 2021. 3, 4",
  "Stefan Winkler.Issues in vision modeling for perceptualvideo quality assessment. Signal Process., 78(2):231252,1999. 1": "Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Re-becca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos,Hongseok Namkoong, Ali Farhadi, Yair Carmon, SimonKornblith, and Ludwig Schmidt. Model soups: averagingweights of multiple fine-tuned models improves accuracywithout increasing inference time. In ICML, pages 2396523998. PMLR, 2022. 5 Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao,Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin.FAST-VQA: efficient end-to-end video quality assessmentwith fragment sampling.In ECCV (6), pages 538554.Springer, 2022. 7"
}