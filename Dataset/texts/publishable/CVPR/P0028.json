{
  "Abstract": "In recent years, image synthesis has achieved remark-able advancements, enabling diverse applications in con-tent creation, virtual reality, and beyond. We introduce anovel approach to image generation using Auto-Regressive(AR) modeling, which leverages a next-detail predictionstrategy for enhanced fidelity and scalability.While ARmodels have achieved transformative success in languagemodeling, replicating this success in vision tasks has pre-sented unique challenges due to the inherent spatial depen-dencies in images. Our proposed method addresses thesechallenges by iteratively adding finer details to an imagecompositionally, constructing it as a hierarchical combina-tion of base and detail image factors. This strategy is shownto be more effective than the conventional next-token pre-diction and even surpasses the state-of-the-art next-scaleprediction approaches. A key advantage of this method isits scalability to higher resolutions without requiring fullmodel retraining, making it a versatile solution for high-resolution image generation.",
  ". Introduction": "Recent advancements in Generative AI for image synthesisand editing have garnered significant interest within bothresearch and industry sectors. Conventional approaches inGenerative AI, including Generative Adversarial Networks(GANs) and Variational Autoencoders (VAEs), typically aim to produce entire scenes in a sin-gle pass. However, human perception and understanding ofvisual scenes are inherently compositional. For example,when creating a scene, an artist typically follows an itera-tive process, beginning with rough outlines, refining shapes,and gradually adding details and shading. Generating entirescenes in one attempt can preclude this iterative addition ofdetail, posing challenges in scaling to high-resolution im-ages. Recent research has introduced step-wise approaches tothe image generation problem, where each step incorporatesa subset of details. For instance, diffusion-based methods initiate with a noisy vector and employ a denoisingmodel to incrementally remove noise, progressively reveal-ing a coherent image. Similarly, auto-regressive (AR) mod-",
  "arXiv:2411.10180v1 [cs.CV] 15 Nov 2024": "els tackle image generation in a patch-wisemanner, further supporting an iterative image generation ap-proach. Specifically, AR models in image generation, suchas VQGAN and DALLE , aim to parallel the suc-cess of AR-based models in large language models (LLMs).These models use visual tokenizers that convert continuousimages into grids of 2D tokens, enabling AR models to learnnext-token prediction.Despite the success of AR methods in natural languageprocessing, replicating similar advancements in computervision has proven challenging. Recent studies in AR mod-eling indicate that the sequence in which image tokensare processed during AR learning can substantially affectmodel performance.In this paper, we introduce a novel Auto-Regressive Im-age Generation approach that constructs high-quality im-ages by progressively assembling a scene in a hierarchicalmanner. The process begins with the creation of a smoothbase image, which is then enhanced through iterative ad-dition of finer details, resulting in a coherent final image(see ).This method closely emulates a humanapproach to image creationstarting with a foundationalsketch and refining it with increasing levels of detail. Ourapproach first decomposes a training image into base anddetail components using an edge-aware smoothing tech-nique. These components are then encoded into multi-scaledetail token maps. The Auto-Regressive process initiateswith a 11 token, predicting successive token-maps to con-struct the base component of the image. Once the base is es-tablished, the model transitions to predicting the detail com-ponents, incrementally layering them to enhance the baseimage. This structured, iterative process aligns with a nat-ural order of image formation, enhancing both quality andinterpretability in the generation process.The Contributions of this paper include:",
  ". Generative Models": "Generative models for image synthesis have evolved at arapid pace over the last decade.Image generation canbe performed either unconditionally or by conditioning themodel on some prior information such as text, class label,etc. Variational Auto Encoders (VAEs) , and Gen- erative Adversarial Networks (GANs) have revo-lutionized the space of Image Generation. GANs train agenerator and discriminator network in an adversarial fash-ion. These models are capable of generating realistic, highquality images in a single step.The more recent Diffusion models are basedon a sequential denoising process that gradually transformsa noise-perturbed image into a realistic sample, effectivelylearning the data distribution by reversing a predefined nois-ing process. This approach allows diffusion models to pro-duce high-quality images and fine-grained textures, whichhas established them as a strong alternative to GenerativeAdversarial Networks (GANs) and other generative meth-ods. Diffusion models are highly flexible and have beenapplied to a variety of generative tasks beyond image syn-thesis, including text-to-image generation , inpaint-ing , super-resolution , 3D reconstruction, and generalized image editing . However,despite their strengths, diffusion models typically requirea substantial number of iterative steps to generate high-quality images, which can lead to long generation times,especially for high-resolution images. This computationaloverhead has limited their scalability for applications thatdemand efficient, real-time synthesis.",
  ". Auto-Regressive Generative Models": "Auto-Regressive (AR) models attempt to predict the nexttoken in a sequence while conditioning on the previous to-kens of the sequence. Recent years have seen the rise of ARmodels. Specifically the Generative Predictive Text (GPT) used transformers to learn Large LanguageModels (LLMs) which led to a significant performance im-provement in tasks like language generation, prediction andunderstanding. Many works have attempted to replicate thissuccess of AR models in CV applications including imagegeneration. One of the first works to do this was DRAW, where a sequential variational auto-encoding frame-work was used with Recurrent Neural Networks (RNN) as the building blocks. Another approach in AR generativemodelling was to predict the pixels of the image in a raster-scan fashion (Pixel CNN , Pixel RNN and ImageTransformer ). Limitation of such models lies in thecomputational complexity required to predict a real imagewith billions of pixels. A large Image-GPT model with6.8B parameters was only capable of predicting a 96 96image. To alleviate this problem, Vector Quantized Varia-tional Auto Encoder (VQ-VAE) was introduced, wherean encoder was used to compress the image into a low-dimensional latent space, followed by quantization to dis-cretize the latent space into tokens which are predicted byan AR model. Recent work has trained transformerbased decoder to auto-regressively generate a realistic im-age using quantized tokens from the VQ-VAE. In , the authors note that the ordering of tokens is critical when itcomes to AR modelling for image generation, and proposea multi-scale approach to tokenization. Instead of using thestandard next-token prediction scheme, the authors adopta next-scale prediction scheme, where the image at higherresolution is predicted at each time-step.",
  "K|u|2dx + |K|,(7)": "where f : Rk is a vector-valued input image withk 1 channels. This model approximates f by a func-tion u : Rk which is smooth everywhere except fora possible (d 1) dimensional jump set K, at which u isdiscontinuous. The weight > 0 controls the length of K.The limiting case imposes zero gradient outsideK and is known as the piecewise constant Mumford-Shahmodel.One of the most common approaches for solving theMumford-Shah functional for images is the Ambrosio-Tortorelli Approach which is given by,",
  "4s2)dx,(8)": "with a small parameter > 0 and an additional variables : R. The key idea is to introduce s as an edge setindicator, in the sense that points x are part of the edgeset K if s(x) 1 and part of smooth region if s(x) 0.The variables u and s are found by alternating minimiza-tion.",
  "I = Bn + Dn + Dn1 + ... + D1,(10)": "where, Bk1 = Bk + Dk, k {1, ..., n}. Equation 10defines the nth order decomposition of I. In this decom-position, the base factor Bn captures the images overallstructure, composition, and global features, while the detailfactors {Dk}nk=1 represent local features that contribute tothe finer details of the image. (a) shows the hi-erarchical base-detail decomposition process. (b)depicts how the Image I is represented by the base and de-tail factors in a vector form.",
  ". Encoding and Tokenization": "In our approach, each image is represented by token maps{r1, r2, ..., rM} within the latent space of a Vector Quan-tized Variational Autoencoder (VQ-VAE), rather than singletokens. This token-map representation preserves the spa-tial coherence of the feature map and reinforces the spatialstructure inherent in the image. We propose a tokenizationscheme such that these token maps represent the base anddetail factors. Specifically, the image representation is com-prised of B base token maps, (r1, ..., rB), where B < Mand (M B) detail token maps, (rB+1, ..., rM).Following the Base-Detail Decomposition, we encodethe original image I along with the Base Factors {Bk}nk=1using a VAE,",
  "m=1P(rm|r1, ..., rm1).(13)": "where each autoregressive unit, rm [V ]hmwm is a tokenmap containing hm wm tokens.For the model architecture,we utilize a standarddecoder-only Transformer architecture similar to that inGPT-2 , VQ-GAN , and VAR . At each auto-regressive step, the Transformer decoder predicts the dis-tribution over all hm wm tokens in parallel as depictedin b. To enforce causality, as in , we applya causal attention mask, ensuring that each token map rmonly attends to its preceding tokens rm.",
  ". Implementation Details": "In order to get the detail decomposition of the training im-age, the Mumford-Shah smoothing operation as describedin Equation 8 with = 10 and = 0.01 is utilized. Eachtraining image is decomposed iteratively such that a 3rd or-der decomposition is obtained, I = B3 + D3 + D2 + D1.A Vanilla VQ-VAE is used along with Z extra con- volutions to realize the Base-Detail quantization scheme asdepicted in a and Algorithm 1. The base and de-tail factors all share the same code book with V = 4096.As in , the tokenizer is trained on OpenImages with Compound loss (Equation 6) and spatial downsampleof 16.The tokenized base-detail factors as obtained by usingAlgorithm 1 are then utilized to train a Transformer De-coder architecture which learns to predict the next-detailtoken. A standard decoder-only transformer architecture isused similar to GPT-2 and VQGAN . During in-ference, the Transformer predicts the codes and the VQ-VAE decoder, D(.) is utilized to obtain the generated im-age. The decoding algorithm is summarized in Algorithm2. The depth of the transformer in CART is varied from 16to 30 to obtain models with varying complexity and learn-ing capability. The model is trained with a initial learningrate of 14.",
  ". Emperical Results": "The proposed CART model was evaluated on the Ima-geNet dataset at resolutions of 256 256 (CART-256)and 512 512 (CART-512) to benchmark its performanceagainst state-of-the-art (SOTA) methods in image genera-tion. The comparative results are presented in Tables 1 and2. We observe that the proposed CART model out-performsthe SOTA VAR approach and also achieves FID lowerthan that of the ImageNet validation set, all while main-taining comparable complexity and number of steps. Un-like VAR, the CART model benefits from the base-detaildecomposition allowing it to dis-entangle the global struc-tures from the local details. This makes the learning pro-cess easier, defining a more natural order of tokens. depicts some of the generated images using the proposedCART method and shows a comparison of imagesgenerated by VAR and CART. It is evident from Fig-ure 5 that CART leads to generated images with enhanceddetails and structure as compared to VAR which does notuse a next-detail prediction scheme. Note that CART sur-passes both Diffusion Transformer as well as theSOTA VAR model in AutoRegressive Image genera-tion.",
  ". Generative Models Comparison on ImageNet 256 256.Suffix -re refers to models that use rejection sampling": "inputs. Empirically, we observe that the base factor encap-sulates global featuressuch as class-conditional structureand overall color compositionwhile the detail factor cap-tures local features, such as textures and fine-grained de-tails. Consequently, the base factor can be upscaled withoutcompromising essential global information, while the detailfactor can be generated in a patchwise manner. Patchwisegeneration of details does not introduce discontinuities, asthe patches in the detail factor inherently lack dependencieson global relationships across the image. depictsthe comparison of bilinear resizing against images gener-",
  ". Generative Models Comparison on ImageNet 512 512": "ated using the method described above.ImageNet 512 512 is utilized to evaluate the high-resolution capability of the CART Model with and withoutre-training the entire model from scratch. summa-rizes the performance of our models as compared to SOTAon ImageNet-512 512. CART-256 in refers tohigh-resolution image generation as described above (with-out re-training), and CART-512 refers to model trainedfrom scratch using 512 512 training images.",
  ". Ablation Study of CART": "the best performance is achieved when the Base factor islearned in a multi-scale fashion and the details are added atthe final image resolution.In we compare the performance of proposedCART model when different order of decomposition is usedfor the learning process. Decomposition order of 0 is equiv-alent to using no detail decomposition and hence is the spe-cial case of VAR. We observe that the best performance isachieved when we utilize a 3rd order Base-Detail decom-position. When the base detail decomposition is taken be-yond the 3rd order, the base image becomes over smoothand begins to loose essential details related to global struc-",
  ". Conclusion": "In this work, we proposed a novel approach to image syn-thesis through an auto-regressive (AR) framework that in-corporates a next-detail prediction strategy, advancing thecapabilities of AR models in high-resolution image genera-tion. By leveraging a structured base-detail decomposition,our method enables iterative refinement that aligns with thenatural hierarchical structure of images, effectively separat-ing global from local features. Our contributions include arobust tokenization scheme which quantizes base and detaillayers separately, preserving spatial integrity and enablingan effective AR process. Experimental results demonstratethat our approach not only achieves state-of-the-art perfor-mance in AR-based image generation but also reduces thecomputational complexity typically associated with scalinghigh-resolution outputs. Furthermore, we address limita-tions seen in conventional next-token and next-scale predic-tion methods, achieving a more accurate and efficient gen-erative process.In conclusion, our next-detail AR modeling frameworkintroduces a scalable and computationally efficient ap-proach to image synthesis, offering a compelling alternativeto diffusion models and other state-of-the-art methods.",
  "Alpha-vllm. large-dit-imagenet. / Alpha - VLLM / LLaMA2 - Accessory / tree /f7fe19834b23e38f333403b91bb0330afe19f79e/Large-DiT-ImageNet. Accessed: 2010-09-30. 6": "Luigi Ambrosio and Vincenzo Maria Tortorelli. Approxima-tion of functional depending on jumps by elliptic functionalvia t-convergence.Communications on Pure and AppliedMathematics, 43(8):9991036, 1990. 3 Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Hen-derson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-derdiffusion: Image diffusion for 3d reconstruction, inpaint-ing and generation. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages1260812618, 2023. 2",
  "Tom B Brown.Language models are few-shot learners.arXiv preprint arXiv:2005.14165, 2020. 2": "Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William TFreeman. Maskgit: Masked generative image transformer.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1131511325, 2022.6, 7 Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-woo Jun, David Luan, and Ilya Sutskever. Generative pre-training from pixels. In International conference on machinelearning, pages 16911703. PMLR, 2020. 2 Ciprian Corneanu, Raghudeep Gadde, and Aleix M Mar-tinez.Latentpaint: Image inpainting in latent space withdiffusion models.In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages43344343, 2024. 2 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 6",
  "Prafulla Dhariwal and Alexander Nichol. Diffusion modelsbeat gans on image synthesis. Advances in neural informa-tion processing systems, 34:87808794, 2021. 6, 7": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Tamingtransformers for high-resolution image synthesis.In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1287312883, 2021. 2, 4, 6,7, 8 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-nications of the ACM, 63(11):139144, 2020. 1, 2",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural informationprocessing systems, 33:68406851, 2020. 1, 2": "Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,Mohammad Norouzi, and Tim Salimans. Cascaded diffu-sion models for high fidelity image generation. Journal ofMachine Learning Research, 23(47):133, 2022. 2, 6 Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,Eli Shechtman, Sylvain Paris, and Taesung Park.Scal-ing up gans for text-to-image synthesis. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1012410134, 2023. 6 Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 44014410, 2019. 3 Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, HuiwenChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 60076017, 2023. 2",
  "Diederik P Kingma. Auto-encoding variational bayes. arXivpreprint arXiv:1312.6114, 2013. 1, 2": "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, StefanPopov, Matteo Malloci, Alexander Kolesnikov, et al. Theopen images dataset v4: Unified image classification, objectdetection, and visual relationship detection at scale. Interna-tional journal of computer vision, 128(7):19561981, 2020.6 Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, andWook-Shin Han.Autoregressive image generation usingresidual quantization. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1152311532, 2022. 4, 6 Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, HuajunFeng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Singleimage super-resolution with diffusion probabilistic models.Neurocomputing, 479:4759, 2022. 2",
  "Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-ating diverse high-fidelity images with vq-vae-2. Advancesin neural information processing systems, 32, 2019. 6": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 6 Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik PKingma.Pixelcnn++: Improving the pixelcnn with dis-cretized logistic mixture likelihood and other modifications.arXiv preprint arXiv:1701.05517, 2017. 2",
  "A Vaswani. Attention is all you need. Advances in NeuralInformation Processing Systems, 2017. 2": "Shiyuan Yang, Xiaodong Chen, and Jing Liao. Uni-paint:A unified framework for multimodal image inpainting withpretrained diffusion model. In Proceedings of the 31st ACMInternational Conference on Multimedia, pages 31903199,2023. 2 Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,and Yonghui Wu.Vector-quantized image modeling withimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 6",
  "Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang,and In So Kweon. Text-to-image diffusion models in gener-ative ai: A survey. arXiv preprint arXiv:2303.07909, 2023.2": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 586595, 2018. 3 Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-tilling view-conditioned diffusion for 3d reconstruction. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1258812597, 2023. 2 Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, andCong Yao.Conditional text image generation with diffu-sion models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1423514245, 2023. 2"
}