{
  "Abstract": "In this work, we propose a novel discriminative frame-work for dexterous grasp generation, named DexterousGrasp TRansformer (DGTR), capable of predicting a di-verse set of feasible grasp poses by processing the objectpoint cloud with only one forward pass. We formulate dex-terous grasp generation as a set prediction task and designa transformer-based grasping model for it. However, weidentify that this set prediction paradigm encounters sev-eral optimization challenges in the field of dexterous grasp-ing and results in restricted performance. To address theseissues, we propose progressive strategies for both the train-ing and testing phases. First, the dynamic-static matchingtraining (DSMT) strategy is presented to enhance the opti-mization stability during the training phase. Second, we in-troduce the adversarial-balanced test-time adaptation (AB-TTA) with a pair of adversarial losses to improve grasp-ing quality during the testing phase. Experimental resultson the DexGraspNet dataset demonstrate the capability ofDGTR to predict dexterous grasp poses with both highquality and diversity. Notably, while keeping high qual-ity, the diversity of grasp poses predicted by DGTR sig-nificantly outperforms previous works in multiple metricswithout any data pre-processing. Codes are available at",
  ". Introduction": "Robotic dexterous grasping stands as a fundamental andcritical task in the field of robotics and computer vision, of-fering a versatile and fine-grained approach with extensiveapplications in industrial production and daily scenarios.With the development of deep learning and large-scaledatasets for dexterous grasp generation, learning-basedmethods achieve considerable performance in graspingquality and generalizability . Concurrently, ac-",
  "N grasps": ".Comparison of DGTR and other dexterous graspingframeworks. The generative models (a) usually learn the distri-bution of the grasp poses conditioned on the object point cloud.At test time, they mainly infer multiple times to generate severalgrasps but produce nearly identical grasp poses with the same con-dition. The vanilla discriminative models (b) mainly learn to pre-dict one grasp pose for the input point cloud. Our DGTR model (c)adopts a transformer decoder and learnable queries, and learns topredict a set of diverse grasps poses with one forward pass. quiring grasping diversity (especially grasping from variousdirections) is also a crucial task as it provides therobot with robustness and task flexibility during the manipu-lation task. Previous learning-based approaches mostly uti-lize generative models to model the grasp distribution con-ditioned on the object point cloud as shown in (a).However, conditional generative models may consistentlygenerate nearly identical outputs (given the same input) atinference time due to the powerful condition , ex-cept for a diffusion-based model , which can generatediverse grasps but with low quality. Alternatively, vanilladiscriminative models shown in (b) can only pre-dict a single grasp pose for one input object . There-fore, to obtain diversity, both of them have to rotate the input point cloud and infer multiple times, which is time-consuming and quality-limiting.In this work, we propose Dexterous Grasp Transformer(DGTR), a novel discriminative framework to tackle thetask of predicting diverse and high-quality dexterous graspposes given the complete object point cloud. We formu-late dexterous grasp generation as a set prediction task anddesign a transformer-based grasping model inspired by theimpressive success of Detection Transformers . Asillustrated by (c), DGTR adopts a transformer de-coder and utilizes learnable grasp queries representing dif-ferent grasping patterns to predict a diverse set of feasiblegrasp poses by processing the object point cloud only once.However, we observe that DGTR faces an optimizationchallenge in our task, which results in the dilemma betweenmodel collapse and unacceptable object penetration of thepredicted grasps. As depicted in (a), applying alarge weight on the object penetration loss causes the modelto learn a trivial solution where all predictions are nearlyidentical. On the contrary, a zero weight for the penetra-tion loss leads to severe object penetration of the grasps,as shown in (b). We identify the main cause ofthis challenge to be the instability of the Hungarian algo-rithm, which is exacerbated by the powerful object pene-tration loss. As the weight of the object penetration loss in-creases, the matching process becomes more unstable. Con-sequently, the unstable matching results misguide the opti-mization process of the model, ultimately causing the modelcollapse. We conduct abundant analysis and experimentsfor this in .3 and 4.5.1.To overcome this challenge, we propose progressivestrategies for both the training and testing phases, whichsimultaneously enhance the diversity and quality of graspposes as demonstrated in (c). Firstly, we present adynamic-static matching training (DSMT) strategy, whichis built on the insight of guiding the model to learn appro-priate targets through dynamic matching training and sub-sequently optimize object penetration through static match-ing training. This strategy ensures effective optimization ofthe object penetration loss while directing the model opti-mization reasonably. Secondly, we present an adversarial-balanced test-time adaptation (AB-TTA) strategy to refinethe predicted grasp poses directly in the parameter space ofthe dexterous hand. Specifically, we utilize a pair of adver-sarial losses: one repels the hand from the interior of theobject, while the other attracts it towards the objects sur-face. The strategic interaction of the adversarial losses sub-stantially enhances the quality of the grasp and mitigates thepenetration. Notably, our AB-TTA neither relies on any 3Dmesh information of the objects nor involves complex forceanalysis or auxiliary models.Extensive experiments on DexGraspNet dataset showthat our methods are capable of generating high-quality and",
  "(c) Ours": ". Comparison of grasp quality and diversity under dif-ferent penetration loss weights. We visualize 3 grasps for eachcircumstance. (a) large object penetration weight; (b) zero objectpenetration weight; (c) our progressive strategies. high-diversity grasp poses on thousands of objects. To thebest of our knowledge, this is the first work to predict a di-verse set of dexterous grasp poses by processing the inputobject just once, without any need for data preprocessing.",
  ". Dexterous Grasp Generation": "Dexterous grasping is a promising task as it endows robotswith the capability to manipulate objects like humans.Meanwhile, it also presents significant challenges due to thehigh degree-of-freedom design of dexterous hands. Earlymethods focus on analytical methods and opti-mize the hand poses with kinematics and physical mech-anisms to a force-closure state.Several works synthesize datasets for dexterous grasps with , but facechallenges in the generating speed and success rate.Recently data-driven methods have received increasing research attention with the devel-opment of deep neural networks. GraspTTA utilizes aCVAE to synthesize grasps with their hand-object con-sistency constraints. UnidexGrasp proposes two vari-ants of IPDF and Glow to predict object orienta-tion, translation and articulation for the dexterous hand re-spectively. Some works explore conditioned nor-malizing flow , generative adversarial network and conditioned diffusion models to learn the prob-abilistic distribution of the dexterous grasps. In contrast,DDG exploits a non-generative model and a differen-tiable Q1 loss to learn one grasp pose for each instance.However, these methods struggle to generate feasibleand diverse grasps given the same input point cloud, eitherbecause the condition (e.g., object point cloud) significantlyrestricts the generation direction of the model, or becauseof the limitation of the model architecture. To alleviate thisproblem, our work learns to predict a diverse set of grasps ofan object at one time with a transformer-based frameworkspecially designed for dexterous grasp generation.",
  "matchwith lossw/o loss": ". Overview of our DGTR framework. The input of DGTR is the complete point cloud O of an object. First, the PointNet++ encoder downsamples the point cloud and extracts a set of object features. Next, the transformer decoder takes N learnable query embed-dings as well as the object features as input and predicts N diverse grasp poses in parallel. In the dynamic matching training stage, ourmodel is trained with the matching result produced by Hungarian Algorithm and without object penetration loss. In the static matchingtraining stage, we use static matching recorded in the DMT stage to train the model with object penetration loss. At test time, we adopt anadversarial-balanced loss to directly finetune the hand pose parameters.",
  ". Vision Transformer": "Vision transformers have received anextensive amount of research attention in recent years, andseveral of them introduce novel paradigms forcomputer vision tasks. In our work, dexterous grasp genera-tion from a complete point cloud is considered a set predic-tion task, which is one of the strengths of detection trans-formers .However, conventional detection trans-formers, which are specially designed for object detection,are unsuited for dexterous grasp generation, because of theabsence of supervision for feasible grasps, as well as theoptimization challenge arising from the grasp losses. Totackle this problem, we equip our model with a series ofgrasp losses for learning diverse and high-quality grasps,and progressive strategies for stable training and penetra-tion optimization.",
  ". Problem Formulation": "In this work, we focus on generating high-quality and di-verse grasp poses from the complete object point cloud.Specifically, given an object point cloud O RM3 of sizeM, our model learns to generate a set of N dexterous graspposes {gi}Ni=1 = {(ri, ti, qi)}Ni=1, where ri SO(3) andti R3 are the global rotation and translation in the worldcoordinate, and qi RJ is the joint angles of the J-DoFdexterous hand (J = 22 for ShadowHand ).",
  "The model architecture of Dexterous Grasp Transformer(DGTR) contributes most to the diversity and efficiency (i.e": "N various grasp poses in one forward pass) of our frame-work. As shown in , it mainly consists of threecomponents: 1) a point cloud encoder to extract the objectfeature, 2) a transformer decoder, and 3) feed-forward net-works to predict the grasp poses.Encoder. We adopt a three-layer PointNet++ as theencoder to extract a set of object features. Given an objectpoint cloud O RM3, our encoder outputs the down-sampled point cloud O RM 3 and the correspondingfeatures F RM C.Decoder.Inspired by previous set-prediction frame-works , we cascade Transformer blocks as ourdecoder to predict an unordered set of grasp poses in paral-lel. This decoder takes as input the point features F and aset of learnable grasping queries {qi}Ni=1 to produce graspfeatures {Gi}Ni=1. Since there is no explicit position infor-mation among the point features, we encode the raw pointsO RM 3 with an MLP module as the position embed-ding of encoder features F.Prediction Heads.The grasp pose set {gi}Ni=1 ={(Ri, ti, qi)}Ni=1 are predicted with the final decoder fea-tures {Gi}Ni=1 by three independent MLPs. Both the trans-lation and the joint angle predictions are passed through asigmoid activation to form a normalized value w.r.t. the lim-its of each dimension. And the rotation prediction is nor-malized to a unit quaternion with the L2 normalization.The unordered predictions are usually matched with theirnearest ground truths using the Hungarian Algorithm before the loss calculation. However, while the HungarianAlgorithm provides an effective solution to train the modelregardless of the permutation of the predictions, it alsobrings ambiguity to the optimizing process of the model,which is a major factor of the dilemma of model collapse",
  ". Dynamic-Static Matching Training Strategy": "Model Collapse vs. Object Penetration. We discover theoptimization challenge when DGTR attempts to learn mul-tiple grasping targets of one object simultaneously. As il-lustrated in , DGTR encounters a dilemma betweenmodel collapse and the issue of unacceptable object pen-etration. On one hand, if we impose a heavy penalty onobject penetration (e.g. pen = 500), the model tends to bestuck in a trivial solution where it predicts nearly identicalgrasps for the object. On the other hand, if we reduce thispenalty (e.g. pen = 5) or even remove it (pen = 0), thepredicted grasps suffer from severe object penetration.We analyze the reasons why the object penetrationpenalty could cause model collapse in the case of set predic-tion. Intuitively, there is a non-trivial gap between the op-timizing difficulties of object penetration and hand pose re-construction. The object penetration loss could be reducedeasily by pulling the hand away from the object. Whilethe latter involves a high-dimensional and non-convex op-timization problem, which is inherently difficult to solve.Empirically, the object penetration loss increases the in-stability of Hungarian Algorithm matching results, whichprofoundly disturbs the optimizing process. As depicted in, the instability of Hungarian matching increases aspen becomes larger, which results in ambiguous optimiza-tion goals for each query and eventually causes themodel to learn similar grasp poses for all queries.DSMT. We serialize the optimizing process and pro-pose a Dynamic-Static Matching Training (DSMT) strat-egy, aiming to alleviate the optimization challenge arisingfrom the instability of the Hungarian Algorithm and the",
  ". Hungarian matching instability during training ofdifferent penetration loss weights. The instability is measuredby the IS metric introduced in , where a higher value indicatesgreater instability": "strong impact of object penetration loss. The key insightis to guide the model learning towards appropriate targetsthrough dynamic training, and subsequently optimizing ob-ject penetration through static training.As illustrated in Algorithm 1, DGTR optimization be-gins with regular training with the hand regression lossand no object penetration loss for T0 epochs (DMT). Thematching results between the predictions and the targets aredynamically generated by the Hungarian Algorithm. Thelearnable queries are adequately trained to learn diversegrasping patterns in this stage.In the Static Matching Warm-up (SMW) stage, we re-move the Hungarian Matching process and utilize fixed andstable matching results recorded in the DMT stage. Theobjective of this stage is to finetune the model and make itadapt to the given static matching. Thus, we still excludethe object penetration loss in this stage.In the Static Matching Penetration Training (SMPT)stage, the object penetration loss and the hand-object dis-tance loss (Eq. (1)) are incorporated into the training pro-cess. The matching results used in the previous stage arepreserved to maintain a stable optimization environment. Inthis way, the severe penetration issue arising from the lackof object penetration penalty in the previous training stagesis significantly alleviated.",
  ". Adversarial-Balanced Test-Time Adaptation": "Object Contact vs. Object Penetration. To further im-prove the practicality of the predicted grasps, we propose anadversarial-balanced test-time adaptation (AB-TTA) strat-egy to refine the predicted grasps during the test phase. It isworth noting that our AB-TTA eliminates the need for com-plex force analysis or auxiliary models. Specifically, thisstrategy mainly minimizes a pair of adversarial losses, theobject penetration loss Lpen and hand-object distance lossLdist in the parameter space of the dexterous hand. How-ever, the comprehensive optimization of these two lossesis challenging. The penetration loss can be easily reduced(i.e., pulling the hand away from the object) in the parame-",
  "Output: Optimized model parameters": "ter space without appropriate constraints, causing the hand-object distance loss to lose efficacy. Hence, we incorporatetwo key designs to facilitate a balanced decrease of theseadversarial losses, which brings considerable improvementin both hand-object contact and hand-object penetration.AB-TTA. Our AB-TTA is based on the perception thatthe generated grasp poses are already or nearly valid, onlyrequiring slight adjustments. Firstly, we propose to mod-erate the displacement of the global translation of the rootlink of the dexterous hand during the optimization processby downscaling its gradient with t. Moderating the globaltranslation constrains the over-optimization of object pene-tration loss, which promotes the effectiveness and stabilityof the adaptation.Secondly, we present a generalized tta-distance loss toaddress the ineffectiveness of vanilla distance loss used in. The vanilla distance loss is defined as:",
  "iI(d(pi) < ) d(pi),(1)": "where I() is the indicator function, is a contact thresholdto filter out the outliers, and d(pi) is the distance betweenthe nearest point on the object point cloud and the ith key-point pi on the predicted hand. We observe that the vanilladistance loss will be 0 if the hand is too far away from theobject, where no point meets the conditions (d(pi) < ).As a result, the hand is unlikely to be pushed towards theobject again since the distance loss has been 0. We improve the hand-object distance loss by defining a more generalcondition which constrains the hand keypoints that initiallytouched the object to remain in contact during optimization.The generalized tta-distance loss is defined as:",
  "iI((d(pci) < ) (d(pri ) < )) d(pri ),": "(2)where pci and pri are the ith keypoints of the initial coarsehand and the refined hand at the current iteration, respec-tively. As a result, a input hand which is nearly valid wouldnot be pulled too far away from the object.In addition, due to the high DoF of dexterous hands, wealso add self-penetration loss Lspen in AB-TTA. Thus, theoverall loss function for AB-TTA is",
  "The optimization of DGTR involves the grasp losses and thebipartite matching between the predictions and the groundtruths. We denote the ith predicted item as xi and the jth": "ground-truth item as xj (x {g, t, r, q}) in the followingparagraphs of this section.Hand Parameters Regression Loss.We utilize thesmooth L1 loss as Ltrans and Ljoints to regress thetranslations and joint angles. For the rotation, we maximizethe similarity of the predicted and ground-truth quaternionswith Lrotation(ri,rj) = 1.0 |ri rj|, where () is the in-ner product operation. The overall regression loss for handparameters is a weighted sum of the above losses:",
  "+ 2 Ljoints(qi, qj) + 3 Lrotation(ri,rj).(4)": "Hand Chamfer Loss. We incorporate a hand chamferloss Lchamfer(gi, gj) to explicitly minimize the discrepan-cies between the actual shapes of the predicted and ground-truth hands. Specifically, we apply gi and gj to the dexter-ous hand and obtain the hand meshes H(gi) and H(gj) byforward kinematics. Then we sample the hand point clouds(gi) and (gj) from the corresponding meshes and cal-culate the Chamfer distance between them.Penetration Loss. We employ two penetration losses:1) Lpen(gi, O) : object penetration calculated by thesigned squared distance function from object point cloudto the hand mesh, and 2) Lspen(gi) : self penetrationdepth from the keypoints of the hand to themselves.Cost Function for Bipartite Matching. To obtain a bi-partite matching between predictions and ground truths, thecost function for each pair of (gi, gj) is defined as:",
  "Camera": ". Visualization of predicted dexterous hand poses. We visualize four grasp poses in five images for each object. The first imagevisualizes all grasps together to demonstrate their global positions. The following four images mainly visualize the details of the grasppose. These visualization results qualitatively indicate that the proposed DGTR framework is capable of generating diverse and feasiblegrasps with the same input and only in one forward pass. More visualization results can be found in Appendix C.",
  "iC(gi, gi).(6)": "The process of computing when M = N is similar, exceptthat we leave the redundant predictions or ground truths un-matched. As a result, there are K = min{M, N} matchedpairs accounting for the overall loss.Overall Loss Function.The overall grasp loss forDGTR training is a weighted sum of the aforementionedlosses, which is formulated as:",
  ". Dataset and Evaluation Metrics": "We evaluate the proposed DGTR framework in the chal-lenging dexterous grasping benchmark DexGraspNet ,which contains 1.32 million grasps of ShadowHand for5355 objects from more than 133 object categories. The of-ficial training-validation split is used in our experiments.We use five metrics to conduct comprehensive evalua-tions of the generating quality of DGTR. That is, 1) MeanQ1 reflects grasp stability. We follow to set the con-tact threshold to 1cm and set the penetration threshold to5mm. 2) Maximal penetration depth (cm) (Pen.), whichis the maximal penetration depth from the object point cloudto hand meshes. 3) Non-penetration ratio np (%), whichis the proportion of the predicted hands with a maximal pen- etration depth of less than 5mm. 4) Torque balance ratiotb (%), denoting the percentage of torque-balanced grasps(i.e. Q1 > 0). 5) Grasping success rate success (%) inIsaac Gym . Following , we consider a grasp posevalid if the grasp can hold the object steadily under any oneof the six gravity directions.For diversity, we introduce the new metrics, 6) occu-pancy proportion of translations t, rotations r and jointangles q (%), to quantitatively measure the ability of amodel to grasp objects from a diverse range of directions,orientations, and joint angles. Generally, we discretize thecontinuous parameter space into = 16 uniform bins andcalculate the proportion of occupied spaces for differentgrasps of each object. For t, we uniformly sample pointsas the bins on a unit sphere with Fibonacci sampling, andthen assign each grasp to a bin based on the cosine simi-larity between its global translation and the correspondingdirection of the point. For r and q, we discretize the rangeof Euler angle into bins. Intuitively, higher values of tindicate that the predicted grasps can move to more areas ofthe object and grasp it from more directions, while higher rand q suggest more various hand orientations and gestures.All details of metrics can be found in Appendix A.",
  ". Implementation Details": "Our DGTR is implemented with PyTorch and trainedon a single RTX 4090 GPU. The number of queries N isset to 16. The training epochs for each stage in DSMT areT0 = 15, T1 = 5, T2 = 5. We set 1 = 2.0, 2 = 1.0,and 3 = 2.0 for the Hungarian Algorithm cost function.During DMT and SMW, the loss weight are 1 = 10.0,2 = 10.0, 3 = 10.0, 4 = 1.0, 5 = 10.0, 6 = 0.0. Inthe SMPT stage, 6 is set to 50.0, and distance loss weightis 10.0. For AB-TTA, we set t = 0, 1 = 5, 2 = 3, and3 = 5. More details can be found in Appendix A.",
  "Comparison with SOTA in one forward pass": "We first compare SOTA dexterous grasp generation meth-ods with DGTR in our setting, where each method is al-lowed to infer once. DDG takes multi-view imagesas input and only predicts one grasp pose for each object,which serves as a quality reference. SceneDiffuser ,GraspTTA and UniDexGrasp samples 16 times ina batch, with the same object point cloud as condition.The evaluation results are shown in . For graspquality, DGTR surpasses the SOTA generative models inseveral important metrics. Note that UniDexGrasp has re-markable performance in np and Pen. but with a low tb,which suggests low contact with the object, while DGTRhas a more balanced performance and higher success rate.Moreover, owing to the capability of generating diversegrasps, DGTR can efficiently select top-4 results (DGTR*)by the number of contact points and object penetration dur-ing inference without extra inputs. In this scenario, DGTR* has comparable results with DDG .For diversity, DGTR surpasses UniDexGrasp andGraspTTA by a large gap in terms of t and r, whichindicates that DGTR is able to grasp the object from a va-riety of directions. SceneDiffuser has higher diver-sity but with much lower quality. More comparisons with",
  ". Comparison of grasp diversity in one forward pass with4 outputs. The diversity of our DGTR significantly surpasses and in one forward pass": "SceneDiffuser are in Appendix B. The results demonstratethat DGTR achieves overall SOTA performance and excelsin generating high-quality and diverse grasps.We visualize the predicted grasp poses of several ob-jects in to provide a qualitative result of DGTR.DGTR is capable of generating high-quality grasps of anobject from various directions with different poses in oneforward pass. Furthermore, highlights the diversityof DGTR in comparison to two other generative methods.",
  "Comparison with SOTA in multiple forward pass": "presents a comparison of grasping diversity andinference time between UniDexGrasp in multiple forwardpasses and DGTR in one forward pass.UniDexGraspfirst utilizes a probabilistic model to sample rotations andthen rotates object point clouds to generate grasps in mul-tiple passes.As shown in , DGTR exhibits sig-nificantly lower time consumption compared to the multi-pass UniDexGrasp. More importantly, DGTR outperformsUniDexGrasp with 16 forward passes in t and r. This in-dicates that DGTR offers more diverse grasping hand posi-tions and enables grasping from a wider range of directions.",
  "Dynamic-Static Matching Training Strategy": "As demonstrated in , our DSMT significantly en-hances Q1 by 3.5 times, while reducing Pen. by nearly50%. provides more details on the performance af-ter each training stage (DMT, SMW and SMPT) in DSMT.The results highlight the critical role of static matching,which optimizes the model towards the proper direction andsignificantly reduces object penetration.",
  "Adversarial-Balanced Test-Time Adaptation": "We conduct ablation studies on our AB-TTA module andthe results are in , , and . As shownin , our AB-TTA significantly increases Q1 by 1.85-fold, and enhances np, and tb at the same time.Fur-thermore, shows that the integration of our key de-signs (i.e., generalized tta-distance loss (GDis) and transla-tion moderation strategy (TM)) are indispensable, while thesimple implementation of TTA (i.e., penetration and vanilladistance loss (VDis)) has limited effect. Furthermore, ourAB-TTA module demonstrates superior grasp quality com-pared to ContactNet-TTA , and it can even boost the Q1and tb performance of ContactNet-TTA.",
  "Loss Weight for Object Penetration": "The results in show that the object penetration de-creases as pen increase, but a severe non-contact issue oc-curs concurrently. As illustrated in and ,the instability of Hungarian matching leads to model col-lapse when we apply a large penetration loss. And it isworth noting that gradually increasing pen from 0 to 50after several warm-up epochs cannot tackle this problem(pen = 0 50 in ). We believe that learningto predict multiple grasps simultaneously is a more difficultoptimization process compared to the previous one-to-one",
  "Number of Grasping Queries": "We conduct experiments to analyze the effect of the num-ber of grasping queries. As shown in , the graspquality Q1 tends to decrease as the number of queries in-creases, which suggests that simultaneous learning a largerset of grasping poses is a challenge. Furthermore, the di-versity increases as the number of queries becomes larger,implying that DGTR can learn a more diverse set of graspwith a greater number of queries.",
  ". Conclusions": "In this work, we propose DGTR (Dexterous Grasp Trans-former), a novel discriminative framework for dexterousgrasp generation.Our progressive strategies, includingdynamic-static matching training (DSMT) strategy andadversarial-balanced test-time adaptation (AB-TTA), sub-stantially improve grasping stability and reduce penetration.To the best of our knowledge, DGTR is the first work to in-troduce set prediction formulation into dexterous grasp do-main and achieves both high quality and diversity with oneforward pass. We believe that DGTR holds good develop-ment potential in robotic dexterous grasping scenarios, suchas task-oriented and real-world dexterous grasp generation.",
  "Acknowledgements": "We thank Jialiang Zhang for his helpful discussion. Thiswork was supported in part by the National Key Researchand Development Program of China (2023YFA1008503),NSFC (U21A20471, U1911401), and Guangdong NSFProject (No. 2023B1515040025, 2020B1515120085). Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European confer-ence on computer vision, 2020. 3, 4 Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao,and Chao Dong.Activating more pixels in image super-resolution transformer. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, 2023. 4 Enric Corona, Albert Pumarola, Guillem Alenya, FrancescMoreno-Noguer, and Gregory Rogez. Ganhand: Predictinghuman grasp affordances in multi-object scenes. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, 2020. 2, 3 Xiyang Dai, Yinpeng Chen, Jianwei Yang, PengchuanZhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-to-end object detection with dynamic attention. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, 2021. 4 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In International Con-ference on Learning Representations, 2020. 4 Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point setgeneration network for 3d object reconstruction from a singleimage. In Proceedings of the IEEE conference on computervision and pattern recognition, 2017. 6 Enrico Maria Fenoaltea, Izat B Baybusinov, Jianyang Zhao,Lei Zhou, and Yi-Cheng Zhang. The stable marriage prob-lem: An interdisciplinary review from the physicists per-spective. Physics Reports, 2021. 5",
  "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-nications of the ACM, 2020. 3": "Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, TengyuLiu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3d scenes.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2023. 2, 3, 8 Hanwen Jiang, Shaowei Liu, Jiashun Wang, and XiaolongWang. Hand-object contact consistency reasoning for hu-man grasps generation. In Proceedings of the InternationalConference on Computer Vision, 2021. 2, 3, 8, 9 Jiayu Jiao, Yu-Ming Tang, Kun-Yu Lin, Yipeng Gao, Jin-hua Ma, Yaowei Wang, and Wei-Shi Zheng. Dilateformer:Multi-scale dilated transformer for visual recognition. IEEETransactions on Multimedia, 2023. 4",
  "Min Liu, Zherong Pan, Kai Xu, Kanishka Ganguly, and Di-nesh Manocha. Deep differentiable grasp planner for high-dof grippers. arXiv preprint arXiv:2002.01530, 2020. 2, 3,8": "Tengyu Liu, Zeyu Liu, Ziyuan Jiao, Yixin Zhu, and Song-Chun Zhu. Synthesizing diverse and physically stable graspswith arbitrary hand structures using differentiable force clo-sure estimator. IEEE Robotics and Automation Letters, 2022.3 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, 2021. 4",
  "Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, 2021. 3, 4": "Kieran A Murphy, Carlos Esteves, Varun Jampani, Sriku-mar Ramalingam, and Ameesh Makadia. Implicit-pdf: Non-parametric representation of probability distributions on therotation manifold. In International Conference on MachineLearning, 2021. 3 George Papamakarios, Eric Nalisnick, Danilo JimenezRezende, Shakir Mohamed, and Balaji Lakshminarayanan.Normalizing flows for probabilistic modeling and inference.The Journal of Machine Learning Research, 2021. 3 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-perative style, high-performance deep learning library. Ad-vances in neural information processing systems, 2019. 7",
  "Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas JGuibas. Pointnet++: Deep hierarchical feature learning onpoint sets in a metric space. Advances in neural informationprocessing systems, 2017. 4": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, 2022. 3 Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-mans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement.IEEE Transactions onPattern Analysis and Machine Intelligence, 2022. 2",
  "Kihyuk Sohn, Honglak Lee, and Xinchen Yan.Learningstructured output representation using deep conditional gen-erative models. Advances in neural information processingsystems, 2015. 3": "Dylan Turpin, Liquan Wang, Eric Heiden, Yun-Chun Chen,Miles Macklin, Stavros Tsogkas, Sven Dickinson, and Ani-mesh Garg. Graspd: Differentiable contact-rich grasp syn-thesis for multi-fingered hands. In European Conference onComputer Vision, 2022. 3 Julen Urain, Niklas Funk, Jan Peters, and Georgia Chal-vatzaki. Se (3)-diffusionfields: Learning smooth cost func-tions for joint grasp and motion optimization through diffu-sion. In 2023 IEEE International Conference on Roboticsand Automation (ICRA), 2023. 3 Jacob Varley, Jonathan Weisz, Jared Weiss, and Peter Allen.Generating multi-fingered robotic grasps via deep learning.In 2015 IEEE/RSJ international conference on intelligentrobots and systems (IROS), 2015. 3 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 2017. 4 Ruicheng Wang, Jialiang Zhang, Jiayi Chen, Yinzhen Xu,Puhao Li, Tengyu Liu, and He Wang.Dexgraspnet: Alarge-scale robotic dexterous grasp dataset for general ob-jects based on simulation. In 2023 IEEE International Con-ference on Robotics and Automation (ICRA), 2023. 3, 6, 7,8 Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,Jose M Alvarez, and Ping Luo. Segformer: Simple and ef-ficient design for semantic segmentation with transformers.Advances in Neural Information Processing Systems, 2021.4 Yinzhen Xu, Weikang Wan, Jialiang Zhang, Haoran Liu,Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng,Yijia Weng, Jiayi Chen, et al.Unidexgrasp: Universalrobotic dexterous grasping via learning diverse proposalgeneration and goal-conditioned policy. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023. 2, 3, 6, 8",
  "Jiaming Zhou, Kun-Yu Lin, Yu-Kun Qiu, and Wei-ShiZheng. Twinformer: Fine-to-coarse temporal modeling forlong-term action recognition. IEEE Transactions on Multi-media, 2023. 4": "Tianqiang Zhu, Rina Wu, Xiangbo Lin, and Yi Sun. Towardhuman-like grasp: Dexterous grasping via semantic repre-sentation of object-hand. 2021 ieee. In CVF InternationalConference on Computer Vision (ICCV), 2021. 3 Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,and Jifeng Dai. Deformable detr: Deformable transformersfor end-to-end object detection. In International Conferenceon Learning Representations, 2020. 4"
}