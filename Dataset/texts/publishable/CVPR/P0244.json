{
  "Abstract": "The Segment Anything Model (SAM) marks a notablemilestone in segmentation models, highlighted by its robustzero-shot capabilities and ability to handle diverse prompts.SAM follows a pipeline that separates interactive segmen-tation into image preprocessing through a large encoderand interactive inference via a lightweight decoder, ensur-ing efficient real-time performance. However, SAM facesstability issues in challenging samples upon this pipeline.These issues arise from two main factors. Firstly, the im-age preprocessing disables SAM to dynamically use image-level zoom-in strategies to refocus on the target object dur-ing interaction. Secondly, the lightweight decoder strug-gles to sufficiently integrate interactive information with im-age embeddings. To address these two limitations, we pro-pose FocSAM with a pipeline redesigned on two pivotal as-pects. First, we propose Dynamic Window Multi-head Self-Attention (Dwin-MSA) to dynamically refocus SAMs im-age embeddings on the target object. Dwin-MSA localizesattention computations around the target object, enhanc-ing object-related embeddings with minimal computationaloverhead. Second, we propose Pixel-wise Dynamic ReLU(P-DyReLU) to enable sufficient integration of interactiveinformation from a few initial clicks that have significantimpacts on the overall segmentation results. Experimen-tally, FocSAM augments SAMs interactive segmentationperformance to match the existing state-of-the-art methodin segmentation quality, requiring only about 5.6% of thismethods inference time on CPUs.Code is available at",
  "IoU=93.2310th Click": ". Interactive segmentation stability on a challenging ex-ample. The bottom-left shows the example overlaid with GT (pur-ple masks). The top and middle rows illustrate the interactive seg-mentation of SAM and the proposed FocSAM, where each clickis placed at the center of erroneously predicted regions and cate-gorized as either positive (green) or negative (red). SAMs perfor-mance is unstable in this example (top row), where the 9th clickyields an IoU of 88.59 (left) but a subsequent click significantlyreduces the IoU to 12.78 (right). In contrast, FocSAM (middlerow) shows consistent performance. The plot (bottom-right) sum-marizes the trends of 20 clickss segmentation, clearly contrastingSAMs IoU fluctuations with FocSAMs stable performance. Model (SAM) excels in real-time, high-quality inter-active segmentation, responding to annotator prompts suchas clicks , bounding boxes , or coarse masks .SAMs generalizability and efficiency in processing di-verse prompts make it a versatile tool across a spectrumof segmentation-related tasks. This study focuses on click-based interactive segmentation building upon SAM .",
  "arXiv:2405.18706v1 [cs.CV] 29 May 2024": "This pipeline incorporates powerful Vision Transformers(ViTs) as the image encoder to preprocess im-ages, generating image embeddings that are applicable toall objects within the same image. During the interaction,these image embeddings and the prompts (e.g. clicks) fromannotators are fed into a lightweight decoder to producesegmentation results. This pipeline combines the power oflarge ViTs with the speed needed for on-the-spot interac-tive segmentation. Following such a pipeline, SAM evenenables annotators to perform real-time, high-quality inter-active segmentation on CPU-only devices, aiding in the sig-nificant expansion of image segmentation annotations .However, SAMs pipeline has two limitations. First, thepipelines image preprocessing disables the efficient imple-mentation of the image-level zoom-in strategy that dy-namically refocuses the model on the target object duringinteraction. Second, SAMs lightweight decoder strugglesto sufficiently fuse the interactive information with the pre-processed image embeddings due to the need for real-timeresponses, thus weakening the interactive feedbacks posi-tive impact on segmentation quality. Consequently, SAMfaces instability issues in challenging scenarios, such ascamouflaged objects almost blending into the back-ground. clearly illustrates the instability of SAMssegmentation results, where an additional click following asufficient number of previous ones (e.g., 9 clicks) can un-expectedly trigger substantial degradation in segmentationquality, exemplified by a drop in IoU from 88.59 to 12.78.Such instability significantly limits SAMs applicability ina broader range of image segmentation annotations.Therefore, we propose FocSAM to address SAMs lim-itations. FocSAMs pipeline builds upon SAM and intro-duces an extra focus refiner. This refiner adjusts SAMs im-age embeddings for each object during the objects interac-tion, adding ignorable computations. The adjustment facili-tates two major improvements. First, the refiner uses initialsegmentation results to refocus the image embeddings onregions containing the target object, inspired by the image-level zoom-in . Second, the refiner sufficiently fuses theembeddings with a few initial clicks that prove to have greatimpact on final segmentation results , further enhancingthe object-related embeddings.To implement FocSAMs focus refiner with minimalcomputational overhead, we introduce Dynamic WindowMulti-head Self-Attention (Dwin-MSA) and Pixel-wise Dy-namic ReLU (P-DyReLU). Dwin-MSA partitions imageembeddings into windows and perform efficient attentioncomputations on a dynamic minimal subset of the win-dowed embeddings that intersect with previously predictedmasks. Such a dynamic manner avoids redundant compu-tations on irrelevant background areas. Dwin-MSA usesthe shifting strategy to ensure long-distance interac-tions among embeddings, preserving dynamic efficiency. P-DyReLU is employed as the non-linear activation in theDwin-MSA to fuse the interactive information from a fewinitial clicks with the image embeddings. Specifically, P-DyReLU adopts DyReLU and utilizes SAM decodersclick-fused query embeddings to enhance the object-relatedimage embeddings and suppress object-unrelated ones.Experimentally, FocSAM demonstrates superior interac-tive segmentation performance over SAM with negligibleadditional computational costs. FocSAM matches the state-of-the-art SimpleClick in Number of Clicks (NoC)across datasets including DAVIS , SBD , Grab-Cut , Berkeley , MVTec and COD10K , butFocSAM requires only about 5.6% of the CPU inferencetime compared to SimpleClick. Moreover, as the number ofobjects per image surpasses 10, FocSAMs time efficiencyfurther improves, demanding roughly 1.2% of the time re-quired by SimpleClick for CPU inference.We summarize our contributions as follows:",
  ". Interactive Segmentation": "The integration of deep networks into interactive segmen-tation is initiated by DIOS , leadingto subsequent advancements in click-based methods likeDEXTR , FCA-Net , BRS , and f-BRS .The following methods focus on en-hancing various aspects of interactive segmentation. Sim-pleClick is the first to introduce large Vision Trans-formers into this field. InterFormer follows witha novel pipeline to reduce model redundancy by reusingimage features. SAM also adopts this pipeline andachieves robust zero-shot capabilities and diverse prompts,leading to various downstream applications . However, SAM is unable to employ the image-levelzoom-in strategy efficiently and integrate interactiveinformation effectively, hindering its broader applications.We introduce FocSAM to address SAMs limitations.",
  "Image Embedding": ". Overview of proposed FocSAM building upon SAM. SAM comprises an image encoder, a prompt encoder and a decoder. Theimage encoder transforms images into image embeddings before interaction. In each interaction of an object, the prompt encoder convertsthe previous mask and annotator clicks into mask and click embeddings, respectively. These three embeddings and a learnable queryembedding are fed into the decoder for segmentation. Upon SAMs pipeline, FocSAM introduces a focus refiner that is employed onceper object during interaction (Figure (a)). In an early step of SAMs interaction, this refiner processes SAMs image embeddings, previousmask and click-fused query embedding through a stack of refine blocks (Figure (b)). Each block receives the image and query embeddingswith the mask shared across all the blocks, and produces the image and query embeddings fed into the subsequent block. The final outputis a refined image embedding, which replaces the original image embedding for subsequent interactions with the object. range of research . One typical way is tolimit the attention region of each token from full-attentionto local/windowed attention . This strategyhas garnered significant interest, as evidenced by variousstudies . More recently, CSwin in-troduces Cross-Shaped Window Self-attention to computeconcurrently in both orientations.Beyond Fixation proposes DW-ViT to fuse multi-scale information. In thispaper, we propose Dwin-MSA to perform dynamic windowattention on object-related image embeddings.",
  ". Pipeline": "SAMs pipeline. In , SAM comprises an im-age encoder, a prompt encoder and a decoder. The imageencoder preprocesses each image only once before the in-teraction, despite the varying number of objects within theimage. Instead, both the prompt encoder and the decoderactively engage in every interaction, rapidly processing an-notator clicks to predict segmentation results.Image encoder. In SAMs preprocessing phase, images areresized and padded to 10241024 and fed into a ViT-based image encoder . This encoder is structured in four stagesof equal depth and utilizes window-based attention in eachstage for efficient computation , with full attention ap-plied at each stages end. Following this, simple convolu-tional layers further reduces the dimensions to produce 256-dimensional embeddings F RH16 W 16 256, correspondingto non-overlapping 16 16 image patches.Prompt encoder. In SAMs interaction phase, the promptencoder transforms annotator prompts into embed-dings. These prompts include N clicks at the Nth interac-tion, each with x, y coordinates and a label indicating pos-itive or negative. A positive click in a false negative regionsignals the model to expand that region and a negative clickin a false positive region suggests removal. Starting fromthe second interaction for each object, the prompt encoderalso converts the previously predicted segmentation maskinto mask embeddings. The transformed click embeddingsc RN256 and mask embeddings E RH16 W 16 256 willbe fed into the SAM decoder, as depicted in .Decoder. Following the prompt encoder, the decoder re-ceives image embeddings F , mask embeddings E, clickembeddings c and learnable query embeddings. The num-ber of query embeddings corresponds to the expected out-put masks by the decoder. In our work, we use a singlequery embedding q R1256. During decoding, the con-catenated embeddings [q; c] R(N+1)256 undergo cross-attention with the mask-fused image embedding F + E.They alternate roles of query and key/value in the cross at-tention without involving image-to-image attention. After",
  "Query Embedding": ". Overview of FocSAMs focus refiner. Figure (a) depicts the overall architecture of the focus refiner. Figure (b) details the refineblock, showing the flow of image and query embeddings through the Dwin and MSA modules. Figures (c) and (d) highlight the windowselection within the Dwin module and the shift strategy. Figure (e) provides a detailed view of the MSA module.",
  "256 that hasbeen upsampled by some convolutions and the click-fusedquery embedding qc R1256, with the click embeddingsdiscarded. Their dot product F cqc RH4 W": "4 1 generateslogits for predicting the final mask M.FocSAMs pipeline. Building upon SAMs pipeline, Foc-SAMs pipeline introduces the focus refiner. The refiner isemployed once for each object. Specifically, at the Kth in-teraction of an object, the refiner receives the image em-bedding F , the previously predicted mask M (K1) andthe previous click-fused query embedding q(K1)c. Then,the refiner produces a refined image embedding F (K)rRH16 W 16 256 that has object-related embeddings. F (K)rre-places the original embedding F in all the subsequent inter-action on this object. As illustrated by (b), this fo-cus refiner comprises a stack of refine blocks. These blocksrefine the image and query embeddings iteratively, sharingthe same previous mask. The image embedding from the fi-nal block serves as the refiner output. We detail these refineblocks in the following subsection.",
  ". Refine Block": "Overview. In (a), the plain refine block and theshift refine block alternately stack within the refiner, refin-ing the image embedding F and click-fused query embed-ding q(K1)cwith the shared mask M (K1). They sharemost modules, differing mainly in the Dwin and Shift Dwin( (b)). Both the Dwin and Shift Dwin identify thebounding box around the object from the mask M (K1)",
  "The refined embeddings and the correspondingly duplicatedquery embeddings are fed into the MSA module ((e)). Then, we detail Dwin and Shift Dwin": "Revisiting image-level zoom-in. Given an image I and abounding box, the image-level zoom-in strategy is for-mulated as resize(I[y1 : y2, x1 : x2], (H, W)), where cor-ner coordinates (x1, y1), (x2, y2) define the bounding boxand (H, W) is the model input size. Adapting this strat-egy to the embeddings typically involves RoIAlign that crops and resizes embeddings using a linear samplingmethod. However, RoIAlign faces two main issues. First,RoIAlign assumes that embeddings can be linearly interpo-lated like images, which may not hold for SAMs imageembeddings due to lack of the corresponding smoothness-aware training. Second, RoIAlign uniformly resizes all ob-jects, ignoring size differences, which limits representationfor larger objects and adds redundancy for smaller ones.",
  "256 can be windowed as F RLSS256": "with L = BHW/(16S)2. Then, the windows intersectingthe box are selected ( (c). For all objects withinthese images, we can simultaneously select all windows in-tersecting with their respective bounding boxes despite theobjects sizes. This leads to the selected embedding win-dows F W RMSS256, with M the number of win-dows interacting with the boxes. Each window performs in-dependent computations like self-attention within the win-dow, and updates its own embeddings with the computationresults, freezing the unselected embedding windows. Long-range patch-to-patch attention. We further employthe shifting strategy in the Shift Dwin ((d)). Alternating the Dwin and Shift Dwin ensures suffi-cient information exchange between all the patches withinthe bounding box. Moreover, the boxes typically limit thespatial distance between embeddings within the same ob-ject, implying that a few blocks and small window sizes stillallow sufficient information exchange.MSA module. The MSA ( (e)) processes F W seach window f RSS256 parallelly, with the duplicatedquery embedding qc = copy(qK1c) R1256. Let",
  ". Pixel-wise Dynamic ReLU": "Dynamic ReLU. DyReLU extends the conventionalReLU by introducing input-dependent activation parame-ters. For an input vector x, the dynamic activation func-tion f(x; (x)) uses parameters (x) that adapt based onx. In details, the traditional ReLU function y = max{x, 0}is generalized in DyReLU to a parametric piecewise linearfunction yc = maxk{akcxc + bkc} for each element xc of x.DyReLU adapts coefficients akc and bkc based on x:",
  "yc = f(x)(xc) = max1kK{akc(x)xc + bkc(x)},(6)": "where all the coefficients {akc}, {bkc} are outputs of the hy-per function (x). The plain ReLU is a special case ofK = 2 with a1 = 1 and b1 = a2 = b2 = 0.Pixel-wise DyReLU. Considering Equation 4, we imple-ment (x) to fuse f RSS256 from Equation 2 withqf R1256 from Equation 3. The implementation is in-spired by the SAM decoders use of a dot product betweenimage and query embeddings to generate logits for maskprediction . This process effectively captures the un-normalized similarity between each image embedding andthe query embedding in a pixel-wise manner. We adopt",
  ". Experimental Setting": "Datasets. Following the previous methods ,we train our models on COCO and LVIS , and thenevaluate all the methods zero-shot interactive segmenta-tion capabilities on various other datasets including Grab-Cut , Berkeley , SBD and DAVIS . Ourevaluation also extends to more challenging datasets includ-ing MVTec and COD10K . Please refer to the sup-plementary materials for more details on the datasets.Implementation details. We utilize the pre-trained ViT-Huge from SAM as the backbone with the prompt en-coder and decoder. For the proposed focus refiner, we con-figure a total of 12 blocks, comprising 6 plain refine blocks",
  "SAM-ViT-H ICCV230.35 (0.02)1.882.097.625.1913.9710.366.85FocSAM-ViT-H (Ours)0.39 (0.02)1.321.474.694.7711.148.915.38": ". Comparison of NoC@90 with previous methods. We report results on GrabCut , Berkeley , SBD , DAVIS ,MVTec and COD10K . The best results are highlighted in bold. signifies that the SPC metric incorporates both decoder inferencetime and encoder inference time averaged over 20 clicks. For our FocSAM, the SPC additionally includes the proposed refiners inferencetime averaged over 20 clicks. The decoder-only SPC is separately noted in parentheses, indicating the actual interaction time. denotesmethods that have not followed the conventional COCO +LVIS training for interactive segmentation. Our FocSAM achievesstate-of-the-art NoC@90 performance, while the SPC on CPUs is only about 5.6% of the previous SOTA SimpleClick-ViT-H . and 6 shift refine blocks. The embedding dimensions ofboth Dwin-MSA and P-DyReLU are set to align with the256-dimensional SAM image embeddings.The windowsize for Dwin-MSA is set to 16. The refine step K is setto 2, i.e., the focus refiner activates after the second click.Further details are available in the supplementary materials.Training strategy. In training FocSAM, we adopt Inter-Formers click simulation strategy for interactive sim-ulation before loss computation. SAMs image encoder andprompt encoder are frozen during training. Moreover, weuse the image encoder to pre-extract and store the COCO-LVIS image embeddings to reduce computational costs. Weresize and pad the images to match SAMs input size of1024 1024. We employ a two-stage training strategy in-volving firstly fine-tuning the SAM decoder for 320k itera-tions at a batch size of 16 and then training FocSAM withthe frozen decoder for 160k iterations in the same settings.This strategy addresses the training instability caused bythe refiners loss dependency on the decoder. Training andevaluations are performed on a server with 4 NVIDIA RTX3090 GPUs and dual Intel Xeon Silver CPUs. More detailsare provided in the supplementary materials.Evaluation. In the evaluation, following SAM , im- ages are resized and padded to 1024, and the segmentationresults from the decoder are then adjusted back to their orig-inal size for IoU calculations. For click simulation in test-ing, we place clicks at the centers of erroneously predictedregions, in line with previous methods . The bi-nary label of each click is determined by the maximum dis-tance to the boundaries of false negative and false positiveregions. FocSAM is evaluated in both inference speed andsegmentation performance. Speed is quantified as SecondsPer Click (SPC) on CPUs, indicating the average inferencetime per click. For segmentation performance, we use theNumber of Clicks (NoC) metric that is the average mini-mum clicks required to reach a specified IoU. We mainlyfocus on NoC@90 under 20 clicks, i.e., the average clicksneeded to achieve 90% IoU. In cases where more than 20clicks are needed, the count is capped at 20 for evaluationconsistency with previous methods . AdditionalNoC metrics are employed in the ablation study.",
  ". Stability Analysis": "Experimental Settings. To evaluate the stability, we con-duct statistical analyses on the three large-scale datasets, i.e.SBD, MVTec and COD10K. Similar to the evaluation onNoC metrics, each click is placed at the center of the erro-neously predicted regions. The number of simulated clicksper sample is increased from 20 to 100. For each sample,from the second interaction click onwards, we calculate theIoU, which is the difference in IoU between consecutiveclicks, and filter out IoU greater than 1%. This ensuresthat only significant deteriorations in segmentation qualityare considered. The remaining IoUs are then visualized.Results. As illustrated in , FocSAM exhibits con-siderably better stability across all datasets compared toSAM. The IoU distribution for FocSAM shows a right-ward shift, indicating fewer samples of deteriorating seg-mentation with subsequent clicks. Although SAM occa-sionally achieves favorable outcomes, its inherent instabil-ity often necessitates additional annotator interactions forcorrecting errors. Therefore, FocSAM represents a stabilityadvance over SAM in terms of real-world interactive effi-ciency, as evidenced by the stability analysis.",
  "SAMFocSAM": ". Qualitative analysis on a challenge Example. The first image from the left displays the challenge example with the image and GT(blue masks). The top and bottom rows on the right respectively show the segmentation results of SAM and FocSAM at the 1st, 5th, 10th,and 20th clicks. Clicks are indicated with green (positive) and red (negative) circles. these modules, we slightly modify the modules. For Dwin-MSA only, we remove all P-DyReLU modules, replacing P-DyReLUs activations in Dwin-MSA with standard ReLU.For P-DyReLU only, we remove the dynamic windows toretain all image embeddings, and remove Dwins atten-tion computations. We evaluate these variants on the threelargest datasets including SBD, MVTec, and COD10K, us-ing NoC@90 within 20 clicks, and extend to NoC@95within 100 clicks for deeper analysis. This NoC@95 metricquantifies the individual contributions of each module, es-pecially on more challenging samples. All ablation modelsare trained with the same protocol of the main experiments.Results. shows that Dwin-MSA and P-DyReLU in-dividually contribute similarly to FocSAMs performance,indicating that they provide comparable interactive infor-mation. Dwin-MSA primarily focuses on initially predictedmasks for locating main object areas, similar to boundingbox prompts in SAM, whereas P-DyReLU leverages ini-tial clicks for primary object outlining. Their interactiveinformation is complementary. Consequently, their combi-nation leads to enhanced overall performance, particularlynoticeable in NoC@95 under 100 clicks. This metric under-scores the increased click requirement to achieve 95% IoUfor challenging samples.The integration of Dwin-MSAand P-DyReLU further stabilizes FocSAMs performanceon challenging samples. More ablation studies are providedin the supplementary materials.",
  ". Qualitative Results": "In , we present a qualitative comparison of Foc-SAM and SAM using a challenging example and visualizethe segmentation results at four different clicks. This visu-alization clearly demonstrates FocSAMs enhanced stabilityover SAM. Our qualitative analysis confirms that FocSAM maintains consistent performance, providing superior seg-mentation quality compared to SAM under such a challeng-ing example. Additional qualitative results are available inthe supplementary materials.",
  ". Conclusion": "SAM provides an efficient real-time pipeline for interac-tive segmentation, significantly advancing this field. How-ever, SAMs real-world application stability is compro-mised, particularly in challenging scenarios. This instabil-ity largely stems from SAMs pipeline, which lacks the ca-pability to effectively focus on the target object. Our pro-posed FocSAM tackles these stability issues by redesigningthe pipeline to dynamically refocus SAMs image embed-dings onto the target object. This adaptation enables Foc-SAM to stabilize the interactive segmentation process ofSAM, even in challenging scenarios. As a result, FocSAMnot only matches the state-of-the-art in segmentation qual-ity but also achieves this with considerably lower compu-tational demands on CPUs. These advancements highlightFocSAMs potential for broader real-world application. This work was supported by National Science andTechnology Major Project (No.2022ZD0118202), theNational Science Fund for Distinguished Young Scholars(No.62025603), the National Natural Science Founda-tion of China (No.U21B2037, No.U22B2051, No.62176222, No. 62176223, No. 62176226, No. 62072386,No. 62072387, No. 62072389, No. 62002305 and No.62272401), and the Natural Science Foundation of Fu-jian Province of China (No.2021J01002, No.2022J06001). David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Ef-ficient interactive annotation of segmentation datasets withpolygon-rnn++. In 2018 IEEE Conference on Computer Vi-sion and Pattern Recognition, CVPR 2018, Salt Lake City,UT, USA, June 18-22, 2018, pages 859868. IEEE ComputerSociety, 2018. 2 Paul Bergmann, Michael Fauser, David Sattlegger, andCarsten Steger.Mvtec ada comprehensive real-worlddataset for unsupervised anomaly detection. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 95929600, 2019. 2, 5, 6, 7, 1",
  "Yinpeng Chen, Xiyang Dai, Mengchen Liu, DongdongChen, Lu Yuan, and Zicheng Liu. Dynamic relu. In Eu-ropean Conference on Computer Vision, pages 351367.Springer, 2020. 2, 5": "Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-ing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.Twins:Revisiting the design of spatial attention in vi-sion transformers.Neural Information Processing Sys-tems,Neural Information Processing Systems, 2021. 3 Xiaoyi Dong, Jianmin Bao, Dongdong Chen, WeimingZhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo.Cswin transformer: A general vision transformer backbonewith cross-shaped windows. In 2022 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), 2022.3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An imageis worth 16x16 words: Transformers for image recognitionat scale. In 9th International Conference on Learning Rep-resentations, ICLR 2021, Virtual Event, Austria, May 3-7,2021. OpenReview.net, 2021. 2, 3 Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng,Jianbing Shen, and Ling Shao.Camouflaged object de-tection.In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 27772787,2020. 2, 5, 6, 7, 1 Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,Xinggang Wang, Tiejun Huang, Xinlong Wang, and YueCao. Eva: Exploring the limits of masked visual representa-tion learning at scale. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1935819369, 2023. 2",
  "Leo Grady. Random walks for image segmentation. IEEETransactions on Pattern Analysis and Machine Intelligence,2006. 2": "Jiaqi Gu, Hyoukjun Kwon, Dilin Wang, Wei Ye, Meng Li,Yu-Hsin Chen, Liangzhen Lai, Vikas Chandra, and David ZPan. Multi-scale high-resolution vision transformer for se-mantic segmentation. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1209412103, 2022. 2 Varun Gulshan, Carsten Rother, Antonio Criminisi, AndrewBlake, and Andrew Zisserman. Geodesic star convexity forinteractive image segmentation. In The Twenty-Third IEEEConference on Computer Vision and Pattern Recognition,CVPR 2010, San Francisco, CA, USA, 13-18 June 2010,pages 31293136. IEEE Computer Society, 2010. 2",
  "Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xi-angyang Xue, and Zheng Zhang. Star-transformer. In Pro-ceedings of the 2019 Conference of the North, 2019. 3": "Agrim Gupta, Piotr Dollar, and Ross B. Girshick. LVIS: Adataset for large vocabulary instance segmentation. In IEEEConference on Computer Vision and Pattern Recognition,CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages53565364. Computer Vision Foundation / IEEE, 2019. 5,6, 1 Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, andGao Huang. Flatten transformer: Vision transformer usingfocused linear attention. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 59615971, 2023. 3 Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen,Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, ZeyuChen, et al. Edgeflow: Achieving practical interactive seg-mentation with edge-guided flow. pages 15511560, 2021.6 Bharath Hariharan, Pablo Arbelaez, Lubomir D. Bourdev,Subhransu Maji, and Jitendra Malik.Semantic contoursfrom inverse detectors. In IEEE International Conference onComputer Vision, ICCV 2011, Barcelona, Spain, November6-13, 2011, pages 991998. IEEE Computer Society, 2011.2, 5, 6, 7, 1",
  "Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.Exploring plain vision transformer backbones for object de-tection. 2, 3": "Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactiveimage segmentation with latent diversity. In 2018 IEEE Con-ference on Computer Vision and Pattern Recognition, CVPR2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 577585. IEEE Computer Society, 2018. 2 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C. LawrenceZitnick. Microsoft coco: Common objects in context. Lec-ture Notes in Computer Science, 2014. 5, 6, 1 Zheng Lin, Zhao Zhang, Lin-Zhuo Chen, Ming-MingCheng, and Shao-Ping Lu. Interactive image segmentationwith first click attention.In 2020 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, CVPR 2020,Seattle, WA, USA, June 13-19, 2020, pages 1333613345.IEEE, 2020. 2",
  "Qin Liu, Meng Zheng, Benjamin Planche, SrikrishnaKaranam, Terrence Chen, Marc Niethammer, and Ziyan Wu.Pseudoclick: Interactive image segmentation with click imi-tation. pages 728745, 2022. 2, 5, 6": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 3, 5 Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.Swin transformer v2: Scaling up capacity and resolution. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1200912019, 2022. 2,5",
  "Jun Ma and Bo Wang. Segment anything in medical images.arXiv preprint arXiv:2304.12306, 2023. 2": "Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, andLuc Van Gool. Deep extreme cut: From extreme points toobject segmentation. In 2018 IEEE Conference on ComputerVision and Pattern Recognition, CVPR 2018, Salt Lake City,UT, USA, June 18-22, 2018, pages 616625. IEEE ComputerSociety, 2018. 2 Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, JichenYang, Nicholas Konz, and Yixin Zhang. Segment anythingmodel for medical image analysis: an experimental study.Medical Image Analysis, 89:102918, 2023. 2 Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams,Luc Van Gool, Markus H. Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodologyfor video object segmentation. In 2016 IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2016, LasVegas, NV, USA, June 27-30, 2016, pages 724732. IEEEComputer Society, 2016. 2, 5, 6, 1",
  "Robin Strudel, Ricardo Garcia, Ivan Laptev, and CordeliaSchmid. Segmenter: Transformer for semantic segmenta-tion. pages 72627272, 2021. 2": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and IlliaPolosukhin. Attention is all you need. In Advances in Neu-ral Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017, December4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017.2, 5 Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas,Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scalinglocal self-attention for parameter efficient visual backbones.In 2021 IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), 2021. 3 Jiaqi Wang, Zhengliang Liu, Lin Zhao, Zihao Wu, ChongMa, Sigang Yu, Haixing Dai, Qiushi Yang, Yiheng Liu,Songyao Zhang, et al. Review of large vision models and vi-sual prompt engineering. arXiv preprint arXiv:2307.00855,2023. 2 Wenxiao Wang, Lu Yao, Long Chen, Deng Cai, Xiaofei He,and Wei Liu. Crossformer: A versatile vision transformerbased on cross-scale attention.arXiv: Computer Visionand Pattern Recognition,arXiv: Computer Vision and Pat-tern Recognition, 2021. 3 Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,Hongsheng Li, et al. Internimage: Exploring large-scale vi-sion foundation models with deformable convolutions. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1440814419, 2023. 2 Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, ZhaoweiWang, Yanwu Xu, Yueming Jin, and Tal Arbel. Medical samadapter: Adapting segment anything model for medical im-age segmentation. arXiv preprint arXiv:2304.12620, 2023.2 Enze Xie, Wenhai Wang, Zhiding Yu, Animashree Anandku-mar, JoseM. Alvarez, and Ping Luo. Segformer: Simple andefficient design for semantic segmentation with transform-ers. Cornell University - arXiv,Cornell University - arXiv,2021. 2, 3 Ning Xu, Brian L. Price, Scott Cohen, Jimei Yang, andThomas S. Huang. Deep interactive object selection. In 2016IEEE Conference on Computer Vision and Pattern Recogni-tion, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016,pages 373381. IEEE Computer Society, 2016. 2 Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,Bin Xiao, Lu Yuan, Jianfeng Gao, MicrosoftResearchAtRedmond, Microsoft Cloud, and + Ai. Focal self-attentionfor local-global interactions in vision transformers. 3",
  "Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, XinJin, Wenjun Zeng, and Zhibo Chen.Inpaint anything:Segment anything meets image inpainting. arXiv preprintarXiv:2304.06790, 2023. 2": "Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, ChaoZhang, Xilin Chen, and Jingdong Wang. Hrformer: High-resolution vision transformer for dense predict. Advancesin Neural Information Processing Systems, 34:72817293,2021. 2 Shiyin Zhang, Jun Hao Liew, Yunchao Wei, Shikui Wei,and Yao Zhao. Interactive object segmentation with inside-outside guidance. In 2020 IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, CVPR 2020, Seattle,WA, USA, June 13-19, 2020, pages 1223112241. IEEE,2020. 2 Shen Zhuoran, Zhang Mingyuan, Zhao Haiyu, Yi Shuai, andLi Hongsheng.Efficient attention: Attention with linearcomplexities. In 2021 IEEE Winter Conference on Appli-cations of Computer Vision (WACV), 2021. 3",
  "A.2. Training Details": "In training our proposed FocSAM on COCO andLVIS , we adopt the AdamW optimizer [? ]. The initiallearning rate is set to 1e 6 for the first 1, 500 iterations,which is then raised to 1e 4. We then apply a polynomialdecay to the learning rate, setting AdamWs 1 to 0.9 and2 to 0.999. Our batch size is 4 per GPU, totaling 16 sam-ples across 4 GPUs, and images are resized and padded to1024 1024. We attempt to jointly train FocSAM with theSAM decoder, but such a strategy results in unstable train-ing. Therefore, we fine-tune the SAM decoder aloneover 320, 000 iterations at the first stage. Then, at the sec-ond stage we freeze the trained decoder and train the Foc-SAMs focus refiner for additional 160, 000 iterations.",
  "A.3. Click Simulation": "During training, we adopt the click simulation strategy fromInterFormer due to its simplicity. We set the upper limitfor simulated clicks at 20. To determine the distribution ofclick counts, we employ a decay coefficient , where theprobability for a given number of clicks decreases progres-sively. Specifically, the probability of having i clicks is times the probability of having i 1 clicks, with the con-straint that < 1. This method ensures a higher likelihoodof selecting fewer clicks, reducing computational costs. Forjoint training on COCO and LVIS datasets, Inter-Former sets at 0.6 for both. Instead, to avoid biastowards small objects in LVIS, we use different values forCOCO ( = 0.6) and LVIS ( = 0.9). This adjustment al-lows for more effective use of LVISs detailed annotationsin the later refinement stages. In FocSAM, we first decidethe number of clicks, N, and then determine the refinementstep, K, using a similar sampling strategy, where we set dis-tinct r values for COCO (r = 0.6) and LVIS (r = 0.35)with N as the upper limit to ensure a similar refinement pro-cess. After determining the N and K (only for FocSAM),SAM and FocSAM perform click simulations on trainingimages using GT as an oracle to specify clicks randomlywithin incorrectly predicted regions.",
  "B.1. Convergence Analysis": "We perform convergence analysis experiments on theSBD , DAVIS , MVTec , and COD10K datasets with sufficient samples. In these experiments, wecompute the average IoU for all samples at each click, com-paring our FocSAM with previous methods .As depicted in , the results showcase FocSAMsfast convergence across these datasets. FocSAM notablyachieves high IoU values with only a few clicks.Suchrapid convergence is particularly pronounced in the chal-lenging MVTec and COD10K datasets, where Foc-SAM outperforms other methods, including the previousstate-of-the-art SimpleClick-ViT-H . In SBD andDAVIS datasets, FocSAM demonstrates a convergencerate on par with SimpleClick-ViT-H , underscoring itsefficiency in various interactive segmentation scenarios.",
  "B.2. SAMs Bounding Box Prompt": "Experimental settings. SAM can simultaneously pro-cess click and bounding box prompts.Notably, in ourproposed FocSAM, the Dwin-MSA module conceptuallyshares similarities with the processing of bounding boxprompts.Therefore, we evaluate SAM with additionalbounding boxes around target objects for ablation studies.Specifically, we utilize the GT to find the bounding boxencompassing the target object and expand it by 1.4 toinclude the context of the surrounding area. During the in-teractive segmentation of SAM, these boxes are supplied asan additional prompt. Likewise, we report the results on . Convergence Analysis. Each subfigure displays the average IoU for all samples at successive clicks. These plots illustrate therapid convergence of FocSAM, which achieves high IoU values with only a few clicks.",
  ". Ablation study on Dwin-MSAs window sizes": "SBD , MVTec , and COD10K datasets, includ-ing the metrics 20NoC@90 and . reveals that integrating interactive infor-mation from bounding boxes offers marginal improvementto SAMs performance. This demonstrates that SAM can-not fully exploit the potential of such interactive informa-tion from the additional boxes. In contrast, FocSAM effec-tively utilizes similar information through its Dwin-MSAmodule. Specifically, FocSAM enhances the performanceby overlaying bounding boxes on previous predictions andfeeding these into the Dwin-MSA module to select win-dows relevant to the object. This approach underscores Foc-SAMs efficiency in leveraging available information forenhanced performance.",
  "GT1st Click5th Click10th Click20th Click": "SAMFocSAMSAMFocSAMSAMFocSAMSAMFocSAM IoU = 12.49IoU = 77.55IoU = 53.88IoU = 8.08 IoU = 22.35IoU = 91.22IoU = 89.75IoU = 3.59 IoU = 33.77IoU = 88.37IoU = 83.71IoU = 71.89 IoU = 4.50IoU = 88.37IoU = 79.11IoU = 5.18 IoU = 58.86IoU = 93.29IoU = 92.80IoU = 91.33 IoU = 1.42IoU = 3.00IoU = 4.38IoU = 14.29 IoU = 1.43IoU = 74.36IoU = 63.96IoU = 57.35 IoU = 77.53IoU = 95.64IoU = 91.97IoU = 79.04 . Qualitative results (2). On the left, an example is depicted with an image overlaid with its GT (blue mask). To the right, two rowsdisplay interactive segmentation results at the 1st, 5th, 10th, and 20th clicks, where the most recent click is highlighted with a star, greenfor positive and red for negative feedback. The top row illustrates the results from SAM, and the bottom row shows those from FocSAM.These visual comparisons reveal the segmentation efficiency of FocSAM and SAM at different stages of annotator clicks."
}