{
  "Abstract": "3D facial landmark localization has proven to be of par-ticular use for applications, such as face tracking, 3D facemodeling, and image-based 3D face reconstruction. In thesupervised learning case, such methods usually rely on 3Dlandmark datasets derived from 3DMM-based registrationthat often lack spatial definition alignment, as compared withthat chosen by hand-labeled human consensus, e.g., how areeyebrow landmarks defined? This creates a gap betweenlandmark datasets generated via high-quality 2D human la-bels and 3DMMs, and it ultimately limits their effectiveness.To address this issue, we introduce a novel semi-supervisedlearning approach that learns 3D landmarks by directlylifting (visible) hand-labeled 2D landmarks and ensures bet-ter definition alignment, without the need for 3D landmarkdatasets. To lift 2D landmarks to 3D, we leverage 3D-awareGANs for better multi-view consistency learning and in-the-wild multi-frame videos for robust cross-generalization.Empirical experiments demonstrate that our method not onlyachieves better definition alignment between 2D-3D land-marks but also outperforms other supervised learning 3Dlandmark localization methods on both 3DMM labeled andphotogrammetric ground truth evaluation datasets. ProjectPage:",
  ". Introduction": "3D facial landmark localization plays a critical role in vari-ous applications, such as talking head generation , 3Dface reconstruction , and learning 3D face mod-els . However, existing 3D facial landmark datasetsbased on 3D Morphable Model (3DMM) often lack align-ment with 2D landmark definitions labeled by humans. Thisleads to a noticeable ambiguity between 2D and 3D datasetsand limits their overall effectiveness, as shown in . Wepropose an algorithm to bridge this ambiguity by directly lift-ing hand-labeled 2D landmarks into 3D, without additional3D landmark localization datasets.Human-labeled 2D datasets are known to exhibit high-",
  ". Comparison of our labels with 300W-LP , DAD3D-Heads , which are both labeled via 3DMM, and MicrosoftsFace Synthetics datasets": "quality facial landmarks for visible facial regions, whileself-occluded regions are labeled in a landmark-marchedstyle , i.e., on the nearest visible boundary. On the otherhand, current 3D datasets leave much to be desired in termsof accuracy and consistency w.r.t 2D landmark definitions.For example, human-labeled 2D facial landmark datasets fo-cus on the apparent brow boundaries, whereas 3DMM-basedmodels define the brow region structurally above the eyes,as fixed mesh vertices. However, the relationship betweenfacial structure and brow appearance varies across identities,and hence, a 2D-3D inconsistency occurs, see . Inconsistencies are particularly evident in fine-scale de-tails not captured by the linear 3DMM fitting, often seenin the mouth and eyes where fine-scale details are crucialfor accurate representation, as noted by , see .Additionally, unlike 2D methods, the SoTA models trainedon such datasets tend to fail to capture blinks, as shownin . Finally, hallucinated self-occluded landmarksare prone to labeling errors due to the difficulty in labelingthe non-visible regions . We observe that the visiblesubset of 2D landmarks is fairly 3D consistent, see ,",
  ".System Pipeline: We preprocess multi-frame videos, {Imft}Tt=1, and multi-view 3D-aware GAN samples, {Imfi=": "GAN(w; ci)}| C|i=1, predicting 2D landmarks for each image. For each GAN latent w, we optimize a set of 3D landmarks to mini-mize a masked, occlusion-aware, reprojection error across views ci C, to obtain 3D pseudo-labels. Next, we train a 3D landmarkregressor on batches of 2D pseudo-labeled multi-frame samples and 3D pseudo-labeled multi-view 3D-aware GAN samples, supervisingvia a combination of 2D confidence-aware losses and 3D landmark and pose losses, masking the 2D pseudo-labels in an occlusion-awaremanner. i.e., they mimic what we refer to as 2.5D (projected from3D) landmarks. Inspired by these observations, we investi-gate whether it is feasible to lift visible 2D landmarks into3D.Thanks to recent advancements, volumetric 3D-awareGANs have enabled the generation of synthetic, yet photo-realistic, multi-view images with controllable ground-truthcamera information. Despite remarkable progress, we ob-serve that available methods are still imperfect w.r.t. multi-view appearance consistency , while improved mod-eling is on-going . In view of the present limitations, wehypothesize that we can exploit existing 3D-aware GANsas a 3D prior and 2D landmarks as 2D image constraints toreveal the 3D awareness of human faces while preservingthe 2D-3D consistency.In order to obtain 2D-3D consistent 3D landmarks, wepropose a semi-supervised approach for 3D landmark detec-tion, which leverages 1) a 3D-aware GAN prior for multi-view and multi-frame information from in-the-wild videosand 2) 2D landmark pseudo-labels1 from a SoTA 2D de-tector. Our method trains jointly on multi-frame samples,pseudo-labeled by the 2D landmark detector, and on multi-view samples, with 3D pseudo-labels obtained via lifting 2Ddetections from multiple views. Training purely on multi-view 3D-aware GAN samples would introduce a bias andlack of sufficient variation in lighting, image quality, andfacial expressions due to the limited diversity of the dataset,",
  "FFHQ and its extrapolation, while in-the-wild videoscontain such diversity": "As previously noted, 3D-aware GAN sampled data, inits current state, is still imperfect, as we observe certainfine-scale details can vary with pose, especially for featureslike eyelids and pupils, while large poses often lead to se-vere appearance degradation and background boundary arti-facts. While multi-frame samples from videos contain richdiversity, we cannot rely on these samples exclusively asthey lack the 3D constraints offered by the multi-view 3D-aware GAN samples, and only a subset of 2D landmarks are2D-3D consistent, as previously noted. Additionally, whilein-the-wild videos are biased toward frontal camera-facinghead poses , sampling from a 3D-aware GAN offersfull controllability over the 3D pose distribution, offeringmore balanced training. Thus, by combining the merits ofmulti-view and multi-frame samples we are, to the best ofour knowledge, the first to achieve this 2D-3D consistencyand thus enable 3D landmark localization consistent with2D human-defined labels. We evaluate our method on the in-the-wild DAD-3DHeads dataset and on high-quality ground-truth tempo-rally consistent 3D mesh tracking dataset, Multiface . Onboth datasets , we achieve state-of-the-art accuracywhen comparing to existing SoTA methods, despite beingtrained without a ground-truth 3D dataset. To summarize,our main contributions are as follows:",
  "We review methods for 2D-to-3D pose and keypoint estima-tion and 3D facial landmark localization. In addition, wediscuss existing facial landmark datasets": "2D-to-3D Uplifting for Pose and Landmark Estimation.Direct estimation of 3D pose and landmarks from imagesis an ill-posed problem , and ground truth 3D imageannotations are often limited . As such, methods in thiscategory often require 3D priors or depth su-pervision . On the other hand, lifting methods leverageintermediate representations, such as 2D pose or landmarkdetectors , or temporal information e.g. via graphconvolutional networks to infer 3D information.Due to the excellent performance of 2D detectors ,2D-to-3D uplifting methods normally outperform direct 3Dregression methods. Interestingly, while 2D-to-3D pose up-lifting has been investigated more extensively, almost nowork for face landmark estimation has been done ,mainly due to the wide availability of 3D face priors and recent photo-realistic synthetic datasets . Wenote that the definitional gap between 2D and 2.5D, seebottom , presents an additional challenge for upliftingfacial landmarks, as 2D labels cannot be modeled simply asprojections from 3D, as in the case of human pose estimation.Despite their impressive performance, 2D-to-3D upliftingremains an inherently ill-posed problem, even when spatio-temporal modeling is adopted, since multiple solutions areavailable, especially when occlusions occur . Recently,transformer-based methods have been introduced, which ex-ploit attention to better reason over temporally neighboring2D poses for temporal-aware lifting . While thesemethods attend to relevant temporal information for liftingto 3D, the problem of 3D facial localization lacks temporalground-truth datasets and remains unexplored. To the bestof our knowledge, no work for 2D-to-3D face landmark up-lifting has been explored with 3D transformer architectureswithout ground truth 3D datasets. 3D Facial Landmark Detection on Images.Methodsfor 3D landmark estimation can be categorized as template-based, 3DMM-based, 3D aware, and 2D-to-3D uplifting.Template-based approaches exploit the templates underly-ing mesh topology for predicting spatial deformation mapsin UV texture space or dense 3D face defor-mations . 3DMM-based methods utilize a 3D facemodel, e.g., BFM or FLAME , directly to estimatemodel parameters , often with surrogate 2D landmarksupervision , or as an intermediate representationfor 3D landmark refinement . While template-and model-based methods have demonstrated robustness for3D landmark localization, the representation power is lim-ited by the underlying 3D dense prior . 3D awaretechniques leverage volumetric representations to embed3D landmarks or generate explicit multi-view imageconstraints for 3D consistent landmark prediction . Wenote that both of these methods require 3D GAN inversionof monocular 2D images, either at inference or training,which is known to fail for large poses and occlusions .Rather than inverting images, we lift 2D landmarks into 3Dby exploiting the multi-view information of 3D-aware GANsamples, avoiding errors introduced by inversion. Unlikeprevious approaches, 2D-to-3D uplifting methods requireno geometry prior and directly regress 3D landmarks or 3D shape consistent landmarks with moving boundariesvia heatmap-based regression . Alternatively, jointcoordinate and adversarial voxel regression have been pro-posed . As far as we are aware, the use of transformer-based 3D architectures without geometric priors for 3Dsparse localization remains unexplored. Facial Landmark Datasets.A variety of 2D, 2.5D, and3D face datasets have been proposed to advance researchin facial landmark localization. 2D datasets contain groundtruth 2D landmarks annotated on real images .Here, visible landmarks are aligned to face image features,while object-occluded landmarks are hallucinated and self-occluded landmarks are snapped to image boundaries, thusdestroying overall 3D face likeness. Kumar et al. par-tially solve this problem by labeling landmarks with threevisibility categories: visible, externally-occluded, and self-occluded. However, these categories, and especially thelatter, are created based on human perception, not metrics,and thus they are error prone.2.5D datasets are eithersynthetically generated from rendered 3D meshes that at-tempt to bridge the photorealism gap or derived fromreal images by automatically fitting a 3DMM . Pure3D datasets are derived from coarse 3DMM-based render-ings or generated by densely fitting a 3DMM to realimages with human supervision . Both 2.5D and 3Dsynthetic datasets have landmarks registered to specific 3Dface mesh locations, which are not always aligned to 2D",
  "Point": ". Masked Multiview 3D Landmark Optimization: Top Right: We define a fixed set of camera views and hand-design view-dependent landmark masks based on 3D landmark visibility and 2D landmark detector competency. For a given identity sampled from the 3DGAN, we render the set of views, predict landmarks via the 2D detector, and optimize 3D landmarks to minimize the view-dependent-maskedreprojection error across all views. Top Left: Rendering of 3D-aware GAN with camera c parameterized by azimuth, elevation, and rollangles , , along a sphere, and translation t. Bottom: Illustration of 2D-3D consistency between optimized 2.5D projections of 3Dpseudo-labels, green, and 2D landmark detections, blue. facial image features, e.g. eyebrows. As 2.5D and 3D realdatasets are derived from 3DMM-based fitting, which isan ill-posed problem, perfect annotations cannot always beachieved, as shown in . While there exist smaller-scale multiview face datasets captured in controlled studiosetups with photogrammetry data ,i.e., metrically accurate 3D reconstructions, these datasetsare not applicable for in-the-wild facial landmark generaliza-tion. Our semi-supervised approach overcomes limitationsof 2D, 2.5D, and 3D datasets by leveraging the accuracy ofvisible 2D landmarks on real images and lifting them via 3Dprior supervision with our novel 3D transformer formula-tion. Thus, our method requires no large-scale annotated 3Ddatasets, which to date are non-existent and nearly impossi-ble to generate.",
  ". Method": "We introduce a semi-supervised approach for learning 3D fa-cial landmarks from a 3D-aware GAN prior and high-quality2D landmarks , without the use of 3D labels, see .Our method consists of a pre-processing stage and a trainingstage. We first pre-process our training data by predicting2D landmarks on multiview 3D-aware GAN samples andin-the-wild videos. The multiview landmarks from GANsamples are lifted to 3D via an occlusion-aware masked opti-mization to obtain 3D landmark pseudo-labels for each GANlatent. In our second phase, we train jointly on multi-viewGAN samples, supervised by ground-truth 3D pseudo-labels,",
  "and multi-frame in-the-wild videos, supervised via pose-dependently masked 2D pseudo-labels": "Pre-ProcessingWe obtain data for training our methodfrom multi-view 3D-aware GAN samples, along with multi-frame in-the-wild videos. For each video frame, Imft, wepredict N 2D landmarks, L2D RN2, using a high-quality 2D landmark detector , which was trained onthe WFLW and LaPa 2D landmark datasets, con-currently. For each GAN sampled latent code, w, we ren-der a set of views and fit 3D landmark pseudo-labels viaan occlusion-aware objective on multi-view 2D detections.In the following, we introduce the camera model of the3D-aware GAN, and describe our landmark pseudo-labeloptimization and 3D landmark localization model. Augmented Camera SpaceWe share a perspective cam-era model between the volumetric rendering of the 3D-aware GAN and projecting 3D landmarks to screen space.Typically, volumetric face GANs use a camera with extrin-sics M = ( R t0 1 ) R44 parameterized by an azimuthangle, , and elevation angle, , such that the camera is situ-ated on a sphere pointing at the look-at point. We augmentM with camera roll and t applied to R and t, respec-tively, see top-left . 3D-aware face GANs also definecamera intrinsics, K R33, with a fixed focal length, asdescribed in . Let C be the space of cameras projectionss.t. c = (K, M) C iff [A, A], [B, B],",
  "D Landmarks": ". 3D Landmark Regressor Architecture: Face images are embedded via a ViT encoder to obtain image tokens. Landmark andpose tokens are initialized from a learned embedding and passed through a 3D landmark and pose decoder, in which landmark and posetokens cross-attend to the image tokens and perform self-attention over the sequence of landmark and pose tokens. Each landmark andpose token are routed to an MLP head to predict 3D landmarks and 3D pose, respectively. Finally, the 3D landmarks are projected to 2.5Dlandmarks via the predicted 3D pose. , and t such that the bounding box of projectedfacial landmarks is contained within the image and has mini-mum dimension greater than half the image dimension. A 3Dlandmark, l3D R3, is projected from the GANs canonicalspace to screen space via the perspective projection functionl2D = (l3D; c):",
  "[l2Dx , l2Dy , w] = K (R l3D + t).(2)": "Model ArchitectureWe employ a transformer encoder-decoder model for predicting 3D landmarks, as shownin . We use a ViT encoder , known as FaRL ,pre-trained for human face perception tasks, which we showyields slightly better performance than Resnet152 . Wedesign a transformer decoder with a token per landmarkand pose tokens for rotation, Txy and Tz. These tokenspass through three blocks, each containing an image-cross-attention layer, landmark-pose self-attention layer, and MLP,with layer-normalization prior to each. Finally, we passthe landmark and pose tokens individually through MLPheads, which predict the 3D landmarks, Cholesky factoriza-tion of the 2D covariances of projected 2.5D landmarks,and the 3D rotation and translation.We apply the 3Dlandmark predictions as offsets to a template, defined asthe landmark-wise mean of our 3D pseudo-labels obtainedduring pre-processing, to obtain 3D landmark predictions,L3D RN3. The pose is predicted via a 6D rotation rep-resentation, akin to from which a rotation matrix, R, isextracted. From R, we compute the 3D translation to the cam-era sphere, tR, and predict t to obtain t = tR + t. Finally,we form our predicted camera, c = [K, M], with fixed intrin-sics, K, and obtain 2.5D landmarks L2.5D = (L3D; c). Training MethodologyWe train our 3D landmark de-tector jointly on multi-view GAN sampled images, andmulti-frame in-the-wild video frames. For multi-view image,Imv = GAN(w; c), we sample a latent code w from a setof pre-processed latents, along with a random camera c C.For each multi-frame sample, we sample a random videofrom our pre-processed video dataset followed by a randomframe Imf from the video. Each batch of training consists of4 multi-view samples, (Imv, L3D, c), and 4 multi-framesamples, (Imft, L2D). We formulate our loss function asa combination of multi-frame and multi-view losses. Themulti-view loss consists of a 2.5D uncertainty-aware land-mark loss, 3D landmark loss, and 3D pose loss. We employ aLaplacian Log Likelihood (LLL) objective parametrized bypredicted Cholesky factorization of landmark covariances,akin to . Such parameterization enables the energylandscape to adapt to noise caused by rendering artifactsand allows the model to weigh the loss for each landmarkprediction based on its 2D anisotropic confidences. For 3Dlandmark loss, along with the translation loss, on t, weadopt mean-squared-error, while for 3D rotation, we followhead pose estimation work and use geodesic loss. Thus,our multi-view loss is defined as:",
  "Lmv = LMSE_L3D + LMSE_t + LLLL_L2.5D + LGeo_R, (3)": "Since the set of views encountered when training on in-the-wild videos is not fixed, such as in the 3D pseudo-labelingoptimization, we employ a simple heuristic for obtainingmasks, m {0, 1}N. We define a template of normal vec-tors for each landmark, apply the estimated rotation to eachnormal, and threshold the dot product with the forward vec-tor to obtain the mask. Thus, we supervise the multi-frame",
  "i=1mn Llll(l2Dn, l2.5Dn; n),(4)": "where n refers to the covariance matrix obtained via pre-dicted Cholesky factorization, and Llll denotes the 2Dlaplacian-log-likelihood. Refer to for details. For videotraining, this anisotropic confidence weighted loss enablesthe energy landscape to adapt to systematic noise from the2D detector, such as extreme pose samples where the 2Ddetector may fail, while GAN-based extreme pose samplesare constrained via fixed 3D losses. Thus, our completeobjective is: L = Lmf + Lmv.",
  ". Results & Analysis": "Training Implementation DetailsWe train our methodjointly on in-the-wild videos obtained from the CelebV-HQ dataset, selecting the first 10K videos, along withGAN samples obtained from IDE-3D , sampling la-tents from the first 10K random seeds. Our pre-processingstage takes roughly 3 days on a single A10G GPU with24GB RAM to obtain pseudo-labels. Since geometric aug-mentations (e.g., scale and translation) would break our3D ground truth under perspective projection, we renderGAN-generated images on the fly during training, sam-pling cameras uniformly from augmented camera space, C.To obtain a sensible pose distribution, we softly decreaseextreme rotation angle combinations by accepting sam-pled rotations with probability e((",
  "B )2+(": ")2), withA = 110, B = 60, = 90. Our IDE-3D renders and videoframe crops are 224x224, to match the input dimension ofour FaRL backbone. We train with a learning rate of1e-5 for 225 epochs, with the Adam optimizer, decay-ing the learning rate exponentially by a factor of 0.9 every3 epochs, taking roughly 4 days on a single GPU machine.We overcome an IDE-3D artifact, where a large pose causesthe background to occlude the face, by exploiting IDE-3Dssemantic field to set the density of background points in thenear half of the viewing frustum to , prior to rendering.We ignore GAN-rendered pupil landmarks during training,as we observe a bias where pupils tend to follow the camera,breaking multi-view consistency. Normalized Mean Local Consistency MetricPreviousworks train and evaluate NME on datasetswhere selected indices of face mesh vertices define the land-marks. Across datasets, we observe that landmark definitionsare globally aligned, i.e., same general semantic position,but suffer local definition bias, see , due to differencesin vertex selections and mesh topology, e.g., 300WLP uses BFM and DAD-3DHeads uses FLAME ,while ours does not use a mesh. We report cross-dataset",
  ". Global alignment of landmarks with local definition bias": "evaluations in the supplementary document, showing thattraditional NME leads to unfair comparisons due to the lo-cal definition bias while still capturing a ballpark notion ofglobal alignment. As such, we need a landmark definitionagnostic metric for meaningful local comparison, which weintroduce as an extension of standard NME. We first defineNME as parametrized by the vertex indices. For a test setof M images, with predicted landmarks L2.5Dm RN2,projected vertex labels, V 2.5Dm R|V |2, vertex indices,K {1, .., |V |}N:",
  "NMLC = minK M(L, V ; K),(6)": "enabling fair cross-dataset comparison. Unlike NME, con-sistent local bias w.r.t. a desired landmark definition, K, willnot be penalized. Trivially, NMLC NME. Non-trivialityof NMLC is ensured by a large test set with pose, identity,and expression variations. Our NMLC comparisons cor-relate with qualitative results, see , as our methodsleading performance appears to be reflected. ComparisonsWe evaluate our method on the studio-captured photogrammetric ground-truth Multiface dataset, along with 3DMM-labeled in-the-wild images fromDAD-3DHeads . As the Multiface dataset is extremelylarge (65TB), we select 6 sequences, which cover a rangeof facial expressions, including asymmetric facial deforma-tions. See supplemental document for curation details. TheDAD3D-Heads training set offers category labels for pose,expression, occlusion, quality, lighting, gender, and age. So,we chose the above for our evaluations to obtain fine-grainedanalysis (quality, lighting, gender, and age reported in sup-plementary). Since our detector outputs 98 landmarks, wegenerate the corresponding 68 landmark subset to compare",
  ". SoTA evaluation (top) and ablations (bottom) on Multi-face . We report the NMLC for each model, when averagingacross various facial regions": "tures more fine-scale details in the eye, mouth, and browregions, and it can properly handle blinks while other meth-ods fail. We remark, however, that our method still fails forextreme expressions such as puckers and asymmetric de-formations. We hypothesize that improving the 2D detectorfor such cases will propagate through our pipeline towardimproving 3D results, as suggested by observations in .Note that the model used for 2D pseudo-labels fails similarlyfor mouth deformations. Ablation StudiesWe conduct ablation studies to observethe effects of the two data sources independently, our choiceof encoder backbone, and the impact of sample size. Wetrain our method with multi-view GAN samples and multi-frame video samples independently, referred to as Ours(MV only) and Ours (MF only), respectively, to observe thestrengths and weaknesses of each in isolation. We also trainour method with a standard Resnet152 backbone, pre-trained on ImageNet , of similar parameter count (60M)to our FaRL backbone (87M), and refer to this model as Ours (Resnet152). Additionally, we include Ours (Resnet50)to compare with the same backbone used by . Finally,we decrease the number of videos/GAN latents from 10k to1k and 100 samples, referred to as Ours (1K) and Ours (100).Tab. 1 and Tab. 2 show that, when training without multi-view samples (MF only), the model failures for the Multifacedataset are quite pronounced for the contours, as the methodstruggles to capture large pose variation without the 3D con-straints of the multi-view training, which is accentuated forcontour landmarks. When training without multi-frame sam-ples (MV only), we observe a sizable performance decreasefor both mouth and occlusions, as its training distribution islimited by the FFHQ-trained GAN. Replacing our ViT back-bone with a Resnet152 of similar parameter count yielded aslight drop in performance. Interestingly, occlusions yielda more significant drop. We hypothesize that it is a resultof the ViTs global reasoning capacity, ability to selectivelyignore occluded regions, and the backbone encoders (FaRL)face image embedding prior. We observe a trend that perfor-mance improves with the number of training pseudo-labelswe generate.",
  ". Conclusion": "In this paper, we have introduced a semi-supervised methodfor geometric prior-free localization with accurate 3D faciallandmarks, aligned with 2D human labels, by exploitingmulti-view 3D-aware GANs and using 2D landmarks withno ground-truth 3D dataset. We have shown, for the firsttime, that SoTA 3D landmarks can be learned without 3D la-bels, paving the way toward improving 3D facial landmarksbeyond the limitations of current 3D labeling techniques. Limitations & Future DirectionsDespite the promisingresults demonstrated by our 3D facial landmark localizationmethod, there are still some limitations. We are heavily de-pendent on the quality of both a 2D landmark detector anda 3D-aware facial GAN. Improvements to either of thesedependencies should result in improvements when trainingwith our approach. Currently, we observe limitations in the2D landmark detector for facial expressions, such as puckersand asymmetric deformations, constraining the performanceof 3D uplifting. However, correcting this may be as simpleas labeling such examples when training the 2D detector.As has been noted by previous approaches , 3D-awareGANs have limited pose and expression distributions, limit-ing their downstream application for multi-view consistency.Noting the failure case observed where the method fails dueto an occlusion , future work may include investigatingGAN sample augmentation via volumetrically generated oc-clusions. Finally, as we observe a strong trend in increasingperformance improvement with the number of pseudo-labelsgenerated, future work may explore the asymptotic limits ofsuch improvement.",
  "Andrew D. Bagdanov, Alberto Del Bimbo, and Iacopo Masi.The florence 2d/3d hybrid face dataset. In ACM HGBU, pages7980. ACM, 2011. 4": "Chandrasekhar Bhagavatula, Chenchen Zhu, Khoa Luu, andMarios Savvides. Faster than real-time facial alignment: A 3dspatial transformer network approach in unconstrained poses.In ICCV, pages 39803989. IEEE CS, 2017. 3 Adrian Bulat and Georgios Tzimiropoulos. How far are wefrom solving the 2d & 3d face alignment problem?(and adataset of 230,000 3d facial landmarks). In ICCV, pages10211030. IEEE CS, 2017. 3, 6, 7, 8, 2, 4",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiFei-Fei. Imagenet: A large-scale hierarchical image database.In CVPR, pages 248255. IEEE CS, 2009. 8": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In ICLR. OpenReview.net, 2021. 5 Bernhard Egger, William A. P. Smith, Ayush Tewari, StefanieWuhrer, Michael Zollhfer, Thabo Beeler, Florian Bernard,Timo Bolkart, Adam Kortylewski, Sami Romdhani, ChristianTheobalt, Volker Blanz, and Thomas Vetter. 3d morphableface models - past, present, and future. ACM TOG, 39(5):157:1157:38, 2020. 3",
  "Diederik P. Kingma and Jimmy Ba. Adam: A method forstochastic optimization. In ICLR, 2015. 6": "Martin Kstinger, Paul Wohlhart, Peter M. Roth, and HorstBischof. Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. InICCVW, pages 21442151. IEEE CS, 2011. 3 Abhinav Kumar, Tim K Marks, Wenxuan Mou, Ye Wang,Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xi-aoming Liu, and Chen Feng. Luvli face alignment: Estimatinglandmarks location, uncertainty, and visibility likelihood. InCVPR, pages 82338243. CVF / IEEE, 2020. 3, 5, 6",
  "Jingtan Piao, Chen Qian, and Hongsheng Li. Semi-supervisedmonocular 3d face reconstruction with end-to-end shape-preserved domain transfer. In ICCV, pages 93979406. IEEE,2019. 3": "Mallikarjun B. R., Ayush Tewari, Hans-Peter Seidel, Mo-hamed Elgharib, and Christian Theobalt. Learning complete3d morphable face models from images and videos. In CVPR,pages 33613371. CVF / IEEE, 2021. 3 Zeyu Ruan, Changqing Zou, Longhai Wu, Gangshan Wu, andLimin Wang. Sadrnet: Self-aligned dual face regression net-works for robust 3d dense face alignment and reconstruction.IEEE TIP, 30:57935806, 2021. 3",
  "Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shotfree-view neural talking-head synthesis for video conferenc-ing. In CVPR, pages 1003910049. CVF / IEEE, 2021. 1": "Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, SebastianDziadzio, Thomas J. Cashman, and Jamie Shotton. Fake ittill you make it: face analysis in the wild using synthetic dataalone. In ICCV, pages 36613671. IEEE, 2021. 1, 3 Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, MatthewJohnson, Jingjing Shen, Nikola Milosavljevic, Daniel Wilde,Stephan J. Garbin, Toby Sharp, Ivan Stojiljkovic, Tom Cash-man, and Julien P. C. Valentin. 3d face reconstruction withdense landmarks. In ECCV, pages 160177. Springer, 2022.3",
  "Yiqian Wu, Jing Zhang, Hongbo Fu, and Xiaogang Jin. Lpff:A portrait dataset for face generators across large poses. ArXiv,abs/2303.14407, 2023. 8": "Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, RohanBali, Danielle Belko, Eric Brockmeyer, Lucas Evans, Timo-thy Godisart, Hyowon Ha, Alexander Hypes, Taylor Koska,Steven Krenn, Stephen Lombardi, Xiaomin Luo, KevynMcPhail, Laura Millerschoen, Michal Perdoch, Mark Pitts,Alexander Richard, Jason M. Saragih, Junko Saragih, TakaakiShiratori, Tomas Simon, Matt Stewart, Autumn Trimble, Xin-shuo Weng, David Whitewolf, Chenglei Wu, Shoou-I Yu, andYaser Sheikh. Multiface: A dataset for neural face rendering.CoRR, abs/2207.11243, 2022. 2, 3, 4, 6, 7, 8, 1 Shengtao Xiao, Jiashi Feng, Luoqi Liu, Xuecheng Nie, WeiWang, Shuicheng Yan, and Ashraf A. Kassim. Recurrent3d-2d dual learning for large-pose facial landmark detection.In ICCV, pages 16421651. IEEE CS, 2017. 3",
  "Hao Zhang, Tianyuan Dai, Yu-Wing Tai, and Chi-Keung Tang.Flnerf: 3d facial landmarks estimation in neural radiancefields. CoRR, abs/2211.11202, 2022. 3": "Xing Zhang, Lijun Yin, Jeffrey F. Cohn, Shaun J. Canavan,Michael Reale, Andy Horowitz, Peng Liu, and Jeffrey M.Girard. Bp4d-spontaneous: a high-resolution spontaneous 3ddynamic facial expression database. Image Vis. Comput., 32(10):692706, 2014. 4 Zheng Zhang, Jeffrey M. Girard, Yue Wu, Xing Zhang, PengLiu, Umur A. Ciftci, Shaun J. Canavan, Michael Reale, An-drew Horowitz, Huiyuan Yang, Jeffrey F. Cohn, Qiang Ji,and Lijun Yin. Multimodal spontaneous emotion corpus forhuman behavior analysis. In CVPR, pages 34383446. IEEECS, 2016. 4",
  "Mingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen.Imface: A nonlinear 3d morphable face model with implicitneural representations. In CVPR, pages 2031120320. IEEE,2022. 1": "Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dong-dong Chen, Yangyu Huang, Lu Yuan, Dong Chen, MingZeng, and Fang Wen. General facial representation learningin a visual-linguistic manner. In CVPR, pages 1867618688.IEEE, 2022. 5, 6, 8 Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq: Alarge-scale video facial attributes dataset. In ECCV, pages650667. Springer, 2022. 2, 6, 1, 5",
  "A. Implementation Details": "3D Landmark Transformer Architecture presentsa more detailed figure of our 3D landmark transformer ar-chitecture. 3D head pose and facial landmarks are estimatedvia cross-attention and self-attention heads and MLP layers. Loss Implementation DetailsIn order to computeocclusion-aware masks, m {0, 1}N, used in Eq. 4, weapply the predicted rotation matrix to a template of normalvectors for each landmark, and threshold the dot productwith the forward vector to obtain the mask. We obtain ournormal template by selecting the landmarks on a face mesh,and computing the normals at those locations. We set thethreshold so that products above 0.5 were considered visible,while lowering this threshold to 0.1 for the nose bridge.We found this conservative masking strategy reasonable inour experiments. Multi-view Camera OptimizationIn obtaining 3Dpseudo-labels for 3D-aware GAN-generated samples, weperform a multi-view 3D landmark optimization over detec-tions from renders of camera views, ci C, represented by(, ) azimuth and elevation pairs. illustrates all these| C| = 41 sample views.",
  "B. Evaluation Set Preparation": "When comparing our model on the DAD3D-Heads dataset, we upsample the meshes to ensure that the meshis dense enough that the distance between vertices is muchsmaller than the models inconsistencies.Due to the enormous size of the Multiface dataset, wesample a subset for our evaluations. We selected 6 sequences:Neutral Eyes Open, Relaxed Mouth Open, Open Lips MouthStretch Nose Wrinkled, Mouth Nose Left, Mouth Open JawRight Show Teeth, Suck Cheeks In, which include closed eyes,wide mouth openings, and asymmetric facial deformations.The data covers a wide range of cameras, and we discardseveral in which the face is not visible, including camerasnumbered 400055, 400010, 400067, 400025, 400008, and400070. To eliminate redundancy in the evaluation set, wesample every 15 frames from the downloaded sequences.",
  "Evaluations on Additional DAD3D-Heads CategoriesIn Tab. 3, we report the DAD3D-Heads evaluationresults for additional categories, including image quality,lighting, gender, and age": "Loss Function AblationsWe compare the loss functionused by our method, Laplacian Log Likelihood, with othercommon loss functions, L1 and MSE, in Tab. 4. Our choiceyields the best results on our benchmark datasets. Cross-Dataset EvaluationsOur investigations into cross-dataset evaluations reveal a notable limitation in model gen-eralizability between datasets with differing labeling con-ventions. We report cross-dataset evaluations in Tab. 5 onboth AFLW2000-3D and the DAD3D-Heads vali-dation set, comparing our method with methods trained onDAD3D-Heads and 300WLP , noting that 300WLPscompatible evaluation set is the AFLW2000-3D dataset. Weobserve that despite a global alignment in how landmarksare defined, cross-dataset scores of every SoTA model are allworse than the SoTA models of the compatible dataset. Thisis expected due to the local definition bias w.r.t. a differentdatasets landmark definition, which yields a consistent error.For each dataset, our model achieves the best cross-datasetscore. The cross-dataset metrics do not disentangle the localdefinition bias from some notion of actual error with respectto the models landmark definition. Intuitively, if our modelslandmark definition were the midway interpolation betweenthe two dataset definitions, our model would incur half of theerror from local definition bias than that of the other models.Hence, for fair comparisons, we compare against other meth-ods using our proposed NMLC metric, which removes thelocal definition bias from the evaluated error. Nevertheless,cross-dataset evaluation remains a useful proxy for assessing",
  "OursFaceLift3.512.78": ". Cross-dataset evaluation of NME on AFLW2000-3D-reannotated and the validation set of DAD3D-Heads . denotes thatthe score is cross-dataset, meaning the training set definition is not compatible with the evaluation dataset and definition. We see that whileour model is the best on the cross-dataset comparisons for each dataset, compatible SoTA models yield better scores since they do not incurthe local definition bias of cross-dataset evaluation. . Additional Qualitative Results on CelebV-HQ dataset. Here, the blue, green, and red axes represent Cartesian coordinatesand denote the forward, up, and right vectors, respectively. Our approach can faithfully reconstruct 3D landmarks under challenging 3Dhead poses and harsh lighting."
}