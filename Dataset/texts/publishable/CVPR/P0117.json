{
  "Abstract": "Recently, multimodal large language models have madesignificant advancements in video understanding tasks.However, their ability to understand unprocessed longvideos is very limited, primarily due to the difficulty in sup-porting the enormous memory overhead. Although exist-ing methods achieve a balance between memory and in-formation by aggregating frames, they inevitably introducethe severe hallucination issue. To address this issue, thispaper constructs a comprehensive hallucination mitigationpipeline based on existing MLLMs.Specifically, we usethe CLIP Score to guide the frame sampling process withquestions, selecting key frames relevant to the question.Then, We inject question information into the queries ofthe image Q-former to obtain more important visual fea-tures. Finally, during the answer generation stage, we uti-lize chain-of-thought and in-context learning techniques toexplicitly control the generation of answers.It is worthmentioning that for the breakpoint mode, we found thatimage understanding models achieved better results thanvideo understanding models. Therefore, we aggregated theanswers from both types of models using a comparisonmechanism.Ultimately, We achieved 84.2% and 62.9%for the global and breakpoint modes respectively on theMovieChat dataset, surpassing the official baseline modelby 29.1% and 24.1%. Moreover the proposed method wonthe third place in the CVPR LOVEU 2024 Long-Term VideoQuestion Answering Challenge. The code is avaiable at",
  "*Equal contribution.Corresponding author": "els struggle to achieve perfect processing. The core ideaof current research is to retain more important informationin the features within the memory overhead supported byexisting models. MovieChat and ChatUnivi haveachieved the aggregation of information from numerousframes through weighted fusion on similar frames and dpc-knn techniques respectively. This purely aggregative ap-proach often leads to severe hallucination issues, especiallyfor the untrained ones. Specifically, these hallucination is-sues can be divided into incorrect references and fabrica-tion.The former refers to answers that include contentfrom the video that is irrelevant to the question, while thelatter refers to answers that contain content not present inthe video. They severely limit the models understanding.Therefore, we believe mitigating hallucination is a crucialstep in advancing current long video understanding tasks.We propose a comprehensive hallucination mitigationpipeline on the basis of TimeChat . To address the issueof incorrect references, we guide the information retrievalusing the question. First, during the sampling stage, we cal-culate the CLIP Score for each frame with respect tothe question and sample the top K frames with the highestscores. Then, in the feature extraction stage, following thedesign of InstructBLIP , we inject question informationinto the queries to guide the extraction of visual features.To address the fabrication issue, we designed a two-stagethinking process where the model first describes the imageand then answers the question based on the description. Ad-ditionally, we designed an automated context example re-trieval scheme to further control the answer generation.Since some question-answer pairs in breakpoint mode donot rely on temporal information, we found that image un-derstanding models perform better on these questions com-pared to video understanding models. In this paper, we usedInternVL to provide candidate answers. Then, we calcu-lated and compared the relevance of the candidate answersto the images using CLIP. Finally, we selected the answerwith the highest similarity as the final answer.The main contributions are summarized as follows:",
  "User Instruction[ROUND 1] CoT. Denote the answer of the the following question is {COT_ANSWER}.Please describe the video in less than 20 words": "[ROUND 2] ICL. {ICL_QUESTION} and {ICL_ANSWER} are the question-answer pair sampled from the trainset.You should follow the format of the example:```Here is the question: {ICL_QUESTION}. Answer the question in less than 20 words: {ICL_ANSWER}```Here is the description: ```{COT_ANSWER}```. Here is the question: ```{QUESTION}```. Please answer the question according to the video and description in less than 20 words.",
  ". Methodology": "We compare several baselines and finally choose the pre-trained TimeChat model as our baseline. To better miti-gate the hallucination issue, we introduce both training andinference technicals to get better results. As shown in Fig-ure 1, for the training process, we handle the instruction-relevant visual content based on the given question in ad-vance instead of leaving it all to the language model, Forthe inference process, we utilize the Chain-of-Thought andIn-Context Learning to get better results to further alleviatethe hallucination issue. Moreover, we introduce an imageunderstanding model for the breakpoint mode and mix theanswers together to enhance the performance.",
  ". Baseline": "We choose TimeChat as our baseline, as it uses a slidingVideo Q-Former which is better for long video understand-ing. It uses ViT-G/14 from EVA-CLIP as the frozen im-age encoder, and LLaMA-2 (7B) as the language foun-dation model. We fine-tune both the Image Q-Former andVideo Q-Former on the MovieChat-1K train dataset while",
  ". Training Process": "We notice that the existing video LLMs usually suffer fromthe hallucination issue when inferring in the MovieChat-1K test dataset. We make the visual content related tothe instruction in advance to mitigate the issue.CLIP-based Frame Sampling.Existing methods oftenemploy uniform sampling to extract the frames from onevideo. Since the video is too long, uniform sampling maylose some critical frames that are relevant to the given ques-tion. To address this problem, referring to the VaQuitA ,we select the frames based on the CLIP similarity scores.Given the input video of L frames in total, we select theframes based on the similarities between the frame fea-tures and the given question. Suppose we need to sampleT frames for the video LLM, we first select T/2 framesuniformly to capture the global semantic information, andthen choose another T/2 frames based on the CLIP scoresto get the most relevant frames based on the given ques-tion. Specifically, we extract the question text using theCLIP Text Encoder, denoted as fq, and extract each frameof the video using the CLIP Image Encoder as f iframe, wherei {1, 2, ..., T/2}, and the similarity can be calculated as:",
  "We choose the indices of the top T/2 similarities as the restof the frames we select. Finally, we sort these frames in as-": "cending order to get the frame features from a long video.Frames that are most related to the question will be selectedby utilizing the CLIP-Score approach, improving represen-tation learning ability.Question-guided Frame Feature Extractor.To furthermake the visual content more related to the question, wehandle the visual information with instruction understand-ing for the input of the Image Q-Former. Specifically, in-spired by the InstructBLIP and TimeChat , we add thequestion as the condition to fuse the visual and instructioninformation, concatenating the condition with the learnablequeries. Since they are learned by the same self-attention,the model can extract the task-relevant visual contents, thusmaking the LLM understand the visual token more easily.",
  ". Inference Process": "Apart from the training process, we also utilize two straight-forward strategies for the inference process, which areChain-of-Thought (CoT) and In-Context Learning (ICL).Both of them can help to mitigate the hallucination issueand obtain better performance.Chain-of-Thought. We directly use CoT technology forour inference. Specifically, we first construct a question tolet the model describe the video. Then in the second round,we use the history, which is the generated description as theinput prompt, hindering the model to complete the question.In-Context Learning. ICL is another effective way to en-hance the generalization ability of the model with the helpof the given examples. Specifically, we calculate the sim-ilarity between the given question and the questions in thetraining dataset, and then select the question-answer pairwith high similarity as the ICL example. With the help ofthe example as the prompt, the model can learn how to an-swer the given question better.The final input of the prompt can refer to . Theglobal mode and breakpoint mode share the same trainingand inference scheme.",
  ". Comparison Strategy": "For the breakpoint mode, we think the current frame needsmore attention than the current video clip and the wholevideo.Therefore, we introduce the InternVL to un-derstand the current frame and mix the results with ourtrained model. Specifically, we take the current frame ofthe breakpoint mode as the visual input, and the questionas the text input, getting answers with both CoT and ICLlearning. Once obtain the answer from InternVL, we mixit with our trained TimeChat by calculating the similaritybetween the answers and the current video clip. Supposethe answer of InternVL is A1, the trained TimeChat is A2for each question, and the video clip is V , we use CLIP Im-age Encoder and Text Encoder to calculate their similarities:simi = sim(CLIPT (Ai), CLIPI(V )), where i=1,2. Then",
  ".Effect of each component.Results are reported onMovieChat-test with the model MovieChat. CFS means CLIP-based Frame Sampling": "The process of designing the strategy and selecting themodel was conducted simultaneously. Therefore, we per-formed ablation experiments on some modules using theMovieChat model. The results are shown in . CoTsignificantly improved performance in both the global andbreakpoint modes. This demonstrates that such an explicitchain-of-thought process helps the model better understandthe video. CLIP-based frame sampling improves the perfor-mance only in the global mode. This demonstrates that thiskey frame sampling strategy helps the model provide moreaccurate answers to questions.However, for the break-point mode, the impact of key frame sampling is minimalbecause we only provide segments near the current framewhich contain few information. Finally, the ICL techniquefurther improved performance in both the global and break-point modes, primarily due to the constraints on the answerformat. shows the effectiveness of the question-guidedframe feature extractor. After incorporating this method,there was an improvement in overall performance, and theaddition of other methods further enhanced the models ef-fectiveness.",
  "Comparison Strategy": "shows the performance of the image understand-ing model and the video understanding model. We foundthat the image understanding model performs better in thebreakpoint mode, indicating that some questions have alower dependency on temporal information. To leveragethe strengths of both the image understanding model andthe video understanding model, we adopted a comparisonstrategy, which achieved a performance of 62.9%.",
  ". Multiple Results": "Considering the variability of evaluation methods based onlarge language models, we provide results from three exper-iments to verify the effectiveness of our approach. The re-sults are shown in . By averaging the multiple resultsof the global mode and the breakpoint mode separately, weobtain the global accuracy 84.0% and the breakpoint accu-racy 65.1%.",
  ". Conclusion": "We propose a hallucination mitigation pipeline for address-ing hallucinations in video understanding models for longvideo understanding tasks. The pipeline comprises threeparts: CLIP-based frame sampling, question-guided framefeature extractor, and CoT&ICL-based generation control.Our ablation experiments demonstrate that these methodsare effective on MovieChat. Additionally, we introduceda CLIP-based comparison strategy to combine the advan-tages of video understanding models and image understand-ing models in the breakpoint mode. The results show thatthis method is effective. Overall, our approach is effective,but there is still ample room for improvement. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,Lewei Lu, et al. Internvl: Scaling up vision foundation mod-els and aligning for generic visual-linguistic tasks. In CVPR,pages 2418524198, 2024. 1, 3 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale NFung, and Steven Hoi. Instructblip: Towards general-purposevision-language models with instruction tuning. NeurIPS, 36,2023. 1, 3 Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao,and Li Yuan. Chat-univi: Unified visual representation em-powers large language models with image and video under-standing. In CVPR, pages 1370013710, 2024. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervision.In ICML, pages 87488763. PMLR, 2021. 1"
}