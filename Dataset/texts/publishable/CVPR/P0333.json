{
  "Abstract": "In Re-identification (ReID), recent advancements yieldnoteworthy progress in both unimodal and cross-modal re-trieval tasks. However, the challenge persists in develop-ing a unified framework that could effectively handle vary-ing multimodal data, including RGB, infrared, sketches,and textual information.Additionally, the emergence oflarge-scale models shows promising performance in vari-ous vision tasks but the foundation model in ReID is stillblank. In response to these challenges, a novel multimodallearning paradigm for ReID is introduced, referred to asAll-in-One (AIO), which harnesses a frozen pre-trained bigmodel as an encoder, enabling effective multimodal re-trieval without additional fine-tuning. The diverse multi-modal data in AIO are seamlessly tokenized into a uni-fied space, allowing the modality-shared frozen encoder toextract identity-consistent features comprehensively acrossall modalities. Furthermore, a meticulously crafted ensem-ble of cross-modality heads is designed to guide the learn-ing trajectory. AIO is the first framework to perform all-in-one ReID, encompassing four commonly used modali-ties.Experiments on cross-modal and multimodal ReIDreveal that AIO not only adeptly handles various modaldata but also excels in challenging contexts, showcasingexceptional performance in zero-shot and domain gener-alization scenarios. Code will be available at:",
  ". Introduction": "Person Re-identification (ReID) aims to retrieve a targetperson captured by multiple non-overlapping cameras .It is widely used in intelligent surveillance, security, andmany other fields. ReID has been deeply studied in recentyears and achieves human-level performance in both uni-modal and cross-modal retrieval tasks .Existing works are capable of retrieving between RGB",
  "Unable to Generalize to Other Modalities": ". Illustration of the proposed AIO and existing meth-ods. (a) Existing ReID methods independently learnthe cross-modal ReID models, incapable of handling the uncertaininput modalities in real-world scenarios. (b) Our proposed AIOframework exhibits the capability to proficiently manage diversecombinations of input modalities, thus addressing the inherent un-certainties prevalent in practical deployment scenarios. images or leveraging different modalities of the query (e.g.infrared (IR), sketch, or text) to find the person in RGB im-ages . However, RGB images are sus-ceptible to environmental light fluctuations, while IR andsketch images lack vital color information crucial for ReIDtasks. The adage a picture is worth a thousand words un-derscores the ease of accessing textual information, thoughit falls short in providing intricate visual details . More-over, as shown in (a) and Tab. 1, existing cross-modal",
  ". Comparison of AIO and existing methods on cross-/multi-modality retrieval. The number in () in the Multi col-umn indicates the number of support modalities at inference time": "methods are confined to specific paired modalities and mod-els during training, rendering them unable to handle diverseinput modalities effectively. Consequently, the generaliz-ability of these methods to unseen modalities is severelyhampered, a significant hurdle given the uncertainty of themodality in real-world query scenarios. This lack of adapt-ability across modalities severely constrains the practicalapplicability of existing methods in a practical real-worlddeployment with uncertain testing environments. Thus, 1)How to improve the generalizability of modality is a signif-icant challenge?Meanwhile, in real-world scenarios, individuals of inter-est frequently encounter unknown environments that are notlearned during training, i.e., zero-shot ReID in the wild.Existing methods explore domain generalizability basedon a single modality, which fails to handle multi-modalzero-shot retrieval. Recently, foundation large models haveshown their power in diverse language and vision tasks. Pi-oneering models such as CLIP and CoCa , exem-plify the prowess of large-scale pre-trained foundation mod-els as robust zero-shot performers. This characteristic holdssignificant relevance for ReID tasks. Despite the existenceof several large-scale ReID pre-trained models ,their zero-shot performance falls short of expectations. Typ-ical down-stream fine-tuning or training strategies wouldbe too resource-demanding in a new challenging scenarios,e.g., data collection and annotation. Moreover, the cost oftraining a large-scale foundation model is too high to af-ford for most researchers and small companies. Thus, 2)Is there a straightforward method to utilize extensive pre-trained foundational models for improving zero-shot per-formance in ReID with uncertain modalities?To address the aforementioned issues, we introduce aninnovative All-in-One (AIO) framework to tackle the chal-lenges inherent in zero-shot multimodal ReID. As shown in(b), the key idea of our work is to explore the poten-tial of leveraging transformer-based foundation models to address uncertain multimodal retrieval, enhancing zero-shotability in multimodal ReID, e.g. any combination of RGB,IR, sketch, text, or simple cross-modal retrieval. AIO repre-sents an experimental effort, being the first framework ca-pable of simultaneously accommodating all four commonlyused modalities in different ReID tasks concurrently.In order to achieve the above goal, AIO firstly designsa lightweight multimodal tokenizer to unify diverse data.It is followed by a frozen foundation model that servesas a shared feature encoder, extracting a generalized se-mantic representation across all modalities and improvingzero-shot performance.Then, to guide cross-modal andmultimodal feature learning, AIO proposes several cross-modal heads which contain: a) A Conventional Classifica-tion Head is utilized as the foundation of the learning guid-ance, learning identity-invariant representations; b) VisionGuided Masked Attribute Modeling is introduced to learnfine-grained features and build a relationship between textand images; c) Multimodal Feature Binding is utilized toclose features of diverse modalities together.Furthermore, the acquisition of multimodal data in real-world scenarios poses considerable challenges, particularlyconcerning IR and Sketch images. The shortage of IR cam-eras and the substantial human labor involved in sketchdrawing contribute to this difficulty. Existing multimodallearning methods demand paired multimodal, apre-requisite not consistently met in realistic environments.In addressing the absence of certain modalities in multi-modal learning, except the proposed Multimodal FeatureBinding, integrates synthetic augmentation strategies, CA and Lineart , to generate synthetic IR and Sketchimages respectively. CA and Lineart have been shown toeffectively diminish the domain gap between RGB-IR andRGB-Sketch modalities. Their utility extends to acting asa bridge that connects feature representations of the sametarget across diverse modalities, thereby facilitating the re-duction of the modality gap .Comprehensive experiments across various zero-shotcross-modal and multimodal ReID scenarios, involving allfour modalities, are conducted to evaluate the performanceof the proposed framework.We also explore differentfoundation models and multimodal input data combinationsto assess the versatility of AIO. The proposed frameworkdemonstrates remarkable performance on multimodal ReIDtasks and competitive performance on cross-modal ReIDtasks without additional fine-tuning, highlighting its poten-tial as a robust zero-shot solution for complex multimodalReID tasks. In summary, our contributions are three-fold: We identify a critical limitation in existing cross-modalReID that they lack generalizability to novel modalities,coupled with poor zero-shot performance.",
  ". Cross-modal ReID": "Person Re-identification (ReID) can be classified intosingle-modal ReID and cross-modal ReID.Specifically, cross-modal ReID considers spe-cial cases in which RGB images of the target are notavailable but non-RGB modalities, such as infrared (IR), sketch , or description of the per-son could be leveraged to enlarge theapplication area of ReID technology.In real-world scenarios, the RGB image of the target maynot be directly available. On the one hand, the crime oftenhappens at night, when RGB cameras cannot capture high-quality images for ReID but IR cameras could give rela-tively good images of the person . Thus, adopting IRimages for target retrieval enters the picture. Zhang et al. proposes a feature-level modality compensation net-work to compensate for the missing modality-specific infor-mation at the feature level to help the model learn discrim-inative features. Wu et al. leverage unlabeled data andpropose a progressive graph matching method to learn a re-lationship between IR and RGB images to alleviate the highcost of annotation. On the other hand, the natural languagedescriptions of the witness or the sketch drawn based onthe textual descriptions are easy to access. Pang et al. first design an adversarial learning method to learn domain-invariant features cross sketch and RGB. Zhai et al. introduce a multimodal method that combines both sketchand text as queries for retrieval.However, in the real world, the modality of the giventarget information is uncertain. The aforementioned workscould only handle exactly two modalities, which hindersthe applicability of these methods. To alleviate this prob-lem, Chen et al. proposes a modality-agnostic retrievalmethod that leverages RGB, sketch, and text modalities tolearn modality-specific features and fuse them based on dif-ferent uni/multimodal tasks. The proposed method couldhandle any combination of three learned modalities, ex-panding usage scenarios and reducing limitations. Never-theless, it does not take IR into account and requires a com-plex design that lacks scalability. Different from that, thearchitecture of the proposed AIO is simple and it is easy tobe extended to more modalities.",
  ". Multimodal Learning": "Multimodal learning methods aim to utilize the comple-mentary properties of various modalities to learn the se-mantics of a task . Recently, multimodal transform-ers have emerged as unified models that fusedifferent modality inputs with token concatenation ratherthan extracting modality-specific and cross-modality repre-sentations. However, most multimodal learning methods are designed based on the assumption of thecompleteness of modality for training or inference, whichis not always held in the real world.To face this chal-lenge, some researchers explore buildingmultimodal methods that could handle missing modalities.ImageBind projects all features of different modali-ties into the same feature space and leverages contrastivelearning to align all modalities to a based modality. SMIL proposes to estimate the latent features of the missingmodality via Bayesian Meta-Learning. GCNet intro-duces graph neural network-based modules to capture tem-poral and speaker dependencies and jointly optimize classi-fication and reconstruction tasks. Similar to previous work,the proposed AIO aspires to project features from diversemodalities into a unified feature embedding. Notably, wecapitalize on the inherent capabilities of the transformer,adept at handling variable input lengths. This strategy en-ables the model to seamlessly accommodate inputs origi-nating from any combination of modalities, enhancing theflexibility and adaptability of the AIO framework.",
  ". Foundation Model": "Foundation models are designed to be adapted to variousdownstream tasks by pre-training on broad data at scale. The efficacy of large-scale pre-trained models is ev-ident in their capacity to enhance data encoding and ele-vate the performance of downstream tasks . Re-cent investigations reveal the notable emer-gent ability exhibited by most large-scale pre-trained foun-dation models, particularly in the context of robust zero-shot performance. CLIP focuses on multimodal con-trastive learning on noisy web image-text pairs to learnaligned image and text representation. Impressively, CLIPachieves accuracy comparable to the original ResNet-50on ImageNet zero-shot, without exposure to any samplesfrom ImageNet. DALLE introduces a simple approachthat autoregressively models sufficient and large-scale textand image tokens and demonstrates commendable zero-shotperformance when compared to preceding domain-specificmodels. Nonetheless, the foundation model within ReID re-mains in a nascent state. The suboptimal zero-shot perfor-mance of extant large-scale pre-trained models persists dueto challenges in data acquisition. Motivated by the impres-sive zero-shot capabilities exhibited by foundation models,the proposed AIO framework strategically employs frozen",
  "IR": ". The schematic of the proposed AIO framework. VA: Vision Guided Masked Attribute Modeling head, FB: Feature Bindinghead, CE: Classification head. Our framework mainly contains three parts: I) a learnable multimodal tokenizer to project diverse modalitiesinto a unified embedding, II) a frozen foundation modal to extract complementary cross-modal representations, and III) several cross-modalheads used to dig cross-modality relationships. In order to alleviate the missing modality problem, we also leverage Channel Augmentation and Lineart to synthesize IR and sketch images that are missing. pre-trained large-scale foundation models as feature extrac-tors, which aims to imbue our framework with the ability tolearn generalized semantics from a diverse range of modal-ities, thereby enhancing its zero-shot performance.",
  ". All in One Framework": "In this section, we describe the proposed AIO frameworkin detail. AIO exhibits the capability to adeptly handle un-certain multimodal data, encompassing RGB, IR, Sketch,and Text modalities. To realize this, we introduce a novelmultimodal tokenizer designed to project data into a uni-fied embedding space. Simultaneously, a large-scale pre-trained foundation model serves as the shared feature ex-tractor, encoding embeddings from diverse modalities. Thelearning process of the multimodal tokenizer is guided bycross-modal heads that are designed specifically for ReIDtasks. The schematic of the AIO is illustrated in .Preliminary.Formally, within the AIO framework,three key components are denoted as follows: I) the mul-timodal tokenizer mod(), II) the frozen multimodal en-coder f(), and III) cross-modal heads head(), wheremod refers to the notation of each modality, such as RGB(R), IR (I), Sketch (S), and Text (T); head refers to thenotation of Classification (CE), Vision Guided Masked At-tribute Modeling (VA), and Feature Binding head (FB), re-spectively. The inputs are denoted as xmod Xmod, theembeddings generated by tokenizers are Emod, and theoutput feature from the frozen multimodal encoder is zmod, which is corresponding to class tokens in . We as-sume 1) each modality possesses a specific parameter spacemod for modality-specific feature representations, and 2)there exists a shared parameter space A, an intersection ofeach modality parameter space for modality-shared featurerepresentations, adhering to the condition:",
  ". Multimodal Tokenizer": "To project various modalities into a unified space, we de-vise a straightforward multimodal tokenizer. This tokenizercomprises four projectors: three dedicated to RGB, IR, andSketch modalities, and one for Text. Furthermore, a multi-modal embedding is constructed by amalgamating the em-beddings from the respective modalities.Image Tokenizers.Given the disparate channel countsin RGB, IR, and Sketch images, for the sake of con-venience, we employ channel replication in both IR and",
  ". The generated synthetic Sketch and IR images. Wealso visualize the feature distribution of RGB, IR, Sketch, and syn-thesized images": "Sketch modalities to align their channel count with the threechannels present in RGB images. Deviating from the origi-nal tokenizer employed in ViT , leading to induce train-ing instability , we opt for the IBN style tokenizerfrom ViT-ICS . The convolutional, batch normalization(BN), and rectified linear unit (ReLU) layers inherent to theIBN-style tokenizer substantially enhance training stability and mitigate data bias which is critical for ReID .Text Tokenizers. In accordance with prior research efforts, we adopt the CLIP tokenizer to directly map thetext. Each word is uniquely associated with a token, andthrough the utilization of word embedding layers, it is pro-jected into a high-dimensional feature space to yield a se-quence of word embeddings.Multimodal Embedding. In the context of multimodal em-bedding, the embeddings originating from various modal-ities are concatenated.Additionally, following previousworks , a learnable token zA is appended to the se-quence of multimodal embeddings. Simultaneously, posi- tion embeddings EP os are employed to enhance positioninformation, seamlessly integrated with the multimodal em-beddings via element-wise addition, a procedure akin to theoriginal operation in ViT . The multimodal embeddingis formulated as follows:",
  ". Missing Modality Synthesis": "Given the insufficiency of multimodal data in ReID, espe-cially in IR, and Sketch, we introduce Channel Augmen-tation (CA) and Lineart as augmentation meth-ods to synthesize absent modalities. The generated samplesare shown in (a)-(c). The incorporation of syntheticmodalities offers two advantages: 1) an expansion in thesize of the multimodal sample, thereby mitigating issues as-sociated with missing modalities; 2) CA and Lineart act asconduits bridging the gap between synthetic and real IR andsketch modalities. This is attributed to the feature distribu-tion of the augmented images aligning between RGB andreal IR and Sketch images. The visual representation of thefeature distribution for RGB-Lineart-Sketch and RGB-CA-IR, as presented in (d) through t-SNE, serves as evi-dence of their efficacy in alleviating the learning challengesarising from modality gaps.Progressively Learning with Synthetic data. We employa progressively learning strategy to train the proposed AIOframework. The strategy involves initially training on syn-thetic images, incorporating real-world RGB and Text, for afew number of epochs. Subsequently, the model undergoesfurther fine-tuning using paired IR and Sketch images fromthe real world. This sequencing is deliberate, as syntheticimages exhibit a reduced domain gap with RGB comparedto real IR and Sketch images, facilitating a more accessiblelearning process for the model. A similar phenomenon isalso found in other cross-modal works .",
  ". Multimodal Modeling and Binding": "All representations extracted by the frozen multimodal en-coder from each embedding are fed into cross-modal headshead, which are specifically designed to learn cross-modalrelationships between different modalities. As illustrated in, there are three heads: 1) Conventional Classifica-tion Head, learning identity invariant representations like inother ReID works ; 2) Vision Guided Masked At-tribute Modeling, seeking to learn fine-grained RGB-Textrelationships; 3) Multimodal Feature Binding, aiming toalign each modality representations together.Conventional Classification (CE). The classification headonly contains a bottleneck and a classifier, which is",
  "Ny log(CE zmod),(4)": "where N is the number of pedestrian IDs, CE indicates theConventional Classification head. The conventional TripletLoss, commonly employed in related frameworks, is omit-ted in our architecture, as we opt for the utilization of amultimodal feature binding loss.Vision Guided Masked Attribute Modeling (VA). At-tributes play a pivotal role in highlighting essential char-acteristics of an individual, encompassing factors such asgender and hair color. These attributes are instrumental incross-modal alignment and the differentiation of distinct in-dividuals. In this context, we investigate the utility of at-tribute information embedded in the Text modality to serveas supervisory signals for learning discriminative personrepresentations. To be specific, in the case of a paired RGBimage and Text, we adopt a strategy from the prior work, where specific attribute keywords in the Text are se-lectively masked. These masked words are then projected toa special token [MASK]. Subsequently, the concatenatedfeatures of the paired RGB image and the masked token arefed into a decoder structured with MLPs and a classifier,represented as follows:",
  "NA My log(V A(zR zm)),(5)": "where V A indicates the Vision Guided Masked AttributeModeling head, zm zM are the features of masked to-kens, NA, M are the number of classes and the number ofmasked tokens, and denotes the concatenation operation.Multimodal Feature Binding (FB). To align all modalitiesonto a shared manifold, we attract features from all modal-ities towards the RGB feature.This alignment is facili-tated through the incorporation of a novel supervised fea-ture binding loss, elucidated in the subsequent section:",
  "(6)": "where || || is the cosine similarity, zRi is the representa-tion of person ith RGB embedding, zmodiare the repre-sentations of person ith other modalities embeddings, zRjare the RGB representations belonging to other people, isthe temperature that controls the smoothness of the softmaxdistribution. Diverging from the conventional InfoNCE ap-proach , our feature binding loss involves bringing to-gether features from all modalities corresponding to thesame individual, while simultaneously creating a separa-tion between RGB features of distinct individuals, ratherthan applying the same principle to all features. This dif-ference is motivated by the prevalence of RGB as the mostcommon modality in real-world scenarios, contributing themost abundant data and consistently present in all publiclyavailable datasets.",
  ". Overall Architecture": "As elucidated earlier, the primary objective of the AIOframework is to learn a multimodal tokenizer through afrozen multimodal encoder, under the guidance of cross-modal heads.We believe that the emergent capabilitiesdemonstrated in large-scale foundation models can effec-tively augment the zero-shot ability in multimodal ReIDtasks. Additionally, capitalizing on the inherent adaptabil-ity of transformer architecture to accommodate variable in-put lengths, AIO exhibits competence in processing diversecombinations of commonly employed modalities in ReID.To realize this objective, AIO is constrained by three cross-modal heads, that can be written as follows:",
  ". Experiment": "In this section, we conduct a comprehensive evaluation ofthe proposed AIO framework across both cross-modal andmultimodal ReID tasks. Our analysis demonstrates the effi-cacy of the AIO framework, particularly in zero-shot sce-narios involving uncertain input modalities within ReIDtasks. Additionally, we delve into the examination of vary-ing foundation models and input modality combinations.",
  ". Experimental settings": "Datasets.Three publicly available datasets SYNTH-PEDES for R-T pairs, LLCM for R-I images,MaSk1K for R-S images are leveraged for training.For zero-shot performance evaluation, five widely used real-world datasets are used for evaluations, Market1501 forRR task, SYSU-MM01 for IR task, PKU-Sketch for SR task, CUHK-PEDES for TR task, andTri-CUHK-PEDES for T+SR task. The dataset statis-tics are shown in Tab. 3. More details can be found in theoriginal papers.Evaluation Protocols. Following existing cross-modalityReID settings , we use the Rank-k match-ing accuracy, mean Average Precision (mAP) metrics, andmean Inverse Negative Precision (mINP) for perfor-mance assessment.In the context of multimodal ReID,we adhere to the evaluation settings outlined in TriReID and UNIReID specifically designed for RGB-Text+Sketch scenarios. To accommodate other multimodaldata combinations, we leverage CA and Lineart to generate simulated IR and Sketch images.While ac-knowledging that this may not perfectly simulate real-worldscenarios, it provides valuable insights into the multimodalperformance of the proposed AIO framework.Implementation Details. We employ the ViT as thebackbone, which is pre-trained on LAION-2B dataset withcontrastive learning, to reinforce the ability for generic to-ken encoding. All parameters of the backbone networks arefrozen. The Text tokenizer is from the pre-trained CLIP to segment sentences into subwords and transform theminto word embeddings. We perform a progressively learn-ing strategy training process in AIO framework, as we dis-cussed in Sec. 3.2. stage 1) In the first 40 epochs, we sample32 paired RGB and text samples from SYNTH-PEDES onlycombined with generated synthetic IR and Sketch imagesusing CA and Lineart . Moreover, we randomlychose two to four embeddings from different modalities tobuild the multimodal embedding. It is worth noting that,multimodal embedding may not contain RGB embedding.stage 2) In the rest 80 epochs, we still select 32 samples for abatch but from all training datasets. For data from SYNTH-PEDES, the sampling, synthetic methods, and constructionof multimodal embedding are unchanged. For data from",
  ". Zero-shot performance with multimodal input on Tri-CUHK-PEDES. Be aware that the IR images are generated byusing CA rather than real-world IR images": "LLCM and MaSk1K, only paired RGB-IR and RGB-Sketchimages are leveraged. The multimodal embedding for sam-ples from these two datasets only contains available modal-ities. We also apply random horizontal flipping and randomcropping for visual modalities. All images are resized to384 192. The framework is optimized by AdamW optimizer with a base learning rate of 1e-4, a cosine weightdecay of 1e 4, and a warmup in the first 5 epochs. Thelearning rate of the CLIP tokenizer is multiplied by 1e-1since they have already been pre-trained. The is set to3e-1 and the is set to 5e-2 as in . The framework isdistributively trained on 8 NVIDIA 3090 GPUs.",
  ". Ablation Study": "Efficacy of Designed Modules. We first evaluate the ef-fectiveness of the designed components. As evident fromTab. 4, each introduced cross-modal head proves crucialfor the overall performance of our All-in-One (AIO) frame-work. Specifically, VA head yields the most substantial per-formance enhancement in the TR task, CE head plays animportant role in the RR task, and FB head improves allcross-modal and multimodal tasks.Different Foundation Models. We explore various foun-dation models, including Uni-Perceiver v2 (Uni), a Vi-sion Transformer (ViT) pre-trained on LuPerson , andthe pre-trained image encoder from CLIP . The per-formance of these diverse foundation models is presentedin Tab. 5. As discernible from the table, the performancedemonstrates an upward trend with the expansion of the pre-training dataset. Notably, despite LuPersons exclusive fo-cus on ReID tasks, its performance lags behind other mod-els due to its comparatively smaller size. This discrepancyunderscores the pronounced zero-shot performance benefits",
  "MultimodalAIO (Ours)-79.659.957.651.970.273.553.443.4": ". Zero-shot performance on cross-modal retrieval. The best Rank-1 and mAP performance are reported. Results with * indicatethat the experiment results are produced by authors. For AGW , it is trained on MSMT17 and LLCM for RR and IR.For IRRA , it is trained on ICFG-PEDES for TR. For UNIReID , it is trained on Tri-ICFG-PEDES for TR and RR.",
  ". Zero-shot performance with multimodal input andgeneralized cross-modal on PKU-Sketch": "associated with large-scale pre-trained foundation models.Influence of Multimodality Input.Because our pro-posed AIO framework supports any combination of diversemodalities as input, we also analyze the influence of dif-ferent combinations of multimodal inputs on Tri-CUHK-PEDES , where the missing IR modality is generated byCA . As presented in Tab. 6, our analysis indicates apreference for RGB and text modalities within our frame-work over other modalities. Furthermore, when the num-ber of input modalities reaches or exceeds three, there is nosignificant alteration in performance. This outcome alignswith expectations, as RGB and Text modalities inherentlyprovide more discriminative details than others.",
  ". Evaluation on Multimodal ReID": "Given the rarity of generalizable works across cross-modal,multimodal, and pre-trained ReID, we conduct a compre-hensive comparative analysis involving the proposed AIOframework, various large-scale pre-trained ReID models,unimodal generalized methods, cross-modal methods, andmultimodal methods, all within the zero-shot setting. Asillustrated in Tab. 7, the existing large-scale pre-trainedReID models, with the exception of PLIP, exhibit unsat-isfactory performance in the zero-shot setting. Moreover,AIO achieves competitive performance compared to uni-modal generalization methods on RR retrieval task andoutperforms cross-modal methods on all cross-modal re- trieval tasks in the zero-shot setting. Notably, existing meth-ods fall short in generalizing to unseen modalities, a lim-itation overcome by AIO, which adeptly handles all fourmodalities in cross-modal tasks. The outcomes presented inTab. 8 unveil the remarkable performance of the proposedAIO framework when incorporating multimodal input. Thissuperior performance stands in stark contrast to methods re-lying solely on unimodal inputs in cross-modal tasks. Ad-ditionally, the results consistently underscore the impact ofdifferent modalities, aligning with the conclusions drawnfrom our preceding ablation studies that AIO is more in fa-vor of Text and RGB modalities than others. Moreover, wealso discuss the difference between AIO and UNIReID indetail and the limitation of AIO in the supplemental part.",
  ". Conclusion": "To the best of our knowledge, this is the first work delv-ing into the uncertain multimodal ReID tasks encompass-ing all four prevalent modalities, e.g. RGB, IR, Sketch, andText.We investigate the feasibility of harnessing large-scale foundation models for multimodal ReID tasks, pre-senting a prospective avenue toward zero-shot multimodalReID in wild conditions. In order to cooperate with foun-dation models, we introduce an innovative multimodal tok-enizer, designed to utilize disparate modality inputs within ashared embedding space, guided by carefully crafted cross-modal heads. Moreover, we introduce synthetic augmenta-tion methods with a progressively learning strategy to alle-viate the missing modality problem and mitigate the cross-modal gap between different modalities. Extensive exper-imentation demonstrates the efficacy and competitive per-formance of the proposed AIO framework across both zero-shot cross-modal and multimodal ReID tasks.Acknowledgement. This work is partially supported byNational Natural Science Foundation of China under Grant(62176188, 62361166629, 62225113, 62306215), and theSpecial Fund of Hubei Luojia Laboratory (220100015).",
  "He Li, Mang Ye, Cong Wang, and Bo Du. Pyramidal Trans-former with Conv-Patchify for Person Re-identification. InACMMM, 2022. 3": "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, HongshengLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,Wenhai Wang, and Jifeng Dai. Uni-Perceiver v2: A Gen-eralist Model for Large-Scale Vision and Vision-LanguageTasks. In CVPR, 2022. 7 Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before Fuse: Vision and Language RepresentationLearning with Momentum Distillation. In NeurIPS, 2021.3",
  "Differences between UNIReID and AIO": "There are three distinctions between UNIReID and AIO:1) Divergent Goals: UNIReID and AIO fundamentally dif-fer in their objectives. UNIReID aims to construct a multi-modal model for intra-domain retrieval with the descriptivequery. At the same time, AIO is explicitly crafted for uni-versal retrieval in real-world scenarios, with four arbitrarymodalities or their combinations. Notably, all experimentsin this paper follow a zero-shot generalizable setting, whichis inapplicable for UNIReID.2) Different Challenges: UNIReID demands paired multi-modal data. In comparison, AIO confronts even more chal-lenging scenarios, involving unpaired heterogeneous multi-modal data, with imbalanced and missing modalities. Thus,we introduce synthesized modalities and build connectionsamong imbalanced modalities.3) Disparate Approach: UNIReID incorporates multipletasks to accommodate uncertain multimodal input.Thenumber of optimization objectives of UNIReID grows ex-ponentially with the number of modalities, making it hardto extend to more modalities and hindering its scalability.Conversely, AIO designs a flexible solution, treating uncer-tain multimodal input as variable input lengths. It lever-ages the adaptable nature of the transformer architecture,simplifying the integration of additional modalities. Fur-thermore, UNIReID employs separate encoders for variousmodalities, resulting in a lack of synergy between distinc-tive modalities. Different from UNIReID, AIO leveragesa shared foundation model as the backbone to collabora-tively learn comprehensive knowledge from heterogeneousmultimodal data to complement each other and enhance itsgeneralizablity in real-world scenarios.All these differences make AIO more robust and generaliz-able than UNIReID in real scenarios.",
  "Limitation": "1) The computational complexity of AIO, necessitatingO(n2 D) operations for processing token embeddingsEA, ER, EI, ES, ET , particularly in the context of multi-modal input, imposes a substantial memory cost and com-putational burden.This complexity poses challenges inscalability for incorporating additional modalities and de-ployment on resource-constrained edge devices. We assessthe inference speed across varying numbers of modalities.Tab. 9 shows that the computation complexity escalates ex-ponentially with the increase in the number of modalities,as anticipated.2) Furthermore, it is worth noting that the implementation",
  ". Computation complexity in the different number ofinput modalities. All results are calculated with 700 samples": "of multimodal ReID on synthetic data may not perfectlyalign with real-world scenarios, but also brings valuable in-sights for future works.3) Moreover, the learnable parameters within the tokenizerare constrained compared to approaches that fine-tune theentire backbone, presenting a double-edged sword. WhileAIO is lightweight and user-friendly, it may not capture asmuch detailed knowledge as some alternatives. To addressthis challenge, a promising way is to selectively unfreeze asubset of deep layers within the backbone model, a directionwe plan to investigate in future work."
}