{
  ", , ,": ". Editing examples and graphical comparison for StyleFeatureEditor. Our approach takes a real image, inverts it to theStyleGAN latent space, edits the found latents, and synthesises the edited image. On the left, we present examples of our approach, whileon the right, we display a comparison with previous approaches. To evaluate inversion quality, we used LPIPS. Additionally, to comparethe editing capabilities, we compute FID for 3 editing directions (see 4.3) and average them with coefficients equal to the averageFID per editing direction. The size of markers indicates the inference time of the method, with larger markers indicating a higher time.StyleFeatureEditor capable of reconstructing even finer image details and preserving them during editing.",
  "Abstract": "The task of manipulating real image attributes throughStyleGAN inversion has been extensively researched. Thisprocess involves searching latent variables from a well-trained StyleGAN generator that can synthesize a real im-age, modifying these latent variables, and then synthesizingan image with the desired edits. A balance must be struckbetween the quality of the reconstruction and the ability toedit. Earlier studies utilized the low-dimensional W-spacefor latent search, which facilitated effective editing butstruggled with reconstructing intricate details. More recentresearch has turned to the high-dimensional feature spaceF, which successfully inverses the input image but losesmuch of the detail during editing. In this paper, we intro-duce StyleFeatureEditor a novel method that enables edit-ing in both w-latents and F-latents. This technique not onlyallows for the reconstruction of finer image details but alsoensures their preservation during editing. We also presenta new training pipeline specifically designed to train ourmodel to accurately edit F-latents. Our method is comparedwith state-of-the-art encoding approaches, demonstratingthat our model excels in terms of reconstruction quality andis capable of editing even challenging out-of-domain ex-amples. Code is available at",
  ". Introduction": "In recent years, GANs have achieved impressive re-sults in image generation, which has led to their use in awide variety of computer vision tasks. One of the most suc-cessful models is StyleGAN , which not only hasa high quality of generation, but also a rich semantic la-tent space. In this space, we can control different semanticattributes of the generated images by changing their latentcode . However, to apply this editing technique to realimages, we must be able to find their internal representationin the StyleGAN latent space. This problem is called GANinversion , and although it is well studied and many ap-proaches have been proposed ,it is still an open problem to develop a method that simul-taneously satisfies three requirements: high-quality recon-struction, good editability, and fast inference. Our work isdedicated to the development of such a method.Existing GAN inversion approaches can be dividedinto two groups: optimization-based and encoder-based.Optimization-based methods learn a latent represen-tation for each input image that best reconstructs that im-age. This results in good inversion quality, but such over-fitted latent codes may deviate from the original distributionof the latent space, resulting in poor editing. While thereare approaches that improve the quality of editing by fine-tuning the generator itself for a given image , this does",
  "arXiv:2406.10601v1 [cs.CV] 15 Jun 2024": "not address the main drawback of such methods, which isthat the inversion is too long, making them impractical touse in real-time applications. In contrast, more practicalencoder-based methods allow us to obtain a la-tent representation of the input image in a single networkpass. However, with these approaches, it is more difficult toachieve both high quality and good editability at the sametime. This is the so-called distortion-editability trade-off. Inversion quality and editability are directly related tothe dimensionality of the latent space in which we encodethe input image. In low-dimensional W and W + spaces,we will get low reconstruction quality but high editability,because the low dimensionality of the latent code is a goodregularizer that keeps it in the StyleGAN manifold. If wetrain the encoder to predict in the high-dimensional Style-GAN feature space Fk, this will significantly increase thequality of the reconstructions at the expense of degradededitability, since in such a space it is easier to overfit to aparticular image and escape the region in the latent spacewhere semantic transformations work. Methods workingin Fk try to challenge this problem by usingadditional transformations over the tensor Fk, but it is notcompletely solved. In particular, the editability problem isamplified when one increases the dimensionality of the Fkfeature space by taking them from earlier layers to improvethe quality of reconstructions. In this paper, we propose a framework that allows us totrain an encoder in a high-dimensional Fk space that simul-taneously achieves both excellent reconstruction quality andgood editability. The main idea of our approach is to dividethe training of our encoder into two phases. In the first, wetrain an encoder that predicts a latent code in Fk space withhigh resolution, which allows us to reconstruct images withhigh quality, but at the same time significantly reduces ed-itability. To recover the editability , we introduce a secondphase of training : we propose to train a new Feature Ed-itor module that task is to modify the feature tensor Fk toobtain the target editing in image generation. The main dif-ficulty in training this module is that we do not have a train-ing data, where for each image there would be its editedversions. Therefore, we proposed to automatically generatesuch data using an encoder operating in W + space. Thatis, as training samples for Feature Editor, we take recon-structions of real images using a standard encoder with lowinversion quality, but with good editability. And on this datawe train the Feature Editor, which predicts F k for the fea-ture tensor Fk of the input image, from which its editedversion should be generated. Thus, thanks to the proposed two-phase encoder learningframework, we are able to train an inversion model that hasboth high reconstruction quality, significantly better thanthe current state-of-the-art, and good editability. We con-ducted extensive experiments, demonstrating a significant improvement over state-of-the-art methods in the inversiontask, and comparable results in the image editing. In par-ticular, we have significantly improved the reconstructionmetrics in terms of LPIPS and L2 by more than a factorof 4 compared to StyleRes, while the running time isequivalent to conventional encoder-based methods.",
  ". Related Work": "Latent Space Manipulation.With the development ofStyleGAN models , they started to be actively usedfor the task of image editing. Many methods have shownthat by changing the latent code of an image in the latentspace of StyleGAN, it is possible to change the semantic at-tributes of the image . There are methods that find suchdirections using supervised approaches utilized attribute la-belled samples or pre-trained classifiers .Unsupervised methods do not use any kind of labelling, instead they, for example, perform PCA ei-ther in StyleGANs feature space or find directions inthe weight space . Other methods use a self-supervisedlearning approach .And there are methodsthat utilize language-image models to find desired editsguided by text . To apply all these methods toreal images, it is necessary first to encode images in Style-GANs latent space.GAN Inversion. The task of GAN inversion is tofind the latent code for a real image, from which it canbe generated by StyleGAN and the result has to be per-ceptually equal to the input image and can be edited bychanging this latent code. Existing GAN inversion methodscan be divided into two types: optimization-based methods and encoder-based methods .Optimization-based methods. Optimization methodsfind latent code by optimizing directly over the reconstruc-tion losses. The first approaches performedoptimization in Z/W/W + spaces. To improve the qualityof the reconstruction, later methods proposed to optimizeadditionally in the StyleGAN feature space . Since thelatent code can escape from the StyleGAN manifold duringthe optimization process and thus negatively affect the ed-itability, it has been proposed to additionally fine-tune thegenerator weights for each image . Although high re-construction quality and good editability can be achievedwith these approaches, the optimization process is too long,requiring up to several minutes for each image, which is notapplicable for real-time interactive editing.Encoder-based methods. Encoder-based methods al-low learning the mapping from the space of real images tothe StyleGAN latent space in one or more passes throughthe neural network. Basically, these methods differ in thelatent spaces they encode to. The first methods trained themapping for the simplest Z, W, W + spaces , . The Inverter training pipeline. Input image X is passed to Feature-Style-like backbone that predicts w W + and Fpred Fk.Then Fw = G(w0:k) is synthesized and passed with Fpred to the Fuser that predicts Fk. Inversion X = G(Fk, wk+1:N) is generated.Additional reconstruction Xw = G(w0:N) is synthesized from w-latents only. Loss is calculated for pairs (X, X) and (X, Xw). which gave good editability but low reconstruction qual-ity.Next methods were proposed that additionally pre-dicted changes in the generator weights using a hypernet-work to better reconstruct the input image . Thisincreased the quality of the reconstruction without sacri-ficing editability. There are also methods that propose touse multiple passes over the encoder to refine the details ofthe image during reconstruction . The most success-ful methods train encoders for StyleGANs feature spaceFk . Such methods achieve the highest recon-struction quality among encoder-based methods, and arecomparable to optimization-based methods. The main re-maining problem is poor editability, since in such a high-dimensional latent space it is very easy to overfit the imageand go out of the natural StyleGAN manifold.In our paper, we propose a framework that preserves theeditability of an encoder trained in the StyleGANs featurespace Fk, and achieves phenomenal reconstruction quality.",
  ". Overview": "The goal of StyleGAN inversion methods is to find an in-ternal representation of the input image in the StyleGANlatent space that contains as much information and detailas possible about the image itself, and at the same time al-lows editing it. This internal representation can be searchedin different StyleGAN latent spaces, which have differentproperties. We can distinguish two main latent spaces thatare considered in the StyleGAN inversion task, namely W + and Fk. W + is the concatenation of N vectors w1, ...,wN, which are fed into each of the N convolutional layersof StyleGAN. Fk is feature space, which is the combinationof the W + space and the space of the feature outputs of thek-th convolutional layer of the StyleGAN.",
  "It is known that the representation of an image in W +": "space preserves few details, but allows good editing. InFk space the situation is the opposite we can almost per-fectly reconstruct the original image, but this representa-tion is difficult to edit. The latest most advanced encodersFeature-Style and StyleRes work in Fk space, andto solve the editing problem, they offer their own techniquesto transform the Fk Fk feature tensor during editing. Butthese techniques do not solve the problem completely. Andit is exacerbated if the resolution of the Fk feature tensoris increased. In this case, the quality of reconstructions im-proves significantly, but the editability completely vanishes.In our work, we propose a way to edit Fk feature tensorthat preserves high quality of the reconstruction with goodeditability. The basic idea is to train an additional mod-ule called Feature Editor, which will transform the featuretensor Fk in the right way for each edit. But to train Fea-ture Editor, we will need a special training dataset, wherefor each image we need to have its edited versions. It isclear that it is very difficult and expensive to manually buildsuch a dataset. Therefore, we generated this dataset usingan encoder that operates in W + space. That is, for each realimage from the dataset, we find its reconstruction in W + space, get its edited version, and use these two images totrain our Feature Editor module. This approach allowed usto significantly improve the quality of edits, even for highresolutions of Fk. Further, we give more details about thearchitecture of StyleFeatureEditor and the training process.",
  ". Architecture": "In this section, we describe StyleFeatureEditor, which con-sists of two parts: Inverter I and Feature Editor H. The taskof Inverter is to extract reconstruction features from the in-put image, while Feature Editor should transform these fea-tures according to the information about the desired edit. Inversion loss calculation Generator4 1024 Inversion loss",
  "Legend": "Fixed weightsFeature Editor . The Feature Editor training pipeline and inference. To obtain editing loss, one need to synthesize training samples: XE training input, and XE training target. The pre-trained encoder E takes the real image X and predicts wE W +. Edited directiond D is randomly sampled, after which wE is edited to wE = wE + d. Image XE and intermediate features FwE are synthesized fromwE, while XE and FwE are synthesized from wE via generator G. XE is used as input and passed to frozen Inverter I that predicts Fk andw that is edited to w according sampled d. Then is calculated, and Feature Editor H edits Fk according . The edited reconstructionXE is synthesized from F k and wk+1:N. Editing loss is calculated between XE and XE. To obtain the inversion loss, the real image Xis passed to I that predicts w and Fk, Fk is edited to F k by H with = 0. The inversion X is synthesized from F k and wk+1:N. TheInversion loss is calculated between X and X. Inference pipeline is the same as synthesizing XE but with the assumption that I takesreal image X instead of XE. Inverter. I consists of Feature-Style-like Encoder Ifseand an additional module Ifus called Fuser. Ifse consistsof Iresnet50 backbone, Feature predictor and Linear layers(see ). First, the input image X is passed to the back-bone, which predicts 4 intermediate features, pools them tothe same dimensionality, concatenates them, and maps tow W + by Linear layers. The third intermediate feature isalso passed to Feature predictor that predicts Fpred Fk:",
  "(w, Fpred) = Ifse(X).(1)": "Despite good inversion quality, Feature-Style Encoderfails to reconstruct fine details of the image, thus we in-creased the predicted feature tensor from Fpred F5to FpredF9 that increases its dimensionality fromR5121616 to R5126464 respectively.To take into account the impact of w0:k we additionallysynthesize output of the k-th generator layer Fw = G(w0:k)via the StyleGAN2 generator G. Fw then fused with pre-dicted Fpred by an additional module Ifus, which predictsFk Fk:",
  "X = G(Fk, wk+1:N).(4)": "It is also possible to synthesize image Xw = G(w) fromw-latents only, which we use during training.Feature Editor. The predicted feature tensor Fk con-tains much of the input image information, which allowseven finer image details to be reconstructed. However, ifwe do not transform Fk during editing, artefacts may ap-pear or editing may not work at all. Therefore, we proposean additional Feature Editor module H that transforms pre-dicted Fk according to the desired edit. In order for H tounderstand what to change, it is necessary to have informa-tion about which regions Fk need to be edited. To obtainsuch information, we propose to use a pre-trained encoderE in W + space that is capable of good editing (we use pre-trained e4e encoder ).E takes input image X and predicts wE = E(x) W +,which is edited to wE = wE +d by editing direction d. Theoutputs of the k-th generator layer FwE and FwE are syn-thesized from wE and wE respectively. Difference betweenFwE and FwE contains information about edited regions:",
  ". Training Inverter (Phase 1)": "This section is related to training Inverter I to reconstructsource images. The pipeline of phase 1 is presented in .The source image X is passed to I which predictsw, Fk = I(X), where w RN512 and Fk R5126464.Then the generator G synthesizes X = G(Fk, wk+1:N) reconstruction of the input image X.The loss functionLphase1 is calculated between X and X. In addition, toforce information flow not only through feature space Fk,we also calculate Lphase1 for reconstruction Xw = G(w)obtained from w-latents only.The loss function Lphase1 consists of two equal parts:the image loss Lim applied to both (X, X) and (X, Xw),and the regularization Lreg for constraining the norm of Fktensor. Lim is calculated as a weighted sum of per-pixelloss L2, perceptual LPIPS loss Llpips , identity-basedsimilarity loss (ID) Lid by utilizing a pre-trained net-work (ArcFace for the face domain and ResNet-based for non-facial domains), adversarial loss Ladv by em-ploying a pre-trained StyleGAN discriminator D which wefine-tune during training. As the regularization loss, we useLreg = Fk2. So, the total loss Lphase1 is calculated as:",
  ". Training Feature Editor (Phase 2)": "The goal of this phase is to train the Feature Editor H toedit Fk. The training pipeline of this phase is available in. In this phase, we assume that I is already trained, sowe froze its weights and train only H weights.For this purpose it is necessary to have a dataset con-sisting of pairs (X, X), where X is the edited version ofthe image X, but it is difficult to collect such data manu-ally. Therefore, we propose to use a pre-trained encoder Ein W + space suitable for editing to generate such data. Etakes input image X and predicts wE, it is edited with speci-fied editing direction d to wE = wE +d, after which imagesXE and XE are synthesized from wE and wE respectively. During this phase, we fix a set of 13 editing directions Dused in training (more details in Appendix 7). The pipelineof training H using synthetic data is:1. Pass X to E to obtain wE and wE = wE + d for theediting direction d randomly sampled from D.",
  ". Synthesize XE = G(F k, wk+1:N) the edited recon-struction": "7. Calculate the loss between XE and XE.However, if H is trained only on synthetic images, the re-construction quality for real images may degrade. To solvethis problem, we propose to train H not only on editing, butalso on the classical inversion task. The training pipeline isthe same, but for inversion we use a real image X as inputand assume = 0. X is passed to I, which predicts Fk andw (Eq. 3), = 0 and Fk goes to the Feature editor whichpredicts F k and reconstruction X is synthesised assumingw = w (Eq. 7). The loss is calculated between X and itsreconstruction X.For this phase we used L2, Llpips and Lid for both in-version and editing tasks with coefficients from phase 1.For inversion task we additionally use adversarial loss Ladv:",
  "Lphase2 = Ledit(X0, X0) + Linv(X, X)(12)": "During training we fixed a set of 13 editing directions D,however SFE is capable of generalising to new directionswithout any retraining. Furthermore, D can be restrictedwhile SFEs editing abilities remain good on both: seen andunseen directions (see Ablation Study 4.4, Appendix 10).This can be explained by the fact that (which containsalmost all editing information) of even one direction will bevery different for different images. Therefore, during train-ing, H does not learn specific directions, but generalizes togather information from . Since depends only on theedited w-latents obtained from E (e4e), our method is ableto apply any editing applicable to E (e4e).More training and architecture detail available in the Ap-pendix 7, 8.",
  "Inpute4eHyperinverterHFGIFSStyleResSFE (ours)": ". Visual comparison of our method with previous encoder-based approaches on face domain. Row 1 represents the inversion, row2 the addition of glasses, row 3 the darkening of the hair colour, row 4 the changing of the hairstyle. HQ dataset for inference. For the car domain, we usedtrain part of Stanford Cars for training and test part forevaluation. For test editings we used InterfaceGAN andStylespace for both face and car domains, StyleClipand GANSpace for face domain. To extract and sam-ple images for editing loss calculation during training phase2, we used pre-trained e4e as E. For the inversion cal-culation, we used our full pipeline including both I and H,assuming = 0 as in .We compare our method with state-of-the-art encoderapproaches such as e4e, psp, StyleTransformer,ReStyle,PaddingInverter,HyperInverter,Hyperstyle, HFGI, Feature-Style, StyleResand optimisation-based PTI.We used authors orig-inal checkpoints, but in car domain, some of them arenot public.We train Feature-Style on Stanford Cars byusing authors code and omitting models without officialcheckpoints.",
  ". Qualitative evaluation": "To demonstrate the performance of our method, in we compare it with previous approaches on several hardout-of-domain examples.Our approach not only recon-structs more detail than previous ones, but also preservesit during editing. For example, in the first row, our method accurately reconstructs womans hat while others smooth itout. In the second row, our method preserves the yelloweye colour while editing the eye zone. In rows 3 and 4, it isevident that our approach is better at reconstructing difficultmake-up and preserving the colours of the source image.Additionally, in we show comparison of ourmethod on car domain. In the first row, our method evenmanages to reconstruct the original shape of a car when theothers do not. Moving on to the second row, our methodmost accurately reconstructs the outline and white lines ofthe original car, while FS Encoder distorts them. Apart fromour approach in the third row, FS Encoder is the only onethat can reconstruct the shadow on the car, but it fails inchanging car colour.",
  ". Quantitative evaluation": "To evaluate the effectiveness of the inversion technique,two key aspects can be examined.Firstly, the accuracyof the inversion, which refers to the degree to which themethod is able to reconstruct the details of the original im-age. Second, the editability how well the inverted imagecan be edited.The comparison in both aspects on CelebA-HQ dataset is presented in .To measure quality of the inversion details, we usedLPIPS , L2 and MS-SSIM . Additionally, we de-",
  ". Additional visual comparison of our method with previous encoder-based approaches in the car domain. Row 1 represents theinversion, Row 2 the addition of grass, Row 3 the change in car colour": ". Quantitative comparison results for inversion quality and editing abilities on face domain. To measure inversion we report LPIPS,L2, MS-SSIM and FID calculated on Celeba HQ test set. To measure editing abilities, we used FID as described in 4.3.We also measuredthe time required to edit a single image on a single TeslaV100.",
  "SFE (ours)0.0190.0023.5350.92224.38873.09841.6770.070": "termined realism of the synthesized images by measuringdistance between distributions of real and inverted imagesusing FID . Our method outperformed all previous ap-proaches. The most notable difference was seen in LPIPSand L2, indicating that our method is capable of extremelyfine detail reconstruction. We also tested our method in thedomain of cars on the Stanford Cars dataset presented in, which confirms the results described above.It is challenging to accurately estimate the quality of theediting numerically in the absence of target images. To per-form these calculations, we use the technique proposed in. We determine the attribute to be edited, then, based on the Celeba HQ markup, we divide the test dataset intoimages A with and B without this attribute. Next, we ap-ply the method to B to add this attribute and synthesize B.The FID between B and B demonstrates the effectivenessof the technique for editing this attribute. We provide ex-periments with 3 attributes: removing smile, adding glassesand increasing age.The results show that our method not only invertswell, but is also comparable to the current state-of-the-artStyleRes in terms of editing capabilities. Furthermore, ourmethod requires only 0.066 seconds to edit a single imageon the TeslaV100, far outperforming optimisation-based",
  ". Ablation Study": "To ensure the importance of each component in the pro-posed pipeline, we conducted several ablation experiments.We present the quantitative results of these experiments in and visual representations in .First, we tried to discard H and use only I as in train-ing phase 1. Despite a small increase in the inversion met-rics, the edits stopped working, proving the significance ofH. We also tried an architecture without Fuser Ifus (whichrefers to the case where Fk = Fpred) and an experimentwhere the inversion loss is omitted during the second train- ing phase. Both of these experiments resulted in a drop inreconstruction quality that is difficult to detect at low reso-lution and only visible at high resolution. The fourth exper-iment was related to omitting E and predicting features for from w obtained from I. The predicted w is much lesseditable than wE from e4e, leading to artefacts during edit-ing () and showing that E should be well editable.We also attempted to train our pipeline with a lower pre-dicted feature dimensionality. We reduced the predicted Fkfrom k = 9 to k = 5, which is the dimensionality of theFeature Space Encoder. Despite the significant decrease ininversion quality, this approach is still capable of good edit-ing, unlike Feature-Style. During the last ablation, we re-duced the number of editing directions in D from 13 to 6in the second training phase. The reduced Dsmall consistsof Age, Afro, Angry, Face Roundness, Bowlcut Hairstyleand Blonde Hair. Despite a slight decrease in metrics, ourmethod is still able to edit directions that were not used dur-ing training, as shown in . 5. ConclusionIn this paper, we have demonstrated StyleFeatureEditor a novel approach to image editing via StyleGAN inversionand introduced a new technique for training it. Even forchallenging out-of-domain images, we have achieved a re-construction quality that makes it almost impossible to tellthe difference between the real and synthetic images withthe naked eye. Thanks to Feature Editor, our method isnot only able to reconstruct finer facial details, but also pre-serves most of them during editing.",
  ". Acknowledgments": "The analysis of related work in sections 1 and 2 were ob-tained by Aibek Alanov with the support of the grant forresearch centres in the field of AI provided by the An-alytical Center for the Government of the Russian Fed-eration (ACRF) in accordance with the agreement onthe provision of subsidies (identifier of the agreement000000D730321P5Q0002) and the agreement with HSEUniversity No.70-2021-00139.This research was sup-ported in part through computational resources of HPC fa-cilities at HSE University. Rameen Abdal, Yipeng Qin, and Peter Wonka.Im-age2stylegan: How to embed images into the stylegan latentspace? In Proceedings of the IEEE/CVF international con-ference on computer vision, pages 44324441, 2019. 1, 2 Rameen Abdal, Yipeng Qin, and Peter Wonka.Im-age2stylegan++: How to edit the embedded images?InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 82968305, 2020. 1, 2 Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka.Styleflow:Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizingflows. ACM Transactions on Graphics (ToG), 40(3):121,2021. 2 Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle:A residual-based stylegan encoder via iterative refinement.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 67116720, 2021. 2, 3, 6, 7, 8 Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, andAmit Bermano.Hyperstyle: Stylegan inversion with hy-pernetworks for real image editing.In Proceedings ofthe IEEE/CVF conference on computer Vision and patternrecognition, pages 1851118521, 2022. 1, 3, 6, 7, 8 Qingyan Bai, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yu-jiu Yang, and Yujun Shen. High-fidelity gan inversion withpadding space. In European Conference on Computer Vision,pages 3653. Springer, 2022. 2, 6, 7",
  "Antonia Creswell and Anil Anthony Bharath. Inverting thegenerator of a generative adversarial network. IEEE transac-tions on neural networks and learning systems, 30(7):19671974, 2018. 2": "Jiankang Deng, Jia Guo, Niannan Xue, and StefanosZafeiriou. Arcface: Additive angular margin loss for deepface recognition.In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages46904699, 2019. 5 Tan M Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-SonHua. Hyperinverter: Improving stylegan inversion via hy-pernetwork.In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1138911398, 2022. 1, 2, 3, 6, 7 Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Trans-actions on Graphics (TOG), 41(4):113, 2022. 2 Lore Goetschalckx, Alex Andonian, Aude Oliva, and PhillipIsola. Ganalyze: Toward visual definitions of cognitive im-age properties. In Proceedings of the ieee/cvf internationalconference on computer vision, pages 57445753, 2019. 2 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. Advances inneural information processing systems, 27, 2014. 1",
  "Erik Harkonen, Aaron Hertzmann, Jaakko Lehtinen, andSylvain Paris. Ganspace: Discovering interpretable gan con-trols. Advances in neural information processing systems,33:98419850, 2020. 2, 6, 1": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. Gans trained by atwo time-scale update rule converge to a local nash equilib-rium. In Neural Information Processing Systems, 2017. 7 Xueqi Hu, Qiusheng Huang, Zhengyi Shi, Siyuan Li,Changxin Gao, Li Sun, and Qingli Li.Style transformerfor image inversion and editing.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1133711346, 2022. 2, 6, 7, 8",
  "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.Progressive growing of gans for improved quality, stability,and variation, 2018. 6": "Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 44014410, 2019. 1, 2,5 Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,Jaakko Lehtinen, and Timo Aila.Training generative ad-versarial networks with limited data.Advances in NeuralInformation Processing Systems, 33:1210412114, 2020. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,Jaakko Lehtinen, and Timo Aila.Analyzing and improv-ing the image quality of stylegan.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 81108119, 2020. Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen,Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-freegenerative adversarial networks. Advances in Neural Infor-mation Processing Systems, 34, 2021. 1, 2 Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.3d object representations for fine-grained categorization. In2013 IEEE International Conference on Computer VisionWorkshops, pages 554561, 2013. 6 Hongyu Liu, Yibing Song, and Qifeng Chen. Delving style-gan inversion for image editing: A foundation latent spaceviewpoint.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1007210082, 2023. 2",
  "national Conference on Computer Vision, pages 20852094,2021. 2, 6, 1": "Hamza Pehlivan,Yusuf Dalva,and Aysegul Dundar.Styleres: Transforming the residuals for real image editingwith stylegan. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 18281837, 2023. 1, 2, 3, 6, 7 Stanislav Pidhorskyi, Donald A Adjeroh, and GianfrancoDoretto. Adversarial latent autoencoders. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1410414113, 2020. 2",
  "Antoine Plumerault, Herve Le Borgne, and Celine Hude-lot. Controlling generative models with continuous factorsof variations. arXiv preprint arXiv:2001.10238, 2020. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2 Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encodingin style: a stylegan encoder for image-to-image translation.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 22872296, 2021. 1, 2,5, 6, 7",
  "editing.In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1137911388, 2022. 2, 3, 6, 7": "Z. Wang, E.P. Simoncelli, and A.C. Bovik. Multiscale struc-tural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Com-puters, 2003, pages 13981402 Vol.2, 2003. 6 Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespaceanalysis: Disentangled controls for stylegan image genera-tion. 2021 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1285812867, 2020. 6,1 Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.Tedigan: Text-guided diverse face image generation and ma-nipulation. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 22562265,2021. 2",
  ". Training details": "The training of the StyleFeatureEditor consists of twophases: Phase 1 training of the Inverter and Phase 2 training of the Feature Editor. A batch size of 8 is used forboth phases. We used Ranger optimiser with a learning rateof 0.0002 to train each part of our model, and Adam opti-miser with a learning rate of 0.0001 to train the Discrimina-tor.Phase 1. During this phase, we duplicate the batch ofsource images X and synthesize the reconstruction X andthe reconstruction from w-latents only Xw for the same im-ages. The loss is computed for both pairs (X, X), (X, Xw)and consists of L2, LPIPS, ID, adversarial loss and regu-larization loss for predicted feature tensor Fk with corre-sponding coefficients loss. We used L2 = 1, lpips =0.8, id = 0.1 for face domain and id = 0.5 for car do-main, adv = 0.01, reg = 0.01. We start applying adver-sarial loss and training the discriminator only after 14000steps. The full training duration of the first phase is 37500steps.Phase 2. As in phase 1, the batch of source images Xis duplicated and the same images are used for both in-version and editing loss. First, training samples XE andXE are synthesized, then XE is passed through StyleFea-tureEditor which tries to reconstruct and edit it to XE, theediting loss is calculated between XE and XE.For in-version loss, the reconstruction X is synthesized for thesame images used for sampling (XE, XE). The inversionloss is calculated between X and X. For both losses weuse L2, LPIPS and ID with the corresponding coefficientsL2 = 1, lpips = 0.8, id = 0.1 for face domain andid = 0.5 for car domain. For inversion loss we addition-ally apply adversarial loss with adv = 0.01. The durationof this phase is 20000 steps.During the second phase, we fix a set D of possi-ble editing directions that we apply to compute the edit-ing loss.D consists of InterfaceGAN directions(Age, Smile, Pose Rotation, Glasses, Make-up),GANSpace direction Face Roundness, StyleClipdirections (Afro, Angry, Bobcut Hairstyle, Mo-hawk Hairstyle, Purple Hair) and Stylespace di-rections (Blonde Hair, Gender). For car domain weused InterfaceGAN (Cube Shape, Grass, ColourChange) and Stylespace (Trees, Headlights). Foreach direction, we empirically choose several editing pow-ers in such a way that E produces non-artefacting edits.",
  ". Architecture details": "Our architecture has 2 parts: Inverter I and Feature Edi-tor H. Inverter consists of Feature-Style-like encoder Ifseand Fuser Ifus. Ifse has been slightly changed comparedto original version. The original Iresnet-50 backbone con-sists of 4 blocks ( (a)), where each block increasesthe number of channels and reduces the spatial resolutionof the input tensor. Each block consists of several layers,whose typical architecture is shown in . As far aswe increased k from 5 to 9 (which increases spatial reso-lution of predicted tensor from 16 16 to 64 64) it isnecessary to extract features from the backbone with cor-responding to the new spatial resolution. However, in theoriginal Iresnet-50 architecture, such a tensor can only begathered after block 2, which means that the original imageis only passed through 3 + 4 = 7 layers, which is not enoughto extract finer detail information. To fix this, we reducedstride in one of the layers in block 3, so that the resolutionof the predicted tensor is changed as shown in (b),and the source image is processed through 21 layers.",
  ". Scheme of the typical layer of Iresnet-50. H and Ifusalso consist of such layers": "It is important to note that in the car domain, the infor-mation of some editings consists only in high-dimensionalfeatures with a spatial resolution of 128 128. To take thisinto account, during computation, we synthesize outputsof the 11-th generator layer FwE, F wE F11 instead of F9.To transform such to size of 5126464, we first applyan additional trainable Irensnet-50 layer, which reduces theresolution, and only then pass processed to H.Ifus and H have the same architecture. They both con-sist of 6 Iresnet-50 layers () with skip connections.During passing through Ifus or H spatial resolution of in-put tensor is not changed. When applying skip connections,we also use 1 1 convolution in case the number of featuremap channels changes.",
  ". Architecture of Iresnet-50 backbone. Red-framed output is the one that is then passed to Feature predictor to predict Fpred": ". Additional editing results for StyleRes, SFE and check-point of SFE trained on a restricted set of editing directions Dxmall(see Ablation Study 4.4 and Appendix 10) on Celeba HQ. Thetechnique used to calculate the editing metric is described in 4.3.*However, since Celeba HQ does not have a rotation attribute, weused a different technique for this direction. We randomly dividedCeleba HQ into 2 equal parts, applied rotation to one of them andcalculated the FID between them to evaluate the realism of theedited images.",
  ". Masking": "To edit images, StyleFeatureEditor uses editing informationfrom the additional encoder E, which allows Inverter to fo-cus only on reconstruction features. However, this is alsoa disadvantage of our method: if wE would have artefactsduring editing, Feature Editor will mostly inherit these arte-facts. Therefore, it is important to choose E carefully. Un-fortunately, there is a more general problem: some direc-tions may not only change the attribute to which they refer,but also influence others.Typically, 2 types of artefacts appear. First, while edit-ing one attribute, another face attribute may be changed. Forexample, when adding glasses with a higher editing power,the mouth starts to open. The second type is that becauseE is only a w-latent encoder, it cannot reconstruct back-ground well and make it smooth, so during editing, such",
  "background could also be affected (for example directionsof bob cut and bowl cut hairstyles)": "Our method is able to fix the second type of such arte-facts. To do this, we propose to use an additional pre-trainedmodel M, which is able to predict the face mask of thesource image. The mask is scaled to a resolution of 64 64and applied to so that all features outside the face zonebecome zeros. As preserves positional information, thismeans that the part of the image outside the face zone is notedited. The results of this approach are shown in . However, such a simple technique could lead to artefactsin cases where editing should be applied outside the facezone, such as pose rotation or afro hairstyle. Therefore, weleft this technique as an optional feature.",
  ". Editings generaization": "In this section we provide additional results from the Abla-tion Study checkpoint Dsmall. This checkpoint was trainedon a restricted set of editing directions Dsmall (see AblationStudy 4.4). In we compare this checkpoint withStyleRes and our main model (SFE) on directions not pre-sented in Dsmall. Both of our models outperform StyleRes,preserving more image detail and providing comparableediting, while the restricted and unrestricted checkpoints areonly slightly different. This proves that our method is gen-eralisable to any direction, even those not represented in thetraining set. The numerical results in Tab. 4 also confirmthis.",
  ". Additional results": "In this section we provide additional visual examples of theStyleFeatureEditor. In we compare our methodwith StyleRes on out-of-domain MetaFaces dataset. Ourmethod is able to preserve the original image style, whileStyleRes makes it more realistic. In we show thework of our method in the face domain for several addi-tional editing directions. In we present an addi-tional comparison between StyleFeatureEditor and previousapproaches in the face domain, and in we presentmore results for the car domain.",
  "Synthetic 1Synthetic 2e4epSpStyleTransOursOurs masked": ". Examples of artefacts created by inaccurate editing directions. The first two columns represent synthetic images synthesizedfrom w (Inversion) or its edited version w (Corresponding Editing Direction), where w is obtained by randomly sampling z and passingit through the StyleGAN Mapping Network. Other columns represent inversion of real images (they are not represented here, but theyare visually indistinguishable from the inversion of Ours method) by different encoders and its edited versions. During Bobcut editing,the background starts to disappear (even for synthetic images); during Glasses editing, the mouth starts to open. The masking technique(last column, for more details see ) allows our method to avoid artefacts that appear during editing within the face zone (Bobcut,Glasses), but does not allow editing correctly while regions outside the face zone should be edited (Rotation)."
}