{
  "Abstract": "Malnutrition among newborns is a top public healthconcern in developing countries. Identification and subse-quent growth monitoring are key to successful interventions.However, this is challenging in rural communities wherehealth systems tend to be inaccessible and under-equipped,with poor adherence to protocol. Our goal is to equip healthworkers and public health systems with a solution for con-tactless newborn anthropometry in the community.We propose NurtureNet, a multi-task model that fusesvisual information (a video taken with a low-cost smart-phone) with tabular inputs to regress multiple anthropome-try estimates including weight, length, head circumference,and chest circumference. We show that visual proxy tasks ofsegmentation and keypoint prediction further improve per-formance. We establish the efficacy of the model throughseveral experiments and achieve a relative error of 3.9%and mean absolute error of 114.3 g for weight estimation.Model compression to 15 MB also allows offline deploy-ment to low-cost smartphones.",
  ". Introduction": "The first 4 weeks of life are critical for a newborns phys-iological and neurological development. Conditions suchas malnutrition and malabsorption during this phase lead toneonatal morbidities and in extreme cases even mortality.Thus, tracking a newborns growth over the first few weeksis an important public health responsibility .The weight of a newborn is an important statistic thatcaptures its overall health and well-being .Other measurements such as length, head circumference,and chest circumference are also useful for assessing growthor related developmental disorders . However, there",
  "d) Proposed Solution": ". Illustration contrasting traditional approaches (a-c) fornewborn anthropometry to what our proposed solution (d) enables.(a) A measuring tape is used to measure head and chest circum-ference. (b) An infantometer is used to capture length. (c) Thenewborn is suspended from a cloth and hooked up to a spring bal-ance to measure weight. (d) Our proposed solution replaces allthe above tasks and only requires the data collector to take a shortvideo with a low-cost smartphone. are several challenges in accurately capturing it in low- andmiddle-income countries (LMICs).As seen in (c), traditional methods for measuringweight in community settings use a spring balance (leastcount 100 g), from which the newborn is suspended. Thisresults in two main sources of error: (i) Human factors: thepanicked mother supporting her baby from the bottom; mo-tion of the spring balance as the newborn moves; difficultyin ascertaining the reading due to parallax; cultural chal-lenges such as reluctance towards outsiders handling ba-bies; and data handling malpractices leading to reporting",
  "arXiv:2405.05530v1 [cs.CV] 9 May 2024": "challenges. (ii) Instrument factors: old machines whosesprings are no longer taut result in positive errors (over-prediction); poorly calibrated or uncertified instruments;and even unavailability of the instrument due to supplychain issues. Similar challenges also apply to other anthro-pometric measurements such as newborn length or head andchest circumference.There are also logistical factors at play.Rural com-munities may be several miles away from health facilitieswith poor mobility options and limited inter-connectivity.Poverty further affects their ability to avail health facilities.Geographical barriers like rough terrain or rivers, and sea-sonal challenges such as extreme heat and heavy rain makeit challenging for both, families (with newborns) to reachhealth centers and for health workers (carrying heavy in-struments) to visit rural communities.Our goal is to develop a contactless, geo-tagged, easy-to-use solution that provides accurate anthropometry estimatesfor a newborn (age 0-42 days). We wish to leverage the pro-liferation of mobile phone adoption in rural areas of LMICsenabling AI technologies to improve the daily activities offrontline health workers while ensuring automatic reportingfor timely public health response and policy formulation.Our technology is suitable primarily in rural community set-tings over health facilities that may have good instrumenta-tion and well-trained staff. In such rural settings, there areabout 1 million health workers in our country, making thissolution amenable for large-scale impact.To facilitate widespread adoption, we make several de-sign choices. (i) We restrict ourselves to RGB videos cap-tured on low-cost mobile devices and forego complex depthsensors that may curtail adoption in rural areas. (ii) A ref-erence object is needed to provide a sense of metric scaleand we use easily available wooden rulers instead of chess-boards. (iii) We develop a simple protocol for capturing thevideo that enables viewing the newborn from multiple an-gles without the need for a dedicated video capture setupor specialized hardware. (iv) We restrict modeling to sim-ple architectures that can be compressed and deployed on alow-cost smartphone to enable offline inference.We present a CNN-based model that can ingest videoframes, aggregate their information, augment it with rele-vant tabular inputs, and estimate the newborns weight. Thisweight estimation model is extended to estimate severalanthropometric measurements through a multi-task setup(NurtureNet). We also propose proxy tasks such as babysegmentation and keypoint estimation that can assist themodel in focusing on the babys shape and pose respec-tively, resulting in improved performance. Notably, build-ing on state-of-the-art segmentation and keypoint estima-tion methods, we show that segmentation masks and key-points need not be annotated for each frame, and pseudo-labels can be used instead. Overall, our contributions can be summarized as follows:(i) We present a vision application for newborn anthropom-etry based on a standard RGB video captured with a low-cost smartphone that enables widespread adoption and im-pact in rural community settings. (ii) We propose and train amulti-task model, NurtureNet, that ingests the video and isassisted by tabular inputs to estimate several anthropometricmeasurements simultaneously. (iii) We show the benefits ofmodeling auxiliary vision tasks (e.g. segmentation and key-points) with pseudo-labels for training anthropometry mod-els; and (iv) We present thorough experiments to show theimpact of various modeling ideas, including evaluation ofcompressed models for offline phone deployment.",
  "Vision techniques have been used for various applications innewborn and infant healthcare, specially for anthropometry": "Computer vision for newborns is used in applicationssuch as heart rate monitoring , General Movement As-sessment (GMA) for early detection of cerebral palsy , postnatal age estimation , and even identificationbased on footprints . Related to anthropometry, thereis work on estimating birth weight using ultrasound videosprior to birth . An infants length is estimated usingeasy-to-detect stickers or markers , childrens (aged 2-5years) height based on point clouds , and even heightfor adults using multiple images and a large reference ob-ject . However, the above methods require specializedhardware or equipment and are therefore not applicable inlow resource areas. A different approach, Baby Naapp aims to use vision tools to eliminate the need for manualtranscription by capturing and analyzing images/videos ofdevices - spring balances for weight and measuring tapesfor circumference. Closest to our work, single image basedweight and height estimation is performed using CNN-based regression . However, their goal is different fromours as they aim to clinically estimate birth weight by cap-turing images in controlled settings (hospital). We showthat such an approach performs poorly in community set-tings where protocol adherence may be difficult. Pose estimation for newborns or infants is a populartask. Tracking body movements for newborns is critical,and early detection of abnormalities can prevent long-termhealth effects . Specifically, tracking baby pose overtime is useful to perform GMA , indicative of condi-tions such as cerebral palsy, autism spectrum disorder, andRett syndrome. Approaches for infant body pose estima-tion include handcrafted features such as histograms of 3Djoints or Random Ferns on depth images . Depth-only videos have also been used along with CNNs to di-rectly regress pose . Towards GMA, CNN-basedpose regression models have been developed that work with RGB or RGB-D data . Transformer modelsare making inroads in infant pose estimation with RGB im-ages or depth and pressure images . As a proxy topose estimation, body part segmentation may also be usedto understand infant movement . 3D parametric mod-els and pose estimates are also used to estimate the heightand weight of adults . For anthropometry, we show thatsegmentation and keypoint detection are good proxy tasksthat help the model focus on the newborn. 3D parametric models for adults and infants.TheSkinned Multi-Person Linear (SMPL) model has beenwildly popular in modeling the 3D shape of a human andhas stemmed a flurry of methods .However, adult human models cannot be used directly fornewborns, predominantly due to changes in body shape pro-portions . Hence, a Skinned Multi-Infant Linear(SMIL) model was proposed . Unfortunately, the modeldoes not fit well to our case as it is trained on European in-fants in the 2-4 months age range with a significantly higherweight distribution. In contrast, we are interested in anthro-pometry for newborns up to 42 days of age in LMICs. Tabular methods. Contrary to vision-based approaches,tabular data in the form of electronic health records (EHR)are used for infant and fetal weight prediction.These methods use maternal attributes, economic factors,and other aspects related to the gestation period as predic-tive features . However, it can be hard totrain or deploy models in regions where such records areinaccessible or not carefully curated. In our work, we usetabular data (birth weight and age) to augment and assist vi-sual features. Importantly, we show that our vision modelaugmented with tabular inputs is robust to errors in the tab-ular data that may be common due to misreporting.",
  ". Method": "We formulate anthropometry as a regression problem andintroduce an end-to-end pipeline to estimate the weight w,length l, head circumference h, and chest circumference cof an infant with age a days. Our model ingestsa video V and is augmented by tabular information such asthe birth weight w0 and the current age to regress:",
  "[w, l, h, c] = f(V, w0, a) ,(1)": "where are the models learnable parameters.The visual component of the model learns an implicitshape representation through the fusion of multiple framesthat capture the newborn from several angles (Sec. 3.1).Furthermore, we encourage the model to focus on the new-born by asking it to predict a segmentation mask and key-points in a bootstrapped multi-task setting (Sec. 3.2). Fi-nally, we show how the visual features can be augmented",
  ". Video-based Anthropometry": "How to record a video? Before addressing modeling, webriefly talk about how we record the video V. Predictingthe metric shape of an entity using a monocular camera of-ten requires a reference object. However, considering thelarge-scale rural use-case of our solution, we switch fromthe chessboard ( a classic and accurate reference objectused for camera calibration ) to a wooden ruler (length 30 cm) that is easily available to health workers. Thenewborn is laid on a bedsheet spread on a flat surface witha placed below the newborn (in the same plane). Whilewe remove all clothes for the newborn, no specific instruc-tions are provided for the bedsheet. The data collectors (orhealth workers) are trained to capture a video by startingfrom the top of the baby and making a smooth arc as illus-trated in . We filter videos by quality to ensure thenewborn and reference object are clearly visible for a ma-jority of the video (see Appendix B).Video-based weight estimation. Consider a video V =[f1, . . . , fT ] with T frames. We sample N < T framesfrom the video and pass them through a CNN backbone() to obtain frame-level representations xl = (fl) foreach selected frame fl, where xl Rd. Our approach com-bines the individual frame-level representations via a pool-ing function z = ({xl}Nl=1), and is followed by a Multi-Layer Perceptron (MLP) with one hidden layer to estimatethe weight w R1 in kg.We explore several pooling functions () ranging fromaverage and max-pooling to complex ones involving vanillaattention . Interestingly, given the nature of the problem,we find that a permutation invariant pooling (such as max-pooling) provides good results, as suggested by previousworks on 3D shape . We estimate the weight:",
  "Points": "Input video CNN backbone Segmentation task Multi-task heads Frame level features Tabular data . Overview of the proposed approach. Input video frames are sub-sampled and processed using a CNN and fused using a poolingmodule. Tabular data is normalized between and concatenated to this video representation. We use independent MLP regressors topredict anthropometry measures: weight, length, head circumference, and chest circumference. Additionally, we introduce two proxy tasksonly used during training: newborn pixel segmentation predicted through an FCN head and keypoint estimation through a simple MLP.",
  ". We observe high correlation (Pearson correlation coef-ficient) across anthropometric measurements on the training set:weight w, length l, head circumference h, chest circumference c": "baby is likely to be heavier, or a baby with a bigger chestmay be better off with respect to nutrition. We computethe Pearson correlation coefficients across pairs of anthro-pometric measurements on our training set. As seen in Ta-ble 1, weight is strongly correlated with length, head cir-cumference, and chest circumference.We attach additional task heads, similar to the MLPused for weight estimation, to the pooled video represen-tation. Specifically, we create three new task heads to pre-dict each measurement in cm: length l = MLPl(z), headcircumference h = MLPh(z), and chest circumferencec = MLPc(z). The model is trained jointly to optimize:",
  "Lanthro = wLw + lLl + hLh + cLc ,(4)": "where Ll, Lh, Lc are L1 losses applied to length, head cir-cumference, and chest circumference respectively; and ()are loss weight coefficients.Visual prediction tasks. While all the above tasks requireground-truth measurements to be collected at the time ofvideo capture, we now present visual tasks that can be anno-tated post data collection. In particular, we consider pixel-level newborn segmentation and keypoint estimation, withthe intent to encourage the model to learn representationsthat focus on the newborn.While annotating segmentation masks or keypoints for each frame of each video is possible, it is an expensive andtime-consuming affair. We circumvent this through a boot-strapped approach. For baby segmentation masks, we fine-tune a PointRend segmentation model on 500 videoswith 10 linearly spaced frames from each. Similarly, forkeypoints, we finetune HRNet on 1500 videos with20 linearly spaced frames from each. We apply both modelsto all video frames of the training set and use the predictionsas pseudo-labels during multi-task training.Our complete multi-task model has a segmentation head,a keypoint estimation head, and all the other anthropometricregression heads (see ). We use a Fully ConvolutionalNetwork (FCN) head to perform segmentation since wedo not need fine precision. For keypoint estimation, we usea simple 2-layer MLP that regresses the spatial coordinatesof keypoints from each frame embedding xl. The model istrained end-to-end through a combination of all losses:",
  "lLk(kl, kl) , (5)": "where ml and kl are the frame-level segmentation maskand keypoints generated by our multi-task model, ml and klare pseudo-labels for the mask and keypoints, Lm is Diceloss , Lk is L1 loss used for keypoints, and m and kare loss weights for masks and keypoints. During inference,we drop both the proxy heads.",
  ". Setup": "Dataset collection.The data has been collected by 28trained personnel from rural home settings across 2 geo-graphically diverse regions.They typically use Androidsmartphones with a 2-5 MP camera, with cost under $150.Our dataset consists of 3439 newborns that are visitedon average 3.75 times in the first 42 days of life. At eachvisit, we capture three videos with different reference ob-jects (only one is used for one experiment). While each visitis treated independently in the context of training and eval-uation, all visits of a newborn are in the same split. Ethicscommittee approvals were obtained prior to data collectionand the process itself involves taking informed consent, cap-turing videos, measuring the weight and anthropometry forthe newborn, and providing home-based care recommenda-tions if the newborn is not doing well. See Appendix A formore details.We split the dataset into train (80%), validation (10%)and test (10%), while ensuring all visits of a baby are in-cluded in the same split. shares the demographics ofthe dataset. The weight distribution across splits is matchedto the overall dataset distribution ( (Left)). Ground-truth.We obtain accurate ground-truth weightreadings by using a calibrated digital weighing machine(least count 10 g). The machine is robust to newborn move-ment as it stabilizes and locks the recorded value. A cal-ibrated infantometer is used to measure the length of thenewborn, and tape measures are used for measuring thehead and chest circumference.",
  "VideoCLIPMax139.9211.3CLIPMax129.0189.0": ". Impact of input, reference object (chessboard , andwooden ruler ), frame subsampling (SS), and pooling methods.MAE and BMAE are reported for weight estimation on the vali-dation set. The backbone is ResNet-50 pretrained either on Ima-geNet (IN) or CLIP . We see consistent performance im-provement across all modifications video-based models, CLIP asbackbone, and including sub-sampling. Evaluation metrics. We adopt the Mean Absolute Error(MAE) as our primary evaluation metric. We also reportBalanced MAE (BMAE) that averages MAE across eachbin of interest as the weight distribution is non-uniform. Fi-nally, as an error of 200 g for a 2 kg newborn is worse thanthat for a 4 kg baby, we report relative errors |w wgt|/wgtto highlight errors for newborns with lower weights. Implementation details.We fine-tune a ResNet-50CNN to produce d=2048 dimensional representationsxl for each frame fl. We choose N=25 frames from a videowith an average duration of 12 s. The selected frames arepadded to form a square and resized to 2242243. Aug-mentations such as vertical / horizontal flips, translations,and color jitter are applied during training.We use theAdam optimizer and train the model for 200 epochs.When not mentioned otherwise, the initial learning rate is105. We use a StepLR scheduler wherein the learning ratesteps down by a factor of 2 every 50 epochs.",
  ". Model Ablations": "Frame- vs. Video-based weight estimation. In the video-based weight estimation approach (Sec. 3.1), we fuse Nrepresentations early on in the network.An alternativeframe-based approach would be to use individual frames toobtain weight estimates and average over multiple framesduring inference (similar to ). A constant learning rateof 105 works best for frame-based models. shows that video-based models outperformframe-based by a large margin: reduce MAE by 66 g. Inparticular, we also observe that max-pooling outperformsaverage pooling and vanilla self-attention (SA) .A key hyperparameter for video-based models is thenumber of sampled frames N. (Middle) shows thatthe weight MAE reduces dramatically with increasing N,reaches the minimum around N=25 and slightly increases Weight (kg) Number of newborns meanmedian Number of frames MAE (g) Noise range (g) MAE (g) NurtureNetW-NurtureNet . Left: Weight distribution for the training set. Middle: Impact of varying the number of frames N during evaluation on thevalidation set. For training, we use N=25. The model used here is NurtureNet, that augments video information with tabular data and usesproxy tasks of baby segmentation mask and keypoints. Right: Effect on weight MAE on the validation set when adding noise sampledfrom a uniform distribution to the birth weight for NurtureNet models.",
  "thereafter. We choose N=25 for the rest of the experiments": "Impact of ResNet-50 parameters. We experiment with aResNet-50 encoder pretrained on the ImageNet (IN) datasetand one pretrained using the Contrastive Language-ImagePretraining (CLIP) technique . For IN models, an initiallearning rate of 104 stepped down by a factor of 2 every 30epochs works best. As seen in , CLIP-based CNNinitialization results in better performance, and this encoderis used in all further experiments. Impact of frame sampling. The frame selection processinfluences the representation and the weight estimate. To re-duce this dependency, we introduce subsampling as an aug-mentation during training. Specifically, we randomly pickN =40 frames, and subsample 10 subsets of N=25 frameswith replacement. By requiring the model to produce thesame estimate (Eq. 2) across these subsets, we make themodel less sensitive to frame selection. During inference,we do not use subsampling. shows that subsam-pling typically results in a modest improvement of 5.5 g.We employ this technique for further experiments. Impact of reference object. shows that using , astandard object for camera calibration, as a reference objectover the gives a 10.9 g improvement on MAE. However,for wider adoption and given s availability in resourceconstrained areas, we restrict our solution to using a . Impact of multi-task approaches. shows theresults of combining various tasks, and the correspond-ing MAE. As a simple baseline, row 0 displays perfor-mance for using the mean value of the training set asthe prediction.In all experiments, the loss coefficientsare set to w=5.0, l=0.1, h=0.1, c=0.1, m=3.0, andk=100.0, to scale the importance of various losses. Rows1-4 show the results when performing each anthropome-try estimation task independently, indicating that they faremuch better to not using any model. Row 5 shows the ef-fect of including proxy visual tasks, which leads to a per-formance gain of 16.5 g. Row 6, compared to rows 1-4,",
  "NurtureNetW M K113.7171.84 NurtureNetW L H C M K115.6181.3": ". W-NurtureNet concatenates the visual representation tothe tabular inputs to regress weight to show improved performance(validation set).NurtureNet is the multi-task equivalent.TheTasks column shows the set of tasks on which the model is trained. shows negligible change. This indicates that we can useone multi-task model that estimates all anthropometric mea-sures with one video. Finally, row 7, combines all tasksshowing marked improvement across all measurements.",
  ". Hand-crafted models perform poorly on weight regres-sion (validation set) compared to proposed models": "ble 5 shows that this model achieves a competitive MAE of202.5 g. However, this model is not useful in practice asit predicts the same weight for all babies with a given birthweight after a days, i.e. this model cannot predict deviationsfrom the mean growth for the population (training set).Augmenting visual information with tabular inputs. Weaugment our visual representation z by concatenating themwith normalized tabular inputs and send them for-ward to regress weight (W-NurtureNet, Sec. 3.3). Row 2of shows that W-NurtureNet achieves an impres-sive 12.2 g improvement in MAE from 139.9 g to 127.7 g.Rows 3 and 4 show results on training the multi-task Nur-tureNet. While row 3 shows a 14 g improvement in MAEon using auxiliary tasks, row 4 is a unified model that canestimate all anthropometry measurements and shows a 12 gimprovement in weight MAE.Effect of errors in recorded birth weight. To deploy mod-els in rural settings, an important factor to consider is theerroneous nature of tabular inputs. We simulate these errorsas w0 = w0 + , where the noise is sampled from a uni-form distribution U(q, q) and q corresponds to the maxi-mum deviation in kg. (Right) shows that our modelsare quite robust to noisy inputs. In fact, when q=0.5 kg,W-NurtureNet achieves an MAE of 136.5 g (worse thanwhen q=0 by 9 g), still better than our video-based modelat 139.9 g.NurtureNet is more robust to noise than W-NurtureNet and results in a 5 g increase in MAE. Finally,the Tab-Only model is highly sensitive to errors in birthweight and results in an MAE of 307.4 g (up by 105 g).",
  ". Baselines, Results Summary": "We now present and evaluate a few baselines for weight es-timation: (i) A nave approach is to predict the mean of thetraining set. This acts like the upper bound of the error forany model. (ii) A second approach uses structural infor-mation that can be extracted from predicted segmentationmasks of the newborn and the ruler. We extract hand-craftedrepresentations in the form of Hu image moments orregion features . (iii) We also evaluate the Histogramof Oriented Gradients (HOG) features that are popu-lar in classical computer vision literature. RBF-kernel Sup-port Vector Regressors (SVR) are used to obtain an-thropometry estimates from all three representations.Baseline ablations. shows the MAE for weightestimation for all three feature representations and a combi-nation. Regionprops features, together with Hu moments",
  "All1291139.0223.24.8114.3179.83.9": ". Results sliced by weight bins on the test set. Metrics:E80 corresponds to the 80th percentile absolute error and indicatesthat 80% of the samples have an error less than this value. % Relcorresponds to mean absolute relative error. NurtureNet improvesover the video-based model across all weight bins. show best performance for weight estimation (393.0 g).Here as well, the reference object is useful and computingfeatures from both the baby and ruler regions improves per-formance. Appendix C presents additional details on fea-tures and ablations. However, performance of all baselinesis far from proposed deep-learning based approaches. Comparing best approaches.We summarize the keymethods on the test set in . In particular, we observethat video-based models (139.0 g) achieve a large improve-ment over the best hand-crafted representations (390.1 g)and single frame based approach (214.0 g).Aug-menting the video-only models with tabular data improvesMAE to 126.4 g. Finally, NurtureNet results in best per-formance, achieving an MAE of 114.3 g, while presentinga unified model for newborn anthropometry. (Un-compressed) presents results on all measures of NurtureNet.",
  ". Analysis and Discussion": "Results sliced by weight bins are presented in . Nur-tureNet brings large improvements in MAE from 15 to 80 gacross all bins, but particularly for the low 1 to 2 kg and high4 to 5 kg bins. Inclusion of the multi-task approach and thetabular inputs also reduces relative and 80th percentile er-rors across all bins. Low errors in the 1 to 2.5 kg bins areespecially important to identify underweight newborns. Predictions vs. Ground-truth Another way to assess theperformance of our proposed model is a scatter plot of themodels predictions against the ground-truth. showsthe scatter plot for NurtureNet on the test set. While we Ground-truth weight, wgt (kg) Predicted weight, w (kg) R2 = 0.94PCC = 0.97 y = x Best fit line",
  ". Performance of the uncompressed NurtureNet, pruned,and quantized models on the test set. We are able to reduce modelsby 8 with minimal loss in weight estimation performance": "observe that the best fit line is close to the y=x diagonal,our model tends to slightly over-predict for low weight andunder-predict for higher weight newborns. This can be at-tributed to the dataset imbalance ( (Left)).Appendix D presents tSNE plots by weight, age, and datacollector; and Bland-Altman plot for weight measurements. Model compression.Training our model requires oneV100 GPU with 32 GB memory. However, our goal is todeploy the NurtureNet model on a low-cost smartphone thatcan be used by health workers in underserved geographies.Furthermore, the lack of internet coverage necessitates adrastic reduction in the memory and computational foot-print of the model to enable on-device and offline inference.We prune NurtureNet using the NNI library . Specifi-cally, we use the L1NormPruner twice to discard half theoutput channels having the smallest L1 norm of weights ineach iteration, effectively reducing the size of the computerequirement by 75%. Further, we perform static quantiza-tion, converting the FP32 weights and activations to INT8.The result is a model that is 8 smaller and 4 fasterwith an acceptable deterioration in performance up to 3.5 gMAE. shows that compression leads to a negligibleperformance drop for weight (W), head circumference (H),and chest circumference (C), but an acceptable increase inlength (L) error.",
  "NurtureNet vs. Conventional practice. We conduct a pre-": "liminary and independent field study to analyze the errorsin weight measurements made through conventional prac-tices and compare them against NurtureNet. Weight read-ings taken by health workers using spring balances are com-pared against calibrated digital weighing machines used forground-truth weight measurements. We observe an MAE of183 g (N=92) for conventional methods indicating the chal-lenges of recording such data in rural community settings.Note, that this result is biased towards being lower as thehealth workers knew that they were being monitored andcan only be expected to be worse in real scenarios. Nur-tureNet achieves a lower MAE at 114.3 g (N=1295), indi-cating the field-readiness of our approach. Limitations. While AI models can provide meaningful ac-curacy in many cases, they cannot be perfect on all samples,particularly when it comes to complex problems like esti-mating the weight of a baby. This may be due to variousfactors such as newborn clothing, lighting conditions, en-vironmental conditions, camera angles, the position of thebaby relative to the camera, or the babys movements. Themodel may also struggle to accurately estimate the weightof babies with certain physical characteristics (e.g. missinglimbs) or rare medical conditions that affect growth.",
  ". Conclusion": "We present a vision system for newborn anthropometryfrom a short video taken with a low-cost smartphone. Ourproposed approach computes a video representation andaugments it with tabular data to obtain weight estimates. Weextend this model through multi-task training to simulta-neously estimate other anthropometric measurements suchas the length, head circumference, and chest circumference(NurtureNet). A proxy task of predicting baby segmenta-tion masks and keypoints further improves the weight esti-mation performance. Using pruning and quantization, wecompress NurtureNet to 15 MB, allowing offline inferenceand deployment on low-cost smartphones.This solution is envisioned as a public health screeningtool and is currently not intended for diagnostic or clin-ical settings where good anthropometric instruments andtrained personnel are available. Such a tool provides a con-venient, geo-tagged, and contactless way for health workersand public health systems to monitor the growth and de-velopment of newborns, enabling targeted interventions todrive better health outcomes. Acknowledgments. We thank the Bill & Melinda Gates Founda-tion (BMGF) and the Fondation Botnar for supporting this work.We thank Niloufer Hospital at Hyderabad for initial data collec-tion. We are grateful to SEWA Rural at Gujarat and PGIMER atChandigarh for collecting data in community settings. We thankall members of WIAI who were involved for their contributions. Hamid Abbasi, Sarah Mollet, Sian Williams, Lilian Lim,Malcolm Battin, Thor F Besier, and Angus McMorland.Deep-Learning for Automated Markerless Tracking of In-fants General Movements. bioRxiv, 2022. 3",
  "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural Machine Translation by Jointly Learning to Align andTranslate. In International Conference on Learning Repre-sentations (ICLR), 2015. 3, 5": "S K Bhargava, S Ramji, Arun Kumar, Man Mohan, JasbirMarwah, and H P Sachdev. Mid-arm and chest circumfer-ences at birth as predictors of low birth weight and neonatalmortality in the community. British Medical Journal, 291:16171619, 1985. 1 Hannah Blencowe, Julia Krasevec, Mercedes De Onis,Robert E Black, Xiaoyi An, Gretchen A Stevens, ElaineBorghi, Chika Hayashi, Diana Estevez, Luca Cegolon, et al.National, regional, and worldwide estimates of low birth-weight in 2015, with trends from 2000: a systematic anal-ysis. The Lancet Global Health, 7(7):e849e860, 2019. 1",
  "Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a libraryfor support vector machines. ACM transactions on intelligentsystems and technology (TIST), 2(3):127, 2011. 14": "Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee.Pose2mesh: Graph convolutional network for 3d human poseand mesh recovery from a 2d human pose. In European Con-ference on Computer Vision (ECCV). Springer, 2020. 3 Parul Christian, Sun Eun Lee, Moira Donahue Angel,Linda S Adair, Shams E Arifeen, Per Ashorn, Fernando CBarros, Caroline HD Fall, Wafaie W Fawzi, Wei Hao,et al.Risk of childhood undernutrition related to small-for-gestational age and preterm birth in low-and middle-income countries. International journal of epidemiology, 42(5):13401355, 2013. 1",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep Residual Learning for Image Recognition. In Confer-ence on Computer Vision and Pattern Recognition (CVPR),2016. 5": "Nikolas Hesse, Gregor Stachowiak, Timo Breuer, andMichael Arens. Estimating Body Pose of Infants in DepthImages Using Random Ferns. In International Conferenceon Computer Vision Workshops (ICCVW), 2015. 2 Nikolas Hesse, Sergi Pujades, Javier Romero, Michael J.Black, Christoph Bodensteiner, Michael Arens, Ulrich G.Hofmann, Uta Tacke, Mijna Hadders-Algra, Raphael Wein-berger, Wolfgang Muller-Felber, and A. Sebastian Schroeder.Learning an Infant Body Model from RGB-D Data for Accu-rate Full Body Motion Analysis. In Medical Image Comput-ing and Computer Assisted Intervention (MICCAI). Springer,2018. 3",
  "Ming-Kuei Hu. Visual pattern recognition by moment invari-ants. IRE transactions on information theory, 8(2):179187,1962. 7": "Xiaofei Huang, Nihang Fu, Shuangjun Liu, and Sarah Os-tadabbas. Invariant representation learning for infant poseestimation with small data. In 2021 16th IEEE InternationalConference on Automatic Face and Gesture Recognition (FG2021), 2021. 3 Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, XiaoweiZhou, and Kostas Daniilidis.Coherent reconstruction ofmultiple humans from a single image.In Conference onComputer Vision and Pattern Recognition (CVPR), 2020. 3 Francois R Jornayvaz, Peter Vollenweider, Murielle Bochud,Vincent Mooser, Gerard Waeber, and Pedro Marques-Vidal.Low birth weight leads to obesity, diabetes and increased lep-tin levels in adults: the CoLaus study. Cardiovascular dia-betology, 15(1):110, 2016. 1",
  "Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman,and Kostas Daniilidis.Probabilistic modeling for humanmesh recovery. In International Conference on ComputerVision (ICCV), 2021. 3": "Stefan Kuhle, Bryan Maguire, Hongqun Zhang, DavidHamilton, Alexander C Allen, KS Joseph, and Victoria MAllen. Comparison of logistic regression with machine learn-ing methods for the prediction of fetal growth abnormalities:a retrospective cohort study. BMC pregnancy and childbirth,18(1), 2018. 3 Daniel G Kyrollos, Kim Greenwood, JoAnn Harrold, andJames R Green. Transfer Learning Approaches for NeonateHead Localization from Pressure Images. In 2022 IEEE In-ternational Symposium on Medical Measurements and Ap-plications (MeMeA), 2022. 3 Daniel G. Kyrollos, Anthony Fuller, Kim Greenwood, JoAnnHarrold, and James R. Green. Under the Cover Infant PoseEstimation using Multimodal Data. IEEE Transactions onInstrumentation and Measurement, 2023. 3",
  "Matthew Loper, Naureen Mahmood, Javier Romero, GerardPons-Moll, and Michael J Black. SMPL: A skinned multi-person linear model. ACM Transactions on Graphics (TOG),34(6), 2015. 3": "Patrcia Loreto, Hugo Peixoto, Antonio Abelha, and JoseMachado. Predicting low birth weight babies through datamining. In New Knowledge in Information Systems and Tech-nologies: Volume 3. Springer, 2019. 3 Yu Lu, Xi Zhang, Xianghua Fu, Fangxiong Chen, andKelvin KL Wong. Ensemble machine learning for estimatingfetal weight at varying gestational age. In Association for theAdvancement of Artificial Intelligence (AAAI), 2019. 3 Kevin D McCay, Edmond SL Ho, Claire Marcroft, andNicholas D Embleton. Establishing pose based features us-ing histograms for the detection of abnormal infant move-ments. In 2019 41st Annual International Conference of theIEEE Engineering in Medicine and Biology Society (EMBC).IEEE, 2019. 2 L Meinecke, N Breitbach-Faller, C Bartz, R Damen, G Rau,and C Disselhorst-Klug. Movement analysis in the early de-tection of newborns at risk for developing spasticity due toinfantile cerebral palsy. Human Movement Science, 25(2),2006. 2 Microsoft. Neural Network Intelligence, 2021. 8 Sara Moccia, Lucia Migliorelli, Virgilio Carnielli, andEmanuele Frontoni. Preterm infants pose estimation withspatio-temporal features. IEEE Transactions on BiomedicalEngineering, 67(8), 2019. 2 Haomiao Ni, Yuan Xue, Liya Ma, Qian Zhang, Xiaoye Li,and Sharon X Huang.Semi-supervised body parsing andpose estimation for enhancing infant general movement as-sessment. Medical Image Analysis, 83, 2023. 3 World Health Organization and World Health Organiza-tion. Nutrition for Health.WHO child growth standards:growth velocity based on weight, length and head circum-ference: methods and development. World Health Organiza-tion, 2009. 1 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort,Vincent Michel, Bertrand Thirion, Olivier Grisel, MathieuBlondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg,et al. Scikit-learn: Machine learning in python. the Journalof machine Learning research, 12:28252830, 2011. 14 Szymon Potka, Michal K Grzeszczyk, Robert Brawura-Biskupski-Samaha, Pawe Gutaj, Micha Lipa, TomaszTrzcinski, and Arkadiusz Sitek. BabyNet: Residual Trans-former Module for Birth Weight Prediction on Fetal Ultra-sound Video. In Medical Image Computing and ComputerAssisted Intervention (MICCAI), 2022. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International Conference on Machine Learning(ICML), 2021. 5, 6 Lorenzo Scalise, Natascia Bernacchia, Ilaria Ercoli, andPaolo Marchionni. Heart rate measurement in neonatal pa-tients using a webcamera. In 2012 IEEE International Sym-posium on Medical Measurements and Applications Pro-ceedings. IEEE, 2012. 2 A. Sebastian Schroeder, Raphael Weinberger Nikolas Hesse,Uta Tacke, Lucia Gerstl, Anne Hilgendorff, Florian Heinen,Michael Arens, Linze J. Dijkstra, Sergi Pujades Rocamora,Michael J. Black,Christoph Bodensteiner,and MijnaHadders-Algra. General Movement Assessment from videosof computed 3D infant body models is equally effective com-pared to conventional RGB video rating. Early Human De-velopment, 144:104967, 2020. 2 Giuseppa Sciortino, Giovanni Maria Farinella, SebastianoBattiato, Marco Leo, and Cosimo Distante. On the estima-tion of childrens poses. In Image Analysis and Processing-ICIAP 2017: 19th International Conference. Springer, 2017.3 D Senthilkumar and S Paulraj. Prediction of low birth weightinfants and its risk factors using data mining techniques. InProceedings of the 2015 international conference on indus-trial engineering and operations management, 2015. 3",
  "Alex J Smola and Bernhard Scholkopf. A Tutorial on Sup-port Vector Regression. Statistics and Computing, 14, 2004.7": "Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas,Jitendra Malik, and Silvio Savarese. Which tasks should belearned together in multi-task learning?In InternationalConference on Machine Learning (ICML). PMLR, 2020. 3 Hang Su, Subhransu Maji, Evangelos Kalogerakis, and ErikLearned-Miller. Multi-view convolutional neural networksfor 3d shape recognition. In Proceedings of the IEEE inter-national conference on computer vision, 2015. 3 Carole H. Sudre, Wenqi Li, Tom Vercauteren, SebastienOurselin, and M. Jorge Cardoso. Generalised Dice overlapas a deep learning loss function for highly unbalanced seg-mentations. In International Workshop on Deep Learning inMedical Image Analysis, 2017. 4",
  "Neerja Thakkar, Georgios Pavlakos, and Hany Farid. The re-liability of forensic body-shape identification. In PCVPRW,2022. 3": "Mercedes Torres Torres, Michel Valstar, Caroline Henry, Ca-role Ward, and Don Sharkey. Postnatal gestational age esti-mation of newborns using small sample deep learning. Imageand vision computing, 83, 2019. 2 Anusua Trivedi, Mohit Jain, Nikhil Kumar Gupta, MarkusHinsche,Prashant Singh,Markus Matiaschek,TristanBehrens, Mirco Militeri, Cameron Birge, Shivangi Kaushik,et al. Height estimation of children under five years usingdepth images. In 2021 43rd Annual International Confer-ence of the IEEE Engineering in Medicine & Biology Society(EMBC), 2021. 2",
  "A. Data Collection Process": "As described in the main paper, each baby is visited multi-ple times in the first 6 weeks of life. The data collector visitsand captures videos of the baby around the 3, 7, 14, 21, 28,and 42 days after birth to match the health programs rec-ommended schedule. However, due to field and logisticalchallenges, we do encourage the data collector to visit thenewborn within a 2 day window. This gives us an averageof 3.75 visits per newborn.The data collectors are trained to capture a video by start-ing from the top of the baby and making a smooth arc asillustrated in .Enrolment. At the first visit, the baby is enrolled using acustom-developed mobile application to ensure data secu-rity. The application generates automatic reminders for thedata collector to do follow-up visits. Prior to enrolment, thedata collectors explain the project to the parents and obtaintheir informed consent in the local language. During en-rolment, we capture basic information such as the mothersand newborns name, address, sex, mode of delivery, dateof birth, and weight at birth.At each visit the data collectors are trained to adhere to thefollowing protocol:1. After greeting the parents, the first task is to setup thevideo capture environment: find a flat, well-lit area inthe house, arrange for a bedsheet on which the baby willbe placed, and prepare the reference objects. 2. Next, the digital weighing machine is prepared for mea-suring ground-truth.The baby is brought in and itsclothes are removed.The newborn is successivelyplaced three times on the weighing machine and readingsare noted for each measurement. The whole process iscaptured in a video to ensure adherence to protocol (seeSec. B.3). As indicated in the main paper, we ensurehigh quality ground-truth (10 g least count) by using acustom-built, calibrated, and certified weighing machinethat averages weight over time. 3. We then capture three videos of the baby with differentreference object conditions: no reference object, chess-board (), and the wooden ruler (). For each video,the data collector places the appropriate reference objectand makes an arc around the baby as indicated in of the main paper. We attempt to capture the newbornsshape by making a steady arc around it while ensuring",
  ". Finally, an oral health assessment is performed byquizzing the parents on aspects such as feeding status,breathing rate, appearance, muscle tone, and dischargefrom the eyes or umbilicus": "6. In case of any concerns or anomalous responses, the datacollectors counsel the parents on potential recourses toaddress them.The data is automatically synced to secure cloud storagewhen the mobile device has access to the internet (note thatrural areas where data collection happens may not neces-sarily have access to the internet) and de-identified beforesharing for further processing.",
  "B. Data Validation Criteria": "We are interested in understanding the data quality throughvarious annotations related to the environment, the use ofappropriate reference object, clothing artifacts on the new-born, and ground-truth. We obtained videos of 16,612 visitsacross two geographically diverse regions. A team of 5 an-notators was trained on the prescribed protocol, and 2 anno-tators independently annotated each video. After validation,we were left with 12,901 usable visits. enlists thecriteria used to discard visits. This validation protocol in-volves three sequential steps as described in the subsectionsbelow.",
  ". Number of visits discarded based on all the data valida-tion criteria": "leads to diverse variations in the visual settings across thecaptured videos and props up classic vision challenges re-lated to poor lighting; bedsheets of different colors, shapes,and textures; and other challenges related to data collection,such as the lack of a video capture setup potentially leadingto motion blur and inconsistency in recorded videos. Thisis far from clinical settings (e.g. a hospital) where all new-borns may be brought to the same room or even the samebed and captured by the same data collector (nurse) withthe same device, making the vision problem easier to solve. Our first check ensures that each video has a newbornwith the correct reference object. Note that for each visitwe collect three videos with different reference object con-ditions: no reference object, with a , and with a . Dueto the simple nature of this task, we use unanimity to ensurethat the annotations are correct. We remove 53 visits afterthis check leaving us with 16,559 visits that are passed onto the next stage.",
  "B.2. Video Quality Validation": "We validate the quality of the videos with the aid of a ques-tionnaire to determine the quality of data collection and en-sure adherence to protocol. The questions are: (i) is thenewborn wearing clothes?(ii) is the newborn cropped?(iii) is the reference object cropped? (iv) is there good andsufficient light? (v) is the video blurry? (vi) is the new-born and reference object on the same plane? (vii) are thereother humans visible in the video? (viii) is the arc smoothor jerky? and (ix) is the newborn captured well from bothleft and right side angles (i.e. how complete is the arc)? We accept partial failures (e.g. newborn cropped for 1to 3 s) in most of the above criteria and observe that com-plete failures (correspondingly, newborn cropped for 3 s)are quite rare. We plan to use the annotations for futureanalysis and potential studies in error attribution. As weare interested in building a robust anthropometry estimationsystem, we realize that all videos will not be captured wellduring deployment. We discard 441 visits in this processand are left with 16,118 visits.",
  "B.3. Ground-truth Weight Validation": "The third and final validation check concerns the ground-truth weight.It involves annotators watching the videorecording in which the ground-truth weight of the newbornis captured and recording the observed weight. Recall thatthe newborn is placed thrice on the weighing machine lead-ing to a total of 6 weight readings across two annotators.Visits that have 4 of 6 weight readings in agreement aredirectly accepted. Alternatively, if no reading is more than50 g away from the mean of all 6 readings, we accept thevisit. All other visits are passed through the criteria be-low. We discard the visits if any of the following is true:(i) the newborn is not visible on the weighing machine;(ii) newborn is wearing clothes while being placed on themachine; (iii) readings are not stable with two or more thantwo readings varying beyond 50 g; and (iv) a large chunk isattributed to other problems such as the weighing machinethat may not be placed on a proper flat surface or is not vis-ible in the video due to occlusions or lack of focus or glare,someones hand touching the weighing pan, the newbornslimbs are touching a nearby wall, etc. Visits that do not failany of the 4 rejection criteria are also accepted. shows the counts of the rejection criteria where we discardthe visits.The stringent ground-truth weight annotation protocolalong with our weighing machine with a hold function al-lows us to capture highly accurate values of the ground-truthweight. We removed 3200 visits in this process and are leftwith 12,918 visits. 17 more visits are finally removed sincethey have short videos (less than 40 frames as required forsubsampling). Finally, 12,901 videos are used as part of ourexperiments.",
  ". Performance of hand-crafted features on weight estima-tion with the validation set": "image. The hand-crafted features across 25 frames for agiven video are concatenated together to create the finalfeature vector ().We evaluate three regression models:ordinary leastsquares based Linear Regression (LR), Multi-Layer Percep-tron (MLP), and kernel Support Vector Regressor (SVR).For LR and MLP, we scale the Hu moments with a log trans-formation to reduce the variability in feature values. The z-score and minmax feature scalers have been experimentedwith.For the MLP, we use the scikit-learn implementation, and for the kernel SVR, we use theLIBSVM implementation. For MLP, we use one hid-",
  "D.1. Metric: Balanced MAE": "The standard metric Mean Absolute Error (MAE) does nottake into account the label distribution (e.g. majority of thenewborns in our dataset have weight between 2.5 to 3.5 kg).As our goal is related to identifying malnutrition in new-borns, it is important to get accurate predictions and metricscorresponding to low birth weight newborns. Thus, we usea more equitable and fair metric: Balanced MAE (BMAE),defined as the average of MAE across multiple weight bins.In our experiments, we use bins of 500 g granularity. Basedon the weight distribution in the dataset (see (Left) inthe main paper), we set the lower limit to 1 kg and the upperlimit to 5.5 kg. The bins are thus defined as follows:",
  "D.2. t-SNE plots": "shows t-SNE embeddings for videos from thevalidation set. We use the simple video-based model for thisanalysis to visualize the feature space to capture the varia-tion of weight without the influence of multiple tasks or tab-ular information. (i) In the left plot, colors indicate the trueweight of the newborn in kg. We see a smooth color dis-tribution across the embeddings indicating that the modelhas optimized to a good representation space. (ii) The cen-ter plot shows the age of the newborn at the time of datacollection in days. We observe a smooth transition here aswell. However, there are some higher age babies at the topright and vice versa. (iii) In the right plot, we color the dotsby the data collector. A good mix is observed which is de-sirable to ensure invariance across data collectors.",
  "D.3. Bland-Altman Plot": "We perform a Bland-Altman analysis on the test set to as-sess the agreement between the predictions of NurtureNetand the ground-truth weight measurements (). Theanalysis shows negligible bias of 1.4 g indicating a strongagreement of the weight estimates against the ground-truths. Notably the plot largely exhibits homoscedasticity,signifying consistent variability across a significant range ofweights. The 80% Limits of Agreement (LoA) is 190 gwhich makes the solution acceptable for deployment basedon inputs from public health experts."
}