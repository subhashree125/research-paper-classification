{
  "{hyunys21, khsohn}@yonsei.ac.kr {hoseok.do, soohyun1.kim}@lge.com": "AbstractWe present a new multi-modal face image generationmethod that converts a text prompt and a visual input, suchas a semantic mask or scribble map, into a photo-realisticface image. To do this, we combine the strengths of Gen-erative Adversarial networks (GANs) and diffusion models(DMs) by employing the multi-modal features in the DMinto the latent space of the pre-trained GANs. We present asimple mapping and a style modulation network to link twomodels and convert meaningful representations in featuremaps and attention maps into latent codes. With GAN inver-sion, the estimated latent codes can be used to generate 2Dor 3D-aware facial images. We further present a multi-steptraining strategy that reflects textual and structural repre-sentations into the generated image. Our proposed networkproduces realistic 2D, multi-view, and stylized face images,which align well with inputs.We validate our methodby using pre-trained 2D and 3D GANs, and our resultsoutperform existing methods. Our project page is avail-able at 1. IntroductionIn recent years, multi-modal image generation has achievedremarkable success, driven by the advancements in Genera-tive Adversarial Networks (GANs) and diffusion mod-els (DMs) . Facial image processing has becomea popular application for a variety of tasks, including faceimage generation , face editing , and style transfer . Many tasks typically utilizethe pre-trained StyleGAN , which can generate real-istic facial images and edit facial attributes by manipulatingthe latent space using GAN inversion . In thesetasks, using multiple modalities as conditions is becoming apopular approach, which improves the users controllabilityin generating realistic face images. However, existing GAN",
  "D face image generation": "3D-aware face image generation Face style transfer The woman has bangs, brown hair. She is smiling. Greek statue silver hair Elf Cartoon style Overview of our method The chubby man has receding hairline, eyeglasses, gray hair, and double chin. Watercolor painting GANOursDiffusion She has blond hair, straight hair, and wears heavy makeup. Visual condition Text condition . We present a method to map the diffusion features to thelatent space of a pre-trained GAN, which enables diverse tasks inmulti-modal face image generation and style transfer. Our methodcan be applied to 2D and 3D-aware face image generation. inversion methods have poor alignment with inputsas they neglect the correlation between multi-modal inputs.They struggle to map the different modalities into the latentspace of the pre-trained GAN, such as by mixing the latentcodes or optimizing the latent code converted from a givenimage according to the input text.Recently, DMs have increased attention in multi-modalimage generation thanks to the stability of training andthe flexibility of using multiple modalities as conditions.DMs can control the multiple modalities andrender diverse images by manipulating the latent or atten-tion features across the time steps. However, existing text-to-image DMs rely on an autoencoder and text encoder,such as CLIP , trained on unstructured datasets col-lected from the web that may lead to unrealistic",
  "arXiv:2405.04356v1 [cs.CV] 7 May 2024": "image generation.Moreover, some approaches address multi-modal faceimage generation in a 3D domain. In GAN inversion , multi-view images can be easily acquired by manip-ulating the latent code with pre-trained 3D GANs. WhileDMs are inefficient in learning 3D representation, whichhas the challenge to generate multi-view images directlydue to the lack of 3D ground-truth (GT) data for train-ing . They can be used as a tool to acquire trainingdatasets for 3D-aware image generation .In this paper, we present a versatile face generativemodel that uses text and visual inputs. We propose an ap-proach that takes the strengths of DMs and GAN and gener-ates photo-realistic images with flexible control over facialattributes, which can be adapted to 2D and 3D domains, asillustrated in . Our method employs a latent map-ping strategy that maps the diffusion features into the la-tent space of a pre-trained GAN using multi-denoising steplearning, producing the latent code that encodes the detailsof text prompts and visual inputs.In summary, our main contributions are:",
  ". Related Work": "2.1. GAN InversionGAN inversion approaches have gained significant popu-larity in the face image generation taskusing the pre-trained 2D GAN, such as StyleGAN .This method has been extended to 3D-aware im-age generation by integrating 3D GANs, suchas EG3D .GAN inversion can be categorized intolearning-based, optimization-based, and hybrid methods.Optimization-based methods estimate the latentcode by minimizing the difference between an output andan input image. Learning-based methods train an en-coder that maps an input image into the latent space of thepre-trained GAN. Hybrid methods combine thesetwo methods, producing an initial latent code and then re-fining it with additional optimizations. Our work employsa learning-based GAN inversion, where a DM serves as theencoder. We produce latent codes by leveraging semantic",
  "features in the denoising U-Net, which can generate imageswith controlled facial attributes": "2.2. Diffusion Model for Image GenerationMany studies have introduced text-to-image diffusion mod-els that generate images by encoding multi-modal inputs, such as text and image, into latent featuresvia foundation models and mapping them to the fea-tures of denoising U-Net via an attention mechanism. Con-trolNet performs image generation by incorporatingvarious visual conditions (e.g., semantic mask, scribbles,edges) and text prompts.Image editing models usingDMs have exhibited excellent perfor-mance by controlling the latent features or the attentionmaps of a denoising U-Net. Moreover, DMs can gener-ate and edit images by adjusting latent features over mul-tiple denoising steps . We focus on using latent featuresof DM, including intermediate features and cross-attentionmaps, across denoising steps to link them with the latentspace of GAN and develop a multi-modal face image gen-eration task. 2.3. Multi-Modal Face Image GenerationFace generative models have progressed by incorporatingvarious modalities, such as text , semantic mask , sketch , and audio . Several methods adoptStyleGAN, which can generate high-quality face imagesand edit facial attributes to control the style vectors. Thetransformer-based models are also utilized, whichimproves the performance of face image generation by han-dling the correlation between multi-modal conditions us-ing image quantization. A primary challenge faced in facegenerative models is to modify the facial attributes basedon given conditions while minimizing changes to otherattributes.Some methods edit facial attributesby manipulating the latent codes in GAN models. Tedi-GAN controls multiple conditions by leveraging anencoder to convert an input image into latent codes andoptimizing them with a pre-trained CLIP model. Recentworks use DMs to exploit the flexibility of takingmultiple modalities as conditions and generate facial im-ages directly from DMs. Unlike existing methods, we usethe pre-trained DM as an encoder to further producethe latent codes for the pre-trained GAN models.",
  ". Method": "3.1. Overview illustrates the overall pipeline of our approach.During the reverse diffusion process, we use the middle anddecoder blocks of a denoising U-Net in ControlNet asan encoder E. A text prompt c, along with a visual conditionx, are taken as input to the denoising U-Net. Subsequently,E produces the feature maps h from the middle block, and",
  "Frozen": ". Overview of our method. We use a diffusion-based encoder E, the middle and decoder blocks of a denoising U-Net, that extractsthe semantic features ht, intermediate features ft, and cross-attention maps at at denoising step t. We present the mapping network M(Sec. 3.2) and the attention-based style modulation network (AbSMNet) T (Sec. 3.3) that are trained across t (Sec. 3.4). M converts htinto the mapped latent code wmt , and T uses ft and at to control the facial attributes from the text prompt c and visual input x. Themodulation codes wt and wt are then used to scale and shift wmt to produce the final latent code, wt, that is fed to the pre-trained GANG. We obtain the generation output It from our model Y and we use the image Id0 from the U-Net after the entire denoising process fortraining T (Sec. 3.4). Note that only the networks with the dashed line () are trainable, while others are frozen. the intermediate features f and the cross-attention maps afrom the decoder blocks. h is then fed into the mapping net-work M, which transforms the rich semantic feature intoa latent code wm. The Attention-based Style ModulationNetwork (AbSMNet), T , takes f and a as input to gener-ate the modulation latent codes, w and w, that determinefacial attributes related to the inputs. The latent code w isthen forwarded to the pre-trained GAN G that generates theoutput image I. Our model is trained across multiple de-noising steps, and we use the denoising step t to indicate thefeatures and images obtained at each denoising step. Withthis pipeline, we aim to estimate the latent code, wt , that isused as input to G to render a GT image, Igt:",
  "wt = arg minwtL(Igt, G(wt)),(1)": "where L(, ) measures the distance between Igt and therendered image, I = G(wt). We employ learning-basedGAN inversion that estimates the latent code from an en-coder to reconstruct an image according to given inputs. 3.2. Mapping NetworkOur mapping network M aims to build a bridge between thelatent space of the diffusion-based encoder E and that of thepre-trained GAN G. E uses a text prompt and a visual input,and these textual and image embeddings are aligned by thecross-attention layers . The feature maps h from themiddle block of the denoising U-Net particularly containrich semantics that resemble the latent space of the gen-erator . Here we establish the link between the latent spaces of E and G by using ht across the denoising steps t.Given ht, we design M that produces a 512-dimensionallatent code wmt RL512 that can be mapped to the latentspace of G:wmt = M(ht).(2)M is designed based on the structure of the map2style blockin pSp , as seen in . This network consists ofconvolutional layers downsampling feature maps and a fullyconnected layer producing the latent code wmt . 3.3. Attention-based Style Modulation NetworkBy training M with learning-based GAN inversion, we canobtain wmtand use it as input to the pre-trained GAN forimage generation. However, we observe that ht shows lim-itations in capturing fine details of the facial attributes dueto its limited spatial resolution and data loss during the en-coding. Conversely, the feature maps of the DMs decoderblocks show rich semantic representations , benefitingfrom aggregating features from DMs encoder blocks viaskip connections.We hence propose a novel Attention-based Style Modulation Network (AbSMNet), T , that pro-duces style modulation latent codes, wt , wt RL512, byusing ft and at from E. To improve reflecting the multi-modal representations to the final latent code wt, we mod-ulate wmt from M using wt and wt , as shown in .We extract intermediate features, ft = {f nt }Nn=1, from Ndifferent blocks, and cross-attention maps, at = {akt }Kk=1,from K different cross-attention layers of the n-th block, inE that is a decoder stage of denoising U-Net. The discrim-",
  "Output": "The person has arched eyebrows, wavy hair, and mouth slightly open. . Visualization of cross-attention maps and intermediatefeature maps. (a) represents the semantic relation information be-tween an input text and an input semantic mask in the spatial do-main. The meaningful representations of inputs are shown acrossall denoising steps and N different blocks. (b) represents N differ-ent cross-attention maps, At, at denoising steps t = T and t = 0.(c) shows the example of refined intermediate feature map F1T at1st block and t = T that is emphasized corresponding to inputmulti-modal conditions. The red and yellow regions of the mapindicate higher attention scores. As the denoising step approachesT, the text-relevant features appear more clearly, and as the de-noising step t approaches 0, the features of the visual input aremore preserved. inative representations are represented more faithfully be-cause ft consists of N multi-scale feature maps that can cap-ture different sizes of facial attributes, which allows for finercontrol over face attributes. For simplicity, we upsampleeach intermediate feature map of ft to same size intermedi-ate feature maps Ft = {Fnt }Nn=1, where Fnt RHW Cnhas H, W, and Cn as height, width and depth.Moreover, at is used to amplify controlled facial at-tributes as it incorporates semantically related informationin text and visual input. To match the dimension with Ft,we convert at to At = {Ant }Nn=1, where Ant RHW Cn,by max-pooling the output of the cross-attention layers ineach decoder block and upsampling the max-pooling out-puts. To capture the global representations, we addition-ally compute At RHW 1 by depth-wise averaging themax-pooling output of at over each word in the text promptand upsampling it. As illustrated in Figures 3 (a) and (b),At and At represent the specific regions aligned with inputtext prompt and visual input, such as semantic mask, acrossdenoising steps t. By a pixel-wise multiplication betweenFt and At, we can obtain the refined intermediate featuremaps Ft that emphasize the representations related to multi-",
  "map2style": "Weighted sum . Style modulation network in T . The refined intermedi-ate feature maps Ft and Ft are used to capture local and globalsemantic representations, respectively. They are fed into the scaleand shift network, respectively. The weighted summations of theseoutputs are used as input to the map2style network, which finallygenerates the scale and shift modulation latent codes, wt , and wt . modal inputs as shown in (c). The improved aver-age feature map Ft RHW 1 is also obtained by mul-tiplying At with Ft, where Ft RHW 1 is obtained byfirst averaging the feature maps in Ft = {Fnt }Nn=1 and thendepth-wise averaging the outputs.Ft and Ft distinguishtext- and structural-relevant semantic features, which im-proves the alignment with the inputs.We use Ft and Ftas input to the style modulation network that produces themodulation codes wt , and wt as shown in . Wecapture both local and global features by using Ft, whichconsists of feature maps representing different local regionson the face, and Ft, which implies representations of theentire face. We concatenate N intermediate feature mapsof Ft, concat(F1t FNt ), and it is forward to the scaleand shift networks that consist of convolutional layers andLeaky ReLU, forming the local modulation feature maps,Flt and Flt . We also estimate global modulation featuremaps, Fgtand Fgt , by feeding Ft to the scale and shiftnetwork. The final scale, Ft , and shift, Ft , feature mapsare estimated by the weighted summation:",
  "Ft = t Fgt+ (1 t )Fgt ,": "where t and t are learnable weight parameters. Throughthe map2style module, we then convert Ft and Ft into thefinal scale, wt RL512, and shift, wt RL512, la-tent codes. With these modulation latent codes, we achievemore precise control over facial details while correspondingto the input multi-modal inputs at the pixel level.Finally, the mapped latent code wmtfrom M is modu-lated by wt and wt from T to get the final latent code wtthat is used to obtain the generated image It as follows:",
  ". Loss Functions": "To optimize M and T , we use reconstruction loss, percep-tual loss, and identity loss for image generation, and reg-ularization loss that encourages the latent codes to becloser to the average latent code w.For training M, we use the GT image Igt as reference toencourage the latent code wmt to generate a photo-realisticimage as follows:",
  "m3 E(zt, t, x, c) w2,": "where R() is pre-trained ArcFace network , F() is thefeature extraction network , zt is noisy image, and thehyper-parameters m() guide the effect of losses. Note thatwe freeze T while training M.For training T , we use Id0 produced by the encoder Einto the reconstruction and perceptual losses. With theselosses, the loss LT encourages the network to control facialattributes while preserving the identity of Igt:",
  "s3E(zt, t, x, c) w2,": "where the hyper-parameters s() guide the effect of losses.Similar to Equation 6, we freeze M while training T .We further introduce a multi-step training strategy thatconsiders the evolution of the feature representation in Eover the denoising steps. We observe that E tends to fo-cus more on text-relevant features in an early step, t = T,and structure-relevant features in a later step, t = 0. Fig-ure 3 (b) shows the attention maps A showing variationsacross the denoising step. As the attention map, we cancapture the textual and structural features by varying the de-noising steps. To effectively capture the semantic details ofmulti-modal conditions, our model is trained across multi-ple denoising steps.",
  ". Experiments": "4.1. Experimental SetupWe use ControlNet as the diffusion-based encoder thatreceives multi-modal conditions, including text and visualconditions such as a semantic mask and scribble map. TheStyleGAN and EG3D are exploited as pre-trained2D and 3D GAN, respectively. See the Supplementary Ma-terial for the training details, the network architecture, andadditional results.Datasets. We employ the CelebAMask-HQ datasetcomprising 30,000 face RGB images and annotated seman-tic masks, including 19 facial-component categories suchas skin, eyes, mouth, and etc.We also use textual de-",
  ". Quantitative results of multi-modal face image generation on CelebAMask-HQ with annotated text prompts": "scriptions provided by describing the facial attributes,such as black hair, sideburns, and etc, corresponding tothe CelebAMask-HQ dataset. For the face image gener-ation task using a scribble map, we obtain the scribblemaps by applying PiDiNet to the RGB images inCelebAMask-HQ. We additionally compute camera param-eters based on for 3D-aware image generation.Comparisons. We compare our method with GAN-basedmodels, such as TediGAN and IDE-3D , and DM-based models, such as Unite and Conquer (UaC) ,ControlNet , and Collaborative diffusion (Collabora-tive) , for face generation task using a semantic maskand a text prompt. IDE-3D is trained by a CLIP loss termlike TediGAN to apply a text prompt for 3D-aware face im-age generation. ControlNet is used for face image gener-ation using a text prompt and a scribble map. We use theofficial codes provided by the authors, and we downsamplethe results into 256 256 for comparison.Evaluation Metrics.For quantitative comparisons, weevaluate the image quality and semantic consistency usingsampled 2k semantic mask- and scribble map-text promptpairs. Frechet Inception Distance (FID) , LPIPS ,and the Multiscale Structural Similarity (MS-SSIM) are employed for the evaluation of visual quality and diver-sity, respectively. We also compute the ID similarity meanscore (ID) before and after applying a text prompt. Additionally, we assess the alignment accuracy between theinput semantic masks and results using mean Intersection-over-Union (mIoU) and pixel accuracy (ACC) for the facegeneration task using a semantic mask. 4.2. ResultsQualitative Evaluations. shows the visual com-parisons between ours and two existing methods for 2D faceimage generation using a text prompt and a semantic maskas input. We use the same semantic mask with differenttext prompts (a)-(c). TediGAN produces results consistentwith the text prompt as the latent codes are optimized us-ing the input text prompt. However, the results are incon-sistent with the input semantic mask, as highlighted in thered boxes. UaC shows good facial alignment with the in-put semantic mask, but the results are generated with unex-pected attributes, such as glasses, that are not indicated inthe inputs. Collaborative and ControlNet produce inconsis-tent, blurry, and unrealistic images. Our model is capable ofpreserving semantic consistency with inputs and generatingrealistic facial images. As shown in , our methodpreserves the structure of the semantic mask, such as thehairline, face position, and mouth shape, while changingthe attributes through a text prompt. compares our method with IDE-3D to val-idate the performance of 3D-aware face image generation",
  ". She has big lips, pointy nose, receding hairline, and arched eyebrows. 4. This man has mouth slightly open, and arched eyebrows. He is smiling": ". Visual examples of 3D-aware face image generation us-ing text prompts and scribble maps. Using (1-4) the text promptsand their corresponding (a) scribble maps, we compare the resultsof (b) ControlNet with (c) multi-view images generated by ours. using a semantic mask and a text prompt.We use thesame semantic mask with different text prompts in Fig-ures 6 (a) and (b), and use the same text prompt with dif-ferent semantic masks in Figures 6 (c) and (d). The resultsof IDE-3D are well aligned with the semantic mask with thefrontal face. However, IDE-3D fails to produce accurate re-sults when the non-frontal face mask is used as input. More-over, the results cannot reflect the text prompt. Our methodcan capture the details provided by input text prompts andsemantic masks, even in a 3D domain. shows visual comparisons with ControlNet on2D face generation from a text prompt and a scribble map.The results from ControlNet and our method are consistentwith both the text prompt and the scribble map. ControlNet,however, tends to over-emphasize the characteristic detailsrelated to input conditions. Our method can easily adapt tothe pre-trained 3D GAN and produce photo-realistic multi-view images from various viewpoints.Quantitative Evaluations. reports the quantitativeresults on CelebAMask-HQ with text prompts . Ourmethod using text prompts and semantic masks shows per-formance increases in all metrics in 2D and 3D domains,compared with TediGAN and UaC. Our model using 2DGAN significantly improves LPIPS, ID, ACC, and mIoUscores, surpassing TediGAN, UaC, ControlNet, and Collab-orative, respectively. It demonstrates our methods strongability to generate photo-realistic images while reflectinginput multi-modal conditions better. For 3D-aware face im-age generation using a text prompt and a semantic mask, it",
  "(e)44.910.280.7883.05": ". Ablation analysis on 3D-aware face image generationusing a text prompt and a semantic mask. We compare (a) and (b)with (e) to show the effect of our style modulation network and(c) and (d) with (e) to analyze the effect of Igt and Id in modeltraining. is reasonable that IDE-3D shows the highest FID score asthe method additionally uses an RGB image as input to esti-mate the latent code for face generation. The LPIPS, SSIM,and ID scores are significantly higher than IDE-3D, withscores higher by 0.116, 0.23, and 0.24, respectively. Ourmethod using 3D GAN exhibits superior ACC and mIoUscores for the 3D face generation task compared to IDE-3D, with the score difference of 35.98% and 32.76%, likelydue to its ability to reflect textual representations into spa-tial information. In face image generation tasks using a textprompt and a scribble map, our method outperforms Con-trolNet in FID, LPIPS, SSIM, and ID scores in both 2D and3D domains. Note that the ACC and mIoU scores are appli-cable for semantic mask-based methods.",
  ". She wears lipstick and has arched eyebrows, and": "mouth slightly open. . Effect of using Id from the denoising U-Net and the GTimage Igt in model training. Using text prompts (1, 2) with (a)the semantic mask, we show face images using our model trainedwith (b) Id0 , (c) Igt, and (d) both. We also show the advantages of using cross-attention mapsin our model. The quantitative and qualitative results arepresented in and , respectively. When usingonly M, we can generate face images that roughly preservethe structures of a given semantic mask in (a), in-cluding the outline of the facial components (e.g. face, eye)in (b). On the other hand, T enables the model toexpress face attribute details effectively, such as hair colorsand mouth open, based on the multi-modal inputs in Fig-ure 8 (c). The FID and ACC scores are higher than themodel using only M in (b). We further presentthe impact of adopting cross-attention maps to T for stylemodulation. (d) shows how the attention-basedmodulation approach enhances the quality of results, par-ticularly in terms of the sharpness of desired face attributesand the overall consistency between the generated imageand multi-modal conditions. (e) demonstrates theeffectiveness of our method by showing improvements inFID, LPIPS, ID, and ACC. Our method, including both Mand T with cross-attention maps, significantly improves theFID showing our models ability to generate high-fidelityimages. From the improvement of the ID score, the cross-attention maps enable relevantly applying the details of in-put conditions to facial components.Model Training. We analyze the effect of loss terms LMand LT by comparing the performance with the modeltrained using either Id0 from the denoising U-Net or GT im-age Igt. The model trained using Id0 produces the images in (b), which more closely reflected the multi-modalconditions (a), such as goatee and hair contour. In Ta-ble 2 (c), the ACC score of this model is higher than themodel trained only using Igt in (d). The imagesgenerated by the model trained with Igt in (c)are more perceptually realistic, as evidenced by the lowerLPIPS score compared to the model trained with Id0 in Ta-",
  ". Visual examples of 3D face style transfer. Our methodgenerates stylized multi-view images by mapping the latent fea-tures of DM and GAN": "ble 2 (c) and (d). Using Igt also preserves more condition-irrelevant features inferred by the ID scores in (c)and (d). In particular, our method combines the strengths oftwo models as shown in (d) and (e). 4.4. Limitations and Future WorksOur method can be extended to multi-modal face styletransfer (e.g. face Greek statue) by mapping the latentspaces of DM and GAN without CLIP losses and additionaldataset, as shown in . For the 3D-aware face styletransfer task, we train our model using Id0 that replaces GTimage Igt in our loss terms. This method, however, is lim-ited as it cannot transfer extremely distinct style attributesfrom the artistic domain to the photo-realistic domain ofGAN. To better transfer the facial style in the 3D domain,we will investigate methods to map the diffusion featuresrelated to the input pose into the latent space of GAN infuture works. 5. ConclusionWe presented the diffusion-driven GAN inversion methodthat translates multi-modal inputs into photo-realistic faceimages in 2D and 3D domains. Our method interprets thepre-trained GANs latent space and maps the diffusion fea-tures into this latent space, which enables the model to eas-ily adopt multi-modal inputs, such as a visual input and atext prompt, for face image generation. We also proposed totrain our model across the multiple denoising steps, whichfurther improves the output quality and consistency withthe multiple inputs. We demonstrated the capability of ourmethod by using text prompts with semantic masks or scrib-ble maps as input for 2D or 3D-aware face image generationand style transfer.",
  "Omri Avrahami, Dani Lischinski, and Ohad Fried. Blendeddiffusion for text-driven editing of natural images. In CVPR,2022. 2": "Sam Bond-Taylor, Peter Hessey, Hiroshi Sasaki, Toby PBreckon, and Chris G Willcocks.Unleashing transform-ers: Parallel token prediction with discrete absorbing diffu-sion for fast high-resolution image generation from vector-quantized codes. In ECCV, 2022. 2 Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas JGuibas, Jonathan Tremblay, Sameh Khamis, et al.Effi-cient geometry-aware 3d generative adversarial networks. InCVPR, 2022. 2, 5, 6",
  "Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Ban-dara, and Vishal M Patel. Unite and conquer: Plug & playmulti-modal synthesis using diffusion models.In CVPR,2023. 2, 6": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. Glide: Towards photorealistic image generationand editing with text-guided diffusion models. arXiv preprintarXiv:2112.10741, 2021. 1, 2 Xingang Pan, Ayush Tewari, Thomas Leimkuhler, LingjieLiu, Abhimitra Meka, and Christian Theobalt. Drag yourgan: Interactive point-based manipulation on the generativeimage manifold. In SIGGRAPH 2023, 2023. 1"
}