{
  "Abstract": "Single-source domain generalization (SDG) for objectdetection is a challenging yet essential task as the distribu-tion bias of the unseen domain degrades the algorithm per-formance significantly. However, existing methods attemptto extract domain-invariant features, neglecting that the bi-ased data leads the network to learn biased features that arenon-causal and poorly generalizable. To this end, we pro-pose an Unbiased Faster R-CNN (UFR) for generalizablefeature learning. Specifically, we formulate SDG in objectdetection from a causal perspective and construct a Struc-tural Causal Model (SCM) to analyze the data bias and fea-ture bias in the task, which are caused by scene confoundersand object attribute confounders. Based on the SCM, we de-sign a Global-Local Transformation module for data aug-mentation, which effectively simulates domain diversity andmitigates the data bias. Additionally, we introduce a CausalAttention Learning module that incorporates a designed at-tention invariance loss to learn image-level features thatare robust to scene confounders. Moreover, we develop aCausal Prototype Learning module with an explicit instanceconstraint and an implicit prototype constraint, which fur-ther alleviates the negative impact of object attribute con-founders. Experimental results on five scenes demonstratethe prominent generalization ability of our method, with animprovement of 3.9% mAP on the Night-Clear scene.",
  ". Introduction": "The problem of distribution shift in unseen domains oftenarises during the deployment of perception systems, lead-ing to a notable decline in the models performance .Consequently, domain generalization hasemerged as a branch of transfer learning, which aims atgeneralizing the knowledge from multiple source domainsto the unseen target domain. Single-source domain gener-",
  "*Corresponding author": ". Comparison between vanilla Faster R-CNN (FR) (top) and our proposed Unbiased Faster R-CNN (bottom). Forvanilla FR , the biased distribution of the input data leads thenetwork to learn biased features that favor the seen environmentand are poorly generalizable to unseen test environments. Thefeature bias can be attributed to the image-level attention bias andobject-level prototype bias. Our method mitigates the data biasin the input space and further learns unbiased attention and proto-types in the representation space. alization (SDG) is a special case of domain generalizationwhere there is only one source domain and itfocuses on exploring the robustness of the model under dif-ferent image corruptions.The existing two methods for SDG in object detectionadopt different generalization strategies on Faster R-CNN. The domain-invariant feature learning-based method explicitly decomposes domain-invariant features anddomain-specific features by imposing constraints on thenetwork, without relying on data augmentation techniques.And the data augmentation-based method perturbs thedata distribution and increases the diversity of input data toenhance model generalization ability.However, both of these strategies have certain limita-",
  ". Illustration of highly changeable data distribution, di-verse context and object attributes in unseen target domains": "tions.Firstly, it has been proven in that domain-invariant features are inherently domain-dependent and bi-ased, as features that are invariant to current domains maybe variant to other domains. This problem can be attributedto the fact that domain-invariant features learned from abiased data distribution are not causal features and cannotadapt well to the unseen target environments. Secondly,the domain generalization methods that use only data aug-mentation without constraining network features fail to cap-ture causal features from a rich augmented data distribu-tion. This results in the network performing well in theaugmented domains but remaining ineffective in the unseendomains, as confirmed in our Ablation Study in .3.We summarize the reasons for the above two limitations asthe existence of data bias in the input space and feature biasin the representation space. The feature bias is further de-composed into image-level attention bias and object-levelprototype bias, as illustrated in and :(1) Data bias. The data distribution of unseen target do-mains is highly changeable, as shown in . Therefore,learning invariant features solely from single-domain datawill exacerbate the statistical dependence between theinput data and labels, leading to biased learned results thatfavor the seen environments.(2) Attention bias. When testing in unseen domains withcomplex context, as depicted in , the context featuresmay confuse the object features and the network may ex-hibit attention bias, directing its focus towards the contextof the object rather than the object itself. We refer to thescene context as scene confounders.(3) Prototype bias.Each category possesses distinctivecausal attributes, such as structural information, that arehighly informative, as opposed to non-discriminative oneslike view and color, which we refer to as object attributeconfounders, as shown in . Therefore, if the learnedcategory features are not constrained, the network may in-correctly take confounding attribute features that occur fre-quently in the source domain as category prototypes, whichare biased and poorly generalizable.To this end, we propose an Unbiased Faster R-CNN (UFR) model, as illustrated in , with a comparisonto vanilla Faster R-CNN . The UFR model simulatesdiverse data distributions through data augmentation to mit-igate the data bias and applies constraints on the featuresto learn scene-level causal attention and object-level causalprototype, thus eliminating the attention bias and prototypebias. Specifically, we define causality in the SDG task as thecause-and-effect relationship among scene, objects, causalfeatures and non-causal features and construct a StructuralCausal Model (SCM) to analyze the data bias and featurebias caused by scene confounders and object attribute con-founders. Additionally, we propose a Global-Local Trans-formation (GLT) module, which augments the data as awhole in the frequency domain and leverages the segmenta-tion capability of SAM to augment local objects in thespatial domain. To address scene confounders, we proposea Causal Attention Learning module with an attention in-variance loss, which transforms the learning of scene-levelcausal features into the learning of causal attention. To fur-ther learn causal features at the object level, we introduce aCausal Prototype Learning module, which encompasses anexplicit constraint on the distribution of instance featuresand an implicit constraint on the relationship between pro-totypes. The effectiveness of our method is demonstratedthrough experiments conducted across five weather condi-tions. The contributions are summarized as follows:(1) We are the first to investigate single-domain general-ized object detection from a causal perspective and analyzethree biases that limit the generalization ability of the detec-tor, including data bias, attention bias and prototype bias.(2) We construct a Structural Causal Model to analyzethe biases caused by two types of confounders and furtherpropose an Unbiased Faster R-CNN with a Global-LocalTransformation module, a Causal Attention Learning mod-ule and a Causal Prototype Learning module to mitigate thedata bias, attention bias and prototype bias respectively.(3) We evaluate our method on five different weatherconditions to demonstrate its effectiveness and superiorityand our method achieves a remarkable 3.9% mAP improve-ment on the Night-Clear scene.",
  ". Domain Generalization": "Common strategies for addressing domain generalizationproblem include domain alignment , metalearning , data augmentation ,ensemble learning , self-supervised learning and disentanglement learning . As a spe-cial case of domain generalization, the solution for single-source domain generalization (SDG) can also be catego-rized into several of the aforementioned strategies. Manyprior studies used data augmentation to gen- erate out-of-distribution samples to extend the distributionof the source domain. For example, Wang et al. pro-posed a style-complement module to generate diverse styl-ized images. Vidit et al. proposed a semantic augmen-tation method for SDG in object detection with the helpof a pre-trained CLIP . In addition, some works em-ployed the feature normalization strategy tolearn domain-invariant features. For example, Fan et al. proposed an ASR-Norm layer to learn standardization andrescaling statistics.",
  ". Causality in Computer Vision": "Causal mechanism considers the fact that statistical de-pendence cannot reliably predict the labels of counterfac-tual inputs .Thus, exploring causality enablesthe acquisition of robust knowledge beyond what is sup-ported by the observed data . Recently, causal mecha-nism has gathered significant attention in computer vision.Many works use causal mechanism totackle domain generalization and do-main adaptation problems. For example,Yue et al. applied the causal mechanism to domainadaptation and used the domain-invariant disentanglementto identify confounders. Lv et al. proposed a repre-sentation learning framework for causality-inspired domaingeneralization. Besides, Liu et al. proposed a De-coupled Unbiased Teacher to solve the source-free domainadaptation problem. Moreover, Xu et al. introduceda causality-inspired data-augmentation strategy and elimi-nated non-causal factors by a Multi-view Adversarial Dis-criminator. In this paper, we first apply the causal mecha-nism to the single-source domain generalized object detec-tion task and propose an Unbiased Faster R-CNN to minecausal features at the image level and object level.",
  ". Structural Causal Model": "Consider the data (e.g. images) from the observed environ-ment (source domain) as X and its target (e.g. detectionlabels) as Y , the objective of single-domain generalized ob-ject detection is to generalize the model trained with (X, Y )in the observed environment to unseen environments (targetdomains). We represent images from a causal perspectiveand construct a Structural Causal Model (SCM) in todescribe the cause-and-effect relationship in object detec-tion task and attempt to eliminate the data bias and featurebias caused by the scene confounders and object attributeconfounders. The rationale behind the SCM is as follows:(1) O X D denotes that an image is composed ofscene D and a set of objects O. D is scene confounder, andthe non-discriminative attributes of O are object attributeconfounders. The GLT module is achieved by perturbingthe scene confounders and object attribute confounders.(2) O ZV D denotes that the non-causal features . The constructed Structural Causal Model (SCM) for theobject detection task. The nodes denote variables, the solid arrowsdenote the direct causal effect and the dashed arrow indicates thatthere exists data dependence. ZV are derived from two components, including the sceneconfounders and non-causal object attribute confounders.(3) O ZC Y represents that the causal features ZCare determined by the discriminative attributes of objects,such as shape. And the prediction labels Y are derived fromthe causal features ZC. The complete feature space Z con-sists of non-causal features ZV and causal features ZC.(4) X f(X) Y models the data stream of the ob-ject detection network parameterized by .Based on the constructed SCM, the ideal feature map-ping of the network is f(X) = ZC, which is also formu-lated as:f(X(al,dl)) = f(X(ak,dk)),(1) where al = ak, dl = dk and the superscript (al, dl) denotesassigning scene and object attributes the distribution dl andal, respectively. Eq. (1) indicates that the ideal represen-tation of an image learned by the model is invariant underdifferent data augmentations.To achieve the above goal, we decompose the objectiveinto image-level attention invariance learning and object-level prototype invariance learning and further propose anUnbiased Faster R-CNN model, as depicted in , andthe details of which are illustrated in .",
  "where F denotes the Fourier Transformation and F is the": ". The overall structure of the proposed Unbiased Faster R-CNN. The input source images are augmented through the Global-LocalTransformation (GLT) module. Both the original images and augmented images are fed into the network for training. During training, therole of the Causal Attention Learning module is to constrain the network to learn scene-level causal attention and select causal features tofeed into the RPN. The purpose of the Causal Prototype Learning module is to constrain the network to learn object-level causal featureswith the help of an explicit instance constraint (solid arrows) and an implicit prototype constraint (dashed arrows). inverse one. H(r) is the band-pass filter with a filter ra-dius of r. G() is a randomization function according to aGaussian distribution and G(X) = X (1 + N(0, 1)).For local transformation, as shown in , we firstobtain the object masks with the help of Segment AnythingModel (SAM) :",
  "We transform the learning of scene-level causal featuresinto the learning of causal attention, which eliminates the": ". Overview of the Global-Local Transformation (GLT)module. The Global Transformation (GT) performs overall aug-mentation of an image in the frequency domain. And The Local-Transformation (LT) performs augmentation of local objects ob-tained by SAM in the spatial domain. The final augmentedimage is obtained by fusing the GT image with the LT image. need for explicit decoupling of causal features from non-causal features, instead focusing on selecting causal fea-tures based on attention.Specifically, it is desirable for the network to capture ob-ject features precisely in different scene context. Thus, weenforce the feature attention maps of different data distri-bution images output by the backbone to converge and theattention invariance loss is defined as:",
  "|X1|+|X2|+1 ,(9)": "where X1 and X2 are two binary maps. The regions with avalue of 1 for the maps are salient attention regions, whilethe regions with a value of 0 are non-salient attention re-gions.Then we select causal features according to the attentionmap for the subsequent Region Proposal Network (RPN) togenerate object proposals P:",
  ". Causal Prototype Learning": "To facilitate the learning of object-level causal features, weintroduce the Causal Prototype Learning module which en-compasses an explicit constraint and an implicit constraint.The explicit constraint is imposed on ROI features ex-tracted from different data distributions. Specifically, giventhe proposal set P generated from the source image, we se-lect a group of proposals with a confidence higher than thethreshold t, denoted as P(t). We represent the ROI featuresgenerated from P(t) as f(x, P(t)) for simplicity. Then theexplicit constraint is defined as:",
  "where KL denotes the Kullback-Leibler divergence,pdo(a0,d0) is the short for p(y|f(x(a0,d0), P(t))), pdo(ak,dk)": "is the short for p(y|f(x(ak,dk), P(t)))and pdo(a,d)=MLP(f(x(a,d), P(t))).The explicit constraint Lexp encourages the within-classdistance of the learned class representations from differentdata distributions to be concentrated and also gives super-visory information to the data-augmented image at the ob-ject feature level, which enhances the saliency of the ob-ject region, thereby improving the object localization per-formance of the data-augmented images.On the other hand, the implicit invariance constraint con-strains the relationship between category prototypes acrossdifferent data distributions. We hypothesize that the dis-tance between causal prototypes of the same category fordifferent data distributions is smaller compared to the dis-tance between prototypes of different categories: dist(v(a0,d0)ci, v(ak,dk)cj) > dist(v(a0,d0)ci, v(ak,dk)ci) 0,(12)where i = j, dist() is a distance metric function, v(a0,d0)ciis the prototype of category ci of the source data distribu-tion and v(ak,dk)cjis the prototype of category cj of the aug-mented data distribution. The prototype vc is computed by",
  "The data streams of the original images and the augmentedimages share the network parameters and the total trainingloss is:L = Lsup + 1Latt + 2Lprot,(16)": "where Lsup is the supervised object detection loss for theoriginal images and the augmented images and 1/2 are hy-perparameters.During inference, our UFR model maintains the sameparameter size as vanilla Faster R-CNN , ensuring thatit does not introduce additional space complexity. However,a key difference lies in the calculation of features that arefed into the RPN network, which is determined by Eq. (10).",
  ". Experimental Setup": "Datasets. We conduct experiments on the dataset built in. The dataset consists of five different weather con-ditions, including Daytime-Clear, Daytime-Foggy, Dusk-Rainy, Night-Clear and Night-Rainy. The Daytime-Clearscene is used as the source domain, comprising 19,395 im-ages for training and 8,313 images for testing. The otherfour scenes are used as unseen target domains, consisting of3,775 images in Daytime-Foggy condition, 3,501 images inDusk-Rainy condition, 26,158 images in Night-Clear con-dition and 2,494 images in Night-Rainy condition.Thedataset contains annotations for seven categories of objects,including person, car, bike, rider, motor, bus and truck.Implementation Details.We adopt Faster R-CNN with ResNet-101 as the object detector. The backboneis initialized with weights pre-trained on ImageNet . We . Qualitative evaluation results of the models generalization ability on the Night-Clear scene. The top-row images are the resultsof vanilla Faster R-CNN . The bottom-row images are the results of our method.",
  ". Quantitative results (%) on the Daytime-Clear scene": "train the model with Stochastic Gradient Descent (SGD) op-timizer with a momentum of 0.9 for 80k iterations. Duringtraining, the learning rate is set to 0.001, the batch size isset to 4. Besides, the threshold t is set to 0.7, the 1 and 2are both set to 0.1.Data Augmentation Setting. For local transformation inspatial domain, we randomly apply the augmentation strate-gies consisting of gaussian blurring, color jittering, randomerasing and grayscale. The fusion weight in Eq. (6) is arandom scalar in .",
  ". Comparison with State-of-the-arts": "Following the setting in , we use the Mean Average Pre-cision (mAP) as our metric and report the results.We compare our method with feature normalization-basedSDG methods, including SW , IBN-Net , IterNorm, and ISW , and the existing two single domain gen-eralized object detection methods, including SDGOD and CLIP-Gap . Besides, we also compare the perfor-mance of our model with vanilla Faster R-CNN (FR) .Results on Daytime-Clear Scene. We evaluate the modelperformance on the source domain. As shown in ,our method achieves an optimal result of 58.6% mAP andhas a 2.4% mAP gain against vanilla FR . It shows thatour method is able to maintain or even improve the perfor-mance of the FR model in the source domain whileimproving its generalization ability in unseen domains.Results on Night-Clear Scene. The qualitative and quanti-",
  ". Quantitative results (%) on the Night-Clear scene": "tative evaluation results of the models generalization per-formance in the Night-Clear scene are shown in and , respectively. As shown in , our methodachieves the best result of 40.8% mAP, outperforming Clip-Gap by 3.9% mAP. Besides, compared with vanilla FR, our method achieves a 5.0% mAP gain, which indi-cates that the learned features is beneficial for improvingmodel generalization ability. Besides, compared with SD-GOD that learns domain-invariant features, our methodhas an improvement of 4.2% mAP, which suggests thatcausal features are more discriminative features and cangeneralize better. For qualitative results demonstrated in, it is obviously observed that our method detectsobjects more accurately and has fewer false positives com-pared with vanilla FR .Results on Dusk-Rainy and Night-Rainy Scenes. and demonstrate the models ability to general-ize to Dusk-Rainy and Night-Rainy scenes. Our methodachieves the best mAP in both scenes, with gains of 0.9%mAP and 0.5% mAP respectively compared with CLIP-Gap. Besides, compared with vanilla FR , our methodachieves improvements of 5.2% mAP and 5.0% mAP re-spectively.Furthermore, the feature normalization-basedmethods have a poor performance in bothscenes, especially in motor category in Night-Rainy scene,with mAP of less than 1.0%, which reinforces the challengeof these two scenes and the superiority of our method.Results on Daytime-Foggy Scene. shows the gen- . Qualitative evaluation results of the models generalization ability on the Daytime-Foggy scene. The top-row and bottom-rowimages demonstrate the results of vanilla Faster R-CNN and our method, respectively. Besides, the first two columns are the resultsof real foggy images and the last two columns are the results of synthetic foggy images.",
  ". Quantitative results (%) on the Night-Rainy scene": "eralization results of our model in Daytime-Foggy scene.We can see that our method outperforms all the methods inthe table and achieves 39.6% mAP. Specifically, comparedwith SDGOD and CLIP-Gap , our method achievesimprovements of 6.1% mAP and 1.1% mAP, respectively.Besides, some visualization results are demonstrated in . Compared with vanilla FR , our method achievesmore accurate object localization and classification in bothreal and synthetic foggy environments.",
  ". Quantitative results (%) on the Daytime-Foggy scene": "on the three weather conditions.Analysis of the GLT module. As shown in , the re-sults of vanilla FR are improved when combined withthe GLT module, with gains of 4.2% mAP in Daytime-Clear scene, 2.8% mAP in Night-Clear scene, and 2.6%mAP in Daytime-Foggy scene. Besides, the 60.4% mAPin Daytime-Clear scene is optimal, which indicates that theGLT module is an effective data augmentation method thatincreases data diversity and performs exceptionally well inthe source domain with supervised learning.Analysis of Latt, Lexp and Limp. As depicted in ,incorporating Latt, Lexp and Limp in addition to the GLTmodule decreases the model performance in the Daytime-Clear scene while improving the performance in the Night-Clear and Daytime-Foggy scenes, which suggests that theseconstraints limit the models performance in the seen envi-ronment with supervised learning, but improve the modelsperformance in unseen environments. The phenomenon isdue to that these constraints encourage the model to extractcausal knowledge, thereby preventing the model from ac-quiring domain-specific knowledge. The domain-specificknowledge, although not generalizable, carries some sup-plementary information for supervised learning. Further-more, we can observe that the attention invariance loss Lattimproves the model generalization performance more sig-nificantly, which indicates that the scene confounders havea greater impact on the model generalization ability. Be- . Visualization of the detection results and attention maps on the Dusk-Rainy scene. The top-row and bottom-row images are theresults of vanilla Faster R-CNN and our method respectively. The dark red region represents the area where the attention is salient.",
  ". Further Analysis": "Attention visualization.We conduct experiments onDusk-Rainy scene and compare the attention maps pro-duced by vanilla FR and our UFR model, as shownin . We can observe that the attention maps gener-ated by FR are unfocused and have more attention onirrelevant background areas. In contrast, our method pro-duces more effective category-related attention and has lessattention on background, which suggests that our methodcan capture discriminative object features in unseen envi-ronments with a superior generalization performance.Hyperparameter analysis. As demonstrated in , wetrain our model with different settings of t, 1 and 2 andtest the generalization performance on the Night-Clear andDaytime-Foggy scenes. The model performance with dif-ferent settings of threshold t in Eq. (11) is reported in (a). The best threshold t is 0.7. Besides, we also report theresults of our model in (b) with different settings of1 and 2 in Eq. (16). It is obvious that the model achievesthe best performance with 1 = 0.1 and 2 = 0.1.",
  ". Conclusions": "In this paper, we analyze the factors that contribute to thelimited single-domain generalization ability of Faster R-CNN from a causal perspective and summarize them as databias, attention bias and prototype bias. To tackle these chal-lenges, we propose a novel Unbiased Faster R-CNN. Ourmodel leverages a Structural Causal Model to analyze thebiases in the task that arise from both scene confoundersand object attribute confounders. We design a Global-LocalTransformation module for data augmentation to mitigatethe data bias. To learn features that are robust to scene con-founders, we introduce a Causal Attention Learning modulefor image-level causal feature selection. To further mitigatethe influence of object attribute confounders, we develop aCausal Prototype Learning module that learns object-levelcausal features. Experimental results and further analysisdemonstrate the effectiveness of our method.",
  ". Connection between causality and our method": "In our method, the non-causal factors are scenes and non-discriminative object attributes, which lead to attention biasand prototype bias in feature space. Our method is to elim-inate the two biases by learning generalized invariancefeatures robust to the change of non-causal factors inunseen domains. However, the training data is biased andcant cover a rich set of these non-causal factors. Thus theGLT is proposed to reduce data bias. The causal attentionlearning (attention debias) and causal prototype learning(prototype debias) modules are designed to learn invari-ance features from a rich data distribution with diverse non-causal factors. Following the causal theory , the prob-abilistic invariance constraint Lexp of the predicted resultsin Eq. (11) is an explicit constraint for causal learning andthe feature invariance constraints Latt and Limpin the rep-resentation space are implicit ones.",
  ". Reasons of using Dice loss with binary signif-icance maps instead of using MSE loss withattention maps": "The binary significance map provides a good representationof the activated and inactivated regions. And the MSE lossconstrains the difference in attention values more. For thistask, it is sufficient that the activated regions are consis-tency with dice loss though there are differences in the at-tention values. I think the MSE loss on attention maps is ahard constraint and the dice loss on the significance maps isa soft constraint. Besides, we conduct experiments to ana-lyze the impact of constraining the attention values withMSE loss and the mAP on Night-Clear scene decreases by1.03%.",
  ". More Implementation Details": "We use Detectron2 on a 24GB GeForce RTX 3090Tito implement our method. During training, the temperature in Limp is set to 0.2. Besides, the details of the localtransformation strategies are as follows: Gaussian Blurring: We blur the object using a randomselected square Gaussian kernel from the size of with standard deviation of 0, as shown in (b).",
  "i=1|pci pcavg|.(2)": "The results are shown in . The results demonstratethat the model can generate more concentrated prototypesof the same category in different domains with our designedLprot, indicating the prototype debias ability of our model.The effectiveness of attention debias is reflected fromthe experiment in paper (). We further demonstratethe attention maps generated by our UFR without Latt in for comparison.",
  ". Hyperparameter analysis": "We analyze the impact of temperature in Limp. The con-trastive loss is widely used and we follow the common set-ting to select from [0.07, 0.2, 0.3, 1.0]. We analyze theimpact of on Night-Clear scene and the results are demon-strated in . The results show that when the value of is small, it has a relatively slight effect on the results, andwhen is increased to 1.0, the results show a significant de-crease. We take the value of as 0.2 based on this analysis.",
  ". Discussion on the adoption of SAM insingle-domain generalization tasks": "In this task, we leverage the powerful segmentation capabil-ities of SAM to produce object masks of training data.However, the use of SAM may give rise to controversyabout whether it violates the single-domain generalizationsetting. We think that it doesnt violate the single domainsetting, for that we dont reach other data or leverage theSAM for testing results. It is just a tool to obtain ob-ject masks. The process can be realized with the help ofarbitrary segmentation models or even extracted manually.However, we use a more accurate large model to realizeit. In addition, with the development of foundation mod-els, there has been a trend of how to leverage them for effi-ciency gains in various tasks. These models are also used inother cross-domain works, such as the CLIP-Gap in ourcomparison experiments, which used a CLIP model fordomain augmentation.",
  "Peter Buhlmann. Invariance, Causality and Robustness. Sta-tistical Science, 35:404426, 2020. 1": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervision.In ICML, pages 87488763, 2021. 3"
}