{
  "Abstract": "Video panoptic segmentation is an advanced task thatextends panoptic segmentation by applying its concept tovideo sequences. In the hope of addressing the challengeof video panoptic segmentation in diverse conditions, Weutilize DVIS++ as our baseline model and enhance it by in-troducing a comprehensive approach centered on the query-wise ensemble, supplemented by additional techniques. Ourproposed approach achieved a VPQ score of 57.01 on theVIPSeg test set, and ranked 3rd in the VPS track of the 3rdPixel-level Video Understanding in the Wild Challenge.",
  ". Introduction": "In order to unify the video tasks including recognition,detection, tracking and segmentation, Kim et al. proposeVideo Panoptic Segmentation(VPS) by extending the con-cept of panoptic segmentation from image domain to videodomain . Video panoptic segmentation can be seen as afusion of video semantic segmentation and video instancesegmentation . More specifically, in a VPS task, eachpixel in an image is required to be assigned both a semanticlabel and an instance ID. The VPS task has received a lot ofattention due to its wide application in downstream tesks in-cluding video editing, video understanding and autonomousdriving.Early studies in VPS domain focused on adapting image-based panoramic segmentation methods to the video do-main by leveraging temporal consistency across videoframes. VPSNet integrated a temporal pixel-level fusionmodule and an object tracking head into a image panop-tic segmentation network UPSNet to achieve panop-tic video results. Video K-Net utilizes cross-temporalkernel interaction based on K-Net to simultaneouslysegment and track both things and stuff in videos.ViP-DeepLab , adapted from Panoptic-DeepLab achieves temporal alignment by utilizing offset predictionsfor consecutive frames, ensuring consistent instance group-ing across the video sequence.Considering that earliermethods necessitated the use of several distinct networks",
  ". Examples of the VIPSeg dataset": "and sophisticated post-processing, Max-DeepLab pro-posed the first end-to-end model for panoptic segmentationthat applies a pixel and memory dual-path modeling. MaX-Tron integrates a mask transformer with trajectory atten-tion to conduct VPS, enhancing temporal coherence with itswithin-clip and cross-clip tracking modules.Recently, DVIS introduces a decoupled frameworkthat divides video segmentation into three independent sub-tasks: image segmentation, referring tracking and temporalrefinement. Due to its outstanding performances, DVIS hasachieved first place in the VPS track of 2nd PVUW Chal-lenge in CVPR 2023. DVIS++ introduced denoisingtraining and contrastive learning strategies into DVIS, re-sulting in a significant improvement and achieving state-of-the-art performance on VIPSeg dataset.In this paper, we utilized DVIS++ as our baseline, andtried several techniques to deal with the inconsistency in the",
  "arXiv:2406.04002v2 [cs.CV] 7 Jun 2024": "predicted results. Note that DVIS++ follows the set predic-tion paradigm, which generates superfluous queries, eachrepresenting a binary segmentation mask for a single class,usual practices of test time augmentation and model ensem-ble cannot be applied here directly. Therefore, we proposesan query-wise ensemble method to solve this problem.With the help of the query-wise ensemble method men-tioned above and other additional techniques, our approachachieved 57.01 VPQ on the test set of VIPSeg dataset andranked 3rd in the VPS track of the 3nd PVUW Challenge inCVPR 2024.",
  ". Segmenter": "Segmenter serves as a image panoptic segmentationmodule. In our case, DVIS++ employ Mask2Former as the segmenter. Mask2Former is a versatile image seg-mentation architecture that outperforms specialized archi-tectures across various segmentation tasks, all while ensur-ing straightforward training for each specific task. It is builton a simple meta-architecture consisting of a backbone, apixel decoder, and a transformer decoder.",
  ". Referring Tracker": "The referring tracker utilizes the modeling paradigm ofreferring denoising to address the inter-frame correlationtask. Its primary goal is to utilize denoising operations torefine initial values and produce more precise tracking out-comes.The referring tracker comprises a sequence of L trans-former denoising (TD) blocks, each composed of a re-ferring cross-attention (RCA), a standard self-attention,and a feed-forward network (FFN). It takes object queriesQiseg|i [1, T]generated by the segmenter as input andproduces object queriesQiT r|i [1, T]for the currentframe that corresponds to objects in the previous frame. Inthis context, T represents the length of the video.Firstly, the Hungarian matching algorithm is used tomatch the Qseg of adjacent frames, as done in , havingQseg as results. Here, Qseg represents the matched objectquery generated by the segmenter. Qseg serves as the initialquery for the reference tracker, albeit with noise. To removenoise from the initial query Qiseg of the current frame, thereference tracker utilizes the denoised object query Qi1T rfrom the previous frame as a reference.Then, Qiseg is processed in the TD block, where the de-noising process is performed using RCA, resulting in the output QiT r. RCA effectively leverages the similarity be-tween object representations of adjacent frames while miti-gating potential confusion caused by their similarity. To ad-dress the issue of ambiguous object representation initial-ization from the previous frame , RCA incorporates anidentity (ID) mechanism, effectively exploiting the similar-ity between the query (Q) and key (K) to generate accurateoutputs.Finally, the denoised object query QiT r is used as inputfor both the class head and mask head. The class head gen-erates the category output, while the mask head producesthe mask coefficient output.",
  ". Temporal Refiner": "Previous offline video segmentation methods have beenlimited by the inadequate utilization of temporal informa-tion in tightly coupled networks, while current online meth-ods lack a refinement step. To address these challenges,we propose an independent temporal refiner module. Thismodule efficiently utilizes temporal information across theentire video to refine the output generated by the referringtracker.The architecture of the temporal refiner plays a crucialrole in enhancing the models utilization of temporal infor-mation. It takes the object query QT r from the referencetracker as input and produces the refined object query QRfby aggregating temporal information from the entire video.The temporal refiner consists of L temporal decoder blocksconnected in a cascaded operation. Each block comprisesa short-term temporal convolutional block and a long-termtemporal attention block, leveraging motion informationand integrating information from the entire video, respec-tively, through 1D convolutions and standard self-attention.Finally, the mask head generates mask coefficients foreach object in every frame using the refined object queryQRf. Additionally, the class head predicts the class andscore of each object across the entire video using the tem-poral weights of QRf.",
  ". Query-wise ensemble": "In order to adapt the test time augmentation and themodel ensemble methods to the set prediction paradigmin DVIS++, we proposed the query-wise ensemble. Moreconcretely, given the original queries and the queries gen-erated from the augmented videos or by the supplementarymodels, we compare the binary mask corresponding to eachsupplementary query with the original ones, if the IOU be-tween the current binary mask and one of the original binary",
  ". Architecture of DVIS++": "mask is bigger than 0.5, then we average the mask logits andclass logits of the two queries. Otherwise, the supplemen-tary query is added to the original query set.In the above methods, if the supplementary querymatches with one of the original query, we will use it torefine the corresponding original query. Otherwise, we canreckon that the query correspond to a missed object, andthus we can leverage this query to supplement the originalresult. Ideally, our method will refine the edge of the exist-ing masks and assign new masks to the pixel of void class.",
  ". Dataset": "VIPSeg. VIPSeg provides 3,536 videos and 84,750frames with pixel-level panoptic annotations, covering awide range of real-world scenarios and categories, whichis the first attempt to address the challenging task of videopanoptic segmentation in the wild by considering a varietyof scenarios.VIPSeg is divided into train set, validation setand test set each contains 2, 806/343/387 videos respec-tively. VIPSeg showcases a variety of real-world scenesacross 124 categories, consisting of 58 categories of thingand 66 categories of stuff. Due to limitations in comput-ing resources, all the frames in VIPSeg are resized into 720P(the size of the short side is resized to 720) for training andtesting.",
  ". Implementation Details": "In our approach, we retrain the model using the mergedtraining and validation sets of the VIPSeg dataset, follow-ing the default setting provided by the author as presentedbelow. We divide it into three stages to train the segmenter,referring tracker, and temporal refiner. In the first stage,we utilize the pretrained offline model weight which is fine-tuned from the COCO pretrained segmenter weight andemploys VIT-L as the backbone segmenter. In the fol-lowing stages, we freezed the module trained before, andretrained the corresponding module. Besides, Training iscarried out for 20k iterations with a batch size of 8 on 8NVIDIA Tesla V100, and the learning rate is decayed by0.1 at 14k iterations.As for the test time augmentation, we tried horizontalflip, brightness augmentation, contrast augmentation andmulti-scale ensemble which combines the result of the 720pand 800p videos. We also attempted to ensemble the resultsof MaxTron and UniVis , but encountered difficul-ties due to the corrupted weights of MaxTron.",
  ". Comparison with Other Methods": "In the 3rd PVUW Challenge, we ranked third in both thetest phase and development. Our method achieved a VPQ of54.55 in the development phase and 57.01 in the test phase,as shown in the Tab. 1 and Tab. 2. The comparison betweenour method and baseline can be found in Tab. 3.",
  "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, TongLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter fordense predictions. arXiv preprint arXiv:2205.08534, 2022.3": "Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.Panoptic-deeplab:A simple, strong, and fast baselinefor bottom-up panoptic segmentation.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1247512485, 2020. 1 Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-der Kirillov, and Rohit Girdhar.Masked-attention masktransformer for universal image segmentation. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 12901299, 2022. 2 Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexan-der Schwing, and Joon-Young Lee.Tracking anythingwith decoupled video segmentation. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 13161326, 2023. 3",
  "Minghan Li, Shuai Li, Xindong Zhang, and Lei Zhang.Univs:Unified and universal video segmentation withprompts as queries. arXiv preprint arXiv:2402.18115, 2024.3": "Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen,Guangliang Cheng, Yunhai Tong, and Chen Change Loy.Video k-net: A simple, strong, and unified baseline for videosegmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1884718857, 2022. 1 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 3 Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yun-chao Wei, and Yi Yang.Large-scale video panoptic seg-mentation in the wild: A benchmark.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2103321043, 2022. 1, 3 Siyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, andLiang-Chieh Chen. Vip-deeplab: Learning visual perceptionwith depth-aware video panoptic segmentation. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 39974008, 2021. 1 Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, andLiang-Chieh Chen.Max-deeplab:End-to-end panopticsegmentation with mask transformers.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 54635474, 2021. 1 Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu,Min Bai, Ersin Yumer, and Raquel Urtasun.Upsnet: Aunified panoptic segmentation network. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 88188826, 2019. 1"
}