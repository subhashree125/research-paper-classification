{
  "Abstract": "Existing methods for reconstructing objects and humansfrom a monocular image suffer from severe mesh collisionsand performance limitations for interacting occluding ob-jects. This paper introduces a method to obtain a glob-ally consistent 3D reconstruction of interacting objects andpeople from a single image.Our contributions include:1) an optimization framework, featuring a collision loss,tailored to handle human-object and human-human inter-actions, ensuring spatially coherent scene reconstruction;and 2) a novel technique to robustly estimate 6 degrees offreedom (DOF) poses, specifically for heavily occluded ob-jects, exploiting image inpainting. Notably, our proposedmethod operates effectively on images from real-world sce-narios, without necessitating scene or object-level 3D su-pervision. Extensive qualitative and quantitative evaluationagainst existing methods demonstrates a significant reduc-tion in collisions in the final reconstructions of scenes withmultiple interacting humans and objects and a more coher-ent scene reconstruction.",
  ". Introduction": "Existing methods for human and object reconstructions areeither limited to single objects and humans or give limitedperformance for complex images with multiple people andobjects . These methods estimate the3D poses of humans and objects independently and do nottake into account the human-human interactions andeven if they do they generally follow a supervised approach.This leads to large collisions between the mesheswith incoherent reconstructions. We consider the full sceneholistically and exploit information from the human-humanand human-object interactions for spatially coherent andmore complete 3D reconstruction of in-the-wild images. PHOSA pioneered the field and proposed the firstmethod that reconstructs humans interacting with objectsfor in-the-wild images. However, PHOSA does not explic-itly model human-human interactions and gives erroneousreconstructions when objects are heavily occluded whichleads to reconstructions with incorrect depth ordering andmesh collisions.Multi-human model-free reconstructionfrom a single image was proposed in , however, thismethod does not deal with interacting humans. Other meth-ods for multi-human reconstructions generate re-constructions with severe mesh collisions because they re-construct each person independently. To address these chal-lenges, in this paper, we have proposed an optimization-based framework for the spatially coherent reconstructionof scenes with multiple interacting people and heavily oc-cluded objects. The method first reconstructs humans and objects in the image independently. The initialposes of people in the scene are then optimized to resolveany ambiguities that arise from this independent composi-tion using a collision loss, depth ordering loss, and inter-action information. To deal with heavily occluded objects,a novel 6 DOF pose estimation is proposed that uses in-painting to refine the segmentation mask of the occludedobject for significantly improved pose estimation. Finally,we propose a global objective function that scores 3D ob-ject layouts, orientations, collision, and shape exemplars.Gradient-based solvers are used to obtain globally opti-mized poses for humans and objects. Our contributions are: A method for generating a cohesive scene reconstructionfrom a single image by capturing interactions among hu-mans and between humans and objects within the scene,all without relying on any explicit 3D supervision.",
  ". Related Work": "3D humans from a single image: Reconstructing 3D hu-man models from images is often achieved through vari-ous methodologies.One widely used approach involvesfitting parametric models like SMPL to input images. Alternatively, learning-based tech-niques directly predict model parameters such as pose andshape. use statisticalbody models and a large number of 3D scans to recover 3Dhumans from a single image. use 2D poses, uses2D body joint heatmaps and uses GraphCNN to esti-mate SMPL model . However, these methods only es-timate the 3D of a single person in the scene. Methods like recover the 3D poses and shapes of multiplepeople focus on resolving ambiguities that arise due to in-correct depth ordering and collisions between people. How-ever, these methods cannot handle large occlusions. Recentadvancements in whole-body mesh recovery from imageshave shifted the focus from solely on regressing body pa-rameters to also accurately estimating hand and face param-eters. An example is FrankMocap , which employs amodular design. This approach initially runs independent3D pose regression methods for the face, hands, and bodyand integrates their outputs through a dedicated module. Aone-stage pipeline named OSX has been introduced for3D whole-body mesh recovery, surpassing existing multi-stage models in accuracy. It introduces a component-awaretransformer (CAT) comprising a global body encoder and alocal face/hand decoder. KBody represents a methodol-ogy for fitting a low-dimensional body model to an image,employing a predict-and-optimize approach. A distinctivefeature of KBody is the introduction of virtual joints, en-hancing correspondence quality and disentangling the opti-mization process between pose and shape parameters. 3D objects from a single image:Single-view 3D re-construction is a complex task, as it necessitates incor-porating reliable geometric priors derived from our 3Dworld.However, these priors often lack in diverse real-world scenarios . Given their robustnessand accessibility, learning-based methods have emerged.Deep learning approaches can be categorized based on theemployed 3D representations, encompassing voxel-basedframeworks , point cloud-based methods, mesh-based techniques , andimplicit function-based approaches . The major-ity of current single-view 3D mesh reconstruction methodsemploy an encoder-decoder framework. Here, the encoderdiscerns perceptual features from the input image, while thedecoder distorts a template to conform to the desired 3Dshape. The pioneering work by introduced deep learn-ing networks to this task. They employed the VGG net-work as the encoder and utilized a graph convolutionalnetwork (GCN) as the decoder. introduced a methodwherein a 3D shape is represented as a collection of para-metric surface elements, allowing for a flexible representa-tion of shapes with arbitrary topology. addressed topol-ogy changes by proposing a topology modification networkthat adaptively deletes faces. These methods are trained andevaluated on identical object categories. Recent research has also devised techniques for 3D re-construction from image collections without explicit 3Dsupervision.This has been achieved by employing dif-ferentiable rendering to supervise the learning process.For instance, proposed a method that reconstructsthe underlying shape by learning deformations on top ofa category-specific mean shape. developed a dif-ferentiable rendering formulation to learn signed distancefunctions as implicit 3D shape representations, overcom-ing topological restrictions. learned both deforma-tion and implicit 3D shape representations, facilitating re-construction in category-specific canonical space. ex-tended category-specific models into cross-category modelsthrough distillation. used GNN trained on a syntheticdataset without any humans to deduce an objects layout.",
  "D Human Estimation": ". Overview of the proposed method to generate spatially coherent reconstruction from a single image. The steps in red box arenovel. The reconstruction before human pose optimization exhibits notable mesh collisions. After human pose optimization, reduced meshcollisions are seen while maintaining relative coherence between humans. studies have demonstrated remarkable success in capturinghand-object interactions from various perspectives, includ-ing 3D , 2.5D , and images .However, these achievements are limited to hand-object in-teractions and do not extend to predicting the full body.The complexity increases when considering full-body in-teractions, with works like PROX successfully recon-structing or synthesizing 3D humansto adhere to 3D scene constraints. Other approaches focuson capturing interactions from multiple views or reconstructing 3D scenes based on human-scene interac-tions . More recently, efforts have extended to modelhuman-human interactions and self-contacts . used information from the RGBD videos of individualsinteracting with interiors to train a model that understandshow people interact with their surroundings. Access to 3Dscenes gives scene constraints that enhance the perceptionof 3D human poses . uses an optimization-based method to enhance 3D human posture estimates con-ditioned on a particular 3D scene obtained from RGBD sen-sors. Another recent method, , creates a 3D scene graphof people and objects for indoor data. represents theoptimal configuration of the 3D scene, in the form of a parsegraph that encodes the object, human pose, and scene layoutfrom a single image. In our work, we overcome the limita-tions of existing methods by handling not only on human-object interactions but also capturing human-human inter-actions and propose a method that deals with major occlu-sions to significantly improved scene reconstruction.",
  "The proposed method takes a single RGB image as inputand gives a spatially coherent reconstruction of interactinghumans and objects in the scene, an overview is shown in": ". We exploit human-human and human-object in-teractions to spatially arrange all objects in a common 3Dcoordinate system. First, objects and humans are detected,followed by SMPL-based per-person reconstruction (Sec.3.1), which gives incorrect spatial reconstructions with col-lisions between meshes.The human 3D locations/posesare translated into world coordinates and refined througha human-human spatial arrangement optimization using acollision loss (Sec. 3.2). To correctly estimate the 3D ob-ject pose (6-DoF translation and orientation) a differentiablerenderer is used that fits 3D mesh object models to the pre-dicted 2D segmentation masks .We correct the oc-cluded object mask using image inpainting (in Sec. 3.3)unlike PHOSA which uses an occluded object mask.Lastly, we perform joint optimization that takes into ac-count both human-human and human-object interactions fora globally consistent output. Our framework produces plau-sible reconstructions that capture realistic human-humanand human-object interactions.",
  ". Estimating 3D Humans": "Using , we estimate the 3D shape and pose parametersof SMPL given a bounding box for a human . The3D human is parameterized by pose R72, shape R10, and a weak-perspective camera = [, tx, ty] R3.To position the humans in the 3D space, is converted tothe perspective camera projection by assuming a fixed focallength f for all images, and the distance of the person is de-termined by the reciprocal of the camera scale parameter .Thus, the 3D vertices of the SMPL model for the ith humanare represented as: Mi = J(i, i) + [tx, ty, f",
  ". Human Pose Optimisation": "Independently analyzing human 3D poses results in incon-sistent 3D scene configurations. Human-human interactionsoffer useful information to determine the relative spatial ar-rangement and not considering this leads to ambiguities likemesh penetration and incorrect depth ordering. We proposean optimization framework that incorporates human-humaninteractions. We first identify interacting humans in the im-age and then optimize the pose through an objective func-tion to correctly adjust their spatial arrangements.Identifying interacting humans - Our hypothesis positsthat human interactions are contingent upon physical prox-imity in world coordinates.Hence we find the interact-ing humans by identifying the overlap of 3D boundingboxes(More details regarding bounding box overlap crite-ria can be found in the supplementary material).Objective function to optimize 3D spatial arrangementOur objective includes collision (LHcollision), interac-tion (LHinteraction), and depth ordering loss (LHdepth)terms to constraint the pose for interacting humans:LHHILoss = 1LHcollision + 2LHdepth+ 3LHinteraction(1) We optimize (1) using a gradient-based optimizer w.r.t. translation ti R3 and scale parameter si and theRotation Ri SO3 for the ith human instance . The humantranslations are initialized from Sec 3.1. The terms in theobjective function are defined below:Collision Loss (LHcollision) - To overcome the problemof mesh collisions, as seen in existing methods in ,we introduce a collision loss LHcollision that penalizesinterpenetrations in the reconstructed people. Let be amodified Signed Distance Field (SDF) for the scene that isdefined as follows: (x, y, z) = min(SDF(x, y, z), 0)where is positive for points inside the human and is pro-portional to the distance from the surface, and is 0 outside ofthe human. Typically is defined on a voxel grid of dimen-sions NpNpNp. While its definitely possible to generate a single voxelized representation for the entire scene, we of-ten find ourselves requiring an extensive fine-grained voxelgrid. Depending on the scenes extent, this can pose pro-cessing challenges due to memory and computational lim-itations. To overcome this a separate i function is com-puted for each person by calculating a tight box around theperson and voxelizing it instead of the whole scene to re-duce computational complexity . The collision penaltyof person j for colliding with person i is defined as follows:Pij = vMj i(v), where i(v) samples the i value foreach 3D vertex v in a differentiable way from the 3D gridusing trilinear interpolation. If there is a collision betweenperson i and a person j, Pij will be a positive value and de-creases as the separation between them increases. If thereis no overlap between person i and j, Pij will be zero. Letthe translation of person i and person j be Ti and Tj respec-tively. Then the collision loss between them is defined as:",
  "Interaction Loss (LHinteraction) - This is an instance-level to pull the interacting people close together, similarto : LHinteraction =": "hi,hjH (hi, hj)||C(hi) C(hj)||2, where (hi, hj) identifies whether human hi andhj are interacting according to the 3D bounding box over-lap criteria. C(hi) and C(hj) give the centroid for human iand human j respectively.Depth-Ordering Loss (LHdepth) - This helps to achievemore accurate depth ordering, as in . The loss is definedas: Ldepth =",
  "Oursfront-view top-view": ". Comparison of the segmentation masks and reconstruction with PHOSA. The segmentation mask of the bicycle is occludedresulting in erroneous reconstruction in PHOSA. The proposed method uses image inpainting to remove the occlusion to generate a bettersegmentation mask, which leads to a more complete reconstruction. in the ground truth segmentation, the person index at pixelposition p is represented by y(p), and the predicted personindex in the rendered 3D meshes is y(p) and y(p) = y(p).Dy(p)(p) and Dy(p)(p) represent the pixel depths.",
  ". 3D Object Pose Estimation": "After estimating the shape and pose of humans, the next stepis to estimate the same for the objects. To estimate the 3Dlocation t R3 and 3D orientation R SO(3) of the ob-jects. For each object category, exemplar mesh models arepre-selected. The mesh models are sourced from .The vertices of jth object are: V jo = sj(RjO(cj, kj) + tj),where cj is the object category from MaskRCNN , andO(cj, kj) determines the kj th exemplar mesh for cate-gory cj. The optimization framework chooses the exemplarthat minimizes re-projection error to determine kj automat-ically and sj is the scale parameter for jth object.Our first objective is to estimate the 6 DOF pose of eachobject independently. It is difficult to estimate 3D objectpose in the wild as there are: (1) no parametric 3D modelsfor objects; (2) no images of objects in the wild with 2D/3Dpose annotations; and (3) occlusions in cluttered sceneswith humans. We address these challenges by proposing anoptimization-based approach that uses a differentiable ren-derer to fit the 3D object to instance masks from in a manner that is robust to minor/major occlusions.As defined in we calculate a pixel-wise L2 loss overrendered silhouettes S versus predicted masks M but thequality of the predicted mask M is impacted by occlusionsas seen in , which results in a poorly estimated 6 DOFpose. To address problems due to occlusions, we propose anovel method that improves the masks as shown in .Given an image I, a total number of objects N, and bounding boxes for rigid Br and non-rigid Bnr objects,along with their masks - Mr for rigid and Mnr for non-rigid objects. Each ith object can be occluded by maxi-mum N 1 objects. To identify occluding objects we cal-culate the Intersection over Union(IOU) between all pairsof bounding boxes.Objects with IOU > 0.3 (Our se-lection of this threshold stems from our empirical obser-vations, wherein we found that objects with IOU > 0.3led to noticeable improvements in reconstruction quality.Conversely, when IOU was less than 0.3, the reconstructionresults obtained using our method closely resembled thoseproduced by PHOSA , more details in supplementary)are occluding objects M for each object. Occluding objectscan be removed in numerous ways, for e.g remove only oneobject at a time. The total possible combinations, in thiscase, areM1, or you remove a pair of objects at a time andthe total possible combinations, in this case, areM2andso on. The total number of all possible combinations canbe described asM0+M1+M2+ .....MM= 2M. Toremove j occluding objects where j M we need a sin-gle mask Moccmask that is a combination of the j masks,so Moccmask = M1 + M2 + .... + Mj. Now we usethe image-inpainting approach proposed by to removethe occluding objects. We pass the current image I and themask Moccmask to get a new image without occlusionsand use this image to get the new segmentation masks andbounding boxes:",
  "where EC is the image inpainting algorithm and OD isthe object detection algorithm. Sometimes, the ith objectin I may not correspond to the same object in Inew. Lets": "say the index of the ith object in Inew be k. We iterate overthe list of new bounding boxes and calculate the IOU ofthese boxes with Br[i] and, k corresponds to the index ofthe bounding box for which IOU is closest to 1. We usethe mask M newr[k] to determine object pose. Estimating areliable pose also depends heavily on the boundary details.To incorporate this we augment the L2 mask loss with amodified version of the symmetric chamfer loss . Givena no-occlusion indicator (0 if pixel only corresponds to amask of a different instance, else 1), the loss is:",
  ". Joint Optimization": "The joint optimization refines both the human and objectposes estimated above, exploiting both human-human andhuman-object interactions through joint loss functions. Es-timating 3D poses of people and objects in isolation fromone another leads to inconsistent 3D scene reconstruction.Interactions between people and objects provide crucialclues for correct 3D spatial arrangement, which is done byidentifying interacting objects and humans and proposingan objective function for refining human/object poses.Identifying human-object interaction.Our hypothesisposits that human-object interactions are contingent uponphysical proximity in world coordinates. We use 3D bound-ing box overlap between the human and object to determinewhether the object is interacting with a person.Objective function to optimize 3D spatial arrangements.We define a joint loss function that takes into account bothhuman-human and human-object interactions. It is crucialto include both of them because if you simply optimize withregard to human-object interactions, it may result in erro-neous relative positions between interacting people even ifit would enhance the relative spatial arrangement betweenthe interacting humans and objects.",
  ". We optimize (8) using a gradient-based optimizer w.r.t. translation ti R3 and intrinsic scale si R for theith human and, rotation Rj SO(3), translation tj R3": "and sj R for the jth object instance jointly. The objectposes are initialized from Sec. 3.3. Loccsil is the sameas (5) except without the chamfer loss which didnt helpduring joint optimization.Interaction loss (LHOinteraction):This loss handlesboth coarse and fine interaction between humans andobjects as in , defined as:LHOinteraction=Lcoarseinter + Lfineinter.Thecoarseinteractionlossis:Lcoarseinter= hH,oO (h, o)||C(h)C(o)||2,where(h, o)identifies whether human h and object o are interactingaccording to the 3D bounding box overlap criteria. C(h)and C(o) give the centroid for human and the object respec-tively. To handle human interactions, the fine interactionloss is defined as:Lfineinter =",
  "hH,oO(": "Ph,PoP (h,o) (Ph, Po)||C(Ph)C(Po)||2), where Ph and Po are the regions of interac-tion between the humans and the object, respectively.(Ph, Po) is the overlap of the 3D bounding box betweenthe interacting objects, recomputed at each iteration.Collision Loss (LHOcollision) - The formulation of thisloss is similar to the collision loss defined in .2.The difference is that here we take into account the meshcollision between interacting humans and objects in con-trast to interacting humans.Let Nh represent the to-tal number of humans and No total number of objects,then the Loss function is defined as: LHOcollision =Noj=1Nhi=1 Lhioj + Lojhi, where hi represents the ith",
  ". Qualitative and Quantitative Analysis": "Figures 5 and 6 show a qualitative comparison withPHOSA, ROMP and BEV. PHOSA reconstructs both hu-mans and objects; ROMP and BEV only reconstruct hu-mans. As seen our approach yields improved reconstructionquality by effectively mitigating ambiguities arising frommesh collisions and occlusions. . Qualitative comparison on test images from COCO 2017against PHOSA with human-object interactions. Our methodgives better spatial reconstruction while substantially reducing col-lisions(the golden circles delineate regions characterized by note-worthy mesh collisions, while the red circles delineate areas show-casing enhancements in reconstructions). More qualitative resultsare shown in 4.3",
  ". Quantitative evaluation with PHOSA , ROMP,and BEV. BEV and ROMP only reconstruct humans. Equa-tions of each evaluation parameter are given in the supplementary": "For quantitative evaluation, we employ a forced-choiceassessment approach similar to PHOSA on COCO-2017 images since there are no 3D ground truth an-notations for people and objects in images in the wild.From the COCO-2017 test set, we randomly selected a sam-ple of images and performed reconstruction on each im-age. We compare our method with PHOSA, ROMP, andBEV by reconstructing the scenes and comparing the degreeof mesh collisions for human-human EHcol and human-object EHOcol and incorrect depth ordering for human-human EHdepth and human-object EHOdepth interac-tions that results from each method. This is averaged acrossall images to estimate values in .Our approachoutperforms the state-of-the-art techniques for both multi-human and multi-human-object reconstruction, as well asresults in a more coherent and realistic reconstruction withsignificantly fewer ambiguities. . Qualitative results of proposed method on test imagesfrom COCO 2017 compared to PHOSA, ROMP, and BEV forhuman-human interactions. Our method gives more realistic andcoherent reconstructions for images with multiple humans.",
  "%62%73%77%": ". In the ablation study we drop loss terms from our pro-posed method. The higher the percentage, the more the effectof the loss term.No Lcollision implies the exclusion of bothLHcollision and LHOcollision. No Ldepth involves omittingLHdepth and LHOdepth. No Linteraction means we omittedthe LHinteraction and LHOinteraction, and lastly No Loccsilcorresponds to dropping the loss term defined in eq. 5 We also perform a subjective study similar to , wherewe show the reconstructions for each image from PHOSA,ROMP, BEV, and our proposed method in a random order tothe users and the users mark whether our result looks betterthan, equal to, or worse than the other methods. We com-pute the average percentage of images for which our methodperforms better in . Overall, the performance of ourmethod is relatively better than the other methods. .Our method, recovers plausible human-object andhuman-human spatial arrangements by explicitly reasoning aboutthem. Here we demonstrate reconstruction on images with bothhumans and objects and compare PHOSAs reconstructions tothose produced by our method.",
  ". Ablation Study": "An ablative study was conducted to assess the significanceof the loss terms in . The identical forced-choicetest similar to PHOSA is conducted for the completeproposed methodology (Equation 7), by omitting loss termsfrom the proposed method and measuring the performance.Our findings indicate that the exclusion of the collision andocclusion-aware silhouette loss has the most notable effect,with the interaction loss following closely behind. The col-lision loss prevents mesh intersection and the silhouette lossguarantees that the object poses remain consistent with theirrespective masks.",
  ". Discussion": "Current approaches for reconstructing humans/ objectsfrom a single image often produce reconstructions that con-tain various ambiguities, especially in situations involvingmultiple interactions between humans and between humansand objects. In this paper, we perform holistic 3D sceneperception by exploiting the information from both human-human and human-object interactions in an optimizationframework. The optimization makes use of several con- . we illustrate the differences in human reconstructionsgenerated by PHOSA, ROMP, BEV, and Our approach when pro-vided with an input image. Our approach produces more plausiblereconstructions with a substantial decrease in mesh collisions, allwhile maintaining relative coherence. straints to provide a full scene that is globally consistent,and reduces collisions, and improves spatial arrangement() over other methods. The proposed human op-timization framework resolves ambiguities between recon-structed people, and the proposed human-object optimiza-tion framework addresses ambiguities between humans andobjects. We further introduce a method that significantlyimproves the pose estimation of heavily occluded objects.We demonstrate via our qualitative and quantitative evalua-tions that the proposed method outperforms other methodsand produces reconstructions with noticeably less ambigu-ity.",
  "Theobalt, and Gerard Pons-Moll. Detailed human avatarsfrom monocular video. In 2018 International Conference on3D Vision (3DV), pages 98109. IEEE, 2018. 2": "Thiemo Alldieck, Marcus Magnor, Weipeng Xu, ChristianTheobalt, and Gerard Pons-Moll. Video based reconstructionof 3d people models. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 83878397, 2018. 2 Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,Christian Theobalt, and Gerard Pons-Moll. Learning to re-construct people in clothing from a single rgb camera. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 11751186, 2019. 2 Bharat Lal Bhatnagar, Xianghui Xie, Ilya A Petrov, CristianSminchisescu, Christian Theobalt, and Gerard Pons-Moll.Behave: Dataset and method for tracking human object in-teractions.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1593515946, 2022. 3 Federica Bogo, Angjoo Kanazawa, Christoph Lassner, PeterGehler, Javier Romero, and Michael J Black. Keep it smpl:Automatic estimation of 3d human pose and shape from asingle image. In Computer VisionECCV 2016: 14th Euro-pean Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 561578. Springer,2016. 2 Samarth Brahmbhatt, Cusuh Ham, Charles C Kemp, andJames Hays.Contactdb: Analyzing and predicting graspcontact via thermal imaging.In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, pages 87098719, 2019. 3 Samarth Brahmbhatt, Ankur Handa, James Hays, and DieterFox. Contactgrasp: Functional multi-finger grasp synthesisfrom contact. In 2019 IEEE/RSJ International Conferenceon Intelligent Robots and Systems (IROS), pages 23862393.IEEE, 2019. 3 Samarth Brahmbhatt, Chengcheng Tang, Christopher DTwigg, Charles C Kemp, and James Hays.Contactpose:A dataset of grasps with object contact and hand pose. InComputer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XIII16, pages 361378. Springer, 2020. 3 Chao Chen, Zhizhong Han, Yu-Shen Liu, and MatthiasZwicker. Unsupervised learning of fine structure generationfor 3d point clouds by 2d projections matching. In Proceed-ings of the ieee/cvf international conference on computer vi-sion, pages 1246612477, 2021. 2 Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, YixinZhu, and Song-Chun Zhu. Holistic++ scene understanding:Single-view 3d holistic scene parsing and human pose es-timation with human-object interaction and physical com-monsense. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 86488657, 2019. 3",
  "Zhiqin Chen and Hao Zhang. Learning implicit fields forgenerative shape modeling. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition,pages 59395948, 2019. 2": "Christopher B Choy, Danfei Xu, JunYoung Gwak, KevinChen, and Silvio Savarese.3d-r2n2: A unified approachfor single and multi-view 3d object reconstruction. In Com-puter VisionECCV 2016: 14th European Conference, Am-sterdam, The Netherlands, October 11-14, 2016, Proceed-ings, Part VIII 14, pages 628644. Springer, 2016. 1, 2 Enric Corona, Albert Pumarola, Guillem Alenya, FrancescMoreno-Noguer, and Gregory Rogez. Ganhand: Predictinghuman grasp affordances in multi-object scenes. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 50315041, 2020. 3 Enric Corona, Gerard Pons-Moll, Guillem Alenya, andFrancesc Moreno-Noguer. Learned vertex descent: A newdirection for 3d human model fitting. In European Confer-ence on Computer Vision, pages 146165. Springer, 2022.2 Shivam Duggal and Deepak Pathak. Topologically-aware de-formation fields for single-view 3d reconstruction. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 15361546, 2022. 2 Kiana Ehsani, Shubham Tulsiani, Saurabh Gupta, AliFarhadi, and Abhinav Gupta. Use the force, luke! learningto predict physical forces by simulating effects. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 224233, 2020. 3 Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point setgeneration network for 3d object reconstruction from a singleimage. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 605613, 2017. 2 Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-IonutPopa, Vlad Olaru, and Cristian Sminchisescu.Three-dimensional reconstruction of human interactions. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 72147223, 2020. 3 Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-IonutPopa, Vlad Olaru, and Cristian Sminchisescu.Learningcomplex 3d human self-contact.In Proceedings of theAAAI Conference on Artificial Intelligence, pages 13431351, 2021. 3 Dariu M Gavrila. Pedestrian detection from a moving vehi-cle. In Computer VisionECCV 2000: 6th European Con-ference on Computer Vision Dublin, Ireland, June 26July 1,2000 Proceedings, Part II 6, pages 3749. Springer BerlinHeidelberg, 2000. 6 Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-hinav Gupta.Learning a predictable and generative vec-tor representation for objects. In Computer VisionECCV2016: 14th European Conference, Amsterdam, The Nether-lands, October 11-14, 2016, Proceedings, Part VI 14, pages484499. Springer, 2016. 1",
  "Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Meshr-cnn. In Proceedings of the IEEE/CVF international confer-ence on computer vision, pages 97859795, 2019. 2": "Thibault Groueix,Matthew Fisher,Vladimir G Kim,Bryan C Russell, and Mathieu Aubry. A papier-mache ap-proach to learning 3d surface generation. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 216224, 2018. 2 Riza Alp Guler and Iasonas Kokkinos.Holopose: Holis-tic 3d human reconstruction in-the-wild. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1088410894, 2019. 2 Zhizhong Han, Chao Chen, Yu-Shen Liu, and MatthiasZwicker. Drwr: A differentiable renderer without render-ing for unsupervised 3d structure learning from silhouetteimages. arXiv preprint arXiv:2007.06127, 2020. 2 Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas,and Michael J Black. Resolving 3d human pose ambiguitieswith 3d scene constraints. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 22822292, 2019. 1, 3 Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dim-itrios Tzionas, and Michael J Black. Populating 3d scenesby learning human-scene interaction.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1470814718, 2021. 3 Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kale-vatykh, Michael J Black, Ivan Laptev, and Cordelia Schmid.Learning joint reconstruction of hands and manipulated ob-jects. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1180711816,2019. 3",
  "Katsushi Ikeuchi and Berthold KP Horn. Numerical shapefrom shading and occluding boundaries.Artificial intelli-gence, 17(1-3):141184, 1981. 2": "Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, XiaoweiZhou, and Kostas Daniilidis.Coherent reconstruction ofmultiple humans from a single image.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 55795588, 2020. 1, 2, 4 Yuheng Jiang, Suyi Jiang, Guoxing Sun, Zhuo Su, Kai-wen Guo, Minye Wu, Jingyi Yu, and Lan Xu. Neuralhofu-sion: Neural volumetric rendering under human-object in-teractions.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 61556165, 2022. 3 Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-emplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In 2021 InternationalConference on 3D Vision (3DV), pages 4252. IEEE, 2021.1, 3 Angjoo Kanazawa, Michael J Black, David W Jacobs, andJitendra Malik. End-to-end recovery of human shape andpose. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 71227131, 2018. 1, 2 Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, andJitendra Malik. Learning category-specific mesh reconstruc-tion from image collections.In Proceedings of the Euro-pean Conference on Computer Vision (ECCV), pages 371386, 2018. 2",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization.arXiv preprint arXiv:1412.6980,2014. 4, 6": "Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-shick. Pointrend: Image segmentation as rendering. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 97999808, 2020. 3, 5, 6 Hedvig Kjellstrom, Danica Kragic, and Michael J Black.Tracking people interacting with objects.In 2010 IEEEComputer Society Conference on Computer Vision and Pat-tern Recognition, pages 747754. IEEE, 2010. 3 Muhammed Kocabas, Nikos Athanasiou, and Michael JBlack.Vibe: Video inference for human body pose andshape estimation.In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages52535263, 2020. 2 Nikos Kolotouros, Georgios Pavlakos, Michael J Black, andKostas Daniilidis. Learning to reconstruct 3d human poseand shape via model-fitting in the loop. In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 22522261, 2019. 1, 2 Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani-ilidis. Convolutional mesh regression for single-image hu-man shape reconstruction. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 45014510, 2019. 2 Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, and Abhi-nav Gupta. 3d-relnet: Joint object and relational network for3d prediction. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 22122221, 2019. 2 Abhijit Kundu, Yin Li, and James M Rehg.3d-rcnn:Instance-level 3d object reconstruction via render-and-compare. In Proceedings of the IEEE conference on com-puter vision and pattern recognition, pages 35593568,2018. 5 Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, VarunJampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervisedsingle-view 3d reconstruction via semantic consistency. InComputer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XIV16, pages 677693. Springer, 2020. 2",
  "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence": "Zitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 2, 6, 7 Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft ras-terizer: A differentiable renderer for image-based 3d reason-ing. In Proceedings of the IEEE/CVF international confer-ence on computer vision, pages 77087717, 2019. 2",
  "R Mask. Cnn/he k., gkioxari g., dollar p., girshick r. In 2017IEEE International Conference on Computer Vision (ICCV),2017. 3, 5": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-bastian Nowozin, and Andreas Geiger. Occupancy networks:Learning 3d reconstruction in function space. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 44604470, 2019. 2 Lea Muller, Ahmed AA Osman, Siyu Tang, Chun-Hao PHuang, and Michael J Black. On self-contact and humanpose. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 99909999,2021. 3 Armin Mustafa, Akin Caliskan, Lourdes Agapito, andAdrian Hilton. Multi-person implicit reconstruction from asingle image. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1447414483, 2021. 1, 2",
  "Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi,and Mehran Ebrahimi.Edgeconnect: Generative imageinpainting with adversarial edge learning.arXiv preprintarXiv:1901.00212, 2019. 5": "Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-ter Gehler, and Bernt Schiele. Neural body fitting: Unify-ing deep learning and model based human pose and shapeestimation. In 2018 international conference on 3D vision(3DV), pages 484494. IEEE, 2018. 2 Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, andKui Jia. Deep mesh reconstruction from single rgb imagesvia topology modification networks. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 99649973, 2019. 2 Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and KostasDaniilidis. Learning to estimate 3d human pose and shapefrom a single color image. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages459468, 2018. 2 Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, andMichael J Black.Expressive body capture:3d hands,face, and body from a single image.In Proceedings of",
  "Gerard Pons-Moll and Bodo Rosenhahn. Model-based poseestimation. Visual Analysis of Humans: Looking at People,pages 139170, 2011. 2": "Stefan Popov, Pablo Bauszat, and Vittorio Ferrari. Corenet:Coherent 3d scene reconstruction from a single rgb image. InComputer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part II 16,pages 366383. Springer, 2020. 2 Stephan R Richter and Stefan Roth. Matryoshka networks:Predicting 3d geometry via nested shape layers. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 19361944, 2018. 2 Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap:A monocular 3d whole-body pose estimation system via re-gression and integration. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 17491759, 2021. 2 Bodo Rosenhahn,Christian Schmaltz,Thomas Brox,Joachim Weickert, Daniel Cremers, and Hans-Peter Seidel.Markerless motion capture of man-machine interaction. In2008 IEEE Conference on Computer Vision and PatternRecognition, pages 18. IEEE, 2008. 3 Antoni Rosinol, Arjun Gupta, Marcus Abate, Jingnan Shi,and Luca Carlone. 3d dynamic scene graphs: Actionablespatial perception with places, objects, and humans. arXivpreprint arXiv:2002.06289, 2020. 3 Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-alignedimplicit function for high-resolution clothed human digitiza-tion. In Proceedings of the IEEE/CVF international confer-ence on computer vision, pages 23042314, 2019. 2 Shunsuke Saito, Tomas Simon, Jason Saragih, and HanbyulJoo. Pifuhd: Multi-level pixel-aligned implicit function forhigh-resolution 3d human digitization.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 8493, 2020. 2",
  "Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. arXivpreprint arXiv:1409.1556, 2014. 2": "Guoxing Sun, Xin Chen, Yizhang Chen, Anqi Pang, PeiLin, Yuheng Jiang, Lan Xu, Jingyi Yu, and Jingya Wang.Neural free-viewpoint performance rendering under complexhuman-object interactions. In Proceedings of the 29th ACMInternational Conference on Multimedia, pages 46514660,2021. 3 Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and TaoMei. Monocular, one-stage, regression of multiple 3d people.In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 1117911188, 2021. 1, 4, 6, 7 Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael JBlack. Putting people in their place: Monocular regressionof 3d people in depth. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1324313252, 2022. 1, 4, 6, 7 Omid Taheri, Nima Ghorbani, Michael J Black, and Dim-itrios Tzionas. Grab: A dataset of whole-body human grasp-ing of objects. In Computer VisionECCV 2020: 16th Eu-ropean Conference, Glasgow, UK, August 2328, 2020, Pro-ceedings, Part IV 16, pages 581600. Springer, 2020. 3",
  "Kalyan Vasudev Alwala, Abhinav Gupta, and Shubham Tul-siani. Pre-train, self-train, distill: A simple recipe for super-sizing 3d reconstruction. arXiv e-prints, pages arXiv2204,2022. 2": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, WeiLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d meshmodels from single rgb images. In Proceedings of the Euro-pean conference on computer vision (ECCV), pages 5267,2018. 2 Zhenzhen Weng and Serena Yeung. Holistic 3d human andscene mesh estimation from single view images. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 334343, 2021. 3 Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, andJosh Tenenbaum. Learning a probabilistic latent space ofobject shapes via 3d generative-adversarial modeling. Ad-vances in neural information processing systems, 29, 2016.2 Masanobu Yamamoto and Katsutoshi Yagishita.Sceneconstraints-aided tracking of human body. In ProceedingsIEEE Conference on Computer Vision and Pattern Recogni-tion. CVPR 2000 (Cat. No. PR00662), pages 151156. IEEE,2000. 3 Hongwei Yi,Chun-Hao P Huang,Dimitrios Tzionas,Muhammed Kocabas, Mohamed Hassan, Siyu Tang, JustusThies, and Michael J Black. Human-aware object placementfor visual environment reconstruction.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 39593970, 2022. 3 Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchis-escu.Monocular 3d pose and shape estimation of mul-tiple people in natural scenes-the importance of multiplescene constraints. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 21482157, 2018. 2 Andrei Zanfir, Elisabeta Marinoiu, Mihai Zanfir, Alin-IonutPopa, and Cristian Sminchisescu. Deep network for the inte-grated 3d sensing of multiple people in natural images. Ad-vances in Neural Information Processing Systems, 31, 2018.2 Jason Y Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan,Jitendra Malik, and Angjoo Kanazawa.Perceiving 3dhuman-object spatial arrangements from a single image inthe wild. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceed-ings, Part XII 16, pages 3451. Springer, 2020. 1, 3, 4, 5,6, 7, 8 Siwei Zhang, Yan Zhang, Qianli Ma, Michael J Black, andSiyu Tang. Place: Proximity learning of articulation and con-tact in 3d environments. In 2020 International Conferenceon 3D Vision (3DV), pages 642651. IEEE, 2020. 3 Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke,Vladimir Guzov, and Gerard Pons-Moll. Couch: Towardscontrollable human-chair interactions. In European Confer-ence on Computer Vision, pages 518535. Springer, 2022.3 Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, andGerard Pons-Moll.Toch: Spatio-temporal object-to-handcorrespondence for motion refinement. In European Con-ference on Computer Vision, pages 119. Springer, 2022. 3"
}