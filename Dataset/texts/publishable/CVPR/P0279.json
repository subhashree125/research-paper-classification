{
  "Abstract": "Self-supervised learning (SSL) has been successful inbuilding patch embeddings of small histology images (e.g.,224 224 pixels), but scaling these models to learnslide embeddings from the entirety of giga-pixel whole-slide images (WSIs) remains challenging. Here, we lever-age complementary information from gene expression pro-files to guide slide representation learning using multi-modal pre-training. Expression profiles constitute highlydetailed molecular descriptions of a tissue that we hypothe-size offer a strong task-agnostic training signal for learn-ing slide embeddings.Our slide and expression (S+E)pre-training strategy, called TANGLE, employs modality-specific encoders, the outputs of which are aligned via con-trastive learning. TANGLE was pre-trained on samples fromthree different organs: liver (n=6,597 S+E pairs), breast(n=1,020), and lung (n=1,012) from two different species(Homo sapiens and Rattus norvegicus). Across three inde-pendent test datasets consisting of 1,265 breast WSIs, 1,946lung WSIs, and 4,584 liver WSIs, TANGLE shows signifi-cantly better few-shot performance compared to supervisedand SSL baselines. When assessed using prototype-basedclassification and slide retrieval, TANGLE also shows a sub-stantial performance improvement over all baselines. Codeavailable at",
  "k=25k=10k=5ABMILINTRA (S)TANGLE (S+E)": ". Few-shot performance. TANGLE linear probing per-formance compared to multiple instance learning (ABMIL) andintra-modality slide SSL (INTRA). TANGLE uses gene expression(E) to guide slide pre-training (S) using multimodal contrastivelearning (S+E). Results on independent cohorts for BRCA sub-typing (human breast, n=1,265 WSIs), NSCLC subtyping (humanlung, n=1,946 WSIs), and TG-GATEs lesion classification (ratliver, n=4,584 WSIs). k: number of training samples per class. Because of this size constraint, most CPath approachesadopt a divide-and-conquer strategy that consists of (1) tes-sellating the WSI into small patches and (2) extracting low-dimensional patch embeddings with a frozen pre-trainednetwork. Until recently, the prevalent practice involved re-lying on networks pre-trained on ImageNet .However, with the advent of SSL, this step is replaced byhistopathology-specific visual encoders orvision-language encoders , in most cases trainedon human cancer samples.The resulting patch embed-dings constituting the WSI can then be fed to weakly-supervised models for classification as done in Multiple In-stance Learning .",
  "arXiv:2405.11618v1 [cs.CV] 19 May 2024": "43, 44, 78, 95]. The resulting slide embeddings can serveas input for various downstream tasks with minimal or notraining, enabling slide classification with few-shot learn-ing and prototyping, slide retrieval, and case stratification.In addition, as the embedding space is learned without ne-cessitating pathologist annotations, the risk of using noisylabels inherent in inter-observer variability is greatly miti-gated . However, building slide embeddings with SSLremains challenging as (1) constructing slide views basedon patch-level augmentations requires extracting multiplepatch embeddings per patch, which is computationally ex-pensive; (2) the visual primitives and invariances that needto be learned (such as being able to detect edges in natu-ral images) become unclear when scaling to very large in-puts; and (3) intra-slide heterogeneity can prevent derivinga consistent and strong training signal, especially when us-ing masked image modeling. Instead, inspired by multimodal vision-language mod-els, we leverage gene expression data to guide slide repre-sentation learning into a slide-expression (S+E) pre-trainingmodel. Gene expression data, such as measured with RNAsequencing, are known to be strong indicators of diseasestate, with molecular signatures predictive of cancer sub-type , patient survival , and drug toxicity , amongothers. Intuitively, the histology slide (S) and correspond-ing expression data (E) provide different views of the sameunderlying biological processes:gene expression formsa highly detailed molecular description of tissue, with asmany descriptors as there are transcriptomic measurements,albeit lacking spatial information.Conversely, histologyslides offer a finely detailed spatial representation of the tis-sue but with only two markers, namely, the hematoxylinand eosin combination represented as RGB channels. Con-sequently, molecular alterations, as detected through bulktranscriptomics, can be exhibited as discernible morpho-logical patterns when examining the associated histologyslides .We hypothesize that guiding sliderepresentation learning with expression constitutes a muchstronger training signal than using slide augmentations ormasking. Here, we follow a multimodal contrastive learningparadigm where (S+E) pairs are aligned during a pre-training stage. Specifically, we address the modality hetero-geneity gap by employing modality-specific encoders yield-ing a slide and expression embeddings that are aligned usinga symmetric contrastive objective. Our models are basedon large cohorts of publicly available (S+E) pairs, namelyThe Cancer Genome Atlas (TCGA) developed for studyinghuman cancer and the Toxicogenomics Project-GenomicsAssisted Toxicity Evaluation System (TG-GATEs) devel-oped for assessing drug toxicity in rat model animals. (S+E)models are trained on multiple species (Homo sapiens andRattus norvegicus) and sites (liver, breast, and lung), that we test on a panel of downstream tasks. To summarize,our contributions are: (1) the first self-supervised visionencoder for rat tissue trained on 15 million patches from47,227 WSIs; (2) TANGLE, a transcriptomics-guided sliderepresentation learning framework trained on thousands of(S+E) pairs using multimodal contrastive learning; (3) a se-ries of few-shot classification, prototype-based classifica-tion, and slide retrieval experiments for lesion classificationin rat liver and cancer subtyping in human breast and lungcancer that show the predictive capabilities of TANGLE; and(4) a post-hoc interpretability analysis that enables derivinginsights about the aligned latent space.",
  ". Self-supervised visual representation learning": "The combination of Vision Transformers (ViTs) and SSL has proved to be a powerful tool for build-ing task-agnostic image representations. SSL can be cate-gorized into (1) contrastive approaches , whose un-derlying principle is to attract different representations ofthe same image (e.g., two distinct augmentations) whilesimultaneously pushing away representations of dissimi-lar images; (2) reconstruction approaches , whichaim to recover specific portions of an occluded image fromthe remaining parts of the same image; and (3) approachescombining both objectives . Representation learn-ing in vision has also evolved towards multimodal vision-language models . Thesame principles remain, where, for instance, the embeddingof an image caption can be pulled close to the image (in acontrastive fashion), or partially masked with the objectiveto reconstruct the caption from the image. Vision-languagemodels are also becoming prevalent in medical applications,by leveraging medical reports and textbooks . Ourwork aligns with this idea, where we align expression pro-files with the slide representation.",
  ". Self-supervised learning in CPath": "Encoding histology patches: Most works applying SSL toCPath focus on building embeddings from image patches(typically 256 256-pixel regions) . State-of-the-art methods are using a com-bination of contrastive- and reconstruction-based objectivestrained with a student-teacher learning paradigm .Patch-level SSL is trained on increasingly large datasetsand models (e.g., ViT-Huge trained on 1.5M slides in ).These can be based on public archives such as TCGA orCPTAC , on internal cohorts , or amix of public and private datasets . Recently, vision-language encoders designed for pathology have also beenproposed , and rely on large-scale paired datascraped from social media, textbooks, or publications. All",
  "( = 1000)": ". Overview of TANGLE for (S+E) pre-training. An input histology slide is tessellated into patches and encoded using a pre-trained vision encoder. The resulting patch embeddings are passed to an ABMIL module to derive a slide embedding. The correspondinggene expression data are encoded using an MLP. A symmetric contrastive objective LsymCL learns to align embeddings from both modal-ities. During inference, a query slide is encoded into a slide embedding by the trained pooling module to be used for downstream tasks. these models are solely based on human tissue, most ofwhich are cancer samples. Here, we complement these byintroducing the first vision encoder for rodent tissue mi-croscopy, which plays a pivotal role in drug safety andbiomarker discovery.Encoding histology slides: Methods to build slide em-beddings using SSL remain relatively scarce.Chen etal. proposed a three-stage pre-training pipeline to hi-erarchically aggregate increasingly large tiles, from patchesto patch embeddings to region embeddings to slide embed-dings. Follow-up works improved pre-training using morecomplex training signals based on intra- and inter-slide sim-ilarity losses , masked autoencoding or patchprototyping .",
  ". Supervised learning in CPath": "Multiple Instance Learning:MIL is the currentde-facto approach for WSI classification.In particular,Attention-based MIL (and its many extensions) has beenused extensively in CPath . Context-aware extensionshave also been proposed, such as based on graph neural net-works and Transformers . During (S+E)pre-training, we also employ MIL to pool pre-extractedpatch embeddings into a slide embedding that we furtheruse for SSL contrastive learning.Multimodal learning: While the representation learningcapabilities of (S+E) pre-training remain poorly under-stood, the multimodal integration of histology with geneexpression data has been extensively studied in cancer-specific and pan-cancer works, especially for prognostica-tion . Several mechanisms have been proposed such as late or early fusion usingmultimodal token aggregration . Although notdirectly connected to our approach, they motivate exploringthe connection between gene expression profiles and tissuemorphology. Notably, recent studies more closely alignedwith (S+E) pre-training and demonstrated improved mul-timodal downstream performance through multimodal pre-training utilizing histology and expression data . Computational Toxicologic Pathology (CompToxPath):The majority of work in CPath is centered around studyinghuman cancer. CompToxPath is emerging as a new sub-field that aims to augment drug safety assessment using AI,especially at the pre-clinical stage . CompToxPath hasbeen used for organ identification , detecting abnormal-ities , such as necrosis and hypertrophy detec-tion. However, none of these works include SSL or large-scale evaluations. This work bridges this gap by applying(S+E) pre-training to large-scale toxicology datasets.",
  ". Method": "Here,wepresentourframework,TANGLE,forTrANscriptomics-GuidedsLidErepresentationlearn-ing (see ). TANGLE is composed of (1) a visionencoder that encodes patches into patch embeddings, fol-lowed by a pooling module for learning a slide embedding(.1), (2) a gene expression encoder that combinestranscriptomic measurements into an expression embedding(.2), and (3) a multimodal alignment module thatlearns to align both spaces (.3). TANGLE is testedon a variety of downstream tasks ().",
  ". Slide encoder": "Given a histology slide Xi Rdxdy3, we follow the MILparadigm , which consists of tessel-lating the slide into small patches, using a pre-trained vi-sion encoder to extract patch embeddings, and pooling theresulting patch embeddings into a slide embedding.Pre-trained patch encoding: For encoding rat tissue, wetrained from scratch a ViT-Base (86 million parameters)with iBOT on 15 million H&E patches extracted from47,227 WSIs for 80 epochs, which we denote as iBOT-Tox.This is, to date, the largest SSL model for non-human his-tology tissue (see Supplemental). For encoding human tis-sue, we use CTransPath , a state-of-the-art publiclyavailable vision encoder.CTransPath was trained on 15million patches from over 32,000 WSIs using a tiny SwinTransformer . We denote the resulting patch embed-dings of the i-th slide Xi as Hi RNHdH, where NH isthe number of patch embeddings and dH their dimension.MIL slide encoding:We learn a function f(Hi):RNHdH Rd that maps the set of patch embeddingsHi RNHdH into a slide embedding hi Rd. Here, weuse the popular attention-based MIL model (ABMIL) ,which consists of learning patch-level attention weightsused for pooling embeddings into a slide embedding.",
  ". Gene expression encoder": "Given a set of raw transcriptomic measurements quanti-fied across NG genes, we compute the log2 fold changerelative to a control group, which represents gene expres-sion deviations from a normal sample and, therefore, en-code the magnitude of gene overexpression or underexpres-sion (see Supplemental). The log2 fold change normalizedtranscriptomics associated with Xi, denoted as ti RNG,can be seen as tabular data, which can efficiently be en- coded with a multilayer perceptron (MLP) and named as(). Specifically, we train a 3-layer MLP to learn a map-ping (ti) : RNG Rd to project a set of selected geneexpressions ti RNG to an expression embedding gi Rd.",
  ". Multimodal alignment": "Pre-training contrastive alignment: We align the em-bedding space of the slide and expression encoders us-ing a symmetric cross-modal contrastive learning objective.This is a widely employed representation learning formu-lation , especially in visual-language pre-training .Formally, we define a batch as a set of M (S+E) pairs(hi, gi)Mi=1, where hi and gi are the i-th slide embeddingand gene expression profiles, respectively. For a given pair(hi, gi), the objective is given by:",
  "(1)": "where the first term represents the slide-to-expression con-trastive loss, and the second term represents the expression-to-slide contrastive loss.Each term maximizes the dot-product similarity between embeddings from the same pairnormalized (with Softmax) by negative pairs, which can beinterpreted as other classes.Complementary objective: As an alternative to the con-trastive loss, we introduce (1) an expression reconstructionobjective LREC framed as an expression regression task, and(2) a vision-only intra-modality objective LINTRA that aimsto align different random subsets of the slide (locallocalalignment) and random subsets with the average patch em-",
  "j HN (i)ijH, and where hi,1 and hi,2 are": "slide embedding views derived from different random patchembedding subsets (e.g., 2048 patches). These variants arereferred to as TANGLE-REC and INTRA, respectively.Inference: During inference, the query slide is passedthrough the vision encoder to extract patch embeddingsand then to the MIL module to derive the slide embeddingthat encodes the morphological manifestations of the corre-sponding molecular signatures. We use the resulting slideembeddings for few-shot classification using linear probingand prototyping, and slide retrieval (see ).",
  ". Dataset": "TG-GATEs: We collected all slides from the TG-GATEsportal , which comprises 23,136 liver and 28,747 kid-ney slides ( 25TB of raw data). All slides are liver andkidney sections from Sprague-Dawley (SD) rats acquired inpre-clinical drug safety studies on 157 compounds. Eachslide represents the morphological changes (lesions) ob-served after the administration of a particular drug dosageat a specified time point of sacrifice, denoted as a samplegroup. We manually curated the liver annotations into sixclasses (multi-label classification). We used a subset of 29studies (n=4,584 WSIs, liver only) as an independent testcohort. Other studies (both liver and kidney slides) are usedfor iBOT-Tox pre-training, (S+E) pre-training, and few-shot training. We additionally collected the correspondinggene expression profiles (microarrays) of 6,597 liver slidesand selected the top 1,000 genes with the largest log2 foldchange (see Supplemental).TCGA: We collected 1,041 primary cases from the TCGABreast Invasive Carcinoma (TCGA-BRCA) cohort, whichcomprises 831 Invasive Ductal Carcinoma (IDC) and 210Invasive Lobular Carcinoma (ILC). We additionally col-lected 1,031 primary cases from the TCGA Non-SmallCell Lung Cancer (TCGA-NSCLC) cohort, among which",
  "or maximal available labeled samples per class": "TANGLE when replacing it with TransMIL (see Fig-ure 2). This modification leads to a performance drop of3.92% AUC. We hypothesize that (1) the tasks we focus on(TG-GATEs lesion classification and TCGA lung and breastsubtyping) are predominantly morphological, thereby re-ducing the utility of context modeling, (2) ABMIL trainingcan use larger batch sizes due to reduced memory require-ments; and (3) our ABMIL implementation uses moderntricks such as a deeper pre-attention network and Layer-Norm (see implementation).Hyper-parameter search: presents a series of ex-periments with different hyper-parameters known to influ-ence contrastive pre-training, namely, the batch size, theSoftmax temperature, and the number of patches sampledper slide. Batches larger than 64 seem to perform equallywell. Softmax temperatures that are too high lead to a se-vere performance drop. Finally, the number of tokens (orpatches) sampled per slide has relatively little influence onthe downstream few-shot performance.",
  ". Linear probing few-shot classification": "We evaluate (S+E) pre-training in a few-shot classifica-tion scenario for lesion detection in liver (), andbreast and lung cancer subtyping (). Following stan-dard practice in SSL , we employ linear probing forbenchmarking TANGLE, TANGLE-REC, and INTRA. In ad-dition, we benchmark HIPT and baselines based onthe average patch embeddings using different backbones(denoted as ResNet50+Avg., CTransPath+Avg. and iBOT-Tox+Avg.). Finally, we include two supervised MIL base-lines, ABMIL and TransMIL (see , left).Baselines are trained five times () and ten times (Ta-ble 2), using k randomly sampled examples per class.TANGLE vs. supervised MIL: TANGLE significantly out-performs all MIL baselines in the three datasets with an ab-solute gain of +5.9% in liver, +11.0% in breast, and +6.2%in lung compared to ABMIL for k=10. ABMIL leads toconsistently better performance than TransMIL, which wehypothesize is due to (1) the use of a simpler architecturebeneficial in low-data regimes and (2) tasks where the cel-lular morphology is more informative than the global tissuestructure.TANGLE vs. averaging vs. MIL: Despite the simplicityof these baselines, averaging provides performance that ison par with MIL in breast subtyping and liver lesion de-tection. We also observe that employing domain-specificvision encoders leads to substantial improvements, withCTransPath+Avg.outperforming ResNet50+Avg., whichour iBOT-Tox+Avg.model in liver lesion detection sig-nificantly outperforms in TG-GATEs (+9.6% and +11.0%compared to CTransPath+Avg.and ResNet50+Avg.fork=10).TANGLE vs. INTRA vs. HIPT: INTRA and HIPT providesimilar performance in breast and lung, but are both signifi-cantly outperformed by TANGLE (+12.9% and +16.1% fork=10 in breast and lung compared to INTRA). Both HIPTand INTRA are only marginally better or similar to the av-erage patch embedding, which highlights the complexity ofslide-level SSL. . Few-shot lesion classification in rat liver. Comparison of lesion classification (multi-label classification) using MIL vs. TANGLEand variations with linear probing, and evaluated using Macro-AUC (as %). All models are tested on an independent test cohort comprising4,584 slides, without any data leakage from unimodal and multimodal pre-training. Standard deviation reported over five runs.",
  "Linear probing": "ResNet50+Avg. 55.0 3.357.7 11.860.5 9.668.6 8.072.7 7.8CTransPath+Avg. 56.9 4.456.5 10.561.9 8.370.5 8.173.9 6.1iBOT-Tox+Avg. (ours)53.9 5.363.5 6.971.5 6.179.7 5.081.9 6.2iBOT-Tox+Intra (ours)56.3 7.362.6 10.372.7 7.480.2 8.483.3 8.0TANGLE-Rec (ours)73.8 13.575.5 14.378.3 12.281.8 10.882.7 8.8TANGLE (ours)72.1 11.680.1 11.384.7 9.086.3 7.986.9 7.6 . Few-shot cancer subtype classification in human breast and lung. All models are tested on an independent test cohortcomprising 1,265 breast slides and 1,946 lung slides and evaluated using Macro-AUC. Standard deviation reported over ten runs.",
  "( 19.1)( 8.0)( 5.0)( 3.7)( 6.0)( 4.1)( 2.1)( 1.3)": "TANGLE vs. TANGLE-REC: TANGLE-REC shows surpris-ingly high performance for k=1, but is outperformed forlarger values of k. We hypothesize that TANGLE-REC ren-ders simplified embeddings (i.e., low-rank, see next Sec-tion), which makes one-shot learning easier but cannot ex-press complex morphological subtleties. Loss ablation: In TG-GATEs relative to TANGLE, adding aTANGLE-REC objective gives +0.05% AUC, adding INTRAon top gives -0.8% AUC, and -2.0% AUC when solelycomplementing TANGLE with INTRA. We hypothesize that staining differences between train and test cause the INTRAobjective to overfit, leading to worse performance. Replac-ing the cross-modal contrastive loss with an L1 objectivegives -6.7% AUC and -7.0% AUC with an L2 (some designsconceptually similar to , see Supplemental).",
  "We also assess the capacity of TANGLE to construct slide-level prototypes capable of predicting specific morpholog-ical characteristics. Specifically, we define a positive slide": ".Prototype-based classification.Comparison ofTANGLE and baselines for identifying study-level morphologiesevaluated using macro-AUC. Prototypes are defined as the aver-age of k slides selected from the study. Full training is an ABMILtrained on TG-GATEs train set (n=18,552). Standard deviationreported over 100 bootstrapping iterations. prototype p+ as the average of k (k=1,3,5) slide embed-dings with a morphology of interest. Similarly, a normalprototype p0 is defined using k normal slides, where themorphology under consideration is absent. Subsequently,we gauge the similarity between a query slide qi and thetwo prototypes using the L2-distance the distances inter-preted as confidence prediction used for classification, i.e.,qi p+ and qi p0, (see , center). We ap-ply this method to detect two types of lesions within theTG-GATEs test set, namely (1) eosinophilic degenerationin thioacetamide (n=170), and (2) bile duct proliferation inmethylene dianiline (n=170). This setup mirrors a realisticapplication of AI, where the identification of a drug-inducedmorphology on k slides enables detecting if this morphol-ogy is present in slides from the same study, thereby en-abling synergies between doctors and AI systems.As shown in , TANGLE and TANGLE-REC out-perform all baselines in both studies. Compared to an AB-MIL model trained on 100% of TG-GATEs (n=18,552),TANGLE with k 3 leads to better performance. Thishighlights that (1) TG-GATEs includes study-specific mor-phologies that can be challenging to model, and (2) proto-typing can help address this gap with minimal effort.",
  "Retrieval performance": ". Slide retrieval. Comparison of TANGLE and baselinesfor retrieving slides with drug-induced lesions from the same sam-ple group in TG-GATEs test. Recall@k quantifies the count of re-trieved instances within the top-k most similar slides normalizedby the number of instances to retrieve (four per sample group).Standard deviation reported over 100 bootstrapping iterations. to retrieve all slides that share the same sample group char-acteristics as the query, thereby demonstrating the capabil-ity of TANGLE to capture compound-, dose- and sacrifice-specific features. Specifically, we compute the Recall@k(k=5, 10, 20), which measures the proportion of relevantslides that appear among the k most similar slides, with fourbeing the total number of slides to retrieve in this context.The slide similarity is quantified using the cosine distancemetric applied to the unnormalized slide embeddings (see, right).As presented in , TANGLE reaches the best re-trieval performance with on average 2.88/4 slides correctlyretrieved among the top-k=10 instances and 3.44/4 amongthe top-k=20 instances. This result highlights that TANGLEcan capture subtle morphological differences, such as thoseinduced by administering different doses or sacrifice times.Overall, results from and 5 ascertain the con-clusion from the few-shot evaluation in that (1) (S+E)pre-training can capture task-agnostic features that can beused for downstream tasks, (2) intra-modality pre-trainingcan outperform averaging, but their training signal remainsweak, and (3) in-domain patch feature extractors greatlyimprove downstream performance. Additional experimentsablating TANGLE and INTRA losses, and showing the im-pact of hyper-parameters (batch size, temperature, numberof sampled patches) are presented in the Supplemental.",
  ". Interpretability": "To better understand TANGLE properties, we analyzed therank of the space spawned by the test slide embeddings(computed using the entropy of the d largest singular val-ues of the embedding matrix, see Supplemental). Indeed,rank has been shown to be a predictor of downstream per-formance and constitute a necessary (but not sufficient)condition for discriminative latent spaces . We observe",
  "mm": ". Interpretability of TANGLE. Top: Visualization of the attention weights of TANGLE in a TG-GATEs liver slide. High-attentionregions highlight lesions (hepatocellular hypertrophy and fatty change). Left: Integrated Gradient (IG) scores of the gene expressionencoder. High-importance genes map to well-known markers of liver toxicity, such as CYP1A1. Right: Percentage of occurrence of thetop-k genes in test. Many genes consistently appear as influential (>40% of tok-k genes). * denotes the number of publications referencingthis gene as connected to drug-induced liver injury according to the CTD database (*: >500, **: >1,000, ***: >2,000). a strong positive correlation between rank and few-shot per-formance in all datasets among methods of the same family,(S+E), (S), and Averaging, as exemplified with k=10 (seeSupplemental). This confirms the importance of buildingdomain-specific feature encoders for increased expressivity.This also suggests that reconstruction-based methods sufferfrom some degree of dimensionality collapse, which we hy-pothesize stems from over-fitting (and might disappear withlarger cohorts).Finally, INTRA models have high ranksdespite performing significantly worse than (S+E), whichmight be explained by the latent space expressing clinicallyirrelevant factors, such as staining variations. Furthermore, we investigated whether salient histologicand expression features align with previously establishedbiological findings.First, we visualized the attentionweights learned during TANGLE pre-training (,top). Important regions with high attention (visible in red)correlate with lesions (fatty change and hepatocellular hy-pertrophy, see Supplemental for heatmaps of lung andbreast cancer samples). Next, we applied Integrated Gra-dients (IG) to derive gene-level importance scores (Fig-ure 6, left) on TG-GATEs test samples with reported le-sions. From there, we identified genes that consistently ap-pear in the top-k most influential genes, such as ABBC3 and CYPP1A1 (, right). We then quantitatively as-sessed their relevance by querying the Comparative Tox-icogenomics Database (CTD) that aggregates all the lit-erature on toxicology. 9/10 of the most important geneshave more than 1,000 references connecting them to drug-induced liver injury, highlighting their relevance for sliderepresentation learning.",
  ". Conclusion": "In this paper, we introduced Slide+Expression (S+E) pre-training for slide representation learning.Our approach,TANGLE, was trained and tested on several species (Homosapiens and Rattus norvegicus) and tissue sites (breast,lung, and liver). Overall, TANGLE outperforms all base-lines significantly on several downstream tasks, includ-ing few-shot classification, prototype-based classification,and slide retrieval.These results highlight the potentialof (S+E) pre-training and pave the way for additional de-velopments .Future work includes exploring multi-modal SSL objectives that extend beyond or synergize with,contrastive approaches, such as reconstruction of multi-modal masks. Concurrently, evaluating (S+E) pre-trainingon more tasks, such as predicting hormone receptor statusfrom H&E slides, are promising research directions. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. Advances inNeural Information Processing Systems, 35:2371623736,2022. 2 Benjamin Alexander-Dann, Lavinia Lorena Pruteanu, ErinOerton, Nitin Sharma, Ioana Berindan-Neagoe, DezsoModos, and Andreas Bender.Developments in toxicoge-nomics: understanding and predicting compound-inducedtoxicity from gene expression data. Mol. Omics, 14:218236, 2018. 2 Jordan Ash, Gregory Darnell, Daniel Munro, and BarbaraEngelhardt. Joint analysis of expression levels and histolog-ical images identifies genes associated with tissue morphol-ogy. Nature Communications, 12, 2021. 3 Shekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa,Sebastien Baur, Simon Kornblith, Ting Chen, Nenad Toma-sev, Jovana Mitrovic, Patricia Strachan, et al.Robustand data-efficient generalization of self-supervised machinelearning for diagnostic imaging. Nature Biomedical Engi-neering, pages 124, 2023. 1, 2 Eun Bok Baek, Ji-Hee Hwang, Heejin Park, Byoung-SeokLee, Hwa-Young Son, Yong-Bum Kim, Sang-Yeop Jun, JunHer, Jaeku Lee, and Jae-Woo Cho. Artificial Intelligence-Assisted image analysis of Acetaminophen-Induced acutehepatic injury in Sprague-Dawley rats. Diagnostics (Basel),12(6), 2022. 3 David Beer, Sharon Kardia, Chiang-Ching Huang, ThomasGiordano, Albert Levin, David Misek, Lin Lin, Guoan Chen,Tarek Gharib, Dafydd Thomas, Michelle Lizyness, RorkKuick, Satoru Hayasaka, Jeremy Taylor, Mark Iannettoni,Mark Orringer, and Sam Hanash. Gene-expression profilespredict survival of patients with lung adenocarcinoma. Na-ture medicine, 8:81624, 2002. 2 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers.In2021 IEEE/CVF International Conference on Computer Vi-sion (ICCV), pages 96309640, 2021. 1, 2, 5 Tsai Hor Chan, Fernando Cendra, Lan Ma, Guosheng Yin,and Lequan Yu.Histopathology whole slide image anal-ysis with heterogeneous graph representation learning. In2023 IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), 2023. 3 Richard J Chen, Ming Y Lu, Jingwen Wang, Drew FKWilliamson, Scott J Rodig, Neal I Lindeman, and FaisalMahmood. Pathomic fusion: an integrated framework forfusing histopathology and genomic features for cancer diag-nosis and prognosis. IEEE Transactions on Medical Imag-ing, 41(4):757770, 2020. 3 Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany YChen, Andrew D Trister, Rahul G Krishnan, and FaisalMahmood. Scaling vision transformers to gigapixel imagesvia hierarchical self-supervised learning. In Proceedings of",
  "the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2022. 1, 3, 5, 6": "Richard J. Chen, Ming Y. Lu, Drew F.K. Williamson,Tiffany Y. Chen, Jana Lipkova, Zahra Noor, MuhammadShaban, Maha Shady, Mane Williams, Bumjin Joo, andFaisal Mahmood. Pan-cancer integrative histology-genomicanalysis via multimodal deep learning. Cancer Cell, 40(8):865878, 2022. 3 Richard J. Chen, Tong Ding, Ming Y. Lu, Drew F. K.Williamson, Guillaume Jaume, Andrew H. Song, BowenChen, Andrew Zhang, Daniel Shao, Muhammad Shaban,Mane Williams, Lukas Oldenburg, Luca L. Weishaupt,Judy J. Wang, Anurag Vaidya, Long Phi Le, Georg Ger-ber, Sharifa Sahai, Walt Williams, and Faisal Mahmood. To-wards a general-purpose foundation model for computationalpathology. Nature Medicine, 2024. 2",
  "Ozan Ciga, Tony Xu, and Anne Louise Martel. Self super-vised contrastive learning for digital histopathology.Ma-chine Learning with Applications, 7, 2022. 1, 2": "Nicolas Coudray, Paolo Santiago Ocampo, Theodore Sakel-laropoulos, Navneet Narula, Matija Snuderl, David Fenyo,Andre L Moreira, Narges Razavian, and Aristotelis Tsirigos.Classification and mutation prediction from nonsmall celllung cancer histopathology images using deep learning. Na-ture Medicine, 24(10):15591567, 2018. 2 Yufei CUI, Ziquan Liu, Xiangyu Liu, Xue Liu, Cong Wang,Tei-Wei Kuo, Chun Jason Xue, and Antoni B. Chan. Bayes-MIL: A new probabilistic perspective on attention-basedmultiple instance learning for whole slide images. In TheEleventh International Conference on Learning Representa-tions, 2023. 3 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE Conference on Computer Vision andPattern Recognition, pages 248255, 2009. 1 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. Bert: Pre-training of deep bidirectional trans-formers for language understanding. Proceedings of the 2019Conference of the North American Chapter of the Associa-tion for Computational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), 2018. 1",
  "Thomas G Dietterich, Richard H Lathrop, and TomasLozano-Perez. Solving the multiple instance problem withaxis-parallel rectangles. Artificial intelligence, 89(1-2):3171, 1997. 1, 3, 4": "Kexin Ding, Mu Zhou, Dimitris Metaxas, and ShaotingZhang. Pathology-and-genomics multimodal transformer forsurvival outcome prediction. In International Conference onMedical Image Computing and Computer Assisted Interven-tion (MICCAI), pages 622631, 2023. 3, 6 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In International Con-ference on Learning Representations, 2021. 2, 1",
  "Baptiste Schiratti.Scaling self-supervised learning forhistopathology with masked image modeling.medRxiv,2023. 1, 2": "Jevgenij Gamper and Nasir Rajpoot. Multiple instance cap-tioning: Learning representations from histopathology text-books and articles. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages1654916559, 2021. 2 Quentin Garrido, Randall Balestriero, Laurent Najman, andYann Lecun. Rankme: Assessing the downstream perfor-mance of pretrained self-supervised representations by theirrank.In International conference on machine learning,2022. 7, 2 Mary J Goldman, Brian Craft, Mim Hastie, KristupasRepecka, Fran McDade, Akhil Kamath, Ayan Banerjee, Yun-hai Luo, Dave Rogers, Angela N Brooks, Jingchun Zhu,and David Haussler. Visualizing and interpreting cancer ge-nomics data via the xena platform. Nature biotechnology, 38(6):675678, 2020. 5 Douglas Gomes, Simone Porto, Debora Balabram, and He-lenice Gobbi.Inter-observer variability between generalpathologists and a specialist in breast pathology in the di-agnosis of lobular neoplasia, columnar cell lesions, atypicalductal hyperplasia and ductal carcinoma in situ of the breast.Diagnostic pathology, 9:121, 2014. 2 Citlalli Gamez Serna, Fernando Romero-Palomo, FilippoArcadu, Jurgen Funk, Vanessa Schumacher, and AndrewJanowczyk. Mmo-net (multi-magnification organ network):A use case for organ identification using multiple magnifica-tions in preclinical pathology studies. Journal of PathologyInformatics, 13:100126, 2022. 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 1, 6 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, PiotrDollar, and Ross Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1600016009, 2022. 2 Holger Hoefling, Tobias Sing, Imtiaz Hossain, Julie Bois-clair, Arno Doelemeyer, Thierry Flandre, Alessandro Piaia,Vincent Romanet, Gianluca Santarossa, Chandrassegar Sar-avanan, Esther Sutter, Oliver Turner, Kuno Wuersch, andPierre Moulin. Histonet: A deep learning-based model ofnormal histology.Toxicologic Pathology, 49(4):784797,2021. PMID: 33653171. 3",
  "Zhi Huang, Federico Bianchi, Mert Yuksekgonul, ThomasMontine, and James Zou.A visuallanguage foundationmodel for pathology image analysis using medical twitter.Nature Medicine, 29:110, 2023. 1, 2": "Ji-Hee Hwang, Minyoung Lim, Gyeongjin Han, Heejin Park,Yong-Bum Kim, Jinseok Park, Sang-Yeop Jun, Jaeku Lee,and Jae-Woo Cho. A comparative study on the implemen-tation of deep learning algorithms for detection of hepaticnecrosis in toxicity studies. Toxicological Research, 39(3):399408, 2023. 3 Yoshinobu Igarashi, Noriyuki Nakatsu, Tomoya Yamashita,Atsushi Ono, Yasuo Ohno, Tetsuro Urushidani, and Hi-roshi Yamada.Open TG-GATEs: a large-scale toxicoge-nomics database.Nucleic Acids Research, 43(D1):D921D927, 2014. 5, 1",
  "Maximilian Ilse,Jakub Tomczak,and Max Welling.Attention-based deep multiple instance learning. In Inter-national conference on machine learning, pages 21272136.PMLR, 2018. 1, 3, 4, 5, 6": "Guillaume Jaume, Anurag Vaidya, Richard Chen, DrewWilliamson, Paul Liang, and Faisal Mahmood.Modelingdense multimodal interactions between biological pathwaysand histology for survival prediction.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, 2024. 3, 5 Syed Ashar Javed, Dinkar Juyal, Harshith Padigela, AmaroTaylor-Weiner, Limin Yu, and aaditya prakash.AdditiveMIL: Intrinsically interpretable multiple instance learningfor pathology. In Advances in Neural Information Process-ing Systems, 2022. 3 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In InternationalConference on Machine Learning, pages 49044916. PMLR,2021. 2",
  "Shuai Jiang, Liesbeth Hondelink, Arief A Suriawinata, andSaeed Hassanpour. Masked pre-training of transformers forhistology image analysis. arXiv preprint arXiv:2304.07434,2023. 3": "Ting Jin, Xingran Xie, Renjie Wan, Qingli Li, and Yan Wang.Gene-induced multimodal pre-training for image-omic clas-sification.In International Conference on Medical ImageComputing and Computer Assisted Intervention (MICCAI),2023. 3, 6 Mingu Kang, Heon Song, Seonwook Park, Donggeun Yoo,and Sergio Pereira.Benchmarking self-supervised learn-ing on diverse pathology datasets.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2023. 2 Mingu Kang, Heon Song, Seonwook Park, Donggeun Yoo,and Sergio Pereira. Benchmarking self-supervised learningon diverse pathology datasets. In 2023 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 33443354, 2023. 2 Jakob Nikolas Kather, Alexander T Pearson, Niels Halama,Dirk Jager, Jeremias Krause, Sven H Loosen, AlexanderMarx, Peter Boor, Frank Tacke, Ulf Peter Neumann, et al.Deep learning can predict microsatellite instability directlyfrom histology in gastrointestinal cancer. Nature Medicine,25(7):10541056, 2019. 2 Jakob Nikolas Kather, Lara R Heij, Heike I Grabsch, ChiaraLoeffler, Amelie Echle, Hannah Sophie Muti, JeremiasKrause, Jan M Niehues, Kai AJ Sommer, Peter Bankhead,et al. Pan-cancer image-based detection of clinically action-able genetic alterations. Nature cancer, 1(8):789799, 2020.2 NavidAlemiKoohbanani,BalagopalUnnikrishnan,Syed Ali Khurram,Pavitra Krishnaswamy,and NasirRajpoot.Self-path:Self-supervision for classificationof pathology images with limited annotations.IEEETransactions on Medical Imaging, 2021. 2 Tristan Lazard, Marvin Lerousseau, Etienne Decenci`ere, andThomas Walter. Giga-ssl: Self-supervised learning for gi-gapixel images. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 43044313, 2023. 2, 3",
  "Y. Lee, J.H. Park, S. Oh, et al. Derivation of prognostic con-textual histopathological features from whole-slide imagesof tumours via graph deep learning. Nat. Biomed. Eng, 2022.1, 3, 4": "Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multipleinstance learning network for whole slide image classifica-tion with self-supervised contrastive learning. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1431814328, 2021. 4 Honglin Li, Chenglu Zhu, Yunlong Zhang, Yuxuan Sun,Zhongyi Shui, Wenwei Kuang, Sunyi Zheng, and Lin Yang.Task-specific fine-tuning via variational information bottle-neck for weakly-supervised pathology whole slide imageclassification. In 2023 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2023. 3 Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation. Advances in neural infor-mation processing systems, 34:96949705, 2021. 2",
  "Ruiqing Li, Xingqi Wu, Ao Li, and Minghui Wang. HFB-Surv: hierarchical multimodal fusion with factorized bilinearmodels for cancer survival prediction. Bioinformatics, 38(9):25872594, 2022. 3": "Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-hofer, and Kaiming He. Scaling language-image pre-trainingvia masking. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2339023400, 2023. 2 Tiancheng Lin, Hongteng Xu, Canqian Yang, and Yi Xu.Interventional multi-instance learning with deconfoundedinstance-level prediction. In 2023 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2022. 3 Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu,Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Con-trastive language-image pre-training using biomedical docu-ments. In International Conference on Medical Image Com-puting and Computer Assisted Intervention (MICCAI), 2023.2 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 1001210022, 2021. 4 Ming Lu, Bowen Chen, Drew Williamson, Richard Chen,Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, An-drew Zhang, Long Le, Georg Gerber, Anil Parwani, andFaisal Mahmood.Towards a visual-language foundationmodel for computational pathology. Nature Medicine, 2024.2 Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard JChen, Matteo Barbieri, and Faisal Mahmood. Data-efficientand weakly supervised computational pathology on whole-slide images. Nature biomedical engineering, 5(6):555570,2021. 1, 3, 4 Ming Y Lu, Bowen Chen, Andrew Zhang, Drew FKWilliamson, Richard J Chen, Tong Ding, Long Phi Le, Yung-Sung Chuang, and Faisal Mahmood. Visual language pre-trained multiple instance zero-shot transfer for histopathol-ogy images. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1976419775, 2023. 1, 2, 4 Shima Mehrvar, Lauren E. Himmel, Pradeep Babburi, An-drew L. Goldberg, Magali Guffroy, Kyathanahalli Janardhan,Amanda L. Krempley, and Bhupinder Bawa. Deep learningapproaches and applications in toxicologic histopathology:Current status and future perspectives. Journal of PathologyInformatics, 12(1):42, 2021. 3 Pooya Mobadersany,Safoora Yousefi,Mohamed Am-gad, David A Gutman, Jill S Barnholtz-Sloan, Jose EVelazquez Vega, Daniel J Brat, and Lee AD Cooper. Pre-dicting cancer outcomes from histology and genomics us-ing convolutional networks.Proceedings of the NationalAcademy of Sciences, 115(13):E2970E2979, 2018. 3",
  "Patience Mukashyaka, Todd Sheridan, Ali pour, and JeffreyChuang.Sampler: unsupervised representations for rapidanalysis of whole slide tissue images.eBioMedicine, 99:104908, 2024. 1": "A. Myronenko, Z. Xu, D. Yang, H.R. Roth, and D. Xu. Ac-counting for dependencies in deep learning based multipleinstance learning for whole slide imaging. In InternationalConference on Medical Image Computing and Computer As-sisted Intervention (MICCAI), pages 329338, 2021. 3 Ramin Nakhli, Allen Zhang, Ali Mirabadi, Katherine Rich,Maryam Asadi, Blake Gilks, Hossein Farahani, and AliBashashati. Co-pilot: Dynamic top-down point cloud withconditional neighborhood aggregation for multi-gigapixelhistopathology image representation. In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), pages 2106321073, 2023. 3 Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.Dinov2: Learning robust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023. 2",
  "lation for whole slide image classification. In Advances inNeural Information Processing Systems, 2022. 3": "Linhao Qu, Zhiwei Yang, Minghong Duan, Yingfan Ma,Shuo Wang, Manning Wang, and Zhijian Song. Boostingwhole slide image classification from the perspectives of dis-tribution, correlation and magnification. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 2146321473, 2023. 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2, 4 Md Mamunur Rahaman, Ewan K. A. Millar, and Erik Mei-jering. Breast cancer histopathology image-based gene ex-pression prediction using spatial transcriptomics data anddeep learning. Sci. Rep., 13(13604):111, 2023. 8 Benoit Schmauch, Alberto Romagnoni, Elodie Pronier,Charlie Saillard, Pascale Maille, Julien Calderaro, AurelieKamoun, Meriem Sefta, Sylvain Toldo, Mikhail Zaslavskiy,Thomas Clozel, Matahi Moarii, Pierre Courtiol, and GillesWainrib. A deep learning model to predict rna-seq expres-sion of tumours from whole slide images. Nature Communi-cations, 11, 2020. 3 Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, JianZhang, Xiangyang Ji, et al. Transmil: Transformer basedcorrelated multiple instance learning for whole slide imageclassification. Advances in Neural Information ProcessingSystems, 34:21362147, 2021. 1, 3, 4, 5, 6, 2 Zhuchen Shao, Yifeng Wang, Yang Chen, Hao Bian, Shao-hui Liu, Haoqian Wang, and Yongbing Zhang.Lnpl-mil:Learning from noisy pseudo labels for promoting multipleinstance learning in whole slide image. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 2149521505, 2023. 3 Taishi Shimazaki, Ameya Deshpande, Anindya Hajra, TijoThomas, Kyotaka Muta, Naohito Yamada, Yuzo Yasui, andToshiyuki Shoda.Deep learning-based image-analysis al-gorithm for classification and quantification of multiplehistopathological lesions in rat liver. Journal of ToxicologicPathology, 35(2):135147, 2022. 3 Artem Shmatko, Narmin Ghaffari Laleh, Moritz Ger-stung, and Jakob Nikolas Kather. Artificial intelligence inhistopathology: enhancing cancer research and clinical on-cology. Nature Cancer, 3(9):10261038, 2022. 1 Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-laume Couairon, Wojciech Galuba, Marcus Rohrbach, andDouwe Kiela. Flava: A foundational language and visionalignment model.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1563815650, 2022. 2 Andrew H. Song, Guillaume Jaume, Drew F. K. Williamson,Ming Y. Lu, Anurag Vaidya, Tiffany R. Miller, and FaisalMahmood. Artificial intelligence for digital and computa-tional pathology. Nature Reviews Bioengineering, 2023. 1",
  "phological prototyping for unsupervised slide representationlearning in computational pathology.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, 2024. 3": "Wenhao Tang, Sheng Huang, Xiaoxian Zhang, FengtaoZhou, Yi Zhang, and Bo Liu.Multiple instance learningframework with masked hard instance mining for whole slideimage classification. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision (ICCV), 2023. 3 Thomas Tavolara, Metin Gurcan, and M. Niazi.Con-trastive multiple instance learning: An unsupervised frame-work for learning slide-level representations of whole slidehistopathology images without labels.Cancers, 14:5778,2022. 2 Chao Tu, Yu Zhang, and Zhenyuan Ning. Dual-curriculumcontrastive multi-instance learning for cancer prognosisanalysis with whole slide images. In Advances in Neural In-formation Processing Systems, pages 2948429497. CurranAssociates, Inc., 2022. 3 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and IlliaPolosukhin. Attention Is All You Need. In Neural Informa-tion Processing Systems (NeurIPS), 2017. 2 Eugene Vorontsov, Alican Bozkurt, Adam Casson, GeorgeShaikovski, Michal Zelechowski, Siqi Liu, Philippe Math-ieu, Alexander van Eck, Donghun Lee, Julian Viret, EricRobert, Yi Kan Wang, Jeremy D. Kunz, Matthew C. H.Lee, Jan Bernhard, Ran A. Godrich, Gerard Oakley, EwanMillar, Matthew Hanna, Juan Retamero, William A. Moye,Razik Yousfi, Christopher Kanan, David Klimstra, BrandonRothrock, and Thomas J. Fuchs. Virchow: A million-slidedigital pathology foundation model, 2023. 1, 2 Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-hammed, Saksham Singhal, Subhojit Som, et al. Image asa foreign language: Beit pretraining for vision and vision-language tasks. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1917519186, 2023. 2 Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, JingZhang, Junzhou Huang, Wei Yang, and Xiao Han. Transpath:Transformer-based self-supervised learning for histopatho-logical image classification. In International Conference onMedical Image Computing and Computer-Assisted Interven-tion, pages 186195. Springer, 2021. 2, 4 Xiyue Wang, Jinxi Xiang, Jun Zhang, Sen Yang, ZhongyiYang, Ming-Hui Wang, Jing Zhang, Yang Wei, JunzhouHuang, and Xiao Han.SCL-WC: Cross-slide contrastivelearning for weakly-supervised whole-slide image classifica-tion. In Advances in Neural Information Processing Systems,2022. 3 Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang,Jing Zhang, Wei Yang, Junzhou Huang, and Xiao Han.Transformer-based unsupervised contrastive learning forhistopathological image classification. Medical image anal-ysis, 81:102559, 2022. 1, 2, 4, 6",
  "Jinxi Xiang and Jun Zhang. Exploring low-rank property inmultiple instance learning for whole slide image classifica-tion. In The Eleventh International Conference on LearningRepresentations, 2023. 3": "Ronald Xie, Kuan Pang, Sai W. Chung, Catia T. Perciani,Sonya A. MacParland, Bo Wang, and Gary D. Bader. Spa-tially resolved gene expression prediction from h&e histol-ogy images via bi-modal contrastive learning. In NeurIPS,2023. 3 Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, JianminBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: a simpleframework for masked image modeling. In 2022 IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 96439653, 2022. 2 Yingxue Xu and Hao Chen. Multimodal optimal transport-based co-attention transformer with global structure con-sistency for survival prediction.In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), 2023. 3 Jiawei Yang, Hanbo Chen, Yuan Liang, Junzhou Huang, LeiHe, and Jianhua Yao. Concl: Concept contrastive learningfor dense prediction pre-training in pathology images.InProceedings of the European Conference on Computer Vi-sion (ECCV), pages 523539, 2022. 3 J. Yao, X. Zhu, J. Jonnagaddala, N. Hawkins, and J. Huang.Whole slide images based cancer survival prediction usingattention guided deep multiple instance learning networks.Medical Image Analysis, 65, 2020. 3",
  "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastivecaptioners are image-text foundation models. arXiv preprintarXiv:2205.01917, 2022. 2": "Zhimiao Yu, Tiancheng Lin, and Yi Xu. Slpd: Slide-levelprototypical distillation for wsis. In International Confer-ence on Medical Image Computing and Computer-AssistedIntervention, pages 259269. Springer, 2023. 2, 3 Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao,Xiaoyun Yang, and Yalin Zheng. Dtfd-mil: Double-tier fea-ture distillation multiple instance learning for histopathologywhole slide image classification. In 2022 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 1878018790, 2022. 3",
  "A. Model & training": "iBOT-Tox pre-training: iBOT-Tox is the first vision en-coder for toxicologic pathology targeting non-human sam-ples.It uses a Vision Transformer Base (ViT-B) as backbone to learn 768-dimensional embeddings from224224 pixels image patches. ViTs are based on the self-attention paradigm to encode spatial interactions amongsmall regions (called tokens) of the input image. iBOT-Tox is trained using the iBOT recipe , a state-of-the-arttraining strategy based on student-teacher knowledge distil-lation . iBOT combines contrastive and reconstructionobjectives: (1) a self-distillation objective to align differ-ent views of the input image based on image crop and aug-mentation. This objective helps to encode contextual andsemantic information from the image, allowing for the cre-ation of representations that are invariant to staining or rota-tion; and (2) a masked image modeling objective that aimsto reconstruct image tokens from the other tokens. Thisobjective helps to encode the image structure and is anal-ogous to masked language modeling in Large LanguageModel training, such as BERT. To train the network,we relied on the public implementation of iBOT. iBOT-Toxwas trained on 15 million patches extracted from different47,227 WSIs (liver and kidney slides). We trained the net-work for 1,176,640 iterations or 80 epochs. The specific hy-perparameters used for training are listed in . Mostparameters were adapted from ImageNet-22K pre-training.ABMIL architecture: TANGLE is using an ABMIL archi-tecture , which is composed of three components:a pre-attention MLP, consisting of 2 layers with 768 hid-den units, layer normalization, GELU activation, and 0.1dropout; a gated-attention network, consisting of 2-layerMLP with 512 hidden units, with Sigmoid and Tanh acti-vation respectively and 0.25 dropout; and a post-attentionnetwork, consisting a linear layer with 768 units.TANGLE pre-training:We pre-trained TANGLE withAdamW optimizer and a batch size of 128 for 50 epochs.The learning rate is linearly ramped up during a 5-epochwarmup from 1e-8 to 1e-4.Then, we employed a co-sine scheduler to reach the final learning rate of 1e-8 af-ter 50 epochs. To increase training diversity and simplifybatch processing, we sample a fixed and random subset ofpatches per slide. In TG-GATEs, we sample 4,096 patches,and in TCGA-BRCA and TCGA-NSCLC, we sample 2,048patches per slide. In slides with fewer patches, we perform",
  "B. Data": "TG-GATEs transcriptomics pre-processing:The rawtranscriptomicsconsistsofmicroarrays(AffymetrixGeneChip) with 31,042 probes.Data were downloadedfrom the toxigates portal that aggregates all omicsdata acquired as part of The Japanese ToxicogenomicsProject . All data followed probe-wise normalizationusing log2 fold change with respect to a control group.Log2 fold change quantifies the proportional difference,on a logarithmic scale, between the expression levels of aparticular probe under two conditions: a control group (onaverage 22 slides per study in TG-GATEs) and a samplegroup (a defined set of compound, time and sacrifice).Each probe was then mapped to a unique gene identifierusing SynGoPortal, resulting in 13,404 gene expressionmeasurements per sample. Finally, studies from the trainset with compounds or chemicals known to induce liverinjury were selected (n=74) to extract the 1,000 genes withthe largest log2 fold change, used for our analysis. Thelog2 fold change gene expression values were not furthernormalized before processing by the deep learning system.In total, we obtained 6,597 transcriptomic samples used fortraining.Histology data overview: A summary of the liver data(TG-GATEs), Breast carcinoma (BRCA), and Lung carci-noma (NSCLC) is presented in , and .",
  "TANGLE pre-training1,033528505Few-shot train1,033528505Independent test1,9461,621325": "classification performance, namely, on cellular infiltration,fatty change, (hepatocellular) hypertrophy, increased mito-sis, (hepatocellular) necrosis, and proliferation of bile ductand oval cells. These lesions can take various sizes, e.g.,necrosis can be focal (located in a small region) or diffuse(scattered all over the tissue). Lesions can also have differ-ent morphologies, such as hepatocellular hypertrophy thatcan be accompanied by eosinophilic or basophilic degener-ation. As presented in , large lesions such as fattychange and hypertrophy are easier to detect than smallerones like cellular infiltration and necrosis. This may be dueto the expression profiles not expressing focal lesions, forinstance, because the amount of tissue that includes the le-sion of interest is too small.Loss ablation: We conduct three types of ablations on TG-GATEs: (1) ablation of the TANGLE loss, (2) ablation ofthe INTRA loss, and (3) experiments where we combineTANGLE and INTRA (see ).First, we compare the symmetric contrastive objective ofTANGLE with a one-sided objective (image expression).Adding a symmetric loss leads to a consistent performanceboost. We also tested with a mean-squared error (L2) andan L1 objective, both leading to a performance drop of 7.0%and 6.7% AUC, respectively. In addition, we compare thegain of using both a local-global and local-local contrastivealignment in INTRA. Both objectives bring complementaryinformation and lead to a performance loss when only oneis employed. Finally, we combine TANGLE objective withan INTRA objective based on contrasting the average token(Contrast w. Avg.) and a random view (Contrast w. Ran-dom View). Combining both leads to a performance dropof -2.0% AUC.Model ablation: TANGLE uses an attention-based MIL(ABMIL) as backbone. We compare the performance of",
  "D. Interpretability": "Rank analysis: Following , we use the rank as a fastand cheap measure of the quality of the underlying latentspace learned during SSL pre-training. Here, we computethe rank as the entropy of the d (assuming d < n) L1-normalized singular values of the slide embedding matrixH Rnd. Specifically, we have:",
  "|(H)|1+ (2)": "where k denotes the kth singular of H (sorted from largeto low), and is small constant set to 1e 7 for numeri-cal stability. presents the smooth rank of the slideembeddings obtained with different methods on the threeindependent test cohorts.Attention heatmaps: We also present attention heatmapsof TANGLE when pre-trained on breast (, top) andlung (, bottom). Interestingly, the attention is as-signed to regions that overlap with tumor, a property thatnaturally emerges from multimodal pre-training without ex-plicit training.",
  "Layers12Heads12Patch size16Head activationGELUEmbedding dimension768Drop path rate0.1": "Global crop scale0.32, 1.0Global crop number2Local crop scale0.05, 0.32Local crop number10Partial prediction shapeBlockPartial prediction ratio0.0, 0.3Partial prediction variance0, 0.2Gradient clipping0.3Normalize last layerShared head AdamW (0.9, 0.999)Batch size1024Freeze last layer epochs3Warmup epochs5Warmup teacher temperature epochs30Max epochs80Learning rate scheduleCosineLearning rate (start)0Learning rate (post warmup)5e-4Learning rate (final)2e-6Teacher temperature (start)0.04Teacher temperature (final)0.07Teacher momentum (start)0.996Teacher momentum (final)1.000Weight decay (start)0.04Weight decay (end)0.4Automatic mixed precisionfp16 Supplemental . Lesion-wise few-shot linear probing performance of TANGLE in rat liver. TANGLE is tested on an independenttest cohort comprising 4,584 slides, without any data leakage (slide- or study-level) from unimodal and multimodal pre-training. AverageAUC and standard deviation are reported over five runs.",
  "a.b.c": "Supplemental . Ablation study on TG-GATES. a. Ablation of the (S+E) loss of TANGLE. We compare a symmetric contrastiveloss with its non-symmetric counterpart, an L1 loss, and a Mean Squared Error loss. b. Combining TANGLE loss with TANGLE-Rec andINTRA. c. INTRA loss ablation using the average patch embedding, a random other view based on a different patch set, or a combinationof both.",
  "Supplemental . Hyper-parameter search on TG-GATES. We assess the influence of the batch size, number of patches sampledper slide, and the Softmax temperature": "Supplemental . Few shot performance vs. smooth rank. TANGLE linear probing performance (k=10) and baselines, plottedagainst the smooth rank of the slide embedding matrix of the independent test cohorts. Test cohorts tested on BRCA subtyping (humanbreast, n=1,265 WSIs), NSCLC subtyping (human lung, n=1,946 WSIs), and TG-GATEs lesion classification (rat liver, n=4,584 WSIs).For each family of methods, we observe a strong positive correlation between performance and rank. Supplemental . TANGLE attention heatmaps of a lung and breast slide. Attention weights of the (frozen) ABMIL slide encoderpre-trained with TANGLE overlaid on randomly chosen samples for our in-house cohorts. The network focuses mostly on tumor regions(marked in red) in both the breast and lung samples. This is a remarkable property of (S+E) pre-training as the network was not explicitlytrained for tumor-related tasks, such as subtyping or grading."
}