{
  "Abstract": "In this paper, the solution of HYU MLLAB KT Team tothe Multimodal Algorithmic Reasoning Task: SMART-101CVPR 2024 Challenge is presented. Beyond conventionalvisual question-answering problems, the SMART-101 chal-lenge aims to achieve human-level multimodal understand-ing by tackling complex visio-linguistic puzzles designed forchildren in the 6-8 age group. To solve this problem, we sug-gest two main ideas. First, to utilize the reasoning ability ofa large-scale language model (LLM), the given visual cues(images) are grounded in the text modality. For this pur-pose, we generate highly detailed text captions that describethe context of the image and use these captions as input forthe LLM. Second, due to the nature of puzzle images, whichoften contain various geometric visual patterns, we utilizean object detection algorithm to ensure these patterns arenot overlooked in the captioning process. We employed theSAM algorithm, which can detect various-size objects, tocapture the visual features of these geometric patterns andused this information as input for the LLM. Under the puz-zle split configuration, we achieved an option selection ac-curacy Oacc of 29.5 on the test set and a weighted optionselection accuracy (WOSA) of 27.1 on the challenge set.",
  "*These authors contributed equally to this work.Corresponding author": "timodal inputs (i.e., a given image and question) andgenerating an appropriate response to the question.Re-cently, several benchmarks, including ScienceQA andMMMU , have been introduced to assess the multi-modal reasoning capabilities in specialized fields. Thesebenchmarks aim to evaluate the expert knowledge of largelanguage models (LLMs) through subject-specific ques-tions spanning diverse areas such as mathematics, science,medicine, business, and the arts. On the other hand, theSMART-101 challenge evaluates the abstraction, deduc-tion, and generalization abilities of neural networks in solv-ing visio-linguistic puzzles designed specifically for chil-dren in the 68 age group. The SMART-101 dataset con-sists of 101 unique root puzzles that require a mix of variousskills such as counting, spatial localization, and mathemat-ical ability. Therefore, we required fine-grained perceptionability, as the MLLM must handle complex synthetic im-ages in diagram form rather than real-world visual scenesand complex reasoning ability is necessary due to the needfor difficult skills and their combinations not typically re-quired in traditional VQA tasks. To tackle the SMART-101 challenge, we propose a newinstruction-tuned vision-language model with two novelideas. First, to utilize the reasoning ability of pre-trainedMLLMs, the given visual cues (images) are grounded in thetext modality. We generate highly detailed text captions thatdescribe the context of the image and use these captions asinput for the pre-trained MLLMs. Second, due to the natureof puzzle images, which often contain complex diagram-matic visual patterns, we utilize an object detection algo-rithm to ensure these patterns are not overlooked in the cap-tioning process. To tackle this problem, the SegmentationAnything Model (SAM) algorithm is introduced to capturethe complex visual feature, and the visual features from theSAM are used as another input of the pre-trained MLLMs.",
  ". Large language model": "In recent years, large language models (LLMs) have beenrapidly evolving in the field of natural language process-ing. LLMs are capable of learning new tasks efficiently withonly a few examples, showing few-shot and zero-shot learn-ing aptitude . However, despite their flexibility, LLMsencounter significant limitations, especially in their reason-ing and computational abilities.For instance, advancedopen API models (e.g. GPT-4) have difficulty solving evenaddition problems with large numbers in a zero-shot settingwithout external tools . This shows that while LLMsexcel at learning language patterns, they have limitations infully understanding and applying the algorithmic nature ofmathematical operations. Furthermore, the performance ofLLMs is below expectations in tasks that require complexlogical reasoning .",
  ". Multimodal large language model": "Recently, multimodal large language models (MLLMs)such as Gemini , BLIP-2 , and LLaVA havegained attention for incorporating different modalities, go-ing beyond large language models (LLMs). These MLLMsprocess information from multiple modalities, making theirthinking more human-like than LLMs which specialize intext-based tasks. However, MLLMs still have limitations,such as a lack of accuracy in alignment between modali-ties, text-biased inference, and imbalanced cross-modal in-teractions that rely only on some attention heads . Tosolve these problems, researchers are trying a variety ofapproaches. BLIP-2 proposed a new pre-training methodthat explicitly models the alignment of image-text pairs.Flamingo uses a gated cross-attention mechanism toeffectively model the interaction between visual and textrepresentation.Our backbone model, InstructBLIP ,enhanced instruction following ability through supervisedfine-tuning.While these studies have been improvingMLLMs performance and image inference, MLLMs stillneed some improvement when it comes to solving visualmath problems .",
  "Various approaches are adopted to extract visual informa-tion from images and convert it into textual representations": "to aid the model in comprehending visual features. Zhenget al. enhance the input text by feeding image-captionpairs into VLM to generate visual questions and answersthat describe the image. Then, they are used to create thefinal caption, which enhances the quality of diffusion-basedtext-to-image generation. J. Park et al. utilize BLIP-2and other descriptors to extract global and local informa-tion from images to enable VLM to extract local informa-tion more effectively. They then instruct LLM to generatea question-answer-rationale (QAR) triplet as a knowledgerepresentation, which describes an image in text using thisinformation. In the end, they refine the BLIP-2 model byselecting only the correct information via the critic model.",
  ". Visual understanding": "A puzzle image is a compressed representation of informa-tion. They are designed to make it easy for humans to un-derstand the information, which makes them difficult forcomputers to understand . While a typical VQA taskfocuses on figuring out the overall meaning of an image,understanding a puzzle image requires more precise analy-sis and interpretation of visual elements. The model needsto be able to distinguish between the foreground and back-ground of the image, and even small changes in the content,such as what is the larger object. Therefore, unlike tradi-tional VQA tasks, we require another perspective of visualunderstanding skills to solve the puzzle image.",
  ". Text enhancement module": "Recent studies have shown that leveraging the text-basedreasoning ability of large-scale language models (LLMs)is effective even for image-based question answering tasks,where questions and answers are derived from given im-ages. These studies adopt a strategy of transforming thecontent of images into a form that LLMs can understand(language grounding) rather than directly utilizing visualfeatures, and then feeding this transformed content into theLLMs.In this study, we also chose to utilize LLMs by convert-ing the content of images into text captions. However, theimages used in the SMART-101 challenge are characterizedby many geometric shapes and are specialized for puzzles,making them unsuitable for general image captioning algo-rithms. To tackle this problem, we used the Qwen-VL . Overall framework of proposed pipeline. In the framework, we extract image captions using a two-stage mechanism. Toenhance the quality of the caption, we first generate three sets of visual question and answer (VQA). The results of these VQA generationsare then used as history and included as additional prompts when generating the caption of the image. The generated caption, along withthe question, is used as a prompt for the backbone model, InstructBLIP. We enhanced the visual understanding ability by concatenatingfeatures from ViT and SAM. Finally, the image and text embeddings are processed through Q-Former and LLM with a specific ensemblestrategy by classifying puzzle categories. . Result of text enhancement module. The left box is the target puzzle to augment text information through Qwen-VL-Chat, andthe middle box is the generated visual question-answer pairs. The right box is the result of generating captions using VQA pairs as history. as the image captioning method, because the Qwen-VL istrained on DocVQA and ChartQA datasets whichcontain various types of documents and charts.Furthermore, we adopted a two-step strategy where wefirst generate question-and-answer pairs about the image,and then create captions based on these pairs. This approachensures that the captions encompass various levels of se-mantics present in the image. The text generation results ofQwen-VL are illustrated in .",
  ". Vision enhancement module": "Although we can achieve high performance in visual ques-tion answering (VQA) by converting the meaning of im-ages into text as much as possible and leveraging the rea-soning ability of LLMs, we also utilized object detection-based image features to prevent potential information lossfrom language grounding. However, since the images usedin the SMART-101 challenge differ in nature from naturalimages, we used the Segmentation Anything (SAM) algo-rithm , which can extract geometric patterns from theimages, rather than relying on general object detection al-gorithms.Since our vision enhancement module was trainedspecifically for segmentation, enabling the extraction offine-grained, object-level features from the image. Thesefeatures are integrated with the features of Vision Trans-former (ViT) from instructBLIP and directed into the q-formers cross-attention mechanism to enrich the availablefeatures for visual reasoning.",
  ". Training with additional datasets": "Although InstructBLIP shows robust instruction followingcapability learned by multi-task finetuning, the model ismostly trained on natural image-based datasets for general-purpose VLM. Furthermore, several crucial visual reason-ing datasets were selected as held-out datasets and not usedin the training phase for InstructBLIP. Therefore, we sup-plement additional datasets to foster visual reasoning ability(e.g., mathematical reasoning, geometric visual reasoning)for the baseline model on complex synthetic puzzle imagesused in the smart-101 challenge. The details of the addi-tional datasets that we choose are described in Sec 4.1.2.",
  ". Multi-VLM training and inference": "To achieve optimal predictive performance, two distinctmodels were trained in parallel according to the type ofpuzzles: (i) For puzzles categorized under logic, counting,spatial reasoning, path tracing, and pattern finding, a modelwas trained to predict the option key. (ii) For puzzles thatdeal with arithmetic, measurement, and algebra, a modelwas trained to predict the answer value. This approachaddresses the performance variability across puzzle types,enabling more accurate predictions by utilizing the most . Inference workflow for Multi-VLM. A zero-shot clas-sifier determines the puzzle category. Based on the classified puz-zle type, either the key prediction model or the value predictionmodel is selected, each of which is specifically trained for the cor-responding answer type. suitable model for each puzzle category. During inference,a zero-shot classifier identifies the puzzle type and selectsthe appropriate model, either the key model or the valuemodel. The zero-shot classifier, designed to improve gener-alization to new puzzles, is based on the key model, whichexhibited high classification accuracy (0.36 for eight puzzletypes and 0.8 for key/value types). The inference workflowand the prompts employed for classification are illustratedin .",
  ". Evaluation metrics": "According to the evaluation guideline of challenge, we em-ploy the zero-shot generalization setting outlined in theoriginal paper, referred to as Puzzle Split. This entails as-sessing novel, previously unobserved root puzzle instancesthat demand the same foundational skills as those in thetraining set. Basically, when we evaluate the model at thetest set in the benchmark, we only use the option selectionaccuracy (Oacc) that measures the frequency with which thecorrection option is selected from one of the five answeroptions by a model. But in the challenge test phase, con-sidering the diverse difficulty of the puzzles, we weigh thepuzzle when scoring the performance. Thus, to compute thefinal score for a model, we use a weighted option selectionaccuracy (WOSA) defined in formula 1 where wi representsthe weight of each puzzle in the test set, where Oiacc is 1 ifthe answer is correct, and 0 otherwise.",
  ". Implementation details": "In our study, the InstructBLIP-Flan-T5-XL model is se-lected as the pretrained MLLM method. The training wasconducted using 4 A100 GPUs, and optimal performancewas achieved at approximately 2 epochs. The learning ratewas set to 1e-5 with a batch size of 16 per GPU. Further-more, we performed LoRA fine-tuning for MLLM with alearning rate of 1e-6, focusing on the attention module.",
  "IB + All (Ours)30.9324.4627.11": ".Test accuracy achieved by the suggested methodduring the competition IB represents InstructBLIP-Flan-T5-XLbaseline model. IB + All represents the suggested method includ-ing Qwen-VL caption, SAM, Multi-VLM inference, and trainingwith additional datasets. InstructBLIP-Flan-T5-XL algorithm as the base MLLM ar-chitecture and show the performances when combining thismodel with the ideas described in .We showthat combining the two main ideas (using highly-detailedcaptions and object-oriented visual features) with the twotraining details (training with additional datasets and multi-VLM inference) results in high performance.",
  ". Conclusion": "In this paper, we present a novel method to solve complexpuzzle problems by enhancing the reasoning performanceof the pre-trained multimodal large-scale language models(MLLMs). Specifically, we suggest leveraging highly de-tailed captions about the given images and object-orientedvisual representations followed by additional training withother datasets.During the competition, we showed thatcombining the ideas results in high performances.",
  "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, and Karel Lenc et al.Flamingo: a visual language model for few-shot learning,2022. 2": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, SinanTan, Peng Wang, Junyang Lin, Chang Zhou, and JingrenZhou. Qwen-vl: A versatile vision-language model for un-derstanding, localization, text reading, and beyond. arXivpreprint arXiv:2308.12966, 2023. 2 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei.Language mod-els are few-shot learners. arXiv preprint arXiv:2005.14165,2020. 2",
  "Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, KevinSmith, and Joshua B Tenenbaum.Are deep neural net-works smarter than second graders?arXiv preprintarXiv:2212.09993, 2022. 1, 4": "Wenliang Dai, Junnan Li, DONGXU LI, Anthony Tiong,Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung,and Steven Hoi.Instructblip:Towards general-purposevision-language models with instruction tuning. In Advancesin Neural Information Processing Systems, pages 4925049267. Curran Associates, Inc., 2023. 2, 5 Shima Imani, Liang Du, and Harsh Shrivastava.Math-prompter:Mathematical reasoning using large languagemodels.Proceedings of the 2023 Annual Conference ofthe Association for Computational Linguistics (ACL 2023),2023. 2",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.Visual instruction tuning, 2023. 2": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, SongyangZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,Ziwei Liu, et al. Mmbench: Is your multi-modal model anall-around player? arXiv preprint arXiv:2307.06281, 2023.5 Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.Iconqa:A new benchmark for abstract diagram under-standing and visual language reasoning.arXiv preprintarXiv:2110.13214, 2021. 5 Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-WeiChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, andAshwin Kalyan.Learn to explain: Multimodal reasoningvia thought chains for science question answering. In The36th Conference on Neural Information Processing Systems(NeurIPS), 2022. 1, 5 Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, MichelGalley, and Jianfeng Gao. Mathvista: Evaluating mathemat-ical reasoning of foundation models in visual contexts. arXivpreprint arXiv:2310.02255, 2023. 5",
  "Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar.Docvqa: A dataset for vqa on document images, 2021. 4": "Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, JohnKirchenbauer, Brian R. Bartoldson, Bhavya Kailkhura, Ab-hinav Bhatele, Jonas Geiping, Avi Schwarzschild, and TomGoldstein. Transformers can do arithmetic with the right em-beddings. arXiv preprint arXiv:2405.17399, 2024. 2 Jae Sung Park, Jack Hessel, Khyathi Raghavi Chandu,Paul Pu Liang, Ximing Lu, Peter West, Youngjae Yu, Qi-uyuan Huang, Jianfeng Gao, Ali Farhadi, and Yejin Choi.Localized symbolic knowledge distillation for visual com-monsense models. 2023. 2",
  "Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, MingjieZhan, and Hongsheng Li.Measuring multimodal mathe-matical reasoning with math-vision dataset. arXiv preprintarXiv:2402.14804, 2024. 5": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, RuoqiLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, WeimingRen, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Ren-liang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, YiboLiu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.Mmmu: A massive multi-discipline multimodal understand-ing and reasoning benchmark for expert agi. In Proceedingsof CVPR, 2024. 1, 5 Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical vi-sual reasoning. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 53175327, 2019. 5 Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-WeiChang, Peng Gao, et al. Mathverse: Does your multi-modalllm truly see the diagrams in visual math problems? arXivpreprint arXiv:2403.14624, 2024. 5 Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-WeiChang, Peng Gao, et al. Mathverse: Does your multi-modalllm truly see the diagrams in visual math problems? arXivpreprint arXiv:2403.14624, 2024. 2"
}