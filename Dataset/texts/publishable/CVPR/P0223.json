{
  "Abstract": "The advent of Vision Transformers (ViTs) marks a sub-stantial paradigm shift in the realm of computer vision.ViTs capture the global information of images through self-attention modules, which perform dot product computationsamong patchified image tokens. While self-attention mod-ules empower ViTs to capture long-range dependencies,the computational complexity grows quadratically with thenumber of tokens, which is a major hindrance to the practi-cal application of ViTs. Moreover, the self-attention mecha-nism in deep ViTs is also susceptible to the attention sat-uration issue.Accordingly, we argue against the neces-sity of computing the attention scores in every layer, andwe propose the Less-Attention Vision Transformer (LaViT),which computes only a few attention operations at eachstage and calculates the subsequent feature alignments inother layers via attention transformations that leverage thepreviously calculated attention scores. This novel approachcan mitigate two primary issues plaguing traditional self-attention modules: the heavy computational burden and at-tention saturation. Our proposed architecture offers supe-rior efficiency and ease of implementation, merely requiringmatrix multiplications that are highly optimized in contem-porary deep learning frameworks. Moreover, our architec-ture demonstrates exceptional performance across variousvision tasks including classification, detection and segmen-tation.",
  "fication , object detection , and semantic seg-mentation": "Inspired by the great success of Transformers in nat-ural language processing, Vision Transformers (ViTs) divide each image into a set of tokens. These tokens arethen encoded to produce an attention matrix that servesas a fundamental component of the self-attention mecha-nism. The computational complexity of the self-attentionmechanism grows quadratically with the number of tokens,and the computational burden becomes heavier with higher-resolution images. Some researchers attempt to reduce to-ken redundancy through dynamic selection or to-ken pruning to alleviate the computational burden ofthe attention computation. These approaches have demon-strated comparable performance to the standard ViT. How-ever, methods involving token reduction and pruning neces-sitate meticulous design of the token selection module andmay result in the inadvertent loss of critical tokens. In thiswork, we explore a different direction and rethink the mech-anism of self-attention. In the attention saturation problemraised in , as the layers of ViTs are progressively deep-ened, the attention matrix tends to remain largely unaltered,mirroring the weight allocation observed in the precedinglayers. Taking these considerations into account, we areprompted to pose the following question:",
  "Is it really necessary to consistently apply the self-attention mechanism throughout each stage of the network,from inception to conclusion?": "In this paper, we propose to modify the fundamen-tal architecture of standard ViT by introducing the Less-Attention Vision Transformer (LaViT). Our framework, asdepicted in , consists of Vanilla Attention (VA) lay-ers and our proposed Less Attention (LA) layers to cap-ture the long-range relationships. In each stage, we exclu-sively compute the traditional self-attention and store theattention scores in a few initial Vanilla Attention (VA) lay-ers. In subsequent layers, we efficiently generate attentionscores by utilizing the previously calculated attention ma-trices, thereby mitigating the quadratic computational ex-pense associated with self-attention mechanisms.More-over, we integrate residual connections within the attention",
  "Softmax": "MatMul . The architecture of our Less-Attention Vision Transformer (LaViT). The bottom part: the proposed Less-Attention layer, whichtogether with conventional Transformer blocks in the preceding layers constitutes the feature extraction module of this stage. layers during downsampling across stages, allowing for thepreservation of crucial semantic information learned in ear-lier stages while still transmitting global contextual infor-mation through alternate pathways. Finally, we carefullydesign a novel loss to preserve the diagonality of atten-tion matrices during the transformation process. These keycomponents enable our proposed ViT model to diminishboth computational complexity and attention saturation, ul-timately leading to notable performance improvements withreduced floating-point operations per second (FLOPs) andconsiderable throughput.To verify the effectiveness of our proposed approach, weconduct comprehensive experiments on various benchmarkdatasets, comparing the performance of our model withexisting state-of-the-art ViT variants (also recent efficientViTs). The experimental results demonstrate the efficacy ofour approach in addressing attention saturation and achiev-ing superior performance in visual recognition tasks.Our main contributions are summarized as follows: We present a novel ViT architecture that generates at-tention scores by re-parameterizing the attention matrixcomputed by preceding layers. This approach addressesboth the attention saturation and the associated computa-tional burden. Moreover, we propose a novel loss function that endeav-ors to preserve the diagonality of attention matrices dur-ing the process of attention re-parameterization. We positthat this is essential to uphold the semantic integrity ofattention, ensuring that the attention matrices accuratelyreflect the relative importance among input tokens. Our architecture consistently performs favorably againstseveral state-of-the-art ViTs, while having similar or evenreduced computational complexity and memory con-sumption, across various vision tasks including classifi-cation, detection and segmentation.",
  ". Vision Transformers": "The Transformer architecture, initially introduced for ma-chine translation , has since been applied to computervision tasks through the growth of ViT . The key inno-vation of ViT lies in its capability to capture long-range de-pendencies between distant regions of the image, achievedthrough the incorporation of self-attention mechanisms.Drawn from the triumph of ViT, a plethora of variantmodels have emerged, each devised to ameliorate specificconstraints inherent to the original architecture.For in-stance, DeiT enhances data efficiency during train-ing by incorporating the distillation token. Additionally,CvT and CeiT integrate the convolutional struc-ture into the ViT framework to combine the strengths ofCNNs (spatial invariance) and ViTs (long-range depen-dency modeling). These advancements underscore the on-going evolution of transformer-based architectures in thefield of computer vision.",
  ". Efficient Vision Transformers": "Though highly effective, ViTs suffer from a huge com-putational burden. The research on efficient vision trans-formers addresses the quadratic cost of the self-attentionoperation by including hierarchical downsampling opera-tions , token reduction , or lightweightarchitectural designs .Hierarchical downsam-pling operations address the quadratic computation of self-attention by reducing the token numbers gradually acrossthe stages and enable ViTs to learn hierar-chical structures. Another research direction introduces thetoken selection module to eliminate the least meaningful to-kens and reduce the computational burden. For instance, reorganize image tokens by preserving informa- DWConv",
  ". Attention Mechanisms": "The key component of ViTs is the attention mechanism,which computes pairwise interactions between all patches,resulting in quadratic complexity with respect to the inputsize. This problem leads to heavy inference computation,which hinders the practical application of ViTs in the realworld. Several studies argue that the computational burdencan be alleviated by utilizing sparse attention mechanisms,which selectively attend to a subset of patches based on theirrelevance or proximity. One notable approach is the Adap-tive Sparse Token Pruning framework , which inducesa sparse attention matrix, effectively addressing computa-tional efficiency concerns. Furthermore, employing tech-niques like structured sparsity patterns can fur-ther reduce computational complexity, thereby enhancingthe overall efficiency of ViTs. Another urgent issue to beaddressed is the problem of attention saturation, where theattention matrix displays limited variation as the layer depthincreases. This issue has been acknowledged in studies suchas DeepViT and CaiT , which report that attentionsaturation hinders the ability of deep ViTs to capture addi-tional semantic information and may even reduce trainingstability. Therefore, it is essential to carefully design theself-attention mechanism in ViTs to avoid sub-optimal so-lutions.",
  ". Methodology": "In this section, we first review the basic design of the hier-archical vision transformer. Then, we discuss two majorweaknesses of its attention mechanism, and propose thatself-attention can be grasped with less attention at eachstage. We dynamically re-parameterize the attention scoresby utilizing the stored attention matrix from the previouslayer, effectively mitigating the issue of attention saturation.Additionally, we integrate residual connections to facilitatethe transfer of global relationships from earlier stages. Lastbut not least, we introduce a novel loss, the Diagonality Pre-serving loss, to preserve basic properties in the transformedattention (i.e., representing the relationships among tokens).",
  ". Vision Transformer": "Let xRHW C represent an input image, whereH W denotes the spatial resolution and C the num-ber of channels.We first tokenize the image by parti-tioning it into N =HW/p2 patches, where each patchPi RppC (i {1, . . . , N}) has a size of p p pix-els and C channels. The patch size p is a hyper-parameter that determines the granularity of the token. The patch em-bedding can be extracted by using a convolution operatorwith the stride and kernel size equal to the patch size. Eachpatch is then projected to the embedding space Z RND through non-overlapping convolution, where D representsthe dimension of each patch.Multi-Head Self-Attention.We first provide a briefoverview on the vanilla self-attention mechanism that pro-cesses the embedded patches and functions within theframework of Multi-Head Self-Attention blocks (MHSAs).In the l-th MHSA block, the input Zl1, l {1, , L},is projected into three learnable embeddings {Q, K, V} RND. The multi-head attention aims to capture the at-tention from different views; for simplicity, we choose Hheads, where each head is a matrix with the dimensionN D",
  "ZMHSA = AV RND.(3)": "Downsampling Operation.Several studies have in-corporated hierarchical structures into ViTs, drawing in-spiration from the success of hierarchical architectures inCNNs. These works partition the Transformer blocks intoM stages and apply downsampling operations before eachTransformer stage, thereby reducing the sequence length.In our study, we employ a downsampling operation usinga convolutional layer with the kernel size and stride set to2. This approach permits the flexible adjustment of the fea-ture maps scale at each stage, thereby establishing a Trans-former hierarchical structure that mirrors the organizationof the human visual system.",
  "The overall framework of our network architecture is illus-trated in . In each stage, we extract the feature rep-resentation in two phases. At the initial several Vanilla At-": "tention (VA) layers, we conduct the standard MHSA opera-tion to capture the overall long-range dependencies. Sub-sequently, we simulate the attention matrices to mitigatequadratic computation and address attention saturation atthe following Less-Attention (LA) layers by applying a lin-ear transformation to the stored attention scores. Herein,we denote the attention score before the Softmax functionof the initial l-th VA layer in the m-th stage as AVA,lm , whichis computed by the following standard procedure:",
  "d, l LVAm .(4)": "Here, Qlm and Klm represent the queries and keys fromthe l-th layer of the m-th stage, following the downsam-pling from the preceding stage. And LVAm is used to denotethe number of VA layers. After the initial vanilla attentionphase, we discard the traditional quadratic MHSA and ap-ply transformations on AVAm to lessen the amount of atten-tion computation. This process entails two linear transfor-mations with a matrix transposition operation in between.To illustrate, let us consider the attention matrix in the l-th(l > LVAm ) layer (LA layer) of the stage:",
  "(5)": "In this context, the transformations denoted by and refer to linear transformation layers with a dimension ofRNN. Here, Lm, LVAm represent the number of layers andthe number of VA layers in the m-th stage, respectively.The insertion of the transposition operation between thesetwo linear layers serves the purpose of maintaining the ma-trix similarity behavior. This step is essential due to the factthat the linear transformation in a single layer conducts thetransformations row-wise, which could potentially result inthe loss of diagonal characteristics.",
  ". Residual-based Attention Downsampling": "When the computation traverses across stages in hierarchi-cal ViTs, downsampling operations are often employed onthe feature map. While this technique reduces the tokennumbers, it may result in the loss of essential contextual in-formation. Consequently, we posit that the attention affinitylearned from the preceding stage could prove advantageousfor the current stage in capturing more intricate global re-lationships. Drawing inspiration from ResNet , whichintroduces shortcut connections to mitigate feature satura-tion issues, we adopt a similar concept and incorporate itinto the downsampling attention computation within our ar-chitecture. By introducing a shortcut connection, we canintroduce the inherent bias into the current MHSA block.This allows the attention matrix from the previous stage to effectively guide the attention computation of the currentstage, thereby preserving crucial contextual information.However, directly applying the shortcut connection tothe attention matrix might pose challenges in this context,primarily due to the difference in the attention dimensionsbetween the current stage and the preceding stage. Here,we design an Attention Residual (AR) module that con-sists of a depth-wise convolution (DWConv) and a Conv11layer to downsample the attention map from the previousstage while keeping the semantic information. We denotethe last attention matrix (at the Lm1 layer) of the previ-ous stage (the m 1-th stage) as Alastm1, and the downsam-pled initial attention matrix of the current (the m-th) stageas Ainitm . Alastm1 has the dimension of RBHNm1Nm1(Nm1 denotes the token number at the m 1-th stage).We view the multi-head dimension H as the channel dimen-sion in regular image space, thus with the DWConv operator(stride = 2, kernel size = 2), we may capture the spatialdependencies among the tokens during attention downsam-pling. The output matrix after the DWConv transformationfits the size of the attention matrix of the current stage, i.e.,RBHNmNm(Nm = Nm1/2 in our case). After depth-width convolution on the attention matrix, we then performConv11 to exchange information across different heads.Our attention downsampling is illustrated in , and thetransformation from Alastm1 to Ainitm can be expressed as:",
  "AVAm AVAm + LS(Ainitm ),(7)": "where LS is the Layer-Scale operator introduced in toalleviate attention saturation.AVAm is the attention scorefor the first layer in the m-th stage, which is calculated byadding the standard MHSA with Eq. 4 and the residual cal-culated by Eq. 6.Two fundamental design principles guide our atten-tion downsampling module. First, we utilize DWConv tocapture spatial local relationships during downsampling,thereby enabling the efficient compression of attention re-lationships. Second, the Conv11 operation is utilized toexchange the attention information across heads. This de-sign is pivotal as it facilitates the efficient propagation ofattention from the preceding stage to the subsequent stage.Incorporating the residual attention mechanism necessitatesonly minor adjustments, typically involving adding a fewlines of code to the existing ViT backbone. It is worth em-phasizing that such a technique can be seamlessly applied tovarious versions of the Transformer architecture. The onlyprerequisite is to store the attention scores from the previ-ous layer and establish the skip-connections to this layeraccordingly. The importance of this module will be furtherilluminated through comprehensive ablation studies.",
  ". Diagonality Preserving Loss": "We have carefully designed the Transformer modules byincorporating attention transformation operators, aiming tomitigate the issues of computational cost and attention sat-uration. However, a pressing challenge remainsensuringthat the transformed attention preserves the inter-token re-lationships. It is well established that applying transforma-tions to attention matrices can compromise their capacity tocapture similarities, largely due to the linear transformationtreating the attention matrix row-wise. Thus, we design aan alternative approach to guarantee that the transformed at-tention matrix retains the fundamental properties necessaryto convey associations among tokens. A conventional at-tention matrix should possess the following two properties,i.e., diagonality and symmetry:",
  ". Complexity Analysis": "Our architecture consists of four stages, each comprisingLm layers. The downsampling layer is applied betweeneach consecutive stage. As such, the computational com-plexity of traditional self-attention is O(N 2mD), whereasthe associated K-Q-V transformation incurs a complex-ity of O(3NmD2). In contrast, our method leverages anNm Nm linear transformation within the transformationlayer, thereby circumventing the need for computing the in-ner products. Consequently, the computation complexityof our attention mechanism in the transformation layer is . Detailed configurations of the LaViT series. Blocks andHeads refer to the number of blocks ([L1, L2, L3, L4]) and headsin four stages, respectively. Channels refers to the input channeldimensions across the four stages. And NLA denotes the layerwithin each stage at which the utilization of the Less-Attentionlayer begins.",
  "LaViT-TLaViT-SLaViT-B": "reduced to O(N 2m), representing a reduction factor of D.Additionally, since our method calculates the query em-beddings solely within the Less-Attention layer, our K-Q-Vtransformation complexity is likewise diminished by a fac-tor of 3.In the downsampling layer between consecutive stages,considering a downsample rate of 2 as an example, thecomputational complexity of the DWConv in the attentiondownsampling layer can be calculated as Complexity =22 Nm",
  "Nm": "2 D = O(N 2mD). Similarly, the complex-ity of the Conv11 operation in the attention residual mod-ule is also O(N 2mD). However, it is important to note thatattention downsampling only occurs once per stage. There-fore, the additional complexity introduced by these opera-tions is negligible when compared to the complexity reduc-tion achieved by the Less-Attention layer.",
  ". Experiments": "In this section, we evaluate our models performance on twobenchmark datasets: ImageNet-1K for classification,COCO2017 for detection and ADE20K for seg-mentation. We compare our model with other state-of-the-art works on these datasets to demonstrate its effectivenessand efficiency. Furthermore, we perform ablation studies toinvestigate the necessity and contributions of each compo-nent in the proposed model. This analysis provides valuableinsights into the role of each part and helps to establish theefficacy of our approach.",
  ". Architecture Variants": "To ensure an equitable comparison with other models whilemaintaining a similar level of computational complexity,we establish three models: LaViT-T, LaViT-S, and LaViT-B. The detailed configuration information is provided in, and we follow the same network structure as 1The 90% notation in brackets indicates that we keep the token ratio of90% to represent the visual data during the training of the correspondingViTsDynamicViT and EviT, respectively. Additionally, given our aimto strike a balance between efficiency and effectiveness, we will not com-pare our results to high-performance but computationally intensive models,such as Swin-B-V2 and ConvNeXt-B .",
  "Settings.The image classification experiments are con-ducted on the ImageNet-1K dataset. Our experimental pro-tocol follows the procedures outlined in DeiT , with the": "exception of the model itself. Specifically, we apply thesame data augmentation and regularization techniques em-ployed in DeiT. We utilize the AdamW optimizer totrain our models from scratch for 300 epochs (with a 5-epoch warm-up). The initial learning rate is set to 0.005 andvaries according to a cosine scheduler. The global batch sizeis set to 1024, distributed across 4 GTX-3090 GPUs. Dur-ing the test on the validation set, the input images are firstresized to 256 pixels, followed by a center crop of 224 x224 pixels to evaluate the classification accuracy.Results. We present the classification results on ImageNet-1K in . The models are classified into three groupsbased on their computational complexity: tiny (approxi-mately 2G), small (approximately 4G), and base (approx-imately 9G). Our approach achieves competitive perfor-mance compared to state-of-the-art ViTs with markedly re-duced computational requirements. Specifically, in the tinyand small model clusters, our method surpasses all other ex-isting models by at least 0.2% and 0.5%, respectively, whilemaintaining a substantially lower computational cost, whichis our principal concern. In the base-size models, our archi-tecture, which incorporates the base structure of PVT butincludes the Less-Attention component, demonstrates supe-rior performance over two PVT-based models (PVT-M andPVT-L). Furthermore, we also compare our architecture toseveral efficient ViT designs (DynamicViT, EViT, LiT, effi-cientViT and PPT). We observe that our results reflect a bet-ter balance between effectiveness and efficiency. Note thatour design necessitates reduced computation cost owing toour resource-efficient Less-Attention mechanism, renderingour lightweight module an attractive option for implement-ing ViT on mobile platforms.",
  ". Object Detection on COCO2017": "Settings. We conduct the detection experiments on COCO2017 dataset. We test the model effectiveness on Reti-naNet . We follow the common practice by initializ-ing the backbone with pre-trained weights obtained fromImageNet-1K. In addition, we use AdamW optimizer,and train the network with the batchsize of 16 on 8 GPUs.Results.We present the results of object detection in Ta-ble 3. It is evident that our LaViT model exhibits a no-table advantage over both its CNN and Transformer coun-terparts. Specifically, with the 1 schedule, our tiny ver-sion LaViT-T achieves 9.9-12.5 APb against ResNet undercomparable settings, while the small version LaViT-S out-performs its CNN counterpart by 8.1-10.3 APb. This trendpersists with the 3 schedule, as our LaViT consistentlydemonstrates competitive performance. Particularly note-worthy is our architectures ability to consistently outper-form the Swin Transformer in terms of detection perfor-mance while imposing a smaller training burden.Thus,the results on COCO2017 reaffirm our assertion that our",
  ". Semantic Segmentation on ADE20K": "Settings.We conduct experiments on semantic segmen-tation using the ADE20K dataset, which comprises 150classes and 20,000 images for training, and 2,000 imagesfor validation. Our backbone networks for segmentationare Semantic FPN and UperNet . We follow thetraining settings established in and resize images to512 512 for training. We train UperNet for 160k itera-tions and SemanticFPN for 80k iterations. The initial learn-ing rate is set to 6 105, utilizing a poly scheduler forlearning rate decay. The experiment is conducted by usingthe batch size of 16 across 4 GTX3090 GPUs.Results. provides an overview of the segmenta-tion results. Our model demonstrates superiority over SwinTransformer, exhibiting an mIoU improvement of +2.6 withSemantic FPN and +2.7 with UperNet. In the Semantic FPNtest, our LaViT-S achieves a relatively modest increase of+0.9 mIoU compared to the baseline (PVT-S), but notablywith significantly fewer computations.When integratedinto the UperNet architecture, LaViT achieves substantialimprovements of +2.7 mIoU, +1.0 mIoU, and +1.4 mIoUwhen compared to various mainstream models. These com-petitive results are maintained even when employing testtime augmentation.In particular, LaViT-S outperformsFocal-T by +1.4 mIoU and +2.5 MS mIOU. These findingsunderscore LaViTs ability to produce high-quality seman-tic segmentation outputs while operating within the frame-work of its computation-efficient attention mechanism.",
  ". Ablation Study": "Attention Saturation. To demonstrate the efficacy of ourLess-Attention module in addressing attention saturation,we present the attention similarity ratio (cosine similaritycomputed by the attention map in the current layer and itsprevious layer) in . We conduct the comparisonusing two backbones, namely, ViT and PVT. In 3a, we se-lect the ViT architecture with 25 layers and no hierarchicalstructure. In 3b, we employ PVT-M as our baseline andassess the attention similarity at the 3rd stage, which con-sists of 18 layers. Both sets of results clearly illustrate thatthe original architecture encounters a significant attentionsaturation issue. However, this phenomenon is effectivelymitigated by incorporating our modules, enabling deepattention to fulfill its intended role. Extendability of Less-Attention Module. We extend ourLess-Attention module to various ViT architectures, andreport the results in . The incorporation of the Less-Attention layer into any of the foundational Transformerarchitectures leads to enhancements in accuracy whileconcurrently reducing computational demands.Notably,the most significant improvement is observed when incor-porating the module into the vanilla ViT/DeiT architecture.This may be attributed to the fact that the vanilla ViT/DeiTdoes not have a hierarchical structure, thereby experiencingconsiderable attention saturation issues. Moreover, whenintegrating our method into DeepViT, we observe the mostsubstantial decrease in computational resources.Thesefindings jointly underscore the scalability of our method,demonstrating that the application of LA module can renderexisting ViT architectures more practical and feasible. Importance of Each Component. We conduct ablationstudies on the proposed module with the ImageNet-1kdataset, and the results are shown in .On bothnetworks (i.e., tiny and small), our proposed modules proveto be indispensible for Transformer training. The baseline,which replaces the Less-Attention layer with MHSA, cor-responds exactly to the PVT model, exhibiting a decreasein predictive accuracy by 0.5% and 0.6% compared toour model. Additionally, removing the attention residualmodules, denoted as w/o AR, results in a reduction ofpredictive accuracy by 0.2% and 0.4%. Lastly, and most",
  "w/o LA---78.782.0w/o AR-79.082.2LaViT79.282.6w/o LDP-59.1( 20.1)57.1( 25.5)": ". Ablation study of the proposed module on the ImageNet-1k dataset. Baseline means we remove all the proposed modules,resulting in the PVT Transformer baseline. AR and LA indicatethe Attention-Residual and Less-Attention modules, respectively.w/o LDP indicates we remove the Diagonality Preserving loss. importantly, we assert that the additional loss functionto preserve diagonal correlations is vital for effectivelycomprehending semantic information in visual data. Whenrelying solely on the CE loss, the models predictionsdeteriorate. This might be attributed to the potential limi-tation of relying solely on the transformation for attentionmatrices, which could compromise their capacity to expresscorrelations among tokens. All these experimental findingscollectively emphasize the contribution of each componentwithin our model architecture.",
  "LaViT-B78.982.583.180.482.383.1": ". Ablation study on the layer where Less-Attention starts.We conduct experiments on the last two stagesStage 3,4. L2beneath Stage 3 means we use the Less-Attention layer to replacethe vanilla encoder from the second layer in the third Stage. Less-Attention Selection. In deep ViTs, careful selectionof the starting layer for Less-Attention is crucial. Thus,we design experiments to select the starting layer for Less-Attention in the network architecture, and the results arepresented in . As shown in the table, directly us-ing the Less-Attention layer from the second layer in thestage leads to a decrease in model performance. This phe-nomenon could be attributed to overly relying on the se-mantics of the first MHSA layer. Thus, leveraging the Less-Attention layer at deeper layers in the stage may mitigatethis issue. Furthermore, while utilizing the Less-Attentionlayer at relatively deeper layers does not affect the modelperformance much, it may lead to increased computationalcosts. This contradicts the design objective of our architec-ture to reduce the computational overhead.",
  ". Conclusion": "Aiming to reduce the costly self-attention computations, weproposed a new model called Less-Attention Vision Trans-former (LaViT). LaViT leverages the computed dependencyin Multi-Head Self-Attention (MHSA) blocks and bypassesthe attention computation by re-using attentions from previ-ous MSA blocks. We additionally incorporated a straight-forward Diagonality Preserving loss, designed to promotethe intended behavior of the attention matrix in represent-ing relationships among tokens. Notably, our Transformerarchitecture effectively captures cross-token associations,surpassing the performance of the baseline while maintain-ing a computationally efficient profile in terms of quan-tity of parameters and floating-point operations per second(FLOPs). Comprehensive experimentation has confirmedthe efficacy of our model as a foundational architecturefor multiple downstream tasks. Specifically, the proposedmodel demonstrates superiority over previous Transformerarchitectures, resulting in state-of-the-art performance inclassification and segmentation tasks.",
  "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet classification with deep convolutional neural net-works. Commun.ACM, pages 8490, 2017. 1, 5": "Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song,Jue Wang, and Pengtao Xie. Not all patches are what youneed: Expediting vision transformers via token reorganiza-tions. arXiv:2202.07800, 2022. 1, 2, 6 Tsung-Yi Lin, Michael Maire, Serge J. Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dollar, andC. Lawrence Zitnick. Microsoft COCO: common objects incontext. In ECCV, pages 740755, 2014. 5, 6",
  "Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, ChengLi, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.Residual attention network for image classification.InCVPR, pages 31563164, 2017. 1": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.Pyramid vision transformer: A versatile backbone for denseprediction without convolutions. In ICCV, pages 568578,2021. 2, 6 Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer.Computational Visual Media, pages 415424, 2022. 2, 6 Cong Wei, Brendan Duke, Ruowei Jiang, Parham Aarabi,Graham W Taylor, and Florian Shkurti. Sparsifiner: Learn-ing sparse instance-dependent attention for efficient visiontransformers. In CVPR, pages 2268022689, 2023. 3"
}