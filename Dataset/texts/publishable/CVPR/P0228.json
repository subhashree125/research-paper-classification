{
  "Abstract": "This paper introduces a novel approach to learning in-stance segmentation using extreme points, i.e., the topmost,leftmost, bottommost, and rightmost points, of each object.These points are readily available in the modern boundingbox annotation process while offering strong clues for pre-cise segmentation, and thus allows to improve performanceat the same annotation cost with box-supervised methods.Our work considers extreme points as a part of the trueinstance mask and propagates them to identify potential fore-ground and background points, which are all together usedfor training a pseudo label generator. Then pseudo labelsgiven by the generator are in turn used for supervised learn-ing of our final model. On three public benchmarks, ourmethod significantly outperforms existing box-supervisedmethods, further narrowing the gap with its fully supervisedcounterpart. In particular, our model generates high-qualitymasks when a target object is separated into multiple parts,where previous box-supervised methods often fail.",
  ". Introduction": "Instance segmentation, the task of predicting classes andmasks of individual objects at the same time, has been ad-vanced remarkably thanks to supervised learning of deepneural networks . However, it is pro-hibitively costly to manually annotate a pixel-level mask perinstance, which often leads to lack of both class diversity andthe amount of training data. This issue steers the researchcommunity towards label-efficient learning approaches suchas weakly supervised learning and semi-supervised learning .Building on this momentum, learning instance segmenta-tion using box supervision has gained considerable attractionrecently . To train an instancesegmentation model with box-supervision, these methodsemploy a bounding box tightness prior , which impliesthat a vertical (or horizontal) line crossing the boundingbox must contain at least one pixel belonging to the object (); this prior has been formulated through various lossfunctions . Although box-supervisionhas proved to be effective for learning instance segmentationwhile keeping annotation costs low, we claim that there isroom for further improvement in this direction, particularlydue to the fact that it has neglected extreme points, a byprod-uct of the common box annotation process providing a strongclue that helps in estimating the instance mask. Today, extreme points are freely available in the boundingbox annotation process , where human annotators areinstructed to click four extreme points of the target object,i.e., topmost, leftmost, bottommost, and rightmost points,rather than to click two corner points of the bounding box.This is because the former usually ends up requiring lessannotation time as the latter often needs to adjust the initialbox label multiple times, as demonstrated by Papadopouloset al. . Moreover, since they are definitely a part of thetrue mask of the target, extreme points provide a strong cluefor segmentation absent in the box supervision.Motivated by this, we study weakly supervised learningfor instance segmentation using extreme points to further im-prove performance without increasing annotation cost. Ourframework for EXtreme point supervised InsTance Segmen-tation, dubbed EXITS, considers extreme points as a part ofthe true instance mask, and exploits them as supervision fortraining a pseudo label generator. Then pseudo segmentationlabels produced by the generator are in turn used for super-vised learning of our final model, which can be any arbitrarynetworks for instance segmentation. The overall procedureof EXITS is illustrated in .The key to the success of EXITS is how to train thepseudo label generator using extreme points. A straightfor-ward way is to consider extreme points as foreground andpoints outside the bounding box as background, and thenexploit them for supervised learning. However, the pseudo la-bel generator trained in this way fails to generate crisp objectmasks since most object regions remain unlabeled duringtraining due to the sparsity of extreme points. To address thisissue, EXITS estimates potential foreground and backgroundpoints within the bounding box by propagating the extremeand background points outside the box. The propagation",
  "Point Supervision": ". Types of weak supervision and how to utilize it for instance segmentation. Top: Box-supervised method relies on bounding boxtightness prior, which is often violated by occlusion (foreground bag contains tree trunk). As a result, the prediction of the method shows anerror in the occluded region. Bottom: Extreme point supervised method (Ours) utilizes extreme points as the initial set of foreground pointsand propagate label through semantic similarity between points. The prediction result demonstrates that our method can predict object maskeven in severe occlusion. Best viewed in color. process is based on pairwise semantic similarity betweenpoints derived by a pretrained transformer encoder so that itreveals foreground and background candidates semanticallysimilar with extreme points and nearby background, respec-tively. The retrieved points together with the extreme anddefinite background points serve as supervision for trainingthe pseudo label generator.As shown in , our pseudo label generator produceshigh-quality pseudo masks, particularly when a target is di-vided into multiple parts, and the enhanced quality of pseudosegmentation labels leads to performance improvement ofour final model. This success is due to the fact that the labelpropagation is conducted on the fully connected graphs ofall the points so that an extreme point can be propagated tospatially distant points. This alleviates the side-effect of thebounding box tightness prior that is violated in the case ofocclusion; the convention box-supervised methods, whichrely heavily on the prior, thus often failed in the case.To quantitatively compare the quality of pseudo labels forseparated objects, we measured the pseudo label quality onSeparated COCO , a subset of COCO comprisingonly separated objects. On the dataset, our method surpassedthe previous best method by 7.3%p in mIoU. We fur-ther evaluated EXITS on three public benchmarks, PASCALVOC , COCO, and LVIS , where EXITS outper-formed all the previous box-supervised methods.In short, the main contribution of this paper is three-fold:",
  ". Related Work": "Instance segmentation. Mask R-CNN proposes a two-stage approach that first detects regions of interest (RoI) andthen predicts segmentation masks within these RoIs. Sub-sequent studies have refined this concept by enhancing fea-ture representation or mask precision .Then, one-stage methods typically builtupon one-stage detectors have gained attractions,thanks to their speed and simplicity. Meanwhile, methodslike SOLO and SOLOv2 introduce box-free one-stage methods without the need for box prediction. Re-cently, query-based methods , inspiredby DETR , offer impressive performance. Although thesefully supervised methods show remarkable performance,they face practical challenges due to their dependence oncostly pixel-wise mask annotation.Weakly supervised instance segmentation. Weakly super-vised methods using image-level class labels ,which depends heavily on class activation maps, have notyet matched fully-supervised performance. Box-supervisedmethods offer better results with lower annotation costs. Thefirst method in this direction refines pseudo masks usingGrabCut , while recent methods incor-porate bounding box tightness priors and Multiple InstanceLearning (MIL) loss, enhanced with techniques like saliency,color-pairwise affinity, and semantic-correspondence. An-",
  "Fully supervised modelLabelInput": ". Overview of entire stages of EXITS. In the first stage,an image cropped around each object is used as an input to trainthe pseudo label generator using point-wise supervision, so thatthe generator learns to predict a binary mask of the object withinthe cropped image. In the second stage, the instance segmentationmodel learns to detect and segment multiple objects, using thegenerated pseudo mask labels from the first stage. other trend includes the Mask Auto Labeler (MAL) ,which uses a two-stage process involving pseudo mask gen-eration and model training. Point-based methods add point labels to boxes for improved localization. In con-trast, our approach leverages extreme points obtained frombox annotations for weak supervision, offering robust cluesfor instance mask estimation. Extreme point for object annotation. An extreme pointlabel is an efficient alternative to a bounding box label, of-fering a faster annotation process . This approach, be-ing five times quicker than traditional methods, has beenincreasingly used in object detection training andobject segmentation tasks . DEXTR ,for instance, utilizes extreme points for segmenting arbi-trary objects by learning the mapping between input imageswith extreme points and their segmentation masks. However,DEXTR still requires expensive pixel-level masks for train-ing. In medical imaging, methods like use extremepoints for training voxel segmentation models, generatingpseudo-scribble labels by linking extreme points via theshortest path. Despite these benefits of extreme point la-bel, it has received limited attention in weakly-supervisedinstance segmentation. Motivated by this, we introduce toleverage extreme point labels for instance segmentation indiverse scenes predicting precise object masks without us-ing pixel-wise annotations. Unlike typical approaches inmedical imaging that generate scribble pseudo labels basedon path-connected object regions, our method uses extremepoints to select pseudo-foreground points, which is crucialin scenarios with occlusions, as demonstrated in .",
  ". Proposed Method": "EXITS consists of two stages: (1) learning a model thatgenerates pseudo segmentation labels of training imagesusing their extreme point labels, and (2) training an instancesegmentation model using the pseudo labels. In the first stage,an object image cropped around each object using extremepoints is used as an input to the pseudo label generator sothat the model learns to predict a binary mask of the objectwithin the cropped image. On the other hand, the instancesegmentation model in the second stage, which is our finalmodel, learns to detect and segment multiple objects. Notethat the pseudo label generator deals with an easier task,i.e., instance segmentation on a single object image, whichenables to improve the quality of pseudo labels it generates.The entire pipeline of EXITS is illustrated in .Since the second stage is the conventional supervisedlearning that can be applied to any instance segmentationmodel, this section elaborates mostly on the first stage, inparticular, how EXITS provides the pseudo label generatorwith effective supervision learning for segmentation. Theoverall pipeline of the first stage is illustrated in . Thekey idea of EXITS is to retrieve pixels likely to belongto the object given the extreme points, and exploit themas supervision for the pseudo label generator. This idea isrealized by propagating the extreme points to other pixelswithin the input object image, while considering the extremepoints as a subset of true pixels of the object.The remainder of this section first discusses extremepoints and advantages of using them (Sec. 3.1), and thenpresents details of the pseudo label generator (Sec. 3.2) andthe second stage (Sec. 3.3) of EXITS.",
  ". Motivation for Using Extreme Points": "Extreme points are defined as the outermost pixels onan object along the cardinal directions: the topmost point(x(t), y(t)), the leftmost point (x(l), y(l)), the bottommostpoint (x(b), y(b)), rightmost point (x(r), y(r)). Papadopou-los et al. demonstrated labeling these points is a moreefficient way to bounding box annotation compared to theconventional method of labeling the top-left (x(l), y(t)) andbottom-right (x(r), y(b)) corner points of a box. This is be-cause such corner points are hard to be identified as theyusually do not belong to the target object area, and thus hu-man annotators often have to adjust their initial corner pointlabels several times. On the other hand, extreme points canbe effortlessly marked and directly converted to a boundingbox. Furthermore, they inherently provide more informationfor the shape and appearance of the target object than cornerpoints since they lie on the object boundary.",
  "(CRF)": ". Overview of the first stage of EXITS framework. The pseudo label generator is trained on images cropped around each object usingthe extreme points, aiming to predict binary masks. Training leverages two loss functions: Lcrf aligns images before and after CRF processing, and Lpoint uses extreme points-derived pseudo point labels for precise pixel-wise supervision. To generate these pseudo pointlabels, EXITS obtains initial foreground and background points from extreme points, then employs the similarity matrix from warm-uptrained similarity extractor for label propagation. After propagation, pseudo point labels are produced based on the difference of propagationscore from the inital foreground and background points. Point dropout is applied as an augmentation generating the final pseudo point labels. of a vision transformer (ViT) encoder and a mask decoder.We retrieve points likely to belong to the object (i.e., fore-ground) or the background, and train the generator using theretrieved points together with the extreme points and definitebackground points outside the box as supervision.To be specific, the initial set of foreground points is de-rived from the extreme points as PFG :=(x(t), y(t) ), (x(l) + , y(l)), (x(b), y(b) + ), (x(r) , y(r)), where is a small margin introduced to push the extreme pointstoward the center of the object so that the points in PFG aremore inward and represent the object more reliably. On theother hand, the initial set of background points PBG con-sists of points located outside the bounding box defined bythe extreme points. To assign pseudo labels to unlabeledpoints within the bounding box, denoted as PBox, the initiallabels from PFG and PBG are propagated to them via randomwalk with a transition probability matrix, i.e., a matrixof pairwise semantic similarity between points in the inputimage. In detail, points in PBox that are highly likely to bepropagated from those in PFG but not from those PBG areconsidered as pseudo foreground. Conversely, points in PBoxthat are more likely to be propagated from PBG than PFG areconsidered as pseudo background.",
  "To capture the semantic similarity between points, EXITSleverages an attention matrix obtained from a multi-head": "self-attention (MHSA) of a ViT encoder. Since the attentionmatrix of a randomly initialized or ImageNet-pretrained ViTis not capable of discriminating between foreground andbackground, we warm-up an extra pretrained ViT encoder,called similarity extractor, that is additionally trained foronly a few epochs on the target dataset with the multipleinstance learning (MIL) loss ; the loss is defined as",
  "(1)": "where M HW is a mask prediction, Ybox {0, 1}HW is the area of the bounding box, Ldice indi-cates the dice loss , and Projx : RHW RW andProjy : RHW RH are projection operations that applythe max operation across each column and each row vectorof the input matrix, respectively. Once trained, the similarityextractor is frozen and used to compute the transition proba-bility matrix during training of the pseudo label generator.We treat each point as a node in a fully connected graphand construct the transition probability between these nodesusing their semantic similarity. To compute the transitionprobability matrix, a cropped image is divided into N Npatches and flattened, then fed into the similarity extrac-tor. The similarity matrix S RN 2N 2 is then derived byaveraging the self-attention matrices from multiple atten-tion heads of a transformer layer. To construct transitionprobability matrix T a doubly stochastic form, the Sinkhorn",
  ", where A = Sinkhorn(S) ,(2)": "where Sinkhorn() is the Sinkhorn-Knopp algorithm .Building the transition probability matrix using MHSA of-fers two advantages. Firstly, since MHSA captures high-levelsemantic relationship between points, the resulting transitionprobability matrix prevents points from being propagated toother points with a similar appearance but different seman-tics. Secondly, MHSA calculates similarities for all pointpairs, thereby naturally yielding a transition probability ma-trix for a fully connected graph. This allows the propagationof labels across separated segments of an object, enhancingthe accuracy of the label assignment process.",
  "PFG :=(xi, yi) : (xi, yi) PBox, (f)i (b)i FG": "PBG :=(xi, yi) : (xi, yi) PBox, (f)i (b)i BG,(5)where FG and BG are threshold hyperparameters.Point dropout. To enhance the diversity of the pseudo pointsupervision and prevent overfitting, we adopt an augmenta-tion technique called point dropout. For each epoch, pointdropout independently eliminates a random subset from bothPFG and PBG, and the removed subsets are excluded fromthe training process during that epoch.",
  "1if pi PFG PFG PBG PBG0otherwise .(7)": "We employ the dice loss between Y and the predicted maskprobability M. Prior to computing the loss, M is downsam-pled to M NN to match the size with Y. Further,we perform an element-wise multiplication between M andK so that the loss signal is applied only to the labeled points.In cases where none of the points is retrieved with the pointretrieval algorithm, i.e., | PFG PBG| = 0, we apply the MILloss in Eq. (1) additionally. The point loss is defined as:",
  "Lpoint = Ldice( MK, Y)+1{| PFG PBG|=0}mil Lmil , (8)": "where is harmard-product operator, 1 is indicator function,and mil is a balancing hyper-parameter.Conditional random field loss. To further refine the pre-dicted mask, EXITS employs CRF loss as in . Specifi-cally, EXITS utilizes a teacher network obtained by the expo-nential moving average of training network, i.e., ViT encoderand mask decoder in pseudo labeled generator parameters.Subsequently, mask predictions from both the training net-work and the teacher network are averaged to obtain Mavg.Then, Mavg is refined through CRF by using the mean-field algorithm and utilized as pseudo ground-truth maskusing the dice loss as follows:",
  ". Learning a Fully Supervised Model": "In the second stage, EXITS employs the trained pseudo labelgenerator to create pseudo mask labels that serve as ground-truth labels for training a fully supervised instance segmen-tation model. To generate the pseudo mask labels, imagescontaining k instances are cropped around the correspondingextreme point annotations and fed into the generator, yield-ing a pseudo mask per object. The decoupled design of theinstance segmentation and pseudo labeling models allowsfor our pseudo labels to be seamlessly integrated into anyfully supervised instance segmentation model.",
  "EXITS (Ours)ESwin-Small Mask2Former44.245.095.995.7": ". Results on COCO val2017 and test-dev. We report performance using Mask Average Precision (Mask AP) and Retention rate(Ret, %). Retention rate is the performance ratio compared to its fully supervised counterpart. Each method is trained with the supervision ofeither a mask (M), bounding box (B), or extreme points (E). Note that the annotation cost of the bounding box and extreme points are equal.",
  ". Experimental Setting": "Datasets. Our method is evaluated on three instance segmen-tation datasets: COCO , PASCAL VOC , and LVISv1.0 . We utilize the 2017 version of COCO, which con-tains 115k images for training, 5k for validation, and 20k fortesting across 80 classes. For PASCAL VOC, we employ theaugmented version that includes 10,582 training and 1,449validation images across 20 semantic classes. LVIS v1.0contains 164k images spanning 1200+ categories, and wefollow the standard partition for training and validation setsas described in . To obtain extreme point annotations, wefollow the protocol described in ExtremeNet 1, whichconverts mask annotations to extreme point annotations.Evaluation metric. Following previous work we use coco-style Mask AP as an evaluation metric. ForCOCO and LVIS v1.0 datasets, we additionally reportRetention Rate as in MAL , which is the ratio of per-formance compared to its fully supervised counterpart.Implementation details (first stage). We followed the ar-chitecture of MAL for consistent comparison. The Stan-dard ViT-Base , pretrained with MAE , served as ourViT encoder, paired with an attention-based mask decoder.",
  ". Qualitative comparison of pseudo mask labels on the Separated COCO dataset. (a) Ours, (b) MAL , (c) Ground Truth": "by cosine annealing scheduler. An input image is croppedaround an object and resized to 512 512, where data aug-mentation same as MAL is applied. We use MHSA of the10th transformer layer of the similarity extractor as similar-ity matrix to construct the TPM. We set the iteration to3, the point dropout rate to 0.9, FG to 1 103, and BG to1104. For COCO and LVIS v1.0 datasets, the similarityextractor is trained for 1 and 10 epochs, respectively. ForVOC, the similarity extractor and the pseudo label generatorare trained for 8 epochs and 80 epochs, respectively. Moredetails are given in the supplementary materials.Implementation details (second stage). Various backbonenetworks and instance segmentation models are adoptedfor the second stage. For COCO dataset, we employResNets , ResNeXts , Swin Transformer asbackbone and SOLOv2 and Mask2Former as in-stance segmentation model. For VOC dataset, we employthe ResNet backbone and SOLOv2 instance segmentationmodel. For LVIS v1.0, we employ ResNeXts backbone andMask R-CNN instance segmentation model. We followthe training configuration of mmdetection 2.",
  ". Comparisons with State-of-the-art": "Results on COCO. In , we compare the performanceof EXITS with the baselines trained with the supervisionof either a mask (M), bounding box (B), or extreme points(E), on the COCO dataset. Note that the extreme point hasthe same labeling cost as the bounding box. EXITS outper-forms the box-supervised baselines in every setting acrossall the compared backbones and instance segmentation mod-els, indicating that EXITS produces high-quality pseudolabels regardless of the backbone or the applied instancesegmentation model. Especially with the ResNet-101-DCNbackbone, EXITS outperforms the state of the arts such as",
  ". Results on LVIS v1.0. Best results are noted as bold": "BoxTeacher(+2.6 AP), SIM(+2.8 AP), and MAL(+1.5 AP)by a significant margin. While the baseline method alreadyachieved a retention rate of over 91%, EXITS further narrowsthe performance gap with its fully-supervised counterparts.Results on PASCAL VOC. In , we compare the per-formance of EXITS with the baselines on the PASCAL VOCdataset. EXITS outperforms the box-supervised baselineswith both the ResNet50 and the ResNet101 backbones. Espe-cially with ResNet50 backbone, EXITS shows a significantimprovement of 1.8%p, compared to the previous arts. Thisshows that EXITS predicts higher-quality masks for instancesegmentation compared to box-supervised methods.Results on LVISv1.0. In , we compare the perfor-mance of EXITS with MAL on the LVIS v1.0 dataset.EXTIS clearly outperforms MAL in both AP and Ret, whichindicates the effectiveness of utilizing extreme points.",
  ". Pseudo-label Quality Comparison": "We evaluate the quality of the generated pseudo mask onCOCO and Separated COCO dataset in mIoU. Sepa-rated COCO is a subset of COCO and consists of objectswhose segmentation masks are separated into multiple partsdue to occlusion. In , we compare the pseudo labelquality with MAL . EXITS shows a significant mIoU . Qualitative results of the final prediction of EXIST on COCO test-dev, using Mask2Former with Swin-Small backbone. Ourgenerated pseudo mask labels, EXITS produces high-quality segmentation results, even in separated objects or complex scenes.",
  ". Pseudo label quality of the first stage": "improvement of 7.3%p compared to MAL on the SeparatedCOCO dataset, indicating that EXITS generates high-qualitymasks for separated objects, thanks to its propagation con-ducted on the fully connected graphs of all points. Thisshows that EXITS successfully alleviates the side-effect ofthe bounding box tightness prior. In , we conduct aqualitative comparison of pseudo mask labels, where EXITSexhibits superior pseudo label quality compared to MAL.Thanks to our high-quality pseudo mask labels, the secondstage model produces delicate prediction even in separatedobjects or complex scenes, as illustrated in .",
  "For the ablation studies, we employ ResNet50 backbone withthe SOLOv2 model evaluated on the PASCAL VOC datasetusing coco-style AP, AP50, AP75 metrics. More analysis canbe found in the supplement": "Contribution analysis of point set in Lpoint. In , weevaluate the contributions of the initially labeled point setPFGPBG, and the pseudo labeled point set PFG PBG, whentraining with Lpoint. We consider MAL as a strong base-line without any point supervision (the first-row of ).The improvement from utilizing PFG PBG is marginal,showing that using extreme points navely is insufficientto utilize their information for segmentation. Pseudo pointsupervision from PFG PBG gives significant performanceimprovement of 2.4%p AP, indicating that our point retrievalalgorithm is effective.",
  ". Conclusion": "We have introduced EXITS, a novel framework for learninginstance segmentation using extreme points cost-effectively.EXITS narrows the gap between weakly supervised instancesegmentation and its fully supervised counterparts, showingparticular strength in segmented objects in severe occlusionscenarios. On the other hand, even with the use of extremepoints, differentiating between occluded objects of the sameclass continues to be a challenging task. Our next agenda isto address this issue by using minimal additional supervision,such as center points.Acknowledgement. This work was supported by the NRFgrant and the IITP grant funded by Ministry of Sci-ence and ICT, Korea (NRF-2018R1A5A1060031, NRF-2021R1A2C3012728, IITP-2019-0-01906, IITP-2022-0-00926).",
  "A. Details of Pseudo Label Generator Architec-ture": "The network of pseudo label generator consists of the ViTencoder and the mask decoder. The architecture of ViT en-coder follows the standard vision transformer design, whichconsists of 12 transformer layers. We do not use a class token,only the output features are fed into the mask decoder. TheViT encoder produces the image features F RNND from the cropped input image.The mask decoder architecture consists of two heads, apixel-wise head, and a prototype head, a design inspired byYOLACT . The pixel-wise head comprises four convo-lutional layers, with bilinear interpolation used to upscalethe feature resolution between the second and third convolu-tional layers. The feature map F goes through the pixel-wisehead and resulting Fpixel RHW D/3. The prototype headconsists of two fully connected layers with ReLU activationfunction and D/3 hidden dimensions. We use average pool-ing along spatial dimension of F, and it go through theprototype head and resulting Fproto RD/3",
  "B. More Experimental Details": "The hyperparameter , which is a small margin to pushextreme points toward the center of the object, is set asfollows: 24 for COCO , 16 for LVIS v1.0 , and 12 forPASCAL VOC . Note that we push the extreme pointswith these margin on the resized image space, which is 512512. The hyperparameters mil, point, crf, which balance each loss term, are set as follows: 10, 0.5, 0.5 for COCOand LVIS v1.0, and 10, 0.05, 0.5 for PASCAL VOC. Notethat MIL loss is applied only to samples where pseudo pointsupervision within the bounding box could not be providedusing the point retrieval algorithm, i.e. | PFG PBG| = 0.This accounts for only about 7% of the total images.",
  "C. Analysis on Propagation": "Similarity matrix. We extract the semantic similarity be-tween points from the multi-head self-attention of the trans-former in the similarity extractor. Table a1 shows the impactof using different transformer layers for the extraction of thesimilarity matrix. Since earlier layers easily miss high-levelsemantics, averaging similarity matrices across all layersdoes not yield the best results. Therefore, we empiricallychoose to use 10th layer for extracting the similarity matrix.Effect of number of hops (). Table a2 shows the effectof in propagation process. when = 1, equivalent togenerating pseudo point labels directly from the similaritymatrix, there is a substantial drop in performance. This indi-cates that the propagation process is crucial for generatingaccurate pseudo point labels. We also measured the perfor-mance when the random walk propagation was continueduntil convergence after an unlimited number of steps, whichis also known as the Absorbing Markov Chain .It is calculated by",
  "(a)(b)(c)(d)": "Figure a1. Average Precision (AP) of our second stage model varying hyperparameters. The model is evaluated on Pascal VOC usingSOLOv2 and ResNet50 backbone. (a) The foreground point threshold FG. (b) The background point threshold BG. (c) Lossbalancing term point. (d) Loss balancing term crf. The blue diamond marker indicates the value selected for our final model. where I denotes identity matrix and denotes blend-ing coefficient between propagated scores and initial scores.In cases where the random walk process converged, we ob-served the best performance at = 0.25; however, it still didnot outperform the results obtained after three propagationsteps. Furthermore, considering the increased computationalcost needed to compute Eq. (a2), we set the optimal value of to 3.",
  "D. Impact of Hyperparameters": "Effect of FG and BG. In Fig. a1 (a) and (b), we demonstratethe effect of two thresholds, FG and BG, respectively. Inthe case of FG, we observe that the hyperparameter valuewe selected are not optimal and there is potential for furtherperformance improvement. This indicates that we did notexhaustively tune these parameters using the validation set.Effect of loss balancing terms. In Fig. a1 (c) and (d), weshow the instance segmentation results by using differentloss coefficients, point and crf. Our model demonstratesrobustness to these hyperparameter changes, surpassing thebaseline in every setting.",
  "F. Time and Memory Complexity of EXITS": "We compare the training time and number of parameters ofEXITS with those of MAL, which is our strong baselinemodel. As shown in Table a5, EXITS shows a 20% increasein training time over MAL due to the warm-up of the simi-larity extractor and point retrieval process in Stage 1, withan increase in the number of parameters by 86M due tothe similarity extractor module. Although EXITS requiresan additional step for warm-up, the consequent increase intraining time is only 5% of the total training time, and thesimilarity extractor does not affect the space-time complexityof inference.",
  "(b) EXITS (Ours)": "Figure a3. Qualitative comparison of instance segmentation results in complex scenes. (a) inputs (b) EXITS (ours), (c) model train withpseudo labels from MAL , (d) model train with the ground-truth label. Figure a4. Failure cases of pseudo labels. Our pseudo label generator sometimes fails to predict when instances of the same class areencompassed by the same bounding box. Red box indicates generated pseudo label from the first stage of EXITS and blue box indicatesground-truth label.",
  "G. More Qualitative Results": "We visualize the final prediction results produced byMask2Former trained with pseudo labels from thepseudo label generator of EXITS using COCO test-devset. We also visualized the results of the state-of-the-art box-supervised instance segmentation method, MAL , andthe upper-bound model trained with the ground-truth labelsas a comparison group. As can be seen in Fig. a2, the instancesegmentation model trained with our method is capable ofgenerating masks for separated objects, excluding the oc-cluder. This demonstrates almost no difference compared tothe results trained with ground-truth labels, while the modeltrained using pseudo labels generated by MAL struggles inthese cases. Additionally, as illustrated in Fig. a3, the modeltrained with our pseudo labels thoroughly predicts even incomplex scenes with numerous instances, in contrast to mod-els trained using pseudo labels generated by MAL, whichoften fail in these scenarios.",
  "H. Limitation": "As observed in Fig. a4, our pseudo label generator oftenmispredicts when multiple objects of the same class areencompassed by the same bounding box. This issue arisesas our point retrieval algorithm assigns pseudo point labelsbased on the results of the propagation difference betweenpoints outside of the bounding box and extreme points. Onepotential clue to solve this issue is to utilize the fact that evenobjects within the same bounding box have different extremepoint annotations. However, this is beyond the scope of thiswork and will be left for future research. Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly su-pervised learning of instance segmentation with inter-pixelrelations. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 22092218,2019. 1, 2",
  "Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: highquality object detection and instance segmentation. IEEEtransactions on pattern analysis and machine intelligence, 43(5):14831498, 2019. 2": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, Au-gust 2328, 2020, Proceedings, Part I 16, pages 213229.Springer, 2020. 2 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the IEEE/CVF International Conference on Com-puter Vision (ICCV), pages 96509660, 2021. 10 Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, XiaoxiaoLi, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybridtask cascade for instance segmentation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2019. 2 Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, YuXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,Jiarui Xu, et al. Mmdetection: Open mmlab detection toolboxand benchmark. arXiv preprint arXiv:1906.07155, 2019. 7",
  "the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 26172626, 2022. 1, 3": "Tianheng Cheng, Xinggang Wang, Lichao Huang, and WenyuLiu. Boundary-preserving mask r-cnn. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, Au-gust 2328, 2020, Proceedings, Part XIV 16, pages 660676.Springer, 2020. 2 Tianheng Cheng, Xinggang Wang, Shaoyu Chen, WenqiangZhang, Qian Zhang, Chang Huang, Zhaoxiang Zhang, andWenyu Liu. Sparse instance activation for real-time instancesegmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 44334442, 2022. 2 Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Qian Zhang,and Wenyu Liu. Boxteacher: Exploring high-quality pseudolabels for weakly supervised instance segmentation. In Proc.IEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 31453154, 2023. 1, 6",
  "Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, andYichen Wei. Solq: Segmenting objects by learning queries.Advances in Neural Information Processing Systems, 34:2189821909, 2021. 2": "Reuben Dorent, Samuel Joutard, Jonathan Shapey, Aaron Ku-jawa, Marc Modat, Sebastien Ourselin, and Tom Vercauteren.Inter extreme points geodesics for end-to-end weakly super-vised image segmentation. In Medical Image Computingand Computer Assisted InterventionMICCAI 2021: 24thInternational Conference, Strasbourg, France, September 27October 1, 2021, Proceedings, Part II 24, pages 615624.Springer, 2021. 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In Proc. InternationalConference on Learning Representations (ICLR), 2021. 6, 10",
  "Mark Everingham, Luc Van Gool, Christopher KI Williams,John Winn, and Andrew Zisserman. The Pascal Visual ObjectClasses (VOC) Challenge. International Journal of ComputerVision (IJCV), 2010. 2, 6, 9": "Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A datasetfor large vocabulary instance segmentation. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 53565364, 2019. 2, 6, 9 Junjie He, Pengyu Li, Yifeng Geng, and Xuansong Xie.Fastinst: A simple query-based model for real-time instancesegmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages2366323672, 2023. 2, 6",
  "vision learners. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1600016009, 2022. 6, 10": "Cheng-Chun Hsu, Kuang-Jui Hsu, Chung-Chi Tsai, Yen-YuLin, and Yung-Yu Chuang. Weakly supervised instance seg-mentation using the bounding box tightness prior. In Proc.Neural Information Processing Systems (NeurIPS). CurranAssociates, Inc., 2019. 1, 4, 6 Jie Hu, Chen Chen, Liujuan Cao, Shengchuan Zhang, AnnanShu, Guannan Jiang, and Rongrong Ji. Pseudo-label align-ment for semi-supervised instance segmentation. In Proc.IEEE International Conference on Computer Vision (ICCV),pages 1633716347, 2023. 1",
  "Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak.Consistency-based semi-supervised learning for object detec-tion. Proc. Neural Information Processing Systems (NeurIPS),32, 2019. 1": "Bowen Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, andMing-Hsuan Yang. Saliency detection via absorbing markovchain. In Proceedings of the IEEE international conferenceon computer vision, pages 16651672, 2013. 9 Anna Khoreva, Rodrigo Benenson, Eddy Ilg, Thomas Brox,and Bernt Schiele. Simple does it: Weakly supervised instanceand semantic segmentation. In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition (CVPR),pages 876885, 2017. 1, 2 Beomyoung Kim, Youngjoon Yoo, Chae Eun Rhee, andJunmo Kim.Beyond semantic to instance segmentation:Weakly-supervised instance segmentation via semantic knowl-edge transfer and self-refinement.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 42784287, 2022. 1, 2 Beomyoung Kim, Joonhyun Jeong, Dongyoon Han, andSung Ju Hwang. The devil is in the points: Weakly semi-supervised instance segmentation via point-guided mask rep-resentation. In Proc. IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 1136011370, 2023.1",
  "Conference on Machine Learning (ICML), pages 282289,2001. 4, 5": "Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Rad-hakrishnan, Guilin Liu, Yuke Zhu, Larry S Davis, and AnimaAnandkumar. Discobox: Weakly supervised instance segmen-tation and semantic correspondence from box supervision. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 34063416, 2021. 1, 2, 6 Shiyi Lan, Xitong Yang, Zhiding Yu, Zuxuan Wu, Jose M. Al-varez, and Anima Anandkumar. Vision transformers are goodmask auto-labelers. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 2374523755, 2023. 1, 2, 3, 5, 6, 7, 8, 10, 11, 12 Jungbeom Lee, Jihun Yi, Chaehun Shin, and Sungroh Yoon.Bbam: Bounding box attribution map for weakly supervisedsemantic and instance segmentation. In Proceedings of theIEEE/CVF conference on computer vision and pattern recog-nition, pages 26432652, 2021. 1, 6 Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni,Heung-Yeung Shum, et al. Mask dino: Towards a unifiedtransformer-based framework for object detection and seg-mentation. arXiv preprint arXiv:2206.02777, 2022. 2",
  "Ruihuang Li, Chenhang He, Yabin Zhang, Shuai Li, LiyiChen, and Lei Zhang. Sim: Semantic-aware instance maskgeneration for box-supervised instance segmentation, 2023.1, 6": "Wentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Xian-Sheng Hua, and Lei Zhang. Box-supervised instance segmen-tation with level set evolution. In Computer VisionECCV2022: 17th European Conference, Tel Aviv, Israel, October2327, 2022, Proceedings, Part XXIX, pages 118. Springer,2022. 1, 2, 6 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft COCO: common objects in context. InProc. European Conference on Computer Vision (ECCV),2014. 2, 6, 9 Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Pathaggregation network for instance segmentation. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2018. 2 Yen-Cheng Liu, Chih-Yao Ma, and Zsolt Kira. Unbiasedteacher v2: Semi-supervised object detection for anchor-freeand anchor-based detectors. In Proc. IEEE Conference onComputer Vision and Pattern Recognition (CVPR), pages98199828, 2022. 1 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 6, 7",
  "object segmentation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 616625,2018. 3": "Yassine Ouali, Celine Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-consistencytraining. In Proc. IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 1267412684, 2020.1 Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, andVittorio Ferrari. Extreme clicking for efficient object annota-tion. In Proceedings of the IEEE international conference oncomputer vision, pages 49304939, 2017. 1, 3 Joseph Redmon, Santosh Divvala, Ross Girshick, and AliFarhadi. You only look once: Unified, real-time object de-tection. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 779788, 2016. 2 Holger Roth, Ling Zhang, Dong Yang, Fausto Milletari, ZiyueXu, Xiaosong Wang, and Daguang Xu. Weakly supervisedsegmentation from extreme points. In Large-Scale Annotationof Biomedical Data and Expert Label Synthesis and HardwareAware Learning for Medical Imaging and Computer AssistedIntervention: International Workshops, LABELS 2019, HAL-MICCAI 2019, and CuRIOUS 2019, Held in Conjunction withMICCAI 2019, Shenzhen, China, October 13 and 17, 2019,Proceedings 4, pages 4250. Springer, 2019. 3",
  "Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. grabcut interactive foreground extraction using iteratedgraph cuts. ACM transactions on graphics (TOG), 23(3):309314, 2004. 2": "Josef Lorenz Rumberger, Jannik Franzen, Peter Hirsch, Jan-Philipp Albrecht, and Dagmar Kainmueller. Actis: Improvingdata efficiency by leveraging semi-supervised augmentationconsistency training for instance segmentation. In Proc. IEEEInternational Conference on Computer Vision (ICCV), pages37903799, 2023. 1 Carole H Sudre, Wenqi Li, Tom Vercauteren, SebastienOurselin, and M Jorge Cardoso. Generalised dice overlap asa deep learning loss function for highly unbalanced segmen-tations. In Deep Learning in Medical Image Analysis andMultimodal Learning for Clinical Decision Support: ThirdInternational Workshop, DLMIA 2017, and 7th InternationalWorkshop, ML-CDS 2017, Held in Conjunction with MICCAI2017, Quebec City, QC, Canada, September 14, Proceedings3, pages 240248. Springer, 2017. 4 Chufeng Tang, Lingxi Xie, Gang Zhang, Xiaopeng Zhang,Qi Tian, and Xiaolin Hu. Active pointly-supervised instancesegmentation. In Computer VisionECCV 2022: 17th Eu-ropean Conference, Tel Aviv, Israel, October 2327, 2022,Proceedings, Part XXVIII, pages 606623. Springer, 2022. 1,3",
  ", 2020, Proceedings, Part I 16, pages 282298. Springer,2020. 1, 2, 6": "Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen. Box-inst: High-performance instance segmentation with box an-notations. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 54435452,2021. 1, 2, 4, 6 Hugo Touvron, Matthieu Cord, Matthijs Douze, FranciscoMassa, Alexandre Sablayrolles, and Herve Jegou. Trainingdata-efficient image transformers & distillation through atten-tion. In International conference on machine learning, pages1034710357. PMLR, 2021. 10 Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, andLei Li. Solo: Segmenting objects by locations. In ComputerVisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part XVIII 16, pages649665. Springer, 2020. 1, 2",
  "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-hua Shen.Solov2: Dynamic and fast instance segmenta-tion. Advances in Neural information processing systems, 33:1772117732, 2020. 1, 2, 6, 7, 10": "Xinggang Wang, Jiapei Feng, Bin Hu, Qi Ding, Longjin Ran,Xiaoxin Chen, and Wenyu Liu. Weakly-supervised instancesegmentation via class-agnostic learning with salient images.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1022510235, 2021. 2 Zhenyu Wang, Yali Li, and Shengjin Wang. Noisy boundaries:Lemon or lemonade for semi-supervised instance segmenta-tion? In Proc. IEEE Conference on Computer Vision andPattern Recognition (CVPR), pages 1682616835, 2022. 1 Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, XueboLiu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask:Single shot instance segmentation with polar representation.In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1219312202, 2020. 2 Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, andKaiming He. Aggregated residual transformations for deepneural networks. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 14921500,2017. 7 Donghun Yeo, Bohyung Han, and Joon Hee Han. Unsuper-vised co-activity detection from multiple videos using absorb-ing markov chain. In Proceedings of the AAAI Conference onArtificial Intelligence, 2016. 9 Donghun Yeo, Jeany Son, Bohyung Han, and Joon Hee Han.Superpixel-based tracking-by-segmentation using markovchains. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 18121821, 2017. 9",
  "Guanqi Zhan, Weidi Xie, and Andrew Zisserman. A tri-layerplugin to improve occluded detection. British Machine VisionConference, 2022. 2, 7, 8": "Gang Zhang, Xin Lu, Jingru Tan, Jianmin Li, ZhaoxiangZhang, Quanquan Li, and Xiaolin Hu. Refinemask: Towardshigh-quality instance segmentation with fine-grained features.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 68616869,2021. 2 Rufeng Zhang, Zhi Tian, Chunhua Shen, Mingyu You, andYouliang Yan. Mask encoding for single shot instance seg-mentation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2020. 2 Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl.Bottom-up object detection by grouping extreme and centerpoints. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 850859, 2019.3, 6 Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, and JianbinJiao. Weakly supervised instance segmentation using classpeak response. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 37913800,2018. 2 Yanzhao Zhou, Xin Wang, Jianbin Jiao, Trevor Darrell, andFisher Yu. Learning saliency propagation for semi-supervisedinstance segmentation. In Proc. IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 1030710316, 2020. 1 Yi Zhu, Yanzhao Zhou, Huijuan Xu, Qixiang Ye, David Do-ermann, and Jianbin Jiao. Learning instance activation mapsfor weakly supervised instance segmentation. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 31163125, 2019. 1, 2"
}