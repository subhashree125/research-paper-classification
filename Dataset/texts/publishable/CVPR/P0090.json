{
  "Abstract": "In this report, we present our solutions to the EgoVis Chal-lenges in CVPR 2024, including five tracks in the Ego4Dchallenge and three tracks in the EPIC-Kitchens challenge.Building upon the video-language two-tower model andleveraging our meticulously organized egocentric videodata, we introduce a novel foundation model calledEgoVideo. This model is specifically designed to cater tothe unique characteristics of egocentric videos and providesstrong support for our competition submissions.In theEgo4D challenges, we tackle various tasks including Natu-ral Language Queries, Step Grounding, Moment Queries,Short-term Object Interaction Anticipation, and Long-termAction Anticipation. In addition, we also participate in theEPIC-Kitchens challenge, where we engage in the ActionRecognition, Multiple Instance Retrieval, and DomainAdaptation for Action Recognition tracks.By adaptingEgoVideo to these diverse tasks, we showcase its versatilityand effectiveness in different egocentric video analysisscenarios,demonstrating the powerful representationability of EgoVideo as an egocentric foundation model.Our codebase and pretrained models are publicly availableat",
  "These authors contributed equally": "person viewpoint analysis, egocentric video understandingfocuses on understanding human activities as they occurfrom the camera wearers viewpoint, often captured throughwearable cameras or head-mounted devices. This task holdssignificant implications across various domains, includinghealthcare , virtual/augmented reality , and human-computer interaction . Egocentric video action under-standing facilitates applications ranging from assistive tech-nologies for the visually impaired to immersive experiencesin virtual environments . Additionally, it fosters ad-vancements in personalized assistance systems , sportsanalytics , and surveillance technologies , thereby un-derscoring its multifaceted impact on both academic re-search and practical applications. In recent years, action recognition methods have under-gone significant advancements, propelled by the surge indeep learning techniques and the availability of large-scaleannotated datasets . With the advent of convolu-tional neural networks (CNNs) and recurrent neu-ral networks (RNNs), action recognition has witnessed aparadigm shift towards end-to-end trainable models capa-ble of automatically learning discriminative features fromvideo clips. Furthermore, the integration of attention mech-anisms, spatial-temporal modeling, and graph-based rep-resentations has further enhanced the performance of ac-tion recognition systems. Benefit from large-scale vision-language datasets , a variety of video founda-tion models have been designed to learn generalvideo representations, which have shown to benefit a se-ries of downstream action recognition tasks . How-ever, as most of these video foundation models are trainedon videos recorded in third-person view, the learned repre-sentations turn out sub-optimal for egocentric video under-standing .",
  "heads": ". The workflow of the training process of EgoVideo. It includes 3 stages: in the first stage, we filter and select high-qualityegocentric video and text pairs from multiple existing datasets. Then we perform post pertaining using the data in stage 1 by standardvideo-text contrastive learning. Finally, we adapt the pretrained EgoVideo model to different downstream tasks. To tackle the challenge, we propose a 3-stage train-ing paradigm for egocentric video understanding, includ-ing multiple tasks like natural language grounding, domainadaptation, and multi-instance retrieval. Specifically, wefirst filter and select high-quality egocentric video and textpairs from multiple existing datasets . Thesehigh-quality data serve as the foundational data for transfer-ring models learned from general domains to the egocen-tric domain. We adopt a video foundation model thatis pre-trained on large-scale video-language datasets .With the help of rich vision features and a wide range ofaction-aware knowledge, this model is capable of extract-ing general video feature representations, acting as a goodstarting point for subsequent feature learning. In the secondstage, to mitigate the domain gap between web-scale videodatasets and egocentric videos, we perform post-training onthe selected data, effectively transferring the general videofeature representations to egocentric domain.We termthe resulting model as EgoVideo, consisting of a strongegocentric video encoder EgoVideo-V and a text encoderEgoVideo-T. In the third stage, we conduct task-specificfine-tuning of EgoVideo-V and EgoVideo-T on three differ-ent egocentric video understanding tasks, e.g., natural lan-guage queries, domain adaptation action recognition, andmulti-instance retrieval. Experimental results show that our 3-stage strategy hasled to a remarkable improvement in overall model perfor-mance. The model excels at understanding fine-grained,action-specific information, demonstrating strong perfor-mance in action recognition and multi-instance retrieval.Moreover, benefiting from the multi-stage training, ourmodel exhibits video understanding ability across a widerange of actions.",
  ". Stage1: Augmented Data Selection": "To better transfer the video foundation model learnedin the general video domain into the egocentric domain,we collect a broad range of paired egocentric video-textpairs from public video datasets, such as Ego4d ,HowTo100M , EgoExoLearn , and Ego4d Goal-Step by automatic filtering techniques. We do this toensure a wider range of egocentric data and maintain thepertaining data quality. This results in around 7M video-text pairs.",
  ". Stage2: Egocentric Video Post-training": "In this work, we adopt InternVideo2 , a novel videofoundation model that is pre-trained on millions of video-text pairs . InternVideo2 is built through a progressivelearning scheme, consisting of feature distillation, multi-modal alignment, and vision-language connection. The pre-trained video foundation model thus acts as a strong startingpoint for the subsequent feature learning process. More de-tails about the foundation model can be found in .We then perform the post-pretraining process and trainthe model for 5 epochs on the hybrid data in Stage 1 toimprove the egocentric video understanding ability. Themodel is optimized via a standard visual-text contrastiveloss. During training, we also examine the models egocen-tric video understanding ability on EPIC-Kitchen-100 zero-shot multi-instance retrieval benchmark , and the resultsare shown in . We term this egocentric video founda-",
  ". Task 1: Natural Language Queries @ Ego4D": "Task Definition Given a video clip and a natural lan-guage query, the Ego4D Natural Language Queriestask aims to identify the temporal window correspondingto the querys answer.Approach Our solution builds upon GroundNLQ and employs our EgoVideo to extract video and text fea-tures.GroundNLQ proposes a multi-modal multiscaletransformer encoder module to encode both video andtext features and then efficiently fuse them.FollowingGroundNLQ, we first pretrain on NaQ data and thenfine-tune on NLQ data. The key driver for this task is Yup-ing He.Implementation Details. 1) Feature Extraction: We lever-age ViT-1B of EgoVideo to extract video feature for eachsnippet, which contains s = 16 consecutive frames withinterval = 16. The text features are extracted by BERT-Large of EgoVideo. 2) Training Setup: In the pretrainingphase, we set the batch size to 8 and the total epochs to 10,with a warmup of 4 epochs, employing a maximum learningrate of 2e-4. In the fine-tuning phase, we set the batch sizeto 2 and the total epochs to 10, with a warmup of 4 epochs,with a maximum learning rate of 5e-5.Results. presents the results of NLQ. #A and #Demploy identical model and training strategy, while oursingle models features( #D) significantly outperform theensemble of EgoVLP and InternVideo(#A). #E combinespredictions from GroundNLQ, GroundNLQ*, and Ground-VQA . GroundNLQ* is a variant of GroundNLQ, dis-tinguished by the integration of a cross-modal layer withinthe encoder. GroundVQA leverages a large language modelto encode visual and language features. Ensemble methodsfurther enhance performance.",
  "Approach Similar to NLQ, we use GroundNLQ as thegrounding model for Step Grounding and adopts EgoVideoto extract video and text features. The key driver for thistask is Yuping He": "Implementation Details. We adopt the consistent config-urations with NLQ for feature extraction. During the fine-tuning phase, we use a batch size of 8, apply dropout witha probability of 0.2, and set the drop path rate to 0.2. Otherhyperparameters remain the same as in NLQ. Results. displays our results on Step-Grounding.The official baseline uses VSLNet as the groundingmodel and Omnivore features. In contrast, our solu-tion leverages stronger video and text features along withadvanced grounding models, resulting in notable improve-ments.After ensembling results from GroundNLQ andGroundNLQ, we achieve further gains.",
  ". Task 3: Moment Queries @ Ego4D": "Task Definition Given an egocentric video and a specificaction category, Moment Queries task aims to retrieve alltemporal segments corresponding to this action category.The action categories are pre-defined and specific to first-person activities. Approach We adopt ASL as our task-specific solution.ASL divides the task into two subtasks: classification andlocalization. It incorporates an action sensitivity evaluatormodule to assess the significance of each frame relative tothe action, guiding the learning process for each subtask.The key driver for this task is Kanghua Pan. Implementation Details. 1) Feature Extraction: For fur-ther enhancing vision-only performance, we finetune thevideo encoder of EgoVideo-V on MQ data and the result-ing model is termed as EgoVideo-MQ. Consistent with theconfiguration of NLQ and GoalStep, we adopt EgoVideo-Vand EgoVideo-MQ to extract two types of video features. 2)Training Setup: InternVideo, EgoVideo-V, and EgoVideo-MQ features are all projected to 512 dimensions, and otherhyperparameters remain consistent with ASL. Results. displays the results for MQ. Comparing#A and #C, our single models features outperform the en-semble of EgoVLP and InternVideo, demonstrating the su-perior performance of EgoVideo. #D combines InternVideowith EgoVideo-V features. Specifically, we project eachfeature and concatenate them. #E incorporates InternVideowith EgoVideo-MQ features. #F combines predictions from#D and #E by averaging the output logits for classificationand localization from each model. Compared with ASL,our solution leverages multiple complementary features andachieves better results.",
  ". Task 4: Short-term Object-interaction Antici-pation @ Ego4D": "Task Definition Short-term object interaction anticipationtask aims to predict the next human-object interaction hap-pening after a given timestamp . Given an input video,the model is required to anticipate at what time and in whatlocation, what kind of object interaction will happen. Approach We choose to use Stillfast as our down-stream solution.This approach separately extracts high-resolution, low-frame-rate image information and low-resolution, high-frame-rate video information, and thenfuses them to obtain multi-modal spatio-temporal features.Stillfast uses X3D-M as the backbone for video fea-ture extraction. We replace the X3D-M with our strongerVideoEgo-V. Differing from the original Stillfast frame-work which fuses multiple multi-scale intermediate layersof X3D-M (fast) and ResNet (still), we interpolate the lastlayer feature map of VideoEgo-V into different sizes andfuse them into the multi-scale still features generated byResNet. The key driver for this task is Guo Chen. Implementation Details. We adopt the training setup con-sistent with Stillfast. The difference is that we set the droppath rate to 0.3, and layer-wise lr decay to 0.9. Meanwhile,we enable BF16 for stable training. Results. displays the results for Short-term object-interaction anticipation on the test set. The results indicatethat our EgoVideo-V is also suitable for direct transfer toforecasting tasks. In particular, the predictions of Verb andTTC are challenging to substantiate with direct evidenceand often rely on advanced cognitive reasoning abilities.",
  ". Task 5:Long-term Action Anticipation @Ego4D": "Task Definition Long-term action anticipation is a task thataims to predict multiple future actions following a given ac-tion. Each action is composed of a verb and a noun. Givenan input video up to a particular timestamp, which corre-sponds to the last visible action, The goal is to predict a listof the twenty subsequent actions.Approach Recent methods leveraging Large Lan-guage Models (LLMs) have shown superior performancein LTA tasks by converting video actions into natural lan-guage sequences, which LLMs then use to predict future ac-tions. For LLM-based methods, better classification predic-tion and stronger LLM intuitively bring stronger languagecomprehension and prediction capabilities. The key driverfor this task is Yicheng Liu.Video Clip Classification. Previous methods typically usedvideo encoders like EgoVLP or CLIP com-bined with a Transformer-based classification head to ob-tain verbs and nouns. We simply finetune EgoVideo-V onLTA data to replace the previous classification predictionswith our better inference results.Anticipation with LLMs. We employed the Vicuna-7B model as the LLM. During fine-tuning, we fixed thehistorical action sequence length to 8 and used the subse-quent 20 actions as labels. We used EgoVLP to extractfeatures and augment the training set.Experiments Implementation Details.Following ,during the fine-tuning phase, we set the learning rate to3e-4, gamma to 0.85, batch size to 32, and the number ofepochs to 3 for all models. We also use LoRA to im-prove the speed and efficiency of fine-tuning.Action Recognition Results. shows the accuracy ofaction recognition on the validation set. The results revealthat our EgoVideo-V can achieve better prediction for thenext long-term anticipation.Action Anticipation Results. shows the LTA re-sults on the validation and testing set. The table shows thatclassification results of EgoVideo-V achieved significantimprovements in anticipation performance compared withEgoVLP , when using LLaMA2-7B for anticipation.",
  ". Task6: Action Recognition @ EPIC": "Task definition. Action recognition considers a short videoclip and requires the model to predict the verb/noun/actionclasses of the action in this segment. The evaluation metricincludes Top-1/5 Accuracy.Training.Following prior works , we train ourmodel for 100 epochs on the training set with a learning rateof 1e-5 and batch size of 48. We conduct warm-up trainingfor 2 epochs using the cross-entropy loss. The model istrained on 16 A100 GPUs.Results. present fine-tuned models performance onEK100 action recognition. The results reveal significant ad-vancements with our proposed method, surpassing state-of-the-art approaches in both Verb/Noun/Action top-1 scores.Our single EgoVideo-V achieves 72.9%/68.7%/56.2%Verb/Noun/Action top-1 scores on the test set. This is farahead of that last challenge champion whose ensembledVerb/Noun/Action top-1 results is 71.7%/65.8%/54.3%.After ensembling three different models, our EgoVideo-Vfurther achieves slight improvement +0.2%/+1.1%/+0.6%,and the final testing results are 73.1%/69.8%/56.8%. Over-all, the results underscore the effectiveness of our approachin enhancing the understanding of daily human activities",
  ". Task7: Multi-instance Retrieval @ EPIC": "Task definition: The primary objective of Epic-KitchenMulti-Instance Retrieval task is to develop models capa-ble of accurately retrieving relevant video segments fromthe Epic-Kitchen-100 dataset given a query in the form of atextual description of the action or activity. The evaluationmetric includes Mean Average Precision (mAP) and nor-malized Discounted Cumulative Gain (nDCG). More de-tailed information can be found in . Training: Following prior works , we train ourmodel for 50 epochs on the training set with a learning rateof 1e-5 and batch size of 8. We conduct warm-up trainingfor 1 epoch using the classic video-text contrastive loss. Themodel is trained on 8 A100 GPUs for 12 hours. Results.Tables 9 and10 present zero-shot and fine-tuned models performance on EK100 multi-instance re-trieval. Comparative analysis revealed significant advance-ments with our proposed method, surpassing state-of-the-art approaches in both mAP and nDCG scores. As shownin , the zero-shot performance of our stage 2 model(after post-training) reveals strong retrieval performance,compared with EgoVLP and LaViLA, indicating the strongperformance of our backbone model and the effectivenessof the multi-stage training strategy. Through task-specifictraining, our model achieves 63.3% and 73.2% averagemAP and nDCG, respectively, exhibiting substantial im-provements in both text-to-video and video-to-text retrievaltasks.This indicates superior performance in capturingfine-grained action semantics within the kitchen domain.Overall, the results underscore the effectiveness of our ap-proach in enhancing the understanding of daily human ac-tivities captured in egocentric views, highlighting its poten-tial for advancing research in activity recognition and videoretrieval domains.",
  ". Task8: Domain Adaptation for Action Recog-nition @ EPIC": "Task definition. Domain Adaptation is defined by utiliz-ing a labelled source domain to train an action recognitionmodel that is capable of adapting to an unlabelled target do-main. According to the data source , this task poses ad-ditional challenges due to the discrepancy in location, hard-ware, and long-term temporal offsets. The evaluation metric",
  "EgoVideo63.358.967.673.271.575.0": "includes Top-1/5 Accuracy.Training. Similar to the training setting of action recogni-tion, our approach differs in that we only train the model onthe source domain.Results. Tables 8 present models performance on EK100domain adaptation action recognition.Notably,ourmodel is only finetuned on the source domain, achiev-ing 61.3%/56.2%/43.2% Verb/Noun/Action top-1 perfor-mance that is much higher than the previous leading results58.2%/40.3%/30.1%. This highlights superior performanceimprovement brought by well-pretrained models.",
  ". Limitation and Conclusion": "Although our solution achieved good results in thecompetition, there are still some limitations worth noting.Firstly, we use a large video-language model and A100 asthe computing GPU during the training process, which re-quires expensive computing resources and results in highercarbon emissions. Secondly, we employ feature-based ap-proaches to solve the temporal localization problem, whichoften fails to obtain the optimal solution. Finally, we findthat in Long-Term Action Anticipation (LTA) tasks, trainingand prediction based on LLMs have high uncertainty, andthe final prediction performance may not be proportional tothe capability of the LLM itself.In conclusion, we have presented our solutions to 8tracks in the EgoVis CVPR2024 Challenge. We find a largervideo-language model can still give an advantage to egocen-tric task performance. This reveals that there is still ample",
  "room for exploration in egocentric video understanding": "Hamed Habibi Aghdam, Elnaz Jahani Heravi, and DomenecPuig. An unsupervised method for summarizing egocentricsport videos. In Eighth international conference on machinevision (ICMV 2015), volume 9875, pages 337341. SPIE,2015. 1 Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisser-man. Frozen in time: A joint video and image encoder forend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 17281738,2021. 1 Joao Carreira and Andrew Zisserman.Quo vadis, actionrecognition? a new model and the kinetics dataset. In pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition, pages 62996308, 2017. 1 Tejo Chalasani, Jan Ondrej, and Aljosa Smolic. Egocentricgesture recognition for head-mounted ar devices. In 2018IEEE international symposium on mixed and augmented re-ality adjunct (ISMAR-Adjunct), pages 109114. IEEE, 2018.1 Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Antonino Furnari, Evangelos Kazakos, Jian Ma, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.Rescaling egocentric vision: Collection, pipeline and chal-lenges for epic-kitchens-100. International Journal of Com-puter Vision, pages 123, 2022. 1, 2, 5, 6 Dima Damen, Teesid Leelasawassuk, Osian Haines, AndrewCalway, and Walterio W Mayol-Cuevas.You-do, i-learn:Discovering task relevant objects and their modes of interac-tion from multi-user egocentric video. In BMVC, volume 2,page 3. Citeseer, 2014. 1 Gerwin de Haan, Josef Scheuer, Raymond de Vries, andFrits H Post. Egocentric navigation for video surveillancein 3d virtual environments. In 2009 IEEE Symposium on 3DUser Interfaces, pages 103110. IEEE, 2009. 1",
  "Christoph Feichtenhofer. X3d: Expanding architectures forefficient video recognition. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition,pages 203213, 2020. 4": "Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens VanDer Maaten, Armand Joulin, and Ishan Misra. Omnivore: Asingle model for many visual modalities. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1610216112, 2022. 3 Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,Valentin Haenel, Ingo Fruend, Peter Yianilos, MoritzMueller-Freitag, et al.The something something videodatabase for learning and evaluating visual common sense.In Proceedings of the IEEE international conference on com-puter vision, pages 58425850, 2017. 1 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1899519012, 2022. 1, 2, 3,4 Kristen Grauman, Andrew Westbury, Lorenzo Torresani,Kris Kitani, Jitendra Malik, Triantafyllos Afouras, KumarAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote,et al.Ego-exo4d:Understanding skilled human activ-ity from first-and third-person perspectives. arXiv preprintarXiv:2311.18259, 2023. 1 Zhijian Hou, Lei Ji, Difei Gao, Wanjun Zhong, Kun Yan,Chao Li, Wing-Kwong Chan, Chong-Wah Ngo, Nan Duan,and Mike Zheng Shou. Groundnlq@ ego4d natural languagequeries challenge 2023. arXiv preprint arXiv:2306.15255,2023. 3, 4 Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.Lora: Low-rank adaptation of large language models. arXivpreprint arXiv:2106.09685, 2021. 4 Yifei Huang, Minjie Cai, Zhenqiang Li, and Yoichi Sato.Predicting gaze in egocentric video by learning task-dependent attention transition.In Proceedings of the Eu-ropean conference on computer vision (ECCV), pages 754769, 2018. 1 Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Li-jin Yang, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang,Limin Wang, et al. Egoexolearn: A dataset for bridging asyn-chronous ego-and exo-centric view of procedural activitiesin real world. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2207222086, 2024. 1, 2 Yifei Huang, Yusuke Sugano, and Yoichi Sato. Improvingaction segmentation via graph-based temporal reasoning. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1402414034, 2020. 1 Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,Chris Bamford, Devendra Singh Chaplot, Diego de lasCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-ple, Lucile Saulnier, et al.Mistral 7b.arXiv preprintarXiv:2310.06825, 2023. 5, 6",
  "Sanghwan Kim,Daoji Huang,Yongqin Xian,OtmarHilliges, Luc Van Gool, and Xi Wang. Lalm: Long-termaction anticipation with language models.arXiv preprintarXiv:2311.17944, 2023. 4": "Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.Mvbench: A comprehensive multi-modal video understand-ing benchmark. arXiv preprint arXiv:2311.17005, 2023. 2 Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, MichaelWray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wen-zhe Zhao, Weijie Kong, et al.Egocentric video-languagepretraining. Advances in Neural Information Processing Sys-tems, 35:75757586, 2022. 4, 6",
  "Howto100m: Learning a text-video embedding by watchinghundred million narrated video clips. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 26302640, 2019. 2": "Francesco Ragusa, Giovanni Maria Farinella, and AntoninoFurnari.Stillfast:An end-to-end approach for short-term object interaction anticipation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 36353644, 2023. 4 Santhosh Kumar Ramakrishnan, Ziad Al-Halah, and Kris-ten Grauman. Naq: Leveraging narrations as queries to su-pervise episodic memory. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 66946703, 2023. 3",
  "Jiayi Shao, Xiaohan Wang, Ruijie Quan, and Yi Yang. Ac-tion sensitivity learning for the ego4d episodic memory chal-lenge 2023. arXiv preprint arXiv:2306.09172, 2023. 3": "Yale Song, Eugene Byrne, Tushar Nagarajan, Huiyu Wang,Miguel Martin, and Lorenzo Torresani. Ego4d goal-step: To-ward hierarchical understanding of procedural activities. Ad-vances in Neural Information Processing Systems, 36, 2024.2 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288, 2023. 5, 6",
  "Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre-sani, and Manohar Paluri. Learning spatiotemporal featureswith 3d convolutional networks. In ICCV, pages 44894497,2015. 1": "Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, YaohuiWang, et al. Internvid: A large-scale video-text dataset formultimodal understanding and generation.arXiv preprintarXiv:2307.06942, 2023. 1, 2 Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, YinanHe, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, ZunWang, et al. Internvideo2: Scaling video foundation mod-els for multimodal video understanding.arXiv preprintarXiv:2403.15377, 2024. 1, 2",
  "Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang,Rui Feng, and Weidi Xie. Retrieval-augmented egocentricvideo captioning. arXiv preprint arXiv:2401.00789, 2024. 1": "Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, So-ham Ghosh, Yonghui Wu, and Jiahui Yu. Videococa: Video-text modeling with zero-shot transfer from contrastive cap-tioners. arXiv preprint arXiv:2212.04979, 2022. 1 Lijin Yang, Yifei Huang, Yusuke Sugano, and Yoichi Sato.Interact before align: Leveraging cross-modal knowledgefor domain adaptive action recognition. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1472214732, 2022. 1 Yu Yao, Mingze Xu, Chiho Choi, David J Crandall, Ella MAtkins, and Behzad Dariush.Egocentric vision-based fu-ture vehicle localization for intelligent driving assistance sys-tems. In 2019 International Conference on Robotics and Au-tomation (ICRA), pages 97119717. IEEE, 2019. 1 Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.Span-based localizing network for natural language video lo-calization. In Dan Jurafsky, Joyce Chai, Natalie Schluter,and Joel R. Tetreault, editors, Proceedings of the 58th An-nual Meeting of the Association for Computational Linguis-tics, ACL 2020, Online, July 5-10, 2020, pages 65436554.Association for Computational Linguistics, 2020. 3, 4 Qing Zhang, Giulia Barbareschi, Yifei Huang, Juling Li,Yun Suen Pai, Jamie Ward, and Kai Kunze. Seeing our blindspots: smart glasses-based simulation to increase design stu-dents awareness of visual impairment. In Proceedings ofthe 35th Annual ACM Symposium on User Interface Soft-ware and Technology, pages 114, 2022. 1 Qi Zhao, Ce Zhang, Shijie Wang, Changcheng Fu, NakulAgarwal, Kwonjoon Lee, and Chen Sun. Antgpt: Can largelanguage models help long-term action anticipation fromvideos? arXiv preprint arXiv:2307.16368, 2023. 4",
  "Yue Zhao and Philipp Krahenbuhl. Training a large videomodel on a single machine in a day.arXiv preprintarXiv:2309.16669, 2023. 5, 6": "Yunhan Zhao, Haoyu Ma, Shu Kong, and Charless Fowlkes.Instance tracking in 3d scenes from egocentric videos. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 2193321944, 2024. 1 Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and RohitGirdhar.Learning video representations from large lan-guage models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 65866597, 2023. 5, 6 Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, ZhuohanLi, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-lez, and Ion Stoica. Judging llm-as-a-judge with mt-benchand chatbot arena, 2023. 4, 5, 6"
}