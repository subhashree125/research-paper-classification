{
  "Abstract": "This technical report summarizes the second-place so-lution for the Predictive World Model Challenge held atthe CVPR-2024 Workshop on Foundation Models for Au-tonomous Systems. We introduce D2-World, a novel Worldmodel that effectively forecasts future point clouds throughDecoupled Dynamic flow. Specifically, the past semanticoccupancies are obtained via existing occupancy networks(e.g., BEVDet). Following this, the occupancy results serveas the input for a single-stage world model, generating fu-ture occupancy in a non-autoregressive manner. To furthersimplify the task, dynamic voxel decoupling is performed inthe world model. The model generates future dynamic vox-els by warping the existing observations through voxel flow,while remained static voxels can be easily obtained throughpose transformation. As a result, our approach achievesstate-of-the-art performance on the OpenScene PredictiveWorld Model benchmark, securing second place, and trainsmore than 300% faster than the baseline model. Code isavailable at",
  ". Introduction": "The predictive world model aims to forecast future statesusing past observations, playing a crucial role in achiev-ing end-to-end driving systems. In the CVPR 2024 Pre-dictive World Model Challenge, participants are required touse past image inputs to predict the point cloud of futureframes. This challenge presents two main difficulties: Thefirst is how to effectively train on large-scale data. Giventhat the OpenScene dataset contains 0.6 million frames,the designed model must be efficient. The second challengeis how to predict faithful point clouds through sore visualinputs. To address these issues, we designed a novel so-lution that extends beyond the baseline model. Regardingthe Problem I, we found that the official baseline model",
  "*Project Lead": "(i.e., ViDAR ) requires very long training times becauseit uses all historical frames to predict all future frames inan autoregressive manner.To address this, we designeda solution that divides the entire training process into twoparts. The first part trains an occupancy prediction modelfor single-frame prediction, while the second part uses pastoccupancy data to predict future point clouds. Specifically,in the first stage, we utilize an existing occupancy network,such as BEVDet , which predicts semantic occupancyby encoding both occupancy states and semantic informa-tion within a 3D volume. In the second stage, a generativeworld model takes the past occupancy results as input andgenerates the future occupancy states, which are then ren-dered into point clouds via differentiable volume rendering.Through this training paradigm, we increased the trainingspeed by 200%.Given the significant development of occupancy net-works in the autonomous driving community recently , for the aforementioned Problem II, we focus on howto construct a world model that maps past occupancy re-sults to future ones. Our framework leverages the advan-tages and potential of single-stage video prediction , en-abling the prediction of multiple future volumes in a non-autoregressive manner. Moreover, we found that directlypredicting the occupancy of each frame results in unsatis-factory performance due to the majority of the voxels beingempty. To address this issue, we use the semantic informa-tion predicted by the occupancy network to decouple vox-els into dynamic and static categories. The world modelthen only predicts the voxel flow of dynamic objects andwarps these voxels accordingly. For static objects, sincetheir global positions remain unchanged, we can easily ob-tain them through pose transformation. By leveraging theabove components, D2-World surpasses the baseline modelby a large margin, achieving a chamfer distance of 0.79 witha single model and securing 2nd place in this challenge.",
  "Feature from current frame": ". The overall pipeline of D2-World. In the first stage, we train a single-frame occupancy network, and in the second stage, wetrain a world model that takes past occupancy as input, forecasting future point clouds. ages with T timestamps, the first stage predicts occupancyframe-by-frame, aiming to recover a rich 3D dense repre-sentation from the 2D images. In the second stage, we ap-proach this as a 4D point cloud forecasting task. Insteadof forecasting the future point cloud in an inefficient au-toregressive manner like ViDAR , we design a noveland versatile 4D point cloud forecasting framework that op-erates in a non-autoregressive manner with decoupled dy-namic flow.",
  ". Stage I: Vision-based Occupancy Prediction": "In this section, we introduce the architecture of the occu-pancy network, which takes visual images as input and pre-dicts the occupancy state and semantics for a single frame.Image Encoder. The image encoder is designed to encodethe input multi-camera 2D images into high-level features.Our image encoder comprises a backbone for high-levelfeature extraction and a neck for multi-resolution featureaggregation. By default, we use the classical ImageNet pre-trained ResNet-50 as the backbone in ablation studies, andSwin-Transformer-B as the backbone for submission.Although employing a stronger image backbone can en-hance prediction performance, we considered the trade-offsbetween resource usage and training time, and ultimatelydecided against using huge backbones such as InternImage-XL .View Transformation. We utilize LSS for view trans-formation, which densely predicts the depth of each pixelthrough a classification method, allowing us to project theimage features into 3D space. Moreover, to introduce thetemporal information in our model, we adopt the techniqueproposed in , dynamically warping and fusing one his-torical volume feature to produce a fused feature.Ocupancy Head. We adopt the semantic scene comple-tion module proposed in as our occupancy head, whichcontains several 3D convolutional blocks to learn a local ge- ometric representation. The features from different blocksare concatenated to aggregate information. Finally, a linearprojection is utilized to map the features into C0 dimen-sions, where C0 is the number of classes.Losses. To alleviate the class-imbalance issue in occupancyprediction, we utilize class-weighted cross-entropy and Lo-vasz losses. Our multi-task training losses are a combina-tion of occupancy prediction loss and depth loss.",
  ". Stage II: 4D Occupancy Forecasting": "In this section, we introduce the process of future pointcloud forecasting. The framework consists of an occupancyencoder, a flow decoder, flow guided warping and refine,and a rendering process.Initially, the 3D occupancy data is preprocessed intospacetime tokens. The spatial-temporal transformer effec-tively captures the spatial structures and local spatiotem-poral dependencies within these tokens. Following the en-coding of historical tokens, the flow decoder is employedto predict future flow in each voxel grid. Then, warpingand refinement generate the final occupancy density. Tofully leverage the temporal information across the entiresequence, we utilize a non-autoregressive approach for de-coding, which achieves impressive forecasting performancealongside high efficiency. Finally, a differentiable volumerendering process is used to generate the point cloud fromthe predicted occupancy.3D Occupancy Encoding. Given a sequence of historicallyobserved Nh frames 3D occupancy OT Nh:T , where eachoccupancy Oi RH0W0D0, we first encode the occu-pancy sequence into spacetime tokens. Here, H0, W0 andD0 represent the resolution of the surrounding space cen-tered on the ego car. Each voxel is assigned as one of C0classes, denoting whether it is occupied and which semanticcategory it is occupied with.To reduce the computational burden, we transform the",
  "Static:": ". Inner structure of SALT & warping and refinement.(a) The detailed structures of SALT, which replace the MLP andFFN (Feed Forward Network) in vanilla transformer with 2D con-volutions and 3D convolutions respectively for capturing spatial-temporal dependencies. (b) We decouple the flow with the dy-namic and static flow and warp the feature of the current frame forforecasting the future frame. The refinement module refines thecoarse warping features. 3D occupancy in the BEV representation. Take a single-frame occupancy as an example, it first uses a learnableclass embedding to map the 3D occupancy into occupancyembedding y RH0W0D0C. Then, it reshapes the 3Doccupancy embedding along the height dimension to ob-tain a BEV representation y RH0W0DC. The BEVembedding then is decomposed into non-overlapping 2Dpatches yp RHW C, where H = H0/P, W = W0/P,C = P 2 C0, and P is the resolution of each image patch.After that, a lightweight encoder composed of several 2Dconvolution layers, i.e., Conv2d-GroupNorm-SiLU, is fol-lowed to extract the patch embeddings. After consideringthe sequence of patch embeddings, we obtain the historicaloccupancy spacetime tokens y RNhHW C.Ego Pose Encoding. We represent the ego pose as relativedisplacements between adjacent frames in the 2D groundplane. Given the historical ego poses, we employ multiplelinear layers followed by a ReLU activation function to ob-tain the ego tokense RNhC.Spatial-Temporal Transformer.The spatial-temporaltransformer jointly models the evolution of the surround-ing scene and plans the future trajectory of the ego vehicle.Inspired by previous works on video prediction ,we incorporate several spatial-aware local-temporal (SALT)attention blocks within the Spatial-Temporal Transformer.As shown in (a), in each SALT block, 2D convolutionlayers are first utilized to generate the query map and pairedkey-value embeddings for the spacetime tokens, effectivelypreserving structural information through this spatial-awareCNN operation. Subsequently, the standard multi-head at- tention mechanism is employed to capture the temporal cor-relations between tokens.This approach allows for thelearning of temporal correlations while preserving the spa-tial information of the sequence. Furthermore, we replacethe traditional feed-forward network (FFN) layer with a 3Dconvolutional neural network (3DCNN) to introduce localtemporal clues for enhanced sequential modeling.Decoupled Dynamic Flow. As illustrated in and(b), we design a decoupled dynamic flow to sim-plify the occupancy forecasting problem. Specifically, theflow decoderwhich comprises multiple stacked SALTblocksprocesses the encoded historical BEV features andforecasts the absolute future flows with respect to the cur-rent ego coordinate. Utilizing the occupancy semantics, wedecouple the dynamic and static grids, forecasting the fu-ture voxel features via the warping operation. For the dy-namic voxels, we transform the absolute flow for each fu-ture timestamp using the future ego poses, ensuring align-ment with the current frame. For the static ones, we directlytransform them through future ego poses. Finally, we applya refinement module composed of several simple CNNs toenhance the coarse warped features.Rendering & Losses. We utilize the same rendering pro-cess and losses as ViDAR for optimizing the pointcloud forecasting, which is a ray-wise cross-entropy lossto maximize the response of points along its correspondingray. For pose regression, we use L1 loss during the training.",
  ". Experimental Setups": "Dataset. We conduct our experiments on the OpenScenedataset , which is derived from the nuPlan dataset .Due to some scenes in OpenScene lacking correspondingoccupancy labels, we ignore these scenes during our ex-periments. For submission, the challenge utilizes an onlineserver that provides historical images along with normal-ized ray directions for point forecasting.Metric.For this challenge, model evaluation is con-ducted using the Chamfer Distance (CD) . The Cham-fer Distance quantifies the similarity between predictedand ground-truth point clouds by computing the averagenearest-neighbor distance from points in one set to thosein the other set, in both directions.Training Strategies.During the training process, bothstages are trained with AdamW optimizer with gradientclipping and a cyclic learning rate policy. The initial learn-ing rates a 2e4 and 1e3 for stage I and stage II, respec-tively. In stage I, we utilize a total batch size of 24, dis-tributed across 24 NVIDIA V100 GPUs. In stage II, thetotal batch size is reduced to 16, leveraging 16 NVIDIAV100 GPUs. For the ablation studies, stage II is trained us-ing 8 NVIDIA V100 GPUs with a total batch size of 8. Both",
  ". Training efficiency comparisons. All experiments aretrained in 8 GPUs with 24 epochs on 1/8 mini training set. indi-cates the efficient version of ViDAR with inferior performance": "stages are trained for 24 epochs.Network Details. For stage I, the input image resolution is5121408 incorporating common data augmentation tech-niques such as flipping and rotation, applied to both the im-ages and the 3D space. The resolution of the generated 3Dvoxel grid is 200 200 16. Prior to feeding the predictedoccupancy into stage II, we apply grid sampling operationsto align the occupancy annotations from the range of [-50m,-50m, -4m, 50m, 50m, 4m] to the LiDAR point cloud rangeof [-51.2m, -51.2m, -5.0m, 51.2m, 51.2m, 3.0m].",
  ". Quantitative Results": "Main Results & Ablation Study. The main results are pre-sented in Tab. 1. In addition to showing the overall per-formance of our model (D2-World), we also demonstratethe performance of our model without decoupled dynamicflow (D2-World vanilla).Our method demonstrates su-perior performance across all timestamps when comparedto the baseline model, with further performance enhance-ments observed upon the introduction of the decoupled dy-namic flow. Our best submission ranks 2nd on the leader-board, achieving a Chamfer Distance (CD) of 0.79, withboth stages trained on the full dataset.Training Efficiency. To further validate the efficiency ofour approach, we compare training hours and GPU memoryusage across different models, as shown in Tab. 2. The base-line method, ViDAR, requires up to 63 GB of GPU memoryand 23.50 hours for training. Even its efficient version ,which does not supervise all future frames, still demandshigh GPU memory (38 GB) and considerable training time(18.5 hours).In contrast, although our method necessi-tates pre-training an occupancy prediction model, our world",
  ". Results analysis. The effects of occupancy predictionperformance": "model can be trained in approximately 3 hours with only28 GB of GPU memory under the same conditions. Addi-tionally, our model, even with the decoupled dynamic flow,maintains reasonable training hours and GPU memory.The Effects of Occupancy Performance.The resultsusing different occupancy performances are presented inTab. 3, where only 1/8 mini dataset are used to train. Wefirst train our world model with binary occupancy predic-tion (empty and occupied) as inputs. The results from Ver-sion A to Version E denote the performance of the worldmodel when the occupancy performance changes. We findthat the world model performs better when the occupancyperformance is improved.Furthermore, introducing decoupled dynamic flow withsemantic occupancy inputs yields additional performanceenhancements, as shown in Versions F to H. Interestingly,the performance does not significantly improve even whenground truth occupancy with 100% mIoU and IoU is usedas input. Our analysis indicates that this is due to the inher-ently sparse nature of point cloud forecasting, which pri-marily requires predicting the foremost visible surfaces ofobjects in the 3D space, whereas IoU evaluation for occu-pancy encompasses the entire dense space.",
  ". Conclusion": "In this report, we present our 2nd solution (D2-World) forthe Predictive World Model Challenge held in conjunctionwith the CVPR 2024 workshop. By reformulating the visualpoint cloud forecasting predictive world model into vision-based occupancy prediction and 4D point cloud forecastingvia decoupled dynamic flow, our solution demonstrates ex-emplary forecasting performance and significant potential. Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye KitFong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom,and Sammy Omari. nuplan: A closed-loop ml-based plan-ning benchmark for autonomous vehicles.arXiv preprintarXiv:2106.11810, 2021. 3",
  "Tarasha Khurana, Peiyun Hu, David Held, and Deva Ra-manan. Point Cloud Forecasting as a Proxy for 4D Occu-pancy Forecasting. In CVPR, 2023. 3": "Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun,and Zeming Li.Bevstereo: Enhancing depth estimationin multi-view 3d object detection with dynamic temporalstereo. arXiv preprint arXiv:2209.10248, 2022. 2 Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:Learning birds-eye-view representation from multi-cameraimages via spatiotemporal transformers. In European con-ference on computer vision, pages 118. Springer, 2022. 1 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 2 Shuliang Ning, Mengcheng Lan, Yanran Li, Chaofeng Chen,Qian Chen, Xunlai Chen, Xiaoguang Han, and ShuguangCui. Mimo is all you need : A strong multi-in-multi-out base-line for video prediction. arXiv preprint arXiv: 2212.04655,2022. 1, 3",
  "Ofir Press, Noah A Smith, and Mike Lewis.Train short,test long: Attention with linear biases enables input lengthextrapolation. arXiv preprint arXiv:2108.12409, 2021. 3": "Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,Hongsheng Li, et al. Internimage: Exploring large-scale vi-sion foundation models with deformable convolutions. arXivpreprint arXiv:2211.05778, 2022. 2 Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, RuiHuang, and Shuguang Cui. Sparse single sweep lidar pointcloud segmentation via learning contextual shape priors fromscene completion. In Proceedings of the AAAI Conference onArtificial Intelligence, pages 31013109, 2021. 2"
}