{
  "Abstract": "Recent advances in large pre-trained vision-languagemodels have demonstrated remarkable performance on zero-shot downstream tasks. Building upon this, recent studies,such as CoOp and CoCoOp, have proposed the use of promptlearning, where context within a prompt is replaced withlearnable vectors, leading to significant improvements overmanually crafted prompts. However, the performance im-provement for unseen classes is still marginal, and to tacklethis problem, data augmentation has been frequently usedin traditional zero-shot learning techniques. Through ourexperiments, we have identified important issues in CoOpand CoCoOp: the context learned through traditional im-age augmentation is biased toward seen classes, negativelyimpacting generalization to unseen classes. To address thisproblem, we propose adversarial token embedding to disen-tangle low-level visual augmentation features from high-level class information when inducing bias in learnableprompts. Through our novel mechanism called AddingAttributes to Prompt Learning, AAPL, we guide the learn-able context to effectively extract text features by focusing onhigh-level features for unseen classes. We have conductedexperiments across 11 datasets, and overall, AAPL showsfavorable performances compared to the existing methods infew-shot learning, zero-shot learning, cross-dataset, anddomain generalization tasks.The code is available at:",
  ". Introduction": "Recent research has shown significant improvements notonly in model generalization performance through the useof large-scale vision-language models (VLMs), but alsoin zero-shot image classification performance .It has been demonstrated that utilizing VLMssuch as contrastive language-image pretraining (CLIP) ,ALIGN , Flamingo , etc., is effective in extractingimage and text information for training classification models.",
  "Text": "features ? . The illustration of AAPL. Training the learnable prompton the class apple, since the training data mainly consists ofred apples, leads to understanding apples as typically red. Whena rare yellow apple is input, the instance bias may overlookthe yellow attribute and incorrectly predict it as a pear. However,AAPL extracts and decomposes attributes from the image, enhanc-ing attribute-specific bias in the semantic features. This enablesrobustly improved generalization performance across domains. The strengths of these VLMs have proven to be effectivein prompt learning and handling both visual and textualinformation efficiently . CoOp and Co-CoOp have effectively produced learnable context vec-tors for classification weights via a text encoder (e.g., Trans-former ) along with CLIP. Specifically, CoCoOp hasenabled the creation of class-specific classification weightsby incorporating additional context information generatedfrom images. In addition, visual prompt tuning (VPT) demonstrated performance improvements in downstreamtasks by introducing a small number of learnable parame-ters into the encoder layer of the Transformer along withimage patches, without the need to replace or fine-tune thepre-trained transformer.However, both CoOp and VPT have learnableparameters that are not manageable, especially in the case ofCoCoOp , where it is unknown how the learnable vectorwill be shifted by the conditional bias based on particularinformation taken from the image that is added to the learn-",
  "arXiv:2404.16804v1 [cs.CV] 25 Apr 2024": "able context vector. This lack of management over learnableparameters can lead to unintentional bias in few-shot clas-sification tasks or domain generalization tasks .To address this, we propose a new approach called AAPL,Adding Attributes to Prompt Learning, as illustrated in. In this context, augmentation generates a learnablebias that can be decomposed, with the augmented imageserving as the visual prompt. Subsequent learning withvisual-text prompts involves the use of a learnable contextvector, which plays an adversarial role and mitigates un-intended overfitting in downstream tasks . Insummary, our contributions are as follows:",
  ". Related Works": "Vision-language modelsVision-language models (VLMs)using image-text pairs have shown superior capabilities overimage-only models, especially in zero-shot transfer tasks forvarious downstream classification tasks .Prominent models such as CLIP and ALIGN ,which have advanced through large-scale web data uti-lization, employ self-supervised learning for enhancedtextual and visual alignment. In the embedding space, thecontrastive loss draws matched image-text representationpairs closer, while it draws the representation of mismatchedpairs farther away. Using this method, CLIP demonstratesexceptional zero-shot image recognition capabilities withoutthe need for further fine-tuning. Our goal is to find efficientmethods for applying pre-trained vision-language models todownstream applications, especially in prompt learning likeCoOp and CoCoOp . Prompt learning in vision-language modelsThe conceptof prompt learning was initially proposed in the domain ofnatural language processing (NLP) . Unlike manu-ally designing prompts, prompt learning research focuses onautomatically selecting prompts during the fine-tuning stage.Recently, this concept has been extended to the field ofcomputer vision . CoOp introduced continuous prompt learning to the vision domain,applying pre-trained vision-language models to various tasks. Instead of using a manual prompt like a photo of a,they transformed the context word into a learnable contextvector to optimize continuous prompts. However, CoOp haslimitations in generalizability due to overfitting on few-shotdatasets. To address this, CoCoOp adds a conditionalbias called meta token extracted from image features to thelearnable prompt. It shifts the focus from static to dynamicprompts, enabling optimization based on the characteristicsof each instance rather than a specific class, consequentlyenhancing CoOps domain generalization performance.However, meta token obtained from an image sample cannotbe claimed to be completely robust against overfittingissues , and it is not interpretable because it isextracted from the shallow network, called metanet, com-posed of Linear-ReLU-Linear layers. Therefore, we proposea new prompt learning method using image augmentationto leverage attribute-specific bias added to learnable prompts. Zero-shot learningFew-shot learning is the process oftraining on a small number of labeled samples before classi-fying the new images. In contrast, zero-shot learning (ZSL)aims to distinguish unseen classes by training exclusively onseen classes . This is achieved by exclusively trainingon a set of base classes and utilizing side information, typi-cally visual attributes like color, shape, and other features,shared with these unseen classes. This auxiliary informa-tion helps the machine understand language or concepts ina way humans do, enabling it to recognize unseen classes.The common methods are learning therelation between a class embedding and the image feature,which represents this auxiliary information. However, thesemethods often exhibit a bias against unseen classes, knownas seen-class bias . Other research efforts concentrateon enhancing visual-semantic embedding , or de-veloping better image feature extractors . However,these methods usually assume a fixed set of auxiliary infor-mation, consisting of attributes labeled by humans. Thisassumption poses challenges, as labeling attributes is expen-sive, requires expert annotators, and is difficult to scale onlarge datasets. Diverging from existing ZSL methods, ourwork focuses on adapting large vision-language models andemploys techniques based on prompting.",
  ". Preliminaries": "Prompt learning for CLIPCLIP employs an imageencoder based on ResNet or ViT and a text encoderbased on Transformer to extract features from imagesand text, respectively. These features are trained with acontrastive loss in the embedding space, aiming to maximizecosine similarities between paired modality features. Whenan input image x is processed through the image encoder AdTriplet loss : tunable : frozen : subtract : pull : push : contrastive learning",
  "Encoder": ". Overview of AAPL. We apply two distinct random augmentations to the input images, each with the class labels 1 and 2. Oncethe image features are extracted from the pre-trained CLIP image encoder , they are passed through the metanet to acquire themeta token. These are then utilized to subtract the other meta tokens obtained from the augmented images for each class, resulting in deltameta tokens. The goal is to instruct them to use these delta meta tokens regardless of their classification. The delta meta tokens, which areassociated with the same augmentation, approach close within the embedding space using the AdTriplet loss, as shown in Eq. 5. The deltameta tokens acquire attribute-specific features, while the meta token learns semantic features derived from image features, enabling the useof attribute-specific bias in the learnable prompt through the decomposed features. f(), it generates an image feature f(x). Using a prompttemplate like a photo of a {class}., where the {class}token is substituted with the name of the i-th class, yields Ktext features with corresponding weight vectors {wi}Ki=1 forthe given K class categories. The prediction probability forCoOp is as Eq. 1, where sim(,) denotes cosine similarityand is a temperature parameter.",
  "Ki=1 exp(sim( f(x),wi)/)(1)": "Conditional context optimization in prompt learningCoOp introduces context tokens as trainable vectors,M learnable context, {v1,v2,...,vM}, departing from a fixedtemplate like a photo of a. The i-th class prompt, ti ={v1,v2,...,vM,ci}, includes these vectors and word embed-dings of the class name, ci. Text features are generated fromti by CLIP text encoder g(), which remained frozen through-out training. CoCoOp proposes instance-conditionalcontext to prioritize individual input instances, reducingthe overfitting of CoOp. This is done by using a metanet,denoted as h() parameterized by , to generate a con-ditional token for each input. Where = h( f(x)) andm {1,2,...,M}, each context token is obtained by vm(x) =vm + . The prompt of the i-th class is conditioned on theinput image feature, i.e., ti(x) = {v1(x),v2(x),...,vM(x),ci}.Jointly updating context vectors {vm(x)}Mm=1 and metanet",
  ". Delta Meta Token": "Effect of augmentation in CoCoOpTo investigate theeffect of augmentation in prompt learning, we conducteda comparative experiment by adapting augmentation intoCoCoOp . We added conditional bias from augmentedimages to the learnable prompt while maintaining other set-tings consistent with CoCoOp. As detailed in , in-corporating augmentation leads to a decrease in base-to-newgeneralization accuracy compared to the original CoCoOpsince the metanet fails to extract the semantic features fromthe augmented images; thus extracting arbitrary noise ratherthan attribute-specific semantics. Additionally, as shown in, it does not show a big difference in class clustering,indicating that the meta token fails to capture the crucialsemantic features for the classification. Consequently, thissuggests that merely using augmentation in prompt learningmight not enhance robustness or performance. It potentiallyleads to detrimental effects due to the metanets inability toidentify meaningful semantic features from the augmented",
  "(b) of CoCoOp with augmentation": "Class 0Class 1Class 2Class 3Class 4Class 5Class 6Class 7Class 8Class 9Class 10Class 11Class 12Class 13Class 14Class 15Class 16Class 17Class 18Class 19Class 20Class 21Class 22Class 23Class 24 Class 0Class 1Class 2Class 3Class 4Class 5Class 6Class 7Class 8Class 9Class 10Class 11Class 12Class 13Class 14Class 15Class 16Class 17Class 18Class 19Class 20Class 21Class 22Class 23Class 24",
  ". The comparison of base-to-new generalization accuracybetween AAPL and CoCoOp with augmentation. HM denotesharmonic mean score": "images, focusing on instance-specific features rather thanclass semantics. To achieve optimal results, augmentationneeds to be applied more carefully, ensuring that the condi-tional biases appropriately capture the semantic informationof the class. Delta meta token: detach attribute featureCoCoOp improves the generalization performance of CoOp byintroducing metanet, which outputs meta token from imagesamples, then adds it to the learnable prompt. It focuses onlearning about individual instance information rather thanclass information. However, its still unclear what infor-mation the meta token contains, as the metanet is a blackbox, and its shallow architecture leads to uncertain featureextraction. As shown in , it fails to demonstrate clearclustering by neither augmentation type nor class. It showsthat the meta token does not effectively capture the semanticinformation of the class or the attribute of the input imagesample. To address this issue and make it possible to adddesired information to the learnable prompt, we propose theconcept of a delta meta token, the attribute-specific bias. Theoverview of AAPL is shown in .To make a delta meta token, two images of each of thetwo different classes are required, e.g., class 1 and class 2,as shown in . Two different augmentation types arerandomly selected from 14 augmentations proposed in Sim-CLR for each pair of input images without any dupli-cation, which is denoted as AugA() and AugB(). Inspiredby TextManiA , which demonstrated the extraction ofattribute information from text using Word Vector Anal-ogy , we generate delta meta token by subtractingimage features in the same class with different augmentation. Delta meta token represents a difference vector from imagefeatures that contain augmentation information. They aregenerated at each iteration. The delta meta token from animage x of class 1 and AugA() can be written as follows:",
  "A = h( f(AugA(x1)))h( f(x1)).(3)": "As TextManiA has shown, utilizing attributes con-taining semantic details derived from class informationdemonstrates its effectiveness in classification tasks. Inother words, while the meta token includes both classand attribute information, the delta meta token preservesmore specific image feature information associated withaugmentation.Adding decomposed auxiliary featuresto the learnable prompts, the delta meta token can learnattribute information. We enable the learnable prompt toincorporate semantic features more abundantly, thus makingthe augmentation more effective. Similar to adversarialprompt learning for natural language processing ,our method involves the adversarial interaction betweenclass and attribute information, where the metanet learns toextract attribute-related information from augmented imagefeatures. The more the learnable prompt learns the semanticfeature information of the class, the better the classificationperformance. Does the delta meta token have exact augmentation infor-mation? In , we used t-SNE to compare the validationresults of metanet of both CoCoOp and AAPL. It showsthat CoCoOp fails to distinguish between augmentationscompared to AAPL. As comparing (c) and (d), whilemeta token cannot perfectly discriminate 14 augmentations,delta meta token shows almost perfect distinction, except fora few augmentations, e.g., vertical flip and rotations. Thisclustering result shows that the delta meta token extractsmore specific information about augmentation than the metatoken. As demonstrated in TextManiA , for the textualcase, subtraction between features can retain specific fea-tures. In the case of the image, we show that delta meta tokenis more effective in making it contain the exact augmentationinformation. To the best of our knowledge, we are the firstto employ feature decomposition through subtraction usingvisual features for prompt learning. It is noteworthy that,while the meta token still retains information about the class,the delta meta token accurately distinguishes between thesemantic feature and the attribute feature.",
  "(d) of AAPL": "color_jitter_redcolor_jitter_greencolor_jitter_bluegrayscalegaussian_noisegaussian_blurhorizontal_flipvertical_fliprotation_90rotation_180rotation_270cropcutoutsobel_filter . t-SNE visualization of meta token and delta meta token of CoCoOp and AAPL for FGVCAircraft dataset. The colors of thepoints represent the 14 different augmentations, and 100 data points from the validation set are used for this. (a) and (c) are the visualizationof meta token, (b) and (d) are the visualization of delta meta token. PULL PUSH Constraints-4",
  ". Comparison of the number of constraints of the AdTripletloss. The constraints-2 settings anchor is just one, e.g., 1B, andthe constraints-4 setting has two anchors, e.g., 1A and 2B": "minimizing it for the same augmentation. For instance, con-sidering anchor as 1A, its positive pair is 2A, whichhas a different class but the same augmentation. In contrast,1B is considered a negative pair because it has the sameclass but a different augmentation. The distance betweenthe anchor and the negative pair should be greater than thedistance between the anchor and the positive pair. The Eu-clidean distance is denoted as 2, and the margin of thetriplet loss is denoted as m in Eq. 4.",
  "= max(0, 1A 2A2 1A 1B2 +m)(4)": "Thus, we introduce the Adtriplet loss, which adversar-ially trains the model to prioritize the alignment of aug-mentation information over class information. This loss isupdated alongside the classification loss, specifically thecross-entropy loss. The AdTriplet loss is used as constraints-4, as illustrated in , to make the connection between theclass information domain and augmentation attribute domainmore balanced .",
  ". Experimental Settings": "DatasetsWe use 11 classification datasets based onCLIP , CoOp , and CoCoOp for base-to-newgeneralization and cross-dataset transfer: ImageNet andCaltech101 for generic object classification, Oxford-Pets , StanfordCars , Flowers102 , Food101 and FGVCAircraft for fine-grained image recognition,EuroSAT for satellite image classification, UCF101 for action classification, DTD for texture classification,and SUN397 for scene recognition. For domain gen-eralization experiments, we use ImageNet as the sourcedataset and 4 other ImageNet-based datasets, i.e., Ima-geNetV2 , ImageNetSketch , ImageNet-A , andImageNet-R , as the target datasets, which each containa different kind of domain shift.Baselines We compare AAPL with 3 baseline methods: thezero-shot CLIP , CoOp , and CoCoOp . CLIPuses the hand-crafted template a photo of a {class} togenerate the prompts for knowledge transfer. CoOp learnsa static prompt that replaces the hand-crafted prompts withthe learnable vectors. CoCoOp generates dynamic promptsby adding the image-conditional prompts to the learnableprompts in CoOp.Training detailsOur implementation is based on Co-CoOp . We employ the pre-trained ViT-B/16 modelfrom CLIP as the backbone. We fix the context lengthto 4 and initialize the context vectors randomly. The pre-sented results are the mean values obtained from experiments conducted with three random seeds. We follow the trainingepochs, batch sizes, and schedules as prescribed by CoCoOp.In the context of few-shot learning, we confine evaluationto the maximum shot, i.e., 16 shots, considered by CoOp.For evaluation, we use the model from the last epoch. Theparameter size of AAPL is the same as CoCoOp, and thehyper-parameter m in Eq. 4 is set to 0.2.",
  "We divided the classes equally into two groups, one for thebase classes and another for the new classes, i.e., unseen": "classes, just like in CoCoOp . Learning-based mod-els are trained solely on base classes. In few-shot learning,the model is evaluated with the base classes, whereas inzero-shot learning, it is evaluated with the new classes totest the models generalizability. In this task, we set hyper-parameters and to 0.2 and 1. presents the per-formance results of AAPL compared to the baseline. AAPLoutperformed in 7 out of 11 datasets, with the harmonicmean of total dataset accuracy exceeding that of CoCoOp.However, performance on the DTD was significantlylower. The geometrical augmentations, especially flips androtations, appear to have minimal effect on AAPL, as theydo not significantly alter the appearance of the original im-ages in the context of texture. This demonstrates that theeffectiveness of AAPL varies across different datasets.",
  ". Cross-Dataset Transfer": "To assess the robustness and adaptability of AAPL, we testedits generalization ability across datasets by training it on all1000 ImageNet classes and then applying it on the other 10datasets, as shown in . We assume that the model canlearn semantic information about image features by learningprecise attributes. To evaluate this, we increased the modelsfocus on learning augmentation information by setting bothhyper-parameters, and , to 1 in this experiment andafterward. AAPL achieves higher generalization in 3 datasets:OxfordPets , FGVCAircraft , and UCF101 ,compared to CoCoOp . However, the performance onDTD and EuroSAT was noticeably poorer than otherdatasets. This suggests that these datasets are vulnerable toAAPLs augmentation-based prompt learning. These datasetsare not object-centric but rather possess global features, e.g.,long-distance satellite images and texture images. Extractingspecific attributes from these datasets is challenging due totheir unique characteristics.",
  ". Augmentation Profiling": "Why should the delta meta token learn about attributesrather than class information?To assess the effec-tiveness of learning attributes, we compared the silhouettescores based on augmentation types. The silhouettescore evaluates how well data points are clustered, consider-ing both cohesion (proximity within the same cluster) andseparation (distance from the nearest neighboring cluster).The silhouette score S(i) for data point i, is calculatedas follows: S(i) =b(i)a(i) max{a(i),b(i)}, where a(i) is the averagedistance of i to all other data points in the same cluster, andb(i) is the average distance of i to the data points in thenearest cluster that i does not belong to. A higher silhouettescore indicates better clustering. In other words, datasetsthat effectively learn information about augmentations fromthe Adtriplet loss have higher silhouette scores. As shownin , the zero-shot classification performance of AAPLgenerally improves. However, there is a sharp decrease inperformance for DTD and EuroSAT . This suggeststhat datasets that cannot effectively extract augmentationinformation do not perform well. Training precise attributesto delta meta token is crucial for zero-shot classification, andits evident that determining what information to add to thelearnable prompt is highly important for datasets sensitive toAAPL. Which dataset is vulnerable for AAPL?To assess theimpact of various datasets on the evaluation of learningattribute features, we applied AAPLs proposed AdTripletloss and the traditional triplet loss method.Unlike theAdTriplet loss, the traditional triplet loss trains the deltameta token to cluster classes rather than augmentationtypes.As shown in , when utilizing Adtripletloss across 6 datasets, performance improvement wasobserved compared to using triplet loss.Particularly,FGVCAircraft exhibited approximately a 7% higher 0.03 -4.54-7.06 6.53 0.11 0.380.01-0.31-0.67 0.46 -8.00 -6.00 -4.00 -2.00 0.00 2.00 4.00 6.00 8.00 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Silhouette scoreH mean delta",
  ". AAPL with Triplet and AdTriplet loss. The comparisonof harmonic means of base-to-new generalization accuracy betweenAAPL trained with AdTriplet loss and traditional Triplet loss": "performance improvement with the triplet loss. Utilizingthe traditional triplet loss method means that the delta metatoken is trained to bring the same class together regardlessof augmentation type. Consequently, datasets that showedimproved performance on AdTriplet loss have a higherdependency on class information. Triplet loss-trained deltameta token extracts class-related information, causing metatoken to contribute noisy features rather than class semanticfeatures when added to prompts. In contrast, AdTripletloss-trained tokens focus on extracting the class semanticfeatures. Datasets with AdTriplet loss perform well becausethey rely more on the class information. This highlightsthe advantage of AAPL based on the datasets characteristics. Which augmentation is effective to prompt learning?The t-SNE visualization of the delta meta token for 14augmentations is shown, along with their silhouette scores,in (a). It turned out that it is difficult to distinguishrotations from flips and between color jitters, while otheraugmentations are obvious. All datasets exhibit difficultyin distinguishing these augmentations. Following selectiveaugmentation training, when trained only on augmentationswhose results are good (shown in (b)), clustering isgreatly enhanced, and silhouette scores are also raised. Also,the average performance for base-to-new generalizationimproved, as seen in . But when training solelywith the opposite, i.e., bad augs ( (c)), there is neithersignificant improvement in silhouette scores nor in the",
  ". AAPL with weighted random sampling for vulnerable 4datasets. The comparison of harmonic means of base-to-new gen-eralization accuracy. WRS is short for weighted random sampledAAPL": "Cars , and SUN397 , which have insufficient learn-ing of augmentation type information. For training, silhou-ette scores were used as thresholds for random samplingweights. As shown in , this improved the perfor-mance of base-to-new generalization across all 4 datasets.Notably, EuroSAT showed a significant 10% improvement,emphasizing the effectiveness of dynamically selecting andemphasizing weaker augmentation types during each epoch.It demonstrates that attribute-specific feature decompositionfor challenging augmentations enables more robust learningof semantic features.",
  ". Conclusion": "Our novel approach efficiently extracts specific semanticfeatures and delta meta tokens by subtracting the augmentedimage feature from the original image feature. LeveragingAdTriplet loss adversarially enhances classification loss,enabling precise discernment of attribute features throughaugmentationsa foundational aspect of our approach. Bydecomposing attribute and semantic features more accu-rately, we introduce attribute-specific bias into the prompt.Furthermore, our study underscores the indispensability ofAAPL in prompt learning with augmentation for zero-shotclassification tasks. In summary, our emphasis on attributedecomposition in prompt learning is underscored throughaugmentation profiling and analysis of dataset correlations,augmentations, and AAPL performance. AcknowledgmentsThanks to Prof. George Kamenosfor his invaluable assistance in reviewing and editing thispaper. This work was partly supported by Innovative HumanResource Development for Local Intellectualization programthrough the Institute of Information & CommunicationsTechnology Planning & Evaluation(IITP) grant fundedby the Korea government(MSIT) (IITP-2024-00156287,40%). This research was supported by the Korea Institutefor Advancement of Technology (KIAT) grant funded by theMinistry of Trade, Industry, and Energy (MOTIE), Korea,(P0025331, 30%). This work was supported by the NationalResearch Foundation of Korea(NRF) grant funded by theKorea government(MSIT) (No. RS-2023-00252616, 30%). Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. NeurIPS, 2022.1",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In CVPR, 2016.2": "Patrick Helber, Benjamin Bischke, Andreas Dengel, andDamian Borth. Eurosat: A novel dataset and deep learningbenchmark for land use and land cover classification. IEEEJournal of Selected Topics in Applied Earth Observations andRemote Sensing, 12(7):22172226, 2019. 5, 6, 7, 8 Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,Samyak Parajuli, Mike Guo, et al. The many faces of robust-ness: A critical analysis of out-of-distribution generalization.In ICCV, 2021. 5, 6"
}