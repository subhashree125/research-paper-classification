{
  "Abstract": "With the rapid growth in deepfake video content, we re-quire improved and generalizable methods to detect them.Most existing detection methods either use uni-modal cuesor rely on supervised training to capture the dissonance be-tween the audio and visual modalities. While the formerdisregards the audio-visual correspondences entirely, the lat-ter predominantly focuses on discerning audio-visual cueswithin the training corpus, thereby potentially overlookingcorrespondences that can help detect unseen deepfakes. Wepresent Audio-Visual Feature Fusion (AVFF), a two-stagecross-modal learning method that explicitly captures thecorrespondence between the audio and visual modalitiesfor improved deepfake detection. The first stage pursuesrepresentation learning via self-supervision on real videosto capture the intrinsic audio-visual correspondences. Toextract rich cross-modal representations, we use contrastivelearning and autoencoding objectives, and introduce a novelaudio-visual complementary masking and feature fusionstrategy. The learned representations are tuned in the secondstage, where deepfake classification is pursued via super-vised learning on both real and fake videos. Extensive exper-iments and analysis suggest that our novel representationlearning paradigm is highly discriminative in nature. Wereport 98.6% accuracy and 99.1% AUC on the FakeAVCelebdataset, outperforming the current audio-visual state-of-the-art by 14.9% and 9.9%, respectively.",
  ". Introduction": "Deepfake generative AI technology enables new opportuni-ties to create rich and quality content in multimedia appli-cations such as virtual reality , movie production ,and telepresence . However, its malicious use hasbecome a major societal threat posing a number of problemsincluding frauds1, defamation2 and disinformation3. As the *This work was completed during an internship at Reality Defender Inc.1Fraudsters Used AI to Mimic CEOs Voice in Cybercrime Case2Deepfake porn documentary explores its life-shattering impact, AIFake Nudes are booming",
  "Masked Tokens": ". We use audio-visual correspondences for deepfake de-tection. Transformer-based encoders are used to extract audio andvisual feature tokens, which are then masked complementarily. Thevisible audio tokens are sent through a learnable A2V network topredict the masked visual tokens. These predicted visual tokensare fused with the visible visual tokens to obtain the full visualembeddings. Full audio embeddings are obtained in a similar wayusing the V2A network. The audio/visual embeddings are thenused for video reconstruction in the MAE sense, and subsequentlyfor deepfake classification. generative AI landscape continues to evolve, there is a grow-ing need for robust deepfake detection that helps preservecontent integrity. In this paper, we study video deepfakedetection where either or both the visual and audio contentare AI-generated.We pursue multi-modal learning and draw inspirationfrom previous works, such as SyncNet , CLIP , andAudioCLIP , where the correspondence between differ-ent modalities (audio, text, visual) was leveraged to signifi-cantly enhance performance on various downstream tasks.We note that in real video face context, the audio-visualcorrespondence is deeply intuitive since there is an intrinsiccorrelation between the mouth articulations (visemes) andthe speech units (phonemes) , as well as analignment of emotional nuances embedded in the facial andspeech expressions . Such inherent audio-visualcorrespondence, for example, in audio-driven emotion, ischallenging to faithfully replicate in deepfake videos. Basedon these observations, we propose a video deepfake detectionmethod that learns efficient representations for audio and vi-",
  "arXiv:2406.02951v1 [cs.CV] 5 Jun 2024": "sual modalities. The proposed method employs a novel com-plementary masking and cross-modal feature fusion strategyto explicitly capture the audio-visual correspondences.Previous literature on audio-visual video deepfake detec-tion use supervised contrastive learning to capturethe audio-visual correspondence. Such methods align theaudio and visual embeddings closer to each other, if the con-tent in both modalities is real, and push them apart if eitheror both modalities are generative. Similarly, others pursue asingle stage supervised learning method, where models aredirectly trained on labeled deepfake datasets for deepfakeclassification . While such methods yield promis-ing results, we conjecture that they may not fully exploit theaudio-visual correspondence. Also, training solely on a deep-fake dataset narrows the models focus to discern separablefeatures within the training corpus, potentially overlookingsubtle audio-visual correspondences that can help detect un-seen deepfake samples (observe in Tabs. 1 and 3, the weakerperformance of other baselines compared to and ours).To circumvent these issues, we propose a two stage train-ing pipeline comprising of (i) a self-supervised represen-tation learning stage that explicitly enforces audio-visualcorrespondence using a novel approach, and (ii) a super-vised downstream classification stage. In the representationlearning stage, we extract audio-visual representations viaself-supervised learning on real face videos, which are avail-able in abundance . Drawing inspirations fromCAV-MAE , we make use of the complementary natureof two learning objectives: contrastive learning and autoen-coding. For extracting rich representations, we supplementthe contrastive learning objective by a novel audio-visualcomplementary masking and fusion strategy that sits withinthe autoencoding objective. In the classification stage, wetrain a classifier that exploits the lack of cohesion betweenaudio-visual features of deepfake videos to separate themfrom real videos.We evaluate our method against existing state-of-the-artapproaches on multiple benchmarks. Our results reveal sub-stantial improvements, when compared against the existingaudio-visual state-of-the-art, enhancing the performance by9.9% in AUC and 14.9% in accuracy when evaluated on theFakeAVCeleb dataset . This underscores the effective-ness of explicitly leveraging audio-visual correspondencesthrough the proposed method. In summary: We propose a novel self-supervised representation learn-ing method that explicitly captures audio-visual correspon-dences in real videos. To learn the correspondences, wepursue a dual-objective of contrastive learning and au-toencoding, and supplement it with a novel audio-visualcomplementary masking and fusion strategy.",
  "efficacy of the proposed representation learning": "We propose a two-stage deepfake detection method com-prising of the aforementioned representation learning stagefollowed by a deepfake classification stage. Our methodyields state-of-the-art performance on deepfake detectionwhen either or both the audio and visual contents are AIgenerated. We achieve 98.6% accuracy and 99.1% AUC onFakeAVCeleb, surpassing the existing audio-visual state-of-the-art by 14.9% and 9.9% respectively.",
  ". Multi-Modal Representation Learning": "Learning a joint representation from multiple modalities hasbeen shown to be effective for different tasks in the state-of-the-art. SyncNet proposes a Siamese Network to esti-mate the lip-sync error between audio and visual modalities.This framework processes each modality through a distinctbranch and employs a contrastive loss to promote the simi-larities in the encoding space. More recently, improvementsin the Natural Language Processing (NLP) field brought byBERT , allowed to use text modality in multi-modalframeworks. Another example is CLIP , a zero-shotimage classification model that leverages separate encodersfor images and captions to find a suitable pairing in the la-tent space. AudioCLIP extends this approach to audio,enabling multi-modal classification.Several self-supervised methods have emerged, inspiredby the Masked Autoencoder (MAE) framework . AV-MAE is a joint masked autoencoder for audio, visual,and joint audio/visual classification. The authors exploredifferent encoding policies for dual-modality inputs, demon-strating the ability to decode one masked modality from theother. CAV-MAE raises concerns about the ability of avanilla masked autoencoder to learn a coordinated representa-tion between audio and visuals (i.e., a representation that en-forces similarity ) and adds a contrastive loss to explicitlyleverage the audio-visual pair information. In this work, wedraw inspiration from CAV-MAE, in using a dual contrastive-autoencoding objective for effective representation learning.Our approach diverges from existing MAE literature: (i)in terms of the masking strategy, where we use a comple-mentary masking strategy post-encoding; (ii) in terms of thecross-modal fusion, where for every modality we replace theshared learnable masked tokens of MAEs with tokens pre-dicted from the other modality. We do this to enforce explicitcorrespondence between audio and visual modalities.",
  "Visible": "Slices, . Audio-Visual Representation Learning Stage. A real input sample, x Dr, with corresponding audio and visual tokens(xa, xv), is split along the temporal dimension, creating K slices, {xa,ti}Ki=1 and {xv,ti}Ki=1 (illustrated with K = 8 in the figure). Thetemporal slices are then encoded using unimodal transformers, Ea and Ev, to yield feature embeddings a and v. We then complementarilymask 50% of the temporal slices in (a, v) with binary masks (Ma, Mv). The visible slices of a and v are passed through A2V and V2Anetworks respectively, to generate cross-modal slices av and va. The masked slices of a and v are then replaced with the correspondingslices in av and va. The resulting cross-modal fusion representations, a and v, are input to unimodal decoders to obtain the audio andvisual reconstructions, xa and xv. For the learning, we use a dual-objective loss function, which computes the contrastive loss between theaudio and visual feature embeddings and the autoencoder loss between the input and the reconstruction of the masked tokens. dataset they shared with the community. Some methodstarget specific face regions for exposing deepfakes. For ex-ample, LipForensics relies on lip movements that mightbe difficult to reproduce by generative methods. Others con-sider inconsistent head pose or eye blinking .Another common approach is to consider both spatial andtemporal domains. FTCN proposes a combination of aCNN and a transformer network to exploit short-time andlong-time temporal incoherence. Similarly, extractsspatial features by means of an attention-based network andthen fuses those features with a temporal module.Several papers based on the Vision Transformer (ViT)have been published since the advent of the original pa-per for image classification. An example is CViT ,where learnable features are extracted by means of a CNNand subsequently fed to a ViT for the classification task.Similar approaches are followed by . Recently,generalization to unseen deepfake methods and theimpact of the identity leakage during training havealso been investigated. RealForensics proposed a hybridapproach that consists of using a multi-modal pre-trainingpipeline, where audio and visuals exclusively from real sam-ples are used for computing internal representations that helpthe classifier to discriminate between real and fake video.This is not considered a pure multi-modal approach as thefinal classification is performed just on visuals, discardingthe audio modality. As modern-day video deepfakes consistof both audio and visual manipulations, uni-modal deepfakedetection methods prove to be less effective. Audio-visual methods. These methods consider audio andvisual signals to target deepfake detection on both modalities.One of the first papers to address multi-modality is EmotionsDont Lie , proposing a Siamese Network where uni-modal features are passed to an emotion recognition networkto compare affective cues corresponding to perceived emo-tion from the two modalities within a video. Not made foreach other explicitly modeled the dissimilarity betweenmodalities, and proposed the Modality Dissonance Score(MDS) network, where a contrastive loss is computed onsingle modality embeddings to expose differences on audio-visual pairs. Voice-Face matching Detection (VFD) isanother example of using contrastive loss for modeling faceand voice homogeneity. A similar concept is exploited in, where the focus is on phoneme-viseme mismatch. Theidea is that a given dynamic of mouth shape (viseme) shouldcorrespond to a given emitted sound (phoneme). The au-thors only focus on the mouth region, showing how deepfakemethods struggle to reproduce certain dynamics. More recently, the paradigm for multi-modality shiftedtowards fusion of single-modality features. AV-DFD proposes a joint audio-visual deepfake detection frameworkin which visual and audio features are aligned and tiled tobe passed onto a cross-attention mechanism on the temporaldimension. More recent papers study the encoding/decodingpotential of ViTs and build feature fusion in the embeddingspace on the decoder side. Examples are AVFakeNet and AVoiD-DF .",
  ". Method": "The proposed algorithm, AVFF, consists of two stages:(i) representation learning, and (ii) deepfake classification.Stage 1 aims to acquire an audio-visual representation withcross-modal correspondence via self-supervised learning,and it solely utilizes real face videos. The model learnsaudio-visual correspondences inherent to real videos via acontrastive learning objective and an effective complemen-tary masking and fusion strategy that sits within an auto-encoding objective (see ). Here, the complementarymasking and fusion strategy takes uni-modal audio and vi-sual embeddings (a, v) and systematically masks them toforce the learning of advanced embeddings (a, v) via re-construction in the MAE sense. To instill cross-modal depen-dency, tokens from one modality are used to learn the maskedembeddings of the other modality via cross-modal token con-version networks (A2V and V2A in ). Since we workexclusively with real face videos in this stage, the modellearns the dependency between real speech audio and thecorresponding visual facial features. In Stage 2, a classifieris trained to distinguish between real and fake videos usingthe learned representations from the first stage. Effectively,the representation learning stage serves as pre-training forthe downstream task of video deepfake detection.",
  ". Preprocessing": "Initially, visual frames and the corresponding audio wave-forms are extracted from raw videos at a sampling rate of5 fps and 16 kHz, respectively. Given our emphasis onaudio-visual correspondence, we align the cropped facialregions and eliminate the background in the visual framesusing FaceX-Zoo . This step is performed since back-ground variations typically exhibit minimal correspondencewith speech audio. Simultaneously, the audio waveform isconverted into a log-mel spectrogram with L frequency bins.Henceforth, we refer to the preprocessed visual frames andlog-mel spectrograms as the visual and audio input represen-tations, respectively.",
  ". Representation Learning Stage": "The primary objective of this stage is to learn a representa-tion that effectively captures audio-visual feature correspon-dences inherently present in real videos (and different thereoffrom the correspondences in fake videos). Drawing inspira-tion from CAV-MAE , we propose a dual self-supervisedlearning approach that incorporates contrastive learning andautoencoding objectives. While contrastive learning helpsbuild cross-modal correlations , we found inpreliminary experiments that relying solely on it does notestablish a strong correspondence between the audio andvisual modalities. Therefore, we supplement it with an au-toencoding objective and embed a complementary masking and cross-modal fusion strategy into the autoencoding frame-work. This allows us to learn rich cross-modal representa-tions that result in improved deepfake detection (see Tab. 4).In , we illustrate the overall pipeline for our represen-tation learning stage and discuss its key components next. Input Tokenization. Given a dataset Dr of real talkinghuman portrait videos, we denote a video sample x Drwith a time duration T as comprising of audio and visualcomponents, xaRTaL and xvRTvCHW ,respectively.In xa, the (Ta, L) denote the number ofaudio frames and mel-frequency bins.In xv, the (Tv,H, W, C) denote the number of visual frames, height,width, and number of channels. We choose Ta and Tvsuch that Ta na = Tv nv = T, where na and nv are thesampling frequencies of the audio and visual sequences.We tokenize xa using 16 16 non-overlapping 2D patches(similar to Audio-MAE ), and xv using 2 16 16non-overlapping 3D spatio-temporal patches (similar toMARLIN ). The resulting representations are denotedas xa and xv.Subsequently, we segment each of thetokenized representations into 8 equal temporal slices,xa = {xa,ti}8i=1 and xv = {xv,ti}8i=1, where the numberof slices was decided empirically. This slicing preserves thetemporal correspondence of each slice between modalities,as xa,ti and xv,ti correspond to the same time interval. Feature Encoding. The two uni-modal audio and visualencoders, Ea and Ev, encode the tokenized inputs, xa andxv, and output uni-modal features a and v respectively:p = {pti}8i=1 = Ep(xp + posep), where, p {a, v} andposep is the learnable positional embedding. Complementary Masking. Within the uni-modal featureembeddings, a and v, we mask 50% of the temporal slicesusing binary masks, (Ma, Mv) {0, 1}, such that Maand Mv are complementary to each other, i.e., Ma = 1 forslices where Mv = 0 and vice-versa. In other words, forevery masked slice in the audio feature, the correspondingvisual slice is visible and vice versa. Let us denote the visibletemporal slices as pvis = Mp p and the masked temporalslices as pmsk = (Mp) p, where p {a, v}, denotesthe Hadamard product and is the NOT operator. Cross-Modal Fusion. Next, the visible temporal slicesavis and vvis are sent through learnable audio-to-visual(A2V) and visual-to-audio (V2A) networks to create theircross-modal temporal counterparts, va = A2V(avis) andav = V2A(vvis), respectively. Here, va contains {vti,a =A2V(ati), ti where ati avis}, and similarly av. Eachof the A2V/V2A networks is composed of a single-layerMLP to match the number of tokens of the other modality fol-lowed by a single transformer block. The audio embeddinga is then created using cross-modal fusion, wherein, we takethe original feature a and replace each masked slice with thecorresponding slice of the same temporal index in the cross- modal vector av given by the V2A network (see ). Thevisual embedding v is similarly obtained from the originalfeature v and the cross-modal feature va. Effectively, thisprocess replaces the masked temporal slices of each modalitywith cross-modal slices generated from the correspondingtemporal slices in the other modality. This is feasible due toour complementary masking strategy, as the masked slicesof one modality are the visible slices of the other modality. Decoding. The uni-modal audio and visual decoders, Gaand Gv, take a and v as input to generate the audioand visual reconstructions, xa = Ga(a + posga) andxv = Gv(v + posgv), where posga and posgv are learnablepositional embeddings for each modality. The decoders usea transformer-based architecture and are entrusted with thetask of effectively utilizing the mix of uni-modal slices andcross-modal slices present in a and v to generate the re-constructions for the two modalities.",
  "(1)": "where p(i) is the mean latent vector across the patch dimen-sion of the uni-modal embeddings of the i-th data sample,N is the number of samples, is the temperature parameter,and i, j are sample indices. The audio-visual contrastive lossenforces similarity constraints between the audio and visualembeddings of a given sample.The autoencoder loss, Lae, is composed of reconstructionand adversarial losses, similar to MARLIN . The recon-struction MSE loss, Lrec, is computed between the inputs(xa, xv), and their reconstructions (xa, xv), and is com-puted only over the masked tokens following the approachin MAEs :",
  "Network": "REAL/FAKE . Deepfake Classification Stage. Given a sample x Ddf,comprising of audio and visual inputs xa and xv, we obtain theunimodal features (a, v) and the cross-modal embeddings (av, va).For each modality, the unimodal and cross-modal embeddings areconcatenated to obtain (fa, fv). A classifier network is then trainedto take (fa, fv) as input and predict if the input is real or fake.",
  ". Deepfake Classification Stage": "The goal of this stage is to detect video deepfakes, whereeither or both audio and visual modalities have been faked.For this, we use the encoders and the cross-modal networkstrained in the representation learning phase. We train aclassifier to tell real videos and deepfakes apart using asupervised learning approach. The classification pipeline isdepicted in . Since the learned representations have ahigh audio-visual correspondence for real videos, we expectthe classifier to exploit the lack of audio-visual cohesion ofsynthesized samples in distinguishing between real and fake. Input Tokenization. The process followed in input tokeniza-tion is identical to Stage 1 except for the dataset used. In thisstage we draw samples from a labeled deepfake dataset (Ddf)consisting of both real and fake videos, i.e., (x, y) Ddf,where x is the video sample and y is the label (real/fake). Feature Extraction. The tokenized inputs (xa, xv), aresent through the backbone of Stage 1 to obtain (i) the featureembeddings (a, v), which are the outputs of the uni-modalencoders, and (ii) the cross-modal embeddings (av, va),which are the outputs of the A2V/V2A cross-modal networks.Here, the cross-modal embeddings are computed for all tem-poral slices (note: we do not use masking in this stage). Weconcatenate the two embeddings of each modality creating",
  "(fa, fv), where fp = p pq, p, q {a, v}, p = q, and is the concatenation operator along the feature dimension": "Classifier Network. The classifier network, Q, takes asinput the combined embeddings of each modality (fa, fv),and predicts if a given sample is real or fake. The classifiernetwork consists of two uni-modal patch reduction networks:(a, v), followed by a classifier head, . Each embedding(fa, fv), is first distilled in the patch dimension using thecorresponding uni-modal patch reduction networks. Thenthe output embeddings are concatenated along the featuredimension and fed into the classifier head which outputs thelogits, l, used to classify if a given sample is real or fake.Formally, l = Q(fa, fv) = (a(fa) v(fv)).",
  "Loss Function. We use the standard cross-entropy loss,denoted by LCE as the learning objective, computed usingthe input labels, y, and the output logits, l": "Deepfake Classifier Inference Stage. During inference,we first split the video into blocks of time T (the samplelength during training) with a step size of T/8, which is theduration of a temporal slice. The output logits are computedfor each of the blocks and the classification decision (real orfake) is made based on the mean of the output logits.",
  ". Implementation": "We train Stage 1 (representation learning), using the LRS3dataset , which exclusively contains real videos. In Stage2 (deepfake classification), we train a classifier that followsa supervised learning approach using the FakeAVCeleb dataset.FakeAVCeleb comprises of both real and fakevideos, where either one or both audio-visual modalitieshave been synthesized using different combinations of sev-eral generative deepfake algorithms (visual: FaceSwap,FSGAN, and Wav2Lip ; audio: SV2TTS ).Please refer to the supplementary for additional details ondatasets, architecture, and implementation.",
  ". Evaluation and Discussion": "We evaluate the performance of our model against existingstate-of-the-art algorithms, on multiple criteria: intra-datasetperformance, cross-manipulation generalization, and cross-dataset generalization following . We compareour results against state-of-the-art audio-visual approachesand uni-modal (visual) approaches for completeness. Wereport accuracy (ACC), average precision (AP), and areaunder the ROC curve (AUC) averaged across multiple runswith different random seeds. For audio-visual algorithms,we label a video as fake if either or both audio and visualmodalities have been manipulated. To maintain fairness, foruni-modal algorithms we consider a video as fake only if thevisual modality has been manipulated. Please refer to the",
  "supplementary for additional results on robustness to unseenaudio and visual perturbations": "Intra-Dataset Performance.Following the methodol-ogy outlined in , our training utilizes 70% of allFakeAVCeleb samples, while the remaining 30% consti-tutes the unseen test set. As denoted in Tab. 1, our approachdemonstrates substantial improvements over the existingstate-of-the-art, both in audio-visual (AVoiD-DF ) anduni-modal (RealForensics ) deepfake detection. Com-pared to AVoiD-DF, our method achieves an increase in accu-racy of 14.9% (+9.9% in AUC), and compared to RealForen-sics the accuracy increases by 8.7% (+4.5% AUC). Overall,the superior performance of audio-visual methods leverag-ing cross-modal correspondence is evident, outperforminguni-modal approaches that rely on uni-modal artifacts (i.e.visual anomalies) introduced by deepfake algorithms. Re-alForensics, while competitive, discards the audio modalityduring detection, limiting its applicability exclusively to vi-sual deepfakes. This hinders its practicality as contemporarydeepfakes often involve manipulations in both audio andvisual modalities. The enhanced results of both RealForen-sics and our proposed method highlight the positive impactof employing a pre-training stage for effective representa-tion learning. This observation aligns with findings in othermulti-modal representation learning research across diversedownstream tasks . Cross-Manipulation Generalization. In this experiment,we aim to assess the models performance on samples gen-erated using previously unseen manipulation methods. Thescalability of deepfake detection algorithms to unseen ma-nipulation methods is crucial for adapting to evolving threats,thus ensuring wide applicability across diverse scenarios.Similar to , we partition the FakeAVCeleb dataset intofive categories: RVFA, FVRA-WL, FVFA-FS, FVFA-GAN,",
  "AVFF (Ours)AV93.392.494.898.2100.100.99.9100.99.499.898.599.5": ". Cross-Manipulation Generalization on FakeAVCeleb. We evaluate the models performance by leaving out one category fortesting while training on the rest. We consider the following 5 categories in FakeAVCeleb: (i) RVFA: Real Visual - Fake Audio (SV2TTS),(ii) FVRA-WL: Fake Visual - Real Audio (Wav2Lip), (iii) FVFA-FS: Fake Visual - Fake Audio (FaceSwap + Wav2Lip + SV2TTS), (iv)FVFA-GAN: Fake Visual - Fake Audio (FaceSwapGAN + Wav2Lip + SV2TTS), and (v) FVFA-WL: Fake Visual - Fake Audio (Wav2Lip+ SV2TTS). The column titles correspond to the category of the test set. AVG-FV corresponds to the average metrics of the categoriescontaining fake visuals. Best result is in bold, and second best is underlined. Our method yields consistently high performance acrossall manipulation methods while yielding state-of-the-art performance for several categories.",
  "AVFF (Ours)AV93.195.5": ". Cross-Dataset Generalization on KoDF. We evaluate ourmodels performance against baselines by testing the model trainedon the FakeAVCeleb dataset, on a subset of the KoDF dataset. Bestresult is in bold, and second best is underlined. and FVFA-WL, based on the algorithms used to generatethe deepfakes. Descriptions of these categories are includedin the caption of Tab. 2, for ease of reference. Using thesecategories, we evaluate the model leaving one category outfor testing while training on the remaining categories. Re-sults are reported in Tab. 2. Our method achieves the bestperformance in almost all cases (and at par with the rest) andnotably, yields consistently enhanced performance (AUC >92+%, AP > 93+%) across all categories, while other base-lines (Xception , LipForensics , FTCN , AV-DFD ) fall short in categories FVFA-GAN and RVFA.While the performance of AVAD (an unsupervised method)is intriguing, we can notice how other supervised baselinesperform better in most cases. Cross-Dataset Generalization. We also evaluate the adapt-ability of the model to a different data distribution, by test-ing on a subset of the KoDF dataset , following theevaluation protocol established by . We report the re-sults in Tab. 3, where we perform at par with RealForensics.Overall, we observe a performance trend similar to cross-",
  "generalization to unseen generative methods. Please refer tothe supplementary for additional cross-dataset generalizationresults on DF-TIMIT and DFDC datasets": "Analysis on the Learned Representation. During the train-ing of the downstream task, we observe notable AUC scoreson the test set, reaching as high as 90% within the initial1-3 epochs. Intrigued by this observation, we explore therepresentations learned at the end of Stage 1 (Sec. 3.2). Wevisualize the t-SNE plots of embeddings for random samplesfrom each category of the FakeAVCeleb dataset in . Wedo not expose Stage 1 to any deepfake samples during train-ing, and still observe clear discrimination between real andfake samples. This explains the high AUC scores achievedeven at the very early stages of the downstream training. Aconsequence of our representation learning stage is that weobserve a clear disentanglement between real and deepfakevideos within the FakeAVCeleb dataset. Further, distinctclusters are evident for each deepfake category which indi-cates that our representations are capable of capturing subtlecues that differentiate different deepfake algorithms despitenot encountering any of them during the training stage. 4 A further analysis of the t-SNE visualizations reveals thatthe samples belonging to adjacent clusters are related interms of the deepfake algorithms used to generate them. Forinstance, FVRA-WL and FVFA-WL, which are adjacent,both employ Wav2Lip to synthesize the deepfakes (referto the encircled regions in ). These findings under-score the efficacy of our novel audio-visual representationlearning paradigm. Please refer to the supplementary for theevaluation of the classification performance on the learnedrepresentation which further reinforces the above analysis.",
  "Stage 1 is exclusively trained on the LRS3 dataset containing only realvideos, while samples for the t-SNE are drawn from FakeAVCeleb": ". The t-SNE Visualization of the Embeddings at the endof the Representation Learning Stage. A clear distinction is seenbetween the representations of real and fake videos, as well as be-tween different deepfake categories. Further analysis indicates thatsamples of adjacent clusters are generated using the same deepfakealgorithm, which we encircle manually to highlight the clusters.",
  ". Ablation Study": "Autoencoding Objective. In this experiment, we train themodel using only the contrastive loss objective, discardingthe autoencoding objective. This deactivates complementarymasking, cross-modality fusion, and decoding modules. Thefeature embeddings at the output of the encoders (a, v), areused for the downstream training. Results (see row (i) inTab. 4) indicate a performance reduction, highlighting theimportance of the autoencoding objective. Cross-Modal Fusion.In this ablation, we discard theA2V/V2A networks, which predict the masked tokens ofthe other modality, and instead use shared learnable maskedtokens similar to MAE approaches . We cansee that the performance of the model diminishes (especiallyAP) (see row (ii) in Tab. 4). This highlights the importanceof the cross-modal fusion module, as it supplements the rep-resentation of a given modality with information extractedfrom the other modality, helping building the correspondencebetween the two modalities. Complementary Masking.Replacing complementarymasking with random masking in Stage 1, results in a notabledrop in AP and AUC scores, affecting the models abilityto learn correspondences (see row (iii) in Tab. 4). We at-tribute this performance drop to the inability of the model tolearn correspondences between audio and visual modalitiesdue to the randomness, which indicates the importance ofcomplementary masking in the proposed method. Concatenation of Different Embeddings. In the deepfakeclassification stage, we concatenate the feature embeddings(a, v), with the cross-modal embeddings (av, va), creatingthe concatenated embeddings (fa, fv). In this experiment,we evaluate the performance using each of the embeddingsin isolation (see rows (iv) and (v) in Tab. 4). While the useof each embedding generates promising results, the synergyof the two embeddings enhances the performance.",
  ". Evaluation on Ablations. Best result is in bold, andsecond best is underlined. The proposed AVFF pipeline performsthe best among the considered ablations": "Uni-Modal Patch Reduction. Replacing the uni-modalpatch reduction networks (a, v) with Mean Poolingslightly dents the performance (see row (vi) in Tab. 4). Thissuggests that a simple linear averaging is sub-optimal in thatsome subtle cues may no longer be preserved when com-pared to using an MLP which effectively performs weightednon-linear averaging.",
  ". Conclusion, Limitations, and Future Work": "In this paper, we propose AVFF, a novel two-staged learn-ing framework for audio-visual deepfake detection. AVFFis composed of a self-supervised representation learningstage that captures the audio-visual correspondence usingcontrastive learning and a novel complementary maskingand cross-modal fusion module within the autoencodingobjective, followed by a supervised deepfake classificationstage. Our method shows significant improvements in bothin-distribution performance as well as generalization on un-seen manipulations over both visual-only and audio-visualstate-of-the-art algorithms. Our results not only validate theeffectiveness of the proposed approach but also emphasizethe potential as a defensive tool against the escalating threatposed by deepfake videos. Limitations. Our work is limited to cases where there isa coherence between the audio and visual modalities, aswe rely on it to distinguish fake from real videos. Hence,asynchronous videos with audio lags or mismatched audio,videos with multiple speakers, and visuals with occlusions(e.g., masks, hands) would be challenging. Our model alsorequires the input to contain both audio and visual modali-ties, i.e., videos with only one modality are not supported. Future Works. A few possible future research avenues in-clude: (i) supplementing our representation learning methodwith a strategy to preserve uni-modal cues to mitigate theaforementioned limitations, (ii) adaptation for other audio-visual downstream tasks such as emotion recognition, and(iii) generalization for non-humanoid audio-driven videosespecially with diffusion-based approaches .",
  "B.1. Representation Learning Stage": "Inputs. We draw samples from the LRS3 dataset , which exclu-sively contains real videos. We preprocess all videos as explainedin Sec. 3.1 in the main paper. The audio stream is converted to aMel-spectrogram of 128 Mel-frequency bins, with a 16 ms Ham-ming window every 4 ms. We randomly sample video clips ofT = 3.2s in duration, sampling 16 visual frames and 768 audioframes (Mel) with clipping/padding where necessary. The 16 visualframes are uniformly sampled such that they are at the first and thirdquartile of a temporal slice (2 frames/slice 8 slices). The visualframes are resized to 224 224 spatially and are augmented usingrandom grayscaling and horizontal flipping, each with a probabilityof 0.5. We make sure that in a given batch, for each sample wedraw another sample from the same video but at a different timeinterval to make sure the model is exposed to the notion of temporalshifts when computing the contrastive loss. Both audio and visualmodalities are normalized. Architecture. We adopt the encoder and decoder architectures ofeach modality from the VideoMAE based on ViT-B . Eachof the A2V/V2A networks is composed of a linear layer to matchthe number of tokens of the other modality followed by a singletransformer block. Optimization. We initialize the audio encoder and decoder usingthe checkpoint of AudioMAE pretrained on AudioSet-2M and the visual encoder and decoder using the checkpoint ofMARLIN pretrained on the YouTubeFace dataset. Subse-quently, we train the representation learning framework end-to-end,using the AdamW optimizer with a learning rate of 1.5e-4with a cosine decay . The weights of the losses are as follows:c = 0.01, rec = 1.0, and adv = 0.1, which were chosenempirically and based on previous research . We train for500 epochs with a linear warmup for 40 epochs using a batch sizeof 32 and a gradient accumulation interval of 2. The training wasperformed on 4 RTX A6000 GPUs for approximately 60 hours.",
  "B.2. Deepfake Classification Stage": "Inputs. We draw samples from FakeAVCeleb , which consistsof deepfake videos where either or both audio and visual modalitieshave been manipulated. The preprocessing and sampling strategyis similar to that of Stage 1, except we do not draw an additionalsample from the same video clip as we do not use a contrastivelearning objective at this stage. We employ weighted sampling to",
  "Architecture. Each of the uni-modal patch reduction networks is a3-layer MLP, while the classifier head is a 4-layer MLP. We do notmake any changes to the representation learning architecture": "Optimization. We initialize the representation learning frameworkusing the pretrained checkpoint obtained from Stage 1. Subse-quently, we train the pipeline end-to-end, using the AdamW opti-mizer with a cosine annealing with warm restarts scheduler with a maximum learning rate of 1.0e-4 for 50 epochs witha batch size of 32. The training was performed on 4 RTX A6000GPUs for approximately 10 hours.",
  "C. Dataset Details": "LRS3 . This dataset introduced by Afouras et al. exclusivelycomprises of real videos. It consists of 5594 videos spanning over400 hours of TED and TED-X talks in English. The videos in thedataset are processed such that each frame contains faces and theaudio and visual streams are in sync. FakeAVCeleb . The FakeAVCeleb dataset is a deepfake de-tection dataset, which consists of 20,000 video clips in total. Itcomprises of 500 real videos sampled from the VoxCeleb2 and19500 deepfake samples generated using different manipulationmethods applied on the set of real videos. The dataset consists ofthe following manipulations where the deepfake algorithms used ineach category are indicated within brackets. RVFA: Real Visuals - Fake Audio (SV2TTS ) FVRA-FS: Fake Visuals - Real Audio (FaceSwap ) FVFA-FS: Fake Visuals - Fake Audio (SV2TTS + FaceSwap) FVFA-GAN:FakeVisuals-FakeAudio(SV2TTS+FaceSwapGAN)",
  "KoDF . This dataset is a large-scale dataset comprising realand synthetic videos of 400+ subjects speaking Korean. KoDF": ". Robustness to Unseen Visual Perturbations. We illustrate AUC scores (%) as a function of different levels of intensities forvarious visual perturbations evaluated on the test set of FakeAVCeleb. Our model is more robust than RealForensics , which is thecurrent state-of-the-art in robustness to unseen visual perturbations. consists of 62K+ real videos and 175K+ fake videos synthesizedusing the following six algorithms: FaceSwap , DeepFaceLab, FaceSwapGAN, FOMM , ATFHP , and Wav2Lip. We use a subset of this dataset following to evaluate thecross-dataset generalization performance of our model (Tab. 3 inthe main paper). DF-TIMIT . The Deepfake TIMIT dataset comprises deepfakevideos manipulated using FaceSwapGAN . The real videosused for manipulation have been sourced by sampling similar-looking identities from the VidTIMIT dataset. We use theirhigher-quality (HQ) version, which consists of 320 videos, in eval-uating cross-dataset generalization performance. DFDC . The DeepFake Detection Challenge (DFDC) datasetis another deepfake dataset that consists of samples with fake audiobesides FakeAVCeleb. It consists of over 100K video clips in totalgenerated using deepfake algorithms such as MM/NN Face Swap, NTH , FaceSwapGAN , StyleGAN , and TTSSkins . We use a subset of this dataset consisting of 3215videos, as used in to evaluate the models cross-datasetgeneralization performance.",
  "D.1. Cross-Dataset Generalization": "In addition to the cross-dataset generalization evaluation reportedon the KoDF dataset (Tab. 3 of the main paper), we further evaluatecross-dataset generalization on the DF-TIMIT dataset and a subsetof the DFDC dataset following . We are limited to com-paring against baselines with open-source codes. We were unableto obtain the models nor results for the other baselines from theauthors. As illustrated in Tab. 5, we achieve the best cross-datasetgeneralization performance, when evaluated on both DF-TIMITand DFDC.",
  "D.2. Robustness to Unseen Perturbations": "In real-world scenarios, videos undergo post-processing (e.g. whensharing through social media platforms), which perturbs both audioand visual modalities. Hence, it is crucial for a model to be robustto unseen perturbations. To this end, we evaluate the performanceof our model (trained without augmentations) on several unseenperturbations applied to each modality. Visual Perturbations. Following , we evaluate the per-formance on the following perturbations: saturation, contrast, block-wise distortion, Gaussian noise, Gaussian blur, JPEG compression,and video compression on five different levels of intensities. Theimplementations for the perturbations and the levels of intensitieswere sourced from the official repository of DeeperForensics-1.0. We compare our models performance against RealForen- . Robustness to Unseen Audio Perturbations. We il-lustrate the variation of AUC and AP scores (%) as a function ofdifferent levels of intensities for various audio perturbations eval-uated on the test set of FakeAVCeleb. Overall our model depictsimpressive robustness to audio perturbations. sics , which has the current state-of-the-art performance inrobustness to unseen visual perturbations . As depicted in, our model demonstrates enhanced robustness against unseenvisual perturbations compared to RealForensics in most scenarios.Particularly, noteworthy improvements are observed in cases ofblock-wise distortion, Gaussian noise, and video compression. Audio Perturbations. In this experiment, we subject the audiostream to a range of perturbations: Gaussian Noise, pitch shift,changes in reverberance, and audio compression. The performanceof our model under these perturbations is illustrated across five in-tensity levels in . To generate the perturbed audio samples, weemploy the following Python libraries: torchaudio (Gaussian noise,pitch shift), pysndfx (reverberance), and pydub (audio compression).The parameters used to generate samples at each intensity level aretabulated in Tab. 6. As seen in , overall our model is robustto various audio perturbations. A slight decrease in performance isseen in cases of Gaussian noise and pitch shift, with the increase inintensity. Notably, the model showcases high robustness to changesin reverberance, with minimal fluctuations across all intensity levels.However, a noticeable reduction in average precision is observedfor high-intensity levels of audio compression, potentially due toartifacts introduced by the reduced bitrate in extreme compressionscenarios.",
  "D.3. Classification Performance on the LearnedRepresentation": "We further evaluate the learned representation at the end of Stage1, by performing the downstream deepfake classification task withthe weights of the encoders and the A2V/V2A networks frozen.We train two classifiers for the downstream task: (i) the classifiernetwork described in the main paper, and (ii) kernel SVM using anRBF kernel (gamma=0.1, C=1.0). The results are reported in Tab. 7.",
  "AVFF (Ours)98.699.1": ". Classification Performance on the Learned Represen-tation. We evaluate the classification performance of the learnedrepresentation by freezing the encoders and A2V/V2A networksand training only the downstream networks. We employ (i) an MLPsimilar to the proposed method, and (ii) a kernel SVM (RBF), as theclassifier. Both classifiers yield reasonably high metrics, indicatingthe effectiveness of the learned representation at the end of Stage 1in distinguishing between real and fake videos. . Visual Examples of a Few Challenging Scenarios. Im-ages from left to right depict examples of extreme poses (e.g. nearprofile), occlusions with masks, and occlusions with hands acrossthe face, which makes it challenging for our model to establishcorrespondence between the audio and visual modalities. Both classifiers yield reasonably high accuracy and AUC values.This is indicative of the highly discriminative nature of the learnedrepresentation at the end of Stage 1. This reinforces the analysison the learned representation at the end of Stage 1, as discussed inSec. 4.2 of the main paper.",
  "Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasser-stein generative adversarial networks. In International con-ference on machine learning, pages 214223. PMLR, 2017.5": "Weiming Bai, Yufan Liu, Zhipeng Zhang, Bing Li, and Weim-ing Hu. Aunet: Learning relations between action units forface forgery detection. In 2023 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages2470924719, 2023. 1 Tadas Baltruaitis, Chaitanya Ahuja, and Louis-PhilippeMorency. Multimodal machine learning: A survey and tax-onomy. IEEE Transactions on Pattern Analysis and MachineIntelligence, 41(2):423443, 2019. 2 Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall,Jianfei Cai, Hamid Rezatofighi, Reza Haffari, and MunawarHayat. Marlin: Masked autoencoder for facial video represen-tation learning. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 14931504, 2023. 4, 5, 6, 8, 9",
  "J. S. Chung and A. Zisserman. Out of time: automated lipsync in the wild. In Workshop on Multi-view Lip-reading,ACCV, 2016. 1, 2": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. Bert: Pre-training of deep bidirectional transform-ers for language understanding. In North American Chapterof the Association for Computational Linguistics, 2019. 2 Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, RussHowes, Menglin Wang, and Cristian Canton Ferrer. Thedeepfake detection challenge (dfdc) dataset. arXiv preprintarXiv:2006.07397, 2020. 2, 7, 10 Shichao Dong, Jin Wang, Renhe Ji, Jiajun Liang, HaoqiangFan, and Zheng Ge. Implicit identity leakage: The stumblingblock to improving deepfake detection generalization. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 39944004, 2023. 3",
  "Baining Guo. Protecting celebrities from deepfake with iden-tity consistency transformer. In 2022 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages94589468, 2022. 3": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Transform-ers for image recognition at scale. In International Conferenceon Learning Representations, 2020. 3, 9 Chao Feng, Ziyang Chen, and Andrew Owens.Self-supervised video forensics by audio-visual anomaly detection.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1049110503, 2023.6, 7, 10, 11 Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, ArenJansen, Wade Lawrence, R Channing Moore, Manoj Plakal,and Marvin Ritter. Audio set: An ontology and human-labeleddataset for audio events. In 2017 IEEE international confer-ence on acoustics, speech and signal processing (ICASSP),pages 776780. IEEE, 2017. 9 Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu TudorIonescu, Mario Lucic, Cordelia Schmid, and Anurag Arnab.Audiovisual masked autoencoders. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1614416154, 2023. 2 Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, DavidHarwath, Leonid Karlinsky, Hilde Kuehne, and James R.Glass. Contrastive audio-visual masked autoencoder. InICLR. OpenReview.net, 2023. 2, 4, 5, 6, 8, 9 Andrey Guzhov, Federico Raue, Jrn Hees, and AndreasDengel. Audioclip: Extending clip to image, text and audio.In ICASSP 2022-2022 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages976980. IEEE, 2022. 1, 2, 4 Alexandros Haliassos, Konstantinos Vougioukas, StavrosPetridis, and Maja Pantic. Lips dont lie: A generalisable androbust approach to face forgery detection. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 50395049, 2021. 3, 6, 7, 9, 10 Alexandros Haliassos, Rodrigo Mira, Stavros Petridis, andMaja Pantic.Leveraging real talking faces via self-supervision for robust forgery detection. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1495014962, 2022. 2, 3, 6, 7, 9, 10, 11 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, PiotrDollr, and Ross Girshick. Masked autoencoders are scalablevision learners. In 2022 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 1597915988,2022. 2, 8 Baojin Huang, Zhongyuan Wang, Jifan Yang, Jiaxin Ai,Qin Zou, Qian Wang, and Dengpan Ye. Implicit identitydriven deepfake face swapping detection. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 44904499, 2023. 3",
  "Florence, Italy, October 7-13, 2012, Proceedings, Part II 12,pages 144158. Springer, 2012. 10": "Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, MichaelAuli, Wojciech Galuba, Florian Metze, and Christoph Feicht-enhofer. Masked autoencoders that listen. Advances in NeuralInformation Processing Systems, 35:2870828720, 2022. 4,5, 6, 8, 9 Hafsa Ilyas, Ali Javed, and Khalid Mahmood Malik. Av-fakenet: A unified end-to-end dense swin transformer deeplearning model for audiovisual deepfakes detection. AppliedSoft Computing, 136:110124, 2023. 3, 6 Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo,Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The powerof sound (tpos): Audio reactive video generation with stablediffusion. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 78227832, 2023. 8 Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, FeiRen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno,Yonghui Wu, et al. Transfer learning from speaker verificationto multispeaker text-to-speech synthesis. Advances in neuralinformation processing systems, 31, 2018. 6, 9 Liming Jiang, Ren Li, Wayne Wu, Chen Qian, andChen Change Loy. Deeperforensics-1.0: A large-scale datasetfor real-world face forgery detection. In Proceedings of theIEEE/CVF conference on computer vision and pattern recog-nition, pages 28892898, 2020. 10",
  "Tackhyun Jung, Sangwon Kim, and Keecheon Kim. Deepvi-sion: Deepfakes detection using human eye blinking pattern.IEEE Access, 8:8314483154, 2020. 3": "Bachir Kaddar, Sid Ahmed Fezza, Wassim Hamidouche, Za-hid Akhtar, and Abdenour Hadid. Hcit: Deepfake videodetection using a hybrid model of cnn features and visiontransformer. In 2021 International Conference on VisualCommunications and Image Processing (VCIP), pages 15,2021. 3 Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks. InProceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 44014410, 2019. 10",
  "Pavel Korshunov and Sbastien Marcel. Deepfakes: a newthreat to face recognition? assessment and detection. arXivpreprint arXiv:1812.08685, 2018. 7, 10": "Iryna Korshunova, Wenzhe Shi, Joni Dambre, and LucasTheis. Fast face-swap using convolutional neural networks.In Proceedings of the IEEE international conference on com-puter vision, pages 36773685, 2017. 6, 9, 10 Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park,and Gyeongsu Chae. Kodf: A large-scale korean deepfakedetection dataset. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1074410753,2021. 7, 9 Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi:Exposing ai created fake videos by detecting eye blinking. In2018 IEEE International Workshop on Information Forensicsand Security (WIFS), pages 17, 2018. 3",
  "Kevin Lutz and Robert Bassett. Deepfake detection withinconsistent head poses: Reproducibility and analysis. CoRR,abs/2108.12715, 2021. 3": "Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang,Yuecheng Li, Fernando De la Torre, and Yaser Sheikh. Pixelcodec avatars. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages6473, 2021. 1 Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, AniketBera, and Dinesh Manocha. Emotions dont lie: An audio-visual deepfake detection method using affective cues. InProceedings of the 28th ACM international conference onmultimedia, pages 28232832, 2020. 1, 2, 3, 6 Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, AniketBera, and Dinesh Manocha. M3er: Multiplicative multimodalemotion recognition using facial, textual, and speech cues. InProceedings of the AAAI conference on artificial intelligence,pages 13591367, 2020. 1 Pedro Morgado, Ishan Misra, and Nuno Vasconcelos. Ro-bust audio-visual instance discrimination. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1293412945, 2021. 4 Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subjectagnostic face swapping and reenactment. In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 71847193, 2019. 6, 9, 10 Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards uni-versal fake image detectors that generalize across generativemodels. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages2448024489, 2023. 3 Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu,Sugasa Marangonda, Chris Um, Mr Dpfks, Carl Shift Facen-heim, Luis RP, Jian Jiang, et al. Deepfacelab: Integrated, flex-ible and extensible face-swapping framework. arXiv preprintarXiv:2005.05535, 2020. 10",
  "Adam Polyak, Lior Wolf, and Yaniv Taigman. Tts skins:Speaker conversion via asr. arXiv preprint arXiv:1904.08983,2019. 10": "KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri,and CV Jawahar. A lip sync expert is all you need for speechto lip generation in the wild. In Proceedings of the 28thACM international conference on multimedia, pages 484492,2020. 1, 6, 9, 10 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 1, 2, 4 Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris-tian Riess, Justus Thies, and Matthias Niener. Faceforen-sics++: Learning to detect manipulated facial images. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 111, 2019. 2, 6, 7, 9 Andrew Rouditchenko, Angie Boggust, David Harwath, BrianChen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, HildeKuehne, Rameswar Panda, Rogerio Feris, et al. Avlnet: Learn-ing audio-visual language representations from instructionalvideos. In Annual Conference of the International SpeechCommunication Association. International Speech Communi-cation Association, 2021. 4 Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, BeiLiu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Bain-ing Guo. Mm-diffusion: Learning multi-modal diffusionmodels for joint audio and video generation. In Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1021910228, 2023. 8 Conrad Sanderson and Brian C Lovell. Multi-region proba-bilistic histograms for robust and scalable identity inference.In Advances in biometrics: Third international conference,ICB 2009, alghero, italy, june 2-5, 2009. Proceedings 3, pages199208. Springer, 2009. 10",
  "Laurens van der Maaten and Geoffrey E. Hinton. Visualizingdata using t-sne. Journal of Machine Learning Research, 9:25792605, 2008. 2": "Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, and Tao Mei.Facex-zoo: A pytorch toolbox for face recognition. In Pro-ceedings of the 29th ACM International Conference on Multi-media, pages 37793782, 2021. 4 Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shotfree-view neural talking-head synthesis for video conferenc-ing. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1003910049, 2021. 1 Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang,and Houqiang Li. Altfreezing for more general video faceforgery detection. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 41294138, 2023. 3",
  "Haojie Wu, Pan Hui, and Pengyuan Zhou.Deepfakein the metaverse:An outlook survey.arXiv preprintarXiv:2306.07011, 2023. 1": "Wenyuan Yang, Xiaoyu Zhou, Zhikai Chen, Bofei Guo,Zhongjie Ba, Zhihua Xia, Xiaochun Cao, and Kui Ren. Avoid-df: Audio-visual joint learning for detecting deepfake. IEEETransactions on Information Forensics and Security, 18:20152029, 2023. 2, 3, 6 Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakesusing inconsistent head poses. In ICASSP 2019 - 2019 IEEEInternational Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 82618265, 2019. 3",
  "Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-Jin Liu. Audio-driven talking face video generation withlearning-based personalized head pose.arXiv preprintarXiv:2002.10137, 2020. 10": "Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, andVictor Lempitsky. Few-shot adversarial learning of realisticneural talking head models. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 94599468, 2019. 10 Xiaohui Zhao, Yang Yu, Rongrong Ni, and Yao Zhao. Ex-ploring complementarity of global and local spatiotemporalinformation for fake face video detection. In ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pages 28842888, 2022. 3 Yinglin Zheng, Jianmin Bao, Dong Chen, Ming Zeng, andFang Wen. Exploring temporal coherence for more generalvideo face forgery detection. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 1504415054, 2021. 3, 6, 7, 9",
  "Yipin Zhou and Ser-Nam Lim. Joint audio-visual deepfakedetection. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1480014809, 2021.3, 7": "Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis,Subhransu Maji, and Karan Singh. Visemenet: Audio-drivenanimator-centric speech animation. ACM Transactions onGraphics (TOG), 37(4):110, 2018. 1 Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang,Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq:A large-scale video facial attributes dataset. In Europeanconference on computer vision, pages 650667. Springer,2022. 2 Wanyi Zhuang, Qi Chu, Zhentao Tan, Qiankun Liu, HaojieYuan, Changtao Miao, Zixiang Luo, and Nenghai Yu. Uia-vit:Unsupervised inconsistency-aware method based on visiontransformer for face forgery detection. In Computer Vision ECCV 2022: 17th European Conference, Tel Aviv, Israel,October 2327, 2022, Proceedings, Part V, page 391407,Berlin, Heidelberg, 2022. Springer-Verlag. 3"
}