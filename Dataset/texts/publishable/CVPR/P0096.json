{
  "Abstract": "Pixel-level Video Understanding in the Wild Challenge(PVUW) focus on complex video understanding.In thisCVPR 2024 workshop, we add two new tracks, ComplexVideo Object Segmentation Track based on MOSE datasetand Motion Expression guided Video Segmentation trackbased on MeViS dataset. In the two new tracks, we provideadditional videos and annotations that feature challengingelements, such as the disappearance and reappearance ofobjects, inconspicuous small objects, heavy occlusions, andcrowded environments in MOSE. Moreover, we provide anew motion expression guided video segmentation datasetMeViS to study the natural language-guided video under-standing in complex environments. These new videos, sen-tences, and annotations enable us to foster the developmentof a more comprehensive and robust pixel-level understand-ing of video scenes in complex environments and realisticscenarios. The MOSE challenge had 140 registered teamsin total, 65 teams participated the validation phase and 12teams made valid submissions in the final challenge phase.The MeViS challenge had 225 registered teams in total, 50teams participated the validation phase and 5 teams madevalid submissions in the final challenge phase.",
  "CVPR 2024 PVUW Workshop & Challenge organizers. All others arechallenge participants from the top-3 teams of MOSE and MeViS tracks": "to perform video segmentation is more reasonable andpractical for realistic applications.To advance the seg-mentation task from images to videos, we will presentnew datasets and competitions in this workshop, aiming atperforming the challenging yet practical Pixel-level VideoUnderstanding in the Wild (PVUW). In this year, we addtwo new tracks, Complex Video Object Segmentation Trackbased on MOSE and Motion Expression guided VideoSegmentation track based on MeViS . In the two newtracks, we provide additional videos and annotations thatfeature challenging elements, such as the disappearance andreappearance of objects, inconspicuous small objects, heavyocclusions, and crowded environments in MOSE. More-over, we provide a new motion expression guided videosegmentation dataset MeViS to study the natural language-guided video understanding in complex environments. Video object segmentation (VOS) focuses on segmentingspecific objects throughout an entire video sequence. Whilestate-of-the-art VOS methods have achieved impressiveresults (e.g., over 90% J &F) on existing datasets, thesedatasets typically feature targets that are salient, dominant,and isolated.Consequently, VOS in complex scenes re-mains underexplored.To address this and enhance real-world applicability, Ding et al. introduce a new datasetnamed coMplex video Object SEgmentation (MOSE), de-signed to study tracking and segmenting objects in complexenvironments. MOSEs standout feature is the inclusion ofcrowded and occluded scenes, where target objects oftenget obstructed or disappear in some frames. The experi-ments on MOSE demonstrate that current VOS algorithmsstruggle with object perception in complex scenes.Forexample, in the semi-supervised VOS setting, the top-performing state-of-the-art method achieves only 59.4%J &F on MOSE, significantly lower than their 90% J &Fperformance on DAVIS. These findings highlight the unre-",
  "arXiv:2406.17005v1 [cs.CV] 24 Jun 2024": ". Example Videos of coMplex video Object SEgmentation (MOSE) dataset . The standout feature of the MOSE dataset isits complex scenes, which include the disappearance and reappearance of objects, small and inconspicuous objects, heavy occlusions, andcrowded environments. The aim of the MOSE dataset is to foster the development of complex video understanding.",
  "solved challenges in complex scenes and indicate a need forfurther research to address these challenges": "Referring Video Object Segmentation (RVOS) focuseson segmenting specific objects throughout an entire videosequence based on sentences describing the target objects.Current referring video object datasets usually emphasizesalient objects and include language expressions with manystatic attributes, allowing target identification in a singleframe . These datasets neglect the role of motion inlanguage-guided video object segmentation. To explore thepotential of using motion expressions for object segmenta-tion in videos, Ding et al. introduce a large-scale datasetcalled MeViS, featuring numerous motion expressions toidentify target objects in complex environments.Theexperiments on MeViS show that current RVOS meth-ods struggle with motion expression-guided segmentation.The image-based referring segmentation methods cannot well understand the motion information invideos. These findings highlight the unresolved challengesin motion understanding under complex scenes and indicatea need for further research to address these challenges.",
  ". Tracks and Datasets": "The MOSE Track is based on the MOSE dataset ,which focuses on the task of Video Object Segmentation(VOS), especially in real-world complex and dense scenes.The dataset contains 2,149 videos and 5200 objects anno-tated with 431,725 segmentation masks. The dataset is splitinto three subsets, including training, validation, and test.The final testing data for the competition is built on partialof the test set. This part of data was private before, and wasmade open for the first time for the competition.One of the most unique features of the dataset is its focuson complex scenes in the task of VOS, such as heavy oc-clusions, crowded scenarios, and objects that disappear andreappear. It emphasizes the need for stronger associationalgorithms to track objects with changing appearances andpromotes research in occlusion understanding, attention tosmall and inconspicuous objects, and tracking in crowdedenvironments. The datasets complexity and length posesignificant challenges for current VOS methods, highlight-ing the need for advancements in complex video objectsegmentation.",
  "-(Baseline)67.374.871.012cqbu63.470.667.0": "The MeViS Track is based on the newly proposed large-scale motion expression-guided video segmentation dataset,MeViS . Methods are required to extract and segment thetarget object based on a expression that describes the motionof the object, in a long video. The dataset is build with 2,006videos. 8,171 objects are annotated with more than 443,000segmentation masks and 28,570 motion expressions. Theannotation data scale of MeViS is significantly larger thanother existing language-guided video segmentation dataset.Similarly with MOSE, partial of the test set of MeViS isused as the testing data for the competition. This part of datais also made public for the first time for the competition.The dataset focuses on describing the motion of objectsin videos through language expressions, emphasizing thesignificance of temporal properties. It challenges currentvideo object segmentation methods by requiring the iden-tification of objects based solely on their motion, withoutrelying on static attributes like color or category names.MeViS presents a complex environment where multipleobjects coexist with motion, making it difficult to identifytargets through saliency or category information alone, thuspushing the boundaries of language-guided video under-standing in dynamic scenarios.Competition Overview. Both tracks are hosted on theCodaLab platform .For valid and challenge phase,participants are only given input data, while the full ground-truth are kept private.All participants are required toregister on the platform for evaluation. Data for the vali-dation phase is always open for download and evaluation,but the data for the final challenge phase is only availablefor download and evaluation during the challenge phase of10 days. The number of submission for each team is notlimited for valid phase but is limited to 5 for test phase. To evaluate the performance of methods, both tracksemploys the standard and commonly recognized Jaccard(J ) metric for region similarity and F-measure (F) forcontour accuracy, as evaluation metrics, as in previousworks . The average of J and F is used asthe overall performance of the methods. The final rankingis based on the average of J and F (denoted as J &F) onthe test set.",
  "Affiliations:1Harbin Institute of Technology (ShenZhen)2Peng Cheng Laboratory3University of California at Merced": "To solve the problems of VOS, we propose a robustsemantic-aware and query-enhanced video object segmen-tation method.In this solution, we first introduce theproposed fusion block, which utilizes the semantic anddetailed information of the pretrained ViT models. Thishelps us deal with complex target appearance variance andID confusion between targets with similar appearances.In detail, we fuse the information of the cls token fromthe ViT to multi-scale features and conduct local fusionbetween frame patches and multi-scale features for detailedfusion.In addition, to ensure the target representationof the target queries, we develop a discriminative queryrepresentation module in the query transformer to capturethe local representation of the targets.",
  "Fusion Block": "Since the VOS task involves generic objects without classlabels, learning semantic representations directly from theVOS dataset during training is challenging. However, theCLS token in a pre-trained ViT captures semantic infor-mation from the entire image, providing a comprehensive,global representation of the image content. By integratingthe CLS token with multi-scale features generated fromCNN networks, we can acquire detailed semantic features",
  "Discriminative Query Generation": "We note that updating the target query memory directlywith entire object patches generated based on online pre-dicted masks is ineffective as the predicted masks oftencover background noise, reducing target distinctiveness andleading to accumulating errors over time.To propagatetarget queries effectively across frames, we update the targetqueries with the most distinctive feature of the target object.In detail, we select the discriminative feature of a targetobject by comparing the target query with every channelactivation in the correlated feature map of the target andtaking the most similar one. Based on the discriminativetarget feature generated from a new target sample, wecan update target queries by dynamically calculating therelationship between the salient query and salient pixelfeatures in an additive manner. The proposed discriminativequery generation scheme adaptively refines target querieswith the most representative features, which helps deal withthe challenges of dramatic appearance variations in long-term videos.",
  "Experiments": "Training.Our training settings are similar to Cuties.To enhance the performance of our model, we utilizethe MEGA dataset constructed by Cutie, which includesthe YouTubeVOS, DAVIS, OVIS, MOSE, and BURSTdatasets. We sample eight frames to train the model, andthree are randomly selected to train the matching process.For each sequence, we randomly choose at most threetargets for training. The point supervision in loss is adoptedto reduce the memory requirements. We train the model for 195k on the MEGA dataset. All our models are trained on8 x NVIDIA V100 GPUs and tested on an NVIDIA V100GPU.Inference. Our feature and query memory is updated every3rd frame during the testing phase. For longer sequences,we employ a long-term fusion strategy for updating. Toenhance storage quality, we skip frames without targets anddo not store them. The test input size contains two scales:720 for general size and 1080 for small targets. The finalscore is a version of multi-scale fusion.Evaluation Metrics. We use mean Jaccard J index andmean boundary F score, along with mean J &F to evaluatesegmentation accuracy.",
  "Results": "The proposed solution achieves 1st place on the complexvideo object segmentation track of the PVUW Challenge2024. In the five submissions, we find that some inferenceparameters influence the performance, which are the testsize, the memory interval, memory or not, the flip augmen-tation, and multi-scale fusion.In a conclusion, we propose a robust solution for thetask of video object segmentation, which helps the modelunderstand the semantic information of the targets andgenerate discriminative queries of the target. In the end, weachieve 1st place on the complex video object segmentationtrack of the PVUW Challenge 2024 with 84.45% J &F.The detailed version is under peer review. The code andfull version will be released as soon as possible.",
  "Baseline model": "To ensure good performance under challenges such as fre-quent disappearance-reappearance, heavy occlusions, smalland similar objects, we introduce Cutie as the strong base-line model, as shown in .Cutie stores a high-resolution pixel memory F and a high-level object memoryS. The pixel memory is encoded from the memory framesand corresponding segmented masks. The object memorycompresses object-level features from the memory frames.When a new query frame comes, it bidirectionally interactswith the object memory in a couple of object transformerblocks. Specifically, given the feature map of the queryframe, the pixel readout R0 is extracted by reading fromthe pixel memory with a sensory memory , then thepixel readout interacts with the object memory and a setof learnable object queries through bottom-up foreground-background masked cross attention.Next, the obtained high-level object query representation communicates backwith the pixel readout through top-down cross attention.The output pixel readout Rl and object queries Xl are sentto the next object transformer block. The final pixel readoutwill be combined with multi-scale features passed from skipconnections for computing the output mask in the decoder.Cutie enriches pixel features with object-level semantics ina bidirectional fashion, hence is more robust to distractionssuch as occlusion and disappearance.",
  "Data augmentation": "Like most state-of-the-art VOS methods, Cutie also adoptsa two-stage training paradigm. The first stage pretraininguses short video sequences generated from static images.Then main training is performed using VOS datasets in thesecond stage. However, the original Cutie fails to performwell when similar objects move in close proximity or suffersfrom serious motion blur.To solve the above problems, we conduct data augmen-tation to enhance the training of Cutie. First, we employthe universal image segmentation model Mask2Former to segment instance targets from the valid set and testset of MOSE. As shown in the left column of ,the segmented small objects represent typical object ap-pearances in MOSE, which is helpful for learning thesemantics of diverse objects in advance.Meanwhile, asshown in the middle column of , we convert theinstance annotations of COCO into independent binarymasks.Here we select object classes such as human,animal and vehicle that frequently occur in MOSE to reducediscrepancy between two data distributions. The acquireddata is used as extra pretraining data to enable more ro-bust semantics and improve discrimination ability againstdiverse objects of MOSE. Second, with the observation thatmotion blur is a significant challenge, we add motion blurwith random kernel sizes and angles to both the pretrainingand main training stages. An example of motion blur is . Examples of generated pretraining data and motion blur.Left: binary mask generated from the valid set and test set ofMOSE. Middle: binary mask generated from COCO, the masksof different classes are merged into one mask. Right: example ofmotion blur in the horizontal direction.",
  "Inference time operations": "TTA.We use two kinds of TTA: flipping and multi-scaledata enhancement.We only conduct horizontal flippingsince experiments show flipping in other directions is detri-mental to performance. In addition, we inference results onthe test set under three maximum shorter side resolutions:600p, 720p and 800p.The multi-scale results are thenaveraged to get the final result.Memory strategy.We find in experiments that largermemory banks and shorter memory intervals lead to betterperformance. Therefore, we adjust the maximum memoryframes Tmax to 18 and the memory interval to 1.",
  "Model": "Our approach is inspired by recent work on video objectsegmentation, particularly the Cutie framework,as shown in. Cutie operates in a semi-supervised video objectsegmentation (VOS) setting, where it takes a first-framesegmentation as input and processes subsequent framessequentially. Cutie encodes segmented frames into a high-resolution pixel memory F and a high-level object memoryS. These memories are used for segmenting future frames. When segmenting a new frame, Cutie first retrieves aninitial pixel readout R0 from the pixel memory using theencoded query features.This initial readout is typicallynoisy due to low-level pixel matching.To enhance this initial readout, Cutie enriches R0 withobject-level semantics using information from the objectmemory S and object queries X. This is done through anobject transformer with multiple transformer blocks. Thefinal enriched output, RL, is then passed to the decoder togenerate the output mask. Cutie introduces three main con-tributions: object-transformer, sec:masked-attention, andobject-memory. The Cutie-base model is based on thebase variant, utilizing ResNet-50 as the query encoderbackbone. It consists of C = 256 channels, L = 3 objecttransformer blocks, and N = 16 object queries. The queryand mask encoders are designed using ResNets. Followingprevious studies, we discard the final convolutional stageand employ the stride 16 feature.The object transformer block integrates both query FFNand pixel FFN components. The query FFN comprises a 2-layer MLP with a hidden size of 8C = 2048. Meanwhile,the pixel FFN utilizes two 3 3 convolutions with areduced hidden size of C = 256 to minimize computationaloverhead.The ReLU activation function is employedthroughout the network.",
  "Inference": "When testing, the input video is upscaled to a resolution of720p, which provides a higher density of pixel informationcompared to lower resolutions such as 480p.In the context of the memory frame encoding, we updateboth the pixel memory and the object memory every r-thframe. The default value of r is set to 3, following thesame configuration used in the XMem framework.Forsubsequent memory frames, we employ a First-In-First-Out (FIFO) strategy, which ensures that the most recentinformation is retained while older data is gradually phasedout. The choice of a predefined limit of Tmax = 15 for thetotal number of memory frames is a practical compromise.Maintaining a history of 15 frames is generally adequate foreffectively exploiting temporal correlations in VOS tasks.Based on these observations, we propose filtering affini-ties to retain only the top-k entries. To further manage thememory capacity, we apply top-k filtering with k = 60to the pixel memory. Setting top-k to 60 has the effect ofprioritizing the most relevant pixel memories based on theirattention scores, which is crucial for maintaining accuratesegmentation over time while preventing the memory frombeing overwhelmed with less significant information.In the final testing phase, we employed filpping Test-Time Augmentation (TTA), which is a strategy that en-hances the robustness and accuracy of predictions by incor-",
  "Encoder": ". Overall framework of Tapall.ai team method, 1st place solution for MeViS Challenge in CVPR 2024. Given an input video, wedivide all frames into N subsets via non-continuous sampling. Here we take two subsets as an example. They are marked with Blue andGreen boxes. Each subset is segmented individually, guided by the input text, and combined for the final results.",
  "Method: Our solution explores the value of static-dominantdata and frame sampling for the challenging MeViS bench-": "mark.As shown in , we consider MUTR asthe baseline architecture. With pre-trained parameters onthe Ref-COCO series and Ref-YouTube-VOS ,we fine-tune them on MeViS. Masks with one-to-more text-object pairs are considered as a whole to encourage adaptiveobject perception based on texts. To balance comprehensiveunderstanding and efficiency, we split long input videos intosub-videos via frame sampling. With these improvements,our solution ranks 1st (54.5 J &F) in the MeViS Track.Experiments on the MeViS valid set (48.6 J &F) in-dicate that the static-dominant data still contribute to thischallenging setting due to their sufficient and well-alignedobject masks and texts. In addition, ablations on samplingschemes reveal that there is much room for improvementin temporal modelling over long videos. Limited by com-putational resources, the temporal modules are trained withpseudo videos with less frames. During inference, however,videos have more temporal contexts.This inconsistencyleads to considering fewer frames (sampled) in temporalmodules outperform the one with all frames. We hope thesefindings are helpful for future research.",
  ". Overall framework of CASIA IVA team method, 2nd place solution for MeViS Challenge in CVPR 2024": "Method: As shown in We attempt to introduceinstance information to mitigate the issue of inconsistentpredicted results across multiple frames. Specifically, weemploy a video instance segmentation model to extract allinstance masks in the video. Next, we utilize a query withrandom initialization to aggregate all instances informationthrough our designed attention-based block including across-attention layer and a set of self-attention layers, FFNlayers. We employ MUTR as our basic model and uti-lize the query with instance information for query initializa-tion. Most previous work in RVOS sample frames arounda center point, allowing model to process part of video.In our solution, we sample frames in a manner of globalsampling.We divide the entire video into a few phasesand sample one frame in every phase to obtain a videoclip. To further improve performance, we employ HQ-SAM with VIT-L backbone utilizing default parameters forspatial refinement. Thanks to the superior performance ofDVIS , MUTR and HQ-SAM, our solution achievesa score of 49.92 J &F on the MeViS validation set and54.20 J &F on the MeViS test set, ranking 2nd Place forMeViS Track in CVPR 2024 PVUW Workshop: MotionExpression guided Video Segmentation.",
  "Affiliations:1School of Software, Shandong University": "As shown in , we propose using frozen pre-trained vision-language models (VLM) as backbones, witha specific emphasis on enhancing cross-modal feature in-teraction. Firstly, we use frozen convolutional CLIP backbone to generate feature-aligned vision and textfeatures.We do not fine tune the CLIP backbone topreserve pre-trained knowledge of vision-language associ-ation.This not only alleviates the issue of domain gap, but also greatly reduces training costs. Secondly, we addmore cross-modal feature fusion in the pipeline to enhancethe utilization of multi-modal information.We designthree cross-modal feature interaction module in the model,including cross-modal encoder, frame query decoder andvideo query decoder. These modules enhance video andtext features through simple cross-attention. Furthermore,inspired by LBVQ , we propose a novel video query ini-tialization method to generate higher quality video queries.Specifically, we perform bipartite matching and reorderframe queries, then aggregate them in a weighted mannerto initialize video queries. Without using any additionaltraining data, our method achieved 46.9 J &F onthe MeViS val set, 51.5 J &F on the MeViS test set andranked 3rd place for MeViS Track in CVPR 2024 PVUWworkshop: Motion Expression guided Video Segmentation.",
  "Cross-modal Encoder": "Given an (Video, Text) pair, we extract multi-framemulti-scale image features Fv with CLIP image encoder,and text features Ft with CLIP test encoder. Due to theuse of convolutional CLIP image encoder , we canextract multi-scale features from the outputs of differentblocks.After extracting vanilla video and text features,we fed them into a cross-modal encoder for cross-modalfeature fusion.The cross-modal encoder is built on topof the pixel decoder of Mask2Former , which leveragesthe Deformable self-attention to enhance image features.We add an image-to-text cross-attention and a text-to-imagecross-attention for feature fusion.These modules helpalign features of different modalities, ultimately obtainingenhanced image features Fv and text features Ft .",
  "Video Query Initializer": "After generating frame-level representation, the next stepis to generate video queries Qv RNvC to representthe entire video clip.Inspired by LBVQ , videoqueries have great similarity to frame queries per frame,and their essence is the fusion of frame queries. Instead ofthe simple text feature initialization strategy, we aggregateframe queries to achieve video query initialization. Firstly,the Hungarian matching algorithm is utilized to match theQf of adjacent frames. The purpose of this operation isto ensure that the instance order of each frame query isconsistent. Then, due to the varying importance of eachframe, we aggregate frame queries using learnable weights.The weights of different frames are maintained as a sum of1 through the Softmax function.",
  "Video Query Decoder": "After obtaining the initialized video queries, they are fedinto the video query decoder for layer by layer refinement.Video queries are fed into a text cross-attention layer tocombine text features, an query cross-attention layer tocombine frame queries features, a self-attention layer, andan FFN layer in each video query decoder layer. The videoqueries of the last layer will be dot multiplied with imagefeatures to generate the final mask.",
  ". Conclusion & Future Work": "This paper summarizes the methods and results ofPVUW 2024 challenge on complex video understanding,including MOSE challenge and MeViS challenge.Inthe challenges, we have seen a significant improvementin performance.In the MOSE challenge, most worksfocus on using memory to preserve long-term videoperception.In the MeViS challenge, there is a growinginterest in modeling language with temporal relationshipswithin videos.Despite these advancements, qualitativeresults indicate that accurately predicting masks remainsa challenge.Looking ahead, it is promising to considerintegrating SAM (Segment Anything Model ) andLarge Language Models (LLM) in future challenges. Wehope that the MOSE and MeViS challenges will continueto attract new researchers and participants to the field ofcomplex video understanding. Bin Cao, Yisi Zhang, Xuanxu Lin, Xingjian He, Bo Zhao,and Jing Liu.2nd place solution for mevis track in cvpr2024 pvuw workshop:Motion expression guided videosegmentation. arXiv preprint arXiv:2406.13939, 2024. 7",
  "Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, YuchenLiang, Jianchao Yang, and Thomas S. Huang. Youtube-vos:A large-scale video object segmentation benchmark. CoRR,abs/1809.03327, 2018. 3": "Zhensong Xu, Jiangtao Yao, Chengjing Wu, Ting Liu, andLuoqi Liu. 2nd place solution for mose track in cvpr 2024pvuw workshop: Complex video object segmentation. arXivpreprint arXiv:2406.08192, 2024. 4 Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, WeiZhang, Hongyang Li, Yu Qiao, Hao Dong, Zhongjiang He,and Peng Gao.Referred by multi-modality:A unifiedtemporal transformer for video object segmentation.InAAAI, 2024. 7, 8"
}