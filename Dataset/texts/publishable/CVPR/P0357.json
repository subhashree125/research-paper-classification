{
  "Abstract": "In Virtual Product Placement (VPP) applications, thediscrete integration of specific brand products into imagesor videos has emerged as a challenging yet important task.This paper introduces a novel three-stage fully automatedVPP system. In the first stage, a language-guided imagesegmentation model identifies optimal regions within im-ages for product inpainting. In the second stage, Stable Dif-fusion (SD), fine-tuned with a few example product images,is used to inpaint the product into the previously identifiedcandidate regions. The final stage introduces an AlignmentModule, which is designed to effectively sieve out low-quality images. Comprehensive experiments demonstratethat the Alignment Module ensures the presence of the in-tended product in every generated image and enhances theaverage quality of images by 35%. The results presentedin this paper demonstrate the effectiveness of the proposedVPP system, which holds significant potential for trans-forming the landscape of virtual advertising and marketingstrategies.",
  "(b) Inpainting": ". An illustration of the proposed VPP system with anAmazon Echo Dot device. The input background image is shownin (a), and the inpainted output image is shown in (b) where anAmazon Echo Dot device is placed on the kitchen countertop byautomatic identification of optimal location. Previous research underscores the impact of product place-ment within realms such as virtual reality and videogames .With the recent advancements in generativeAI technologies, the potential for product placement hasbeen further expanded through the utilization of diffusionmodels. Significant research has focused on the develop-ment of controlled inpainting via diffusion models, albeitlargely without an explicit emphasis on advertising applica-tions . However, these methods can be fine-tunedwith a small set of 4 to 5 product sample images to generatehigh-quality advertising visual content.In this paper, we propose a novel, three-stage, fullyautomated system that carries out semantic inpainting ofproducts by fine-tuning a pre-trained Stable Diffusion (SD)model . In the first stage, a suitable location is identifiedfor product placement using visual question answering andtext-conditioned instant segmentation. The output of thisstage is a binary mask highlighting the identified location.Subsequently, this masked region undergoes inpainting us-ing a fine-tuned SD model. This SD model is fine-tuned by",
  "arXiv:2405.01130v1 [cs.CV] 2 May 2024": "DreamBooth approach utilizing a few sample imagesof the product along with a unique identifier text prompt.Finally, the quality of the inpainted image is evaluated by aproposed Alignment Module, a discriminative method thatmeasures the image quality, or the alignment of the gen-erated image with human expectations. An illustration ofthe proposed VPP system is presented in with anAmazon Echo Dot device.Controlled inpainting of a specific product is a challeng-ing task. For example, the model may fail to inpaint theintended object at all. If a product is indeed introducedthrough inpainting, the product created may not be real-istic and may display distortions of shape, size, or color.Similarly, the background surrounding the inpainted prod-uct may be altered in such a way that it either meaning-fully obscures key background elements or even completelychanges the background image. This becomes especiallyproblematic when the background images contain humanelements, as models can transform them into disturbing vi-suals. As a result, the proposed Alignment Module is de-signed to address these complications, with its primary fo-cus being on the appearance, quality, and size of the gener-ated product.To exert control over the size of the generated product,morphological transformations, specifically erosion, and di-lation, are employed. By adjusting the size of the maskthrough dilation or erosion, the size of the inpainted productcan be effectively increased or decreased. This allows thesystem to generate a product of an appropriate size.In summary, the main contributions of this paper aretwofold.The first pertains to the design of a fully au-tomated Virtual Product Placement (VPP) system capableof generating high-resolution, customer-quality visual con-tent. The second involves the development of a discrimi-native method that automatically eliminates subpar images,premised on the content, quality, and size of the productgenerated.The remainder of this paper is organized as follows. In section 2 we will delve into the related literature, with a spe-cific emphasis on semantic inpainting methods utilizing dif-fusion models, and section 3 will highlight the broad contri-butions of the paper. Next, the proposed end-to-end pipelinefor automatic VPP will be discussed in section 4. This in-cludes a detailed examination of the three primary stagesof the solution, along with the three sub-modules of theAlignment Module. Thereafter, we will elucidate the exper-imental design and evaluation methodologies adopted andreport the corresponding results in section 5. Subsequently,deployment strategy and web application design will be ex-plained in section 6. Finally, the paper will conclude with anoutline of the identified limitations of our proposed method-ology in section 7, complemented by a discussion on poten-tial avenues for future research.",
  ". Related Works": "Recently, there has been significant progress in developingsemantic or localized image editing using diffusion mod-els - largely without an explicit focus on digital market-ing. Nevertheless, new generative AI approaches promisesignificant advances in VPP technology. For instance, inBlended Diffusion , the authors proposed a method of lo-calized image editing using image masking and natural lan-guage. The area of interest is first masked and then modifiedusing a text prompt. The authors employed a pre-trainedCLIP model along with pre-trained Denoising Diffu-sion Probabilistic Models (DDPM) to generate naturalimages in the area of interest.Similar to Blended Diffusion, Couairon et. al. pro-posed a method of semantic editing with a mask using adiffusion model. However, instead of taking the mask fromthe user, the mask is generated automatically. Neverthe-less, a text query input from the user is utilized to generatethe mask. The difference in noise estimates, as determinedby the diffusion model based on the reference text and thequery text, is calculated. This difference is then used toinfer the mask. The image is noised iteratively during theforward process and in the reverse Denoising Diffusion Im-plicit Model (DDIM) steps, the denoised image is in-terpolated with the same step output of the forward processusing masking.Paint by Word proposed by Bau et. al. , is also similar,however instead of a diffusion model they utilized a Gener-ative Adversarial Networks (GAN) with a mask for se-mantic editing guided by text. On the other hand, Imagic also performs text-based semantic editing on images us-ing a diffusion model but without using any mask. Theirapproach consists of three steps. In the beginning, text em-bedding for a given image is optimized. Then the genera-tive diffusion model is optimized for the given image withfixed-optimized text embedding. Finally, the target and op-timized embedding are linearly interpolated to achieve inputimage and target text alignment. Likewise, a semantic edit-ing method using a pre-trained text-conditioned diffusionmodel focusing on the mixing of two concepts is proposedby . In this method, a given image is noised for severalsteps and then denoised with text condition. During the de-noising process, the output of a denoising stage is also lin-early interpolated with the output of a forward noise mixingstage.Hertz et. al. took a different approach to semanticimage editing where text and image embeddings are fusedusing cross-attention. The cross-attention maps are incor-porated with the Imagen diffusion model . However,instead of editing any given image, their approach edits agenerated image using a text prompt which lacks any inter-est when VPP is concerned. Alternatively, Stochastic Dif-ferential Edit (SDEdit) synthesizes images from stroke paintings and can edit images based on stroke images. Forimage synthesis, coarse colored strokes are used and forediting, colored stroke on real images or image patches ontarget images is used as a guide. It adds Gaussian noiseto an image guide of a specific standard deviation and thensolves the corresponding Stochastic Differential Equations(SDE) to produce the synthetic or edited image.To generate images from a prompt in a controlled fash-ion and to gain more control over the generated image, Liet. al proposed grounded text-to-image generation (GLI-GEN) . It feeds the model the embedding of the guidingelements such as bounding boxes, key points, or semanticmaps. Using the same guiding components, inpainting canbe performed in a target image.DreamBooth fine-tunes the pre-trained diffusionmodel to expand the dictionary of the model for a specificsubject. Given a few examples of the subject, a diffusionmodel such as Imagen is fine-tuned using random sam-ples generated by the model itself and new subject imagesby optimizing a reconstruction loss. The new subject im-ages are conditioned using a text prompt with a unique iden-tifier. Fine-tuning a pre-trained diffusion model with a newsubject is of great importance in the context of VPP. There-fore, in this paper DreamBooth approach is utilized to ex-pand the models dictionary by learning from a few sampleimages of the product.",
  ". Contributions": "In this paper, a method of automated virtual product place-ment and assessment in images using diffusion models isdesigned. Our broad contributions are as follows:1. We introduce a novel fully automated VPP system thatcarries out automatic semantic inpainting of the productin the optimal location using language-guided segmen-tation and fine-tuned stable diffusion models.",
  "Volume Score": "Stage 2 Stage 1 Stage 3 . The block diagram of the proposed solution for the VPPsystem where each of the three stages is distinguished by variedcolor blocks. In stage 1, a suitable placement for product inpaint-ing is determined by creating a mask using CLIPSeg and VILTmodels.Next, in stage 2, semantic inpainting is performed inthe masked area using the fine-tuned DreamBooth model. Finally,stage 3 contains the cascaded sub-modules of the Alignment Mod-ule to discard low-quality images.",
  ". Proposed Method": "For semantic inpainting, we utilized the DreamBooth algo-rithm to fine-tune stable diffusion using five representa-tive images of the product and a text prompt with a uniqueidentifier. Even with a limited set of five sample images,the fine-tuned DreamBooth model was capable of generat-ing images of the product integrated with its background.Nevertheless, when inpainting was conducted with thisfine-tuned model, the resulting quality of the inpaintedproduct was significantly compromised.To enhance thequality of the product in the inpainted image, we augmentedthe sample images through random scaling and randomcropping, consequently generating a total of 1,000 productimages used to fine-tune SD.",
  ". Product Localization Module": "The proposed VPP system operates in three stages.Acore challenge in product placement lies in pinpointing asuitable location for the item within the background. Inthe first stage, this placement is indicated via the genera-tion of a binary mask. To automate this masking process,we leveraged the capabilities of the Vision and LanguageTransformer (ViLT) Visual Question Answering (VQA)model in conjunction with the Contrastive Language- Image Pretraining (CLIP) -based semantic segmenta-tion method, named CLIPSeg . Notably, each producttends to have a prototypical location for its placement. Forexample, an optimal location for an Amazon Echo Dot de-vice is atop a flat surface, such as a desk or table. Thus,by posing a straightforward query to the VQA model, suchas Which object in the image has a flat surface area?, wecan pinpoint an appropriate location for the product. Sub-sequently, the identified locations name is provided to theCLIPSeg model, along with the input image, resulting in thegeneration of a binary mask for the object.",
  ". Product Inpainting Module": "In the second stage, the input image and the generated bi-nary mask are fed to the fine-tuned DreamBooth model toperform inpainting on the masked region. Product inpaint-ing presents several challenges: the product might not man-ifest in the inpainted region; if it does, its quality could becompromised or distorted, and its size might be dispropor-tionate to the surrounding context. To systematically detectthese issues, we introduce the third stage: the AlignmentModule.",
  ". Product Alignment Module": "The Alignment Module comprises three sub-modules: Con-tent, Quality, and Volume. The Content sub-module servesas a binary classifier, determining the presence of the prod-uct in the generated image. If the products probability ofexistence surpasses a predefined threshold, then the Qual-ity score is calculated for that image. This score evaluatesthe quality of the inpainted product in relation to the sampleimages originally used to train the SD model. Finally, if theimages quality score exceeds the set quality threshold, theVolume sub-module assesses the products size in propor-tion to the background image. The generated image will besuccessfully accepted and presented to the user only if allthree scores within the Product Quality Alignment Modulemeet their respective thresholds.Within the Content module, an image captioning model is employed to generate a caption, which is then refinedby incorporating the products name. The super-class nameof the product can also be utilized. Both the captions andthe inpainted image are fed into the CLIP model to derive aCLIP score. If the modified caption scores above 70%, itsinferred that the product exists in the inpainted image. TheQuality module contrasts the mean CLIP image features ofthe sample images with the CLIP image feature of the gen-erated image. The greater the resemblance of the inpaintedproduct to the sample images, the higher the quality score.A threshold of 70% has been established. The Volume mod-ule finally gauges the size of the inpainted product. Thegenerated image is processed through the CLIP model, ac-companied by three distinct textual size prompts. Given that a small dog sitting on a desk next to a computer a small dog sitting on a desk next to a computer with an echo dot",
  "(c) Volume Sub-module": ". Block diagram of each of the components of the Align-ment Module. The Content sub-module is built using a pre-trainedcaption generator and CLIP models shown in (a). The generatedcaption is fine-tuned by adding the name of the intended product tothe caption. For the Quality sub-module, the image features of thesame CLIP model are utilized shown in (b). Finally, in the Volumesub-module, the same CLIP model with three different size textprompts is used shown in (c). size perception can be subjective and varies based on cam-era proximity, a milder threshold of 34% (slightly above arandom guess) has been selected. The comprehensive blockdiagram of the proposed VPP system is illustrated in Fig-ure 2, with the three stages distinguished by varied colorblocks. The block diagrams for each sub-module can befound in . The Volume sub-module provides insights regarding thesize of the inpainted product. To modify the products size,the masks dimensions must be adjusted. For this task, mor-phological transformations, including mask erosion and di-lation, can be employed on the binary mask. These trans-formations can either reduce or augment the mask area, al-lowing the inpainting module to produce a product imageof the desired size. The relationship between alterations inthe mask area and the size of the inpainted product acrossvarious erosion iterations is depicted in . Approxi-mately, 25 iterations of erosion consume around 3 millisec-onds, making it highly cost-effective. . Application of erosion to the mask where a kernel of size(5 5) is used for 0, 10, 20, and 25 iterations shown in the figureconsecutively. The resulting output is presented at the bottom ofthe corresponding mask to show the size reduction of the generatedproduct in the output image.",
  ". Experimental Results": "Experiments were conducted to evaluate the performance ofthe proposed VPP system. For these experiments, five sam-ple images of an Amazon Echo Dot were chosen. 1, 000augmented images of each product created from these fivesample images were used to fine-tune the DreamBoothmodel using the text prompt A photorealistic image of asks Amazon Alexa device. The model was fine-tuned for1, 600 steps, employing a learning rate of 5 106, and abatch size of 1.The fine-tuned model can inpaint products into themasked region. However, issues such as lack of product ap-pearance, poor resolution, and disproportionate shape per-sist. The goal of the proposed Alignment Module is to auto-matically detect these issues. If identified, the problematicimages are discarded, and a new image is generated fromdifferent random noise. Only if a generated image meetsall the modules criteria it is presented to the user. Other-wise, a new image generation process is initiated. This loopcontinues for a maximum of 10 iterations.",
  ". Assessing Alignment Module": "To assess the effectiveness of the Alignment Module, im-ages were generated both with and without it. For each sub-module, as well as for the overall Alignment Module, 200images were generated: 100 with the filter activated and 100without (referred to as the Naive case).To prevent bias, all images were given random namesand were consolidated into a single folder.These im-ages were also independently evaluated by a human, whosescores served as the ground truth. This ground truth in-formation was saved in a separate file for the final evalua-tion, which followed a blindfolded scoring method. All theexperiments were also repeated for another product namedLupure Vitamin C.",
  "The evaluation and scoring method of each of the sub-modules of the Alignment module is described in the con-secutive segments": "Content ScoreFor the image content score, images arecategorized into two classes: success if the product ap-pears, and failure otherwise. When the content moduleis utilized, the Failure Rate (FR), defined as the ratio ofFailure to Success, is below 10% for both of the products. Quality ScoreFor the quality score, images are ratedon a scale from 0 to 10: 0 indicates the absence of aproduct, and 10 signifies a perfect-looking product. Toevaluate in conjunction with the CLIP score, both theMean Assigned Quality Score (MAQS) and Mean Qual-ity Score (MQS) are calculated. MAQS represents theaverage score of images labeled between 0 and 10, whileMQS is the output from the quality module, essentiallyreflecting cosine similarity. Volume ScoreFor the volume module, images are alsorated on a scale from 0 to 10: 0 for a highly unrealis-tic size, and 10 for a perfect size representation. Whenevaluating the volume module, the content module is notutilized. Since the size score necessitates the presenceof a product, images without any product are excludedfrom this evaluation. To gauge performance, the MeanAssigned Size Score (MASS) is calculated in addition tothe CLIP score.",
  "Overall Results": "The results of individual evaluations are presented in Ta-ble 1. It can be observed from this table that using anyof the sub-modules consistently produced better outcomescompared to when no filtering was applied across variousmetrics. The results of the comprehensive evaluation, en-compassing all sub-modules, can be found in . . Individual evaluation of content, quality, and volume sub-modules within the overall Alignment Module. Naive represents theoutputs without any filtering sub-modules. Content classifies the presence of the product in the generated images. Quality measures theproximity of the generated product to the sample product images used to fine-tune the diffusion model. Finally, Volume identifies the sizecategory of the product.",
  "FR14.94%0.0%MQS0.81 0.130.86 0.04": ". Comparison of the proposed method with and without using the Alignment Module in addition to the Paint-By-Example (PBE) inpainting model. The Naive performance represents the generated output without applying the Alignment Module. The Alignmentcolumn represents the generated outputs where three cascaded filtering sub-modules are used, i.e., the Alignment Module.",
  "(d)": ". Inpainted product image of Paint-by-Example (PBE).PBE generates high-quality images which explains the higherCLIP score in the case of Lupure Vitamin C. However, the in-painted product does not look similar to the desired product at allresulting in very poor mean assigned quality and size scores. Out-put images for Amazon Echo Dot is shown in (a) and (b), and forLupure Vitamin C is shown in (c) and (d). . Empirical performance of Alignment Module for Ama-zon Echo Dot. Noticeably, no output is generated without anyproduct when the Alignment Module is employed. Moreover, themean quality score has increased from 4.65 to 6.31.",
  ". Comparison with Paint-By-Example": "The proposed method is compared with the Paint-By-Example (PBE) inpainting model and showsthe performance comparison of the proposed method alongwith PBE. PBE can generate very high-quality images,however, the inpainted product in the generated image doesnot look alike the desired product at all as shown in resulting in very poor MAQS and MASS. Whereas the in-painted product of our proposed method resembles much ofthe original product shown in Figure .",
  ". Product API": "The location identifier, fine-tuned model, and AlignmentModule are combined to develop an easy-to-use VPPStreamlit web app 1. This app is hosted on Amazon Sage-maker using an ml.p3.2xlarge instance, which is a singleV100 GPU with 16GB of GPU memory. The demo appsinterface is illustrated in . In the top-left Imagesection, users can either upload their own background im-age or choose from a selection of sample background im-ages to generate an inpainted product image.The web app provides extensive flexibility for tuningthe parameters of the Alignment Module so that users cancomprehend the effects of these parameters. In the seedtext box, a value can be input to control the system out-put. The segmentation threshold for CLIPSeg defaults to0.7, but users can refine this value using a slider. Within theMask Params section, the number of dilation and erosioniterations can be set and visualized in real-time.The filter, represented by the Alignment Module, can betoggled on or off. The Max Attempt slider determines thenumber of regeneration attempts if the model doesnt pro-duce a satisfactory output. However, if a seed value is spec-ified, the model will generate the output only once, regard-less of the set value. Lastly, in the Filter Params section,users can fine-tune the threshold values for each sub-moduleof the Alignment Module, specifically for content, quality,and volume.The show stats button beneath the input image displaysthe mask alongside details of the model outputs.Thesedetails include the seed value, placement, generated andmodified captions, and the content, quality, and volume/sizescores. By visualizing the mask and its area, users can applyerosion or dilation to adjust the products size. The defaultthreshold values for content, quality, and volume are 0.7,0.7, and 0.34, respectively. While these values can be ad-justed slightly higher, its recommended to also set the MaxAttempt to 10 in such cases. A higher threshold means thatthe generated output is more likely to fail the criteria set bythe Alignment Module.",
  ". Future Considerations for Product Scalability": "Fine-tuning stable diffusion using DreamBooth can take upto 30 minutes, depending on dataset size, image resolution,and extent of training. When considering a customer withhundreds or thousands of products, this process could takedays to complete model training across different products. Our pipeline is deployed on Amazon SageMaker, a man-aged service that supports the automatic scaling of de-ployed endpoints. This service can dynamically accommo-date large computational needs by provisioning additionalinstances as required. As such, fine-tuning 100 SD mod-els for 100 different products would still only take about 30minutes if 100 instances were utilized in parallel. The fine-tuned models are stored in an Amazon S3 (Sim-ple Storage Service) bucket, with each model being 2.2 GBin size. Consequently, 100 fine-tuned models would occupyapproximately 220 GB of storage space. A pertinent ques-tion arises: Can we strike a space-time trade-off by traininga single model with a unique identifier for each product? If this is feasible, the space requirement would be re-duced to a consistent 2.2 GB. However, that one modelwould need more extensive training - specifically trainingsteps would increase by a factor of 100 for 100 products,thereby lengthening the computation time. This approachremains untested and warrants future exploration .",
  ". Conclusion": "In this paper, we present a novel, fully automated, end-to-end pipeline for Virtual Product Placement. The pro-posed method automatically determines a suitable locationfor product placement into a background image, performsproduct inpainting, and finally evaluates image quality toensure only high-quality images are presented for the down-stream task. Using two different example products, experiments wereconducted to evaluate the effectiveness of the proposedpipeline, the performance of the individual sub-modules,and the overarching Alignment Module.Notably, whenupon employing the Alignment Module, the Failure Ratio(FR) plummeted down to 0.0% for both investigated prod-ucts. Additionally, images produced with the AlignmentModule achieved superior CLIP, quality, and size scores. Qualitatively, the produced images present a clean andnatural semantic inpainting of the product within the back-ground image. The accompanying web application facil-itates pipeline deployment by enabling image generationthrough a user-friendly interface with extensive image fine-tuning capabilities. The high-quality integration of productsinto images underscores the potential of the proposed VPPin the realms of digital marketing and advertising.",
  "Amazon Echo Dot": "Background and Inpainted Images Lupure Vitamin C Background and Inpainted Images . Qualitative results of the proposed VPP system. Experiments are performed using two different products, Amazon Echo Dotas shown on top, and Lupure Vitamin C as shown on bottom. The original training images are shown on the left, and then the pairs ofbackground and inpainted output images are presented side by side. . The interface of the VPP web app demo was built using Streamlit hosted in Amazon SageMaker. The uploaded backgroundimage is shown under the title Input Image and the inpainting image with an Amazon Echo Dot is shown under the title Output Image.Moreover, the generated mask produced by the location identifier and the other intermediate details of the proposed VPP system is alsopresented in the interface. Omri Avrahami, Dani Lischinski, and Ohad Fried. Blendeddiffusion for text-driven editing of natural images. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1820818218, 2022. 1, 2",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural informationprocessing systems, 33:68406851, 2020. 2": "Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, HuiwenChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 60076017, 2023. 1, 2 Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region su-pervision. In International Conference on Machine Learn-ing, pages 55835594. PMLR, 2021. 3 Nupur Kumari, Bingliang Zhang, Richard Zhang, EliShechtman, and Jun-Yan Zhu. Multi-concept customizationof text-to-image diffusion. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 19311941, 2023. 7 Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.Gligen: Open-set grounded text-to-image generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2251122521, 2023. 1, 3",
  "ments. Journal of Promotion Management, 16(1-2):2538,2010. 1": "Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guidedimage synthesis and editing with stochastic differential equa-tions. arXiv preprint arXiv:2108.01073, 2021. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2, 4 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. Dreambooth: Finetuning text-to-image diffusion models for subject-drivengeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2250022510, 2023. 2, 3 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. 2, 3"
}