{
  "Abstract": "Event camera has significant advantages in capturingdynamic scene information while being prone to noise in-terference, particularly in challenging conditions like lowthreshold and low illumination. However, most existing re-search focuses on gentle situations, hindering event cameraapplications in realistic complex scenarios. To tackle thislimitation and advance the field, we construct a new pairedreal-world event denoising dataset (LED), including 3K se-quences with 18K seconds of high-resolution (1200*680)event streams and showing three notable distinctions com-pared to others: diverse noise levels and scenes, larger-scale with high-resolution, and high-quality GT. Specifi-cally, it contains stepped parameters and varying illumi-nation with diverse scenarios.Moreover, based on theproperty of noise events inconsistency and signal eventsconsistency, we propose a novel effective denoising frame-work(DED) using homogeneous dual events to generate theGT with better separating noise from the raw.Further-more, we design a bio-inspired baseline leveraging Leaky-Integrate-and-Fire (LIF) neurons with dynamic thresholdsto realize accurate denoising.The experimental resultsdemonstrate that the remarkable performance of the pro-posed approach on different datasets.The dataset and codeare at",
  ". Introduction": "Event cameras possess unique imaging advantages andare demonstrating tremendous potential for various applica-tions . Unlike traditional frame cameras usingintegration sampling , event camera is differential sam-pling , evading the restriction of exposure period. How-ever, due to this sampling mechanism, random fluctuationsin analog signals can easily form noise events. In contrastto the dominance of signal components in images, the noiseand signal in event cameras are encoded with same mag-nitude, easily disturbing inherent structural features, suchas lane lines shown in . BA noise detrimentally im-pact on subsequent tasks like reconstruction , mo-tion estimation and event-based object detection .Therefore, event denoising becomes a fundamental issue,notably in the booming event-based vision community.Paired data is a key issue for event denoising in the deep-learning era, thus an intuitive idea is to construct paired datathrough simulators . However, the image-basedsimulation cannot be equivalent to reality because of the do-main gap. To obtain the clean part from the real-worldevent, multi-modal data was incorporated to help pairedevent generation. APS images gradient combined with IMUare used to indicate the event probability within frame pe-riod . By jointly estimating the motion parameters with",
  ". Summary of existing event denoising datasets": "APS images and event streams , the temporal mis-match can be alleviated. Constructing a fixed-track setup, the events under various illuminations can be referredto APS images in good lighting. Essentially, these meth-ods align with events via transformed multi-mode features.Unfortunately, this heterogeneous aided information cannotfully match with the event, and their effectiveness is un-available in situations such as low light and motion blur.Hence, few datasets have considered practical scenes whereBA noise is easily deteriorated by camera setting and illumi-nation, appearing as complex and nonuniform distributions.To address this issue, we construct a large-scale, high-quality paired dataset LED for event denoising. LED hasthree distinct advantages. Firstly, it contains diverse noiselevels, covering complex situations under various camerasettings and illumination. Secondly, LED includes such asoutdoor and indoor scenarios, with abundant objects. Lastbut not least, LED is collected with the current largest-scale and high-resolution. Moreover, inspired by the multi-sampling images denoising methods , weare the first to explore event multi-sampling for denoising.Based on the homogeneous data, we proposed a dual-eventsdenoising framework that can generate higher-quality GTby more accurately separating BA noise from the raw.The contributions of this paper can be summarized as:(1) We provide a large-scale real-world paired dataset forevent denoising.So far, LED is the largest paired real-world event denoising dataset (3000 sequences) and high-resolution (1200*680) with diverse noise levels and scenes.(2) We propose a novel dual events denoising frameworkDED based on noise inconsistency. We provide detailedanalysis to show that DED can better separate mixed sig-nals and noisy event streams across various camera thresh-old parameters, light conditions and motion patterns.(3) We introduce a novel baseline DTSNN for event de-noising based on the spiking neural network, which utilizesthe learnable dynamical spike threshold of the LIF neuronto accurately denoising. Extensive experiments on differentreal datasets verify the superiority of the proposed method.",
  "pared to the task-specific ones in the event-based commu-nity. In , we provide an all-sided summary of exist-ing datasets, highlighting the scarcity of real paired datasets": "To address the lack of explicit labels, some paireddatasets can be conveniently generated via simulators suchas DVSCLEAN and DND21 . However, due tothe gap between simulation and real, these simulators failto fully reflect the actual situation. Other works focus onreal paired event denoising data. DVSNOISE20 cap-tures data in 16 pure background scenes. RGB-DAVIS provides 20 indoor/outdoor sequences under good lightingconditions.ED-KoGTL constructs indoor data withfour illumination levels under constant trajectory. Thesepaired datasets have achieved favorable results, primarilyattributable to the gradient cues extracted from APS images.However, the heterogeneity between images and events lim-its the availability of diverse lighting conditions, motionpatterns, and scenarios for constructing paired data. Event-based Denoising. Research on event denoising canbe categorized into two approaches: filtering-based meth-ods and deep learning methods. The former relies on man-ual priors to design discriminative models for noise re-moval, such as utilizing density distinction or motion continuity distinction and motioncompensation with CM framework in the spa-tiotemporal domain.However, the validity of this priorknowledge is sensitive to different signal/noise event dis-tributions, limiting their denoising accuracy. For learning-based methods, such as the CNN-based EDNCNN ,the PointNet-based AEDNet and GNN-transformerwere proposed successively. Additionally, the event denois-ing and super-resolution tasks are combined in . Multi-Sampling Image Denoising.The multi-samplingprinciple for image denoising has been extensively studied.Techniques like long exposure time or burst captures bene-fit the direct acquisition of clear data with increased SNR instatic scenes. Further, the multi-sampling was explored tothe paradigm of indirectly recovering noise-free data. Notonly using paired noisy observations to learn the mappingfrom different noisy samples to clear counterpart ,but also employing spatial sub-sampling on a single noisyimage to learn the mapping from adjacent noisy samples toGT . Evidently, these denoising approaches imply",
  "4th level(Th -30%)": ". Illustration of the proposed dataset LED. (a) Distribution of noise level and scene of the proposed dataset. (b) Our proposedLED outperforms others in terms of sequences, capture, and resolution (Circles with numbers to indicate). (c) LED collects diverse eventstreams across various conditions of illumination, depth of field, and target scale.",
  ". Dataset Statistics and Features": "The primary factors affecting BA noise in event camerascan be categorized into intrinsic camera settings and extrin-sic lighting conditions. By setting different camera thresh-olds, we can selectively record events with varying noiselevels. Generally, a lower threshold means higher sensitiv-ity but also increases the likelihood of BA noise. In con-trast to fixed parameters in other datasets, we use four levelthresholds to collect events, starting from the default valueof 0% and gradually decreasing to -10%, -20%, and -30%, as shown in (a). To cope with the challenges posedby complex lighting conditions, data collection is from day-time to nighttime, encompassing a transition from high tolow illumination and a mixture of natural light with artifi-cial light sources. Thus, the impact of external conditionson the event stream distribution can also be controlled.Due to the difficulty of constructing real paired datasets,the development of existing datasets remains limited, asshown in . In this work, we used a vehicle-mountedor pan-tilt equipped with event cameras featuring a 16mmlens. Totally, we collected approximately 5 hours of eventstream, including 3K sequences. The composition of noiselevels and scenes is depicted in (a), with roughlyequal amounts collected during daytime and nighttime.Nearly 20 typical scenes were captured, including urbanbuildings, traffic roads, natural scenery, indoor exhibits, etc.Our dataset goes beyond just the number of sequencesand data volume, primarily aiming to consider the intricateinfluences of relevant factors on noise. Thus, we endowLED with diversity to reflect the impact of objects and en-vironments on the event, which helps cover the cases ofdensely and non-uniformly distributed noisy situations inreality both spatially and temporally. In (c), we ex-hibit that LED diversity is not only embodied in the rich-ness of noise and illumination levels but also features suchas depth of field and scene semantics. Meanwhile, high-resolution imaging aids the event cameras in capturing spa-tial information more precisely. Unfortunately, most exist-ing ones with low resolution may hinder advanced visualtasks. Therefore, we collected data with 1200*680 resolu-tion(after cropping), achieving high spatiotemporal resolu-tion. Additionally, LED includes abundant objects of var-ious scales, including aerial targets, pedestrians, and vehi-cles, making it well-suited for downstream tasks.",
  "(a) Ideal intensity input (b) Circuit dual-sampled simulation (c) Realistic dual-Sampled events (d) Statistic of dual-sampled events": ". Illustration of event camera dual-sampling analysis: (a) Given an ideal intensity input, which includes varying and steady stages,generates a series of signal events. (b) The twice samplings of the same input in the circuit model, both generate additional BA noise inpreviously steady stages, resulting in consistency discrepancies between noise events and signal events. (c) The visualization results of theactual dual-sampled events cumulative frame demonstrate the misalignment of inconsistent dual-sampled noise events and the alignmentof coexisting signal events (two color groups indicating the respective polarities of the dual events). (d) The statistical results of the twotests also prove a low overlap rate between dual-sampled event pixels and a much higher one where signal events are present.",
  "binarygrid": ". Overview of the dual events denoising framework. (a)Our collection device consists of two identical EVK4s forminga co-axial system with a 1:1 beamsplitter. (b) We first performspatial similarity processing to retain the consistent parts, followedby sequentially spatiotemporal correlation constraints to removethe residual small amount of noise event from the previous step.",
  ". Dual Events Denoising Framework": "Given a noisy event stream, the key lies in properlyobtaining paired GT. In this section, we explore utilizingmulti-sampling to achieve event stream denoising.Background. Multi-sampling denoising methods are com-monly used in image denoising. For frame images, aver-aging the results of temporal multi-samples can approxi-mate the true value for each pixel owing to multi-samplessmoothing effect on random noise components, as shown in (a). For event stream, due to their dynamic samplingand the binary values, temporal multi-samples may containboth signal events and BA noise, as illustrated in (b),directly taking the average of multi-samples cannot yieldtrue signal value. Although the noise forms differ betweenthe two modalities, they ultimately originate from the in-herent noise in the circuitry analog signal, while resamplingfundamentally helps suppress the random noise source.Analysis. This concept inspired us to utilize multi-sampling for event denoising, particularly when considering the bi-nary state of events (presence or absence) where just twosamplings are required to potentially identify noise. Thus,we initially investigated the noise inconsistency at the pixellevel. Based on the DVS working principle , we builda circuit model in the Cadence platform to simulatethe dual-sampling with twice identical intensity input. Thenoisy analog signal converted from input generates BAnoise at the originally steady stage compared to the idealcase in (a). Clearly, the two sets of noise exhibitinconsistency, while the signals demonstrate high consis-tency, causing distinct alignment differences between thetwo cases as shown in the close-up of (b). To further explore the inter-class differences betweennoise and signal events in real-world dual-sampled eventsstream, we established a collection device as shown in (a), to construct genuinely spatiotemporal dual-sampling.Two experimental conditions to validate the aforesaid cir-cuit simulation phenomenon are a stationary camera anda moving one respectively. In the former, pure BA noiseevents was obtained, while the latter captured mixed eventscontaining both noise and signal. In static tests, althoughat the highest noise level (-30% threshold), both two setsof events exhibit significant spatiotemporal disparity. Asobserved in the right zoomed-in patches at the upper row in (c), the global spatial misalignment of dual BA noisesevents was evident. The overlap rate of the pixels triggeringevent is also to be less than 1%, as shown in the upper-row histogram of (d). Not unexpectedly, in dynamictests, regions with BA noise events remain inconsistency,while the coexistent signal events showing spatiotemporalalignment as depicted in lower-row counterpart, leading toa dramatic increase in overlap rate compared to the pure BAnoise cases, as shown in the bottom histogram of (d).",
  "ONOFF": ". Analysis of different event denoising results on our dataset. From left to right, the first column is the raw events, and the remainingfive columns represent different methods, namely DWF, STDF, TimeSurface, EvFlow, and the proposed DED. From top to bottom, the firstrow shows the denoised results, the second row is the residual noise, the third row denotes the statistical distribution of the denoised resultsand the last row represents the intensity image reconstruction corresponding to the zoomed region denoised events. Formulation. These results support the insight that the con-sistency discrepancies between dual-sampled events couldnaturally help distinguish noise. Therefore, we develop adenoising framework with the dual events stream, calledDED. In DED framework, given the spatiotemporal syn-chronized dual event streams Y1 and Y2, they are composedof common latent signal X and respective noise streams N1and N2. The noise model can be defined as follows:Y1 = X + N1 , Y2 = X + N2.(1)Because of the consistency of signal events and the incon-sistency of noise events in Y1 and Y2, after binary grid oper-ation of event stream according to certain temporal windowt, we can perform spatial similarity to process them:X = X + N1 X + N21 ,(2)where denotes the Hadamard product used to obtain thesimilar part X of two groups of raw event binary frames.Due to the probability of a few noises occurring simultane-ously in dual-sampling, X may still contain some much-isolated noise compared to the raw. As shown in (b), for finer denoising, we further utilize the spatiotem-poral correlation of the signal event stream to remove theresidual noise from the previous step. Specifically, we ac-cumulate the events from the relevant spatiotemporal rangein corresponding X within several consecutive t to forma spatiotemporal relationship set. The presence of these fewisolated noise events, which are randomly triggered in thespatiotemporal domain, disrupts the inherent spatiotempo-ral correlation of signal reflecting a certain motion model.Therefore, to leverage this spatiotemporal continuity, wetransform it into the following minimization problem:",
  "eti+1x etix ,tint,x,X,(3)": "where etix denotes the ith occurring event according to thesorted timestamp, x means the event spatial coordinates, nis the number of selected consecutive time windows, rep-resents the candidate spatial neighborhood, and N is totalevents number in . Additionally, the aforesaid processingis all performed on the two channels of event polarity.",
  ". Evaluation and Discussion": "The quality of GT is critical for a real paired eventdataset. However, it is challenging to objectively evaluatethe denoising results since directly obtaining clean data isunpractical. To fairly assess the denoising effects of differ-ent methods, we conducted a comprehensive evaluation. presents comparative results of representativeevent denoising methods could be used for GT generation:spatiotemporal density filter-based (DWF, STDF),smoothness optimization-based (TimeSurface),andmotion estimation-based (EvFlow). The first and sec-ond rows display the denoised and residual noise visual-ization, respectively. It can be observed that DED effec-tively eliminates almost all the scattered BA noise withgood preservation of the inherent structured feature, whilethe others exhibit residual noise and varying degrees of sig-nal event damage. Statistical analysis on the global dis-tribution of the denoised event is shown in the third row.Compared to others, DED demonstrates a higher number ofblank patches while maintaining a high event preservationrate, which better reflects the overall sparsity and local con-centration of the ideal event stream. Moreover, better de-noising often leads to higher-quality reconstruction. In thelast row, the intensity images reconstructed by E2VID indicate that DED accurately restores nighttime scene in-formation, such as leaves, building contours, and road signs",
  ". DTSNN for Event Denoising": "Compared to artificial neural networks(ANN), spikingneural network (SNN) have lower precision but are inher-ently suited for processing event-driven data due to their andinformation transfer mode temporal dynamics, graduallygaining prominence in event-based tasks . Dynamic Threshold Mechanism. Adjustable spiking neu-ron offers enhanced biological plausibility . In-deed, the neuron firing threshold, similar to the event cam-era threshold parameter, functions as controlling the out-put. Intuitively, the dynamic threshold (DT) mechanism canadapt to more complicated situations since a high thresholdcould suppress noisy input when noise dominates, while alow one helps to sensitively preserve the expected signals.Inspired by this, we transformed the fixed threshold into dy-namic ones based on the LIF neuron model to mimicthis anisotropic biomechanism. The tendency of decreasingspike quantity of the postsynaptic neurons responses withan increasing threshold as illustrated in (a). DTSNN Model. Therefore, we propose a fully SNN (allsynaptic operations are SNN-based) incorporating learnableDT for event denoising, namely DTSNN. As illustrated in (b), DTSNN consists of a dynamic threshold branch(DTB) and event denoising branch (EDB). The former dy-namically generates a threshold map based on the succes-sive event input, indicating the approximate spatial regionsof signal or noise. This map is then passed to the latter, con-trolling the spiking process of LIF neurons in the last layer,",
  "(4)": "where n and t denote the layer number and time step respec-tively. Hn,t is the membrane potential which is producedby coupling the spatial feature U n,t and the temporal inputV n,t1. The DT map F(V tth) determines whether the outputspiking matrix Sn,t should be fired or stay as zero, formu-lating the final denoised event matrix X. Hea(x) = 1 rep-resents the Heaviside step function when x 0, otherwiseHea(x) = 0, and means a element-wise multiplication.Notably, both two branches receive the same consecutivedata as input, which is a binary event frame within a certaintime window of t. The denoising branch is supervisedby the GT of signal events within t, while the thresholdmap label of the dynamic threshold branch comes from thesignal events over a longer temporal period.",
  ". Experiments Setup": "Implementation Details. We use a combined loss consist-ing of the L1-norm and BCE to train the proposed network.Our models are implemented with the open-source frame-work SpikingJelly , using NVIDIA A100 GPU. TheAdam optimizer is employed with a batch size of 8 and alearning rate of 0.002. A fixed threshold of 0.5 is chosen forthe rest of the neurons except at the last membrane potentiallayer. Besides, the reset value Vreset and the membrane timeconstant of all LIF neurons are set to 0 and 2, respectively.Datesets. We conduct quantitative experiments on exist-ing paired datasets. Due to the enormous scale of billionsof events in LED, training them all would be extremelytime-consuming and resource-intensive. Therefore, 600 se-quences were randomly selected for training with each con-sisting of consecutive 10 segments, and 60 sequences wererandomly chosen for testing. To further evaluate the perfor-mance of the discrepancy of different datasets, we qualita-tively test on typical public datasets DSEC and E-MLB. For event-based denoising methods, besides the meth-ods in Sec.3.3 we select the representative supervised de-noising methods including EDnCNN and AEDNet.Evaluation Methods. We use an index of event denois-ing accuracy DA = 1",
  "GP + T N": "GN ) to measures the denois-ing performance on LED, where TP, TN, GP and GN arethe number of true signal, true noise, total signal, and totalnoise respectively, forming two parts: signal retain (SR) andnoise removal (NR). The metrics on other paired datasetsare adopted from their proposed ones. Moreover, visualiza-tion results was qualitatively evaluated on other datasets.",
  ". Quantitative Evaluation": "The main quantitative results are presented in .In summary, the proposed method achieves the best over-all results on three datasets, followed by AEDNet. Particu-larly, DTSNN balances signal retention and noise removaleffectively, demonstrating the highest denoising accuracy.Meanwhile, our model also achieved faster inference speedon average among them. Notably, the runtime from 120 *120 events indicates that the efficiency of grid-based eventrepresentation and processing is significantly higher thanthe manner based on a single-event level, although the latterbetter preserves the asynchronous property of the event.",
  "Raw Event": ". Illustration of the LED diversity. We train DTSNN ondifferent datasets: DVSCLEAN, DVSNOISE20 and LED, and teston real unpaired datasets (from left to right: DSECE-MLB).The model trained on LED has achieved better denoising results. tours, but also preserve the structural features of the sceneand objects well. For example, the zoomed region of firstrow in highlights that our method is the only one suc-cessfully preserving the information of small airborne targetafter denoising. Similarly, our method also stands out in re-tain lane marking located in the bottom-left of second row.Evaluation on Public Datasets. To evaluate the general-ization across different datasets, we also train DTSNN onsynthetic event denoising dataset DVSCLEAN and realone DVSNOISE20 respectively, testing on DSEC and E-MLB , the former is resolution of 640*480 andthe latter is resolution of 346*260. As shown in , themodel trained on DVSCLEAN performs poorly due to thehuge domain gap between. DVSNOISE20 excels at remov-ing noise effectively but inevitably loses some details. The",
  ". Ablation Study and Discussion": "Effectiveness of DED Framework. The DED frameworkaims to fully exploit the property of noise inconsistency/signal consistency in dual events and the self-correlationin a single event stream. As shown in , the small tar-get signal is overwhelmed by noise events, utilizing solelyspatiotemporal correlation constraints can only remove par-tial noise events because some of them also satisfy this re-lationship. Relying on the spatial similarity of dual eventstreams effectively disassembles the mutual relationship inthe dense noise event group. Therefore, combining bothprocedures can achieve a more refined denoising result.Effectiveness of Spiking Neuron. Intuitively, the inherentspatiotemporal sparsity of event streams seamlessly alignswith SNN because of the temporal information ability ofspiking neurons. To compare the performance differencesbetween SNN and ANN architectures in the event denois-ing tasks, the evaluation was conducted on the same struc-ture. As shown in , the SNN based on LIF neuronachieves better denoising accuracy while maintaining 18lower power consumption advantage compared to its ANNversion (the spiking neurons replaced with ReLU).Effectiveness of DT Module. To evaluate the effects ofthe proposed learnable threshold mechanism of DTSNN,we train the network with various fixed threshold neuronsand dynamic threshold neurons, respectively. As can be ob-served from and (a), the denoising accu-racy improves after introducing the DT module due to thenetwork has the tendency to increasing spiking probabilityfor signal pixels according to threshold prediction, and the (b) illustrates DT module could assist in locatingthe approximate signal or noise region to retain more signalevents from structural area compared to the FT model.",
  ". Inference accuracy with proposed methods implemented": "Discussion. Essentially, true events comes from intensitychanges. When artificial light source variations exist, thelight source itself, its coverage area and reflection fromground texture would result in relevant events. These signalevents exhibit noise-like features with discontinuous struc-ture, while the proposed DED framework considers them assignal event that should be preserved. Thus, with the super-vision of such signal labels, the proposed model also tendto retain such events arise from lamp as shown in left twocolumns of despite their visually unappealing effect.Limitation. The DED framework may fail in extreme caseswhere similar part are dominated by noise events. How-ever, such globally anomalous high-frequency noise is rarein practice. Moreover, the proposed model struggles to cap-ture those insignificant signals attaching to structural pe-riphery, limiting signal retain performance in metrics.",
  ". Conclusion": "In this paper, we construct a new large-scale high-qualitypaired real event denoising dataset LED, which provides di-verse noise levels and scenes which aims to cover variousinternal and external conditions. Based on the consistencydifference of noise and signal events, the proposed DEDframework could effectively generate high-quality GT anddetailed analysis confirms our better results than others. Inaddition, we propose a novel baseline DTSNN for eventdenoising featuring with dynamic threshold mechanism ofLIF neuron. The experimental results demonstrate the su-periority of the proposed dataset and denoising method.Acknowledgments. The computation is completed in theHPC Platform of Huazhong University of Science andTechnology. This work was supported by the National Nat-ural Science Foundation of China under Grant 62371203. Yusra Alkendi, Rana Azzam, Abdulla Ayyad, Sajid Javed,Lakmal Seneviratne, and Yahya Zweiri. Neuromorphic cam-era denoising using graph neural network-driven transform-ers. IEEE Transactions on Neural Networks and LearningSystems, 2022. 2 R Baldwin, Mohammed Almatrafi, Vijayan Asari, and KeigoHirakawa. Event probability mask (epm) and event denois-ing convolutional neural network (edncnn) for neuromor-phic cameras. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 17011710, 2020. 1, 2, 6, 7",
  "Clment Cabriel, Tual Monfort, Christian G Specht, and Ig-nacio Izeddin. Event-based vision sensor for fast and densesingle-molecule localization microscopy. Nature Photonics,pages 19, 2023. 1": "Haosheng Chen, David Suter, Qiangqiang Wu, and HanziWang. End-to-end learning of object motion estimation fromretinal events for event-based object tracking. In Proceed-ings of the AAAI Conference on Artificial Intelligence, pages1053410541, 2020. 1 Jinze Chen, Yang Wang, Yang Cao, Feng Wu, and Zheng-JunZha. Progressivemotionseg: Mutually reinforced frameworkfor event-based motion segmentation. In Proceedings of theAAAI Conference on Artificial Intelligence, pages 303311,2022. 2 Jianchuan Ding, Bo Dong, Felix Heide, Yufei Ding, Yun-duo Zhou, Baocai Yin, and Xin Yang. Biologically inspireddynamic thresholds for spiking neural networks. Advancesin Neural Information Processing Systems, 35:60906103,2022. 6",
  "Saizhe Ding, Jinze Chen, Yang Wang, Yu Kang, WeiguoSong, Jie Cheng, and Yang Cao. E-mlb: Multilevel bench-mark for event-based camera denoising. IEEE Transactionson Multimedia, 2023. 2, 6, 7": "Peiqi Duan, Zihao W Wang, Boxin Shi, Oliver Cossairt,Tiejun Huang, and Aggelos K Katsaggelos. Guided eventfiltering: Synergy between intensity images and neuromor-phic events for high performance imaging. IEEE Transac-tions on Pattern Analysis and Machine Intelligence, 44(11):82618275, 2021. 2 Peiqi Duan, Zihao W Wang, Xinyu Zhou, Yi Ma, and BoxinShi. Eventzoom: Learning to denoise and super resolve neu-romorphic events. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1282412833, 2021. 2",
  "data. In Proceedings of the 30th ACM International Confer-ence on Multimedia, pages 14271435, 2022. 2, 6, 7": "Wei Fang, Zhaofei Yu, Yanqi Chen, Timothe Masquelier,Tiejun Huang, and Yonghong Tian. Incorporating learnablemembrane time constant to enhance learning of spiking neu-ral networks. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 26612671, 2021. 6 Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, TimotheMasquelier, Ding Chen, Liwei Huang, Huihui Zhou, GuoqiLi, and Yonghong Tian. Spikingjelly: An open-source ma-chine learning infrastructure platform for spike-based intel-ligence. Science Advances, 9(40):eadi1480, 2023. 6",
  "Yang Feng, Hengyi Lv, Hailong Liu, Yisa Zhang, YuyaoXiao, and Chengshan Han. Event density based denoisingmethod for dynamic vision sensor. Applied Sciences, 10(6):2024, 2020. 2, 5, 7": "Guillermo Gallego, Henri Rebecq, and Davide Scaramuzza.A unifying contrast maximization framework for event cam-eras, with applications to motion, depth, and optical flowestimation.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 38673876,2018. 2 Daniel Gehrig, Mathias Gehrig, Javier Hidalgo-Carri, andDavide Scaramuzza.Video to events:Recycling videodatasets for event cameras. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 35863595, 2020. 1",
  "Alireza Khodamoradi and Ryan Kastner.o(n)o(n)-spacespatiotemporal filter for reducing noise in neuromorphic vi-sion sensors.IEEE Transactions on Emerging Topics inComputing, 9(1):1523, 2018. 7": "Alexander Krull, Tim-Oliver Buchholz, and Florian Jug.Noise2void-learning denoising from single noisy images. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 21292137, 2019. 2 Xavier Lagorce, Garrick Orchard, Francesco Galluppi,Bertram E. Shi, and Ryad B. Benosman. Hots: A hierarchyof event-based time-surfaces for pattern recognition. IEEETransactions on Pattern Analysis and Machine Intelligence,39(7):13461359, 2017. 5, 7 Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, SamuliLaine,Tero Karras,Miika Aittala,and Timo Aila.Noise2noise: Learning image restoration without clean data.arXiv preprint arXiv:1803.04189, 2018. 2 Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, TianfanXue, Nikhil Karnad, Qiurui He, Jonathan T Barron, DillonSharlet, Ryan Geiss, et al. Handheld mobile photography invery low light. ACM Trans. Graph., 38(6):1641, 2019. 2",
  "Ziwei Liu, Lu Yuan, Xiaoou Tang, Matt Uyttendaele, andJian Sun. Fast burst images denoising. ACM Transactionson Graphics (TOG), 33(6):19, 2014. 2": "Ali Maleky, Shayan Kousha, Michael S Brown, and Mar-cus A Brubaker.Noise2noiseflow: realistic camera noisemodeling without clean images.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1763217641, 2022. 2 Rohit Mangalwedhekar, Nivedita Singh, Chetan SinghThakur, Chandra Sekhar Seelamantula, Mini Jose, andDeepak Nair. Achieving nanoscale precision using neuro-morphic localization microscopy. Nature Nanotechnology,pages 110, 2023. 1 Ben Mildenhall, Jonathan T Barron, Jiawen Chen, DillonSharlet, Ren Ng, and Robert Carroll. Burst denoising withkernel prediction networks. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages25022510, 2018. 2",
  "Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-ing a completely blind image quality analyzer. IEEE Sig-nal processing letters, 20(3):209212, 2012. 6": "Liyuan Pan, Richard Hartley, Cedric Scheerlinck, MiaomiaoLiu, Xin Yu, and Yuchao Dai. High frame rate video re-construction based on an event camera. IEEE Transactionson Pattern Analysis and Machine Intelligence, 44(5):25192533, 2020. 1 Etienne Perot, Pierre De Tournemire, Davide Nitti, JonathanMasci, and Amos Sironi. Learning to detect objects with a1 megapixel event camera. Advances in Neural InformationProcessing Systems, 33:1663916652, 2020. 1",
  "Henri Rebecq, Daniel Gehrig, and Davide Scaramuzza.Esim: an open event camera simulator. In Conference onrobot learning, pages 969982, 2018. 1": "Henri Rebecq, Ren Ranftl, Vladlen Koltun, and DavideScaramuzza. High speed and high dynamic range video withan event camera. IEEE transactions on pattern analysis andmachine intelligence, 43(6):19641980, 2019. 1, 5 Timo Stoffregen and Lindsay Kleeman. Event cameras, con-trast maximization and reward functions: An analysis. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1230012308, 2019. 1 Timo Stoffregen, Guillermo Gallego, Tom Drummond,Lindsay Kleeman, and Davide Scaramuzza.Event-basedmotion segmentation by motion compensation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 72447253, 2019. 1",
  "Cadence Design Systems. Software downloads-cadence de-sign systems. 4": "Siqi Wang, Tee Hiang Cheng, and Meng-Hiot Lim. Ltmd:Learning improvement of spiking neural networks withlearnable thresholding neurons and moderate dropout. Ad-vances in Neural Information Processing Systems, 35:2835028362, 2022. 6 Yanxiang Wang, Bowen Du, Yiran Shen, Kai Wu, Guan-grong Zhao, Jianguo Sun, and Hongkai Wen. Ev-gait: Event-based robust gait recognition using dynamic vision sensors.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 63586367, 2019. 2,5, 7 Zihao W Wang, Peiqi Duan, Oliver Cossairt, Aggelos Kat-saggelos, Tiejun Huang, and Boxin Shi. Joint filtering of in-tensity images and neuromorphic events for high-resolutionnoise-robust imaging. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages16091619, 2020. 2 Kaixuan Wei, Ying Fu, Jiaolong Yang, and Hua Huang. Aphysics-based noise formation model for extreme low-lightraw denoising. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 27582767, 2020. 1 Jinjian Wu, Chuanwei Ma, Leida Li, Weisheng Dong, andGuangming Shi. Probabilistic undirected graph based de-noising method for dynamic vision sensor. IEEE Transac-tions on Multimedia, 23:11481159, 2020. 2 Man Yao, Huanhuan Gao, Guangshe Zhao, Dingheng Wang,Yihan Lin, Zhaoxu Yang, and Guoqi Li. Temporal-wise at-tention spiking neural networks for event streams classifica-tion. In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision, pages 1022110230, 2021. 6 Jiqing Zhang, Bo Dong, Haiwei Zhang, Jianchuan Ding, Fe-lix Heide, Baocai Yin, and Xin Yang. Spiking transform-ers for event-based single object tracking. In Proceedings ofthe IEEE/CVF conference on Computer Vision and PatternRecognition, pages 88018810, 2022. 6",
  "Pei Zhang, Zhou Ge, Li Song, and Edmund Y L. Neuromor-phic imaging with density-based spatiotemporal denoising.IEEE Transactions on Computational Imaging, 2023. 2": "Xiang Zhang, Wei Liao, Lei Yu, Wen Yang, and Gui-SongXia.Event-based synthetic aperture imaging with a hy-brid network. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1423514244, 2021. 6 Lin Zhu, Xiao Wang, Yi Chang, Jianing Li, Tiejun Huang,and Yonghong Tian. Event-based video reconstruction viapotential-assisted spiking neural network. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 35943604, 2022. 6"
}