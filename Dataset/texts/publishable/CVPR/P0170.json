{
  "Abstract": "This paper presents Neural Visibility Field (NVF), anovel uncertainty quantification method for Neural Radi-ance Fields (NeRF) applied to active mapping. Our keyinsight is that regions not visible in the training views leadto inherently unreliable color predictions by NeRF at thisregion, resulting in increased uncertainty in the synthesizedviews. To address this, we propose to use Bayesian Networksto composite position-based field uncertainty into ray-baseduncertainty in camera observations. Consequently, NVF nat-urally assigns higher uncertainty to unobserved regions, aid-ing robots to select the most informative next viewpoints.Extensive evaluations show that NVF excels not only in un-certainty quantification but also in scene reconstruction foractive mapping, outperforming existing methods. More de-tails can be found at",
  ". Introduction": "Active 3D reconstruction plays a pivotal role in robotics. Thechallenge lies in enabling the robot to precisely reconstruct atarget using the fewest views possible. Consider the example,illustrated in , where the agents objective is to thor-oughly explore an unknown object (the Hubble telescope).To achieve this, the robot assesses the uncertainty of poten-tial views, choosing actions that significantly diminish thisuncertainty. A crucial aspect of this process is the represen-tation of the scene. It should not only facilitate high-qualityreconstruction but also be cognizant of uncertainties.Recently,implicitscenerepresentations,notablyNeRF have shown remarkable ability in high-qualityscene reconstructions. The result has motivated applyingNeRF for active reconstruction . However, dueto the opaque nature of neural networks, estimating theuncertainty of NeRF remains challenging. Previous workshave developed various proxy measurements to representthe uncertainty in NeRF, in which they aim to maximize theNeRFs reconstruction accuracy and geometric faithfulnessto the scene. However, these approaches neglect a crucial",
  "NeRF Rendering": ". Neural Visibility Field (NVF) is an uncertainty estimationframework for NeRF that accounts for visibility: whether a regionis covered by the training views of a NeRF. Visible regions shouldhave low uncertainty (bottom row), and unobserved should havehigh uncertainty (top row). In this paper, we show that many ex-isting methods in NeRF uncertainty quantification can be viewedas special cases of our framework, and NVF outperforms themempirically in uncertainty quantification and active mapping tasks. factor to optimize for, namely, visual coverage.In active reconstruction, an agent makes a tradeoff be-tween exploring new areas of a scene and revisiting previ-ously explored ones. Since NeRF is a multiview reconstruc-tion method, a natural strategy is to explore regions that havenot been observed by previous views and have these regionshold a high degree of uncertainty. Surprisingly, prior methodshave largely failed to account for visibility and instead fo-cus on estimating uncertainty via density or NeRF-predictedposition-based RGB variance. Another gap in prior researchis the integration of position-based uncertainty factors (e.g.,emitted color, opacity, and visibility) into ray-based obser-",
  "arXiv:2406.06948v2 [cs.CV] 15 Jun 2024": "vation uncertainty. Previous approaches typically employ asimple (weighted) average or sum of position-based uncer-tainties to approximate the observation uncertainty. However,these methods often lack a solid theoretical foundation andcan underperform in complex scenarios.To address these challenges, we propose Neural VisibilityField (NVF). Our key insight is that if a region has neverbeen visible in the training views, the color prediction forthis point by NeRF is unreliable. To effectively integratethis location-based uncertainty into ray-based camera ob-servations, we view NeRF through the lens of a BayesianNetwork. Within this framework, the distribution of a coloralong a ray can be interpreted as a Gaussian mixture Model.Subsequently, we calculate the entropy of the GMM and em-ploy it as a cost function, guiding the agent to select the nextbest view for active mapping. We observed that all previousmethods can be interpreted as specific approximations withinour proposed theoretical framework, yet, they consistentlyoverlook a crucial aspect, namely, visibility.Our evaluation of the proposed approach is multi-facetedand spans a range of environments, encompassing objects, in-door rooms, and spaces. We illustrate how our method offersa superior metric for assessing uncertainty in NeRF. We alsoapply our approach to active mapping tasks. Specifically,we demonstrate that employing our metric in Next-Best-View (NBV) planning facilitates the planning of trajectoriesthat not only enhance reconstruction quality but also maxi-mize visual coverage of the scene. As a result, our proposedmethod demonstrates significant improvements over theseprior approaches in experimental evaluations. To summarize,our main contributions are: We propose a principled uncertainty estimation methodfor NeRF that takes into account visibility, called NeuralVisibility Field (NVF).",
  ". Related Work": "Active Mapping. Research on active mapping or NBV se-lection is a long-studied problem with the goal ofsearching for observation poses to create an optimal recon-struction of an environment. Scott et al categorizes theseapproaches as model-based approaches, which utilize knowl-edge of the geometry and appearance of a scene ,and model-free approaches, which use information extractedfrom data gathered online . More relevant are view-point selection strategies, including frontier-based ,sampling-based , and uncertainty based .In particular, our method is inspired by the line of workthat uses probabilistic volumetric occupancy to facilitate vis-",
  "ibility operations , which employs the concept ofentropy to estimate uncertainty": "ImplicitSceneRepresentation.Implicitneuralfields represent 3D scenes as a continuousdifferentiable signal parameterized via a neural network.The seminal work of Neural Radiance Fields (NeRFs) learns a density and a radiance field supervised by mul-tiview 2D images. New views can be queried from atrained NeRF through volumetric rendering. Along thisdirection, significant progress has been made in novelview rendering , 3D reconstruction ,3D generation and videos .Despite their success, the quality of representation hingeson using a large number of well-posed images which limitstheir applicability in real-time applications. To counterthese problems, recent work has focused on few-shotneural rendering , handling unknown ornoisy camera pose estimates , using heuristiccamera placement strategy , or adding a notion ofuncertainty to quantify information gain fornext-best-view selection. Uncertainty Estimation for NeRF. This work focuseson quantifying the epistemic uncertainty of a NeRF model todetermine the next best view for improving its reconstruction.Direct approaches such as ensemble-based methods areconceptually simple but computationally expensive or re-quire prior data collection . Our method improvesupon and unifies a recent line of work .ActiveNerf and NeurAR model RGB color distri-bution at a specific spatial point as a Gaussian distribution,and directly use NeRF to predict its variance. However, thepredicted variance tends to be inaccurate in instances wherea region has never been visible from the training views. Incomparison, ignore the spatial RGB uncer-tainty, and approximating the entropy through the probabilityof occupancy by using NeRFs density prediction. In partic-ular, treats the sampled points in volumetric renderingthat are displayed by pixels as discrete random variables andcomputes the entropy based on it. In , the entropyis approximated by utilizing the probability of a ray beingoccluded at a point. However, it is worth noting that a remain-ing gap exists in all previous methods as they lack theoreticalgrounding for bridging the position-based uncertainty or oc-cupancy uncertainty with ray-based observation uncertainty.In our work, we proposed a theoretically principled methodbased on Bayesian Network to address this challenge. More-over, a crucial aspect of uncertainty estimation is that if aregion is never visible by any of the previous views, theNeRF prediction at this region is not reliable, and high un-certainty should be associated with these regions, yet thisaspect is overlooked by all relevant previous works. Our pro-posed theoretical framework enables us to properly modelthis aspect through visibility, and all previous work could",
  ". Method": "Active mapping aims to reduce uncertainties in the recon-structed map and achieve visual coverage of the entire scene.This is achieved by assessing the current uncertainties inthe reconstructed map and predicting the potential informa-tion gained from proposed viewpoints. However, the chal-lenge arises when utilizing NeRF for active mapping. Theopaque and complex nature of neural networks presents sig-nificant challenges for accurately quantifying uncertaintywithin NeRF. Although several methods have been proposedto approximate uncertainty in NeRF, they often lack a theoret-ical foundation and may underperform in complex scenarios.Our key insight is that if a region has never been visiblein the training views, the color prediction for this point byNeRF is inherently unreliable. To effectively incorporate thisposition-based uncertainty into camera observations, we pro-pose to use a Bayesian network. This allows for the seamlessintegration of uncertainty from the implicit field into the ob-served images uncertainty. In this section, we will start witha review of NeRF, followed by a detailed explanation of howto model NeRFs volume rendering process using a Bayesiannetwork. Subsequently, we will delve into the integrationof visibility aspects within the framework. Finally, we willdiscuss the application of this framework to active mapping.",
  ". Problem Formulation": "A NeRF is defined as an implicit function F:(x, d) (c, ), where x represents the 3D position, d =(, ) the viewing direction, c the emitted RGB color at x,and the volume density at x. The volume density function(x) is a differentiable measure of the probability that a rayis occluded at position x. Considering a ray r(t) = o + tdwith near and far bounds tn, tf, the observed color at therays origin is given by:",
  ". Volume Rendering as Bayesian Network": "While NeRF synthesizes novel views, NeRF cannot estimatethe uncertainty in the views. We introduce a method that com-posites position-based uncertainty into ray-based uncertaintyusing a probabilistic graphical model. This framework en-ables the integration of visibility factors into the uncertaintyestimation process (see Sec. 3.3). We consider the observedcolor along a ray, C(r), as a random variable instead of aconstant. In this subsection, we detail the computation of thisvariables distribution by using a Bayesian network to modelthe volume rendering process. We use a binary random vari-able Di to denote whether the ray is occluded in the interval[ti, ti+1] (Di = 1 for occluded, Di = 0 for transparent). Thecontinuous random variable Ci then represents the emittedcolor at ti in the direction d, and Zi is a continuous randomvariable for the observed color at ti. Here, Z0 correspondsto the cameras observed color at the origin, and hence, thegoal is to compute p(z0). Notice that, although both Ci andZi represent colors, their difference lies in the objects theyrepresent: Ci represents the color distribution associatedwith a specific position in R3, whereas Zi corresponds to thecolor distribution of a camera observation, associated witha particular ray. For simplicity, we omit the ray index r forDi, Ci, and Zi in Sections 3.2 and 3.3 below.Note that the value of Zi only depends on Di, Ci, and Zi+1. Specifically, if the interval [ti, ti+1] occludes the ray,Zi assumes the emitted color Ci; otherwise, Zi equals Zi+1,as the interval is transparent. The conditional probabilityp(zi|Di, ci, zi+1) is thus:",
  "p(zi) = ip(ci) + (1 i)p(zi+1).(5)": "Note that this formulation utilizes the relationship P(Di =0) = 1 i = exp (isi). where i was previously de-fined as the probability of occlusion at the ith point alongthe ray. If we assume p(ci) is a Gaussian distribution withmean ci and covariance Qci, as predicted by the NeRFmodel (see Sec. 3.4 for details), by using recursion Eq. (5),the marginal probability of z0 is computed as a Gaussianmixture model (GMM):",
  "Views (Sec 3.6)": ". Active Mapping with NVF. Starting with a small set of initial views, a trained NVF is used to quantify uncertainties amongsampled candidate views and chooses the view with maximum uncertainty as the next view to be observed by the agent. with wi = ii1j=1(1i). The distribution of the camerasobservation, p(z0), implies that E[z0] = i wici whichaligns with the original NeRFs volume rendering expression(Eq. (3)).So far, we have developed a framework based on a proba-bilistic graphical model to bridge position-based uncertaintywith ray-based observational uncertainty. In the followingsubsection, we will discuss the integration of the visibilityfactor into this framework.",
  ". Uncertainty with Visibility": "With the Bayesian network formulation, we can now addvisibility into the uncertainty estimation. Let a binary randomvariable Vi represent whether point i is visible to any camerain the training set. When a point is visible (Vi = 1), wecan rely on NeRFs output for RGB and its variance. If apoint is unobserved (Vi = 0), the NeRFs output at this pointbecomes unreliable, and we assign a prior color distributionN(0, Q0) to it, as follows:",
  "(8)": "where i = (1 ) exp(0si) + exp(isi), and thehyperparameter represents the accuracy of occlusion pre-diction, specifically indicating the likelihood that a predic-tion about occlusion is correct for points that are invisible.In situations where the occlusion prediction is incorrect, weresort to using a constant prior density 0 to estimate theocclusion probability. This approach helps in adjusting our models predictions on density, particularly for points notvisible to any camera in the training set.By combining Eqs. (2), (7), (8), the marginal probabil-ity of zi satisfies the recursive formula similar to Eq. (5):p(zi) = i N(ci, Qci) + (1 i )N(0, Q0), wherei =vi + (1 vi)1 exp(isi)+ (1 )(1 vi)(1exp(0si)), and vi = P(Vi = 1) is the probabilityof point i being visible to at least one camera in the train-ing set. The marginal probability of p(z0) can be computedsimilarly as follows",
  ". Neural Visibility Field": "So far, we have established a framework that bridgesposition-based uncertainty with ray-based observation un-certainty, while also incorporating visibility factors. Next,we discuss a method to determine visibility vi, which is theprobability that a point xi is visible to at least one camera inthe training set. Let P = {p1, p2, . . .} be the set of cameraposes in the training set. If point xi is within the field ofview of a camera p in P, the visibility of xi to camera p canbe expressed as vp(xi) = T p(tpi ), where xi = op + tpi dp ison a ray from camera p, and Tp(tpi ) denotes the probabilityof the ray being transmitted from op to tpi without occlusion,as defined in Eq. (2). Therefore, the probability that point xiis visible to at least one camera in the set P is given by:",
  "pP(1 vp(xi)).(10)": "However, directly computing Eq. (10) during volume ren-dering is impractical. For each point along a ray r, it wouldrequire generating an additional ray rp from camera p andsampling points along this ray to determine the points visi-bility to camera p. Doing this for all existing views is compu-tationally expensive. To address this, we propose to amortizethe cost by training an implicit model to predict the visibility. We introduce Neural Visibility Field (NVF), an aug-mented NeRF that outputs both color uncertainty and vis-ibility. The enhanced model is defined as F : (x, d) (, c, Qc, v), where v represents the visibility with respectto the training views, and c and Qc denote the mean andcovariance of the color vector, respectively. The parametersc and are trained with Mean Square Error loss as in .To train Qc, we employ the Negative Log-Likelihood Lossas follows:",
  "iwiNCg(r); ci, Qci,(11)": "where R denotes the set of rays in each batch, and Cg(r)represents the ground truth color of ray r. For training, werandomly sample points within the scene. The ground truthvisibility, derived using Eq. (10), is then utilized to train thevisibility head, using cross-entropy loss. Please refer to Supp.for further details on network architecture and training.",
  ". Active Mapping with NVF": "In this section, we apply the PDF of ray color, derivedfrom Sec. 3.4, for active mapping purposes. Let Zmnpbethe color of the ray corresponding to pixel index m, n fromcamera pose p. The PDF of Zmnp, denoted as p(zmnp), canbe obtained using the formulation provided in Eq. (9). We de-fine Zp as a random variable in RHW 3, representing thecollective observation of all pixels in an image with heightH and width W.The goal of active mapping is to identify a camera pose,denoted as p, that maximizes the entropy of the observationZ at that pose. This is formally expressed as:",
  "p = arg maxpH(Zp).(12)": "Note that we can deduce Eq. (12) from the information gainor mutual information I(Zp; M) = H(Zp) H(Zp|M),where M represents the random variable of the entire map.This assumes that H(Zp|M) is constant, specifically, thatmeasurement noise remains constant given a known map.To compute H(Zp), we initially assume that the colorof each pixel is independent of the others. Under this as-sumption, the entropy of Zp can be calculated as H(Zp) = m,n H(Zmnp). However, this assumption of independencemay not always hold true. For instance, when the camera isin close proximity to an object, the pixels in the image areoften strongly correlated, particularly since they are measur-ing points that are spatially close. To account for this spatialcorrelation, we introduce a correction term:",
  "Here, fcorr(H(Zmnp); dmnp ) incorporates spatial correlationbased on the expected depth dmnp . Furthermore, we use the": "upper bound as proposed in to closely approximate theentropy of the GMM H(Zmnp), as it is known that there isno analytical solution for the entropy of GMM . Furtherdetails on fcorr(H(Zmnp); dmnp ) and entropy computationare included in supp material.Within our theoretical framework for estimating uncer-tainty in NeRF and active mapping, all prior works, to ourbest knowledge, can be viewed as special cases. Specifi-cally, if we drop the visibility factor, each prior work canbe viewed as a specific approximation of our method. Forinstance, Lee et al focuses only on the discrete randomvariable, computing the Shannon entropy with wi log wi,which can be regarded as a simplified version of ours, albeitexcluding the differential entropy term Similarly, uses the weighted average of position-based color varianceto approximate the rays-based observation variance, whichlacks theoretical grounding and ignores the visibility fac-tor in weight computation. In addition, Zhan et al ap-proximate the entropy of ray-based observation by directlysumming the position-based entropy of occlusion, whereas,similarly, Yan et al use the weighted average of position-based entropy of occlusion to approximate the ray-basedobservation entropy.",
  ". Active Mapping Pipeline": "Here we briefly describe the active mapping pipeline usingNVF (illustrated in ). Please refer to Supp. materialfor more details. The process starts with training the NVFon a small batch of initial views. We employ two strategiesfor next view selection. In the sampling-based strategy, wesample N views from a prior distribution, estimate their un-certainty using Eq. (12), and select the view with maximumuncertainty. The gradient-based strategy is implemented byadjusting the selected pose through gradient-based optimiza-tion, aimed at maximizing entropy, leveraging the inherentdifferentiability of our uncertainty estimation method. Lastly,the agent proceeds to collect and integrate the new observa-tions into the training views to re-train the NVF model andplan the next view.",
  ". Experiments": "In this section, we seek to verify our hypothesis that (a) NVFoutperforms existing methods in both uncertainty quantifica-tion and active mapping both quantitatively and qualitatively;and (b) the visibility term plays a vital role in this result.Simulation Environments and Learning Setup. We con-duct experiments on three datasets of varying difficulty lev-els for active mapping: all assets from the original NeRFdataset , the Hubble Space Telescope, and a custom syn-thetic indoor Room scene. In particular, the Room sceneconsists of two spaces divided by a wall. Successfully map-ping the scene requires traversing both spaces. We assumeaccess to a coarse bounding box that contains the region of . Qualitative results of entropy estimation: NVF assigns a higher entropy to previously unobserved regions while the baselines donot distinguish between the observed (View 1) and unobserved regions (View 2/3). Schematic illustrations of the poses of View 1, 2, and 3can be found in supp. material. Note that within each method and scene, all rendered views share the same color bar. interest. All ground truth images used for training NeRF andassessing reconstruction quality were rendered using Blenderat a resolution of 512512. We utilized Instant-NGP asan efficient backbone for all uncertainty estimation methods.All NeRF models were trained for 5,000 iterations. For NVF,it first trains the Instant-NGP backbone, freezes its weights,and then trains variance and visibility heads, to ensure theperformance improvements are attributed to better entropyestimation instead of a change in the loss function. Baselines. We compared our method with state-of-the-artNeRF uncertainty quantification and active mapping meth-ods. This includes the weight distribution-based entropyapproximation (WD) ; occlusion-based entropy approxi-mation (ActiveRMAP) ; weighted occlusion-based en-tropy approximation - ActiveImplicitRecon (AIR) , andspatial RGB variance-based uncertainty estimation ActiveN-eRF and NeurAR . As discussed earlier, all of theseworks can be viewed as a special case of our method, while",
  ". Uncertainty Estimation": "Setup. We qualitatively compare the uncertainty (entropy)maps produced by our method and the baselines given a setof training views. For each scene, we design scenarios whereonly certain regions are visible in the training views. We thentrain all methods on the same training views and query foruncertainty estimation at an unseen test view. An effectiveuncertainty estimation method should be able to differentiateregions unobserved in the training set, as reconstruction inthese areas is noisy and inaccurate. For the Hotdog scenefrom the original NeRF dataset, we randomly sample 20training views from a 90-degree sector above the plate. Inthe Hubble scene, we sample 20 training views from a 90-degree sector on one side, keeping the opposite side of the . Reconstruction results and camera view distribution: NVF demonstrates superior reconstruction and scene coverage across alldatasets in comparison to baselines. For room scene, only comparable baselines are presented, full results are provided in supp. material. Hubble out of view. For the Room scene, we sample 30training views oriented toward the back wall, the commonwall, and the floor of one of the rooms, ensuring that theother room is unobserved.Results. In the Hubble and Hotdog scenes, as illustratedin , all baseline methods fail to accurately capturethe uncertainty in the unobserved areas of the scenes. Sev-eral baselines assign greater or similar uncertainty to theobserved views as compared to the unobserved ones. A no-table example is the Room scene. The first view focuses onthe room observed in the training view, while the secondview targets the common wall between the two rooms. Theresult indicates that our method differentiates between theuncertainties in regions seen in training views and unseenregions by modeling their visibilities. In Hubble and Hotdog,ActiveNeRF and NeurAR estimate a similar level of uncer-tainty for both unobserved and observed regions. However,in the Room scene, the uncertainty in the unobserved regionis estimated to be lower than the observed. This shows that incomplex scenarios, the uncertainty formulations of ActiveN-eRF and NeurAR are ineffective, and such formulation aloneis insufficient as guidance to explore unobserved regions.",
  ". Active Mapping": "Setup. We deploy active mapping agents with the pipelinedescribed in Sec. 3.6 with different uncertainty estimationmethods. To ensure a fair comparison, all methods are eval-uated under the same conditions during the comparison, toensure that the planning is driven solely by the uncertaintyestimation. Specifically, all candidate views are uniformlysampled within the space, without any prior constraints (suchas the hemisphere constraint employed in ActiveNeRF). This approach ensures that a more accurate uncertainty estimationmethod will enable the robot to achieve more precise map-ping results. For all original NeRF assets and Hubble scenes,we utilize 3-5 initial views covering only a portion of thescene, to realistically simulate active mapping scenarios. TheRoom scene presents the greatest challenge, with nine ini-tial views sampled from one room, leaving the second roomentirely unexplored. All agents start without knowledge ofthe second rooms existence and are expected to discover itthrough uncertainty estimation and reconstruct the scene in20 steps. Please refer to Supp. material for more details.Evaluation metric. Our evaluation employs three types ofmetrics. For novel view synthesis quality, evaluations areperformed at fixed testing viewpoints. We compare viewssynthesized by NeRF with ground truth renderings. The er-rors are quantified using Peak Signal-to-Noise Ratio (PSNR),Perceptual Image Patch Similarity (PIPS), Learned Percep-tual Image Patch Similarity (LPIPS) , and RGB loss. Forreconstructed mesh quality, we quantitatively evaluate thegeometric accuracy of the scene reconstructions. We employthe metrics, Accuracy (Acc), Completion (Comp), and Com-pletion Ratio (CR) as proposed in . For visual coverage(Vis), we assess the proportion of faces in the ground truthmesh observed without occlusion during the experimentsover all faces. The visibility of each face in the mesh istracked using the ground truth mesh and a rasterizer.Results. In Tab. 1, we show the quantitative results of ourapproach in comparison to other baselines. For the originalNeRF Assets, we only include the average of the resultsacross all scenes due to space limitation, detailed resultsare provided in supp. material. Our method significantlyoutperforms baseline methods achieving higher-quality re-",
  "w/o Vis.21.110.8440.1870.382w/o Var.23.770.8970.1130.551Ind. Rays20.320.8220.2360.482Loose22.540.8810.1370.504NVF (Ours)24.420.9020.1080.546": "construction and improved visual coverage. This is espe-cially evident in challenging scenarios such as the Hubbleand Room scenes, where our method successfully exploresthe entire scene and excels across all metrics. In contrast,baseline methods failed to fully explore these scenes, of-ten revisiting previously explored areas (see ) due toinadequate uncertainty estimation that overlooks visibility.",
  ". Ablation studies": "We ablate key components in NVF to examine their role.First, we negate the visibility factor by presuming all sam-pled points as visible to the camera, setting the visibilityhead output to 1 for any input. Second, we disregard thespatial color variance estimation from NeRF, assuming aconstant small uncertainty for all sampled points. Third, weomit the correlation correction factor, treating all rays asindependent. Lastly, for entropy computation, we substitutethe upper bound proposed by with a looser bound, treat-ing multiple Gaussians as a single Gaussian following .The average results across all scenes are shown in Tab. 2, highlighting the crucial role in the visibility factor, remov-ing it significantly drops the performance. We also observethat the correction of independence in Eq. (13) (Ind. Rays.in Tab. 2), and a tighter upper bound (Loose) positivelyimpact performance. However, the position-based color un-certainty directly predicted by NeRF (w/o Var.) plays a lessimportant role, underscoring visibility as the most criticalfactor in uncertainty estimation for active mapping.",
  ". Conclusion": "In this work, we present Neural Visibility Field, a principledapproach that accounts for visibility in uncertainty quan-tification and provide a unifying view of prior research inthis direction. We empirically demonstrated that NVF sig-nificantly outperforms baselines in reconstruction qualityand visual coverage across three scenes with varying levelsof complexity. A limitation of our current active mappingpipeline is that it does not account for the constraints im-posed on the planned trajectory of an agent. A possible futuredirection is to integrate NVF with cost-aware path planning.",
  "Thomas M Cover. Elements of information theory. JohnWiley & Sons, 1999. 12": "Anna Dai, Sotiris Papatheodorou, Nils Funk, DimosTzoumanikas, and Stefan Leutenegger. Fast frontier-basedinformation-driven autonomous exploration with an mav. In2020 IEEE international conference on robotics and automa-tion (ICRA), pages 95709576. IEEE, 2020. 2 Tung Dang, Christos Papachristos, and Kostas Alexis. Vi-sual saliency-aware receding horizon autonomous explorationwith application to aerial robotics. In 2018 IEEE interna-tional conference on robotics and automation (ICRA), pages25262533. IEEE, 2018. 2 Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan.Depth-supervised nerf: Fewer views and faster training forfree. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1288212891,2022. 2",
  "Christian Dornhege and Alexander Kleiner. A frontier-void-based approach for autonomous exploration in 3d. AdvancedRobotics, 27(6):459468, 2013. 2": "Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum,and Jiajun Wu. Neural radiance flow for 4d view synthesis andvideo processing. In 2021 IEEE/CVF International Confer-ence on Computer Vision (ICCV), pages 1430414314. IEEEComputer Society, 2021. 2 Francis Engelmann, Jorg Stuckler, and Bastian Leibe. Jointobject pose estimation and shape reconstruction in urbanstreet scenes using 3d shape priors.In Pattern Recogni-tion: 38th German Conference, GCPR 2016, Hannover, Ger-many, September 12-15, 2016, Proceedings 38, pages 219230. Springer, 2016. 2 Georgios Georgakis, Bernadette Bucher, Anton Arapin,Karl Schmeckpeper, Nikolai Matni, and Kostas Daniilidis.Uncertainty-driven planner for exploration and navigation. In2022 International Conference on Robotics and Automation(ICRA), pages 1129511302. IEEE, 2022. 2",
  "Speech and Signal Processing-ICASSP07, pages IV317.IEEE, 2007. 8, 13": "Matthew D Hoffman, Tuan Anh Le, Pavel Sountsov, Christo-pher Suter, Ben Lee, Vikash K Mansinghka, and Rif ASaurous. Probnerf: Uncertainty-aware inference of 3d shapesfrom 2d images. In International Conference on ArtificialIntelligence and Statistics, pages 1042510444. PMLR, 2023.2 Marco F Huber, Tim Bailey, Hugh Durrant-Whyte, andUwe D Hanebeck. On entropy approximation for gaussianmixture random vectors. In 2008 IEEE International Confer-ence on Multisensor Fusion and Integration for IntelligentSystems, pages 181188. IEEE, 2008. 5, 8, 13 Stefan Isler, Reza Sabzevari, Jeffrey Delmerico, and DavideScaramuzza. An information gain formulation for activevolumetric 3d reconstruction. In 2016 IEEE InternationalConference on Robotics and Automation (ICRA), pages 34773484. IEEE, 2016. 2 Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel,and Ben Poole. Zero-shot text-guided object generation withdream fields. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 867876,2022. 2 Liren Jin, Xieyuanli Chen, Julius Ruckin, and Marija Popovic.Neu-nbv: Next best view planning using uncertainty estima-tion in image-based neural rendering. In 2023 IEEE/RSJInternational Conference on Intelligent Robots and Systems(IROS), pages 1130511312. IEEE, 2023. 2",
  "Georgios Kopanas and George Drettakis. Improving nerfquality by progressive camera placement for free-viewpointnavigation. 2023. 2": "Simon Kriegel, Christian Rink, Tim Bodenmuller, andMichael Suppa. Efficient next-best-scan planning for au-tonomous 3d surface reconstruction of unknown objects. Jour-nal of Real-Time Image Processing, 10:611631, 2015. 2 Soomin Lee, Le Chen, Jiahao Wang, Alexander Liniger,Suryansh Kumar, and Fisher Yu. Uncertainty guided pol-icy for active robotic 3d reconstruction using neural radiancefields. IEEE Robotics and Automation Letters, 7(4):1207012077, 2022. 2, 5, 6 Kejie Li, Yansong Tang, Victor Adrian Prisacariu, andPhilip HS Torr. Bnv-fusion: Dense 3d reconstruction using bi-level neural volume fusion. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 61666175, 2022. 2 Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green,Christoph Lassner, Changil Kim, Tanner Schmidt, StevenLovegrove, Michael Goesele, Richard Newcombe, et al. Neu-ral 3d video synthesis from multi-view video. In Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 55215531, 2022. 2",
  "on Computer Vision and Pattern Recognition, pages 64986508, 2021. 2": "Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and SimonLucey. Barf: Bundle-adjusting neural radiance fields. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 57415751, 2021. 2 Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-worth. Nerf in the wild: Neural radiance fields for uncon-strained photo collections. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 72107219, 2021. 2 Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-worth. Nerf in the wild: Neural radiance fields for uncon-strained photo collections. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 72107219, 2021. 2 Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-bastian Nowozin, and Andreas Geiger. Occupancy networks:Learning 3d reconstruction in function space. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 44604470, 2019. 2 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. Communications of the ACM, 65(1):99106, 2021. 2,5 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view synthe-sis. Communications of the ACM, 65(1):99106, 2021. 1, 3,5",
  "Xuran Pan, Zihang Lai, Shiji Song, and Gao Huang. Activen-erf: Learning where to see with uncertainty estimation. InEuropean Conference on Computer Vision, pages 230246.Springer, 2022. 1, 2, 5, 6, 13": "Jeong Joon Park, Peter Florence, Julian Straub, Richard New-combe, and Steven Lovegrove. Deepsdf: Learning continuoussigned distance functions for shape representation. In Pro-ceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 165174, 2019. 2 Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qian-qian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Im-plicit neural representations with structured latent codes forhuman body modeling. IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 2023. 2",
  "Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprintarXiv:2209.14988, 2022. 2": "Santhosh K Ramakrishnan, Ziad Al-Halah, and Kristen Grau-man. Occupancy anticipation for efficient exploration andnavigation. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceedings,Part V 16, pages 400418. Springer, 2020. 2 Yunlong Ran, Jing Zeng, Shibo He, Jiming Chen, LinchengLi, Yingfeng Chen, Gimhee Lee, and Qi Ye. Neurar: Neuraluncertainty for autonomous 3d reconstruction with implicitneural representations. IEEE Robotics and Automation Let-ters, 8(2):11251132, 2023. 2, 5, 6, 13, 18 Korbinian Schmid, Heiko Hirschmuller, Andreas Domel, IrisGrixa, Michael Suppa, and Gerd Hirzinger. View planningfor multi-view stereo 3d reconstruction using an autonomousmulticopter. Journal of Intelligent & Robotic Systems, 65:309323, 2012. 2",
  "James P Sethna. Statistical mechanics: entropy, order param-eters, and complexity. Oxford University Press, USA, 2021.13": "Jianxiong Shen, Adria Ruiz, Antonio Agudo, and FrancescMoreno-Noguer. Stochastic neural radiance fields: Quanti-fying uncertainty in implicit 3d representations. In 2021 In-ternational Conference on 3D Vision (3DV), pages 972981.IEEE, 2021. 2 Edward J Smith, Michal Drozdzal, Derek Nowrouzezahrai,David Meger, and Adriana Romero-Soriano. Uncertainty-driven active vision for implicit scene reconstruction. arXivpreprint arXiv:2210.00978, 2022. 2 Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davi-son. imap: Implicit mapping and positioning in real-time. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 62296238, 2021. 7 Niko Sunderhauf, Jad Abou-Chakra, and Dimity Miller.Density-aware nerf ensembles: Quantifying predictive uncer-tainty in neural radiance fields. In 2023 IEEE InternationalConference on Robotics and Automation (ICRA), pages 93709376. IEEE, 2023. 2 Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, BrentYi, Terrance Wang, Alexander Kristoffersen, Jake Austin,Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: A modularframework for neural radiance field development. In ACMSIGGRAPH 2023 Conference Proceedings, pages 112, 2023.12, 13 Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang,Rahul Krishna Thomas, Leonidas Guibas, and Ke Li. Nerfrevisited: Fixing quadrature instability in volume rendering.arXiv preprint arXiv:2310.20685, 2023. 2",
  "Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-tor Adrian Prisacariu. Nerf: Neural radiance fields withoutknown camera parameters. arXiv preprint arXiv:2102.07064,2021. 2": "Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim.Space-time neural irradiance fields for free-viewpoint video.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 94219431, 2021. 2 Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin,Vincent Sitzmann, and Srinath Sridhar. Neural fields in visualcomputing and beyond. In Computer Graphics Forum, pages641676. Wiley Online Library, 2022. 2 Dongyu Yan, Jianheng Liu, Fengyu Quan, Haoyao Chen,and Mengmeng Fu. Active implicit object reconstructionusing uncertainty-guided next-best-view optimization. IEEERobotics and Automation Letters, 2023. 1, 2, 5, 6, 18 Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-proving few-shot neural rendering with free frequency regu-larization. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 82548263,2023. 2",
  "Embedding": ". NVF Architecture: The MLP block consists of fully connected layers that use the ReLU activation function. The numbers insidethe block denote the size of the layer. The final output from the visibility (v) MLP and RGB (c) MLP are passed through the sigmoidactivation function while the RGB Variance (Qc) MLP uses softplus activation",
  "A.1. NVF architecture and training details": "NVF is an augmentation of a NeRF consisting of two addi-tional MLP heads for predicting RGB variance and visibility.Specifically, we implement NVF on top of a nerfstudio implementation of Instant-NGP , where the color MLPhead represents c. Alongside the color head is a MLP headfor RGB variance, outputting a 3x1 vector Qc. Similarly, thevisibility MLP head is attached alongside the density head.For a visualization of the architecture, see Appendix .In practice, we train Instant-NGP, variance, and visibilityseparately and in sequence. First, we train the NeRF back-bone for 5000 iterations using a learning rate of 0.01 and4096 rays per batch. Next, the variance head is trained for500 iterations using a learning rate of 0.001 and 4096 raysper batch. Finally, the visibility head is trained for 500 itera-tions using a learning rate of 0.001 and 65536 samples perbatch. We train all modules using the Adam optimizer .",
  "A.2. Entropy computation details": "Joint Entropy of the Camera Observation. We discuss thedetails on the computation of the joint entropy H(Zp) asformulated in Eq. (13). For simplicity in this discussion, wedenote the joint entropy as H(Z) in this section. We modelthe joint observation of all rays as a Bayesian network, wherethe observation of each pixel only depends on its adjacent",
  ")2 , if x < ,0 , otherwise(18)": "This formula indicates that two points located within adistance threshold of each other are strongly correlated,whereas those beyond this threshold are considered indepen-dent. It is noteworthy that this term bears resemblance tothe correlation function (x) = exp( x ), which is com-monly applied in statistical physics , and representsthe correlation length. Empirical evaluations indicate thatthe use of either correlation function expression significantlyoutperforms the scenario where all rays are assumed to beindependent ( = 0). Notably, a marginal improvement wasobserved when utilizing Eq. (18).Therefore, we can approximate the correlation betweentwo adjacent rays based on their expected depth, expressedas mn = (dmn), where is the angular resolution ofeach pixel, dmn is the expected depth of ray at pixel (m, n).This implies that when the camera is closer to an object,the observations in adjacent pixels of the camera exhibitstronger correlation. Hence the actual total information gainis smaller than the sum of the information gain of each pixel.Accordingly, the correction function fcorr in Eq. (13) can bedefined as:",
  "fcorr(H(Zmn); dmn) = (dmn)H(Zmn)(19)": "In our experiments, we let the correlation length = kD,where D represents the diameter of the coarse bounding boxenclosing the object, and k is a hyperparameter, and we letk = 0.25.Entropy of GMM. We then introduce the details tocompute the entropy for each ray, which is modeled asa Gaussian Mixture Model (GMM). For the sake of sim-plicity, we denote the GMMs distribution as p(x) =",
  "and is the weighted mean of the Gaussian components,defined as =": "i wii. It is worth mentioning thatthe baseline method use the weighted average ofposition-based color variance to approximate the rays-basedobservation variance by employing a single Gaussian whosemean and variance are the weighted averages of the meansand variances of all samples along the rays, respectively ;in other words, = i wiQi. This approach resemblesthe first term in Eq. (21) but misses the covariance term(i )(i )T . Additionally, it does not take into ac-count the visibility to the training views.In summary, we derive an upper bound for the pixel-wise entropy, and consequently, for the joint entropy of eachview, this upper bound is utilized to closely approximatethe information gain at a given pose. In the planning phase,given a candidate pose, we first apply Appendix Eq. (20) tocompute the entropy for each ray, subsequently, we computethe joint entropy of the image observation as per Eq. (13)at that pose, which then serves as a reward function in theplanning process.",
  "A.3. Active mapping implementation details": "Active Mapping Pipeline. To train NVF within an activemapping framework, we build our pipeline on top of nerf-studio and NerfBridge . Every time a new viewis added to NVF, the model is trained from scratch on thecollection of its observed views.After training, we sample candidate poses in the scene,without collision with the object, by filtering all poses withina density threshold. In the Room scene, the sampler addi-tionally thresholds for collisions between view poses andthe current pose, to make sure the agent could move to thenew pose without collision. After candidate view poses aregenerated, NVF computes the entropy of each pose. Theview with the highest entropy is next rendered in the sceneand added to the observations. This procedure repeats untilthe horizon step is met, as is shown in Alg. 1. In the exper-iments, we sample N = 512 candidate views and run theactive mapping for 20 steps; the evaluations are performedafter the last planning step.Gradient-based Optimization for Planning. In additionto the method of finding the best view among a randomlysampled candidate poses set, we also performed experimentson 6 DoF pose-refinement on the camera poses, p SE(3), . Schematics of the proposed Bayesian Network: Z represents the observed (ray-based) color, C represents the emitted (position-based) color, D represents if the interval is occluded, V represents the visibility,",
  "p = arg maxpSE(3)H(Zp)(22)": "We first find the top k poses with the highest entropy Pk andperform gradient-based optimization to refine the poses. Toreduce the size of the computation graph and the memoryrequirements, a subset of pixels Zi Zp with an image isused to estimate the expected entropy, instead of the full im-age. We perform backpropogation on this estimated entropyusing an Adam optimizer with a learning rate of 1e 4, tofind the optimum pose.",
  "B.2. Mesh metrics implementation details": "For computing Accuracy, Completion, and Completion Ra-tio metrics, ground truth points are sampled from the groundtruth scene meshes. Points from NVFs reconstructed meshare sampled from the observation view rays. Accuracy mea-sures the mean distance of sampled points from the recon-structed mesh to the nearest corresponding points in theground truth mesh. Completion instead measures the meandistance of sampled ground truth points to the nearest re-constructed mesh points. Completion Ratio calculates thepercentage of completion distances being below a threshold.For the original NeRF assets and Hubble scene, the thresholdis set to 0.01. For the Room scene, as the scale is larger, thethreshold is set to 0.1.Visual coverage quantifies the surface area a trajectoryof views covers a scene. We compute this with rasterization.Given a ground truth mesh of the scene, we project the meshonto all of the observation views. In each rendered image,we record the number of mesh faces visible to the corre-sponding view. We append all observed faces to a visible set. . Uncertainty Experiment Scene Setups: Illustration of the training views and evaluation views in . The black frustumscorrespond to the training views, the green frustums are the evaluation views. For more video results, please refer to",
  "C.2. Gradient-based Pose-Optimization results": "Certain methods compare uncertainty among a finite setof pre-defined scene-specific view candidates. This limitstheir applicability to previously unseen scenes as well astheir ability to reach an optimal solution. Gradient-basedpose estimation aims to find the next-best-view (NBV) ona continuous manifold which broadens its applicability todifferent scenarios and results in optimal view selection.The results in Tab. 1 highlight our approachs ability toselect the optimal view from proposed candidates, intention-ally omitting gradient-based optimization to ensure a faircomparison. To extend our analysis, we conducted a further . Qualitative Results: Comparisons on novel view synthesis results. Our method demonstrates superior novel-view synthesisrendering fine details in comparison to all baselines. For more video results, please refer to",
  "AblationsPSNRSSIMLPIPSRGBAcc.Comp.C.R.Vis": "w/o Vis.21.110.8440.1870.01190.04660.07650.4790.382w/o Var.23.770.8970.1130.00490.02760.03050.6390.551Ind. Rays20.320.8220.2360.01250.05600.05060.4510.482Loose22.540.8810.1370.01000.02470.06090.6000.504NVF (Ours)24.420.9020.1080.00410.02870.03240.6280.546 comparison with gradient-based optimization methods forview selection, detailed in Tab. 3. This comparison, whichincludes our method and two others , utilizes gradi-ent descent to refine the selection of views. As demonstratedin Appendix Tab. 3, the integration of gradient-based opti-mization considerably improves our methods performance,allowing it to surpass competing gradient-based approaches.This superior performance is attributed to our methods moreprecise estimation of uncertainty."
}