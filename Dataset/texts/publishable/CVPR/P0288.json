{
  "Abstract": "Real driving-video dehazing poses a significant chal-lenge due to the inherent difficulty in acquiring preciselyaligned hazy/clear video pairs for effective model training,especially in dynamic driving scenarios with unpredictableweather conditions. In this paper, we propose a pioneer-ing approach that addresses this challenge through a non-aligned regularization strategy. Our core concept involvesidentifying clear frames that closely match hazy frames,serving as references to supervise a video dehazing net-work. Our approach comprises two key components: ref-erence matching and video dehazing. Firstly, we introducea non-aligned reference frame matching module, leverag-ing an adaptive sliding window to match high-quality ref-erence frames from clear videos. Video dehazing incorpo-rates flow-guided cosine attention sampler and deformablecosine attention fusion modules to enhance spatial multi-frame alignment and fuse their improved information. Tovalidate our approach, we collect a GoProHazy datasetcaptured effortlessly with GoPro cameras in diverse ru-ral and urban road environments. Extensive experimentsdemonstrate the superiority of the proposed method overcurrent state-of-the-art methods in the challenging task ofreal driving-video dehazing. Project page.",
  ". Introduction": "Haze significantly degrades visual quality, leading to chal-lenges such as limited visibility and low contrast. This de-terioration adversely affects high-level visual tasks crucialfor safety in autonomous driving , including object de-tection , semantic segmentation , and depth estima-tion . The degradation of haze effect can be expressed *Corresponding authorsPCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Labof Image and Video Understanding for Social Security, School of Com-puter Science and Engineering, Nanjing University of Sci. & Tech.",
  "I(x) = J(x)t(x) + A()(1 t(x)),(1)": "where I(x) and J(x) represent the hazy image and theclear image at a pixel position x, respectively.A de-notes the infinite airlight.The transmission map is de-fined as t(x) = e()d(x), where d(x) and () signifythe scene depth and the scattering coefficient associatedwith the wavelength of light , respectively. Although im-age/video dehazing has been extensively studied",
  "arXiv:2405.09996v1 [cs.CV] 16 May 2024": "over many years, there has been limited research on driving-video dehazing as dynamic driving scenarios with unpre-dictable weather conditions results in the inherent difficultyin acquiring precisely aligned ground truth (GT) videos formodel training in (a).Here, we introduce a new paradigm for data collectionthat involves capturing driving videos under both hazy andclear conditions within the same scenes. This relaxes ef-fectively the stringent requirement of strictly aligned GT.To assess its efficacy, we compile a GoPro-Hazy dataset,effortlessly recorded using GoPro cameras across variousrural and urban road environments. Despite the ease of col-lecting hazy/clear video pairs, two challenges persist: tem-poral misalignment and spatial misalignment in the hazyand clear video pairs. Firstly, inconsistent driving speedsresult in temporal misalignment. For example, as illustratedin (b), frame 81 in the hazy video corresponds toframe 109, not frame 81 in the clear video. Secondly, dis-tinct driving paths and moving objects contribute to spatialmisalignment. As depicted in (c), the car in the hazyvideo is not aligned with the corresponding scene.To address spatial and temporal misalignment, this paperintroduces an innovative driving-video dehazing method in-corporating a non-aligned regularization learning approach.The fundamental concept involves identifying clear framesthat closely match hazy frames as references to super-vise a video dehazing network.Our method comprisestwo key components: reference matching and video dehaz-ing. To enhance the quality of references, we introduce aNon-aligned Reference Frames Matching (NRFM) module,which pairs the input hazy frame with the clearest frame thatmost closely resembles the scene. Subsequently, we presenta video dehazing model featuring a Flow-guided CosineAttention Sampler (FCAS) module and a Deformable Co-sine Attention Fusion (DCAF) module. FCAS utilizes pre-trained coarse optical flow for multi-scale cosine attentionsampling, improving offset accuracy and aligning multipleframes. Unlike the warp operation relying on precise op-tical flow, cosine attention sampling achieves more accurateoffset learning using coarse optical flow. DCAF aggregatesmulti-frame features by combining deformable convolution(DConv) with a large receptive field and leveraging therobustness of cosine similarity for correlation computation.Prior to inputting the video dehazing model, we employan image dehazing network to pre-remove haze from eachframe. Our contributions can be summarized as follows: To our best knowledge, we are the first to propose a non-aligned regularization strategy for the real driving-videodehazing task. Its key idea is to selectively identify high-quality reference frames from the non-aligned clear videofor supervision, reducing reliance on ground truth.",
  ". Related Work": "Image dehazing. Early approaches to single-image dehaz-ing primarily concentrated on integrating atmospheric scat-tering models with various priors . Incontrast, later advancements in the field showcased supe-rior performance through deep learning techniques, lever-aging extensive datasets of hazy/clear images . Thesemethods employ deep neural networks to either learn physi-cal model parameters or directly capture the mapping between hazyand clear images . Forthe latter category, recent works have introduced more so-phisticated network structures, including transformer net-works . However, these approaches heav-ily rely on aligned synthetic data for supervised learning,leading to suboptimal dehazing performance in real-worldscenarios. To tackle this limitation, some studies have pro-posed domain-adaptive techniques andunpaired dehazing models tailored for realscenes. Despite these efforts, when applying image dehaz-ing models to videos, the outcomes often exhibit disconti-nuities due to the disregard for temporal information.Video dehazing.Compared to single-image dehazing,video dehazing offers advantages by leveraging tempo-ral cues from neighboring frames.Early approaches fo-cused on enhancing temporal consistency in dehazing re-sults, achieved through the optimization of transmissionmaps and the elimination of artifacts . Some meth-ods also addressed multiple tasks concurrently, such asdepth estimation , detection , within hazy videos.Recently, Zhang et al. collected a real indoor smokevideo dataset with ground truth, named REVIDE, and intro-duced a confidence-guided and improved deformable net-work (CG-IDN) for video dehazing. Building upon REV-IDE, Liu et al. proposed a novel phase-based memorynetwork designed to enhance video dehazing by integratingboth phase and color memory information. Similarly, Xu etal. introduced a memory-based physical prior guidancemodule that encodes prior-related features into long-termmemory for video dehazing.Furthermore, certain videorestoration methods , demonstrate superior perfor-mance on the REVIDE dataset for adverse weather condi-tions. However, its crucial to note that these approachesare primarily trained and evaluated in indoor smoke scenes.As a result, their effectiveness in addressing complex real-",
  "C": ". (a) The overall framework of our driving-video dehazing (DVD) comprising two crucial components: frame matching and videodehazing. This involves applying frame dehazing to proactively eliminate haze from individual frames. One significant benefit is is theeffectiveness and efficiency of our method in training the video dehazing network using authentic driving data without requiring strictalignment, ultimately producing high-quality results. (b) The illustration depicts the matching process of non-aligned, clear referenceframes through the utilization of an adaptive sliding window using feature cosine similarity. Our input consists of two frames. world outdoor haze conditions remains limited.Video alignment. The primary objective of alignment is tocapture spatial transformations and pixel-wise correspon-dence between adjacent frames. Video-related tasks, likerestoration and super-resolution, often face alignment chal-lenges . Recent works rely on precise optical flow es-timation to align adjacent images/features .Alternatively, some approaches leverage deformable con-volution (DConv) to learn feature alignment offsets. Other methods employ atten-tion mechanisms to combine optical flow and DConv forfeature alignment. However, these alignment methods facetwo challenges: 1) obtaining accurate optical flow with pre-trained models is difficult, and 2) DConv training is unstableunder large motion conditions.In comparison to the aforementioned supervised videodehazing methods , our approach surpasses pre-vious video dehazing models. This is achieved by train-ing on non-aligned real-world hazy datasets and extract-ing effective features from clear and misaligned referenceframes within the same scene. Furthermore, we introducea Flow-guided Cosine Attention Sampler (FCAS) module,which more accurately aligns multi-frame features under in-accurate optical flow conditions by incorporating learnablemulti-scale cosine attention sampling.",
  ". Methodology": "Here, we present an innovative driving-video dehazingmethod illustrated in (a). Initially, we introduce aNon-aligned Reference Frame Matching (NRFM) module,employing an adaptive sliding window that utilizes featuresimilarity to match high-quality reference frames for super-vising the video dehazing network in subsection 3.1. Subse- quently, we propose a video dehazing module that integratesa flow-guided cosine attention sampler and deformable co-sine attention fusion. This integration aims to improve spa-tial multi-frame alignment and fuse the enhanced informa-tion from multiple frames in subsection 3.2. Before display-ing them, we first pre-process the hazy frames.For a given continuous hazy/clear video pair (I=I[0:N], J = J[0:M]) with N M + 2, we utilize an im-age dehazing method to pre-remove haze from each frame,",
  "Jt = P(It),(2)": "where P denotes an image dehazing network, and we em-ploy the non-aligned supervision network .So, thevideo pair is rewritten as (J = J[0:N], J = J[0:M]).Prioritizing frame dehazing offers two key advantages.First, easily acquiring non-aligned image pairs simplifiesthe training process with a large dataset, leading to high-quality pre-processing outcomes. Second, superior framedehazing enhances the video dehazing stages capability tolearn pixel correlations among adjacent frames.",
  ". Non-aligned Reference Frame Matching": "In this subsection, for the hazy video I, our main objectiveis to establish its corresponding clear and non-aligned ref-erence frames derived from the clear video J in (b).These reference frames serve as supervision for the videodehazing network. Further, we curate a set of non-alignedvideo pairs characterized by temporal and spatial misalign-ments in (b) and (c).To solve temporal misalignment, we introduce a non-aligned reference frame matching module to match the clearreference frames in (b).For each hazy frame It,we formally denote its corresponding sliding window clear",
  "(b)(c)": ". (a) Overview of guided pyramid cosine attention sam-pler (GPCAS). (b) The proposed FCSA module uses coarse opticalflow sampling to enhance the receptive field for cosine correlationcalculations. (c) Sampling and calculating cosine correlation. frames as J[its:ite], where its and ite denote the starting andending indexes, respectively. When t = 0, we initialize i0sand i0e as 0 and (M N)/2, respectively. To iterativelymatch clear reference frames, we define the iterated indexesat the t-th frame as:",
  "d(It), ( Ji),(5)": "where denotes the VGG-16 network. Consequently,we obtain the matching reference frames Jkt and Jkt+1 forthe hazy frame It. The overall procedure of our NRFM isoutlined in Algorithm 1.Multi-frames Reference Loss.In addition to temporalmisalignment, our collected data also exhibits pixel and se-mantic misalignment in (c). To tackle spatial mis-alignment, we devise a multi-frame reference loss to ensurefeature consistency between the video dehazing result Jt,and the reference frames Jkt and Jkt+1. Based on the con-textual loss and cosine distance, our multi-frame refer-ence loss is formulated as",
  ". Video Dehazing": "In video tasks, previous studies have revealedthe significance of a larger receptive field. This attributeproves beneficial for aligning and fusing adjacent frames,as it extends the search range and facilitates the learning ofpixel correlations between neighboring frames. The prevail-ing approaches often involves using optical flow for warp-ing alignment . However, these methods are lim-ited by optical flow precision, especially when dealing withblurry images after pre-dehazing.Motivated by these observations, we propose a novelFlow-guided Cosine Attention Sampler (FCAS) module.This module leverages coarse optical flow for sampling,thereby expanding the receptive field for cosine correlationcalculations.This augmentation enhances computationalaccuracy and yields superior alignment results, as depictedin . Additionally, we extend this concept to introducea Deformable Cosine Attention Fusion (DCAF) module, il-lustrated in . The DCAF module employs deformableconvolutions (DConv) to broaden sampling receptive fields,capturing long-term dependencies and thereby improvingfeature aggregation across multiple frames.",
  "In (b), our FCAS module aims to align the featuresof the previous frame Ft1 with those of the current frame": "Ft. The FCAS module produces the offset between adja-cent frame features [Ft1, Ft] RCHW , where C, H,and W denote the channel, height, and width of the fea-tures, respectively. Additionally, an optical flow Ot1t islearned to capture pixel-to-pixel correspondence from theprevious frame to the current frame.Specifically, Ft1 and Ft are derived from a fea-ture extraction network applied to the pre-dehazing results[Jt1, Jt]. The optical flow Ot1t R2HW is ob-tained by fine-tuning SpyNet , denoted as spy, duringtraining. The flow offset map at each position p = (x, y)in It1 is mapped to its estimated correspondence in It asp = (x + u, y + v), which is defined as",
  "(p)k = {p + e | e Z2, ||e||1 (k 1)/2},(8)": "where k represents the sampling kernel size and Z2 denotesa two-dimensional space. Linear projected query vectorsQx,y = Ft1W q, key vectors Kx,y = FtW k, and valuevectors Vx,y = FtW v at coordinate p = (x, y) of Ft1 andFt are defined using the parameters W q, W k, and W v RCd, where d is the dimension of the projected vector. (c) illustrates the use of the coarse Ot1t toguide learnable sampling from Kx,y and Vx,y, expandingthe receptive field for cosine correlation calculations to en-hance accuracy. Within the sampled grid coordinates, thesampling key and value elements are described as",
  "Deformable Cosine Attention Fusion": "Similar to the central concept discussed in .2.1,enhancing the accuracy of cosine correlation calculation in-volves expanding the receptive field. However, a distinc-tion arises in DCAF (refer to ), where we broadenthe receptive field using deformable convolution (DConv). To fully leverage the spatial cues from multiple frames, theDCAF module is employed to fuse the aligned feature F alignt1with the current frame feature Ft to achieve further align-ment. Initially, we transform F alignt1 and Ft to compute theembedding query Qalignt1, key Kt, and value Vt through con-volutional operations with a 11 kernel size, denoted byC1. Subsequently, the key Kt and value Vt undergo down-sampling via a 44 maxpooling operation, denoted by M.They are computed by",
  "Lall = Ladv + Lmfr + Lalign + Lcr,(15)": "Ladv represents the adversarial loss , and Lmfr corre-sponds to the multi-frames reference loss as defined inEq. (6).Since we lack the ground truth for the alignedfeature F alignt1 , we optimize the guided pyramid cosine at-tention sampler (GPCAS) module by using the currentframe feature Ft as the label.Our objective is to mini-mize the discrepancy between F alignt1 and Ft, expressed asLalign = ||F alignt1 Ft||1. Inspired by , we introduce aself-supervised temporal consistency regularization to en-sure the consistency (i.e., color and brightness) of pixelsbetween consecutive frames. It can be formulated as:",
  ". Collection Details": "Camera Parameters Setting. We utilized a GoPro 11 cam-era with anti-flicker set to 60Hz, video output resolution at1920x1080, frames per second (FPS) set to 30, and defaultfocal length range of 19-39mm.Collection Settings. Firstly, as shown in (a), weuse an electric vehicle to collect the GoProHazy dataset,ensuring controlled speed for higher-quality non-aligned",
  "Non-aligned NSDNet Image0.79964.15470.93484.0529-0.89344.3835-11.3856.86arXiv23DVD (Ours)Video0.75983.77530.82073.58250.4400.8745 3.74800.55515.3773.12-": ". Quantitative results on three real-world hazy video datasets. denotes the lower the better. denotes the higher the better. Dueto PM-Net and MAP-Net rely on GT for training, we use Lcx to train them on GoProHazy dataset. Note that we only selected the latestdehazing methods (i.e., RIDCP, PM-Net and MAP-Net) and our DVD for the user study. Moreover, DrivingHazy and InternetHazy weretested on dehazing models trained using GoProHazy and pre-trained dehazing models provided by the authors, respectively.",
  ". Vehicles with different speeds for data collection": "hazy/clear video pairs at lower speeds (30 - 35 km/h). Sec-ondly, as illustrated in (b), we employ a car to capturethe DrivingHazy dataset, testing performance under higherdriving speeds (60 - 80 km/h) in a real-world environment.Collection Method. To collect non-aligned of hazy/clearvideo pairs, follow these steps: 1). As illustrated in (a-i), we capture hazy videosin various scenes under hazy weather conditions. 2). In (a-ii), to maintain consistent scene bright-ness, we choose overcast days with good visibility forcapturing clear video pairs. Besides, to ensure the ref-erence clear video matches the hazy scene, we align clearvideo capture with the starting point of the hazy videos.",
  ". Statistical Analysis": "In , within the GoProHazy dataset, urban roads dom-inate our scenes, with 40% exhibiting heavy haze and 47%moderate haze. Overall, 87% of scenes depict visibility be-low 100 meters. In the DrivingHazy dataset, real high-speedscenario videos increased to 21%, with hazy density mainlyin the 0-50 meters visibility range, constituting 54% of thedataset. In summary, both the GoProHazy and DrivingHazydatasets predominantly feature urban road scenarios, withhazy density concentrated within a 0-100m visibility range.",
  ". Experimental Setting": "Three real-world hazy video datasets.One of thesedatasets is GoProHazy, where videos were captured using aGoPro camera under hazy and clear conditions. The record-ings were made at the starting and ending points of the sameroad, with a total of 22 training videos (3791 frames) and 5testing videos (465 frames). Each hazy video in the datasetis paired with a corresponding clear reference video, and thefootage was obtained by driving an electric vehicle. In con-trast, DrivingHazy was collected by driving a car at a rela-tively high speed in real hazy conditions. It comprises 20testing videos (1807 frames), providing a unique perspec-tive on hazy conditions during fast-paced driving. More-over, we curated two distinct sets of hazy videos, contribut-ing to the creation of the InternetHazy. This dataset, com-",
  ". Testing results on DrivingHazy. Our method can perform dehazing in real driving environments while preserving the brightness": "prising 328 frames, features hazy videos with distributionsdistinct from those found in GoProHazy. It enriches ourstudy by introducing diverse hazy scenarios for analysis.Implementation details. In training processing, we useADAM optimizer with default parameter (1 = 0.9, 2= 0.99) and MultiStepLR scheduler. The initial learning rateis set as 1105. The batch size is 1, and the image size ofinput frames is 256256. Our model was trained for 95Kiterations by Pytorch with two NVIDIA RTX 3090 GPUs.",
  ". Main Results": "Quantitative comparison. In Table. 1, our method out-performs SOTAs performance in terms of FADE andNIQE across all collected datasets. Specifically, onthe GoProHazy dataset, our approach achieves the highestFADE score of 0.7598 and the best NIQE score of 3.7753,surpassing previous SOTA methods. Notably, our methodexhibits a FADE improvement of 0.0412 over RIDCP andan NIQE gain of 0.3458 over PM-Net.On the DrivingHazy dataset, our method achieves aFADE improvement of 0.1227 and a NIQE gain of 0.3119over PM-Net, the leading competitor. Evaluating the gener-alization performance of our proposed DVD on the Internet-Hazy dataset without retraining or fine-tuning, our methodconsistently outperforms other approaches, solidifying itsposition as the top-performing model for generating dehaz-ing results across diverse datasets.In summary, our method surpasses supervised counter-parts, leveraging non-aligned regularization.Unlike su-pervised approaches requiring pixel-wise alignment, ourmethod excels by imposing robust constraints, such as ob-taining image pairs within similar scenes to ensure a con-sistent distribution of clear and hazy images. Compared to",
  ". Ablation study for our NRFM on GoProHazy": "unpaired competitors like D4, our approach applies strongerconstraints, leading to more effective dehazing results.Visual comparison. The dehazing visualizations in highlight the performance of our approach. Overall, ourmethod exhibits superior brightness and texture detailscompared to other state-of-the-art (SOTA) techniques. No-tably, D4 and RIDCP fail to eliminate distant haze, withRIDCP additionally displaying color distortion. While PM-Net and MAP-Net successfully clear distant haze, theycompromise on texture details, resulting in blurred images.Figs. 8 and 9 showcase visualizations on the DrivingHazyand InternetHazy datasets.Despite their advancements,state-of-the-art dehazing methods share a common limita-tionthey struggle to effectively remove distant haze whilepreserving texture details and brightness in the images.Moreover, we validated the effectiveness of our method inthe user study results presented in Table. 1.",
  ". Ablation Studies": "Effect of NRFM. To assess the effectiveness of our pro-posed NRFM, we conducted experiments by excluding theNRFM module and training our video dehazing model inan unpaired setting, where clear reference frames were ran-domly matched. The results in and show anotable improvement in video dehazing by integrating ourNRFM module. This enhancement is due to a more robustsupervisory signal from non-aligned clear reference frames,distinguishing it from the unpaired setting.",
  ". Ablation studies of FCAS and DCAF on DrivingHazy": "Effect of FCAS and DCAF. We conducted a series of ex-periments to validate the efficacy of the FCAS and DCAFmodules on the DrivingHazy dataset. Initially, we devel-oped a baseline video dehazing framework that compriseda frame dehazing module, a pyramid deformable convolu-tion alignment module, and a non-local fusion module, re-ferred to as model (i). This model was trained using ad-versarial loss (Ladv) and multi-frames reference loss ( Lmfr).Subsequently, to assess the impact of the FCAS module, weintegrated it into the pyramid deformable convolution align-ment module, resulting in a comparative model (ii). Simi-larly, to evaluate the effectiveness of the DCAF module, wereplaced the non-local fusion module with the deformablecosine fusion module, denoted as model (iii).Follow-ing this, we introduced our proposed modules (FCAS andDCAF) by replacing the pyramid deformable convolutionalignment module and the non-local fusion module in thebaseline model, forming our proposed method. The quan-titative results are presented in , where our methodexhibits the lowest FADE and NIQE values, indicating itsexcellent real-world video dehazing performance.Additionally, the ablation results for different modulesare visualized in . (a) displays the frame dehazingresults used as input for video dehazing. (b), (c), (d), and(e) illustrate the visualized dehazing results for models (i),(ii), (iii), and our method, respectively. The dehazing resultsof models (i) and (iii) appear blurrier in comparison to ourresult in (e). Moreover, (c) exhibits reduced blurriness butlacks structural information of objects in the image.",
  ". Comparison with different kernel sizes on GoProHazy": "Effect of sampling kernel size.We conducted experi-ments using various kernel sizes to evaluate their influenceon video dehazing outcomes. Due to computational con-straints, we opted for kernel sizes of 3, 5, 7, and 9. indicates that a 7 7 kernel size yields the most favorableresults. Optimal sampling kernel size should account formotion magnitude between frames. A kernel size of 1 1corresponds to a wrapping operation.",
  ". Conclusion": "We introduce an innovative and effective video dehaz-ing framework explicitly tailored for real-world drivingscenarios with hazy videos.By leveraging non-alignedhazy/clear video pairs, we address the challenges of tem-poral and spatial misalignment through the incorporationof a non-aligned reference frame matching module. Thismodule utilizes high-quality clear and misaligned refer-ence frames, providing robust supervision for video dehaz-ing. we enhance spatial multi-frame alignment and aggre-gation through the integration of flow-guided cosine atten-tion sampler and deformable cosine attention fusion mod-ules. Our frameworks experimental results unequivocallydemonstrate superiority over recent state-of-the-art meth-ods, not only enhancing video dehazing but also promisingimproved visibility and safety in real driving scenarios.Acknowledgements. This work was supported by the Na-tional Natural Science Foundation of China under GrantNo.62361166670 and No.62072242. Codruta O Ancuti,Cosmin Ancuti,and Radu Timo-fte.Nh-haze: An image dehazing benchmark with non-homogeneous hazy and haze-free images. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition workshops (CVPRW), pages 444445, 2020. 2",
  "Dana Berman, Shai Avidan, et al. Non-local image dehazing.In Proceedings of the IEEE conference on computer visionand pattern recognition (CVPR), pages 16741682, 2016. 2": "Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, andChen Change Loy.Basicvsr++: Improving video super-resolution with enhanced propagation and alignment. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition (CVPR), pages 59725981, 2022. 3,4 Chen Chen, Minh N Do, and Jue Wang. Robust image andvideo dehazing with visual artifact suppression via gradientresidual minimization.In Computer VisionECCV 2016:14th European Conference, Amsterdam, The Netherlands,October 11-14, 2016, Proceedings, Part II 14, pages 576591. Springer, 2016. 2 Jie Chen, Cheen-Hau Tan, Junhui Hou, Lap-Pui Chau, andHe Li. Robust video content alignment and compensationfor rain removal in a cnn framework. In Proceedings of theIEEE conference on computer vision and pattern recognition(CVPR), pages 62866295, 2018. 3 Xiang Chen, Zhentao Fan, Pengpeng Li, Longgang Dai, Cai-hua Kong, Zhuoran Zheng, Yufeng Huang, and Yufeng Li.Unpaired deep image dehazing using contrastive disentan-glement learning. In European Conference on Computer Vi-sion (ECCV), pages 632648. Springer, 2022. 2, 6, 1 Zeyuan Chen, Yangchao Wang, Yang Yang, and Dong Liu.Psd: Principled synthetic-to-real dehazing guided by phys-ical priors.In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition (CVPR), pages71807189, 2021. 2, 6, 1 Lark Kwon Choi, Jaehee You, and Alan Conrad Bovik. Ref-erenceless prediction of perceptual fog density and percep-tual image defogging. IEEE Transactions on Image Process-ing (TIP), 24(11):38883901, 2015. 7, 2 Xiaofeng Cong, Jie Gui, Kai-Chao Miao, Jun Zhang, BingWang, and Peng Chen. Discrete haze level dehazing net-work. In Proceedings of the 28th ACM International Con-ference on Multimedia (ACMMM), pages 18281836, 2020.2 Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, GuodongZhang, Han Hu, and Yichen Wei. Deformable convolutionalnetworks. In Proceedings of the IEEE international confer-ence on computer vision (ICCV), pages 764773, 2017. 2,3 Peng Dai, Xin Yu, Lan Ma, Baoheng Zhang, Jia Li, WenboLi, Jiajun Shen, and Xiaojuan Qi. Video demoireing withrelation-based temporal consistency.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1762217631, 2022. 5",
  "for single image dehazing. In European conference on com-puter vision (ECCV), pages 722738. Springer, 2020. 2": "Zijun Deng, Lei Zhu, Xiaowei Hu, Chi-Wing Fu, XuemiaoXu, Qing Zhang, Jing Qin, and Pheng-Ann Heng.Deepmulti-model fusion for single-image dehazing. In Proceed-ings of the IEEE/CVF international conference on computervision (ICCV), pages 24532462, 2019. 2 Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, and YuQiao. Fd-gan: Generative adversarial networks with fusion-discriminator for single image dehazing. In Proceedings ofthe AAAI Conference on Artificial Intelligence (AAAI), pages1072910736, 2020. 2",
  "Raanan Fattal. Dehazing using color-lines. ACM transac-tions on graphics (TOG), 34(1):114, 2014. 2": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. Advances inneural information processing systems (NeurIPS), 27, 2014.5 Chun-Le Guo, Qixin Yan, Saeed Anwar, Runmin Cong,Wenqi Ren, and Chongyi Li. Image dehazing transformerwith transmission-aware 3d position embedding. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 58125820, 2022. 2 Martin Hahner, Christos Sakaridis, Dengxin Dai, and LucVan Gool.Fog simulation on real lidar point clouds for3d object detection in adverse weather. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 1528315292, 2021. 1",
  "Kaiming He, Jian Sun, and Xiaoou Tang. Single image hazeremoval using dark channel prior.IEEE transactions onpattern analysis and machine intelligence (TPAMI), 33(12):23412353, 2010. 1, 2, 6": "Cong Huang, Jiahao Li, Bin Li, Dong Liu, and Yan Lu. Neu-ral compression-based feature learning for video restoration.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 58725881,2022. 2, 3 Tae Hyun Kim, Mehdi SM Sajjadi, Michael Hirsch, andBernhard Scholkopf. Spatio-temporal transformer networkfor video restoration. In Proceedings of the European con-ference on computer vision (ECCV), pages 106122, 2018.3",
  "Boyun Li, Yuanbiao Gou, Jerry Zitao Liu, Hongyuan Zhu,Joey Tianyi Zhou, and Xi Peng.Zero-shot image dehaz-ing.IEEE Transactions on Image Processing (TIP), 29:84578466, 2020. 2": "Boyun Li, Yuanbiao Gou, Shuhang Gu, Jerry Zitao Liu,Joey Tianyi Zhou, and Xi Peng.You only look yourself:Unsupervised and untrained single image dehazing neuralnetwork. International Journal of Computer Vision (ICCV),129:17541767, 2021. 2 Jinlong Li, Runsheng Xu, Jin Ma, Qin Zou, Jiaqi Ma, andHongkai Yu.Domain adaptive object detection for au-tonomous driving under foggy weather. In Proceedings ofthe IEEE/CVF Winter Conference on Applications of Com-puter Vision (WACV), pages 612622, 2023. 1 Runde Li, Jinshan Pan, Zechao Li, and Jinhui Tang. Singleimage dehazing via conditional generative adversarial net-work. In Proceedings of the IEEE conference on computervision and pattern recognition (CVPR), pages 82028211,2018. 2 Ruoteng Li, Loong-Fah Cheong, and Robby T Tan. Heavyrain image restoration: Integrating physics model and condi-tional adversarial learning. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition(CVPR), pages 16331642, 2019. 2 Yunan Li, Qiguang Miao, Wanli Ouyang, Zhenxin Ma, Hui-juan Fang, Chao Dong, and Yining Quan. Lap-net: Level-aware progressive network for image dehazing. In Proceed-ings of the IEEE/CVF international conference on computervision (ICCV), pages 32763285, 2019. 2 Zhuwen Li, Ping Tan, Robby T Tan, Danping Zou, StevenZhiying Zhou, and Loong-Fah Cheong. Simultaneous videodefogging and stereo reconstruction. In Proceedings of theIEEE conference on computer vision and pattern recognition(CVPR), pages 49884997, 2015. 2 Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan,Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, RaduTimofte, and Luc V Gool. Recurrent video restoration trans-former with guided deformable attention. Advances in Neu-ral Information Processing Systems (NeurIPS), 35:378393,2022. 3 Huan Liu, Zijun Wu, Liangyan Li, Sadaf Salehkalaibar,Jun Chen, and Keyan Wang. Towards multi-domain singleimage dehazing via test-time training.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 58315840, 2022. 2 Jiaying Liu, Wenhan Yang, Shuai Yang, and Zongming Guo.Erase or fill? deep joint recurrent rain removal and recon-struction in videos. In Proceedings of the IEEE conferenceon computer vision and pattern recognition (CVPR), pages32333242, 2018. 3 Xiaohong Liu, Yongrui Ma, Zhihao Shi, and Jun Chen. Grid-dehazenet: Attention-based multi-scale network for imagedehazing.In Proceedings of the IEEE/CVF internationalconference on computer vision (ICCV), pages 73147323,2019. 2",
  "Srinivasa G Narasimhan and Shree K Nayar.Vision andthe atmosphere.International journal of computer vision(IJCV), 48:233254, 2002. 1": "Yanwei Pang, Jing Nie, Jin Xie, Jungong Han, and XuelongLi. Bidnet: Binocular image dehazing without explicit dis-parity estimation. In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition (CVPR),pages 59315940, 2020. 2 Prashant W Patil,Sunil Gupta,Santu Rana,SvethaVenkatesh, and Subrahmanyam Murala. Multi-weather im-age restoration via domain translation.In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 2169621705, 2023. 2 Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, andHuizhu Jia. Ffa-net: Feature fusion attention network forsingle image dehazing. In Proceedings of the AAAI confer-ence on artificial intelligence (AAAI), pages 1190811915,2020. 2 Yuwei Qiu, Kaihao Zhang, Chenxi Wang, Wenhan Luo,Hongdong Li, and Zhi Jin. Mb-taylorformer: Multi-branchefficient transformer expanded by taylor formula for im-age dehazing.In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision (ICCV), pages1280212813, 2023. 2",
  "Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie.Enhanced pix2pix dehazing network.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition (CVPR), pages 81608168, 2019. 2": "Anurag Ranjan and Michael J Black. Optical flow estima-tion using a spatial pyramid network. In Proceedings of theIEEE conference on computer vision and pattern recognition(CVPR), pages 41614170, 2017. 3, 5 Wenqi Ren, Jingang Zhang, Xiangyu Xu, Lin Ma, XiaochunCao, Gaofeng Meng, and Wei Liu.Deep video dehazingwith semantic segmentation. IEEE transactions on imageprocessing (TIP), 28(4):18951908, 2018. 1, 2 Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, andNong Sang. Domain adaptation for image dehazing. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition (CVPR), pages 28082817, 2020. 2",
  "Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. arXivpreprint arXiv:1409.1556, 2014. 4": "Taeyong Song, Youngjung Kim, Changjae Oh, HyunsungJang, Namkoo Ha, and Kwanghoon Sohn.Simultaneousdeep stereo matching and dehazing with feature attention.International Journal of Computer Vision (IJCV), 128:799817, 2020. 1, 2 Yapeng Tian, Yulun Zhang, Yun Fu, and Chenliang Xu.Tdan: Temporally-deformable alignment network for videosuper-resolution. In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition (CVPR),pages 33603369, 2020. 3, 4 Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal MPatel. Transweather: Transformer-based restoration of im-ages degraded by adverse weather conditions. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 23532363, 2022. 2 Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, andChen Change Loy. Edvr: Video restoration with enhanceddeformable convolutional networks.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition workshops (CVPRW), pages 00, 2019. 3 Qingbo Wu, Jingang Zhang, Wenqi Ren, Wangmeng Zuo,and Xiaochun Cao. Accurate transmission estimation for re-moving haze and noise from a single image. IEEE transac-tions on image processing (TIP), 29:25832597, 2019. 2 Rui-Qi Wu, Zheng-Peng Duan, Chun-Le Guo, Zhi Chai,and Chongyi Li.Ridcp: Revitalizing real image dehaz-ing via high-quality codebook priors.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 2228222291, 2023. 1, 2, 6 Jiaqi Xu, Xiaowei Hu, Lei Zhu, Qi Dou, Jifeng Dai, Yu Qiao,and Pheng-Ann Heng.Video dehazing via a multi-rangetemporal alignment network with physical prior. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 1805318062, 2023. 2,3, 6, 1 Wenhan Yang, Jiaying Liu, and Jiashi Feng.Frame-consistent recurrent video deraining with dual-level flow.In Proceedings of the IEEE/CVF conference on computervision and pattern recognition (CVPR), pages 16611670,2019. 3 Xitong Yang, Zheng Xu, and Jiebo Luo. Towards percep-tual image dehazing by physics-based disentanglement andadversarial training. In Proceedings of the AAAI conferenceon artificial intelligence (AAAI), 2018. 2",
  "of the IEEE/CVF conference on computer vision and patternrecognition (CVPR), pages 20372046, 2022. 2, 6, 1": "Yijun Yang, Angelica I Aviles-Rivero, Huazhu Fu, Ye Liu,Weiming Wang, and Lei Zhu.Video adverse-weather-component suppression network via weather messengerand adversarial backpropagation.In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), pages 1320013210, 2023. 2 Tian Ye, Yunchen Zhang, Mingchao Jiang, Liang Chen, YunLiu, Sixiang Chen, and Erkang Chen. Perceiving and model-ing density for image dehazing. In European Conference onComputer Vision (ECCV), pages 130145. Springer, 2022. 2 Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and FengZhao. Source-free domain adaptation for real-world imagedehazing.In Proceedings of the 30th ACM InternationalConference on Multimedia (ACMMM), pages 66456654,2022. 2 Jiyang Yu, Jingen Liu, Liefeng Bo, and Tao Mei. Memory-augmented non-local attention for video super-resolution. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1783417843,2022. 4",
  "Huicong Zhang, Haozhe Xie, and Hongxun Yao.Spatio-temporal deformable attention network for video deblurring.In European Conference on Computer Vision (ECCV), pages581596. Springer, 2022. 3, 4": "Xinyi Zhang, Hang Dong, Jinshan Pan, Chao Zhu, YingTai, Chengjie Wang, Jilin Li, Feiyue Huang, and Fei Wang.Learning to restore hazy video: A new real-world dataset anda new method. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages92399248, 2021. 2, 3, 1 Shiyu Zhao, Lin Zhang, Ying Shen, and Yicong Zhou. Re-finednet: A weakly supervised refinement framework for sin-gle image dehazing. IEEE Transactions on Image Processing(TIP), 30:33913404, 2021. 2, 6, 1",
  "Figure S1. Visual comparison on REVIDE dataset": "To further verify the effectiveness of our proposedmethod, we evaluate the proposed method against SOTAmethods that require aligned ground truths. Table S1 re-ports the evaluation results on the REVIDE dataset in termsof PSNR and SSIM. We can see that our proposed methodobtains higher PSNR. In this work, we mainly focus onthe real-world video dehazing in driving scenarios. How-ever, we have also obtained good results on smoke data(REVIDE), indicating that our method is effective for bothsmoke/haze removal.We further present visual observation comparisons inFig. S1. The dehazing results of all the competitive meth-ods contain artifacts, and the detail restoration is not ideal.",
  "B.1. Spatio-temporal Misalignment Causes": "Here, due to real-world collection scenarios, as depictedin , avoidance maneuvers for pedestrians and vehi-cles on the road result in varying durations of collectedhazy/clear video pairs with the same starting and endingpoints.Consequently, temporal misalignment occurs inhazy/clear video pairs. Similarly, avoidance maneuvers alsolead to different shooting trajectories, causing spatial mis-alignment (i.e., pixel misalignment). Additionally, the dy-namic movement of pedestrians and vehicles contributes tospatial misalignment (i.e., semantic misalignment).",
  "B.2. Compare with Other Datasets": "Compared to the 1981 pairs of indoor smoke data from theREVIDE dataset, our non-aligned dataset GoProHazyconsists of a total of 4256 pairs, and the no-reference Driv-ingHazy dataset comprises 1807 frames of hazy images.Moreover, our outdoor scenes are more numerous and re-alistic compared to indoor settings. Furthermore, in con-trast to the large-scale synthetic dataset HazeWorld fromMAP-Net , our proposed GoProHazy and DrivingHazydatasets represent real driving scenarios under real-worldhazy weather conditions. This makes them more valuablefor research aimed at addressing dehazing in videos cap-tured under real-world conditions.",
  "C. More Ablation Studies": "The number of input frames. Table S2 demonstrates thatoptimal performance is achieved when using a three-frameinput. This is attributed to the advantage of utilizing mul-tiple frames to mitigate alignment issues, but it also intro-duces cumulative errors in alignment. As shown in Fig. S2,we also present the influence of different input frames onLmfr. Here, balancing efficiency considerations, we choosetwo frames as the input.",
  "D. More Discussions": "The impact of non-aligned scale.No doubt, the morealigned the hazy/clear frame pairs, the better the dehazingeffect. However, our primary focus here is on the bound-ary issues related to non-aligned scales. In the ablation ex-periments of NSDNet , it was revealed that, comparedto cases with ground truth (GT), a non-aligned pixel off-set exceeding 90 pixels (for an image size of 256256)results in a 0.7 dB decrease in PSNR, a 0.2 reduction instructural similarity (SSIM), and a decrease of 0.02 and 0.5in FADE and NIQE , respectively. We think that,in contrast to training with synthetic datasets, which mayresult in suboptimal dehazing in real-world scenes, the mi-nor performance decline introduced by non-alignment is en-tirely acceptable. Moreover, during real-world data collec-tion, we can easily control non-alignment within 90 pixels.",
  "E. More Visual Results": "The visualization of FCAS module. Here, we visualize theeffectiveness of the flow-guided attention sampler (FCAS)in feature alignment, as shown in Fig. S3. We observe thatthe features aligned by the FCAS module are nearly con-sistent with the features of the current frame. The opticalflow used to guide sampling is visualized in Fig. S3 (c).Note that the ablation study on the FCAS is visualized inthe main text.More visualizations of video dehazing. We present ad-ditional visual comparison results with state-of-the-art im-age/video dehazing methods on the GoProHazy dataset inFig. S6. We observe that our proposed DVD method out-performs in dehazing performance, particularly in distantvisibility and local detail restoration (i.e., texture and bright-ness of scenes). The same dehazing issues are evident in thevisual comparisons on the DrivingHazy and InternetHazydatasets. We present their visual comparisons separately inFig. S7 and Fig. S8.Applications. To highlight the benefits of dehazing resultsfor downstream tasks, we employ the image segmentation",
  "Figure S4. Visual results of object detection on the InternetHazydataset": "method FastSAM 1 to assess the gains brought by var-ious image/video dehazing methods. The test results, asshown in Fig. S9, reveal that our proposed method achievessuperior segmentation performance, particularly in the skyregion. For the parameter settings of FastSAM, We em-ployed the FastSAM-x model, setting the intersection overunion (IoU) to 0.8 and the object confidence to 0.005.In Fig. S4, we conducted an object detection (yolov82)to validate the driving-safety assistance. We see that bothvehicles and pedestrians are readily detected, enabling earlydetection by drivers and ensuring their safe operation.Video demo. To validate the stability of our video dehazingresults, we present a video result captured in a real drivingenvironment and compare it with the latest video dehazingstate-of-the-art method, MAP-Net . We have includedthis video-demo.mp4 file in the supplementary materials.Limitations. In dense hazy scenarios, our method may ex-hibit slight artifacts in the sky region during dehazing. Fromthe reported inference times in Table S1, it can be observedthat our method still fails to meet real-time requirements."
}