{
  "Abstract": "We introduce a simple method that employs pre-trainedCLIP encoders to enhance model generalization in the AL-FRED task. In contrast to previous literature where CLIPreplaces the visual encoder, we suggest using CLIP as anadditional module through an auxiliary object detection ob-jective. We validate our method on the recently proposedEpisodic Transformer architecture and demonstrate that in-corporating CLIP improves task performance on the unseenvalidation set. Additionally, our analysis results supportthat CLIP especially helps with leveraging object descrip-tions, detecting small objects, and interpreting rare words.",
  ". Introduction": "Embodied instruction following (EIF) tasks entail ex-ecuting fine-grained navigation and interaction action se-quences according to natural language directives.Thisrequires processing and understanding information fromheterogeneous sources to successfully navigate and inter-act with unseen environments .In the multimodalresearch community, large-scale pre-trained models havebeen shown to improve multimodal alignment and general-ization performance . In particular, several recentworks evaluate the CLIP (Contrastive Language Image Pre-training) models capabilities for embodied AI tasks,including object navigation and vision language nav-igation . The most common approach in this di-rection has been simply replacing the visual encoder withCLIPs visual encoder.In this work, we hypothesize that pre-training on large-scale image-text pairs will induce more generalizable multi-modal representations, leading to better performance in un-seen environments of the ALFRED task . In contrastto previous literature, we propose a simple model-agnosticmethod to use CLIP as an auxiliary module to take advan-tage of CLIPs multimodal alignment capabilities.Con-",
  ". ET-CLIP model as modified from": "cretely, we introduce a novel object detection loss with-out having to change the models architecture. We investi-gate the proposed method through preliminary experimentsbased on the Episodic Transformer (ET) architecture, acompetitive system on the ALFRED leaderboard. Our em-pirical results suggest that our novel loss objective improvesgeneralization to unseen environments, especially by allevi-ating the difficulty of detecting small objects and interpret-ing rare words which are challenging error conditions incurrent state-of-the-art models.",
  ". Proposed Approach": "We use CLIP as an auxiliary source of informationfor object detection and interaction by including CLIP asan additional module in ET . During training, we feedcamera observation inputs from ET into CLIP along with alist of all ALFRED object words (with none also beingan option). A predicted object for each camera observa-tion is obtained from both the CLIP module and ET, andwe compute their object prediction losses: LCLIP(obj) andLET(obj), respectively. The final object loss in ET is as fol-lows:",
  ". Preliminary Experiments & Results": "Experimental settingWe run our baseline experimentsbased on the code released by the authors of the ET paper1.More specifically, we use the base ET model, which doesnot employ the data augmentation strategy. We train boththe ET baseline and the ET-CLIP models for 20 epochs,and refer to the original ET model for hyperparameters. Theweighting coefficient of the auxiliary CLIP loss was cho-sen as 0.5 based on the magnitude of the two loss terms toensure that the loss ranges are similar in the two models.We note that the discrepancy of our results from stemsfrom different random seeds, as noted by the authors2. Results shows the results for success rate andgoal-conditioned success rate of the ET-Baseline and theET-CLIP models for the unseen validation splits. As seenin , the ET-CLIP model performs better in unseenscenes. This suggests that adding CLIP object detection asan auxiliary loss helps with generalization. We further an-alyze how CLIP aids in performance improvement for spe-cific error conditions, pertaining to task instruction charac-teristics in .",
  ". Analysis": "We investigate how integrating CLIP helps the ETmodels performance on natural language directives. In par-ticular, we look into three subsets of instructions that con-tain common sources of error: instructions including fine-grained object properties, small objects, and rare semantics.We report our results in . Object propertiesInterestingly, we find that ET-CLIPexcels at instructions, noting specific object characteris-tics such as colors (e.g., Turn around, walk to the redarm chair), improving the goal-conditioned success rateby 0.3%.The addition of our CLIP module facilitatesthe model to leverage specific visual cues stated in the",
  ". Goal-conditioned success rates on the unseen validationset of the ET-Baseline and ET-CLIP on subsets of instructions": "language directives more effectively, due to the vision-language alignment learned from pre-training. This is im-portant for correct object detection in embodied interactiontasks, especially when the environment requires semanti-cally disambiguating objects of the same class. Small objectsExisting state-of-the-art models in AL-FRED struggle with detecting small objects , asthey take up a negligible portion of the input image. Therange of success rates in this instruction subset (5.1-5.6)is lower compared to the global average (7.8-7.9), whichaligns with previous findings. Surprisingly, ET-CLIP im-proves the goal-conditioned success rate by 0.5% in in-structions that involve manipulating smaller objects, suchas pencil or keys. As the pre-trained CLIP model istrained with image-caption pairs, it is likely that the result-ing representations are conducive to the semantics of theimage, even when objects are small in size. Rare semanticsWe additionally validate the hypothesiswhether CLIP helps ET better understand instructions withrare words, which we define as words that appear less than30 times in the training set. Since CLIP is trained with nu-merous captions, it is likely that ET-CLIP can benefit fromthis knowledge and in turn interpret rare words better thanthe baseline. Our results show that ET-CLIP improves ETby 0.8% for rare semantics, which affirms our hypothesis.",
  ". Conclusion": "In this work, we explore the potential of incorporatingpre-trained CLIP encoders to the ALFRED task. The nov-elty of our method lies in leveraging CLIP as an additionalmodule through an auxiliary object detection loss. Our ap-proach can be easily applied to other models that employobject detectors. Our modification upon the Episodic Trans-former model shows that using CLIP improves task per-formance especially in unseen environments, enhancing themodels ability to deal with object properties, small objects,and rare semantics. In future work, we hope to validate theeffectiveness of our approach on other models in the field ofembodied instruction following, to further improve wherecurrent models are failing.",
  "We would like to thank Yonatan Bisk for his guidancethroughout this project": "Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, andYoav Artzi. A persistent spatial semantic representation forhigh-level natural language instruction execution. In Confer-ence on Robot Learning, pages 706717. PMLR, 2022. 1 Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, andJing Shao. Democratizing contrastive language-image pre-training: A clip benchmark of data, model, and supervision.arXiv preprint arXiv:2203.05796, 2022. 1",
  "Alexander Pashevich, Cordelia Schmid, and Chen Sun.Episodic Transformer for Vision-and-Language Navigation.In ICCV, 2021. 1, 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision. In Marina Meilaand Tong Zhang, editors, Proceedings of the 38th Interna-tional Conference on Machine Learning, volume 139 of Pro-ceedings of Machine Learning Research, pages 87488763.PMLR, 1824 Jul 2021. 1",
  "Mohit Shridhar, Jesse Thomason, Daniel Gordon, YonatanBisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer,": "and Dieter Fox.Alfred:A benchmark for interpretinggrounded instructions for everyday tasks. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1074010749, 2020. 1 Allison C Tam, Neil C Rabinowitz, Andrew K Lampinen,Nicholas A Roy, Stephanie CY Chan, DJ Strouse, Jane XWang, Andrea Banino, and Felix Hill. Semantic explorationfrom language abstractions and pretrained representations.arXiv preprint arXiv:2204.05080, 2022. 1"
}