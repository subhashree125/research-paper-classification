{
  "Abstract": "Existing perception methods for autonomous driving fallshort of recognizing unknown entities not covered in thetraining data.Open-vocabulary methods offer promis-ing capabilities in detecting any object but are limited byuser-specified queries representing target classes. We pro-pose AutoVoc3D, a framework for automatic object classrecognition and open-ended segmentation. Evaluation onnuScenes showcases AutoVoc3Ds ability to generate pre-cise semantic classes and accurate point-wise segmenta-tion. Moreover, we introduce Text-Point Semantic Similar-ity, a new metric to assess the semantic similarity betweentext and point cloud without eliminating novel classes.",
  ". Introduction": "Existing perception methods for au-tonomous driving often rely on an inclusiveness assump-tion that all potential categories of interest must exist in thetraining dataset. Nevertheless, public datasets often anno-tate instances with pre-defined categories, which can varyfrom three (e.g. vehicle, cyclist and pedestrian) toseveral dozen types , and fail to annotate rare objectswith correct semantic labels. Failing to recognize atypicalobjects or road users poses a significant risk to the percep-tion models adaptability to diverse real-life scenarios.The development of Vision-Language Models (VLMs)strengthens the connection between vision and languageand promotes progress in multi-modal tasks, such as im-age captioning and open-vocabulary learning .Open-vocabulary learning methods often utilize pre-trainedVLMs to find the correspondence between visual entitiesand a semantic vocabulary, thereby creating the potentialto detect any category of interest . However, thesemethods rely on user-specified queries, and thus can notspontaneously recognize the object categories in a scene.This shortcoming severely limits the real-life applicabilityof such methods, as newly encountered object categoriescould be unknown - i.e. not yet specified - to the model.In this work, we propose AutoVoc3D, a framework that",
  "AutoVoc3D (Ours)": ". Comparison between Human Annotation and Au-toVoc3D on nuScenes Dataset . AutoVoc3D segments regionswith precise class names, e.g. building and pole (highlighted bygreen boxes), while the dataset is annotated with a more generalterm, namely manmade. automatically recognizes what objects are present in thescene, generates a vocabulary for them and segments thetarget categories using a point cloud segmentor. We evalu-ate our method on the nuScenes dataset and introduce aText-Point Semantic Similarity (TPSS) metric to assess themodel performance based on semantic consistency in CLIPspace. compares the human annotations based on afixed vocabulary with the result of AutoVoc3D with an auto-",
  "arXiv:2406.09126v2 [cs.CV] 25 Jul 2024": "matically generated vocabulary. AutoVoc3D generates con-vincing semantic classes as well as accurate point-wise seg-mentations. Moreover, whereas the pre-defined categoriesare general and ambiguous, e.g. manmade, AutoVoc3D rec-ognizes buildings and poles, which are semantically moreprecise. Our contributions can be summarized as follows: We extend auto-vocabulary segmentation to the3D domain, namely auto-vocabulary LiDAR point seg-mentation. In this task, images and LiDAR points aregiven, but no text queries indicate the target categories.In contrast, the semantic label of each point is automati-cally generated.",
  ". Open-Vocabulary Segmentation": "Open-Vocabulary Segmentation (OVS) aims to performsegmentation based on a list of arbitrary text queries.CLIP achieves this in 2D domain by aligning visionand text in a latent space using natural language as super-vision on 400 million text-image pairs.However, thereis no point cloud dataset of the same nature that allowsfor training at such scale. Additionally, captions in pointcloud datasets are usually much sparser.Therefore, acommon solution is to freeze the text encoder and imageencoder, and push point features to vision-language fea-ture space . ULIP conducts con-trastive learning between text-image-point triplets, whichdistills vision and language knowledge to a point encoder.CLIP2Scene trains the point encoder in a self-supervisedlearning manner. Specifically, it contrasts features betweenpoint-text pairs and pushes point features to vision-languagefeature space by considering spatial-temporal consistency.OpenScene projects LiDAR points into images to findpoint-pixel correspondences and supervises the point en-coder by pixel features generated by the CLIP-based imageencoder. Although OVS shows encouraging performancein both 2D and 3D domains, it still relies on user-specifiedcategories as prompts to guide segmentation. Conversely,our approach automatically generates categories that poten-tially appear in the scene without any human in the loop, asshown in . . Open-Vocabulary vs. Auto-Vocabulary Segmenta-tion. The main difference is that auto-vocabulary methods gener-ate text queries automatically from the input, while they need tobe provided by a user in the open-vocabulary case.",
  ". Auto-Vocabulary Segmentation": "In Open-Vocabulary Segmentation, images or point cloudsare grouped into coherent semantic regions and classifiedwith user-provided target categories. Auto-Vocabulary Seg-mentation (AVS), however, performs segmentation withoutthe need to specify target categories. Relevant target cate-gories are directly inferred from the image - usually withoutany additional training, finetuning, data sourcing or anno-tation effort - instead. The Zero-Guidance Segmentationparadigm was the first to achieve this . The authorsused clustered DINO embeddings to obtain binary ob-ject masks. These masks were subsequently utilized to ad-just the attention of CLIP, resulting in embeddings that aremore accurately targeted to individual segments. Finally, atrained large language model is tasked to output texts clos-est to these embeddings. While this required switching be-tween three different latent representations, AutoSeg proposed a more direct approach based on BLIP em-beddings only. They introduced the BLIP-Cluster-Captionprocedure, in which multi-scale BLIP embeddings are en-hanced through clustering, alignment and denoising. Theenhanced embeddings are captioned using the BLIP de-coder and parsed into nouns. Finally, the nouns are passedto an OVS model for segmentation.",
  ". Problem Definition": "Given a point cloud P = {(pn)}Nn=1 RN3 with N be-ing the number of points, the aim is to compute the corre-sponding semantic labels for every point. Note that the setof candidate labels differs among different task settings. Inconventional LiDAR segmentation, the label set is predeter-mined according to the dataset. Open-vocabulary segmen-tation allows users to specify the label candidates. In auto-vocabulary segmentation, the label set is scene-specific andautomatically generated. In other words, the label set is nei-ther pre-defined nor user-specified.",
  "In this work, we adapt and tailor 2D auto-vocabulary seg-mentation, as mentioned in Sec. 2.2, to the 3D domain. Li-": "DAR point cloud datasets consist of both images and Li-DAR scans, which enables the automatic creation of a vo-cabulary with an image-based captioning model. To thisend, we employ AutoSeg , a training-free method basedon BLIP , to generate high-quality nominal queries. Asshown in (b), given a LiDAR point cloud and its cor-responding images, we feed the images into AutoSeg. Sincea point cloud often corresponds to several images, we obtainseveral sets of queries. These queries are merged, dedupli-cated and then input to an open-vocabulary LiDAR pointsegmentor. We employ OpenScene as the segmentor.OpenScene is an open-vocabulary segmentor for 3D pointclouds, where the point encoder is aligned to CLIP vision-language latent space by point-wise feature distillation fromthe CLIP image features. We utilize the distilled weights ofOpenScene, input generated queries and LiDAR points toOpenScene and obtain a point-wise segmentation.",
  ". Challenges": "Since auto-vocabulary segmentation is a novel task, thereis no established benchmark to compare different meth-ods.Open-vocabulary segmentation, which is the mostsimilar but easier task, can be evaluated on conventionalsegmentation datasets by using pre-defined categories asqueries. However, auto-vocabulary segmentation lacks ac-cess to these categories, which poses a hurdle when eval-uating its performance. On the other hand, as pointed outin AutoSeg , natural language is ambiguous, leading tocomplex relations between two classes, such as synonymity,hyponymy, hypernymy, meronymy or holonymy. There-fore, it is challenging to assert that an instance is accuratelyallocated an appropriate semantic label. For instance, a roadcan be identified as a drivable surface, street, or roadway.In addition, a tyre can be classified as either an independententity or as a constituent part of a vehicle. Therefore, weexplore two quantitative metrics to assess our method andpresent qualitative results as and Fig 3.",
  ". LLM-based Auto-Vocabulary Evaluator": "In order to enable the evaluation of segmentation with au-tomatically generated categories, proposed an LLM-based Auto-Vocabulary Evaluator (LAVE) employing theLlama 2 7B Large Language model1. LAVE is designedto map each unique auto-vocabulary category (e.g. sedanand air) to a category in the list of fixed ground truth classes(e.g. car and sky) in the dataset. Hence, after the segmen-tation of the LiDAR point cloud with auto-vocabulary cate-gories, each classification is updated according to the map-ping generated by LAVE. For instance, points segmentedas a sedan become points belonging to the car category.",
  ". Text-Point Semantic Similarity": "In this section, we elaborate on the intuition and thedefinition of the proposed Text-Point Semantic Similarity(TPSS). The CLIP model comprises an image encoderand a text encoder. The optimization objective is to maxi-mize the similarity between the features of a paired text andimage. CLIP is believed to properly align visual and textfeatures due to its superior performance on vision-languagetasks. In other words, the outputs of its image encoder andtext encoder reside in a vision-language latent space. Attest time, the inference relies on the assumption that a textmatches an image better if its text feature is more similarto the image feature than other texts. We design the TPSSbased on a similar assumption that a text matches a pointbetter if its text feature is more similar to the point featurethan other texts. Formally, let P = {(pn)}Nn=1 be a pointcloud with N points and L = {(lm)}Mm=1 be a set of Munique semantic labels generated for this point cloud. Thetext embeddings E = {(em)}Mm=1 and the point feature em-beddings F = {(fn)}Nn=1 are obtained as follows:",
  "TPSS(P, L, hp, ht) = meann(Sn)(5)": "where Sn is a point-wise similarity score for the point n.TPSS(P, L, hp, ht) measures the text-point semantic sim-ilarity between the point cloud P and the label set L giventwo encoders. Therefore, if both the point cloud and thepoint encoder remain unchanged, the TPSS can comparewhich label set matches the point cloud better. Note that theTPSS only applies if the point encoder is aligned with theCLIP encoder, which is where our framework fits.",
  ". Experiments": "Qualitative comparisons are presented in and .Because both human annotations and OpenScene takepre-defined categories as references, they are unable to in-vestigate novel categories proactively.In contrast, Au-toVoc3D yields more precise categories, e.g. building, poleand sign. Tomakeaquantitativecomparison,weemployLAVE to map all generated novel categories backto pre-defined categories.Next, we calculate segmenta-tion metrics, namely mean IoU (mIoU) and mean accuracy(mAcc), on the nuScenes validation set (see Tab. 1).Note that the three types of methods are solving differenttasks. Compared to open-vocabulary methods, AutoVoc3Ddoes not require user-specified class names at test time, yetachieves comparable performance. However, using LAVE makes a concession, as itdiscards novel categories after the mapping. The proposedTPSS metric does not suffer from this issue. Moreover, itprovides a more accurate assessment of the semantic sim-ilarity between the text and point cloud. AutoVoc3D out-performs OpenScene (see Tab. 2), indicating the gen-erated label set is a better match with the point clouds thanpre-defined categories.",
  ". Conclusion": "In this work, we presented AutoVoc3D, the first method forauto-vocabulary LiDAR point segmentation. Unlike OVSmethods which require the user to specify target classes, ourmethod is capable of automatically generating and segment-ing them. Our experiments demonstrate that our model isable to segment regions with more precise object namings,as well as competitive accuracy, without the need for anyadditional training, finetuning or additional data. Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall.Se-manticKITTI: A dataset for semantic scene understanding ofLiDAR sequences. In ICCV, pages 92969306. IEEE, 2019.1",
  "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for unifiedvision-language understanding and generation.In ICML,2022. 1, 2, 3": "Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-hao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer:Learning birds-eye-view representation from multi-cameraimages via spatiotemporal transformers. In ECCV, 2022. 1 Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang,Jingheng Chen, Xiaodan Liang, Yamin Li, Chaoqiang Ye,Wei Zhang, Zhenguo Li, Jie Yu, Hang Xu, and Chunjing Xu.One million scenes for autonomous driving: Once dataset.In NeurIPS, 2021. 1 Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles R. Qi, XinchenYan, Scott Ettinger, and Dragomir Anguelov. Unsupervised3d perception with 2d vision-language distillation for au-tonomous driving. In ICCV, 2023. 2",
  "Pitchaporn Rewatbowornwong, Nattanat Chatthee, EkapolChuangsuwanich, and Supasorn Suwajanakorn.Zero-guidance segmentation using zero segment labels. In ICCV,2023. 2": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, AurelienChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Et-tinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang,Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov.Scalability in perception for autonomous driving: Waymoopen dataset. In CVPR, pages 24432451, 2020. 1",
  "Andrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchi-cal multi-scale attention for semantic segmentation. arXivpreprint arXiv:2005.10821, 2020. 1": "Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, HenghuiDing, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong,Xudong Jiang, Bernard Ghanem, and Dacheng Tao. Towardsopen vocabulary learning: A survey. TPAMI, 2024. 1 Le Xue, Mingfei Gao, Chen Xing, Roberto Martn-Martn,Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles,and Silvio Savarese. Ulip: Learning a unified representationof language, images, and point clouds for 3d understanding.In CVPR, 2023. 2",
  "Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In CVPR, 2021. 1": "Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han,Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, ZhenYang, Xiaodan Liang, and Hang Xu.Clip$2$:Con-trastive language-image-point pretraining from real-worldpoint cloud data. In CVPR, 2023. 2 Xingcheng Zhou, Mingyu Liu, Bare Luka Zagar, Ekim Yurt-sever, and Alois C. Knoll.Vision language models inautonomous driving and intelligent transportation systems.arXiv preprint arXiv:2310.14414, 2023. 1"
}