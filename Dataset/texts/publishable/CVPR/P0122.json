{
  "OursGT": ". Given images under different camera pose and light positions (a), our method learns the neural radiance fields that enable novelview synthesis and relighting (b), and intrinsic decomposition (c) simultaneously. Other image editing applications (d), such as reflectanceediting, reflectance editing + relighting, and shading editing (simulating two lights), can also be employed.",
  "Abstract": "The task of extracting intrinsic components, such as re-flectance and shading, from neural radiance fields is ofgrowing interest.However, current methods largely fo-cus on synthetic scenes and isolated objects, overlookingthe complexities of real scenes with backgrounds. To ad-dress this gap, our research introduces a method that com-bines relighting with intrinsic decomposition. By leverag-ing light variations in scenes to generate pseudo labels, ourmethod provides guidance for intrinsic decomposition with-out requiring ground truth data. Our method, groundedin physical constraints, ensures robustness across diversescene types and reduces the reliance on pre-trained mod-els or hand-crafted priors. We validate our method on bothsynthetic and real-world datasets, achieving convincing re-sults. Furthermore, the applicability of our method to imageediting tasks demonstrates promising outcomes.",
  ". Introduction": "Recent advances in neural rendering have made significantstrides in novel view synthesis , ranging from smallobjects to large-scale scenes. Concurrently, there has beenan exploration towards scene editing , such as recol-oring and relighting . To facilitate editing, itoften becomes necessary to decompose scenes into editable sub-attributes. Within the task of scene decomposition intogeometry, reflectance, and illumination using neural render-ing, two lines of work are particularly noteworthy: inverserendering and intrinsic decomposition.The first approach integrates inverse ren-dering with neural rendering methods for scene decomposi-tion. They often employ the BRDF model, such as the sim-plified Disney BRDF model, to model material proper-ties and jointly optimize geometry, BRDF, and environmen-tal lighting. However, inverse rendering presents a highlyill-posed challenge: separating material properties and illu-mination in images often yields ambiguous results, and trac-ing light within scenes is computationally intensive. Thesefactors limit inverse rendering to object-specific scenarios.Thesecondapproach,basedonintrinsicdecomposition,aimstoprovideaninterpretablerepresentation of a scene (in terms of reflectance andshading) suitable for image editing. It can be considereda simplified variant of inverse rendering, making it moreapplicable to a broader range of scenarios, including indi-vidual objects and more complex scenes with backgrounds.However, despite simplifications over inverse rendering,previous attempts at applying intrinsic decompositionto neural rendering have shown limited success.Thismotivates our work in this paper.Our inspiration is drawn from the idea of using neuralrendering to combine relighting and intrinsic decomposi-",
  "arXiv:2406.11077v1 [cs.CV] 16 Jun 2024": "tion, aiming not only to enhance the quality of intrinsic de-composition but also to expand editing capabilities. Just asexperts in mineral identification illuminate specimens fromvarious angles to reveal their features, varying light sourcepositions are essential for uncovering a scenes intrinsic de-tails. In fact, the connection between relighting and intrin-sic decomposition has been discussed in previous works on2D images , but it has yet to be explored in neuralrendering. Additionally, the field of neural rendering hassignificantly explored relighting . While IntrinsicN-eRF has pioneered the integration of intrinsic decom-position within NeRF, they have not utilized relighting orfully leveraged the 3D information available through neuralrendering. Instead, we focus on physics-based constraintsto enhance the intrinsic decomposition performance.In this paper, we propose a two-stage method. In thefirst stage, we train a neural implicit radiance representa-tion to enable novel view synthesis and relighting. Basedon the results of this stage, we calculate normals and lightvisibility for each training image, which allows us to de-velop a method for generating pseudo labels for reflectanceand shading. In the second stage, we treat reflectance andshading as continuous functions parameterized by Multi-Layer Perceptrons (MLPs). During training, we apply con-straints based on physical principles and our pseudo labels.Notably, our approach does not depend on any pre-trainedmodels or ground truth data for intrinsic decomposition, yetachieves convincing results, as shown in . Our contri-butions are summarized as follows: We propose a method that integrates relighting with in-trinsic decomposition, allowing for novel view synthesis,lighting condition altering, and reflectance editing.",
  ". Related Work": "Intrinsic decomposition. Intrinsic decomposition is a clas-sical challenge in computer vision , with much of the pre-vious research focused on the 2D image. A keydifficulty in this area is the scarcity of real datasets, whichneed complicated and extensive annotation. This limita-tion has spurred interest in semi-supervised and unsuper-vised techniques. IntrinsicNeRF has beena pioneer in applying intrinsic decomposition to neural ren-dering. Similar to previous unsupervised methods in 2D,it utilizes hand-crafted constraints, including chromaticityand semantic constraints, for guidance. However, these con-straints do not accurately reflect physical principles and of- ten fall short in complex scenarios. Our approach leans on3D information and physical constraints (e.g., variations inillumination) to achieve superior results.Relighting.Relighting has recently garnered attentionfrom various perspectives within the field . Data-drivenapproaches have been explored, with research focusingon portrait scenes and extending tomore complex scenarios . Kocsis et al. have also investigated lighting control within diffusionmodels, enabling the generation of scenes under varyinglighting conditions. Meanwhile, relighting has also receivedwidespread attention within the field of neural rendering, achieving impressive relighting outcomeswithin individual scenes.",
  "I(i, j) = R(i, j) S(i, j) + Re(i, j)(1)": "where R, S and Re denote Reflectance, Shading and Resid-ual, respectively.Our method extends implicit neural representation for re-lighting and intrinsic decomposition. We propose a two-stage approach, illustrated in . In the first stage, wetrain our model to represent scenes under varying camerapositions and lighting conditions, enabling novel view syn-thesis and relighting. We then apply three steps to gener-ate pseudo labels for reflectance and shading. In the sec-ond stage, we expand the model to decompose intrinsics us-ing these pseudo labels as constraints. Our proposed modelachieves novel view synthesis, relighting, and intrinsic de-composition simultaneously.3.1. Stage 1: Learning to Relight",
  "multiplication": ". Method Framework: Stage 1 involves learning the neural field with relighting (top left). Post-processing and generating pseudolabels (right). In Stage 2, the learning process continues to learn intrinsic decomposition based on the model trained in Stage 1 and thepseudo labels (bottom left).3.2. Physics-based Pseudo Label Generation Our proposed post-processing aims to generate pseudo la-bels for reflectance and shading in three steps, as illustratedin (right). Based on the physics modeling of imageformation, we start with generating pseudo shading by thenormal and light visibility. We then generate pseudo re-flectance using multiple images and shadings under differ-ent illumination. Details can be found in the supplementary.Step A. The normals are derived from the SDF network.The geometry network also provides depth informationwhich is used to estimate the intersection points in conjunc-tion with sphere tracing. Light visibility, which indicateswhether a point is directly illuminated, is obtained by spheretracing based on the light position and intersection points.Step B. The generation of pseudo-shading follows the for-mula, S = (( N L)V ), where the optimal shading S isthe multiplication of the light visibility V and the dot prod-uct of the normal N and the light ray L. () represents forgamma correction. This correction is essential because thehuman eyes perception of brightness is not linear. Thus,we apply it to accommodate the perceptual effect, yieldingto our pseudo shading.Step C. Our method infers pseudo reflectance from pseudoshading using R = I/S. For pseudo labels, this calcula-tion is only applicable in the case of direct illumination.We utilize various lighting conditions to obtain differentreflectance values; and by employing K-means alongwith the confidence related to pseudo shading, we merge toform the most probable reflectance map. Areas lacking di-rect illumination are filled using a strategy considering pixeldistance, normals, and RGB colors, resulting in the final",
  ". Stage 2: Learning Intrinsic Decomposition": "As illustrated in (bottom-left), we jointly learn the re-lighting and intrinsic decomposition. Expanding the modelfrom Stage 1, we add two extra MLPs dedicated to gener-ating reflectance and shading outputs, while the geometrynetwork is frozen. Note that, while all MLPs receive SDFfeature inputs, the RGB color MLP accepts spatial points,camera pose, and light positions as input, the reflectanceMLP only receives spatial points, and the shading MLPtakes spatial points and light positions.After volume rendering, we obtain RGB images, alongwith reflectance and shading. Subsequently, the residual isderived from Eq. (1). During training, the pseudo labels areused to impose constraints on reflectance and shading.",
  "Lintrinsic = WR R R1 + WS S S1(4)": "where R and S represent the predicted reflectance and shad-ing, respectively, and R and S are their correspondingpseudo labels. WR and WS represent weight maps for re-flectance and shading, derived during pseudo label gener-ation.As demonstrated in , the diffuse componentsdominate the scene, so it is crucial to prevent the trainingfrom converging to undesirable local minima (R = 0, S =0, Re = I). Therefore, we introduce a regularization term,Lreg = Re1, to ensure that the image is primarily recov-ered through R and S. Finally, the Stage 2 loss is:",
  ". Experiments": "We conduct experiments on both the NeRF (synthetic)and the ReNe (real) datasets.Detailed setup canbe found in the supplementary. We compare our methodwith traditional learning-based methods (PIE-Net andCareaga et al. ) and the state-of-the-art neural renderingapproach (IntrinsicNeRF ).Tab. 1 displays our methods quantitative results com-pared with other methods on the NeRF dataset. Since In-trinsicNeRF struggles with datasets that have lighting vari-ations, we use the numbers from the original publicationfor comparison. For both the Hotdog and Lego scenes, ourapproach surpasses others in terms of both reflectance andshading across all metrics. presents the qualitative comparison of our methodagainst others on both the synthetic NeRF dataset and thereal-world ReNe dataset. On the NeRF dataset, we firstshowcase the outcomes of our synthesized novel views andlighting conditions on the left, demonstrating results closelyaligned with the GT. Then, we display the results of in-trinsic decomposition compared to other approaches. It isevident that our results are quite convincing and outper-form those of others, with almost no lingering cast shad-ows in the reflectance. The latter part shows results fromthe challenging Rene dataset, characterized by real sceneswith backgrounds. Our rendering effects, displayed on theleft, closely approximate the GT. Moreover, our method isthe only one that achieves credible results in intrinsic de- composition. In terms of reflectance, the objects textureedges are sharp, the colors are vibrant, and shadows are ac-curately eliminated. In contrast, the results from PIE-Netand Careaga et al. are blurry and fail to remove shad-ows correctly. The other neural rendering method, Intrin-sicNeRF , also fails to achieve correct decomposition,primarily attributed to the failure in distinguishing intrinsiccomponents and also the difficulty in scene reconstruction. 5. ConclusionWe introduce a neural rendering method that learns relight-ing and intrinsic decomposition from multi-view imageswith varying lighting without the intrinsic GT. This ap-proach supports the creation of new views, relighting, anddecomposition simultaneously, serving as a versatile toolfor editing tasks like reflectance and shading adjustments.Our tests on both synthetic and real-world datasets validateour methods effectiveness. This method, grounded in ba-sic physical concepts rather than predefined priors, showspromise for more complex scene analyses. In the future, weaim to extend our experiments to explore a more compre-hensive set of scenes.Acknowledgement: Thanks to Hassan Ahmed Sial for hisassistance in generating the synthetic scenes.YY, MVand RB were supported by Grant PID2021-128178OB-I00funded by MCIN/AEI/10.13039/501100011033, and Gen-eralitat de Catalunya 2021SGR01499. YY is supported byChina Scholarship Council.",
  "Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, andDavid Wipf. Revisiting deep intrinsic image decompositions.2018. 2": "Duan Gao, Guojun Chen, Yue Dong, Pieter Peers, Kun Xu,and Xin Tong. Deferred neural lighting: free-viewpoint re-lighting from unstructured photographs. ACM Transactionson Graphics (TOG), 39(6):258, 2020. 2 Elena Garces, Carlos Rodriguez-Pardo, Dan Casas, andJorge Lopez-Moreno. A survey on intrinsic images: Delv-ing deep into lambert and beyond. International Journal ofComputer Vision, 130(3):836868, 2022. 2",
  "preprint arXiv:2009.12798, 2020. 2": "Andrew Hou, Michel Sarkis, Ning Bi, Yiying Tong, andXiaoming Liu. Face relighting with geometrically consis-tent shadows. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 42174226, 2022. 2 Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Song-fang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, and HaoSu. Tensoir: Tensorial inverse rendering. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2023. 1",
  "IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 51245133, 2020. 2": "Rohit Kumar Pandey, Sergio Orts Escolano, Chloe LeGen-dre, Christian Haene, Sofien Bouaziz, Christoph Rhemann,Paul Debevec, and Sean Fanello. Total relighting: Learningto relight portraits for background replacement. 2021. 2 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B.Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M.Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machinelearning in Python. Journal of Machine Learning Research,12:28252830, 2011. 3",
  "Computer Vision (ECCV), 2022. 2": "Pratul P Srinivasan,Boyang Deng,Xiuming Zhang,Matthew Tancik, Ben Mildenhall, and Jonathan T Barron.Nerv: Neural reflectance and visibility fields for relight-ing and view synthesis. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 74957504, 2021. 2 Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, ZexiangXu, Xueming Yu, Graham Fyffe, Christoph Rhemann, JayBusch, Paul E Debevec, and Ravi Ramamoorthi.Singleimage portrait relighting. ACM Trans. Graph., 38(4):791,2019. 2 Marco Toschi, Riccardo De Matteo, Riccardo Spezialetti,Daniele De Gregorio, Luigi Di Stefano, and Samuele Salti.Relight my nerf: A dataset for novel view synthesis andrelighting of real world objects.In Proceedings of the",
  "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-moncelli. Image quality assessment: from error visibility tostructural similarity. IEEE transactions on image processing,13(4):600612, 2004. 2": "Ziyi Yang, Yanzhen Chen, Xinyu Gao, Yazhen Yuan, YuWu, Xiaowei Zhou, and Xiaogang Jin.Sire-ir: Inverserendering for brdf reconstruction with shadow and illumi-nation removal in high-illuminance scenes. arXiv preprintarXiv:2310.13030, 2023. 1 Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, MarcPollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicN-eRF: Learning Intrinsic Neural Radiance Fields for EditableNovel View Synthesis.In Proceedings of the IEEE/CVF",
  ". Pseudo Label Generation": "Here, we elaborate on the post-processing steps () inthe main paper Sec. 3.2. It starts with generating pseudoshading based on Lambertian reflection principles. Underthe assumption that the light intensity and color remain con-stant, shading can be approximated by the dot product be-tween the normal ray and the light ray. The light ray en-compasses both direct/indirect illumination and necessitateslight visibility to account for occlusion effects.",
  "S = (( N L) V )(6)": "where the optimal shading S is the multiplication of thelight visibility V and the dot product of the normal Nand the light ray L. () represents for gamma correction.This correction is crucial because the human eyes percep-tion of brightness is not linear. Most images we see haveundergone gamma correction to accommodate this percep-tual effect. Therefore, calculating shading also necessitatesgamma correction, yielding to our defined pseudo shading.",
  ". Step C: generate pseudo reflectance": "This step entails inferring the most probable pseudo re-flectance from the pseudo shading, principally based on theequation R = I/S. The approach has two main points thatshould be noted here.First, the current pseudo shading only considers directlight.As seen in previous papers , solvingfor indirect light is a complex and computationally expen-sive process.Our novel approach leverages the trainedmodel to generate multiple versions of images under dif-ferent lighting conditions, each accompanied by respective",
  ". Post-processing and generating pseudo labels": "pseudo shadings. As direct light strengthens on a pixel,the influence of indirect light diminishes, making the re-flectance derived from higher pseudo shading values morereliable. We compare the outcomes under multiple lightingconditions and synthesize the most credible reflectance foreach pixel based on the intensity of pseudo shading.Second, the residual term includes specularity and othereffects that are not considered in R = I/S. Specular high-lights, which have high pseudo shading values, do not re-flect the object color but rather the light source color (e.g.,white reflections). By analyzing different lighting condi-tions, where highlights typically vanish except under spe-cific angles, we can deduce the object color by selecting themost common reflectance outcomes.Our implementation employs the K-means algorithm, in-corporating the weights of pseudo shading. This approachallows us to achieve a merged reflectance under varied light-ing conditions, as shown in the intermediate result at thebottom in . However, some regions within the mergedreflectance may appear vacant due to the absence of directillumination in all lighting conditions. So, we address theseareas with a filling strategy. This strategy specifically con-siders the distance between void and non-void pixels, theirnormals, and their colors in the RGB image, thereby achiev-ing the final pseudo reflectance.Additionally, we compute weight maps WR and WS for both pseudo reflectance and pseudo shading based on theedges of pseudo shading and visibility. Areas with higherpseudo shading values, or those further from visibility edges(where visibility calculations may be prone to errors), ex-hibit greater credibility in their pseudo labels; conversely,areas closer to visibility edges or with lower pseudo shad-ing values are deemed less reliable.",
  ". Experimental Settings": "Datasets. To validate our approach, we conduct experi-ments on both synthetic and real-world datasets.For the synthetic dataset, models are obtained fromNeRF , with lighting configurations borrowed fromZeng et al. . To facilitate quantitative analysis, GT forreflectance, shading, and residuals are rendered in Blender.Each scene comprises 500 images for training, 100 for val-idation, and 100 for testing, including intrinsic componentsfor each image. Importantly, adhering to the configurationsin , the settings for lighting and camera poses are man-aged independently.The real dataset we use is the ReNe dataset , wherelighting and camera poses are grid-sampled. This datasetfeatures 2000 images across scenes, captured from 50 dif-ferent viewpoints under 40 lighting conditions. Followingtheir dataset split, we use 1628 images (44 camera poses 37 light positions) for training.Additionally, given that the settings of lights and cam-eras are dependent on the former one and grid-sampled inthe latter, our proposed method is designed to accommodateboth configurations.Metrics. To evaluate the comparison between predicted im-ages and ground truth (GT), we employ the following met-rics: Peak Signal-to-Noise Ratio (PSNR), Structural Sim-ilarity Index (SSIM) , and Learned Perceptual ImagePatch Similarity (LPIPS) .Implementation details. Our models hyperparameters in-clude a batch size of 2048 and each stage was trained for500k iterations.We implemented the model in PyTorchand used the AdamW optimizer with a learning rate of1e3 for optimization. The experiments can be conductedon a single Nvidia RTX 3090 or A40 GPU. The weights oflosses, weik, wcurv, wintrinsic, wreg are set to 0.1, 5e4, 1.0,and 1.0, respectively."
}