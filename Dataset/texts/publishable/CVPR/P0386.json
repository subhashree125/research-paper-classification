{
  "Abstract": "Exploring dense connectivity of convolutional operatorsestablishes critical synapses to communicate feature vec-tors from different levels and enriches the set of transfor-mations on Computer Vision applications. Yet, even withheavy-machinery approaches such as Neural ArchitectureSearch (NAS), discovering effective connectivity patternsrequires tremendous efforts due to either constrained con-nectivity design space or a sub-optimal exploration processinduced by an unconstrained search space. In this paper,we propose CSCO, a novel paradigm that fabricates ef-fective connectivity of convolutional operators with mini-mal utilization of existing design motifs and further utilizesthe discovered wiring to construct high-performing Con-vNets. CSCO guides the exploration via a neural predictoras a surrogate of the ground-truth performance. We intro-duce Graph Isomorphism as data augmentation to improvesample efficiency and propose a Metropolis-Hastings Evo-lutionary Search (MH-ES) to evade locally optimal archi-tectures and advance search quality. Results on ImageNetshow 0.6% performance improvement over hand-craftedand NAS-crafted dense connectivity. Our code is publiclyavailable here.",
  ". Introduction": "The fundamental success of Convolutional Neural Net-work (CNN) on Computer Vision lies in the effectivewiring pattern, represented by dense connectivity withinconvolutional layers and atomic-level neurons .Throughout neural synapses, a convolution operator, as anelementary atomic building operator, establishes receptivefields to extract spatial-local information in 2D images.However, from classic CNNs to modernizedCNNs driven by Neural Architecture Search (NAS) [40, 41], the construction of CNNs mainly innovates an effec-tive building block composed of a combination of buildingoperators and directly stacks a few copies of these operatorsto construct the overall architecture. On images, most CNNdesigns are constrained to a chain-like structure without del-icate consideration of building block connectivity. On theone hand, hardware is designed to handle better chain-likearchitectures such as MobileNets . On the otherhand, chain-like CNN architectures are easier to study, re-quiring less extensive efforts to fully explore, thus yieldinga better rate of improvement (ROI) in research and devel-opment on vision benchmarks. The limitations in the afore-mentioned chain-like designs may prevent the discovery ofeffective inter-block synapses that enhance feature interac-tion in different positions of CNN architectures.As a result, more recent works start to scratch the surfaceof dense connectivity by constructing a graph representa-tion of the network design space . Theseworks explicitly seek a building cell with searchable wiringof building operators via Directed Acyclic Graphs (DAGs)as the design motif for CNN architectures. Various searchstrategies are implemented to achieve a good architec-ture outcome, such as differentiable-based search ,Bayesian Optimization , and local search .However, these methods employ brutal-force optimizationalgorithms such as differentiable-based search ,reinforcement-learning without any topology con-sideration when seeking the optimal wiring of building op-erators. Despite the remarkable success, existing methodsmay have the following challenges: (1) The constrained de-sign space does not support the dense connectivity of ver-satile building operators that integrate feature vectors fromall building cell levels without limitations. (2) With uncon-strained dense connectivity, exploring such design space isdifficult without topological information in the graph rep-resentation of building cells, such as isomorphism in adja-cency matrices and locality contained with similar graphs.",
  "arXiv:2404.17152v1 [cs.CV] 26 Apr 2024": "In this paper, we tackle the above challenges by propos-ing a new paradigm, CSCO (Connectivity Search ofConvolutional Operators), that enables the delicate explo-ration of the structural wiring within building cells for CNNarchitectures. CSCO establishes a hierarchical structure ofCNN architectures via a meta-graph comprising several Di-rected Acyclic Graphs (DAGs).Each DAG represents abuilding cell in each hierarchy. Within each building cell,CSCO integrates a structural design space with versatilebuilding operators (e.g., convolution, depthwise convolu-tion) with varying transformation capacities of input fea-tures, allowing dense connectivity searches. The combi-nation of dense edge connectivity and versatile heteroge-neous operators we employ can craft various design mo-tifs. For example, depthwise separable convolution ,Inverted Bottleneck , Inception-like , and Con-densenets , etc. This covers most design motifs fromhand-crafted CNN design principles and more recent block-based search spaces in NAS (e.g., ProxylessNAS , Mo-bileNetV3 etc.), providing more opportunities to ob-tain top-performing CNN architectures with minimal designspace constraints and priors.Intuited by predictor-based NAS that accu-rately models the design space via a surrogate model ofthe ground-truth performance, we follow this principle andaddress two key factors to demystify search on dense con-nectivity. First, we propose Graph Isomorphism to enricharchitecture-performance pairs during the sampling phaseof predictor-based NAS, enhancing the quality of perfor-mance prediction with improved sample efficiency.Asa result, Graph Isomorphism advances prediction reliabil-ity in dense connectivity design space with large cardinal-ity. Second, we propose Metropolis-Hastings EvolutionarySearch (MH-ES), which evades local optimal solutions dur-ing search space exploration. This allows us to approacha better region of the dense connectivity design space anddiscover better building cells for CNNs.CSCO improves both the evaluation strategy (i.e., thequality and reliability of prediction) and the search strategy(i.e., the quality of top-performing architectures discovered)in predictor-based NAS. As a result, CSCO discovers goodCNN architectures that achieve impressive empirical resultsover existing hand-crafted and NAS-crafted connectivity onImageNet. We summarize our contributions as follows: We propose a new paradigm, CSCO, to automatically ex-plore dense connectivity within building cells to fabri-cate CNN architectures. CSCO supports dense connectiv-ity search on structural wiring of versatile convolutionalbuilding operators to seek the optimal CNN architecture. We pioneer using predictor-based NAS in dense connec-tivity search and demonstrate two essential techniquesthat advance search quality and efficiency. Specifically,we propose Graph Isomorphism to improve sample ef-",
  ". Related Work": "Dense Connectivity of Convolutional Neural Networks.Existing NAS methods emphasize cell design on a graph de-sign space ; these methods are primarily topology-agnostic. For example, DARTS implies addressing the bi-level optimization problem without considering any graphinformation (e.g., graph adjacency and graph isomorphism).More recent works manage to incorporatetopological information into search space design and im-prove the flexibility of crafted CNN architectures. Yet, theystill focus on a constrained search space emphasizing ei-ther a single-cell design (i.e., normal cell and./or reductioncell) or macro-level connections between building cells withconstraints and limitations. Another line of research takesadvantage of network generators to obtain thebest cluster of CNN architectures. Yet, these methods em-phasize discovering the top-performing local regions of thesearch space and may miss the opportunity to discover anindividual architecture with a globally optimal solution.Predictor-based NAS. Neural predictors are potenttools to map candidate architectures to their performancein a search space. Predictor-based NAS has two significantphases: (1) train an accuracy predictor based on exploitablearchitecture-performance samples collected from a searchspace, and (2) utilize the accuracy predictor to probe thewhole search space and obtain the top-performing archi-tectures. Existing works enhance predictor-based NAS insample-efficiency , quality of prediction , andbetter regions of the search space . Yet, these worksfocus on a constrained block-based search space withoutconsidering topology, making them unsuitable for explor-ing cell structures for better sample efficiency.In ad-dition, existing approaches mostly employ EvolutionarySearch as the backbone methodology to obtaintop-performing architectures on neural predictors, graduallyapproaching a narrower local region of the dense connectiv-ity design space and ending up with locally optimal archi-tectures.",
  ". Overview of dense connectivity design space": "cells to build deeper architectures. Our dense connectivitydesign space delicately seeks the wiring of versatile convo-lutional operators in a building cell. demonstratesan overview of our dense connectivity design space. Weutilize existing positional settings (i.e., D, C denotes Dcopies of DAG with C base channels) and delicately seekthe edge connections that wire versatile building operatorsindependently for all building cells. We first discuss thegraph representation of CNN architectures in dense con-nectivity design space and then discuss versatile buildingoperators.",
  ". Graph Representation of CNN": "Given a CNN architecture with K stages, we specify in-dependent Directed Acyclic Graphs (DAG) for each hierar-chical level of input features to represent the dense connec-tivity of building versatile operators. A DAG is a buildingcell with multiple CNN layers and identical spatial featuresizes (i.e., image height and width). Each DAG G(k) =(V(k), E(k)) contains N vertices and [N (N 1)/2] edgeswith total capacity for dense connectivity search. Among Nvertices, vertex 0 is defined as the input vertex that receivesan output from the previous building cell, and vertex N isthe output vertex that sends an output to a succeeding build-ing cell. We define vertex 1 N 1 as intermediate ver-tices. Each intermediate vertex takes input features X froman arbitrary number of preceding vertices and produces anoutput Y via an assigned building operator op. More specif-ically, each intermediate vertex v can make a connection toany preceding vertex u0, ..., uM, concatenates all these in-puts, and use the assigned building operator op to produceoutput representations as follows:",
  "YN = Concat({Yui|dout(i) = 0}),(2)": "where dout() denotes the out degree of a vertex.Ameta-graph G=(G(1), ..., G(K))=(V, E) com-poses K DAGs to build the overall CNN architec-ture that processes input features of different hierar-chies.We construct each candidate architecture A by afunction of vertices and edge connections on the meta-graph (i.e., the union of all independent DAGs): A =farch(V(1), ..., V(K), E(1), ..., E(K); op(1), ..., op(K)).Indense connectivity design space, we assign each vertex anatomic convolutional operator from a set of versatile build-ing operators and seek the best connectivity by optimizingedge connectivity E as follows:",
  "argmaxEEP erf(farch( V(1), ..., V(K), E(1), ..., E(K); op(1), ..., op(K))),": "(3)where Perf() denotes the performance metric, andfarch transforms a meta-graph representation to a concreteCNN architecture. Given a meta-graph with K indepen-dent DAGs (e.g., K stages) and N vertices each, a denseconnectivity design space optimizes O(K N 2) dense edgeconnections to seek the optimal architecture and contains amuch richer source of architecture fabrications.",
  ". Graph Isomorphism creates extra training examples without extra cost": "convolution operator should contain precisely one convo-lution operation, followed by batch normalization andReLU activation . We collect the popular design mo-tifs from the existing literature and use the following set ofbuilding operators to craft the dense connectivity space: Convolution 11. Depthwise convolution 33, 55, or 77.Convolution 11 and depthwise convolution are hetero-geneous building operators with distinct functionality on in-put features: convolution 11 learns a transformation of lo-cal input features, and depthwise convolution learns a trans-formation of spatial input features. Next, we instantiate twodense connectivity spaces for ImageNet and CIFAR-10 asfollows: ImageNet Dense Connectivity Space. Each meta-graphcontains 4 stages which corresponds to the 4 4, 8 8,1616, 3232 down-sampling region of an input image.In each DAG, we employ one input vertex, one outputvertex, and 16 intermediate vertices assigned with one ofthe aforementioned convolutional building operators. Wefollow MobileNetV2 for the design of stem archi-tecture (i.e., first three blocks) and head architecture (i.e.,last two blocks). CIFAR-10 Dense Connectivity Space. Each meta-graphcontains 4 stages which corresponds to the 1 1, 2 2,4 4 down-sampling region of an input image. In eachDAG, we employ one input vertex, one output vertex, and16 intermediate vertices assigned with one of the afore-mentioned convolutional building operators. We followResNet to design stem architecture (i.e., first block)and employ no head architecture.Notably, we set the number of vertices to far exceed thatof versatile building operators in dense connectivity designspace to ensure scalability. The dense connectivity designspace is prohibitively large. Even with N = 8 vertices,a single DAG contains 4.5 106 architectures.Conse-",
  ". Demystifying Dense Connectivity Search": "A dense connectivity design space contains a rich source ofcandidates to ensure flexibility. However, versatile buildingoperators and the dense connectivity design space challengethe efficiency and quality of search. CSCO incorporates twokey techniques that facilitate architecture exploration in thedense connectivity design space. First, CSCO adopts GraphIsomorphism to augment architecture-performance pairs inthe dense connectivity design space to boost the accuracypredictors prediction quality without additional cost. Sec-ond, CSCO proposes a novel search strategy, Metropolis-Hastings Evolutionary Search (MH-ES), as an in-place im-provement over evolutionary search during full search spaceexploration via a trained predictor. Inspired by the defi-nition of Markov Chains, MH-ES rejects weaker sampleswith a lower probability and effectively evades local opti-mal solutions in discovery.",
  ". Graph Isomorphism as Data Augmentation": "As is discussed in .2, a DAG within a dense con-nectivity design space contains more building operatorsthan the number of vertices.This provides a chance tofind isomorphic structures in the dense connectivity designspace and further exploit these architectures to enhance theperformance of predictor-based NAS. We formally definethe isomorphism of meta-graphs as follows:",
  ". Accuracy surface of a performance predictor with/without Graph Isomorphism": "in the dense connectivity search space. This is because iso-morphic meta-graphs have an identical set of building oper-ators and identical dense connectivity of these building op-erators, see . As a result, isomorphic meta-graphsrepresent the same neural architecture, leading to the samelevel of performance during evaluation.Thus, we propose Graph Isomorphism to augment thearchitecture samples. Graph Isomorphism conducts a validvertex permutation to one of the DAGs within each sam-pled meta-graph to construct a new isomorphic meta-graphand incorporate it as a new architecture sample with no ex-tra search cost. These isomorphic samples can augment thearchitecture-performance pairs to brew a more accurate per-formance predictor without additional search costs.Prediction Surface. We visualize the prediction surface ofperformance predictors with/without Graph Isomorphism in. Here, a higher z-axis value denotes better pre-dictive performance on the target dataset. Notably, GraphIsomorphism not only enhances the prediction quality of aperformance predictor but also provides a smooth perfor-mance surface that eases the following search process in anample, dense connectivity design space.",
  ". Metropolis-Hastings Evolutionary Search": "Evolutionary Search is a popular method that efficiently ex-plores the best architecture in predictor-based NAS. Yet,these methods may not efficiently explore our dense con-nectivity design space, thus suffering from the sub-optimalquality of discovered CNN architectures. We follow thesame intuition of evolutionary search and first define themutation space as follows: Re-sample a random edge connection for one DAG. Randomly add an edge connection for one DAG. Randomly remove an edge connection for one DAG.Given an intermediate meta-graph with N vertices andK stages, the mutation space covers up to O(N 2K) pos-sible candidate architectures, thus being prohibitively largefor existing evolutionary search algorithms to explore fully. For example, (1) tremendous samples are needed to coverthe good regions of the dense connectivity space and obtainthe best child architecture, and (2) the complexity of predic-tion surface in dense connectivity design space may lead tothe discovery of locally optimal solutions. This is becauseevolutionary search judiciously accepts the strongest childarchitectures during the evolutionary process and, thus, ob-tains locally optimal solutions with high concentration on aspecific region of the dense connectivity design space.We are inspired by Markov Chain Monte Carlo(MCMC) optimization, especially Metropolis-Hastings Al-gorithm , extensively addressing such issues by adopt-ing an acceptance-rejection mechanism. Such mechanismmaintains a current best solution and admits weaker solu-tions with an acceptance-rejection probability AC, definedas follows:",
  "AC = min(1, exp ((score score)/T)),(4)": "Where score denotes the score (e.g., predicted performanceof architectures) for a weaker solution, score denotes thescore of the current best solution, and T denotes the tem-perature. The acceptance-rejection probability is propor-tional to the gap between the weaker solution and the cur-rent best solution. As a result, the optimization process maynot greedily stick to the current best solution and thus havebetter potential to avoid locally optimal solutions.Thus, we propose Metropolis-Hastings EvolutionarySearch (MH-ES) as an alternative to existing evolutionarysearch algorithms on dense connectivity design space. MH-ES allows the discovery of better architectures within thelarge dense connectivity design space. MH-ES maintainsthe best parent architecture, which is initialized among P0randomly sampled candidate architectures in the initial pop-ulation. Then, child architectures are obtained by randomlymutating one of the stages (i.e., DAGs) in the parent archi-tecture (i.e., meta-graph). Each evolution round selects thebest child architecture in the current population as a weaker",
  ". The search progress and predicted accuracy of discovered architectures via MH-ES compared to ES and RS baselines": "solution. The aforementioned MH acceptance-rejection ra-tio AC is applied to update the current best architecture.The proposed MH-ES generalizes to local search whenT 0 and evolutionary search when T . MH-ES alsoadopts a cosine simulated annealing of the temperatureto eliminate locally optimal solutions at early evolutionaryrounds.Optimization Curve of MH-ES. We compare the op-timization curve of MH-ES versus Evolutionary Search(ES) and Local Search (LS) to demonstrate its ef-fectiveness and efficiency. depicts the optimiza-tion curve of accuracy on top-performing CNN architec-tures for both CIFAR-10 and ImageNet. On a small-scaleCIFAR-10 dense connectivity design space, MH-ES per-forms on-par as local search yet significantly outperformsevolutionary search by 0.01. This is greatly attributed tothe capability of MH-ES to evade locally optimal solutionsduring architecture exploration over the dense connectivitydesign space. On large-scale ImageNet dense connectiv-ity design space, MH-ES significantly edges other searchalgorithms, highlighting its efficiency and effectiveness inexploring dense connectivity design space.",
  ". CSCO Setup": "We first elaborate on the search settings on CSCO, includingsearch space configuration and MH-ES guided by a trainedneural predictor. Then, we discuss the evaluation settings ofCSCO over a dense connectivity design space.Search Space Settings. We set a search budget of 4 GPUdays and employ a fixed assignment of the building oper-ators in all DAGs of the meta-graph. This enhances the reproducibility of our methods. We employ a large-scaledense connectivity design space with N = 18 vertices,where vertex 1, 5, 9, 13 are assigned with convolution 11,vertices 2, 6, 10, 14 are assigned with depthwise convolu-tion 33, vertex 3, 7, 11, 15 are assigned with depthwiseconvolution 55, and vertex 4, 8, 12, 16 are assigned withdepthwise convolution 77. We define vertex 0/17 as theinput/output vertex. Predictor Training and MH-ES. We train the aboveMLP performance predictor on the sampled architecture-performance pairs for 300 epochs with batch size 128, ini-tial learning rate 0.1, and an L2 weight decay of 1e-4 forImageNet. During MH-ES, we employ an initial populationof 4096 to ensure the discovery of a good parent architec-ture. We proceed with 10K rounds of MCMC optimizationwith 96 child architectures sampled and evaluated in eachround. We set the sensitivity parameter to 0.001 for the bestsolution in the dense connectivity design space. Evaluation Settings. The outcome of CSCO leads to a poolof candidate CNN architectures for both CIFAR-10 and Im-ageNet, respectively. We simply evaluate the top-5 mod-els on CIFAR-10/ImageNet proxy dataset for 20/10 epochsand scale the best model to 600M Multiply-Accumulates(MACs) mobile computation budget.",
  ". CIFAR-10 Experiments": "We first evaluate each component of the CSCO paradigmand then proceed to evaluate the top-performing architec-ture discovered on CIFAR-10.Evaluating Best CIFAR-10 Model. In CIFAR-10, we fol-low DARTS-series architectures ) and stack 6 build-ing cells in each stage to construct the final CNN architec-ture. To match the number of parameters reported in theDARTS-series paper, we apply a width multiplier to scaleup the candidate networks to ensure a fair comparison withexisting state-of-the-art.We follow the DARTS protocol to train the best network.Specifically, we train the best network discovered by CSCOon 50K CIFAR-10 training data from scratch for 600 epochswith batch size 96. We employ an initial learning rate of0.025 with a cosine learning rate schedule . Follow-ing DARTS series works, we employ Dropout , Drop-Path and Cutout with a L-2 weight-decay of 3e-4 tocombat overfitting.The key results of CSCO on the CIFAR-10 dataset aresummarized in . CSCO outperforms SMBO-basedPNAS and EA-based AmoebaNet by up to 0.42%. Com-pared with DARTS and GDAS, CSCO achieves up to 0.2%better accuracy within a reasonable 4 GPU day search cost.",
  ". ImageNet Classification": "We evaluate the best architectures crafted by CSCO on Ima-geNet by training them from scratch using the same trainingprotocol as previous works. We compare the accuracy ver-sus various metrics such as Multiply-Accumulates (MACs)with hand-crafted and NAS-crafted models. We train thebest-discovered model on 1.28M ImageNet-1K trainingdata from scratch for 450 epochs with batch size 768. We",
  "employ an initial learning rate of 0.6 with cosine learningrate schedule .Following DARTS series works, weemploy Inception pre-processing , Dropout , DropPath , an L2 weight-decay of 1e-5": "demonstrates the critical results of CSCO onthe ImageNet-1K validation set within the mobile com-putation regime (i.e., 600M MACs).CSCO outper-forms Condensenet by 3% higher accuracy, demon-strating its superiority over prior art with dense connec-tivity of building operators in CNN architectures. CSCOoutperforms SOTA hand-crafted models MobileNetV2 1.4by 2.0% higher top-1 accuracy with similar MACs, wherethe latter one is crafted via manual architecture engineer-ing.Compared to existing NAS works that emphasizedense connectivity , CSCO achieves 0.6% accuracy gain under the mobile computation regimewith comparable parameter consumption. Despite havinga sizeable dense connectivity design space, CSCO main-tains a reasonable search cost of 8 GPU days thanks toGraph Isomorphism, which boosts sample efficiency. Fi-nally, CSCO also achieves competitive performance com-pared to existing block-based NAS methods under awell-designed MobileNetV2-like search space, demonstrat-ing the potential of optimizing dense connectivity to seekhigh-performing CNN architectures in the future.",
  ". Ablation Studies": "Due to the time-consuming process of complete architec-ture evaluation from scratch, we adopt a simple trainingprotocol to evaluate the top models discovered by differentGraph Isomorphism and search strategies. demon-strates the detailed evaluation result, including the accuracyof best-performing architectures and the statistics on top-5models to reflect the stability of the proposed method. Here,we compute all accuracies using top models selected bythe trained predictor. Notably, each model contains only 3building cells in each stage, and the building operator adopts16, 32, and 64 filters, respectively, within each stage. Wecan see that the combination of MH-ES and Graph Isomor-phism yields up to a 0.3% accuracy gain on the mean ac-curacy of top-performing models while achieving the besttop-performing architecture among other baseline methods.",
  ". Ranking with Graph Isomorphism": "We analyze the performance predictor trained with/withoutGraph Isomorphism. Before Graph Isomorphism, we sam-ple 800 samples from the dense connectivity design spacefor both CIFAR-10 and ImageNet benchmark and aug-ment 10 12 more samples via Graph Isomorphismwithout extra computation cost. We split all architecture-performance pairs into 85% training pairs and 15% testingpairs. We utilize a Multi-Layer Perceptron (MLP) perfor-mance predictor to map architectures (i.e., the union of ad-jacency matrices in each DAG within a meta-graph) to theirpredicted performance (i.e., evaluated accuracy). We deli-cately train the performance predictor on the training splitwith/without Graph Isomorphism.Prediction Quality. We measure the prediction quality ofneural predictors on the testing architecture-performancepairs via two famous ranking metrics: Pearsons andKendalls in . Here, all ranking coefficients arecomputed on the testing pairs, which are not utilized to trainthe performance predictor.Using Graph Isomorphism, the prediction ranking qual-ity (i.e., Kendalls ) significantly increases from 0.215 to0.904 on ImageNet and from 0.428 to 0.874 on CIFAR-10. In addition, the ranking evaluation reveals that moreDAGs in the meta-graph lead to poorer sample efficiencyin predictor-based NAS and thus lead to a more challeng-",
  ". Conclusion": "We propose CSCO, a novel paradigm that allows flexibleexploration of the dense connectivity of building operatorsand innovates building cells in CNN architectures. CSCOaims to seek the optimal building cells of CNN architec-tures represented by Directed Acyclic Graphs (DAGs), con-taining rich sources of dense connectivity of versatile build-ing operators to cover CNN architecture designs flexibly.CSCO crafts a dense connectivity space to fabricate thebuilding cells of the CNN architectures and further lever-ages a performance predictor to obtain the best dense con-nectivity.To enhance the reliability and quality of pre-diction, we propose Graph Isomorphism as data augmen-tation to boost sample efficiency and Metropolis-HastingsEvolutionary Search (MH-ES) to efficiently explore denseconnectivity space and evade locally optimal solutions inCSCO. Experimental on ImageNet demonstrates 0.6%accuracy gain over other NAS-crafted dense connectivitydesigns under mobile computation regime.Acknowledgement.This project is partly supportedby NSF 2112562, ARO W911NF-23-2-0224 and NSFCAREER-2305491.",
  "Alain Chedotal and Linda J Richards. Wiring the brain: thebiology of neuronal guidance. Cold Spring Harbor perspec-tives in biology, 2(6):a001917, 2010. 1": "Hsin-Pai Cheng, Tunhou Zhang, Yixing Zhang, Shiyu Li,Feng Liang, Feng Yan, Meng Li, Vikas Chandra, Hai Li,and Yiran Chen.Nasgem: Neural architecture search viagraph embedding method. In Proceedings of the Thirty-FifthConference on Association for the Advancement of ArtificialIntelligence (AAAI), 2021. 2 Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zi-jian He, Zhen Wei, Kan Chen, Yuandong Tian, Matthew Yu,Peter Vajda, et al. Fbnetv3: Joint architecture-recipe searchusing predictor pretraining. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1627616285, 2021. 2",
  "Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMTMeyarivan. A fast and elitist multiobjective genetic algo-rithm: Nsga-ii. IEEE transactions on evolutionary computa-tion, 6(2):182197, 2002. 2": "Tom Den Ottelander, Arkadiy Dushatskiy, Marco Virgolin,and Peter AN Bosman. Local search is a remarkably strongbaseline for neural architecture search.In EvolutionaryMulti-Criterion Optimization: 11th International Confer-ence, EMO 2021, Shenzhen, China, March 2831, 2021,Proceedings 11, pages 465479. Springer, 2021. 1 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 6",
  "Xuanyi Dong and Yi Yang.Searching for a robust neu-ral architecture in four gpu hours.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 17611770, 2019. 6": "Lukasz Dudziak, Thomas Chau, Mohamed Abdelfattah,Royson Lee, Hyeji Kim, and Nicholas Lane.Brp-nas:Prediction-based nas using gcns. Advances in Neural Infor-mation Processing Systems, 33:1048010490, 2020. 2 Jiemin Fang, Yuzhu Sun, Qian Zhang, Yuan Li, Wenyu Liu,and Xinggang Wang. Densely connected search space formore flexible neural architecture search. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1062810637, 2020. 2, 7",
  "Foundations of genetic algorithms, pages 6993. Elsevier,1991. 6": "Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang. Mile-nas: Efficient neural architecture search via mixed-level re-formulation. Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, 2020. 7 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 770778, 2016. 1, 4 Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, VincentVanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deepneural networks for acoustic modeling in speech recognition.In IEEE Signal processing magazine, 2012. 7 Andrew Howard, Mark Sandler, Grace Chu, Liang-ChiehChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3. In Proceedings of the IEEE International Confer-ence on Computer Vision, pages 13141324, 2019. 1, 2 Andrew G Howard, Menglong Zhu, Bo Chen, DmitryKalenichenko, Weijun Wang, Tobias Weyand, Marco An-dreetto, and Hartwig Adam. Mobilenets: Efficient convo-lutional neural networks for mobile vision applications. InarXiv preprint arXiv:1704.04861, 2017. 1, 2 Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger.Densely connected convolutional net-works. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 47004708, 2017. 1,6 Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kil-ian Q Weinberger. Condensenet: An efficient densenet usinglearned group convolutions. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages27522761, 2018. 1, 2, 7 Tao Huang, Shan You, Yibo Yang, Zhuozhuo Tu, Fei Wang,Chen Qian, and Changshui Zhang. Explicitly learning topol-ogy for differentiable neural architecture search.arXivpreprint arXiv:2011.09300, 2020. 1, 7",
  "Sergey Ioffe and Christian Szegedy. Batch normalization:Accelerating deep network training by reducing internal co-variate shift. arXiv preprint arXiv:1502.03167, 2015. 4": "Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider,Barnabas Poczos, and Eric P Xing.Neural architecturesearch with bayesian optimisation and optimal transport. Ad-vances in neural information processing systems, 31, 2018.1 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet classification with deep convolutional neural net-works. In Advances in Neural Information Processing Sys-tems. Curran Associates, Inc., 2012. 1",
  "Mingxing Tan and Quoc Le. Efficientnet: Rethinking modelscaling for convolutional neural networks. In InternationalConference on Machine Learning, pages 61056114, 2019.1": "Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,Mark Sandler, Andrew Howard, and Quoc V Le.Mnas-net: Platform-aware neural architecture search for mobile.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 28202828, 2019. 1, 7 Wei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Ben-der, and Pieter-Jan Kindermans. Neural predictor for neuralarchitecture search. In European Conference on ComputerVision, pages 660676. Springer, 2020. 2",
  "Mitchell Wortsman, Ali Farhadi, and Mohammad Raste-gari.Discovering neural wirings.arXiv preprintarXiv:1906.00586, 2019. 1, 2": "Junru Wu, Xiyang Dai, Dongdong Chen, Yinpeng Chen,Mengchen Liu, Ye Yu, Zhangyang Wang, Zicheng Liu, MeiChen, and Lu Yuan. Stronger nas with weaker predictors.Advances in Neural Information Processing Systems, 34,2021. 2 Saining Xie, Alexander Kirillov, Ross Girshick, and Kaim-ing He. Exploring randomly wired neural networks for im-age recognition. In The IEEE International Conference onComputer Vision (ICCV), 2019. 2, 7 Sirui Xie, Shoukang Hu, Xinjiang Wang, Chunxiao Liu,Jianping Shi, Xunying Liu, and Dahua Lin.Understand-ing the wiring evolution in differentiable neural architecturesearch. In International Conference on Artificial Intelligenceand Statistics, pages 874882. PMLR, 2021. 2 Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-JunQi, Qi Tian, and Hongkai Xiong. Pc-darts: Partial channelconnections for memory-efficient architecture search. In In-ternational Conference on Learning Representations (ICLR),2020. 7",
  "Sergey Zagoruyko and Nikos Komodakis. Wide residual net-works. arXiv preprint arXiv:1605.07146, 2016. 6": "Tunhou Zhang, Hsin-Pai Cheng, Zhenwen Li, Feng Yan,Chengyu Huang, Hai Li, and Yiran Chen. Autoshrink: Atopology-aware nas for discovering efficient neural architec-ture. In Proceedings of the AAAI Conference on ArtificialIntelligence (AAAI 2020), 2019. 2 Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc VLe. Learning transferable architectures for scalable imagerecognition.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 86978710,2018. 1, 7"
}