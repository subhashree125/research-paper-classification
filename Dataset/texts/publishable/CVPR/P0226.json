{
  "Abstract": "Open-world semi-supervised learning (OWSSL) extendsconventional semi-supervised learning to open-world sce-narios by taking account of novel categories in unlabeleddatasets. Despite the recent advancements in OWSSL, thesuccess often relies on the assumptions that 1) labeled andunlabeled datasets share the same balanced class prior dis-tribution, which does not generally hold in real-world ap-plications, and 2) unlabeled training datasets are utilizedfor evaluation, where such transductive inference might notadequately address challenges in the wild. In this paper, weaim to generalize OWSSL by addressing them. Our worksuggests that practical OWSSL may require different train-ing settings, evaluation methods, and learning strategiescompared to those prevalent in the existing literature.",
  ". Introduction": "OWSSL has been introduced to discover novel classeswithin an unlabeled dataset while accurately classifyingknown classes. However, we argue that OWSSL may notreflect real-world scenarios for the following reasons: 1) re-cent works on OWSSL assume balanced and identical classprior distribution between labeled and unlabeled datasetsduring the learning process, and 2) they only consider atransductive learning setting, which focuses on categorizinginstances from the unlabeled training datasets.Indeed, in-the-wild data naturally follow a long-taileddistribution and are exposed to label distribution shifts , i.e., labels are missing not at random (MNAR; aright) rather than missing completely at random (MCAR;a left). Class prior distribution mismatch between la-beled and unlabeled datasets happens for multiple reasons,e.g., the data distribution itself could change over time, orannotators might prefer to annotate relatively easy classesor they could miss difficult classes. However, most OWSSLmethods assume a balanced class prior for training, whichoften hampers performance when the assumption does nothold. Also, most OWSSL methods assume a transductive",
  ". Examples of scenarios considered in ROWSSL": "learning setting, which is specialization on given unlabeledtraining data rather than generalization on unseen test dataas illustrated in b. While transductive learning is use-ful for category discovery, it does not guarantee reliable per-formance when classifying discovered categories from un-seen test data. Instead, inductive learning is important insafety-critical applications such as medical diagnosis, e.g.,a model that can discover novel diseases in a specific patientcohort might still misclassify diseases in unseen patients. To this end, we extend OWSSL by addressing suchpractical training and evaluation settings, coined RealisticOpen-World Semi-Supervised Learning (ROWSSL). Inthis task, we consider long-tailed distribution with classprior distribution mismatch between labeled and unlabeleddatasets for training, and inductive and transductive infer-ences for evaluation. To address the aforementioned chal-lenges, we introduce Density-based Temperature scalingand Soft pseudo-labeling (DTS) to learn class-balanced rep-resentations taking account of local densities and reduce",
  "arXiv:2405.20829v1 [cs.CV] 31 May 2024": "classifier bias toward the head and known classes simultane-ously. To achieve this, we propose to measure the tailednessas a proxy for the unknown class prior via density estima-tion on the representation space. With these proxies, weintroduce a dynamic temperature scaling approach for bal-anced contrastive learning, which dynamically adjusts thetemperature parameter for each anchor to shape a represen-tation space to have better linear separability between headand tail classes. Also, we address the classifier bias via classuncertainty-aware soft pseudo-labeling, considering a den-sity variance as an uncertainty measure.",
  ". Training setting": "Suppose we have a partially labeled dataset D = Dl Du,where Dl = {xi, yi}Nli=1 X Yl is the labeled datasetwith Nl samples which belongs to one of the Cold knownclasses, and Du = {xi}Nui=1 X is the unlabeled datasetwith Nu samples, with its underlying label space Yu con-taining both of Cold known classes and Cnew novel classes.In GCDW, the labeled dataset Dl has a long-tailed distribu-tion with an imbalance ratio l > 1, while the unlabeleddataset can have an arbitrary class prior, including MCARand MNAR scenarios as depicted in a.Following, the total number of classes C = Cold + Cnew iseither assumed to be known a priori or estimated throughoff-the-shelf methods . Our objective is to train a para-metric classifier f : Rd C to correctly assign classlabels to both known and novel classes.",
  ". Evaluation setting and metrics": "In , we compare the balanced overall accuracy ofprevious methods with different evaluation strategies onCIFAR-100-LT, where indicates the result aligned with, with a maximum discrepancy of 1.3%. The ratio of thenumber of known and novel classes is 80:20 for Split 1 and50:50 for Split 2. Transductive inference.Prior works have per-formed transductive inference for their methods on the un-labeled training dataset (Train evaluation set in ).Following them, we measure the clustering accuracy be-tween the ground truth labels yi and the models predictionsyi through the Hungarian algorithm :",
  "Ours54.161.755.852.153.751.953.748.1": "Transductive inference on the test set.BaCon uti-lizes a balanced test dataset following the common practicein long-tailed recognition. However, they perform k-meansclustering on the entire test dataset for evaluation, i.e., theclassification result depends on other test data, which cor-responds to transductive inference (Recluster and Re-match in ). Also, they ignore the clusters foundduring training and match the clusters of the test set withthe classes, resulting in unintentional concept shifts, e.g.,the cat class during training might be matched with the lionclass at test time. Hence, their evaluation results do notproperly reflect the generalizability of the models to onlineinference, which is often required in real-world scenarios,and they cannot identify the semantics of classes. Further-more, k-means assumes the presence of the uniform clusterprior, which leads to biased results in relation to the bal-anced test set statistics . We found that the high per-formance of BaCon might be due to the uniform prior as-sumption of k-means and concept shifts by rematching forthe best performance, e.g., when evaluated on the imbal-anced training dataset, BaCon is on par with other methodsin Split 1, and outperformed by other methods in Split 2. Inductive inference.To evaluate the generalizability ofmodels, we consider inductive inference. Specifically, weevaluate the models on the disjoint test dataset by nearestcentroid classification, where the center of clusters foundby optimal matching p from (1) on the training set are uti-lized as parametric class centers (Recluster and Rematch in ). To confirm that concept shifts are benefi-cial to maximize the performance, we also apply Hungarianmatching between the parametric clustering results with theclasses (Recluster and Rematch in ). Whilerematching results in better performance, this ignores thesemantics of categories discovered during training. In fact,rematching corresponds to transductive inference, as it re-quires gathering the parametric clustering results. Through-out experiments, we focus on evaluation without recluster-ing and rematching for inductive inference.",
  "We propose an end-to-end approach that jointly learns therepresentation and parametric classifier, similar to Wen etal. . The network architecture is composed of an en-": "coder E followed by two heads f and g. The encoder E canbe a pre-trained model, e.g., a ViT pre-trained with DINO, z = E(x) Rd is a feature vector representing theinput image x, f is an 2-normalized linear classifier, andg is a multi-layer perceptron (MLP) projecting z to a lowerdimensional vector h for representation learning.",
  ". Training objectives": "Representation learning.We adopt contrastive learning(CL) loss for representation learning. From a mini-batch B,two views of an image are obtained through random aug-mentation, represented as x, and x. These images are thenfed into the query and key networks E g and E g, yield-ing a pair of 2-normalized embeddings h = (Eg)(x) andb = (E g)(x), respectively, where the key network isupdated by exponential moving average (EMA), followingMoCo . Self-supervised learning loss is defined as:",
  "bQ exp (hi b/).(2)": "Here, b+ is a positive key, and the queue Q = {bj}Qj=1 isupdated sequentially with key embeddings b following thefirst-in-first-out (FIFO) scheme, where Q is the predefinedqueue size. Q = {bj}Qj=1 is a queue that contains the keyembeddings b of a predefined size Q. For effective utiliza-tion of label information, we adopt the variation of the su-pervised contrastive loss lsup(xi, yi) which maintainsmultiple positive pairs on-the-fly by comparing the querylabel to a label queue . Overall representation learningloss is defined as:",
  "(3)where Bl corresponds to the labeled subset of B and rep isa balancing factor": "Classifier learning.Our parametric classification frame-work follows the self-distillation methods . We employa prototypical classifier where the weight parameters of lin-ear classifier f are regarded as cluster centroids. To dis-cover novel classes and allocate each sample to the optimalcluster, we condition the cluster centroids to contain classinformation through multi-tasking self-supervised and su-pervised objectives . Classification loss is defined as:",
  ". Constructing tailedness prototypes": "Tailedness estimation.Different from prior OWSSL set-tings, the true class prior is unknown in ROWSSL, asMNAR is considered.To learn a model without know-ing the true class prior, we define tailedness as a surro-gate for the class prior based on density estimation withinthe representation space. Since tail classes often exhibitlower intra-class consistency than head classes, samples oftail classes tend to sparsely distribute on the representationspace . Building on this, we learn tailedness proto-types, aiming to explore stable and efficient proxies to dis-cover tail class samples. To begin with, we utilize the queueQ = {bi}Qi=1 of the CL branch in Sec. 3.1, as the neigh-bors in the entire dataset cannot be captured by looking atonly a mini-batch. We initialize 2-normalized tailednessprototypes M = {mi}Mi=1 by k-means on the features ofthe queue, and estimate density di of a prototype mi basedon the weighted average of the cosine similarity of its K-nearest neighbors:",
  ". Density-based learning strategy": "Dynamic temperature scaling.We aim to handle long-tailed data through self-supervised representation learningby controlling temperature parameter , which has beenshown to play a significant role in learning good rep-resentations .Specifically, we view the contrastiveloss through the average distance maximization perspec-tive .From this view, a large allows the modelto maximize the average distance across a wide range ofneighbors, which is advantageous for preserving local se-mantic structures. On the other hand, a small helps tolearn instance-specific features by encouraging a uniformdistribution of embeddings across the representation space.Based on this perspective, we present a novel representationlearning method, the dynamic temperature scaling for CL.Specifically, we adjust the temperature parameter in (2) asa function of the anchors tailedness score si:",
  "maxt(dt) mint(dt)(max min),(9)": "where min and max are hyperparameters, denoting the min-imum and maximum values of temperature, respectively.As tail classes benefit from learning instance-specific fea-tures while head classes are required to preserve their localsemantic structure , our approach dynamically assignssmaller to tail classes and larger values to head classes.This allows the model to learn class-balanced representa-tions, achieving better linear separability between the long-tailed classes without knowing the true class prior. Classuncertainty-awaresoftpseudo-labeling.Forpseudo-labeling in long-tailed recognition, the distributionof pseudo-labels on unlabeled data tends to be biased to-ward head classes . For conventional long-tailed recog-nition, the effect of bias can be mitigated by giving moreweight to tail classes inversely proportional to their classprior . However, this approach might not work well inROWSSL, as pseudo-labels tend to be biased toward knownclasses, such that they are often more biased toward known-tail classes than novel-head classes . To this end, wepropose to adjust the soft pseudo-label qi in (4) with re-gard to the class uncertainty. Intuitively, for classes that areeasy to learn, their samples will consistently be assigned toa specific tailedness prototype. Conversely, samples frommore difficult and uncertain classes will be arbitrarily dis-tributed across various prototypes. Based on this idea, wepropose to use the standard deviation of tailedness scoresamong samples within each class as a measure of the rela-tive learning uncertainty of each class as the additive classuncertainty. At each training iteration, we gather the tailed-ness scores in the dataset with respect to each samples . Results on CIFAR-100-LT. Tr: transductive, In: induc-tive, ACC: average accuracy in Eq. (1), bACC: average of per-class accuracy. The best and second-best results are highlighted inbold and underlined, respectively.",
  ". Experimental Results": "We compare our method with the state-of-the-art OWSSLmethods . We report the results onCIFAR-100-LT with an imbalance ratio = 100 in .We explore two scenarios with different class priors of theunlabeled dataset Du: 1) the class prior of Du is consistentwith Dl, i.e., MCAR (l = u; a left), and 2) theclass prior of Du is reversed from Dl, leading to a discrep-ancy in class prior distribution between them, i.e., MNAR(l = u; a right). In most cases, our method out-performs others in terms of overall accuracy for both trans-ductive and inductive inferences. Specifically, our methodshows superior novel class accuracy, demonstrating that thedensity-based approach is effective in compensating for thedifficulty of learning novel classes.",
  "ROWSSLClassifyDiscover & Classify **ImbalancedTransductive & Inductive": "* Evaluated on the disjoint test dataset, but it requires to see the entire test dataset for inference, i.e., transductive inference.** Discover novel classes on the unlabeled training dataset and classify them on the disjoint test dataset. Open-world semi-supervised learning (OWSSL)or generalized category discovery (GCD) is a transductive learningsetting which extends semi-supervised learning (SSL) and novel category discovery (NCD) by classifying known classesas well as discovering novel classes in the unlabeled training dataset. Vaze et al. addresses this task via contrastivelearning (CL) on a pre-trained vision transformer (ViT) followed by constrained k-means clustering . Sincethen, a plethora of works have explored CL to achieve robust representations in OWSSL. XCon learns fine-graineddiscriminative features by dataset partitioning. PromptCAL , DCCL , OpenNCD , and CiPR construct anaffinity graph, and OpenCon utilizes a prototype-based novelty detection strategy to mine reliable positive pairs forthe contrastive loss. GPC introduces a novel representation learning strategy based on a semi-supervised variant ofthe Gaussian mixture model. SPTNet proposes an iterative optimization method which optimizes both model and dataparameters. In parallel with them, ORCA , NACH , and OpenLDN utilize pairwise learning, generating pseudo-labels for unlabeled data by ranking distances in the feature space. ORCA and NACH also propose uncertainty-based loss toalleviate known class bias caused by different learning speeds between known and novel classes.However, these advances are mostly based on the assumption that the class prior of the training dataset is balanced;indeed, data imbalance poses further challenges in OWSSL. For example, while a majority of methods proposed for OWSSLemployed CL, it has been known that CL is not immune to data imbalance, such that representations learned on long-taileddistribution might be biased toward head classes . Also, they mostly rely on k-means clustering, which assumes thepresence of isotropic data clusters , such that the uniform cluster prior assumption often hampers representationlearning . In the case of pairwise learning-based methods, the classifier is learned to be biased toward head classes due tothe lack of positive pairs in tail classes . Learning pace-based methods only take account of the uncertainty of known andnovel classes, such that it might be difficult to distinguish between known-tail classes and novel-head classes . Lastly,non-parametric methods apply k-means clustering at inference time, which requires access to the entire test datasetfor inference, often unattainable in real-world scenarios and hinders online inference, i.e., inductive learning.In this paper, we advance OWSSL to a more practical setting, considering long-tailed distribution with class prior distri-bution mismatch, and inductive inference. Also, we address the aforementioned problems by density estimation on the latentfeature space to achieve balanced CL and reduce the classifier bias toward the head and known classes. Long-tailed recognitionconsiders imbalanced class prior, which is natural in real-world scenarios. Early approaches tocombat the imbalance include data re-sampling , re-weighting , and margin-based approach with respect to givenclass-wise sample sizes. Based on this, DARP and CReST introduce long-tailed semi-supervised learning methodsutilizing distribution alignment. Recently, ACR and PRG suggest realistic long-tailed semi-supervised learningsetting considering class prior distribution mismatch between labeled and unlabeled datasets, i.e., MNAR. However, theirclosed-world assumption hinders direct application to ROWSSL. On the other hand, self-supervised learning under long-tailed distribution has also been investigated . As the temperature parameter plays a significant role in shaping the learning dynamics of CL , Kukleva et al. adjusts the temperature parameter with cosine scheduling to improvelinear separability between head and tail classes. Different from prior works, our DTS dynamically adjusts the temperatureof the contrastive loss based on the estimated density rather than predefined cosine scheduling.BaCon proposed distribution-agnostic GCD, but their setting is different from ours in that 1) it performs transductiveinference on the test set by k-means clustering on the entire test dataset for evaluation, and 2) it does not assume the potentialclass prior distribution mismatch. Our density-based approach effectively addresses ROWSSL, outperforming BaCon in bothinductive and transductive learning settings.",
  "B. Implementation Details": "Our algorithm is developed using PyTorch and we conduct all the experiments with a single NVIDIA RTX A5000 GPU.Following , all methods are trained with a ViT-B/16 backbone with DINO pre-trained weights, and use theoutput [CLS] token with a dimension of 768 as the feature representation, and the MLP g projects the feature representationto a 256-dimensional vector. All methods were trained for 200 epochs with a batch size of 128, and we fine-tuned the finaltransformer block using standard SGD with momentum 0.9, and an initial learning rate of 0.1 which we decay with a cosineannealed schedule. The balancing factor rep for the contrastive loss is set to 0.35. For the classification objective, we sets to 0.1, and t is initialized to 0.07, then warmed up to 0.04 with a cosine schedule in the starting 30 epochs. The weightof the mean-max regularization is set to 4. We set the number of tailedness prototype M to be the same as the total classnumber C, with the moving average factor tail to 0.9. The queue size Q is set to 4096 and the K-nearest neighbor distanceis computed on K = 15. The default hyperparameters min and max for dynamic temperature scaling are set to 0.05 and 1,and we set var to 1.",
  "D. More Experiments and Discussions": "We present ablation studies to evaluate and understand the contribution of each component of our method. To examine howthe performance is influenced by the long-tail distribution, we categorize known and novel classes into three separate groups:{Many, Median, Few}, based on the number of data per class. All experiments are conducted in MCAR or the distributionmatched setting.",
  "Figure D.2. Comparison between known and novel classes of ourmethod": "serves as an indicator of the ability to identify tail samples: when the target group is head (tail), lower (higher) indicatesthat the method effectively localizes tail samples. In Fig. D.1, we compare our prototype-based estimation (Prototype) withthe instance-wise estimation baseline (Instance) and the loss-based estimation (Loss) . As shown in Fig. D.1, ourmethod discovers tail samples more effectively than others. To observe the effect of our method on discriminating knownand novel classes, we further divided head and tail groups into known and novel classes. In Fig. D.2, we observe that thedifference between known and novel classes is not captured well at the beginning of training, but samples from novel classesbegin to exhibit larger tailedness in the representation space than those from known classes through training. This impliesthat the learning difficulty could be captured by our density-based approach, which is further discussed in the following.",
  "D.2. Design choices for the temperature in CL": "To validate the effectiveness of the dynamic temperature for the contrastive loss, we experiment with different choices of thetemperature. In addition to the constant temperature ( = 0.07), we compare our density-based approach with the estimatedclass prior by hard pseudo-labels for the classifier following as baselines, and the true class prior as an oracle, wherewe apply the same min-max normalization as our method. We observe that our proposed method achieves better overallaccuracy compared to baselines. Interestingly, our method even outperforms the oracle with the true class prior, implyingthat the learning difficulty of classes is not strictly proportional to the class prior, and our density-based approach can be moreeffective in addressing it.",
  "D.3. Design choices for the class uncertainty": "In this experiment, we validate the effectiveness of the choice of u, which is the standard deviation of class-wise tailednessscores. We compare the variance of the maximum softmax probability as confidence for each class and the estimated distri-bution as baselines and the ground-truth class prior as an oracle. For both estimated and ground-truth class prior, weconvert the class frequency into a normalized probability distribution. As shown in Table D.2, our method achieves compa-rable performance to the oracle performance. Notably, our method boosts performance by 6.2% in novel classes and 3.8% intail classes. This result confirms that focusing on class uncertainty is more effective than using class prior for mitigating thebias of the classifier in the ROWSSL setting.",
  "D.4. Contribution of each component": "We examine the impact of each component in Table D.3. Specifically, starting from the baseline , we ablate the momen-tum encoder and dynamic temperature scaling and class uncertainty-aware pseudo-labeling.Comparing experiments (b)and (c), the proposed dynamic temperature scaling improves performance by 2.7% and 1.4% for head classes, alongside 6.1%and 14% for tail classes on the CIFAR-100-LT and CUB-200-LT datasets, respectively. This indicates that our method learnsdiscriminative semantic structures for both head and tail classes. From (b) and (d), the proposed class uncertainty-awarepseudo-labeling yields a notable improvement in all metrics. Specifically, introducing u enhances performance by 7.2% and8.4% in novel classes, with 5.6% and 11.0% in tail classes for each dataset, effectively mitigating classification bias towardsknown and head classes. The full version of our method (e) shows superior performance on all evaluation metrics, whichexperimentally demonstrates that our approach plays a crucial role in addressing ROWSSL.",
  "D.5. Unknown class numbers": "In real-world applications, we often do not have prior knowledge of the true number of classes C. In Table D.4, we estimatethe number of classes C and use it for evaluation depending on the type of methods: for non-parametric clustering-basedmethods , we apply Brents algorithm to estimate C as in , and for parametric classification methods andours, we provide an arbitrarily large number, e.g., Cinit = 2C, and estimate C by eliminating inactivated classes, i.e., classeswithout mappings from any training data. Notably, the uniform prior assumption in the k-means algorithm leads GCDand BaCon to significantly underestimate the total class number in long-tailed datasets, resulting in overall performancedegradation. In the case of ORCA, its pairwise learning could be dominated by known and head classes as pairs mostlyconsist of data from known and head classes, and its binary uncertainty estimation would not be suitable for distinguishingknown-tail and novel-head classes, resulting in significant inactivation of classification heads. Our method demonstratescomparable performance to scenarios where the number of classes is known, with only a 1.0% decrease in overall inductiveaccuracy.",
  "D.6. Number of tailedness prototypes": "To evaluate the performance sensitivity in relation to the number of tailedness prototypes M, we conduct an ablation studyon different prototype numbers. As shown in Table D.5, aligning the number of prototypes with the class number yieldsthe best performance. In general, our method demonstrates robustness across various numbers of prototypes, yielding thebest performance among compared methods in most cases. Note that matching the number of prototypes with the truenumber of classes might not always result in the best performance, because multiple fine-grained classes might form a singlecoarse-grained class or a class might consist of multiple local clusters .",
  "D.7. Results with different imbalance ratios": "In previous experiments, we use = 100 for CIFAR-100-LT. In Tables D.6 to D.7, we conduct an ablation study for differentimbalance ratios ( = 1, 10) on CIFAR-100-LT. Our method shows superior performance in overall accuracy for variousimbalance ratios, showing its generalization ability for the different class priors. Bacon often outperforms our method innovel class accuracy in transductive inference, however, its performance is degraded in inductive inference, while our methodmaintains good performance in inductive inference.",
  "E.1. Number of neighbors for the K-NN": "We consider K = {5, 10, 15, 20, 25} for inspecting the impact of the number of nearest neighbors on tailedness estimation.As shown in Fig. E.1a, the optimal number of nearest neighbors is 15. When the neighborhood size is increased to includelarge neighbors (K > 20), we observe a slight degradation in performance, implying that the larger neighborhoods mightaccurately capture the local density that represents the class prior distribution.",
  "E.2. Hyperparameter max": "In Fig. E.1b, we investigate the effect of the range of by considering max = {0.5, 0.7, 0.9, 1.0, 1.5} with min = 0.05, wheremax = 1.0 shows the best performance. We argue that it optimally balances the uniformity and alignment of representation.A narrow range of tau (max < 0.7) may disrupt the semantic representation, while a wide range of tau (max > 1.0) couldnegatively impact learning instance-specific features.",
  "E.3. Hyperparameter min": "In Fig. E.1c, we examine the effect of the range of by considering min = {0.01, 0.02, 0.05, 0.1, 0.3} with max = 1.0, wheremin = 0.05 shows the best performance. We argue that it optimally balances the uniformity and alignment of representation.A high minimum value of tau (min > 0.1) may hinder the learning instance-specific features, while a low minimum value oftau (min < 0.05) may disrupt the semantic representation.",
  "E.4. Hyperparameter var": "We examine the effect of the weight parameter var as illustrated in Fig. E.1d, where we consider var = {0.5, 1.0, 2.0, 3.0}.Among them, var = 1 shows the best performance. Notably, a larger weight parameter appears to adversely affect theinformation contained in the original output logits of the cosine classifier.",
  "F. Detailed Results of Main Experiments": "To better examine the impact of dataset imbalance, we conduct a detailed comparison in Tables F.1 to F.4. In Table F.4, wereport the performance in Missing Not At Random (MNAR) scenarios for the in-nature long-tailed dataset Herbarium19. Ourapproach demonstrates a significant performance improvement for novel and tail classes, where the conventional open-worldand long-tailed learning strategies do not take into account the importance of learning tail and novel classes, respectively.This validates that our method effectively addresses known and head class bias issues.",
  "G. Results on ImageNet-100-LT": "While ImageNet-100 has been often used for OWSSL in literature, we argue that ImageNet-100 might not be appropriate forthe conventional OWSSL settings built on top of ImageNet-1K pre-trained backbone, e.g., DINO-ViT , as it alreadyobserved data from novel classes during pretraining. In other words, the performance could be boosted by preserving thepre-trained knowledge rather than learning to discover novel classes and classify all classes. Nevertheless, below we reportthe performance on ImageNet-100 with the data split from BaCon . In Tables G.1 to G.2, our proposed method achievessignificantly better performance on novel and tail classes, surpassing the baseline performance. Notably, the classic baselineGCD often shows the best performance (mostly in transductive inference), implying that it preserves the pre-trainedknowledge better.",
  "H. Visualizations": "To inspect the learned semantic discriminativeness of the proposed DTS on the long-tailed dataset, we visualize embeddingsby t-SNE algorithm, trained on CIFAR-100-LT with distribution match. We show the feature embedding of pretrainedDINO , GCD , SimGCD , and DTS (Ours), in Fig. H.1. Compared to other models, the model trained with our DTSlearns less ambiguous features which exhibits a larger margin between different classes, with more compact clusters. Thisindicates that our method is more effective in learning a discriminative semantic structure, even under long-tailed datasets.",
  "I. Conclusion and Limitations": "In this paper, we formulate the practical ROWSSL setting, which considers the long-tailed distribution and the class priordistribution mismatch between labeled and unlabeled data for training, and inductive and transductive inferences for evalua-tion. To tackle ROWSSL, we introduce a novel method called Density-based Temperature scaling and Soft pseudo-labeling(DTS), which learns class-balanced representations and mitigates the classification bias based on local densities. Neverthe-less, we acknowledge several limitations inherent in DTS and existing methods. First, the labeled and unlabeled data aresampled from the same dataset, which might not reflect domain shifts in real-world scenarios. Second, estimating the numberof novel classes with off-the-shelf methods can result in inaccurate prediction due to the imbalanced class prior distribution.We believe that ROWSSL will establish a robust foundation for future research and contribute to the development of morereliable methods for practical applications of OWSSL.",
  "J. Negative Societal Impact": "While our work itself is not inherently harmful to society, there is a risk that it could be misused by those with maliciousintent. For example, the proposed method could be used to unfairly single out and target certain groups, such as minorities.Consequently, we urge that this work must be utilized within ethical and legal boundaries.",
  "Eric Arazo, Diego Ortego, Paul Albert, Noel E OConnor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deepsemi-supervised learning. In IJCNN, 2020. 4": "Mahmoud Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, MichaelRabbat, and Nicolas Ballas. The hidden uniform cluster prior in self-supervised learning. In ICLR, 2023. 2, 1 Jianhong Bai, Zuozhu Liu, Hualiang Wang, Ruizhe Chen, Lianrui Mu, Xiaomeng Li, Joey Tianyi Zhou, Yang Feng, Jian Wu, andHaoji Hu. Towards distribution-agnostic generalized category discovery. arXiv preprint arXiv:2310.01376, 2023. 2, 4, 1, 5",
  "N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. SMOTE: Synthetic minority over-sampling technique. Journal ofArtificial Intelligence Research, 2002. 1": "Zhang Chuyu, Xu Ruijie, and He Xuming. Novel class discovery for long-tailed recognition. In TMLR, 2023. 1 Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. InCVPR, 2019. 1 Zhigang Dai, Bolun Cai, and Junying Chen. Unimoco: Unsupervised, semi-supervised and fully-supervised visual representationlearning. 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2022. 3",
  "Mamshad Nayeem Rizve, Navid Kardan, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Openldn: Learning to discovernovel classes for open-world semi-supervised learning. In ECCV, 2022. 1": "Mamshad Nayeem Rizve, Navid Kardan, and Mubarak Shah. Towards realistic semi-supervised learning. In ECCV, 2022. 4 Yiyou Sun and Yixuan Li. Opencon: Open-world contrastive learning. In TMLR, 2023. 4, 1 Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. In JMLR, 2008. 9 Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Generalized category discovery. In CVPR, 2022. 2, 4, 1, 8, 9 Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In CVPR, 2021. 2 Haotao Wang, Aston Zhang, Yi Zhu, Shuai Zheng, Mu Li, Alex J Smola, and Zhangyang Wang. Partial and asymmetric contrastivelearning for out-of-distribution detection in long-tailed recognition. In ICML, 2022. 4, 1",
  "Chen Wei, Kihyuk Sohn, Clayton Mellina, Alan Yuille, and Fan Yang. Crest: A class-rebalancing self-training framework forimbalanced semi-supervised learning. In CVPR, 2021. 1": "Tong Wei and Kai Gan. Towards realistic long-tailed semi-supervised learning: Consistency is all you need. In CVPR, 2023. 1 Xin Wen, Bingchen Zhao, and Xiaojuan Qi. A simple parametric classification baseline for generalized category discovery. In ICCV,2023. 2, 3, 4, 9 Junjie Wu, Hui Xiong, and Jian Chen. Adapting the right measures for k-means clustering. In KDD, 2009. 1 Muli Yang, Liancheng Wang, Cheng Deng, and Hanwang Zhang. Bootstrap your own prior: Towards distribution-agnostic novelclass discovery. In CVPR, 2023. 1, 3 Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, and Fahad Shahbaz Khan. Promptcal: Contrastiveaffinity learning via auxiliary prompts for generalized novel category discovery. In CVPR, 2023. 4, 1"
}