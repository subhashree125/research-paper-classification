{
  "Abstract": "Despite the recent progress made in Video Question-Answering (VideoQA), these methods typically function asblack-boxes, making it difficult to understand their reason-ing processes and perform consistent compositional rea-soning. To address these challenges, we propose a model-agnostic Video Alignment and Answer Aggregation (VA3)framework, which is capable of enhancing both composi-tional consistency and accuracy of existing VidQA methodsby integrating video aligner and answer aggregator mod-ules. The video aligner hierarchically selects the relevantvideo clips based on the question, while the answer ag-gregator deduces the answer to the question based on itssub-questions, with compositional consistency ensured bythe information flow along question decomposition graphand the contrastive learning strategy.We evaluate ourframework on three settings of the AGQA-Decomp datasetwith three baseline methods, and propose new metrics tomeasure the compositional consistency of VidQA methodsmore comprehensively. Moreover, we propose a large lan-guage model (LLM) based automatic question decomposi-tion pipeline to apply our framework to any VidQA dataset.We extend MSVD and NExT-QA datasets with it to evaluateour VA3 framework on broader scenarios. Extensive exper-iments show that our framework improves both composi-tional consistency and accuracy of existing methods, lead-ing to more interpretable real-world VidQA models.",
  "With the development of representing video, questionand their alignment, numerous works have achieved considerable success in both open-endedVidQA and multi-choice VidQA": "However, existing VidQA methods often function asblack-box models, making it difficult to understand the rea-soning process behind their predictions and leading to in-consistent compositional reasoning. For example, in Fig-ure 1, HQGA can answer the question Is a phone thefirst object that the person is touching after taking a pic-ture? as Yes. However, HQGA can neither clearly iden-tify the video clips that contain touch a phone or take apicture nor predict all the sub-questions correctly. There-fore, the lack of reasoning transparency can lead to poorcompositional consistency, which reveals limited composi-tional reasoning ability, and further limits the accuracy ofVidQA models, particularly on questions that involve tem-poral relations and multiple visual clues . To tackle this issue, we introduce the Video Align-ment and Answer Aggregation (VA3) framework, whichaddresses these challenges by improving their composi-tional consistency and accuracy. This framework is model-agnostic and can be applied to various VidQA methods,such as memory-based , graph-based , and hierarchy-based methods. In detail, our VA3 framework includes two ad-ditional modules, the video aligner and answer aggrega-tor.The video aligner hierarchically aligns the questionwith the video clips from the object-level, appearance-levelto motion-level.The answer aggregator takes the ques-tions from the same Question Decomposition Graph (QDG)as input and deduces their answers based on their video-question joint representation. To the enhance compositionalconsistency, we further explore a contrastive learning strat-egy on the edge type of QDG. Overall, the VA3 frameworkimproves both compositional consistency and accuracy ofexisting VidQA methods, provides a more transparent com-positional reasoning process, and further leads to more in-",
  "A: Yes, Prediction: Yes": ". The Question Decomposition Graph (QDG) of a ques-tion from AGQA-Decomp . The predicted answer to eachquestion is from HQGA . Green (resp., Red) represents thepredicted answer is right (resp., wrong) terpretable VidQA models in real-world applications.As for the evaluation metrics, AGQA-Decomp pro-poses the compositional accuracy (CA), right for the wrongreasons (RWR), and delta (CA RWR) system to evaluatethe compositional consistency of VidQA methods. How-ever, these metrics only focus on reasoning failure basedon the sub-questions correctness without considering themain question correctness, leading to asymmetric and un-stable problems. To address this, we extend it to provide asymmetric and stable measurement for compositional con-sistency. In detail, our metrics include consistency preci-sion (cP), consistency recall (cR), and consistency F1 (c-F1)along with their negative versions. These metrics can evalu-ate the compositional consistency of VidQA methods froma balanced viewpoint. More details are in .We conduct the experiments on the AGQA-Decompdataset to verify the effectiveness of our frame-work. This dataset decomposes the questions into the sub-questions and the directed acyclic graphs, i.e. the QDGs,making it applicable to evaluate the compositional consis-tency for VidQA methods. To validate the effectiveness andcompositional consistency of our VA3 framework, we con-duct comprehensive experiments with three baseline meth-ods: HME , HGA , and HQGA , which arethe representations of the memory-based, graph-based andhierarchy-based methods, in three different settings: bal-anced, novel compositions, and more compositional steps.Moreover, we propose an automatic question decomposi-tion pipeline for VidQA datasets with the help of large language models (LLMs) to generalize our framework todatasets that do not have QDGs (e.g., MSVD andNExT-QA ) to verify the applicability of our frame-work. Additionally, we visualize the aligned video clips andthe variation of predicted answers while equipping videoaligner and answer aggregator successively to the backbonemodel on QDG to verify the interpretability of our frame-work. Our contribution can be summarized as: Dataset: We propose an automated question decomposi-tion pipeline for any VidQA dataset to generate the QDGsand the sub-questions with the help of LLMs and furtherextend MSVD and NExT-QA dataset with it. Framework: We propose a model-agnostic VA3 frame-work, which provides a more transparent compositionalreasoning process and increases both the interpretabilityand the accuracy of existing VidQA models. Metric: We extend the compositional consistency metricsas consistency precision (cP), consistency recall (cR) andconsistency F1 (c-F1) along with their negative versionsfor a more balanced and comprehensive evaluation. Experiments:Comprehensive experiments with threebaselines on five benchmark settings of three datasets re-veal that our framework significantly boosts these base-lines in compositional consistency and accuracy.",
  ". Video Question-Answering": "While the architecture of VidQA methods has undergonesignificant changes over the years, the essential componentsof these methods remain the same: video representation,question representation, and video-question aligned repre-sentation.For the video representation, appearance fea-tures and motion features were commonly used,then the object-level representation was introduced .For the question representation, most existing works reliedon word embeddings with RNNs, while BERT fea-tures became widely used in more recent works .In the early research, the video-question alignment was im-plemented using cross-modal attention or memorynetworks , then graph reasoning became popular. Recently, the natural hierarchy in videorepresentation received more attention.Despite these advancements, existing methods still facechallenges in achieving satisfactory levels of compositionalconsistency . To address these challenges, in this paper,we propose a model-agnostic framework for compositionalreasoning by combining the visual alignment and the an-swer aggregation to improve current VidQA methods.",
  "Video": ". Our model-agnostic Video Alignment and Answer Aggregation (VA3) framework. (v, q) is a video-question pair, where qmdenotes main-question and qs1, , qsn denote the n sub-questions derived from qm. vm and {vs1, , vsn} denote the aligned videosaccording to corresponding questions. fvm,qm and {fvs1 ,qs1 , , fvsn ,qsn } denote the video-question joint features. Gqm is the questiondecomposition graph (QDG) associated with qm, which is a direct acyclic graph describing the compositional relationship among questions.Moreover, Gqm stores in which manner the questions are decomposited (i.e., the operators in the decomposition program) as the attributeof edges. am denotes the predicted answer for qm. In VidQA, most of earlier efforts broke down ques-tions into modular programs that were defined in a neuralmodular network to answer the question. AGQA explored spatio-temporal scene graphs to represent the pro-grams for VidQA. However, such a reasoning program can-not be directly used by existing VidQA methods. To addressthis issue, AGQA-Decomp transferred each reasoningprogram into several sub-questions and QDG to evaluate thecompositional consistency of existing VidQA methods.The Neural Modular Network (NMN)-based methods(e.g., DSTN ) modularized the VidQA task into mul-tiple modules (e.g., FindObj, TemporalFilter, etc.) and gen-erated the reasoning program through a modular policy. Al-though the NMN-based approaches provide perfect inter-pretability, there still exist three main challenges: 1) the ba-sic modulars and logic rules have to be pre-defined, makingany novel modulars and logic rules incompatible; 2) com-pared to conventional neural networks, training NMNs canbe more challenging because the learning process optimizesthe composition strategy other than the individual modules;3) as the number of modules increases, the search spacefor optimal modules grows exponentially, which hinders itsscalability to more complex tasks or larger datasets.",
  ". Video Grounding": "Video grounding seeks to identify the most rele-vant moment in a video based on language queries , and has received growing attention from down-stream video-language tasks . Previous workssuch as IGV and EIGV have focused on differ-entiating between causal and environment clips in VidQAthrough a simple grounding indicator and encouraging sen-sitivity to semantic changes in the causal scene, respec-tively. In contrast to these approaches, our framework hier-archically aligns the question with video clips from object-level, appearance-level, to motion-level to provide a morerefined video context along with an answer aggregator. This",
  ". Model-agnostic VidQA Framework": "Our VidQA framework is illustrated in .Foreach original question in dataset, it is decomposed (eitheroracularly or with our question decomposition pipeline)into several sub-questions.Formally, the original ques-tion (regarded as main question) qm and its decomposedsub-questions qmsi form a question cluster on correspond-ing QDG Gqm.First, we introduce a hierarchical videoaligner, which selects the question-related video clips byhierarchically interacting the video clips with the questionamong the object-level, appearance-level and motion-level.After that, the questions in the cluster and their correspond-ing video clips are send into a VidQA model, regarded asF : (v, q) fv,q Rh, where h is the hidden dimensionfor joint feature space, to generate the joint feature fv,q.The answer aggregator takes all the joint features from thequestions cluster and associates each joint feature with re-gard to the question node in the QDG. By aggregating theinformation in joint features of each node through QDG, weadjust the question representations and predict the answers.",
  ". Video Alignment": "The structure of video aligner is shown in . Thevideo aligner takes video-question pair (v, q) as input, andgives the video clips that are most relevant to the ques-tion. Former research on localizing video clips with ques- tions failed to use the natural hierarchy of video fea-tures, thus limited the representation ability of the aligner.In our video aligner, a video v is represented withinthree-level features: object feature Fo Rncnf nohv,appearance feature Fa Rncnf hv and motion featureFm Rnchv, where nc, nf and no represents the num-ber of clips per video, frames per clip and objects per framerespectively, and hv is the hidden dimension for each fea-ture vector. As these three-level features naturally follow ahierarchical relationship, we designed a hierarchical videoaligner to capture them for a better alignment.Our hierarchical video aligner follows a bottom-upvideo-question interaction scheme. Starting from Fo, weaggregate its information among objects with the condi-tion of question, concatenate it with corresponding Fa, andfurther aggregate it with other frames in control of ques-tion. Then, such cross-frame representation is concatenatedwith Fm, and interacted with question feature to generatethe grounding score. During each aggregation, we fuse thevideo feature and question feature through a transformerlayer, where the video feature is regarded as query whilequestion feature serves as the key and value. Formally, theaggregation from object feature to appearance feature is",
  "nonoWoF jo + boF jo ; F ca =[F ao ||Fa];": "where ni is the softmax function along ni dimension, TF isthe transformer encoder layer, Wo and bo are trainable pa-rameters, and [||] denotes the concatenation operator. Theupdated appearance feature F ca is used to produce aggre-gated motion feature F cm in a similar manner. Further, weuse the F cm to generate the binary indicator of relevant clipfor the video, which can be formulated as",
  "sirr = MLP2(F jm); I = Gumble-Softmax([srel||sirr]),": "where MLP is the multi-layer linear projection.Since the ground-truth of aligned video is not applicablein VidQA dataset, we exploit the contrastive learning to guide this module. Formally, given a video-question pair(v, q) in training data, the indicator specifies the relevantvideo clips vr and irrelevant video clips vc within v. Thus,the anchor fvr,q, positive sample fv,q, and negative sam-ple fvc,q of the contrastive loss is presented as",
  ". Answer Aggregation": "Existing VidQA methods predict the answers of differ-ent questions independently, which ignores the correlationsamong questions from the same cluster, leading to insuffi-cient compositional consistency. Therefore, we introducean answer aggregator to supplement the main-question withsub-questions and enhance the compositional consistency.Specifically, assume {fqi,vi|i {s1, , sn, m}} is thevideo-question joint feature extracted by backbone VidQAmodel for all questions in QDG Gqm = (Vqm, Eqm). Weexplore the graph attention network (GAT) to aggregate thejoint feature along the given QDG. Formally, given the k-th layer of the GAT, the main question qm and its sub-questions {qs1, , qsn}, the information aggregation fornode associated with qi is formulates as",
  "(8)": "where Wo2 and bo2 are trainable parameters.Moreover, to enhance the compositional consistency, weintroduce an additional contrastive training scheme. As thetype of relation (i.e., edge) between the questions providescrucial clues in question decompositing and compositionalreasoning, we introduce a heuristic prior, where edges withthe same type shall have similar representations, and thedistance between different types of edges shall be relativelylarge. By leveraging this prior, we can raise the level ofabstraction for more accurate and consistent answer reason-ing. Formally, {f e = We[f aqi,vi||f aqj,vj] + be|e Eqm} AGQADataset Class1:object-relationQuestion 1-1Question 1-2...... ...... LLM Rearrange the questions in the following set according to the similarity with <Question>. The most related K questions in AGQA",
  ". The automatic question decomposition pipeline. Thequestion to be decomposed is denoted as <Question>": "denotes the set of node relation representation for edgesin graph Gqm, where We and be are the trainable param-eters. Moreover, we use te T to denote the class of edgee, where T is the set of edge types. For t T, we usetc = T/{t} to represent the complementary set of t, anduse et to denote a random edge sampled from all edges withtype t. Thus, the triplet loss for main question qm is",
  ". Automatic Question Decomposition Pipeline": "For some VidQA dataset (e.g., MSVD and NExT-QA), theQDGs are not applicable as they only provide the mainquestions. To address this issue, we explore an automaticquestion decomposition pipeline for VidQA data using theknowledge in LLMs. Since directly asking the LLM to de-compose questions may result in poor results, and randomexamples could cause unstable quality as the chosen ex-amples may have different compositional structure with thequeried question, we proposed the decomposition pipelineas shown in . Firstly, we construct a candidate ex-ample set based on the AGQA-Decomp dataset manually,in which each subset consists of a few main questions cho-sen with a main question type in AGQA-Decomp dataset tocover as many types of main questions as we can. Then,we construct a selection prompt which ask the LLM to se-lect the most similar K question, and they form K-shot ex-amples with their QDGs. Finally, these K-shot examplesare provided to LLM with a decomposition prompt askingthe LLM to decompose the target question, resulting in thedecomposed sub-questions and corresponding QDGs withbetter quality. More details and explanations of our pipelineand prompts are in the supplementary material.",
  ". Metrics": "Compositional consistency measures whether a method canprovide the correct answer for the right reason. AGQA-Decomp propose compositional accuracy (CA), rightfor the wrong reasons (RWR), and Delta (CA-RWA), whichoffer some insight on it. However, as we are to illustratein .1, they cannot fully reveal the reasoning capa-bilities, leading to asymmetric and unstable problem. Toaddress this issue, we extend them with compositional pre-cision (cP), recall (cR), and F1 (c-F1), along with their neg-ative versions, providing a more comprehensive assessmentof reasoning consistency. We name qj as parent question,and qi, qk as children question of qj for QDG edges likeqi qj qk. Note that we only consider the 1-st orderparent-children relation, and the intermediate sub-questionscan be both parent or children regarding the viewpoint.",
  ". Another View of Existing Metrics": "CA and RWR are designed to evaluate the accuracy of theparent questions, conditional on whether all their childrenquestions are correct. Let N+ denote the number of cor-rectly answered main-question with any sub-question incor-rect, while N+ denotes the number of falsely answered mainquestion with all sub-questions correct. N and N++ is sim-ilarly defined. Thus, CA and RWR is formulated as",
  "CA=N++/(N+++N + ); RWR=N+/(N++N),(11)": "and Delta = RWRCA. Therefore, both CA and 1RWRcan be viewed as precisions, as the conditions only in-spect the correctness of the children questions and missedout the correctness of the parent questions, leading to asym-metric and unstable problems illustrated in .For row 1 to row 4, the corresponding model shall havethe same compositional consistency, because in all 210 par-ent questions, 110 of them can be answered for the rightreasoning and the opposite. However, the CA, RWR andDelta varies significantly among these models, which indi-cate that the CA-RWR-Delta metrics cannot treat N++, N,N+ and N+ asymmetrically and cannot correctly identifythe compositional consistency. Moreover, such failure alsolead to instability when facing imbalanced child questionaccuracy distribution, as shown in row 5 to row 7. Thesemodels have similar compositional consistency, but the CA,RWR and Delta may vary to the extreme opposite value.",
  "NcP+NcR. (13)": "In such definition, is a hyper-parameter to balance cPand cR. To measure the two kinds of error in a equal weight,we set = 1, and thus use c-F1 to measure the compo-sitional consistency of models. Such metric considers thecompositional consistency in a symmetric manner, raisinga comprehensive evaluation on models ability. As shownin , our c-F1 and Nc-F1 metrics raises more reason-able evaluations (row 1 to row 4), and is also more stablefacing extreme cases (row 5 to row 7). Note that althoughc-F1 and Nc-F1 provides a balanced view of compositionalconsistency, we still need to the accuracy, cP, and cR for adetailed analysis regarding prediction ability and composi-tional bias. More analysis and comparison between ourmetrics and original ones are in the supplementary.",
  ". Experiment Setting": "Dataset As described in , we conduct our ex-periment on AGQA-Decomp benchmark, which extendsAGQA 2.0 by decomposing each question into several sub-questions with a QDG, and evaluates the compositional reasoning ability by supplying extensive challenging com-plex questions with their decomposed sub-questions andanswers. Moreover, we test the improvement on MSVDand NExT-QA dataset to verify the applicability of our VA3 framework and question decomposition pipeline.Baselines and Metrics We conduct experiment on allthree categories of VidQA mothods, i.e., memory-based,graph-based and hierarchy-based methods. Specifically, wechoose HME , HGA and HQGA as the baselinemethods from each category respectively. For the evaluationmetrics, we measure the open-ended, binary, and overall ac-curacy for main questions and sub-questions. Moreover, toillustrate the improvement in terms of compositional consis-tency, we evaluate the cR, cP, c-F1, NcR, NcP and Nc-F1.More detailed settings are in supplementary material.",
  ". Main Results": "The result of our framework with various baselines is shownin . Compared the 1-st to 3-rd row with the 4-th to6-th row correspondingly, our framework outperforms allbaseline models, including memory-, graph- and hierarchy-based models, significantly in terms of both accuracy andcompositional consistency. For accuracy, the overall mainquestion accuracy improves 1.23% to 2.71%, while the sub-question accuracy raises 3.16% to 3.29%. The accuracy im-provement on sub-questions are usually more than that onmain questions. For video aligner, it is more hard to alignthe corresponding video clips for main questions, besides,for answer aggregator, it is also more challenge to aggregateall the sub-questions to deduce the main question, leadingto such improvement gap. Moreover, the accuracy improve-ments on binary and open-ended questions are not equal.The reasons include the the following two aspects: 1) thevideo aligner helps open-ended questions more since theyare more sensitive to irrelevant clips as they have to chooseanswer from a much larger candidate set than binary ques-tions, and may be mislead by the actions in irrelevant clipsmore easily; 2) the open-ended questions provide and re-ceive more severe information in answer aggregation, mak-ing the answer aggregator contribute more on them.Moreover, the compositional consistency also raises sig-nificantly compared to the baseline models. Specifically,the c-F1 significantly improves 2.97% to 3.54%, while theNc-F1 raises 0.11% to 0.64%. The c-F1 indicates how muchthe model answers correctly with correct inference, there-fore, the c-F1 is the most important overall measurement forthe reasoning ability of models. The significant improve-ment in terms of c-F1 further indicates that our frameworkdoes help the VidQA models reasoning correctly and con-sistently. And the Nc-F1, which measures if the model isstill consistent even when the answer is incorrect, showsthat our framework also slightly helps improve the overallconsistency on the incorrect main questions. Furthermore,",
  "VA3(HQGA) 36.33+2.12 40.76+1.85 68.20+0.24 47.91+1.27 51.75+1.10 63.26+0.29": ". The comparison with baseline methods on the AGQA-Decomp novel composition setting and more composition stepsetting . Comp. is the abbreviation for compositional. Theimprovements of our framework are highlighted as superscript. we can also find that the the improvement on c-F1 are nat-urally more significant that on Nc-F1 as our constraint onanswer aggregation mainly focus on correctly deducing themain question based on the correct sub-questions.",
  ". Generalization Ability": "We further test the improvement of our framework whengeneralizing to new situations on two extra settings, i.e.novel composition and more composition step . TheNovel composition setting tests if models can generalize tounseen composition types, and the more composition stepsetting tests the generalization ability when facing morecomplex questions than training. The results are in .When facing novel composition setting, the clear accu-racy drop compared to indicates generalizing tonovel composition setting is much harder for VidQA mod-els than the standard setting. However, our framework isstill capable of significantly boosting baseline methods un-der this challenging setting. The main question accuracyraises 1.87% to 2.12%, while the c-F1 improves 1.85% to5.03% and the Nc-F1 improves 0.14% to 0.24%, on differ-ent baselines, implying that our framework provides bettergeneralization ability on unseen composition types, in termsof both accuracy and compositional consistency.For the more composition step setting, our frameworkstill significantly improves the baseline methods on boththe accuracy and compositional consistency, indicating ourframework is effective when generalizing to more complexquestions. In detail, there is a 1.12% to 1.76% accuracy",
  ". Ablation Study": "To measure the effectiveness and necessity of our modules,we conduct ablation studies in this section. To clearly re-veal the contribution of each module and loss, we chooseHME as the baseline method in this section, since theimprovement over HME is the largest among all three base-lines. For the video aligner, we test its improvement overthe EIGV aligner brought by the hierarchy structure.For the answer aggregator, we test how much it boosts theoriginal model even without our contrastive loss Lqmc , thenmeasure the improvement of applying Lqmcon answer ag-gregator. The results are summarized in .By comparing row 3 with rows 1 and 2, we can concludethat the video aligner substantially improves the accuracy ofboth main questions and sub-questions, but helps little oncompositional consistency. This is reasonable since videoaligners do not exploit the relations between main questionsand sub-questions, thus cannot improve the compositionalconsistency among them. Moreover, the comparison be-tween row 3 and row 2 shows that our aligner outperformsthe video grounding module in EIGV due to its hierarchi-cal structure, which could effectively extract informationfrom different level of video feature and align them withthe question. As for the answer aggregation module, bycomparing row 4 with row 1, we can infer that answer ag-gregator, even without the contrastive loss Lqmc , can signifi-",
  "VA (HME)": ". Quantitive results of Video Aligner and the visualization of improvements on accuracy and compositional consistency broughtby our modules. Best viewed in color and zoom in. More visualizations and explanations are in the supplementary material. cantly improve the accuracy and compositional consistency,as the information exchange along main-sub questions rela-tion helps both correct answering and ensuring their consis-tency. Further, as the comparison between row 4 and row 5implies, the contrastive loss Lqmcfurther improves the accu-racy and compositional consistency. Finally, the improve-ment of row 6 over row 3 and 5 indicates that our combi-nation of video aligner and answer aggregator is mutuallybeneficial on both accuracy and compositional consistency,proving the effectiveness of our framework.",
  ". Quailititave Study": "In , we firstly visualize the result of video aligner.As the left part of shows, our video aligner suc-cessfully aligns the related video clips (denoted by the dot-ted boxes) with the corresponding questions.Therefore,the VidQA backbones can use more accurate informationto improve the accuracy of both main questions and sub-questions. Moreover, we also visualize the predicted an-swers of HME, HME with video aligner, and VA3(HME)respectively. The original model failed to answer the mainquestion correctly due to several factual errors in sub-questions and the potential reasoning failure. With the helpof video aligner, we may reduce the irrelevant noise by onlyprocessing the most correlated clips, thus eliminating sev-eral factual errors (i.e., qs1, qs3 and qs6), but may not help onthe main question since the whole video is fatal in generat-ing its answer. Moreover, the video aligner may not correctthe compositional reasoning failure as it does not use inter-question relation information. However, with the help of theanswer aggregator, we can correct the reasoning failure byaggregating the information from sub-questions (e.g., cor-rect the answer of qs2 by aggregating video-question jointfeatures of qs4, qs5 and qs6), and deduce the correct answerof main questions with higher compositional consistency.",
  ". The comparison with baseline methods on MSVD andNExT-QA datasets. Improvements are highlighted as superscripts": "composition pipeline. The results are summarized in Ta-ble 5. By comparing the 1st to 3rd columns with 4th to 6thcolumns in the table, we could find that our framework, withour automatic decomposition pipeline, significantly booststhe performance of baselines on both MSVD and NExT-QAdataset. Specifically, the overall accuracy improves 3.23%to 4.76% on MSVD, while improves 3.58% to 4.51% onNExT-QA, which indicates that with the help of our ques-tion decomposition pipeline, our framework can still raisethe performance significantly even on datasets which origi-nally do not have QDGs, further implying the applicabilityof our framework in real world scenarios.",
  ". Conclusion": "In this work, we have focused on the VidQA from in-terpretability and proposed a model-agnostic align-and-aggregate framework for VidQA. It firstly aligns thevideo representation towards both main question and sub-questions, then aggregates the video-question joint repre-sentation through the QDG. Further, we have revisited thecompositional consistency metrics and have proposed morecomprehensive c-F scores. Extensive experiments on var-ious VidQA models have revealed that our framework im-proves both compositional consistency and accuracy signif-icantly, leading to more interpretable VidQA models.",
  "Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, JosefSivic, Trevor Darrell, and Bryan Russell. Localizing mo-ments in video with natural language. In ICCV, pages 58035812, 2017. 3": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners. In NeurIPS, 2020.",
  "Jiangtong Li, Li Niu, and Liqing Zhang. From Representa-tion to Reasoning: Towards both evidence and commonsensereasoning for video question-answering.In CVPR, pages2124121250, 2022. 1": "Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, SiliangTang, Fei Wu, Yi Yang, Yueting Zhuang, and Xin EricWang. Compositional temporal grounding with structuredvariational cross-graph correspondence learning. In CVPR,pages 30323041, 2022. 3 Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu,Wenbing Huang, Xiangnan He, and Chuang Gan. BeyondRNNs: Positional self-attention with co-attention for videoquestion answering. In AAAI, pages 86588665, 2019. 2"
}