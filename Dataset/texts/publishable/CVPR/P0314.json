{
  "Abstract": "Generalizability in deep neural networks plays a piv-otal role in medical image segmentation. However, deeplearning-based medical image analyses tend to overlookthe importance of frequency variance, which is critical el-ement for achieving a model that is both modality-agnosticand domain-generalizable.Additionally, various modelsfail to account for the potential information loss that canarise from multi-task learning under deep supervision, afactor that can impair the models representation abil-ity. To address these challenges, we propose a Modality-agnostic Domain Generalizable Network (MADGNet) formedical image segmentation, which comprises two keycomponents: a Multi-Frequency in Multi-Scale Attention(MFMSA) block and Ensemble Sub-Decoding Module (E-SDM). The MFMSA block refines the process of spatialfeature extraction, particularly in capturing boundary fea-tures, by incorporating multi-frequency and multi-scalefeatures, thereby offering informative cues for tissue out-line and anatomical structures. Moreover, we propose E-SDM to mitigate information loss in multi-task learningwith deep supervision, especially during substantial upsam-pling from low resolution. We evaluate the segmentationperformance of MADGNet across six modalities and fif-teen datasets. Through extensive experiments, we demon-strate that MADGNet consistently outperforms state-of-the-art models across various modalities, showcasing superiorsegmentation performance. This affirms MADGNet as a ro-bust solution for medical image segmentation that excels indiverse imaging scenarios. Our MADGNet code is avail-able in GitHub Link.",
  ". Introduction": "Various types of cancers continue to pose a substantialthreat to human life, contributing significantly to globalmortality rates. In this context, the importance of medicalimage analysis becomes evident, serving as a linchpin in the . Scale vs Frequency distribution per modality. The scaledenotes the size of lesions, measured as the ratio of foregroundpixels to the total number of pixels. Frequency is calculated bythe power spectrum ratio of the high-frequency and full-frequency.We observe that the frequency variance is higher than the scale,which mainly focuses on solving the various sizes in other meth-ods. early detection of malignant tumors or abnormal cells, ulti-mately contributing to extending patients lives . How-ever, the manual analysis of noisy and blurred medical im-ages presents notable challenges, rendering it susceptible tohuman errors . In response to these issues, computer-aided diagnosis using traditional segmentation algorithms has garnered attention from clinical experts.Nevertheless, these algorithms still lack generalizability tonew patient cases due to issues such as uneven intensity dis-tribution, unexpected artifacts, and severe noise in medicalimages .",
  "arXiv:2405.06284v1 [eess.IV] 10 May 2024": "Recent advancements in deep learning havepaved the way for their application in medical image seg-mentation. In particular, UNet has emerged as a fun-damental approach for medical image segmentation, ow-ing to its effective utilization of skip connections withinits U-shaped architecture.An example of this evolutionis UNet++ , which employs nested UNet and deep su-pervision with skip connections to reduce the semantic gapbetween the encoder and decoder. With the recent surgein attention mechanisms , the evolution ofUNet has witnessed the integration of attention, extractingdiscriminative features from noisy medical images, as seenin Attention UNet and Focus UNet . Nevertheless,owing to disparities in medical image acquisition methods(modality) and the limited accessibility of datasets by pa-tient privacy, these models often lack generalizability forunseen clinical settings, rendering practical implementationchallenging.Therefore, exploring the question, How awell-trained model in a modality can be generalized intoother modalities (modality-agnostic) and be applicable inunseen clinical settings (domain generalizability)? is acritical task to address these challenges.",
  "Plotting the scale and frequency distribution graph (Fig": "1) unveiled a significant finding:medical images ex-hibit distinct distributions in both dimensions. While ac-knowledging the inherent interdependence between multi-frequency and multi-scale, emphasizes that they stillreveal distinctive information. Multi-frequency, in particu-lar, displays more variance, highlighting unique and valu-able insights. This challenges the prevailing focus on ei-ther multi-scale or multi-frequency information and underscores the necessity of recognizing the in-dependent and complementary nature of both dimensions inmedical image segmentation. This discovery prompted usto efficiently utilize both dimensions, recognizing that evenwith interdependence, each dimension contributes distinctand valuable information. To address the aforementioned problem, we proposea novel attention mechanism called Multi-Frequency inMulti-Scale Attention (MFMSA) block.This block em-ploys multi-frequency channel attention (MFCA) with 2DDiscrete Cosine Transform (2D DCT) to produce achannel attention map by extracting frequency statistics.Subsequently, multi-scale spatial attention (MSSA) is ap-plied to extract discriminative boundary features and ag-gregate them from each scale. By effectively leveragingthe sequential attention method to suppress the influenceof noisy channels via MFCA and extracting discriminativefeature maps for boundaries across various scales throughMSSA, the MFMSA blocks dual utilization enables thecomprehensive capture of a broad spectrum of informa-tion from medical images. In more detail through multi-frequency analysis, it extract diverse features across dif- ferent frequency bands while simultaneously performingmulti-scale analysis to capture intricate details and broaderstructural information, ensuring a comprehensive under-standing of the image content.Additionally, we intro-duce a Ensemble Sub-Decoding Module (E-SDM) to pre-vent information loss caused by drastic upsampling duringmulti-task learning with deep supervision. The resultingmodel, MADGNet, which mainly comprises the MFMSAblock and E-SDM, achieved the highest segmentation per-formance in various modalities and clinical settings. Thecontributions of this paper can be summarized as follows: We propose MADGNet for medical image segmenta-tion in various modalities (modality-agnostic) and un-seen clinical settings (domain generalizability) by effi-ciently fusing multi-frequency and multi-scale features. Our novel MFMSA block is an efficient solution forintegrating multi-scale and multi-frequency information,both of which are crucial for generalizing across vari-ous modalities and unseen clinical settings. E-SDM pre-vents information loss caused by drastic upsampling dur-ing multi-task learning with deep supervision in an en-semble manner. We evaluated MADGNet on fifteen different datasets withsix different modalities and demonstrated that our modelachieves the highest segmentation performance on vari-ous datasets. Furthermore, we showed that MADGNetcan be utilized for multi-label medical image segmenta-tion.",
  ". Related Works": "Recently, the significance of multi-scale information in thecontext of semantic segmentation has garnered increasingattention. For instance, DCSAUNet retains the mainfeatures of the input image while combining feature mapswith various receptive fields, resulting in enhanced seg-mentation quality. Simultaneously, several works have demonstrated the effectiveness of enlarg-ing the receptive field to capture multi-scale objects, of-ten in conjunction with attention mechanisms. MS-CNN improves context alignment through attention layersand multi-scale features, whereas DMSANet proposesa lightweight module that integrates local and global fea-tures with spatial and channel attention.In parallel, research efforts have surged to integratemulti-frequency techniques and attention mechanisms, aim-ing to enhance the extraction of local and global contextfor fine to coarse information by implementing various fre-quency transformation methods . In particular,2D DCT is widely used in computer vision to extract thefrequency statistic feature for increasing the representationpower due to its compression ability. For instance, FCANet proposed a generalized SE block based on 2DDCT basis functions. However, current medical image seg- . (a) The overall architecture of the proposed MADGNet mainly comprises MFMSA block and E-SDM (See ). (b) MFMSAblock contains S scale branches (S = 3 in this figure) where the s-th branch input feature map are downsampled into s1 ( = 1 2 in thisfigure). As our MFMSA block considers two dimensions (scale and frequency), MADGNet achieves the highest performance in variousmodalities and other clinical settings. Additionally, since E-SDM predicts a core task from sub-tasks, the final output is more accurate thanwhen processed parallelly. mentation methods tend to focus on multi-scale or multi-frequency aspects, ignoring the potential benefits of com-bining both. To address this limitation, we integrate multi-frequency information with multi-scale features, therebyimproving the models ability to detect subtle variations inlesion characteristics, which is crucial for accurate medicalimage segmentation.Additionally, multi-task learning and deep supervisionare recognized strategies for enhancing the representationpower of models by training various tasks at low levels.These strategies are widely used in medical image seg-mentation, which demands dense predictions . However, when training multiple tasks withdeep supervision, a challenge arises during upsampling of alow-resolution feature map to a high resolution, potentiallyleading to information loss. This effect is particularly pro-nounced when predicting detailed boundaries, negativelyimpacting model training. To address this challenge, wepropose a multi-task learning scheme with deep supervisionin a ensemble manner.",
  ". Multi-Frequency in Multi-Scale AttentionBlock": "Motivation: The human visual system seamlessly integratesmulti-scale and multi-frequency information to accuratelyinterpret the environment . Our approach mirrorsa comprehensive analysis of visual information across di- verse scales and frequencies designed to enhance tasks inthe medical domain, motivated by and the operationalalgorithm of the human visual system. This process encom-passes the wide variation in lesion sizes in medical images,necessitating multi-scale features for precisely segmentingregions such as tumors, polyps, and cells. Moreover, asmedical images exhibit higher frequency variance than scaledue to modality characteristics, facilitating multi-frequencyinformation is vital for crafting an effective medical imagesegmentation model. Propelled by these insights, we pro-pose the innovative Multi-Frequency in Multi-Scale Atten-tion (MFMSA) block, efficiently integrating multi-scale andmulti-frequency information to effectively address a criti-cal aspect often neglected in prior approaches. The overallarchitecture of the MADGNet and MFMSA block is illus-trated on . And, the MFMSA block can be dividedinto three steps: 1) Scale Decomposition, 2) MFCA, and 3)MSSA.Feature Extraction. We use a pre-trained ResNeSt consisting of split-attention residual blocks to extract fea-ture maps from input images due to enhanced feature rep-resentations.We also conducted experiments on variousbackbone (ResNet , Res2Net , Vision Transformer), in Appendix 9.1. Let fi be the feature maps from thei-th stage encoder block for i = 1, 2, 3, 4, 5. As the numberof channels in each stage influences a decoders complexity,we reduce the number of channels into Ce using 2D convo-lution. Furthermore, during decoding to restore the featuresto input image resolutions, we fuse two features from the",
  "Xi = Cat(Conv2D1(f5i), Up2(Yi1)),(1)": "where Conv2Dk(), Cat(, ) and Upm() denote the 2Dconvolution with a kernel size of k, concatenation betweenfeature maps along the channel dimension and a upsamplingwith scale factor 2m1, respectively. And, Y0 = f5. Forconvenience, we assume that Ci = Ce + Ci1 = C, Hi =H, Wi = W.Scale Decomposition.To produce multi-scale featuresfrom input feature map Xi, we assume a total of S branchesoperating on S different scales. For each s-th scale branch(1 s S), we reduce the resolution and channels of in-put feature map Xi with a channel and resolution reductionratio (0, 1), respectively, for computational efficiency.Thus, each input feature map at the s-th scale branch can bewritten as follows:",
  "2s1 , Hmin), and Ws=max( W": "2s1 , Wmin), respectively.Multi-Frequency Channel Attention (MFCA). Recently,the compression ability of 2D DCT by expressing an im-age into a weighted sum of basis images produced by co-sine functions oscillating at different frequencies has gainedattention for feature extraction in the frequency domain. Features at each scale branch can be char-acterized using 2D DCT with basis images D as follows:",
  "d{avg,max,min}W2((W1Zd))) RCs,(4)": "where and denote ReLU and Sigmoid activation func-tions, respectively. Finally, we recalibrate the feature mapXsi using Msi at the s-th scale branch as Xsi = Xsi Msi.Multi-Scale Spatial Attention (MSSA). The channel-recalibrated feature map Xsi is used to determine discrimi-native boundary cues with various scales in the spatial do-main. At this stage, we introduce the two learnable param-eters (si and si ) for each scale branch to control the infor-mation flow between foreground and background, respec-tively, as follows:",
  "Xsi = Conv2D3si( Xsi Fsi) + si ( Xsi Bsi),(5)": "where Fsi = (Conv2D1( Xsi)) is a foreground attentionmap. Accordingly, the background attention map can bederived via elementary-wise subtraction between Fsi and amatrix filled with one, that is, Bsi = 1 Fsi. At that point,we restore the number channel at the s-th scale branch Cs into C. Finally, to aggregate each refined feature from differ-ent scale branches, we apply upsampling to match the reso-lutions. For more stable training, we apply residual connec-tion after the spatially refined feature map as follows:",
  "Yi = Xi + A(X1i , Up2(X2i ), . . . , UpS(XSi )),(6)": "where A() is the feature aggregation function.Subse-quently, Yi is concatenated with feature map from encoderand E-SDM to apply stable deep supervision. As the chan-nel attention is initially applied in the MFCA, the chan-nel information is enriched for representation, reducing theeffect of noisy contained medical images. Subsequently,more discriminative feature maps for boundary cues robustto various scale and noise are produced using the MSSA.This dual process allows our model understand the sub-tle anatomical differences between various modalities andcomplex characteristics of irregular lesions in noisy medi-cal images.Parameter Analysis.To further analyze the MFMSAblock, we compare its efficiency with the most simple vari-ant of our block, a single-frequency in single-scale block. Atheoretical proof of the number of parameters for each stepis reported in Appendix 7.1. Consequently, the ratio of thenumber of parameters to the original scale branch at the s-thscale branch is ps",
  ". Comparison of multi-task learning method between (a)parallel and (b) ensemble manners": "resentation power and preventing gradient vanishing prob-lems; this is achieved through co-training with core task andother L sub-tasks Oi = {Tci, Ts1i , . . . , TsLi } at each i-thdecoder stage . However, low-resolution featuremaps must be upsampled into a high resolution to calculatethe loss function between the ground truth and prediction.This drastic upsampling interferes with the models repre-sentation ability due to information loss for predicting de-tailed boundaries and structures.To solve this problem, we propose a ensemble sub-decoding module (E-SDM), a novel training strategy formulti-task learning with deep supervision. illustratesthe difference between (a) parallel and (b) ensemble man-ners. The main idea is to cascadingly supplement the in-formation loss by incorporating sub-task predictions afterupsampling, thereby improving the core task prediction.Forward Stream.During the forward stream, core andsub-task pseudo predictions {Pci, Ps1i , . . . , PsLi } are pro-duced at the i-th decoder stage as follows:",
  "t{c,s1,...,sL}tLtGt, Up5i(Tti)(10)": "where Gt and Tti are the ground truth and predictions fortask t and i-th decoder, respectively. Additionally, Lt andt are the loss function and ratio for the task t, respectively.For multi-task learning with deep supervision, we de-fined a core task as region R and two sub-tasks as boundaryB and distance map D. The loss function for region predic-tion is defined as LR = LwIoU +Lwbce, where LwIoU and Lwbceare the weighted IoU and bce loss functions, respectively.Additionally, we defined boundary and distance map lossfunction LB and LD as bce and mse loss, respectively.",
  "DSCmIoUDSCmIoUDSCmIoUDSCmIoUDSCmIoUDSCmIoU": "UNet 87.3 (0.8)80.2 (0.7)47.7 (0.6)38.6 (0.6)69.5 (0.3)60.2 (0.2)91.1 (0.2)84.3 (0.3)76.5 (0.8)69.1 (0.9)80.5 (0.3)72.6 (0.4)5.2E-06AttUNet 87.8 (0.1)80.5 (0.1)57.5 (0.2)48.4 (0.2)71.3 (0.4)62.3 (0.6)91.6 (0.1)85.0 (0.1)80.1 (0.6)74.2 (0.5)83.9 (0.1)77.1 (0.1)4.1E-06UNet++ 87.3 (0.2)80.2 (0.1)65.6 (0.7)57.1 (0.8)72.4 (0.1)62.5 (0.2)91.6 (0.1)85.0 (0.1)79.7 (0.2)73.6 (0.4)84.3 (0.3)77.4 (0.2)7.5E-07CENet 89.1 (0.2)82.1 (0.1)76.3 (0.4)69.2 (0.5)79.7 (0.6)71.5 (0.5)91.3 (0.1)84.6 (0.1)89.3 (0.3)83.4 (0.2)89.5 (0.7)83.9 (0.7)1.0E-05TransUNet 87.3 (0.2)81.2 (0.8)75.6 (0.4)68.8 (0.2)75.5 (0.5)68.4 (0.1)91.8 (0.3)85.2 (0.2)87.4 (0.2)82.9 (0.1)86.4 (0.4)81.3 (0.4)9.9E-08FRCUNet 88.9 (0.1)83.1 (0.2)77.3 (0.3)70.4 (0.2)81.2 (0.2)73.3 (0.3)90.8 (0.3)83.8 (0.4)91.8 (0.2)87.0 (0.2)88.8 (0.4)83.5 (0.6)6.6E-02MSRFNet 88.2 (0.2)81.3 (0.2)75.2 (0.4)68.0 (0.4)76.6 (0.7)68.1 (0.7)91.9 (0.1)85.3 (0.1)83.2 (0.9)76.5 (1.1)86.1 (0.5)79.3 (0.4)8.8E-07HiFormer 88.7 (0.5)81.9 (0.5)72.9 (1.4)63.3 (1.5)79.3 (0.2)70.8 (0.1)90.7 (0.2)83.8 (0.4)89.1 (0.6)83.7 (0.6)88.1 (1.0)82.3 (1.2)1.8E-05DCSAUNet 89.0 (0.3)82.0 (0.3)75.3 (0.4)68.2 (0.4)73.7 (0.5)65.0 (0.5)91.1 (0.2)84.4 (0.2)80.5 (1.2)73.7 (1.1)82.6 (0.5)75.2 (0.5)6.2E-07M2SNet 89.2 (0.2)83.4 (0.2)81.7 (0.4)74.7 (0.5)80.4 (0.8)72.5 (0.7)91.6 (0.2)85.1 (0.3)92.8 (0.8)88.2 (0.8)90.2 (0.5)85.1 (0.6)2.0E-05 SFSSNet88.8 (0.3)81.9 (0.2)80.3 (0.8)73.0 (0.7)66.1 (0.6)59.3 (0.8)91.5 (0.2)84.0 (0.2)90.7 (0.4)83.0 (0.7)88.1 (0.6)82.2 (0.7)2.2E-06MFSSNet88.5 (0.2)81.8 (0.2)80.4 (0.7)73.1 (0.4)81.0 (0.1)73.2 (0.2)91.6 (0.1)85.1 (0.2)92.3 (0.5)87.7 (0.5)89.9 (0.6)84.7 (0.7)5.1E-07SFMSNet89.2 (0.3)82.5 (0.3)81.4 (0.3)74.5 (0.3)80.8 (0.4)73.0 (0.3)91.5 (0.2)84.9 (0.4)92.3 (0.3)88.0 (0.3)89.0 (0.6)84.1 (0.5)1.4E-04",
  "DSCmIoUDSCmIoUDSCmIoUDSCmIoUDSCmIoUDSCmIoUDSCmIoU": "UNet 90.3 (0.1) 83.5 (0.1) 47.1 (0.7) 37.7 (0.6) 71.6 (1.0) 61.6 (0.7) 29.2 (5.1)18.9 (3.5)66.1 (2.3) 58.5 (2.1) 56.8 (1.3)49.0 (1.2)41.6 (1.1) 35.4 (1.0) 1.1E-09AttUNet 89.9 (0.2) 82.6 (0.3) 43.7 (0.8) 35.2 (0.8) 77.0 (1.6) 68.0 (1.7) 39.0 (3.1)26.5 (2.4)63.0 (0.3) 57.2 (0.4) 56.8 (1.6)50.0 (1.5)38.4 (0.3) 33.5 (0.1) 6.7E-09UNet++ 88.0 (0.3) 80.1 (0.3) 50.5 (3.8) 40.9 (3.7) 77.3 (0.4) 67.8 (0.3) 25.4 (0.8)15.3 (0.5)64.3 (2.2) 58.4 (2.0) 57.5 (0.4)50.2 (0.4)39.1 (2.4) 34.0 (2.1) 1.0E-05CENet 90.5 (0.1) 83.3 (0.1) 60.1 (0.3) 49.9 (0.3) 86.0 (0.7) 77.2 (0.9) 27.7 (1.5)16.9 (1.0)85.4 (1.6) 78.2 (1.4) 65.9 (1.6)59.2 (0.1)57.0 (3.4) 51.4 (0.5) 4.5E-06TransUNet 89.5 (0.3) 82.1 (0.4) 56.9 (1.0) 48.0 (0.7) 41.4 (9.5) 32.1 (4.2) 15.9 (8.5)9.6 (5.5)85.0 (0.6) 77.3 (0.3) 63.7 (0.1)58.4 (0.3)50.1 (0.5) 44.0 (2.3) 1.6E-06FRCUNet 90.6 (0.1) 83.4 (0.2) 62.9 (1.1) 52.7 (0.9) 86.5 (2.3) 77.2 (2.7) 26.1 (5.6)16.8 (4.3)86.7 (0.7) 79.4 (0.3) 69.1 (1.0)62.6 (0.9)65.1 (1.0) 58.4 (0.5) 2.3E-05MSRFNet 90.5 (0.3) 83.5 (0.3) 58.3 (0.8) 48.4 (0.6) 84.0 (5.5) 75.2 (8.2)9.1 (1.0)5.3 (0.7)72.3 (2.2) 65.4 (2.2) 61.5 (1.0)54.8 (0.8)38.3 (0.6) 33.7 (0.7) 1.0E-07HiFormer 86.9 (1.6) 79.1 (1.8) 54.1 (1.0) 44.5 (0.8) 80.7 (2.9) 71.3 (3.2) 21.9 (8.9)13.2 (5.7)84.7 (1.1) 77.5 (1.1) 67.6 (1.4)60.5 (1.3)56.7 (3.2) 50.1 (3.3) 2.5E-07DCSAUNet 89.0 (0.4) 81.5 (0.3) 52.4 (1.2) 44.0 (0.7) 86.1 (0.5) 76.5 (0.8)4.3 (0.3)2.4 (0.9)68.9 (4.0) 59.8 (3.9) 57.8 (0.4)49.3 (0.4)42.9 (3.0) 36.1 (2.9) 1.3E-07M2SNet 90.7 (0.3) 83.5 (0.5) 68.6 (0.1) 58.9 (0.2) 79.4 (0.7) 69.3 (0.6) 36.3 (0.9)23.1 (0.8)89.9 (0.2) 83.2 (0.3) 75.8 (0.7)68.5 (0.5)74.9 (1.3) 67.8 (1.4) 4.9E-02 SFSSNet89.8 (0.2) 82.2 (0.4) 65.1 (1.6) 55.5 (1.3) 59.1 (0.3) 49.3 (0.7) 21.5 (7.2)14.3 (5.0)81.7 (0.3) 74.7 (0.4) 65.6 (0.4)58.4 (0.5)56.4 (0.7) 49.4 (0.4) 2.0E-07MFSSNet90.2 (0.8) 83.3 (0.9) 67.6 (0.5) 57.9 (0.3) 66.1 (0.8) 59.3 (0.2) 30.1 (7.5)20.5 (5.5)83.3 (1.4) 76.1 (1.2) 66.0 (0.7)59.1 (0.8)59.3 (0.2) 52.6 (0.6) 3.9E-04SFMSNet90.8 (0.3) 83.9 (0.5) 67.7 (1.1) 58.0 (1.3) 84.5 (0.2) 74.3 (0.1) 28.1 (9.9)18.2 (7.1)84.2 (1.2) 78.1 (1.0) 75.9 (0.8)68.3 (0.8)68.9 (0.3) 62.7 (0.4) 7.9E-03",
  "MADGNet91.3 (0.1) 84.6 (0.1) 72.2 (0.3) 62.6 (0.3) 88.4 (1.0) 79.9 (1.5) 46.7 (4.3)32.0 (2.9)87.4 (0.4) 79.9 (0.4) 77.5 (1.1)69.7 (1.2)77.0 (0.3) 69.7 (0.5)-": ". Segmentation results on five different modalities with unseen clinical settings. We also provide one tailed t-Test results (P-value)compared to our method and other methods. () denotes the standard deviations of multiple experiment results. Microscopy , Colonoscopy , and Fundus Imag-ing , to validate the modality-agnostic ability. More-over, we evaluated the domain generalizability on eightexternal datasets with different modalities, including Der-moscopy , Radiology , Ultrasound , Microscopy, Colonoscopy , and Fundus Imaging .To evaluate the performance of each model, we selected twometrics, DSC and mIoU, which are widely used in medicalimage segmentation. We compared the MADGNet (Ours)with ten medical image segmentation models, includingUNet , Attention UNet (AttUNet ), UNet++ ,CENet , TransUNet , FRCUNet , MSRFNet ,HiFormer , DCSAUNet , and M2SNet . Wereport the mean performance of three trials for all results.Red and Blue are the first and second best performance re-sults, respectively. Due to the page limit, we present thedetailed dataset description and results using other metrics in Appendix 6 and 12.",
  "with an initial learning rate of 104 using the Adam opti-mizer and reduced the parameters of each model to 106": "using a cosine annealing learning rate scheduler . Weoptimized each model with a batch size of 16 and epochs of50, 100, 100, 100, and 200 for Colonoscopy, Dermoscopy,Microscopy, Ultrasound, and Radiology modalities, respec-tively. During training, we used horizontal/vertical flipping,with a probability of 50%, and rotation between 5 and 5 based on the multi-scale training strategy, which is widelyused in medical image segmentation . At thisstage, because images in each dataset have different resolu-tions, all images were resized to 352 352.",
  "Hyperparameters of MADGNet.Key hyperparametersfor MADGNet on all datasets were set to Ce = 64, Cmin =32, Hmin = Wmin = 8, = 1": "2, r = 16 for efficiency andS = 3 and K = 16 for combining multi-scale and multi-frequency features. The aggregation function A() aver-aged refined feature maps from each scale branch. To re-duce the sensitivity of hyperparameters, the loss ratio be-tween tasks was fixed as t = 1 for all tasks t {R, D, B}.In this paper, the core task indicates the region prediction,while sub-tasks indicate the distance map and boundary pre-",
  ". Comparison with State-of-the-art models": "Binary Segmentation.As listed in Tab.1 and 2,MADGNet achieved the highest segmentation performanceon various modalities and clinical settings. In particular,compared to M2SNet, which acheived the second high-est segmentation performance in most modalities in Tab.1, MADGNet improved the DSC and mIoU by 1.1% and1.0% on average, respectively. Additionally, when com-pared to FRCUNet, which uses multi-frequency atttention,MADGNet improved the both DSC and mIoU by 2.0% onaverage.To evaluate the domain generalizability of themodel trained in each modality, we tested each model onexternal dataset from unseen clinical settings. As a result,MADGNet improved its performance by a higher marginthan M2SNet, except for CVC-300. In addition, when com-pared FRCUNet and MADGNet on BUSI, the performancegap was tight. Nevertheless, MADGNet exhibits signifi-cant improvement of 1.9% and 2.7% on DSC and mIoUon STU, respectively.These results indicate that othermodels, which do not consider scale and frequency dimen-sions simultaneously, cannot comprehend intricate anatom-ical knowledge. indicates that MADGNet containsalmost 31M parameters with reasonable inference speed(0.024 sec/image), which has obvious advantages in termsof computational efficiency. illustrates the qualitative results on various modal-ities. The UNet relies solely on skip connection and pro-gressive decoding; therefore, it produces noisy predictions.Such unreliable predictions are also observed in other mod-els, such as AttUNet and UNet++, which do not considermulti-scale and multi-frequency features.And the othertwo transformer-based models, TransUNet and HiFormer,cannot segment in low-contrast Dermoscopy due to lackof inductive bias. As CENet, MSRFNet, DCSAUNet, andM2SNet utilize multi-scale information, they depict de-tailed boundary prediction; however, they struggle with se-vere noise in images from Radiology, Colonoscopy, and Ul-trasound. Additionally, FRCUNet depicts detailed bound-",
  "ODOCODOC": "UNet 79.9 (0.9)79.2 (0.8)62.2 (1.1)38.6 (1.2)AttUNet 80.8 (0.1)79.4 (0.2)72.6 (2.6)73.8 (1.4)UNet++ 80.6 (0.1)79.5 (0.2)76.1 (1.4)71.3 (1.4)CENet 80.4 (0.1)74.1 (0.4)87.6 (0.6)78.8 (1.2)TransUNet 81.3 (0.2)40.7 (0.2)76.0 (3.2)38.0 (3.1)FRCUNet 84.1 (1.3)45.2 (0.1)87.2 (1.1)44.4 (0.1)MSRFNet 83.6 (0.6)81.3 (0.7)71.3 (0.8)32.3 (0.9)HiFormer 80.5 (0.1)75.3 (0.4)79.9 (1.3)68.6 (2.5)DCSAUNet 81.2 (0.2)59.6 (0.5)53.3 (3.7)28.3 (3.6)M2SNet 81.0 (1.1)60.1 (1.2)84.5 (3.3)69.4 (1.2)",
  ". Segmentation results between methods on two FundusImage segmentation datasets (REFUGE and Drishti-GS )": "ary on Ultrasound, which contains high-frequency images;but, has difficulties in predicting in Radiology and Der-moscopy. Despite this severe noise and lesions of varioussizes, MADGNet successfully depicts detailed boundariesfor all modalities due to the dual utilization of multi-scaleand multi-frequency information.Multi-label Segmentation. For medical image analysis,certain medical image segmentation datasets contain multi-label objects that need to be segmented.To satisfy thisdemand, we evaluated all models on two Fundus Imagingdatasets, including REFUGE (seen) and Drishti-GS (unseen), with different labels, Optic Disk (OD) andOptic Cup (OC). Except for data augmentation, we trainedall models for 200 epochs with the same setting presented in.2. We cropped 512 512 ROIs centering OD andapplied data augmentations, which are the same settings asthose reported in the literature .As listed in Tab. 3, compared to CENet, MADGNet im-proved the average DSC of OD and OC by 2.9% and 1.9%,respectively. This result indicates that our model can begeneralized to a multi-label segmentation task compared toother methods. The last row in presents the qualita-tive results for each method.",
  ". Ablation Study on MADGNet": "To demonstrate the effectiveness of MADGNet, we con-ducted ablation studies on the MSMFA block and E-SDM.Ablation Study on MFMSA Block.To demonstratethat multi-scale and multi-frequency features are crucialin building the medical image segmentation model, wecompared three variant models of MADGNet; Single-Frequency in Single-Scale Network (SFSSNet), Multi-Frequency in Single-Scale Network (MFSSNet), Single-Frequency in Multi-Scale Network (SFMSNet). These vari-ants differs in (S, K); SFSSNet, MFSSNet, and SFMSNetwere set to (1, 1), (1, 16), and (3, 1), respectively. As listedin Tab. 1 and 2, as SFSSNet suffers from capturing the var-ious sizes and detailed features in medical images, this ap-proach has demonstrated unsatisfactory results in multipleclinical settings and modalities. The performance gain is . Qualitative comparison of other methods and MADGNet. (a) Input images. (b) UNet . (c) AttUNet . (d) UNet++ .(e) CENet . (f) TransUNet . (g) FRCUNet , (h) MSRFNet . (i) HiFormer . (j) DCSAUNet . (k) M2SNet . (l)MADGNet (Ours). Green and Red lines denote the boundaries of the ground truth and prediction, respectively.",
  ". Feature visualization of SFSS, MFSS, SFMS, MFMS": "achieved by utilizing multi-scale and multi-frequency fea-tures when these features are applied separately. This exper-imental evidence demonstrates the critical significance ofboth features in medical image segmentation, as illustratedin . Moreover, we observed that using both multi-scale and multi-frequency significantly improves the featurerepresentation power and extracts enhanced boundary fea-tures, as illustrated in . Consequently, MADGNetsimultaneously employs both features, resulting in signif-icantly better performance across various modalities andclinical settings due to the enriched feature representationrobust to noise and scale.",
  "EnsembleR D B92.087.380.973.3": ". Ablation study of E-SDM on the seen () and un-seen () datasets on Colonoscopy. DS denotes Deep Su-pervision. R, D, B are region, distance map, and boundary task,respectively. and denote E-SDM without and with backwardstream, respectively. to demonstrate the effectiveness of E-SDM on Colonoscopyimages. As listed in Tab. 4, our experimentation with multi-task learning and deep supervision in a parallel manner re-vealed a potential issue of performance degradation. Thisissue can be addressed by tackling the information loss dueto drastic upsampling during deep supervision training. Forthis reason, when we trained a model in an ensemble man-ner, we observed that E-SDM had higher DSC of 1.2% and7.2% on seen and unseen datasets, comparing two decod-ing flows (Parallel and Ensemble) with deep supervision,respectively. By using the ensemble method, we can over-come these challenges and obtain better results by lever-aging an ensemble approach that combines the predictions .Qualitative results between ensemble and parallelmanners. (a) Input Image, (b) and (c) Predictions from Stage1(16 Up) and Stage4 (2 Up). First and second rows in (b) and(c) are predictions with ensemble (Ours) and parallel manners. from different tasks. This process enables us to identifycomplex boundaries B and structures D of the lesion R, re-sulting in higher performance compared to the parallel ap-proach. illustrates the qualitative results of ensembleand parallel manners. E-SDM predict more reliably than ina parallel manner, even though it exhibits a high upsamplingrate (16). The ensemble method also depicts the detailedboundary and distance map predictions than in a parallelmanner. Finally, we examined which forward and backwardstreams had a larger impact on segmentation performance.Our study showed that the backward stream is crucial formaintaining the overall output quality as it compensates forthe information loss caused by upsampling. While the for-ward stream only identifies areas requiring emphasis, thebackward stream preserves critical information.",
  ". Conclusion": "Based on the outcomes of extensive experiments on vari-ous modalities and clinical settings, we can summarize theeffectiveness of MADGNet into three key aspects: 1) Forthe design of medical image segmentation, multi-scale andmulti-frequency features prove pivotal. 2) MFCAs adept-ness at extracting discriminative features from noisy med-ical image feature maps translates into the acquisition ofmore sophisticated boundary cues through the MSSA. 3)E-SDM compensates for information loss caused by dras-tic upsampling when performing multi-task learning withdeep supervision through an ensemble of tasks. Further-more, our method demonstrated outstanding performancein segmenting OD and OC not only in binary classifica-tion but also in multi-label segmentation. In conclusion, wepropose a novel medical image segmentation model calledMADGNet, which can be utilized in various modalitiesand clinical settings. It includes two key components: theMFMSA block and E-SDM, which extract distinctive fea-tures and compensate for information loss during multi-tasklearning with deep supervision. Through rigorous experi-ment, we discovered that MADGNet is a highly effectivemodel that surpasses other state-of-the-art options regard-ing segmentation performance. Furthermore, we will focus",
  "Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled,and Aly Fahmy. Dataset of breast ultrasound images. Datain brief, 28:104863, 2020. 5, 6, 1, 3, 4, 8, 11": "Reza Azad, Afshin Bozorgpour, Maryam Asadi-Aghbolaghi,Dorit Merhof, and Sergio Escalera.Deep frequency re-calibration u-net for medical image segmentation. In Pro-ceedings of the IEEE/CVF International Conference onComputer Vision, pages 32743283, 2021. 2, 6, 7, 8, 1, 5, 9,10, 11 JorgeBernal,FJavierSanchez,GloriaFernandez-Esparrach, Debora Gil, Cristina Rodrguez, and FernandoVilarino.Wm-dova maps for accurate polyp highlightingin colonoscopy: Validation vs. saliency maps from physi-cians. Computerized medical imaging and graphics, 43:99111, 2015. 6, 8, 1, 3, 4, 10, 11 Juan C Caicedo, Allen Goodman, Kyle W Karhohs, Beth ACimini, Jeanelle Ackerman, Marzieh Haghighi, CherKengHeng, Tim Becker, Minh Doan, Claire McQuin, et al. Nu-cleus segmentation across imaging experiments: the 2018data science bowl.Nature methods, 16(12):12471253,2019. 6, 1, 3, 4, 9, 11 Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, EhsanAdeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.Transunet: Transformers make strong encoders for medi-cal image segmentation. arXiv preprint arXiv:2102.04306,2021. 6, 7, 8, 1, 5, 9, 10, 11 Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,Kevin Murphy, and Alan L Yuille. Semantic image segmen-tation with deep convolutional nets and fully connected crfs.arXiv preprint arXiv:1412.7062, 2014. 2 Xu Chen, Bryan M Williams, Srinivasa R Vallabhaneni,Gabriela Czanner, Rachel Williams, and Yalin Zheng. Learn-ing active contour models for medical image segmentation.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1163211640, 2019. 1 Alan S Coates, Eric P Winer, Aron Goldhirsch, Richard DGelber,MichaelGnant,MPiccart-Gebhart,BeatThurlimann, H-J Senn, Panel Members, Fabrice Andre, et al.Tailoring therapiesimproving the management of earlybreast cancer: St gallen international expert consensus onthe primary therapy of early breast cancer 2015. Annals ofoncology, 26(8):15331546, 2015. 1 Runmin Cong, Yumo Zhang, Ning Yang, Haisheng Li, XueqiZhang, Ruochen Li, Zewen Chen, Yao Zhao, and SamKwong. Boundary guided semantic learning for real-timecovid-19 lung infection segmentation system. IEEE Trans-actions on Consumer Electronics, 68(4):376386, 2022. 3 Tuan Le Dinh, Seong-Geun Kwon, Suk-Hwan Lee, andKi-Ryong Kwon.Breast tumor cell nuclei segmentationin histopathology images using efficientunet++ and multi-organ transfer learning. Journal of Korea Multimedia So-ciety, 24(8):10001011, 2021. 6, 1, 3, 4, 9, 11 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 3 Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and AliBorji. Structure-measure: A new way to evaluate foregroundmaps. In Proceedings of the IEEE international conferenceon computer vision, pages 45484557, 2017. 6",
  "Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming Cheng, and Ali Borji.Enhanced-alignment mea-sure for binary foreground map evaluation. arXiv preprintarXiv:1805.10421, 2018. 6": "Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, HuazhuFu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverseattention network for polyp segmentation. In Internationalconference on medical image computing and computer-assisted intervention, pages 263273. Springer, 2020. 3, 5,6 Deng-Ping Fan, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen,Huazhu Fu, Jianbing Shen, and Ling Shao. Inf-net: Auto-matic covid-19 lung infection segmentation from ct images.IEEE transactions on medical imaging, 39(8):26262637,2020. 3, 5 Shuanglang Feng, Heming Zhao, Fei Shi, Xuena Cheng,Meng Wang, Yuhui Ma, Dehui Xiang, Weifang Zhu, andXinjian Chen. Cpfnet: Context pyramid fusion network formedical image segmentation. IEEE transactions on medicalimaging, 39(10):30083018, 2020. 2",
  "A-Rom Gu, Ju-Hyeon Nam, and Sang-Chul Lee. Fbi-net:Frequency-based image forgery localization via multitasklearning with self-attention. IEEE Access, 10:6275162762,2022. 4": "Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang Zhou, Huay-ing Hao, Yitian Zhao, Tianyang Zhang, Shenghua Gao, andJiang Liu. Ce-net: Context encoder network for 2d medicalimage segmentation. IEEE transactions on medical imaging,38:22812292, 2019. 2, 6, 7, 8, 1, 5, 9, 10, 11 David Gutman, Noel CF Codella, Emre Celebi, Brian Helba,Michael Marchetti, Nabin Mishra, and Allan Halpern. Skinlesion analysis toward melanoma detection: A challenge atthe international symposium on biomedical imaging (isbi)2016, hosted by the international skin imaging collaboration(isic). arXiv preprint arXiv:1605.01397, 2016. 5, 6, 1, 3, 4",
  "Robert M Haralick, Stanley R Sternberg, and XinhuaZhuang.Image analysis using mathematical morphology.IEEE transactions on pattern analysis and machine intelli-gence, (4):532550, 1987. 1": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3 Moein Heidari, Amirhossein Kazerouni, Milad Soltany,Reza Azad, Ehsan Khodapanah Aghdam, Julien Cohen-Adad, and Dorit Merhof. Hiformer: Hierarchical multi-scalerepresentations using transformers for medical image seg-mentation. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, pages 62026212,2023. 6, 7, 8, 1, 5, 9, 10, 11",
  "Yuzhu Ji, Haijun Zhang, and QM Jonathan Wu. Salient ob-ject detection via multi-scale attention cnn. Neurocomputing,322:130140, 2018. 2": "Ma Jun, Ge Cheng, Wang Yixin, An Xingle, Gao Jiantao,Yu Ziqi, Zhang Minqing, Liu Xin, Deng Xueyuan, CaoShucheng, Wei Hao, Mei Sen, Yang Xiaoyu, Nie Ziwei, LiChen, Tian Lu, Zhu Yuntao, Zhu Qiongjie, Dong Guoqiang,and He Jian. COVID-19 CT Lung and Infection Segmenta-tion Dataset. 2020. 5, 6, 1, 3, 4, 7",
  "Ran Margolin, Lihi Zelnik-Manor, and Ayellet Tal. How toevaluate foreground maps? In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages248255, 2014. 6": "Teresa Mendonca, Pedro M Ferreira, Jorge S Marques,Andre RS Marcal, and Jorge Rozeira. Ph 2-a dermoscopicimage database for research and benchmarking.In 201335th annual international conference of the IEEE engineer-ing in medicine and biology society (EMBC), pages 54375440. IEEE, 2013. 6, 1, 3, 4, 5 Ju-HyeonNam,Seo-HyeongPark,NurSurizaSyazwany,Yerim Jung,Yu-Han Im,and Sang-ChulLee.M3fpolypsegnet:Segmentation network withmulti-frequency feature fusion for polyp localization incolonoscopy images. In 2023 IEEE International Confer-ence on Image Processing (ICIP), pages 15301534. IEEE,2023. 3",
  "Shinya Nishida, Timothy Ledgeway, and Mark Edwards.Dual multiple-scale processing for motion in the human vi-sual system. Vision research, 37(19):26852698, 1997. 3": "Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee,Mattias Heinrich, Kazunari Misawa, Kensaku Mori, StevenMcDonagh, Nils Y Hammerla, Bernhard Kainz, et al. At-tention u-net: Learning where to look for the pancreas. arxiv2018. arXiv preprint arXiv:1804.03999, 1804. 2, 6, 7, 8, 1,5, 9, 10, 11 Jose Ignacio Orlando, Huazhu Fu, Joao Barbosa Breda,Karel Van Keer, Deepti R Bathula, Andres Diaz-Pinto,Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim, JoonHo Lee,et al. Refuge challenge: A unified framework for evaluat-ing automated methods for glaucoma assessment from fun-dus photographs. Medical image analysis, 59:101570, 2020.6, 7, 1",
  "Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In SoKweon. Bam: Bottleneck attention module. arXiv preprintarXiv:1807.06514, 2018. 2": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, Zem-ing Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-son, Andreas Kopf, Edward Yang, Zachary DeVito, Mar-tin Raison, Alykhan Tejani, Sasank Chilamkurthy, BenoitSteiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:An imperative style, high-performance deep learning library.In Advances in Neural Information Processing Systems 32,pages 80248035. Curran Associates, Inc., 2019. 6",
  "Zequn Qin, Pengyi Zhang, Fei Wu, and Xi Li.Fcanet:Frequency channel attention networks.In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 783792, 2021. 2, 4": "Daniel Riccio, Nadia Brancati, Maria Frucci, and DiegoGragnaniello. A new unsupervised approach for segment-ing and counting cells in high-throughput microscopy imagesets. IEEE journal of biomedical and health informatics, 23(1):437448, 2018. 1 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III18, pages 234241. Springer, 2015. 2, 6, 7, 8, 1, 5, 9, 10, 11",
  "Mufan Sang and John HL Hansen. Multi-frequency informa-tion enhanced channel attention module for speaker repre-sentation learning. arXiv preprint arXiv:2207.04540, 2022.4": "Juan Silva, Aymeric Histace, Olivier Romain, Xavier Dray,and Bertrand Granado.Toward embedded detection ofpolyps in wce images for early diagnosis of colorectal can-cer. International journal of computer assisted radiology andsurgery, 9:283293, 2014. 6, 8, 1, 3, 4, 10, 11 Jayanthi Sivaswamy, S Krishnadas, Arunava Chakravarty, GJoshi, A Syed Tabish, et al. A comprehensive retinal imagedataset for the assessment of glaucoma from the optic nervehead analysis. JSM Biomedical Imaging Data Papers, 2(1):1004, 2015. 6, 7, 1 Abhishek Srivastava, Debesh Jha, Sukalpa Chanda, Uma-pada Pal, Havard D Johansen, Dag Johansen, Michael ARiegler, Sharib Ali, and Pal Halvorsen. Msrf-net: a multi-scale residual fusion network for biomedical image segmen-tation. IEEE Journal of Biomedical and Health Informatics,26(5):22522263, 2021. 2, 6, 7, 8, 1, 5, 9, 10, 11 Nima Tajbakhsh, Suryakanth R Gurudu, and JianmingLiang. Automated polyp detection in colonoscopy videosusing shape and context information. IEEE transactions onmedical imaging, 35(2):630644, 2015. 6, 8, 1, 3, 4, 10, 11 Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet:Scalable and efficient object detection. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1078110790, 2020. 2",
  "Hamid R Tizhoosh. Image thresholding using type ii fuzzysets. Pattern recognition, 38(12):23632372, 2005. 1": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 2 David Vazquez, Jorge Bernal, F Javier Sanchez, GloriaFernandez-Esparrach, Antonio M Lopez, Adriana Romero,Michal Drozdzal, Aaron Courville, et al.A benchmarkfor endoluminal scene segmentation of colonoscopy images.Journal of healthcare engineering, 2017, 2017. 6, 8, 1, 3, 4,10, 11 Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, ChengLi, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.Residual attention network for image classification. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 31563164, 2017. 2 Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-WingFu, and Pheng-Ann Heng. Boundary and entropy-driven ad-versarial learning for fundus image segmentation. In Medi-cal Image Computing and Computer Assisted InterventionMICCAI 2019: 22nd International Conference, Shenzhen,China, October 1317, 2019, Proceedings, Part I 22, pages102110. Springer, 2019. 7",
  "et al. Xu, Kai. Learning in the frequency domain. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, 2020. 4": "Qing Xu, Zhicheng Ma, HE Na, and Wenting Duan. Dcsau-net: A deeper and more compact split-attention u-net formedical image segmentation.Computers in Biology andMedicine, 154:106626, 2023. 2, 6, 7, 8, 1, 5, 9, 10, 11 Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and KuiyuanYang. Denseaspp for semantic segmentation in street scenes.In Proceedings of the IEEE conference on computer visionand pattern recognition, pages 36843692, 2018. 2 Tongda Yang, Weiming Wang, Gary Cheng, Mingqiang Wei,Haoran Xie, and Fu Lee Wang. Fddl-net: frequency domaindecomposition learning for speckle reduction in ultrasoundimages. Multimedia Tools and Applications, 81(29):4276942781, 2022. 2 Michael Yeung, Evis Sala, Carola-Bibiane Schonlieb, andLeonardo Rundo. Focus u-net: A novel dual attention-gatedcnn for polyp segmentation during colonoscopy. Computersin biology and medicine, 137:104815, 2021. 2 Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller,R Manmatha, et al. Resnest: Split-attention networks. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 27362746, 2022. 3 Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu.Auto-matic polyp segmentation via multi-scale subtraction net-work. In Medical Image Computing and Computer AssistedInterventionMICCAI 2021: 24th International Conference,Strasbourg, France, September 27October 1, 2021, Pro-ceedings, Part I 24, pages 120130. Springer, 2021. 6 Xiaoqi Zhao, Hongpeng Jia, Youwei Pang, Long Lv, FengTian, Lihe Zhang, Weibing Sun, and Huchuan Lu. M2SNet:Multi-scale in multi-scale subtraction network for medi-cal image segmentation. arXiv preprint arXiv:2303.10894,2023. 5, 6, 7, 8, 1, 9, 10, 11 Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, NimaTajbakhsh, and Jianming Liang. Unet++: A nested u-net ar-chitecture for medical image segmentation. In Deep Learn-ing in Medical Image Analysis and Multimodal Learningfor Clinical Decision Support: 4th International Workshop,DLMIA 2018, and 8th International Workshop, ML-CDS2018, Held in Conjunction with MICCAI 2018, Granada,Spain, September 20, 2018, Proceedings 4, pages 311.Springer, 2018. 2, 3, 6, 7, 8, 1, 5, 9, 10, 11 Zhemin Zhuang, Nan Li, Alex Noel Joseph Raj, Vijayalak-shmi GV Mahesh, and Shunmin Qiu. An rdau-net model forlesion segmentation in breast ultrasound images. PloS one,14(8):e0221535, 2019. 6, 1, 3, 4, 8, 11",
  ". Details of the medical segmentation unseen clinical set-tings used in our experiments": "Breast Ultrasound Segmentation: The BUSI com-prises 780 images from 600 female patients, including133 normal cases, 437 benign cases, and 210 malignanttumors.On the other hand, the STU includesonly 42 breast ultrasound images collected by ShantouUniversity. Due to the limited number of images in theSTU, it is used only to evaluate the generalizability ofeach model across different datasets. Skin Lesion Segmentation: The ISIC 2018 comprises2,594 images with various sizes. We randomly selectedtrain, validation, and test images with 1,868, 465, and261, respectively. And, we used PH2 to evaluatethe domain generalizability of each model.Note thatISIC2018 and PH2 are seen, and unseen clinicalsettings, respectively. COVID19 Lung Infection Segmentation:COVID19-1 comprises 1,277 high-quality CT images.Werandomly selected train, validation, and test imageswith 643, 251, and 383, respectively.And, we usedCOVID19-2 to evaluate the domain generalizabilityof each model. Note that COVID19-2 is used for onlytesting. Cell Segmentation: The 2018 Data Science Bowl dataset comprises 670 microscopy images.The datasetconsisted of training, validate, and test images with 483,120, and 67, respectively. We also used MonuSeg2018 for evaluating the domain generalizability of eachmodel. Note that MonuSeg2018 is used for onlytesting. Polyp Segmentation: Colorectal cancer is the third mostprevalent cancer globally and the second most commoncause of death.It typically originates as small, non-cancerous (benign) clusters of cells known as polyps,which develop inside the colon. To evaluate the proposedmodel, we have used five benchmark datasets, namelyCVC-ColonDB , ETIS , Kvasir , CVC-300, and CVC-ClinicDB . The same training set asthe latest image polyp segmentation method has beenadopted, consisting of 900 samples from Kvasir and550 samples from CVC-ClinicDB for training.Theremaining images and the other three datasets are usedfor only testing. Fundus Image Segmentation: To evaluate our method onmulti-label segmentation, we utilize the training part ofthe REFUGE challenge dataset as the training (280)and testing (80) dataset, and the public Drishti-GS dataset as the testing (50) dataset.",
  ". Frequency selection strategies (Top, Bot, Low) . (uk, vk) denotes the frequency indices according to frequency selectionstrategy": "convolution with a kernel size of 3 and dilation size of s.For instance, in . (b), the convolution with kernel sizeof 5 and 7 in second and third scale branch are replaced intothe dilated convolution with a kernel size of 3 and dilationsize of 2 and 3, respectively. This replacement can achievea parameter reduction of 9/(2s + 1)2 in each scale branch.Suppose that the number of channel at i-th MFMSA block isC. Then, by channel reduction ratio (0, 1), the numberof parameters for each scale branch is 9 C2s1.",
  ". Ensemble Sub-Decoding Module for Multi-label Segmentation": "In this section, we show that E-SDM can be utilized to anysegmentation dataset with M multi-label.Forward Stream.During the forward stream, core andsub-task pseudo predictions {Pc,mi, Ps1,mi, . . . , PsL,mi} foreach label m {1, 2, . . . , M} are produced at i-th decoderstage as follows: Pc,mi= Conv2D1(Yi)Psl,mi= Conv2D1(Yi (Psl1,mi)) for l = 1, . . . , L(13)where Ps0,mi= Pc,mi. This stream enables the follow-ing sub-task prediction cascadingly focus on the region byspatial attention starting from core pseudo prediction Pc,mi.Backward Stream. After producing L-th sub-task pseudoprediction PsL,mi, to produce final core task predictionTc,mifor m-th label, we apply backward stream as follows:",
  ". Quantitative results for each Seen ()and Unseen () datasets according to thenumber of scale S. We presents the mean performance for eachdomain": "Inthissection,wepresenttheperformanceofMADGNet according to the number of scales S{1, 2, 3, 4, 5, 6} with F = 16 in Tab. 9 and . Wereport the mean performance of seen and unseen datasets.Number of Frequency branch K.Inthissection,wepresenttheperformanceofMADGNet according to the number of frequencies F {1, 2, 4, 8, 16, 32} with S = 3 in Tab. 10 and . We",
  ". Technical Innovation, Design Principle andInterpretability of MADGNet": "Motivated by papers , to extract discriminative fea-tures in both the frequency and spatial domains, we intro-duced dual attention modules with multiple statistic infor-mation of frequency (MFCA) and two learnable informa-tion flow parameters in multi-scale (MSSA). The causal ef-fect of MFMSA block is interpreted as follows: 1) MFCAemphasizes the salient features while reducing the influ-ence of noisy features by focusing on the frequency of in-terest, characterized by high variance of frequency in med-ical domain (). 2) MSSA captures more reliable dis-criminative boundary cues () for lesions of various sizes by combining foreground and background attentionwith multi-scale attention and information flow parameters.Our approach distinguishes itself by successfully integrat-ing both attentions with dilated convolution and downsam-pling, a pioneering endeavor in the medical domain.",
  ". More Qualtative and Quantitative Results": "In this section, we provide the quantitative results with var-ious metrics in Tab. 12, 13, 14, 15, and 16. We report themean performance of three trials for all results. () denotesa standard deviation of three trials. Red and Blue are thefirst and second best performance results, respectively. Wealso present more various qualitative results on datasets in, 13, 14, 15, and 16.",
  "DSC mIoU F w S EmaxMAE": "UNet 41.6 (1.1)35.4 (1.0)39.5 (1.0)67.2 (0.6)61.7 (0.2)2.7 (0.2)AttUNet 38.4 (0.3)33.5 (0.1)37.6 (0.4)65.4 (0.2)59.7 (1.2)2.6 (0.1)UNet++ 39.1 (2.4)34.0 (2.1)38.3 (2.4)65.8 (1.0)59.3 (1.9)2.7 (0.1)CENet 57.0 (3.4)51.4 (0.5)56.0 (0.4)74.9 (0.1)73.7 (0.4)2.2 (0.2)TransUNet 50.1 (0.5)44.0 (2.3)48.8 (1.8)70.7 (1.3)68.7 (2.0)2.6 (0.1)FRCUNet 65.1 (1.0)58.4 (0.5)62.9 (0.6)78.7 (0.4)81.0 (1.3)2.2 (0.3)MSRFNet 38.3 (0.6)33.7 (0.7)36.9 (0.6)66.0 (0.2)58.4 (0.9)3.6 (0.5)HiFormer 56.7 (3.2)50.1 (3.3)55.2 (3.0)74.1 (1.7)74.7 (2.5)1.8 (0.1)DCSAUNet 42.9 (3.0)36.1 (2.9)40.5 (3.5)67.9 (1.4)69.3 (2.0)4.1 (0.9)M2SNet 74.9 (1.3)67.8 (1.4)71.2 (1.6)84.6 (0.1)87.2 (0.7)1.7 (0.3)",
  ". Segmentation results on COVID19 Infection Segmen-tation (Radiology) . We train each model on COVID19-1 train dataset and evaluate on COVID19-1 and COVID19-2 test datasets": ". Qualitative comparison of other methods and MADGNet on Skin Lesion Segmentation (Dermoscopy) . (a) Inputimages, (b) UNet . (c) AttUNet , (d) UNet++ , (e) CENet , (f) TransUNet , (g) FRCUNet , (h) MSRFNet , (i)HiFormer , (j) DCSAUNet , (k) M2SNet , (l) MADGNet (Ours). Green and Red lines denote the boundaries of the groundtruth and prediction, respectively. . Qualitative comparison of other methods and MADGNet on COVID19 Infection Segmentation (Radiology) . (a) Inputimages, (b) UNet . (c) AttUNet , (d) UNet++ , (e) CENet , (f) TransUNet , (g) FRCUNet , (h) MSRFNet , (i)HiFormer , (j) DCSAUNet , (k) M2SNet , (l) MADGNet (Ours). Green and Red lines denote the boundaries of the groundtruth and prediction, respectively. . Qualitative comparison of other methods and MADGNet on Breast Tumor Segmentation (Ultrasound) . (a) Inputimages, (b) UNet . (c) AttUNet , (d) UNet++ , (e) CENet , (f) TransUNet , (g) FRCUNet , (h) MSRFNet , (i)HiFormer , (j) DCSAUNet , (k) M2SNet , (l) MADGNet (Ours). Green and Red lines denote the boundaries of the groundtruth and prediction, respectively. . Qualitative comparison of other methods and MADGNet on Cell Segmentation (Microscopy) . (a) Input images, (b)UNet . (c) AttUNet , (d) UNet++ , (e) CENet , (f) TransUNet , (g) FRCUNet , (h) MSRFNet , (i) HiFormer, (j) DCSAUNet , (k) M2SNet , (l) MADGNet (Ours). Green and Red lines denote the boundaries of the ground truth andprediction, respectively. . Qualitative comparison of other methods and MADGNet on Polyp Segmentation (Colonoscopy) . (a) Inputimages, (b) UNet . (c) AttUNet , (d) UNet++ , (e) CENet , (f) TransUNet , (g) FRCUNet , (h) MSRFNet , (i)HiFormer , (j) DCSAUNet , (k) M2SNet , (l) MADGNet (Ours). Green and Red lines denote the boundaries of the groundtruth and prediction, respectively.",
  "MethodCVC-ClinicDB + Kvasir-SEG CVC-ClinicDB DSC mIoU F w S EmaxMAE": "UNet 76.5 (0.8)69.1 (0.9)75.1 (0.8)83.0 (0.4)86.4 (0.6)2.7 (0.0)AttUNet 80.1 (0.6)74.2 (0.5)79.8 (0.7)85.1 (0.4)88.5 (0.5)2.1 (0.1)UNet++ 79.7 (0.2)73.6 (0.4)79.4 (0.1)85.1 (0.2)88.3 (0.5)2.2 (0.0)CENet 89.3 (0.3)84.0 (0.2)89.1 (0.2)89.8 (0.2)96.0 (0.6)1.1 (0.0)TransUNet 87.4 (0.2)82.9 (0.1)87.2 (0.1)88.5 (0.2)95.2 (0.1)1.3 (0.0)FRCUNet 91.8 (0.2)87.0 (0.2)91.3 (0.3)91.1 (0.1)97.1 (0.3)0.7 (0.0)MSRFNet 83.2 (0.9)76.5 (1.1)81.9 (1.2)86.4 (0.5)91.3 (1.0)1.7 (0.0)HiFormer 89.1 (0.6)83.7 (0.6)88.8 (0.5)89.5 (0.2)96.1 (0.8)1.1 (0.2)DCSAUNet 80.5 (1.2)73.7 (1.1)79.6 (1.1)84.9 (0.6)89.9 (1.0)2.4 (0.2)M2SNet 92.8 (0.8)88.2 (0.8)92.3 (0.7)91.4 (0.4)97.7 (0.5)0.7 (0.1)",
  "MethodCVC-ClinicDB + Kvasir-SEG Kvasir-SEG DSC mIoU F w S EmaxMAE": "UNet 80.5 (0.3)72.6 (0.4)78.2 (0.4)79.9 (0.2)88.2 (0.2)5.2 (0.2)AttUNet 83.9 (0.1)77.1 (0.1)83.1 (0.0)81.9 (0.0)90.0 (0.1)4.4 (0.1)UNet++ 84.3 (0.3)77.4 (0.2)83.1 (0.3)82.1 (0.1)90.5 (0.2)4.6 (0.1)CENet 89.5 (0.7)83.9 (0.7)88.9 (0.7)85.3 (0.3)94.1 (0.4)3.0 (0.2)TransUNet 86.4 (0.4)81.3 (0.4)85.4 (0.4)83.0 (0.4)92.1 (0.5)4.0 (0.3)FRCUNet 88.8 (0.4)83.5 (0.6)88.4 (0.6)85.1 (0.2)93.6 (0.4)3.3 (0.1)MSRFNet 86.1 (0.5)79.3 (0.4)84.9 (0.7)82.8 (0.1)92.0 (0.4)4.0 (0.1)HiFormer 88.1 (1.0)82.3 (1.2)87.3 (1.1)84.6 (0.5)93.9 (0.6)3.1 (0.3)DCSAUNet 82.6 (0.5)75.2 (0.5)80.7 (0.3)81.3 (0.7)90.1 (0.1)4.9 (0.2)M2SNet 90.2 (0.5)85.1 (0.6)89.4 (0.8)85.6 (0.5)94.6 (0.7)2.8 (0.1)",
  "MethodCVC-ClinicDB + Kvasir-SEG CVC-ColonDB DSC mIoU F w S EmaxMAE": "UNet 56.8 (1.3)49.0 (1.2)55.9 (1.2)72.6 (0.6)73.9 (1.6)5.1 (0.1)AttUNet 56.8 (1.6)50.0 (1.5)56.2 (1.7)73.0 (0.7)72.3 (1.3)4.9 (0.1)UNet++ 57.5 (0.4)50.2 (0.4)56.6 (0.3)73.3 (0.3)73.9 (0.5)5.0 (0.1)CENet 65.9 (1.6)59.2 (0.1)65.8 (0.1)77.7 (0.1)79.5 (0.4)4.0 (0.2)TransUNet 63.7 (0.1)58.4 (0.4)62.8 (0.9)75.8 (0.4)79.3 (1.6)4.8 (0.0)FRCUNet 69.1 (1.0)62.6 (0.9)68.5 (1.0)79.3 (0.6)81.4 (0.6)4.0 (0.1)MSRFNet 61.5 (1.0)54.8 (0.8)60.8 (0.8)75.4 (0.5)76.1 (0.9)4.5 (0.1)HiFormer 67.6 (1.4)60.5 (1.3)66.9 (1.4)78.6 (0.7)81.2 (1.4)4.2 (0.0)DCSAUNet 57.8 (0.4)49.3 (0.4)54.9 (0.6)73.3 (0.3)76.0 (1.3)5.8 (0.3)M2SNet 75.8 (0.7)68.5 (0.5)73.7 (0.7)84.2 (0.3)86.9 (0.1)3.8 (0.1)"
}