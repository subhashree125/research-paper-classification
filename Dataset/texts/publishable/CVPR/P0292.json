{
  "Abstract": "The systematic evaluation and understanding of com-puter vision models under varying conditions require largeamounts of data with comprehensive and customized labels,which real-world vision datasets rarely satisfy. While cur-rent synthetic data generators offer a promising alternative,particularly for embodied AI tasks, they often fall short forcomputer vision tasks due to low asset and rendering qual-ity, limited diversity, and unrealistic physical properties. Weintroduce the BEHAVIOR Vision Suite (BVS), a set of toolsand assets to generate fully customized synthetic data forsystematic evaluation of computer vision models, based onthe newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parame-ters at the scene level (e.g., lighting, object placement),the object level (e.g., joint configuration, attributes such asfilled and folded), and the camera level (e.g., field ofview, focal length). Researchers can arbitrarily vary theseparameters during data generation to perform controlledexperiments. We showcase three example application sce-narios: systematically evaluating the robustness of mod-els across different continuous axes of domain shift, eval-uating scene understanding models on the same set of im-ages, and training and evaluating simulation-to-real trans-fer for a novel vision task: unary and binary state predic-tion. Project website:",
  "equal contribution correspondence to ,{yihetang, zharu}@stanford.edu": "35, 43, 47, 57, 67]. Driven by these datasets and bench-marks, thousands of models and algorithms tackling differ-ent perception challenges are being proposed every year, onthe topics of object detection , segmentation , ac-tion recognition , video understanding and beyond.Despite their success, real-world datasets face inherent lim-itations. First, the ground-truth object/pixel-level labels areeither prohibitively expensive to acquire (e.g., segmentationmasks) or suffering from inaccuracies (e.g., depth sens-ing) . Consequently, each real dataset often only offerslimited labels, hindering the development and evaluation ofcomputer vision models that perform a wide range of per-ception tasks on the same input. Even when annotations areaffordable and accurate, real-world datasets are limited bythe availability of source images. For example, images ofrare events, such as traffic accidents or low-light conditions,might be difficult to acquire from the Internet or real-worldsensors. Finally, once collected, these real-world datasetshave a fixed data distribution and cannot be easily changed.This makes it challenging for researchers to conduct cus-tomized experiments, often leading to models that overfitthe datasets and eventually rendering the entire benchmarksobsolete .To avoid this limitation, researchers and practitionershave devised various methods to generate synthetic datasetsthat complement the real ones . In the realm of indoorscene understanding, 3D reconstruction datasets provide a promising avenue to generate source imagesfrom arbitrary viewpoints and free (geometric) annotations.However, due to the imperfect nature of 3D reconstruc-tion techniques, the rendered images are not very realis-tic. Since each entire scene is a static mesh, these datasetsoffer very limited customizability beyond camera trajecto-ries. Recent synthetic indoor datasets (often designed by3D artists) not only offer free geometric andsemantic annotations, but also support object layout recon-figuration as objects are usually independent CAD models.",
  "Controlled Evaluation of Vision Algorithms": ". Overview of BEHAVIOR Vision Suite (BVS), our proposed toolkit for computer vision research. BVS builds upon the extendedobject assets and scene instances from BEHAVIOR-1K , and provides a customizable data generator that allows users to generatephotorealistic, physically plausible labeled data in a controlled manner. We demonstrate BVS with three representative applications. However, these datasets do not guarantee physical plausi-bility, as object penetration and levitation occur frequently,and offer no customization capability beyond changing ob-ject poses. 3D simulators , on theother hand, guarantee physical plausibility with their under-lying physics engines. They allow users to customize thejoint configuration of articulated objects and even more ad-vanced object states such as cooked or sliced .Yet these 3D simulators generally cater to embodied AI androbotics researchers, and as a result, they lack photorealismcompared to the synthetic datasets mentioned before (oftendue to speed constraints), and do not offer ready-made toolsto generate customized image/video datasets for computervision researchers.To overcome the aforementioned challenges, we pro-pose BEHAVIOR Vision Suite (BVS), a customizable datageneration tool that enables systematic evaluation and un-derstanding of computer vision models (see for anoverview). To do so, we expand the 3D asset library inBEHAVIOR-1K , focusing on enhancing both objectdiversity and scene variety, as well as adding features to in-crease the value of the assets for vision tasks. We also intro-duce a customizable dataset generator, which uses the sim-ulator from the BEHAVIOR-1K benchmark to gen-erate custom vision datasets. We build a versatile and cus-tomizable toolbox to generate high-quality synthetic datafor systematic model evaluation and understanding.In summary, BEHAVIOR Vision Suite possesses the fol-lowing unique combination of desirable features: BVS offers image/object/pixel-level labels (scene graph,point cloud, depth, segmentation, etc.);",
  "BVS covers a wide variety of indoor scenes and objects(8K+ objects, 1K scene instances, fluid, soft bodies);": "BVS provides physical plausibility and photorealism; BVS supports customization in terms of object models,poses, joint configurations, semantic states, lighting, tex-ture, material, camera setting, etc.; BVS includes easy-to-use tooling to generate customizeddata for new use cases.To demonstrate the usefulness of BVS, we show threeexample applications: 1) parametrically evaluating modelrobustness across different conditions such as lighting andocclusion, 2) evaluating different types of representativecomputer vision models on the same set of images, and 3)training and evaluating sim2real transfer for object state andrelation prediction. We hope that BVS can unlock morepossibilities for the computer vision community.",
  ". Related works": "In this section, we compare BEHAVIOR Vision Suite withother real RGB-D datasets, 3D reconstruction datasets, syn-thetic datasets, and 3D simulators in terms of customizabil-ity and visual quality (see Tab. 1). Real Indoor Scene RGB-D Datasets.RGB-D imagedatasets of real indoor scenes have drivenadvances in 3D perception and holistic scene understand-ing, with recent additions like ARKitScenes and Scan-Net++ offering dense semantic and 3D annotations.Despite having minimum domain gaps with respect to real-world applications, these real datasets are expensive to an-notate and inherently static, limiting users ability to gener-ate images from novel camera views, acquire new types ofannotations, or alter scenes. Our work complements theselimitations by offering a fully customizable generator forphotorealistic synthetic data.",
  "CustomizabilityVisualQuality": ". Comparison of real and different types of syntheticdatasets with BEHAVIOR Vision Suite.Camera denotes theability to render images from any viewing angle. Obj. Pose refersto the modifiability of the object layout. Obj. State indicateswhether an objects physical states (e.g., open/close, folded) andsemantic states (e.g., cooked, soaked) can be modified. Toolkitindicates the availability of utility functions for sampling objectlayout and camera poses under specified conditions (e.g., view-ing half-open kitchen cabinets filled with grocery items). VisualQuality evaluates the photorealism of rendered images. 3D Reconstruction Datasets. 3D reconstruction datasetssuch as Gibson, Matterport, and HM3DSem allow the rendering of novel views. While these datasetshave tremendously benefited the embodied navigation com-munity, their utility for broader computer vision applica-tions remains limited. Each scene, being a single 3D mesh,restricts further customization, such as modifying the ob-ject layout. Moreover, the visual quality of rendered novelviews depends on the reconstructions fidelity, often result-ing in artifacts. While Taskonomy and Omnidata have extended mid-level visual cues such as surface normalfor these datasets, semantic label acquisition remains ex-pensive. In contrast, our work offers the flexibility to gener-ate images with customized object layouts with consistentvisual quality, while also providing comprehensive labels atno additional cost. Synthetic Datasets.Synthetic datasets offer an alterna-tive approach that eliminates the need for manual seman-tic labeling by rendering realistic images from interiorscenes composed of independent object models . Ob-ject layouts are usually created by artists orparsed from real scans , offering semantic realism ofthe scenes . Methods like OpenRooms andUnity Synthetic Homes also allow users to configurerendering options, such as lighting. However, despite theirphotorealism, the rendered images often lack physical plau-sibility, with common issues like object penetration or slightlevitation. In addition, the object models are mostly fullyrigid and support very limited semantic states. Our gen-erator not only ensures the physical plausibility of imagescreated, but also supports broader relationship customiza-tion (e.g., cooked or filled) and more granular controlover the sampled state, such as openness level through jointlimit annotations.",
  "D Simulators.A large number of 3D simulators with": "physical realism have been developed recently. Kubric focuses on generating physically plausible object clusterswithout full-scene simulation. iGibson and Habi-tat 2.0 offer reconfigurable indoor scenes with artic-ulated assets, the former notably supporting extended ob-ject states such as wetness level. ThreeDWorld empha-sizes physical prediction, especially with non-rigid objects.ProcTHOR automates the large-scale generation of se-mantically plausible virtual environments. Since these 3Dsimulators cater to the embodied AI and robotics commu-nity, their visual quality is often not prioritized. In con-trast, we use OmniGibson, a new simulator that surpassesthe photorealism of the aforementioned ones, according toa user study , positioning our work as more suitable forcomputer vision research. Moreover, we provide a rangeof utility functions in this work, allowing for easy creationof diverse images tailored to specific needsa feature mostexisting 3D simulators lack.",
  ". Extended BEHAVIOR-1K Assets": "The extended BEHAVIOR-1K assets comprises a diversecollection of 8,841 object models and 1,000 scene instances,derived from 51 artist-designed raw scenes. Of these ob-jects, 2,156 are structural elements like walls, floors, andceilings, while the remaining 6,685 nonstructural itemsspan 1,937 categories, including food, tools, electronics,clothing, and office supplies, among others. This catego-rization is detailed in . Predominantly indoor, the 51raw scenes also incorporate outdoor elements such as gar-dens and encompass a wide variety of environments: houses(23), offices (5), restaurants (6), grocery stores (4), hotels(3), schools (5), and generic halls (4), as well as a simulatedtwin of a mock apartment in our research lab. This collec-tion of assets is the result of a year-long effort to extend theBEHAVIOR-1K assets to enhance their applicability incomputer vision.We expanded the object collection from 5,215 to 8,841by adding more everyday objects, segmenting buildingstructures into individual objects for more precise 3Dbounding box labels, and procedurally generating slicedfood. In addition, we have developed functionality that en-ables the generation of diverse scene variations by alteringfurniture object models and incorporating additional every-day objects.We will release 1000 scene instances aug-mented from the 51 raw scenes.",
  "scene instances": ". Overview of extended BEHAVIOR-1K assets: Covering a wide range of object categories and scene types, our 3D assets havehigh visual and physical fidelity and rich annotations of semantic properties, allowing us to generate 1,000+ realistic scene configurations. To improve physical realism,we refined collisionmeshes using V-HACD and CoACD , manuallyselecting the best parameters to ensure a balance betweenphysical accuracy, affordance preservation, and simula-tion efficiency. For more than 2,000 objects, where thismethod was insufficient, we manually designed their col-lision meshes.We enhanced lighting realism by annotating actual lightsource objects, such as lamps and ceiling lights, to mimicreal-world illumination. For more detailed semantic prop-erties, we annotated appropriate container fillable volumes(e.g., cups, pots) and fluid source/sink locations (e.g.,faucets, drains, sprayers), enabling us to spawn fluids inthe scene realistically. Scene objects were annotated if theycannot be freely moved, e.g., when they physically supportother objects. Cluttered objects were distinctly annotated,allowing them to be replaced with alternative clutters.Altogether, we designed the assets to form a strong ba-sis for custom data generation (discussed in 3.2), with afunctional organization that allows accurate object random-ization, and the annotations to provide a large number ofmodifiable parameters at both the object and scene levels.",
  "The customizable dataset generator, the software compo-nent of the BEHAVIOR Vision Suite, is designed to gen-": "erate synthetic datasets tailored to particular specifications.Built on OmniGibson , it leverages NVIDIA Omni-verses photorealistic, real-time renderer and OmniGibsonsprocedural sampling functions for object states to gener-ate custom images and videos that satisfy arbitrary require-ments. The produced datasets include rich, comprehensiveannotationssegmentation masks, 2D/3D bounding boxes,depth, surface normals, flows, and point cloudsat no ad-ditional cost. Crucially, it empowers users with extensivecontrol over the dataset generation process, allowing themto specify requirements on scene layouts, object states, cam-era angles, and lighting conditions, all while ensuring phys-ical plausibility through the physics engine.",
  "Capabilities. The generator has the following capabilities:": "Scene Object Randomization:It can swap scene ob-jects with alternative models from the same category,which are grouped based on visual and functional simi-larities. This randomization significantly varies scene ap-pearances while maintaining layouts semantic integrity. Physically Realistic Pose Generation: The generator canprocedurally change the physical states of objects to sat-isfy certain predicates. This includes 1) placing objectswith respect to other objects in the scene in a certain way(e.g., inside, on top of, or under), 2) opening or closingarticulated objects, 3) filling containers with fluids, and",
  ") folding or unfolding pieces of cloth. The generator cangenerate multiple valid configurations for the same pred-icate and ensures physical plausibility": "Predicate-Based Rich Labeling: Beyond usual labels (se-mantic & instance segmentation, bounding boxes, surfacenormals, depth, etc.), the generator also provides annota-tions including unary states of an object (e.g., whetheran articulated object is open, or an appliance is toggledon), binary predicates between two objects (e.g., if one istouching, on top of, next to another) or between an objectand a substance (e.g., if an object is filled/covered/soakedwith a substance), and continuous labels (e.g., joint open-ness for articulated objects, filled fraction for containers). Camera Pose and Trajectory Sampling: Finding propercamera pose in a 3D scene is a challenging but crucialstep in the rendering pipeline: the camera shall not beoccluded and points at the subject of interest. The gener-ator uses occupancy grids and hand-crafted heuristics togenerate both static camera poses and plausible traversaltrajectories that satisfy these constraints to curate imageor scene traversal video datasets. Configurable Rendering: Through a user-friendly API,the generator allows for the customization of renderingparameters, including lighting and camera specifics suchas aperture and field of view.Dataset Generation. Images in the BVS dataset can begenerated as follows. First, we select one of the 51 rawscenes from the user-configured scene category (say, an of-fice). Scene objects are randomized with instances from thesame category. Depending on the user configuration, wedetermine additional objects to add to the scene. We placethe objects using the pose generation capabilities based onuser-specified requirements. This might include clutteringcertain areas (e.g., filling a fridge with perishables) or indi-vidually manipulating object states (e.g., making a cabinetopen or a table covered with water) for predicate prediction.We then generate a camera pose (or a sequence of posesas a camera trajectory), as well as randomize the sceneslighting parameters and the cameras intrinsics based onthe users specifications. Finally, we render an image (ora sequence of images) and record it alongside all relevantlabels requested by the user, including additional modali-ties (depth/segmentation/etc.), bounding boxes, and predi-cate and object state values.",
  ". Parametric Model Evaluation": "Parametric model evaluation is essential for developing andunderstanding perception models, enabling a systematic as-sessment of performance robustness against various domainshifts. Previous efforts, such as 3DCC , have exploredimage corruption generation using 3D information, yet theirscope is constrained by the static nature of input meshes,limiting the type and extent of possible variations. Lever-aging the flexibility of the simulator, our generator extendsparametric evaluation to more diverse axes, including scene,camera, and object state changes.Task Design and Dataset Generation. We focus on fivekey parameters difficult to rigorously control in real-worlddatasets yet significantly influence model performance: ob-ject articulation, lighting, object visibility, camera zoom,and camera pitch. Each parameter varies along a continu-ous axis for evaluating baseline models. For instance, objectvisibility varies from fully occluded to fully visible.We generate 200 to 500 videos for each axis (Tab. 2), us-ing our collection of more than 8,000 3D assets. Each videoincludes a target object with changes focused on a single pa-rameter under examination. shows examples of targetobjects with variations along each axis. We maintained con-sistency in other aspects of the environment, systematicallysynthesizing images to isolate the main parameters impact.To validate our findings in real-world conditions and fur-ther assess the sim2real transfer capability of parametricevaluation, we collected a smaller-scale real dataset for eachof the 5 axes and replicated the evaluation. For setup detailsand additional results, please refer to the appendix.Baselines and Metrics. We explore two vision tasks: open-vocabulary detection and open-vocabulary segmentation,hypothesizing that models for these tasks may be sensitiveto object-centric domain shifts. For baselines, we selectthe current state-of-the-art (SOTA) models on real datasets:GLIP , RAM , and Grounding DINO for de-tection, and ODISE , OpenSeeD , and GroundingSAM for segmentation. . Parametric evaluation of object detection models on fiveexample video clips. Selected frames from these clips are shownon the left, with the target object highlighted in magenta. AveragePrecisions (APs) for our baseline models in 4.2 are plotted on theright. Since BVS allows for full customization of scene layout andcamera viewpoints, we can systematically evaluate model robust-ness against variations in object articulation, lighting conditions,visibility, zoom (object proximity), and pitch (object pose). As il-lustrated, current SOTA models exhibit limited robustness to theseaxes of variation. Results and Analysis. In and , we presentexample images when varying each parameter as well asrespective detection Average Precision (AP) performance.AP, calculated exclusively for the target object (highlightedin magenta), assesses the models recognition accuracy. De-tailed analyses reveal the following: Articulation varies the joint angles of the articulatedtarget object, from fully closed to fully open, including pro-cesses such as opening/closing drawers or doors, and fold-ing/unfolding laptops. A notable negative correlation be-tween the degree of articulation and model performancesuggests that models, typically trained or evaluated on exist-ing benchmarks featuring mostly closed articulated objects(e.g., closed washing machines and microwaves), strugglewith recognizing objects in open states.",
  "Lighting adjusts global illumination of the environ-": ". Mean performance of open-vocab object detection andsegmentation models across five axes. The larger a models col-ored envelope, the more robust it is. Through BVS, new visionmodels can be systematically tested for their robustness alongthese five dimensions and beyond: our users can easily add newaxes of domain shift with just a few lines of code. ment from dark to bright. We observed improving modelperformance up to a midpoint brightness level of 0.5, be-yond which it plateaus.This suggests that, while cur-rent models suffer from low-light conditions, their perfor-mance saturates once the brightness level surpasses a cer-tain threshold. Visibility shifts the visibility of the target object fromfully occluded to fully visible, which is computed as the ra-tio of visible to total pixels of the target object. We observea steep decline in model performance as visibility drops be-low 0.5, which highlights a significant opportunity to en-hance model robustness to partial occlusions. Zoom controls camera zoom from zoomed-in tozoomed-out.Results show that extremely close views,where a partial view of the target object occupies the en-tire image, hinder performance due to a lack of contextualinformation. In contrast, too-zoomed-out views make theobject too small for models to detect it effectively. Optimalperformance is achieved at moderate zoom levels. Pitch varies camera pitch from looking up to look-ing down.We find that models perform inconsistentlywith seemingly benign changes in camera viewpoint, gener-ally showing improved performance when the camera looksdown at the target object. One potential explanation is thatobjects in large-scale real datasets are often captured fromabove, making this perspective more familiar to the models. To summarize, we observe significant performance dis-crepancies across three models on all five axes, with ourparallel experiments in real settings (see the appendix) con-firming that these trends observed in synthetic data mirrorthose in real-world scenarios. This underscores the lack ofrobustness of the current SOTA models in extreme or out-of-distribution test environments. By generating large-scalesynthetic datasets with controlled variability, BVS providesa unique and powerful test bed to evaluate model perfor-mance. Furthermore, consistent with 4.2, the relative per-formance remains steady across the five axes, highlighting",
  ". Holistic Scene Understanding": "One of the major advantages of synthetic datasets, includingBVS, is that they offer various types of labels (segmentationmasks, depth maps, and bounding boxes) for the same setsof input images. We believe that this feature can fuel the de-velopment of versatile vision models that can perform mul-tiple perception tasks at the same time in the future. Sincesuch models are not currently available, we instead evaluatethe current SOTA methods on a subset of the tasks that BVSsupports (see below). This will also serve as a validation ofthe photorealism of our datasets, i.e., models trained on realdatasets should perform reasonably without fine-tuning. Task Design and Dataset Generation.Equipped withBVSs powerful generator (see 3.2), we generated 100+full scene traversal videos with a total of 266240 frameswith per-frame ground truth annotations in multiple modal-ities. shows an overview of the generated dataset. Baselines and Metrics. In Tab. 3, we assess 11 models infour tasks. Specifically, we consider Detection and Segmen-tation tasks, both in the challenging open vocabulary set-ting , as well as Depth Estimation and Point CloudReconstruction, with standard metrics used. Results and Analysis. We summarize all our evaluation re-sults in Tab. 3. We observe that the relative performanceof these models on our synthetic dataset has a high corre-lation with that on real datasets such as MS COCO or NYUv2 , indicating that our generated syntheticdatasets can be a faithful proxy for real datasets.In summary, we provide a comprehensive benchmark toscore and understand a wide range of existing models foreach of the four tasks on exactly the same images.Al-though most current vision models focus on a single outputmodality, we hope BEHAVIOR Vision Suite could moti-vate researchers and practitioners to develop versatile mod-els that concurrently predict multiple modalities in the fu-ture, where our benchmarking results for single-task SOTAmethods in this section could serve as a useful reference.",
  ". Object States and Relations Prediction": "BVSs capabilities extend beyond model evaluation shownin 4.1 and 4.2. Users can also leverage BVS to generatetraining data with specific object configurations that are dif-ficult to accumulate or annotate in the real world. This sec-tion illustrates BVSs practical application in synthesizinga dataset that facilitates the training of a vision model ca-pable of zero-shot transfer to real-world images on the taskof object relationship prediction . Additional",
  "experiments focusing on unary object states, filled andfolded, are detailed in the appendix": "Task Design and Dataset Generation. Predicting objectrelationships, such as open and inside, is a crucial yetchallenging perception task due to the difficulties in col-lecting such data in the real world, let alone the costlyannotations. We use our generator to synthe-size 12.5k images with five labels (open, close, ontop,inside, under), depicting relationships between targetobjects. We also collected and labeled 910 real images withunseen object instances and scenes to test sim2real perfor-mance. Examples are shown in . Baselines and Metrics. Adapting from , our modeltakes an image and target objects bounding boxes as input,and outputs a five-way classification over the five labels.We specifically define open/close as a binary relation-ship between the movable link and the unmovable base ofan articulated object, enabling fine-grained articulation stateassessment. For example, the model can be queried for theopen or closed status of individual drawers of a cabinet. De-tailed model architecture is available in the appendix.We compare our model with zero-shot CLIP, which isnot trained on our synthetic dataset, in terms of precision,recall, and F1, on the synthetic evaluation set and the realtest set. Specifically, by harnessing CLIPs zero-shot ca-pabilities , this baseline outputs a five-way classifica-tion prediction by comparing the image embeddings withthe five verbalized prompts text embeddings. Results and Analysis. Tab. 4 presents the quantitative re-sults on the held-out synthetic dataset and the real datasetfor our method. Although there is some performance gap,our model trained on only synthetic data can transfer toreal images with promising overall accuracy. Additionally,unary state prediction experiments, detailed in the appendix,also reveal high accuracy in both domains. These results",
  ". Classification results on the real test set. Task-specifictraining on synthetic data boosts performance on real images": "underscore that BVS offers a promising way to obtain re-alistic synthetic data that researchers can use not only forevaluation (as shown in 4.1 and 4.2), but also for train-ing models that can then be transferred to the real world.In fact, from Tab. 5, we observe that task-specific trainingon synthetic data is a very effective method to obtain goodperformance on real images.",
  ". Conclusion": "We have introduced the BEHAVIOR Vision Suite (BVS),a novel toolkit designed for the systematic evaluation andcomprehensive understanding of computer vision models.BVS enables researchers to control a wide range of parame-ters across scene, object, and camera levels, facilitating thecreation of highly customized datasets. Our experimentshighlight BVSs versatility and efficacy through three keyapplications. First, we show its ability to evaluate modelrobustness against various domain shifts, underscoring itsvalue in systematically assessing model performance underchallenging conditions. Second, we present comprehensivebenchmarking of scene understanding models on a unifieddataset, illustrating the potential for developing multi-taskmodels using a single BVS dataset. Lastly, we investigateBVSs role in facilitating sim2real transfer for novel visiontasks, including object states and relations prediction. BVShighlights synthetic datas promise in advancing the field,offering researchers the means to generate high-quality, di-verse, and realistic datasets tailored to specific needs. Acknowledgments.We are grateful to SVL members fortheir helpful feedback and insightful discussions.Thework is in part supported by the Stanford Institute forHuman-Centered AI (HAI), NSF CCRI #2120095, RI#2338203, ONR MURI N00014-22-1-2740, N00014-21-1-2801, Amazon, Amazon ML Fellowship, and Nvidia. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry,Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe,Daniel Kurz, Arik Schwartz, and Elad Shulman.ARK-itscenes - a diverse real-world dataset for 3d indoor sceneunderstanding using mobile RGB-d data. In Thirty-fifth Con-ference on Neural Information Processing Systems Datasetsand Benchmarks Track (Round 1), 2021. 2 Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,and Juan Carlos Niebles. Activitynet: A large-scale videobenchmark for human activity understanding. In Proceed-ings of the ieee conference on computer vision and patternrecognition, pages 961970, 2015. 1 Paola Cascante-Bonilla, Khaled Shehada, James SealeSmith, Sivan Doveh, Donghyun Kim, Rameswar Panda, GulVarol, Aude Oliva, Vicente Ordonez, Rogerio Feris, et al.Going beyond nouns with vision & language models usingsynthetic data.In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 2015520165,2023. 7 Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-ber, Matthias Niessner, Manolis Savva, Shuran Song, AndyZeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on3D Vision (3DV), 2017. 1, 3 Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-ber, Thomas Funkhouser, and Matthias Niener. Scannet:Richly-annotated 3d reconstructions of indoor scenes.InProc. Computer Vision and Pattern Recognition (CVPR),IEEE, 2017. 2 Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,Oscar Michel, Eli VanderBilt, Ludwig Schmidt, KianaEhsani, Aniruddha Kembhavi, and Ali Farhadi.Obja-verse: A universe of annotated 3d objects. arXiv preprintarXiv:2212.08051, 2022. 3 Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,Aniruddha Kembhavi, and Roozbeh Mottaghi.Procthor:Large-scale embodied ai using procedural generation. Ad-vances in Neural Information Processing Systems, 35:59825994, 2022. 2, 3 Matt Deitke, Ruoshi Liu, Matthew Wallingford, HuongNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, EliVanderBilt, Aniruddha Kembhavi, Carl Vondrick, GeorgiaGkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprintarXiv:2307.05663, 2023. 3 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 1",
  "Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xin-long Wang, and Yue Cao. Eva-02: A visual representationfor neon genesis. arXiv preprint arXiv:2303.11331, 2023.19": "Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, BinqiangZhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d fur-niture shape with texture. International Journal of ComputerVision, 129:33133337, 2021. 1, 3 Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca,Martin Schrimpf, James Traer, Julian De Freitas, Jonas Ku-bilius, Abhishek Bhandwaldar, Nick Haber, et al. Threed-world: A platform for interactive multi-modal physical sim-ulation. arXiv preprint arXiv:2007.04954, 2020. 2, 3 Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar,Neel Joshi, Yale Song, Xin Wang, Laurent Itti, and VibhavVineet. Neural-sim: Learning to generate training data withnerf. In European Conference on Computer Vision, pages477493. Springer, 2022. 7, 19",
  "Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Laurent Itti, andVibhav Vineet. Em-paste: Em-guided cut-paste with dall-e augmentation for image-level weakly supervised instancesegmentation, 2022": "Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Lau-rent Itti, and Vibhav Vineet. Dall-e for detection: Language-driven compositional image synthesis for object detection.arXiv preprint arXiv:2206.09592, 2022. Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Lau-rent Itti, and Vibhav Vineet. Beyond generation: Harnessingtext to image models for object detection and segmentation.arXiv preprint arXiv:2309.05956, 2023. 1, 7, 19 Yunhao Ge, Hong-Xing Yu, Cheng Zhao, Yuliang Guo,Xinyu Huang, Liu Ren, Laurent Itti, and Jiajun Wu. 3d copy-paste: Physically plausible object insertion for monocular 3ddetection. Advances in Neural Information Processing Sys-tems, 36, 2024. 3 Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the kitti vision benchmarksuite. In 2012 IEEE conference on computer vision and pat-tern recognition, pages 33543361. IEEE, 2012. 1",
  "Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-national conference on computer vision, pages 14401448,2015. 18": "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,Valentin Haenel, Ingo Fruend, Peter Yianilos, MoritzMueller-Freitag, et al.The something something videodatabase for learning and evaluating visual common sense.In Proceedings of the IEEE international conference on com-puter vision, pages 58425850, 2017. 1 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video. In Pro-",
  "ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1899519012, 2022. 1": "Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-gasam, Florian Golemo, Charles Herrmann, Thomas Kipf,Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, DerekNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-wan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-able dataset generator. 2022. 3 Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, ChiLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.Vizwiz grand challenge: Answering visual questions fromblind people.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 36083617,2018. 1 Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, WenqingZhang, Philip Torr, Song Bai, and XIAOJUAN QI. Is syn-thetic data from generative models ready for image recogni-tion? In The Eleventh International Conference on LearningRepresentations, 2022. 7 Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,Samyak Parajuli, Mike Guo, et al. The many faces of robust-ness: A critical analysis of out-of-distribution generalization.In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 83408349, 2021. 1 Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-hardt, and Dawn Song. Natural adversarial examples. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1526215271, 2021. 1",
  "Oguzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and AmirZamir.3d common corruptions and data augmentation,2022. 5": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 1, 18 Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3denvironment for visual ai. arXiv preprint arXiv:1712.05474,2017. 2 Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-tidis, Li-Jia Li, David A Shamma, et al.Visual genome:Connecting language and vision using crowdsourced denseimage annotations. Int J Comput Vis, 123:3273, 2017. 1, 8 Chengshu Li, Fei Xia, Roberto Martn-Martn, Michael Lin-gelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, CemGokmen, Gokul Dharan, Tanish Jain, et al.igibson 2.0:Object-centric simulation for robot learning of everydayhousehold tasks. arXiv preprint arXiv:2108.03272, 2021. 2,3 Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen,Sanjana Srivastava, Roberto Martn-Martn, Chen Wang,Gabrael Levine, Michael Lingelbach, Jiankai Sun, MonaAnvari, Minjune Hwang, Manasi Sharma, Arman Aydin,Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou,Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang,Claire Tang, Fei Xia, Silvio Savarese, Hyowon Gweon,Karen Liu, Jiajun Wu, and Li Fei-Fei.Behavior-1k: Abenchmark for embodied ai with 1,000 everyday activitiesand realistic simulation. In Proceedings of The 6th Confer-ence on Robot Learning, pages 8093. PMLR, 2023. 1, 2, 3,4 Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, LuYuan, Lei Zhang, Jenq-Neng Hwang, et al.Groundedlanguage-image pre-training.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1096510975, 2022. 5, 7 Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark,Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang,and Stefan Leutenegger.Interiornet: Mega-scale multi-sensor photo-realistic indoor scenes dataset. arXiv preprintarXiv:1809.00716, 2018. 1, 3 Zhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, MengSong, Yuhan Liu, Yu-Ying Yeh, Rui Zhu, Nitesh Gun-davarapu, Jia Shi, et al. Openrooms: An end-to-end openframework for photorealistic indoor scene datasets. arXivpreprint arXiv:2007.12868, 2020. 1, 3",
  "Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shiftmodule for efficient video understanding. In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 70837093, 2019. 1": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.Bourdev, Ross B. Girshick, James Hays, Pietro Perona, DevaRamanan, Piotr Dolla r, and C. Lawrence Zitnick. MicrosoftCOCO: common objects in context. CoRR, abs/1405.0312,2014. 1, 7 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 1",
  "Khaled Mamou. Volumetric approximate convex decompo-sition. In Game Engine Gems 3, chapter 12, pages 141158.A K Peters / CRC Press, 2016. 4": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua BTenenbaum, and Jiajun Wu.The neuro-symbolic conceptlearner: Interpreting scenes, words, and sentences from nat-ural supervision. In International Conference on LearningRepresentations, 2018. 8 Roberto Martin-Martin, Mihir Patel, Hamid Rezatofighi,Abhijeet Shenoi, JunYoung Gwak, Eric Frankel, AmirSadeghian, and Silvio Savarese. Jrdb: A dataset and bench-mark of egocentric robot visual perception of humans in builtenvironments. IEEE transactions on pattern analysis andmachine intelligence, 2021. 1",
  "Pushmeet Kohli Nathan Silberman, Derek Hoiem and RobFergus.Indoor segmentation and support inference fromrgbd images. In ECCV, 2012. 1, 2, 7": "Luigi Piccinelli, Christos Sakaridis, and Fisher Yu. idisc: In-ternal discretization for monocular depth estimation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 2147721487, 2023. 7 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 8, 18 Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans,Oleksandr Maksymets, Alex Clegg, John Turner, Eric Un-dersander, Wojciech Galuba, Andrew Westbury, Angel XChang, et al. Habitat-matterport 3d dataset (hm3d): 1000large-scale 3d environments for embodied ai. arXiv preprintarXiv:2109.08238, 2021. 1, 3",
  "Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-sion transformers for dense prediction. ArXiv preprint, 2021.7": "Mike Roberts, Jason Ramapuram, Anurag Ranjan, AtulitKumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,and Joshua M Susskind. Hypersim: A photorealistic syn-thetic dataset for holistic indoor scene understanding.InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1091210922, 2021. 1, 3 Bokui Shen, Fei Xia, Chengshu Li, Roberto Martn-Martn,Linxi Fan, Guanzhi Wang, Claudia Perez-DArpino, Shya-mal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson1.0: A simulation environment for interactive tasks in largerealistic scenes. In 2021 IEEE/RSJ International Conferenceon Intelligent Robots and Systems (IROS), pages 75207527.IEEE, 2021. 1, 2, 3 Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid,Ali Farhadi, and Karteek Alahari. Actor and observer: Jointmodeling of first and third-person videos.In The IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2018. 1 Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao.Sun rgb-d: A rgb-d scene understanding benchmark suite.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2015. 2 Sanjana Srivastava, Chengshu Li, Michael Lingelbach,Roberto Martn-Martn, Fei Xia, Kent Elliott Vainio, ZhengLian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behav-ior: Benchmark for everyday household activities in virtual,interactive, and ecological environments. In Conference onRobot Learning, pages 477490. PMLR, 2022. 2 Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wi-jmans, Yili Zhao, John Turner, Noah Maestre, MustafaMukadam, Devendra Singh Chaplot, Oleksandr Maksymets,et al. Habitat 2.0: Training home assistants to rearrange theirhabitat. Advances in Neural Information Processing Systems,34:251266, 2021. 2, 3",
  "Heng Wang and Cordelia Schmid. Action recognition withimproved trajectories.In Proceedings of the IEEE inter-national conference on computer vision, pages 35513558,2013. 1": "Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao,Jianming Zhang, Ke Xian, and Guosheng Lin. Neural videodepth stabilizer. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 94669476,2023. 7 Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, andSanja Fidler. Neural light field estimation for street sceneswith differentiable virtual object insertion. In European Con-ference on Computer Vision, pages 380397. Springer, 2022.3",
  "Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, andDieter Fox. Posecnn: A convolutional neural network for6d object pose estimation in cluttered scenes. arXiv preprintarXiv:1711.00199, 2017. 1": "Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-long Wang, and Shalini De Mello. Open-vocabulary panop-tic segmentation with text-to-image diffusion models.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 29552966, 2023. 5,7 Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakr-ishnan, Theo Gervet, John Turner, Aaron Gokaslan, NoahMaestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva,et al. Habitat-matterport 3d semantics dataset. arXiv preprintarXiv:2210.05633, 2022. 3 Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Niener,and Angela Dai. Scannet++: A high-fidelity dataset of 3dindoor scenes. In Proceedings of the International Confer-ence on Computer Vision (ICCV), 2023. 2 Amir R. Zamir, Alexander Sax, William B. Shen, Leonidas J.Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:Disentangling task transfer learning.In IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR). IEEE,2018. 3 Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, ChunyuanLi, Jianwei Yang, and Lei Zhang. A simple framework foropen-vocabulary segmentation and detection. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 10201031, 2023. 5, 7 Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo,Yaqian Li, Shilong Liu, et al. Recognize anything: A strongimage tagging model.arXiv preprint arXiv:2306.03514,2023. 5, 7 Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hu-jun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Polle-feys. Nice-slam: Neural implicit scalable encoding for slam.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1278612796, 2022.7",
  "C.1. Parametric Model Evaluation": "Details of Dataset Generation Process.We synthesizedthe evaluation videos for each axis (Object articulation,Lighting, Visibility, Zoom, Pitch) according to the follow-ing pipeline.As shown in the main paper Sec. 4.1, each video includesa target object with changes focused on a single parameterunder examination. First, we sample one of the scene in-stances and randomly choose a target object in the scene.For the object articulation axis, we only sample objects withmovable parts, such as cabinets, microwaves, refrigerators,etc. Next, we sample a random camera angle and distancewith the target object placed in the center. Then, for all ex-cept pitch, we keep this camera pose and perform an axis-specific manipulation to generate a video with the desiredvariation: Object articulation: We linearly interpolate the joint an-gle from being closed to fully open, utilizing the jointmaximum range annotations provided in BVS assets. Werecord the image with the joint in each intermediate state.For objects with multiple movable parts, e.g., a cabinetwith three drawers, we randomly sample a subset of jointsto manipulate and keep the rest closed.",
  "Lighting: We linearly increase the intensity of all indoorlight sources in the scene simultaneously": "Visibility: There are three key components in the visibil-ity (occlusion) setting: camera, target object, and occlud-ing object. We first set the camera centering on the tar-get object, then we place an occluding object (relativelylarge object, e.g., cabinet) in the line between the cameraand the target object, fully occluding the target in view. Then, we fix the distance between the camera and the tar-get object and move the camera around the target objectuntil the target is fully visible. The visibility score (num-ber of visible pixels/number of total pixels) of each frameis calculated by rendering the video again and removingthe occluding cabinet. Although the object orientation incamera view might slightly change since the camera is notstatic, we implemented the following practices to elimi-nate the effect of this factor. First, we set the camera rela-tively far from the target but occluding objects close to thecamera, allowing minimal camera pose change needed tocapture the fully occluded to fully visible process. Inaddition, the initial object pose is randomized, so whenwe average evaluation performance, the effect of this fac-tor shall largely cancel out. Zoom: With the camera pose fixed, we change its fo-cal length to model the zooming effect. We strategicallychanged the focal length such that the resulting video il-lustrates an approximately linear zooming behavior. Wealways make the target object at the center of the view,and it remains mostly unaffected by distortion, even un-der extreme focal length. Pitch: We linearly change the camera pitch angle whilekeeping the original camera distance as well as the yawangle unmodified.After each video was collected, we filtered out the videoswhere the target object was not properly visible. The de-tailed statistics for each axis is shown in main . Metric Details.Each generated video contains exactlyone target object (main paper magenta). We usedifferent open-vocabulary object detection and segmenta-tion models to detect or segment the target object. Thesemodels act as indicators of performance in challenging en-vironments, such as those with limited lighting or long-distance zoom. Therefore, we compute the Average Pre-cision (AP) metric using the target object as the sole groundtruth, considering only predictions that classify the targetobject class. However, it is plausible to encounter scenarioswhere multiple objects of the same category as the targetobject exist. For instance, in a video, there might be severalchairs, and the target object is one of those chairs. Mod-els might have correctly detected all chairs, but since onlyone is the ground truth, all rest will be marked as incorrect.To counter such an undesired situation, we employ a simpleheuristic to filter predictions for the non-target object: Fornon-target objects sharing the same category, we calculatethe IoU with each prediction and exclude those with an IoUexceeding a predefined threshold of 0.3. This means treat-",
  "(j) Fillable volumes for containers": ". More examples of Extended BEHAVIOR-1K Assets: (a-f) examples for different main categories. (g) examples of improvedcollision mesh quality. (h) examples of articulation objects. (i) examples of different light sources, (j) examples of fillable volumes forcontainers. ing these predictions as valid for non-target objects ratherthan false positives. This threshold is chosen empiricallybased on a few selected cases where objects of the same cat-egory are densely packed together. We believe this choicegeneralizes well to less crowded scenes and ensures the re-liability of our evaluation process. Failure Case Analysis on Five Evaluation Parameters.In , we present failure case examples of GroundingDINO across five evaluation axes: Object articulation,Lighting, Visibility, Zoom, and Pitch. Each row in our pre-sentation represents one axis and comprises four examplegroups. In each group, there are two images: the left im-age illustrates the ground truth, highlighting the target ob-ject in magenta, while the right image shows the GroundingDINOs prediction for the target object, as indicated at thetop of the first image in each group. The example groups arearranged such that, from left to right, the intensity along therespective axis increases (e.g., progressing from zoomed into zoomed out), the intensity value (0-1) is shown on topof each prediction. We find and highlight some interest-ing findings for each parametric evaluation in anddetailed below. For the full axis, we prepare a video in sup-plementary to show qualitative examples about running dif-ferent detection models on the same video. Articulation. The model may have limited exposure toopen states for articulation objects, which makes it lesslikely to predict a microwave with its door open correctly. Lighting. When the environment is dark, the model per-formance is negatively affected. However, when the light-ing exceeds a certain threshold, in this case 0.5, the modelbecomes robust to increasing illumination.",
  "Pitch. We find that, generally, the model can achieve bet-ter performance in a look-down angle compared to a look-up angle": "Segmentation Results on Five Axes. of themain paper shows the performance of open-vocabulary de-tection models on five axes. In , we show the per-formance of open-vocabulary segmentation models instead.The average performance for each axes corresponds to oneangle in the radar plot (main ). Observations inmain section 3.1 also apply to segmentation tasks. Real Experiment Setup and ResultsIn order to evaluatethe sim2real transfer capability of parametric evaluation re-sults, we curated a set of real images to perform the sameevaluation. In total, we manually collected 430 images of15 to 22 objects from various categories. For each axis, wetook photo of each target object with 5 levels of the corre-sponding distribution shift that matches the intensity level0, 0.25, 0.5, 0.75, 1 in the synthetic data. For example,when collecting data for a microwave object for the articula-tion axis, we collect 5 images of the microwave being fullyclosed, 25% open, half open, 75% open, and fully open. Anexample object from the real dataset (and the comparison tothe most similar counterpart in simulation) is shown in . Forthe zoom axis, due to the limited focal length range of ourreal cameras, we only covered intensity levels 0 to 0.3.We evaluated the SOTA detection methods with man-ually labeled bounding boxes. shows that underdifferent types of distribution shift, the performance of theSOTA methods varies on real data just as it varies in simu-lation.",
  "C.2. Holistic Scene Understanding (main paper Sec.4.2)": "Details of Generation Process.To generate a scenetraversal video, we adhere to a standard process. Initially,we sample a scene instance and subsequently define a cam-era trajectory using the BVS toolkit. Following this, werender the traversal video, incorporating all required labels.This section will detail the specifics of the trajectory sam-pling procedure. In general, we want the sampled video toprovide rich information (good coverage) about the scene,which can be broken down into two aspects. Firstly, we aimfor the camera to physically cover the room. That meansthe sampled camera positions should enable visiting mostopen spaces in the scene, rather than just focusing on thelargest open space. This guides our design for camera posi-tion sampling. Second, we want the actual video to captureas many objects in the scene as possible while still beingrealistic (i.e., facing the direction of movement while mov-ing). This guides our design for camera orientation sam-pling. Next, we will establish the detailed steps. We willopen-source all codes and generate a video dataset. To sample camera positions within the trajectory, a basicapproach might involve randomly selecting traversablepoints within the scene. However, this brings the issuethat points in larger rooms are more likely to be selectedcompared to those in smaller rooms. Our objective is toachieve a more uniform coverage across the entire scene,avoiding overconcentration in larger open areas. Thus,we used the farthest point sampling method to sample aset of key points that sparsely span the scene. Our focusis on ensuring the trajectory covers the main open spaces,without the necessity of navigating narrow spaces such as",
  "(e) Pitch. We find that, generally, the model can achieve better performance in a look-down angle compared to a look-up angle": ". Error analysis for Grounding DINO . Similar trends are also observed in other detection models. Each row in our presentationrepresents one axis and comprises four example groups. In each group, there are two images: the left image illustrates the ground truth,highlighting the target object in magenta, while the right image shows the Grounding DINOs predictions (colored differently) for the targetobject, as indicated at the top of the first image in each group. The example groups are arranged such that, from left to right, the intensityalong the respective axis increases (e.g., progressing from zoomed in to zoomed out), and the intensity value (0-1) is shown on top of eachprediction. the gap between a cabinet and a wall. To achieve this, weperform the sampling on an eroded version of the traver-sal map. This technique effectively highlights the largeropen areas in a scene while eliminating smaller gaps andcorners. Now we have a set of candidate key points sampled in thescene, but a view from many of them may provide simi-lar information about the scene (for example, two nearbypoints in the same room). We dont need to visit both of them in the same trajectory. To ensure efficiency andavoid redundancy, we need to select a subset of thesekey points while still preserving a comprehensive viewof the scene (such as not excluding all points from a spe-cific room). Our selection process begins by assessing theunique information each key point provides, specificallythe objects visible from that point. We place a virtualcamera at each key point and rotate it 360 degrees, record-ing the angle at which the maximum number of objects in",
  ". Similar to in the main (.1), we plot the Segmentation model results for each of the five axes. AP is calculatedusing target object only": "the scene are visible. We note both the visible objects andtheir corresponding angles for each key point. Next, werandomize the order of the key points and go over eachpoint sequentially to select the points that offer additionalinformation. If a key point reveals an object not visiblefrom all previously selected points, we retain it. Other- wise, we discard it. This method results in a smaller, moreefficient subset of key points referred to as waypointsin the subsequent step which still allows a comprehen-sive observation of most objects in the scene.",
  ". We also observe a comparable pattern in the parametric evaluation conducted on real data as observed in synthetic data (main)": "shortest possible path. To achieve this, we frame the taskas a traveling salesman problem, treating the waypointsas nodes to be visited on the scene traversal graph. In thisstep, to ensure the camera maintains a safe distance fromwalls and furniture, we slightly erode the scene traversalgraph. This adjustment prevents the camera from gettingtoo close to these obstacles, ensuring smoother navigationthrough the scene. After establishing the sequence of positions in the previ-ous step, we focus on determining the cameras orienta-tion at each position. Our goal is to mimic the behaviorof a real agent exploring the scene. Therefore, while inmotion, the camera faces the direction it is moving to-wards. Additionally, at each waypoint, the camera makesa stop and turns to an optimal angle. This angle, pre-determined in an earlier step, allows the camera to cap-ture the most objects from that viewpoint. This approachserves two purposes: firstly, to ensure that the trajectoryincludes keyframes or angles for optimal object visibility,and secondly, to simulate the natural behavior of an agentpausing to observe the surroundings.",
  "C.3. Object States and Relations Prediction (mainpaper Sec. 4.3)": "Details of Generation Process.For binary object rela-tionship prediction, we synthesized 12.5k images, each an-notated with one or more of the following five labels: open,close, ontop, inside, under. For instance, an imagemight depict a toy inside an open cabinet, thus making bothinside and open labels applicable.The image sampling process is as follows: Firstly, weselect a scene and a piece of furniture to serve as the pri-mary object in the relation (e.g., a table for placing itemson top). Subsequently, we determine a plausible relation-ship related to this base object, with annotations providedvia BVS. For example, an item might be positioned ontopor under a table, but not inside it. Following this, weselect a random object to place in the scene. This object isthen integrated into the scene, employing the physical statesampling function from BVS to ensure its placement alignswith the predetermined relationship. For instance, we mightsample a cupcake and place it at a random location on topof the table. Lastly, we sample a random camera pose, en-suring the placed object is centered in the frame. We then filter out any instances where the objects of interest are notadequately visible. This procedure is repeated iteratively tocompile our final dataset.For unary object state prediction, we generated 500 im-ages that either consist of a filled or empty (not filled)container, similarly 500 images for folded. The samplingprocess for unary states is simpler we randomly samplea scene, then place a random container/cloth object in thescene, by 50% chance sample a filled or folded state for thetarget object, then also sample a random camera pose withthe target object in the center. Details of Architecture Design and Hyperparameters.Our model architecture is adapted from . Given animage input with two (or one) bounding boxes, the modelpredicts the binary spatial (or unary) relationship betweenobjects corresponding to boxes (). First, the modelutilizes a Segment Anything image encoder to extracthidden features.Subsequently, RoIAlign is appliedto the extracted features using the two (or one) boundingboxes. In the binary case, where the objective is to predictthe spatial relationship between the two objects, RoIAligneffectively captures spatial information from the represen-tationAdditional features are incorporated into the aligned rep-resentations to enhance semantic information. Unlike ,which relies on word2vec vectors and category namesfor the objects (which may not always be readily availablein the world), we opt to use the Segment Anything extractedfeature, from the cropped image encompassing the union ofthe two bounding boxes, as the additional feature. This ap-proach preserves both spatial and semantic information.The concatenated features are then fed into a trainableCNN to predict seven-way logits. To prevent overfitting, wefreeze the Segment Anything image encoder, ensuring thatthe only learnable parameters are those of the randomly ini-tialized CNN. Under a 0.3 learning rate with linear schedul-ing, the model is trained on 13.5k synthetic images only butcan achieve strong performance in the real test set (in the main paper).Lastly, we discuss the details of zero-shot CLIP baseline, which is used to mimic scenarios where syntheticdatasets are not accessible. In this scenario, we have to relyon CLIPs zero-shot capacity. Specifically, akin to our ar-",
  ". Relationship prediction model architecture used in Sec 3.3": "chitecture, the image is cropped to maximize the semanticinformation. Then the image embedding of the cropped im-ages is compared with the label text embeddings from allverbalized prompts. A verbalized prompt can take the formof <A> on top of <B> where <A> and <B> are theplaceholder for the actual object category name. Empiri-cally we find including the category name in the promptoutperforms using predicate only (e.g., only on top of).We emphasize that having prior knowledge of categorynames in advance renders this task more straightforwardand less fair to our approach where we have no assumptionon access to any category name.Shown in main Tables 4 and 5, BVS is capable ofgenerating high-quality synthetic training data by demand. Models trained on such synthetic training dataare able to capture the essence of predicate prediction andbridge the sim-to-real gap. Folded and Filled PredictionBVS also supports nu-anced unary object predicate such as folded andfilled. Models training on synthesized photo-realisticimages can transfer well to real images. We train two linearprobes on top of the EVA02 encoded representation topredict folded and filled, respectively. We manuallycollect 50 real test images for each of the two predicatesand observe that linear probes can achieve 86% and 93%real test accuracy for folded and filled, respectively."
}