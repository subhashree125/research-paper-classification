{
  "Abstract": "Point cloud is a critical 3D representation with manyemerging applications. Because of the point sparsity andirregularity, high-quality rendering of point clouds is chal-lenging and often requires complex computations to recoverthe continuous surface representation. On the other hand,to avoid visual discomfort, the motion-to-photon latencyhas to be very short, under 10 ms.Existing renderingsolutions lack in either quality or speed. To tackle thesechallenges, we present a framework that unlocks interac-tive, free-viewing and high-fidelity point cloud rendering.We train a generic neural network to estimate 3D ellip-tical Gaussians from arbitrary point clouds and use dif-ferentiable surface splatting to render smooth texture andsurface normal for arbitrary views.Our approach doesnot require per-scene optimization, and enable real-timerendering of dynamic point cloud.Experimental resultsdemonstrate the proposed solution enjoys superior visualquality and speed, as well as generalizability to differentscene content and robustness to compression artifacts. Thecode is available at",
  "Point cloud is a versatile 3D representation that can be di-rectly acquired by various sensors such as LiDAR, multi-view, or RGB-D cameras without heavy processing, thus": "enabling real-time capturing and streaming.It is alsomore flexible than polygonal mesh when representing non-manifold geometry. These benefits have led to growing de-ployment of point cloud for applications such as cultureheritage, autonomous driving, immersive visual communi-cations, and VR/AR. Its importance is also evidenced bythe MPEG point cloud compression standardization activi-ties since 2014 . However, rendering a point cloudgiven a users viewpoint is uniquely challenging: Unlikemeshes, the point representation does not provide explicitsurface information, making the rendering suffer from pointsparsity, geometric irregularity, and sensor noise . Onthe other hand, to avoid visual discomfort, experimentalstudies have demonstrated that the motion-to-photon (MTP)latency should not exceed 10 ms .These chal-lenges and demands have become the main roadblocks ofthe aforementioned point-cloud-enabled applications, espe-cially for scenarios where a dynamic point cloud needs tobe captured, streamed, and rendered in real time. Various solutions have been presented to render pointclouds as images from arbitrary viewpoints, including pointsplatting and ray tracing . However, exist-ing methods either require heavy computation (e.g., ),far exceeding the MTP threshold (see ), or gener-ate blurred images that miss details or have holes (see ). An alternative solution is cloud-based ren-dering using a powerful server in the cloud . This,however, requires the total latency including the round-triptransmission between the user and the server, rendering at",
  "arXiv:2409.16504v1 [cs.CV] 24 Sep 2024": "the server, and (de)compression of the rendered view, tobe completed within the MTP threshold, necessitating ex-tremely low-latency communication links. Even when theserver is positioned close to the network edge, the typicaldelay in todays access networks is still prohibitively largeto meet the MTP requirement.We develop a point cloud rendering framework that en-ables high visual quality, six degrees of freedom (6-DoF)viewing, and satisfies the MTP requirement with consumer-grade computers at the user end. Our solution does not re-quire per-scene neural network training nor heavy surfacereconstruction computation. Our method leverages the 3Dsurface representation using 3D elliptical Gaussians and thecorresponding differentiable 3D splatting renderer . Wetrain a light-weight 3D sparse convolutional neural networkcalled Point-to-Ellipsoid (P2ENet) to transfer each point inthe colored point cloud into an ellipsoid. The ellipsoids arethen splatted to render a frame at the most current view-point. The differentiable renderer enables us to train theP2ENet to optimize the rendering quality. The 3D Gaussianrepresentation enables high-quality rendering. The P2ENetalso derives a normal vector with each Gaussian, enables thegeneration of a normal map beyond a rendered image, and,therefore, unlocks practical applications such as relightingand meshing.We evaluate our method with both high- and low-densitypoint clouds, as well as a variety of scenes of both dynamichuman in actions and outdoor objects. Experimental resultsshow that our method can render high-quality and hole-lessimages faster than 100 FPS after an initial delay of less than30 ms. It is also robust to point cloud capturing and com-pression noise. Given the affordability of RGB-D capturingdevices, we hope the research enables the high quality real-time streaming and rendering of live captured 3D scenesfrom remote studios, and interactive VR/AR applicationsamong multiple remote participants. To this aim, we willrelease our code to the public upon acceptance. In sum-mary, we make the following main contributions: a generalizable neural network P2ENet that transformspoint clouds to 3D Gaussian representations without per-scene training;",
  ". Point Cloud Rasterization": "Rasterization is a common and efficient method for render-ing. However, since points do not naturally have spatialdimensions, they are converted to oriented disk or 3DGaussians , which are then rasterized to pixels. Gen- erally, with screen-space alpha blending by the projectedGaussian kernels, the surface splatting methods render smoother surface geometries and visually pleasingtexture when the points are dense (i.e. the surface is ade-quately sampled). Although the initially proposed 3D Gaus-sian representation in is general and can represent anarbitrary tilted ellipsoid through a non-diagonal covariancematrix, earlier methods considered only isotropic Gaussians(i.e. a diagonal covariance matrix with equal diagonal ele-ments). The variance of the isotropc Gaussian kernel waseither set to a global constant and determined by globalpoint density, or spatially varying depending on the localcovariance of point coordinates. To ensure that points in thefront surface are correctly occluding points in the back sur-face, a pixel-level visibility check is usually required .The later work in proposed an approach to estimate el-liptical parameters from a point cloud and further demon-strated that this kind of method can be used for real-timerendering of point clouds. However, due to the diversityin point clouds and existence of capturing and quantizationnoise, it is hard to determine the optimal variance or ellipti-cal parameters, leading to either holes in the rendered imageor blurry texture. This motivates our idea of using a neuralnetwork to estimate the optimal elliptical parameters andthe displacement for each point by analyzing the local andglobal structure of the colored point cloud.",
  ". Learning-based Renderer": "Neural networks are capable of learning complex map-ping from inputs to outputs.With a differentiable ren-derer , neural networks can be end-to-end trainedwith supervision on the rendered images.Recent worksleveraging a differentiable version of the 3D Gaussian splat-ting renderer have demonstrated capability of inverse ren-dering, geometry editing , and novel-view synthesisfrom multi-view images . Pursuing a differentdirection, presents a point cloud renderer based on a dif-ferentiable ray tracer. By introducing a transformer to esti-mate the intersection between a camera ray and a local setof points within a cylinder of the ray, the method achievesstate-of-the-art rendering quality and enables relighting bysimultaneously predicting the surface normal at the inter-section point. However, the ray-tracing process, which in-volves inference of a complex transformer when shadingevery pixel, is too slow for real-time rendering. Our work is inspired by 3D Gaussian Splatting(3DGS) which demonstrates that 3D Gaussians canbe used to represent smooth surface through per-scene opti-mization. However, our research is fundamentally orthogo-nal to the application scenarios of 3DGS and Neural Point-Based Graphics (NBPG) , which focus on novel viewsynthesis from multiview images and rely on these imagesas inputs to generate the point cloud. In contrast, our work is",
  ". Rendering by estimating elliptical parameters from pointcloud using sparse 3D convolutional neural network": "concerned with applications when only pre-captured pointcloud data are available and need to be streamed and ren-dered in real time. Furthermore, 3DGS requires per sceneoptimization for each individual frame. Therefore, 3DGScannot support our target application. In comparison, oncetrained, our model can be used for real-time rendering ofvarious point cloud datasets with not only humans but alsonatural scenes.",
  ". Alternative Image-Based 3D Representation": "Beyond explicit representation, such as point cloud or mesh,a 3D environment can also be represented in the imagespace, such as panoramas, light fields, or unstructuredmulti-view images.Transforming those data to a givenviewpoint requires a view synthesis approach by tacklingdis-occlusion and relighting challenges. Example solutionsinclude multi-layer image inpainting , or neural ra-diance fields . As an orthogonal scope of work, wefocus on rendering the 3D point cloud data with high qualityand speed.",
  ". Adaptive Surface Splatting Using EllipticalGaussians": "We are given color points sampled from the visible sur-face in a 3D scene. Each point consists of a 3D coordi-nate p = (x, y, z) and an RGB color c = (r, g, b). Thecollection of the points form a frame of 3D point cloudP = {(pi, ci)|i = 1, , N}, and multiple frames forma point cloud video that captures a dynamic scene. Thesepoint clouds are usually captured by RGB-D cameras, Li-DAR, or reconstructed from multi-view images, and areusually preprocessed to remove noise and outliers.Given the point cloud, our goal is to render the 3D sceneinto 2D images from arbitrary views. We mainly addresstwo technical problems. First, points are originally with-out spatial dimensions. To render smooth textured surfacefrom different views out of points, we need to convert zero-dimensional points into 3D primitives with spatial volumes.Second, since the point clouds may be unevenly spaced and contain quantization noise introduced in compression, weneed to adjust the coordinates of the primitives accordingly.Generally, we require the generated primitives to, 1) ap-proximate the surface; 2) avoid visibility leakage; 3) pro-duce smooth texture.Previous work shows that 3D elliptical Gaussians havethe capability to represent a scene and render smooth tex-ture with hole-free surface .Inspired by this, wepropose to estimate an ellipsoid (a 3D Gaussian with anarbitrary covariance matrix) from each point, serving asthe rendering primitive. For each input point ((p, c)) inthe original point cloud P, we estimate a Gaussian cen-ter offset = (x, y, z) and a covariance matrix R33, in effect transforming the point into a 3D Gaus-sian G(p + , ). Specifically, as in the covariancematrix is parameterized as = RT ST SR, where R isa rotation matrix and S = diag(x, y, z) is a diago-nal matrix. This parameterization guarantees to be pos-itive semi-definite.The rotation matrix R is calculatedfrom a quaternion q = (qw, qx, qy, qz), which is also es-timated by the neural network. The neural network alsoestimates an opacity value o for each point.Combinedwith the original color of the point, the final 3D primi-tive is < G(p + , ), o, c >, consisting of 11 parameters:x, y, z, x, y, z, qw, qx, qy, qz, o to be estimated by theneural network.We render the 3D Gaussians by splatting them onto thescreen space and then rasterize. In practice, we set a thresh-old of three times the standard deviation of the Gaussian todetermine the boundaries of the splats. In the rasterization,one pixel may be covered by multiple splats. Following, We use alpha blending to combine the colors of thesplats. Let x be the screen-space coordinate of a pixel andU = < G(pk + k, k), ok, ck > |k = 1, , K be the setof splats that cover the pixel, sorted by the depth (i.e. thez coordinate after transforming pk + k into the cameraspace). The rendered color at x is",
  ",(4)": "where fx and fy are the focal lengths of the camera, andx, y, z are the camera-space coordinate of the projectedpoint.To enable re-lighting of the rendered images, we alsoestimate the surface normal of each point nk. The surfacenormal is estimated by the same neural network as threeadditional output channels for each point. In the scenariothat requires re-lighting, we first render the surface normalto 2D pixels by subsituting ck in Eq. (1) with nk, and thenuse the rendered surface normal to calculate the shading.We then conduct late shading with a pixel shader.",
  ". Neural Elliptical Parameter Estimation": "Unlike the work in which estimates the elliptical pa-rameters from multi-view images using a per-scene opti-mized neural network, we employ a feed-forward 3D sparseconvolutional neural network to estimate the elliptical pa-rameters and surface normal from a given point cloud. Theneural network is based on Minkowski Engine , which al-lows efficient 3D voxelized convolutions on sparse data likepoint clouds. However, the input point clouds may not beoriginally voxelized. Hence, we need to voxelize the pointclouds before feeding them into the neural network. We de-sign a voxelization method that is adaptive to the densityof the point cloud and at the same time preserves the infor-mation. We first determine a scaling factor such that afterscaling, the point cloud has an average density of 1 point pervoxel. We then voxelize the scaled point cloud with a voxelsize of 1. Since there will be coordinates offset after thevoxelization, we also calculate the coordinate residuals, andmake it as part of the input feature to the neural network.This ensures that the geometric information is kept. Finallyeach point we feed into the neural network has the attributes{p = (x, y, z), c = (r, g, b), and r = (x, y, z)}, namelythe absolute coordinates, the color, and the coordinate resid-uals, respectively.Approximating the underlying surface represented bythe point cloud requires local and global information of thepoint cloud. To learn the global and local features of thepoint cloud, we use a UNet-like architecture with 3D con-volutions, where the downsampling in the encoder allowsthe neural network to extract global features from pointsthat are far away from the center point. The network archi-tecture is shown in . We use the Minkowski Engine pooling method to downsample the sparse voxel grid. Itsimply takes every 2 2 2 voxels and aggregate to onevoxel. The new voxel conceptually lies in the center of theoriginal 2 2 2 voxels and has the mean feature of thosevoxels. If some of these voxels are not occupied, only non-empty voxels will take part in the average calculation. Inour architecture, there are 3 downsampling layers in the en-coder, effectively enlarging the voxel size by a factor of 83.The upsampling in the decoder is done by transposedconvolution. In an upsampling layer, each non-empty voxelin the input will generate 2 2 2 voxels in the outputby transposed 3D convolution. Because the correspondingvoxel grid in the encoder may not be fully occupied, weprune the output voxel grid with the ground truth occupancyof the corresponding voxel grid in the encoder. This guar-antees the same geometry of two sparse voxel grid comingfrom the transposed convolution and the skip connectionfrom the encoder. Symmetrically, the decoder also has 3layers, with the last layer having the same set of points asthe original point cloud. Finally, for each occupied point inthe final layer, the network predicts the set of 3D Gaussianparameters < G(p + , ), o, c, n > from the features ofthat point in the decoder using a shared MLP.One of the advantage of using sparse 3D convolution toprocess voxelized point cloud is that it allows efficient han-dling of large point clouds with millions of point on GPU.Alternatives like DGCNN or PointNet++ involvenearest neighbor search, which is memory and time con-suming on large scale point clouds. In our experiments, weare able to process high density point clouds in real-time ona single RTX 4090 GPU.",
  ". Model Training": "We train the neural network using the THuman 2.0dataset . This dataset includes meshes of 3D capturedhuman subjects. The human subjects are captured in real-time using a multi-view system with three RGB-D cameras.After that, high-quality texture meshes are reconstructedfrom the capture and provided as assets.The training of our system requires input point cloudsand the ground truth images from arbitrary viewing direc-tions. We synthesize the training pairs from the meshes.The ground truth rendered RGB images and surface nor-mal maps are directly rasterized from the mesh with ran-dom cameras. We obtain the input point clouds for train-ing by mixing both the quantized and non-quantized pointclouds sampled from the mesh. We normalize the verticesof the training meshes to be within a bounding box of size2 2 2 centered at the origin (0, 0, 0). We then use thePoisson Disk algorithm to sample 800K points fromeach mesh. To obtain the quantized point clouds, we applya scaling factor of 512 to the points within the 2 2 2bounding box, and round the scaled coordinates into 10 bit",
  ". The UNet-like architecture with 3D convolutions. The network takes a point cloud as input and predicts the elliptical parametersand surface normal for each point": "integers. The quantized point clouds are scaled back to bealigned with the non-quantized ones, together forming thetraining inputs.To simulate real-world scenarios where the point cloudmay have sparser areas due to capturing limitation or lossycompression, we randomly downsample the point cloud fordata augmentation during training. For quantized point co-ordinate x, we get the downsampled version x by x =x/, = [0.25, 1]. For non-quantized point cloudswith N points, we randomly choose N points out of theoriginal point set, where [0.125, 1].We randomly place virtual cameras around the scene andcalculate the following loss function between the groundtruth rasterized RGB images I and normal maps n gener-ated from the mesh data, with the differential spaltting re-sults I and n, as,",
  ". Experimental Settings": "DatasetTo evaluate our methods on various contents anddifferent point cloud qualities, we conduct the experimentswith the following datasets: THuman 2.0 .The THuman 2.0 dataset containstextured meshes of human subjects, captured and recon-structed in real-time by three RGBD camaras. We trainon the training split in this dataset with 250 meshes, andevaluate on the testing split with 8 unseen subjects. 8iVFB . This is the standard testing dataset for MPEGpoint cloud compression standardization.It containshigh-quality dynamic voxelized point cloud videos of hu-man in action, with each point cloud frame containing700K to 900K points. BlendedMVS .This dataset provides texturedmeshes of outdoor scenes, captured and reconstructedfrom a multi-view system. We use this dataset to evaluatethe generalizability of our method to various content. CWIPC . This dataset provides real-time capturedraw point cloud from a multi-camera setting, specificallyfor social XR applications.Each frame contains 1Mpoints. Since the production of the point cloud does notinclude surface construction, the point clouds have dis-continuous surface and inaccurate point coordinates. Weevaluate on this dataset to show the capability of methodto be used for live streaming. Baselines for ComparisonThe main benefit of ourmethod is to jointly provide speed and fidelity. We comparewith the following methods in terms of quality and latency.Considering the total latency from the point cloud to finalimages into account, we categorize these methods into twogroups. The first group includes offline methods targetingat high fidelity without considering speed. Per-scene Optimized Surface Splatting .Thismethod builds a collection of 3D Gaussians from mul-tiview images and then rasterize the Gaussians to renderarbitrary views. Note that this method requires per-sceneoptimization with known multi-view images. We gener-ate these multi-view supervision images by rendering 144images from the mesh with a virtual camera following aspiral trajectory. Because of the per-scene optimizationrequirement, it cannot be applied for real-time rendering.We recognize that this method is developed for 3D scenereconstruction from multi-view images and then novel-view generation, rather then for rendering a point cloud.We use it as a benchmark for the best possible quality wemay obtain by building 3D Gaussians from a point cloud. Pointersect .A state-of-the-art point cloud render-ing method utilizing a transformer to calculate ray-point-cloud intersection for ray tracing. Despite the good qual-ity, ray tracing is slow and hence cannot be used for real-time rendering. Poisson surface reconstruction . Rendering a pointcloud by first reconstructing a water-tight triangle meshand then rasterize.The second group includes real-time methods that can ren-der within the MTP constraint: OpenGL. Each point is converted to a 1 pixel wide square",
  "in the screen space and rasterized to pixels. We use thepackaged implementation provided in Open3D": "Surface splatting using the same isotropic Gaussianrepresenting each point. We use the same splatting ren-derer as our proposed method but we use a diagonal co-variance matrix with isotropic variance for all three axes,determined globally using average point density.For datasets that have mesh representations, we mea-sure the fidelity of the rendered images from the pointcloud compared to the ground truth obtained from the orig-inal scene mesh, using quality metrics PSNR and MS-SSIM .In addition to the original point cloud, wealso evaluate these methods in the scenario where the pointclouds are lossily compressed for streaming. We use thestandard G-PCC to compress the point clouds at differ-ent bit-rate. For other datasets without meshes, we presentvisual comparisons for selected point clouds.",
  ". Fidelity, Latency, and Robustness to Compres-sion Artifacts": "We first evaluate the methods capability of rendering high-quality point clouds sampled from a smooth surface. Fromthe testing split of the THuman 2.0 dataset, we create twocategories of point cloud for each mesh: 1) High-qualitynon-quantized point cloud sampled from the mesh surfaceusing Poisson Disk . We sample 800K points fromeach individual mesh asset; 2) Compact point cloud uni-formly sampled from the mesh, with 280K points on av-erage, with coordinates quantized to a 10-bit depth. (seemore details in Sec. 3.3). Quantization and down-samplingof points are common tools used for compression of pointcloud data codec , necessary for efficient point clouddata storage and delivery.We render point clouds from 12 different viewing angles,forming a circle trajectory surrounding the subject. We re-port the PSNR and MS-SSIM between the rendered viewsand the ground truth views rasterized from the meshes. Forthis evaluation, we render images with a 512 512 resolu-tion.Usually a point cloud rendering method consists of atwo-stage procedure: 1) constructing primitives from thepoint cloud (preprocess), and 2) render from a camera viewby rasterization. In real-time video streaming applications,it is important for the method to finish preprocessing fasterthan the content frame rate, in addition to have a renderingtime that is within the MTP threshold. Hence we also com-pare these two latencies among different methods. The la-tencies are measured on a computer with an Intel i7-9700KCPU and an NVIDIA RTX 4090 GPU.As shown in , our method demonstrates higherrendering quality than all compared methods, with morethan 4dB improvement in PSNR than real-time baselines(Global parameters and OpenGL), for both high-",
  ". Average PSNR of rendered views from THuman 2.0compact point clouds compressed to different bit-rates by G-PCC": "quality and compact data. Our method is also substan-tially more robust to quantization noise and reduced pointdensity, compared to all other methods. Whereas the PSNRonly reduced from 34.1 dB for the unquantized 800K datato 33.8 dB for the quantized 280K data, some of the base-line methods suffer a reduction of 2 to 3 dB. We attribute therobustness of our method to training of the neural networkusing both unquantized and quantized data at randomizedpoint densities. The model can generate accurate Gaussianparameters even when the point cloud is compressed.We visualize the rendering results for a sample compactpoint cloud in . As shown, despite the high ren-dering latency, Poisson mesh reconstruction still has notice-able blurring artifacts, and Pointersect produces noisy edgesdue to inaccurate intersection calculation. Among the real-time methods, OpenGL fails to reproduce hole-less surfaceand leads to visible holes and gaps. 3D Gaussians withglobal parameters produce inaccurate geometry and noisyedges. Our method generally produces better visual qual-ity than both offline and real-time methods. Please referto the supplementary material for results with high-qualitynon-quantized point clouds.Our method also enjoys very fast rendering speed oncethe Gaussian parameters are inferenced using the P2ENet,comparable to the real-time baselines, shorter than the MTPthreshold.The preprocessing time (for inferencing theGaussian parameters) is under 30 ms, which should be ac-ceptable for most applications. Therefore, our renderingmethod enables the streaming and free viewing of a 30 fpspoint cloud video after an initial delay of 30 ms (consideringonly the preprocessing delay). Given the rendering time ofunder 1 ms, the proposed method enables a display framerate of more than 100 fps. Since the rendering resolutionis irrelevant to the preprocessing, our method is capable ofrendering at even higher resolution at high frame-rate, e.g.1K1K at 1.1 ms and 2K2K at 2.0 ms, per frame. Notethat such parallel inference-rendering framework is com-patible with most consumer grade computers with an in-tegrated GPU on the CPU chip and a discrete GPU in thegraphics card. The discrete GPU can be used for inference,while the integrated GPU can be used for rendering.To more thoroughly evaluate the robustness of the ren- . Average PSNR (dB) and MS-SSIM scores of rendered views from original point clouds in the THuman 2.0 testing set. We alsoreport preprocessing (P) and rendering (rasterization or ray-tracing) (R) latencies.",
  "Ground Truth Mesh": ". Comparison of rendering results of a point cloud (1.7M points) in the BlendedMVS dataset. Quality metrics are shown informat (PSNR, MS-SSIM) and calculated from the rasterization results of the ground truth mesh. The insets visualize local details with 3zooming. dering methods to compression, we use the standard pointcloud codec G-PCC to further compress the Com-pact versions of the point clouds at different bit-rates, anduse different renderers to render the decoded point clouds.Note that G-PCC compresses each frame independentlyand achieves different rates by rescaling and quantizing thepoint coordinates (which has the effect of reducing the pointdensity) plus additional color quantization. As shown in, our method consistently achieves higher render-ing quality at different bit-rate levels.The robustness ofour rendering method to compression, in addition to its fastspeed, makes it more suitable for streaming applications.",
  ". Generalizability": "Although our model is trained on the THuman dataset withonly high-quality 3D human scans, it generalize to differ-ent types of point clouds including outdoor scenes in theBlendedMVS dataset and noisy point clouds in the CWIPCdataset. We show rendering results with the BlendedMVSdataset in . Please refer to the supplementary mate-rial for results with the CWIPC dataset. Despite the discrep-",
  "The method does not directly provide surface normal. In the bracketis the time for point cloud normal estimation by Open3D on CPU.2Implemented on CPU by Open3D": "ancy in contents and capturing quality between the testedpoint clouds and training samples, our model still pro-duces comparable quality among the best rendering meth-ods. However, since our training data do not simulate mis-alignment capturing artifacts and severe noise, it has limita-tions in handling these quality degradations. We thereforeleave it for an aspect of future work.",
  ". Limitations and Future Work": "Our pre-trained 3D sparse CNN (.2) allows us toestimate the elliptical Gaussian attributes from point clouds,without the per-scene training in current view synthesismethods . It also ensures consistent real-time speed.However, different scene content types also exhibit variedimage quality metrics. For example, the human-body-basedpoint clouds show elevated quality than natural scenes andwe observe that our method tends to produce over smoothedimages when the point clouds are sparse. Our model alsoneeds to be further finetuned for scenes with a particularirregularity in point density caused by specific capturingsetup for better robustness. On the other hand, since ourmethod is optimized to produce high quality renders fromeach frame of the point cloud video, and does not specifi-",
  "cally optimize for temporal consistency, the temporal jitter-ing caused by point cloud capturing can be still observed inthe rendered video": "In the future, we will invest data augmentation ap-proaches that balances various scene types and noise levels.We also plan to include temporal coherence constraints inmodel training, and further make the neural network gener-ate denser 3D Gaussians for areas of complex texture, to im-prove render quality in both spatial and temporal domains.",
  ". Conclusion": "In this paper, we present an end-to-end and learning-basedframework that addresses the challenging dilemma betweenspeed and quality in point cloud rendering. To this end, weleveraged a differentiable, splatting-in-the-loop approachthat can generate fine-grained geometric and textural detailsthrough a learnt 3D sparse neural network. Extensive com-parisons with a broad spectrum of datasets and alternativesolutions demonstrated the effectiveness of the method. Wehope the research to contribute a new building block for en- abling the flexible point cloud as a promising medium to-ward high-fidelity interactive computer graphics, VR/AR,and immersive visual communications, while passing therequired MTP for user-centric applications.",
  "Giang Bui, Truc Le, Brittany Morago, and Ye Duan. Point-based rendering enhancement via deep learning. The VisualComputer, 34:829841, 2018. 1": "Jen-Hao Rick Chang,Wei-Yu Chen,Anurag Ranjan,Kwang Moo Yi, and Oncel Tuzel.Pointersect: Neuralrendering with cloud-ray intersection.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 83598369, 2023. 1, 2, 5, 3, 4, 6, 7, 8 Anthony Chen, Shiwen Mao, Zhu Li, Minrui Xu, HongliangZhang, Dusit Niyato, and Zhu Han. An introduction to pointcloud compression standards. GetMobile: Mobile Comput-ing and Communications, 27(1):1117, 2023. 1",
  "H Childs, T Kuhlen, and F Marton. Auto splats: Dynamicpoint cloud visualization on the gpu. In Proc. EurographicsSymp. Parallel Graph. Vis., pages 110, 2012. 2": "Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4dspatio-temporal convnets: Minkowski convolutional neuralnetworks. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, pages 30753084,2019. 4 EugenedEon,HarrisonBob,TaosMyers,andPhilip A. Chou.8i voxelized full bodies - a vox-elized point cloud dataset.In ISO/IEC JTC1/SC29JointWG11/WG1(MPEG/JPEG)inputdocumentWG11M40059/WG1M74006, 2017. 5, 1 Peter Eisert, Oliver Schreer, Ingo Feldmann, CorneliusHellge, and Anna Hilsmann. Volumetric videoacquisition,interaction, streaming and rendering.In Immersive VideoTechnologies, pages 289326. Elsevier, 2023. 1 Danillo Graziosi, Ohji Nakagami, Shinroku Kuma, Alexan-dre Zaghetto, Teruhiko Suzuki, and Ali Tabatabai.Anoverview of ongoing point cloud compression standardiza-tion activities: Video-based (v-pcc) and geometry-based (g-pcc). APSIPA Transactions on Signal and Information Pro-cessing, 9:e13, 2020. 6 Adam Grzelka, Adrian Dziembowski, Dawid Mieloch, Ol-gierd Stankiewicz, Jakub Stankowski, and Marek Domanski.Impact of video streaming delay on user experience withhead-mounted displays. In 2019 Picture Coding Symposium(PCS), pages 15. IEEE, 2019. 1 Anton S Kaplanyan, Anton Sochenov, Thomas Leimkuhler,Mikhail Okunev, Todd Goodall, and Gizem Rufo.Deep-fovea:Neural reconstruction for foveated rendering andvideo compression using learned statistics of natural videos.ACM Transactions on Graphics (TOG), 38(6):113, 2019. 1",
  "Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler,and George Drettakis.3d gaussian splatting for real-timeradiance field rendering. ACM Transactions on Graphics, 42(4), 2023. 2, 3, 4, 5, 7": "Kai-En Lin, Zexiang Xu, Ben Mildenhall, Pratul P Srini-vasan, Yannick Hold-Geoffroy, Stephen DiVerdi, Qi Sun,Kalyan Sunkavalli, and Ravi Ramamoorthi.Deep multidepth panoramas for view synthesis. In European Confer-ence on Computer Vision, pages 328344. Springer, 2020.3 Katerina Mania, Bernard D Adelstein, Stephen R Ellis, andMichael I Hill. Perceptual sensitivity to head tracking latencyin virtual environments with varying degrees of scene com-plexity. In Proceedings of the 1st Symposium on Applied Per-ception in Graphics and Visualization, pages 3947, 2004. 1 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. Communications of the ACM, 65(1):99106, 2021. 2,3, 7",
  "Ohji Nakagami, Sebastien Lasserre, Sugio Toshiyasu, andMarius Preda. White paper on g-pcc. In ISO/IEC JTC 1/SC29/AG 03 N0111, 2023. 1": "Hanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, andMarkus Gross. Surfels: Surface elements as rendering primi-tives. In Proceedings of the 27th annual conference on Com-puter graphics and interactive techniques, pages 335342,2000. 1, 2 Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas JGuibas. Pointnet++: Deep hierarchical feature learning onpoint sets in a metric space. Advances in neural informationprocessing systems, 30, 2017. 4 Ignacio Reimat, Evangelos Alexiou, Jack Jansen, Irene Vi-ola, Shishir Subramanyam, and Pablo Cesar.Cwipc-sxr:Point cloud dynamic human dataset for social xr. In Pro-ceedings of the 12th ACM Multimedia Systems Conference,pages 300306, 2021. 5",
  "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,Michael M Bronstein, and Justin M Solomon.Dynamicgraph cnn for learning on point clouds. ACM Transactionson Graphics (tog), 38(5):112, 2019. 4": "Yujie Wang, Praneeth Chakravarthula, Qi Sun, and Bao-quan Chen. Joint neural phase retrieval and compression forenergy- and computation-efficient holography on the edge.ACM Trans. Graph., 41(4), 2022. 1 Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Mul-tiscale structural similarity for image quality assessment. InThe Thrity-Seventh Asilomar Conference on Signals, Systems& Computers, 2003, pages 13981402. Ieee, 2003. 6 Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo networks. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 17901799, 2020. 5 Wang Yifan, Felice Serena, Shihao Wu, Cengiz Oztireli,and Olga Sorkine-Hornung. Differentiable surface splattingfor point-based geometry processing. ACM Transactions onGraphics (TOG), 38(6):114, 2019. 2 Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-hai Dai, and Yebin Liu. Function4d: Real-time human vol-umetric capture from very sparse consumer rgbd sensors. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 57465756, 2021. 4, 5",
  "Experiments on Texture Adaptability": "In addition to the benefit of rendering more accurate sur-face contour by allowing 3D Gaussians to have ellipticalshapes, since the model also takes point color as input, ithas the ability to adapt to local texture edges. We comparethe rendering results from learned and globally set Gaussianparameters to elucidate this advantage. For the globally setparameters, we choose three different values of Gaussianstandard deviation relative to d, the average distance be-tween points in the point cloud. As shown in , us-ing a lower global can lead to visible holes, while settinga larger result in inaccurate edges. The learned model caninstead produce spatially varying 3D Gaussian parametersincluding center translations to provide clearer edges andsmooth surface at the same time.",
  "Additional Rendering Results": "We visualize more rendering results with high-quality(800K points) point clouds from the THuman 2.0 datasetin and 10, outdoor scenes (1.5M points) from theBlendedMVS dataset in , and noisy raw captures(1M points) from the CWIPC dataset in and Fig-ure 13.We also evaluate our method in dynamic quality by ren-dering the point cloud video sequences from the 8iVFBdatabase . We use a video framebuffer at the resolutionof 1024 1024 and the frame rate at 30 fps, in accordanceto the original point cloud framerate. Please refer to thesupplementary video for the results. As shown, our methodcan render high-quality videos without visible holes whichare observed with the standard OpenGL renderer. There-fore, our method is more suitable for rendering high-qualitypoint cloud videos in real-time applications."
}