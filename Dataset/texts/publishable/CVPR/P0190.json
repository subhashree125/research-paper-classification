{
  "Abstract": "Music is a universal language that can communicateemotions and feelings.It forms an essential part of thewhole spectrum of creative media, ranging from movies tosocial media posts. Machine learning models that can syn-thesize music are predominantly conditioned on textual de-scriptions of it. Inspired by how musicians compose musicnot just from a movie script, but also through visualizations,we propose MELFUSION, a model that can effectively usecues from a textual description and the corresponding im-age to synthesize music. MELFUSION is a text-to-musicdiffusion model with a novel visual synapse, which effec-tively infuses the semantics from the visual modality into thegenerated music. To facilitate research in this area, we in-troduce a new dataset MeLBench, and propose a new eval-uation metric IMSM. Our exhaustive experimental evalua-tion suggests that adding visual information to the musicsynthesis pipeline significantly improves the quality of gen-erated music, measured both objectively and subjectively,with a relative gain of up to 67.98% on the FAD score. Wehope that our work will gather attention to this pragmatic,yet relatively under-explored research area.",
  ". Introduction": "Music is an essential tool for creative professionals and con-tent creators. It can complement and set the mood for anaccompanying still image, animation, video, or even textdescriptions while creating a social media post. Findingmusic that matches a specific setting, can indeed be an ar-duous task. A conditional music generation approach, thatcan synthesize a music track by analyzing the visual contentand the textual description can find a wide range of practical",
  "Long Caption": "A tranquil scenery is captured with the view of thenight sky just before sunrise. The sky consists of dynamic spiralling clouds which symbolizes movement and aliveness. The stars are bright and prominent with strokes of yellows and whites represent a vivid yet peaceful moment.The villagein the scene has houses whose windows emit warm and glowing light, giving a contrast to the cool, celestial tones of sky depicting pleasant emotions.OVL: 83.86",
  "An": "AlternateApproach Text-to-Music .We present MELFUSION, a music diffusion modelequipped with a novel visual synapse, that can effectively in-fuse image semantics into a text-to-music diffusion model. Thistask indeed requires a detailed understanding of the concepts inthe image. An alternate approach like using a caption generator toconvert image to text space to be further used with existing text-to-music methods leads to a sub-optimal overall audio quality (OVL)score. Our approach can knit together complementary informationfrom both modalities to synthesize high-quality music. applications in various fields including social media.Inspired by the progress in generative modeling of im-ages, music generation has also garnered significant atten-tion from the community . Recently, Agostinelliet al. , Copet et al. proposed conditioning in the formof melody or humming. While Sheffer and Adi pursueimage-guided audio generation. Despite these efforts, mu-sic generation conditioned on multiple modalities like textand image, is largely uncharted.Images are more expressive than text-only infor-mation and capture more fine-grained semantic informationabout various visual aspects. For example, as depicted inFig 1, to generate a musical track that goes well with agiven image, without indeed using it, one has to make thetedious effort of producing long, descriptive captions (eithergenerated by an image captioning model or human annota-",
  "arXiv:2406.04673v1 [cs.CV] 7 Jun 2024": "tors) before employing a typical text-to-music generationmodel. Moreover, the model has to be supplied with criticalattributes like tranquil, aliveness etc (highlighted in fig-ure) to aptly capture the essence of the image. This posesa major bottleneck in the scalability of such systems espe-cially for social media content creators and necessitates di-rect image conditioning with textual control in music gen-eration.Music is indeed different from generic audio. Music con-tains an arrangement of elements structured to form a coher-ent and complete entity. These musical elements includemelody, harmony, rhythm, dynamics, and form .Unlike audio, music contains harmonies from different in-struments forming intricate structures. Prior studies show that the human brain is extremelysensitive to disharmony. As a result, the margin of errorespecially in producing musical pieces is low compared togeneric audio tracks. This makes music generation a hardertask as the model should be equipped to control the fine-grained nuances of a composition involving melody, the in-terplay of the instruments, and genre.An alternative to generating music would be to retrievethem. Retrieval-based systems struggle to matchthe right track for a given input prompt thereby limitingtheir practical applicability in open-world scenarios primar-ily because (a) they tend to search from a pre-existing col-lection of tracks and (b) finding the correct association be-tween the input prompt and the audio track can be challeng-ing. The problem is inherently complex due to the mul-tifaceted nature of music and the abstract associations be-tween auditory experiences and other sensory modalities.To overcome these shortcomings, we introduce the firstmusic generation model that can be conditioned on imageand text instruction. We observe that the features from apre-trained text-to-image diffusion model that consumes theDDIM-inverted latent of the image can guide a text-to-audiodiffusion model. Our key novelty is to facilitate this infor-mation exchange by incorporating a visual synapse to thetext-to-music model, which includes a set of parameters thatlearn to combine the signals from both modalities.We summarise our main contributions below:(1) We formalize a novel task of generating music thatis consistent with a reference image and an associated textprompt.(2) We present MELFUSION, a novel diffusion modelthat can address this pragmatic task.(3) We introduce MeLBench dataset comprising 11,250image, text, music triplets. To the best of our knowledge,this is the largest collection of these three modalities. Fur-ther, we extend the MusicCaps dataset by supplement-ing the text, and music pairs with suitable images extractedfrom corresponding YouTube videos or the web.(4) In order to quantitatively establish the correspon- dence between the image-music pairs we propose a newmetric IMSM. We demonstrate that the score follows hu-man perception closely, through a user study.(5) Finally, our exhaustive experimental results revealthat our approach outperforms existing text-to-music gener-ation pipelines on both subjective as well as objective eval-uation with a relative gain of up to 67.98% on FAD score,thereby setting a new benchmark for multi-modal musicsynthesis.",
  ". Related Works": "Music Generation Approaches:Music generation hasgarnered significant attention for a considerable amount oftime. While some approaches deploy GANsto tackle this task, Ycart et al. introduced recurrentneural networks to model polyphonic music. Bassan et al. proposed an unsupervised segmentation using ensembletemporal prediction errors. Jukebox tackles the longcontext of raw audio using a multiscale VQ-VAE to com-press it to discrete codes, modeling those using autoregres-sive Transformers. Another stream of work thatpredicts the MIDI notes to produce music has gained popu-larity in this space. However, the scope of these approachesis relatively limited as they need additional decoders to pro-duce the musical pieces from the notations.MusicLM generates high-fidelity music from textdescriptions by casting the process of conditional musicgeneration as a hierarchical sequence-to-sequence model-ing task. Mubert is an API-based service that employsa Transformer backbone. The encoded prompt is used tomatch the music tags and the one with the highest similar-ity is used to query the audio generation API. MusicGen comprises a single-stage transformer LM together with ef-ficient token interleaving patterns. This eliminates the needfor hierarchical upsampling. Despite significant progress,none of these approaches utilize the semantic informationof images to condition the audio generation.Diffusion Models for Music Generation: With the pro-lific success of diffusion models in conditional image gen-eration, there have been recent efforts in music generationusing them. Riffusion base their algorithm on fine-tuning a stable diffusion model on mel-spectrogramsof music pieces from a paired music-text dataset. This isone of the first text-to-music generation methods. Mousai is a cascading two-stage latent diffusion model that isequipped to produce long-duration high-quality stereo mu-sic. Noise2Music introduced a series of diffusion mod-els, a generator, and a cascade model. The former generatesan intermediate representation conditioned on text, whilethe later can produce audio conditioned on the intermediaterepresentation of the text. MeLoDy pursues an LM-guided diffusion model by reducing the forward pass bot-tleneck and applies a novel dual-path diffusion mode. We find that the visual guidance that is incorporated into our ap-proach significantly enhances the music generation qualitywhen compared to all these approaches. We elaborate thisfurther in Sec. 4.4.Diffusion Models for Audio Generation: Diffusion-basedmethods achieve remarkable results inspeech synthesis too.FastDiff deploys time-awarelocation-variable convolutions of diverse receptive field pat-terns to efficiently model long-term time dependencies withadaptive conditions. AudioLDM is a text-to-audio sys-tem that is built on a latent space to learn continuous audiorepresentations from contrastive language-audio pretrain-ing (CLAP) embeddings. Ghosal et al. simplifies thearchitecture of AudioLDM, and uses FLAN-T5 as thetext encoder. Another line of work involves text-conditional discrete diffusion models to generate discretetokens as a representation for spectrograms. However, thequality of the sound produced by such methods leaves roomfor improvements in terms of both subjective and objectivequalities, thereby limiting their practical usability. In con-trast to these approaches, our method generates music sam-ples conditioned on visual and textual signals.",
  ". Synthesizing Music from Image and Text": "We propose to learn a conditional distribution M(w|I, Y ),that can generate music waveforms w from an image I anda paired textual description Y . We materialize M as MEL-FUSION, a diffusion model that can succinctly interleave thesemantic cues from the image and textual modality whilegenerating acoustically pleasing music. provides an overview of our approach.On ahigh level, our novel methodology consists of two sub-components: 1) an approach to extract relevant visual in-formation from the image conditioning I and 2) a methodto induce this conditioning into the text-to-music generativemodel, in a parameter efficient way. We describe each ofthese in the subsequent subsections.",
  ". Extracting Visual Guidance": "Latent diffusion models (LDMs) for text-to-image genera-tion have had phenomenal success in generating high-quality images that are well-grounded in their textual condi-tioning. We hypothesize that the latent representations andtheir transformations encode rich semantic knowledge, thatcan guide our audio diffusion model. In our exploration,we make use of a pre-trained Stable Diffusion model .It contains a VQ-VAE for encoding and decoding theimage to the latent space, a text encoder, and a UNet that carries out the diffusion process on the latent.TheUNet contains an encoder, a bottleneck layer, and a decoder.Each encoder and the decoder further contain a set of blockswith cross-attention layers, self-attention layers, and convo-lutional layers. Given any intermediate latent image feature",
  "V ,(1)": "where dk is the dimension of the query and key features.During cross-attention, the key and value matrices operateon the external text conditioning c Rsdk: K = W kc,V= W vc. Here, W q, W k and W v are the attentionweight matrices that transform either the image features ortext conditions into the output of each block.We want to transfer over the semantic information thatis present within these attention layers corresponding to theimage I into the music LDM. For this, we first invert Iinto the latent space using DDIM Inversion to get zIT .This will guarantee that we will be able to generate I fromzIT . Next, we do the reverse diffusion steps using a pre-trained text-to-image LDM starting from zIT and save theself-attention features K = W kf, V = W vf, to be in-jected into the music LDM. The intuition behind leverag-ing the self-attention features is that they control the fea-ture transformations responsible for generating the visualsemantics of the image.This is mathematically evidentfrom Eq. (1). In the subsequent section, we elaborate onhow we construct the synapse that can transfer the guid-ance information from I to the music-diffusion model.",
  ". Text-to-Music LDM with Visual Synapse": "Inspired by recent text-to-audio generation ap-proaches, our text-to-music model is also formulated as a la-tent diffusion model. During training, the music waveformw is first converted to a spectrogram s REF , which isa visual representation obtained via Fourier Transformationon w. E and F denote the number of time slots and fre-quency slots respectively. Then we encode S using Audio-VAE to get a latent representation zM1 RCE/rF/r,where C is the number of channels and r is the compressionlevel.The forward diffusion process involves corrupting zM1using a Markovian noise process q, which gradually addsnoise to zM1through zMT over T steps with the followingGaussian function:",
  "Encoder": "SA feats from layer of CA feats from layer of CACross-Attn SASelf-Attn . Our approach MELFUSION generates music waveform w conditioned on an image I and a given textual instruction Y . Visualsemantics from I is instilled into a text-to-music diffusion model (bottom green box) using a pre-trained and frozen text-to-image diffusionmodel (top blue box). The image I is first DDIM inverted into a noisy latent zIT . The self-attention features from the decoder layers ofthe text-to-image LDM that consumes zIT is infused into the cross-attention features of text-to-music LDM decoder layers, modulated bylearned parameters. This fusion operation that happens in the decoder (green stripes) is detailed on the right side of the figure. The musicencoder projects the spectrogram representation of the music to the latent space, and the music decoder retrieves back the spectrograms.Finally, a vocoder generates the waveform w from the spectrograms. Please refer to Sec. 3 for more details. In the reverse diffusion process, an LDM (, , ) (im-plemented as a UNet), learns to de-noise zMT N(0, I) torecover zM1 . The architecture of the UNet is kept exactlysimilar to the text-to-image UNet described in Sec. 3.1. Toincorporate the additional guidance from image condition-ing the cross-attention key and value features KMland V Mlin each of the decoder layer l of the UNet is modified as fol-lows:",
  "V Ml= lV Il + (1 l)V Ml,(6)": "where KIl and V Il are the self-attention features for the cor-responding layer l of the image conditioning LDM fromSec. 3.1. Most importantly, the convex combination be-tween these features is modulated by learned layer spe-cific parameters. We find that this simple formulationelegantly incorporates the image guidance into the text-to-music diffusion model without hampering its expressivity.As the parameters facilitate the information exchange be-tween the text-to-audio and text-to-image diffusion models,analogous to how a synapse in a nervous system facilitatesthe transfer of electrical and chemical signals between neu-rons, we refer to this handshake as the visual synapse of atext-to-music LDM.Finally, the parameters of the LDM and the parame-ters are trained end-to-end with the following loss function:",
  ": return w": "channel through which we can guide the text-to-music dif-fusion model toward the semantic concepts contained in thecorresponding image conditioning. This synapse is de-tailed in Line 7 to Line 10. The rest of the algorithm followsthe standard LDM training flow.During inference, we make use of the trained text-to-image and text-to-music diffusion models, along with thelearned parameters.As seen in Lines 8 and 9 in Al-gorithm 2, the cross-attention features of the text-to-musicLDM decoder are updated to incorporate the visual con-ditioning in each denoising step.Once the denoising(Line 10) is complete, the latent representation is projectedback into a spectrogram using the decoder of Audio VAE, and then the waveform is generated using HiFi-GANvocoder in Lines 11 and 12 respectively.",
  ". Datasets": "To the best of our knowledge, there is no publicly avail-able dataset that contains the Image, Text, Music tripletsthat are required to train and evaluate MELFUSION. Wecollect a new dataset MeLBench, which contains 11,250manually annotated triplets of Image, Text, Music. Fur-ther, we extend the MusicCaps dataset which containsText, Music pairs by adding the corresponding image.",
  ". Some image and text pairs from MeLBench. We includemore examples in the Appendix": "MeLBench: We hired 18 professional annotators to find10-second snippets of YouTube videos corresponding to 15predefined genres.The annotators are trained musicianswith at least 5 years of practice. For each of these videos,they were asked to provide (a) a free-form text descriptionfor up to three sentences, expressing the composition and(b) any other music-related details such as describing thegenre, mood, tempo, singer voices, instrumentation, disso-nances, rhythm, etc. A carefully selected frame and mu-sic from the snippet along with text description from an-notators forms Image, Text, Music triplets. We performstrict sanity checks to ensure the quality of these tripletsin MeLBench. shows some image and text samplesfrom the dataset and shows the distribution of differ-ent genres in MeLBench. Before annotating YouTube snip-pets (containing music-albums, art-performance, ensemblesetc.), they were asked to check for complementary rele-vance between visuals and music.Further, we performmanual validation to filter lower-quality samples. We in-clude more examples and more statistics of the dataset inthe Appendix.Extended MusicCaps: MusicCaps is a subset of theAudioSet dataset, which contains music and a cor-responding textual description of the same. We carefullychoose two images from the web or YouTube that can go",
  ". Evaluation Metrics": "We use objective evaluation and human subjective evalua-tion metrics to measure the efficacy of MELFUSION.4.2.1Objective Evaluation: Following previous works, Frechet Audio Distance (FAD), Frechet Dis-tance (FD) and KL Divergence scores are used for objec-tive evaluation.FAD is a perceptual metric that isadapted from Frechet Inception Distance (FID) for the au-dio domain. It uses a VGG-like backbone for featureextraction. FD is similar to FAD but uses PANNs as thefeature extractor. Unlike reference-based metrics, FAD andFD measure the distance between the generated audio dis-tribution and the real audio distribution without using anyreference audio samples. On the other hand, KL Diver-gence is a reference-dependent metric that computesthe divergence between the distributions of the original andgenerated audio samples based on the labels generated bya pre-trained classifier. While FAD is more related to hu-man perception, KL Divergence captures the similaritiesbetween the original and generated audio signals based onbroad concepts present in them.FAD, FD, and KL Divergence score captures the good-ness of generated music, while it doesnt measure whetherthe generated music is consistent with the image condition-ing. We identify this as a gap and propose IMSM metric.Image Music Similarity Metric (IMSM): CLIP score isone of the widely used metrics for measuring the similaritybetween an image and a corresponding textual description.N pairs of images and texts are passed through respectiveencoders (pre-trained using CLIP loss ) to obtain cor-responding feature embeddings which are used to computeCLIP score matrix ACLIP RNN. In a very similar fash-ion, CLAP scores are computed amongst N audio-text pairsyielding CLAP score matrix ACLAP RNN . It isworth noting that in both the matrices the columns repre-sent text modality. This motivates us to develop a metricIMSM, which is a measure of the perceptual similarity be-tween given image-music pairs bridged by the text modal-ity. In particular, we use CLIP image and text encoderswhich are contrastively aligned to compute the imageand text feature embeddings. As a second step, we lever-age language as the bridging modality by freezing the CLIPtext encoder and aligning the music (audio) encoder viacontrastive training . Finally, for Image, Text, Musicpairs we obtain IMSM by suitably combining ACLIP andACLAP using the given mathematical expression:",
  "AIMSM = ACLIP ATCLAP(8)": "4.2.2 Subjective Evaluation: Following earlier works intext-to-audio generation , we use overall audioquality (OVL) and relevance to image-text inputs (REL) toanalyze the results of our subjective user study involving75 participants. They were presented with 100 randomlygenerated samples from MELFUSION. Each of the metrics(OVL and REL) is a score between 1-100 with 1 being thelowest. For the OVL score, the users are asked to assigna score based on how perceptually realistic the generatedaudio is, while the REL score requires them to carefullyexamine the image and the text prompts before providinga rating based on their relevance with the generated music.We add more details of the user study in the Appendix.",
  ". Baseline Methods": "We compare MELFUSION against strong baselines to testits mettle. To the best of our knowledge, there doesnt exista music diffusion model that is conditioned on visual andtextual modality. Hence, we introduce two baselines: 1)caption the image with Instruct BLIP and then pass italong with the caption to MusicLM . We call this base-line MusicLM + InstructBLIP. 2) we adapt a recent text-to-audio diffusion model TANGO into our setting, asexplained next. We call its modified version TANGO++.Further, we compare ourselves with 7 other text-to-musicapproaches. We elaborate on them below:TANGO++:TANGO is a powerful text-to-audiomodel based on LDMs. They condition the diffusion modelon text embeddings from FLAN-T5 text encoder ztext.To facilitate joint conditioning from text and image I, weembed I to the latent space as zimage using a ViT based CLIP encoder, and align them together through anImage-Text Contrastive loss. Once they are aligned, boththe embeddings are fused and the LDM is jointly condi-tioned.Text-to-Music Baselines: To bring out the utility of con-ditioning on both visual and textual modality, we compareMELFUSION with seven other text-to-music methods too:Riffusion , Mubert , MusicLM , Mousai ,Noise2Music , MeLoDy and MusicGen . Weprovide more details of each of these in the Appendix.",
  ". Results": "We present exhaustive objective and subjective comparisonof MELFUSION with the baseline approaches in Tab. 1.When compared with text-to-music approaches in the firstsection of the table, our results show the significant utilityof adding extra visual conditioning on the generations. Thefine-grained contextual information from visual modality isable to supplement the information from the correspond-ing text, thereby enhancing the quality of music genera-tion. Further, MELFUSION is able to consistently outper-form MusicLM + InstructBLIP and TANGO++ (which has",
  "MELFUSION- MusicGen--+67.05%+27.64%--+3.84%+3.29%+67.98%+40.49%+13.17%-+4.53%+4.97%": ". Our proposed approach MELFUSION offers significant gains over state-of-the-art text-to-music methods (first section), and ouradapted text-and-image conditioned baselines (second section) across multiple objective and subjective metrics on two datasets. IMSM isapplicable only when the model is conditioned on visual modality. We skip comparison with MuBERT, Noise2Music, and MeLoDy onMeLBench dataset as their codebases are not public. Please refer to Sec. 4.4 for more details. similar conditioning as ours). This highlights the efficacyof our visual synapse, which infuses the right amount ofvisual conditioning to enable the model to synthesize per-ceptually congruent music tracks. Captions from Instruct-BLIP are superfluous and vague when compared to expert-annotated, high-quality MusicCaps captions on which Mu-sicLM is trained. This distributional shift leads to a per-formance drop as shown in the table. TANGO++ uses con-trastive loss to align CLIP image features and FLAN-T5 textfeatures, further, we use simple addition for joint condition-ing these design choices can be sub-optimal.Our subjective human evaluation in Tab. 1 also suggeststhat conditioning the music generation on both visual andtextual modality improves its perceptual quality.",
  ". Discussions and Analysis": "5.1 Analyzing the Design Choice of parameters: parameters introduced in Eq. (6) controls how the self-attention features from the blocks within the text-to-imagediffusion model interact with the cross-attention featuresfrom the text-to-music diffusion model. MELFUSION hasone alpha parameter per block within the decoders of bothdiffusion models. We vary this design choice in Tab. 2.Attaching the synapse in the decoder offers better perfor-mance.This is because the decoder controls the majortransformations that contribute to generating the image.Further, learning different per block helps to learn block-specific mixing co-efficient, which slightly improves theperformance.We also perform a sensitivity analysis on the learningrate (LR) used while learning parameters in Tab. 3. Basedon this analysis, we use a LR of 1e 5 in our experiments.5.2 Efficacy of Conditioning on both Modalities: In or-der to study the contribution of each modality on MELFU-SION, we train three different variations of the model byselectively turning off visual conditioning and textual con-",
  ". Performance of MELFUSION with varying verbosity oftext prompts collected from MeLBench": "strength of classifier-free guidance during inference. Simi-lar to the findings from Ghosal et al. , increasing T helpsto generate more pleasing music. This can be attributed toenhanced refinement from more denoising. CFG strengthof 7 gives the best result, and we use it in our experiments.5.4 Verbose Text versus Image Conditioning: The vi-sual synapse infuses fine-grained semantics from the im-age into text-to-music diffusion models. Another alterna-tive to infuse such semantics would be to use verbose textdescriptions. To study this, we remove the visual synapsefrom MELFUSION and train a music generation model con-ditioned only on text.Then, we vary the length of textprompts and report results in Tab. 6. Interestingly, we findthat using image conditioning with a small text promptoutperforms using lengthier prompts.This underscoresthe utility of visual conditioning and the ability of visualsynapses to modulate the LDM effectively.5.5 Effect of visual-cues: We analyse the effect of using adifferent image and the same text prompt (please refer to theproject page). When we change from the walkway image tothe blue forest, the music becomes more calm and distant.We change the image to an abandoned amusement park, car-nival orchestra, foggy seaside concert and forest at night.We observe a prevalence of eerie ambient sound echoingthrough the deserted park, occasional circus-inspired mo-tifs, distant sounds of waves and foghorn-like effects andatmospheric strings imitating rustling leaves respectively.5.6 Comparison with Text-to-Audio Methods: We in-clude a comparison of MELFUSION with text-to-audio gen-eration approaches in Tab. 7. We finetune their pre-trainedcheckpoints on our benchmark datasets for this experiment.The complementary information from both modalities al-lows our approach to outperform these methods too.5.7 Effectiveness of IMSM: We conduct a user study with64 participants to check whether the proposed IMSM metricis indeed capturing the relatedness between generated mu-",
  ". While comparing MELFUSION with state-of-the-art text-to-audio approaches, we see significant improvement in quality": "sic and the conditioning image. We randomly choose 300samples from Extended MusicCaps and MeLBench each.We compute the IMSM score, and also ask users for theirimage-music similarity on a scale of , for these sam-ples. The average score from IMSM metric and the usersfor Extended MusicCaps and MeLBench are (0.76, 0.71)and (0.83, 0.85) respectively. The high correlation under-scores the validity and usefulness of IMSM metric.5.8 Using IMSM to Measure Purity of the Datasets: Wecompute the IMSM scores for all image-music pairs presentin both Extended MusicCaps and MeLBench datasets andobtain a score of 0.91 and 0.93 respectively. The purposeof this is to quantitatively establish that the curated samplesare perceptually in sync and are meaningful. The high val-ues of the IMSM scores demonstrate that the curated imagesamples are highly perceptually similar and have ample as-sociation with the musical compositions.",
  ". Conclusion and Future Works": "We explore the utility of infusing image semantics into atext-to-music diffusion model, enabling us to generate mu-sic, consistent with both visual and textual semantics in thiswork. To the best of our knowledge, ours is the first efforttowards such a multi-conditioned music generation. We de-velop MELFUSION with a novel visual synapse to effec-tively infuse the image semantics into music generation, in-troduce a new dataset MeLBench, and propose a new eval-uation metric. We conduct exhaustive experimental analy-sis on MeLBench and a modified version of MusicCaps and compare MELFUSION against 7 text-to-music meth-ods, and a modified baseline. The results suggest: 1) theextra information from the image conditioning significantlyboosts music generation quality 2) our visual synapse iseffective in modulating and infusing the required semanticinformation into the generative process.MELFUSION can be an essential tool for a creative pro-fessional or a social-media content creator who needs togenerate music that can go well with their multi-modal post(consider a user posting about their recent picnic theirphotos can be the image conditioning while a short descrip-tion of the trip can be the textual input to MELFUSION).Creating music with semantic lyrics that can go well with avideo can be some interesting open-ended follow-up works. Andrea Agostinelli, Timo I Denk, Zalan Borsos, Jesse En-gel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al.Musiclm:Generating music from text.arXiv preprintarXiv:2301.11325, 2023. 1, 2, 5, 6, 7, 8, 14, 19",
  "Shang-Yi Chuang, Hsin-Min Wang, and Yu Tsao. Improvedlite audio-visual speech enhancement. IEEE/ACM Transac-tions on Audio, Speech, and Language Processing, 30:13451359, 2022. 21": "Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al.Scalinginstruction-finetuned language models.arXiv preprintarXiv:2210.11416, 2022. 3, 6, 14 Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant,Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez.Simple and controllable music generation.arXiv preprintarXiv:2306.05284, 2023. 1, 2, 6, 7, 14 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, PascaleFung, and Steven Hoi.Instructblip:Towards general-purpose vision-language models with instruction tuning.2023. 6, 7 Paromita Das, Somsubhra Gupta, and Biswarup Neogi. Mea-surement of effect of music on human brain and consequentimpact on attentiveness and concentration during reading.Procedia computer science, 172:10331038, 2020. 2",
  "ial networks for symbolic music generation and accompani-ment. In Proceedings of the AAAI Conference on ArtificialIntelligence, 2018. 2": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 6, 13 Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail,and Huaming Wang. Clap learning audio concepts from nat-ural language supervision. In ICASSP 2023-2023 IEEE In-ternational Conference on Acoustics, Speech and Signal Pro-cessing (ICASSP), pages 15. IEEE, 2023. 6",
  "Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, andBryan Pardo. Vampnet: Music generation via masked acous-tic token modeling. arXiv preprint arXiv:2307.04686, 2023.3": "Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, ArenJansen, Wade Lawrence, R Channing Moore, Manoj Plakal,and Marvin Ritter.Audio set: An ontology and human-labeled dataset for audio events.In 2017 IEEE interna-tional conference on acoustics, speech and signal processing(ICASSP), pages 776780. IEEE, 2017. 5, 19 Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish,and Soujanya Poria.Text-to-audio generation usinginstruction-tuned llm and latent diffusion model.arXivpreprint arXiv:2304.13731, 2023. 3, 6, 8, 14, 21",
  "Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti,Judith Yue Li, and Daniel PW Ellis. Mulan: A joint em-bedding of music audio and natural language. arXiv preprintarXiv:2208.12415, 2022. 2, 14": "Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,Jiahui Yu, Christian Frank, et al.Noise2music:Text-conditioned music generation with diffusion models. arXivpreprint arXiv:2302.03917, 2023. 7 Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,Jiahui Yu, Christian Frank, et al.Noise2music:Text-conditioned music generation with diffusion models. arXivpreprint arXiv:2302.03917, 2023. 2, 6, 14",
  "Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and ZhouZhao. Generspeech: Towards style transfer for generaliz-able out-of-domain text-to-speech synthesis. arXiv preprintarXiv:2205.07211, 2022": "Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, ChenyeCui, and Yi Ren. Prodiff: Progressive fast diffusion modelfor high-quality text-to-speech. In Proceedings of the 30thACM International Conference on Multimedia, pages 25952605, 2022. 3 Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, XiangYin, and Zhou Zhao.Make-an-audio: Text-to-audio gen-eration with prompt-enhanced diffusion models.In Inter-national Conference on Machine Learning, pages 1391613932. PMLR, 2023. 21 Ye Jia, Ron J Weiss, Fadi Biadsy, Wolfgang Macherey,Melvin Johnson, Zhifeng Chen, and Yonghui Wu.Directspeech-to-speech translation with a sequence-to-sequencemodel (2019). arXiv preprint arXiv:1904.06037, 1904. 21 Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, JonathanShen, Fei Ren, Patrick Nguyen, Ruoming Pang, IgnacioLopez Moreno, Yonghui Wu, et al. Transfer learning fromspeaker verification to multispeaker text-to-speech synthe-sis. Advances in neural information processing systems, 31,2018. 21 Junyan Jiang, Gus G Xia, Dave B Carlton, Chris N Ander-son, and Ryan H Miyakawa. Transformer vae: A hierarchi-cal model for structure-aware and interpretable music rep-resentation learning. In ICASSP 2020-2020 IEEE Interna-tional Conference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 516520. IEEE, 2020. 21",
  "neural vocoding with parallel wavenet.In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pages 60446048. IEEE,2021. 21": "Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury,Norman Casagrande, Edward Lockhart, Florian Stimberg,Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Ef-ficient neural audio synthesis. In International Conferenceon Machine Learning, pages 24102419. PMLR, 2018. 21 Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan:Generative adversarial networks for efficient and high fi-delity speech synthesis.Advances in Neural InformationProcessing Systems, 33:1702217033, 2020. 5 Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan:Generative adversarial networks for efficient and high fi-delity speech synthesis. Advances in neural information pro-cessing systems, 33:1702217033, 2020. 21 Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,Wenwu Wang, and Mark D Plumbley. Panns: Large-scalepretrained audio neural networks for audio pattern recogni-tion. IEEE/ACM Transactions on Audio, Speech, and Lan-guage Processing, 28:28802894, 2020. 6 Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taig-man, and Yossi Adi. Audiogen: Textually guided audio gen-eration. arXiv preprint arXiv:2209.15352, 2022. 6",
  "Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Bddm:Bilateral denoising diffusion models for fast and high-qualityspeech synthesis. arXiv preprint arXiv:2203.13508, 2022. 3": "Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, SiyuanFeng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, XuchenSong, et al. Efficient neural music generation. arXiv preprintarXiv:2305.15719, 2023. 1, 2, 6, 7, 14, 21 Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, ChangLiu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, and Tie-Yan Liu. Priorgrad: Improving conditional denoising dif-fusion models with data-dependent adaptive prior.arXivpreprint arXiv:2106.06406, 2021. 3 Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audi-oldm: Text-to-audio generation with latent diffusion models.arXiv preprint arXiv:2301.12503, 2023. 3, 5, 6, 8, 14, 21 Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-uqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang,and Mark D Plumbley. Audioldm 2: Learning holistic audiogeneration with self-supervised pretraining. arXiv preprintarXiv:2308.05734, 2023. 15",
  "Mubert Inc. Mubert. URL Mu-bert inc. mubert. url 2023. Mubert Inc.Mubert. URL 2023, 2023. 2, 6, 7, 14": "Aashiq Muhamed, Liang Li, Xingjian Shi, Suri Yaddana-pudi, Wayne Chi, Dylan Jackson, Rahul Suresh, Zachary CLipton, and Alex J Smola. Symbolic music generation withtransformer-gans. In Proceedings of the AAAI conference onartificial intelligence, pages 408417, 2021. 2 Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. Glide: Towards photorealistic image generationand editing with text-guided diffusion models. arXiv preprintarXiv:2112.10741, 2021. 21 Sam V Norman-Haignere, Nancy Kanwisher, Josh H Mc-Dermott, and Bevil R Conway. Divergence in the functionalorganization of human and macaque auditory cortex revealedby fmri responses to harmonic tones. Nature neuroscience,22(7):10571060, 2019. 2",
  "Dipjyoti Paul, Yannis Pantazis, and Yannis Stylianou.Speaker conditional wavernn:Towards universal neuralvocoder for unseen speaker and recording conditions. arXivpreprint arXiv:2008.05289, 2020. 21": "Vadim Popov, Ivan Vovk, Vladimir Gogoryan, TasnimaSadekova, and Mikhail Kudinov. Grad-tts: A diffusion prob-abilistic model for text-to-speech. In International Confer-ence on Machine Learning, pages 85998608. PMLR, 2021.3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 6, 14, 15",
  "Peter J Liu. Exploring the limits of transfer learning witha unified text-to-text transformer. The Journal of MachineLearning Research, 21(1):54855551, 2020. 14": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.Zero-shot text-to-image generation. In International Confer-ence on Machine Learning, pages 88218831. PMLR, 2021.21 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 2, 3, 14, 21 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III18, pages 234241. Springer, 2015. 3 Mostafa Sadeghi, Simon Leglaive, Xavier Alameda-Pineda,Laurent Girin, and Radu Horaud.Audio-visual speechenhancement using conditional variational auto-encoders.IEEE/ACM Transactions on Audio, Speech, and LanguageProcessing, 28:17881800, 2020. 21 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. 21 Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Ar-naud Stiegler, Teven Le Scao, Arun Raja, et al.Multi-task prompted training enables zero-shot task generalization.arXiv preprint arXiv:2110.08207, 2021. 14",
  "Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf.Mo\\ usai: Text-to-music generation with long-context la-tent diffusion. arXiv preprint arXiv:2301.11757, 2023. 1, 2,6, 7, 14, 21": "Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid.Avformer: Injecting vision into frozen speech models forzero-shot av-asr. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pages2292222931, 2023. 21 Roy Sheffer and Yossi Adi.I hear your true colors: Im-age guided audio generation. In ICASSP 2023-2023 IEEEInternational Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 15. IEEE, 2023. 1 Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,Oran Gafni, et al. Make-a-video: Text-to-video generationwithout text-video data. arXiv preprint arXiv:2209.14792,2022. 21",
  "JiamingSong,ChenlinMeng,andStefanoErmon.Denoising diffusion implicit models.arXiv preprintarXiv:2010.02502, 2020. 3": "Kim Sung-Bin, Arda Senocak, Hyunwoo Ha, AndrewOwens, and Tae-Hyun Oh. Sound to visual scene genera-tion by audio-to-visual latent alignment. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 64306440, 2023. 21 Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang,Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He,et al.Naturalspeech: End-to-end text-to-speech synthesiswith human-level quality.IEEE Transactions on PatternAnalysis and Machine Intelligence, 2024. 21",
  "Aaron Van Den Oord, Oriol Vinyals, et al. Neural discreterepresentation learning. Advances in neural information pro-cessing systems, 30, 2017. 3, 21": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 3 Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu,Jiang Bian, et al. Audit: Audio editing by following instruc-tions with latent diffusion models. Advances in Neural In-formation Processing Systems, 36, 2024. 21",
  "Junjie Wu, Junsong Zhang, Xiaojun Ding, Rui Li, andChangle Zhou. The effects of music on brain functional net-works: a network analysis. Neuroscience, 250:4959, 2013.2": "Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan WeixianLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, XiaohuQie, and Mike Zheng Shou. Tune-a-video: One-shot tuningof image diffusion models for text-to-video generation. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 76237633, 2023. 21 Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, ChaoWeng, Yuexian Zou, and Dong Yu.Diffsound: Discretediffusion model for text-to-sound generation.IEEE/ACMTransactions on Audio, Speech, and Language Processing,2023. 3",
  "Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 38363847, 2023. 15": "Ya-Jie Zhang, Shifeng Pan, Lei He, and Zhen-Hua Ling.Learning latent representations for style control and transferin end-to-end speech synthesis. In ICASSP 2019-2019 IEEEInternational Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 69456949. IEEE, 2019. 21 In this appendix we provide additional information on thefollowing:A More Details on TANGO++B Problem Motivation RevisitedC Other Baseline ApproachesD Implementation DetailsE More Experimental AnalysisF Dataset DetailsG User Study DetailsH Inspiration from Conditional Image GenerationI Related Audio Concepts",
  "A. More Details on TANGO++": "Our modified baseline model TANGO++ comprises anearly-fusion approach, where we align the visual and thetextual modalities through an Image-Text Contrastive (ITC)loss. As the generated music is conditioned on both modal-ities, bringing them to a common latent space is imper-ative to the success of the system.The text input ispassed through the FLAN-T5 text encoder which we keepas frozen. For image encoding we use ViT . We projectthe visual and the textual inputs to a common embeddingspace and align them using ITC loss. The diffusion modelis conditioned on this hybrid embedding to produce audiosignals. It is then converted into spectrograms using the de-coder and then passed through a HiFi GAN vocoder to pro-duce the music signal. The expression for ITC loss (LITC)is as follows:",
  "B. Problem Motivation Revisited": ". A mock-up of a social media post that contains an imageand associated textual content. Our approach MELFUSION, canconsume such image-textual pairs as input and synthesize musicthat can go well with them. Social media platforms have become ubiquitous and pro-vide a channel for everyone to express their creativity andshare their happenings with the world. It is very commonfor users to upload an image, and write an associated textwith it (). Adding music to these social media postsenhances its visibility and appeal. Instead of retrieving mu-sic from an existing database, our approach MELFUSION,will be able to generate music tracks that are custom-made,conditioned on the uploaded image and its description. Wenote that ours is the first approach that operates in this prag-matic setting, to generate music conditioned on both visualand textual modality.",
  "C. Other Baseline Approaches": "In addition to our proposed baseline approach, we com-pare MELFUSION against the following methods.Notethat these are text-to-music generation methods unlike ourapproach and dont support multi-conditioning in inputprompts. Hence a direct comparison might not be entirelyfair. In most cases these methods dont support introducingan additional modality conditioning as a result we compareour approach against these baselines directly to study the benefits of MELFUSION.Riffusion base their algorithm on fine-tuning a Sta-ble Diffusion model on mel spectrograms of musicpieces from a paired music-text dataset. This is one of thefirst text-to-music generation methods. Mubert is anAPI-based service that employs a Transformer backbone.The encoded prompt is used to match the music tags andthe one with the highest similarity is used to query the audiogeneration API. It operates over a relatively smaller set asit produces a combination of audio from a predefined col-lection. MusicLM generates high-fidelity music fromtext descriptions by casting the process of conditional mu-sic generation as a hierarchical sequence-to-sequence mod-eling task. They leverage the audio-embedding network ofMuLan to extract the representation of the target audiosequence. Mousai is a cascading two-stage latent diffu-sion model that is equipped to produce long-duration high-quality stereo music. It achieves this by employing a spe-cially designed U-Net facilitating a high compression rate.Noise2Music introduced a series of diffusion models,a generator, and a cascader model. The former generatesan intermediate representation conditioned on text, whilethe later can produce audio conditioned on the intermediaterepresentation of the text. MeLoDy pursues an LM-guided diffusion model by reducing the forward pass bottle-neck and applies a novel dual-path diffusion mode. Music-Gen comprises a single-stage transformer LM togetherwith efficient token interleaving patterns. This eliminatesthe need for hierarchical upsampling.",
  "D. Implementation Details": "Our text-to-music LDM contains 3 encoder blocks and 3decoder blocks, similar to Ghosal et al. . Empiricallywe find that finetuning from its pre-trained checkpoint helpsconvergence.FLAN-T5 is used as the text encoder.MELFUSION is trained for 30 epochs using AdamW op-timizer . We attach our visual synapse only on the de-coder layers of the LDM. Similar to earlier works ,we find that using classifier-free guidance improves the re-sult. Our training takes 42 hours on 4 NVIDIA A100 GPUs.",
  ". MELFUSION with different versions of Stable Diffusion": "We study the effect of employing different variants of thetext-to-image Stable Diffusion model (V1.2 through V1.5)in Tab. 8. We note that the best results are obtained with thelatest variant. This brings to light that our proposed visualsynapse is able to cascade the usage of better text-to-imagemodels into improving the quality of music generation. TheStable Diffusion V1.4 and V1.5 checkpoints were initial-ized with the weights of the Stable Diffusion V1.2 check-point and subsequently fine-tuned on 225k steps at resolu-tion 512 512 on the LAION dataset and 10% droppingof the text-conditioning to improve classifier-free guidancesampling.",
  ".A study on the diversity analysis of MELFUSION.We evaluate the performance of our model on generating musi-cal tracks of five different genres on MeLBench": "Tab 10 reports the performance of MELFUSIONacrossthe 5 most popular genres (chosen through a study under-taken by ) on the genre-wise test set collected fromMeLBench.We find a steady performance of our ap-proach across different genres substantiating the ability ofthe model to capture the musical nuances like the compo-sition of the instruments, track progression, sequence of in-struments introduced, rhythm, tonality, tempo, and beats. Due to the highly subjective nature of the problem, we alsoperform a human evaluation by subject matter experts. Tothis end, we employ 7 individuals formally trained in mu-sic to independently listen and report OVL and REL scoresconsidering the aforementioned aspects to assess the qual-ity of genre-wise samples. We report the mean OVL andREL values from all the evaluators on a subset of the corre-sponding genre-wise test splits. We find that the overall per-formance of our method is highly encouraging as reportedin Tab 10.",
  "E.4. Ablating choice of layers": "When we fuse subset of Decoder Blocks, we see drop inperformance in Tab. 11, as coupling becomes weak. Wealso ablate encoder and decoder layer separately (refer toTab. 2 of main paper). Learned values for each blocks(0.37, 0.59 and 0.63 respectively) improves over =0.5 onall metrics, thus avoiding an extra hyper-parameter to tune.With a few layers to account for dimension mismatch, vi-sual synapse can scale to different architectures and avoidlayer-to-layer correspondence. We will explore this in a fu-ture work.",
  "E.5. On conditioning image": "MELFUSIONgeneratesmusicfromcomplementary information from text and image modal-ities.While selecting images randomly, we have lowerFAD/KL/FD scores of 6.38/1.73/26.45 and 8.33/1.57/28.64on the extended MusicCaps and MeLBench datasets respec-tively, as it gets conditioned on random image semantics.We see similar trend in the baselines too, and MELFUSIONstill outperforms them.Retrieving or generating imagefrom conditioning text, will also have similar effect due tosemantic similarity in both conditioning domains.",
  "E.6. Alternate visual conditioning": "We compare alternate conditioning from ViT features andControlNet here. The semantics contained in these repre-sentations are inferior to those from text-to-image models(similar to findings in ). Further, our visual synapseeffectively adapts them by learning to modulate the repre-sentations, specific to music synthesis. Moreover comparedto the generalist model (that consumes multiple modalities)in AudioLDM2 , our specialist synaptic model gener-",
  ". Analyzing the effect of having fixed versus learnable": "We study the impact when is kept frozen as compared tobeing learnable here. The first five entries in Tab. 14 denotethe cases where the value of is unaltered during trainingand kept constant at 0, 0.10, 0.50, 0.90, and 1.0 respectively.Experimental results demonstrate that a learnable value of produces significantly better results as compared to thefixed counterpart, as the model has the flexibility to learnthem to effectively balance between both the conditioningmodalities.",
  ". Image categories in MeLBench": "Tab. 16 presents the distribution of the image samples inMeLBench. To maintain a fair balance across different dis-tributions we collect samples from 4 different categories:natural images, animations, posters, paintings/sketches.This ensures that MELFUSION is trained with ample ex-amples from each of these classes and is equipped to tackleimages from any of these very frequent and popular classesbetter. MeLBench comprises 11,250 samples which is 2xlarger than the next largest dataset MusicCaps . presents the frequency of the top 90 words in MeL-Bench. The annotators were asked to write free-form textdescriptions of the musical pieces with an emphasis on themusicality of the samples. We observe that the annotationcontains important cues about the nature of the audio track(e.g., live performance, chaotic, forceful vocals, etc).These can supplement a model with useful pieces of infor-",
  "F.2. Dataset Hierarchy and Samples": "Tab. 15 contains the genre and sub-genre-wise division ofthe samples collected in MeLBench. We categorise the col-lected musical samples into 15 broad categories with eachof them having 22 sub-genres to facilitate fine-grained con-trol over the composition through the image (theme) andtext-instructions (details on musicality). The samples aredivided across different genres roughly equally to maintaina good balance. presents one sample from each of the remaining13 categories (Electronic and Folk Acoustic present in themain paper). As can be seen from the examples, the cap-tions are of varied lengths and the images are from differentdistributions (natural images, animation, paintings, etc.).",
  "F.3. Extended MusicCaps Data Collection": "MusicCaps is a music caption dataset comprising mu-sic clips from AudioSet paired with corresponding textdescriptions in English. The collection consists of a total of5,521 examples, out of which 2,858 are from the AudioSeteval and 2,663 are from the AudioSet train split. The au-thors further tag 1,000 samples as a balanced subset of thedataset - equally divided across genres. All examples in thebalanced subset are from the AudioSet eval split. As oursetup is not restricted to text and requires joint conditioningin the form of images as well, we supplement this dataset by",
  "G. User Study Details": "presents the user study interface. To obtain the OVLand REL scores, we provide the participants with an image-text pair and the audio sample generated by MELFUSION.For the overall audio quality score (OVL) the participantsare instructed to add their score between while for therelevance score (REL), they are required to rate the samplebased on its similarity with the input image-text pairs.",
  "H. Inspiration from Conditional Image Gener-ation": "Powered by architectural improvements and the availabil-ity of large-scale, high-quality paired training data, condi-tional image generation methods have made considerableprogress in the generative AI space. Promising results fromtransformer-based auto-regressive approaches wereboosted by diffusion model-based methods .These approaches have been naturally extended to gener-ate videos from text prompts too . Latent diffu-sion models do the diffusion process in the latent spaceof a pre-trained VQ-VAE . This significantly reducedthe compute requirements when compared with image dif-fusion methods. Ho and Salimans proposed classifier-free guidance to enhance image quality. Text-to-music andtext-to-audio methods are heavily inspired by the success oftext-to-image generative methods, and so are we.",
  "I. Related Audio Concepts": "The Multimodal Variational Auto-encoders (MVAEs) arelatent variable generative models to learn more generaliz-able representations from diverse modalities through jointdistribution estimation.Arik et al. pioneered a neu-ral audio synthesis model based on VAEs. Their approachdemonstrated promising results in generating realistic au-dio samples by learning a latent representation of the audiodata. Inspired by this VAEs have been widely used in theaudio processing domain for speech synthesis ,audio generation , and audio denoising .Vocoders are used for a variety of purposes across differ-ent domains due to their ability to manipulate and synthe-size audio signals efficiently. Among other prominent appli-cations of vocoder, neural voice cloning , voice con-version , and speech-to-speech synthesis are verypopular. GAN-based vocoders have been employed togenerate high-fidelity raw audio conditioned on mel spec-trogram. More recently, WaveRNN has been appliedfor universal vocoding task .Spectrograms are a powerful tool for analyzing time-varying signals such as audio and speech. They providea visual representation of the frequency content of a sig-nal over time, making them widely used in speech process-ing , music analysis , and audio synthe-sis in general. Audio spectrograms arealso massively deployed in different audio visual applica-tions . Acknowledgements: We would like to sincerely thank thedata annotators and the volunteers who took part in the userstudy. We would also like to extend our gratitude to theanonymous reviewers for their constructive and thoughtfulfeedbacks."
}