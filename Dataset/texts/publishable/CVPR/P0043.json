{
  "Division of Image Processing, Department of Radiology, Leiden University MedicalCenter, Leiden, the Netherlands{r.gao, d.lyu}@lumc.nl": "Abstract. Medical imaging is essential for the diagnosis and treatmentof diseases, with medical image segmentation as a subtask receiving highattention. However, automatic medical image segmentation models aretypically task-specific and struggle to handle multiple scenarios, suchas different imaging modalities and regions of interest. With the intro-duction of the Segment Anything Model (SAM), training a universalmodel for various clinical scenarios has become feasible. Recently, sev-eral Medical SAM (MedSAM) methods have been proposed, but thesemodels often rely on heavy image encoders to achieve high performance,which may not be practical for real-world applications due to their highcomputational demands and slow inference speed. To address this is-sue, a lightweight version of the MedSAM (LiteMedSAM) can providea viable solution, achieving high performance while requiring fewer re-sources and less time. In this work, we introduce Swin-LiteMedSAM, anew variant of LiteMedSAM. This model integrates the tiny Swin Trans-former as the image encoder, incorporates multiple types of prompts,including box-based points and scribble generated from a given bound-ing box, and establishes skip connections between the image encoderand the mask decoder. In the Segment Anything in Medical Imageson Laptop challenge (CVPR 2024), our approach strikes a good bal-ance between segmentation performance and speed, demonstrating sig-nificantly improved overall results across multiple modalities comparedto the LiteMedSAM baseline provided by the challenge organizers. Ourproposed model achieved a DSC score of 0.8678 and an NSD scoreof 0.8844 on the validation set. On the final test set, it attained aDSC score of 0.8193 and an NSD score of 0.8461, securing fourthplace in the challenge. The code and trained model are available at",
  "Introduction": "Medical imaging diagnosis is fundamental for evaluating diseases, and medicalimage segmentation, which involves the extraction of specific structures such astumors and organs from medical images, consistently receives significant atten-tion. Deep learning methods have demonstrated effectiveness in this field, leadingto the development of numerous models tailored for specific scenarios. However,each scenario typically requires training a dedicated segmentation model, de-manding substantial effort. In recent years, inspired by the rapid development oflarge language models (LLMs) in the natural language processing (NLP) field,researchers have begun exploring the application of large models in computer vi-sion. Segment Anything Model (SAM) is one such innovation, aiming to unifythe segmentation task for general images by training with a huge amount of data.while SAM holds potential, the distinct features of medical images can hinderits performance in medical image segmentation. Therefore, recent works focus on adapting the SAM model for medical applications by re-training with alarge volume of medical images. Despite achieving high performance in variousmedical image segmentation tasks, SAM models large parameter volume andthe high spatial resolution of medical images require substantial computationalresources and processing time. This poses challenges for practical deploymentof SAM models in real-world applications, or even for non-industry academicgroups conducting research on them. Consequently, lite SAM models are gain-ing more attention as a solution to this problem.The original SAM model is composed of three main components: an image en-coder, a prompt encoder, and a mask decoder. Among these, the image encoderis the primary factor contributing to high computational and memory costs dueto the usage of ViT-H . To mitigate resource consumption and accelerate pro-cessing, various studies have aimed to make the image encoder more lightweight.For instance, FastSAM introduces a CNN-based framework, while Mobile-SAM tackles this issue by distilling knowledge from the ViT-H image en-coder into a tiny ViT-based encoder. Additionally, EfficientSAM employsthe Masked Autoencoders (MAE) framework to efficiently transfer knowledgefrom a large image encoder to a small one, resulting in a more resource-efficientdesign with better performance. EfficientViT-SAM further enhances thisapproach by incorporating EfficientViT with fused MBConv blocks to cre-ate a lightweight image encoder. Recently, the challenge Segment Anything inMedical Images on Laptop1, hosted at CVPR 2024, sought universal prompt-able medical image segmentation models deployable on laptops or edge deviceswithout GPU reliance. The organizers developed LiteMedSAM2 as a baseline,using the distillation strategy described in . Although LiteMedSAM focuseson optimizing the image encoder to reduce resource usage, segmentation perfor-mance is compromised. Therefore, our goal is to enhance performance withouthighly sacrificing efficiency. To achieve this, we use a lightweight Swin Trans-",
  "Swin-LiteMedSAM for Medical Image Segmentation3": "former as image encoder and also introduce two additional prompts, box-basedpoints and box-based scribble, except the original box prompt. To this end, weintroduce our model, Swin-LiteMedSAM. The key contributions of our modelare as follows: Instead of transferring knowledge to a tiny ViT, we employ a tiny SwinTransformer as the image encoder. The Swin Transformer is designedto handle large images more efficiently, both in terms of computation andmemory usage compared to ViT. Moreover, skip connections are establishedbetween the image encoder and mask decoder to enhance feature integration. We introduce additional types of prompts beyond boxes, including box-basedpoints and box-based scribble. These prompts are automatically generatedfrom the given bounding box and effectively improve model performancewithout significantly increasing resource costs.",
  "Data preprocessing": "To accelerate the models training and inference stages and reduce memory con-sumption, we resize the input image to 256 256. This is achieved by firstresizing the images while maintaining their original aspect ratio based on thelongest side, and then do zero padding to reach the final size of 256 256. Fordata normalization, we use the method described in . Please refer to formore details.Note that gray-scale images such as CT, MR, US, and PET typically haveonly one channel, whereas RGB images from modalities like endoscopy, der-moscopy, and fundus imaging usually have three channels. To maintain consis-tency during model training, we replicated the channel dimension for gray-scaleimages, converting them from one channel to three channels.",
  "Proposed method": "Our models structure is shown in . It mainly comprises three components:an image encoder, a prompt decoder, and a mask decoder. The function of thesethree components are detailed below.The image encoder architecture is inspired by the original tiny ViT designof LiteMedSAM. The input first passes through two convolutional layers, whichcapture low-level spatial features and adjust the number of channels to 64. Fol-lowing this, the encoder consists of four stages, with their depths arranged ac-cording to the tiny ViT configuration as (2, 2, 6, 2). The structure of the Swinblock used in our encoder is illustrated in . We have slightly modifiedthe standard Swin block by adding a convolutional block with batch normal-ization between the windowed multi-head self-attention (W-MSA) module and",
  ". Overview of the Swin-LiteMedSAM architecture": "the Multi-Layer Perceptron (MLP). This modification enables our encoder toeffectively capturing both global and local features. Furthermore, the numberof channels and spatial resolution across four stages remain consistent with theoriginal design. Finally, a head branch consisting of several convolutional layersand layer normalization adjusts the channel number to 256. In the prompt encoder, we introduce two additional types of prompts: box-based points and a box-based scribble, alongside the original box prompt. Thebox-based points and the box are combined to form a sparse embedding, whilethe box-based scribble is used for dense embedding. For the box-based prompt,drawing from insights provided by and , which demonstrate the effective-ness of using multiple points over a single point, we opt to utilize four points inour prompt encoder. To achieve this, we divide the bounding box area into fourequivalent sub-parts based on the central point. We then randomly generate onepoint in the non-zero area of each sub-part, resulting in four points distributedinside the box. If a sub-part contains only zeros, we select the central point. Thisapproach ensures a relatively sparse distribution of points covering more area.",
  "(c)": ".(a) Box-based points and scribble generation strategies during the trainingstage. (b) Box-based points generation strategy during the inference stage. (c) Box-based scribble generation strategy during inference stage. Furthermore, a box-based scribble is randomly generated within the box usingthe algorithm in . All pixels in the scribble are set to 1 and placed into thecorresponding part of an all-zero matrix with a shape of (256, 256) to create amask for the dense embedding. Similarly, if all pixels in the box are zeros, thescribble is set to an all-zero matrix of shape (256, 256) to ensure the promptencoder focuses on the sparse prompt embedding part, as illustrated in (a). Then in the mask decoder, we follow the SAM original design by using atwo-way transformer to process embeddings from the prompt encoder and imageencoder. Moreover, we build skip connections between the image encoder andmask decoder, concatenating outputs from the last three stages and fusing themwith several convolutional layers. This output is then combined with the two-waytransformers output and passed through an upscaling block to double the imageresolution. Similarly, the upscaled output is concatenated with the first stagesoutput from the image encoder, and the resulting output is further upsampledto return back to the original spatial resolution.",
  "Inference strategy for box-based points and box-based scribble": "The strategy for generating box-based points and box-based scribble has somedifference between the training and inference stages. During the training stage,the range for generating four points is within the entire bounding box, whichaims to expose the model to diverse cases and helps improve its generalizationcapabilities. However, randomly generating points within the whole box mightnot be ideal during the inference stage, as objects are typically located near thecentral part of the box. Random generation can easily place some points nearthe boundary, which is less effective and even negatively impact performance.Furthermore, for the situation of a single point prompt, the central point ofthe box is always the first choice . Likewise, the two corner points of the boxalready provide some external and surrounding information of objects. Therefore,points should be better distributed in the relatively central part of the box. Fora given bounding box, represented by its upper left point (xmin, ymin) andbottom right point (xmax, ymax), we introduce two variables, shifth and shiftw,to adjust the coordinates along height and width directions so that four pointsdo not occur in the peripheral area, as shown in (b). This adjustment isdenoted as follows:xmin = xmin + shiftw,",
  "5h, 2": "5h) to ensure that thedistribution of points is closer to the center. Additionally, shiftw and shifth arerandomly adjusted within their ranges for each sample to achieve better overallperformance. We also follow the same points distribution strategy as in thetraining stage to ensure that the four points are positioned in the four quadrantsof the image.Then (c) illustrates the strategy of generating a scribble in the inferencestage. Considering the empirical distribution of points, we believe that placingthe scribble closer to the edges is more effective than points for capturing contourinformation. Therefore, we adjusted the range of shiftw to ( 1",
  "Dataset and evaluation metrics": "Training and validation dataset We only use the provided challenge dataset,without additional public datasets. This dataset includes 11 modalities: CT,MRI, PET, X-ray, ultrasound, mammography, OCT, endoscopy, fundus, der-moscopy, and microscopy, totaling more than one million 2D image-mask pairs.Testing dataset The testing set in this challenge is hidden, with all testingimages newly collected from 20+ different institutions worldwide.Evaluation metrics The evaluation metrics are the Dice Similarity Coefficient(DSC) and Normalized Surface Dice (NSD) for accuracy, and Docker containerrunning time for efficiency. These metrics together determine the ranking. Notethat only mean results are available. The evaluation platform environment ispresented in .",
  "Implementation details": "Training environment settings The training environments are presented in.Training protocols Our training strategy consists of two stages. In the firststage, we utilize knowledge distillation to transfer information from the largeViT-B image encoder to the tiny Swin Transformer as our image encoder. Tonote, we pre-saved the output image embeddings from the ViT-B encoder to",
  "Number of flops47.70G55.20G": "speed up the distillation process. In the second stage, we take the pre-trainedimage encoder from the first stage and proceed to train the entire model. Thetraining details of these two stages are listed in .Data sampling strategy During the training, we randomly sample image casesfrom the dataset. If the case is 3D, such as a CT, MR, or PET scan, we ran-domly sample a slice from the 3D image. If the case is 2D, such as an X-rayor microscopy image, we use the image directly. This strategy significantly re-duces training time and ensures a more balanced distribution of training samplesacross different modalities.Data augmentation We apply vertical and horizontal flips to the image, eachwith a 50% probability.Inference environment settings During the inference stage, the running en-vironment differs from the training stage. A docker container is built, startingwith a python:3.10-slim image and installing the CPU version of PyTorch 2.2.2.All other aspects still remain same with the training stage.",
  "Quantitative results on validation set": "shows that Swin-LiteMedSAM achieves higher average DSC (86.70%)and NSD (88.55%) scores compared to LiteMedSAM, which recorded 83.81%for DSC and 83.26% for NSD. In general, Swin-LiteMedSAM achieved a morebalanced and comprehensive performance across the nine modalities comparedto LiteMedSAM. It showed significant improvement in PET and Microscopywhile maintaining strong performance in most modalities. However, the modelexperienced a noticeable drop in DSC and NSD scores for the US modality.Then further highlights the importance of each component in ourproposed method, particularly the inclusion of skip connections, as well as both",
  "Quantitative results on testing set": "As shown in , our proposed method significantly outperforms LiteMed-SAM across most imaging modalities in terms of DSC and NSD, while alsoreducing runtime of all the modalities. Specifically, for CT images, our methodachieved an absolute DSC improvement of 17.15%, corresponding to a relativeimprovement of 30.76%, and an NSD increase of 18.51%, corresponding to arelative improvement of 31.75%, compared to LiteMedSAM, also with a fasterruntime. For PET and X-ray modalities, our method demonstrated competitiveDSC and NSD results. In PET, it achieved a marginal NSD improvement whilemaintaining similar DSC performance, and significantly reduced runtime. ForX-ray, despite a slightly lower DSC compared to LiteMedSAM, the difference isminimal, demonstrating a still competitive result.",
  "Average78.6480.5813.9981.9384.6111.01": "Furthermore, we observed significant instability in the original LiteMedSAM.Taking CT modality as an example, LiteMedSAM performed exceptionally wellon the validation set, surpassing Swin-LiteMedSAM. However, when evaluatedon the testing set, performance of CT experienced a significant performancedrop, with DSC falling from 92.26% to 55.75% and NSD dropping from 94.90%to 58.48%. Although Swin-LiteMedSAM encounters a similar issue with the CTmodality, the performance drop is much less severe. Furthermore, this issue is ob-served in other modalities as well, further approving that the Swin-LiteMedSAMmodel offers better stability and generalization, which are essential for the realworld applications.",
  "Qualitative results on external public dataset": "Since the ground truth for the challenge validation and testing set is not avail-able, we select SegRap2023 , a public head and neck CT dataset containingannotations for multiple organs, to verify the models performance.As depicted in , we showcase three representative examples from Seg-Rap2023 to visually check our models performance. In the first case, our modeldemonstrates strong performance in brain segmentation. This is primarily at-tributed to the brains large size and distinct contrast with surrounding tissues.Moving to the second case, we observed that our model maintains good perfor-mance even with smaller targets such as the spinal cord, esophagus, and trachea.However, in the third case, our models performance falls short compared to theground truth. The main issue arises from the ambiguous semantics in medicalimages. For instance, when aiming to segment the oral cavity, our method onlyidentifies the teeth. This discrepancy stems from the fact that the box promptfor the oral cavity can also be interpreted as segmenting teeth. It is hard toprovide a more precise prompt in this case to specify the intended target forsegmentation.",
  "Limitation and future work": "One main limitation of our method for this challenge is that our model is using2D images for training and validation, whereas medical imaging data, such asCT, MRI, and PET, are typically in 3D format. Currently, we process these 3Dimages by making predictions on individual 2D slices, which does not fully utilizethe 3D anatomical information and might hinder the performance improvement.The key issue is that the prompts input to the model are generally based on2D information, such as bounding boxes and points. In the future, we aim toexplore how to provide effective prompt information in 3D and adapt the modelto handle 3D images directly.Additionally, we applied certain manual rules to control the distribution ofbox-based points and the scribble, which is impossible to find the optimal setting",
  "R. Gao et al": "and can easily do harm to the overall performance if not set properly. Further-more, due to variations in medical modalities and the shapes of segmentationtargets, the distribution of points and scribble should be adjusted accordingly.Therefore, developing a learning-based method for generating box-based pointsand scribble would be highly beneficial and could further enhance the modelsperformance.",
  "Conclusion": "In this paper, we introduce Swin-LiteMedSAM, a lightweight box-based seg-ment anything model. Our model utilizes the tiny Swin Transformer as imageencoder, enabling it to extracts high-level features more effectively. Additionally,the introduction of box-based points and box-based scribble provide more spa-tial cues, which improve segmentation accuracy without substantially increasingcomputational costs or demanding extensive manual annotation. Overall, ourapproach achieves stronger and more stable performance across different med-ical imaging modalities while maintaining fast inference speed, outperformingthe LiteMedSAM model. Acknowledgements This study utilized computing resources from the Aca-demic Leiden Interdisciplinary Cluster Environment (ALICE) provided by Lei-den University. We thank all the data owners for making the medical images pub-licly available and CodaLab for hosting the challenge platform. This workwas supported by the China Scholarship Council (No. 202207720085) and theproject ROBUST: Trustworthy AI-based Systems for Sustainable Growth withproject number KICH3.LTP.20.006, which is (partly) financed by the Dutch Re-search Council (NWO), Philips Research, and the Dutch Ministry of EconomicAffairs and Climate Policy (EZK) under the program LTP KIC 2020-2023.",
  ". Cheng, D., Qin, Z., Jiang, Z., Zhang, S., Lao, Q., Li, K.: Sam on medical images:A comprehensive study on three prompt modes. arXiv preprint arXiv:2305.00035(2023) 4, 6": "3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image isworth 16x16 words: Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929 (2020) 2 4. He, K., Chen, X., Xie, S., Li, Y., Dollr, P., Girshick, R.: Masked autoencoders arescalable vision learners. In: Proceedings of the IEEE/CVF conference on computervision and pattern recognition. pp. 1600016009 (2022) 2",
  "Swin-LiteMedSAM for Medical Image Segmentation13": "5. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,Whitehead, S., Berg, A.C., Lo, W.Y., Dollar, P., Girshick, R.: Segment anything.In: Proceedings of the International Conference on Computer Vision. pp. 40154026 (2023) 2 6. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swintransformer: Hierarchical vision transformer using shifted windows. In: Proceedingsof the IEEE/CVF international conference on computer vision. pp. 1001210022(2021) 3 7. Luo, X., Fu, J., Zhong, Y., Liu, S., Han, B., Astaraki, M., Bendazzoli, S., Toma-Dasu, I., Ye, Y., Chen, Z., et al.: Segrap2023: A benchmark of organs-at-risk andgross tumor volume segmentation for radiotherapy planning of nasopharyngealcarcinoma. arXiv preprint arXiv:2312.09576 (2023) 10",
  ". Tan, M., Le, Q.: Efficientnetv2: Smaller models and faster training. In: Interna-tional conference on machine learning. pp. 1009610106. PMLR (2021) 2": "10. Wu, J., Fu, R., Fang, H., Liu, Y., Wang, Z., Xu, Y., Jin, Y., Arbel, T.: Medicalsam adapter: Adapting segment anything model for medical image segmentation.arXiv preprint arXiv:2304.12620 (2023) 2, 4 11. Xiong, Y., Varadarajan, B., Wu, L., Xiang, X., Xiao, F., Zhu, C., Dai, X., Wang,D., Sun, F., Iandola, F., et al.: Efficientsam: Leveraged masked image pretrainingfor efficient segment anything. arXiv preprint arXiv:2312.00863 (2023) 2"
}