{
  "Abstract": "We propose a simple strategy for masking image patchesduring visual-language contrastive learning that improvesthe quality of the learned representations and the trainingspeed. During each iteration of training, we randomly maskclusters of visually similar image patches, as measured bytheir raw pixel intensities. This provides an extra learningsignal, beyond the contrastive training itself, since it forcesa model to predict words for masked visual structures solelyfrom context. It also speeds up training by reducing theamount of data used in each image. We evaluate the effec-tiveness of our model by pre-training on a number of bench-marks, finding that it outperforms other masking strategies,such as FLIP, on the quality of the learned representation.",
  ". Introduction": "Images contain a great deal of redundant information, mak-ing it challenging to efficiently learn representations fromthem at scale.Recent work has addressed this problemby masking image patches during vision-language con-trastive learning .One simple approachis to drop a large fraction of the patches at random, mak-ing training more efficient by reducing the computationalcost and memory usage in each training iteration . Analternative strategy is to mask sets of semantically relatedpatches , such as those that belong to the sameobject. This forces the learned model to predict words thatdescribes missing scene structures from context, improvingthe learned representation. However, this approach requiresa separate mechanism to group together semantically re-lated patches, which adds considerable complexity to thelearning procedure and is computationally expensive.We propose a simple masking strategy for multimodalcontrastive learning that avoids these shortcomings. Duringtraining, we mask random clusters of patches (). Forthis clustering, we use the patches raw RGB values as thefeature representation. Our approach takes advantage of thefact that simple measures of visual similarity can often cap-ture coherent visual structures, such as object parts ,",
  "(c) Two surf boarson a beachnear the water": ". Cluster masking. We mask random clusters of visuallysimilar image patches when training contrastive vision-languagemodels (bottom).This masking strategy distinguishes our ap-proach from methods that independently mask image patches forefficiency (middle), while providing a similar improvement intraining speed. It provides an extra learning signal, since it forcesa model to predict words for missing scene structures solely fromcontext. especially when clusters are sampled randomly ().Our approach thus leads to more efficient training, like ap-proaches that independently drop patches , while im-proving the learned representation via context prediction.We take inspiration from masked region classification,a pre-training task widely used in vision-language models. These models extract object features, then pre-dict object labels for the randomly masked out regions. Ourmasking approach provides a similar training signal, sincemeaningful labels are included in the image caption. For ex-ample, as shown in (a), the model is tasked with as-sociating the words fire hydrant with an image even withthe hydrant itself is mostly masked out.",
  "arXiv:2405.08815v1 [cs.CV] 14 May 2024": "We train our model on Conceptual 12M dataset andevaluate our learned representation on a number of down-stream tasks. These tasks include the zero-shot classifica-tion and linear probing on ImageNet , text and imageretrieval on MS-COCO , and the SUGARCREPE lan-guage composition benchmark . In our experiments,our model outperforms FLIP and CLIP on down-stream performance, while having efficiency comparablewith FLIP. We also show that the performance can furtherbe improved by using the models learned feature embed-ding during clustering.",
  ". Related Work": "ContrastiveVision-LanguagePre-training.Vision-Language Pre-training (VLP) focuses on establishingconnections between images or their components andhuman-interpretable language. This field initially evolvedfrom transferring supervised learning models,whichincorporated object detection modules to generate fine-grained visual labels .Subsequently, therewas a shift towards large-scale learning using noisy webdata, moving away from reliance on fine-grained labels. A significant development in thisdomain was CLIP , which applied contrastive learningtechniques to train models to associate correctimage-text pairs and dissociate incorrect ones.CLIPscaled contrastive visual-language models significantlybeyond previous work, enabling strong feature learningand zero-shot performance.However, further scalingsignificantly increases the pre-training demands, requiringlarger datasets and batch sizes.In response to these challenges, recent research has ex-plored incorporating masking into images to reduce trainingtime and allow for more samples per batch .Methods such as MaskClip , FLIP , and VIOLET have implemented random masking strategies. Yet, ithas been noted that random masking may not be as effectiveon relatively small datasets . To address this, ACLIP introduced a method of masking tokens with low cross-attention scores with text. However, this approach neces-sitates two forward passes to generate the attention mapand requires additional computational modules . In ourwork, we aim to avoid these limitations, proposing an ef-fective masking method that is based on a patchs raw RGBvalues. Masked Image Modeling.In the field of language mod-eling, the effectiveness of models that learn to reconstructcorrupted inputs for generating robust features has beenrecognized . This approach, known as Mask Lan-guage Modeling (MLM), has been adapted in the realm ofimage processing as Mask Image Modeling (MIM). MIMtechniques involve reconstructing either image patches or their features . The pioneeringwork in BEIT introduced the reconstruction of discretetokens, akin to VQ-VAE , using block-wise masking.This method demonstrated results on par with contrastivelearning and self-distillation methods during modelfine-tuning. Later approaches include PeCos novel vi-sual codebook learning method and BEIT V2s inte-gration of self-distillation methods, using a teacher-studentbackbone and feature-level KL divergence loss . Fur-ther exploration in this field has led to the use of naturalimage signals as reconstruction targets, moving away fromlearned features. Examples include SimMIM , whichreconstructs pure RGB values, MaskFeat , introduc-ing reconstruction of the Histogram of Oriented Gradients(HOG) features, and MAE , which reconstructs pixel-normalized RGB values. Our work draws inspiration fromthese studies, particularly in using pixel-normalized RGBvalues to compute patch similarities, arguing for a more ef-fective distribution of patch features. Masking Strategies in MIM.Parallel investigations havefocused on masking strategies in MIMs .Early works like BEIT and its successors usedblock masking, while others such as SimMIM, MaskFeat,and MAE applied random patch-wise masking. Attention-based masking strategies have also been explored, typicallyusing attention maps from vision transformers. MST masks less essential parts with low attention scores, usinga reconstruction loss approach. In contrast, AttnMask masks highly attentive patches and applies self-distillationloss. These methods involve simultaneous updates of at-tention maps and masks during training. A potential lim-itation of this approach is that insufficiently trained atten-tion maps may not capture structured features effectively.SemMAE , starting with iBot features , adopts aneasy-to-hard masking strategy, starting with masking partswithin clusters and gradually expanding to entire clusters.Wilf et al. introduce a unique entity-reinforced lan-guage model for masking objects in video frames. However,the reliance on pre-trained features or extracting attentionmaps can be computationally intensive. Evolved Part Mask-ing proposes using EM algorithm on attention maps toget a clustering before performing SemMAE style masking.Our approach also adopts a cluster based masking strategyin vision-language pre-training, enabling faster pre-trainingwithout requiring additional modifications to the model.",
  "Anchor PatchDistance HeatmapMasked Image": ". Choosing clusters. The process begins by randomlyselecting anchor patches from the image. We then calculate thepairwise distances among all patches. Clusters formed within adistance threshold are masked out. We show cluster obtained froma single anchor patch. ters are then masked entirely. To enhance accuracy in clus-ter formation, we introduce an adaptive layer for refiningthe distance matrix. Additionally, attention masks and ahard patch cutoff are used to ensure uniform input sizes areconsistent in batches for auto differentiation.",
  ". Contrastive Vision-Language Pre-training": "Our approach builds on contrastive vision-language pre-training methods, such as CLIP . We use contrastivelearning to align embeddings of matching text-image pairsand separate those of non-matching pairs. This process issteered by two symmetric InfoNCE losses : the vision-to-language loss Lvl and its counterpart, the language-to-vision loss Llv. The vision-to-language loss is defined as:",
  ". Cluster Masking": "We introduce a masking strategy that drops out randomclusters.While one option would be to use an off-the-shelf clustering method, such as K-Means , we chooseinstead to use a simple and efficient method that resultsin a random clustering each training iteration ().Our approach resembles a single iteration of K-Means, andworks by selecting a set of exemplar patches, which eachdefine a cluster. In our experiments, we also evaluate mask-ing clusters obtained using K-Means as an alternative ap-proach.We split an input H W image into patches, follow-ing . We then compute the pairwise cosine similaritybetween every pair of normalized patches, which we useas a distance function d(x, y). We choose a small subset(less than 5%) of these patches at random to act as clustercenters. For each of these selected anchor patches, we de-fine a cluster consisting of patches that lie within a distance r. The cluster for an exemplar patch x is represented by:Sx = {y | d(x, y) r} for image patches.All patches within a cluster are masked out. The dis-tance threshold r is automatically searched before trainingaccording to an average masking ratio. We provide a sim-plified pseudocode of the masking strategy in Algorithm 1. Clustering Embedding Features.Another variant ofpatch feature is the combination of pure RGB values andpatch embedding layer features from transformers .When computing similarity scores, we integrate these twomeasures into a weighted sum, where the weight of eachmeasure is determined by:",
  "d(x, y) = drgb(x, y) + (1 ) demb(x, y),(2)": "where x and y represent two patches, drgb is the cosinesimilarity based on pure RGB values, and demb is the co-sine similarity based on the transformers embedding fea-tures. The weight parameter linearly increases from 0 to1 during training.The embedding layer is calculated before the patches en-ter the transformer, thus we could reuse the patch embed-dings in the transformer without computing them twice. Us-ing the embedding layer is advantageous because it incorpo-rates positional encodings . This integration potentiallyintroduces spatial constraints, which we believe can furtherenhance our masking strategy. Handling Batched Inputs.Deep learning libraries, likePyTorch , typically process batched inputs of uniformsize. However, in our method, the mask ratio would varyacross different images, leading to fluctuations in the num-ber of patches. To accelerate the process, we introduce aminimum mask ratio threshold for each image. If the cal-culated mask ratio for an image doesnt meet this predefinedthreshold, we proceed to randomly drop patches until thedesired ratio is achieved. Conversely, for images with patchcounts less than the threshold, we use attention masks toavoid masked parts engaging in attention calculation .",
  ". Implementation Details": "Datasets and Training Details.We train our model us-ing the Conceptual 12M (CC12M) dataset, containing12 million unique image-text pairs, for pre-training ourvision-language models.We use ViT-B/16 as backbonefor image encoder. The text encoder is a 12-layer trans-former, equipped with 8 multi-head attention units and 512-dimensional embeddings. Input images are processed at a",
  "bmm: batch matrix multiplication": "resolution of 224 224, and text inputs are adjusted to 77tokens, either by truncation or padding. A class token istransformed into a 512-dimensional feature embedding viaa multi-layer perceptron (MLP). For optimization, we usethe AdamW optimizer with a learning rate of 5 104,1 = 0.9, and 2 = 0.98. We use a batch size of 256 perGPU, and train using 8 NVIDIA A40 GPUs.Our method comes in three variants: K-Means, RGB andEmbedding. The RGB model clusters based on raw imagepatches, while the embedding model integrates patch em-bedding features with RGB for clustering. In the K-Meansvariant of the model, we mask out half of the clusters ran-domly. The model constructs 12 clusters and runs for amaximum of 10 iterations. For both RGB and embeddingmodels, we set an average masking ratio of 50%, follow-ing the recommendations of FLIP for optimal mask-ing ratios. In the RGB approach, we use a 50% cutoff forOurs-RGB0.5 and 30% for Ours-RGB0.3, whereas the Ours-Embedding model uses a 30% cutoff. Also, the RGB modelselects anchor patches at a 3% ratio, compared to a 5% ratioin the embedding model. Baselines.In our study, we establish baselines using threemodels:CLIP, FLIP, and FLIPAttn, each trained fromscratch on the CC12M dataset.These baseline mod-els are derived from the open-source implementation ofCLIP, known as OpenClip .For bothFLIP and FLIPAttn, we implement a patch dropout ratio of50%. Specifically, FLIP uses a random dropout approach,whereas FLIPAttn adopts an attention-based masking strat-egy inspired by ACLIP . This strategy involves pro-cessing the image through the encoder and then averaging across attention heads in the final transformer block to de-termine attention scores. Patches that receive the highestattention in relation to the [CLS] token are retained.To ensure a fair comparison among these methods, wekeep the number of patches consistent the same as ourswithin a single batch, which means for FLIP and FLIPAttn,we apply a batch size of 256 for one GPU and for CLIP, weuse 128 instead. Additionally, we apply a scaling law onlearning rate across different models. Evaluation Details.Our models are tested across vari-ous benchmarks to ensure its robustness and effectiveness.We conduct zero-shot image-to-text and text-to-image re-trieval tasks on COCO and Flickr , assessing itsperformance meticulously. Further more, we evaluate themodels image representation quality by reporting both thezero-shot classification and linear probing performance onthree mainstream datasets: ImageNet , CIFAR-10, andCIFAR-100 . Zero-shot results of some other datasetslike ImageNet variants , Caltech101 ,Flowers and Pets , are also reported to verify themethods robustness.For these tasks, our approach adheres strictly to the im-plementation used in the CLIP benchmark, ensuring consis-tency and reliability in the evaluation process. Furthermore,we assess the effectiveness of our methodology on languagecomposition tasks using SUGARCREPE . This evalua-tion aims to determine its adaptability and efficiency acrossvarious contexts, including object, attribute, and relationmanipulations. Within the SUGARCREPE framework, mod-els are tasked with identifying the correct caption that accu-rately describes an image, distinguishing it from a closelyrelated but incorrect text hard negatives. The hard negativesis characterized by minor composition differences from theaccurate caption.",
  ". Main Results": "Visualization of Clusters. offers a visual depic-tion of our cluster based masking technique as outlined inthe methodology section. For this illustration, we randomlyselect a number of image-text pairs from the COCO vali-dation set and apply our masking method to the pure RGBdata of the images. The visualization is showing the mask-ing result of the two-stages. In the first stage, a subset ofpatches (5%) is randomly selected as anchor patches fromthe pool of all image patches, which are annotated with thered boxes. In the second stage, we visualize the maskedclusters that are calculated based on the similarity matrix,where each cluster is represented by a distinct color.",
  ". Visualization of cluster masks. Different colors represent distinct clusters formed by the similarity matrix calculated from thechosen anchor patches": "retrieval tests on several leading retrieval benchmarks. Theresults, detailed in , provide insights into the per-formance of our approach against others, particularly in thecontext of Image2Text and Text2Images recall precision attop1(R1), top5(R5) and top10(R10) metrics.In the evaluation on the MS-COCO , Flickr8k, andFlickr30k datasets, our model outperforms both thebaselines in most parts. Notably, in the Image-to-Text tasks,our model performs best in most datasets, with the excep-tion of a slight performance decrease compared to FLIPAttnon the MS-COCO dataset.We attribute this success toour training strategy, which prioritizes primary clusters andminimizes the influence of noise. Furthermore, we observethat methods combining RGB information with token em-beddings outperform those relying solely on RGB. We hy-pothesize that this is because the embedding layer, whichcontains slightly higher-level information.When comparing FLIP to CLIP, FLIPs performance isnoticeably weaker, even with large batch sizes. We suspectthat FLIPs sub-optimal results in our experimental settingsmay not fully exploit its strengths. This aligns with find-ings from other studies, such as Yang et al.s research onACLIP , which also noted FLIPs limitations. We ob-serve that using attention scores for masking can improveperformance compared to purely random masking. How-ever, random masking still falls short of our cluster basedmasking or even the original CLIP method in some bench-marks.",
  "Results on Zero-shot Classification and Linear Probing.We evaluate our model on several widely recognized clas-": "sification benchmarks. The zero-shot classification resultsare presented in , while the linear probing resultscan be found in . For better evaluating the time spenton training, we normalize all methods training time by theCLIPs training time, which is considered as 1.When comparing our models performance to CLIP (i.e.,no masking), our model demonstrates superior results onthe majority of test cases, showcasing an average improve-ment of +2.1%, with about +36% speeding up. In compar-ison to the FLIP strategy, which has a similar training dura-tion, our model has an improvement of +5.5%. In compar-ison to the FLIPAttn, our model does not need the attentionmap for guidance, which gives a much fast training speed,while having a performance of +2.6% on average.Out of 12 datasets on the zero-shot classification bench-mark, our RGB and embedding model achieves the top per-formance on 11 of them. In particular, it obtains strongperformance on the ImageNet variants: ImageNet-A ,ImageNet-O , ImageNet-R , and ImageNet-S ,which often contain challenging and diverse images. TheRGB version of our method also significantly outperformsFLIP and surpasses the CLIP model, especially on Ima-geNet and its variants, which demonstrates the effectivenessof our method with even the natural guidance.The linear probing results further suggest the effective-ness of our method. Our models achieve +1.8% accuracyon ImageNet, +3.1% on CIFAR-10, and +4.2% on CIFAR-100.",
  ". Linear probing result. All methods are trained for 10epochs at learning rate of 1e-3. For CIFAR-10 and CIFAR-100,we use a batch size of 64 and for ImageNet-1k the batch size is1024": "in language. As we mask out clusters, there is a risk thatthe model may increasingly adopt bag-of-words tendencies, which could impede its ability to learn the relation-ships between objects. For example, if an image is cap-tioned with dog on grass, the grass may be masked fora large portion in our model as they are highly similar toeach other. This will make learning the relation on diffi-cult. Therefore, we apply SUGARCREPE benchmarksto test the models ability to understand language compo-sitions. SUGARCREPE benchmarks assess this by gener-ating negative captions through manipulations like adding,swapping, or replacing concepts in sentences, followed bytext retrieval tests to evaluate the models accuracy in se-lecting the correct answer. From the our test results, whichshown in , our model yields comparable results inRelation tests and demonstrates a significant enhancementin Object and Attribution tests, with an average improve-ment of +3.9% and +3.0% respectively, compared to FLIP.This improvement may stem from the masking of entire ob-jects, which simplifies the challenge of contrastive learningby reducing ambiguity. This clarity facilitates the modelslearning of relationships, a crucial factor for composition",
  "understanding": "Qualitative Comparison of Masking StrategiesOurmethod outperforms the random masking strategy by pre-serving more semantic content in the unmasked imagepatches, a comparison showcased in . The advan-tage of our technique is further explored by the caption-ing experiment detailed in , wherein two sets ofimages, each masked differently, are fed into a captioner,GPT-4 . The captioner is prompted to generate MS-COCO-style captions for the unobscured sections. Whencomparing these captions to the standard references, it be-comes clear that our cluster based masking not only retainskey elements but also the interrelations among them. Forinstance, our approach accurately enables the captioningsystem to identify an airplane in the first example and todescribe the baseball players action in the second, whilethe random masking strategy failed to achieve this clarity.These results indicate that our masking method provides amore detailed comprehension of the image.",
  ". Ablation Study": "Ablation on Anchor Patch Ratio.In our study, we con-ducted an ablation on finding the optimal proportion ofpatches to serve as anchor patches. The results of this abla-tion are summarized in . We use zero-shot learningresults on the ImageNet-1k dataset as the benchmark forassessing the quality of the representations learned by ourmodel. Additionally, we calibrate the threshold for each ex-periment to ensure that the average final masking ratio wasmaintained at 50%.Our findings indicate that a smaller proportion of anchorpatches tends to yield superior performance. We hypoth-esize that this improvement is due to the decreased ran-",
  "Ablation on Minimum Mask Ratio.We further demon-strate the capability of our method by setting the minimummask ratio the same as the average masking ratio of FLIP,": "as shown in , 2, and . For our method, thecutoff ratio denotes the minimum mask ratio applied andthe true visible patch ratio is shown in visible ratio. Forthe FLIP counterpart, it maintains a consistent mask ratioacross all images. The results indicate that our method notonly matches the speed of FLIP but also surpasses FLIPin zero-shot ImageNet-1K classification accuracy with a+1.6% improvement even by seeing fewer patches. The at-tention based masking method with less mask ratio achievessimilar perfromance as ours but the speed is much slower.These findings suggest that cluster based masking servesas an effective denoising technique for the dataset. A reasonfor this enhanced performance is that we could easily maskout typically irrelevant areas, such as uniformly coloredbackgrounds, which are less informative and oftentimes donot correspond to any word in the caption. This targetedapproach enables the model to focus on more meaningfulcontent within the images.Additionally, our findings reveal an improved featurelearning by the model when a smaller random masking isapplied. By reducing the cutoff ratio from 50% to 30%,we observed a 1% enhancement in classification accuracy.Thus, there is some trade-off between the models perfor-mance and speed. Despite this, our model with larger mask-ing cut off still remains significantly faster compared toattention-based masking or the original CLIP method.",
  "FLIP50%50%0.8434.4FLIP30%70%1.0035.4FLIPattn50%50%1.7335.2FLIPattn30%70%1.9736.6Ours-RGB50%43%0.8436.0Ours-RGB30%50%1.0036.6": ". Ablation on minimum mask ratio . Comparison ofvarious methods against different minimum masking ratios. Thezero-shot ImageNet-1k classification results are used as metric.The time is normalized to ours RGB model with =30%. Ablation on Pixel Normalization.In our experiments,we incorporate pixel normalization (making each patchmean zero and unit standard deviation 1) into the processof computing the similarity matrix for images. This yields a performance improvement of +1.1%, as shown in the re-sults presented in a. The underlying rationale forthis enhancement is attributed to the standardization of im-age patches. By using pixel normalization, we focus on therelative intensity of pixels, thereby diminishing the impactof lighting variations among different images.This normalization process is particularly beneficial inscenarios where the dynamic range of pixel values variessignificantly across different patches.By scaling thepatches to a common range, pixel-norm mitigates the riskof disproportionate influence from patches with higher in-tensity values. Consequently, this leads to a more balancedand equitable comparison among patches, enhancing themodels ability to discern and quantify similarities more ef-fectively.",
  "(b) Ablation on polynomial coef-ficient k": ". Ablation Study. a presents an ablation study onthe application of pixel normalization when calculating the simi-larity matrix for clustering. b explores the effects of vary-ing the polynomial coefficient k, which adjusts the adaptive rateused when combining RGB and embedding features. Effect of Features used in Clustering.In Tables 1 and 2,embedding-based methods surpass those dependent solelyon RGB data, especially in image-to-text retrieval tasks.One reason for this may be the fact that the embeddingmodel has access to the positional encoding, whereas theRGB-based model solely uses the appearance of each patch. qualitatively shows this advantage:while theRGB-only approach masks extra areas (such as hair or shad-ows in the first scenario; a laptop and phone in the sec-ond) due to color similarities, the embedding-based methodmasks more complete object parts. Ablation on Adaptive Rate.In our approach, we interpo-late between using RGB features and the patch embeddinglayer feature using a coefficient, , which varies with eachepoch. Denoting the current running epoch as Ec and thenumber of total training epochs as Et, this coefficient is de-",
  ". Limitations": "Our methodology uses a uniform threshold for all images, astrategy that, while effective, may not be the most optimal.Future research could explore the implementation of indi-vidualized thresholds for each image, potentially leading toa more intelligent and adaptive masking process.All of our approaches use the popular backbone archi-tecture ViT-B/16 and are trained solely on the CC12Mdataset . Expanding the scope of the experiments couldoffer additional insights.",
  ". Conclusion": "In our study, we introduce a novel cluster based mask-ing strategy designed for vision-language pret-raining. Us-ing either pure RGB values or shallow features from thepatch embedding layer, our method effectively clusters im-age patches, maintaining essential visual semantics.Wethen randomly mask out these clusters, enabling efficienttraining. Our approach demonstrates success across variousdownstream evaluation tasks, including both pure image-based tasks such as image classification and multimodaltasks like image-text retrieval and language compositiontests. We believe our work marks a considerable progres-sion in this domain and anticipate that it will stimulate fur-ther research into optimizing masking strategies for similarapplications. Author ContributionsAll authors contributed to design-ing projects, launching experiments, and writing the paper.Zixuan Pan focused on algorithm optimization; Zihao Weifocused on code design; Andrew Owens supervised thisproject, offered feedback, and assisted in writing the paper. AcknowledgementsThis research is supported by a SonyResearch Award. We are grateful to Jeongsoo Park, ChaoFeng, Yiming Dou, Daniel Geng, Ziyang Chen, and AyushShrivastava for their valuable suggestions and discussion. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. Advances inNeural Information Processing Systems, 35:2371623736,2022. 2",
  "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:Bert pre-training of image transformers.arXiv preprintarXiv:2106.08254, 2021. 2": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 96509660, 2021. 2 Soravit Changpinyo, Piyush Sharma, Nan Ding, and RaduSoricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 35583568, 2021. 2, 3,8 Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-woo Jun, David Luan, and Ilya Sutskever. Generative pre-training from pixels. In International Conference on Ma-chine Learning, pages 16911703. PMLR, 2020. 2 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-offrey Hinton. A simple framework for contrastive learningof visual representations. In International conference on ma-chine learning, pages 15971607. PMLR, 2020. 2",
  "X. Chen, S. Xie, and K. He. An empirical study of train-ing self-supervised vision transformers. In 2021 IEEE/CVFInternational Conference on Computer Vision (ICCV), pages96209629, 2021. 2": "Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin,Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo,Gang Zeng, and Jingdong Wang.Context autoencoderfor self-supervised representation learning. arXiv preprintarXiv:2202.03026, 2022. 2 Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:Universal image-text representation learning. In Europeanconference on computer vision, pages 104120. Springer,2020. 1, 2 Mehdi Cherti, Romain Beaumont, Ross Wightman, MitchellWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-ing laws for contrastive language-image learning. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 28182829, 2023. 4 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE Conference on Computer Vision andPattern Recognition, pages 248255, 2009. 2, 4, 6",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. Bert: Pre-training of deep bidirectional trans-formers for language understanding. 2018. 3": "Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen,Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, NenghaiYu, and Baining Guo. Peco: Perceptual codebook for bertpre-training of vision transformers. In Proceedings of theAAAI Conference on Artificial Intelligence, pages 552560,2023. 2 Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang,Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang,Lu Yuan,Dong Chen,Fang Wen,and Nenghai Yu.Maskclip:Masked self-distillation advances contrastivelanguage-image pretraining, 2023. 1, 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. ICLR, 2021. 3, 8",
  "Zhanzhou Feng and Shiliang Zhang.Evolved part mask-ing for self-supervised learning.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1038610395, 2023. 2": "Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William YangWang, Lijuan Wang, and Zicheng Liu. Violet: End-to-endvideo-language transformers with masked visual-token mod-eling. arXiv preprint arXiv:2111.12681, 2021. 2 Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and RossGirshick. Momentum contrast for unsupervised visual rep-resentation learning. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages97299738, 2020. 2",
  "Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-clip, 2021. If you use this software, please cite it as below.4": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In Internationalconference on machine learning, pages 49044916. PMLR,2021. 2 Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yan-nis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, andNikos Komodakis.What to hide from your students:Attention-guided masked image modeling. In Computer Vi-sion ECCV 2022, pages 300318. Springer Nature Switzer-land, 2022. 2",
  "LAION-AI. Clip benchmark. 2023. 6": "Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, BingSu, and Changwen Zheng. Semmae: Semantic-guided mask-ing for learning masked autoencoders. Advances in NeuralInformation Processing Systems, 35:1429014302, 2022. 1,2 Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation. Advances in neural infor-mation processing systems, 34:96949705, 2021. 2",
  "Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-hofer, and Kaiming He. Scaling language-image pre-trainingvia masking, 2023. 1, 2, 4, 6": "Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, YousongZhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, MingTang, et al. Mst: Masked self-supervised transformer forvisual representation. Advances in Neural Information Pro-cessing Systems, 34:1316513176, 2021. 2 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zit-nick. Microsoft coco: Common objects in context. In ECCV.European Conference on Computer Vision, 2014. 2, 4, 5, 6 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, MandarJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-moyer, and Veselin Stoyanov. Roberta: A robustly optimizedbert pretraining approach. arXiv preprint arXiv:1907.11692,2019. 2",
  "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-sentation learning with contrastive predictive coding. arXivpreprint arXiv:1807.03748, 2018. 3": "OpenAI. Gpt-4 technical report, 2023. 6, 7, 12 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-roll Wainwright, Pamela Mishkin, Chong Zhang, SandhiniAgarwal, Katarina Slama, Alex Ray, John Schulman, JacobHilton, Fraser Kelton, Luke Miller, Maddie Simens, AmandaAskell, Peter Welinder, Paul F Christiano, Jan Leike, andRyan Lowe. Training language models to follow instructionswith human feedback. In Advances in Neural InformationProcessing Systems, pages 2773027744. Curran Associates,Inc., 2022. 6, 7, 12",
  "Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and FuruWei. Beit v2: Masked image modeling with vector-quantizedvisual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,Gabriel Goh, Sandhini Agarwal, Girish Sastry, AmandaAskell, Pamela Mishkin, Jack Clark, Gretchen Krueger, andIlya Sutskever. Learning transferable visual models from nat-ural language supervision. In ICML, 2021. 4 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2, 3, 6 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2",
  "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, andVaishaal Shankar. Do imagenet classifiers generalize to im-agenet?, 2019. 4, 6": "Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade W Gordon, Ross Wightman, Mehdi Cherti, TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, Patrick Schramowski, Srivatsa R Kundurthy, KatherineCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and JeniaJitsev.LAION-5b: An open large-scale dataset for train-ing next generation image-text models. In Thirty-sixth Con-ference on Neural Information Processing Systems Datasetsand Benchmarks Track, 2022. 4",
  "Andrea Vedaldi. Cats and dogs. In Proceedings of the 2012IEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), page 34983505, USA, 2012. IEEE ComputerSociety. 4, 6": "Haohan Wang, Songwei Ge, Zachary Lipton, and Eric PXing. Learning robust global representations by penalizinglocal predictive power. In Advances in Neural InformationProcessing Systems, pages 1050610518, 2019. 4, 5, 6 Yiqing Wang, Zihan Li, Jieru Mei, Zihao Wei, Li Liu, ChenWang, Shengtian Sang, Alan Yuille, Cihang Xie, and YuyinZhou. Swinmm: Masked multi-view with swin transformersfor 3d medical image segmentation. In MICCAI, 2023. 2 Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, AlanYuille, and Christoph Feichtenhofer. Masked feature predic-tion for self-supervised visual pre-training. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1466814678, 2022. 2",
  "Jiahao Xie, Wei Li, Xiaohang Zhan, Ziwei Liu, Yew SoonOng, and Chen Change Loy.Masked frequency model-ing for self-supervised visual pre-training.arXiv preprintarXiv:2206.07706, 2022. 2": "Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, JianminBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simpleframework for masked image modeling. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 96539663, 2022. 2 Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park,Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gan-gopadhyay, Andrew Owens, et al. Binding touch to every-thing: Learning unified multimodal tactile representations.arXiv preprint arXiv:2401.18084, 2024. 2",
  "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastivecaptioners are image-text foundation models. arXiv preprintarXiv:2205.01917, 2022. 2": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,Boxin Li,Chunyuan Li,et al.Florence:A newfoundation model for computer vision.arXiv preprintarXiv:2111.11432, 2021. 2 Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,Dan Jurafsky, and James Zou.When and why vision-language models behave like bags-of-words, and what todo about it?In The Eleventh International Conference onLearning Representations, 2022. 6",
  "A. Qualitative Comparison of Masking Strat-egy": "Our method outperforms a random masking strategy bypreserving more semantic content in the unmasked imagepatches, a comparison showcased in . The advan-tage of our technique is underscored by the captioning ex-periment detailed in , wherein two sets of images,each masked differently, were fed into a captioner, GPT-4 . The model was tasked to generate MSCOCO-style captions for the unobscured sections. When compar-ing these captions to the standard references, it becomesclear that our cluster based masking not only retains keydepicted elements but also the interrelations among them.For instance, our approach accurately enabled the caption-ing system to identify an airplane in the first example andto describe the baseball players action in the second, whilethe random masking strategy failed to achieve this clarity.These results indicate that our masking method provides amore detailed comprehension of the image.",
  "B. Visualization of attention-based masking": "We extend with examples from the attention-guided baseline (). In contrast to our RGB model,the behavior of the attention-based method changes dur-ing training. In early iterations, it masks randomly, whilelater in training it produces fairly consistent clusters that donot vary much between iterations, since the attention mapschange less over time, potentially limiting the diversity oftraining examples."
}