{
  ". Introduction": "This technical report outlines our method for generating asynthetic dataset for semantic segmentation using a latentdiffusion model, as described in . Our approach elim-inates the need for additional models specifically trainedon segmentation data and is part of our submission tothe CVPR 2024 workshop challenge, entitled CVPR 2024workshop challenge SyntaGen - Harnessing GenerativeModels for Synthetic Visual Datasets. In the following,we describe the development of our pipeline and the train-ing of a DeepLabv3 model on the generated dataset. Ourmethodology is inspired by the attention interpretation tech-niques used in Dataset Diffusion . To implement the self-attention gathering, we leverage the DAAM block. Theseself-attentions facilitate a novel head-wise semantic infor-mation condensation, thereby enabling the direct acquisi-tion of class-agnostic image segmentation from the StableDiffusion latents. Furthermore, we employ OVAM fornon-prompt-influencing cross-attentions from text to pixel,thus facilitating the classification of the previously gener-ated masks. Finally, we propose a mask refinement step byusing only the output image by Stable Diffusion. Our codeis available on GitHub1.",
  ". Diagram of the Pipeline (Figure Adapted from OVAM)": "self-attentions across all heads and timesteps to managethese large tensors, our approach considers each heads fea-tures independently in the final iteration step. This allowsus to generate masks that isolate the semantic content of theoutput image. The overall pipeline is illustrated in FIGURE1, with our contribution highlighted in the red dotted area.",
  ". Process Self-Attentions": "Each head in the denoising network targets distinct featuresand objectives. We hypothesize that the attention mecha-nism, being critical for image reconstruction, conveys se-mantic information. Hence, our objective is to extract se-mantic meanings that are interpretable by humans and cor-respond specifically to the Pascal VOC classes. To achievethis, we employ Principal Component Analysis (PCA) foreach head, reducing the feature dimensionality from 64 to 3.This procedure is repeated at each upsampling layer, froma resolution of 16 16 up to 64 64. By performing such",
  ". Clustering and Classification": "After computing the principal components for each head,we upsample all smaller outputs to a resolution of 64 64.Subsequently, we concatenate the principal components ofeach head for every individual pixel to form a single tensor.To enhance robustness, we normalize the features and in-clude the relative pixel positions as additional features. As,according to the challenge rules, the use of annotated masksis prohibited, we employ unsupervised K-Means clusteringmultiple times with varying numbers of clusters. In do-ing so, this approach generates clusters by minimizing thesquared Euclidean distance of the head-wise principal com-ponents. To ensure reproducibility, we fix the random ini-tialization of clusters.Determining the optimal number of clusters is complexas it depends on multiple factors, which cannot be exhaus-tively defined and extend beyond class, scene, and seman-tic meaning. To address this and maintain flexibility, wecluster using clusters, aiming for a rough segmen-tation of main objects and a finer segmentation of smallerparts. We separate clusters that are not directly connectedvia 4-connected neighborhood pixels.For class assignment to the masks, we utilize cross-attention maps from OVAM , enabling class identifica-tion independent of the input prompt. We avoid optimizedtokens to keep the approach generalized and free from anysegmentation labels.Additionally, we observed that thestart-of-text (SoT) token can serve as an indicator for thebackground class. Furthermore, we replace certain classnames with more descriptive token names (see TABLE 1).",
  "diningtabletabletvmonitormonitorpottedplantpot plantaeroplaneairplane": "We iterate over all clusters and apply varying confi-dence thresholds to the class-wise cross-attention mapsfrom OVAM. Initially, we normalize all original attentionvalues to the range , followed by multiple binarythresholding at 0.3, 0.5, and 0.8. For the background class,derived from the SoT token, we increase the confidencethreshold by 20% of the current threshold to mitigate theexcessive influence of the background. For the final clas-sification, we compute the Intersection over Union (IoU) for each cluster against the class-wise binary map, assign-ing the class with the highest IoU to all pixels within thatcluster for the defined thresholds. This process is repeatedacross all clusters and thresholds, with the most frequentclass for each pixel determined by the mode. If the dom-inant class for a pixel constitutes 50% or less, the pixel islabeled as uncertain. This procedure yields a 64 64 res-olution mask. To enhance this low-resolution mask to theimage resolution, we utilize the output image knowledgefrom Stable Diffusion.",
  ". Mask Refinement": "All previous operations were exclusively applied to the ex-tracted latents of Stable Diffusion. For the final mask refine-ment, we utilize the RGB values of the output image andreintroduce the pixel positions. We employ K-Means clus-tering with a fixed cluster count of 20 and separate discon-nected clusters. This approach allows us to identify regionsthat are coherent in terms of color and proximity, irrespec-tive of the images semantic content. A class is assignedto these new clusters if the dominant class constitutes morethan 66% of the pixels within the cluster, ensuring accurateclassification with high confidence. Due to the large andfixed cluster count, the K-Means clustering algorithm maygenerate artifacts in certain segmentation masks. These ar-tifacts are filled with the uncertainty class, and such pixelsare excluded from further influence during training.",
  ". Results": "To evaluate the effectiveness of our dataset generationmethod, we produced just under 10,000 images for our fi-nal submission to the challenge, constrained by time limi-tations. To assess the quality of the synthetic dataset, weexclusively trained a DeepLabv3 model on the gener-ated images. We identified the best-performing checkpointby calculating the mean Intersection over Union (mIoU) onthe Pascal VOC validation set at every 1000 training it-erations. The checkpoint exhibiting the highest mIoU wassubmitted to the challenge organizers, accompanied by itschecksum.",
  "The entry was disqualified due to accidentaluse of segmentation labels in a module. Thelisted score is the postmortem correction": "In TABLE 2 the results on the private test set are pre-sented. Our submitted model performed slightly below thebaseline established by Nguyen et al., with a differenceof 0.4 mIoU. Some illustrative examples are provided in theappendix in FIGURE 2. Further, a comprehensive analysisof the performance of a DeepLabv3 model trained on ourdataset is detailed in TABLE 3 in the appendix, encompass-ing both the private Syntagen and Pascal VOC validationset.",
  ". Conclusion": "Certain classes in the dataset exhibit varying levels of diffi-culty. Our method encountered significant challenges withthe person class due to its close embedding with otherclasses, which resulted in a lower IoU. Structurally chal-lenging classes, such as sofa and chair, also presented dif-ficulties. In Pascal VOC, the primary distinction betweenthese two classes is that sofas are typically two-seaters,while chairs are usually one-seaters. This subtle differenceseems to be challenging for the model to accurately discern.These challenges underscore the need for enhanced promptengineering to improve performance.Overall, we have developed a novel approach for seman-tically segmenting images generated by Stable Diffusion byleveraging only their latent representations. Our method isparticularly effective for classes such as potted plant, bird,dog, cat, sheep, boat, producing highly precise masks (seeFIGURE 2 in the appendix). This precision is achieved byutilizing head-wise features from both the latent and overallimage spaces."
}