{
  "ABSTRACT": "Compact UAV systems, while advancing delivery and surveil-lance, pose significant security challenges due to their smallsize, which hinders detection by traditional methods. Thispaper presents a cost-effective, unsupervised UAV detec-tion method using spatial-temporal sequence processing tofuse multiple LiDAR scans for accurate UAV tracking inreal-world scenarios. Our approach segments point cloudsinto foreground and background, analyzes spatial-temporaldata, and employs a scoring mechanism to enhance detectionaccuracy.Tested on a public dataset, our solution placed4th in the CVPR 2024 UG2+ Challenge, demonstratingits practical effectiveness.We plan to open-source all de-signs, code and sample data for the research community @github.com/lianghanfang/UnLiDAR-UAV-Est.",
  ". INTRODUCTION": "Drones have revolutionized various industries by allowingprecise fertilization in agriculture and allowing detailed in-spection of hard-to-reach structures .However, thepotential for malicious drone use is a major concern. Theycan be used for unauthorized surveillance, drug trafficking,smuggling, and even the deployment of grenades in warzones. This threat highlights the urgent need for advanceddetection systems to detect hostile drones effectively.Detecting compact UAVs is challenging. Existing solu-tions rely on UAV control signals to detect, but can beeasily bypassed by changing frequencies, using 5G networks,or fully autonomous drones .Visual-based methods struggle with small objects at high altitudes. Narrowfield-of-view cameras mounted on buildings can be operatedmanually to see the drone , but this is impracticalfor field operations. Wide field of view cameras can monitora larger area, but often only capture a few pixels of the droneas shown in . Radar can detect drones effectively,but cheaper models are noisy and expensive ones are ex-pensive and power demanding . Audio-based detection is intuitive but often less effective, with mostcommercial drones being very quiet at a distance. LiDAR",
  ". Illustration of detecting and tracking compact dronesusing a single low-cost sparse LiDAR to identify threats": "can detect drones, but its data is sparse at long ranges . Ingeneral, there is no perfect solution for drone detection.This work aims to accurately detect drones regardless oftheir control signal frequency or autonomy, including smalldrones at high altitudes, without manual operation. It ensurespracticality for wide field operations and affordability for usein a single person or a single vehicle, as shown in .In this paper, we propose a concurrent clustering methodfor analyzing point clouds from a low-cost 3D LiDAR sys-tem. First, we perform global-local clustering to exclude largestatic objects. Then, we refine clustering using spatiotempo-ral density and voxel attributes to identify moving targets andisolate the UAV trajectory. Finally, we use spline fitting toreconstruct the UAVs spatial trajectory, enhancing detectionaccuracy, reducing noise, and eliminating irrelevant data forclearer insights into drone movements.The main contributions of our work are as follows:",
  "Spatio-Temporal Analysis: Our spatio-temporal voxeland density analysis method, with a scoring mecha-nism, isolates the correct trajectory point set": "Extensive Benchmarking:We benchmarked andtested various modalities with different methods tovalidate the performance of the system. To the best ofour knowledge, this is the first benchmark of its kindfor Anti-UAV study. This paper has been accepted for presentation at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2025. 2025IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses.",
  ". RELATED WORKS": "This section reviews the limited literature on UAV detectionand tracking, focusing on a few key approaches.Vision-based UAV detection has evolved through deeplearning, addressing challenges highlighted in studies such asDet-Fly and MAV-VID, Drone-vs-Bird, and Anti-UAV. The methods have improved accuracy by augmentingthe data and optimizing YOLOv4 for small UAV detection, and through transfer learning and adaptive fusion usingsimulated data .Motion-assisted MAV detection integrates motion andappearance features using fixed and mobile cameras. Fixedcamera methods employ background subtraction and CNN-based classification , while mobile cameras utilize spatio-temporal characteristics -, but can struggle in dy-namic environments.Another approach combines appear-ance and motion-based classification to distinguish MAVsfrom distractors , albeit facing challenges with similarmoving objects.Detection from moving cameras is complex due to thebackground and target motion mixing together. Methods us-ing UAV-to-UAV datasets and hybrid classifiers contendwith background interference. Two-stage segmentation andfeature super-resolution offer advancements but grap-ple with issues like motion blur and occlusions in complexsettings.LiDAR systems, widely used for object detection andtracking, face unique challenges with UAVs due to their small size, shape variability, diverse materials, high speeds, and un-predictable movements. One method adjusts the integrationtime of the LiDAR frame based on drone speed and distanceto improve the density and size of the point cloud, but thisapproach is intricate and sensitive to parameter settings .Another strategy reduces LiDAR beams with probabilisticanalysis and repositions the sensor for wider coverage, yet itstruggles with continuous tracking of small points .Segmentation methods combined with object modelsand temporal information improve the effectiveness of UAVdetection and tracking effectiveness, though they are con-strained by segmentation and object model accuracy .Euclidean distance clustering and particle filtering algorithmsoffer accurate yet computationally efficient solutions, albeitsensitive to data noise and outliers . In summary, whileseveral methods address the challenges of UAV detection andtracking with LiDAR, each method presents distinct limita-tions and complexities, underscoring the need for ongoingresearch and development in this domain.",
  ". Global-Local Point Set clusterings": "Let F denote a sequence of LiDAR scan frames, with f de-noting the number of frames. P represents a set of 3D pointsfrom a single scan in F . The number of points in set P isdenoted card P. C denotes a cluster (subset) of points fromP and denotes the density of points, V denotes the voxelsof a set of points.For local representation, (P|Fi) represents the set ofpoints P within the i-th frame F. Cj (P|Fi) denotes the j-thcluster category of points from (P|Fi). For global represen-tation, ji F denotes frame sequences from frame i to frame",
  "j. CkP| ji Fdenotes the k-th category of the cluster": "after merging the points from ji F.To distinguish between the results of global and localclustering, CkP, Fn | ji Frepresents the k-th cluster ofpoints in the n-th frame, where the clustering is derived fromthe sequence of frames ji F.And V (C) indicates the size of the voxel occupied bycluster C in the space. Let C be an operator that denotesthe density of points in a cluster C derived from a set of pointsP in the context of a frame sequence F.We first superimpose the point cloud on the global timeframes to obtainP| ji F, then use DBSCAN to perform",
  ". Scoring Mechanism and Trajectory Prediction": "For moving objects, the positions of the voxels change overtime frames, causing a lower alignment compared to that ofstationary objects. Stationary surfaces show an increase inpoint cloud density over time, while moving objects maintaina consistent density. We propose a scoring mechanism basedon these density and voxel shifts, using a logarithmic functionto stabilize and scale voxel IoU.We define a voxel coincidence score for cluster k betweenlocal point set frame C (P|Fi) and C (P|Fj) as kIoU.Definethe score of point set density matching between (C (P|F))",
  "k = k + kIoU(6)": "Based on the proposed scoring scheme, the category withthe highest score k can be identified as the final target withthe highest confidence.For the final trajectory based on the time frame, we usespline fitting on the UAV point cloud and interpolate basedon the time frame to determine the spatial position at the cor-responding time points. Define the cloud frame of the kthpoint after segmenting the background as P ks . Sort the pointclouds of each time frame according to the timestamp andmerge them into a set of points Puav =P 0s , P 1s , ..., P ks.Among them, the points in the set of points Puav are selectedas control points, and the three-dimensional spline S(u) canbe expressed as:",
  ". Dataset": "We evaluated our algorithm on the difficult part of theMMAUD , namely MMAUD v2 and MMAUD v3 se-quences, featuring visual, LiDAR array, RADAR, and audioarray sensors, with over 1700 seconds of multi-modal datain 50 sequences. Each sequence includes millions of sam-pling points of visual, LiDAR, audio, and radar data. ForMMAUD V1 sequences, most detections are easy as UAVstypically fly within 40 meters. However, for MMAUD v2 andMMAUD v3 sequences, the 100-meter range makes smallerUAVs harder to detect with LIDAR.",
  ". Evaluation Metrics": "We evaluate our algorithm using RMSE error, which directlyevaluates system prediction accuracy in various conditions.By varying the lighting conditions, we can better understandthe performance of each baseline method. The overall visualperformance can be seen from , where green representsdrone trajectories segmented through global and local clus-tering, red denotes actual drone positions, and blue indicatespredicted positions. Our approach excels in noise reductionand precise drone trajectory extraction from point cloud data.",
  "The proposed solution demonstrates robust performance un-der various lighting conditions, as shown in table 1. Tradi-tional supervised LiDAR-based methods often expect dense": "data with large object sizes and end up with some of the worstperformance due to sparse LiDAR data reflected by compactUAVs. Visual-based approaches perform well during the daywith denser sampling points but exhibit significant perfor-mance drops at night. Audio-based methods show consistentperformance day and night, but the overall accuracy is low.Our proposed solution manages to perform robust dronepose estimation for both day and night, even with very sparsepoint clouds. This shows that it is a practical solution for earlywarning applications of UAVs.",
  ". CONCLUSION AND FUTURE WORKS": "This paper introduces an unsupervised approach for ro-bust ground-based UAV detection using spatial-temporaland global-local clustering of sparse point cloud sequences.Our method extracts precise UAV trajectories from sparseand noisy data. We plan to open-source our design, codes,scripts, and sampled data. In future work, we aim to integrateactive countermeasures, leveraging UAVs or EMP devices, toeffectively neutralize drone threats using proposed perceptioninputs.",
  ". REFERENCES": "Muqing Cao, Kun Cao, Xiuxian Li, Shenghai Yuan, Yang Lyu, Thien-Minh Nguyen, and Lihua Xie, Distributed multi-robot sweep coveragefor a region with unknown workload distribution, Autonomous Intelli-gent Systems, vol. 1, no. 1, pp. 13, 2021. Yang Lyu, Muqing Cao, Shenghai Yuan, and Lihua Xie,Vision-based plane estimation and following for building inspection with au-tonomous uav, IEEE Transactions on Systems, Man, and Cybernetics:Systems, vol. 53, no. 12, pp. 74757488, 2023. Zheng Si, Chao Liu, Jianyu Liu, and Yinhao Zhou, Application of snnsmodel based on multi-dimensional attention in drone radio frequencysignal classification, in ICASSP 2024 - 2024 IEEE International Con-ference on Acoustics, Speech and Signal Processing (ICASSP), 2024. Ryan J Wallace, Kristy M Kiernan, Tom Haritos, John Robbins, andJon M Loffi, Evaluating small uas operations and national airspacesystem interference using aeroscope, Journal of Aviation Technologyand Engineering, vol. 8, no. 2, pp. 24, 2019.",
  "Shenghai Yuan, Han Wang, and Lihua Xie, Survey on localizationsystems and algorithms for unmanned systems, Unmanned Systems,vol. 9, no. 02, pp. 129163, 2021": "Thien-Minh Nguyen, Muqing Cao, Shenghai Yuan, Yang Lyu,Thien Hoang Nguyen, and Lihua Xie, Viral-fusion: A visual-inertial-ranging-lidar sensor fusion approach, IEEE Transactions on Robotics,vol. 38, no. 2, pp. 958977, 2022. Angelo Coluccia, Alessio Fascista, Lars Sommer, Arne Schumann,Anastasios Dimou, Dimitrios Zarpalas, and Nabin Sharma, Drone-vs-bird detection grand challenge at icassp2023, in ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP), 2023. Sahaj K Mistry, Shreyas Chatterjee, Ajeet K Verma, Vinit Jakhetiya,Badri N Subudhi, and Sunil Jaiswal,Drone-vs-bird: Drone detec-tion using yolov7 with csrt tracker,in ICASSP 2023 - 2023 IEEEInternational Conference on Acoustics, Speech and Signal Processing(ICASSP), 2023.",
  "Chenxing Wang, Jiangmin Tian, Jiuwen Cao, and Xiaohong Wang,Deep learning-based uav detection in pulse-doppler radar,IEEETransactions on Geoscience and Remote Sensing, vol. 60, pp. 112,2022": "Sara Al-Emadi, Abdulla Al-Ali, Amr Mohammad, and Abdulaziz Al-Ali, Audio based drone detection and identification using deep learn-ing, in 2019 15th International Wireless Communications & MobileComputing Conference (IWCMC), 2019, pp. 459464. Allen Lei, Tianchen Deng, Han Wang, Jianfei Yang, and ShenghaiYuan,Audio array-based 3d uav trajectory estimation with lidarpseudo-labeling,in ICASSP 2025 - 2025 IEEE International Con-ference on Acoustics, Speech and Signal Processing (ICASSP), 2025. Zhenyuan Xiao, Huanran Hu, Guili Xu, and Junwei He, Tame: Tem-poral audio-based mamba for enhanced drone trajectory estimation andclassification, in ICASSP 2025 - 2025 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), 2025. Zhenyuan Xiao, Yizhuo Yang, Guili Xu, Xianglong Zeng, andShenghai Yuan,Av-dtec:Self-supervised audio-visual fusionfor drone trajectory estimation and classification,arXiv preprintarXiv:2412.16928, 2024. Matous Vrba, Viktor Walter, Vaclav Pritzl, Michal Pliska, Tomas Baca,Vojtech Spurny, Daniel Hert, and Martin Saska, On onboard lidar-based flying object detection, IEEE Transactions on Robotics, vol. 41,pp. 593611, 2025. Hanfang Liang, Jinming Hu, Xiaohuan Ling, and Bing Wang, Sep-arating drone point clouds from complex backgrounds by cluster fil-ter technical report for cvpr 2024 ug2 challenge,arXiv preprintarXiv:2412.16947, 2024. Ye Zheng, Zhang Chen, Dailin Lv, Zhixing Li, Zhenzhong Lan, andShiyu Zhao,Air-to-air visual detection of micro-uavs: An experi-mental evaluation of deep learning, IEEE Robotics and AutomationLetters, vol. 6, no. 2, pp. 10201027, 2021. Brian K. S. Isaac-Medina, Matt Poyser, Daniel Organisciak, Chris G.Willcocks, Toby P. Breckon, and Hubert P. H. Shum, Unmanned aerialvehicle visual detection and tracking using deep neural networks: Aperformance benchmark, in 2021 IEEE/CVF International Conferenceon Computer Vision Workshops (ICCVW), 2021, pp. 12231232.",
  "Ulzhalgas Seidaliyeva, Daryn Akhmetov, Lyazzat Ilipbayeva, andEric T Matson,Real-time and accurate drone detection in a videowith a static background, Sensors, vol. 20, no. 14, pp. 3856, 2020": "Jiayang Xie, Chengxing Gao, Junfeng Wu, Zhiguo Shi, and JimingChen, Small low-contrast target detection: Data-driven spatiotemporalfeature fusion and implementation, IEEE Transactions on Cybernet-ics, vol. 52, no. 11, pp. 1184711858, 2022. Jiayang Xie, Jin Yu, Junfeng Wu, Zhiguo Shi, and Jiming Chen,Adaptive switching spatial-temporal fusion detection for remote fly-ing drones, IEEE Transactions on Vehicular Technology, vol. 69, no.7, pp. 69646976, 2020. Ye Zheng, Canlun Zheng, Xiaoyu Zhang, Fei Chen, Zhang Chen, andShiyu Zhao, Detection, localization, and tracking of multiple mavswith panoramic stereo camera networks, IEEE Transactions on Au-tomation Science and Engineering, vol. 20, no. 2, pp. 12261243, 2023. Jing Li, Dong Hye Ye, Mathias Kolsch, Juan P. Wachs, and Charles A.Bouman,Fast and robust uav to uav detection and tracking fromvideo, IEEE Transactions on Emerging Topics in Computing, vol. 10,no. 3, pp. 15191531, 2022. Muhammad Waseem Ashraf, Waqas Sultani, and Mubarak Shah, Dog-fight: Detecting drones from drones videos, in 2021 IEEE/CVF Con-ference on Computer Vision and Pattern Recognition (CVPR), 2021,pp. 70637072. Hanzhuo Wang, Xingjian Wang, Chengwei Zhou, Wenchao Meng, andZhiguo Shi, Low in resolution, high in precision: Uav detection withsuper-resolution and motion information extraction, in ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP), 2023. Li Qingqing, Yu Xianjia, Queralta Queralta, Jorge Pena, and TomiWesterlund, Adaptive lidar scan frame integration: Tracking knownmavs in 3d point clouds, in 2021 20th International Conference onAdvanced Robotics (ICAR), 2021, pp. 10791086.",
  "Yin Zhou and Oncel Tuzel, Voxelnet: End-to-end learning for pointcloud based 3d object detection, in 2018 IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2018, pp. 44904499": "R. Qi Charles, Hao Su, Mo Kaichun, and Leonidas J. Guibas, Point-Net: Deep Learning on Point Sets for 3D Classification and Segmen-tation , in 2017 IEEE Conference on Computer Vision and PatternRecognition (CVPR), Los Alamitos, CA, USA, July 2017, pp. 7785,IEEE Computer Society. Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang,and Oscar Beijbom, Pointpillars: Fast encoders for object detectionfrom point clouds, in 2019 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), 2019, pp. 1268912697. Zhipeng Ding, Xu Han, and Marc Niethammer,Votenet: A deeplearning label fusion method for multi-atlas segmentation, in Medi-cal Image Computing and Computer Assisted Intervention MICCAI2019, Dinggang Shen, Tianming Liu, Terry M. Peters, Lawrence H.Staib, Caroline Essert, Sean Zhou, Pew-Thian Yap, and Ali Khan, Eds.,Cham, 2019, pp. 202210, Springer International Publishing."
}