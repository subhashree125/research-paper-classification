{
  "Abstract": "The performance of Federated Learning (FL) hinges onthe effectiveness of utilizing knowledge from distributeddatasets. Traditional FL methods adopt an aggregate-then-adapt framework, where clients update local models basedon a global model aggregated by the server from the previ-ous training round. This process can cause client drift, es-pecially with significant cross-client data heterogeneity, im-pacting model performance and convergence of the FL al-gorithm. To address these challenges, we introduce FedAF,a novel aggregation-free FL algorithm. In this framework,clients collaboratively learn condensed data by leveragingpeer knowledge, the server subsequently trains the globalmodel using the condensed data and soft labels receivedfrom the clients.FedAF inherently avoids the issue ofclient drift, enhances the quality of condensed data amidnotable data heterogeneity, and improves the global modelperformance. Extensive numerical studies on several pop-ular benchmark datasets show FedAF surpasses variousstate-of-the-art FL algorithms in handling label-skew andfeature-skew data heterogeneity, leading to superior globalmodel accuracy and faster convergence.",
  ". Introduction": "Federated Learning (FL) algorithms typically follow an it-erative aggregate-then-adapt paradigm in which clients usetheir local private data to refine a global model provided bya central server. These locally updated models are subse-quently returned to and aggregated by the server, where theglobal model is updated through averaging the local modelsaccording to predetermined rules . Given the decen-tralized nature of client data in FL, substantial variationsin the clients data distribution are common. This scenarioresults in non-Independent and Identically Distributed (non-IID) data across clients, often referred to as data heterogene-ity. Such heterogeneity presents considerable challenges tothe convergence of FL algorithms, primarily due to the sig-nificant drift in local learning paths among clients .This phenomenon, known as client drift, can significantly",
  "(b) Aggregation-free FL approach": ". The conventional aggregate-then-adapt approach (a) isprone to client drift in data-heterogeneous scenarios, as clients up-date a downloaded global model and risk forgetting prior knowl-edge. In contrast, the aggregation-free paradigm (b) has the servertrain the global model directly using condensed synthetic datalearned and shared by clients, which circumvents the client driftissue. decrease the accuracy of the global model. .Most existing efforts to address the challenge of dataheterogeneity have focused either on modifying the localmodel training with additional regularization terms or on utilizing alternative server-side aggrega-tion or model update schemes . Nev-ertheless, these methods remain constrained by the conven-tional aggregate-then-adapt framework, as depicted in Fig-ure 1a. In scenarios with strong cross-client non-IID datadistribution, the fine-tuning of the global model over localdata becomes susceptible to catastrophic forgetting. Thismeans that clients tends to forget the knowledge learned inthe global model and diverge away from the stationary pointof global learning objective when they update the model in-",
  "arXiv:2404.18962v1 [cs.CV] 29 Apr 2024": "dividually .Recent works on data condensation suggest an aggregation-free paradigm that has the poten-tial to overcome the limitations mentioned above in FLwith privacy preservation.As depicted in b, inthis new framework, each client first learns a compact setof synthetic data (i.e., the condensed data) for each classand then shares the learned condensed data with the server.The server then utilizes the received condensed data to di-rectly update the global model. However, in the current re-search on the aggregation-free method , two criticalopen challenges emerge: First, significant cross-client dataheterogeneity can compromise the quality of locally con-densed data, adversely affecting the global model training.Second, relying exclusively on condensed data for globalmodel training can result in reduced convergence perfor-mance and robustness, particularly when the quality of thereceived condensed data is sub-optimal.Motivated by the above research gap, this paper presentsFedAF, a novel aggregation-free FL algorithm tailored tocombat data heterogeneity.At the heart of our researchis the question of how to optimally harness the inherentknowledge in each clients original data to enhance bothlocal data condensation and global model training.Toachieve this, we first introduce a collaborative data con-densation scheme. In this scheme, clients condense theirlocal dataset by minimizing a loss function that integrates astandard distribution matching loss with an additionalregularization term based on Sliced Wasserstein Distance(SWD). This regularization aligns the local knowledge dis-tribution with the broader distribution across other clients,granting individual clients a more comprehensive perspec-tive for data condensation. Furthermore, to train the globalmodel with superior performance, we incorporate a local-global knowledge matching scheme. This approach enablesthe server to utilize not only the condensed data shared byclients but also soft labels extracted from their data, therebyrefining and stabilizing the training process. As a result, theglobal model retains more knowledge from earlier rounds,leading to enhanced overall convergence performance.Extensive experiments demonstrate that FedAF can con-sistently deliver superior model performance and acceler-ated convergence speed, outperforming several state-of-the-art FL algorithms across various degrees of data heterogene-ity. For instance, on CIFAR10, we achieve up to 25.44%improvement in accuracy and 80% improvement in conver-gence speed, compared with the state-of-the-art methods. Insummary, our contributions are threefold as follows: We propose a novel aggregation-free FL algorithm,termed FedAF, to tackle the challenge of data heterogene-ity.Unlike traditional approaches that aggregate localmodel gradients, FedAF updates the global model usingclient-condensed data, thereby effectively circumventing",
  "client drift issues": "We introduce a collaborative data condensation schemeto enhance the quality of condensed data. By employinga Sliced Wasserstein Distance-based regularization, thisscheme allows each client to leverage the broader knowl-edge in the data of other clients, a feature not adequatelyexplored in existing literature. We further present a local-global knowledge matchingscheme which equips the server with soft labels ex-tracted from client data for enhanced global insights. Thisscheme supplements the condensed data received fromclients, thereby facilitating improved model accuracy andaccelerating convergence speed.",
  ". Background and Related Works": "FL Algorithms for Heterogeneous Data.The founda-tional FedAvg algorithm , widely used in FL, calcu-lates the global model by averaging the local models fromeach client. Among its variants, FedAvgM adds server-side Nesterov momentum to enhance the global model up-date. FedNova normalizes aggregation weights basedon the amount of local computation. FedBN specif-ically excludes batch normalization layer parameters fromglobal aggregation.FedProx integrates a proximalterm in local training loss to mitigate the issue of clientdrift, while SCAFFOLD employs variance reductionand a control variate technique to directly address clientdrift. FedDyn allows clients to update the regulariza-tion in their local training loss to align more closely withthe global empirical loss.In contrast, FedDC pro-poses learning a drift variable to actively mitigate discrep-ancies between local and global model parameters. MOON uses model-contrastive regularization to foster simi-larity in feature representations between the global and lo-cal models. Meanwhile, FedDF and FedBE focuson knowledge distillation-based model fusion and Bayesianmodel ensemble, respectively, to transfer knowledge intothe global model.To forego the need for an unlabelledtransfer dataset, FedGen enables the server to learn andshare a generator model with clients, facilitating knowledgedistillation from the global model to local models throughgenerated feature representations. Data Condensation. Recent years have witnessed the riseof data condensation (or data distillation) techniques. Thesemethods aim to compress a large training dataset into asignificantly smaller set of synthetic data, enabling modelstrained on this condensed dataset to achieve performancecomparable to those trained on the original dataset. For ex-ample, explores a bi-level learning framework to learnthe condensed data, allowing models trained on it to mini-mize the loss on the original data. matches the gradi-ents produced by training models on both the original and",
  "Clients download from the server": ". Overview of FedAFs workflow. Left: Clients download the global model w and the class-wise mean logits V, averaged fromVk at the server. They then update the condensed data Sk using a combination of Distribution Matching (DM) loss and Collaborative DataCondensation (CDC) loss, with local real data Dk and V as inputs. Right: The server updates the global model w by employing bothcross-entropy loss and Local-Global Knowledge Matching (LGKM) loss. This utilizes both condensed data Sk and soft labels Rk receivedfrom each client k {1, 2, . . . , N}. The entire process iterates over a pre-defined number of communication rounds. condensed data. This approach is further enhanced by ,which applies differential Siamese augmentation to enablethe learning of more informative condensed data. Differingfrom single-step gradient matching, suggests matchingmultiple steps of training trajectories resulting from boththe original and condensed data. To reduce the complexityof the expensive bi-level optimization used in earlier works, introduces a distribution matching (DM) protocol. Inthis approach, condensed data is optimized to closely matchthe distribution of the original data in the latent featurespace. Building upon DM, further develops this con-cept by aligning layer-wise features between the real andcondensed data.Unlike Generative Adversarial Networks (GANs) ,which focus on generating realistic-looking images, the ob-jective of data condensation methods is to boost data ef-ficiency by creating highly informative synthetic trainingsamples. A recent study by indicates that condenseddata not only provides robust visual privacy, but also en-sures that models trained on this data exhibit resistance tomembership inference attacks.The potential of condensed data has recently drawn inter-est in the FL community, prompting efforts to combine datacondensation with FL within an aggregation-free frame-work. FedDM implements a DM-based data condensa-tion on the client side, with the server using condensed datafrom clients to approximate the original global training lossin FL. Employing the condensation method in as thebackbone, introduces a dynamic weighting strategy forlocal data condensation and enhances global model train-ing with pseudo data samples obtained from a conditionalgenerator. However, both of these works do not adequatelyinvestigate how to utilize knowledge from other clients toimprove the quality of local condensed data and the perfor-mance of the global model. While allows the sharingof condensed data among clients, it relies on an assumptionthat all clients possess data of the same class, which maynot be true under strong cross-client data heterogeneity.",
  "To facilitate a clearer understanding of our proposedmethod, we begin by introducing some essential notationsand preliminaries": "Federated Learning. FL is a decentralized machine learn-ing framework where K clients jointly learn a global modelparameterized by w without uploading local raw data D ={D1, D2, . . . , DK}, where Dk = {(xik, yik)}|Dk|i=1 is the lo-cal data owned by client k and | | refers to the number ofsamples in Dk. These clients are then set to learn the modelparameter w by solving the following problem collabora-tively:",
  "|D| so thatKk=1 pk = 1": "Data Condensation with Distribution Matching. Sup-pose a client k is tasked with learning a set of local con-densed data denoted as Sk. The client first initializes eachclass of condensed data by sampling from the local orig-inal data Dk or Gaussian noise. Subsequently, the clientemploys a feature extractor, denoted as hw(), which com-prises all the layers preceding the last fully connected layerin a backbone model parameterized by w, and extracts thefeature representations from each class c {0, 1, . . . , C 1} of data in Dk and Sk. The means of these feature repre-",
  "j=1hw(xjk,c). (3)": "Here xjk,c Dk and xjk,c Sk represent the j-th sam-ple of class c drawn from Dk and Sk, respectively. Nk,crefers to the number of samples of class c in Dk, whereasMk,c denotes the number of samples of class c in Sk. Thenclient k {0, 1, . . . , K 1} can learn and update Sk byoptimizing the DM loss as follows,",
  ". The Proposed Method": "The overall mechanism of our proposed FedAF is illustratedin . In each learning round, clients first engage incollaborative data condensation, updating their local con-densed data. They then share this condensed data and softlabels with the server. Subsequently, the server uses thisinformation to update the global model. Collaborative Data Condensation. Other than the extrac-tion of feature representations needed by DM loss in (4),we allow clients to compute class-wise mean logits and re-lated soft labels from their local original data Dk. With thelatest global model updated by the server as the backbonenetwork, clients perform the following:",
  "(10)": "where we refer to the second term on the right-hand-sideof (10) as the collaborative data condensation (CDC) loss.Without loss of generality, we let F(, ) in the above lossfunction be a distance metric to promote the alignment be-tween the local knowledge uk,c and the global knowledgevc. Inspired by recent studies , we select the SlicedWasserstein Distance (SWD) as the choice of F(, ). SWDserves as an effective approximation of the exact Wasser-stein distance , enabling the efficient capture of dis-crepancies between the knowledge distributions of locallycondensed data and the original data owned by other clients.This approach allows clients not only to match the distribu-tions of condensed and original data in the latent featurespace but also ensures a matching across clients in logitspace. By leveraging the CDC loss as a regularization term,each client can learn condensed data with the assistance ofglobal insights shared by their peer clients, thereby avoidingthe pitfall of biased matching towards their local data andfacilitating the learning of higher-quality condensed data. Local-Global Knowledge Matching. The data condensa-tion process inevitably leads to a certain degree of infor-mation loss from the original data.Consequently, rely-ing solely on the condensed data received from clients forglobal model training might result in limited convergenceperformance or even instability.To address this, we in-troduce a local-global knowledge matching approach, en-abling the server to harness a broader spectrum of knowl-edge about the original data distributed across clients.To be more specific, we let clients compute local class-wise soft labels about its original data, represented by",
  ". Experiments": "Datasets. We conduct experiments to evaluate and bench-mark the performance of our proposed methods on bothlabel-skew data heterogeneity and feature-skew data het-erogeneity. For label-skew scenarios, we adopt Fashion-MNIST (FMNIST) , CIFAR10, and CIFAR100 .The FMNIST dataset consists of 70,000 grayscale imagesof fashion products that fall into 10 categories. The CIFAR-10 and CIFAR-100 datasets are both collections of 60,000color images. CIFAR10 images are grouped into 10 classeswith 6,000 images per class, while CIFAR100 categorizesimage into 100 classes with 600 images per class.For",
  "feature-skew scenario, we use DomainNet which con-tains 600,000 million images in six domains: Clipart, Info-graph, Painting, Quickdraw, Real, and Sketch, each domainhas data of 345 classes": "Baselines. We compare FedAF against the aggregate-then-adapt baselines, including typical FedAvg and var-ious state-of-the-art FL algorithms designed for handlingdata heterogeneity: FedProx , FedBN , MOON, FedDyn , and FedGen .We also considerthe prior work that also employs aggregation-free FL,namely FedDM , to demonstrate the effectiveness of ourproposed collaborative data condensation and local-globalknowledge matching schemes.",
  ". Results for Label-skew Data Heterogeneity": "Configuration and Hyperparameters. We consider K =10 clients and partition the training split of each bench-mark dataset into multiple data shards to simulate the localtraining dataset for every client. Specifically, we leverageDirichlet distribution to partition data among clients. Forevery benchmark dataset, we consider three degrees of dataheterogeneity, represented by =0.02, =0.05, and =0.1,respectively. The hyperparameter controls the strength ofheterogeneity. Notably, a smaller implies a higher non-IID in the data distribution among clients. We choose these values to simulate harsh scenarios of data heterogeneitythat can be encountered in real world applications.For aggregate-then-adapt baseline methods, we adopt 10local epochs with local learning rate of 0.01 and local batchsize of 64. For both FedDM and FedAF, we adopt a localbatch size of 256, local update steps of 1000, 50 image-per-class (IPC) for local data condensation, while using 500epochs with a batch size of 256 and learning rate of 0.001for global model training. We initialize each class of con-densed data using the average of randomly sampled localoriginal data. For FedAF, the image learning rate is set to1.0, 0.1, and 0.2, for CIFAR10, CIFAR100, and FMNIST,respectively. For FedDM, we adopt an image learning rateof 1.0 and clip the norm of gradients at 2.0 to ensure itsstability during local data condensation. The global modelre-sampling coefficient is set to 0.9 whereas we set =5 inFedDM (See Appendix A for more implementation details). Model accuracy. We first evaluate the highest accuracyof the global model achieved by each algorithm within20 communication rounds. and showthat, FedAF significantly outperforms all aggregate-then-adapt baselines in various settings, showing notable im-provements in both mean accuracy and variance. A detailedanalysis in indicates that FedAF enhances perfor-mance compared to FedAvg by up to 25.44%, 17.91%, and31.03% on CIFAR10, CIFAR100, and FMNIST, respec-tively. Even against FedDyn, the leading aggregate-then-",
  "(i)": ". Comparison of convergence performance amongst baseline approaches. (a) to (c): learning curves obtained CIFAR10, (d) to (f):learning curves obtained on CIFAR100, (g) to (i): learning curves obtained on FMNIST. In addition to the the improvement in accuracy,FedAF also stands out to deliver considerably accelerated convergence speed, especially on harder dataset. adapt baseline, FedAF maintains an edge of up to 19.43%,13.70%, and 17.74% on the same datasets.Moreover,FedAF consistently outperforms FedDM, with accuracy ad-vantages reaching 4.87%, 4.56%, and 2.17% on CIFAR10,CIFAR100, and FMNIST, respectively. Notably, FedAFsperformance is more pronounced under stronger data het-erogeneity, such as at = 0.02, demonstrating the effec-",
  "FedAF51.247.0562.5364.652.6450.0654.68": ".Comparison of global model accuracy across variousFL algorithms on the DomainNet dataset. The domains are rep-resented by C (Clipart), I (Infograph), P (Painting), Q (Quick-draw), R (Real), and S (Sketch), Avg denotes the avarage accu-racy across domains. The highest and second-highest accuraciesin each column are indicated by boldface and underline, respec-tively. . Comparison of convergence performance amongst base-line approaches on DomainNet dataset. FedAF also outperformthe other baselines on both accuracy and convergence in feature-skew heterogeneous data distribution.",
  ". Impact of IPC on the global model accuracy for learningCIFAR10 under three different degrees of heterogeneity": "sistently surpasses other baseline methods in convergencespeed, particularly under significant data heterogeneity. Forinstance, at = 0.02, FedAF achieves the highest ac-curacy of other aggregate-then-adapt baselines within justtwo rounds. Compared to FedDM, FedAF also maintains asimilar edge; on CIFAR10 with = 0.02, while FedDMreaches a mean accuracy of 60% in fifteen rounds, FedAFattains this target in only three rounds, marking an 80% in-crease in convergence speed.",
  "Configuration and Hyperparameters.For the experi-ments in feature-skew scenario, we follow to form": "a sub-dataset of DomainNet comprising only the top tenmost frequent classes across all domains. We configure sixclients, each holding data from a unique domain, to mimicreal-world scenarios such as different hospitals using dis-tinct imaging protocol and equipment, influencing the dataheterogeneity. All algorithms are run for ten communica-tion rounds to compare the resulting global models accu-racy for every domain and the average accuracy across do-mains. For aggregate-then-adapt baselines, we maintain thesame hyperparameters as in the label-skew scenarios. BothFedDM and FedAF use an image learning rate of 1.0, withthe other settings such as batch size and number of localsteps consistent with those in the label-skew scenarios (seeAppendix A for more details). Model Accuracy and Convergence Performance. From and , it is evident that FedAF outperformsall other baseline methods in average accuracy and relatedconvergence performance across all domains. This com-parison highlights FedAFs consistent superiority, rankingas either the top or the second-best performer in every do-main. Similar to the label-skew scenarios, aggregate-then-adapt FL approaches are found to be less effective com-pared to FedAF. Moreover, FedAF not only demonstrateshigher accuracy but also exhibits faster convergence per-formance than FedDM. For instance, FedAF achieves theaccuracy level in just two rounds that FedDM requires tenrounds to reach, indicating an 80% acceleration. These ad-vantages underscore the benefit of integrating knowledgefrom other domains through our collaborative data conden-sation and local-global knowledge matching strategies, val-idating the effectiveness of these approaches.",
  ". Performance Analysis of FedAF": "Impact of IPC. We conduct an ablation study on CIFAR10to examine how variations in IPC might potentially influ-ence the performance of FedAF under various degrees ofdata heterogeneity. We summarize the results in ,and also illustrate and compare the resulting learning per-formance in . It can be concluded that higher IPCvalues generally lead to higher accuracy. Besides, for eachIPC value, the resulting model accuracy does not vary sig-nificantly over different values of (with the gap betweenthe highest and the lowest being 3.96%). From dwe find that although remarkable performance gain can beobserved when IPC ranges from 10 to 50, this improve-ment starts to become marginal using an even higher IPC.Considering the increase in communication cost is approx-imately linear with the value of IPC. On the other hand,the lower the compression ratio (i.e., the ratio between theamount of condensed and original data), the higher the pri-vacy retention . Therefore, an IPC value lower than 50should be generally considered as a trade-off between per-formance, communication cost, and privacy.",
  "FedDM60.280.8262.970.9664.880.35": ". Impact of core design on the global model accuracy forlearning CIFAR10 under three different degrees of heterogeneity.w/o CDC denotes the FedAF without collaborative data conden-sation, w/o LGKM denotes the FedAF without the local-globalknowledge matching. Impact of Core Designs. The collaborative data condensa-tion and local-global knowledge matching act as two funda-mental techniques of FedAF. We conduct additional exper-iments on CIFAR10 to further reveal how these two tech-niques contribute to performance improvement.Specifi-cally, we compare the full FedAF with two other config-urations where each fundamental technique is not utilized.We also compare it with FedDM, where none of these tech-niques exist. As shown in , the mean accuracy re-sulting from using just one of the fundamental techniquesalone still shows noticeable improvement over that obtainedwith FedDM. Moreover, the full FedAF exhibits further im-provement in the mean accuracy. These results verify thatby promoting the utilization of additional knowledge de-rived from data distributed across clients, FedAF indeed canempower clients to learn higher quality condensed data andtrain the global model with improved performance, whicheventually contributes to the enhancement in the overalllearning performance. Impact of Model Re-sampling. As the value of model re-sampling coefficient in (17) controls the interpolation be-tween the parameters of the global model received from theserver and those parameters randomly sampled, we analyzethe effect of this technique by running ten rounds of FedAFon CIFAR10 with =0.1 and comparing the resulting accu-racy over different choices of . Note that a larger in-dicates the re-sampled model tends to retain more knowl-edge learned in the global model from the previous round,while =0 implies clients use a model with randomly ini-tialized parameters for data condensation. As expected, Ta-",
  ". The influence of global model re-sampling on learningperformance. The first row lists the values of tested and thesecond row reports the corresponding accuracy": "ble 5 shows that larger values generally lead to highermodel accuracy. However, the optimal is found to be 0.9,rather than the maximal possible value of 1.0, verifying thata suitable random perturbation to the true global model canindeed regulate the data condensation and emhance learningperformance. Notably, =0.8 also yields comparable accu-racy, suggesting that FedAFs performance is robust to vari-ations in . Additionally, we experimented with =0. How-ever, using purely random weight parameters, the modelstruggled to stabilize the data condensation process, lead-ing to its exclusion from the further comparison.",
  ". Conclusion": "This paper presents FedAF, a novel FL algorithm designedto tackle data heterogeneity with an aggregation-free frame-work. FedAF grants clients and the server richer insightsabout the original data distributed across clients throughcollaborative data condensation and local-global knowledgematching. This strategy effectively addresses cross-clientdata heterogeneity and boosts the learning performance ofboth condensed data and the global model. Consequently,FedAF demonstrates considerable improvement over state-of-the-art FL methods in terms of model accuracy and con-vergence speed. Acknowledgement. This work is supported by the Agencyfor Science, Technology and Research (A*STAR) underits IAF-ICP Programme (Award No: I2301E0020). Thiswork is also supported by the National Research Founda-tion, Singapore under its AI Singapore Programme (AwardNo: AISG2-TC-2021-003). This work is also partially sup-ported by A*STAR Central Research Fund A Secure andPrivacy Preserving AI Platform for Digital Health. Thiswork is also supported by A*STAR Career DevelopmentFund (No. C222812010). Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro,Matthew Mattina, Paul N Whatmough, and VenkateshSaligrama.Federated learning based on dynamic regular-ization. arXiv preprint arXiv:2111.04263, 2021. 1, 2, 5",
  "Hong-You Chen and Wei-Lun Chao.FedBE: Makingbayesian model ensemble applicable to federated learning.In International Conference on Learning Representations,2021. 1, 2": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. IEEE, 2009. 1 Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing.Generative modeling using the sliced wasserstein distance.In Proceedings of the IEEE conference on computer visionand pattern recognition, pages 34833491, 2018. 4",
  "Tian Dong, Bo Zhao, and Lingjuan Lyu. Privacy for free:How does dataset condensation help privacy?In Interna-tional Conference on Machine Learning, pages 53785396.PMLR, 2022. 3, 7": "Chun-Mei Feng, Bangjun Li, Xinxing Xu, Yong Liu, HuazhuFu, and Wangmeng Zuo. Learning federated visual promptin null space for mri reconstruction.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 80648073, 2023. 2 Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, andCheng-Zhong Xu. Feddc: Federated learning with non-iiddata via local drift decoupling and correction. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 1011210121, 2022. 1, 2 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. Advances inneural information processing systems, 27, 2014. 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 1",
  "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Mea-suring the effects of non-identical data distribution for feder-ated visual classification. arXiv preprint arXiv:1909.06335,2019. 1, 2": "Wenke Huang, Mang Ye, and Bo Du. Learn from others andbe yourself in heterogeneous federated learning. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1014310153, 2022. 2 Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,Sashank Reddi, Sebastian Stich, and Ananda TheerthaSuresh. Scaffold: Stochastic controlled averaging for feder-ated learning. In International conference on machine learn-ing, pages 51325143. PMLR, 2020. 1, 2 Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Slicedwasserstein kernels for probability distributions. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 52585267, 2016. 4",
  "Qinbin Li, Bingsheng He, and Dawn Song.Model-contrastive federated learning.In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, pages 1071310722, 2021. 1, 2, 5": "Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Fed-erated learning on non-iid data silos: An experimental study.In 2022 IEEE 38th International Conference on Data Engi-neering (ICDE), pages 965978. IEEE, 2022. 1 Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,Ameet Talwalkar, and Virginia Smith. Federated optimiza-tion in heterogeneous networks.Proceedings of Machinelearning and systems, 2:429450, 2020. 2, 5",
  "Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, andZhihua Zhang.On the convergence of fedavg on non-iiddata. In International Conference on Learning Representa-tions, 2020. 1": "Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp,and Qi Dou. FedBN: Federated learning on non-IID featuresvia local batch normalization. In International Conferenceon Learning Representations, 2021. 2, 5, 7, 3 Tao Lin, Lingjing Kong, Sebastian U Stich, and MartinJaggi. Ensemble distillation for robust model fusion in fed-erated learning. Advances in Neural Information ProcessingSystems, 33:23512363, 2020. 1, 2",
  "Ping Liu, Xin Yu, and Joey Tianyi Zhou. Meta knowledgecondensation for federated learning. In The Eleventh Inter-national Conference on Learning Representations, 2023. 2,3": "Brendan McMahan, Eider Moore, Daniel Ramage, SethHampson, and Blaise Aguera y Arcas.Communication-efficient learning of deep networks from decentralized data.In Artificial intelligence and statistics, pages 12731282.PMLR, 2017. 1, 2, 5 Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, KateSaenko, and Bo Wang. Moment matching for multi-sourcedomain adaptation. In Proceedings of the IEEE InternationalConference on Computer Vision, pages 14061415, 2019. 5",
  "Laurens van der Maaten and Geoffrey Hinton. Visualizingdata using t-sne. Journal of Machine Learning Research, 9(86):25792605, 2008. 1": "Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, andH Vincent Poor. Tackling the objective inconsistency prob-lem in heterogeneous federated optimization.Advancesin neural information processing systems, 33:76117623,2020. 1, 2 Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi,H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew,Salman Avestimehr, Katharine Daly, Deepesh Data, et al.A field guide to federated optimization.arXiv preprintarXiv:2107.06917, 2021. 1 Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, andYang You.Cafe: Learning to condense dataset by align-ing features. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1219612205, 2022. 2, 3",
  "Han Xiao, Kashif Rasul, and Roland Vollgraf.Fashion-mnist: a novel image dataset for benchmarking machinelearning algorithms. arXiv preprint arXiv:1708.07747, 2017.5": "Yuanhao Xiong, Ruochen Wang, Minhao Cheng, Felix Yu,and Cho-Jui Hsieh. FedDM: Iterative distribution matchingfor communication-efficient federated learning. In Workshopon Federated Learning: Recent Advances and New Chal-lenges (in Conjunction with NeurIPS 2022), 2022. 2, 3, 5 Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. Fine-tuning global model via data-free knowledgedistillation for non-iid federated learning. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1017410183, 2022. 1",
  "A. Implementation Details": "Computing Platform. In this paper, we use PyTorch toimplement all the algorithms and experiments in both themain paper and this supplementary material. We run ourexperiments using an Nvidia RTX3090 GPU with 24 GB ofmemory. Model Architecture. In , we adopt a convolu-tional neural network (ConvNet) that follows the same ar-chitecture as reported in . The encoder of this modelconsists of three convolutional layers, each followed by aReLU activation function and average pooling. To facilitatecomparison with FedBN, batch normalization is incorpo-rated into the model. A fully-connected layer serves as theclassifier and is attached on top of the encoder. Training Details. In addition to the configurations and hy-perparameters detailed in Sections 5.1 and 5.2, we haveset specific values for other algorithms. For FedProx andMOON, the hyperparameter is set to 0.001 and 1.0, re-spectively. In FedDyn, we use 0.01 for the hyperparam-eter . For experiments on the DomainNet dataset withFedDM and FedAF, we employ an image-per-class (IPC)of 20, to achieve a similar condensation ratio as label-skewscenarios. Additionally, we resize the images of Domain-Net into 6464 resolution. For FedAF, the regularizationweights (loc, glob) for collaborative data condensation andlocal-global knowledge matching are set to (0.0001,0.01),(0.0001,0.01), and (0.001, 2.0) on CIFAR10, CIFAR100,and FMNIST, respectively. On DomainNet, we use (0.01,0.1) for (loc, glob). Furthermore, for all algorithms, weemploy PyTorchs built-in SGD optimizer with a momen-tum of 0.9. The same implementation is also used for theoptimizer of local data condensation in FedDM and FedAF.The accuracy evaluation for all algorithms is conducted overthe testing split of each benchmark dataset, to simulate cen-tralized validation or test data at the server. For all exper-iments, we average the accuracy and learning curves overthree trial runs, each with a different random seed. Visualizing Data Distribution. For label-skew data hetero-geneity, we explore three degrees of non-IID in cross-clientdata distribution, represented by values of 0.02, 0.05, and0.1, where a smaller indicating stronger heterogeneity.Figures 6, 7, and 8 show the class-wise data distribution perclient for CIFAR10, CIFAR100, and FMNIST datasets, re-spectively, using a random seed from our experiments. Inthese figures, the size of the blue circles corresponds to thenumber of data samples. We observe that with a smaller, clients tend to possess data concentrated in fewer classesand share fewer common classes, indicating a more pro-nounced label-skew non-IID distribution. For feature-skewheterogeneity, we analyze feature distribution in .Here, domain features are extracted using the encoder of a",
  "B. More Experiment Results with ResNet18": "In further experimentation, we evaluate the performanceof FedAF and compare it to baseline methods using aResNet18 model on CIFAR10 dataset.Conducting20 communication rounds for all algorithms, the resultingaccuracy are presented in . As expected, FedAFconsistently outperforms the baseline methods in both ac-curacy and related standard deviation across three differentdegrees of data heterogeneity. This advantage is particularlypronounced under strong heterogeneity, such as at =0.02,where FedAF achieves a 14.62% higher accuracy than Fe-dAvg and an 11.51% improvement over MOON, the topperformer of aggregate-then-adapt baselines. Additionally,FedAF maintains steady accuracy advantages of 2% overFedDM throughout all the settings.",
  "C. Communication Cost Analysis": "Baseline Methods and Model Size. For typical aggregate-then-adapt baseline methods like FedAvg, only the modelparameters are communicated back and forth betweenclients and the server. In our experiments, the ConvNet andResNet18 model has 381,450 and 11,181,642 parameters,respectively. With float32 precision, each parameter takes4 bytes, so that the size of these two models is evaluated at1.46 MB, and 42.65 MB, respectively. FedAF and Size of Condensed Data.In FedAFs up-stream communication, each client k sends three items tothe server: 1) the local condensed data Sk, 2) the class-wise mean logit Vk, and 3) the class-wise mean soft la-bels Rk, whereas in the downstream communication, eachclient k downloads two items: 1) the global model w whichshares the same architecture as that in aggregate-then-adaptbaselines, and 2) the class-wise mean logits from all other",
  ". Visualization of cross-client data distribution for FMNIST dataset under three different degrees of label-skew heterogeneity": "clients, denoted by V in (9). The matrices Vk and Rk sharethe same size, for ten-class datasets like CIFAR10, FM-NIST, and the sub-dataset we extracted from the Domain-Net, both Vk and Rk include ten vectors with ten values infloat32, making the size of them is approximately 4104 MB each. Whereas for CIFAR100 that contains data of 100classes, the size of Vk and Rk altogether is then evaluatedat approximately 0.076 MB. Assuming that the condenseddata is stored and transmitted in the PIL format so that onecan use 8-bit unsigned integer (or 1 byte) for each pixel perchannel, the size of every ten such condensed data samplesfrom FMNIST is evaluated at 7.5103 MB. Similarly, thesize of every ten condensed data learned from CIFAR10 orCIFAR100 is about 0.03 MB. Comparison with FedAvg. With the above calculation asa base, we compare the per-round upstream communicationcost incurred by FedAF and that of typical aggregate-then-adapt method such as FedAvg in , where FedAF us-ing an image-per-class (IPC) of 50. As described earlier, weuse three random seeds to generate the three sets of data dis-tribution and report the average communication overhead.Note that the communication cost of transmitting Vk, V, andRk is negligible compared to transmitting the condenseddata and the model, so the downstream communication costis essentially the same as that incurred by downloading theglobal model from the server, which is the same for FedAvgand FedAF. From , one can observe that for trainingthe ResNet18 model, FedAF is much more efficient in com-munication cost compared to FedAvg. When learning the",
  ". Per-round upstream communication cost incurred by Fe-dAvg and FedAF for learning CNN and ResNet18 on FMNIST,CIFAR10, and CIFAF100. FedAF uses an IPC of 50": "ConvNet model, which is relative smaller in size, FedAFstill achieves significantly higher communication efficiencythan FedAvg, especially on FMNIST and CIFAR10. WhileFedAvg incurs slightly less communication than FedAF forlearning the ConvNet model on CIFAR100, FedAF dras-tically outperforms FedAvg in accuracy and convergence(see performance comparison in .1). Moreover,unlike FedAvg, where the communication cost is solely de-termined by the model size and thus becomes increasinglyexpensive when a larger model is being learned, FedAFscommunication cost is irrespective of the size of the under-lying model. More interestingly, FedAF incurs less commu-nication overhead in stronger label-skew data heterogene-ity scenarios. These merits mark the extraordinary cost-effectiveness of FedAF."
}