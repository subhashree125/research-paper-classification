{
  "Abstract": "Pillar-based 3D object detection has gained traction inself-driving technology due to its speed and accuracy fa-cilitated by the artificial densification of pillars for GPU-friendly processing. However, dense pillar processing fun-damentally wastes computation since it ignores the inherentsparsity of pillars derived from scattered point cloud data.Motivated by recent embedded accelerators with native spar-sity support, sparse pillar convolution methods like sub-manifold convolution (SubM-Conv) aimed to reduce theseredundant computations by applying convolution only onactive pillars but suffered considerable accuracy loss.Our research identifies that this accuracy loss is dueto the restricted fine-grained spatial information flow (f-SIF) of SubM-Conv in sparse pillar networks. To overcomethis restriction, we propose a selectively dilated (SD-Conv)convolution that evaluates the importance of encoded pillarsand selectively dilates the convolution output, enhancingthe receptive field for critical pillars and improving objectdetection accuracy. To facilitate actual acceleration withthis novel convolution approach, we designed SPADE+ asa cost-efficient augmentation to existing embedded sparseconvolution accelerators. This design supports the SD-Convwithout significant demands in area and SRAM size, realizingsuperior trade-off between the speedup and model accuracy.This strategic enhancement allows our method to achieveextreme pillar sparsity, leading to up to 18.1 computationalsavings and 16.2 speedup on the embedded accelerators,without compromising object detection accuracy.",
  ". Introduction": "With the shift of priorities in autonomous driving from conve-nience to safety, there is a growing need for robust perceptionsystems that can accurately interpret time-critical semanticinformation in real-time, such as identifying and locating road users . At the heart of these systems is 3D objectdetection that leverages LiDAR-generated point cloud data,providing comprehensive depth information and obstacledetection . In pursuit of real-time 3D object detection,research has gravitated towards grid-based methods that con-vert point clouds into 3D voxels or 2D pillars. One primeexample is PointPillars , which utilizes a birds-eye-viewencoding technique, aggregating 3D point cloud featuresinto sparse 2D pillars and transforming them into a densepseudo-image for GPU-friendly 2D convolution (Conv2D).Thanks to this improved GPU efficiency, PointPillars hasemerged as a leading solution for time-critical 3D objectdetection .However, emphasizing the fundamental inefficiency indense pillar processing, which disregards the inherent spar-sity of pillars stemming from dispersed point cloud data, theemergence of dedicated embedded accelerators with nativesupport for sparse point cloud data imposes significant op-portunities for sparse pillar-based object detection towardfurther speedup . A notable attempt is Sparse-PointPillars , which introduces submanifold convolution(SubM-Conv ) to sparsify pillar representation and sig-nificantly reduces the number of computations. AlthoughSubM-Conv effectively preserves point clouds original spar-sity by limiting convolution dilation, the improved sparsitycomes at the cost of significant degradation in 3D objectdetection accuracy; as shown in , Sparse PointPillars,which employs SubM-Conv, results in significant accuracyloss, especially for the complex mode (Hard). Therefore,a comprehensive understanding of the trade-off betweensparsity and accuracy on sparse pillar convolution is lacking.In this work, we identify that the key cause of the accuracyloss of previous sparse pillar convolutions is the sparsifica-tion structure that limits the fine-grained spatial informationflow (f-SIF) from the increase of receptive field via dilation.Note that prior works on voxel-based methods havenoticed a similar issue on the SIF, but they primarily have",
  "arXiv:2408.13798v1 [cs.CV] 25 Aug 2024": ". The accuracy and computation trade-off of 3D objectdetection. The pillar-based baseline, PointPillars , delivers highaccuracy but uses redundant computation. Sparse PointPillars employs SubM-Conv, reducing computation but losing consider-able accuracy. In contrast to FS-Conv s inferior trade-off, ourselectively dilated convolution (SD-Conv) retains accuracy whilecutting computations by 18.1, promising for embedded 3D objectdetection. focused on the extension of the receptive field by stridedsparse convolution only at each stage of 3D object detectionbackbone. This approach only enhances the coarse-grainedSIF, thus showing a limited improvement in accuracy. Incontrast, we reveal that the increase of the receptive fieldvia fine-grained selective dilation at every convolution layerwithin the same stage plays a crucial role in constructing nec-essary receptive fields for identifying 3D objects in the scene.Therefore, we propose a simple yet effective operation calledselectively dilated convolution (SD-Conv) that can identifyimportant pillars at every convolution layer, based on theirmagnitude for selective dilation. To achieve actual speedup, we have designed a special-ized sparse point cloud accelerator architecture that operatesin a streaming manner to support SD-Conv for acceleration.We evaluate the proposed method on various state-of-the-artpillar-based 3D object detection networks, including Point-Pillars , CenterPoint , and PillarNet , as well aspopular benchmarks like KITTI and Nuscene . Theexperimental results consistently demonstrate that the pro-posed SD-Conv can simply replace SubM-Conv to recoveraccuracy while achieving higher sparsity. The achieved ac-curacies are on par with or even surpass the accuracy ofthe dense baseline models, while reducing the number ofcomputations by 94.5%, 72.3%, 41.3% for PointPillars, Cen-terPoint, and PillarNet, respectively. With in-depth ablationstudy, we further demonstrate that our SD-Conv achievesuperior accuracy-sparsity trade-offs compared to the priorsparse convolution approaches . Moreover, whensimulated on SPADE+, the method exhibited significantsparsity-proportional speedup of 16.2, 3.1, 1.7, respec-tively. These findings emphasize the effectiveness of ourapproach and its potential to enable real-time 3D object de-tection, making it suitable for time-critical applications suchas autonomous driving.",
  ". Pillar-based 3D Object Detection": "Essential for autonomous driving, 3D object detection canbe implemented through a variety of approaches, includingpoint-based, voxel-based, and pillar-based techniques. Point-based methods, such as PointNet and PointNet++,deal directly with point cloud data, but they can encountercomplexity issues due to sampling and sorting processes.Conversely, voxel-based methods like VoxelNet par-tition space into 3D grids, but the inherent sparsity of 3Dvoxels can pose difficulties in GPU utilization. Pillar-basedmethods, such as PointPillars as seen in (a), whichsegment space into 2D grids and utilize bird-eye-view (BEV)encoding, have risen in popularity for real-time 3D objectdetection . However, densify sparseBEV-based 2D convolution can lead to redundant computa-tion, suggesting that improvements can be made in PointPil-lars current implementation.(b) illustrates the details of this feature extractionconsisting of a backbone, neck, and head. The backboneconsists of multiple stages of convolutions led by sparsedown-sample convolution layers for the increased receptivefields. The outcome of all these stages is deconvoluted andthen concatenated in the neck for box, class, and directionprediction in the head. Note that variations exist; Center-Point incorporates center-based prediction heads whilePillarNet strengthens pillar encoding with additionalSubM-Conv layers in front.To achieve real-time speed, the key challenge in pillar-based methods is to reduce computations while extractingsufficient features for accurate object detection. To this end,the sparsification of pillars and utilizing sparse convolutionsare getting significant attention for embedded 3D objectdetection since dedicated point cloud accelerators with na-tive sparse convolution support have emerged as attractivealternatives for GPU .",
  ". Sparse Convolution": "Given the intrinsic sparsity of point cloud data, 3D objectdetection methods employ sparse 3D convolution forefficiency. While conventional sparse convolution uses onlynon-zero elements in the input feature map, reducing thenumber of floating point operations (FLOPs) and memorydemands, its dilation property can compromise overall spar-sity.Submanifold Sparse Convolution (SubM-Conv) addresses this by forming a receptive field only around non-zero elements without dilation, further reducing computa-tional requirements. However, this limited receptive fieldcan lead to significant accuracy loss.Spatial Pruned Sparse Convolution (SPS-Conv) & Focal Sparse Convolution (FS-Conv) both offering . (a) Pillar-based 3D object detection network structure. (b) Feature extraction steps: Backbone, Neck, and Head. (c) Comparison ofthe receptive field of various sparse convolution operations within a stage: Dense-Conv, SubM/SPS-Conv, FS-Conv, and SD-Conv. coarse adaptive dilation based on voxel importance onlyonce at each stage. SPS-Conv measures importance basedon magnitude, whereas FS-Conv calculates the importanceof each voxel through additional parameters learned duringtraining. Furthermore, FS-Conv also learns the importanceof dilation direction using extra parameters, enabling partialdilation.Pruned Sparse Convolution (PS-Conv) initiallyperforms dilation on all non-zero values, akin to Dense-Conv as illustrated in (c), to then increase sparsity byapplying pruning for computational reduction. However,employing high sparsity pruning during training can hinderstable learning, thereby imposing a limitation on accuracy.Hardware for Sparse Convolution:To accelerateSparse Convolution on Pillars, its essential to operate onlyon non-zero values. This involves utilizing mapping in-formation that represents the relationship between sparseinput and sparse output . One approach applicable togeneral-purpose processors like GPUs is employing hashtables. However, implementing hash tables introduces over-head from mapping that often outweighs the reduction incomputation, thereby falling short of fully realizing thebenefits of sparse convolution. Dedicated accelerators likePointAcc or SPADE address this by parallelizingthe mapping process, reducing mapping overhead, and effi-ciently managing data to achieve speedup based on sparsity.",
  ". Challenges: Spatial Information Flow (SIF)within a Stage": "To address SparsePointPillars limitations and the challengesof existing sparse convolutions for pillars, we implementedSPS-Conv and FS-Conv into the pillar-based object detectionbackbone. As illustrated in (c), these methods aimed to balance computational savings and model accuracy throughadaptive dilation but still failed to provide sufficient spatialinformation flow (SIF) that constructs spatial expansionof features of important pillars to supply necessary cues forobject detection. Both SubM-Conv and SPS-Conv do notincrease receptive fields within a stage, while FS-Conv onlyallows one-time dilation towards deformable directions. Thiscoarse dilation severely limits SIF, impeding the connectionof sparse pillars inside the bounding box unless they proceedthrough downsampling layers in subsequent stages.The main challenge lies in augmenting SIF by expandingthe foreground pillar group, triggered by dilation and activepillars generated by point clouds. Previous convolution oper-ations have not achieved this, either Dense-Conv facilitatingthe growth of foreground pillars without discerning back-ground, or SubM-Conv suppressing both background andforeground pillar dilation together. SPS-Conv and FS-Convenhance only the coarse-grained SIF, resulting in insufficientforeground information. Given that such SIF discrepancies,specifically the insufficient dilation of the foreground, hin-der appropriate feature extraction for object detection, wepropose a novel convolution method that selectively and ef-fectively increases the receptive field for important pillars.As shown in (c), our proposed methods promote thefine-grained dilation of important pillars at every sparse con-volution, achieving extreme sparsity while preserving modelaccuracy.",
  "Ip = Gigp(M(fi)),(1)": "where gp denotes a group of pillars in the vicinity of a pillarp, while M() signifies an importance measure applied toa feature vector fi RC. An overview of these proposedmethods is depicted in . This module performs aregular convolution selectively dilating important pillars,determined by Ip, with gp = p, G() = identity() andM() = mean(| |). The intuition is to allow each pillarto expand its feature to its neighbor, provided the feature isstrong enough. The ablation study for justification of theproposed importance measure is more discussed in Sec. 5.3.For an efficient on-the-fly decision of important pillars,we propose a dilation threshold learned throughout training.The top-k method is employed during this training to iden-tify the important pillars, choosing pillars with the top t%importance for dilation, and performing SubM-Conv for theremaining less significant pillars. To prevent unnecessarydilation and preserve sparsity, the selection ratio t is keptsmall. Through an ablation study in , we confirmedthat t = 2 is sufficient for PointPillars on KITTI ,while t = 4 is preferred in other cases. After training, animportance threshold satisfying the selection ratio becomesthe dilation threshold for efficient inference. . (a) Training curve for SparsePointPillars with SD-Convemploying magnitude-based and trainable-based importance inhigh sparsity. (b) Performance comparison of the car detectiontask in SparsePointPillars using different methods to determine thedilation directions of SD-Conv applied to the KITTI dataset. . Feature representation of a single car object based onsparse convolution type. Input Pillars represents the initial inputof the backbone network, while the images corresponding to SubM-Conv, FS-Conv, and SD-Conv are the outputs of the last layer inStage 2. The white rectangles indicate the boundaries of GT-boxes.",
  ". Convergent Selective Dilation": "SD-Conv employs two intuitive design aspects: a magnitude-based importance measure and dilation in all directions.These choices contrast with previous studies that advocatedfor more flexible dilation using learnable parameters. How-ever, our findings showed that this parameterized approachis not suitable for extreme pillar sparsity.(a) presents the training curve of SD-Conv on KITTIfor PointPillars with 14% sparsity, comparing importancemeasured by either pillar magnitude or learnable parameters.The graph shows that the loss associated with the learnableparameters is consistently higher than that of magnitude-based importance, indicating optimization challenges undersignificant sparsity. Furthermore, (b) contrasts 3Dobject detection accuracy and FLOPs for various dilationdirection choices. Surprisingly, random direction dilationoutperforms the learned dilation direction in both FLOPsand mAP, while our all-direction dilation yields the best per-formance. Consequently, we choose to measure importancebased on magnitude and implement dilation in all directions,eliminating additional overhead associated with learnable pa-",
  ". Fine-Grain Spatial Information Flow": "contrasts SIF of various sparse convolution methodsby observing output features from Stage 2 of PointPillarson KITTI. Specifically, we examine a focused region cor-responding to a car (indicated by the white ground truthbounding box). Note that the input pillars are sparsely dis-tributed and disconnected along the bounding box. TheSubM-Conv output feature illustrates a discontinuous SIF,failing to provide sufficient features for object detection. FS-Conv, on the other hand, offers a more connected, albeitdeformed, SIF that doesnt adequately cover the boundingbox. Conversely, our proposed SD-Conv extends the SIF tofill most features within the bounding box, demonstratingthat selective dilation within a stage by employing SD-Convenhances feature provision for object detection.",
  ". SPADE+": "We adapted SPADE to support not only Sparse Convolu-tion, PS-Conv and SubM-Conv, but also SD-Conv, creatingnew one called SPADE+. SPADE effectively handles op-erations only for non-zero points but considers dilation forall active points. In contrast, SPADE+ supports the gen-eration of mapping for SD-Conv, where only importantactive pillars are dilated, through four stages: Alignment( 1 ), Row Merge ( 2 ), Dilation Check Window ( 3 ), and Column-wise Dilation ( 4 ) by rule generation unit (RGU).While the mapping operation in the original SPADE onlyconsists of Alignment, Row Merge, and Column-wise Dila-tion, SPADE+ adds a Dilation Check Window ( 3 ) betweenRow Merge and Column-wise Dilation. Additionally, infor-mation regarding dilation status is added to the FIFOs ofeach stage in SPADE+, facilitating the generation of map-ping information based on dilation status.In SPADE+, only important input pillars dilate to gener-ate output, so output index calculation depends on whetherthe merged column index dilates, determined by neighboringcolumns within Dilation Check Window ( 3 ). For dilatedmerged columns, the SPADE+ behaves like SPADE. For non-dilated ones, rule generation depends on nearby columnsinformation. As illustrated in the example of , whengenerating the mapping for the 4-th target output row, theMerge Row results in a total of three merged columns. Sub-sequently, in cycle 0, column-wise dilation is applied to thefirst merged column, followed by column-wise dilation forthe second merged column in cycle 1. In cycle 1, since I0and I3 are not dilated and there are no adjacent columnswith merged columns, no output is generated for the mergedcolumn (C3), resulting in the mapping for W:,0 not beingcreated. However, for W:, and W:,+, adjacent columnsare expanded to generate mapping information. As a result,it can be observed that I0 is shifted by W,+ to be com-puted with O10, indicating that when the positions of alloutputs are known, I0 is indeed shifted to the O10 by W,+.With minor tweaks to the rule generation unit, the SPADE+operates akin to SPADE, facilitating SD-Conv operations.SPADE+ retains the rest of SPADEs components un-",
  "Sparse Conv, PS-Conv,SubM-Conv, SD-Conv": "changed, except for the RGU, which is modified to generatemapping information for SD-Conv. To streamline mappingin SPADE , it simultaneously identifies output positionsfrom convolution operations and generates relevant mappingdetails using a RGU. Additionally, a gather-scatter unit man-ages input, weight, and output to minimize data movementduring convolutions, enhancing performance based on spar-sity. Furthermore, in SPADE, PS-Conv was proposed tominimize computational overhead without sacrificing per-formance. However, as evident from the results,PS-Conv fails to address the discontinued SIF issue properly,resulting in higher computational complexity compared toSD-Conv.Since SPADE+ only modifies the RGU portion to supportSD-Conv, the area overhead of SPADE+ is only about 1%compared with original SPADE as shown in the .When comparing the effective TOP/W of SPADE for PS-Conv and SPADE+ for SD-Conv, the latter utilizing SD-Convshows a 3.5 improvement thanks to the small hardwareoverhead and computation efficiency of SD-Conv.",
  "We employed three state-of-the-art pillar-based 3D objectdetection networks, PointPillars (PP) , CenterPoint(CP) , and PillarNet (PN) , for evaluation of the": "proposed method on KITTI (for PP) or nuScenes (for CP and PN) benchmarks. We followed the baselinesettings of SparsePointPillars to replace the existingconvolution operations (Conv2D and SubM-Conv) of PP,CP, and PN with SD-Conv. All the experimental settingsare implemented with PyTorch-based frameworks, includingOpenPCDet1 for PP and the popular CenterPoint2 code-basefor PN and CP, and run on the NVIDIA A100 GPU.",
  ". Main Results": "PointPillars (PP): We begin by comparing the performanceof our proposed SD-Conv with other sparse convolution op-erations, SubM-Conv, PS-Conv and FS-Conv, on the Point-Pillars (PP) with the popular KITTI dataset. As shown in, SubM-Conv achieves a remarkable 95% reductionin computational operations (FLOPs). However, this comesat the cost of a significant decrease in accuracy, particularlyevident in the Hard category (-3.16 mAP). This outcomeunderscores the adverse impact of constrained SIF causedby SubM-Conv on object detection. PS-Conv demonstratesa lesser accuracy degradation in the Hard category, with a-1.46 mAP decrease. While PS-Conv alleviates accuracydegradation to some extent, it offers a significantly lesserreduction in computational operations compared to the othermethods. This highlights that merely applying pruning whileconcurrently training does not suffice to efficiently maintainSIF. To improve accuracy while saving computations, wesystematically vary the target FLOPs for both FS-Conv andSD-Conv. Notably, FS-Conv demands 5.99 GFLOPs to main-tain the original object detection accuracy. Any reduction inFLOPs below this threshold results in a substantial declinein accuracy, similar to what we observed with SubM-Conv.Conversely, SD-Conv achieves a remarkable 94.5% reduc-tion in FLOPs while preserving the original accuracy. Thisdemonstrates the effectiveness of our proposed fine-grainedselective dilation in constructing essential SIF, thereby en-abling accurate object identification.CenterPoint (CP): We evaluate SD-Conv using Center-Point (CP), another popular pillar-based 3D object detectionframework. The results in show the FLOPs and 3Dobject detection accuracies in two categories, and errors infive categories, following the nuScenes val set convention.In the case of SubM-Conv, we observe a substantial 74.1%reduction in computations, but this comes at the cost of no-ticeable accuracy degradation across all categories. PS-Convreduces computational load by 61.28%, which is smallerthan the reduction achieved by SubM-Conv, but it demon-strates improved accuracy. In contrast, FS-Conv maintainsoriginal accuracy while achieving only up to a 66.7% re-duction in computational load. SD-Conv, on the other hand,safely reduces computation by 72.3% while surpassing FS-",
  "SubMSD-ConvDesne162.2959.4567.40": "Conv in terms of accuracy. This highlights the advantageoustrade-off provided by SD-Conv between FLOPs and accu-racy, primarily due to its fine-grained SIF construction.PillarNet (PN): We further assessed SD-Convs benefitsin the context of PillarNet (PN), a cutting-edge pillar-basedobject detection method, where its backbone incorporatesmultiple layers of SubM-Conv to enhance feature extraction.Given that a significant portion of computation resides in itsneck, our focus was on sparsifying it using both SubM-Convand SD-Conv to reduce overall FLOPs while preserving ac-curacy. reports key accuracy metrics, including mAPand NDS, using the nuScene val dataset, with consistentoverall trends. Replacing the dense convolution in PNs neckwith SubM-Conv results in substantial computational sav-ings of 45.0% but noticeable mAP degradation. Conversely,transitioning from SubM-Conv to SD-Conv fully restoresaccuracy (its NDS even surpasses the baseline) with only amarginal increase in FLOPs compared to . These findingsunderscore SD-Convs versatile applicability in maintainingor enhancing performance while simultaneously reducingcomputational overhead.",
  ". Ablation Study": "In this section, we validate several design choices for ourpillar-based 3D object detection: 1) the metric for measuringimportance of SD-Conv, 2) types of sparse convolution fordown-sampling, and 3) methods for enhancing SIF.Importance Metric: presents findings regardingvarious metrics related to Ip, as discussed in Sec. 3. Weexplore different metrics within a magnitude-based approach",
  "Mean identityp3.4887.4477.4375.54Maxidentityp3.0787.6276.9372.67Mean Avg-Pool SubM(p)3.3486.9976.4972.84Mean Max-Pool SubM(p)3.4387.1477.1274.73": "for Ip. Regarding the selection of important pillars, the meanacross the channel consistently demonstrates the best per-formance. Conversely, the max metric tends to excessivelyemphasize outliers. Average pooling (Avg-Pool) faces chal-lenges in making accurate assessments when neighboringvalues of a particular pillar were zero, diminishing its rele-vance. Meanwhile, max pooling (Max-Pool) underperformsas it places excessive emphasis on high-magnitude pillars.Types of Down-Sample Sparse Convolution: As dis-cussed in Sec. 2.1, down-sample sparse convolution at thebeginning of stages increases the receptive field, affectingthe pillar-based object detections overall sparsity. Twodown-sampling methods exist: sparse convolution with a 2x2kernel used by SparsePointPillars and spatial prunedregular sparse convolution (SPRS-Conv ). The formerincreases sparsity by shrinking the dilation window from 3x3to 2x2. Meanwhile, SPRS-Conv dilates only vital features,risking pruning essential elements due to its pre-feature-importance stride mask. shows that SPRS-Convresults in lower accuracy than the case with a 2x2 kernelwindow, thus we employed the 2x2 kernel approach in ourwork.SD-Conv vs.Additional Down-Sampling for SIF:Computation savings from SubM-Conv restrict the SIF, com-promising object detection accuracy. A recent method, Vox-elNext , introduces an additional down-sampling (ADS)to enhance SIF in SubM-Conv. While ADS can boost SIFvia a larger receptive field, it also increases the computa-tional load. To compare SD-Conv with ADS, we adjusted",
  "SD-Conv3.4887.4477.4375.54": "Sparse PointPillars to include an extra down-sampling andSubM-Conv for ADS. shows SubM+ADSs mixedimpact on 3D object detection accuracy (a +1.32 increaseon Moderate but drops of -0.29 on Easy and -0.08 on Hard)at the cost of additional 1.45G FLOPs of computation. Incontrast, SD-Conv matches the dense baselines accuracybut with fewer computations than SubM+ADS, underscoringSD-Convs advantages.",
  ". Hardware Evaluation": "Harnessing the sparsity of point cloud processing for runtimesavings on conventional GPUs is ineffective due to architec-tural constraints. However, several point cloud-based sparseconvolution accelerators, featuring dedicated logic for sparsedata structures, have been introduced . Theseaccelerators support sparse convolution by mapping activeinputs to outputs, focusing only on non-zero value GEMMoperations. To assess the proposed SD-Convs feasibilityand benefits, we created cycle-accurate simulators for recentpoint cloud accelerators .More specifically, mapping information calculates input-weight-output index tuples, indicating the output from eachinput with a specific kernel index. This allows for efficientstorage of each weights product with an active point, opti-mizing sparse convolution calculations. While GPUs oftenuse hash tables to create this mapping, leading to collisionsand increased overhead, PointAcc employs merge sort-ing, and SPADE uses a pipelined strategy with sortedinputs to reduce overhead. Unlike PointAcc, which usesa cache, deterministically processes the mapping detailsand employs scratch pads to minimize memory overhead.Due to these architectural enhancements, both PointAcc andSPADE achieve sparsity-related speedup, as verified in ourcycle-accurate simulators.",
  ". Comparison of relative cycle with and without SD-Convin embedded accelerators (SPADE+, PointAcc), using dense con-volution accelerators as the baseline": "To assess the hardware speedup of SD-Conv, weimplementSD-ConvsmappingalgorithmintoourPointAcc/SPADE simulators and tested it on three mod-els (PP, CP, PN). These simulators also feature a baselinearchitecture for executing dense convolution in a systolicmanner. shows relative execution cycles compared tothe baseline, with SD-Conv configurations as per Tables2-5.For PP, CP, and PN, SPADE+ attains a 16.2, 3.1, 1.7speedup, nearing the ideal 18.1, 3.6, 1.7 speedup fromcomputational savings, respectively. PointAcc lags behindSPADE+ in all tests due to cache misses but still significantlyoutperforms the baseline using sparsity.",
  ". Conclusion": "This research demonstrated that pillar-based 3D object detec-tion, an efficient approach in autonomous driving technology,outperforms point-based and voxel-based methods in speedand accuracy, despite the computational redundancy fromdensifying the intrinsically sparse pillar data. We discoveredthat the accuracy loss in recent submanifold convolution(SubM-Conv) methods is due to their limited receptive field.To address this, we introduced a selectively dilated convolu-tion (SD-Conv) that enhance accuracy by focusing on keypillars and eliminating non-essential ones. Evaluation acrossseveral state-of-the-art models validated that our approachmaintains superior sparsity without sacrificing mAP.",
  "Acknowledgement": "This work was supported by Institute of Information & com-munications Technology Planning & Evaluation (IITP) grantfunded by the Korea government (MSIT) (No.2022-0-00957,Distributed on-chip memory-processor model PIM (Proces-sor in Memory) semiconductor technology development foredge applications) and the Technology Innovation Program(1415178807, Development of Industrial Intelligent Tech-nology for Manufacturing, Process, and Logistics) fundedBy the Ministry of Trade, Industry & Energy (MOTIE, Ko-rea). Eduardo Arnold, Omar Y Al-Jarrah, Mehrdad Dianati, SaberFallah, David Oxtoby, and Alex Mouzakitis. A survey on 3dobject detection methods for autonomous driving applications.IEEE Transactions on Intelligent Transportation Systems, 20(10):37823795, 2019. 1 Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-ancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodaldataset for autonomous driving. In 2020 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, CVPR2020, Seattle, WA, USA, June 13-19, 2020, pages 1161811628. IEEE, 2020. 2, 6 Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and JiayaJia. Focal sparse convolutional networks for 3d object detec-tion. Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 54285437, 2022. 1,2, 3",
  "Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, andJiaya Jia. Voxelnext: Fully sparse voxelnet for 3d objectdetection and tracking. arXiv preprint arXiv:2303.11301,2023. 7, 8": "Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, andJiaya Jia. Voxelnext: Fully sparse voxelnet for 3d objectdetection and tracking. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,2023. 1, 2 Yu Feng, Boyuan Tian, Tiancheng Xu, Paul Whatmough,and Yuhao Zhu. Mesorasi: Architecture support for pointcloud analytics via delayed-aggregation. In 2020 53rd AnnualIEEE/ACM International Symposium on Microarchitecture(MICRO), pages 10371050. IEEE, 2020. 1, 2, 8 Yu Feng, Gunnar Hammonds, Yiming Gan, and Yuhao Zhu.Crescent: taming memory irregularities for accelerating deeppoint cloud analytics. In Proceedings of the 49th AnnualInternational Symposium on Computer Architecture, pages962977, 2022. 1, 2, 8 Andreas Geiger, Phlip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the KITTI vision benchmarksuite. In 2012 IEEE Conference on Computer Vision andPattern Recognition, Providence, RI, USA, June 16-21, 2012,pages 33543361. IEEE, 2012. 2, 4, 6",
  "Benjamin Graham and Laurens van der Maaten.Sub-manifold sparse convolutional networks.arXiv preprintarXiv:1706.01307, 2017. 1, 2": "Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encodersfor object detection from point clouds. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1269712705, 2019. 1, 2, 4, 6 Minjae Lee, Hyungmin Kim, Seongmin Park, Minyong Yoon,Janghwan Lee, Junwon Choi, Nam Sung Kim, Mingu Kang,and Jungwook Choi. Spade: Sparse pillar-based 3d objectdetection accelerator for autonomous driving. In 2024 IEEE",
  "International Symposium on High Performance ComputerArchitecture (HPCA). IEEE, 2024. 1, 2, 3, 5, 6, 8": "Zhichao Li, Feng Wang, and Naiyan Wang. Lidar r-cnn: Anefficient and universal 3d object detector. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 75467555, 2021. 1, 2 Yujun Lin, Zhekai Zhang, Haotian Tang, Hanrui Wang, andSong Han. Pointacc: Efficient point cloud accelerator. InMICRO-54: 54th Annual IEEE/ACM International Sympo-sium on Microarchitecture, pages 449461, 2021. 1, 2, 3,8 Jianhui Liu, Yukang Chen, Xiaoqing Ye, Zhuotao Tian, XiaoTan, and Xiaojuan Qi. Spatial pruned sparse convolution forefficient 3d object detection. Advances in neural informationprocessing systems, 35, 2022. 1, 2, 7, 8",
  "Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hong-sheng Li. 3d object detection for autonomous driving: Areview and new outlooks. arXiv preprint arXiv:2206.09474,2022. 1": "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.Pointnet: Deep learning on point sets for 3d classificationand segmentation. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 652660,2017. 2 Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas JGuibas. Pointnet++: Deep hierarchical feature learning onpoint sets in a metric space. Advances in neural informationprocessing systems, 30, 2017. 2",
  "Guangsheng Shi, Ruifeng Li, and Chao Ma. Pillarnet: Real-time and high-performance pillar-based 3d object detection.arXiv preprint arXiv:2205.07403, 2022. 1, 2, 6": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, AurelienChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Benjamin Caine, et al. Scalability in perceptionfor autonomous driving: Waymo open dataset. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 24462454, 2020. 1, 2 Kyle Vedder and Eric Eaton. Sparse pointpillars: Maintainingand exploiting input sparsity to improve runtime on embeddedsystems. International Conference on Intelligent Robots andSystems (IROS), 2022. 1, 2, 6, 7, 8 Yue Wang, Alireza Fathi, Abhijit Kundu, David A Ross, Car-oline Pantofaru, Tom Funkhouser, and Justin Solomon. Pillar-based object detection for autonomous driving. In EuropeanConference on Computer Vision, pages 1834. Springer, 2020.1, 2",
  "Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely em-bedded convolutional detection. Sensors, 18(10):3337, 2018.3": "Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1178411793, 2021. 1, 2, 6 Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning forpoint cloud based 3d object detection. In Proceedings of theIEEE conference on computer vision and pattern recognition,pages 44904499, 2018. 2 Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, JiyangGao, Tom Ouyang, James Guo, Jiquan Ngiam, and VijayVasudevan. End-to-end multi-view fusion for 3d object detec-tion in lidar point clouds. In Conference on Robot Learning,pages 923932. PMLR, 2020. 1, 2"
}