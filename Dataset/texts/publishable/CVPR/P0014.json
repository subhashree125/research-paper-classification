{
  "Abstract": "This technical report describes the methods we employed for the Driving with Language trackof the CVPR 2024 Autonomous Grand Challenge. We utilized a powerful open-source multimodalmodel, InternVL-1.5, and conducted a full-parameter fine-tuning on the competition dataset,DriveLM-nuScenes. To effectively handle the multi-view images of nuScenes and seamlessly inheritInternVLs outstanding multimodal understanding capabilities, we formatted and concatenated themulti-view images in a specific manner. This ensured that the final model could meet the specificrequirements of the competition task while leveraging InternVLs powerful image understandingcapabilities. Meanwhile, we designed a simple automatic annotation strategy that converts thecenter points of objects in DriveLM-nuScenes into corresponding bounding boxes. As a result, oursingle model achieved a score of 0.6002 on the final leadboard.",
  "Introduction": "This competition primarily aimed to evaluate the perception, prediction, and planning capabilitiesof multimodal models in autonomous driving scenarios. Specifically, DriveLM [SRC+23] designed aseries of diverse natural language questions based on various autonomous driving scenarios, and themodels were scored based on their responses. Different types of questions were evaluated using differentscoring strategies. Notably, the competition placed greater emphasis on the perception capabilities ofthe multimodal models. Only if a model correctly perceives a specific object is it eligible to answerrelated questions.In the following sections, we will continue to introduce the competition dataset, our methodologies,and the final results.",
  "Dataset": "DriveLM-nuScenes [SRC+23, CBL+20] consists of 378k question-answer pair in training split.Asshown in Tab. 1, we show 4 examples of questions in DriveLM-nuScenes dataset. It designed a specialformat to represent key objects, consisting of the object ID, camera name, and the objects centercoordinates, for example <c1,CAM BACK,1088.3,497.5>.We chose to change the representation of the objects center point to the objects bounding box forthe following two reasons:",
  "arXiv:2412.07247v1 [cs.CV] 10 Dec 2024": "typically corresponds to the complete object we need. Therefore, we consistently selected thelargest mask and derived the final bounding box coordinates from the mask. This method workswell in most cases. However, if the objects center point is not on the main body of the object,it may produce incorrect bounding boxes. This situation can occur with traffic light objects.",
  "What are the important objects in the current scene? Those objects will be considered for thefuture reasoning and driving decision": "3What object should the ego vehicle notice first when the ego vehicle is getting to the nextpossible location? What is the state of the object that is first noticed by the ego vehicle andwhat action should the ego vehicle take? What object should the ego vehicle notice secondwhen the ego vehicle is getting to the next possible location? What is the state of the objectperceived by the ego vehicle as second and what action should the ego vehicle take? Whatobject should the ego vehicle notice third? What is the state of the object perceived by theego vehicle as third and what action should the ego vehicle take?",
  "Model": "We selected InternVL-1.5 as our base model, as shown in .It consists of an InternLM-20Blanguage model, a 6B InternViT, and a connector, and has been extensively pre-trained on multimodaldata. To handle high-resolution images, InternVL employs a dynamic high-resolution training approachthat effectively adapts to the varying resolutions and aspect ratios of input images. This methodleverages the flexibility of segmenting images into tiles, enhancing the models ability to process detailedvisual information while accommodating diverse image resolutions. Although InternVL has multi-image inference capabilities, it is trained by default using a single image. Since each sample in nuScenescorresponds to six images and can also be extended temporally, we performed a concatenation operationon the multi-view images to reduce the number of images that InternVL needs to process. Specifically,we first added text to each image to indicate its orientation, such as CAM FRONT. We then resizedeach image to 896 448 pixels. The six images were arranged into a single composite image in a 2 3grid. The resizing ensures easier subsequent image segmentation and preserves the integrity of eachindividual image as much as possible. The final concatenated image size is 2688x896, as shown in the.The complete image is divided into twelve 448x448 sub-images, with each view corresponding to twosub-images. Additionally, the entire image is resized to a 448x448 thumbnail for processing. Finally,each image is transformed into 256 image tokens through a VIT-MLP and pixel shuffle.At same time, we also include layout descriptions in the system prompt as: System Prompt:You are an Autonomous Driving AI assistant. You receive an image that con-sists of six surrounding camera views. The layout is as follows: The first row contains three images:FRONT LEFT, FRONT, FRONT RIGHT. The second row contains three images: BACK LEFT,BACK, BACK RIGHT. Your task is to analyze these images and provide insights or actions based onthe visual data.It is important to note that since the large language model predicts bounding box coordinates bypredicting the next token, InternVL normalizes all box coordinates to integers between 0 and 1000.Therefore, after image concatenation, we also process the bounding box coordinates accordingly tomeet InternVLs requirements.",
  "Experiment": "Our experimental results are shown in . Our temporal version InternVL4Drive-T had errorsdue to data format issues and achieved a lower score, which requires further exploration. Our bestsingle model InternVL4Drive-v2 achieves a final score of 0.6002. The v1 version are trained on a subsetof training set, which is around 10% of the full data. Based in this sub-dataset, our model actuallyachieves all higher score except on the ChatGPT score. While emplying ensemble on v1 and v2 result,actually we can obtain a much higher final score.",
  ": The results on DriveLM dataset": "[CBL+20]Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodaldataset for autonomous driving. 2020. [CWW+23] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan,Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundationmodels and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238,2023. [KMR+23]Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, LauraGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Seg-ment anything. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 40154026, 2023. [SRC+23]Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie,Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual questionanswering. arXiv preprint arXiv:2312.14150, 2023."
}