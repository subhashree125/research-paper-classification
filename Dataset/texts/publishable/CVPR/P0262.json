{
  "Abstract": "Multi-task learning for dense prediction has emerged asa pivotal area in computer vision, enabling simultaneousprocessing of diverse yet interrelated pixel-wise predictiontasks. However, the substantial computational demands ofstate-of-the-art (SoTA) models often limit their widespreaddeployment. This paper addresses this challenge by intro-ducing network binarization to compress resource-intensivemulti-task dense predictors.Specifically, our goal is tosignificantly accelerate multi-task dense prediction mod-els via Binary Neural Networks (BNNs) while maintain-ing and even improving model performance at the sametime.To reach this goal, we propose a Binary Multi-task Dense Predictor, Bi-MTDP, and several variants ofBi-MTDP, in which a multi-task dense predictor is con-structed via specified binarized modules. Our systemati-cal analysis of this predictor reveals that performance dropfrom binarization is primarily caused by severe informa-tion degradation.To address this issue, we introduce adeep information bottleneck layer that enforces represen-tations for downstream tasks satisfying Gaussian distribu-tion in forward propagation.Moreover, we introduce aknowledge distillation mechanism to correct the directionof information flow in backward propagation. Intriguingly,one variant of Bi-MTDP outperforms full-precision (FP)multi-task dense prediction SoTAs, ARTC (CNN-based)and InvPT (ViT-Based).This result indicates thatBi-MTDP is not merely a naive trade-off between perfor-mance and efficiency, but is rather a benefit of the redun-dant information flow thanks to the multi-task architecture.Code is available at BiMTDP.",
  "Estimation": "Bi-MTPD-F Bi-MTPD-C(CNN-Based)Bi-MTPD-C(ViT-Based) . (Left) A conceptual illustration of binary dense predic-tions in a multi-task manner. In contrast to approaching a seriesof relevant tasks individually, the multitask model benefits frominformation supplementation among different tasks via cross-talkstructures, but the cumbersome cross-talk modules also add addi-tional computational burden. (Right) Performance summary onNYUD-v2. X-axis and Y-axis denote the performance on depthestimation (lower is better) and segmentation (higher is better), re-spectively. Size of dots denotes FLOPs. ATRC and InvPT are previous CNN-based and ViT-based SoTAs, respectively. Benefited from the information supplementation mecha-nism via cross-talk structures in the multi-task models, theoverall performance for the series of dense prediction taskshas been greatly improved (see ). However, thecomputational demands of State-of-the-Art (SoTA) multi-task dense prediction models, which process multiple com-plex pixel-wise tasks concurrently, are substantial.Thishigh computational requirement limits their application inresource-constrained environments like autonomous driv-ing, robotics, and virtual reality. Our goal is to optimizethese heavy SoTA models for edge devices, balancing speedwith performance.Several strategies for neural network compression havebeen explored, including pruning , network quantiza-tion and knowledge distillation . Notably,network binarization, a form of quantization, minimizesweights and activations to 1, enabling the replacement ofcomputationally intensive inner-product operations in full-precision networks with more efficient xnor-bitcount opera-tions in Binary Neural Networks (BNNs) . Binarizationtheoretically reduces memory costs by 32 and increasesinference speed by 64, making BNNs suitable for edge-device.",
  "arXiv:2405.14136v1 [cs.CV] 23 May 2024": "While BNNs have shown impressive results in imageclassification, achieving nearly full-precision ResNet-levelaccuracy , their application has largely been limited tosmall-scale models, overlooking other computationally in-tensive computer vision tasks . Extend-ing BNNs to larger models should be the next step. How-ever, this expansion has been hindered by issues such asoverfitting and information degradation .Techniques effective in full-precision models, like labelsmoothing , dropout , and mixup have lesseffect on BNNs . Furthermore, SoTA multi-task dense prediction tasks often require deep and com-plex models, equipped with multi-modality fusion struc-tures , exacerbating the challenges in implement-ing binarization effectively. The primary barrier to applying binarization in multi-task dense prediction tasks is the significant degradation ofinformation flow in deep models , leading to re-duced performance. To address this issue, we first proposea Binary Multitask Dense Predictor (Bi-MTDP) baseline,where a multi-task dense predictor is formulated via bina-rized modules. Based on a thorough review of this base-line, we conclude that the binarization operation destroysthe information flows in multi-task models, and thus repre-sentations for downstream tasks are not informative com-pared with their full-precision counterparts. To tackle thisproblem, we update Bi-MTDP with additional informationflow calibration mechanisms in two directions. First, weimplement variational information bottleneck enforcing theembeddings to follow Gaussian distribution with sparsity inforward propagation, in order to filter out the task-irrelevantfactors. Second, we leverage the existing FP models viafeature-based knowledge distillation to calibrate the gradi-ent of the binary network in backward pass. The benefits of Bi-MTDP can be analyzed from twoorthogonal perspectives. On one hand, from the perspec-tive of network binarization, the accomplishment of bridg-ing binarization with the multitask dense prediction frame-work testifies that Bi-MTDP can effectively supplement in-formation, and consequently improve the performance ofthe individual binary models. On the other hand, from theperspective of multitask dense prediction task, acceleratingthose cumbersome models is profitable to design more ef-fective and efficient cross-talk modules in it, as shown in. Since existing dense prediction models have severelimitation in modelling the cross-talk modules due to theirheavy utilization of convolution operation, it is critical forthe multitask dense predictions to learn interactions and in-ference covering various scopes of the multitask context viathe cross-talk mechanism . Intrigu-ingly, a variant of Bi-MTDP outperforms SoTA approachATRC by 4% over the segmentation task while remain-ing 43% faster in speed, implying that our proposed method is not a naive trade-off between performance and efficiency.By empirically investigating this free lunch achievement,we conclude that the win-win outcome is benefited from ourdesigned information supplementation mechanism whichstrengthens the representation ability of the binary model.",
  ". Related Work": "Multitask Dense Prediction. Multi-Task Learning (MTL)methods can be generally categorized into two mainparadigms in terms of the way where model learns sharedrepresentations: hard and soft parameter sharing. Hard pa-rameter sharing characterizes architectures which typicallyshare the first hidden representations among the tasks whilebranching to independent task-specific representations at alater stage. Most approaches split to task-specific heads at asingle branch point . However, such naivebranching can be sub-optimal, raising interest in mecha-nisms that allow for finely branched architectures .As a result, in soft parameter sharing, each task is assignedits own set of parameters and a feature sharing mechanismrealize the cross-talk as demonstrated in Fig 1. The follow-ing works devise the cross-talk mechanisms focusing on thelocations in the network where information or features areexchanged or shared between tasks. Apart from the loca-tions, the feature sharing modules are also widely studied.For example, feature fusing can be introduced along the en-tire network depth ; PAD-Net uses multi-modaldistillation to enhance task-specific predictions, in which in-formation flow from each source to target task is regulatedwith a sigmoid-activated gate function; and MTI-Net combines the multi-modal distillation module of PAD-Netwith a multi-scale refinement scheme to facilitate cross-tasktalk at multiple scales.Although increasing the number of cross-talk modulesintuitively benefits the overall performance of the models,computational cost is often an obstacle. To handle this is-sue, ATRC introduces NAS to automatically designan efficient information fusing modules. From the perspec-tive of the efficient representation cross-talk, our proposedmodels with the binarization module can be interpreted as anew pathway to feature fusing within a notably lower infer-ence speed level.Neural Network Binarization. As pioneers, use thesign function to quantize weights and activations to 1, ini-tiating the trends of studies of network binarization.Totackle the vanishing gradient issue induced by the bina-rization operations, the straight-through estimator (STE) is introduced for the gradient approximation.Rooted inthis archetype, considerable studies contribute to improvingthe performance of BNNs, particularly on ImageNet. Forexample, propose Bi-Real introducing double resid-ual connections with FP downsampling layers to mitigatethe excessive gradient vanishing issue caused by binariza- tion, and consequently demonstrate that delicately design-ing additional connections within BNNs benefits the gra-dient propagation. design a proxy matrix as a basisof the latent parameter space to guide the alignment of theweights with different bits by recovering the smoothness ofBNNs. In summary, a large number of methods have ex-tended the boundary of the network binarization w.r.t. ac-curacy over classification (e.g., ReActNet within com-parable FLOPs of binary ResNet-18 achieves 65.9% Top-1accuracy on ImageNet, while full-precision version is only70.5%).However, most of those works validate their effec-tiveness over classification with relatively small architec-tures (mostly ResNet18 and ResNet34). Meanwhile, thenetwork-based models for dense prediction tasks are big-ger and deeper than those toy models, as the informationflow in networks is severely degraded. Consequently, di-rectly implementing existing binarization methods can notachieve supposed success. To mitigate the degradation, wepropose to binarize those dense prediction models in a mul-titask way.",
  ". Binary Neural Network": "To begin with, we briefly review the general idea of binaryneural networks (BNNs) in . Here, we only elabo-rate the speedup mechanism and the degradation of infor-mation flow of the binarization. We define a full-precision(FP) neural network with K layers, f(x) = (WK WK1 W1)(x), where x is the input sample andWk : Rdk1 Rdk(k = 1, ..., K) stands for the weightmatrix connecting the (k 1)-th and the k-th layer, withdk1 and dk representing the sizes of the input and outputof the k-th network layer, respectively. The () functionperforms element-wise activation for the feature maps.BNNs vary from FP neural networks in terms of the for-ward operation and the backward gradient approximation.Specifically, in the forward propagation, the BNN main-tains FP latent weights WF for gradient updates, and thek-th weight matrix WkF is binarized into 1, obtaining thebinary weight matrix WkB via the binarize function sign(),i.e. WkB = sign(WkF ). Then the intermediate activationmap (full-precision) of the k-th layer is produced by AkF =WkBAk1B. The same quantization method is used to bina-rize the full-precision activation map as AkB = sign(AkF ),and the whole forward pass of binarization is performed byiterating this process for L times, as shown in . ForBNNs, the weights and activations are 1-bit, by which thenetwork is accelerated 32 times in terms of memory cost.Importantly, inference of BNN is accelerated 64 times, asthe FP multiplication in FP networks is replaced by Xnor-Bitcount in BNNs.",
  "sgn(x)|x| 10|x| > 1.(1)": "It is worth noting that we do not implement the aforemen-tioned vanilla approximation method in practice, while weutilize the prevalent Bi-Real and IR-Net to gradu-ally approximate the sign function, which have been provento be better estimation approaches .Even though numerous methods have been proposed toeliminate the deterioration of the information flow inducedby the binarization, the deterioration is still inevitable dueto the severe accuracy loss of weights, activations and gra-dients . Consequently, binarization destroys the per-formance of the complicated computer vision models.",
  ". Multitask Dense Predictor": "After deploying binarization techniques in the models fordense prediction tasks, the performance of the binarizedmodels is unacceptable, as shown in and the binarysingle result in Table. 3. Since the architectures of thoseSoTA dense prediction models are relatively heavier anddeeper (e.g., HRNet-48 or ResNet-101 with a task-specifichead ) than the ones for classification (e.g., ResNet-18 with a fully-connected layer as the classification head).Moreover, the information passing in the binary models viaback-propagation, especially in deep models, is notoriouslyinefficient .Dense prediction tasks can mutually supplement infor-mation, e.g., surface normals and depth can directly be de-rived from each other, which can be modeled as the regular-ization of each other . The relevancy among dense pre-diction tasks is worth being utilized to improve the overallperformance of models. For example, before the deep learn-ing era, pioneering work utilizes RGB-D images withdepth information to predict scene semantics to improve thequality of the prediction. In the deep learning era, recentattention-based multitask learning methods ex-plicitly and implicitly distill information from other tasks asa complementary signal to improve the targeted task perfor-mance. Briefly, the above-mentioned methods are achieved BinarizedBackbone",
  "Binarized Matrix Multiplication": "-1+1-1-1 -1 -1 -1 -1 . (Left) The illustration of the baseline multitask framework. (Middle) The designed MMD modules for binary representa-tions. Importantly, the MMD module can pass information among different predictions, acting as a cross-talk mechanism. (Right) Asall fundamental modules in Bi-MTDP baseline are binarized, inferences can be performed by complete Bool operations, which are verycomputationally cheap.",
  "by combining an existing backbone network for initial taskpredictions with a subsequent decoding process, as shownin (Left)": "Specifically, the shared features extracted by the back-bone network are then processed by a set of task-specificheads, which produce a series of initial predictions for Ttasks, i.e. {Y ki } (k = 1, , T) (the backbone and thetask-specific heads are referred as the front-end of the net-work ). Transforming and binarizing Y ti into the formof a 1-bit feature map, we obtain a set of corresponding bi-nary feature maps of the scene, i.e. {F tB,i} (t = 1, , T)which are more task-aware than the shared binary featuresof the backbone network. The information from these task-specific feature representations is then fused via a multi-modal distillation via binarized attention mechanism beforemaking the final task predictions. As previous work fea-tured, our method is also task-saleable. Especially, it is pos-sible that some tasks are only predicted in the front-end ofthe network (initial prediction). The initial tasks are alsocalled auxiliary tasks since they serve as proxies in orderto improve the performance of the final tasks, as shown in. Multi-Modal Distillation (MMD) via Binarized Atten-tion Mechanism. The multi-modal distillation module isthe key in the multi-task dense prediction model. Specif-ically, we utilize the attention mechanism for guiding theinformation passing between the binary feature maps gen-erated from different modalities for different tasks. Sincethe passed information flow is not always helpful, the atten-tion can act as a gate function to control the flow, in otherwords, to make the network automatically learn to focus orto ignore information from other binary features .Including the binarization operations, we can formalize theMMD via binarized attention as follows. While passing in-formation to the k-th task, we first obtain a binarized atten-",
  "t=1,t=kAkB,i (WB,t FtB,i)": "(3)where element-wise multiplication. The general demon-stration of the distillation process is presented in Left.The output binary feature map Fo,kB,i is then used by the headfor the corresponding t-th task in Right. By using thetask-specific distillation activations, the network can pre-serve more information for each task , which es-pecially benefits the BNNs where the deteriorated informa-tion flow mainly induce the performance drop.On the other hand, multitask dense prediction modelsbenefit from the network binarization in terms of perfor-mance.Although these multitask models have achieveda promising performance, they are still limited by the na-ture of convolution-based distillation modules that are heav-ily used in a multi-scale way, which model critical spatialand task-related interactions in relatively local perceptivefields . Theoretically, more distillation modules in dif-ferent network nodes can contribute to model performance,yet we cannot unrestrictedly add distillation modules to theexisting model due to the computational limitation. Fortu-nately, with the saved computational cost of the binary net-works, we can implement additional distillation modules inour model.BinaryBaselineforMultitaskDensePrediction,Bi-MTDP. To obtain dense predictions with BNNs undera multitask learning framework, we practically binarize",
  "+ - +- - +- + --3.19": "-3.19 -3.19 5.23 -7.39 -6.11 -1.01 -3.19 4.56 . The pipeline of Bi-MTDP. We introduce a VIB layer after the backbone network to filter-out the nuisance factors which maylead to model overfitting issue in the forward propagation. In addition, we deploy a feature-based knowledge distillation mechanism toguide the optimization direction in the backward propagation. the MTI-Net as the binary baseline. Specifically, themain modules in the full-precision MTI-Net, includingbackbone, heads, and multi-modal distillation module, arereplaced with binary modules (both weights and activationsare 1-bit). We call this baseline as Bi-MTDP.",
  ". Information Flow Supplementation": "Although we build a fully binarized baseline Bi-MTDP formultitask dense predictions and train the pipeline with com-mon techniques, the performance is still of major concern.The baseline suffers an immense information degradationas the nuisance factors are over-fitted in the forward prop-agation and optimization directions severely mismatch inthe backward propagation.To solve these problems, inthis section, we further propose the variant of Bi-MTDP,Bi-MTDP-F. Specifically, we introduce a variational infor-mation bottleneck (VIB) layer after the output of the sharedbinary backbone to precisely enforces the feature extractorto preserve the minimal sufficient information of the inputdata. As well, we deploy the feature-based knowledge dis-tillation to guided the optimization direction. We presentmore details in the following section.Variational Information Bottleneck for Filter-Out Nui-sance Factors. Obtaining the initial binary representationsof input images by the shared backbone, we need to traina series of targeted heads to split them.A straightfor-ward strategy is to feed these representations into the fol-lowing MMD module. However, the binarized representa-tions lack homogenization leading to model overfitting is-sue . Therefore, the need to regularize the binarizedrepresentations, while the regularization would not to con-taminate the information flow in the representations. For-tunately, the information bottleneck (IB) principle directlyrelates to compression with the best hypothesis that the datamisfit and the model complexity should simultaneously beminimized . As the VIB could effectively capture the relevant partsand filter out the irrelevant ones from inputs, we design anovel VIB-based layer after the backbone. In particular,it explicitly enforces the feature extractor to preserve theminimal sufficient information of the input data. In otherwords, it can help ensure the information flow flexibly tolearn clean representation for the targeted tasks. The objec-tive function of our VIB-based classification can be formu-lated as a term of information loss, written as follows:",
  "Lvib = KL [p(Z | AB), r(Z)] ,(4)": "where AB is the input binary backbone representation, Bis the latent representation variable, p(Z | AB) is a multi-variate Gaussian distribution, and r(Z) is a standard normaldistribution. Generally, the latter is a regularization termcontrolling how much information of the input is filteredout. A more detailed discussion about the VIB for binarizedmodels filtering out irrelevant information is in Supplemen-tal Materials.Feature-based Knowledge Distillation for Guiding theDirection of Information Flow. Distillation is a commonand essential optimization approach to alleviate the per-formance drop of quantized models on ultra-low bit-widthsettings, which can be flexibly deployed for any architec-tures to utilize the knowledge of a full-precision teachermodel . The usual practice is to distill theactivations in a layerwise manner from the full-precisionteacher to the quantized counterparts, i.e., FB,l and FF P,l(l = 1, , L, where L represents the number of net-work layers), respectively. We use the mean squared errors(MSE) as the distance function to measure the differencebetween corresponding from features student and teacher.The knowledge distillation loss can be written as follows:",
  ". Counter-Intuitive Results of Bi-MTDP-A": "Intuitively, implementing binarization on FP network in-evitably induces representations degradation, as the gradi-ent of the sign function cannot be perfectly estimated .Thus, binarized models are impossible to outperform theirfull-precision counterpart models. However, Bi-MTDP-C,a variant of Bi-MTDP (i.e., full-precision backbone withonly binarized multi-modal distillation) outperforms itsfully FP version.Specifically, just binarizing the multi-modal distillation can simultaneously accelerate the modelby 39% and improve the mIoU for segmentation by 4%,as shown in .This result demonstrates that ourmethod is not a naive trade-off between model performanceand efficiency but a powerful tool for boosting multitaskdense predictors. This exciting free-lunch achievement iseven a bit of counter-intuitive. We speculate the reasons arethat i) binarization on MMD can filter out task-irrelevant in-formation; ii) and thus the information flow in the networkis more effective. To testify this speculation, we conduct aseries of experiments in two aspects, the representation abil-ity of Bi-MTDP-C and information flow supplementationwithin the network.QualitativeStudyofLearnedFeatureswithBi-MTDP To investigate the representation ability ofBi-MTDP-C and its FP counterparts, we visualize i) thefeature maps behind the Binarized Multi-Modal Distillation(MMD) module in 2-D space via the t-SNE algorithm,and ii) the regions where the network considers importantvia the Grad-Cam algorithm . The results are shownin . It is clear that binarized model, Bi-MTDP-C isable to filter the irrelevant information out via the binarizedattention module (see (a)), and thus helps learnmore discriminative features (see (b)) resulting inhigher quantitative results. Overall, the generated spatialfeature maps for segmentation are better.The enhanced representative ability can contribute to higher quantitativeresults.Analysis of Information Flow Supplementation withinNetwork via Centered Kernel Alignment. Analyzing dis-tributional information flow within layers of neural net-works is challenging because outputs of layers are dis-tributed across a large number of neurons. Centered ker-nel alignment (CKA) can address these chal-lenges, by quantitatively comparing activations within oracross networks. Specifically, for a network feed by m sam-ples, CKA algorithm takes X Rmp1 and Y Rmp2as inputs which are output activations of two layers (withp1 and p2 neurons respectively). Letting K XX andL YY denote the Gram matrices for the two layersCKA computes:",
  "n11 andthe centered Gram matrices K = HKH and L = HLH,HSIC =vec(K)vec(L)": "(m1)2, the similarity between these cen-tered Gram matrices. Importantly, CKA is invariant to or-thogonal transformation of representations (including per-mutation of neurons), and the normalization term ensuresinvariance to isotropic scaling.These properties enablemeaningful comparison and analysis of neural network hid-den representations.Therefore, we introduce CKA to study the informationflow in the multitask dense prediction models. In the heat-map, the lighter the dot, the more similar the two corre-sponding layers. Higher similar score between two layersoutput representations means those two layers share moreinformation. The results are presented in (c), we cansee that the similar scores among front layers and back lay-",
  ". Experiments": "In this section, we conducted comprehensive experimentsto evaluate our proposed method on two datasets for denseprediction tasks: PASCAL Context and NYUD-v2 .We first describe the implementation details of Bi-MTDP,and then compare our method with SoTA binary neural net-works in the task of object detection to demonstrate superi-ority of the proposed method. Finally, we validate the effec-tiveness of information bottleneck and feature-based knowl-edge distillation by a series of ablative studies.",
  ". Datasets, Evaluation, Implementation Details": "Datasets. PASCAL-Content is a popular dataset for denseprediction tasks. We use the split from PASCAL-Contextwhich has annotations for semantic segmentation, humanpart segmentation, semantic edge detection, surface nor-mals prediction and saliency detection.Note that someannotations were distilled by using pre-trained SoTAmodels . NYUD-v2 contains various indoor scenes suchas offices and living rooms with 795 training and 654 test-ing images. It provides different dense labels, including se-mantic segmentation, monocular depth estimation, surfacenormal estimation and object boundary detection.Evaluation. Semantic segmentation (Semseg) and humanparsing (Parsing) are evaluated with mean Intersection overUnion (mIoU); monocular depth estimation (Depth) is eval-uated with Root Mean Square Error (RMSE); surface nor-mal estimation (Normal) is evaluated by the mean error(mErr) of predicted angles; saliency detection (Saliency) isevaluated with maximal F-measure (maxF); object bound-ary detection (Boundary) is evaluated with the optimal-dataset-scale F-measure (odsF). To evaluate the model ef-ficiency w.r.t. memory cost and inference speed, we adopt the number of parameters and FLOPs for a single round ofthe model inferring an input image.Implementation details.We build our approach on themost prevalent backbone architecture, i.e., HRNet as pre-vious SoTA methods . The task-specific heads arealso implemented as two basic residual blocks, i.e., bina-rized BasicBlock and binarized Bottleneck with additionalbinary shortcuts as BiReal-Net . We use 1 loss fordepth estimation and cross-entropy loss for semantic seg-mentation on NYUD-v2. As in the prior work, the edgedetection task is trained with a positive weighted wpos =0:95 binary cross-entropy loss. We do not adopt a particu-lar loss weighing strategy on NYUD-v2, but simply sum thelosses together. On PASCAL, we reuse the training setupfrom to facilitate a fair comparison. We reuse the lossweights from there. The initial task predictions in the front-end of the network use the same loss weighing as the finaltask predictions. We refer to the supplementary material forfurther implementation details. Importantly, we use Adamoptimizer in training, but with different learning ratesfor binary parameters (1e-5) and FP parameters (1e-4), asAdam with a larger learning rate for binary parameters canlead to better training results . Note that our project isbased on the codebases for MTI-Net and more detailscan be found in the codes in the Supplemental Materials.",
  ". Comparison with State-of-the-Art": "Tabs.1 and 2 present a comparative analysis of theproposed Binary Multitask Dense Predictor (Bi-MTDP)with current state-of-the-art models on the NYUD-v2and PASCAL-Context datasets.This comparison in-cludes notable CNN-based methods such as PAD-Net ,ASTMT , MTI-Net , and ATRC , among oth-ers. Bi-MTDP demonstrates exceptional performance, out-performing other models in 6 of the 9 evaluated metrics,particularly in complex scene understanding tasks like Se-mantic Segmentation and Parsing.Remarkably, on theNYUD-v2 benchmark, Bi-MTDP surpasses the previously",
  "InvPT + Bi-MTDP-C79.83 (0.80)68.17(0.56)84.92 (0.11)13.92(0.23)73.03 (0.03)401.2621.67382.68": "best-performing CNN-based method (ATRC) by a marginof +1.8 (mIoU) in Semantic Segmentation, while requiringonly 62% of the storage space for weights and 56% of thecomputational FLOPs.Furthermore,the application of Bi-MTDP to theViT-based state-of-the-art method, InvPT , show-cases Bi-MTDPs ability to enhance model performancewhile also improving efficiency.This demonstratesthe broad applicability and generalization capability ofBi-MTDP across different architectural frameworks.For a qualitative assessment, Figs. 7 and 6 displays pre-diction examples from various models. These examples il-lustrate that Bi-MTDP-C not only competes with but occa-sionally surpasses the state-of-the-art ATRC in qualitativeperformance.",
  ". Ablative Studies": "Tab. 3 shows a series of ablative studies of Bi-MTDP-C andBi-MTDP-F with an HRNet-48 backbone on NYUD-v2.We verify how different components of our model con-tribute to the multi-task improvements. In summary, everydesigned module positively impacts the overall model. Wewould like to highlight the main intuition of our methodthat binarized dense prediction models in a multitask man-ner largely outperform the binarized single models throughBi-MTDP-F w.o. VIB & KD vs. Bi-Single.",
  ".Qualitative comparison with MTI-Net on PASCAL-Context": "modeled and optimized under a multitask framework to sup-plement degraded information caused by binarization op-erations. Based on this binary baseline, we further intro-duce variational information bottleneck and feature-basedknowledge distillation to supplement information flow. Ex-periment results show that our method significantly acceler-ates existing SoTA methods with comparably small perfor-mance drop over the mainstream dense prediction tasks onPASCAL VOC and NYUD-v2. Intriguingly, Bi-MTDP notonly reaches SoTA w.r.t. performance but also saves com-putational costs, compared with SoTA method ARTC .Acknowledgments: This research is partially supportedby NSF IIS-2309073, ECCS-212352101 and Cisco unre-stricted gift. This article solely reflects the opinions andconclusions of its authors and not the funding agencies."
}