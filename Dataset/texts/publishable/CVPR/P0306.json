{
  "with": ".Brief illustration of the current multi-modality 3D pre-training paradigm and our proposed scene-level consistency. (a) weshow the process of superpixel-superpoint association and clearly observe that superpixels with the same semantics can exist in the samescene or in different scenes, e.g., the green squares. (b) We summarize existing pre-training methods and find that they all use frame-levelconsistency to learn 3D representations. Moreover, we believe that this constraint breaks the semantic consistency across views/framesand visualize the drawback. (c) We show our proposed scene-level consistency, which keeping the semantic consistency across variousscenes. As a result, we achieve SOTA on three perceptual tasks with limited 3D annotation (Tab. 1). Our CSC framework builds a strongpre-training baseline for universal 3D Large-scale perception. AbstractAn effective pre-training framework with universal 3Drepresentations is extremely desired in perceiving large-scale dynamic scenes.However, establishing such anideal framework that is both task-generic and label-efficientposes a challenge in unifying the representation of thesame primitive across diverse scenes.The current con-trastive 3D pre-training methods typically follow a frame- level consistency, which focuses on the 2D-3D relationshipsin each detached image. Such inconsiderate consistencygreatly hampers the promising path of reaching an uni-versal pre-training framework: (1) The cross-scene seman-tic self-conflict, i.e., the intense collision between primitivesegments of the same semantics from different scenes; (2)Lacking a globally unified bond that pushes the cross-scenesemantic consistency into 3D representation learning. To",
  "arXiv:2405.07201v1 [cs.CV] 12 May 2024": "address above challenges, we propose a CSC frameworkthat puts a scene-level semantic consistency in the heart,bridging the connection of the similar semantic segmentsacross various scenes. To achieve this goal, we combinethe coherent semantic cues provided by the vision founda-tion model and the knowledge-rich cross-scene prototypesderived from the complementary multi-modality informa-tion. These allow us to train a universal 3D pre-trainingmodel that facilitates various downstream tasks with lessfine-tuning efforts. Empirically, we achieve consistent im-provements over SOTA pre-training approaches in seman-tic segmentation (+1.4% mIoU), object detection (+1.0%mAP), and panoptic segmentation (+3.0% PQ) using theirtask-specific 3D network on nuScenes. Code is released at hoping to inspirefuture research.",
  ". Introduction": "As one of the most promising applications of artificial intel-ligence, autonomous driving has undergone rapid develop-ment in recent years . In it, 3D scene per-ception plays a fundamental role to perceive and understandsurroundings, and thus has become increasingly attended inrecent researches . Within the 3D percep-tion, there are three essential tasks across different granular-ities, i.e., semantic segmentation ,object detection , and panoptic segmentation. All of them build upon powerful 3D represen-tations and recent success of vision foundation model showsgreat potential for such fabulous goal .Thanks to the paired image-lidar data captured by themulti-sensors , it lays the foundation for im-proving 3D representation through cross-modality learningfrom 2D image priors.However, how VFM knowledgecould benefit 3D scene perception without using any or lim-ited point cloud annotations remains under-explored.Inspired by the groundbreaking work SLidR , a suc-cession of multi-modality 3D self-supervised approaches has been proposed subsequently.To obtaindesired 3D representation, all of them adopt a frame-level consistency, which conduct superpixel-superpointcontrastive distillation from the association between a pointcloud frame and a image. We show the 2D-3D associationin (a). We observe that achieving the pre-trained 3Dbackbone with strong generalization ability still faces thefollowing two challenges: (1) the superpixels sharing theidentical semantic from the same or different images areerroneously treated as the negative pair. Despite the intro-duction of VFMs or the adoption of semantically tol-erant loss as coping strategies, the challenge remainsunresolved, especially in the presence of numerous identi-cal semantic superpixels observed across various views or scenes. As presented in (b), the cross-scene super-pixels (F a2D & F e2D) with the same semantic Cmsem are notconsistent in these methods. (2) The challenge of keepingglobal semantic consistency in large-scale scenarios. As anexample, for objects with identical semantic labels but fromdifferent frames, their features should be as close as pos-sible, whether the frames are from the same or disparatescenes.In this paper, we introduce a strong pre-trained baseline,termed CSC (Coherent Semantic Cues Framework), foruniversal 3D large-scale perception by learning the scene-level consistency (as shown in (c)).The coreidea is that we push the cross-scenes semantic consistencyinto the pristine 3D backbone, leveraging the coherent se-mantic cues provided by powerful VFMs and information-rich semantic prototypes from multi-modality. Specifically,CSC consists of two key components: (i) A VFM-AssistedSemantic Prototype Generation, where we first utilize theVFM to provide reliable and coherent semantic cues forall superpixels across diverse scenes and then produce themulti-modality semantic prototypes to cover the represen-tative features of involved semantics. (ii) A Coherent Se-mantic Consistency for 3D universal representation learn-ing. In this component, we propose a multi-modality proto-type blending module designed to fuse these unaligned pro-totypes that, while semantically aligned, reside in distinctfeature spaces.We individually process each prototypesthrough a modality-specific prototype projection module,yielding implicitly aligned prototypes. Subsequently, theseupdated prototypes are combined and fed into a multi-modality prototype fusion module, resulting in the mixedprototypes that incorporate information from both imageand lidar modalities. Ultimately, we perform a cross-scenesemantic contrastive loss between superpoints and mixedprototypes, thereby obtaining the universal 3D representa-tion with scene-level semantic consistency.Compared to the current methods, the CSC brings a newSOTA performance to all three mainstream 3D perceptiontasks, semantic segmentation, object detection, and panop-tic segmentation. This is particularly notable when onlylimited 3D annotations are available for a specific task. Themain contributions of this work are summarized as follows: To the best of our knowledge, CSC is the first work to ex-plore cross-scene semantic consistency in multi-modality3D pre-training, which achieves the semantic consistencyof all frames from all scenes.",
  ". Vision Foundation Models": "In light of the advancement of massive and diverse im-age sources and inspired by large-scale vision-languagepre-training techniques, the computer vision community is now witnessing hot atten-tion in building powerful vision systems.Among thesevision foundation models, DINOV2 and SAM have gath-ered the most widespread attention. The DINOv2 isan advanced self-supervised learning framework that lever-ages vision transformers (with 1B parameters) and enoughcurated data sources (about 142M images) to producinghigh-performance visual features. The segment anythingmodel (SAM) demonstrates a new paradigm with ro-bust zero-shot transferability, excelling in new image dis-tributions and tasks. Apart from the mentioned works, ap-proaches including SEEM and OneFormer alsoexpand the visual model landscape, offering alternatives forthe vision community. In this study, we explore the poten-tial of VFMs for universal 3D scene perception. We lever-age the VFM to acquire reliable and stable semantic cuesacross images from diverse scenes, harnessing these cues topromote global semantic consistency learning of 3D repre-sentations.2.2. Self-Supervised 3D Representation Learning Here, we focus on the line of contrastive-based self-supervised methods .According to the types of input modalities, these meth-ods can be further categorized into uni-modality and multi-modality self-supervised framework. For uni-modality, theycommonly perform the multi-view consistency constraint,which seek to the consistency of points/regions from vari-ous view transformations. TARL exploits vehicle mo-tion to extract different views of the same object in con-secutive point cloud frames to learn spatio-temporal view-consistent. When we pay attention to the point cloud acqui-sition, we will find that almost most of the point cloud datawill have its corresponding image data. For multi-modality,current studies not only consider 2D images but also involvetext. In this paper, we primarily discuss the assistance of2D images to advance 3D perception. SLidR is the pi-oneering work that take superpixels obtained from SLIC as units, and achieves superpixel-driven contrastive distilla-tion to initialize the 3D network. Subsequently, Seal ,the current SOTA method, introduces the popular VFMinto this field and gains breakthrough performance improve-ments. Compared to these methods, we propose semanticprototype to manage the coherent semantic cues of all vi-",
  ". Prototype-based Self-Supervised Learning": "In 2D self-supervised realm, a wide range of applicationsadopts the idea of clustering or prototype. In the USL-VI-ReID , the existing SOTA methods are developedon the ClusterContrast , which first generates prototypesusing a clustering algorithm and then optimizes their net-works via a cluster comparison mechanism. Meanwhile, inthe field of unsupervised semantic segmentation, many ex-cellent works use the concept of prototypes andshow inspiring performance. Inspired by these promisingstudy, our CSC use prototypes for 3D pre-training. Throughthe prototypes, we could bridge the connection of varioussuperpixels from different views or scenes.",
  ". Preliminaries": "To learn powerful 3D representations, the current 3D pre-training paradigm (stemming from SLidR ) utilizes thecalibrated relationships between 2D images and 3D pointclouds with the assistance of powerful 2D models. Techni-cally, for a point cloud frame P = {Pk | k = 1, . . . , K}comprising K points, each point Pk R4 representsthe k-th points three-dimensional location (x, y, z) cou-pled with its intensity feature.Meanwhile, the pointcloud frame is equipped with L surrounding images I ={Il | l = 1, . . . , L}, where Il RHW 3 denotes l-thRGB image with the shape of H W.In the pre-training phase, the images I and points Pare respectively fed into a 2D embedding network, I :RHW 3 RHW D, and a 3D embedding network,P:RK4RKD, to produce pixel-wise fea-tures and point-wise features. Next, grouping these fea-tures via 2D masks S2D (a.k.a, superpixels introduced in) obtained from a 2D segmentation algorithm, F :RHW 3 RHW 1, we can harvest superpixel embed-dings F2D and superpoint embeddings F3D. Finally, the ex-isting state-of-the-art methods will conduct frame-level superpixel-superpoint contrastive loss by utilizingthe 2D-3D mapping, which pulls the matched superpixel-superpoint features while pushing away unmatched pairs,to optimize the 3D backbone P .Problem Formulation.Extended on the paradigm,our goal is to build a general 3D self-supervised learningframework, that affords a wide range of downstream 3Dperception tasks like semantic segmentation, object detec-tion, and panoptic segmentation. We seek to perform scene-level consistency of 2D-3D elements relying on coherent se-mantic cues provided by coupling the recent popular VFMwith our multi-modality semantic prototypes.Thus, our",
  "of": ". Overview of the CSC framework. CSC leverages the scene-level semantic consistency to obtain the universal 3D representations(Sec. 3), and then fine-tunes the pre-trained 3D backbone for three downstream perception tasks (Sec. 4). To achieve the scene-levelsemantic consistency, CSC consists of the VFM-assisted semantic prototype generation module (Sec. 3.2) and the coherent semanticconsistency module (Sec. 3.3). self-supervised objective is more explicit by replacing theframe-level consistency, which is prone to be to be influ-enced by spatio-temporal movement or scene changes.Approach Overview.Our framework is outlined in. CSC framework consists of two key components:(1) a VFM-assisted semantic prototype generation coveringsemantic categories for large-scale cross-views/scenes (Sec.3.2) and (2) a coherent semantic consistency between super-points and prototypes for 3D representation learning (Sec.3.3). The main differences from the current paradigm arethat i) we leverage the VFM to obtain semantic-aware su-perpixels with cross-view/scene associations and ii) we pro-pose multi-modality semantic prototypes to mine coherentsemantic consistency for general 3D representation learn-ing.Formally, according to the VFM-assisted superpixelsS2D, we obtain the semantic prototype features F2D & F3Dfor 2D and 3D data. Then, coupling these features withsemantic cues Csem from VFM, we maintain two separatemodality prototypes P2D & P3D , which represent coherentsemantic representations for cross-scene objects with thesame semantic. Moreover, to fully utilize two prototypesfrom heterogeneous space, we design a multi-modality pro-totype blending mechanism, which consists of modality-specific prototype projection and multi-modality prototypefusion modules, resulting in a mixed prototype Pmix con-taining rich multi-modality information. Upon on the Pmixand F3D, we derive a coherent semantic consistency lossLpro to pushes close superpoint embeddings F3D to themixed prototypes Pmix. Due to our cross-scene semanticprototypes, we achieve the scene-level semantic consistency for 3D representation learning. According to the observa-tions of the universal improvement on three downstreamtasks (Tab. 2), our scene-level semantic consistency con-straint endows the generalization of 3D representations tovarious scene perception tasks.",
  ". VFM-Assisted Semantic Prototype Generation": "Let us start by generating the semantic-aware superpixelS2D from 2D vision foundation model, and then evolve twoseparate yet semantically aligned prototypes from comple-mentary modalities.Suerpixel & Superpoint Embeddings.Firstly, weutilize the pre-calibrated pose information to project eachpoint Pk onto a camera image Il.Then, we leverage aVFM, DINOv2 by default, to group visually similarregions into Q superpixels S2D = {Sq2D | q = 1, . . . , Q},where S2Dqdenotes the group of pixels belonging to theq-th superpixel. Combining the 2D-3D mapping and su-perpixels S2D, we can obtain the associated superpointsS3D = {Sq3D | q = 1, . . . , Q}. Subsequently, pairing withpixel/point-wise features generated from the 2D/3D embed-ding networks, we are able to obtain superpixel and su-perpoint embeddings, F2D = {F q2D | q = 1, . . . , Q} andF3D = {F q3D | q = 1, . . . , Q}, by averaging pooling thepixel and point features, where F q2D / F q3D is the q-th su-perpixel/superpoint embeddings.Multi-Modality Prototype Generation.For all pairsof superpixels and superpoints from diverse scenes, we uni-formly assign them with the semantic signs Csem shared allscenes. The Csem is obtained from the category-sensitiveVFM, where we get the refined VFM on arbitrary se- mantic segmentation benchmark.Subsequently, accord-ing to the Csem, we can group superpixel&superpoint em-beddings F2D & F3D with the same semantic sign, toobtain the 2D&3D semantic prototype features, P2D =P t2D | t = 1, . . . , T& P3D =P t3D | t = 1, . . . , T, byperforming an averaging operation.The process of themulti-modality prototype generation can be expressed asfollows:P t2D =1Ctsem",
  "Sq3D=ctF t2D,(1)": "where |Ctsem| is the count of the superpixels with the samesemantic sign t. The total number of semantic signs is T. Inour experiments, we use mask2former-based DINOV2 as the mask network to generate Csem, where the network isfine-tuned on the ADE20K dataset including T = 150semantic classes.Dicussion.From the pioneering work SLidR tothe amazing study Seal , the superpixels generationhas transitioned from the non-learning segmentation algo-rithm (i.e., SLIC in ) to the category-insensitiveVFM (i.e., SAM in ).The driving force be-hind improving segmentation algorithms stems from the an-noying self-conflict challenge within contrastive-based self-supervised frameworks.To circumvent this impediment,Seal first introduces category-insensitive VFMs (suchas SAM and SEEM ) to improve the quality of su-perpixels, significantly reducing the self-conflict issue be-tween over segmentation and semantic consistency in eachimage.Attracted by the dramatic performance improve-ment from incorporating VFMs, we also adopt the pow-erful VFM and further devise our scene-level consistencypre-training framework. Compared to Seal, we exploit be-lievable and consistent semantic cues provided by category-sensitive VFMs to alleviate self-conflict across all scenes.",
  ". Coherent Semantic Consistency": "Based on the VFM-assisted semantic prototype, we pro-pose a coherent semantic consistency to alleviate the chal-lenge of cross-scenes self-conflict and conduct scene-levelsemantic regularization for 3D representation learning. Toachieve an ideal 3D backbone, it is imperative to explorethe information-rich multi-modality prototypes. However,although the two prototypes of different modalities havebeen semantically aligned, they do not lie in a uniform fea-ture space. To reduce this gap, we design a multi-modalityprototype blending module consisting of modality-specificprototype projection and multi-modality prototype fusionsub-modules. By parallel performing feature projection oneach modality prototype followed by the fusion of multi-modality prototypes, the blending module will generate information-rich hybrid prototypes Pmix. Considering bothPmix and P3D, we could achieve the scene-level semanticcontrastive loss Lpro to endow the pre-trained 3D backbonewith the ability to stable semantic discrimination on com-plex and dynamic large-scale autonomous driving scenes.Inthe next, we would illustrate each component in detail.Multi-Modality Prototype BlendingThe MMPBmodule sequentially achieves feature alignment and fusionof heterogeneous modality prototypes via the modality-specific prototype projection and multi-modality prototypefusion modules. The two modules are defined as follows:1. Modality-Specific Prototype Projection.Given theP2D and P3D, several linear layers are employed in par-allel to implicitly project the feature space of differentmodality to uniform one, resulting the updated 2D proto-types P2D =Pt2D | t = 1, . . . , Tand 3D prototypes",
  "{P 13D, . . . , P T3D}Linear Layers { P13D, . . . , PT3D}.(2)": "2. Multi-Modality Prototype Fusion.Then, we fusethe prototypes from two modality prototypesP2D& P3D that are both semantic category and featurespace aligned, resulting in mixed prototypes Pmix =Ptmix | t = 1, . . . , T. The computation is: { P12D, . . . , PT2D, P13D, . . . , PT3D}Linear LayersP 1mix, . . . , P Tmix.(3)Following the MMPB, we obtain the blended prototypesPmix consist of the complementary multi-modality informa-tion. Thus, if using Pmix, the resulting 3D backbone willequip the comprehensive discrimination from both 2D im-age modality and 3D lidar modality.Prototype-based Loss.We propose a scene-level se-mantic contrastive loss Lproto between P3D and Pmix, toendow the pre-trained 3D backbone with the ability tocoherence semantic discrimination on complex and dy-namic large-scale autonomous driving scenes. Formally, theprototype-based contrastive loss Lpro is defined as follows:",
  "|Csem|i=0 expF 3D, P imix/pro,(4)": "where , denotes the scalar production. The sign P +mix isthe positive prototype of superpoint embedding F 3D. Thesymbol pro is a temperature hyper-parameter.Discussion.Here, we argue that using coherent se-mantic cues from VFM is much better than the commonlyadopted traditional cluster algorithm in the benefits formulti-modality prototypes fusion. Mostly self/unsupervisedmethods leverage an unsupervised clustering algo-rithm, such as K-Means and DBSCAN, to produce the their prototypes.However, these methods all face a commonchallenge that requires manually adjusting clustering pa-rameters based on the distribution of a specific dataset. Inaddition to the problem of hand-crafted parameters, thereexisting an other tricky challenge in the multi-modality self-supervised pre-training task, which is the alignment prob-lem across different modalities. Fortunately, the introduc-tion of class-sensitive VFM is able to bypass the above trou-ble challenges without any additional effort.",
  "Qj=0 expF i3D, F j2D/sp,(5)": "where i-th superpoint feature F i3D and i-th superpixel fea-ture F i2D are matched according to the calibration betweenpoint cloud frames and the related surround images. (2) Weleverage the proposed prototype-based loss Lpro to providethe global semantic consistency for 3D representation learn-ing. Our total loss is given by:Ltotal = Lsp + 1{n>}Lpro,(6)where the indicator of 1{n>} takes the value 1 if n > and 0 otherwise, where n is the current training epoch and is the hyper-parameter that controls the starting epoch ofusing Lpro. By default, = 5, sp = 0.07, and pro = 1.0.",
  ". Experiments": "In this section, we present the experimental results of threedifferent 3D perception tasks, each implemented by thepopular 3D backbone of its domain. Specifically, semanticsegmentation implemented by MinkUNet in Sec. 4.1,object detection implemented by VoxelNet in Sec. 4.2,and panoptic segmentation implemented by Cylinder3D in Sec. 4.3. Conveniently, we draw Tab. 1 to show thecomprehensive comparison of CSC with existing methodson three perception tasks with limited labeling. In addition,we study the role of each component in Sec. 4.4. Due to thelimited space, visualization and other experiments would beshown in the supplementary materials.Datasets. We pre-train all three models on nuScenesdataset, which is a large-scale autonomous driving datasetincluding 1,400,000 camera images as well as 90,00 Li-dar sweeps across 1000 scenes. On nuScenes dataset, eachpoint cloud keyframe is equipped with six calibrated sur-round images. During pre-training phase, we use the un-labeled RGB images and point clouds from 600 scenes toupdate backbones in our CSC, same as SLidR. About fine-tuning on three 3D perception tasks, we all conduct exper-iments on nuScenes, to evaluate the quality of pre-trained",
  "Ours19.3 / 74.5 / 24.6 (+3.0 PQ)23.1 / 76.9 / 28.5 (+1.5 PQ)": ". On nuScenes, CSC is compared with current state-of-the-art methods in three downstream tasks with limited annotation.Obvious improvement in term of semantic segmentation, objectdetection, and panoptic segmentation could be found. 3D backbone with various percentage annotations.ThenuScenes dataset is also used in our fine-tuning for threeperception task, to evaluate the annotation-efficient of thevarious pre-training methods under different percentages oflabeling.Pre-training Details. Due to variances in network ar-chitectures, various 3D networks require different config-urations in pre-training. For MinkUNet, we use the SGDoptimizer with the 2.0 initial learning rate and a cosine an-nealing learning rate scheduler with a total of 50 epochs.The pre-training configuration of VoxelNet is similar to thatof MinkUNet, the difference is that the initial learning rateis 0.01. As for Cylinder3D, we use the Adam optimizer of0.001 initial learning rate and also employ the cosine an-nealing learning rate scheduler with a total of 15 epochs.All pre-trained 3D backbones are done with 2 RTX A6000with a batch size 16.",
  ". Annotation-Efficient Semantic Segmentation": "In this section, we measure the information of seman-tics learned by the 3D representations using various self-supervised frameworks. Overall, we compared CSC withthe state-of-the-art methods on two benchmark datasets. Indetails, we evaluate the fine-tuned semantic segmentationperformance of pre-trained 3D backbone on nuScenes and SemanticKITTI datasets.Following SLidR , we fine-tune the pre-trained 3Dbackbone using various percentage point cloud subsets with1%, 5%, 10%, 25%, and 100% of annotations for nuScenesand 1% for SemanticKITTI. Meanwhile, we conduct a lin-ear evaluation using 100% annotations, which trains only",
  "Ours-46.0 (+1.1) 47.0 (+1.2) 57.0 (+1.4) 63.3 (+0.4) 68.6 (+0.2) 75.7 (+0.1) 47.2 (+0.6)": ". Results (mIoU) of different pre-training methods on semantic segmentation fine-tuning. On nuScenes, we use 100% annotatedscans for linear probing and 1%, 5%, 10%, 25%, 100% annotation for fine-tuning. In addition, we use 1% labels for fine-tuning onSemanticKITTI. a linear head and freezes other layers of the 3D back-bone, to investigate the generalizability of representationslearned via self-supervised learning without task-specificfine-tuning. We report the metric of mean Iou (mIoU) toevaluate various methods.In Tab.2, we show the comparison of the previ-ous methods with CSC. It is evident that the 3D back-bone with pre-trained parameters derived from arbitrary 3Dself-supervised pre-training framework substantially out-performs the random initialized one. Compared with thecurrent state-of-the-art method Seal on nuScenes, ourCSC provides significant mIoU improvements of +1.1% forlinear probing, +1.2% and +1.4% for 1% and 5% few-shotfine-tuning settings, respectively.In addition, CSC alsoachieves better generalization of +0.6% boosting on the out-of-distribution annotation-efficient semantic segmentationin the SemanticKITTI. Compared to Seal, who only utilizesa class-insensitive VFM for each individual image to alle-viate the self-conflict problem, our CSC presents a better3D network with strong discriminative power. This sug-gests the importance of embracing the coherent semanticcues from the class-sensitive VFM and the scene-level se-mantic consistency in the pre-training phase. Due to pagelimitations, we show the average per-class performance of1% annotation for detailed analysis in supplementary mate-rials.",
  ". Annotation-Efficient Object Detection": "In the vision system of autonomous driving, 3D object de-tection is a common and challenging task. Thus, we fur-ther evaluate the quality of our pre-trained lidar represen-tation on this object-level task on the nuScenes. Followingthe previous works, we fine-tune the pre-trained 3D back-bone with 5%, 10%, and 20% of the labeled data, respec-tively. Moreover, we embed the pre-trained Cylinder3D intotwo detection models, CenterPoint and SECOND. We re-fer to the evaluation protocol of nuScenes and reportthe mean average precision (mAP) and nuScenes detectionscore (NDS), where NDS is a weighted average of mAP that",
  ". Results (mAP and NDS) when fine-tuning the pre-trainedbackbones to object detection using two models (CenterPoint andSECOND) with 5%, 10%, and 20% labels on nuScenes": "measures the quality of the detection in various terms.In Tab. 3, we compare the existing methods with ourCSC. Compared to SLidR, the current SOTA method TriCChas gain the significantly improvement of 1.3% and 1.2%mAP in 5% mAP annotations by using the temporal consis-tency loops on both detection models. Taking the excellentwork TriCC as the comparison, our CSC achieves the im-provement of 0.7% mAP and 0.4% mAP without explicittemporal consistency. Surprisingly, the growth in perfor-mance by our CSC is steady, even using more annotation.",
  ". Annotation-Efficient Panoptic Segmentation": "In this experiment, we compare the various pre-trainingmethods for panoptic segmentation, which evaluates bothsemantic and instance recognition ability of the learned 3Dbackbone. To our knowledge, CSC is the first pre-trainingframework that transferring the pre-trained 3D backbone,which absorbs the prior knowledge from 2D realm, to themore challenging and annotation-intensive panoptic seg-mentation. Considering fair comparisons, we refer to boththe setting from the previous lidar-only pre-training method",
  "Ours + OneFormer19.578.325.023.475.728.8": ". Results (PQ, SQ, and RQ) when fine-tuning the pre-trained models to panoptic segmentation with 1% and 5% labels onnuScenes. Considering the absence of existing pre-training meth-ods, besides random initialization and the default CSC settings,we additionally establish three sets of experiments: original SLidRwith SLIC, SLidR with DINOV2, and CSC with OneFormer. and the current state of development in the panop-tic segmentation.Specifically, we employ the Panoptic-PolarNet with Cylinder3D , just like the currentsupervised SOTA method . About utilization rate of la-bels, we select the percentages of 1%, 5%, and 10% to fine-tune the pre-trained network. Given the absence of exist-ing methods conducted on panoptic segmentation, we treatSLidR as the primary comparison approach and using dif-ferent 2D segmentation methods for superpixel generation.Overal, there are five experiments: random initialization,pre-training by SLidR and SLIC, pre-training by SLidR andDinov2, pre-training by CSC and Dinov2, and pre-trainingby CSC and OneFormer. About the evaluation metrics, wereport segmentation quality (SQ), recognition quality (RQ),and panoptic quality (PQ).Tab. 4 shows that multi-modality pre-training methodis consistently better than the random initialization. With1% annotation, replacing the superpixel generation methodfrom SLIC to Dinov2 and further adopting our CSC can im-prove the PQ metric by 1.3% and 3.0%, respectively, com-pared to the original SLidR. On top of CSC, replacing DI-NOv2 by other semantic segmentation networks, such asOneFormer, will result in a considerable performance gainof 3.2%. Observing the changes in the SQ and RQ met-rics of panorama segmentation across 1% and 5% ratios,we can find that the improvement brought from SLidR toour CSC is mainly in the SQ metrics (i.e., 65.7 74.5 and73.5 76.9) while the improvement in the RQ metrics isrelatively slight (i.e., 21.4 24.7 and 27.1 28.5). Thisphenomenon is consistent with previous results on semanticsegmentation and object detection, i.e., our approach signif-icantly improves 3D networks ability to recognize semanticcategories while providing a limited increase in the abilityto discriminate different instances with the same semantics.4.4. Ablation StudyWe perform ablation experiments to examine the contribu-tion of VFM for superpixel generation and multi-modalityprototype blending (MMPB). We also investigate the im-pact of discarding MMPB to directly employ raw 3D pro-totypes for 3D backbone learning. All the ablation studies",
  "(1)38.038.252.258.8(2)44.041.152.160.9(3)44.040.353.360.5(4)46.047.057.063.3": ". Ablation study of each component pre-trained and fine-tuned on nuScenes. SP: Superpixel-Superpoint contrastive lossLsp. VFM: Vision foundation models. 3D Pro.: The mixed pro-totypes is replace by the raw 3D prototypes in the Lpro. MMPB:Multi-Modality Prototype blending. are conducted by 1%, 5%, and 10% semantic segmentationfine-tuning and 100% linear evaluation on nuScenes dataset.Compared to the original SLidR #(1) at 1% labeling, in-troducing our VFM #(2) and MMPB #(4) in turn, resultingin the steep rise of mIoU, from 38.2 41.4 47.0. Thisupward trend persists across various annotation ratios, how-ever, it tends to decelerate with the increase in the numberof labels. Interestingly, when comparing #(3) with #(4), wecan observe that directly employing the 3D semantic pro-totypes assisted by the VFM for 3D representation learningresults in a performance deterioration of 0.8% mIoU, yet in-corporating our proposed MPPB reverses this degradationand gains the promotion of 6.7%.5. Conclusion In this paper, we study the multi-modality 3D pre-trainingtask from the drawbacks of the existing methods, includ-ing the self-conflict of cross-scene semantic segments andthe absence of building the global semantic units. Our CSCperforms scene-level semantic consistency via the combi-nation VFM-assisted semantic cues and multi-modality se-mantic prototypes.Firstly, obtain the coherent semanticsuperpixels based on the VFM and use the semantics togenerate prototypes for two modalities.Then, computethe unified prototypes by modality-specific prototype pro-jection with multi-modality prototype blending. Thereby,we can achieve the cluster contrastive loss between 3D su-perpoint features and mixed prototypes for learning uni-versal 3D representation. Extensive experiments show thatour method delivers state-of-the-art results on semantic seg-mentation, object detection, and panoptic segmentation.Acknowledgment.This work is supported by the Na-tional Natural Science Foundation of China No.62302167,U23A20343, 62222602, 62206293, 62176224, 62106075,and 62006139, Shanghai Sailing Program (23YF1410500),Natural Science Foundation of Shanghai (23ZR1420400),ScienceandTechnologyCommissionofShang-haiNo.21511100700,NaturalScienceFoundationofChongqing,China(CSTB2023NSCQ-JQX0007,CSTB2023NSCQ-MSX0137),CCF-Tencent Rhino-BirdYoung Faculty Open Research Fund (RAGR20230121),CAAI-Huawei MindSpore Open Fund, and DevelopmentProject of Ministry of Industry and Information Technology(Grant Number: ZTZB.23-990-016). Radhakrishna Achanta, Appu Shaji, Kevin Smith, AurelienLucchi, Pascal Fua, and Sabine Susstrunk. Slic superpix-els compared to state-of-the-art superpixel methods. IEEEtransactions on pattern analysis and machine intelligence,34(11):22742282, 2012. 3, 5 Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall.Se-mantickitti: A dataset for semantic scene understanding oflidar sequences.In Proceedings of the IEEE/CVF inter-national conference on computer vision, pages 92979307,2019. 2, 6 Alexandre Boulch, Corentin Sautier, Bjorn Michele, GillesPuy, and Renaud Marlet.Also: Automotive lidar self-supervision by occupancy estimation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1345513465, 2023. 3 Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-ancarlo Baldan, and Oscar Beijbom.nuscenes: A multi-modal dataset for autonomous driving. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1162111631, 2020. 2, 6, 7 Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu,Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wen-ping Wang.Clip2scene: Towards label-efficient 3d sceneunderstanding by clip. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages70207030, 2023. 2, 3 Zhong Chen, Zhizhong Zhang, Xin Tan, Yanyun Qu, andYuan Xie.Unveiling the power of clip in unsupervisedvisible-infrared person re-identification. In Proceedings ofthe 31st ACM International Conference on Multimedia, page36673675, New York, NY, USA, 2023. Association forComputing Machinery. 3, 5 Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4dspatio-temporal convnets: Minkowski convolutional neuralnetworks. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 30753084,2019. 6 Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Siyu Zhu,and Ping Tan. Cluster contrast for unsupervised person re-identification. In Proceedings of the Asian Conference onComputer Vision, pages 11421160, 2022. 3 Chengjian Feng, Zequn Jie, Yujie Zhong, Xiangxiang Chu,and Lin Ma. Aedet: Azimuth-invariant multi-view 3d ob-ject detection. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2158021588, 2023. 2 Runyang Feng, Yixing Gao, Xueqing Ma, Tze Ho Elden Tse,and Hyung Jin Chang. Mutual information-based temporaldifference learning for human pose estimation in video. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1713117141,2023. 3",
  "suite. In 2012 IEEE conference on computer vision and pat-tern recognition, pages 33543361. IEEE, 2012. 2": "Jingyu Gong, Jiachen Xu, Xin Tan, Haichuan Song, YanyunQu, Yuan Xie, and Lizhuang Ma. Omni-supervised pointcloud segmentation via gradual receptive field componentreasoning.In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1167311682, 2021. 2 Jingyu Gong, Jiachen Xu, Xin Tan, Jie Zhou, Yanyun Qu,Yuan Xie, and Lizhuang Ma. Boundary-aware geometric en-coding for semantic segmentation of point clouds. In Pro-ceedings of the AAAI Conference on Artificial Intelligence,pages 14241432, 2021. 2 Jingyu Gong, Fengqi Liu, Jiachen Xu, Min Wang, Xin Tan,Zhizhong Zhang, Ran Yi, Haichuan Song, Yuan Xie, andLizhuang Ma.Optimization over disentangled encoding:Unsupervised cross-domain point cloud completion via oc-clusion factor manipulation.In European Conference onComputer Vision, pages 517533. Springer, 2022. 2 Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, WenhaiWang, et al. Planning-oriented autonomous driving. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1785317862, 2023. 2",
  "Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, NikitaOrlov, and Humphrey Shi. OneFormer: One Transformer toRule Universal Image Segmentation. 2023. 3": "Yang Jiao, Zequn Jie, Shaoxiang Chen, Jingjing Chen, LinMa, and Yu-Gang Jiang.Msmdfusion: Fusing lidar andcamera at multiple scales with multi-depth seeds for 3d ob-ject detection. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2164321652, 2023. 2 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment anything. In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), pages 40154026, 2023. 3, 5 Marvin Klingner, Shubhankar Borse, Varun Ravi Kumar,Behnaz Rezaei, Venkatraman Narayanan, Senthil Yogamani,and Fatih Porikli.X3kd: Knowledge distillation acrossmodalities, tasks and stages for multi-camera 3d object de-tection.In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1334313353, 2023. 2 Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wen-wei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu.Robo3d: Towards robust and reliable 3d perception againstcorruptions. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1999420006, 2023.2 Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu.Lasermix for semi-supervised lidar semantic segmentation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 2170521715, 2023.2 Xia Kong, Zuodong Gao, Xiaofan Li, Ming Hong, Jun Liu,Chengjie Wang, Yuan Xie, and Yanyun Qu. En-compactness:Self-distillation embedding & contrastive generation for gen-eralized zero-shot learning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 93069315, 2022. 3 Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya Jia.Spherical transformer for lidar-based 3d recognition. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1754517555, 2023. 2 Jinke Li, Xiao He, Yang Wen, Yuan Gao, Xiaoqiang Cheng,and Dan Zhang.Panoptic-phnet: Towards real-time andhigh-precision lidar panoptic segmentation via clusteringpseudo heatmap.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1180911818, 2022. 2 Jiale Li, Hang Dai, Hao Han, and Yong Ding.Mseg3d:Multi-modal 3d semantic segmentation for autonomous driv-ing. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 2169421704,2023. 2 Kehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, YianZhao, Guoli Song, Chang Liu, Li Yuan, and Jie Chen. Ac-seg: Adaptive conceptualization for unsupervised semanticsegmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 71627172, 2023. 3 Li Li, Hubert PH Shum, and Toby P Breckon. Less is more:Reducing task and model complexity for 3d point cloud se-mantic segmentation. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages93619371, 2023. 2 Xiaofan Li, Yachao Zhang, Shiran Bian, Yanyun Qu, YuanXie, Zhongchao Shi, and Jianping Fan. Vs-boost: boostingvisual-semantic association for generalized zero-shot learn-ing.In Proceedings of the Thirty-Second InternationalJoint Conference on Artificial Intelligence, pages 11071115, 2023. 3 Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:Learning birds-eye-view representation from multi-cameraimages via spatiotemporal transformers. In European con-ference on computer vision, pages 118. Springer, 2022. 2 Hanxue Liang, Chenhan Jiang, Dapeng Feng, Xin Chen,Hang Xu, Xiaodan Liang, Wei Zhang, Zhenguo Li, and LucVan Gool. Exploring geometry-aware contrast and cluster-ing harmonization for self-supervised 3d object detection. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 32933302, 2021. 7 Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong,Yuchen Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, YuexinMa, Yikang Li, et al.Uniseg: A unified multi-modal li-dar segmentation network and the openpcseg codebase. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 2166221673, 2023. 2",
  "any point cloud sequences by distilling vision foundationmodels. arXiv preprint arXiv:2306.09347, 2023. 2, 3, 5,6, 7": "Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, andWinston H Hsu.Learning from 2d: Contrastive pixel-to-point knowledge transfer for 3d pretraining. arXiv preprintarXiv:2104.04687, 2021. 7 Zhenguang Liu, Haoming Chen, Runyang Feng, Shuang Wu,Shouling Ji, Bailin Yang, and Xun Wang. Deep dual consec-utive network for human pose estimation. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 525534, 2021. 3",
  "Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He,and Tianrui Li. SegCLIP: Patch aggregation with learnablecenters for open-vocabulary semantic segmentation. ICML,2023. 3": "Anas Mahmoud, Jordan SK Hu, Tianshu Kuai, Ali Harakeh,Liam Paull, and Steven L Waslander. Self-supervised image-to-point distillation via semantically tolerant contrastive loss.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 71027110, 2023. 2,3, 6, 7 LucasNunes,LouisWiesmann,RodrigoMarcuzzi,Xieyuanli Chen, Jens Behley, and Cyrill Stachniss. Tempo-ral consistent 3d lidar representation learning for semanticperception in autonomous driving.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 52175228, 2023. 3, 8 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V.Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-janowski. Dinov2: Learning robust visual features withoutsupervision, 2023. 3, 4, 5 Bo Pang, Hongchi Xia, and Cewu Lu.Unsupervised 3dpoint cloud representation learning by triangle constrainedcontrast for autonomous driving.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 52295239, 2023. 2, 3, 6, 7 SongyouPeng,KyleGenova,ChiyuJiang,AndreaTagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.Openscene: 3d scene understanding with open vocabularies.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 815824, 2023. 2, 3 Luigi Riz, Cristiano Saltori, Elisa Ricci, and Fabio Poiesi.Novel class discovery for 3d point cloud semantic segmenta-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 93939402,2023. 2 Corentin Sautier, Gilles Puy, Spyros Gidaris, AlexandreBoulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidarself-supervised distillation for autonomous driving data. InProceedings of the IEEE/CVF Conference on Computer Vi-",
  "Corentin Sautier, Gilles Puy, Alexandre Boulch, RenaudMarlet, and Vincent Lepetit. Bevcontrast: Self-supervisionin bev space for automotive lidar point clouds. arXiv preprintarXiv:2310.17281, 2023. 3": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, AurelienChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Benjamin Caine, et al. Scalability in perceptionfor autonomous driving: Waymo open dataset. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 24462454, 2020. 2 Xin Tan, Qihang Ma, Jingyu Gong, Jiachen Xu, ZhizhongZhang, Haichuan Song, Yanyun Qu, Yuan Xie, and LizhuangMa.Positive-negative receptive field reasoning for omni-supervised 3d segmentation. IEEE Transactions on PatternAnalysis and Machine Intelligence, 45(12):1532815344,2023. 2 Jiangming Wang, Zhizhong Zhang, Mingang Chen, YiZhang, Cong Wang, Bin Sheng, Yanyun Qu, and Yuan Xie.Optimal transport for label-efficient visible-infrared personre-identification. In European Conference on Computer Vi-sion, pages 93109. Springer, 2022. 3 Yuqi Wang, Yuntao Chen, and Zhaoxiang Zhang. Frustum-former: Adaptive instance-aware resampling for multi-view3d detection. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 50965105, 2023. 2 Xin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, andXiaojuan Qi. Self-supervised visual representation learningwith semantic grouping.Advances in Neural InformationProcessing Systems, 35:1642316438, 2022. 3 Yanhao Wu, Tong Zhang, Wei Ke, Sabine Susstrunk, andMathieu Salzmann. Spatiotemporal self-supervised learningfor point clouds in the wild. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 52515260, 2023. 3 Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, LeonidasGuibas, and Or Litany.Pointcontrast: Unsupervised pre-training for 3d point cloud understanding.In ComputerVisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part III 16, pages574591. Springer, 2020. 7 Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, XizhouZhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao,Lewei Lu, et al.Bevformer v2: Adapting modern imagebackbones to birds-eye-view recognition via perspective su-pervision. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1783017839, 2023. 2 Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and IshanMisra.Self-supervised pretraining of 3d features on anypoint-cloud. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1025210263, 2021.7",
  "Proceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1761917629, 2023. 5": "Zhiwei Zhang, Zhizhong Zhang, Qian Yu, Ran Yi, YuanXie, and Lizhuang Ma. Lidar-camera panoptic segmenta-tion via geometry-consistent and semantic-aware alignment.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 36623671, 2023. 2, 8 Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-dler, Adela Barriuso, and Antonio Torralba. Semantic under-standing of scenes through the ade20k dataset. InternationalJournal of Computer Vision, 127(3):302321, 2019. 5 Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learningfor point cloud based 3d object detection. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 44904499, 2018. 6 Zixiang Zhou, Yang Zhang, and Hassan Foroosh. Panoptic-polarnet: Proposal-free lidar point cloud panoptic segmenta-tion. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1319413203,2021. 2, 8 Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, YuexinMa, Wei Li, Hongsheng Li, and Dahua Lin.Cylindricaland asymmetrical 3d convolution networks for lidar seg-mentation. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 99399948,2021. 6, 8"
}