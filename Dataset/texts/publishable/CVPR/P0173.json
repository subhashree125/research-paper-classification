{
  "Abstract": "Digital mammography is essential to breast cancer de-tection, and deep learning offers promising tools for fasterand more accurate mammogram analysis. In radiology andother high-stakes environments, uninterpretable (blackbox) deep learning models are unsuitable and there isa call in these fields to make interpretable models.Re-cent work in interpretable computer vision provides trans-parency to these formerly black boxes by utilizing proto-types for case-based explanations, achieving high accuracyin applications including mammography. However, thesemodels struggle with precise feature localization, reasoningon large portions of an image when only a small part is rel-evant. This paper addresses this gap by proposing a novelmulti-scale interpretable deep learning model for mammo-graphic mass margin classification. Our contribution notonly offers an interpretable model with reasoning alignedwith radiologist practices, but also provides a general ar-chitecture for computer vision with user-configurable pro-totypes from coarse- to fine-grained prototypes.",
  ". Introduction": "Digital mammography plays a crucial role in detecting anddiagnosing breast cancer, a pervasive health concern world-wide. Advancements in deep learning and computer visionhave increased the speed and accuracy of lesion classifica- . Activation maps for FPN-IAIA-BL in comparisonto IAIA-BL. FPN-IAIA-BL can learn human interpretable proto-types at any scale, including fine-grained details most salient tomass margin classification. tions for mammography. However, when used for high-stakes tasks like medical diagnoses, deep learning modelsshould be inherently interpretable so that, among other ad-vantages, models can be fact checked .Recent work has shown that interpretable, case-basedmachine learning models can provide accurate, human un-derstandable explanations for their predictions while per-",
  "arXiv:2406.06386v1 [cs.CV] 10 Jun 2024": "forming on par with other state-of-the-art models .These prototype-based deep learning models have also beenapplied to digital mammography by Barnett et al. , whodeveloped the Interpretable AI Algorithm for Breast Le-sions (IAIA-BL) model, an interpretable model for massmargin classification.They focused on classification onmargins, a descriptor of the edges around the mass, becauseit is a key factor in identifying cancerous lesions under theBreast Imaging Reporting and Data System (BI-RADS).IAIA-BL successfully classified margins using prototypes,as shown in the third column of . However, the pro-totypes often identified more than just the margin or eventhe entire lesion, leaving any detailed analysis of the mar-gin to the user.To address this gap, we develop FPN-IAIA-BL, a multi-scale interpretable deep learning model for mammographicmass margin classification. It can be configured to provideprototypes at various levels of granularity, with multiplescales within the same model. We build the models ar-chitecture using both the Feature Pyramid Network (FPN)and IAIA-BL model. We developed a new training sched-ule and objective function, as the training methods and lossterms used by these predecessors were insufficient to trainthe combined architecture. The main contributions of thiswork are that: We develop an inherently interpretable deep learning ar-chitecture that learns prototypes at multiple scales.",
  ". Related Work": "Interpretability of deep learning models is critical for high-stakes applications like breast cancer detection and diag-nosis. In recent years, inherently interpretable deep neuralnetworks have grown in popularity. As compared to posthocexplanation techniques such as saliency visualizations , activation maximization , andimage perturbation methods which approximatemodel reasoning after training, inherently interpretabletechniques such as provideexplanations guaranteed to be faithful to the models under-lying decision-making process.FPN-IAIA-BL uses inherently interpretable case-basedreasoning with prototypes by building upon IAIA-BL ,a case-based model for mass margin classification. IAIA-BL was limited to learning prototypes at only one scale,with prototypes often identifying more of the image than isrelevant for margin classification. In contrast, FPN-IAIA-BL learns prototypes at various scales including highly-localized, fine-grained prototypes that select small details,as shown in . This is possible because FPN-IAIA-BL incorporates features at various scales.Typically, a key challenge in mammogram analysis is capturing information at various scales, since traditionalCNN architectures focus on a single image resolution.Multi-scale approaches like and address this chal-lenge by incorporating features extracted at different scaleswithin the network. A foundational architecture for multi-scale predictions is the Feature Pyramid Network (FPN) which introduces a bottom-up and top-down pyramidalarchitecture that produces multiple feature maps from fine-grained to coarse. As a result, FPNs are able to localize toobjects of multiple scales for object detection.Our FPN-IAIA-BL architecture leverages this bottom-upand top-down pyramidal architecture to learn prototypes atmultiple scales by augmenting IAIA-BLs VGG-16 back-bone with a similar structure, detailed in . Further-more, our model also provides visual, human interpretable,case-based reasoning for each classification.",
  ". FPN-IAIA-BL Architecture": "Inspired by the Feature Pyramid Network (FPN), the FPN-IAIA-BL model adds lateral and top-down connections tothe original VGG-16 convolutional layers used in the IAIA-BL architecture as its foundation. illustrates thisarchitecture. The model consists first of an FPN that ex-tracts useful feature maps at multiple scales, allowing formore varied representation than single-scale IAIA-BL. TheFPN is followed by the prototype layer g in which the inputimages feature maps are compared to learned prototypes toproduce similarity scores. Fully connected layer h then usesthe similarity scores to produce margin class predictions.",
  ". Multi-Scale Feature Maps from Feature Pyra-mid Network": "IAIA-BL uses a CNN to create a single feature map zwhich limits the network to prototypes at the scale of thatoutput feature map. In contrast, FPN-IAIA-BL uses the la-tent feature maps from multiple layers in the CNN, whichhave different spatial and semantic scales. Thus, the outputof the set of convolutional layers f in FPN-IAIA-BL is a setof feature maps of varying spatial scale, which we refer toas the feature pyramid f(x) = Z = {z(2), z(3), z(4), z(5)}.For our implementation, the coarsest feature maps were 14by 14, and finest were 56 by 56.For the VGG-16 backbone, we use the output from eachblocks max-pooling layer to form the intermediate fea-ture map levels in the bottom-up pathway (left column ofbackbone in ).We also include the final out-put of the convolutional layers as a feature map at thetop.We denote these bottom-up feature maps as C ={c(2), c(3), c(4), c(5)} where c(2) is the base of the bottom-up pyramid, and c(5) is the top.As in FPN , the top-down pathway produces a sec-ond feature pyramid. For each level, an upsampled featuremap with spatially coarser information is combined with . FPN-IAIA-BL Architectre. The input image x passes through convolutional layers f consisting of an FPN with a VGG-16backbone, which creates an pyramid of feature maps f(x). Each patch of each level of the feature pyramid (referred to as FPN level)is then compared to each prototype of the same FPN level using a cosine distance to produce an activation map. The activation map isthen used to calculate an overall similarity score sj between the input image and the prototype for each prototype. Finally, a set of fullyconnected last layer produces logits ymargin for each margin class. a corresponding laterally connected feature maps from thebottom-up pyramid. Then, each combined feature map ispassed through a 3 3 convolution to reduce the aliasingeffect of upsampling and output the feature map z(l).",
  ". Prototype Layer": "In the prototype layer g, we have m prototypes where eachprototype can be configured to represent a specific class cand FPN level l. For m prototypes, let S = {(cj, lj, j)}mj=1represent our prototype configuration, and denote our pro-totypes as P = {p(c,l,j)}S where the j-th prototype is fromclass c with FPN level l. Each prototype is 1 1 d sothat each prototype has the same feature dimension d as theconvolutional feature pyramid. As in IAIA-BL , the pro-totypes can be interpreted as a characteristic pattern repre-senting a specific class. It can be visually understood byexamining a segment of the training image where this pat-tern was derived.Once we have computed each feature map in the con-volutional feature pyramid f(x), we compute the similaritybetween each prototype in prototype layer g and the corre-sponding feature map. The FPN-IAIA-BL similarity scoresj differs from that of IAIA-BL in three ways.First, because the prototypes are assigned to specificFPN levels l, similarities for a set of prototypes p(,l,) arecomputed only using the feature map from the same FPNlevel z(l). Second, instead of using inverted L2 distancebased similarity, we use a cosine similarity as described in. The cosine similarity is calculated between a pro-totype and each 1 1 d patch within the corresponding",
  ". Data and Training": "The dataset, previously studied in , includes 2D digitalbreast x-rays from patients at the Duke University HealthSystem taken between 2008 and 2018. Data collection wasapproved by Duke Health IRB and labeled by a fellowship-trained breast imaging radiologist. While IAIA-BL usedonly the subset of the images that contained a lesion, wealso introduce a negative class which consists of images oftissue without lesions. Supplement Section C details howthe data for this class were generated.The training of FPN-IAIA-BL consists of three stages:(A) a warmup stage, (B) a projection of prototypes, and (C) . Case-based explanation generated by FPN-IAIA-BL. This circumscribed (circ.) lesion is correctly classified as circumscribed.a, Test images. b, Activation of prototype on test images. c, Most relevant part of prototype. d, Learned prototypical lesion. e, Prototypeself-activation. f, Contribution to class score. This visualization format for this figure matches that of . full network fine-tuning. Because we use a trained VGG-16 backbone from IAIA-BL to construct our FPN, we firstfreeze the VGG-16 backbone in Stage A to warm up all theother layers. Stage B projects the learned prototype vectorsonto a patch from any input images corresponding featuremap in the same fashion as in . Stage C continues thesetwo stages and unfreezes the VGG-16 backbone to allow forfine-tuning of the full network.For stages A and C, we minimize the loss function:",
  "= CE + 1clust + 2sep + 3ortho + 4fine(4)": "where cross entropy (CE) penalizes misclassification and1, 2, 3, 4 are coefficients chosen empirically to bal-ance the cluster (clust), separation (sep), and orthogo-nality (ortho) losses as defined in and fine-annotationloss (fine) modified from . The modifications to thefine-annotation loss introduce user-configurable coefficientswhich encourage and penalize the model for activating in-side and outside the fine annotations differently for eachclass pair. Supplement Section B details the fine-annotationcoefficients.These loss terms have not previously been combined.",
  ". Experiments and Results": "In our experiments, we find that FPN-IAIA-BL is able tolearn localized prototypes that achieve acceptable perfor-mance. An interpretable visual result of the FPN-IAIA-BLis shown in and is compared to baselines in . The best performing FPN-IAIA-BL model was able to achieve an average AUROC of 0.88 with one-vs-rest AU-ROCs of 0.865 for circumscribed, for indistinct, and 0.908for spiculated margin classes. A further comparison of theperformance with IAIA-BL and an uninterpretable baseline(VGG16) is presented in . The confusion matrix ofthis model is shown in Supplement Section A.",
  ". AUROC metrics for FPN-IAIA-BL as compared to IAIA-BL and the uninterpretable baseline (VGG16)": "As shown in , prototypes from each FPN levelrepresent relevant features from multiple scales. FPN-level2 localizes to the most fine-grained features, and FPN-level5 activations cover large swaths of the image. The modelsuccessfully learned prototypes at each FPN-level thatcaptured information of different scales. In our applica-tion for mass margin classification, FPN-level 3 providedprototypes that activated on the most specific and salientparts of the margin. In other applications, the FPN-levelof each prototype can be configured such that the prototypescapture the most relevant scale of information for the appli-cation. compares the activation maps provided byFPN-IAIA-BL, IAIA-BL, ProtoPNet, GradCAM and Grad-CAM++. The explanations from FPN-IAIA-BL highlightthe most important parts of the lesion margin. . FPN-IAIA-BL in comparison to other saliency methods (adapted from ). We compare explanations from FPN-IAIA-BLwith GradCAM , GradCAM++ , ProtoPNet , and IAIA-BL . GradCam and GradCAM++ are two popular saliency explanationmethods, and ProtoPNet and IAIA-BL are case-based explanation methods. The explanations from FPN-IAIA-BL highlight the mostimportant parts of the lesion margin.",
  ". Limitations": "While FPN-IAIA-BL consistently produces prototypes forcircumscribed and spiculated lesion that our radiology teamfinds compelling, the prototypes for indistinct margins oftenactivate outside of the lesion. This could be because an in-distinct margin is defined as a faded, soft boundary betweenthe lesion and normal tissue, and soft boundaries can occurin healthy breast tissue. Additionally, the AUROC for FPN-IAIA-BL is lower than that of IAIA-BL (0.951 overall) andthe uninterpretable baseline (0.947 overall). This is becauseFPN-IAIA-BL architecture is larger and harder to train thanIAIA-BL and the baseline.",
  "This study was supported by National Science Foundation(grant HRD-2222336), Duke TRIPODS CCF-1934964,Duke MEDx: High-Risk High-Impact Challenge, and theDuke Incubation Fund": "Sebastian Bach, Alexander Binder, Gregoire Mon-tavon, Frederick Klauschen, Klaus-Robert Muller, andWojciech Samek.On pixel-wise explanations fornon-linear classifier decisions by layer-wise relevancepropagation. PLOS ONE, 10(7):146, 07 2015. 2 Alina Jade Barnett, Zhicheng Guo, Jin Jing, WendongGe, Cynthia Rudin, and M Brandon Westover. Inter-pretable machine learning system to eeg patterns onthe ictal-interictal-injury continuum.arXiv preprintarXiv:2211.05207, 2022. 2 Alina Jade Barnett, Zhicheng Guo, Jin Jing, Wen-dong Ge, Cynthia Rudin, and M Brandon West-over.Mapping the ictal-interictal-injury continuumusing interpretable machine learning. arXiv preprintarXiv:2211.05207, 2022. 2 Alina Jade Barnett, Fides Regina Schwartz, Chao-fan Tao, Chaofan Chen, Yinhao Ren, Joseph Y Lo,and Cynthia Rudin. A case-based interpretable deeplearning model for classification of mass lesions indigital mammography. Nature Machine Intelligence,3(12):10611070, 2021. 2, 3, 4, 5 AdityaChattopadhay,AnirbanSarkar,PrantikHowlader, and Vineeth N Balasubramanian.Grad-cam++: Generalized gradient-based visual explana-tions for deep convolutional networks. In 2018 IEEEWinter Conference on Applications of ComputerVision (WACV), pages 839847. IEEE, 2018. 5 Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett,Cynthia Rudin, and Jonathan K Su. This Looks LikeThat: Deep Learning for Interpretable Image Recog-nition. In Advances in Neural Information ProcessingSystems 32 (NeurIPS), pages 89308941, 2019. 2, 4,5",
  "Maksims Ivanovs, Roberts Kadikis, and KasparsOzols.Perturbation-based methods for explainingdeep neural networks: A survey. Pattern Recogn. Lett.,150(C):228234, oct 2021. 2": "Nal Kalchbrenner, Edward Grefenstette, and PhilBlunsom. A convolutional neural network for mod-elling sentences. In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 655665,2014. 3 Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin.Deep Learning for Case-Based Reasoning throughPrototypes: A Neural Network that Explains Its Pre-dictions. In Proceedings of the Thirty-Second AAAIConference on Artificial Intelligence (AAAI), 2018. 2",
  "Tsung-Yi Lin, Piotr Dollar, Ross Girshick, KaimingHe, Bharath Hariharan, and Serge Belongie. Featurepyramid networks for object detection, 2017. 2": "Chiyu Ma, Brandon Zhao, Chaofan Chen, and CynthiaRudin. This looks like those: Illuminating prototypi-cal concepts using multiple visualizations. Advancesin Neural Information Processing Systems, 36, 2024.2 Meike Nauta, Ron Van Bree, and Christin Seifert.Neural prototype trees for interpretable fine-grainedimage recognition. In Proceedings of the IEEE/CVFconference on computer vision and pattern recogni-tion, pages 1493314943, 2021. 2 A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, andJ. Clune. Synthesizing the preferred inputs for neuronsin neural networks via deep generator networks. InAdvances in Neural Information Processing Systems29 (NIPS), pages 33873395, 2016. 2",
  "DawidRymarczyk,ukaszStruski,MichaGorszczak,Koryna Lewandowska,Jacek Tabor,and Bartosz Zielinski. Interpretable image classifica-tion with differentiable prototypes assignment, 2022.2, 3": "Ramprasaath R. Selvaraju, Michael Cogswell, Ab-hishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra. Grad-CAM: Visual Explanations FromDeep Networks via Gradient-Based Localization. InThe IEEE International Conference on Computer Vi-sion (ICCV), Oct 2017. 5 Karen Simonyan, Andrea Vedaldi, and Andrew Zis-serman. Deep inside convolutional networks: Visual-ising Image Classification Models and Saliency Maps.In International Conference on Learning Representa-tions (ICLR) Workshop, 2014. 2",
  "JostTobiasSpringenberg,AlexeyDosovitskiy,Thomas Brox, and Martin Riedmiller.Striving forSimplicity: The All Convolutional Net. arXiv preprintarXiv:1412.6806, 2014": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Ax-iomatic Attribution for Deep Networks. In Doina Pre-cup and Yee Whye Teh, editors, Proceedings of the34th International Conference on Machine Learning,volume 70 of Proceedings of Machine Learning Re-search, pages 33193328, International ConventionCentre, Sydney, Australia, 0611 Aug 2017. PMLR.2 Jiaqi Wang, Huafeng Liu, Xinyue Wang, and LipingJing. Interpretable image recognition by constructingtransparent embedding space. In 2021 IEEE/CVF In-ternational Conference on Computer Vision (ICCV),pages 875884, 2021. 2 Jiaqi Wang, Huafeng Liu, Xinyue Wang, and LipingJing. Interpretable image recognition by constructingtransparent embedding space. In Proceedings of theIEEE/CVF international conference on computer vi-sion, pages 895904, 2021. 2, 3 Naoya Yoshimura, Takuya Maekawa, and TakahiroHara. Toward understanding acceleration-based activ-ity recognition neural networks with activation maxi-mization. In 2021 International Joint Conference onNeural Networks (IJCNN), pages 18, 2021. 2",
  "B. Fine Annotation Coefficients": "FPN-IAIA-BLintroducesfine-annotationcoefficients(y(i),c)in, (y(i),c)outwhich are used in the fine-annotationloss to encourage and penalize the model for activatinginside and outside the fine annotations. For example, it isconsidered worse for a spiculated prototype to activate ona circumscribed lesion than for a circumscribed prototypeto activate on a spiculated lesion.The fine-annotationcoefficients designed by board-certified radiologist F.S. areas follow in tables 2 and 3.",
  "C. Negative Class Data": "As discussed in , we include a negative class dur-ing training to discourage classification by elimination.The negative class data consist of 5,000 image-mask pairs.The negative class images were created by sampling thefull-size mammogram images and cropping to a sectionwithout any of the lesion region of interest for each image.We pair each image with a fully negative mask where noregion of interest is identified in the mask. For training, werandomly select a subset of 200 negative samples."
}