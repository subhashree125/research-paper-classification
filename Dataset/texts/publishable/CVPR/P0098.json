{
  "Jaime Corsetti, Davide Boscaini, Francesco Giuliari, Changjae Oh, Andrea Cavallaro, and Fabio Poiesi": "AbstractThe generalisation to unseen objects in the 6Dpose estimation task is very challenging. While Vision-LanguageModels (VLMs) enable using natural language descriptions tosupport 6D pose estimation of unseen objects, these solutionsunderperform compared to model-based methods. In this workwe present Horyon, an open-vocabulary VLM-based architecturethat addresses relative pose estimation between two scenes of anunseen object, described by a textual prompt only. We use thetextual prompt to identify the unseen object in the scenes andthen obtain high-resolution multi-scale features. These featuresare used to extract cross-scene matches for registration. Weevaluate our model on a benchmark with a large variety ofunseen objects across four datasets, namely REAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves state-of-the-art performance on all datasets, outperforming by 12.6 inAverage Recall the previous best-performing approach.",
  "Index TermsObject 6D pose estimation, open-vocabulary": "I. INTRODUCTIONTHE estimation of the 6D pose of an object requires theprediction of its 3D rotation matrix and 3D translationvector with reference to the camera. Accurate object 6Dpose estimation is a fundamental phase in a wide range ofapplications, such as augmented reality , robot grasping ,and autonomous driving . Data-driven methods , canachieve reliable pose estimation, but require expensive objectannotations. This problem is mitigated by the developmentof techniques to generate large-scale synthetic datasets ,. Another line of research instead defines the unseen-objectsetting , which assumes no overlap between the set oftraining and test objects , . However, most methods forthe unseen-object setting are model-based, as they requirea CAD model of the object at test time , , asdetailed in the first group of Tab. I. Recent works changedthis assumption, and instead require a video sequence of theobject at test time , , from which a set of referenceviews is extracted (second group in Tab. I). These methods use (Corresponding author: Jaime Corsetti)Jaime Corsetti is with Fondazione Bruno Kessler (FBK), 38123 Trento,Italy, and also with the Department of Information Engineering and ComputerScience, University of Trento, 38122 Trento, Italy (e-mail: ).Davide Boscaini, Francesco Giuliari, and Fabio Poiesi are with FondazioneBruno Kessler (FBK), 38123 Trento, Italy (e-mail: ; ; ).Changjae Oh is with the School of Electronic Engineering and ComputerScience, Queen Mary University of London, E1 4NS London, United King-dom (e-mail: ).Andrea Cavallaro is with the Idiap Research Institute, CH-1920 Martigny,Switzerland, and also with the Ecole polytechnique federale de Lausanne(EPFL), CH-1015 Lausanne, Switzerland (e-mail: ). Structure-from-Motion (SfM) to reconstruct the object 3Dmodel from the reference views , .To eliminate the need for the object models or multipleviews, in our previous work, Oryon , we showed that theobject can be described with a textual prompt, thus definingthe open-vocabulary object 6D pose estimation setting. Thissetting assumes the availability of two RGBD scenes, alongwith a natural language description of the object of interest,provided by the user. The object 6D pose in a scene isestimated with respect to the object in the other scene.In this work, we present Horyon (High-resolution Oryon),a VLM-based architecture for open-vocabulary object 6D poseestimation that generalises to unseen objects. Horyon uses aVLM to extract visual and textual representations from theinput scene pair and the natural language description (i.e.,the prompt). The two representations are fused using cross-attention layers that enable information exchange betweeneach scene patch with each prompt token, without losingkey details contained in the object description. The resultingfeature maps are upsampled and fused with the high-resolutionfeature maps from a visual encoder. Horyon estimates theobject pose in the query scene with respect to the anchorscene by segmentation and pixel-level view matching, andsubsequently backprojecting and registering the 3D matches.This work significantly extends our previous work inseveral aspects. Instead of processing the whole scene, weidentify and localise the object of interest in both scenes withGroundingDino to obtain a bounding box. This leads toa higher resolution feature map without losing generalisationto unseen objects. Moreover, we remove the need for theguidance backbone and instead extract the guidance featuresdirectly from the VLM, reducing the training time and thenumber of parameters to 50% and 37% of the original method,respectively. To validate our choices, we test our method onan extended version of the Oryon benchmark, which includestwo popular datasets for pose estimation, i.e., Linemod and YCB-Video , in addition to REAL275 andToyota-Light . Linemod features scenes with high clutter,occlusions and objects that are small and unusual. YCB-Video presents a large variety of poses, and objects thatoften belong to similar categories (e.g., boxes and cans). Weprovide extended ablation studies to justify each choice, andalso evaluate the effect of the prompt on the final results. Insummary, our main contributions are:",
  "arXiv:2406.16384v2 [cs.CV] 11 Jul 2024": "TABLE ICOMPARISON OF THE DATA REQUIREMENTS OF HORYON WITH EXAMPLES OF STATE-OF-THE-ART METHODS FOR UNSEEN-OBJECT 6D POSEESTIMATION. WE CLASSIFY THE METHODS BASED ON: INPUT: THE TYPE OF INPUT DATA, TYPICALLY RGB OR RGBD; REFERENCE: ADDITIONALDATA USED TO IDENTIFY THE UNSEEN OBJECT AT TEST TIME; POSE: WHETHER THE METHOD IS CAPABLE OF ESTIMATING THE 6D POSE OR IS LIMITEDTO THE ROTATION COMPONENT. METHODS THAT CAN ESTIMATE THE TRANSLATION COMPONENT UP TO A SCALE ARE IDENTIFIED BY R,t.; OBJECTPREPROCESSING: EVENTUAL PROCESS REQUIRED AT TEST TIME TO ACQUIRE INFORMATION ABOUT THE OBJECT OF INTEREST; LOCALISATION:EVENTUAL EXTERNAL MODULES USED TO LOCALISE THE OBJECT, TYPICALLY A SEGMENTOR OR A DETECTOR; ZERO-SHOT: TRUE IF THELOCALISATION MODULE WAS NOT SPECIFICALLY TRAINED FOR THE TEST DATASET.",
  "Horyon (ours)Single supp. viewR, tExpression of textual promptDetector": "We propose to use a detector that localises and crops theobject of interest from the input images using a naturallanguage description. Despite its simplicity, this operationeffectively mitigates the influence of extraneous factors,such as background clutter and unrelated objects, on thefeature representations used for matching. We provide a new multi-scale formulation for the VLM-based feature extractor and decoder modules, which en-ables extraction of higher-quality features for correspon-dence matching, resulting in an increase of +12.6 inAverage Recall compared to the previous version.",
  "II. RELATED WORK": "In Tab. I we report examples of methods for unseen-object6D pose estimation, along with their assumptions. We classifythem in model-based (first group), model-free (second group),relative pose estimation methods (third group) and open-vocabulary methods (last group). In the following paragraphs,we describe the assumptions and discuss some example meth-ods for each of the last three groups, as they have the mostsimilar assumptions to Horyon. We also discuss the usage oflocalisation modules for pose estimation, to contextualise ourchoice of an open-vocabulary detector to crop the object.Model-free pose estimation. Recent research has focusedon developing methods for pose estimation of unseen objectswithout requiring their 3D models during inference, as these3D models might not always be available. Model-free methodsonly require a video sequence captured from multiple view-points around the object of interest. The video sequence isthen used to reconstruct an approximate 3D model of theobject through structure-from-motion (SfM) techniques .Exemplary methods include OnePose++ and Gen6D . OnePose++ employs a graph neural network based on at-tention to establish correspondences between the inputimage and the 3D reconstruction of the unseen object. Gen6Dleverages three distinct modules for localisation, viewpointestimation, and pose refinement. Both methods require a videosequence of the unseen object during inference, assuming thephysical availability of the object. Additionally, a preprocess-ing procedure is needed to perform the 3D reconstructionof the object, typically involving the execution of an SfM-based algorithm on the video sequence. This procedure canbe cumbersome and challenging for users without technicalexpertise. A related approach, FS6D , instead relies on asparse set of reference images annotated with their respectivecamera poses. In contrast, Horyon does not require annotatedimages or complex preprocessing procedures; the user onlyneeds to provide a natural language description of the object. Relative pose estimation approaches estimate the relativepose of an object with respect to one or more referenceviews . They are independent on the objects 3Dmodel. NOPE estimates the rotation of an object in ascene given a single reference view, but the usage of RGBinformation only does not allow it to estimate the translationcomponent of the pose. Similarly, RelPose estimates therelative rotation of an object using as few as three referenceviews, but it requires that these views come from the samescene. RelPose++ enhances its predecessor by introduc-ing a reference frame, enabling the estimation of both thetranslation and rotation components of the pose using as fewas two RGB views of the same scene. However, RelPose++only estimates the pose up to a scale factor; the translationcomponent retrieved is scaled under the assumption that thefirst camera has q unitary distance from the object. Moreover,RelPose++ requires the images to be approximately centredon the object of interest and does not handle the presence ofmultiple objects in the same scene. Similarly to RelPose++,PoseDiffusion addresses camera pose estimation up to .The main modules of our proposed method, Horyon. (a) Overview of a processing branch: Horyon first crops the object of interest from the scenegiven a textual prompt, and subsequently extracts visual and textual features with DINO and BERT, respectively from the image crop and the prompt. Thefusion module T V outputs a multimodal representation of the scene, which is upsampled by a decoder D. At this stage, skip connections from the imageencoder V are used to enrich the final representation. The output features F are used to obtain the object segmentation mask M. (b) Optimisation procedure.FA, FQ are optimised by a hardest contrastive loss which uses ground-truth matches as supervision, while the segmentation is supervised by a Dice loss.(c) Test procedure. The predicted masks are used to filter FA, FQ, and matches are obtained by nearest neighbor. Finally, the matches are backprojected in3D, and a registration algorithm is used to retrieve the final pose TAQ. a scale factor in the translation component. It allows for theestimation of camera poses using 2-8 views of the object byenforcing consistency between pairwise matches through ge-ometric constraints. However, like RelPose++, PoseDiffusionalso requires object-centric images and is therefore limited toworking with a single object per image. In contrast, Horyoncan estimate an absolute value for the translation componentof the pose and does not assume the views are captured fromthe same scene.Language models for pose estimation. So far, the useof VLMs for pose estimation tasks has been limited. Anearly approach adopted a grounding network to extendthe capabilities of a grasping robot in a category-level poseestimation scenario. Subsequent works similarly used textualprompts for instance-level or category-level poseestimation, but their generalisation capability is limited tothe training categories. Recently, Oryon has shown aneffective approach based on joint image matching and seg-mentation in order to estimate the pose of a common objectgiven two scenes. Instead of relying on an object model or aset of reference views, the object of interest is described with atextual prompt, and localised by segmentation. The referencescene and the usage of depth information allows Oryon toestimate the relative 6D pose between the two scenes, even forobjects unseen at test time. However, the limited resolution ofOryons feature map limits its ability to estimating accurateposes. In this work, we address this limitation by proposing a set of key modifications, which include a higher-resolutionfeature map for fine-grained feature matching, and an updatedVLM to better exploit the textual prompt.Object localisation for pose estimation. Adopting externalmodules to localise the object of interest within input imagesis standard practice in the pose estimation community ,, . This is often necessary to deal with highly occludeddatasets , and is common both in the classic setting , and in the unseen-object setting , . Crops areobtained by training a detector on the specific objects (e.g.,YOLOv5 in OnePose and OnePose++, FCOS inZebraPose ) or by using ground-truth bounding boxes ,, . Notable exceptions are methods that train the detec-tor contextually to the rest of the pipeline, such as Gen6D and OSOP . For non-generalisable methods, adoptingsupervised detectors to obtain a bounding box isreasonable, as they assume to have access to the object modelat test time. Therefore using them to train a detector doesnot lead to loss of generalisation. Recently, CNOS hasbeen proposed to provide accurate segmentation masks givena CAD model of the object, without training on the specificobject instances. This method can be naturally paired withmodel-based methods for unseen-object pose estimation ,, , as CNOS shares their assumptions (i.e., object 3Dmodel available at test time, but no specific training on it).Horyon retains the open-vocabulary assumptions, as we usean open-vocabulary detector to locate and crop the object.",
  "N Layers": ".(a) Overview of a layer of the fusion module T V . Note that the left-side output of the module is not used in the last layer. (b) Architecture of thedecoder D. E1,E2,E3: visual features obtained from V ; Transp.Conv.: transposed 2D convolution; Group.Conv.: group composed by two blocks, eachcontains a 2D convolution, a group normalisation and a ReLU; [|] denotes feature concatenation.",
  "Let two RGBD scenes, an anchor A and a query Q, depicttwo scenes with a common object. Each scene is representedby the RGB channels RGBA (RGBQ) and a depth map DA": "(DQ). The intrinsic camera parameters used to capture bothA and Q are available. For each pair (A, Q) a textual promptT describing the common object O is provided. The objectpresent at test time was not observed at training time. Theobjective is to estimate the 6D pose of the object in Q withrespect to the reference provided by A.(a) shows the proposed Horyon architecture. Horyonconsists of four main modules: an open-vocabolary detector,the vision and text encoders, the fusion module, and the de-coder. The open-vocabulary detector identifies the object O inboth scenes. The two pre-trained networks: the vision encoderV and the text encoder T , encode the cropped A and Qin multi-scale feature maps and generate an embedding ofthe prompt T, respectively. The fusion module T V providesa joint representation across the visual and text modalities.Finally, the decoder D upsamples the features from the fusionmodule to provide a set of features that can be used for imagematching. The final features are processed by a convolutionallayer which outputs the localisation masks of the object. In thefollowing sections, we describe each component, along withthe training and test procedures.",
  "B. Object identification and cropping": "We perform 6D pose estimation by image matching, there-fore the accuracy of the predicted matches is crucial to the finalperformance. To this end, we use feature maps that encodethe single object of interest instead of the whole scene. Weprocess each scene with the open-vocabulary object detectorGroundingDino , which allows a good accuracy and can generalise to previously unseen objects. We use the sameprompt T given as input to Horyon, and square and resize eachbounding box to avoid deformation. The predicted boundingboxes are used only at test time, and instead we use theground-truth ones for training. In this way, we can obtainaccurate detections without losing generalisation capability.",
  "C. Vision-Language backbone": "The features used for matching should be (1) conditionedon the textual prompt T, and (2) robust to unseen objects, forwhich Horyon was not specifically trained. We use DINO as vision encoder V to extract multi-scale visual feature mapsE1, E2, E3 and BERT as text encoder T , to encodethe prompt in a sequence of textual features eT . Previousworks have proven this combination to be effective in ob-taining fine-grained feature maps with good generalisation ca-pabilities. Using BERT provides an additional advantage: eT isnot a global vector, but instead is a sequence of feature vectors,one for each token. This allows to preserve the informationrelated to the description of the object, and therefore providesa better representation for the natural language description.Unlike Horyon, Oryon uses CLIP as Vision-Languagebackbone. Although CLIP showed impressive capabilities ontasks based on localisation , it was trained for alignment ofglobal embeddings, and therefore was not designed to providespatially-informed feature maps.",
  "D. Fusion module": "The fusion module T V is based on cross-attention betweenpatches of the visual feature maps E1 and tokens of theprompt eT (see (a)). This strategy allows Horyonto preserve the information contained in each specific tokenin the prompt, and therefore the model can learn to associateeach visual patch with the most suitable component of theprompt (e.g., a token representing red in the prompt can easily be associated to red patches on the object). In contrast,Oryon adopts a global representation from CLIP as textualembedding, thus collapsing in a single feature vector all theinformation contained in the prompt. We found the new designto be more effective, particularly when the prompt is noisy (seethe ablation study on the prompt type in Tab. IV.",
  "E. Decoder": "The feature map produced by the fusion module T V retainsthe low resolution of the original input features E1. In orderto obtain high-resolution features necessary for matching,we use a decoder D based on transposed convolutions toupsample the input features. Prior to each upsample layer, weconcatenate the purely visual features obtained from DINO,respectively E1, E2 and E3, to enrich the semantic repre-sentation of the Vision-Language backbone. We define suchfeature map as guidance features. In Oryon , guidancefeatures are extracted from a pretrained Swin transformer .Instead, we directly use the features from DINO, thereforesubstantially reducing the number of parameters. Moreover,the training strategy of DINO encourages generalisation tonovel concepts , and thus it is more effective than Swin inour task. The output of the decoder is a high-resolution featuremap F. To obtain the segmentation mask M, we process Fwith a convolutional layer.",
  "F. Optimisation": "We train Horyon with an optimisation procedure based onjoint image matching and segmentation. The feature maps FA,FQ are optimised with a hardest-contrastive loss F , ,supervised by ground-truth matches between A and Q. Thepositive component P forces the matches to be close in thefeature space, while the negative component N increases thedistance between a feature point and its hardest negative.Let f A, f Q RCD be the features extracted respectivelyfrom the feature maps FA, FQ, by using the ground-truthmatches P. C = |P| is the total number of matches and D isthe feature dimension. The positive component P is",
  "+ ,(1)": "where ()+ = max(0, ) and P is a positive margin, i.e., thedesired distance in the feature space of a positive pair.We consider a set of features f and their correspondingcoordinates x on the scene to define the negative pairs. Givenfi, its candidate negative set is defined as Ni = {k: xk x, k = i, xi xk }. This excludes all points closer thanthe distance from the reference point xi in the scene, therebyexcluding features describing the same points. Candidate neg-atives sets are computed for all points of xA and xQ, and thenegative component N is",
  "(2)": "For each fi, the nearest fk in the feature space (i.e. the hard-est negative) is selected. Given a negative pair, the negativemargin N is the desired distance of the two points in thefeature space. In Eqs. (1) and (2), dist() is the inverted andnormalised cosine similarity. We implement the segmentationloss M as a Dice loss . The final loss is defined as",
  "= M + NN + P P ,": "where N and P are hyperparameters.In Horyon, we change the loss hyperparameters with respectto the ones used in Oryon, in order to adapt to the new contextin which the object is cropped. Intuitively, a higher resolutionimplies more ground-truth matches, therefore we raise thenumber of matches to C = 2000, as opposed to C = 500in Oryon. For the same reason, we set the excluding distancefor the hardest negatives to = 20.",
  "G. Matching and registration": "At test time, we obtain the predicted mask MA, MQ andthe features FA, FQ from A and Q. The predicted masksare used to filter the features belonging to the objects, thusobtaining two lists of features FAM RC1D, FQM RC2D.We compute the nearest neighbor f Qi FQM in the featurespace for each feature f Ai FAM, and reject the pairs f Ai , f Qifor which dist(f Ai , f Qi ) > T .The resulting points are backprojected to the 3D space, byusing the depth maps and the intrinsic camera parameters ofA and Q, thus obtaining two point clouds PA, PQ RC3.Finally, to obtain the pose TAQ, we use PointDSC asits spatial consistency formulation allows to reject inconsistentmatches, thus leading to more accurate poses.",
  "A. Experimental setup": "During training, the weights of the image and text encodersV , T are frozen, we only update the weights of the fusionand decoder modules. We train our model using the Adamoptimiser for 20 epochs with learning rate 104, weightdecay 5104, and a cosine annealing scheduler to lowerthe learning rate to 105. We randomly augment training databy applying horizontal flipping, vertical flipping, and colourjittering. The output resolution of FA, FQ is 192 192, andtheir feature dimension is F = 32. Loss weights are set asP = N = 0.5. We set the positive and negative margins inthe loss as P = 0.2 and N = 0.9, and the excluding distancefor hardest negatives as = 20. At test time we set themaximum feature distance to identify a match as T = 0.25.We limit the number of matches to C = 2000. We implementHoryon with PyTorch Lightning . We set the batch sizeto 8 and train on four Nvidia V100 GPUs. In this standardsetting, a training requires about 12 hours.",
  "We train on ShapeNet6D , the same synthetic datasetused by Oryon . For evaluation, we introduce a novel": "benchmark that extends the one proposed in Oryon. This com-prises four real-world datasets: REAL275 and Toyota-Light , which are also part of the original benchmark,as well as two additional datasets, Linemod and YCB-Video , featuring more severe occlusions, clutter andobject variety. To comply with our relative pose estimationsetting, we form image pairs (A, Q) by randomly samplingA, Q from their image distribution while ensuring that theyare captured from different scenes. We extract 2K imagepairs from each test dataset. To accommodate our open-vocabulary setting, we provide natural language descriptionsfor each object across the four test datasets. In the followingparagraphs, we detail the training and testing datasets.ShapeNet6D (SN6D for short) is a large-scale syntheticdataset comprising a diverse collection of scenes generated byrendering ShapeNet objects in various poses against ran-dom backgrounds. SN6D adhere to our zero-shot assumption,as it does not contain any object instances present in the testdatasets. We provide natural language descriptions for eachobject of SN6D by leveraging ShapeNetSem , a subsetof ShapeNet that includes additional metadata associated withthe 3D models, to extract both the object name and a set ofsynonyms. To generate an object description, we randomlyselect either the object name or one of the provided synonyms(e.g., possible synonyms for television are tv, telly, andtelevision receiver). This augmentation strategy increasesthe object description variability and reduces overfitting. Byfollowing this procedure, we collect a set of 20K data tuples(A, Q, T) to form our training set. Note that T contains onlysemantic information and does not include additional detailssuch as the object colour, material, or physical attributes.REAL275 features 18 objects spanning six differentcategories, arranged in realistic configurations within diverseindoor scenarios (e.g., on tables, floors). Its challenges are thepresence of multiple instances of the same object categoriesand a wide variety of viewpoints captured in the scenes.Toyota-Light (TOYL for short) contains scenes whereone of 21 different objects is randomly positioned on variousfabric types. The images are captured under challenging light-ing conditions, which is particularly relevant in our setting.As we process pairs of images, significant lighting differencesbetween them pose a major challenge for image matching.Linemod (LM for short) comprises 15 different objectsarranged on a table in various configurations, along with otherobjects acting as clutter. It is challenging because the objectsare often small, poorly textured with mostly uniform colours,and less conventional compared to those in REAL275 (e.g.,plastic toy figures of an ape and a cat).YCB-Video (YCBV for short) includes 22 householdobjects, mainly boxes or cans, arranged in 12 different sceneswith distractor objects in the background. Many objects sharesimilar geometries, making photometric information crucialfor accurate identification and pose estimation. It also containsocclusions, including objects stacked atop one another.",
  "C. Evaluation metricsWe evaluate the poses using Average Recall (AR) andADD(S)-0.1d (abbreviated as ADD) . AR measures pose": "error using three metrics: VSD (Visible Surface Discrepancy),MSSD (Maximum Symmetry-aware Surface Distance), andMSPD (Maximum Symmetry-Aware Projection Distance). Foreach metric, AR computes the average recall over a set ofthresholds. For VSD and MSSD, the thresholds range from5% to 50% of the objects diameter, while for MSPD, thethresholds range from 5% to 50% of the image size. Thefinal AR score is the average of these individual recalls. ADDmeasures pose error as the average of the pairwise distancesbetween the 3D model points transformed according to theground-truth and predicted poses. It then computes the recallof pose errors that are smaller than 10% of the objectsdiameter . Both AR and ADD are designed to be robust toobject symmetries. Compared to AR, ADDs more restrictivethreshold makes it more effective in measuring highly accurateposes. As the quality of the masks influences the matches,we also evaluate the segmentation by mean Intersection-over-Union (mIoU) , across the image pairs (A, Q).",
  "D. Comparison procedure": "We consider three groups of methods for comparison. Ascrop-free methods, we report the results of ObjectMatch ,LatentFusion and a pipeline built from SIFT and PointDSC . Additionally, in this group we reportOryons results. As crop-based methods, we report Ho-ryon along with the results from ObjectMatch, LatentFusion,and SIFT+PointDSC obtained by using a detector to cropthe objects. We refer as these versions of the baselines asObjectMatch, LatentFusion and SIFT respectively. Further-more, as sparse-view methods we report results with PoseDif-fusion and RelPose++ . These are RGB-only methodsthat require an object detector, but no segmentation mask. Toensure a fair comparison, our main focus is on crop-basedmethods that use RGBD data (i.e., the crop-based group), aspresented in rows 13-21 of Tab. II.Oryon serves as our primary baseline for comparison,being the only existing method for open-vocabulary object 6Dpose estimation. In contrast to approaches that use detectorsto crop the input images around the object of interest, Oryonprocesses entire images without requiring any cropping. Weevaluate Oryons performance using the official checkpointsmade available through its repository.ObjectMatch was proposed to tackle point cloud regis-tration of scenes with low overlap. ObjectMatch is based onSuperGlue to estimate the matches, and on a custom poseestimator. To compare with it, the mask is used to filter thekeypoints obtained by the first step (i.e., keypoint estimationwith SuperGlue), and the remaining matches are forwardedto the pose estimator model. When evaluating with the crop(ObjectMatch), we first crop the object according to thepredicted box, and then follow the same procedure as above.We use ObjectMatchs model trained on ScanNet fromthe official repository. In Oryon , results with ObjectMatchwere reported by using the mask to crop the image, andsubsequently run the method on the resulting crop.SIFT is used to extract keypoints and descriptors fromeach image pair, from which matches are computed through descriptor similarity. We use the mask to filter the matches, andsubsequently backproject them to the 3D space. The final poseis obtained by registering the points with PointDSC. Whenevaluating with the crop (SIFT), we first crop the object ofinterest according to the predicted bounding box, and thenfollow the same procedure as above.LatentFusion is a method for RGBD-based pose es-timation of unseen objects. Given a set of RGBD supportviews with associated masks and poses, LatentFusion firstbuilds a latent representation of the object, by aggregating thefeatures of each view. Subsequently, given a query view ofthe same object, the optimal pose is obtained by optimising adifferentiable renderer with the latent object representation asinput. To compare with LatentFusion we use A to build therepresentation, and Q as query view. Although LatentFusionhas been designed to use 8-16 references, the ablation studiesshows that it retains a good performance even with a single ref-erence view, as in our setting. With the crop (LatentFusion),we fed the cropped images of A and Q.For sparse-view methods (rows 1-4) we report results withground-truth detections and the ones obtained from Ground-ingDino (GDino). Crop-free methods (rows 5-12) arereported with the ground-truth mask (Oracle) and the oneobtained from by Oryon . For crop-based methods (rows13-24), we report results with ground-truth mask (Oracle),the one predicted by CATSeg and the one predictedby Horyon (Ours). We use the ground-truth detection whenevaluating with the ground-truth mask, and the detectionobtained from GDino otherwise.",
  "E. Quantitative results": "We report our results in Tab. II, along with the ones obtainedfrom the baselines and the ones reported in Oryon . In rows1-4 we report the results of sparse-view methods, which onlyuse RGB data. Note that in their evaluation procedure, bothRelPose++ and PoseDiffusion use the ground-truthtranslation component of the pose to obtain a translation inmeters. We observe that PoseDiffusion reaches an overall lowperformance (8.5 average AR when using GDino as detector),while RelPose++ obtains 21.1 AR with the same detection,which is on par with Oryon (row 12) and also SIFT (row18). PoseDiffusion is based on matches between the twoviews, while RelPose++ uses an energy-based formulation torecover the relative pose. In the context of our benchmark, inwhich images are crowded and distractor objects are present,the matches learned by PoseDiffusion fail at recovering anaccurate pose, while RelPose++ can better generalise to oursetting. Nonetheless, both methods score significantly lowerthen Horyon, which surpasses RelPose++ and PoseDiffusionby 12.3 and 24.9 in average AR, respectively.Rows 5 to 12 show the results obtain from methods thatdo not use any detection crop. In this setting Oryon reaches20.8 average AR, while SIFT obtains 15.7 average AR, thusshowing that SIFT matches can lead to good pose estimationperformance when paired with a robust registration method(i.e., PointDSC ). On the other hand, LatentFusion andObjectMatch only obtain 12.1 and 5.3 AR, respectively. We attribute ObjectMatchs low performance to the domain shift:ObjectMatch was trained for registration of scenes with lowoverlap, and in our datasets the only overlapping part is theone showing the object of interest. Moreover, the presenceof distractor objects may lead to ambiguities in the matches,which results in an overall low score. LatentFusion was insteadtrained on renderings of ShapeNet, and therefore it suffersfrom the domain shift introduced by our benchmark, whichshow real images. Moreover, LatentFusion was not tested withpredicted segmentation maps, which by nature may be noisyand led to inaccurate latent representations.Rows 16 to 21 show results with crop-based baselines,which are our main comparison with Horyon. For mostmethods, working on the cropped version of the images isbeneficial, even when the resulting matches are filtered byan oracle mask: ObjectMatch improves by 3.9 points onaverage AR (row 13 vs 5), SIFT by 2.2 points (row 16 vs7), while LatentFusion loses 0.2 points (row 19 vs 10). Thisis reasonable, as the cropping can be considered a normalisingoperator on the image scale that reduces the effect of thecamera pose on the image (i.e., farther objects are smaller).Horyon beats SIFT, the next best method, by 11.4 averageAR when using our predicted mask (row 24 vs 18), while thegap falls to 8.6 points when the mask predicted by CATSegis used (rows 23 vs 17). By comparing the mIoU results onrows 23 and 24, we can observe that CATSeg on averageoutperforms Horyon, as the first reaches 73.0 mIoU against a70.7 of the latter. We can gain more insight on this result byobserving the mIoU performances separately on each dataset:while Horyons and CATSegs results on TOYL are on par,CATSeg performs slightly better on REAL275 (+3.2 mIoU)and is much more accurate on YCBV (+19.7 mIoU). On theother hand, on LM Oryon outperforms CATSeg by 13.2 mIoUpoints. We observe that REAL275 and YCBV contain manydifferent variations of quite common objects (e.g., cans, cups,laptops), while LM is comprised of many unusually objects(e.g., an office hole puncher, a toy ape). These results suggestthat Oryon is more effective than CATSeg in identifyingunusual objects, and therefore exhibit better generalisation ca-pabilities in LM. On the other hand, CATSeg is more effectivewith common objects, and possibly can easily disambiguatebetween similar objects.In rows 23-24 we can observe how Horyons pose estima-tion performance changes when a different mask is used. Thebetter average AR is obtained with our segmentation mask(33.4 vs 29.4), albeit CATSeg scores on average an highermIoU (73.0 vs 70.7). This result is even more evident whenconsidering the results of YCBV in which CATSeg producesbetter masks than our method, but the AR is still higher whenour predicted masks are used (20.6 vs 17.9 in average AR).As observed in Oryon , an accurate segmentation mask isonly important up to a point to determine the quality of poseestimation performance. In Horyon, our joint optimisationprocedure enables to learn masks which are optimised to thematching objective, thus resulting in a better performance thanthe one obtained with an external mask.Finally, in row 25 we report the increments of Horyon withrespect to Oryon, by comparing the settings with predicted TABLE IIWE COMPARE THE RESULTS OF HORYON WITH OUR BASELINES. EXPERIMENTS ARE REPORTED WITH DIFFERENT CROP AND SEGMENTATION PRIORS.RESULTS IN BOLD AND UNDERLINED REPRESENT BEST AND SECOND-BEST METHODS WHEN USING PREDICTED PRIORS, RESPECTIVELY. IN THE LASTROW WE REPORT THE INCREMENT WITH RESPECT TO ORYON . ALL METRICS ARE THE HIGHER THE BETTER.",
  "score+25.5 +27.3 +14.8 +3.0+4.2+14.0 +9.8 +10.2 +37.5 +12.0 +10.7 +12.8 +12.6 +13.0 +19.7": "masks and detection boxes (i.e., row 24 vs row 12). Whileall the changes are positive, the behaviour is different if eachdataset is considered separately. REAL275 shows the largestimprovements in pose estimation, with an AR incrementof 25.5. The increment in ADD(S) is even higher (+27.3),showing that Horyon can produce a much higher ratio of veryaccurate poses. On the other hand, TOYL exhibits the smallerimprovement, as AR and ADD(S) only improve by 3.0 and4.2 respectively. This dataset is less affected by the usage ofa detector, as it presents a single object for each scene. It isalso the dataset with the largest change in light and point ofview across scenes: this suggests that architectural changescan be made to address this specific setting and provide alarger improvement. LM and YCBV both exhibit consistentimprovements in AR (9.8 and 12.0 respectively), albeit theirperformance is still low compared the REAL275 and TOYL.We attribute this fact to the clear higher difficult of thesedatasets compared to the original ones used in Oryon, whichis due to high clutter, object occlusion and object similarity.",
  "F. Qualitative results": "In we report some qualitative result on the posepredicted by Horyon, compared to the ones predicted byOryon , SIFT and the ground-truth pose. Oryonsresults are reported using its predicted mask, while results forSIFT and Horyon are reported using Horyons predicted maskand detections from GroundingDino. (a) show results on REAL275 . In this exampleHoryon obtains fairly accurate poses, albeit the black camerapresents a small rotation error, which causes it to be notcompletely aligned to the ground truth. SIFT and Oryonobtain correct localisations, but they present clearly visiblerotation and translation errors.On Figs. 3(b), we can observe that Horyon is quite accu-rate on TOYL. SIFT similarly obtains good performances,with only small errors in translation. Instead, Oryon failscompletely at localising the object, which results in a largetranslation error. We observe that the high variation in light-ning conditions makes particularly difficult to obtain correctmatches, especially when the images are represented with alow-resolution feature map as in Oryon.Figs. 3(c) show results on LM. Horyon retrieves a posewith a small rotation error, which is larger in the predictionof SIFT. In this case Oryon fails, likely due to a wronglocalisation of the blue iron. This example is quite difficultdue to the high variation in viewpoint between the two scenes.Figs. 3(d) present results on YCBV. we can observe thatOryon and SIFT both fail in estimating the pose of thewater jug: the first presents a large translation error, whilethe second shows a wrong rotation. In this case, Horyonpredicts a pose which is mostly aligned, but one of the rotationcomponents is wrong due to the partial symmetry of the object.This suggest that, while our method benefits from the high-resolution feature map, it still has difficulties in performing",
  "(c) Prompt: Light blue iron": "(d) Prompt: Blue plastic water jug.Sample pose results from REAL275 (a), Toyota-Light (b), Linemod (c) and YCB-Video (d). All the results use crop fromGroundingDino and segmentation mask predicted by Horyon. We show the object model coloured by mapping its 3D coordinates to the RGB space.Query images are darkened to highlight the object poses. TABLE IIIWE SHOW THE RESULTS OBTAINED BY ABLATION THE ARCHITECTURAL COMPONENTS OF ORYON, WITH THE BASELINE AT ROW 8 TO BETTER COMPAREWITH THE OTHER RESULTS. ALL THE RESULTS ARE COMPUTED WITH GROUNDINGDINO AS DETECTOR PRIOR AND OUR PREDICTED MASK ASSEGMENTATION PRIOR. ALL METRICS ARE THE HIGHER THE BETTER.",
  "G. Ablation study": "We report in Tab. III an ablation study on the componentsof Horyon, with the baseline at row 8.What is the effect of cropping? The effect of the cropis strictly related to the change in loss hyperparameters, asusing the crop raises the number of matches due to thehigher resolution. Therefore, in rows 1 to 3 we examine howthe addition of the crop and the change in hyperparameters influence each other. In row 1 we do not use any crop andkeep the original loss hyperparameters, resulting in averageAR similar to Oryon (20.9 vs 20.8) and much worse than ourbaseline of 33.4 AR. Only using the crop (row 2) improvesthe performance, but still results in a significant gap withrespect to our baseline (25.3 vs 33.4 AR). In row 3 we observethat only updating the loss hyperparameters leads to a worseperformance than Oryon (17.8 vs 20.8 in AR), thus motivatingour choice. The most significant change in the loss is in thedimension of the excluding kernel of the negative loss, whichrestricts the pool of negative candidates. Without using the TABLE IVWE REPORT THE RESULTS OBTAINED BY CHANGING THE PROMPTS AT TEST TIME WITH AN ALTERNATIVE VERSION, WHICH CAN HAVE AN INCORRECTDESCRIPTION (MISLEADING), SHOW ONLY THE OBJECT NAME (GENERIC), OR SHOW ONLY THE OBJECT DESCRIPTION WITHOUT THE NAME (NO NAME).CROP PRIORS CAN BE ORACLE OR PREDICTED BY GROUNDINGDINO SEGMENTATION PRIORS CAN BE ORACLE OR PREDICTED BY HORYON. ALLMETRICS ARE THE HIGHER THE BETTER.",
  "GDino Ours57.951.681.3 33.025.182.1 22.020.467.6 20.612.352.0 33.427.370.7": "crop, a larger kernel is detrimental as it removes a significantportion of the candidate negatives from the object region.How important is the fusion design? To study the fusionimpact, we remove (row 4) and replace it with the one fromOryon (row 5). Both experiments result in a worse AR (30.6and 29.4 vs 33.4 AR), and also the segmentation quality islower (64.1 and 67.9 vs 70.7 mIoU). Therefore, a fusion mod-ule based on cross-attention on visual and text modalities ismore effective than aggregating the cross-modalities similaritymap as in Oryon , as shown in row 5.How important are the decoder design and the guidancefeatures? In row 6, we use a different design for the decodermodule, which uses two guidance feature maps from DINOinstead of three, as the lowest-resolution feature map is dis-carded. This results in a drop of 2 AR points with respect toour baseline, which is mostly due to the REAL275 dataset(50.5 vs 57.9 AR). Intuitively, the low-resolution featuremap captures useful information for larger objects, for whichthe high-resolution could encode noise related to local pixelvariations. This explains such behaviour in REAL275, whichconsists on larger objects on average. In row 7, we remove theskip connections between DINO and the decoder. While thischange does not significantly impact the mask quality (70.6 vs70.7 mIoU), there is an important drop in pose quality, as theAR drops from 33.4 to 30.6. Similarly, switching to a SwinTransformer as guidance backbone (row 9) lowers the AR to31.3. These results underline the importance of high-resolutionfeatures representative of appearance, to counterbalance thesemantic content of the embeddings from the VLM.What is the influence of the VLM choice? In rows 10and 11 we replace our VLM (GDino) with CLIP andALIGN , respectively. Both backbones similarly under-perform our default choice, as they score 23.1 and 23.4 AR(CLIP and ALIGN respectively) against 31.3 AR of row9. Note that CLIP and ALIGN encode the prompt in asingle global embedding, while we use BERT, which outputsa sequence of token-wise features. This allows Horyon toretain the information related to the object description, whichinstead may be lost in a global representation, and thereforeis beneficial to the type of prompts we use.What is the most important part of the prompt? In Tab. IV we answer this question by changing the type of prompts usedat test time. The experiments in this table only affect the eval-uation procedure, as all the training prompts and parametersremains the same. We evaluate three alternative prompt types:No name, in which the object name is replaced by objectin the prompt (e.g., brown open object instead of brownopen laptop); Misleading, in which the object descriptionis changed to be different from the object appearance (e.g.,white closed laptop for a laptop that appears brown andopen); Generic, that only includes the object name, withoutany description (e.g., laptop). In the same table we reportour baseline results, with the standard prompts. For a fairevaluation, when evaluating with GDino as detector, we usethe same alternative prompt type we fed to Horyon. In row 2, we report the results with the No name prompt.This experiment results in a very significant drop in both AR(-14.7) and mIoU (-18.1) compared with the baseline at row8. While the drop is present and significant in all datasets,it is less catastrophic on TOYL. This dataset is the only onewith a single object for each scene, and therefore changingthe prompt introduces less ambiguities. This setting reflectsthe case in which the user providing the prompt is faced withan unknown object they cannot name, thus resulting in a partialdescription about the object characteristics. Row 4 shows thatproviding a wrong description greatly impacts the averageperformance, resulting in a drop of -10.9 AR and -14.9 mIoU.Similarly to the previous prompt, this change has less impacton the TOYL dataset, while LM is more significantly affected,losing 14.7 and 47.4 points in mIoU respectively. It is clearthat in this case the main source of error is due to wrong local-isation (either from GDino or from the segmentation mask).In row 6 the object description is dropped, resulting in thebest average results among the ones with alternative prompts,with an AR of 26.1 and an mIoU of 62.0. While the averagedrop is still significant compared to the baseline, TOYL isagain a notable exception. On this dataset, using a genericdescription is beneficial, as the experiment outperforms ourbaseline by 3.1 and 3.2 points in AR and mIoU, respectively.In a context where no ambiguity is possible (i.e., a singleobject is present), adding a description is detrimental to thepose estimation performance. Finally, rows 1, 3 and 5 report the results with the ground-truth localisation and segmentation.We observe that on average the performances are very closeto the baseline with the same mask and detector on row 7.Unsurprisingly, with a perfect localisation Horyons featuresare enough to obtain a good performance in pose estimation,even with a suboptimal prompt.In conclusion, in our architecture the object name is themost important part of the prompt, as only retaining it stillprovides a good performance, while completely removingit leads to failure, particularly in case of complex sceneswith multiple objects. This experiment highlights an importantlimitation to the usage of VLMs for pose estimation: suchmodels are still unable to identify and reason about objectsgiven only a description of their visual characteristics.",
  "V. CONCLUSIONS": "We presented Horyon, an approach that significantly im-proves upon previous open-vocabulary 6D pose estimationmethods, by increasing the feature map resolution and pro-viding more accurate matches to perform registration. Ourexperiments show that Horyon obtains good performancesalso in scenarios with unusual objects (Linemod) and mildocclusions (YCB-Video). The ablation studies show that thetoken-wise representation provided by the updated VLM,together with the new fusion strategy, greatly benefit Horyon.The improvement is consistent in terms of generalisationcapabilities and robustness to prompt noise as well.Horyons limitations are the need for depth maps andintrinsic camera parameters to perform registration. Such datarequirements could be relaxed by exploring monocular depthestimation methods such as DepthAnything on eachRGB image. While Horyon is more robust to suboptimalprompts than Oryon, the resulting drop in performance is stillsignificant. Moreover, the variety of prompt usable at test timeis limited by the training data, which provides prompts withoutdescriptions. To enrich the prompts at training time, LLMs orimage captioners could be used on image samples to provideprompts that also include a description of the objects colourand physical attributes.Acknowledgements. This work was supported by the Euro-pean Unions Horizon Europe research and innovation pro-gramme under grant agreement No 101058589 (AI-PRISM),and it made use of time on the Tier 2 HPC facility JADE2,funded by EPSRC (EP/T022205/1).",
  "H. Yisheng, W. Yao, F. Haoqiang, C. Qifeng, and S. Jian, FS6D: Few-shot 6D pose estimation of novel objects, in CVPR, 2022": "Y. Labbe, L. Manuelli, A. Mousavian, S. Tyree, S. Birchfield, J. Trem-blay, J. Carpentier, M. Aubry, D. Fox, and J. Sivic, Megapose: 6D poseestimation of novel objects via render & compare, in CoRL, 2022. T. Hodan, M. Sundermeyer, Y. Labbe, V. N. Nguyen, G. Wang, E. Brach-mann, B. Drost, V. Lepetit, C. Rother, and J. Matas, BOP challenge2023 on detection, segmentation and pose estimation of seen and unseenrigid objects, in CVPR, 2024.",
  "L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depthanything: Unleashing the power of large-scale unlabeled data, in CVPR,2024": "Jaime Corsetti is a PhD student at University ofTrento, affiliated with the Technologies of VisionLab in Fondazione Bruno Kessler in Trento, Italy. Hereceived his MsC Degree in Artificial IntelligenceSystems at the University of Trento in October2023. His research interests are in object 6D poseestimation and 3D scene understanding. Davide Boscaini is a Research Scientist at theTechnologies of Vision Lab of the Fondazione BrunoKessler in Trento, Italy. He received his PhD inComputational Science from the Universit`a dellaSvizzera italiana in Lugano, Switzerland, under thesupervision of Michael Bronstein. He has been apioneer in the field of geometric deep learning.His current research interests include 3D perceptionand understanding, with a focus on object 6D poseestimation and 3D scene understanding. Francesco Giuliari is a Researcher at the Technolo-gies of Vision Centre in Fondazione Bruno Kesslerin Trento, Italy. He received his PhD degree inDeep Learning from the University of Genoa in2024. His research interests include visual sceneunderstanding using scene graphs, and vision-basedrobot navigation using classical and deep learningplanners. Changjae Oh is a Lecturer at the School of Elec-tronic Engineering and Computer Science and theCentre for Intelligent Sensing from Queen MaryUniversity of London, UK. He received his PhD inElectrical and Electronic Engineering from YonseiUniversity, Seoul, South Korea, in 2018. From 2018to 2019, he was a Postdoctoral Research Assistantat Queen Mary University of London, UK. Hiscurrent research interests include embodied agentsand vision-based robot manipulation. Andrea Cavallaro is the director of the Idiap Re-search Institute and a Full Professor at Ecole Poly-technique Federale de Lausanne (EPFL), Switzer-land. He has been a Full Professor at Queen MaryUniversity of London since 2010. He is a Fellowof the International Association for Pattern Recogni-tion. His research interests include machine learningfor multimodal perception, computer vision, audioprocessing and information privacy. Fabio Poiesi is the Head of the Technologies of Vi-sion (TeV) Lab, Fondazione Bruno Kessler, Trento,Italy. He received the PhD degree from the QueenMary University of London, U.K. and was a post-doctoral researcher with the Queen Mary Universityof London before moving to TeV. His researchinterests include 3D scene understanding, objectdetection and tracking, and extended reality."
}