{
  "Abstract": "In recent years, Neural Radiance Field (NeRF) hasdemonstrated remarkable capabilities in representing 3Dscenes.To expedite the rendering process, learnableexplicitrepresentationshavebeenintroducedforcombination with implicit NeRF representation, whichhowever results in a large storage space requirement.In this paper, we introduce the Context-based NeRFCompression (CNC) framework, which leverages highlyefficient context models to provide a storage-friendly NeRFrepresentation.Specifically, we excavate both level-wiseanddimension-wisecontextdependenciestoenableprobability prediction for information entropy reduction.Additionally, we exploit hash collision and occupancy gridsas strong prior knowledge for better context modeling. Tothe best of our knowledge, we are the first to construct andexploit context models for NeRF compression. We achievea size reduction of 100 and 70 with improved fidelityagainst the baseline Instant-NGP on Synthesic-NeRF andTanks and Temples datasets, respectively. Additionally, weattain 86.7% and 82.3% storage size reduction againstthe SOTA NeRF compression method BiRF. Our code isavailable here:",
  ". Introduction": "High-quality photo-realistic rendering at novel viewpointsremains a pivotal challenge in both computer vision andcomputer graphics. Traditional explicit 3D representations,such as voxel grids , have earned theirplace due to their efficiency across numerous applications.However, their discrete nature makes them susceptible tothe limitations imposed by the Nyquist sampling theorem,often necessitating exponentially increased memory forcapturing detailed nuances.In the past few years, Neural Radiance Field (NeRF) has emerged as a game-changer for novel view synthesis.NeRF defines both density and radiance at a 3D point as",
  "Context Models": ".Motivation of our work.Instant-NGP represents3D scenes using 3D hash feature embeddings along with arendering MLP, which takes a non-negligible storage size with theembeddings accounting for over 99% of storage size (upper-left).To tackle this, we introduce context models to substantiallycompress feature embeddings, with the three key technicalcomponents (bottom-left). Our approach achieves a size reductionof over 100 while simultaneously improving fidelity.1 functions of the 3D coordinates. Its implicit representation,encapsulated within a Multi-Layer Perceptron (MLP),captures continuous signals of a 3D scene seamlessly.Leveraging frequency-based positional embeddings of 3Dcoordinates , NeRF has showcased superiornovel view synthesis quality in comparison to traditionalexplicit 3D representations.While NeRF exhibits goodcharacteristics in memory efficiency and image quality, itscomplex queries of the MLP slow down its rendering speed.To boost NeRFs rendering speed, recent approacheshave converged towards a hybrid representation, mergingexplicit voxelized feature encoding with implicit neuralnetworks.This combination promises faster renderingwithout compromising on quality. These methods includevaried data structures such as dense grids ,octrees ,sparse voxel grids ,and hashtables . Among them, Instant-NGP (INGP) which",
  "arXiv:2406.04101v1 [cs.CV] 6 Jun 2024": "introduces multi-resolution learnable hash embeddings isthe most representative one.These hybrid strategies arefast becoming staples in modern NeRF architectures .Yet, with gains in rendering quality and speed, storageis becoming the new constraint.For example, with theoccurrence of large-scale NeRFs , the totalstorage of their parameters restricts their accessibility anddeployment.The storage challenge becomes even morepressing when further considering numerous 3D scenes.This leads us to ponder: Can we reduce the storagecost of modern NeRFs with hybrid representations such asInstant-NGP while maintaining high fidelity and renderingspeed?A few NeRF compression methods have beenproposed to address this. The common idea is to followthe Deep Compression concept, which relies onpruning and quantization techniques to squeeze the explicitfeature encoding segment.For example, VQRF pioneers the trimming of redundant voxel grids and employsvector quantization for parameter reduction.BiRF goes a step further, using 1-bit binarization for featureembeddings compression.While these methods notablyreduce storage needs, we advocate that the efficiency ofvoxel feature encoding can be further improved from adata compression perspective.Our core principle is todecrease the information uncertainty (entropy) of voxelfeature encoding, which has been widely investigated inimage and video compression but rarely explored in NeRFcompression. By leveraging efficient entropy codecs likeArithmetic Coding (AE) , we aim to achieve a balancebetween minimizing storage cost and maintaining renderingquality and speed.In this paper, we propose a Context-based NeRFCompression (CNC) framework, a pioneering approachto create a storage-optimized NeRF model.Based onInstant-NGP and its multi-resolution hash encoding,our model offers both rendering quality and efficiency.Our core proposition lies in the entropy minimizationofexplicitfeatureencodingusingaccuratecontextmodels. Specifically, we introduce a meticulously designedentropy estimation function for each resolution in featureembeddings, on the assumption of Bernoulli distribution.This is coupled with both level-wise and dimension-wisecontext models that combine different aspects of thehashing embeddings, see .We also leverage thehash collision and the occupancy grid from Instant-NGP tofurther ensure our context models accuracy. In summary,the major contributions of this work are threefold: 1. To our knowledge, we are the first to propose to modelthe contexts of INGPs multi-resolution hashing featureembeddings to effectively reduce storage size whilemaintaining fidelity and speed simultaneously.",
  ". Related work": "Neural radiance field:from implicit to explicit.Inrecent years, Neural Radiance Field (NeRF) hassignificantly advanced the area of novel view synthesis byeffectively reconstructing 3D radiance fields in a neuralimplicit way. Specifically, NeRF utilizes a coordinate-basedimplicit Multi-Layer Perceptron (MLP) to enable synthesisfrom arbitrary views.Nevertheless, due to the absenceof scene-specific information in the input coordinates, theMLP is designed to be relatively complex to encompass allnecessary information. Such complexity slows down theentire rendering process, resulting in days for training.To expedite rendering, diverse data structures havebeen introduced as input to explicitly carry scene-specificinformation, to reduce or even eliminate the MLP toachieve much faster rendering. For example, Instant-NGP(INGP) , TensoRF and K-Planes employlearnable embeddings or voxels to represent 3D scenes,which significantly reduce the computational burden ofthe rendering MLP. Plenoxels and DVGO takethis a step further by eliminating the entire implicit MLPand opting for a purely explicit representation of thewhole 3D scene. However, one major downside of theseexplicit representations is the substantial parameter size,sometimes reaching hundreds of MBs , which resultsin undesirably large storage costs, especially taking intoaccount a vast number of 3D scenes. To address this issue,compression techniques are emerging for more compactNeRF representations. In this paper, we explore contextmodels for the representative INGP-based structure andpush NeRF compression to a new level.Compression techniques: which is the most suitable?Before delving into NeRF compression, we would like tostart with a glance at existing compression techniques. Firstand foremost, model compression stands as a significantcategory. Given that different model weights exert varyingimpacts on the final results, various approaches compressthem based on weight significance via pruning ,quantization,andlow-rankapproximation[14,",
  "].Knowledge distillation is another avenue inwhich student models are guided by teachers to createmuch more compact versions.With the evaluated": "importance of parameters in NeRF models, some NeRFcompression algorithms select the most representativeones to retain information using codebooks orgradients .Among the existing NeRF compressionalgorithms, BiRF achieves SOTA Rate-Distortion(RD) performance by utilizing quantization techniques tobinarize hash embeddings of INGP-based NeRF.Apart from leveraging weight importance, contextualdependenciesamongneighboringelementsareanother essential source for compression,which hasbeen widely exploited as spatial relations in imagecompression , and as both spatial and temporalrelations in video compression .Somerecent NeRF compression methods also exploit spatialrelations by utilizing techniques such as rank-residualdecomposition ,wavelet decomposition ,orprobability models to achieve better compression.However, all these approaches often overlook the uniquestructures of NeRFs, failing to fully extract contextualinformation.In contrast, our work discovers that themulti-level embeddings in INGP-based NeRFs exhibithighlyorganizedstructures,andintroducesefficientcontext models to effectively model contextual relations atdifferent levels and dimensions, which leads to remarkableimprovement in rate-distortion (RD) performance.",
  ". Method": "Our objective is to develop a storage-friendly NeRF withefficient rendering speed and high fidelity. Our approachbuilds upon Instant-NGP (INGP) .As shown inthe right of , the primary storage of INGP comesfrom explicit hash feature embeddings. To minimize theoverall model size, we introduce a novel framework namedContext-based NeRF Compression (CNC), comprisingvarious modules as depicted in . The technical detailsare elaborated in the following subsections.",
  ". Preliminaries": "Neural Radiance Field renders a 3D scene through animplicit rendering MLP. This MLP, when provided with theinput coordinate x R3 and viewing direction d R2,can generate density (x) and color c(x, d) for rendering.Given a ray r(v) = o + vd casting from the camerao R3, the rendered pixel color C can be calculated byaccumulating the density and color along the ray , i.e.:",
  "and T(v)=exp vvn (r(u))dumeasures thetransmittance along the ray. To enhance the representationof high-frequency details, NeRF proposes to map the input": "coordinates with a frequency-based position encoding .However, the extensive querying of the heavy MLP slowsdown the training and inference processes.Instant-NGP . To expedite the rendering process ofNeRF, INGP introduces the concept of multi-levelfeature embeddings as a novel approach to positionalencoding, where deeper levels correspond to voxels withhigher resolutions. This allows for the utilization of a morecompact rendering MLP without compromising the quality.For a given 3D coordinate x, it is situated within a voxelat each level. For each resolution level l {1, . . . , L},the feature at x can be calculated by interpolating fromthe features on the vertex features in the surrounding voxelgrid, i.e. f l(x) = interp(x, ), where = {li=(l,1i , . . . , l,Fi) RF |i = 1, . . . , T l} is the trainablefeature embedding collection, F is the dimension of eachfeature vector li, and T l is the size of the feature embeddingset . For each level, when the resolution of the voxel gridexceeds a specified threshold, the vertex features will beacquired through a spatial hashing function to query for efficiency. The interpolated features from differentlevels are then concatenated together and fed into thesize-reduced rendering MLP for reconstruction. Anothertechnique that INGP employs to accelerate rendering isthe occupancy grid, which skips the empty space byefficient ray sampling. More details can be found in .Consequently, the total storage of INGP includes the featureembeddings, the occupancy grid and the rendering MLP, asshown in .BiRF . While the use of implicit feature embeddingssignificantly enhances rendering speed, it concurrentlyimposes a storage burden.The state-of-the-art methodBiRF introduces an innovative approach by binarizing in feature embeddings to {1, +1} using a sign functionand backpropagating them through a straight-throughestimator .This quantization solution reduces themodel size by a large margin.Additionally, BiRFshows that introducing extra tri-plane features can enhancereconstruction quality with a similar number of parameters.In this work, we follow their model design with hybrid2D-3D feature embeddings for the radiance field and buildour context models on top of that.",
  ". Compress Embeddings with Context Model": "Without loss of generality, we omit the notation ofresolution level l from li and assume the feature dimensionF is 1 for simplicity, for which i = i. The fundamentalprinciple of our framework is to decrease the informationuncertainty of i. Inspired from the binarization conceptof BiRF , we model each value i to conform to aBernoulli distribution, i.e. i {1, +1}. This results in adifferentiable bit consumption estimator, based on entropy,",
  "!!": ". Overview of the proposed level-wise and dimension-wise context models. In the level-wise context model (dashed blue box), wefirst find the vertex ni of the feature vector i using hash function and then estimate its distribution probability pi by a Context Fuser Cpwith aggregated contexts from previously decoded levels. Its worth noting that while the illustration here is 2D, the same approach appliesto 3D using trilinear interpolation. In the dimension-wise context models (dashed orange box), the last level of 3D voxel is projected onto2D planes to obtain Projected Voxel Feature (PVF), which is then used for context interpolation. Deep-blue areas on the voxels indicateempty cells of the occupancy grid. At bottom-right (dashed black box), the formula of the entropy-based Bit Estimator Ep is provided,which is carefully designed to ensure a more efficient backward gradient.",
  "(2)A straightforward method to estimate pi is to use theoccurrence frequency fG =#{i|i=+1,i}": "#{i|i}, where #denotes the number counting, such that pi = fG for i =1, . . . , T.However, we find this manner is suboptimalas fG is not accurate for all the embeddings.Our keyinsight is that the spatial context in 3D space can enhancethe precision of pi estimation.For instance, if a pointis empty in 3D space, we should spend fewer bits tostore the corresponding features in . This motivates usto introduce context models in the spatial domain whenestimating pi. Particularly, we propose two types of contextmodels: level-wise and dimension-wise.",
  ". Context models themselves also consume storagespace.This limitation prevents us from adoptingarbitrarily large context models, even though havingmore parameters could enhance their prediction": "3. The order of contextual dependencies is of greatimportance. If more informative parts are decoded first,they can provide more context to others but at the cost ofconsuming more bits to store themselves.In light of these considerations, we have designedour level-wise context models in a coarse-to-fine manner,as illustrated in the dashed blue box of (upper).Consider an example of vertex ni with associated featurei at the current level l = 4, as shown in . Followingthe coarse-to-fine principle, the context of ni depends on theinterpolated features at the corresponding location from theprevious Lc levels, where Lc is a preset constant (e.g., Lc =3 in ). We also incorporate the occurrence frequencyfG of the current level as an auxiliary guidance for contextmodeling. All the context information is then concatenatedand fed into a tiny 2-layer MLP named Context Fuser Cpto estimate the probability pi at vertex ni.It is worth noting that if the number of previous layers isless than Lc, we set Lc = l 1 (i.e., using all the availableprevious layers for context). For level l = 1, we only utilizeits occurrence frequency f 1G to estimate the bit consumption",
  "!\"#!$": ".Illustration of Hash Fusion.In this toy example,the resolutions of the voxel and occupancy grids are 12 and7, respectively.The weight of each hash collided vertex k ofi is normalized from its AOE, AOEki , which measures theintersection area between the vertex grid (semitransparent dashedred square around the vertex) and occupied cells (light-colored).",
  ". Hash Fusion with Occupancy Grid": "One important design to alleviate extensive storage at thefiner resolution of the trainable embeddings is adoptingspatial hashing indexing . However, this introducesan issue of hash collision when building the context models.Here, we provide a solution to address it for regressionof more accurate predictions, with the assistance of theoccupancy grid. The occupancy grid plays a pivotal role inour approach, which partitions the entire 3D scene into gridcells and records occupancy conditions in binary format.Generally, only cells on the surfaces of the objects areoccupied, while the rest are empty, resulting in the sparsity.This inherent sparsity makes the occupancy grid a spatialprior that greatly enhances our context modeling. illustrates the details of the proposed hash fusionsolution to address the hash collision issue. Particularly,suppose a feature vector i corresponds to K vertices,denoted as {nki |k = 1, . . . , K} (e.g., K = 4 in ).This implies for each i, multiple probabilities {pki |k =1, . . . , K} will be estimated.We define the Area ofEffect (AOE) of a vertex as the intersection between thesurrounded voxel grid and the occupied cell to weigh theprobability prediction, which effectively determines thesignificance of a vertex. For example, vertex n3i in",
  ". Dimension-Wise Context Models": "ConsideringBiRFintroduceshybrid2D-3Dfeatureembeddings to improve the reconstruction quality, besidesmodeling contextual dependencies at various levels, we alsoemphasize the importance of cross-dimensional relations.The main idea of dimension-wise context models is toleverage the inherent relationship between tri-plane featuresand voxel features.Through extensive experiments, wefound that 2D tri-plane feature embeddings cannot providesufficient contextual information to predict the probabilityof 3D voxelized feature embeddings, likely due to missingone dimension. Thus, we turn to a more natural approach,i.e., estimating the probability of 2D plane embeddingsfrom the 3D context. Specifically, we employ a dimensionprojection design, as illustrated in the dashed orange boxof (bottom-left). We first reconstruct the entire 3Dvoxel using the spatial hashing function. Then, we projectthis 3D voxel along three different axes and record thefrequency of +1s along each axis direction to obtain 2DProjected Voxel Features (PVF). Here, we leverage the priorknowledge of valid 3D space by the occupancy grid duringprojection.If the AOE of a vertex is 0, then it will beomitted from the calculation during the projection.ThePVF will serve as one additional previous level contextto estimate the probability pi for each 2D i.Notably,PVFs can be obtained from three distinct 2D planes, i.e.,the xy, xz, and yz planes. In our work, we only utilize 3Dfeature embeddings that correspond to the largest resolutionto generate PVFs, as they contain the most informative data.Training loss.With the establishment of our contextmodels, we can calculate the entropy loss Lentropy, whichis defined as the sum of the bits associated with all validfeature vectors s. The overall loss function then becomes",
  "L = Lmse + Lentropy/M(4)": "where Lmse is the image reconstruction Mean SquaredError (MSE) loss and is a tradeoff factor to balance thetwo terms for variable bitrates. M is the number of s inthe embeddings, including both valid and invalid ones.Decoding and rendering process. In the testing process,3D embeddings are firstly decoded from shallow todeep levels using level-wise context models.Thenthe last 3D level is utilized to generate dimension-wisecontext for 2D embeddings.Finally, 2D embeddingsare decoded in a coarse-to-fine order with the assistanceof the dimension-wise context.It takes about 1 secondfor encoding/decoding.Its worth noting that once theembeddings are decoded, all the rendering processes arethe same as INGP, requiring no additional time. . Performance overviews and detailed local zoom-in results of our proposed CNC and other methods. We apply log10 x-axis onthe overviews for better visualization while linear x-axis on the zoom-in charts for better comparison. The more a curve goes upper-left,the better the rate-distortion (RD) performance is. Note that we achieve variable bitrates in our approach by changing from 0.7e 3 to8e 3, while BiRF achieves that by changing feature dimensions F from 1 to 8. The dashed line ours-upperbound represents theupper fidelity bound of our binary NeRF model (i.e., = 0).",
  ". Implementation Details": "Our model is implemented based on NerfAcc underPyTorch framework and is trained using a singleNVIDIA RTX 3090 GPU. We use Adam optimizer with an initial learning rate of 0.01 and train for 20000iterations. For 3D embeddings, it contains 12 levels withresolutions from 16 to 512.For 2D embeddings, theresolutions range from 128 to 1024 with 4 levels.Thenumbers of maximum feature vectors are set to 219 and 217 per level for 3D and 2D, respectively. The resolution of theoccupancy grid is 128. We set the feature vector dimensionF as 8, and the number of context levels Lc as 3. Thestructure of the rendering MLP is the same as but witha width of 160. During training, we vary in Eq. 3 from0.7e 3 to 8e 3 to obtain different bitrates. More detailscan be found in Sec. A of the supplementary.",
  ". Performance Evaluation": "Baselines.We mainly compared our method with therecent NeRF compression approaches.Among them,BiRF and MaskDWT minimize NeRF model sizeduring training, while VQRF and Re:NeRF arepost-training compression algorithms. We also comparedseveral major variants of NeRF to see their storagecost, including DVGO , Plenoxels , TensoRF ,CCNeRF , INGP and K-Planes .Datasets. Experiments are conducted on a synthetic dataset Synthetic-NeRF and a real-world large-scale datasetTanks and Temples . We follow the setting as BiRF .Metrics.Besides the conventional PSNR versus sizeresults,we also employ BD-rate to assess theRate-Distortion (RD) performance of these approaches,which measures the relative size change under the samefidelity quality. A reduced BD-rate signifies decreased bitconsumption for the same quality.Results. We report the quantitative and qualitative resultsin and , respectively. For more fidelity metrics(SSIM and LPIPS ) and visual comparisons, pleaserefer to Tab. B-C and Fig. A-B of the the supplementary.Our proposed CNC achieves a significant RD performanceadvantage over others. Compared to the SOTA (i.e., BiRF),our proposed CNC achieves 86.7% and 82.3% BD-ratereduction on the two datasets. For Synthetic-NeRF dataset,our CNC closely approaches the upper fidelity boundwhile maintaining a much smaller size, showcasing theeffectiveness of CNC. For the Tanks and Temples dataset,our CNC even surpasses the upper-bound. We conjecturethat, to some extent, the entropy constraint from the contextmodels serves as regularization to prevent overfitting.Bitstreams.Our bitstream comprises four components:3D and 2D feature embeddings, the rendering MLP,context models and the occupancy grid.Their averagesizes are 0.220MB, 0.148MB, 0.011MB and 0.039MBin Synthetic-NeRF dataset with =4e 3.Theyare encoded/stored as follows.Feature embeddings areentropy encoded by Arithmetic Coding (AE) withprobabilities predicted by context models. The renderingMLP parameters are quantized from the original 32 bitsto 13 bits, which only causes a slight performance dropof less than 0.02 dB in PSNR while saving up to 0.216MB. Context models are preserved in float32 to maintain",
  ". Ablation Study": "We contemplate what the optimal design is for contextmodels.To address this question, we first deactivatecertain context models to observe the extent of performancedrop. Then, we delve into the detailed effect of inter-leveldependencies in level-wise context models. Regarding thehash fusion module, we explore the crucial function ofAOE, which can address the hash collision issue.Which context model is the most useful?First of all,we evaluate the capabilities of context models by disablinglevel-wise (3D and 2D embeddings) or dimension-wisecontext models.When context models are disabled,occurrence frequency f lG is utilized to estimate all vectors in embeddings for each level l. Note that f lG is updatedwith the training. The corresponding results are shown inTab. 1.It can be observed that a lack of either contextmodel leads to a significant BD-rate increase. 3D contextmodels contribute more than 2D ones since they occupymore storage space and are more sparse, thus having morepotential for compression.Even though context modelsthemselves introduce extra bits, the savings they bring infeature embeddings are remarkable, thanks to the accurateprediction of the probabilities.",
  ". Ablation study on context dependencies and hash fusionon Synthetic-NeRF dataset": "How to design the level-wise context model? We nowdelve into the contribution of context models from eachlevel. Specifically, we gradually replace the context modelwith f lG from deeper to shallower layers, where we use Ldto indicate the level starting from which context modelsare disabled. Experimental results are shown in byorange points.We observe that as Ld becomes smaller,RD performance decreases.This suggests that a singlef lG is inadequate to predict the feature distribution foreach level l.In contrast, our context models exhibitgreater capability in context aggregation. Experiments oncontext levels Lc are also conducted, as shown in greenpoints in .Increasing Lc does not always leadto improved performance, as a distant level may providelimited information but introduce additional complexity.Which contextual order is suitable? We investigate theorder of fine-to-coarse in level-wise context models inTab. 2. It can be seen that the coarse-to-fine context modelsperforms much better than the reverse one. This suggestsa coarse-to-fine flow aligns better with the informationrestoration behavior for a multi-resolution structure.To which extent should invalid vectors be discarded inhash fusion? Lastly, we conduct experiments to assess theeffectiveness of hash fusion, for which a key function is todiscard invalid feature vectors using AOEs to save storagespace. To demonstrate its effectiveness, we vary the extentof discarding to observe the impact on RD performance.For ablation purposes, we disable both level-wise anddimension-wise context models and only use the frequencyf lG to estimate probabilities for each level l. The results arepresented in Tab. 2. Initially, no discarding of s: we do notdiscard any of the feature vector s and retain all of them,leading to significant storage waste on invalid vectors. Thissetting is the same as the last line of Tab. 1. Moving onestep further, proper discarding of s: we apply f lGs onlyto valid s at each level l and encode them, whose validityis judged by AOEs. This approach aligns with our currentmethodology. Finally, over discarding of s: we alter thecriterion by simply determining the validity of based onwhether it is located in an occupied cell, rather than usingAOE. However, this may cause over-discarding, wherevertices necessary for rendering might be undecodable.This leads to a significant degradation in fidelity to anextremely low level (approximately 27.2 dB in PSNR under",
  "Truck": ". Qualitative quality comparisons of drums in Synthetic-NeRF dataset and Truck in Tanks and Temples dataset. We mainly comparerecent NeRF compression approaches, along with our base model Instant-NGP. While some compression algorithms can achieve a low sizeof 1MB, they significantly sacrifice reconstruction fidelity. Our approach exhibits the best visual quality at the low size. Quantitative resultsof PSNR/size are shown in the upper right.",
  ". Fidelity Upper-Bound Influences Performance": "In this subsection, we delve into a fundamental differencebetween NeRF compression and other data formats (suchas image compression). To be specific, the ground-truthimage for image compression is always available, whichtheoretically allows for perfect fidelity if no entropyconstraint is applied.However, this is not the casefor NeRF compression.The ground truth 3D sceneis not known in advance, and the upper-bound of thefidelity is fundamentally determined by the capabilityof the reconstruction algorithm.In our case, it is theCNC model without the entropy constraint, i.e., =0. shows our fidelity upperbounds under different feature dimensions F, ranging from 1 to 8.We cansee that larger feature dimensions result in higher fidelityupperbounds and better RD performance. This is becausea larger feature dimension allows more room for contextmodels to eliminate redundancy and perform compression.However, a higher upper-bound also leads to increasedtraining and rendering time, and compression becomesmore challenging when approaching the upper-bound.",
  ". Conclusion": "In this paper, we have proposed a Context-based NeRFCompression (CNC) framework, where context modelsare carefully designed to eliminate the redundancy ofbinarized embeddings.Hash collision and occupancygrid are also fully exploited to further improve predictionaccuracy. Experimental results on two benchmark datasetshave demonstrated that our CNC can significantly compressmulti-resolution Instant-NGP-based NeRFs and achieveSOTA performance.The success of NeRF compressionon static scenes provides a solid proof of concept for moreadvanced and space-taking applications such as dynamic orlarge-scale NeRFs.Limitation.The main drawback of our approach is theslowdown in training time, resulting in about 1.3 longertraining duration over the one without context models.However, this limitation can be mitigated by: 1) reducingfidelity upper-bound;2) adjusting context models;3)improving the code to execute context models and therendering MLP concurrently.",
  "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, andHao Su. Tensorf: Tensorial radiance fields. In EuropeanConference on Computer Vision, pages 333350. Springer,2022. 2, 6": "Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and JiroKatto. Learned image compression with discretized gaussianmixture likelihoods and attention modules. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 79397948, 2020. 3 Chenxi Lola Deng and Enzo Tartaglione.Compressingexplicit voxel grid representations: fast nerfs become alsosmall. In Proceedings of the IEEE/CVF Winter Conferenceon Applications of Computer Vision, pages 12361245,2023. 3, 6 Sara Fridovich-Keil, Alex Yu, Matthew Tancik, QinhongChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:Radiance fields without neural networks. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 55015510, 2022. 1, 2, 6 Sara Fridovich-Keil, Giacomo Meanti, Frederik RahbkWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:Explicit radiance fields in space, time, and appearance.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1247912488, 2023.2, 6 Sharath Girish, Abhinav Shrivastava, and Kamal Gupta.Shacira:Scalable hash-grid compression for implicitneural representations.In Proceedings of the IEEE/CVFInternationalConferenceonComputerVision,pages1751317524, 2023. 3",
  "Nelson Max. Optical models for direct volume rendering.IEEE Transactions on Visualization and Computer Graphics,1(2):99108, 1995. 3": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for viewsynthesis.Communications of the ACM, 65(1):99106,2021. 1, 2, 3, 6 Thomas Muller,Alex Evans,Christoph Schied,andAlexander Keller.Instant neural graphics primitives witha multiresolution hash encoding.ACM Transactions onGraphics (ToG), 41(4):115, 2022. 1, 2, 3, 5, 6, 4 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: Animperative style, high-performance deep learning library.Advances in neural information processing systems, 32,2019. 6",
  "Seungjoo Shin and Jaesik Park.Binary radiance fields.Advances in neural information processing systems, 2023.2, 3, 6, 4": "Vincent Sitzmann, Justus Thies, Felix Heide, MatthiasNiener,GordonWetzstein,andMichaelZollhofer.Deepvoxels:Learning persistent 3d feature embeddings.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 24372446, 2019. 1 Cheng Sun, Min Sun, and Hwann-Tzong Chen.Directvoxel grid optimization: Super-fast convergence for radiancefields reconstruction.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 54595469, 2022. 1, 2, 6 Towaki Takikawa, Joey Litalien, Kangxue Yin, KarstenKreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson,Morgan McGuire, and Sanja Fidler. Neural geometric levelof detail: Real-time rendering with implicit 3d shapes. InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1135811367, 2021. Towaki Takikawa, Alex Evans, Jonathan Tremblay, ThomasMuller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.Variable bitrate neural fields.In ACM SIGGRAPH 2022Conference Proceedings, pages 19, 2022. 1 Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, SaraFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, RaviRamamoorthi, Jonathan Barron, and Ren Ng.Fourierfeatures let networks learn high frequency functions in lowdimensional domains.Advances in Neural InformationProcessing Systems, 33:75377547, 2020. 1 Matthew Tancik, Vincent Casser, Xinchen Yan, SabeekPradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan TBarron, and Henrik Kretzschmar.Block-nerf:Scalablelarge scene neural view synthesis.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 82488258, 2022. 2 Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,Brent Yi, Terrance Wang, Alexander Kristoffersen, JakeAustin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: Amodular framework for neural radiance field development.In ACM SIGGRAPH 2023 Conference Proceedings, pages112, 2023. 2",
  "Ian H Witten, Radford M Neal, and John G Cleary.Arithmetic coding for data compression. Communicationsof the ACM, 30(6):520540, 1987. 2, 6, 7": "Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.Bungeenerf: Progressive neural radiance field for extrememulti-scale scene rendering.In European conference oncomputer vision, pages 106122. Springer, 2022. 2 Jiawei Yang, Marco Pavone, and Yue Wang.Freenerf:Improving few-shot neural rendering with free frequencyregularization. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition,pages82548263, 2023. 1 Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, RenNg, and Angjoo Kanazawa.Plenoctrees for real-timerendering of neural radiance fields. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 57525761, 2021. 1 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,and Oliver Wang. The unreasonable effectiveness of deepfeatures as a perceptual metric. In Proceedings of the IEEEconference on computer vision and pattern recognition,pages 586595, 2018. 6 Michael Zhu and Suyog Gupta. To prune, or not to prune:exploring the efficacy of pruning for model compression.In International Conference on Learning Representations(ICLR), Vancouver, CANADA, 2018. 2",
  "Supplementary Material": "In this supplementary material, we first report moreimplementation details in Sec. A, then exhibit the efficientbackward of our bit estimator function in Sec. B, andwe also present a notation table in Sec. C for clarity inunderstanding our paper. Additionally, more quantitativeand qualitative results are included at the end of thedocument.",
  "A. More Implementation Details": "Context Fuser. Context Fuser is able to aggregate contextsfrom previous Lc levels. For 3D embeddings, it is a 3-layerMLP. It has an input channel of F Lc + 1, a hiddenchannel of 32 and an output channel of F with LeakyReLUactivation, where +1 is for fG.For 2D embeddings,it is a 1-layer linear module. It has an input channel ofF Lc +1+F and an output channel of F, where +F isfor dimension-wise context. Note that different levels of thesame Lcs share one Context Fuser to save storage space.Learning Rate. Our initial learning rate is 0.01 and thetotal iteration is 20K. For the first 1K iterations, we adopta linear warm-up stage to stabilize the training process. Forthe rest iterations, the learning rate is decayed by a factor of0.33 at 9K, 12K, 15K, 17K and 19K iterations.SamplingStrategy.Duringtraining,feedingallembeddings to the context models in a single iterationwill lead to out-of-memory (OOM). To address this, werandomly sample 150K feature vectors s for training 3Dembeddings under F = 8 in each iteration, and 200K underF = 1, 2, 4. For 2D embeddings, we do not employ thisstrategy but feed them all together in one iteration.Quantization of the Rendering MLP. We utilize 13 bits toquantize the rendering MLP. For each parameter i ,",
  "MAX() MIN()(5)": "where is the parameter collection of the rendering MLPand qi is the quantized parameter. D = 13 represents thenumber of digits for quantization. MIN and MAX representoperations to calculate minimum and maximum elements,respectively.Inverse Hash Mapping.While the hash function provides only a unidirectional mapping of n , we are inneed of its inverse mapping of n. To accomplish this,during the initialization stage, we traverse all ns in voxelsusing the hash function and store their corresponding s,which takes a GPU memory of 5 GB. Consequently, wecan retrieve all associated vertices {nki |k = 1, . . . , K} ofa random vector i by querying this recorded informationduring training.",
  "Notation Definition": "xAn input coordinate for renderingdViewing direction of the input coordinate xoCamera center to observe the input coordinate xrA ray for renderingvIndex of a sampled point along the ray rDensity of the sampled point vcColor of the sampled point vTTransmittance to the sampled point v along the ray rCThe rendered pixel color of the ray rfThe interpolated input feature for positional encoding LTotal resolution level number of embeddingslA level out of LCollection of feature embeddings in one levelA vector element of embeddings A scalar of , which can be either 1 or +1iIndex of a randomly sampled TSize of embeddings fGOccurrence frequency of +1 in embeddings nAssociated vertex of in the voxelpEstimated probability for entropy modelingLcNumber of previous levels for contextLdLevel from which context models are disabledFDimension of feature vectors CpContext Fusor to aggregate contextsEpBit Estimator to calculate bit consumptionKHash collision number of kA collided vertex out of KAOEArea of effect of the vertex nPV FProjected voxel feature for dimension-wise context of 3D to 2DwNormalized weights of vertices for hash fusionLmseMean Squared Error (MSE) loss, which measures fidelityLentropy Entropy loss, which measures embedding sizeTradeoff parameter to balance fidelity and sizeParameter collection of the rendering MLPA parameter of the collection qThe quantized parameter of DNumber of digits for quantizing the rendering MLPMNumber of s in the embeddings",
  "SIZE(MB)": "F = 10.8270.8160.8060.9220.8380.8220.8020.9010.842F = 21.4451.4341.4251.5301.4561.4421.4211.5311.460F = 42.6992.6972.6802.8202.7102.6972.6762.7732.719F = 85.2105.2025.1915.3345.2225.2085.1875.2815.229 Table D. Detailed quantitative results of upper bounds (i.e. = 0) of our CNC model on NeRF-Synthetic dataset. In this case, no entropyconstraint is applied to the embeddings, thus their size is equal to the amount of s as each parameter consumes 1 bit. The rendering MLPis not quantized but retained in float32. Context models are excluded.",
  "F = 85.3265.2775.3155.3625.2635.309": "Table E. Detailed quantitative results of upper bounds (i.e. = 0) of our CNC model on Tanks and Temples dataset. In this case, no entropyconstraint is applied to the embeddings, thus their size is equal to the amount of s as each parameter consumes 1 bit. The rendering MLPis not quantized but retained in float32. Context models are excluded."
}