{
  "Abstract": "The prevalent approaches of unsupervised 3D object de-tection follow cluster-based pseudo-label generation anditerative self-training processes.However, the challengearises due to the sparsity of LiDAR scans, which leadsto pseudo-labels with erroneous size and position, result-ing in subpar detection performance. To tackle this prob-lem, this paper introduces a Commonsense Prototype-basedDetector, termed CPD, for unsupervised 3D object de-tection.CPD first constructs Commonsense Prototype(CProto) characterized by high-quality bounding box anddense points, based on commonsense intuition.Subse-quently, CPD refines the low-quality pseudo-labels by lever-aging the size prior from CProto. Furthermore, CPD en-hances the detection accuracy of sparsely scanned objectsby the geometric knowledge from CProto.CPD outper-forms state-of-the-art unsupervised 3D detectors on WaymoOpen Dataset (WOD), PandaSet, and KITTI datasets by alarge margin. Besides, by training CPD on WOD and test-ing on KITTI, CPD attains 90.85% and 81.01% 3D Aver-age Precision on easy and moderate car classes, respec-tively.These achievements position CPD in close prox-imity to fully supervised detectors, highlighting the sig-nificance of our method.The code will be available at",
  ". Introduction": "Autonomous driving requires reliable detection of 3D ob-jects (e.g. vehicle and cyclist) in urban scenes for safe pathplanning and navigation. Thanks to the power of neural net-works, numerous studies have developed high-performance3D detectors through fully supervised approaches. However, these models heavily depend on humanannotations from diverse scenes to guarantee their effective-ness across various scenarios. This data labeling process istypically laborious and time-consuming, limiting the widedeployment of detectors in practice .",
  ". Illustration of commonsense prototypes for unsupervised3D object detection in autonomous driving scenes": "Several studies have explored approaches to reduce la-beling requirements by weakly supervised learning , decreasing the label cost by over 80%. Notably, theobjects within a 3D scene exhibit distinguishable attributesand can be easily identified through certain commonsensereasoning (see ). For example, the objects are usu-ally located on the ground surface with a certain shape;the object sizes are fixed across frames. This insight hasprompted us to develop an unsupervised 3D detector thatoperates without using human annotations.In recent years, traditional methods leveraged ground re-moval and clustering technique for unsupervised3D object detection. However, these methods often strug-gle to achieve satisfactory performance due to the sparsityand occlusion of objects in 3D scenes. Advanced methodscreate initial pseudo-labels from point cloud sequences byclustering and bootstrap a good detector by iteratively train-ing a deep network . Nevertheless, the sparse and view-limited nature of LiDAR scanning leads to pseudo-labelswith inaccurate sizes and positions, misleading the networkconvergence and resulting in suboptimal detection perfor-mance. A subset of objects, denoted as complete objects T ,",
  "Completeness": "65%35% Box length Box length (d)(e) 0.5 (f) (g) Frame number . Illustration and statistics of complete and incompleteobjects on WOD validation set (large enough to demonstratethe general problem). (a) Pseudo-labels of complete object T arerefined by temporal consistency. (b) Pseudo-labels of incompleteobject J fail to be refined by temporal consistency. (c) 65% ob-jects lack full scan coverage and generate inaccurate pseudo-labels( Max IoU (Intersection over Union) < 0.5 with GT (GroundTruth)). (d) The vehicle GT of complete object GT T and incom-plete object GT J have similar size distributions. (e) The pseudo-label of complete object PseT and incomplete object PseJ havedifferent size distributions. (f)(g) The nearby stationary objects arewith high completeness in consecutive frames. benefit from having at least one complete scan across the en-tire point cloud sequence, allowing their pseudo-labels to berefined through temporal consistency (see (a)).However, the majority of objects (e.g. 65% on WOD ,as shown in (c)), termed incomplete objects J , lackfull scan coverage (see (b)), and cannot be recoveredby temporal consistency.To tackle this issue, this paper proposes a CommonsensePrototype-based Detector, termed CPD, for unsupervised3D object detection. CPD is built upon two key insights:(1) The ground truth of intra-class objects keeps a similarsize (length, width, and height) distribution between incom-plete objects and complete objects (see (d)). (2) Thenearby stationary objects are very complete in consecutiveframes and can be recognized accurately by commonsenseintuition (see (f)(g)). Our idea is to construct a Com-monsense Prototype (CProto) set representing accurate ge-ometry and size from complete objects to refine the pseudo-labels of incomplete objects and improve the detection ac-curacy. To this end, we first design an unsupervised Multi-Frame Clustering (MFC) method that yields high-recall ini-tial pseudo-labels. Subsequently, we introduce an unsuper-vised Completeness and Size Similarity (CSS) score thatselects high-quality labels to construct the CProto set. Fur-thermore, we design a CProto-constrained Box Regulariza- tion (CBR) method to refine the pseudo-labels by incorpo-rating the size prior from CProto. In addition, we developCProto-constrained Self-Training (CST) that improves thedetection accuracy of sparsely scanned objects by the ge-ometry knowledge from CProto.The effectiveness of our design is verified by exper-iments on widely used WOD , PandaSet , andKITTI dataset . Besides, the individual components ofour design are also verified by extensive experiments onWOD . The main contributions of this work include: We propose a Commonsense Prototype-based Detector(CPD) for unsupervised 3D object detection. CPD out-performs state-of-the-art unsupervised 3D detectors by alarge margin.",
  ". Related Work": "Fully/weakly supervised 3D object detection.Recentfully-supervised 3D detectors build single-stage , two-stage or multiplestage deep networks for 3D object detection. How-ever, these methods heavily rely on a large amount of pre-cise annotations.Some weakly supervised methods re-place the box annotation with low-cost click annotation.Other methods decrease the supervision by only annotatinga part of scenes or a part of instances .Unlike all of the above works, we aim to design a 3D detec-tor that does not require human-level annotations.Unsupervised 3D object detection. Previous unsuper-vised pre-training methods discern latent patterns withinthe unlabeled data by masked labels or contrastiveloss . But these methods require human labels forfine-turning. Traditional methods employ groundremoval and clustering for 3D object detection without hu-man labels, but suffer from poor detection performance.Some deep learning-based methods generate pseudo-labelsby clustering and use the pseudo-labels to train a 3D detec-tor iteratively. Recent OYSTER improves pseudo-label quality with temporal consistency.However, mostpseudo-labels of incomplete objects cannot be recoveredby temporal consistency. Our CPD addresses this problemby leveraging the geometry prior from CProto to refine thepseudo-label and guide the network convergence.Prototype-based methods. The prototype-based meth-ods are widely used in 2D detection when novel classes are incorporated.Inspired by these Commonsense Prototype for Outdoor Unsupervised 3D Object Detection1. Introduction (c) CProto-constrained self-training RoIs",
  "Detection network": "Prototype network . CPD framework. (a) Initial pseudo-labels are generated by multi-frame clustering. (b) The commonsense prototype (CProto) isconstructed from high-quality pseudo-labels based on CSS score. The low-quality labels are further refined by the shape prior from CProto.(c) A prototype network fed with dense points from CProto produces high-quality features to guide the detection network convergence. methods, Prototypical VoteNet constructs geometricprototypes learned from basic classes for few-shot 3D ob-ject detection.GPA-3D and CL3D build geo-metric prototypes from a source-domain model for domainadaptive 3D detection. However, both the learning frombasic class and training on the source domain require high-quality annotations. Unlike that, we construct CProto usingcommonsense knowledge and detect 3D objects in a zero-shot manner without human-level annotations.",
  ". Initial Label Generation": "Recent unsupervised methods detect 3D objects ina class-agnostic way. How to classify objects (e.g. vehi-cle and pedestrian) without annotation is still an unsolvedchallenge. Our observations indicate that some stationaryobjects in consecutive frames, appear more complete (see (f)) and can be classified by predefined sizes. Thismotivates us to design a Multi-Frame Clustering (MFC)method to generate initial labels. MFC involves motion ar-tifact removal, clustering, and post-processing.Motion Artifact Removal (MAR). Directly trans-forming and concatenating 2n + 1 consecutive frames{xn, ..., xn} (i.e., past n, future n, and the current frame) into a single point cloud x0 introduces motion artifacts frommoving objects, leading to increased label errors as the ngrows (see (a)). To mitigate this issue, we first trans-form the consecutive frames to global system and calculatethe Persistence Point Score (PPScore) by consecutiveframes to identify the points in motion. We keep all thepoints from x0 and remove moving points from the otherframes xn, ..., x1, x1, ..., xn.After this removal, weconcatenate the frames to obtain dense points x0.Clustering and post-processing.In line with recentstudy , we apply the ground removal, DBSCAN and bounding box fitting on x0 to obtain a set ofclass-agnostic bounding boxes b. We observe that the ob-jects of the same class typically have similar sizes in 3Dspace. Therefore, we pre-define class-specific size thresh-olds (e.g.the length of vehicle is generally larger than0.5m) based on human commonsense to classify b intodifferent categories. We then apply class-agnostic track-ing to associate the small background objects with fore-ground trajectories, and enhance the consistency of objectssizes by using temporal coherency . This process re-sults in a set of initial pseudo-labels b = {bj}j, wherebj = [x, y, z, l, w, h, , , ] represents position, width,length, height, azimuth angle, class identity, and trackingidentity, respectively.",
  ". (a) Length absolute error with different frames. (b)Multi-level occupancy score. (c) Mean size error of initial labels": "tackle this issue, we introduce the CProto-constrained BoxRegularization (CBR) method. The key idea is to constructa high-quality CProto set based on unsupervised scoringfrom complete objects to refine the pseudo-labels of incom-plete objects. Different from OYSTER , which can onlyrefine the pseudo-labels of objects having at least one com-plete scan, our CBR can refine pseudo-labels of all objects,significantly decreasing the overall size and position errors.Completeness and Size Similarity (CSS) scoring. Ex-isting label scoring methods such as IoU scoring aredesigned for fully supervised detectors. In contrast, we in-troduce an unsupervised Completeness and Size Similarityscoring (CSS) method. It aims to approximate the IoU scoreusing commonsense knowledge alone (see ).Distance score. CSS first assesses the object complete-ness based on distance, assuming labels closer to the egovehicle are likely to be more accurate. For an initial labelbj, we normalize the distance to the ego vehicle within therange to compute the distance score as",
  "(bj) = 1 N(cj),(1)": "where N is the normalization function and cj is the loca-tion of bj. However, this distance-based approach has itslimitations. For example, occluded objects near the ego ve-hicle, which should receive lower scores, are inadvertentlyassigned high scores due to their proximity. To mitigate thisissue, we introduce a Multi-Level Occupancy (MLO) score,further detailed in (b).MLO score. Considering the diverse sizes of objects, wedivide the bounding box of the initial label into multiplegrids with different length and width resolutions. The MLOscore is then calculated by determining the proportion ofgrids occupied by cluster points, via",
  ". Completeness and size similarity scoring": "they fall short in assessing classification quality. To bridgethis gap, we introduce the SS score. This score utilizes aclass-specific template box a (average size of typical objectsin Wikipedia) and calculates a truncated KL divergence .Note that, this score is decided by ratio difference, ratherthan their specific values. Simple commonsense of l, w, hratios (2:1:1 for Vehicle, 1:1:2 for Pedestrian, 2:1:2 for Cy-clist) can also be used here.",
  "where qa {la, wa, ha}, qb {lb, wb, hb} refer to the nor-malized length, width, and height of the template and label.We linearly combine the three metrics S(bj)=": "i ii(bj) to produce final scoring, where i is theweighting factor (in this study we adopt a simple average,i = 1/3). For each bj b, we compute its CSS scorescssj= S(bj) and obtain a set of scores s = {scssj}j.CProto set construction. Regular learnable prototype-based methods require annotations , which are un-available in the unsupervised problem. We construct a high-quality CProto set P = {Pk}k, representing geometry andsize centers based on the unsupervised CSS score. Here,Pk = {xpk, bpk}, where xpk indicates the inside points, and bpkrefers to the bounding box. Specifically, we first categorizethe initial labels b into different groups based on their track-ing identity . Within each group, we select the high-qualityboxes and inside points that meet a high CSS score thresh-old (determined on validation set, using 0.8 in this study).Then, we transform all points and boxes into a local coor-dinate system, and obtain bpk by averaging the high-qualityboxes and xpk by concatenating all the points.Box regularization. We next regularize the initial la-bels by the size prior from CProto. Based on the statisticson WOD validation set , we observe that the height ofthe initial labels is relatively correct than length and width(see (c)). Intuitively, the intra-class 3D objects withthe same height have similar length and width. Therefore,we associate the initial label bj with CProto Pk by the min-imum difference in box height. The initial pseudo-labelswith the same Pk and similar length and width are natu-rally classified into the same group. We then perform re-size and re-localization for each group to refine the pseudo-",
  ". The size of the initial label is replaced by the CProtobox, and the position is also corrected": "labels. (1) Re-size. We directly replace the size of bj usingthe length, width, and height of bpk Pk. (2) Re-location.Since points are mostly on the objects surface and bound-ary, we divide the object into different bins and align the boxboundary and orientation to the boundary point of the dens-est part (see ). Finally, we obtain improved pseudo-labels b = {bj}j.",
  ". CProto-constrained Self-Training (CST)": "Recent methods utilize pseudo-labels for train-ing 3D detectors. However, even after refinement, somepseudo-labels remain inaccurate, diminishing the effective-ness of correct supervision and potentially misleading thetraining process. To tackle these issues, we propose twodesigns: (1) CSS-Weighted Detection Loss, which assignsdifferent training weights based on label quality to sup-press false supervision signals. (2) Geometry Contrast Loss,which aligns predictions of sparsely scanned points with thedense CProto, thereby improving feature consistency.Network architecture. We adopt a dense-sparse align-ment architecture ( (c)), consisting of a prototype net-work Fpro and a detection network Fdet, constructed fromtwo-stage CenterPoint . During training, for each bj,we add its corresponding points xpk from CProto Pk to thescene to obtain a dense point cloud xpro. We feed xpro toFpro to produce relatively good features and detections. Wethen feed randomly downsampled points xdet as a sparsesample to the Fdet. We align the features and detectionsfrom two branches by the detection loss and contrast loss.During testing, we feed points without downsampling to thedetection network Fdet to perform detection.CSS weight. Considering that the false pseudo-labelsmay mislead the network convergence, we first calculate aloss weight based on different label qualities. Formally, weconvert a CSS score scssiof a pseudo-label to",
  "i i(Lproi+ Ldeti),(5)": "where Lproiand Ldetiare detection losses of Fpro andFdet, respectively. The losses are calculated by pseudo-labels b and network predictions.Geometry contrast loss.We formulate two contrastlosses that minimize the feature and predicted box differ-ence between the prototype and detection network.(1)Feature contrast loss. For a foreground RoI ri from thedetection network, we extract features f pi from the proto-type network by voxel set abstract , and extract featuresf di from detection network. We then formulate the contrastloss by cosine distance:",
  ". Datasets": "Waymo Open Dataset (WOD). We conducted extensiveexperiments on the WOD due to its diverse scenes.The WOD contains 798, 202 and 150 sequences for train-ing, validation and testing, respectively. We adopted simi-lar metrics (3D AP L1 and L2) as fully/weakly supervisedmethods . No annotations were used for training.PandaSet dataset.To compare with recent unsuper-vised methods , we also conducted experiments on thePandaSet . Like , we split the dataset into 73 train-ing and 30 validation snippets and use class-agnostic BEVAP and recall metrics with 0.3, 0.5, and 0.7 IoU thresholds.KITTI dataset. Since the KITTI detection dataset did not provide consecutive frames, we only tested ourmethod on the 3769 val split . We used similar met-rics (Car 3D AP R40 with 0.5 and 0.7 IoU thresholds) asemployed in fully/weakly supervised methods .",
  ". Implementation Details": "Network details. Both prototype and detection networksadopt the same 3D backbone as CenterPoint and thesame RoI refinement network as Voxel-RCNN . For theWOD and KITTI datasets, we use the same detection rangeand voxel size as CenterPoint . For the Pandaset, weuse the same detection range as OYSTER .Training details. We adopt the widely used global scal-ing and rotation data augmentation. We trained our networkon 8 Tesla V100 GPUs with the ADAM optimizer. We useda learning rate of 0.003 with a one-cycle learning rate strat-egy. We trained the CPD for 20 epochs.",
  ". Comparison with Unsupervised Detectors": "Results on WOD. The results on the WOD validation setand test set are presented in and . All meth-ods use identical size thresholds to define the object classesand use single traversal. Our method significantly outper-forms existing unsupervised methods. Notably, under the3D AP L2 with IoU thresholds of 0.7, 0.5, and 0.5, our CPDoutperforms OYSTER by 18.03%, 13.08%, and 4.55%on Vehicle, Pedestrian, and Cyclist, respectively. These ad-vancements come from our MFC, CBR, and CST designs,which yield superior pseudo-labels and enhanced detectionaccuracy.CPD also surpasses the Proto-vanilla method,which uses class-specific prototype .",
  ". Vehicle detection comparison with fully/weakly super-vised detectors on WOD validation set": "Results on PandaSet.The class-agnostic results onPandaSet are presented in .Our method outper-forms OYSTER by 6.5% AP and 9.3% Recall under 0.7 IoUthreshold. This improvement is largely due to our CPDsenhanced label quality.Unlike OYSTER, which suffersfrom the misleading effects of false labels during training,our CPD leverages the size prior from CProto to signifi-cantly improve these labels.",
  ". Comparison with Fully/Weakly Supervised De-tectors": "Results on KITTI dataset. To further validate our method,we pre-trained our CPD, along with OYSTER andMODEST , on WOD and tested them on the KITTIdataset using Statistical Normalization (SN) . The cardetection results are in .We first compared ourmethod with a sparsely supervised method (weakly super-vised with 2% labels) that annotates a single instanceper frame for training.Our unsupervised CPD outper-forms this sparsely supervised method by 23.52% 3D AP @IoU0.7 on moderate car class. Additionally, our method at-tains 90.85% and 81.01% 3D AP for the easy and moderate",
  "Main Results": "Initial Pseudo Label Comparison with SOTA Fig: IoU distributionAnd boxes errors. 00.50.90.7 IoU (c) Ours DBSCANMODESTOYSTEROurs DBSCANMODESTOYSTEROurs (f) Angle error(e) Position error DBSCANMODESTOYSTEROurs 00.50.90.7 IoU Number of labels (a) MODEST 00.50.90.7 IoU (b) OYSTER 1.6 1.1 0.6 0.1 Mean absolute error Detection distance (m)Detection distance (m)Detection distance (m) (d) Size error",
  ". Pseudo-label Comparison": "To validate our pseudo-labels, we analyzed their 3D recalland precision on the WOD validation set. As shown in Ta-ble 6, our method surpasses the previous best-performingOYSTER with a 9.42% recall and 5.29% precision im-provement (under a 0.7 IoU threshold). To understand thesources of this improvement, we examined the IoU betweenthe pseudo-labels and ground truth, and compared the IoUdistributions in (a)(b)(c). We also present the meanabsolute error of size, position, and angle between differentpseudo-labels in (d)(e)(f). The IoU distribution ofour method is much closer to 1 than other methods, and italso exhibits lower errors in size, position, and angle. Theseresults verify that our MFC and CBR significantly reducelabel errors. y AP curve of CSS compared with baseline IoU=0.5IoU=0.7 26.88 18.14 IoU=0.5IoU=0.7 26.98 18.16 Input frames Input frames 00.10.20.3 0.0 0.5 1.0",
  ". Ablation Study": "Components analysis of CPD. To evaluate the individualcontributions of our designs, we incrementally added eachcomponent and assessed their impact on vehicle detectionusing the WOD validation set. The results are shown in. Our MFC method surpasses Single Frame Cluster-ing (SFC) by 2.52% in AP, attributed to the more completepoint representation of objects across consecutive framescompared to a single frame. The CBR further enhances per-formance by 19.27% in AP, as it reduces size and locationerrors in pseudo-labels. The CST contributes an 8.09% in-crease in AP, demonstrating the effectiveness of geometricfeatures from CProto in detecting sparse objects.Frame number of MFC. To examine the effect of framecount on initial pseudo-label quality, we experimented withdifferent numbers of past and future point cloud frames onthe WOD validation set. The BEV results, shown in (a)(b), indicate optimal performance with frames(five past, five future, and the current frame). Additionalframes did not significantly improve recall or precision.Consequently, we used 11 frames for initial pseudo-labelgeneration in this study.Component analysis of CSS Scoring. To assess the ef-fectiveness of our scoring system, we calculated the BEVAP of initial pseudo-labels with different scores.These (1) MODEST(2) OYSTER(3) CPD (Ours) Ground truthDetections Ground truthDetections (1.1) (1.2) (2.1)(2.2) (3.1) (3.2) Ground truthDetections",
  ". CBR component analysis results on WOD validation set": "evaluations, reported in , show that incorporating allcomponents (distance, MLO, and SS) yields the highest AP.The recall-precision curve, plotted in (c), also sup-ports this finding. These indicate the significance of eachcomponent in accurately measuring pseudo-label quality.Components analysis of CBR. To evaluate the im-pact of re-sizing and re-localization in CBR, we conductedexperiments and analyzed pseudo-label performance. Asshown in , re-sizing results in a 3.91% and 3.4%increase in BEV recall at the 0.5 and 0.7 IoU thresh-olds, respectively; re-localization further enhances recall by12.68% and 6.43% at these thresholds, while also increas-ing precision. These results indicate the importance of bothcomponents, which effectively refine pseudo-labels.Components analysis of CST. To assess the effective-ness of each component in CST, we established a base-line using only CBR-generated pseudo-labels for traininga two-stage CenterPoint detector, then incrementally addedour loss components and evaluated vehicle detection per-formance on the WOD validation set.As shown in Ta-ble 10, all loss components contribute to performance im-provement. Specifically, our Lcssdet mitigates the influenceof false pseudo-label using CSS weight, and improves the3D AP L2 at IoU0.7 by 4.79%. Our Lcssfeat and Lcssbox im-prove the 3D AP L2 at IoU0.7 by 0.75% and 2.55% respec-tively, through leveraging geometric knowledge from denseCProto for more effective sparse object detection.",
  ". CST component analysis results on WOD validation set": "distant, sparse objects ((1.1)), while OYSTER de-tects them but inaccurately reports their sizes and positions((2.1)). In contrast, CPD, using our CProto-based de-sign, not only recognizes these objects but also accuratelypredicts their sizes and positions ((3.1)). Furthermore,since our CST reduces the influence of false pseudo-labels,the false positives ((3.2)) are also much fewer than theprevious methods ((1.2)(2.2)).",
  ". Conclusion": "This paper presents the CPD framework, a novel approachfor accurate unsupervised 3D object detection. First, wedevelop an MFC method to generate initial pseudo-labels.Then, a CProto set is constructed using CSS scoring. Next,we introduce a CBR method to refine these pseudo-labels.Lastly, a CST is designed to enhance detection accuracy forsparse objects. Extensive experiments have verified the ef-fectiveness of our design. Notably, for the first time, our un-supervised CPD method surpasses some weakly supervisedmethods, demonstrating the advancement of our approach.Limitations.One notable limitation of our work isthe significantly lower Average Precision (AP) for minorityclasses, such as cyclists (), compared to more preva-lent classes like vehicles. This disparity is largely due to thescarce instances of these minority classes within the dataset.Future efforts to collect such objects could be a promisingavenue to tackle this issue.Acknowledgements. This work was supported in partby the National Natural Science Foundation of China(No.62171393), the Fundamental Research Funds for theCentral Universities (No.20720220064).",
  "Qi Cai, Yingwei Pan, Ting Yao, and Tao Mei. 3d cascadercnn: High quality object detection in point clouds. IEEETransactions on Image Processing, 31:57065719, 2022. 2": "Zehui Chen, Zhenyu Li, Shuo Wang, Dengpan Fu, and FengZhao. Learning from noisy data for semi-supervised 3d ob-ject detection.In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 69296939,2023. 1, 2 Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wen gang Zhou,Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towardshigh performance voxel-based 3d object detection. In Pro-ceedings of the AAAI Conference on Artificial Intelligence,2021. 1, 2, 5, 6 Martin Ester, Hans-Peter Kriegel, Jorg Sander, and XiaoweiXu. A density-based algorithm for discovering clusters inlarge spatial databases with noise. In Knowledge Discoveryand Data Mining, 1996. 3, 6, 7 Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the kitti vision benchmarksuite. In Proceedings of the IEEE conference on ComputerVision and Pattern Recognition (CVPR), pages 33543361,2012. 2, 5 Goldberger, Gordon, and Greenspan. An efficient image sim-ilarity measure based on approximations of kl-divergence be-tween two gaussian mixtures. In Proceedings Ninth IEEE In-ternational conference on computer vision, pages 487493.IEEE, 2003. 4 Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua,and Lei Zhang. Structure aware single-stage 3d object detec-tion from point cloud. In Proceedings of the IEEE conferenceon Computer Vision and Pattern Recognition (CVPR), pages1187311882, 2020. 2",
  "Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,Huizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified birds-eye view repre-sentation. ArXiv, 2022. 1": "Xiaonan Lu, Wenhui Diao, Yongqiang Mao, Junxi Li, Pei-jin Wang, Xian Sun, and Kun Fu.Breaking immutable:information-coupled prototype elaboration for few-shot ob-ject detection. In Proceedings of the AAAI Conference onArtificial Intelligence, pages 18441852, 2023. 2 Qinghao Meng, Wenguan Wang, Tianfei Zhou, JianbingShen, Yunde Jia, and Luc Van Gool. Towards a weakly su-pervised framework for 3d point cloud object detection andannotation. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 44(8):44544468, 2022. 2",
  "Xidong Peng, Xinge Zhu, and Yuexin Ma. Cl3d: Unsuper-vised domain adaptation for cross-lidar 3d detection. In Pro-ceedings of the AAAI Conference on Artificial Intelligence,pages 20472055, 2023. 3": "Gheorghii Postica, Andrea Romanoni, and Matteo Mat-teucci. Robust moving objects detection in lidar data exploit-ing visual cues. In 2016 IEEE/RSJ International Conferenceon Intelligent Robots and Systems (IROS), pages 10931098.IEEE, 2016. 2 Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-cnn: 3d object proposal generation and detection from pointcloud.In Proceedings of the IEEE conference on Com-puter Vision and Pattern Recognition (CVPR), pages 770779, 2019. 2 Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, JianpingShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Pro-ceedings of the IEEE conference on Computer Vision andPattern Recognition (CVPR), pages 10526 10535, 2020. 4 Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang,and Hongsheng Li. From points to parts: 3d object detectionfrom point cloud with part-aware and part-aggregation net-work. IEEE Transactions on Pattern Analysis and MachineIntelligence (TPAMI), 43:26472664, 2021. 2",
  "Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, SenWang, Di He, Bernt Schiele, and Liwei Wang.DSVT:Dynamic Sparse Voxel Transformer With Rotated Sets. InCVPR, 2023. 2": "Yan Wang, Xiangyu Chen, Yurong You, Li Erran Li, BharathHariharan, Mark Campbell, Kilian Q Weinberger, and Wei-Lun Chao. Train in germany, test in the usa: Making 3d ob-ject detectors generalize. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1171311723, 2020. 6 Aming Wu, Yahong Han, Linchao Zhu, and Yi Yang.Universal-prototype enhancing for few-shot object detection.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 95679576, 2021. 2 Hai Wu, Jinhao Deng, Chenglu Wen, Xin Li, and ChengWang. Casa: A cascade attention network for 3d object de-tection from lidar point clouds. IEEE Transactions on Geo-science and Remote Sensing, 2022. 1, 2 Hai Wu, Chenglu Wen, Wei Li, Ruigang Yang, and ChengWang. Learning transformation-equivariant features for 3dobject detection. In Proceedings of the AAAI Conference onArtificial Intelligence, 2023. 2, 5 Hai Wu, Chenglu Wen, Shaoshuai Shi, Xin Li, and ChengWang. Virtual sparse convolution for multimodal 3d objectdetection. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2023. 5 Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, ChenxiHuang, Chengqi Deng, Haifeng Liu, and Deng Cai. Sparsefuse dense: Towards high quality 3d detection with depthcompletion. In Proceedings of the IEEE conference on Com-puter Vision and Pattern Recognition (CVPR), 2022. 1, 2 Qiming Xia, Jinhao Deng, Chenglu Wen, Hai Wu, ShaoshuaiShi, Xin Li, and Cheng Wang. Coin: Contrastive instancefeature mining for outdoor 3d object detection with very lim-ited annotations. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, 2023. 2, 5, 6 Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang,Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, KunJiang, Yunlong Wang, and Diange Yang.Pandaset: Ad-vanced sensor suite dataset for autonomous driving. 2021IEEE International Intelligent Transportation Systems Con-ference (ITSC), 2021. 2, 5 Honghui Yang, Tong He, Jiaheng Liu, Huaguan Chen, BoxiWu, Binbin Lin, Xiaofei He, and Wanli Ouyang.Gd-mae: Generative decoder for mae pre-training on lidar pointclouds.In Conference on Computer Vision and PatternRecognition (CVPR), pages 94039414, 2023. 2 Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Ji-aya Jia. Std: Sparse-to-dense 3d object detector for pointcloud. In Proceedings of the IEEE International Conferenceon Computer Vision (ICCV), pages 19511960, 2019. 2 Junbo Yin, Dingfu Zhou, Liangjun Zhang, Jin Fang, Cheng-Zhong Xu, Jianbing Shen, and Wenguan Wang. Proposal-contrast: Unsupervised pre-training for lidar-based 3d objectdetection. In European conference on computer vision, pages1733, 2022. 2 Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In Proceedings of theIEEE conference on Computer Vision and Pattern Recogni-tion (CVPR), 2021. 2, 5, 6, 7 Yurong You, Katie Luo, Cheng Perng Phoo, Wei-Lun Chao,Wen Sun, Bharath Hariharan, Mark E. Campbell, and Kil-ian Q. Weinberger. Learning to detect mobile objects fromlidar scans without labels. In 2022 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), 2022.1, 2, 3, 5, 6, 7, 8 Lunjun Zhang, Anqi Joyce Yang, Yuwen Xiong, SergioCasas, Bin Yang, Mengye Ren, and Raquel Urtasun. To-wards unsupervised object detection from lidar point clouds.In 2023 IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), 2023. 1, 2, 3, 4, 5, 6, 7, 8 Quanshi Zhang, Xuan Song, Xiaowei Shao, Huijing Zhao,and Ryosuke Shibasaki. Unsupervised 3d category discoveryand point labeling from a large urban environment. In 2013IEEE International Conference on Robotics and Automation,pages 26852692. IEEE, 2013. 1",
  "Xiao Zhang, Wenda Xu, Chiyu Dong, and John M Dolan. Ef-ficient l-shape fitting for vehicle detection using laser scan-ners.In 2017 IEEE Intelligent Vehicles Symposium (IV),pages 5459. IEEE, 2017. 3": "Yixin Zhang, Zilei Wang, and Yushi Mao. Rpn prototypealignment for domain adaptive object detector. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1242512434, 2021. 2 Zehan Zhang, Yang Ji, Wei Cui, Yulong Wang, Hao Li, XianZhao, Duo Li, Sanli Tang, Ming Yang, Wenming Tan, et al.Atf-3d: Semi-supervised 3d object detection with adaptivethresholds filtering based on confidence and distance. IEEERobotics and Automation Letters, 7(4):1057310580, 2022.2",
  "Shizhen Zhao and Xiaojuan Qi. Prototypical votenet for few-shot 3d point cloud object detection. Advances in NeuralInformation Processing Systems, 35:1383813851, 2022. 3,4": "Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. Se-ssd: Self-ensembling single-stage object detector from pointcloud.In Proceedings of the IEEE conference on Com-puter Vision and Pattern Recognition (CVPR), pages 1449414503, 2021. 2 Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learningfor point cloud based 3d object detection. In Proceedings ofthe IEEE conference on Computer Vision and Pattern Recog-nition (CVPR), pages 44904499, 2018. 2",
  ". More Details of Method": "More details of MFC. In our main paper section 3.1, we in-troduced the Multi-Frame Clustering (MFC) for initial labelgeneration. For a more intuitive understanding, we providea framework illustration in . Here we present moredetails of post-processing. As mentioned in our main paper,we pre-defined a set of class-specific size thresholds basedon human commonsense to classify pseudo labels into dif-ferent categories. Taking the WOD as an example, we pre-define five categories: Discard Small, Pedestrian, Cy-clist, Vehicle, and Discard Large. Formally, for a clus-ter box bj, we determine the class identity by sequentiallymatching from the thresholds:",
  "(8)": "Where l, w, h refers to the length, width, and height of bj,respectively. The Discard Large boxes mostly with treesand buildings are directly removed. The Discard Smallboxes contain both potential foreground objects and back-ground objects. We then apply class-agnostic tracking toassociate the small background objects with foreground tra-jectories, and enhance the consistency of objects sizes by 3.2 Completeness and Shape Scoring (CSS) for C-Proto Constructio Fig5object completeness motivation 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8",
  ". The comparison of different scoring methods": "using temporal coherency.More details of CSS scoring. In our main paper sec-tion 3.2, we presented the CSS scoring. To better under-stand how the CSS scoring approximates the IoU score, wepresent the IoU-score carve in , where we show threemethods: density scoring (sden), distance scoring (sdis) andour CSS scoring (scss). Intuitively, good scoring shouldkeep consistent with IoU scoring. In other words, with theincrease of score, the selected pseudo labels should havelarger IoUs with the ground truth. We found that our CSSscoring keeps the most consistent increase along with theIoU increase. Here we also provide the length, width andheight of the template box for calculating the Size Similar-ity in the main paper Eq. 3:",
  ". More Experimental Results": "More visualization results. To better understand how ourmethod improves detection results, here we present morevisualization results. From , we observe that boththe recognition and localization performance of our method(3.1-3.4) are much better than previous methods(1.1-1.4,2.1-2.4), thanks to our CProto-based design.BEV AP and 3D APH results on WOD validation set.Some fully supervised methods also reported the BEV APL2 and 3D APH performance. Here we presented the resultsin and , respectively. Our CPD outper-forms the previous MODEST and OYSTER in both BEVAP L2 and APH L2 by a large margin, further demonstrat-ing the effectiveness of our method. MODESTOYSTEROurs Ground truthDetections Ground truthDetections Ground truthDetections (1.1) (1.1) (1.2) (1.3)(1.4) (1.2)(1.3)(1.4) (2.1) (2.2) (2.3)(2.4) (2.1) (3.1) (3.2) (3.3)(3.4) (2.2)(2.3)(2.4) (3.1)(3.2)(3.3)(3.4) (1.1) (1.2) (1.3) (1.4) (3.1) (3.2) (3.3) (3.4) (2.1) (2.2) (2.3) (2.4) MODESTOYSTEROurs Ground truthDetections Ground truthDetections Ground truthDetections (1.1) (1.2)(1.3)(1.4)(2.1) (2.2)(2.3)(2.4) (3.1)(3.2)(3.3)(3.4)"
}