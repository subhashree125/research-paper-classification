{
  "Abstract": "Dominant dual-encoder models enable efficient image-text retrieval but suffer from limited accuracy, while thecross-encoder models offer higher accuracy at the expenseof efficiency.Distilling cross-modality matching knowl-edge from cross-encoder to dual-encoder provides a nat-ural approach to harness their strengths. Thus, we investi-gate the following valuable question: how to make cross-encoder a good teacher for dual-encoder? Our findingsare threefold: (1) Cross-modal similarity score distributionof cross-encoder is more concentrated, while the result ofdual-encoder is nearly normal, making vanilla logit distil-lation less effective. However, ranking distillation remainspractical, as it is not affected by the score distribution.(2) Only the relative order between hard negatives conveysvalid knowledge, while the order information between easynegatives has little significance. (3) Maintaining the coordi-nation between distillation loss and dual-encoder trainingloss is beneficial for knowledge transfer. Based on thesefindings, we propose a novel Contrastive Partial RankingDistillation (CPRD) method, which implements the objec-tive of mimicking relative order between hard negative sam-ples with contrastive learning. This approach coordinateswith the training of the dual-encoder, effectively transfer-ring valid knowledge from the cross-encoder to the dual-encoder. Extensive experiments on image-text retrieval andranking tasks show that our method surpasses other distil-lation methods and significantly improves the accuracy ofdual-encoder.",
  "EE": ". (a) Similarity score distribution of dual-encoder andcross-encoder. (b) Student predictions and targets for differenttypes of distillation methods. For partial ranking distillation, therelative order between easy negatives is disregarded.learning to facilitate global image-text alignment. Althoughdual-encoder has high efficiency, its retrieval accuracy issub-optimal due to the lack of modality interactions ex-cept for the final dot product. In contrast, the cross-encoderarchitecture of another line of works perform deep cross-modal interaction between images and texts in a single en-coder. This allows them to retrieve more accurately, but theneed to calculate on all possible image-text pairs sacrificesretrieval efficiency.To obtain an efficient and accurate retriever, a naturalidea is to distill the cross-modal matching knowledge fromthe cross-encoder into the dual-encoder. However, the dis-crepancies between the two models make the existing atten-tion and logit distillation methods less versatile or effective.On the one hand, attention distillation requires extra mod-ules to compute cross-modal attention of dual-encoder anda cross-encoder with specific architecture to provide distil-lation targets. On the other hand, the differences in train-ing objectives and architectures between two models leadto significantly distinct and non-directly alignable cross-modal similarity score distributions.To explore a general and efficient way to achieve ef-fective knowledge transfer from cross-encoder to dual-",
  "arXiv:2407.07479v1 [cs.CV] 10 Jul 2024": "encoder, we analyze their distinct characteristics and thedifficulties hindering distillation, and make the followingobservations: (1) As shown in (a), the cross-modalsimilarity distribution of the dual-encoder constrained bycontrastive learning tends to be moderate and normallydistributed, whereas the similarity score distribution ofcross-encoder optimized with image-text matching is moreaggressive and mainly concentrates on 0/1. Vanilla logitdistillation based on KL-divergence enforces the similar-ity score distribution of dual-encoder to approximate thatof cross-encoder. It is difficult due to the substantial dif-ferences between them and may interfere with the origi-nal training objective of dual-encoder, thereby disruptingthe learning of coarse image-text alignment. However, wefind that mimicking the ranking of negatives derived fromsorting similarity scores is an effective distillation scheme,which we refer to as ranking distillation, as it solely dependson the relative ranking of the scores and is not influenced bythe sharpness of score distribution. (2) Furthermore, perfor-mance gap between dual-encoder and cross-encoder mainlystems from inaccurate ranking of top-ranked items. Sup-porting this claim, re-ranking top 32 items retrieved by dual-encoder with cross-encoder substantially improves retrievalaccuracy. Further expanding the number of re-ranked itemsyields negligible gains, indicating that distillation should fo-cus more on partial ranking of top-ranked samples ratherthan all samples. The illustrations of vanilla logit distilla-tion, ranking distillation, and partial ranking distillation areshown in (b) for clarity. (3) Different types of losshave varied effects on the image-text embedding space ofdual-encoder. Therefore, it is beneficial to maintain coor-dination between ranking distillation loss and the originaltraining loss of dual-encoder to avoid interfering with thelearning process. Based on the above observations, we propose a novelContrastive Partial Ranking Distillation (CPRD) method toenable effective ranking knowledge distillation.Specifi-cally, we learn the ranking of hard negative samples via con-trastive learning. Given an image, we first utilize the dual-encoder to identify the top-K hard negative texts and obtainthe ranking of these texts. Then, we feed the image andnegative texts into cross-encoder to compute the matchingscores and divide the negative texts into valid and invalidnegative texts. Valid negative texts have a larger match-ing score with the image, and their relative order containsrich cross-modal matching knowledge. Therefore, we em-ploy contrastive learning to pull higher-ranked valid nega-tive texts closer to image while pushing the lower-rankedtexts away, ensuring the consistency between ranking ofvalid negative texts from dual-encoder and that from thecross-encoder.On the other hand, invalid negative textshave a smaller matching score with the image, and theirrelative order does not contain valid information. Thus, we only employ contrastive learning to push all invalid negativetexts away and disregard the relative order between them.This approach does not require the similarity distribu-tions of dual-encoders and cross-encoders to be similar,overcoming the distillation difficulties caused by the sub-stantial differences between similarity distributions. Fur-thermore, we achieve the objective of hard negative rankinglearning with contrastive learning, aligning seamlessly withthe dual-encoder training process. The contributions of thiswork can be summarized as follows: We conduct a comprehensive investigation into the effec-tive knowledge distillation from cross-encoders to dual-encoders and identify three key aspects. We propose the Contrastive Partial Ranking Distillation(CPRD) method, which attains the objective of learningthe relative order between valid hard negatives via con-trastive learning, enabling effective knowledge transferfrom cross-encoder to dual-encoder.",
  ". Related Work": "Pre-training for Image-text Retrieval. Pre-training worksfor image-text retrieval can be categorized into dual-encoder methods and cross-encoder methods. Dual-encodermethods adopt two separate encodersto extract the image and text features separately, and em-ploy contrastive learning to align global representationsin a shared embedding space.Cross-encoder methods employ a single encoderfor the joint encoding of image-text features. Many proxytasks are proposed to facilitate cross-modal interaction inthe cross-encoder, e.g., masked language modeling (MLM),masked region modeling (MRM), and masked image mod-eling (MIM), etc.In order to improve the retrieval ac-curacy of dual-encoders, one line of approaches draw inspiration from cross-encoder and enhance the cross-modal interaction by adapting MLM, MRM, and MIM tasksto dual-encoder. However, another approach, transferringknowledge from cross-encoder to dual-encoder through dis-tillation, has not been fully explored.Knowledge Distillation from Cross-Encoder to Dual-Encoder.Previous works distilling knowledge from thecross-encoder to the dual-encoder can be divided intoattention distillation methods and logit distillationmethods. Attention distillation methods aim to alignthe cross-modal attention of two models, which requirestwo prerequisites: (1) Both models have to adopt attention-based backbone, e.g., ViT, BERT, to produce atten-tion map. (2) Input to dual-encoder and cross-encoder isexactly identical to guarantee their attention maps have thesame shape and semantics. These make attention distilla- tion less versatile. Inspired by the distillation works in im-age classification, Miech et al. and Lei et al. introduce logit distillation into image-text retrieval.Thecore idea is to constrain the consistency of the image-textsimilarity score distribution of the dual encoder and cross-encoder through KL-divergence based loss. But substan-tial similarity distribution differences between dual-encoderand cross-encoder make it difficult to transfer knowledgeeffectively. To explore an effective and general distillationmethod to improve the performance of dual-encoders, weinvestigate and identify three key factors and further pro-pose the Contrastive Partial Ranking Distillation method.Ranking Distillation for Neural Ranking. In the field oftextual neural ranking, there are some studies on rankingdistillation .Sashank et al. propose rankdistillation losses with cross-entropy or MSE loss to con-strain the consistency of positive sample scores, which isless effective due to the significant differences in similarityscore distribution in image-text retrieval. Sebastian et al.propose Margin-MSE, which requires the same margin be-tween positive and negative sample scores for student andteacher models. Aditya et al. further propose M3SE,requiring the student and teacher models to have the samemargin between positive and the hardest negative sample.However, it only considers the hardest negative samples,limiting the knowledge that can be transferred. Moreover,the MSE loss is not coordinated with the contrastive learn-ing for dual-encoder training, resulting in interference in thelearning process. In contrast, our proposed CPRD methodconsiders the relative order among multiple hard negativesamples via contrastive learning, aligning with the originaltraining loss of dual-encoder.",
  ". Method": "In this section, we first introduce the architecture and train-ing objective of vanilla dual-encoder and cross-encoder, andexplain the reasons for the formation of their similarity dis-tribution characteristics in Sec 3.1. Then, we identify thesource of the performance gap between them, underscorethe importance of hard negatives ranking, and elaborateon our proposed Contrastive Partial Ranking Distillation(CPRD) method in Sec 3.2. Finally, we present the over-all training objective of dual-encoder in Sec 3.3.",
  ". Dual-Encoder and Cross-Encoder": "Dual-Encoder.The dual-encoder takes an image andits paired text as input, and extracts global visual andtextual representations vi and ti with separate encoders.The dot product of vi and ti is used to measure theimage-text similarity. During pre-training, following pre-vious works, we maintain two queues Qv and Qt topreserve the momentum features from current mini-batch{vj}Bj=1 and {tj}Bj=1 and previous iterations {vj}B+Nqj=B+1 and {tj}B+Nqj=B+1. For every image/text in the current mini-batch, its related text/image is deemed a positive sample,while the unmatched texts/images within the mini-batch andall samples in the previous iterations are considered as neg-atives. The contrastive learning (i.e., InfoNCE loss) is em-ployed to maximize the similarity between positive image-text pairs while minimizing the similarity between negativepairs, which is formulated as:",
  "(1)": "where is the temperature and B is the batch size. The totalloss for image-text contrastive learning is defined as:Lalign = (LI2T + LT2I)/2.(2)The characteristics of independent encoding for eachmodality in the dual-encoder and the objective of con-trastive learning lead to a moderate similarity distribution.Cross-Encoder. Cross-encoders typically start by utilizingvisual and textual encoders to extract detailed image andtext representations Vi and Tj from the input image-textpair. Then the obtained primary representations are fed intoa multi-modal encoder that employs self-attention or cross-attention to enable cross-modal interactions, thereby calcu-lating the similarity score pi,j for the input image-text pair. The image-text matching task serves as the train-ing objective for the cross-encoder, which aims at judgingwhether the input image-text pair is matched or not and isformulated as:Litm = 1",
  "(i,j)Syijlogpi,j + (1 yij)log(1 pi,j),": "where yij {0, 1} is the ground-truth label indicatingwhether the image-text pair is matched or not. S is theindex set of image-text pairs, obtained by random sam-pling or hard negative sampling strategies. The objective ofimage-text matching pushes pi,j to either 0 or 1, making thesimilarity distribution of cross-encoder more concentrated.The substantial difference between similarity distributionof dual-encoder and cross-encoder makes KL-divergencebased distillation less effective.",
  ". Contrastive Partial Ranking Distillation": "To achieve effective knowledge transfer from cross-encoderto dual-encoder, it is essential to first identify the source ofperformance gap between dual-encoder and cross-encoder.We test the performance of the pre-trained dual-encoder andcross-encoder from on MSCOCO dataset. Notethat the retrieval results of cross-encoder are obtained byre-ranking top-K retrieved items from dual-encoder. Theresults are presented in , and we observe that (1) AsK increases from 0 to 32, the retrieval performance signifi-cantly improves. (2) When more less-challenging negatives",
  "DE73.892.195.755.880.688.1486.2": "CE, K=073.892.195.755.880.688.1486.2CE, K=476.691.395.660.680.587.8492.4CE, K=1678.193.095.861.483.589.3501.1CE, K=3278.493.496.461.583.890.0503.4CE, K=12878.693.996.961.483.789.9504.4CE, K=25678.593.896.961.483.789.9504.2 are introduced, i.e., K continues to increase from 32 to 256,the performance saturates. It indicates that dual-encoder hasfiltered easy negatives and ranked most relevant items (in-cluding positives and hard negatives) into top 32, but lacksaccurate ranking of them. Furthermore, given the orginalloss of the dual-encoder (InfoNCE loss) encompasses theobjective of ranking positives ahead of hard negatives, thedistillation only needs to focus on the cross-encoders rank-ing knowledge of hard negatives. Based on the above findings, we propose ContrastivePartial Ranking Distillation (CPRD), a method that lever-ages contrastive learning to enforce consistency in the rank-ing of hard negatives between dual-encoders and cross-encoders. As shown in , in order to transfer knowl-edge about hard negatives ranking effectively, our CPRDmethod first mining sufficient hard negative samples withdual-encoders similarity score, then construct the rankingtarget of hard negatives with cross-encoder, and finally per-form partial ranking distillation via contrastive learning totransfer cross-modal matching knowledge effectively. 3.2.1Hard Negative MiningA straightforward approach to obtain hard negatives is se-lecting the most similar unmatched images/texts within themini-batch. However, the batch size must be sufficientlylarge to find adequately challenging hard negatives. There-fore, we expand the mining scope to the queues Qv andQt. Here, we demonstrate the process of image-to-text re-trieval, while the text-to-image retrieval is performed sym-metrically. For the i-th image, we use dual-encoder to com-pute its similarity scores with all negative texts in Qt, whichcontains samples from both current mini-batch and previousiterations. We then obtain the ranking result for the nega-tive texts, denoted as di = {dij}B+Nq1j=1, by sorting thesimilarity scores in the descending order, where:vi tdij > vi tdik, j, k [1, ..., B+Nq 1], j < k, (3)and indices of top-K hard negative texts is hi = {dij}Kj=1.",
  "Ranking Target Construction": "Then we construct the ranking target of top-K hard nega-tives with cross-encoder. To compute the similarity scoresof the top-K hard negatives with the cross-encoder, we alsoneed to maintain two additional queues Qvc and Qtc with sizeNc to preserve the image and text inputs from mini-batchand previous iterations for the cross-encoder. The similarityscore set Pi of the cross-encoder for all hard negative textsis denoted as Pi = {pi,dij|j [1, 2, ..., K]}. The cross-encoders ranking of hard negative texts ci = {cij}Kj=1 canthen be obtained by sorting Pi, where cij satisfies:pi,cij > pi,cik, j, k [1, ..., K], j < k.(4)However, the cross-encoders ranking ci can not be usedas target directly because not all information in the rank- ing ci is beneficial for dual-encoder. There are some low-ranked negative texts in ci that have small similarity scorepi,cij. We refer these texts as invalid hard negative texts{tcij|pi,cij < m}, where m is the hyper-parameter to dis-tinguish between valid and invalid hard negative texts. Therelative order among invalid hard negative texts does notcontain helpful knowledge, and enforcing the dual-encoderto learn such relative order may hurt the model perfor-mance. Therefore, we construct a partial ranking ci as thetarget, where the relative order between valid hard negativetexts is the same with ci but the relative order between in-valid ones is disregarded.It is worth noting that using the cross-encoder to calcu-late similarity scores online brings additional training costs.To address this issue, we can employ an offline approach.Specifically, we first use the dual-encoder to perform a fastretrieval on the entire pre-training dataset to obtain the top-N hard negative texts for each image. Then we computesimilarity scores of these negative image-text pairs withcross-encoder and store them in a similarity bank. Duringthe training iterations, for the negative text tdij, if pi,dij is inthe similarity bank, we load the pre-computed value; other-wise, we term tdij as invalid negative texts. We set N Kto ensure that most valid hard negatives are considered.",
  "Contrastive Partial Ranking Learning": "Given the ranking target ci from cross-encoder, we expectthe rankings from dual-encoder (hi) can be partial consis-tent with ci , which means only the relative order betweenvalid hard negatives should be maintained and the relativeorder between invalid hard negatives is ignored. The learn-ing objective is formulated as:minDist(hi, ci ),(5) where is the parameters of dual-encoder, Dist(, ) is themetric to measure the partial consistency between two rank-ing results. Optimizing this objective faces two challenges:(1) An appropriate metric to measure the partial consistencybetween rankings; (2) The sort operation used to computeci and hi is non-differentiable, hindering the end-to-endtraining. Both are hard to tackle.To implement effective partial ranking learning, wetransform the non-differentiable ranking mimicking into theoptimization of relative similarity scores. Specifically, foreach valid hard negative text in ci , we require its similaritywith the i-th image computed by dual-encoder to be largerthan the similarities of other negative texts ranked behind it.For invalid hard negative texts, we do not impose this con-straint, which avoids learning relative order between them.The new objective can be formulated as:vi tcij > vi tcik,",
  "where Ji denotes the index of the first invalid hard negativetext in the ci": "Although Equation 6 can already be used as a differen-tiable loss function for optimizing the dual-encoder, we findthat an inappropriate loss function type may interfere withthe coarse image-text alignment learned by dual-encoder.Formulating the ranking learning process in the form ofcontrastive learning is more coordinated with training ofdual-encoder and yields better results. Therefore, we pro-pose a new contrastive partial ranking loss based on the In-foNCE loss, which is formulated as:",
  ". Datasets": "Pre-training Datasets. We pre-train our model with CC4Mdataset, which contains 4 million images and 5.1 millioncaptions from four public image-text datasets, includingConceptual Captions 3M, SBU, MSCOCO andVisual Genome.Downstream Datasets. We conduct downstream image-text retrieval evaluation on two widely used datasets:MSCOCO and Flick30K. In addition, we validatethe effectiveness of our method on improving the rankingability with the CrissCrossed Caption dataset. The de-tails of these downstream datasets and the evaluation met-rics can be found in the supplemental material.",
  "None34.060.872.527.352.764.0311.3": "15M image-text pairs, and BLIP pre-trained on >200Mimage-text pairs. We use the AdamW optimizer witha weight decay of 0.02. The learning rate is warmed up to3e4 in the first 2000 iterations and decays to 1e5 follow-ing a cosine schedule. We pre-train the model for 20 epochswith a batch size of 512 on 8 NVIDIA V100 GPUs. We takethe image resolution of 256256 for pre-training and in-crease the image resolution to 384384 for fine-tuning. Themomentum coefficient for updating momentum encoders isset as 0.995. The number of hard negative K and the thresh-old m is set as 16 and 0.75. The queue size Nq and Ncare set as 57856 and 16384 respectively. The learnable tem-perature hyper-parameter for contrastive loss is initializedto 0.07. More implementation details can be found in thesupplementary materials.",
  ". Teach Dual-Encoder with Cross-Encoder": "In this section, we provide detailed steps to demonstratehow to use the cross-encoder as teacher for the dual-encodereffectively. All models in this section are pre-trained onCC3M using an image resolution of 224 224 with 8NVIDIA V100 GPUs, and then tested for zero-shot image-text retrieval performance on MSCOCO. The teacher modelis ALBEF pre-trained on CC3M and is kept frozen duringthe distillation process, while the student model is a dual-encoder trained from scratch.The effect of queue size Nc. The queue size Nc deter-mines the number of negative samples, which affects thenumber of valid hard negatives and the final distillation per-",
  "CPRD34.361.473.227.052.864.5313.2": "formance. Therefore, choosing a sufficiently large queuesize is crucial. As shown in , when we set the queuesize to 0, only the negative samples in the current batchare utilized for distillation, resulting in a limited numberof valid hard negatives and less valuable knowledge. Con-sequently, the performance improvement compared to thebaseline is relatively weak. When we increase the queuesize to 1024, the number of valid hard negatives increases,and the model performance achieve significant improve-ment. As the queue size further increases, the model per-formance gradually improves and tends to saturate. This isbecause, in the training dataset, the number of valid hardnegatives for a specific sample is limited. Further increas-ing the size of the queue leads to a saturation in the numberof valid hard negatives. We set the queue size to 16384 bydefault.The effect of hard negatives number K. Similarly, thechoice of K also affects the number of valid hard negativesand the effectiveness of knowledge distillation. As shownin , when set K to 0, we do not consider the rankingof any negative samples, causing the model to degrade tothe baseline model. Gradually increasing K leads to morevalid hard negative samples being considered, making thedistillation more effective and the model performance im-proves. When K is increased to 32, the model performancetends to saturate, as most of the valid hard negatives areconsidered. Moreover, increasing K results in heavier com-putation, thus we set K to 16 by default. It is worth notingthat we can calculate the similarity of hard negative pairsin an offline manner to avoid extra training costs. For eachimage/text in the training dataset, we use the cross-encoderto calculate the scores of the top-1000 hard negative samplepairs as mentioned in .2.2. As shown in ,the offline calculation approach achieves comparable per-formance with the online approach. It should be noted thatwhen using the offline approach, we directly search for valid . Comparative results for fine-tuned image-text retrieval results on the Flickr30K (1K) test set and MSCOCO (5K) test set. We makecomparisons with dual-encoder methods and cross-encoder methods. DE and CE represent the baseline dual-encoder and the teachercross-encoder. Our method improves the performance of baseline model significantly, surpasses previous state-of-the-art dual-encodermethods by a large margin, and achieves comparable performance with some cross-encoder methods while keeping the high retrievalefficiency. Higher R@K indicates better performance. PT Pairs: the number of image-text pairs for pre-training. is ensemble result oftwo models. models use 940M tagged images for visual encoder pre-training.",
  "CE15.2M95.999.8100.85.697.598.9577.777.694.397.260.784.390.5504.6": "hard negatives from the current batch and queue without theneed of setting K.The effect of valid hard negative threshold m.Thethreshold m controls the difficulty of negative samples forwhich dual-encoder needs to learn their relative order. Asshown in , when setting m = 0, all relative orderamong top-K hard negatives are learned by dual-encoder,and the performance declines significantly because dual-encoder is also enforced to learn the relative order betweeneasy negatives, which contains no valid knowledge and in-troduces interference for the dual-encoder. When settingm = 1, none of negative samples are considered in thepartial ranking learning, and thus the model degeneratesinto the baseline model.We also empirically test withm = 0.5, 0.75, 0.9. We find that the performance of allthree models is better than the baseline model, showing therobustness of our method toward the threshold m. The bestresult is achieved when m = 0.75. Therefore, we set thedefault value of m to 0.75. These experimental results vali-date our findings that only relative order between valid hardnegatives conveys valuable knowledge.Comparison with Different Distillation Methods. Basedon the optimal setting identified through the above experi-ments, we explore different distillation methods to transferknowledge from to dual-encoder. KL is the KL-divergenceloss which constrains the consistency between similarityscore distribution. M3SE requires that (1) the similar-ity margin between positive and hardest negative should besame for dual-encoder and cross-encoder, (2) similarity of",
  "CE71.991.195.258.781.788.5487.1": "other negative samples should be smaller than that of hard-est negative.As shown in , KL method achieves limited per-formance improvement due to the difference between simi-larity score distribution of dual-encoder and cross-encoder.M3SE even results in performance decline because the sim-ilarity range is different for dual-encoder and cross-encoder.To tackle this problem, we propose a modification R-M3SEto first re-scale similarity score with min-max normaliza-tion, and we observe the retrieval performance is improvedbut still worse than our method. To validate whether theform of loss function affects the distillation, we propose avariant of our method CPRDm, which adjusts the thresholdm adaptively to ensure there always has only one valid hardnegative. CPRDm has the highly-similar objective with R-M3SE but is implemented with infoNCE loss. It can be seen",
  ". Comparison with SOTA": "Image-Text Retrieval. We compare with state-of-the-artimage-text retrieval methods on Flickr30K and MSCOCOdatasets. The experimental results under fine-tuning set-ting are shown in . Compared with our dual-encoderbaseline, our proposed distillation method achieves signifi-cant improvement. On the Flickr30K dataset, we achievehigher performance by 1.7% and 2.3% on the R@1 ofimage-to-text and text-to-image retrieval. On the MSCOCOdataset, we also surpass the baseline by 3.0% and 3.6% onthe R@1 of two retrieval tasks. We also experiment with astronger cross-encoder trained with 15.2M data as teacher,the retrieval performance is further improved, showing thata stronger teacher can transfer more valid knowledge to stu-dent even with the same training data.Under a fair comparison experimental setting (excludingVSE and COOKIE as they use 940M tagged imagesfor visual-encoder pre-training), our method also outper-forms other dual-encoder methods by a large margin underall evaluation metrics. Specifically, compared with the cur-rent state-of-the-art dual-encoder method COTS with5.3M pre-training data, our method with similar data sizeachieves higher performance by 1.8% and 1.6% on the R@1of MSCOCO dataset. Moreover, our method with 5.1Mpre-training data outperforms COTS pre-trained on 15.3Mimage-text pairs. Furthermore, our method also achievescomparable performance with the cross-encoder methodsVinVL-base and ALBEF while much more efficient.The experimental results without fine-tuning are shownin . Our distillation method improves the perfor-mance of dual-encoder on all evaluation metrics. With asimilar pre-training data size, our method outperforms theCOTS by 2.0% and 1.3% on the R@1 of two retrievaltasks. Our method also outperforms CLIP and achievescomparable performance with ALIGN , which utilize78 and 356 pre-training data than our method respec-tively. Moreover, we also evaluate our method under postpre-train setting, where we initialize our dual-encoder withthe pre-trained weights from BLIP-129M and continue topre-train the model with image-text contrastive loss and ourdistillation loss. Our method can further improve the perfor-mance of the pre-trained model with only a small additionalcomputation cost (10k iterations).Image-Text Ranking.To further validate the effective-ness of our CPRD method on improving the ranking abil-ity of dual-encoder, we perform evaluation on the Criss-Crossed caption dataset for image-text ranking task. We re-port the Spearmans R bootstrapped correlation for image-text ranking task, i.e., SITS in , which reflects . Spearmans R Bootstrap Correlation (100) on Criss-crossed Captions dataset. FT indicates the model is fine-tunedon the MSCOCO dataset. STS, SIS and SITS represent the taskof semantic text similarity, semantic image similarity and semanticimage-text similarity.",
  "CE-FT69.41.1": "the consistency between human ranking and model ranking.Compared with vanilla dual-encoder, our CPRD methodachieves 2.5 improvement. The performance of our methodis further improved after fine-tuning on the MSCOCOdataset, and achieves comparable results with ALIGN. Inaddition, we also report the results on text-text similarityranking (STS) and image-image similarity ranking (SIS). Itis worth noting that although our CPRD method only distillsthe knowledge of cross-modal matching, it can also improvethe accuracy of measuring intra-modal similarity.",
  ". Conclusion": "In this work, we investigate how to effectively distillcross-modal knowledge from cross-encoder to dual-encoderfor image-text retrieval.We identify three key factorsand propose a novel Contrastive Partial Ranking Distilla-tion(CPRD) method. Our method focuses on learning rel-ative order among valid hard negatives while disregardingrelative order among invalid hard negatives and easy neg-atives. We implement our method with contrastive learn-ing, which aligns with training of dual-encoder and trans-fers knowledge effectively without disrupting the learningof image-text alignment. Comprehensive experiments onimage-text retrieval and ranking show the superiority of ourmethod compared to other distillation methods. Moreover,our method significantly improves retrieval and ranking ac-curacy of dual-encoder under various experiment settings.Acknowledgments This work is supported by the National Sci-ence and Technology Major Project (Grant No.2022ZD0118501),Beijing Natural Science Foundation (Grant No.JQ21017,L223003), the Natural Science Foundation of China (GrantNo.62222206, 62036011, 62192782, 62302501, 62202470,U2033210), The Project of Beijing Science and technology Com-mittee(Project No.Z231100005923046), Research Fund of ARCLab, Tencent PCG. Hangbo Bao,WenhuiWang,LiDong,Qiang Liu,Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som,Songhao Piao, and Furu Wei. Vlmo: Unified vision-languagepre-training with mixture-of-modality-experts. Advances inNeural Information Processing Systems, 35:3289732912,2022. 2 Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, andChanghu Wang. Learning the best pooling strategy for visualsemantic embedding. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages1578915798, 2021. 7",
  "Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and SanjaFidler. Vse++: Improving visual-semantic embeddings withhard negatives. 8": "Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,Jeff Dean, MarcAurelio Ranzato, and Tomas Mikolov. De-vise: A deep visual-semantic embedding model. Advancesin neural information processing systems, 26, 2013. 11 Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. Advances in NeuralInformation Processing Systems, 33:66166628, 2020. 7 Sebastian Hofstatter, Sophia Althammer, Michael Schroder,Mete Sertkan, and Allan Hanbury. Improving efficient neu-ral ranking models with cross-architecture knowledge distil-lation. arXiv preprint arXiv:2010.02666, 2020. 3",
  "Connecting language and vision using crowdsourced denseimage annotations. International journal of computer vision,123:3273, 2017. 5": "Jie Lei, Xinlei Chen, Ning Zhang, Mengjiao Wang, MohitBansal, Tamara L Berg, and Licheng Yu. Loopitr: Com-bining dual and cross encoder architectures for image-textretrieval. arXiv preprint arXiv:2203.05465, 2022. 3 Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation. Advances in neural infor-mation processing systems, 34:96949705, 2021. 2, 3, 5, 7,8, 11 Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for uni-fied vision-language understanding and generation. In In-ternational Conference on Machine Learning, pages 1288812900. PMLR, 2022. 2, 3, 6 Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and YunFu. Visual semantic reasoning for image-text matching. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 46544662, 2019. 8",
  "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,and Kai-Wei Chang.Visualbert:A simple and perfor-mant baseline for vision and language.arXiv preprintarXiv:1908.03557, 2019. 2": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, XiaoweiHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, FuruWei, et al.Oscar: Object-semantics aligned pre-trainingfor vision-language tasks. In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part XXX 16, pages 121137. Springer,2020. 2, 7 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 3, 5, 11",
  "Ilya Loshchilov and Frank Hutter.Decoupled weight de-cay regularization. In International Conference on LearningRepresentations, 2018. 6": "Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu,and Ji-Rong Wen. Cots: Collaborative two-stream vision-language pre-training model for cross-modal retrieval.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1569215701, 2022. 1,2, 7, 8, 11 Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:Pretraining task-agnostic visiolinguistic representations forvision-and-language tasks. Advances in neural informationprocessing systems, 32, 2019. 2 Aditya Menon, Sadeep Jayasumana, Ankit Singh Rawat, Se-ungyeon Kim, Sashank Reddi, and Sanjiv Kumar. In defenseof dual-encoders for neural ranking. In International Con-ference on Machine Learning, pages 1537615400. PMLR,2022. 3, 7 Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, JosefSivic, and Andrew Zisserman. Thinking fast and slow: Effi-cient text-to-visual retrieval with transformers. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 98269836, 2021. 2, 3",
  "Vicente Ordonez,Girish Kulkarni,and Tamara Berg.Im2text: Describing images using 1 million captioned pho-tographs. Advances in neural information processing sys-tems, 24, 2011. 5": "Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Wa-ters, and Yinfei Yang. Crisscrossed captions: Extended in-tramodal and intermodal semantic similarity judgments forms-coco. In Proceedings of the 16th Conference of the Euro-pean Chapter of the Association for Computational Linguis-tics: Main Volume, pages 28552870, 2021. 5, 8, 11 Bryan A Plummer, Liwei Wang, Chris M Cervantes,Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-nik. Flickr30k entities: Collecting region-to-phrase corre-spondences for richer image-to-sentence models.In Pro-ceedings of the IEEE international conference on computervision, pages 26412649, 2015. 5 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 1, 2, 7, 8 Sashank Reddi, Rama Kumar Pasumarthi, Aditya Menon,Ankit Singh Rawat, Felix Yu, Seungyeon Kim, Andreas Veit,and Sanjiv Kumar. Rankdistil: Knowledge distillation forranking.In International Conference on Artificial Intelli-gence and Statistics, pages 23682376. PMLR, 2021. 3 Piyush Sharma, Nan Ding, Sebastian Goodman, and RaduSoricut. Conceptual captions: A cleaned, hypernymed, im-age alt-text dataset for automatic image captioning. In Pro-ceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages25562565, 2018. 5 Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, YuweiFang, and Jingjing Liu. Lightningdot: Pre-training visual-semantic embeddings for real-time image-text retrieval. InProceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics:Human Language Technologies, pages 982997, 2021. 2, 7 Zekun Wang, Wenhui Wang, Haichao Zhu, Ming Liu, BingQin, and Furu Wei. Distilled dual-encoder model for vision-language understanding. In Proceedings of the 2022 Confer-ence on Empirical Methods in Natural Language Processing,pages 89018913, 2022. 2 Keyu Wen, Jin Xia, Yuanyuan Huang, Linyang Li, JiayanXu, and Jie Shao. Cookie: Contrastive cross-modal knowl-edge sharing pre-training for vision-language representation.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 22082217, 2021. 1, 2, 7",
  "Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vi-sion language pre-training: Aligning texts with visual con-cepts. arXiv preprint arXiv:2111.08276, 2021. 2": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.Vinvl: Revisiting visual representations in vision-languagemodels.In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 55795588,2021. 2, 7 In this supplementary materials, we further explain the differ-ences and connections between score distribution distillation andranking distillation, in order to analyze the advantages of rank-ing distillation in the process of distilling knowledge from cross-encoder to dual-encoder. We also elaborate on (1) details aboutpre-training datasets, downstream datasets, and evaluation met-rics of downstream tasks; (2) Visualizations about image-to-textretrieval and text-to-image retrieval. (3) More ablation study forCPRD loss.",
  ". (a) KL-divergence-based distillation targets from cross-encoder. (b) Predicted similarity scores from student dual-encoderafter softmax operation": "Scoredistributiondistillation(i.e.,KL-divergence-basedknowledge distillation) requires the student and teacher modelshave the same score distribution over multiple samples. Upon fur-ther analysis, we find that score distribution can be interpreted asranking distillation with additional constraints. As shown in Fig-ure 3, given an image and multiple texts ti, i {1, 2, , 5}, wecompute their similarity pi with cross-encoder and construct dis-tillation target qi by applying softmax operation over these scores.A hyper-parameter is employed to control the sharpness of dis-tillation target. Without loss of generality, we assume that:",
  "s1 s2 > s2 s3,(15)": "where s1, s2, s3 is the similarity scores (after softmax) from stu-dent model. Note that the objective of Equation 14 is the same asranking distillation. However, the additional constraint of Equa-tion 15 may interfere with the learning of image-text alignmentdue to the significant difference between the similarity distribu-tions of dual-encoder and cross-encoder, which is validated by ourexperimental results.",
  "image113K100K2.81M825Ktext567K769K2.81M825K": "Pre-training datasets. We show the statistics of the images andtexts of pre-training datasets in the MSCOCO. MSCOCO is a large image-text dataset of 123Kimages, where each image has 5 human-annotated captions. Fol-lowing , we adopt the Karpathy split of MSCOCO,where 5K/5K/113K images are used for testing, validation andtraining respectively.Flickr30K. Flickr30K contains 31K images and 159K captions.Each image is usually annotated with 5 captions. Following ,we 1K/1K/29K images for testing, validation and training respec-tively.Crisscrossed Captions. Crisscrossed Captions dataset is anextension of MS-COCO dataset with human semantic similarityjudgments for intra- and inter- modality pairs.It contains hu-man ratings for 267,095 pairs (derived from 1,335,475 indepen-dent judgments), a massive extension in scale and detail to the 50koriginal binary pairings.",
  "C. Evaluation Metrics": "Retrieval. We report the widely-used R@k (k=1,5,10) for cross-modal retrieval, which is the proportion of matched samples foundin the top-k retrieved results. We also report R@S to reveal theoverall performance, which is defined as the sum of R@k met-rics at k={1,5,10} of both image-to-text and text-to-image retrievaltasks.Ranking. We report the Spearmans bootstrap correlation follow-ing to assess whether a model ranks pairs similarly tohuman raters. For each correlation estimate, we sample half of thequeries (to increase diversity across samples) and for each selectedquery, we choose one of the items for which Crisscross captiondataset supplies a paired rating. We compute Spearmans correla-tion between the ground-truth scores and the model scores for theselected pairs. The final correlation is the average over 1000 ofthese bootstrap samples.",
  "D. Visualizations": "Image-to-text Retrieval. We show image-to-text retrieval resultson the MSCOCO test set in the . We can observe that: (1)Our model has a more precise perception of detailed objects andactions in the image, e.g., the baseline model erroneously identifieswhite cap, run from the (a), while our method accurately de- termines that it is a man hitting a ball with a racket; (2) Our modelcorrectly recognizes detailed relation nuzzling and leaning inthe (b), while the baseline model fails to achieve such recognition;(3) Our model achieves better cross-modal matching for rare con-cepts, as shown in (c), where our model recognizes the coconutand aligns it with the corresponding text.Text-to-image Retrieval. The text-to-image results are shown in. It can be seen that: (1) Our model perceives abstractadjectives more accurately, e.g., a modern train in (a); (2) Ourmodel understands local text semantics in the midst of repairsbetter and find the image that contains repair tools in (b), but thebaseline model only finds the images with kitchen and cabi-nets; (3) Our model has better understanding on the number, e.g.,our model find the image with only two white vans accurately in(c).",
  "-1653.161.350.760.017-3217.022.816.821.733-4810.114.715.712.849-647.110.023.127.4": "The effect of ranking mimicking.To validate whether ourmethod mimics the ranking of cross-encoder, we use dual-encoderto retrieve the top 64 texts/images given each image/text ofMSCOCO test dataset. Then we re-rank the retrieved texts/imagesin the different rank interval (i.e., 1-16, 17-32, 33-48, 49-64)with cross-encoder and compute the spearmans rank correlation.As shown in , applying our CPRD method on the dual-encoder improves the rank correlation on most of the rank in- tervals, validating the effectiveness of our method in mimickingcross-encoders ranking. It is worth noting that the rank correla-tion degrades for top 33-48 retrieved images given texts, but therelative order between these lower-ranked samples is not impor-tant and our method is designed to disregard this order.",
  "None32.059.471.524.449.561.0297.8Lij31.359.771.123.948.159.5293.6Lij34.361.473.227.052.864.5313.2": "The variant of our proposed contrastive partial ranking distil-lation loss. Here, we want to explore Does it important to con-strain that valid hard negatives have higher score than easy nega-tives in our proposed loss?. Without such constraint, the scoresof hard negatives ranked lower are trained to have smaller simi-larity with CPRD, and might even be lower than those easy neg-atives, which have a negative impact on the performance of thedual-encoder. We test the variant loss Lij which does not have theabove constraint. The original Lij and Lij are formulated as:",
  "k=jexp(vi tcik/)": "As shown in , Lij is not as good as Lij, and it even hasa negative impact on the baseline model, validating the importanceof ensuring that valid hard negatives have higher score than easynegatives in the distillation loss.The choices between online hard negatives similarity cal-culation and offline approach. As mentioned in Sec 3.2.2, usingthe cross-encoder to calculate similarity scores online brings addi-tional training costs. To reduce the training cost, we can calculatethe similarity of hard negative pairs in an offline manner. It isworth noting that, compared to online method, the offline compu-tation for one teacher is heavier due to larger candidate numberbut only occurs once. Offline method is thus more efficient whenreusing ranking targets (e.g., training multiple students with oneteacher). Otherwise (e.g., training a student with varying teach-ers), online method is more efficient. The method choice dependson the scenarios."
}