{
  "Abstract": "Reducing computational costs is an important issue fordevelopment of embedded systems. Binary-weight NeuralNetworks (BNNs), in which weights are binarized and ac-tivations are quantized, are employed to reduce computa-tional costs of various kinds of applications. In this paper, adesign methodology of hardware architecture for inferenceengines is proposed to handle modern BNNs with two oper-ation modes. Multiply-Accumulate (MAC) operations canbe simplied by replacing multiply operations with bitwiseoperations. The proposed method can effectively reduce thegate count of inference engines by removing a part of com-putational costs from the hardware system. The architec-ture of MAC operations can calculate the inference resultsof BNNs efciently with only 52% of hardware costs com-pared with the related works. To show that the inferenceengine can handle practical applications, two lightweightnetworks which combine the backbones of SegNeXt and thedecoder of SparseInst for instance segmentation are alsoproposed. The output results of the lightweight networksare computed using only bitwise operations and add oper-ations. The proposed inference engine has lower hardwarecosts than related works. The experimental results showthat the proposed inference engine can handle the proposedinstance-segmentation networks and achieves higher accu-racy than YOLACT on the Person category although themodel size is 77.7 smaller compared with YOLACT.",
  "Deep neural networks, including convolutional neuralnetworks (CNN) and vision transformers (ViT) , have": "received considerable attention in recent years.Variouskinds of networks have already been applied to differentcomputer vision tasks. Not only can these algorithms beapplied to face detection and object detection ,they can also be employed in other dense-prediction taskssuch as semantic segmentation and instance segmenta-tion . For mobile devices and embedded systems with limitedcomputational resources, it is necessary to reduce compu-tational costs and power consumption. In modern neuralnetworks, Multiply-Accumulate (MAC) operations have amuch higher ratio than any other operations, such as max-pooling or up-sampling operations. Many kinds of quanti-zation algorithms and low-bit networks areproposed to simplify the MAC operations, and many kindsof hardware architectures are proposed to handle mixed-precision networks with low bit widths and binary-weight neural networks (BNNs) with low computationalcosts . However, the accuracy mightdecrease and fail to satisfy the requirement of some appli-cations, such as instance segmentation, when the weightsof networks are binarized or quantized to low bits. To de-sign a suitable system for embedded vision, it is necessaryto seek the balance between accuracy and bit widths in thequantized networks. In this paper, we focus on both hardware implementa-tion methods and algorithm design approaches of BNNs, inwhich activations are quantized and weights are binarized.Two lightweight networks for instance segmentation anda new hardware architecture of inference engine are pro-posed. The contribution of this paper is twofold. First, wepropose a new design methodology to reduce the gate countof the inference engine by removing a part of computational costs from the hardware system. The methodology can beapplied to different variations of BNNs, and no multiply op-erations are required. The values of binary weights can beeither {0, 1} or 1. Second, we show that the proposed in-ference engine can handle instance segmentation algorithmswith only 9.8% of computational costs compared with therelated works. The proposed algorithm and hardware canhandle dense-prediction tasks with acceptable accuracy.The paper is organized as follows. In Sec. 2, the relatedworks are introduced. The architecture of the proposed in-ference engine for BNNs is shown in Sec. 3. The experi-mental results are discussed in Sec. 4. The conclusions aregiven in Sec. 5.",
  ". Binary-Weight Neural Networks (BNNs)": "Quantization is an important topic for hardware imple-mentation of neural networks. Many algorithms are pro-posed to quantize both activations and weights of CNN.Choi et al. propose a quantization scheme for activa-tions during training. Sambhav et al. propose a methodof training quantization thresholds, where the quantizersare suitable for hardware implementation. Gao et al. propose a systematic approach to transform the parametersof low-bit quantized networks into multiple thresholds forhardware implementation.A number of researchers have implemented diverse ap-proaches to design BNNs . BinaryConnect is a methodwhich focuses on training networks with binary weightsduring forward and backward propagations . To furtherreduce the computational costs, weights or feature maps(activations) can be quantized to 1 bit. XNOR-Net is anetwork where weights and activations are binarized, so thatadd operations can be replaced by logical operations, suchas Exclusive-OR (XOR) and Exclusive-NOR (XNOR)operations. However, when BNNs are employed in somepractical applications, the accuracy decreases because thefeatures extracted by the networks with binary weights andbinary activations are not sufcient. Some algorithms areproposed to improve the training algorithm and to increasethe accuracy .Most of BNNs can be used to handle object detectionproblems and image classications problems to achieve ac-ceptable accuracy.However, there are few research pa-pers showing that BNNs can be applied to difcult dense-prediction problems, such as instance segmentation. Bolyaet al. propose a real-time instance segmentation algo-rithm, YOLACT, which includes a feature pyramid archi-tecture.Cheng et al. propose a fully convolutionalframework for real-time instance segmentation, SparseInst,which contains a sparse set of instance activation maps.However, these real-time networks are not quantized andmight not be suitable for embedded devices with limited re-sources because a large number of processing elements are",
  ". Proposed Inference Engine": "The proposed inference engine, which can handle BNNsefciently, is introduced in this section.We focus onthe networks where the activations are multi-bit, and theweights are binary. There are I activations, and each ac-tivation has J bits, where J is set to 8 in this work. Each bitof the ith activation is represented as ai,j, where i denotesthe index of activations, and j denotes the index of bits ofthe ith activation. One weight has only 1 bit, and wi or wirepresents the ith weight. shows the two kinds ofoperations to handle modern BNNs and the correspondingoutput results. Either of the two operations is selected ac-cording to the operation mode m, where m {0, 1}. Therst mode handles convolutions where wi {1, 1}, andthe second mode handles convolutions or matrix multipli-cations where wi {0, 1}. Multiply operations canbe removed from all MAC operations because only binaryweights are employed in the BNNs. The output of MACoperations is shown in Eq. 1.",
  "I1i=0 aiwi + otherwise,(1)": "where ai,j {0, 1}, wi {1, 1}, and wi {0, 1}.The operations of the rst mode are shown in (a).The concept of XNOR-Net is adopted to handle BNNswith binary weights and binary activations, where both bi-nary weights and binary activations are represented by 1.Since there are 3 possible values in the results, some addi-tional operations are included in order to apply the XNORoperations.The output of MAC operations is shown in",
  "It is shown that the convolution includes XNOR-accumulateoperations and add operations. The variable is a correc-tion term, which can be computed according to the weights,": "wi and the number of activations, I. Note that does not re-quire any information related to the activations, and it doesnot have to be computed during the inference process. Thecomputations of BNNs can be simplied using the quanti-zation algorithms of the IFQ-Net after substituting thebias term with ( + ).The operations of the second mode are shown in Ta-ble 1(b). Different from the rst operation, there are only2 possible values in the results. The output of the MACoperations is shown in Eq. 6.",
  "+ ,(6)": "where ai,j {0, 1} and wi {0, 1}. Both activationsand weights have the same values as their original values.It is shown that the MAC operations include only AND-accumulate operations and add operations. No correctionterms are required in this mode. shows the architecture of the dedicated infer-ence engine, which includes 3 main components and thememory. The main components are the bitwise operationand accumulation array, the adder array, and the quan-tization and activation unit. The bitwise operation and ac-cumulation array computes the results of Eq. 2 and Eq. 6.It includes multiple bitwise operation units. The architec-ture is shown in , where each bitwise operation unitincludes J logical operation units to calculate the resultsof XNOR or AND operations. When the rst operationmode is enabled, m is set to 0, and the circuit is equivalentto multiple XNOR gates for the input weights and activa-tions. When the second operation mode is enabled, m is setto 1, and the circuit is equivalent to multiple AND gates forthe input weights and activations.The adder array includes parallel adders to computethe sum of the MAC results, the bias term , and the cor-rection term . The correction term does not have to becomputed during the inference process. Since it does notrequire any information related to the activations, it can becomputed using any other processors and stored as param-eters in advance. This approach can effectively reduce thegate count of the inference engine. Removing the computa-tions only related to weights from embedded devices is oneof the key ideas of this work. Besides, the adder array cancompute the results of element-wise add operations, whichare frequently employed in modern neural networks. Thequantization and activation unit quantizes the output ofthe adder array and computes the pooling results accord-ing to the network architecture.",
  ". BNNs for Instance Segmentation": "To evaluate the performance of BNNs, two lightweightnetworks for instance segmentation, MSCAN-SparseInstBNN and ConvNeXtV2-SparseInst BNN, are designed. In-stance segmentation is a representative task of dense pre-diction, in which pixel-level labeling is required. It is notsimple to obtain high accuracy after binarizing the weightsor activations in dense-prediction networks. To reduce thecomputational costs, the decoder of SparseInst and thebackbone networks of SegNeXt and ConvNeXtV2 are combined. Besides, some regular convolutions are re-placed by the partial convolutions in FasterNet to reducecomputational costs of the proposed networks. The net-works are trained using GeForce GTX TITAN X with 12GBmemory. Microsoft COCO is employed for trainingand evaluation, and the optimization method is AdamW.The techniques in HWGQ , PACT , LSQ , andLSQ+ are used for quantization. The training parame-ters are shown in . The number of training epochsis 16, and the batch size is 16. To increase the accuracy ofBNNs, the training algorithm in PROFIT , which pro-gressively freezes the most sensitive layer of the network, isalso employed.The architecture of one of the proposed networks,MSCAN-SparseInst BNN, is shown in . BConv de-notes the convolutions with binary weights, and PConv rep-resents partial convolutions in FasterNet .The back-bone network is modied from MSCAN-Tiny, which is in-cluded in the architecture of SegNeXt . It contains 3down-sampling layers and 4 stages, which are followed by4 batch normalization (BN) layers, and the number of build-ing blocks in each stage is different. The architecture ofthe building block in each stage in shown in , whereBDWConv denotes the depthwise convolutions with binaryweights. The rectangular lters in the original MSCAN areremoved to simplify the network. The activations from 3BN layers are sent to the FPN-Encoder, which is modiedfrom the architecture of SparseInst . The FPN-Encodercontains 4 nearest-neighbor up-sampling layers, 6 convolu-tion layers, and 1 coordinate position embedding layer. Theinput of the coordinate position embedding layer is 126, andthe output is 128 channels since 2 coordinate channels (xand y) are added into the layer. The activations from the co-ordinate position embedding layer are sent to the decoder,which is modied from the architecture of SparseInst .The decoder contains 6 convolution layers, and 2 Batch-Matrix-Matrix (BMM) layers. The inputs of 2 BMM lay-ers are binarized to {0, 1} and 1, respectively. To com-pare different network architectures, a network architecturemodied from ConvNeXtV2-femto, which is the backbone",
  "network in ConvNeXtV2 , is also evaluated": "In the two lightweight networks, the weights in the con-volutions layers are all quantized to 1, and the rst mode(m = 0) of the inference engine can be applied. In the de-coder of the networks, there are some matrix operations inBMM layers employed to increase the accuracy. Onematrix multiplication has 2 input activations. One of themis quantized to {0, 1} or 1, so that the MAC operations canbe handled with the two modes (m = 0 or m = 1) of theproposed hardware. The proposed networks, ConvNeXtV2-SparseInst and MSCAN-SparseInst, are compared withYOLACT . The model sizes and the computational costsare shown in . The full-precision (oating-point)version of ConvNeXtV2-SparseInst is 9.6 smaller thanYOLACT when the bit width of weights is 8 bits, and the binary version of ConvNeXtV2-SparseInst (ConvNeXtV2-SparseInst BNN) is 77.7 smaller than YOLACT becausethe weights are reduced from 8 bits to 1 bit. Compared withYOLACT, ConvNeXtV2-SparseInst BNN has only 9.8%of MACs, and the weights are binarized. The results ofMSCAN-SparseInst and MSCAN-SparseInst BNN are alsosimilar. The computational costs and the model sizes ofthe two networks are effectively reduced compared withYOLACT although they have more layers than YOLACT. The accuracy is shown in . The mean averageprecision (MAP) of each of the three categories, Person,Car, and Bus, is evaluated. Some examples of instancesegmentation results are shown in , where it is ob-served that the two lightweight networks have higher detec-tion rates on the Person category than YOLACT . Two",
  ". Architecture of the building block in each stage ofMSCAN-SparseInst BNN": "different versions of binary networks are designed for thetwo proposed networks. The BMM layer in the decoder ofSparseInst includes two oating-point input activations.In order to make them compatible with the proposed hard-ware, one of the input activations is quantized to 1 bit. Also,the normalization operations in the Instance ActivationMaps (IAM) of SparseInst are removed to simplify theoperations. The results show that both MSCAN-SparseInstBNN and ConvNeXtV2-SparseInst achieve higher accu-",
  "Activations are quantized to J bits, and weights are quantized to 1 bit": "racy than YOLACT on the Person category after bina-rizing the input of BMM operations. Moreover, MSCAN-SparseInst BNN-A and ConvNeXtV2-SparseInst BNN-Aare designed for ablation study. MSCAN-SparseInst BNN-A and MSCAN-SparseInst BNN have the same network ar-chitecture, but the values of the inputs to the BMM layers inMSCAN-SparseInst BNN-A are quantized to only 1, not{0, 1} and 1. MSCAN-SparseInst BNN has higher ac-curacy on the Person category (+18.6%) than MSCAN-SparseInst BNN-A. It means that the functions to supporttwo kinds of binary weights effectively increase the accu-racy of instance segmentation. The results of ConvNeXtV2-SparseInst BNN also show the importance of the input val-ues of binary operations.",
  "Modied XNOR-based multiplier with additional logics": "supported by the related works are 1.The architectures implemented with XNOR andXOR operations are designed to handle 1-bit weightsand 1-bit activations.To compare them with this work,some additional logic operations and adders are added intothe XNOR-based multiplier to support multi-bitweights. The modied architecture is shown in (a).The hardware architecture implemented with select opera-tions is designed to handle multi-bit weights and 1-bitactivations. The architecture, which is shown in (b),can be directly compared with this work. The proposedwork is designed to handle multi-bit activations and 1-bitweights, and the values of binary weights can be either",
  "{0, 1} or 1": "The relation between the gate count of the architecturesand the bit width of activations is shown in . The mod-ules of the related works are re-implemented to t the pro-posed hardware architecture and synthesized with the reg-ular threshold voltage (RVT) device model in the ASAP7library . The results in (a) show that the mod-ied XNOR-based multiplier has similar gate counts withthe selector-based multiplier , and the trend does notchange with the bit width of activations.The results in(b) show that, compared with the related works, thegate count of this work is reduced to 52% of the modiedXNOR-based multiplier and 59% of the selector-based mul-",
  ". Hardware architecture of (a) the XNOR-based multi-plier with additional logics, and (b) the selector-basedmultiplier": "tiplier when the bit width of activations is 8 and theoperating frequency is 2GHz. The higher the operating fre-quency, the more reduction of gate counts. The comparisonof the related works and this work is summarized in .The proposed work has lower costs than the related worksbecause the computations of the correction term shown inEq. 5 are removed from the MAC operations. Since thecorrection term is merged into the bias term, no additionaloperations are required in the inference process. Moreover,this work can support two kinds of binary weights, {0, 1}and 1, which increase the accuracy of instance segmenta-tion as shown in Sec. 4.1.",
  ". Conclusion": "A hardware architecture and a design methodology ofdedicated inference engines for BNNs are proposed. Theproposed inference engine can handle instance segmenta-tion with only bitwise operations and add operations. Thearchitecture of MAC operations can calculate the inferenceresults of BNNs efciently with only 52% of hardware costscompared with the related works. A part of computationcosts can be removed from the system because they are notdependent on activation maps and can be performed in ad-vance using any other processors.In addition, two lightweight networks for instance seg-mentation and hardware architecture of inference engineare proposed. The experimental results show that the pro-",
  "This paper is based on results obtained from a project,JPNP23015, commissioned by the New Energy and Indus-trial Technology Development Organization (NEDO)": "Ankur Agrawal, Sae Kyu Lee, Joel Silberman, MatthewZiegler, Mingu Kang, Swagath Venkataramani, NianzhengCao, Bruce Fleischer, Michael Guillorn, Matthew Cohen,Silvia Mueller, Jinwook Oh, Martin Lutz, Jinwook Jung,Siyu Koswatta, Ching Zhou, Vidhi Zalani, James Bonanno, Robert Casatuta, Chia-Yu Chen, Jungwook Choi, HowardHaynie, Alyssa Herbert, Radhika Jain, Monodeep Kar, Kyu-Hyoun Kim, Yulong Li, Zhibin Ren, Scot Rider, MarcelSchaal, Kerstin Schelm, Michael Scheuermann, Xiao Sun,Hung Tran, Naigang Wang, Wei Wang, Xin Zhang, VinayShah, Brian Curran, Vijayalakshmi Srinivasan, Pong-Fei Lu,Sunil Shukla, Leland Chang, and Kailash Gopalakrishnan.A 7nm 4-core AI chip with 25.6TFLOPS hybrid FP8 train-ing, 102.4TOPS INT4 inference and workload-aware throt-tling. In Proceedings of IEEE International Solid-State Cir-cuits Conference, pages 144146, Feb. 2021. YashBhalgat,JinwonLee,MarkusNagel,TijmenBlankevoort, and Nojun Kwak. LSQ+: Improving low-bitquantization through learnable offsets and better initializa-tion, 2020. CoRR, abs/2004.09576. Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.YOLACT: real-time instance segmentation, 2019.CoRR,abs/1904.02689. Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconce-los. Deep learning with low precision by half-wave Gaussianquantization, 2017. CoRR, abs/1702.00953. Jierun Chen, Shiu-hong Kao, Hao He, Weipeng Zhuo, SongWen, Chul-Ho Lee, and S.-H. Gary Chan. Run, dont walk:Chasing higher FLOPS for faster neural networks, 2023.CoRR, abs/2303.03667v3. Tse-Wei Chen, Motoki Yoshinaga, Hongxing Gao, Wei Tao,Dongchao Wen, Junjie Liu, Kinya Osa, and Masami Kato.Condensation-Net: memory-efcient network architecturewith cross-channel pooling layers and virtual feature maps,2021. CoRR, abs/2104.14124. Tianheng Cheng, Xinggang Wang, Shaoyu Chen, WenqiangZhang, Qian Zhang, Chang Huang, Zhaoxiang Zhang, andWenyu Liu. Sparse instance activation for real-time instancesegmentation, 2022. CoRR, abs/2203.12827v1. Jungwook Choi, Zhuo Wang, Swagath Venkataramani,Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and KailashGopalakrishnan.PACT: parameterized clipping acti-vation for quantized neural networks, 2018.CoRR,abs/1805.06085v2. Ching-Che Chung, Yu-Pei Liang, Ya-Ching Chang, andChen-Ming Chang.A binary weight convolutional neuralnetwork hardware accelerator for analysis faults of the CNCmachinery on FPGA. In Proceedings of International VLSISymposium on Technology, Systems and Applications (VLSI-TSA/VLSI-DAT), Apr. 2023. Matthieu Courbariaux, Yoshua Bengio, and Jean-PierreDavid.BinaryConnect:training deep neural networkswith binary weights during propagations, 2015.CoRR,abs/1511.00363. Gabriela Csurka, Riccardo Volpi, and Boris Chidlovskii. Se-mantic image segmentation: Two decades of research, 2023.CoRR, abs/2302.06378. Jiankang Deng, Jia Guo, Yuxiang Zhou, Jinke Yu, Irene Kot-sia, and Stefanos Zafeiriou. RetinaFace: Single-stage denseface localisation in the wild, 2019. CoRR, abs/1905.00641. StevenK.Esser,JeffreyL.McKinstry,DeepikaBablani,RathinakumarAppuswamy,andDharmen- dra S. Modha. Learned step size quantization, 2019. CoRR,abs/1902.08153. Hongxing Gao, Wei Tao, Dongchao Wen, Tse-Wei Chen,Kinya Osa, and Masami Kato. IFQ-Net: integrated xed-point quantization networks for embedded vision, 2019.CoRR, abs/1911.08076. Lunyi Guo, Shining Mu, Yijie Deng, Chaofan Shi, BoYan, and Zhuoling Xiao. Efcient binary weight convolu-tional network accelerator for speech recognition. Sensors,23(3):1530, May 2023. Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu,Ming-Ming Cheng, and Shi-Min Hu.SegNeXt: rethink-ing convolutional attention design for semantic segmenta-tion, 2022. CoRR, abs/2209.08575. Peng Guo, Hong Ma, Ruizhi Chen, and Donglin Wang. Ahigh-efciency FPGA-based accelerator for binarized neu-ral network. Journal of Circuits, Systems and Computers,28(supp01), Apr. 2019. Yuwei Hu, Jidong Zhai, Dinghua Li, Yifan Gong, YuhaoZhu, Wei Liu, Lei Su, and Jiangming Jin. BitFlow: Exploit-ing vector parallelism for binary neural networks on CPU. InProceedings of International Parallel and Distributed Pro-cessing Symposium, pages 244253, May 2018. Sambhav R. Jain, Albert Gural, Michael Wu, and Chris H.Dick. Trained quantization thresholds for accurate and ef-cient xed-point inference of deep neural networks, 2020.CoRR, abs/1903.08066v3. Tsung-Yi Lin, Michael Maire, Serge Belongie, LubomirBourdev, Ross Girshick, James Hays, Pietro Perona, DevaRamanan, C. Lawrence Zitnick, and Piotr Dollar.Mi-crosoft COCO: common objects in context, 2014. CoRR,abs/1405.0312. Eunhyeok Park and Sungjoo Yoo. PROFIT: a novel train-ing method for sub-4-bit MobileNet models, 2020. CoRR,abs/2008.04693. Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai,Jingkuan Song, and Nicu Sebe. Binary neural networks: Asurvey, 2020. CoRR, abs/2004.03333. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.Vision transformers for dense prediction, 2021.CoRR,abs/2103.13413v1. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,and Ali Farhadi. XNOR-Net: ImageNet classication us-ing binary convolutional neural networks, 2016.CoRR,abs/1603.05279v4. Juan Terven and Diana Cordova-Esparza. A comprehensivereview of YOLO architectures in computer vision: FromYOLOv1 to YOLOv8 and YOLO-NAS, 2024.CoRR,abs/2304.00501v7. Tsung-Han Tsai and Yuan-Chen Ho.A CNN acceleratoron FPGA using binary weight networks.In Proceedingsof IEEE International Conference on Consumer Electronics(ICCE), pages 12, Sept. 2020. Vinay Vashishtha, Manoj Vangala, and Lawrence T. Clark.ASAP7 predictive design kit development and cell designtechnology co-optimization: Invited paper. In Proceedingsof The International Conference on Computer-Aided Design(ICCAD), pages 992998, Nov. 2017. Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, XinleiChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-vNeXt V2: co-designing and scaling ConvNets with maskedautoencoders, 2023. CoRR, abs/2301.00808. Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, LingShao, Yue Gao, Yonghong Tian, and Rongrong Ji. ReCU:reviving the dead weights in binary neural networks, 2021.CoRR, abs/2103.12369v2. Haruyoshi Yonekawa and Hiroki Nakahara. On-chip mem-ory based binarized convolutional deep neural network ap-plying batch normalization free technique on an FPGA.In Proceedings of IEEE International Parallel and Dis-tributed Processing Symposium Workshops, pages 98105,May 2017. Shien Zhu, Luan H. K. Duong, and Weichen Liu. XOR-Net:An efcient computation pipeline for binary neural networkinference on edge devices. In Proceedings of InternationalConference on Parallel and Distributed Systems (ICPADS),pages 124131, Feb. 2020."
}