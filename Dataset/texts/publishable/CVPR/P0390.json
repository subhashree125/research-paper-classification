{
  "Abstract": "Visual Instruction Tuning represents a novel learningparadigm involving the fine-tuning of pre-trained languagemodels using task-specific instructions.This paradigmshows promising zero-shot results in various natural lan-guage processing tasks but is still unexplored in vision emo-tion understanding. In this work, we focus on enhancing themodels proficiency in understanding and adhering to in-structions related to emotional contexts. Initially, we iden-tify key visual clues critical to visual emotion recognition.Subsequently, we introduce a novel GPT-assisted pipelinefor generating emotion visual instruction data, effectivelyaddressing the scarcity of annotated instruction data inthis domain. Expanding on the groundwork established byInstructBLIP, our proposed EmoVIT architecture incorpo-rates emotion-specific instruction data, leveraging the pow-erful capabilities of Large Language Models to enhanceperformance. Through extensive experiments, our modelshowcases its proficiency in emotion classification, adept-ness in affective reasoning, and competence in comprehend-ing humor.The comparative analysis provides a robustbenchmark for Emotion Visual Instruction Tuning in the eraof LLMs, providing valuable insights and opening avenuesfor future exploration in this domain. Our code is availableat",
  ". Introduction": "Visual emotion recognition, a key area within artificial in-telligence and computer vision, aims to predict human emo-tions based on visual cues such as facial expressions andbody language.This technology is essential in bridgingthe gap between human affective states and machine under-standing. Its diverse applications , spanningfrom improving human-computer interaction to aiding inmental health assessment, underscore its significance. Ac-curate emotion recognition is vital for enhancing user expe-",
  ". Illustration of the importance of instruction-following abil-ity in visual emotion understanding": "rience and ensuring information security, as it helps preventemotional manipulation and misinformation . Develop-ing robust emotion recognition models is not only a techni-cal challenge but also a step towards more empathetic andintuitive AI systems, paving the way for more efficient andnatural human-computer interactions.The AI community has recently shown a growinginterest in developing foundational vision models, e.g.,Flamingo , LLaVA , BLIP2 . These models ex-cel in open-world visual understanding, tackling several vi-sion tasks such as classification, detection, segmentation,and captioning. In contrast, current large-scale multimodalmodels are still in its infancy when it comes to emotion per-ception . As illustrated in , when directly querythe GPT-4 about the emotional category of an image,the model tends to provide incorrect responses. However,the model delivers accurate responses when provided withrevised instructions. To fully leverage the potential of ex-isting vision-based large models, our approach is based onthe concept of Instruction Tuning. This effective strategyis aimed at teaching language models to follow natural lan-guage instructions, a technique proven to enhance their gen-eralization performance across unseen tasks .",
  "arXiv:2404.16670v1 [cs.CV] 25 Apr 2024": "In this work, we focus on developing the models profi-ciency in understanding and following instructions relatedto emotional contexts. This approach highlights the impor-tance of fine-tuning the models instruction-following ca-pabilities, enabling it to interpret and respond to emotionalcontent effectively. This is achieved by leveraging its pre-existing knowledge base, thereby eliminating the necessityfor an emotion-specific architectural framework.To address the notable challenges encountered in In-struction Tuning for visual emotion recognition, especiallythe lack of specific instruction data, we introduce a novelself-generation pipeline explicitly crafted for visual emo-tion recognition by using GPT-4 .This innovativepipeline excels in generating a diverse array of (image, in-struction, output) instances, thereby notably enhancing thedataset with a more extensive and task-oriented variety ofexamples. This approach not only overcomes the challengeof limited data availability but also reduces the dependenceon human labor. Therefore, it streamlines the process, en-abling more efficient and effective emotion recognition.Additionally, Instruction Tuning has been criticized forits emphasis on surface-level features like output patternsand styles, rather than achieving a profound comprehen-sion and assimilation of tasks .To tackle this issueand enhance the diversity and creativity of instruction data,our dataset includes instructions that demand complex rea-soning, going beyond basic question-and-answer formats.This is further enriched by incorporating visual cues suchas brightness, colorfulness, scene type, object class, facialexpressions, and human actions. These aspects are pivotalin fostering a nuanced comprehension of visual emotions,thus allowing the model to generate more precise and con-textually appropriate interpretations .After generating the emotion visual instruction data,weproposeanEmotionVisualInstructionTuning(EmoVIT) framework, leveraging the foundation of In-structBLIP . This framework incorporates an emotion-centric, instruction-aware module that proficiently guidesLarge Language Models (LLMs) in assimilating the nu-ances of emotion instructions.Our work signifies aparadigm shift, presenting a new era of instruction-basedlearning for visual emotion understanding that relies lesson explicit training data. Remarkably, as shown in ,our approach requires almost 50% of the training data typi-cally needed yet exceeds the performance of previous visualemotion recognition methods and popular Visual Instruc-tion Tuning methods.Our contributions can be summarized as follows:",
  ". Visual Emotion Recognition": "A key challenge in visual emotion recognition is bridgingthe gap between an images visual cues and the emotionsit portrays . While traditional efforts, e.g., Xuet al.s multi-level dependent attention network , fo-cus on visual models for emotional feature learning, recentadvancements like EmoSet offer rich emotion-ladendatasets with 3.3 million images. The rise of multimodalmodels, such as the GPT series , has further propelledVision-Language Recognition. However, fully leveragingthese models in emotion recognition is an area ripe for ex-ploration. Our work leads the way in utilizing large-scalemodels for Emotion Visual Instruction Tuning.",
  ". The comparison of different visual tuning paradigms": "between training goals and user expectations. LLMs aretrained to minimize prediction errors, whereas users ex-pect helpful and safe instruction-following.InstructionTuning addresses this by teaching models to follow natu-ral language instructions, enhancing generalization to newtasks. FLAN demonstrated that training a large modelon instruction-based datasets improves zero-shot perfor-mance.This approach has extended to vision-languagetasks, with BLIP2 and LLaVA adapting instruction-tuned LLMs for visual inputs.InstructBLIP intro-duces instruction-aware visual feature extraction and the Q-Former, enabling more flexible, instruction-driven featureextraction.As a novel area, visual emotion instruction tuning lacksbenchmarks or guidelines for creating emotion instructiondata. Our work pioneers the use of large-scale models todevelop an emotion instruction data pipeline, overcomingthe limitations of manual annotation.",
  ". Preliminary of Visual Instruction Tuning": "In the deep learning era, visual tuning has experienced sig-nificant paradigm shifts, as depicted in .In (a), conventional tuning methodologies en-compass Full fine-tuning, Head-oriented, and Backbone-oriented techniques, capitalizing on large-scale pre-trainedmodels. Predominantly, thoroughly fine-tuning these mod-els for specific tasks, conducted end-to-end, is recognized asa highly effective strategy. However, this method requiresmaintaining separate copies of the backbone parameters foreach distinct task, posing challenges in storage and deploy-ment.Alternatively,VisualPromptTuning(VPT),presents an efficient substitute for full fine-tuning within large-scale vision Transformer models. It achieves this byemploying a minimal fraction of trainable parameters in theinput space while maintaining a frozen backbone model.The objective function for Visual Prompt Tuning is givenby:minP L(f(X, P; P), Y )(1) where minP is the minimization over the prompt parame-ters P, L is the loss function, f represents the model func-tion with input image X, prompt parameters P, and learn-able model parameters P as input, and Y is the target out-put.Visual Prompt Tuning focuses on optimizing LLMs us-ing a small set of parameters, whereas Visual InstructionTuning (VIT) aims to improve the models comprehen-sion of instructions, thereby addressing the models short-comings in specific domains. This type of method aimsto enhance the models proficiency in following instruc-tions, leveraging the capabilities of the latest foundationmodels, e.g., Llama , and BLIP2 .Instructionsserve as guiding constraints, shaping the models outputsto conform to specific response characteristics and domain-relevant knowledge. This approach enables human moni-toring of the models behavior, thereby assuring alignmentwith the desired outcomes. Moreover, Instruction Tuning iscomputationally efficient, allowing LLMs to swiftly adaptto particular domains without extensive retraining or archi-tectural alterations.The objective function for Visual Instruction Tuning isgiven by:mintunable L(g(X, I, C; tunable), Y )(2) where mintunable denotes the minimization over the tunableparameters tunable in the Instruction Tuning Module, L isthe loss function, g is the model function with instruction I,image X, other contexts C, and tunable parameters tunable,",
  "Image Embeddings": ". The overall architecture of our proposed method. The Emotion Instruction data generated by (a) will be used for EmotionVisual Instruction Tuning in (b). During Emotion Visual Instruction Tuning, given an input image, the frozen Image Encoder initiates theprocess by extracting visual features. Emotion Instruction generated by (a) are subsequently interacting with Queries embedding throughthe learnable Q-Former. This interaction is key to drawing out image features that are relevant to the task at hand. As a result, the frozenLLM receives visual information conducive to instruction following. and Y denotes the target output. The optional context Cis not just raw data; it encompasses descriptive or directiveinformation guiding the model on how to process input orwhich task to execute, e.g., image caption. Its integral tothe models understanding and execution of tasks based onspecific instructions or guidelines.",
  ". GPT-assisted Emotion Visual Instruction DataGeneration": "Previous methodologies commonly employed a consistenttemplate-based set of instructions for every image withina dataset across various specific tasks . For instance, astandard instruction such as Briefly describe the content ofthe image was employed uniformly across all images forImage Captioning. In this way, the model may not be ableto adequately capture the unique characteristics of each im-age. Moreover, this one-size-fits-all approach often leads tosuboptimal performance in emotion recognition tasks thatrequire nuanced perception and differentiation of ambigu-ous emotion classes.Since the topic of Emotion Visual Instruction Tuningis still in its infancy, no benchmarks or guidelines havebeen proposed so far for constructing emotion instructiondata. Based on the recent successes of machine-generatedinstructions demonstrated in LLaVA , our work pio-neers the use of existing LLMs to create a pipeline forself-generating emotion instructions. Different from previ-ous template-based and one-size-fits-all instruction data, wepropose an instance-wise and LLM-assisted visual emotioninstruction data pipeline. This methodology transcends the constraints of manual annotation by employing GPT-4 to generate instance-wise, tailored instruction data that dy-namically corresponds to visual content.Prior to the development of instructional data for the vi-sual emotion recognition task, it is imperative to confront afundamental academic problem: What types of visual cluesare pivotal in identifying emotions?This necessitates acareful consideration of the unique characteristics inherentto the task, along with a comprehensive understanding ofthe potential visual cues associated with human emotions.In this work, we propose a novel visual instruction datamechanism to remove the inherent subjectivity and ambi-guity in emotional interpretation. Specifically, we integratea broad spectrum of emotion attributes across multiple lev-els: low-level attributes (e.g., brightness, colorfulness), mid-level attributes (e.g., scene type and object class), and high-level attributes (e.g., facial expressions and human actions),building upon insights from previous work . This com-prehensive strategy not only aligns with the intricate natureof emotions but also significantly enhances the models ca-pability to interpret and understand visual emotional cuesmore accurately and holistically.The overall pipeline of our proposed emotion visual in-struction data is shown in (a). For an image Ximg,three types of image-related contexts are essential for GPT-4 to generate emotion instruction data: (i) a caption Xc, (ii)an emotion attribute list Xattr, which includes emotion class,brightness, colorfulness, scene type, object class, facial ex-pression, and human action, and (iii) the system prompt,designed to enable GPT-4 to comprehend the specific task requirement1.We first manually design a few examples which are usedas seed examples for in-context learning to query GPT-4.This operation leverages the models ability to extrapolatefrom given examples, enhancing its understanding and re-sponse accuracy based on the principles of few-shot learn-ing . Our generated emotion instruction data includesthree types: Categorical, Conversation, and Reasoning.Building upon previous research , our generated instruc-tion data adheres to the dialogue format, exemplified in.Our strategy for generating emotion instruction dataadopts a progressive approach from simple to complex. Ini-tially, for the Categorical data, we transform the associatedemotion class of the image into a structured format. Thisprocess serves as the foundational component of our emo-tion instruction data.For the Conversation data, our framework is designed tocreate dialogues in which the GPT assistant interacts withan inquirer, focusing on the emotion attributes of the image.In this setup, the assistants responses are tailored to inter-pret and describe the image as though it were within its ownvisual field, thereby providing insights from an observa-tional viewpoint. The scope of questions posed is compre-hensive, encompassing the types of objects depicted, theiractions, and the dynamics of their interrelationships. Thedialogues we generate fall into two categories: (i) BasicInteraction, focusing on the provided emotion attribute listwith simple, direct characteristics, and (ii) Advanced Inter-action, which builds on the first type to reach greater con-versational complexity and sophistication.For the Reasoning data, our approach extends beyondmere visual content, prompting the model to generate in-depth reasoning questions. To enhance the dialogues cred-ibility and structure, detailed examples are incorporatedalongside logical reasoning steps, ensuring that the dis-course convincingly captures the intricacies of the visualcontent.",
  ". Emotion Visual Instruction Tuning": "After acquiring the emotion visual instruction data as de-tailed in Sec. 3.2, our goal is to employ this data in en-hancing the existing Visual Instruction Tuning model. Thisenhancement aims to align the LLMs existing knowledgewith the emotion understanding domain.As shown in b, we have developed an EmotionVisual Instruction Tuning (EmoVIT) architecture based onInstructBLIP . This architecture specifically leverages itsInstruction-aware Q-Former Module, as depicted in c, for emotion-centric instructional tasks.",
  ". The sample of our generated visual emotion instructiondata": "In our held-out evaluation, we focus on determininghow instruction tuning bolsters the models ability to trans-fer learning to new and unseen data. Its crucial to high-light that our methodology sets a distinct path from In-structBLIPs framework. Our dataset is specifically curatedwith emotion-centric content, presenting unique categoriessuch as cheerfulness and enthrallment found in WEBEmo,which are not typically included in other datasets. Con-versely, common emotional categories like anger and fearare shared with other collections, such as FI and Emotion6.This distinctive mix in our dataset implies that our held-outevaluation operates on a cross-domain level, examining themodels ability to interpret and adapt to diverse emotionalcontexts not strictly confined to zero-shot scenarios.",
  ". Implemental Details": "Our implementation is based on the LAVIS library .Our EmoVIT starts with a pre-trained InstructBLIP baselineand proceeds to fine-tune exclusively the Q-Former module,whilst keeping both the image encoder and the languagemodel frozen. The parameters for our training adhere tothe default settings established by InstructBLIP.Datasets.We evaluate our framework on ten bench-mark datasets annotated under different scenarios andclass number, namely EmoSet , WEBEmo , Emo-tion6 , the Flickr and Instagram (FI) , Art-photo , IAPS , Abstract , EmotionROI ,UnbiasedEmo , and OxfordTVG-HIC .Held-in Pretraining. Following previous work , we di-vide our dataset into two categories: held-in for pretrain-ing and held-out for evaluation 2. Considering the EmoSetdatasets comprehensive inclusion of emotion attributes foreach image, it has been chosen as the primary resource forour held-in pretraining phase. Simultaneously, for a broaderassessment, we perform held-out evaluations using the testsets from various other datasets.For the generation of emotion visual instruction data, weinitially employ the BLIP2 model for image captioning, fol-lowed by leveraging the GPT-4 API to generate emotion in-struction data. In total, our collection comprises Categori-cal, Conversation, and Reasoning instruction data, derivedfrom 51,200 unique images. This represents less than 50%of the entire EmoSet.",
  ". Held-out Evaluation": "As shown in Tab. 1, our proposed methodology exhibits amarked superiority in performance relative to the burgeon-ing Visual Instruction Tuning Methods. Since they havebeen pre-trained on dozens of large-scale datasets, it is ev-ident that our generated emotion visual instruction data isparticularly effective for emotional understanding Our re-sults signify a paradigm shift, heralding a new era of modeltraining that relies less on explicit supervision and more onthe robustness of emotion instruction-driven learning.The Effectiveness of Our Proposed Emotion VisualInstruction Data. As the first to introduce the concept ofemotion visual instruction data, our study seeks to evaluatethe generalizability of this newly generated instruction data.Our goal is to test its efficacy not only with InstructBLIP butalso across other Visual Instruction Tuning model, to un-derstand its broader applicability. As depicted in , weemploy two Visual Instruction Tuning models, LLaVA andInstructBLIP, which were fine-tuned on our specially gen- 2Unlike the setup in InstructBLIP, our dataset exclusively comprisesemotion-related content. Consequently, our held-out evaluation does notconstitute a strict zero-shot evaluation in the conventional sense.",
  ". The improvement of our proposed emotion visual instruc-tion tuning data tuning on LLaVA and InstructBLIP": "erated emotion visual instruction data. Subsequent testingacross five distinct datasets reveals notable improvementsin both models, substantiating the efficacy of our generateddata. Notably, InstructBLIP demonstrated a more substan-tial overall enhancement compared to LLaVA. This can beattributed to InstructBLIPs specialized Instruction-awareQ-Former Module, which adeptly extracts the salient fea-tures of our emotion instructions and synergizes them ef-fectively with the corresponding images, thereby yieldingimproved performance.",
  "Ablation Study of Different Instruction Data": "The ablation study outlined in Tab. 2 provides a compre-hensive analysis of the impact that different instructionaldata types have on model performance, specifically con-cerning accuracy metrics on the EmoSet test set. Initially,the model, referred to as InstructBLIP , operates withoutthe integration of the three types of instructional data andattains a baseline accuracy of 42.20%. This foundationalperformance is significantly enhanced with the inclusion ofCategorical data, which alone contributes to a substantialincrease in accuracy. The introduction of Conversation datafurther amplifies this effect, underscoring the value of con-versational context in improving the models predictive ca-pabilities. The addition of Reasoning data notably boostsperformance, achieving a peak accuracy of 83.36%. Thisindicates that the model significantly benefits from the nu-anced cues in reasoning, aiding in understanding complexemotional instructions.The gradual improvements witheach data type support the idea that a diverse approach toinstructional data markedly enhances model comprehensionand performance.",
  "Instruction Sensitivity": "This work is dedicated to the creation of a varied corpus ofvisual emotion instruction data, alongside the developmentof a robust instruction-based model. Our objective is for themodel to demonstrate stability, producing consistent resultsin the face of minor variations in instruction phrasing, pro-vided the core objective of the task persists unchanged. Tothis end, we employ the Sensitivity evaluation metric, as in-troduced by , to assess the models fidelity in generatinguniform outcomes irrespective of instructional nuances.We employ two semantically similar instructions as in-put prompts for the model, testing their impact on the Sen-sitivity score across three visual emotion datasets for differ-ent Visual Instruction Tuning models. The first instructionis: From the given options: cls 1, cls 2, cls 3, etc.,identify the emotion that most accurately reflects the image.Ensure your selection is confined to the listed options. Re-spond in the format: Predicted emotion: The second onestates: Please choose the emotion that best correspondsto the image from the following options: cls 1, cls 2,cls 3, etc. (Do not provide answers beyond the providedcandidates.) Please reply in the following format: Predictemotion:As illustrated in , our approach, along with BLIP2,exhibited exceptionally low Sensitivity values, demonstrat-ing robustness in understanding the instructions.Con-versely, Flamingo and InstructBLIP displayed a higher de-gree of sensitivity, indicating a relative susceptibility tovariations in instruction wording.",
  ". The sensitivity score comparison (the lower the better)": "to evaluate the generalization ability of various learningstrategies more impartially.Hence, we selected the Un-BiasedEmo test set , which is uniquely suited for rec-ognizing intricate emotions, such as those associated withidentical objects or scenes, e.g., landscapes, crowds, fami-lies, babies, and animals, where the emotional undertonescan be particularly subtle and complex. As depicted in Tab. 3, our proposed methodologydemonstrates superior performance when benchmarkedagainst conventional supervised emotion recognition tech-niques, thereby underscoring the efficacy of our approachin more accurately discerning complex emotional contexts.",
  "Affective Reasoning": "In the domain of visual emotion recognition, where ambi-guity and subjectivity are pervasive, the advent of an in-terpretable model is of considerable value. Such a modelelucidates its cognitive processes, enhancing its trustworthi-ness and practicality in scenarios requiring a delicate graspof emotional subtleties.Leveraging Visual Instruction Tuning, our model tran-scends mere categorization of emotions; it articulates theunderlying rationale for its classifications. The executingcommands for identifying emotions and elucidating the de-cision basis is illustrated below:",
  "Predicted emotion: [emotion].Reason: [explanation]": "Our model delineates the visual features influencing itsdeterminations, thereby addressing the complexities inher-ent in discerning and explaining emotion-related nuances.The explanations provide us with visual clues containedwithin the images, as exemplified in . It provides inter-pretable visual indicators that inform the models outputs,as demonstrated in our example, by disambiguating the of-ten abstract emotional categories.",
  ". Ablation study of different portion of pre-training data.Accuracy (%) on EmoSet test set": ". The sample of our generated humour caption vs humanwriting humour caption from OxfordTVG-HIC.modifying the models architecture, specifically testing themodels proficiency in generating humorous captions. Forthis purpose, we select 50 images from the OxfordTVG-HIC dataset and generate corresponding captions us-ing our model. Subsequently, the captions produced by ourmodel are compared with manually annotated captions fromthe dataset in a user study. Thirty participants were asked tovote on which captions were more humorous. Our model-generated captions receive 60% of the votes, demonstratingits effective humor generation capabilities. One sample isvisualized in .",
  ". Conclusion": "In our study, drawing upon the distinctive visual cues keyto visual emotion recognition, we present a GPT-assistedpipeline specifically designed for generating emotion vi-sual instruction data. The developed EmoVIT model incor-porates emotion-specific instructions, leveraging LLMs forenhanced performance.Our comprehensive experimentsvalidate its effectiveness in emotion classification, affec-tive reasoning, and humor understanding. This comparativeanalysis sets a benchmark for Emotion Visual InstructionTuning with LLMs, providing valuable insights and direc-tions for future research in this field.",
  ". Our Experiment Settings": "Held-out vs supervised learning. We adopt the terminol-ogy held-in and held-out as defined in the work of Instruct-BLIP . For the held-in, we utilize the training subsetof the EmoSet dataset for Emotion Visual Instruction Tun-ing, with its corresponding test subset serving the purposeof held-in evaluation. The outcomes of this evaluation aredepicted in of the main manuscript.",
  "The system prompt inputted into ChatGPT for the purposeof gathering instruction-based data is presented below": "You are an AI visual assistant, and you areseeing a single image. What you see are providedwith one caption and some emotion related at-tributes, describing the same image you are look-ing at. Answer all questions as you are seeing theimage. The range of brightness is from 0 (darkest)to 1 (brightest), and the range of colorfulness isfrom 0 (black-and-white) to 1 (the most colorful).Design two questions for a conversation be-tween you and a person asking about this photo.The answers should be in a tone that a visual AIassistant is seeing the image and answering thequestion. Ask diverse questions and give corre-sponding answers.Include questions asking about the visual con-tent of the image, including the object types,object actions, relationship among objects, etc.Only include questions that have definite an-swers: (1) one can see the content in the imagethat the question asks about and can answer con-fidently; (2) one can determine confidently fromthe image that it is not in the image. Do not askany question that cannot be answered confidently.Please answer with the format Question: Answer:Also include one complex question that is rel-evant to the content in the image, for example,asking about background knowledge of the ob-jects in the image, asking to discuss about eventshappening in the image, etc. Again, do not askabout uncertain details. Provide detailed answerswhen answering complex questions. For example,give detailed examples or reasoning steps to makethe content more convincing and well-organized.You can include multiple paragraphs if necessary.",
  ". Details of the Q-Former": "Similar to the approach in InstructBLIP, Q-Former is alightweight transformer architecture that utilizes a collec-tion of trainable query vectors to distill visual features froma static image encoder. The Q-Former acts as the trainablemodule to bridge the gap between a frozen image encoderand a frozen LLM. Its role is to curate and present the mostpertinent visual information, thereby enabling the LLM togenerate the targeted textual output efficiently. Followingthe default setting, in our experimental setup, we employ32 distinct queries, each with a dimensionality of 768.",
  ". Ablation Study of LLM Model Size": "In our attempts with the EmoVIT architectures LLM, weexplored the use of models of varying sizes (as shown inTab. 5). The results indicated that the smaller model, Vi-cuna7B, outperformed its larger counterparts. This may beattributed to the limited training data available for our task,which potentially underutilizes the capabilities of largermodels.Consequently, we anticipate that an increase intraining data in the future could enhance the effectivenessof Emotion Visual Instruction Tuning.",
  ". Adding In-context Samples in Held-outEvaluation": "Recent LLMs are capable of in-context learning when pro-vided with a limited number of examples in a few-shot man-ner. In this work, we have also embarked on such an explo-ration. For instance, Tab. 6 presents the in-context samplesutilized within the EmotionROI dataset. During our held-out evaluation, we incorporated three in-context samples foreach category, consisting of a caption paired with its corre-sponding emotion class. Nevertheless, in our experimentalobservations, we did not witness any enhancement in per-formance attributable to furnishing the LLM with these in-context examples. Consequently, our finalized methodol-ogy did not incorporate in-context samples during the held-out evaluation phase.",
  "DescriptionEmotion": "Unleashed Fury: A portrait of raw, unfiltered anger etched on the subjects face.AngerVolcanic Eruption in Human Form: A Portrait of Unrestrained Fury.AngerAn explosive portrait of raw fury, where every clenched jaw and furrowed brow tells a tale of unchecked anger.AngerFace contorted in a grimace of pure disgust, as if they just tasted a year-old lemon.DisgustCaught in the throes of revulsion, a face grimaces as if it just tasted the worlds sourest lemon.DisgustPicture Perfect: A Masterclass in the Art of Disgust ExpressionDisgustA chilling moment of pure terror, etched in every detail.FearA chilling moment of pure terror etched on the face, a stark embodiment of fear.Fearsomeone with a wide smile, a groupJoyOverflowing with joy, like a puppy at a park!JoyA poignant portrait of sorrow, where teardrops are the silent language of grief.SadnessAn evocative portrayal of sorrow, with shadows seemingly swallowing the light, reflecting the heavy weight ofsadness.Sadness An abstract portrayal of solitude, where the vivid hues of melancholy paint a poignant picture of sadness.SadnessCaught in a moment of pure astonishment, eyes wide and mouth agape.SurpriseCaught in the headlights of astonishment: a jaw-dropping moment of surprise!SurpriseCaught in the Act! A persons wide-eyed gasp of sheer surprise.Surprise",
  ". Limitation and future work": "Due to the reliance on the GPT-API and cost considerations,our held-in pretraining phase utilized less than 50% of theEmoSet dataset. Despite outperforming other methods, werecognize the potential for significant improvements in fu-ture work by expanding the data scale. We anticipate thatadvancements in visual emotion understanding will parallelincreases in both data and model scale.",
  "H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tun-ing, in Advances in Neural Information Processing Systems,2023. 1, 2, 3, 4, 5, 6, 7": "J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Has-son, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al.,Flamingo: a visual language model for few-shot learn-ing, in Advances in Neural Information Processing Systems,vol. 35, 2022, pp. 23 71623 736. 1, 2, 7 W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang,B. Li, P. Fung, and S. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, inAdvances in Neural Information Processing Systems, 2023.1, 2, 3, 4, 5, 6, 7",
  "J. Yang, J. Li, X. Wang, Y. Ding, and X. Gao, Stimuli-awarevisual emotion analysis, IEEE Transactions on Image Pro-cessing, vol. 30, pp. 74327445, 2021. 1, 2": "R. Panda, J. Zhang, H. Li, J.-Y. Lee, X. Lu, and A. K. Roy-Chowdhury, Contemplating visual emotions: Understand-ing and overcoming dataset bias, in Proceedings of the Eu-ropean Conference on Computer Vision, 2018, pp. 579595.2, 6, 7 L. Xu, Z. Wang, B. Wu, and S. Lui, Mdan: Multi-level de-pendent attention network for visual emotion analysis, inProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, 2022, pp. 94799488. 2 J. Yang, Q. Huang, T. Ding, D. Lischinski, D. Cohen-Or, andH. Huang, Emoset: A large-scale visual emotion datasetwith rich attributes, in Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, 2023, pp. 20 38320 394. 1, 2, 4, 6 J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrappinglanguage-image pre-training with frozen image encoders andlarge language models, in Proceedings of the InternationalConference on Machine Learning, 2023. 1, 2, 3, 7",
  "M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Har-iharan, and S.-N. Lim, Visual prompt tuning, in Pro-ceedings of the European Conference on Computer Vision.Springer, 2022, pp. 709727. 3": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro,F. Azhar et al., Llama: Open and efficient foundation lan-guage models, arXiv preprint arXiv:2302.13971, 2023. 3,5 M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek,J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdul-mohsin et al., Scaling vision transformers to 22 billion pa-rameters, in International Conference on Machine Learn-ing.PMLR, 2023, pp. 74807512. H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, Au-assisted graph attention convolutional network for micro-expression recognition, in Proceedings of the 28th ACMInternational Conference on Multimedia, 2020, pp. 28712880.",
  "E. Parliament, Eu ai act: first regulation on artificial intelli-gence, Accessed June, vol. 25, p. 2023, 2023. 1": "R. Li, S. Sun, M. Elhoseiny, and P. Torr, Oxfordtvg-hic:Can machine make humorous captions from images? inProceedings of the IEEE/CVF International Conference onComputer Vision, 2023, pp. 20 29320 303. 6, 8 K.-C. Peng, T. Chen, A. Sadovnik, and A. C. Gallagher, Amixed bag of emotions: Model, predict, and transfer emo-tion distributions, in Proceedings of the IEEE conference oncomputer vision and pattern recognition, 2015, pp. 860868.6 Q. You, J. Luo, H. Jin, and J. Yang, Building a large scaledataset for image emotion recognition: The fine print andthe benchmark, in Proceedings of the AAAI conference onartificial intelligence, vol. 30, no. 1, 2016. 2, 6 J. Machajdik and A. Hanbury, Affective image classifica-tion using features inspired by psychology and art theory,in Proceedings of the 18th ACM international conference onMultimedia, 2010, pp. 8392. 6 J. A. Mikels, B. L. Fredrickson, G. R. Larkin, C. M. Lind-berg, S. J. Maglio, and P. A. Reuter-Lorenz, Emotional cat-egory data on images from the international affective picturesystem, Behavior research methods, vol. 37, pp. 626630,2005. 6 K.-C. Peng, A. Sadovnik, A. Gallagher, and T. Chen, Wheredo emotions come from?predicting the emotion stimulimap, in 2016 IEEE international conference on image pro-cessing (ICIP).IEEE, 2016, pp. 614618. 6 H. Xie, H. Chung, H.-H. Shuai, and W.-H. Cheng, Learn-ing to prompt for vision-language emotion recognition, in2023 11th International Conference on Affective Computingand Intelligent Interaction Workshops and Demos (ACIIW),2023, pp. 14. 1"
}