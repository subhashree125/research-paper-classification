{
  "Abstract": "A long-standing challenge in developing machine learn-ing approaches has been the lack of high-quality labeleddata. Recently, models trained with purely synthetic data,here termed synthetic clones, generated using large-scalepre-trained diffusion models have shown promising resultsin overcoming this annotation bottleneck. As these syntheticclone models progress, they are likely to be deployed inchallenging real-world settings, yet their suitability remainsunderstudied. Our work addresses this gap by providing thefirst benchmark for three classes of synthetic clone models,namely supervised, self-supervised, and multi-modal ones,across a range of robustness measures. We show that ex-isting synthetic self-supervised and multi-modal clones arecomparable to or outperform state-of-the-art real-imagebaselines for a range of robustness metrics shape bias,background bias, calibration, etc. However, we also findthat synthetic clones are much more susceptible to adver-sarial and real-world noise than models trained with realdata. To address this, we find that combining both real andsynthetic data further increases the robustness, and that thechoice of prompt used for generating synthetic images playsan important part in the robustness of synthetic clones.",
  ". Introduction": "Most modern machine learning methods are bottleneckedin performance by the quality and quantity of labeled data.Several works have shown that the generalizationerror of neural networks follows the neural scaling law withrespect to the dataset size, i.e. the test error reduces linearlywith the log of the dataset size. Moreover, the datasetsdiversity and fairness are also factors that play animportant role in the generalization performance of modern",
  "neural networks. Unfortunately, curating diverse, fair, andlarge datasets is time-consuming and expensive.The advent of large-scale image generation models like": "Stable Diffusion has revived the interest in utilizinggenerated images to train models for various downstreamtasks in hopes of alleviating the need for high-quality anno-tations. Models like use only generated imagesfrom Stable Diffusion for supervised training of a down-stream classifier. show that it is also possible totrain self-supervised models like SimCLR and multi-modal models like CLIP using only synthetic imagesand prompts. These models can match or outperform theircounterparts trained on real data for downstream tasks likeclassification and segmentation. We term such models thatare trained using only generated data as synthetic clones.Modern machine learning models are increasingly em-ployed in solving real-world problems like autonomousdriving and automated medical assistance .With therapid progress of using synthetic data for training models,it is imperative that we understand how robust these modelsare before deploying them in the real world. Recent workon synthetic clones has not focused on evalu-ating the robustness of these models. Yet, models trained",
  "arXiv:2405.20469v2 [cs.CV] 1 Jul 2024": "on synthetic or generated datasets have been known to suf-fer from shortcomings such as model collapse , i.e.when the model forgets the long tail classes or learns a dif-ferent distribution than the training dataset.Our work aims to provide a comprehensive benchmarkfor the robustness of synthetic clone models compared tostate-of-the-art (SOTA) baseline models that are trained onreal image datasets. We benchmark three classes of syn-thetic clone models supervised , self-supervised, and multi-modal ones against nine strong base-line models trained using real images. We evaluate robust-ness metrics regarding shape, background, and context bias.We also benchmark these models against adversarial andcommon image corruptions. Finally, we test how well thesemodels are calibrated in comparison to models trained withreal data. Our results are visually summarized in .To overcome some of the drawbacks of using syntheticdata alone, we conduct extensive ablations regarding howthe robustness of synthetic clones changes with (i) jointtraining with synthetic and real data, (ii) increasing thenumber of synthetic samples, and (iii) the effect of promptswhen generating images with Stable Diffusion.Let us summarize our findings: (i) On many robustnessmetrics (calibaration, background bias, shape bias, etc.)self-supervised and multi-modal models trained on syn-thetic data perform on par with their counterparts trained onreal imagery. (ii) Supervised synthetic models, on the otherhand, lag behind baselines trained on real datasets w.r.t. sev-eral key robustness measures like calibration, OOD detec-tion, adversarial robustness, etc. (iii) Synthetic clones aremuch more vulnerable to adversarial and common corrup-tion than models trained with real images. (iv) A mixture ofreal and synthetic data is the best combination for obtainingrobustness. (v) The choice of prompt for image generationplays a crucial role in the robustness of synthetic clones.",
  ". Related Work": "Self-supervised learning (SSL)methods have emerged as promising alternatives to solve the dataannotation bottleneck. These models learn by solving pre-text tasks like context prediction , image denoising ,patch prediction , and many others .In recent years, they have come increasingly close to su-pervised models. For example, the downstream classifica-tion accuracy for DINOv2 with supervised linear prob-ing is 84.5% (using ViT-B) while that of EfficientNet ,a strong supervised model, is 88.4% on the ImageNet-1Kdataset . However, SSL methods suffer from scaling is-sues, i.e. augmenting an already large-scale dataset in sizehas little effect on the model performance . Another ap-proach is using large-scale uncurated multimodal web data. However, this data is often noisy, biased, and limitedin diversity (e.g., certain concepts may have only a few data",
  "points )": "Generative neural networksare a class of models that,given random noise samples, learn to transform these noisesamples into data. Modern generative models can broadlybe categorized into implicit models, such as GANs and diffusion-based models , or explicitmodels like normalizing flows and VAEs .Diffusion models are SOTA for image generation sincethey address the limited diversity and image quality issues,which impaired the use of previous generative models . Synthetic datahas found usage in a myriad of com-puter vision tasks or applications like semantic segmenta-tion , object detection , and autonomous driving .Recently, generated data from large-scale pre-trained dif-fusion models was used to train better object classificationmodels . Particularly, showed that synthetic datais extremely useful in transfer learning, zero-shot, and few-shot classification. show that even training oflarge-scale self-supervised models, such as CLIP andSimCLR , is possible with synthetic data. Our workfocuses on such synthetic clone models where the trainingdata was generated using large-scale pre-trained diffusionmodels. We use diffusion models because of the superiorquality and diversity of the generated data. Robustness.An often overlooked aspect when evaluatingmodels trained with synthetic datasets is evaluating themfor robustness. Recently, some efforts have been made tobenchmark models trained with synthetic data for adversar-ial robustness and out-of-distribution (OOD) detection. Still, no comprehensive robustness evaluation of thesemodels exists. In our work, we aim to benchmark syntheticclone models in a more comprehensive manner and on vari-ous robustness benchmarks. Besides adversarial robustness and OOD detection, we also benchmark these mod-els on common 2D and 3D image corruptions , andw.r.t. shape bias , context bias , background bias, and calibration .Previous works haveonly benchmarked small-scale supervised synthetic mod-els, while we analyze synthetic clones trained with 100s ofmillions of synthetic images across three classes of models,namely supervised, self-supervised, and multi-modal ones.",
  ". Background: Synthetic Clones": "Before analyzing various synthetic clone models below, letus briefly recapitulate how synthetic images can be gen-erated using diffusion models and how various classes ofmodels have been trained on these synthetic images. Synthetic data generation.The synthetic images in syn-thetic clone models are typically generatedusing large-scale pre-trained image generation models, e.g.,Stable Diffusion or Imagen .The input to thegeneration model is Gaussian noise and a conditional text . Setups for training different classes of models using synthetic images. Supervised learning (bottom) uses the ground-truthlabel for conditionally generating a synthetic image, while self-supervised (top left) and multi-modal methods (top right) make use of aconcept bank along with a large language model (LLM) for prompt generation. Please see text for more details. prompt. Synthetic clones can be divided into three cate-gories, namely supervised synthetic models, self-supervisedsynthetic models, and multi-modal synthetic models. Wenow describe how each model creates the prompt for gener-ating the image and which losses are used to train the model. Supervised models using generated data.For traininga supervised classifier, Saryldz et al. first generatean image using Stable Diffusion conditioned on the promptc, hc inside b. Here, c is the ground-truth class name sam-pled from all class labels of a dataset (e.g., ImageNet-1K), hc is the hypernym associated with c, and b denotesone of the 365 classes from the Places365 dataset. Ahypernym of c is the parent node of c in the WordNet hierarchy. The classifier is then trained end to end withthe cross-entropy loss (LCE) between the predicted labelof the generated image and the sampled ground-truth classlabel used for generating the image; see (bottom).Saryldz et al. created 1.2M such prompts and gener-ated corresponding images to train a ResNet50 model. Sim-ilarly, Fan et al. used just class names c for generat-ing 16M images. They then train a ViT-B model on thegenerated images and ground-truth class labels. Self-supervised models using generated data.Syn-thetic self-supervised models, namely SynCLR andStableRep , first sample a concept label from a con-cept bank. The concept bank is typically constructed usingextracted synsets of WordNet or common unigrams,bigrams, and titles from Wikipedia . This sampled con-cept label is then fed into a large language model (LLM) for generating extra contextual information. The final prompt is formed by concatenating the concept labeland the contextual information. This prompt is then usedto generate n images.After this, several augmentations(Aug.) also used in the SimCLR model are applied.The SynCLR model is trained using a multi-positive con-trastive loss (LContra) , see in (upper left).Multi-modal model using generated data.The multi-modal synthetic CLIP models also use a conceptlabel sampled from a concept bank. This concept label,along with a random place label sampled from the classesof Places365 dataset, is fed into an LLM for generatinga caption, which is subsequently used for conditional imagegeneration. These images are used to train a CLIP model using a contrastive loss between the generated imageand the prompt that was used for generating the image. Thearchitecture is shown in (upper right).",
  ". Robustness Analysis": "Setup.We divide the models to be analyzed into super-vised, self-supervised, and multi-modal models. For syn-thetic supervised models, we use a ResNet50 from anda ViT-B model from , which were trained on approx.1M images generated using prompts as described in Sec. 3.The class labels used for creating the prompts were sampledfrom the classes of the ImageNet-1K dataset . For clar-ity of notation, we term them SynResNet50 and SynViT-Bfor all our experiments. We compare these models againststrong supervised models trained on the real ImageNet-1Kdataset like ResNet50 , ViT-B , DeiT-III , Swintransformer , and ConvNeXt . All baselines are",
  "from the PyTorch Image Models library": "For the self-supervised case, we use the SynCLR model, which has been trained on 600M synthetic images. Weuse SOTA self-supervised models like DINOv2 , MAE, and MOCOv3 trained on ImageNet-1K as self-supervised baselines. All checkpoints for the baseline mod-els were obtained from the timm library. For a fair compar-ison, we use the ViT-B backbone with a patch size of16 for all models. We perform linear probing on all self-supervised models, training a single-layer linear classifica-tion head on the top of these models for 90 epochs using theImageNet-1k dataset. We searched over ten learningrates to find the optimal linear classifier for each model.Finally, for the multi-modal case, we analyze the syn-thetic CLIP model from , which we term as SynCLIP,trained on 371M synthetic images.We compare thismodel with the CLIP implementation from OpenCLIP ,trained on 400M real images. We used the ViT-B backbonefor these models to allow for a fair comparison. For CLIPand SynCLIP we report the zero-shot results.",
  ". Calibration": "As neural networks become adopted for safety-critical taskslike autonomous driving and healthcare, it is not only im-portant to predict accurate results, but also to accurately re-port the confidence in their prediction . Calibration canhelp to understand how reliable the models prediction isand whether an end user can trust the models output. Thecalibration of neural networks is commonly measured usingthe Expected Calibration Error (ECE) . The ECE mea-sures the expected absolute difference between the modelconfidence and the model accuracy. In our work, we studythe effect that training on synthetic images has on the cali-bration of a model compared to training with real data.We report the results for the ECE metric with 20 bins forall models. shows the results for both in-distribution(ID) calibration (train and test splits are from the samedataset) on the ImageNet-1k dataset and for out-of-distribution (OOD) calibration (train and test split are fromdifferent datasets) on the ImageNet-R and ImageNet-A datasets. We can conclude the following: Observation 1: Synthetic clones are mostly well cali-brated for the in-distribution case and even to some ex-tent out-of-distribution on ImageNet-R. The OOD cal-ibration of synthetic clones suffers on ImageNet-A. This may be because the synthetic data generated frompre-trained diffusion models (trained on data scraped fromthe web) already captures the distribution of the ImageNet(images scraped from the web) and ImageNet-R (consistingof cartoons and sketches, which are abundant on the inter-net) datasets. ImageNet-A, on the other hand, consists of naturally adversarial examples that are hard to find on theinternet; hence, synthetic clones and even baseline modelstrained on real images exhibit a rather poor calibration forthis dataset. However, models trained with real datasets aregenerally better calibrated for ImageNet-A, likely due to theinherent noise in the dataset (see also Sec. 4.2). Out of distribution (OOD) detectiondeals with findingout how well a model can distinguish between samples fromthe training data distribution (ID in distribution) and sam-ples from another distribution. OOD detection is critical toincreasing an end users trust in the safety and reliability ofthe model. We thus aim to evaluate how training on syn-thetic data affects a models capability for OOD detection.The OOD detection task can be formulated as a bi-nary classification task on the models predictive probabil-ity.A model F with weights classifies an input sam-ple xi as ID if the maximum predictive probability of thesample is higher than a pre-defined threshold value , i.e.max F(xi) , and as OOD if max F(xi) < . OODdetection can be evaluated using standard metrics for binaryclassification, such as the area under the receiver operatingcharacteristic curve (AUROC). We also report the false pos-itive rate of OOD samples when the true positive rate ofin-distribution samples is at 95% (FPR@95). Tab. 1 showsthe results of all models on three OOD datasets, namelySUN397 , Places365 , and iNaturalist , whereImageNet-1K is the ID dataset. We conclude the following: Observation 2: SynCLR and SynCLIP are compara-ble to the baseline models in their category for OODdetection. Even with 16 times more data than the base-line, SynViT-B clearly lags behind supervised modelstrained with real data.",
  ". Robustness": "Adversarial robustness.Adversarial learning aims to un-derstand model robustness to examples manipulated by anadversary in a way that the examples seem similar to the hu-man eye but change the models predictions. In our work,we want to explore whether models trained on syntheticdata are more susceptible to adversarial attacks. We use twopopular white-box attacks, the Fast Gradient Sign Method(FGSM) and the Projected Gradient Descent (PGD) at-tack . These white-box attacks require that the modelsgradient be known to the adversary. The FGSM attack per-turbs the input image with the gradient of the models pre-diction w.r.t. its input, scaled by a small step . This can bewritten as xi = xi +xiJ(, xi, yi), where xi denotes theinput image, xiJ denotes the gradient of the loss functionw.r.t. xi, and yi denotes the label for the input image xi.",
  "ImageNet-A": ". Test error vs. ECE for ID and OOD datasets. We report the resulting ECE metric and test error metrics for both ID (ImageNet)and OOD datasets (ImagNet-{R,A}). Filled markers indicate real models, empty markers indicate synthetic clones. . OOD detection with ImageNet-1K being in-domain (ID). We report the AUROC and FPR@95 metrics for the OOD detectiontask on the three OOD datasets, namely SUN, iNatualist, and Places. In addition, we also report the avg. performance of all models on allthree datasets. The best performing model in each category is highlighted in bold.",
  "CLIP400M0.820.740.680.880.780.760.760.79SynCLIP371M0.730.750.740.750.700.790.720.76": "The PGD attack is an iterative version of the FGSM attack,followed by a projection of the adversarial input to an ballaround the input x. The value denotes the maximum per-turbation allowed. We use values of 1/255 for the PGDand FGSM attacks. The number of steps is set to 20 forthe PGD attack. We report the accuracy of the clean andthe adversarial examples from the test set. We define theadversarial robustness metric, Radv, as the relative accuracybetween adversarial and clean samples as Radv =AccadvAccclean ,",
  "where Accadv is the accuracy on the adversarial samples andthe Accclean is the accuracy on the clean samples. Tab. 2shows the results. We can conclude the following:": "Observation 3: Synthetic clone models are signifi-cantly more vulnerable to adversarial examples, par-ticularly supervised synthetic clones, than modelstrained with real data. The self-supervised syntheticclone model trained with large amounts of syntheticdata, i.e. SynCLR, is loosely comparable to real-imagebaseline models in its respective category. We find that MAE performs the worst among allmodels (synthetic and real) regarding adversarial robust-ness, indicating that the training objective along with thetraining dataset size are important factors in determining amodels adversarial robustness.",
  "CLIP68.278.7512.826.319.24SynCLIP55.112.404.352.023.67": "Robustness against common corruptions.Next, weevaluate the performance of all models on real-world noisecorruptions that occur frequently. For this, we evaluate onthe ImageNet-C and ImageNet-3DCC datasets.ImageNet-C consists of 19 naturally occurring image cor-ruptions like Gaussian noise, shot noise, motion blur, elas-tic transforms, etc. ImageNet-3DCC includes 12 commoncorruptions that take depth into account, e.g., z-axis blur, . Common corruptions robustness results (in %). We report the individual and average accuracy for various 2D and 3D commoncorruptions. We also report the relative average accuracy (Avg. Rcc) for all models.",
  "Avg. Rcc()66.1033.5074.2180.4083.8069.7449.7851.0483.9070.8161.9966.8142.64": "far and near focus errors, etc. Due to time and resourceconstraints, we report the results only on ten common cor-ruption tasks (five each from ImageNet-C and ImageNet-3DCC). We report the accuracy of the clean and corruptedsamples and the average accuracy over all corruptions. Wealso report the Avg. Rcc metric, which is defined as the rel-ative accuracy between the clean samples and the averageaccuracy over all corruptions, i.e. Avg. Rcc = Avg. Acccc",
  "Observation 4: Synthetic clones are significantly lessrobust to common corruptions in images than base-lines trained with real images": "The Avg. Rcc is significantly lower for synthetic clonesacross all categories of models. Real datasets inherentlyhave these common corruptions present in the imagery,hence training on real data already makes the resulting mod-els more robust to noise. Synthetic images currently lackthese corruptions, making synthetic clones highly suscepti-ble to common image corruptions.",
  ". Biases": "Context bias.We define context bias as the affinity of amodel to use contextual cues, e.g., location for classifyingobjects, rather than actually using the object appearance.This context bias exists because most large-scale datasetsconsist of uncurated data scraped from the internet. For ex-ample, images of airplanes in a forest are highly unlikelywhen compared to airplanes on a taxiway. We use the FO-CUS (Familiar Objects in Common and Uncommon Set-tings) dataset to evaluate the context bias, which con-sists of around 21K images. Each image in the dataset is an-notated with the object class, the time of day, location, andweather labels. FOCUS subdivides the dataset into a subsetof common and uncommon samples. Uncommon samplesare uncommon in the real world, like airplane in forest",
  "CLIP76.7763.09SynCLIP71.4754.39": "or uncommon in the ImageNet dataset due to labels usedfor its construction (e.g., there is no label for seaplane inImageNet). The dataset is partitioned into mutually exclu-sive partitions Pk where k is the number of uncommon at-tributes. The total dataset is divided into four partitions, P0(containing only common objects) to P3 (containing threeuncommon attributes). We report the CBk metrics (ContextBias with k uncommon attributes), which is defined as therelative accuracy between the accuracy on the partition withno uncommon attributes P0 and a partition with k uncom-mon attributes Pk, i.e. CBk =AccPkAccP0 . For example, CB2measures the relative accuracy between P0 and P2. The re-sults are given in Tab. 4 and yield the following: Observation 5: Self-supervised synthetic clones arerobust to changes in context compared to baseline su-pervised and self-supervised models trained with realdata. The supervised synthetic clone SynViT-B is com-parable in performance to the ViT-B model trainedon real data. Meanwhile, SynCLIP is more prone tochanges in context compared to CLIP, but it is stillcomparable to models like DINOv2 and ConvNeXt.",
  "SynCLIP": ". Shape bias. (left) Average shape bias of models trainedwith synthetic (dashed bars) and real data. Synthetic clones aregenerally more biased toward shape than texture compared tomodels trained with real datasets. (right) Class-wise shape biasof synthetic clones and their counterparts trained using real data.Solid and dashed lines represent the mean shape bias of modelstrained with real images and synthetic images, respectively. Shape-texture bias.Children learn to recognize and or-ganize objects based on shape and are more biased towardsobject shape rather than color and textures . It hasbeen shown that biasing a network towards shapeincreases its robustness against common corruptions. Thissuggests that the robustness of neural networks generallybenefits from biasing towards categorizing objects by shaperather than textures. Generated images from GANs typi-cally have high-frequency artifacts (indicating high texturebias) . Diffusion models also exhibit similar pat-terns, though these are more muted .Such artifactscontrast with real images that do not contain these high-frequency artifacts. To understand if training on syntheticimages from Stable Diffusion biases the networks towardstexture, we use the cue conflict dataset . This datasetconsists of about 1200 images of 16 classes where the tex-ture and shape of an image are in conflict with each other. shows the shape bias of all the models averagedacross all classes. We also show the class-wise shape biasresults for synthetic clones and some baseline models. Weconclude the following: Observation 6:Synthetic clones tend to be moreshape-biased than texture-biased.In particular,SynCLIP outperforms all models on the shape biasmetric, while SynViT-B outperforms all classificationand self-supervised models. The SynCLR model hascomparable performance to the MOCOv3 model andoutperforms the MAE model on the shape-bias metric. A similar result was observed in with synthetic datafrom StyleGANv2 models. Our results indicate thatsynthetic data is diverse in terms of shape, leading to ahigher shape bias of synthetic clone models, but this couldindicate that the generated images lack texture diversity,",
  "making the network rely more on shape for classification": "Background bias.The background bias of models can beused to identify if the model is using the background of theimage to make the classification decision instead of usingthe object itself. Learning if a model is biased towards thebackground is an effective way to understand if the modelhas learned shortcuts instead of learning good featuresfor the given category. For evaluating a models backgroundbias, we utilize the Mixed-Rand and Mixed-Same partitionsfrom the IN-9L dataset . The Mixed-Rand dataset seg-ments the foreground object in an image and switches theoriginal background with a random background from a dif-ferent class label, while the Mixed-Same partition places thesegmented foreground object on a random background fromthe same class label. Tab. 5 shows the accuracy of all mod-els on the original, Mixed-Rand, and Mixed-Same parti-tions from the IN-9L dataset, along with BG-Gap. The BG-Gap measures the difference in performance between ac-curacies on the Mixed-Rand and Mixed-Same datasets andassesses how decisions can be manipulated just by chang-ing the background to a different class than the foreground.We conclude the following:",
  "Effect of prompts.Here, we analyze the effect that theprompt has on the robustness of the synthetic clone models": ". Effect of prompt type and dataset size on various performance metrics for the supervised SynViT-B model. Radv (FGSM)denotes the adversarial robustness for the FGSM attack, and Rcc (2DCC) the robustness for 2D common corruptions. Bold indicates thebest performance within a prompt type, and color indicates the best performance across all prompts and dataset sizes.",
  "M128M256M371M64M128M256M371M64M128M256M371M": "Acc. ()0.550.600.650.660.470.510.540.550.560.620.650.66Radv (FGSM, )0.090.070.100.120.050.030.040.040.090.080.100.12Rcc (2D-CC, )0.440.460.510.520.290.280.310.310.460.480.520.52CB2 ()0.520.520.570.610.580.530.550.540.610.580.630.61Shape Bias ()0.510.510.510.520.540.550.590.580.540.510.560.60BG-Gap ()0.730.760.570.720.650.510.570.530.730.630.630.56FPR@95 (SUN, )0.920.840.810.820.860.750.750.750.860.810.820.78ECE ()0.220.190.160.140.250.200.170.160.160.130.110.11 Tab. 6 shows results for a SynViT-B model trained onsynthetic images generated using different prompts such as(i) class names, (ii) 80 CLIP templates, e.g. high-qualityphoto of {class name}, used in evaluating the zero-shotclassification performance of the CLIP model, and (iii) classnames combined with a generated caption from BLIP2 ,e.g. Tench [class label], a man holding a fish. As can beseen in Tab. 6, captions and CLIP templates are much betterfor creating robust synthetic clones compared to just usingclass names. This can be attributed to more diverse imagesbeing generated with more descriptive text.Effect of adding real data.Next, we study the effect ofusing a mixture of real and synthetic image data on the ro-bustness of the CLIP model. Fan et al. trained the CLIPmodel with a fixed dataset size (for example, 371M images)where the real and synthetic images are picked randomly tocreate a subset containing both real and synthetic images,which are then used for training the CLIP model. Tab. 7shows that adding real data as suggested by improvesthe performance on many key metrics (ECE, adversarial ac-curacy, shape bias) while remaining comparable on others.Also, we see that training with just synthetic images or acombination of synthetic and real images creates more ro-bust models compared to models trained just on real data.Size of generated data.We evaluate the effect of datasetsize on the training of synthetic clones. As seen in Tabs. 6and 7, adding more data, in general, helps with the robust-ness of both SynViT-B and SynCLIP models. In some cases,adding more data may slightly decrease performance, which",
  ". Conclusion": "Our work is the first to perform a detailed analysis ofmodels trained with synthetic data across different robust-ness measures. Specifically, we show that certain syntheticclones, namely SynCLIP and SynCLR, perform within tol-erable limits of their counterparts trained on real images;this holds for all robustness metrics except for common cor-ruptions and OOD detection. Supervised models, namelySynViT-B, on the other hand, are outperformed by theirreal-image counterparts on all metrics except shape bias,which clearly shows the need for better supervised syn-thetic clones. Through detailed ablations, we find that usingcaptions or CLIP templates produces more robust syntheticclones. Importantly, we find that mixing real data with syn-thetic data can improve the robustness measures across mostmetrics. We hope our work encourages the development ofmore robust synthetic clones. Acknowledgements.This project has received funding fromthe European Research Council (ERC) under the EuropeanUnions Horizon 2020 program (grant agreement No. 866008).The project was also supported in part by the State of Hessethrough the project The Third Wave of Artificial Intelligence(3AI). Hassan Abu Alhaija,Siva Karthik Mustikovela,LarsMescheder, Andreas Geiger, and Carsten Rother.Aug-mented reality meets computer vision: Efficient data genera-tion for urban driving scenes. IJCV, pages 961972, 2018. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.GPT-4 technical report. arXiv:2303.08774 [cs.CL], 2023.",
  "Christiane Fellbaum.WordNet.In Theory and Applica-tions of Ontology: Computer Applications, pages 231243.Springer, 2010": "Felix Friedrich, Manuel Brack, Lukas Struppek, DominikHintersdorf, Patrick Schramowski, Sasha Luccioni, andKristian Kersting. Fair Diffusion: Instructing text-to-imagegeneration models on fairness. arXiv:2302.10893 [cs.LG],2023. Robert Geirhos,Patricia Rubisch,Claudio Michaelis,Matthias Bethge, Felix A. Wichmann, and Wieland Brendel.ImageNet-trained CNNs are biased towards texture; increas-ing shape bias improves accuracy and robustness. In ICLR,2018. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis,Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-lix A Wichmann. Shortcut learning in deep neural networks.Nature Machine Intelligence, pages 665673, 2020.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-sion probabilistic models. NeurIPS, pages 68406851, 2020": "Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, CadeGordon,Nicholas Carlini,Rohan Taori,Achal Dave,Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-CLIP, 2021. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,Chris Bamford, Devendra Singh Chaplot, Diego de lasCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-ple, Lucile Saulnier, et al. Mistral 7B. arXiv:2310.06825[cs.CL], 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.Brown, Benjamin Chess, Rewon Child, Scott Gray, AlecRadford, Jeffrey Wu, and Dario Amodei. Scaling laws forneural language models. arXiv:2001.08361 [cs.LG], 2020."
}