{
  "Abstract": "Deepfake detection aims to contrast the spread of deep-generated media that undermines trust in online content.While existing methods focus on large and complex mod-els, the need for real-time detection demands greater effi-ciency. With this in mind, unlike previous work, we intro-duce a novel deepfake detection approach on images us-ing Binary Neural Networks (BNNs) for fast inference withminimal accuracy loss. Moreover, our method incorporatesFast Fourier Transform (FFT) and Local Binary Pattern(LBP) as additional channel features to uncover manipu-lation traces in frequency and texture domains. Evaluationson COCOFake, DFFD, and CIFAKE datasets demonstrateour methods state-of-the-art performance in most scenar-ios with a significant efficiency gain of up to a 20 re-duction in FLOPs during inference. Finally, by exploringBNNs in deepfake detection to balance accuracy and effi-ciency, this work paves the way for future research on effi-cient deepfake detection.",
  ". Introduction": "The rise of deepfakes and media manipulated with sophisti-cated artificial intelligence threatens to erode the foundationof trust in the digital age. From fabricated revenge pornog-raphy to doctored political speeches , these syntheticcreations have the power to deceive, defame, and destabi-lize. As deepfake creation tools become more accessibleand the quality of these fakes rapidly improves , theability to distinguish authentic content from malicious ma-nipulation has become a critical battleground for preservingtruth and preventing the diffusion of fake information. Thisurge to identify deepfakes has brought much attention to therapidly evolving field of deepfakes detection .As the name suggests, this field concentrates on creating in-telligent algorithms that are able to depict common patterns . Depiction of the trade-off between performance (accu-racy) and the computational complexity measured in FLOPs (G)on the COCOFake dataset. Points with the same color indicatemodels that share the same architecture. The size of a point repre-sents the number of parameters of its model. This model sharesthe architecture with the one with the same name but is trained ona different dataset. In these models, which are the ones on thebottom left part, the backbone is kept frozen. that distinguish real from fake content .Deepfake detection is an evolving task driven by the re-lentless advancement of generative models, especially inimage generation. In such regard, recent years have seenremarkable progress in generating realistic images, thanksto Generative Adversarial Networks (GANs) .Thisspurred the creation of deepfake detection algorithms, of-ten based on deep neural networks , that accu-rately identified characteristic GAN-based artifacts. How-ever, the emergence of diffusion models , ca-pable of even higher-quality fakes, challenges these meth-ods .In tackling the challenges of identify-ing fake images generated by diffusion models, previousworks concentrate on training large neural networks withmillions of parameters to identify the inherent patterns ofgenerated images . While these detection methods show",
  "arXiv:2406.04932v1 [cs.CV] 7 Jun 2024": "promise, their reliance on large, complex models raises con-cerns. Deepfakes primarily spread on social media plat-forms and web applications , where devices like mobilephones and personal computers have limited computationalresources. This raises a critical question: Can we developefficient deep-learning-based deepfake detection methodswithout sacrificing accuracy?In the pursuit of answering our question, we shift the at-tention to Binary Neural Networks (BNNs). BNNs offer ex-ceptional memory and computational savings by quantizingboth weights and activations to 1-bit values .This makes them ideal for real-time deepfake detection onresource-constrained devices like phones and personal com-puters. Specifically, we employ the BNext convolu-tional neural network for its proven feature extraction ca-pabilities on RGB images. Additionally, since generativemethods often leave subtle artifacts, particularly aroundedges and in frequency domains , we augmentour RGB input with features derived from two specializedfilters: the Fast Fourier Transform (FFT) magnitude, andthe Local Binary Patterns (LBP). Such features further en-hance the models ability to identify generated artifacts byemphasizing micro-patterns and textures.To assess the performance of our methods ability toidentify deep-generated content and its efficiency in termsof computational resources, we conduct extensive experi-ments in three deepfakes detection datasets, including CO-COFake , DFFD , and CIFAKE . Our methodcompetes or improves the results of existing SOTA in al-most all scenarios while reducing up to 20 in computa-tional consumption measured in FLOPs. A glimpse of theperformance and computational complexity trade-off of ourmethod compared to the current SOTA can be seen in .In summary, the contributions of this work are four-fold:",
  ". an ablation study that highlights the impact of each de-sign choice made in building the proposed method;": "4. quantitative results that underline the quality of the pro-posal and promote further investigation.The remaining parts of this work are structured as fol-lows: delves into the existing literature, situatingour work within the broader context of deepfake detectionand BNN advancements; details the methodol-ogy employed, including our use of BNNs and the rationalebehind augmenting input images with FFT magnitude andLBP channels for enhanced detection capabilities; Section 4 presents the experimental setup, the datasets utilized, themetrics for evaluation, and the results obtained, alongsidean ablation study to discern the impact of each augmenta-tion of the input images; finally, summarizes thefindings and the limitations of our work on the deepfake de-tection task, outlining avenues for future research.",
  ". Deepfakes detection": "The fight against deepfakes has spurred intense research ef-forts, yielding a range of detection strategies. Early meth-ods focused on low-level artifacts stemming from genera-tive processes, pinpointing anomalies like unnatural blink-ing patterns or inconsistencies in physiological signalspatterns . However, these techniques are vulnerable toincreasingly sophisticated deepfake generation methods. Amore robust approach lies in analyzing mesoscopic features,such as facial warping artifacts , inconsistencies in headpose , and textural anomalies . Deep neural net-works, specifically CNNs, have emerged as powerful de-tection tools . Subsequent works have explored tailoredarchitectures like XceptionNet and attention mecha-nisms to enhance artifact detection. To address thedata scarcity issue and improve adaptability to new deep-fake techniques, self-supervised and semi-supervisedmethods are gaining traction. However, current tech-niques primarily detect GAN-generated samples, limitingtheir effectiveness across the full range of deepfake creationmethods . In recent years, diffusion models introducednew milestones in deepfake generation with high-qualityimages resembling natural ones . The detection of suchmodels images presents more challenges than those createdwith traditional GANs, often avoiding the telltale grid-likeartifacts found in GAN outputs and requiring a shift in de-tection strategies . Promising research focuseson analyzing the intrinsic local dimensionality of diffusion-generated images , which differs from natural images.Another approach investigates how diffusion models tendto overfit training data, leaving detectable traces in the formof reconstruction errors . While tailored methods areemerging, it is important to note that most works rely on theuse of advanced neural networks like heavy CNNs or Trans-formers to detect deepfakes. Despite notable progress,deepfake detection faces ongoing challenges. Models oftenstruggle to generalize to unseen deepfake generation tech-niques, and their performance degrades when encounteringreal-world distortions (e.g., video compression ).",
  ". Binary Neural Networks": "The BNN architecture, pioneered by , involves binariz-ing weights and activations through the sign function, sub-stituting the bulk of arithmetic operations in deep neuralnetworks with bit-wise operations.To address quantiza-tion error, the XOR-Net introduced a channel-wisescaling factor for reconstructing binarized weights, a tech-nique pivotal in subsequent BNN models.As proposedin , ABC-Net endeavors to approximate full-precisionweights with a linear combination of binary weight basesand employs multiple binary activations to diminish infor-mation loss. Inspired by the full-precision networks ResNet and DenseNet architectures, Bi-Real Net integrates shortcuts to minimize the performance gap be-tween 1-bit and real-valued CNN models. Concurrently,BinaryDenseNet enhances BNN accuracy by increas-ing the number of shortcuts. Further, IR-Net proposesthe Libra-PB method, aimed at reducing information lossduring forward propagation through the maximization ofquantized parameters information entropy and minimizingquantization error, bounded within {1, +1}. ReActNet develops a generalized version of traditional sign andPReLU functions, named RSign and RPReLU, respectively,facilitating explicit learning of distribution reshaping andshifting with minimal computational overhead. RBNN examines and mitigates angular biass effect on quantizationerror. SiMaN reveals that removing L2 Regularizationduring training maximizes the entropy. ReCU intro-duces a rectified clamp unit to revive the so-called deadweights, thus reducing quantization error. AdaBin integrates equalization methods for weights and introduceslearnable parameters for activations, employing the Max-Out nonlinear activation function to add a negligible countof floating-point operations. B-Next proposed a hy-brid approach with a basic block with binarized convolu-tions and INT-4 linear layers, achieving state-of-the-art per-formances.",
  ". Proposed method": "The proposed method, whose architecture is shown in Fig-ure 2, processes an RGB image to classify it as either realor generated. Initially, the method augments the input im-age by adding two additional channels that correspond tothe FFT magnitude and the LBP . Subsequently, theseaugmented images undergo adaptation through an Adapterto revert them to a 3-channel format, which is then fed intothe backbone for feature extraction. The extracted featuresare then classified as real or fake.",
  "The model takes an RGB image in R3hw as input, whereh and w are its height and width, respectively. This study": "enriches this image with two channels representing its FFTmagnitude and LBP. These augmentations are specificallyselected to underscore the subtle yet significant micro-patterns that deepfakes often disrupt, leveraging the intu-ition that specific texture and edge information can be piv-otal in distinguishing between genuine and generated im-agery.We assume that deepfakes could introduce distortions inthe frequency domain which are typically not present ingenuine images. Thus, we exploited the FFT magnitudechannel to highlight these anomalies. This channel is ob-tained by applying the FFT to the image to extract its mag-nitude spectrum.The LBP is a texture descriptor that encapsulates the lo-cal spatial structure of an image. It is introduced in thepipeline to capture the unique textures of facial features.Those are areas where deepfakes typically struggle to main-tain accuracy. LBP enriches the models input with robusttexture pattern features by comparing each pixel with itsneighbors and encoding this comparison into a new image.",
  ". Adapter": "As the backbone is compatible with 3-channel images andthe augmented image has more than those channels, we in-troduced a particular layer just before the backbone, namelythe Adapter, to manage the new features. It is a convolu-tion layer that takes as input an image with 5 channels (red,green, blue, FFT magnitude, and LBP) and squeezes theminto an image of the same height and width but with just 3channels, according to the shape of the input accepted bythe backbone.",
  ". Binary backbone": "The concept of a BNN was pioneered by . Their inno-vative approach entailed using the sign function to binarizeweights and activations, effectively replacing the majorityof arithmetic computations in deep neural networks withbit-wise operations, obtaining a theoretical speedup of 58in inference speed and 32 times less memory needed. Be-fore the explanation of our proposed BNN for this work,we first describe what are the differences between a full-precision and a binary CNN.We describe a CNN by representing its layer-specificreal-valued weights as Wr and the inputs as Ar. Conse-quently, the output Y of a convolution is formulated as fol-lows:",
  "+1,if xr 0,1,otherwise.(2)": "To address the significant quantization error inherent indeep neural networks binarization, XNOR-Net intro-duces dual scaling factors for both weights Wb and activa-tions Ab. Following the methodology outlined by , thisdocument simplifies the representation of these scaling fac-tors to a single parameter . Hence, the output of a binaryconvolution is expressed as follows:",
  "where denotes bit-wise operations including XNOR andPOPCOUNT, and stands for element-wise multiplication": "The proposed method uses BNext as a backbone,which is pre-trained on the ImageNet dataset.InBNext, each convolution operation is binary, and other op-erations are quantized to INT-4, resulting in much moreefficient networks in terms of operations. The backboneuses a binary convolution module with full precision skip-connection and a branch with precision INT-4 to facilitateinformation propagation and alleviate possible bottlenecks.",
  ". Datasets": "We leverage three distinct datasets, each offering uniquechallenges and characteristics, to effectively evaluate ourdeepfake detection method. The datasets are the COCO-Fake , the Diverse Fake Face Dataset (DFFD) , andthe CIFAKE .The COCOFake dataset builds upon the COCO dataset and augments it with images using the Stable Diffu-sion text-to-image model. In particular, for each realimage in COCO, which is accompanied by 5 captions, 5corresponding synthetic images are created. This approachmaintains the integrity of the original COCO datasets di-vision into training, validation, and test sets. The datasetcomprises more than 650K training images, 30K validationimages, and 30K test images.The DFFD is an extensive collection of images aimedat enhancing the detection and localization of facial manip-ulations. It covers four primary types of facial manipula-tions: identity swaps, expression swaps, attribute manipula-tions, and entirely synthesized faces. Real face samples aresourced from the FFHQ , CelebA datasets (whichoffer a wide range of demographic and quality diversity),and additional real images from the FaceForensics++ dataset.For fake samples, PGGAN and StyleGAN are employed to create manipulated images in line withthe 4 manipulation categories, summing up to 58K real and240K fake images.Half of the samples are used in thetraining set, while 5% for validation and 45% for test sets,respectively, ensuring that manipulations derived from the same source image remain within the same set.The CIFAKE dataset is structured to parallel the CIFAR-10 dataset, featuring a balanced composition of realand synthetic 32 32 pixels images across ten classes.Specifically, it includes 60K real images directly taken fromCIFAR-10 and an equal number of synthetic images gener-ated using the Stable Diffusion model, summing up to120K images. The dataset is split into training and test sets,with 50K images designated for the first and 10K reservedfor the latter.",
  ". Metrics": "Several metrics have been employed when comparing ourmethod with the state of the art. These include metrics re-lated to classification performance (accuracy, Area Underthe Curve (AUC)), and computational requirements (float-ing point operations per second (FLOPs). These metricsrepresent the standard in the deepfake detection and Binary Neural Networks fields.Accuracy is defined as the ratio of correctly predictedobservations, namely True Positives (TP) and True Nega-tives (TN), to the total observations, which includes FalsePositives (FP) and False Negatives (FN). It is a measure ofthe models overall correctness across all classes and is par-ticularly useful for balanced datasets.The AUC measures the ability of a model to discrim-inate between positive and negative classes. The AUC isthe area under the Receiver Operating Characteristic (ROC)curve, which plots the TP rate against the FP rate at variousthresholds.The count of FLOPs is a measure of the computationalcomplexity of a model, indicating the total number of float-ing point precision required for a single forward pass. Thismetric is crucial for understanding neural networks compu-tational demand and efficiency, especially when deployingmodels in resource-constrained environments.",
  ". Experimental setup and training details": "Following the standard methodology used in related litera-ture regarding the preprocessing of images, an initial resiz-ing step was undertaken to standardize the longest dimen-sion of each image to 252 pixels. Subsequently, a centralcrop measuring 224 224 pixels was extracted from im-ages within the validation and test sets. In contrast, cropsfrom the training set were randomly obtained to introducesome variability. Data augmentation techniques were em-ployed to augment the training datasets diversity further.These techniques included random flips (both horizontaland vertical), rotations (either 90 or 270), and color jitter-ing within a range of [0%, 20%]. All images are then nor-malized to have mean [0.485, 0.456, 0.406] and standard de-viation [0.229, 0.224, 0.225]. This particular choice of im-age size and color normalization has been done to maintain continuity with that of the data used during the pre-trainingof the backbone.The model optimization was achieved using the AdamW optimizer in combination with the binary cross-entropyloss. It was configured with an initial learning rate of 104,with the first and second-moment estimates (1 and 2) setto 0.9 and 0.999, respectively, accompanied by a weight de-cay parameter of 102. Additionally, a learning rate sched-uler was implemented to methodically reduce the learningrate to 105 by the conclusion of the fifth epoch, optimiz-ing the training process over time. The batch size was setto 128. We experimented with two configurations of themodels to evaluate their performance under different com-putational constraints: a version where the backbone is keptfrozen to assess the models behavior in conserving com-putational resources during training a trainable backboneto maximize the accuracy. Based on the involved dataset,a maximum epoch limit was established throughout theexperimental phase. For COCOFake and DFFD datasets(which details are provided in .1) the epoch limitwas set at 5, a decision driven by the observation that themodel convergence was typically achieved well before thisthreshold. Conversely, for the CIFAKE dataset, which isnotably smaller in size, the epoch limit was extended to 20to accommodate the datasets unique characteristics and en-sure adequate model training.The code was implemented in Python, leveraging the Py-Torch framework for Deep Learning. Computational taskswere performed on an NVIDIA RTX 2080Ti GPU with12GB of VRAM.",
  ". Results": "We report the results of our benchmark on the COCOFakedataset in .Our results were compared with themethod proposed in . To maintain a fair comparison,we set our model with ResNet-50 and ViT-B/32 ,both pre-trained on ImageNet as the models we used. Inthe case of our models with a frozen backbone, we surpassthe result of ResNet-50 by 2.84 accuracy points with thetables second-best model, BNext-S. When we also trainthe backbone, the margin of outperformance over ResNet-50 expanded to 8.97 accuracy points. This proves that ourmodel can perform better than full-precision models initial-ized on the same dataset while having substantially lowerFLOPs.Furthermore, our approach remains competitivewhen comparing our method with models pre-trained onsubstantially larger datasets. With its 99.28% accuracy, ourbest model trails the best-performing model, OpenCLIP-ViT-B/32 trained on LAION-2B, by just 0.4 points. No-tably, the latter model was trained on a dataset comprising2 billion images, in contrast to the 1.2 million images ofthe ImageNet dataset in which our models were pre-trained;this fact highlights the efficacy of our method despite its re-",
  "The outcomes on the DFFD dataset, detailed in Table": "2, are compared against the performances of Xception and VGG16 as outlined in . As noticed, our mod-els with a trainable backbone consistently perform betterthan the two compared models, achieving an improvementof 0.27 AUC points while having up to 17.1 fewer GFLOPs. Regarding the results on the CIFAKE dataset, shown in, we compare our method with the models proposedin . To keep a fair comparison, we consider the resultsof BNext with a trainable backbone given the same trainingconditions. However, does not specify the exact ver-sion of VGG and DenseNet , making it impossible todirectly compare the number of parameters and FLOPs. Toaid the reader, we have included metrics for the smallestversions of these models. Despite this, BNext demonstratescompetitive performance while requiring fewer GFLOPs.",
  ". Ablation study": "We conducted an ablation study to ascertain the optimalamalgamation of features incorporated into the input. Theresults are delineated in . Specifically, this studyjuxtaposed a baseline model against various configurationsincorporating supplementary channels, using accuracy as ametric since the differences in the number of FLOPs arenegligible. The baseline model processes an RGB imageas input, directly channeling it into a pre-trained BNext-Tbackbone without integrating an Adapter. Conversely, thealternative models evaluated entail baseline variations, eachretrained with the inclusion of one or more of the additionalchannels, each paired with a congruent Adapter. These newchannels are the FFT magnitude and LBP described in Sec-tion 3, plus the one obtained by applying a Sobel filter tothe image. The latter is employed to accentuate the edge in-formation of the image and is obtained by applying a pair of3 3 convolution kernels, one estimating the gradient hor-izontally and the other vertically, to approximate the gradi-ent magnitude of the image at each point. By emphasizing",
  ". Ablation study on the COCOFake Dataset": "edges and contours, we thought the Sobel filter would aidin highlighting discrepancies in the boundary regions of-ten overlooked by deepfakes, focusing on the premise thatgenuine images possess naturally smooth transitions, whichmanipulated images struggle to replicate accurately. The ablation study highlights that FFT magnitude andLBP channels, when combined, markedly improve modelperformance in detecting manipulated images, surpassingthe baseline and other variations.The increase amountsto 1.25% over the second best-performing configuration,which is the baseline. This synergy stems from their com-plementary analytical approaches: FFT magnitude exposesanomalies in the frequency domain indicative of digital ma-nipulation. At the same time, LBP captures nuanced localtexture patterns disrupted by such manipulations. Alone,each channel is seen to have a partial view: FFT magni-tude might miss subtle textural alterations, and LBP couldoverlook frequency-based distortions. Together, they coverspectral and spatial discrepancies, enhancing detection ca-pabilities. The Sobel filters marginal impact suggests thatedge information alone is insufficient for deepfake detec-tion, underscoring the importance of integrating featuresthat address global and local image characteristics for op-timal performance.",
  ". Conclusion": "In this study, we investigate the performance of more com-putationally efficient neural networks, particularly BNNs,in the context of deepfake detection tasks. Our findings re-veal that the proposed BNN-based method, which requiresup to 5 times fewer FLOPs compared to a ResNet-50 modeland nearly 10 times fewer FLOPs than a ViT-B/32 model, iscapable of matching the performance of their full-precisioncounterparts with minimal loss in classification accuracy.These results suggest a promising direction for enhancingthe efficiency of deepfake detection methodologies. One notable limitation of our study is the emphasis ontheoretical FLOP reductions, as the real-world applicationof BNNs necessitates a specialized framework or accelera-tor to fully realize the benefits of reduced precision. Fur-thermore, our evaluation was confined to a network pre-trained on the ImageNet dataset, whereas other investiga-tions have leveraged larger datasets for pre-training, therebyachieving enhanced transfer-learning capabilities. For future research, there is potential for practical im-plementation of our proposed method on specialized hard-ware or within specific computational frameworks to actual-ize the theoretical efficiency gains. Additionally, exploringalternative pre-training datasets could further augment thetransfer-learning efficacy of the network, potentially lead-ing to more robust and efficient deepfake detection sys-tems.",
  ". Acknoledgements": "The research leading to these results has received fund-ing from Project Ecosistema dellinnovazione - RomeTechnopole financed by EU in NextGenerationEU planthrough MUR Decree n.1051 23.06.2022 - CUPH33C22000420001. Darius Afchar, Vincent Nozick, Junichi Yamagishi, and IsaoEchizen. Mesonet: a compact facial video forgery detectionnetwork. In International Workshop on Information Foren-sics and Security, pages 17. IEEE, 2018. 1, 2 RobertoAmoroso,DavideMorelli,MarcellaCornia,Lorenzo Baraldi, Alberto Del Bimbo, and Rita Cucchiara.Parents and children: Distinguishing multimodal deepfakesfrom natural images.arXiv preprint arXiv:2304.00500,2023. 1, 2, 4, 5, 6",
  "Umur Aybars Ciftci, Ilke Demir, and Lijun Yin. Fakecatcher:Detection of synthetic portrait videos using biological sig-nals. IEEE transactions on Pattern Analysis and MachineIntelligence, 2020. 1, 2": "Matthieu Courbariaux, Itay Hubara, Daniel Soudry, RanEl-Yaniv, and Yoshua Bengio. Binarized neural networks:Training deep neural networks with weights and activationsconstrained to+ 1 or-1.arXiv preprint arXiv:1602.02830,2016. 3, 4 Davide Cozzolino, Justus Thies, Andreas Rossler, ChristianRiess, Matthias Niener, and Luisa Verdoliva. Forensictrans-fer: Weakly-supervised domain adaptation for forgery detec-tion. Conference on Computer Vision and Pattern Recogni-tion, 2018. 2 Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, andAnil K. Jain. On the detection of digital face manipulation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2020. 2, 4, 5, 6 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale, 2021. 5",
  "neural network ticket.arXiv preprint arXiv:2211.12933,2022. 2, 3, 4, 5": "Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu,and Chang Xu.Training binary neural networks throughlearning with noisy supervision. In International Conferenceon Machine Learning, pages 40174026. PMLR, 2020. 2 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3, 5",
  "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet classification with deep convolutional neural net-works. Advances in neural information processing systems,25:10971105, 2012. 4, 6, 7": "Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi:Exposing ai created fake videos by detecting eye blinking. InInternational Workshop on Information Forensics and Secu-rity (WIFS), pages 17. IEEE, 2018. 1, 2 Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and SiweiLyu. Celeb-df: A large-scale challenging dataset for deep-fake forensics. In Conference on Computer Vision and Pat-tern Recognition, pages 32073216, 2020. 1, 2 Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, YanWang, Yongjian Wu, Feiyue Huang, and Chia-Wen Lin. Ro-tated binary neural network. Advances in Neural InformationProcessing Systems, 33, 2020. 3, 5",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.Deep learning face attributes in the wild. In Proceedings ofInternational Conference on Computer Vision (ICCV), 2015.4": "Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu,and Kwang-Ting Cheng.Bi-real net: Enhancing the per-formance of 1-bit cnns with improved representational ca-pability and advanced training algorithm. In Proceedings ofthe European conference on computer vision (ECCV), pages722737, 2018. 3 Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural net-work with generalized activation functions.In EuropeanConference on Computer Vision, pages 143159. Springer,2020. 3 Peter Lorenz, Ricard L Durall, and Janis Keuper. Detectingimages generated by deep diffusion models using their lo-cal intrinsic dimensionality. In International Conference onComputer Vision, pages 448459, 2023. 2",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decayregularization, 2019. 5": "Huy H Nguyen, Fuming Fang, Junichi Yamagishi, and IsaoEchizen.Multi-task learning for detecting and segment-ing manipulated facial images and videos. In InternationalConference on Biometrics Theory, Applications and Systems,pages 18. IEEE, 2019. 2 Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, PranavShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, andMark Chen. Glide: Towards photorealistic image generationand editing with text-guided diffusion models. arXiv preprintarXiv:2112.10741, 2021. 1, 2 Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen,Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward andbackward information retention for accurate binary neuralnetworks.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 22502259, 2020. 3",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,and Mark Chen. Hierarchical text-conditional image gener-ation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 1, 2": "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,and Ali Farhadi. Xnor-net: Imagenet classification using bi-nary convolutional neural networks. In European conferenceon computer vision, pages 525542. Springer, 2016. 2, 3 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1068410695, 2022. 1, 2, 4, 5 Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris-tian Riess, Justus Thies, and Matthias Niessner. Faceforen-sics++: Learning to detect manipulated facial images.InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), 2019. 1, 2, 4 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. 1, 2 Christoph Schuhmann, Richard Vencu, Romain Beaumont,Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, TheoCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:Open dataset of clip-filtered 400 million image-text pairs,2021. 6 Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon,Ross Wightman,Mehdi Cherti,TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, Patrick Schramowski, Srivatsa Kundurthy, KatherineCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and JeniaJitsev. Laion-5b: An open large-scale dataset for trainingnext generation image-text models, 2022. 6"
}