{
  "Abstract": "In this paper, we present a simple but effective method toenhance blind video quality assessment (BVQA) models forsocial media videos. Motivated by previous researches thatleverage pre-trained features extracted from various com-puter vision models as the feature representation for BVQA,we further explore rich quality-aware features from pre-trained blind image quality assessment (BIQA) and BVQAmodels as auxiliary features to help the BVQA model tohandle complex distortions and diverse content of socialmedia videos. Specifically, we use SimpleVQA, a BVQAmodel that consists of a trainable Swin Transformer-B anda fixed SlowFast, as our base model. The Swin Transformer-B and SlowFast components are responsible for extractingspatial and motion features, respectively. Then, we extractthree kinds of features from Q-Align, LIQE, and FAST-VQAto capture frame-level quality-aware features, frame-levelquality-aware along with scene-specific features, and spa-tiotemporal quality-aware features, respectively. Throughconcatenating these features, we employ a multi-layer per-ceptron (MLP) network to regress them into quality scores.Experimental results demonstrate that the proposed modelachieves the best performance on three public social me-dia VQA datasets. Moreover, the proposed model won firstplace in the CVPR NTIRE 2024 Short-form UGC VideoQuality Assessment Challenge. The code is available at",
  ". Introduction": "Blind video quality assessment (BVQA) aims toprovide a perceptual quality score of the video without ac-cess to any reference information (i.e., high-quality sourcevideos), which has increasingly played a crucial role invideo processing systems of steaming media applications,ensuring that end-users can view high-quality videos andhave a superior Quality of Experience (QoE). Towards",
  "Although knowledge-driven BVQA models [5, 12, 35,": "43, 44] have better interpretability, they often exhibit rel-atively poor performance and higher computational com-plexity compared to data-driven approaches, mainly due tocomplex human perception processes involved in assessingvisual quality. With the rapid development of deep neuralnetwork (DNN), data-driven BVQA models have demon-strated excellent performance on various kinds of videos,including professionally generated content (PGC) videoswith synthetic distortions and user-generated content(UGC) videos with realistic distortions . The success of data-driven BVQA models can be at-tributed to two factors. The first is the adoption of moreadvanced neural networks, including convolutional neu-ral network (CNN)-based methods (e.g., VSFA , Sim-pleVQA , Li22 , etc.), Transformer-based meth-ods (e.g. StarVQA , FAST-VQA , etc.), and re-cent large multi-modality (LLM)-based methods (e.g. Q-Align ). The second is the construction of large-scalesubjective labeled video quality assessment (VQA) datasets(e.g., LSVQ , etc.), enabling the DNN models learnquality-aware feature representation from the videos and thecorresponding quality labels. As data-driven methods, the performance of BVQAmodels relies heavily on the human-rated VQA datasets.However,the videos in current mainstreaming VQAdatasets were typically captured byoutdated cameras or collected from video sharing websitesseveral years ago. The distortion types and video contentmay not align with the videos in current streaming videoapplications especially social media applications, as theshooting devices and video processing algorithms includingpre-processing, compression, and enhancement algorithms,have greatly improved. Therefore, the BVQA model trainedby these VQA datasets may not have sufficient capability toevaluate the perceptual quality of the millions of newly so-cial media videos uploaded daily.",
  ". The comparison of PGC videos, UGC videos, and the processed UGC videos": "In this paper, we focus on BVQA models for socialmedia videos 1, which exhibit two distinct characteristics:1) the video content usually includes lots of special ef-fects, text descriptions, subtitled, etc., 2) the videos undergocomplex processing workflows including pre-processing,transcoding, and enhancement. We show some typical so-cial media videos in .In the literature, Sun et al. propose a BVQA frameworknamed SimpleVQA, comprising a trainable spatial qual-ity module and a fixed temporal quality module, achiev-ing competitive performance compared to state-of-the-artmethods. This framework shows excellent extensibility inaccommodating various scenarios, including surveillancevideo quality assessment and point cloud quality as-sessment , etc.Moreover, Zhang et al. extractgeometry features (i.e., dihedral angle, gaussian curvature,and NSS parameters) of the mesh of digital human and in-tegrated them into the SimpleVQA framework to assess thequality of dynamic digital human.Wen et al. pro-pose a spatial rectifier and temporal rectifier within theSimpleVQA framework to address variable spatial reso-lution and frame rate video quality assessment problems.These studies indicate that with proper quality-aware fea-tures, SimpleVQA can effectively handle various types ofquality assessment problems.Therefore, we also resort to the SimpleVQA frameworkto address the social media BVQA problem. Given the di-verse content of social media videos and the variety of videoprocessing algorithms they undergo, training SimpleVQAend-to-end may require a large-scale of VQA datasets toachieve the robust quality feature representation, whilethe newest social media VQA dataset, KVQ , com-prises only 3,600 quality-labeled videos. Inspired by prior",
  "We use the social media video to refer to UGC videos represented onsocial media applications like Kwai and TikTok": "works , we enhance SimpleVQA with rich quality-aware features derived from state-of-the-art blind imagequality assessment (BIQA) and BVQA models, which helpsto alleviate the models reliance on training data and im-prove its robustness.To be more specific, we choose two BIQA models,LIQE and Q-Align , and one BVQA model, FAST-VQA , to extract frame-level quality-aware features,frame-level quality-aware along with scene-specific fea-tures, and spatiotemporal quality-aware features, respec-tively. LIQE and Q-Align are both vision-language basedBIQA models. For LIQE, we use the textual template: aphoto of a(n) {s} with {d} artifacts, which is of {c} qual-ity and calculate the cosine similarity between the visualembedding of the test image and the textual embedding ofthe text prompt. The parameters s, d, and c belong tonine scene categories, eleven distortion types, and five dis-tortion levels respectively, and total 495 text prompts aretested to derive 495 dimensional LIQE features. For Q-Align, we use the conversation formats: #User: image How would you rate the quality of this image? #Assis-tant: The quality of the image is level ., where image and level denote the image token and image qualitylevel respectively. We extract Q-Align features by comput-ing the hidden embedding of the last encoder layer. FAST-VQA features are computed by global average pooling thelast-stage feature maps. Then, we use Swin Transformer-B as the spatial quality analyzer and the temporal path-way of the SlowFast network as the temporal qualityanalyzer. To further enhance the spatial feature representa-tion, we add a multi-head self-attention (MHSA) afterthe feature maps extracted by the Swin Transformer-B tocapture salience information and guide the spatial featureextraction. Finally, we concatenate the SimpleVQA fea-tures (including both spatial and temporal features), LIQEfeatures, Q-Align features, and FAST-VQA features and regress them into video quality scores by a two-layer multi-layer perception (MLP) network.The Pearson correla-tion coefficient (PLCC) loss is used to optimize the entireBVQA model. Our model achieves the best performanceon three UGC VQA datasets and achieve the first place inthe CVPR NTIRE 2024 Short-form UGC Video Quality As-sessment Challenge .The core contributions of this paper are summarized asfollows: We enhance the SimpleVQA framework with threekinds of quality-aware pre-trained features, yieldingoutstanding performance on social media UGC VQAdatasets and also exhibiting remarkable robustness andgeneralizability.",
  ". VQA Datasets": "Early VQA datasets primarily focus on synthetic dis-tortions introduced by different video processing stages,such as spatiotemporal downsampling ,compression , transmission , etc.These datasets typically consist of a limited number ofhigh-quality source videos and the corresponding distortedones.Due to limited video content and not consider-ing the realistic distortions, these datasets are not suit-able for training general BVQA models.Therefore, re-cent VQA datasets have shifted fo-cus towards realistic captured distortions.For example,LIVE-Qualcomm consists of 208 videos captured by8 smartphones across 54 unique scenes. LIVE-VQC includes 585 videos captured by 80 mobile cameras, en-compassing different lighting conditions and diverse lev-els of motion, each video corresponding to a unique scene.LSVQ consists of 38, 811 videos sampled from the In-ternet Archive and YFCC100M datasets by matching sixvideo feature distributions. In general, These datasets havegreatly promoted the development of objective BVQA mod-els.However, for videos on social media platforms like Kwaiand TikTok, their quality is influenced by both in-captureddistortions and distortions caused by video processing al-gorithms. Hence, some studies have started to constructsocial media VQA datasets.For instance, Li et al. selected 50 source videos from TikTok and then two en-coders (i.e. H.264 and H.265) were used to compress eachvideo with five QPs to simulate the video transcoding proce-dure. Yu et al. sampled 55 1080p videos from LIVE-VQC , downscaled them to four different resolutions, and subsequently compressed using H.264 across 17 com-pression levels.To streamline the human study, a sam-pling strategy was employed to select 220 represented dis-torted video for the subjective VQA study. Zhang et al. constructed the TaoLive dataset, containing 418 raw videosfrom the TaoLive platform and 3, 762 distorted videos com-pressed at 8 different CRF levels using H.265.Gao etal. studied the impact of video enhancement algorithmson UGC videos and constructed the VDPVE dataset, whichincludes 184 low-quality videos and 1, 211 videos enhancedby light/contrast/color, deblurring, stabilization algorithms.Wu et al. introduced the MaxWell dataset with 4, 543videos labeled with multi-attribute scores on 16 dimensions.Lu et al. introduced the KVQ dataset to further studythe impact of complete video processing workflows, includ-ing pre-processing, transcoding, and enhancement, on videoquality. The dataset consists of 600 user-upload social me-dia videos and 3, 600 processed videos.In this paper, we focus on quality assessment for UGCvideos processed by multiple video processing algorithms(called social media videos in this paper), which is morechallenging to BVQA models because of their diverse dis-tortions introduced during both capture and video edit-ing/processing stages.",
  ". BVQA Models": "As stated in , we can roughly divide the BVQAmodels into knowledge-driven methods and data-drivenmethods.Knowledge-driven BVQA models uti-lize carefully designed handcrafted features to quantify thevideo quality. For example, V-BLIINDS utilizes spa-tiotemporal natural scene statistics (NSS) models to quan-tify the NSS features of frame differences and motion co-herency characteristics, and then regresses these features tovideo scores by support vector regressor (SVR). Mittal etal. propose a training-free blind VQA model namedVIIDEO that exploits intrinsic statistics regularities of nat-ural videos to quantify disturbances introduced due to dis-tortions.TLVQM extracts rich spatiotemporal fea-tures such as motion, jerkiness, blurriness, noise, blocki-ness, color, etc. from both high and low complexity levels.VIDEVAL employs the sequential forward floating se-lection strategy to choose a set of quality-aware featuresfrom typical BI/VQA methods, followed by training anSVR model to regress them into the video quality. TLVQMand VIDEVAL demonstrate that leveraging rich quality-aware handcrafted features enables the BVQA model toachieve better performance. In this paper, we show thatcombining diverse quality-aware features extracted fromDNNs with a base BVQA model (e.g. SimpleVQA) canalso achieve superior performance. .Data-driven BVQA methods [14,15,19,20,38,40,49,51, 58, 59] mainly leverage DNNs to extract the quality-awarefeatures. For instance, Liu et al. introduce a multi-taskBVQA model, optimizing the 3D-CNN for quality assess-ment and compression distortion classification simultane-ously. VSFA first extracts semantic features from apre-trained CNN model, followed by utilizing a gated re-current unit (GRU) network to capture the temporal rela-tionship among the semantic features of video frames. Yiet al. propose an attention mechanism based BVQAmodel, which employs a non-local operator to handle un-even spatial distortion problems. Ying et al. introducea local-to-global region-based BVQA model, combing thequality-aware features extracted from a BIQA pre-trainedand spatiotemporal features from a pre-trained action recog-nition network. Li et al. also employ the IQA modelpre-trained on multiple databases to extract quality-awarespatial features and the action recognition model to extracttemporal features, subsequently utilizing a GRU network isused to regress spatial and temporal features into the qualityscores. Sun et al. propose SimpleVQA, a BVQAframework that consists of a trainable spatial feature ex-traction module and a pre-trained motion feature extractionmodel. In this paper, we adopt SimpleVQA as our basemodel. Wu et al. propose FAST-VQA, which sam-ples spatio-temporal grid mini-cubes from original videosand trains a fragment attention network consisting of a Swintransformer and the gated relative position biases in an end-to-end manner. Wu et al. further propose DOVER,which integrates FAST-VQA with an aesthetics quality as-sessment branch to evaluate video quality from both tech-nique and aesthetics perspectives. With the popularity oflarge multi-modality models (LMMs), some LMM-basedquality assessment models have been proposedto evaluate the image/video quality by providing predefinedtext prompts to LMMs. Recently, there have been efforts to integrate varioustypes of DNN features to enhance BVQA performance andprovide explainability. For example, Wang et al. pro-pose a feature-rich BVQA model that assesses quality fromthree aspects including compression level, video content,and distortion type, with each aspect evaluated by a separateneural network. Liu et al. extract seven types of fea-tures extracted by EfficientNet-b7 , ir-CSN-152 ,CLIP , Swin Transformer-B , TimeSformer ,Video Swin Transformer-B , and SlowFast to rep-resent content-aware, distortion-aware, and motion-awarefeatures of videos. They incorporate these quality represen-tations as supplementary supervisory information to train alightweight BVQA model in a knowledge manner. Thesestudies demonstrate the potential for BVQA models to ben-efit from various computer vision tasks. In this paper, wefurther demonstrate that BVQA models can achieve betterperformance with quality-aware pre-trained features.",
  ". Proposed Model": "As depicted in , our BVQA model builds uponSimpleVQA, incorporating Swin Transformer-B for learn-ing spatial quality feature representation and leveraging thetemporal path of SlowFast for modeling motion character-istics. We integrate three kinds of quality-aware features in-cluding LIQE, Q-Align, and FAST-VQA into SimpleVQAto enhance its quality-aware feature representation, therebyimproving its capability to handle complex distortions of so-cial media videos introduced during capture and video edit-ing/processing procedures.",
  "Given a video x = {xi}N1i=0 , where xi RHW 3": "represents the i-th frame. Here, H and W denote the heightand the width of each frame respectively, and N is the totalnumber of frames. The features extracted by our methodcan be categorized into three levels: spatial, temporal, andspatiotemporal. Therefore, we partition the video into threeparts: key frames, video chunks, and the entire video. Forkey frames, we sample the first frame of every one-secondvideo frame sequence as the key frame, denoted as:",
  ". The Base Model": "We adopt SimpleVQA as our base model, whichutilizes a trainable spatial quality analyzer to extract spatialquality-ware features and employs a fixed temporal qualityanalyzer to capture motion features. Recent study sug-gests that most VQA datasets are dominated by spatial dis-tortions and pose little challenge to the temporal quality an-alyzer. Therefore, we choose a high-performance backboneSwin Transformer-B as our spatial quality analyzer.We drop out the classification head of Swin Transformer-B and add a MHSA module to guide the spatial qualityanalyzer to focus on salience regions of video frames thataffect video quality. We finally apply global average pool-ing to obtain the spatial quality representation. We denotethese procedures as:",
  "T": ". The framework of the proposed BVQA model. We use SimpleVQA as the base model, which consists of a Swin Transformer-Band a SlowFast. We extract three quality-aware features using LIQE, Q-Align, and FAST-VQA as the auxiliary features. These featuresare then concatenated and regressed into the quality score via a MLP network. where GP, MHSA, and SwinB represent global averagepooling, MHSA module, and Swin Transformer-B withoutthe classification head operators. Fsi is the spatial featuresof i-th key frames.The temporal quality analyzer is designed to extractvideo motion information, which is important for detect-ing distortions such as jitter caused by unstable shootingequipment or lagging resulting from low bandwidth dur-ing streaming. Following the approaches , we usethe fast pathway of SlowFast to extract motion features foreach video chunk. We also remove the classification headof SlowFast and calculate the temporal features by globalaverage pooling the last-stage feature maps:",
  ". LIQE Features": "LIQE is a multi-task learning based visual-languagemodel for BIQA. It employs the CLIP model, including animage encoder and a text encoder, to compute the cosinesimilarity between text features and image features. Specif-ically, it can take a text prompt t(s, d, c) = a photo of a(n){s} with {d} artifacts, which is of {c} quality and an imageas the inputs, and calculate the cosine similarity betweentext features and image features as the probabilities to rep-resent how well that the text prompt describes the test im-age. Subsequently, the probabilities can be used to infer thescene type, artifact type, and quality level of the test image. Therefore, we utilize the probabilities from differenttypes of text prompts as the features to represent the scene,artifact, and quality-level characteristics of video frames.Here, we consider nine scene categories:sS={animal, cityscape, human, indoor scene, land-scape, night scene, plant, still-life, and others},eleven distortion types: d D = {blur, color-related,contrast, JPEG compression, JPEG2000 compres-sion, noise, overexposure, quantization, under-exposure, spatially-localized, and others}, and fivequality levels: c C = {1, 2, 3, 4, 5} = {bad, poor,fair, good, perfect}. So, in total, we have 495 textprompt candidates to compute the probabilities:",
  ". Q-Align Features": "Q-Align is a large multi-modality model designed forquality assessment tasks.Specifically, Q-Align is pre-trained on multiple large-scale image/video quality assess-ment databases.The quality labels of the databases arefirst transformed into qualitative adjective descriptions (ex-cellent, good, fair, poor, bad) and are then integratedinto question-answer pairs for instruction fine-tuning ofQ-Align.After training, Q-Align operates by taking inthe prompt of How is the quality of this image? |img| The quality of the image is [SCORE TOKEN], where[SCORE TOKEN] is the quality rating token responded byQ-Align and [SCORE TOKEN] can be translated into thelog probabilities to the predefined qualitative adjective de-scriptions.However, to form a more comprehensive quality repre-sentation from the Q-Align perspective, we extract the fea-ture map from the last hidden layer of Q-Align rather than[SCORE TOKEN] for analysis, which can be derived as:",
  ". FAST-VQA Features": "FAST-VQA is an efficient algorithm specially designedfor BVQA. It notices that videos contain a high degree ofspatio-temporal redundancy, and correspondingly proposesa grid mini-cude sampling (GMS) algorithm to pre-samplethe video data before feeding them to the backbone, i.e.Video Swin Transformer Tiny (VSwin-T) . For videoas x, the sampled fragments (xf) are formulated as fol-lows:",
  ". Experimental Protocol": "Test Datasets. We test our model on three VQA datasets:KVQ , TaoLive , and LIVE-WC , all of whichfocus on assessing the quality of streaming UGC videos.For KVQ, we train our model on the publicly released datafrom NTIRE 2024 Short-form UGC Video Quality Assess-ment Challenge2 and subsequently test the trained modelon both validation and test sets. For TaoLive and LIVE-WC, we conduct random splits of the videos with an 80%- 20% train-test ratio based on the video scenes, and repeatthis process five times and report the average performance. Implementation Details. As stated in , we uti-lize Swin Transformer-B and SlowFast R50 as thebackbones of the spatial and temporal quality analyzers inthe basic model. To improve the generalization ability ofthe basic model, we first train it on the LSVQ dataset ,following the training strategy in . Regarding the spa-tial quality analyzer, we resize the resolution of the mini-mum dimension of key frames as 384 while preserving theiraspect ratios. During the training and test stages, the keyframes are randomly and centrally cropped with a resolutionof 384384. As for the temporal quality analyzer, the res-olution of the video chunks is resized to 224224 withoutrespecting the aspect ratio. For LIQE, Q-Align, and FAST-VQA, we adhere to the original setups of these methodswithout making any alterations to extract the correspond-ing features. The Adam optimizer with the initial learningrate 1 105 and batch size 6 is used to train the proposedmodel on a server with 2 NVIDIA RTX 3090. We decay thelearning rate by a factor of 10 after 10 epochs and the totalnumber of epochs is set as 30. Compared Models.We compare the proposed methodwitheighttypicalBVQAmethods,includingfourknowledge-driven methods:NIQE , TLVQM ,VIDEVAL , and RAPIQUE , and four data-drivenmethods: VSFA , SimpleVQA , FAST-VQA ,",
  "and Q-Align .Except for Q-Align, we train otherBVQA models for fair comparison": "Evaluation Criteria. We employ two criteria to evaluatethe performance of VQA models: PLCC and Spearmanrank-order correlation coefficient (SRCC). Note that PLCCassesses the prediction linearity of the VQA model, whileSRCC evaluates the prediction monotonicity. An outstand-ing VQA model should achieve SRCC and PLCC valuesclose to 1. Before computing PLCC, we adhere to the pro-cedure outlined in to map model predictions to MOSsby a monotonic four-parameter logistic function to compen-sate for prediction nonlinearity.",
  ". Experimental Results": "We list the experimental results in , from whichwe can obtain several conclusions. First, it is evident thatall knowledge-driven methods perform poorly on three so-cial media VQA datasets, suggesting that they lack the ca-pability to effectively evaluate the quality of social mediavideos. Second, the proposed model achieves the best per-formance on both the KVQ and LIVE-WC datasets, sur-passing competing BVQA methods by a substantial margin.This demonstrates that by incorporating rich quality-awarefeatures, the proposed model has more powerful feature rep-",
  ". Ablation Studies": "In this section, we investigate the effectiveness of fea-tures used in the proposed model. Specifically, we ablateQ-Align, LIQE, and FAST-VQA features from the proposedmodel respectively, and test them on the KVQ test set. The experimental results are listed in . From ,it is evident that regardless of which features are ablated,there is a performance degradation. When all features areintegrated, the proposed model achieves the highest perfor-mance, which validates the effectiveness of extracted fea-tures.",
  ". Conclusion": "In this paper, we attempt to enhance BVQA models withdiverse quality-aware features and propose a strong BVQAmodel for social media videos.We use SimpleVQA asthe base BVQA model and extract three kinds of quality-aware features from two BIQA models, LIQE and Q-Align,and one BVQA model, FAST-VQA. We simply concatenatethese features with SimpleVQA and then regress them intothe video quality score via a MLP network. Experimentalresults show that the proposed model achieves the best per-formance on three social media VQA datasets.",
  ". Acknowledgement": "This work was supported in part by the National Natu-ral Science Foundation of China under Grants 62071407,62301316,62225112,62376282 and 62271312,theChina Postdoctoral Science Foundation under Grants2023TQ0212 and 2023M742298, the Postdoctoral Fel-lowship Program of CPSF under Grant GZC20231618,the Fundamental Research Funds for the Central Uni-versities,the National Key R&D Program of China(2021YFE0206700), the Science and Technology Com-mission of Shanghai Municipality (2021SHZDZX0102),and the Shanghai Committee of Science and Technology(22DZ2229005).",
  "Gedas Bertasius, Heng Wang, and Lorenzo Torresani.Isspace-time attention all you need for video understanding?In ICML, volume 2, page 4, 2021. 4": "Chao Chen, Lark Kwon Choi, Gustavo De Veciana, Con-stantine Caramanis, Robert W Heath, and Alan C Bovik.Modeling the timevarying subjective quality of http videostreams with rate adaptations. IEEE Transactions on ImageProcessing, 23(5):22062221, 2014. 3 Francesca De Simone, Marco Tagliasacchi, Matteo Naccari,Stefano Tubaro, and Touradj Ebrahimi. A h. 264/avc videodatabase for the evaluation of quality metrics. In 2010 IEEEInternational Conference on Acoustics, Speech and SignalProcessing, pages 24302433. IEEE, 2010. 3",
  "No-reference video quality prediction via space-time chips.IEEE Transactions on Image Processing, 30:80598074,2021. 1, 3": "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, andKaiming He. Slowfast networks for video recognition. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 62026211, 2019. 2, 4, 6 Yixuan Gao, Yuqin Cao, Tengchuan Kou, Wei Sun, YunlongDong, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai.Vdpve: Vqa dataset for perceptual video enhancement. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 14741483, 2023. 3 Deepti Ghadiyaram, Alan C Bovik, Hojatollah Yeganeh, Ro-man Kordasiewicz, and Michael Gallant. Study of the ef-fects of stalling events on the quality of experience of mo-bile streaming videos. In 2014 IEEE Global Conference onSignal and Information Processing (GlobalSIP), pages 989993. IEEE, 2014. 3 Deepti Ghadiyaram, Janice Pan, Alan C Bovik, Anush Kr-ishna Moorthy, Prasanjit Panda, and Kai-Chieh Yang. In-capture mobile video distortions: A study of subjective be-havior and objective algorithms. IEEE Transactions on Cir-cuits and Systems for Video Technology, 28(9):20612077,2017. 1, 3 Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, HuiMen, Tamas Sziranyi, Shujun Li, and Dietmar Saupe. Thekonstanz natural video database (konvid-1k). In 2017 Ninthinternational conference on quality of multimedia experience(QoMEX), pages 16. IEEE, 2017. 1, 3",
  "Jari Korhonen.Two-level approach for no-reference con-sumer video quality assessment. IEEE Transactions on Im-age Processing, 28(12):59235938, 2019. 1, 3, 6, 7": "Dae Yeol Lee, Somdyuti Paul, Christos G Bampis, Hyun-suk Ko, Jongho Kim, Se Yoon Jeong, Blake Homan, andAlan C Bovik. A subjective and objective study of space-time subsampled video quality. IEEE Transactions on ImageProcessing, 31:934948, 2021. 3 Bowen Li, Weixia Zhang, Meng Tian, Guangtao Zhai, andXianpei Wang. Blindly assess quality of in-the-wild videosvia quality-aware pre-training and motion perception. IEEETransactions on Circuits and Systems for Video Technology,32(9):59445958, 2022. 1, 3, 4",
  "quality assessment: A subjective and objective study. IEEETransactions on Multimedia, 25:154166, 2021. 3": "Zhuoran Li, Zhengfang Duanmu, Wentao Liu, and ZhouWang. Avc, hevc, vp9, avs2 or av1?a comparative study ofstate-of-the-art video encoders on 4k videos. In Image Anal-ysis and Recognition: 16th International Conference, ICIAR2019, Waterloo, ON, Canada, August 2729, 2019, Proceed-ings, Part I 16, pages 162173. Springer, 2019. 3 Hongbo Liu, Mingda Wu, Kun Yuan, Ming Sun, YansongTang, Chuanchuan Zheng, Xing Wen, and Xiu Li. Ada-dqa:Adaptive diverse quality-aware feature acquisition for videoquality assessment. In Proceedings of the 31st ACM Interna-tional Conference on Multimedia, pages 66956704, 2023.1, 3, 4",
  "Wentao Liu, Zhengfang Duanmu, and Zhou Wang.End-to-end blind quality assessment of compressed videos usingdeep neural networks. In ACM Multimedia, pages 546554,2018. 3, 4": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 2, 4, 6 Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,Stephen Lin, and Han Hu. Video swin transformer. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 32023211, 2022. 4, 6 Wei Lu, Wei Sun, Zicheng Zhang, Danyang Tu, XiongkuoMin, and Guangtao Zhai. Bh-vqa: Blind high frame ratevideo quality assessment. In 2023 IEEE International Con-ference on Multimedia and Expo (ICME), pages 25012506.IEEE, 2023. 1 Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, YunpengQu, Ming Sun, Chao Zhou, and Zhibo Chen. Kvq: Kalei-doscope video quality assessment for short-form videos. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, 2024. 2, 3, 6",
  "Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-ing a completely blind image quality analyzer. IEEE Sig-nal processing letters, 20(3):209212, 2012. 6, 7": "Anush Krishna Moorthy, Lark Kwon Choi, Alan ConradBovik, and Gustavo De Veciana.Video quality assess-ment on mobile devices: Subjective, behavioral and objec-tive studies. IEEE Journal of Selected Topics in Signal Pro-cessing, 6(6):652671, 2012. 3 Rasoul Mohammadi Nasiri, Jiheng Wang, Abdul Rehman,Shiqi Wang, and Zhou Wang. Perceptual quality assessmentof high frame rate video. In 2015 IEEE 17th InternationalWorkshop on Multimedia Signal Processing (MMSP), pages16. IEEE, 2015. 3 Mikko Nuutinen, Toni Virtanen, Mikko Vaahteranoksa, TeroVuori, Pirkko Oittinen, and Jukka Hakkinen. Cvd2014adatabase for evaluating no-reference video quality assess-ment algorithms. IEEE Transactions on Image Processing,25(7):30733086, 2016. 1, 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 4",
  "Zeina Sinno and Alan Conrad Bovik. Large-scale study ofperceptual video quality. IEEE Transactions on Image Pro-cessing, 28(2):612627, 2018. 1, 3": "Wei Sun, Xiongkuo Min, Wei Lu, and Guangtao Zhai. Adeep learning based no-reference quality assessment modelfor ugc videos. In Proceedings of the 30th ACM Interna-tional Conference on Multimedia, pages 856865, 2022. 1,3, 4, 5, 6, 7 Wei Sun, Tao Wang, Xiongkuo Min, Fuwang Yi, andGuangtao Zhai.Deep learning based full-reference andno-reference quality assessment models for compressed ugcvideos. In 2021 IEEE International Conference on Multime-dia & Expo Workshops (ICMEW), pages 16. IEEE, 2021.1 Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, GuangtaoZhai, and Kede Ma. Analysis of video quality datasets viadesign of minimalistic video quality models. IEEE Transac-tions on Pattern Analysis and Machine Intelligence, 2024. 1,3, 4, 5, 6",
  "quality assessment for user generated content. IEEE Trans-actions on Image Processing, 30:44494464, 2021. 1, 3, 6,7": "Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck,Balu Adsumilli, and Alan C Bovik.Rapique: Rapid andaccurate video quality prediction of user generated content.IEEE Open Journal of Signal Processing, 2:425440, 2021.1, 3, 6, 7 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 2, 4",
  "Phong V Vu and Damon M Chandler.Vis 3: An algo-rithm for video quality assessment via analysis of spatialand spatiotemporal slices. Journal of Electronic Imaging,23(1):013016013016, 2014. 3": "Yilin Wang, Sasi Inguva, and Balu Adsumilli. Youtube ugcdataset for video compression research. In 2019 IEEE 21stInternational Workshop on Multimedia Signal Processing(MMSP), pages 15. IEEE, 2019. 1, 3 Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim,Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, and FengYang.Rich features for perceptual quality assessment ofugc videos.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1343513444, 2021. 1, 3, 4",
  "Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, LiZhang, and Kede Ma. Modular blind video quality assess-ment. arXiv preprint arXiv:2402.19276, 2024. 2": "Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao,Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Fast-vqa: Efficient end-to-end video quality assessment with frag-ment sampling. In European conference on computer vision,pages 538554. Springer, 2022. 1, 2, 3, 4, 6 Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-wen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and WeisiLin.Exploring video quality assessment on user gener-ated contents from aesthetic and technical perspectives. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 2014420154, 2023. 4, 6, 7 Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-wen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and WeisiLin. Towards explainable in-the-wild video quality assess-ment: A database and a language-prompted approach. InProceedings of the 31st ACM International Conference onMultimedia. ACM, 2023. 3 Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, QiongYan, Guangtao Zhai, et al.Q-bench:A benchmarkfor general-purpose foundation models on low-level vision.arXiv preprint arXiv:2309.14181, 2023. 4",
  "visual abilities for multi-modality foundation models. arXivpreprint arXiv:2311.06783, 2023. 4": "Haoning Wu, Zicheng Zhang, Weixia Zhang, ChaofengChen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang,Erli Zhang, Wenxiu Sun, Yan Qiong, Min Xiongkuo, ZhaiGuangtao, and Lin Weisi. Q-align: Teaching lmms for vi-sual scoring via discrete text-defined levels. arXiv preprintarXiv:2312.17090, 2023. 1, 2, 4, 7 Fengchuang Xing, Yuan-Gen Wang, Hanpin Wang, Leida Li,and Guopu Zhu.Starvqa: Space-time attention for videoquality assessment. In 2022 IEEE International Conferenceon Image Processing (ICIP), pages 23262330. IEEE, 2022.1 Fuwang Yi, Mianyi Chen, Wei Sun, Xiongkuo Min, YuanTian, and Guangtao Zhai. Attention based network for no-reference ugc video quality assessment. In 2021 IEEE In-ternational Conference on Image Processing (ICIP), pages14141418. IEEE, 2021. 1, 3, 4 Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram,and Alan Bovik.Patch-vq:patching upthe video qual-ity problem. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1401914029, 2021. 1, 3, 4, 6 Xiangxu Yu, Neil Birkbeck, Yilin Wang, Christos G Bampis,Balu Adsumilli, and Alan C Bovik. Predicting the qualityof compressed videos with pre-existing distortions.IEEETransactions on Image Processing, 30:75117526, 2021. 3,6 Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang,and Kede Ma. Blind image quality assessment via vision-language correspondence: A multitask learning perspective.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1407114081, 2023. 2 Zicheng Zhang, Wei Lu, Wei Sun, Xiongkuo Min, TaoWang, and Guangtao Zhai. Surveillance video quality as-sessment based on quality related retraining. In 2022 IEEEInternational Conference on Image Processing (ICIP), pages42784282. IEEE, 2022. 2",
  "Zicheng Zhang, Wei Sun, Yucheng Zhu, Xiongkuo Min, WeiWu, Ying Chen, and Guangtao Zhai. Evaluating point cloudfrom moving camera videos: A no-reference metric. IEEETransactions on Multimedia, 2023. 2": "Zicheng Zhang, Wei Wu, Wei Sun, Danyang Tu, Wei Lu,Xiongkuo Min, Ying Chen, and Guangtao Zhai. Md-vqa:Multi-dimensional quality assessment for ugc live videos. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 17461755, 2023. 3,6 Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, andGuangtao Zhai. Geometry-aware video quality assessmentfor dynamic digital human.In 2023 IEEE InternationalConference on Image Processing (ICIP), pages 13651369.IEEE, 2023. 2"
}