{
  "Abstract": "Model Inversion (MI) attacks aim to reconstruct pri-vate training data by abusing access to machine learn-ing models. Contemporary MI attacks have achieved im-pressive attack performance, posing serious threats to pri-vacy.Meanwhile, all existing MI defense methods relyon regularization that is in direct conflict with the train-ing objective, resulting in noticeable degradation in modelutility.In this work, we take a different perspective,and propose a novel and simple Transfer Learning-basedDefense against Model Inversion (TL-DMI) to render MI-robust models. Particularly, by leveraging TL, we limit thenumber of layers encoding sensitive information from pri-vate training dataset, thereby degrading the performanceof MI attack.We conduct an analysis using Fisher In-formation to justify our method. Our defense is remark-ably simple to implement.Without bells and whistles,we show in extensive experiments that TL-DMI achievesstate-of-the-art (SOTA) MI robustness.Our code, pre-trained models, demo and inverted data are available at:",
  ". Introduction": "Model Inversion (MI) attack is a type of privacy threat thataim to reconstruct private training data by exploiting accessto machine learning models. State-of-the-art (SOTA) MIattacks have demonstrated increased ef-fectiveness, achieving attack performance of over 90% inface recognition benchmarks. The implications of this vul-nerability are particularly concerning in security-critical ap-plications .The aim of our work is to propose new perspective todefend against MI attacks and to improve MI robustness.In particular, MI robustness pertains to the tradeoff be-tween MI attack accuracy and model utility. MI robust-",
  "Work done while at SUTD": "ness involves two critical considerations: Firstly, a MI ro-bust model should demonstrate a significant reduction inMI attack accuracy, making it difficult for adversaries toreconstruct private training samples. Secondly, while de-fending against MI attacks, the natural accuracy of a MIrobust model should remain competitive. A model with im-proved MI robustness ensures that it is resilient to MI whilemaintaining its utility.Research gap. Despite the growing threat arising fromSOTA MI, there are limited studies on defending againstMI attacks and improving MI robustness. Conventionally,differential privacy (DP) is used for ensuring the privacy ofindividuals in datasets. However, DP has been shown tobe ineffective against MI . Meanwhile, a fewMI defense methods have been proposed. Particularly, allexisting SOTA MI defense methods are based on the ideaof dependency minimization regularization : theyintroduce additional regularization into the training objec-tive, with the goal of minimizing the dependency betweeninput and output/latent representation. The underlying ideaof these works is to reduce correlation between input andoutput/latent, which MI attacks exploit during the inver-sion. However, reducing correlation between input and out-put/latent directly undermines accuracy of the model, re-sulting in considerable degradation in model utility . Topartially restore the model utility, BiDO proposes tofurther introduce another regularization to compensate forthe reduced correlation between input and latent. However,with two additional regularization along with the originaltraining objective, BiDO requires significant effort in hyper-parameter tuning based on intensive grid search , and issensitive to small changes in hyperparameters (see our anal-ysis in Supp.)In this paper, our main hypothesis is that a model withfewer parameters encoding sensitive information from pri-vate training dataset (Dpriv) could achieve better MI ro-bustness.Based on that, we propose a novel TransferLearning-based Defense against Model Inversion (TL-DMI) ().Leveraging on standard two-stages TLframework , with pre-training on public dataset as",
  "arXiv:2405.05588v1 [cs.LG] 9 May 2024": ". (I) Our proposed Transfer Learning-based Defense against Model Inversion (TL-DMI) (Sec. 3). Based on standard TLframework with pre-training (on public dataset) followed by fine-tuning (on private dataset), we propose a simple and highly-effectivemethod to defend against MI attacks. Our idea is to limit fine-tuning with private dataset to a specific number of layers, thereby limiting theencoding of private information to these layers only (pink). Specifically, we propose to perform fine-tuning only on the last several layers.(II) Analysis of layer importance for classification task and MI task (Sec. 4.2). For the first time, we analyze importance of target modellayers for MI. For a model trained with conventional training, we apply FI and find that the first few layers of the model are important forMI. Meanwhile, FI analysis suggests that last several layers are important for a specific classification task, consistent with TL literature. This supports our hypothesis that preventing the fine-tuning of the first few layers on private dataset could degrade MI significantly,while such impact for classification could be small. Overall, this leads to improved MI robustness. (III) Empirical validation (Sec. 4.3).The sub-figures clearly show that at the same natural accuracy, lower MI attack accuracy can be achieved by reducing the number ofparameters fine-tuned with private dataset. (IV) Comparison with SOTA MI Defense (Sec. 4.4). Without bells and whistles, our methodachieves SOTA in MI robustness. Visual quality of MI-reconstructed images from our model is inferior. User study confirms this finding.Extensive experiments can be found in Sec. 4.5. Best viewed in color with zooming in. the first stage and fine-tuning on private dataset as the sec-ond stage, we propose to limit private dataset fine-tuningonly on a specific number of layers. Specifically, in the sec-ond stage, we perform private dataset fine-tuning only onthe last several layers of the model. The first few layers are frozen during the second stage, preventing private in-formation encoded in these layers. We hypothesize that byreducing the number of parameters fine-tuned with privatedataset, we could reduce the amount of private informationencoded in the model, making it more difficult for adver- saries to reconstruct private training data.To justify our design, we conduct, for the first time, ananalysis of model layer importance for the MI task. Wepropose to apply Fisher Information (FI) to quantify im-portance of individual layers for MI . Our analysissuggests that first few layers are important for MI. There-fore, by preventing private information encoded in the firstfew layers as in our proposed method, we could degrade MIsignificantly. Meanwhile, during pre-training, the first fewlayers learn low level information (edges, colour blobs). Itis known that low level information is generalizable acrossdatasets . Therefore, our proposed TL-DMI has onlysmall degrade in model utility.Overall, TL-DMI couldachieve SOTA MI robustness. We remark that TL-DMI isvery easy to implement. In our experiments, we apply TL-DMI to a range of models (CNN, vision transformers), seeSec. 4.5. On the contrary, BiDO has been applied to onlyVGG16 and ResNet-34 . Our contributions are: We propose a simple and highly effective TransferLearning-based Defense against Model Inversion (TL-DMI). Our idea is a novel and major departure fromexisting MI defense based on dependency minimizationregularization. Furthermore, while majority of TL workfocuses on improving model accuracy , our workfocuses on degrading MI attack accuracy via TL. We conduct the first study to analyze layer importancefor MI task via Fisher Information. Our analysis resultssuggest that the first few layers are important for MI,justifying our design to prevent private information en-coded in the first few layers. We conduct empirical analysis to validate that lower MIattack accuracy can be achieved by reducing the numberof parameters fine-tuned with private dataset. Our anal-ysis carefully removes the influence of natural accuracyon MI attack accuracy. We conduct comprehensive experiments to show thatour proposed TL-DMI achieves SOTA MI robustness.As TL-DMI is remarkably easy to implement, we extendour experiments for a wide range of model architecturessuch as vision transformer , which MI robustnesshas not been studied before.",
  ". Background": "The target model T is trained on a private training datasetDpriv = {(xi, yi)Ni=1}, where xi RdX is the facial imageand yi {0, 1}K is the identity. The target classifier T isa K-way classifier T: RdX RK, with the parametersT Rd.Model Inversion (MI) Attack. In MI attacks, an adver-sary exploits a target model T trained on a private datasetDpriv. However, Dpriv should not be disclosed. The maingoal of MI attacks is to extract information about the pri-vate samples in Dpriv. The existing literature formulates MI attacks as a process of reconstructing an input x that Tis likely to classify into the preferred class (label) y. Thisstudy primarily focuses on whitebox MI attacks, which arethe most dangerous, and can achieve impressive attack ac-curacy since the adversary has complete access to the targetmodel. For high-dimensional data like facial images, thereconstruction problem is challenging. To mitigate this is-sue, SOTA MI techniques suggest reducing the explorationarea to the meaningful and pertinent images manifold us-ing a GAN. Under white-box MI, the adversary can accessT(x), the K-dim vector of soft output, and public datasetDpub used to train GAN. The Eq. 1 represents the step ofexisting SOTA white-box MI attacks . Thedetails for SOTA MI attacks can be found in the Supp.",
  "w = arg minw ( log PT (y|G(w)) + Lprior(w))(1)": "where log PT (y|G(w)) denotes identity loss in MI at-tack, which guides the reconstructed x = G(w) that is mostlikely to be classified as class y by T. G refers to generatorto generate reconstructed data x from latent vector w. TheLprior is the prior loss, which makes use of public infor-mation to learn a distributional prior through a GAN. Thisprior is used to guide the inversion process to reconstructmeaningful images. The hyper-parameter is to balanceprior loss and identity loss.Model Inversion (MI) Defense. In contrast, the MI de-fense aims at minimizing the disclosure of training sam-ples during the MI optimization process. First MI-specificdefense strategy is MID , which adds a regularizationd(x, T(x)) to the main objective during the target classi-fiers training to penalize the mutual information betweeninputs x and outputs T(x). Another approach is BilateralDependency Optimization (BiDO) , which minimizesd(x, f) to reduce the amount of information about inputsx embedded in feature representations f, while maximiz-ing d(f, y) to provide f with enough information about yto restore the natural accuracy. However, both MID andBiDO suffer from the drawback that their regulariza-tion, i.e., d(x, T (x)) for MID and d(x, f) for BiDO,conflict with the main training objective, resulting inan explicit trade-off between MI robustness and modelutility. BiDO improves this trade-off with d(f, y) but ishyperparameter-sensitive due to the optimization of threeobjectives, making it difficult to apply.Model inversion (MI) vs. Membership inference. Be-side MI, membership inference is an-other privacy attack on machine learning models. However,the focus of our work, i.e., vision MI attacks, is funda-mentally different from membership inference attacks.In a membership inference attack, the attackers objectiveis to determine whether a specific data point was part ofthe training dataset used to train the target model. Mem-",
  "Fine-tune only C with standard ob-jective on Dpriv": ". Training procedure for no defense, existing MI defense methods and our proposed TL-DMI. Stage 1 (pre-training) iscommonly used in existing methods to reduce the requirement for labeled datasets. TL-DMI takes advantage of such setup to defend MI. bership inference attacks are typically formulated as a pre-diction problem, where an attacker model is trained to out-put the probability of a given data point being a memberof the training dataset. In contrast, vision model inversionattacks are usually formulated as an image reconstructionproblem. The attacker aims to output the reconstruction ofhigh-dimensional training images. While membership in-ference attacks are limited to determining membership sta-tus (in or out of the training dataset) and may not providefine-grained information about the training data, model in-version attacks attempt to recover the training data itself,which can be more invasive .",
  ". Transfer Learning-based Defense againstModel Inversion (TL-DMI)": "Transfer Learning (TL). TL is an effective ap-proach to leverage knowledge learned from a general taskto enhance performance in a different task. By perform-ing pre-training on a large general dataset and then fine-tuning on a target dataset, TL mitigates the demand forlarge labeled datasets, while simultaneously improving gen-eralization and overall performance. In machine learning,TL works mostly focus on improving the model perfor-mance by adapting the knowledge to new tasks and domains.Our proposed defense TL-DMI. In contrast, our work isthe first to apply TL to defend against MI attacks aiming atdegrading MI attack accuracy. Therefore, our study is fun-damentally different from existing TL works which aim toimprove model utility . Our idea is toapply TL to reduce the leak of private information by lim-iting the number of parameters updated on private trainingdata. Specifically, as illustrate in , we propose to trainthe target model T as T = C E in two stages: pre-trainingand then fine-tuning. Particularly, in the fine-tuning stage,E comprises parameters that are frozen, i.e., not updatedby the private dataset Dpriv, while C comprises parametersthat are updated by Dpriv. Stage 1: Pre-training with Dpretrain. We first pre-train T using a dataset Dpretrain. Dpretrain can be ageneral domain dataset, e.g., Imagenet1K, or it can besimilar domain as the private dataset Dpriv. Importantly,Dpretrain has no class/identity intersection with Dpriv.",
  "Both C and E are updated based on Dpretrain in thisstage": "Stage 2: Fine-tuning with Dpriv. To adapt the pre-trained model from Stage 1 for Dpriv, we freeze E, i.e.parameters of E are unchanged. We only update C withDpriv.Tab. 1 provides a comparison between our defense TL-DMI and existing MI defenses. We remark that pre-traininghas already been commonly adopted in previous works ofMI attack. Therefore, in many cases, our method does notincur additional overhead . As an example,we consider the main setup of BiDO where VGG16 is used as the target classifier T. Following the pre-vious works on MI attack, T including E and C are firstpre-trained on Dpretrain = Imagenet1K . Then, for TL-DMI, we fine-tune C with Dpriv = CelebA while E isfrozen. In contrast, for other MI defense, both E and C areupdated with Dpriv. We explore the design of T with differ-ent number of layers updated by Dpriv, leading to differentnumber of parameters in C (|C|) updated by Dpriv. Usingdifferent |C|, we limit the amount of private informationencoded in the parameters of T. We show that our approachTL-DMI improves MI robustness.Regarding hyperparameter in our proposed TL-DMI, wedetermine |C| by simply deciding at the layer-level of adeep neural network.Note that during training we usethe same objective of classification task, i.e. no change intraining objective is needed. Therefore, TL-DMI is muchsimpler and faster than SOTA MI defense BiDO (seeSupp.). In Sec. 4.2, we present our Fisher Information-based analysis to justify TL-DMI.",
  ". Exploring MI Robustness via TransferLearning": "We introduce the experiment setup in Sec. 4.1. In Sec. 4.2,we provide the first analysis on layer importance for MI taskvia Fisher Information suggesting that earlier layers are im-portant for MI. Then, Sec. 4.3 empirically validate that MIrobustness is obtained by reducing the number of parame-ters fine-tuned with private dataset. With the established un-derstandings, we then compare our proposed method withcurrent SOTA MI defenses in Sec. 4.4.Addi-tionally, since our method offer higher practicality com- pared with the SOTA MI defenses, we extensively accessour approach on 20 MI attack setups in Sec. 4.5 and Supp.,spanning 9 architectures, 4 private datasets Dpriv, 3 publicdatasets Dpub, and 7 MI attacks.While the above sections assume a consistent pre-traineddataset Dpretrain for the target classifier to ensure fair com-parison with existing works, we also delve into novel anal-ysis on the effect of various Dpretrain on MI robustness.We observe that less similarity between pretrain and privatedataset domains can improve defense effectiveness. The de-tails for this analysis can be found in Supp.",
  ". Experimental Setup": "To ensure a fair comparison, our study strictly fol-lows setups in SOTA MI defense method BiDO indatasets, attack methods, and network architectures.Furthermore, we also examine our defense approach withadditional new datasets, recent MI attack models, and newnetwork architectures. Note that these have not been in-cluded in BiDO. All the MI setups in our study are summa-rized in Tab. 2. The details for the setup can be found inSupp.MI Defense Baseline. In order to showcase the effi-cacy of our proposed TL-DMI, we compare TL-DMI withseveral existing SOTA model inversion defense methods,which are BiDO and MID .Evaluation Metrics.Following the previous MI de-fense/attack works, we adopt natural accuracy (Acc), AttackAccuracy (AttAcc), K-Nearest Neighbors Distance (KNNDist), and 2 distance metrics to evaluate MI robustness.Moreover, we also provide qualitative results and user studyin the Supp.",
  ". Analysis of Layer Importance for ClassificationTask and MI Task": "In this section, we provide an analysis to justify our pro-posed TL-DMI to render MI robustness. We aim to under-stand importance of individual layers for MI reconstructiontask, justifying our design in TL-DMI to prevent encodingof private data information in the first few layers as an ef-fective method to degrade MI. We study layer importancebetween classification and MI tasks. To quantify the impor-tance, we compute the Fisher Information (FI) for the twotasks for individual layers.Fisher Information (FI) based analysis. Fisher Infor-mation F has been applied to measure the importance ofmodel parameters for discriminative task and gener-ative task . For example, in , FI has been appliedto determine the importance of model parameters to over-come the catastrophic forgetting in continual learning. Ourstudy extends FI-based analysis for model inversion, whichhas not been studied before. Specifically, given a model Tparameterized by T and input X, FI can be computed as",
  "MIRROR FFHQ VGGFace2 ResNet-50": ". Setups of our comprehensive experiments. We followthe exact setups in the previous MI attacks. Beside the standardMI setups on GMI /KEDMI on VGG16, and VMI on Resnet-34, we also evaluate our defense approach on currentSOTA MI setups. Due to the need of intensive grid-search forhyper-paramters, it is very time consuming to expand the exisitingSOTA MI Defense to these additional MI setups. In total,there are 20 MI setups spanning 7 MI attacks, 3 Dpub, 4 Dpriv, 9architectures of T. The experimental setups are described in moredetail in the Supp.",
  "(2)": "Here, L is the loss function for a particular task. Specifi-cally, we investigate FI on classification task and MI task.For classification, we follow Achille et al. and Le et al. to use cross entropy E [ log p(yi|xi)] as L and vali-dation set Dvalpriv = {(xi, yi)Mi=1} as X. For MI task, wepropose to use the 2 distance between the feature represen-tations of reconstructed images and the private images as L:",
  "(3)": "Here, for a given input image, computes the penulti-mate layer representation using the target model, and xjuis one of the MI reconstructed images for identity j, andE(xjpriv)is the centroid feature of private images foridentity j. Therefore, we use the distance between MI re-constructed image and private image of the same identity asthe loss in FI analysis. The set of MI reconstructed images{xju}Jj=1 for different identity is used as X. We exploredifferent setups to compute L, see Supp. In one setup, weperform FI analysis only at the last iteration (i.e., 3000, forthe result in -II). As we are interested in FI at the layerlevel, we compute the average FI of all parameters within alayer. We use the main MI attack setup in Peng et al. ,i.e., VGG16 with KEDMI attack, for FI analysis. Observation. The FI results in -II clearly suggestthat the first few layers of a target model are important forMI task. Meanwhile, FI analysis suggests that the first fewlayers do not carry important information for a specific clas-sification task. This observation is consistent with previousfinding in work suggesting that the earlier layers carrygeneral features. The FI analysis justifies our design to pre-vent encoding of private information in the first few layersin order to degrade MI attacks, while keeping the impacton classification small. Overall, this leads to improved MIrobustness. Further results with different loss (1 andLPIPS ) and different MI iterations can be found inSupp.",
  ". Empirical Validation": "As shown in -IV, we observe a significant improve-ment in MI robustness when reducing the number of pa-rameters fine-tuned with Dpriv. However, the relationshipbetween MI attack accuracy and natural accuracy is stronglycorrelated , which makes it unclear if the decrease in MIattack accuracy is due to the drop in natural accuracy.In this section, we empirically investigate the hypothesisthat a model with fewer parameters encoding private infor-mation from Dpriv has better MI robustness. The empiri-cal validation is reported in -III. Note that the num-ber of parameters for the entire target model: |C| = 16.8Mfor VGG16 with KEDMI setup and |C| = 11.7M forResnet-18 with PPA setup. The additional empiricalvalidation for GMI can be found in the Supp. To separatethe influence of model accuracy on MI attack accuracy, weperform PPA/KEDMI attacks on different checkpoints foreach training setup, varying a wide range of natural accu-racy. This is presented by multiple data points on each line.The results clearly show that fine-tuning fewer parame-ters on Dpriv enhances MI robustness compared with fine-tuning all parameters on Dpriv, regardless of the effect onnatural accuracy. For instance, in the KEDMI setup, with acomparable natural accuracy of 83%, fine-tuning only |C|= 13.9M reduces a third attack accuracy compared to fine-tuning |C| = 16.8M. The result in the PPA setup is evenmore supportive, where with a natural accuracy of around91%, fine-tuning |C| = 8.9M reduces the attack accuracyto 22.36% from 91.7% in |C| = 11.7M.Across all configurations, we observe that the fewer pa-rameters fine-tuned on Dpriv, the more robust the model.However, it is important to note that if the number of fine-tuned parameters on Dpriv is insufficient, such as |C| =9.1M for KEDMI setup, the models natural accuracy maydrop drastically, rendering it unusable. Overall, our experi-ments strongly suggest that better MI robustness can beachieved by reducing the number of parameters fine-tuned on Dpriv.",
  ". Comparison with SOTA MI Defense": "We provide a comprehensive comparisons between our pro-posed TL-DMI and BiDO and MID under 6 attacks: VMI,LOMMA, PPA, KEDMI, GMI, and BREPMI. To avoid theeffect of randomness in our comparison, we calculate un-der 3 attacks of different random seeds. We summarize thecomparison in Tab. 10. All results consistently supportthat TL-DMI outperforms BiDO and MIDFor BiDO reproducibility, we follow the exact hyper-paremeters from their work. Note that BiDO is the best de-fense by far, but it requires extensive grid-search for hyper-parameters. For MID reproducibility, we adopt their imple-mentation and hyperparameters. Furthermore, we providethe results for MID with different hyperparameter choicesin Tab. 8.",
  "ResNet-101 No Def.94.8683.00-BiDO90.3167.263.46TL-DMI90.1031.8210.75": ".The comparison between our proposed TL-DMI andSOTA MI denfense BiDO , where the Acc and AttAcc aregiven in %.Our evaluation covers a wide range of MI at-tack setups.We follow previous work for MI setups (see de-tails in Tab. 2 and Supp.).To implement TL-DMI, we set|C| = 21.14M/13.90M/8.90M/16.05M for T = ResNet-34/VGG-16/ResNet-18/ResNet-101, respectively. MI robustness is quan-tified by the , the ratio of drop in attack accuracy to drop innatural accuracy. As shown in the results, our proposed TL-DMIsignificantly improves MI robustness comparing to BiDO.",
  ". Extended MI Robustness Evaluation": "Our proposed TL-DMI is simple, easy to implement, andless sensitive to hyperparameters than BiDO, which re-quires intensive grid search for hyperparameter. This signif-icant advantage allows us to extend the scope of experimen-tal setups for the MI defense to align with the remarkableincrease in MI attack setups, which are not yet evaluated inprevious MI defenses .Results on different Dpub.We evaluate TL-DMIagainst KEDMI and GMI attacks on three architectures(VGG16, IR152, FaceNet64) with varying public datasets(CelebA, FFHQ), spanning 12 facial domain MI setups.These are standard setups in KEDMI/GMI, however, only2 out of 12 setups examined in the current SOTA MI de-fense were presented in . The results in Tab. 4 demon-strate that TL-DMI consistently achieves significantly morerobust models across all setups while maintaining accept-able natural accuracy, with significant improvements in ro-bustness across a wide range of attack scenarios (13.33%-42.60% for KEDMI, 11.14%-31.94% for GMI). On aver-age, TL-DMI significantly reduces the accuracy of MI at-tacks by more than a half.Results on SOTA high resolution MI attacks. Further-more, we provide our defense results against SOTA HighResolution MI attacks, i.e., PPA and MIRROR inTab. 3 and Tab. 5. To the best of our knowledge, our work isthe first MI defense approach against such high resolutionMI attack. The results are very encouraging. We observeonly a small reduction in natural accuracy, while the attack accuracy experiences a significant drop thanks to our de-fense TL-DMI.Results on different architectures of T . Unlike BiDO,TL-DMI does not require an intensive grid search for hy-perparameter selection for a specific architecture. There-fore, TL-DMI offers high practicality and is readily appli-cable to a range of architectures, whereas existing state-of-the-art MI defenses lack this advantage . We conductevaluations on a range of architectures, including residual-based networks such as ResNet-18/50/101, ResNeSt-101,IR152, as well as the more recent MaxViT architecture .Across all these experiments in Tab. 5 and Tab. 3, TL-DMIconsistently demonstrate superior performance, highlight-ing its effectiveness and robustness across various architec-tures.Result on different Dpriv. Regarding private datasetDpriv, in addition to CelebA, which is standard for MI re-search, and other large-scale facial datasets including Face-scrub and VGGFace2 , our experiments go beyondthese datasets by studying the animal domain, i.e., StanfordDogs dataset . The result is illustrated in Tab. 5. Via ourcomprehensive evaluation, we find that our approach con-sistently demonstrates its efficacy across various datasets,regardless multiple factors such as the number of train-ing/attack classes or the specific domain under considera-tion. This versatility highlights the robustness and adapt-ability of our defense TL-DMI across a wide range of sce-narios.Overall, all these extensive results consistently supportthat our method is effective in defending against advancedMI attacks. Our approach is simple and can be easily ap-plied, with minimal changes to the original training of targetclassifier T. Additional results and analysis are includedin the Supp.",
  ". Conclusion": "In this paper, we propose a simple and highly effectiveTransfer Learning-based Defense against Model Inversion(TL-DMI). Our method is a major departure from existingMI defense based on dependency minimization regulariza-tion. Our main idea is to leverage TL to limit the number oflayers encoding private data information, thereby degrad-ing the performance of MI attacks. To justify our method,we conduct the first study to analyze layer importance forMI task via Fisher Information. Our analysis results sug-gest that the first few layers are important for MI, justify-ing our design to prevent private information encoded in thefirst few layers. Our defense TL-DMI is remarkably simpleto implement. Through extensive experiments, we demon-strate SOTA effectiveness of TL-DMI across 20 MI setupsspanning 9 architectures, 4 private datasets Dpriv, and 7 MIattacks.",
  "FaceNet64 No Def.35.4/35.488.5013.13 4.9630.33 5.40 1746TL-DMI34.4/35.483.612.60 1.498.67 3.642009": ". Our evaluation covers multiple MI attack setups, target models, and public, private and pre-trained datasets. Here, the resultsare given in %. Specifically, we reports the MI defense results against different MI attack methods (KEDMI and GMI), as well as usingdifferent public datasets Dpub (CelebA and FFHQ), and pre-trained datasets Dpretrain (Imagenet1K and MS-CelebA-1M), for severaltarget model T: VGG16, IR152, FaceNet64.",
  "MIRRORVGGFace2ResNet-50No Def.99.4484.00--602.41-TL-DMI99.4050.00--650.28-": ". The defense results for SOTA MI attacks on 224x224 images. We strictly follow experimental setups from PPA and MIRROR,presenting results for Acc and AttAcc in %. Additionally, we employ PPA-introduced metrics, F aceNet and Eval, alongside MIRROR-introduced metric l2 Dist for the evaluation. Our proposed TL-DMI successfully defends against SOTA MI attacks on high resolution224x224. To train our TL-DMI defense models, we set |C| = 18.3M/27.9M/32.9M for T = MaxViT/ResNeSt-101/ResNet-50, respectively. Limitation. Following other MI attack and defense re-search , our current focus is on classifica-tion. However, our future work will extend to studying MIattacks and defenses for other machine learning tasks, suchas object detection. Ethical consideration. Our research on improving MIrobustness addresses a significant ethical concern in mod-ern data-driven machine learning: data privacy. Our studyis based on publicly available standard data and does notinvolve the collection of sensitive information. Acknowledgement. This research is supported by the Na-tional Research Foundation, Singapore under its AI Sin-gapore Programmes (AISG Award No.: AISG2-TC-2022-007); The Agency for Science, Technology and Research(A*STAR) under its MTC Programmatic Funds (Grant No.M23L7b0021). This material is based on the research/worksupport in part by the Changi General Hospital and Sin-gapore University of Technology and Design, under theHealthTech Innovation Fund (HTIF Award No.CGH-SUTD-2021-004). Milad Abdollahzadeh, Touba Malekzadeh, Christopher THTeo, Keshigeyan Chandrasegaran, Guimeng Liu, and Ngai-Man Cheung.A survey on generative modeling withlimited data, few shots, and zero shot.arXiv preprintarXiv:2307.14397, 2023. 1 Alessandro Achille, Michael Lam, Rahul Tewari, AvinashRavichandran, Subhransu Maji, Charless C Fowlkes, Ste-fano Soatto, and Pietro Perona. Task2vec: Task embeddingfor meta-learning. In Proceedings of the IEEE/CVF inter-national conference on computer vision, pages 64306439,2019. 5 Shengwei An, Guanhong Tao, Qiuling Xu, Yingqi Liu,Guangyu Shen, Yuan Yao, Jingwei Xu, and Xiangyu Zhang.Mirror: Model inversion for deep learning network with highfidelity. In Proceedings of the 29th Network and DistributedSystem Security Symposium, 2022. 3, 4, 5, 7, 16, 17, 18 Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and An-drew Zisserman. Vggface2: A dataset for recognising facesacross pose and age. In 2018 13th IEEE international con-ference on automatic face & gesture recognition (FG 2018),pages 6774. IEEE, 2018. 5, 7, 16 Xuankai Chang, Wangyou Zhang, Yanmin Qian, JonathanLe Roux, and Shinji Watanabe. End-to-end multi-speakerspeech recognition with transformer. In ICASSP 2020-2020IEEE International Conference on Acoustics, Speech andSignal Processing (ICASSP), pages 61346138. IEEE, 2020.1 Si Chen, Mostafa Kahla, Ruoxi Jia, and Guo-Jun Qi.Knowledge-enriched distributional model inversion attacks.In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 1617816187, 2021. 1, 3, 4, 5, 6, 8,15, 16, 17, 18, 19, 20 Yu Cheng, Jian Zhao, Zhecan Wang, Yan Xu, KarlekarJayashree, Shengmei Shen, and Jiashi Feng. Know you atone glance: A compact vector representation for low-shotlearning.In Proceedings of the IEEE International Con-ference on Computer Vision Workshops, pages 19241932,2017. 5, 16, 19 Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.Stargan v2: Diverse image synthesis for multiple domains.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 81888197, 2020. 5",
  "E Dataset. Novel datasets for fine-grained image categoriza-tion. In First Workshop on Fine Grained Visual Categoriza-tion, CVPR. Citeseer. Citeseer. Citeseer, 2011. 5, 7": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 4, 16 Jonas Dippel, Steffen Vogler, and Johannes Hohne. Towardsfine-grained visual representations by combining contrastivelearning with image reconstruction and attention-weightedpooling. arXiv preprint arXiv:2104.04323, 2021. 1",
  "Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis,Michele Wessa, Paolo Brambilla, Pauline Favre, Mircea": "Polosan, Colm Mcdonald, Camille Marie Piguet, et al. Con-trastive learning with continuous proxy meta-data for 3d mriclassification. In Medical Image Computing and ComputerAssisted InterventionMICCAI 2021:24th InternationalConference, Strasbourg, France, September 27October 1,2021, Proceedings, Part II 24, pages 5868. Springer, 2021.1 Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin,David Page, and Thomas Ristenpart. Privacy in pharmaco-genetics: An end-to-end case study of personalized warfarindosing. In 23rd USENIX Security Symposium (USENIX Se-curity 14), pages 1732, 2014. 1 Arthur Gretton, Olivier Bousquet, Alex Smola, and BernhardScholkopf. Measuring statistical dependence with hilbert-schmidt norms. In Algorithmic Learning Theory: 16th Inter-national Conference, ALT 2005, Singapore, October 8-11,2005. Proceedings 16, pages 6377. Springer, 2005. 16",
  "Arthur Gretton, Ralf Herbrich, Alexander Smola, OlivierBousquet, Bernhard Scholkopf, et al. Kernel methods formeasuring independence. 2005. 16": "Jianzhu Guo, Xiangyu Zhu, Chenxu Zhao, Dong Cao, ZhenLei, and Stan Z Li. Learning meta face recognition in un-seen domains. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 61636172, 2020. 1 Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, andJianfeng Gao. Ms-celeb-1m: A dataset and benchmark forlarge-scale face recognition.In Computer VisionECCV2016: 14th European Conference, Amsterdam, The Nether-lands, October 11-14, 2016, Proceedings, Part III 14, pages87102. Springer, 2016. 16 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3, 5, 6, 7, 16, 19 Xinlei He, Hongbin Liu, Neil Zhenqiang Gong, and YangZhang.Semi-leak: Membership inference attacks againstsemi-supervised learning. In European Conference on Com-puter Vision, pages 365381. Springer, 2022. 3 Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu,Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang.Curricularface: adaptive curriculum learning loss for deepface recognition.In proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages59015910, 2020. 1",
  "Uday Kamath, John Liu, James Whitaker, Uday Kamath,John Liu, and James Whitaker. Transfer learning: Domainadaptation. Deep learning for NLP and speech recognition,pages 495535, 2019. 4": "Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 44014410, 2019. 5 Aditya Khosla, Nityananda Jayadevaprakash, BangpengYao, and Li Fei-Fei. Novel dataset for fine-grained imagecategorization. In First Workshop on Fine-Grained VisualCategorization, IEEE Conference on Computer Vision andPattern Recognition, Colorado Springs, CO, 2011. 16 James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz,Joel Veness, Guillaume Desjardins, Andrei A. Rusu, KieranMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku-maran, and Raia Hadsell. Overcoming catastrophic forget-ting in neural networks. CoRR, abs/1612.00796, 2016. 3,5 Myeongseob Ko, Ming Jin, Chenguang Wang, and RuoxiJia. Practical membership inference attacks against large-scale multi-modal models: A pilot study.In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 48714881, 2023. 3 Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, JoanPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.Large scale learning of general visual representations fortransfer. arXiv preprint arXiv:1912.11370, 2(8), 2019. 4 Gautam Krishna, Co Tran, Jianguo Yu, and Ahmed H Tew-fik. Speech recognition with no speech or with noisy speech.In ICASSP 2019-2019 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages10901094. IEEE, 2019. 1",
  "Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-ing. IEEE Transactions on Knowledge and Data Engineer-ing, 22(10):13451359, 2010. 1, 3, 4": "Xiong Peng, Feng Liu, Jingfeng Zhang, Long Lan, Junjie Ye,Tongliang Liu, and Bo Han. Bilateral dependency optimiza-tion: Defending against model-inversion attacks. In KDD,2022. 1, 3, 4, 5, 6, 7, 8, 15, 16, 18, 19 Nicolas Pinto, Zak Stone, Todd Zickler, and David Cox.Scaling up biologically-inspired computer vision: A casestudy in unconstrained face recognition on facebook.InCVPR 2011 WORKSHOPS, pages 3542. IEEE, 2011. 16",
  "Shahbaz Rezaei and Xin Liu. On the difficulty of member-ship inference attacks. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages78927900, 2021. 3": "Florian Schroff, Dmitry Kalenichenko, and James Philbin.Facenet: A unified embedding for face recognition and clus-tering. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 815823, 2015. 1, 19 Avital Shafran, Shmuel Peleg, and Yedid Hoshen. Member-ship inference attacks are easier on difficult problems. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 1482014829, 2021. 3 Reza Shokri, Marco Stronati, Congzheng Song, and VitalyShmatikov. Membership inference attacks against machinelearning models. In 2017 IEEE symposium on security andprivacy (SP), pages 318. IEEE, 2017. 3",
  "Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. arXivpreprint arXiv:1409.1556, 2014. 3, 4, 5, 7, 19": "Lukas Struppek, Dominik Hintersdorf, Antonio De AlmeidaCorreia, Antonia Adler, and Kristian Kersting. Plug & playattacks: Towards robust and flexible model inversion attacks.arXiv preprint arXiv:2201.12179, 2022. 3, 4, 5, 6, 7, 16, 17,18 Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,Peyman Milanfar, Alan Bovik, and Yinxiao Li.Maxvit:Multi-axis vision transformer. In European conference oncomputer vision, pages 459479. Springer, 2022. 3, 5, 7, 16 Kuan-Chieh Wang, Yan Fu, Ke Li, Ashish Khisti, RichardZemel, and Alireza Makhzani. Variational model inversionattacks.Advances in Neural Information Processing Sys-tems, 34:97069719, 2021. 1, 5, 6, 7, 16, 17, 18, 19 Tianhao Wang, Yuheng Zhang, and Ruoxi Jia. Improvingrobustness to model inversion attacks via mutual informationregularization. In Proceedings of the AAAI Conference onArtificial Intelligence, pages 1166611673, 2021. 1, 3, 4, 5,6, 7, 8, 15, 16, 19 Jiawei Yang, Hanbo Chen, Jiangpeng Yan, Xiaoyu Chen, andJianhua Yao. Towards better understanding and better gener-alization of few-shot classification in histology images withcontrastive learning. 2022. 1 Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang.Neural network inversion in adversarial setting via back-ground knowledge alignment. In Proceedings of the 2019ACM SIGSAC Conference on Computer and Communica-tions Security, pages 225240, 2019. 4 Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Man-mohan Chandraker. Feature transfer learning for face recog-nition with under-represented data.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 57045713, 2019. 4",
  "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.How transferable are features in deep neural networks? Ad-vances in neural information processing systems, 27, 2014.1, 2, 3, 6, 14": "Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller,R Manmatha, et al. Resnest: Split-attention networks. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 27362746, 2022. 5, 16 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 586595, 2018. 6, 14, 15 Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, BoLi, and Dawn Song. The secret revealer: Generative model-inversion attacks against deep neural networks. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 253261, 2020. 1, 3, 4, 5, 6, 8,16, 17, 18, 19",
  "TL-DMI83.4142.004.901517.38": ". Empirical results for BREPMI . Following the ex-act experimental setups from BREPMI, Dpriv = CelebA, Dpub= CelebA, evaluation model = FaceNet, and target classifier T =VGG16, there are a total of 300 attacked classes. Our proposedTL-DMI achieves better MI robustness, which is quantified by MIrobustness is quantified by the , the ratio of drop in attackaccuracy to drop in natural accuracy",
  ".We follow the MI setup from MIRROR, where T =ResNet-34, Dpriv = Stanford Cars, Dpub = LSUN Cars, Dpretrain= ImageNet1K": ". Empirical Validation on VGG16 with GMI. Each linerepresents one training setup for T with a different |C| updatedon Dpriv. Note that number of parameters for the entire targetmodel |T | = 16.8M for this MI setup. To separate the influenceof natural accuracy on MI attack accuracy, we perform GMI at-tacks on different checkpoints for each training setup, varying awide range of natural accuracy. This is presented by multiple datapoints on each line. For a given natural accuracy, it can be clearlyobserved that attack accuracy can be reduced by decreasing |C|,i.e., decreasing parameters updated on Dpriv. attack has not been included yet in SOTA MI defense BiDO.The results in Tab. 11 shown that TL-DMI is able to defenseagainst SOTA MI attack LOMMA. For a fair comparison,we strictly follow LOMMA for the MI setups.",
  ". The effect of pretrain dataset to MI robustness": "In these above sections, we use a consistent and standardpre-trained dataset to ensure fair comparison with othermethods in the literature. Since the pre-trained backbonecan be produced with different datasets in practice, we in-vestigate the impact of different pre-trained datasets on MIrobustness in this section. Specifically, we implement thesame setup as the KEDMI setup for VGG16, but vary threedifferent pre-trained datasets: ImageNet1K, Facescrub, andPubfig83. The results are shown in .Updating all parameters |C| = 16.8M on Dpriv, yieldsno significant differences among different Dpretrain. Thisis expected and align with our understanding, where all theparameters in T are exposed to private data during the train-ing of T. With fewer trainable parameters on Dpriv, wenotice clearer differences. Overall, pre-training on a closerdomain (Pubfig83 and Facescrub) restores natural accuracymuch better than pre-training on a general domain (Ima-genet1K).For instance, with |C| = 2.1M, pre-training on Face-scrub and Pubfig83 achieve 81.48% and 69.41% accuracy,respectively, compared to 29.59% in the Imagenet1K setup.Nevertheless, pre-training on a closer domain also increasesthe risk of MI attack.As those frozen parameters dur-ing the fine-tuning on Dpriv keep the feature representa-",
  "FaceNet64 No Def.35.4/35.488.5030.60 5.2162.00 5.69 1625TL-DMI34.4/35.483.419.33 4.5524.33 4.55 1909": ". Our extended MI robustness evaluation on SOTA MI attack LOMMA . The results of AttAcc and Acc are given in %. Wereports the MI defense results against different LOMMA attack setups including LOMMA+KEDMI (LOMMA-K) and LOMMA+GMI(LOMMA-G) with the varying in different public datasets Dpub (CelebA and FFHQ), and pre-trained datasets Dpretrain (Imagenet1K andMS-CelebA-1M). tions from Dpretrain, thus, the closer the Dpretrain, theriskier it is for the model against MI attack. Notably, with|C| = 15.0M, models pre-training on ImagNet1K and Pub-fig83 achieve comparable accuracy. However, using Ima-geNet1K as Dpretrain renders a more robust model (de-creasing MI attack accuracy by 8.06%) than the setup ofPubfig83. In conclusion, when using our TL-DMI to traina MI robust model, it is critical to choose the Dpretrain fora trade-off between restoring model utility and robustness.Specifically, less similarity between pretrain and privatedataset domains can improve defense effectiveness.",
  ". Layer-wise MI Vulnerability Analysis": "we conduct the following experiments which strongly cor-roborate our analytical results. Specifically, instead of fine-tuning the middle layers, we fine-tune the first layers, see in this rebuttal.This single change significantlydegrades the defense performance and helps MI attacks, which corroborate our analytical results: first layers are im-portant for MI based on our Fisher Information analysis;therefore, fine-tuning the first layers with private datasethelps MI attacks significantly. As another detail to furthercorroborate our analysis, we remark that first layers haveless parameters than middle layers. Yet, MI attacks performbetter with fine-tuning private dataset in first layers. Thisfurther supports first layers are important for MI. We remarkthat last layers are critical for classification task, consistentwith TL literature. The natural accuracy is much degradedif fine-tuning of last layers is removed.",
  ". Additional Analysis of Layer Importance": "FI across MI iterations. MI is a multiple iteration process.The FI for MI in the main manuscript is computed at the lastiteration (the iteration that we present the result throughoutour submission). also provides the FI across multipleiterations. We observe that after a few iterations, the FI for . The effect of different Dpretrain, i.e., ImageNet1K, Pub-fig83, and Facescrub. We use T = VGG16, Dpriv = CelebA. Theresults suggest that the less similarity between pretrain and privatedataset domains can improve defense effectiveness. . We follow KEDMI-VGG16 and PPA-ResNet-18 setupsin -II. Fine-tuning first layers (green line), rather than middlelayers (orange line), enhances MI attack accuracy, corroboratingour analysis: first layers are important for MI. earlier layers keeps dominant compared to the later layers.Different MI losses.In the main manuscript, we usel2 distance to compute the MI loss. In addition, we provideFI results using l1 distance and LPIPS to compute theMI loss. The FI results obtained using different MI lossfunctions are consistent with our main FI observation in themain manuscript.These additional FI results are consistent with those inour main FI observation in the main manuscript.",
  ". MI Robustness via the False Positive Concept": "We provide additional analysis in this Appendix to providea clear understanding of how our proposed TL-DMI effec-tively defends against MI attacks, leading to more false pos-itive during MI attacks and decrease in attack accuracy.As discussed, it has been shown that when a deep neu- ral network-based classifier, denoted as T = C E, ispre-trained on a large-scale dataset Dpretrain, the featureslearned in the earlier layers E are transferable to anothersomewhat related classifier on datasets Dpriv, enabling themodel to maintain its natural accuracy without explicitlyupdating its parameters on Dpriv in the earlier layers .This transferability of features benefits our proposed TL-DMI through maintaining the model classification perfor-mance and natural accuracy.In contrast, MI attacks require accurate features to re-construct the private training dataset Dpriv. By refrainingfrom updating E on Dpriv, we limit the leakage of privatefeatures into E, thereby improving MI robustness. Specifi-cally, recall MI attacks are usually formulated as:",
  "w = arg minw ( log PT (y|G(w)) + Lprior(w))(4)": "Therefore, MI attacks aim to seek w with high likeli-hood PT (y|G(w)). We make this key observation to under-stand how our proposed TL-DMI can degrade MI task: WithTL-DMI defense, while latent variables with high likelihoodPT (y|G(w)) can still be identified via Eq. 4, many w arefalse positives, i.e. G(w) do not resemble private sam-ples. This results in decrease in attack accuracy. This canbe observed from the likelihood distributions PT|C |=16.8Mand PT|C |=13.9M for both KEDMI (see ) and GMI(see ), which are similar and close to 1. These find-ings indicate that with TL-DMI, Eq. 4 could still performwell to seek latent variables w to maximize the likelihoodPT (y|G(w)). However, although likelihood distributionsPT|C |=16.8M and PT|C |=13.9M are similar under attacks, theattack accuracy of model with |C| = 13.9M is signifi-cantly lower than that with |C| = 16.8M. This suggeststhat, due to lack of private data information in E in our pro-posed TL-DMI model |C| = 13.9M, many w do not cor-respond to images resembling private images.In the setup where |C| = 16.8M, the optimization pro-cess causes the latent variables w to converge towards re-gions that are closer to the private samples. This outcome isexpected since the model possesses richer low-level featuresfrom the private dataset Dpriv in both E and C. Conse-quently, we observe more true positives after MI optimiza-tion, where the likelihood PT (y|G(w)) is well maximized,and the evaluation model successfully classifies them as la-bel y.In contrast, in the setup where |C| = 13.9M, the lackof low-level features from Dpriv in E hinders the optimiza-tion process. As a result, we observe a higher number offalse positives after MI optimization. Although these in-stances successfully maximize the likelihood PT (y|G(w)),the evaluation model is unable to classify them as label ycorrectly. Therefore, this behavior indicates a higher levelof robustness against the MI attack. . FI distributions across layers during all MI steps. We conduct FI analysis on the main setup in Peng et al where the MIattack is KEDMI , T=VGG16, Dpriv=CelebA and Dpub=CelebA. In the main manuscript, we present the FI analysis at the last MIiteration, i.e., iteration 3000. This figures present a more comprehensive FI analysis across multiple iterations. After first few iterations,we consistently observe that the earlier layers are more important to MI task. . FI distributions across layers via different FI losses. We conduct FI analysis on the main setup in Peng et al where the MIattack is KEDMI , T=VGG16, Dpriv=CelebA and Dpub=CelebA. In the main manuscript, we use l2 distance between reconstructedimages and private images as MI loss for the FI analysis. This figure presents the FI analysis through other distances including l1, LPIPS-VGG , LPIPS-ALEX . The results show the consistent observation that the earlier layers of a network are more important to MIattacks compared with later layers.",
  "BiDO is sensitive to hyper-parameters. BiDO ,while attempting to partially recover model utility, suffers": "from sensitivity to hyper-parameters. Optimizing three ob-jectives simultaneously is a complex task, requiring carefulselection of weights to balance the three objective terms.The Tab. 15 results in an explicit accuracy drop whenadjusting hyper-parameters x and y even with a smallchange.The optimized values for x and y in BiDOare obtained through a grid search .For example,in the case of BiDO-HSIC, the authors tested values ofx [0.01, 0.2] and y",
  "x . Furthermore, BiDOrequires an additional parameter, , for applying Gaussian": ". Visualization of the distribution of PT for two models: the no defense model (with |C| = 16.8M) and TL-DMI proposedapproach (with |C| = 13.9M). The visualization is conducted using KEDMI as the attack method, with Dpriv = CelebA, Dpub =CelebA, Dpretrain = Imagenet1K, and T = VGG16. We observe that both our proposed TL-DMI model and the model without defenseexhibit similar distributions of PT . The values of PT for both successfully and unsuccessfully reconstructed images are very close to 1 inboth cases. However, the attack accuracy shows a significant drop from 90.87 to 51.67 when our proposed TL-DMI is applied.",
  ". Detailed MI Setup": "Attack Dataset. Following existing MI works , our work forcuses on the study of CelebA . Fur-thermore we demonstrate the efficacy of our proposed TL-DMI on other facial datasets with more attack classes (Face-scrub ) or larger scale (VGGFace2 ) and on the an-imal dataset Stanford Dogs .The details for thesedatasets used in the experimental setups can be found inTab. 14.Attack Data Preparation Protocol. Following previousworks approaches, we split the datasetinto private Dpriv and public Dpub subsets with no classintersection. Dpriv is used to train the target classifier T,while Dpub is used to extract general features only. Target Classifier T . We select VGG16 for T for a faircomparison with SOTA MI defense . As our proposedTL-DMI is architecture-agnostic, we also extend the de-fense results on more common and recent architectures: i.e.,IR152 , FaceNet64 , Resnet-34, Resnet-18, Resnet-50 , ResNeSt-101 , and MaxViT , which are notexplored in previous MI defense setups . Pre-trained Dataset for Target Classifier Dpretrain.We use Imagenet-1K for VGG16, Resnet-18/50,ResNeSt-101, and MaxViT, and MS-CelebA-1M forIR152 and FaceNe64, following previous works .For Resnet-34, since it is trained from scratch in the orig-inal VMI setup , we freeze the layers initialized fromscratch. In Sec. 7, we also study two additional pre-traineddatasets, Facescrub and Pubfig83 . MI Attack Method. Our work focuses on white-box at-tacks, the most effective method in the literature. Followingthe SOTA MI defense , we evaluate our proposed TL-DMI against three well-known attacks: GMI , KEDMI, and VMI . We further evaluate our proposed TL- . Visualization of the distribution of PT for two models: the no defense model (with |C| = 16.8M) and TL-DMI proposedapproach (with |C| = 13.9M). The visualization is conducted using GMI as the attack method, with Dpriv = CelebA, Dpub = CelebA,Dpretrain = Imagenet1K, and T = VGG16. We observe that both our proposed TL-DMI model and the model without defense exhibitsimilar distributions of PT . The values of PT for both successfully and unsuccessfully reconstructed images are very close to 1 in bothcases. However, the attack accuracy shows a significant drop from 90.87% to 51.67% when our proposed TL-DMI is applied.",
  "KEDMI73.40 4.1076.27 4.0976.20 3.9675.29 4.05FaceNet64GMI83.6115.73 4.5883.0115.93 5.2082.7113.6 3.9783.1115.09 4.58": ". We present the results for running experiments multiples time to show the reproducibility of our proposed TL-DMI. For KEDMI/GMI , we conduct the attacks with Dpriv = CelebA, Dpub = CelebA, Dpretrain = Imagenet1K, and T = VGG16/IR152/FaceNet64.For VMI , we conduct the attacks with Dpriv = CelebA, Dpub = CelebA, T = Resnet-34, and there is no Dpretrain for this setup.",
  "Natural Acc80.3573.6976.4676.1323.2757.5757.04": ". The SOTA MI defense, BiDO is sensitive to hyper-parameters, posing challenges for applying effectively to differ-ent architectures of target classifier T or private dataset Dpriv.BiDO simultaneously optimizes two objectives: d(x, f) (limitinginformation of input x and feature representations f) and d(f, y)(providing sufficient information about label y to f), in addi-tion to the main objectives L. Therefore, the final objective isL + xd(x, z) + yd(f, y), where careful weight selection for xand y is necessary to achieve a balanced training among three ob-jectives. It is clear that inappropriate values of x and y in BiDOcause an unstable training T. Note that requires an extensivegrid search to determine suitable values for x and y",
  "In the main manuscript, we make use of Natural Accuracy,Attack Accuracy, and K-Nearest-Neighbors Distance (KNNDist) metrics to evaluate MI robustness. These metrics aredescribed as:": "Attack accuracy (AttAcc).To gauge the effective-ness of an attack, we develop an evaluation classifierthat predicts the identities of the reconstructed images.This metric assesses the similarity between the gener-ated samples and the target class. If the evaluation clas-sifier attains high accuracy, the attack is considered suc-cessful. To ensure an unbiased and informative evalu-ation, the evaluation classifier should exhibit maximalaccuracy. Natural accuracy (Acc). In addition to assessing theAttack Acc of a released model, it is also necessary toensure that the model performs satisfactorily in terms ofits classification utility. The evaluation of the modelsclassification utility is typically measured by its natural",
  "accuracy, which refers to the accuracy of the model inthe classification problem": "K-Nearest Neighbors Distance (KNN Dist). The KNNDist metric provides information about the proximitybetween a reconstructed image associated with a par-ticular label or ID, and the images that exist in the pri-vate training dataset. This metric is calculated by de-termining the shortest feature distance between the re-constructed image and the actual images in the privatedataset that correspond to the given class or ID. To cal-culate the KNN Dist, an l2 distance measure is used be-tween the two images in the feature space, specificallyin the penultimate layer of the evaluation model. This",
  "distance measure provides insight into the similarity be-tween the reconstructed and the real images in the train-ing dataset for a particular label or ID": "EvalNet and F aceNet These metrics are measuredby the squared l2 distance between the activation in thepenultimate layers. EvalNet is computed via Evalua-tion Model while EvalNet is computed via pre-trainedFaceNet . A lower value indicates that the attackresults are more visually similar to the training data. 2 distance.2 distance measures how similar theinverted images are to the private data by computingthe distance between reconstructed features the centroidfeatures of the private data. A lower distance means thatthe inverted images are more similar to the target class. Frechet inception distance (FID). FID is commonlyused to evaluate generative model to access the gener-ated images. The FID measures the similarity betweentwo sets of images by computing the distance betweentheir feature vectors. Feature vectors are extracted usingan Inception-v3 model that has been trained on the Im-ageNet dataset. In the context of MI, a lower FID scoreindicates that the reconstructed images are more similarto the private training images.",
  ". The details for training T": "Training target classifier T .In this work, we employVGG16 , IR152 , and FaceNet64 for our inves-tigation. All target classifiers are trained on CelebA dataset.For GMI and KEDMI , the target classifiers trainedwere VGG16, IR152, and FaceNet64, while Resnet-34 wasused as the target classifier for VMI . As mentioned inthe main manuscript, we employ Imagenet-1K as the pre-trained dataset for VGG16, while MS-CelebA-1M was usedas the pre-trained dataset for IR152 and FaceNet64. The de-tails of the training procedure are shown in Tab. 13 below. Important Hyper-parameters. In our work, we per-formed an analysis of our proposed TL-DMI against exist-ing SOTA model inversion defense methods: MID andBilateral Dependency Optimization (BiDO). MID adds a regularizer d(x, T(x)) to the main objective duringthe target classifiers training to penalize the mutual infor-mation between inputs x and outputs T(x). BiDO at-tempts to minimize d(x, z) to reduce the amount of infor-mation about inputs x embedded in feature representationsz, while maximizing d(z, y) to provide z with enough in-formation about y to restore the natural accuracy. For sim-plicity, we use MID, x, and y to represent d(x, T(x)),d(x, z), and d(z, y) respectively.The settings of thesehyper-parameters are detailed in Tab. 17. . Qualitative results to showcase the effectiveness of our proposed TL-DMI, using KEDMI with Dpriv = CelebA, Dpub =CelebA, Dpretrain = Imagenet1K, and T = VGG16. The visual comparison reveals that our proposed TL-DMI achieves competitivereconstruction of private data, while the hybrid approach combining our method with BiDO-HSIC demonstrates a significant degradationin MI attack and reconstruction quality.",
  ". Error Bars": "For this section, we ran a total of 7 setups (3 times for eachsetup) across 4 different architectures of the target classi-fiers, and report their respective natural accuracy and attackaccuracy values. For each experiment, we use the same MIattack setup and training settings for target classifiers as re-ported in the main setups comparing with BiDO and Tab. 13respectively. We show that the results obtained are repro-ducible and do not deviate much from the reported valuesin the main paper. These results can be found in Tab. 12below.",
  ". Visual Comparison": "We evaluate the efficacy of our proposed TL-DMI alongwith BiDO for preventing privacy leakage on CelebA andalso provide visualisation of the samples produced usingthe KEDMI MI attack method. In below, eachcolumn represents the same identity and the first row rep-resents the ground-truth private data while each subsequentrow shows the attack samples reconstructed for each MI de-fense method.",
  "We conduct our user study via Amazon MTurk with theinterface as shown above. We adapt our user study from": "MIRROR. In the setup, participants are presented with areal image of the target class, and then asked to pick oneof two inverted images that is more closely aligned with thereal image. The order is randomized, with each image pairdisplayed on-screen for a maximum duration of 60 seconds.The assessment encompassed all 300 targeted classes. Eachpair of inverted images is assigned to 10 unique individ-uals, thus our user study involves a total of 3000 pairs ofinverted images. We use KEDMI as the MI attack withDpriv = CelebA, Dpub = CelebA, T = FaceNet .Consistent with the AttackAcc, the user study shows thatour proposed TL-DMI provides better defense against thereconstruction of private data characteristics compared toBIDO."
}