{
  "Abstract": "In this report, we present our champion solution forEgo4D EgoSchema Challenge in CVPR 2024. To deeply in-tegrate the powerful egocentric captioning model and ques-tion reasoning model, we propose a novel HierarchicalComprehension scheme for egocentric video QuestionAnswering, named HCQA. It consists of three stages: Fine-grained Caption Generation, Context-driven Summariza-tion, and Inference-guided Answering. Given a long-formvideo, HCQA captures local detailed visual informationand global summarised visual information via Fine-grainedCaption Generation and Context-driven Summarization, re-spectively. Then in Inference-guided Answering, HCQA uti-lizes this hierarchical information to reason and answergiven question. On the EgoSchema blind test set, HCQAachieves 75% accuracy in answering over 5,000 human cu-rated multiple-choice questions. Our code will be releasedat",
  ". Introduction": "The Ego4D EgoSchema challenge involves choosing thecorrect answer from five options based on a three-minute-long egocentric video and its related question. The eval-uation of this challenge is performed on the EgoSchemadataset , which consists of over 5,000 human curatedmultiple-choice question answer pairs, spanning over 250hours of real video data, covering a very broad range ofnatural human activity and behavior. Therefore, this chal-lenge is particularly interesting for evaluating long-contextunderstanding, as it benefits from long temporal certifi-cate lengths, i.e. the minimum video duration a humanneeds to answer the question accurately .Existing work can be broadly categorized into twogroups: 1) train a powerful question answering model foregocentric videos . This approach tends to per-form better in capturing details in specific fields.How-ever, this is often limited by large-scale data as well as time and resources. 2) fine-tune a large language model (LLM)via prompt . This method opens up new so-lution ideas to leverage existing strong LLM for adaptingdownstream tasks. Although this approach may be limitedby LLM itself to achieve optimal performance, it requiresmuch less computational resources and time, as well as cap-italizing on the extensive knowledge embedded in LLM.Therefore, considering the time cost, we adopt the secondapproach to design our solution.In , the baseline LifelongMemory achievesthe optimal performance in the available methods by apply-ing the effective captioning model LaViLa and the rea-soning model GPT-41. However, we find that this methoddoes not establish a temporal correlation between the differ-ent captions such that GPT-4 is unable to fully understandthe visual scene and complete activity reflected by the cap-tions. At the same time, in-context learning is not intro-duced, which has been well demonstrated to lead LLM tobetter perform new tasks.Therefore, to address the above limitations, we propose ahierarchical comprehension scheme, referred to as HCQA,which incorporates both context-driven summarization andinference-guided answering. Summarization motivates theLLM to better understand the temporal information of thescene, and in-context learning makes it easier for the LLMto understand and perform EgoSchema task. By applyingthis simple but effective program, we surpass other teamsin this challenge and our pipeline achieves a significant im-provement compared to the best baseline LifelongMemory(i.e., 68% 75% on accuracy in ).",
  ". An illustration of our multi-stage pipeline": "a given question.Fine-grained Caption Generation. To obtain detailed vi-sual information of the video, we segment a 180-secondvideo into clips at 4-second intervals, resulting in 45 videoclips.Then, we employ LaViLa as our captioningmodel, which is a excellent vision-language model pre-trained on Ego4D . Specifically, we uniformly sample4 frames from each clip and use LaViLa to produce fivecorresponding textual captions for diversity. In this way, wecan obtain 455 captions, which could provide sufficientvisual semantics for the subsequent question answering.Context-driven Summarization. The original caption pro-vides a detailed description of each clip in the video, how-ever these descriptions are discrete and unrelated. In or-der to integrate these captions from the temporal dimension,we propose in-context aware caption summarization, whichcouples these captions to establish associations and aggre-gates them into an overall overview of the video. Specif-ically, we employ prompt learning to instruct GPT-4o2 togenerate a comprehensive overview of a video based ongiven captions. To achieve a more accurate and detailedvideo summary, we employ in-context learning in ourapproach. Empirically, we set the number of cases for in-context learning to 1. By leveraging this high-quality caseas references, we guide the model to consider both local andglobal information during the summary generation process.Inference-guided Answering.To enhance the modelsreasoning ability, we propose using the Chain-of-Thought(CoT) method . This method guides the model to cap-ture key information from captions and summaries, explic-itly outputting the reasoning process for questions. Con-sequently, it improves the models accuracy in answeringcomplex visual questions.Notably we also employ in- context learning, using three high-quality examples fromthe EgoSchema subset. Moreover, inspired by the impact ofreflection mechanism on enhancing the performance ofLLM, we incorporate a reflection mechanism into the ques-tion answering process. Specifically, after generating an an-swer, we prompt the model to output a confidence score forits response. If the confidence is below a certain threshold(5 in our settings), we require the model to reflect on itsprevious answer, assess any potential errors, and correct itif necessary.",
  ". Performance Comparison": "displays the existing methods as well as our primaryleaderboard results. From the results, we can see that theoptimal existing method LifelongMemory achieves 68% ac-curacy on the EgoSchema full set, and yet this approach canonly be ranked 5th on the public leaderboard. Our frame-work achieves a accuracy of 75%, ranked first, significantlyoutperforming all the other teams and existing work. Thisthereby proves the superiority of our method.",
  ". Solution Evolution": "presents the iterative process of our solution. Wefirst utilize the LifelongMemory as our backbone andtest its results on the EgoSchema dataset. To enhance theexpression of video clip, we increase the number of cap-tion from 1 to 3, achieving an absolute improvement of2.4%.After observing this obvious leap, we further in-crease the number of caption while introducing an exam-ple for in-context learning. And in order to strengthen theunderstanding of the LLM for global temporal information,we add inductive summarization in addition to predicting",
  "HCQA (iLearn)10.75": "answers. This produces a gain of 3.2%. In order to re-duce the difficulty of performing simultaneous summariza-tion and prediction for LLM, we separate the process intotwo phases: summarization followed by prediction.Al-though the length of the generated summaries increased sig-nificantly, this does not result in a significant accuracy im-provement. Considering that one example may not provideguidance for different samples, we enrich the original ex-ample by filtering three typical examples from the subset,which gains 0.5% increments. It is worth noting that all ofthe above solutions generate reason and confidence in ad-dition to predicting answer. For uncertain predictions, i.e.,low confidence, we ask LLM to reflect on and re-predictprevious answers. This results in a 0.2% benefit.",
  ". Ablation Study": "In , we investigate the effects of different caption-ing and reasoning models. The results show that LaViLaachieves the best results over EgoVLP and VideoRe-cap under consistent application of GPT-3.5. After fix-ing LaViLa for the captioning model, we also study theeffectiveness of different reasoning models. These resultssuggest that GPT-4os reasoning is superior to that of GPT-3.5 and GPT-4, leading to a superior performance.",
  ". Case Analysis": "shows two examples of our framework, includinga successful one and a failed one. In a, captionsand summary are correctly generated to describe the cam-era wearers short-term actions and long-term activities, i.e.,cleaning dishes and others. With the hierarchical narrations,our framework understands the complete activity and cor-rectly answers the question with appropriate explanations.",
  "Question: Describe the main objective and overall process depicted in the video,mentioning the primary actions performed by c and the person": "Reason: Based on the summary and the recurring actions described in the captions, themain objective and overall process depicted in the video involve playing a board gamewith elements of cards and dice. The actions performed by C, including picking andplacing cards, shuffling, arranging, and interacting with dice and other game pieces suchas buttons and pellets, all indicate that C and the other person are engaged in a strategicboard game. The mention of playing the Catan game specifically supports thisconclusion.Answer: Currently, c is actively engaged in playing a strategic board game.Ground Truth: C is playing a card game with the person.",
  ". Two examples of our framework on EgoSchema subset": "In b, although the video content is appropriatelydescribed, our reasoning model does not predict the correctanswer. This may be due to the fact that the words cardgame and board game in the answer are too close to eachother, causing the LLM to fail to specify the boundary be-tween the two.",
  "Rohan Choudhury, Koichiro Niinuma, Kris M Kitani, andLaszlo A Jeni. Zero-shot video question answering with pro-cedural programs. arXiv preprint arXiv:2312.00937, 2023.3": "Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma,Zhifang Sui, and Furu Wei. Why can gpt learn in-context?language models implicitly perform gradient descent asmeta-optimizers. arXiv preprint arXiv:2212.10559, 2022. 2 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:Around the world in 3,000 hours of egocentric video. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1899519012, 2022. 1, 2 Weili Guan, Xuemeng Song, Haoyu Zhang, Meng Liu,Chung-Hsing Yeh, and Xiaojun Chang. Bi-directional het-erogeneous graph hashing towards efficient outfit recom-mendation. In Proceedings of the 30th ACM InternationalConference on Multimedia, page 268276. Association forComputing Machinery, 2022.",
  "emotion recognition. In Findings of the Association for Com-putational Linguistics: ACL 2022, pages 16101618, 2022": "Zaijing Li, Ting-En Lin, Yuchuan Wu, Meng Liu, FengxiaoTang, Ming Zhao, and Yongbin Li. Unisa: Unified generativeframework for sentiment analysis. In Proceedings of the 31stACM International Conference on Multimedia, pages 61326142, 2023. Zaijing Li, Gongwei Chen, Rui Shao, Dongmei Jiang, andLiqiang Nie. Enhancing the emotional generation capabil-ity of large language models via emotional chain-of-thought.arXiv preprint arXiv:2401.06836, 2024.",
  "Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dong-mei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodalmemory empowered agents excel in long-horizon tasks. InNeurIPS, 2024": "Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, MichaelWray, Rui Yan, Eric Z Xu, Difei Gao, Rong-Cheng Tu, Wen-zhe Zhao, Weijie Kong, et al.Egocentric video-languagepretraining. Advances in Neural Information Processing Sys-tems, 35:75757586, 2022. 3, 4 Karttikeya Mangalam, Raiymbek Akshulakov, and JitendraMalik. Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural In-formation Processing Systems, 36, 2024. 1 Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak,Justin Chiu, Joe Heyward, Viorica Patraucean, Jiajun Shen,Antoine Miech, Andrew Zisserman, and Aida Nematzdeh.A simple recipe for contrastively pre-training video-first en-coders beyond 16 frames. arXiv preprint arXiv:2312.07395,2023. 1, 3 Machel Reid, Nikolay Savinov, Denis Teplyashin, DmitryLepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, RaduSoricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit-twieser, et al.Gemini 1.5: Unlocking multimodal under-standing across millions of tokens of context. arXiv preprintarXiv:2403.05530, 2024. 1, 3",
  "Ying Wang, Yanlai Yang, and Mengye Ren. Lifelongmem-ory: Leveraging llms for answering queries in egocentricvideos. arXiv preprint arXiv:2312.05269, 2023. 1, 2, 3, 4": "Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, YinanHe, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, ZunWang, et al. Internvideo2: Scaling video foundation mod-els for multimodal video understanding.arXiv preprintarXiv:2403.15377, 2024. 1, 3 Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large lan-guage models. Advances in Neural Information ProcessingSystems, 35:2482424837, 2022. 2 Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Yaya Shi, et al.mplug-owl: Modularization empowerslarge language models with multimodality. arXiv preprintarXiv:2304.14178, 2023. 3 Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang,Shoubin Yu, Mohit Bansal, and Gedas Bertasius.A sim-ple llm framework for long-range video question-answering.arXiv preprint arXiv:2312.17235, 2023. 1, 3 Haoyu Zhang, Meng Liu, Zan Gao, Xiaoqiang Lei, YinglongWang, and Liqiang Nie. Multimodal dialog system: Rela-tional graph-based context-aware question understanding. InProceedings of the 29th ACM International Conference onMultimedia, page 695703. Association for Computing Ma-chinery, 2021. Haoyu Zhang, Meng Liu, Yuhong Li, Ming Yan, Zan Gao,Xiaojun Chang, and Liqiang Nie. Attribute-guided collab-orative learning for partial person re-identification.IEEETransactions on Pattern Analysis and Machine Intelligence,45(12):1414414160, 2023.",
  "Haoyu Zhang, Meng Liu, Yaowei Wang, Da Cao, WeiliGuan, and Liqiang Nie.Uncovering hidden connections:Iterative tracking and reasoning for video-grounded dialog.arXiv preprint arXiv:2310.07259, 2023": "Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, YaoweiWang, and Liqiang Nie. Multi-factor adaptive vision selec-tion for egocentric video question answering. In Proceedingsof the 41st International Conference on Machine Learning,pages 5931059328. PMLR, 2024. Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and RohitGirdhar.Learning video representations from large lan-guage models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 65866597, 2023. 1, 2, 4"
}