{
  "Abstract": "The goal of this paper is speech separation and en-hancement in multi-speaker and noisy environments usinga combination of different modalities. Previous works haveshown good performance when conditioning on temporal orstatic visual evidence such as synchronised lip movementsor face identity. In this paper, we present a unified frame-work for multi-modal speech separation and enhancementbased on synchronous or asynchronous cues. To that endwe make the following contributions: (i) we design a mod-ern Transformer-based architecture tailored to fuse differ-ent modalities to solve the speech separation task in the rawwaveform domain; (ii) we propose conditioning on the tex-tual content of a sentence alone or in combination with vi-sual information; (iii) we demonstrate the robustness of ourmodel to audio-visual synchronisation offsets; and, (iv) weobtain state-of-the-art performance on the well-establishedbenchmark datasets LRS2 and LRS3.",
  ". Introduction": "Humans have the remarkable ability to focus on conver-sations even in a room full of talking people, a phenomenonknown as the cocktail party effect . Our brains carryout this feat by concentrating their attention to a specificspeaker while filtering out the rest of the stimuli originat-ing from interfering voices and other environmental noises.Although this ability manifests to a limited extent throughhearing alone, it is greatly enhanced when simultaneous in-formation from other modalities is available. For exam-ple watching a speakers lips can significantly help disam-biguate speech in noise , while understanding the nat-ural language context of a sentence enables the listener toanticipate the potential next words of the speaker.In recent years, progress in audio-visual learning hasmade it possible for machines to also achieve this abilityand very effectively isolate individual voices out of multi- . We propose VoiceFormer, a framework for multi-modalspeech separation and enhancement, which isolates speech accord-ing to either the text content of the target speakers utterance, theirlip movements, or both. Our framework allows conditioning oncues from multiple modalities, without requiring them to be tem-porally synchronised or have a common temporal rate. This givesit multiple advantages, such as robustness to temporal misalign-ments between the inputs. speaker mixtures of speech or noisy audio . Solv-ing this problem enables a great range of practical applica-tions, such as improving subtitle generation in videos withnoisy audio, developing smart audio-visual hearing aids toenhance speech conditioned on visual input, or facilitatingteleconferencing in noisy settings such as airports or cars.Previous works have principally taken two approaches:either using synchronous cues, most commonly the lipmovements of the target speaker ; or using static(fixed embedding) cues such as the voice or face char-acteristics . The former has the advantage of usingdynamic evidence that is very strongly correlated to the de-sired speech output. However, relying on lip movementshas several disadvantages. First, they may be momentarilydisrupted e.g. from visual occlusions therefore strongreliance will make the model sensitive to this form of visualnoise; and second, they require synchronisation between theaudio and visual streams. On the other hand, static cuesarising from the biometric characteristics of the speaker aremore robust to temporary disruptions, however, they arenot dynamically correlated to the speech (so are a weakersignal) and may be common among different people. Forexample, it may be increasingly harder to separate speech",
  "arXiv:2501.01518v1 [eess.AS] 2 Jan 2025": "among individuals with similar voice timbre or appearance.Recent works have attempted to deal with the inadequacy ofconditioning on a single source by either jointly condition-ing on more than one modality using a naive fusion of staticcues with the lip movement or by learning the separationtask jointly with a cross-modal prior .However, to date, there is no unified framework for: (i)conditioning on asynchronous information (such as a delaybetween the audio and visual streams); or (ii) for seamlessconditioning (and fusing) on multiple sources of informa-tion or on different types of modalities; or (iii) for using alarge temporal context so that predictions can be made us-ing a language model.Our first contribution is to enable conditioning on asyn-chronous visual (lip) streams. Most previous work relieson costly pre-processing steps to synchronise the audio andvideo streams, and their performance deteriorates in real-world situations where out-of-sync data is a regular occur-rence due to transmission delays, jitter or technical issues.We show that in our work there are no detrimental effectswith timing delays of 5 frames (200 ms) or more. Further-more, the audio and visual streams do not even have to havethe same temporal sampling rate.Our second contribution is to enable enhancement byconditioning on textual input. This new functionality al-lows speech to be enhanced without requiring biometric in-formation or even a visual stream. It is applicable where thetextual content of the speech is known in advance, e.g. froma prepared speech or lyrics of a song, or where AutomaticSpeech Recognition (ASR) or lip reading can be usedto transcribe what is said, even imprecisely, and then subse-quently used to isolate the speaker from background noise.Textual conditioning is asynchronous, as only the order ofwords (or more precisely the phonemes) is required, but nottheir precise temporal alignment.Both of these contributions are facilitated by a newTransformer-based speech separation and enhancement net-work, where we use the positional encoding of the Trans-former to record the timestamp (of the audio and visualsamples) or the ordering (of the words in the text) of theconditioning signal. The network operates directly on thewaveform level, without requiring spectrograms as an in-termediate step of the audio processing.It uses a U-Net architecture to encode noisy audio and thendecode it into clean speech, with the Transformer as the net-works bottleneck, where the conditioning information canbe visual and/or text. The Transformer also enables model-ing a longer temporal context (e.g. compared to an LSTM)allowing the network to explicitly model structure in naturallanguage. By having the ability to anticipate what followsa certain sequence of words the model can then better ap-proximate the target speech output.In summary, we make the following contributions: First, we design a modern multi-modal speech enhancement ar-chitecture, VoiceFormer, that uses a Transformer-based bot-tleneck to fuse heterogeneous modality streams, mean-ing that it can simultaneously condition on multiple non-aligned modalities. Second, we introduce text-conditionedspeech enhancement as a novel multi-modal task and showthat our proposed architecture is well designed to handleit. Third, we demonstrate that our trained models are ro-bust to audio-visual synchronisation offsets. Fourth, we ex-hibit state-of-the-art performance, surpassing other audio-only and audio-visual baselines in the tasks of speaker sep-aration and speech enhancement.",
  ". Related work": "Our work is related to a large body of previous workswhich ranges from traditional audio-based speech enhance-ment to methods for multi-modal speech and sound-sourceseparation.Audio-based speech separation and enhancement.Speech enhancement is a well-studied problem with a longhistory in audio processing. Audio-only methods are bydesign person-agnostic; they often work well for enhance-ment, retaining speech and filtering out background noise,but struggle with speaker separation. Recent methods aimto tackle this by solving the label permutation problem as-signing audio sources to their corresponding speakers in theaudio . Wang et al. proposed a method tolocalize individual speakers and train an enhancement net-work on spatial as well as spectral features. Lou et al. introduced a deep learning framework for speech separa-tion that addresses label permutation and does not requireknowledge of the number of speakers. Yu et al. de-vised a deep learning training criterion for solving the labelpermutation problem. Chen et al. perform separationbased on clustering of the audio sources in the embeddingspace. Defossez et al. recently proposed the Denoiser,a real-time speech enhancement network that is trained end-to-end on raw waveforms.Multi-modal methods based on static cues.Vari-ous recent methods attempt to solve the audio separationproblem based on external cues that contain informationabout the sound source.Examples of such methods areworks that perform identity-specific speech separation us-ing voice , or face identity embeddings. Relatedworks that also fall in this category are various audio-visualmethods for separating the sounds of musical instrumentsbased on stationary appearance cues .Multi-modal methods based on dynamic cues. An-other family of methods solve the source separation taskbased on dynamic cues that exhibit some variation overtime. Such cues are more commonly contained in a syn-chronised visual stream, therefore these methods are in their . Overview of the proposed multi-modal speech enhancement with transformers (VoiceFormer) architecture. it consists of a u-net style encoder-decoder for the audio stream, with the bottleneck layers conditioned on a transformer that can ingest textual and visualmodalities. the u-net encoder ingests the raw audio waveform of the target speaker with noise (background or other speakers) and produces asequence of audio embeddings. the multi-layer transformer conditions on the audio embeddings, the phoneme sequence extracted from thetext being spoken, and/or the visual embeddings from the video of the target speaker. the u-net decoder inputs the sequence of refined audioembeddings from the output of the transformer, and produces the clean audio waveform of the target speaker (with the noise removed). Inboth training and inference, the conditioning can include video or text or both. majority audio-visual. For example, utilising visual featureshas proven to be very beneficial in separating speakers inaudio clips where the corresponding video is accessible. In-deed, recent works conditioned deep learn-ing frameworks on the lip movements of target speakers inorder to isolate their voice among multiple other speechsignals. Wu et al. presented an audio-visual speechseparation network that operates in the time-domain (rawwaveforms) instead of frequency-domain, while Sadeghi etal.solved the task by employing audio-visual VAEs .Owens et al. use spatio-temporal video features forseparating on-screen from off-screen speech.In a simi-lar line of work, recent methods have proposed using mo-tion cues in videos in order to separate musicalinstruments belonging to the same class, thus overcomingthe limitations of static appearance features. A few recentworks investigated combining static and dynamic cues toimprove speech separation. For example, Gao et al. in-vestigated training an audio-visual speaker separation net-work jointly with voice-face embeddings that provide aprior to aid the separation process. Another proposed direc-tion is conditioning a separation network on both lip-movements and an embedding of the target speakers voiceto improve robustness to visual occlusions. However, noneof those works propose a unified framework for condition-ing on multiple non-aligned dynamic sources of informa-tion. Multi-modal fusion. Our method is more broadly re-lated to works that fuse different modalities to solve multi-modal tasks, such as audio-visual fusion using Transform-ers for audio-visual event detection oraudio-visual synchronization , video-text fusion for vi-",
  ". Method": "In this section, we describe our proposed method formulti-modal speech enhancement which we call the Voice-Former. Given a noisy speech signal, the goal is to sepa-rate a target speech component corresponding to other in-put modalities (text or video), and to filter out the rest of thesignal (other speakers or background noise). An overviewof the architecture is shown in . We use a U-Netstyle audio encoder-decoder (similar to ), with a multi-modal Transformer in its bottleneck, where the noisy audiois fused with the conditioning inputs (video and text). Therest of this section describes the individual components andthe training of the model. We refer the reader to the projectwebpage for full architectural details.",
  ". Architecture": "Audio, Visual, and Text representations.The model has three input streams: one ingesting an au-dio waveform a RTa, one the corresponding video in-put v R3TvHW , and one a textual representations = (s1, s2 , sns) of the sentence being uttered.Similar to , we extract a representation of the noisyaudio, A Rtac, directly from the input waveform a us-ing the encoder part of a U-Net, which consists of 1D con-volution layers. We also use a VTP network to ob-tain visual representations, V Rtvc. The textual repre-sentation is obtained using the Phonimizer library withespeak-ng as its backend; the words in the input sentenceare first mapped to a phonetic sequence of length tq based on the International Phonetic Alphabet and are then mappedto a sequence of learnable embedding vectors Q Rtqc.Transformer bottleneck. In order to inform the modelof the temporal order of its inputs, i.e. signal timestampsfor the video/audio features and phoneme ordering for thetext, we add positional encodings, PE{a,v,q} Rt{a,v,q}c.PEa and PEv are implemented as sinusoidal vectors andPEq as learnable embedding vectors.Moreover, in or-der to allow the model to distinguish which signal comesfrom which modality, we also add modality encodings,ME{a,v,q} Rc, which are three learnable vectors, onefor each modality type. In summary, the order and modalityaware uni-modal representations are calculated as",
  "Y = TRANSFORMER-ENCODER(Z)": "The Transformer bottleneck fuses the three inputs together,allowing for full cross-attention between all modality com-binations. In particular, the textual and video evidence isattended upon and used to extract only the relevant parts ofthe embedded audio.We note that neither explicit alignment nor commonframe rates between the different signals are required. Theoutput of the Transformer corresponding to its audio input,Y1:ta, contains the representation of the separated/enhancedaudio (the outputs corresponding to video and text are dis-carded).U-Net Decoder. The enhanced audio representation isdecoded into a waveform ac with the decoder part of the U-Net, which is comprised of a stack of transposed 1D convo-lutions, including shortcut connections from the audio en-coder. The resulting output ac is an enhanced waveformcontaining only the speech corresponding to the visual andtext input.",
  ". Synthetic sequences": "Following previous work we train and evaluateour models by creating synthetic noisy samples by addingthe waveforms of two separate clips and requiring the modelto reconstruct the individual waveforms in the output, af-ter conditioning on the corresponding video/text input. Inparticular, one of the clips always contains clean speech ofa single speaker, while the interfering audio might be ei-ther speech from another speaker in the speaker separationexperiments, or a noise audio clip, simulating backgroundnoise for the speech enhancement experiments. Note thatalthough we train and evaluate the model on synthetic au-dio mixtures it is applicable to real noisy sequences as thedomain gap between synthetic and real samples is small.",
  ". Datasets, training & evaluation protocol": "Data. We obtain audio-visual speech samples from theLRS2 and LRS3 lip reading datasets. LRS2 con-tains broadcast footage from British television while LRS3has been created from TED and TEDx clips downloadedfrom YouTube. Both datasets contain audio-visual tracksof tightly cropped talking heads, engaging in continuousspeech. All tracks are accompanied by text transcriptionsof the utterances that are well aligned to the video and au-dio, which have also been automatically synchronised. Onexamining the datasets we determined that some of the sam-ples include two speakers while others included backgroundnoise such as clapping, bird chiming, music or crowd laugh-ter. These samples were removed from the dataset such thatevery sample contains speech from only one speaker with-out other background noise. A combination of diarizationand background noise detection methods were employed todetect and remove the noisy samples. As a result, 57 hoursout of 197 hours was retained from the LRS2, and 439 hoursout of 440 hours was kept from LRS3.Moreover, for obtaining noise for our denoising exper-iments we follow and use a subset of the DNS dataset, which contains approximately 181 hours of noiseaudio from a wide variety of events. These samples wereused as background noise to construct synthetic noisy audiowaveforms during training and evaluation.Training sequences.Approximately 23,000 samplesfrom the LRS2 dataset and over 100,000 samples from theLRS3 dataset were used at training time. The samples weremixed together randomly at each training pass leading tothe generation of numerous new and unseen examples. Thespeech signals are mixed together in sequences of 4 sec-onds for the version of the model trained on audio and videoinput. The starting point of each sequence was randomlychosen as a data augmentation method. The length of thesequences for the models that have text as their input is dic- tated by the number of words in the text that correspondto the associated audio and videos, ranging between 1 to 6seconds. The samples with similar lengths were batched to-gether at training time to avoid padding the sequences withzeros whenever possible. Each audio track was indepen-dently normalised before mixing them together to createsynthetic mixtures or feeding them into the network.Evaluation sequences. We evaluate on synthetic mix-tures of two speakers, or a mixture of one speaker and anoise sample.To distinguish between these two relatedtasks we refer to the first one as speaker separation and thesecond as denoising.We created separate test sets from LRS2 and LRS3 with2515 and 3229 samples from each dataset respectively.These test sets were used to evaluate the aforementionedtasks, comparing our model with baselines and for perform-ing model ablations and robustness tests. The samples in allthe test sets contain noisy audio, video and text. The du-ration of the samples varies as they are cropped based onthe length of the corresponding text. The text samples are 9words long and do not necessarily form a sentence. We in-clude qualitative examples of real sequences on the projectwebpage.Evaluation metrics. For evaluating our methods andbaselines we use standard speech enhancement metrics, in-cluding Signal-to-Distortion-Ratio (SDR) , a com-mon blind source separation criterion, measuring the ratiobetween the energy of the target signal and of the errorscontained in the separated output, Short-Time Objective In-telligibility (STOI) , which measures the intelligibilityof the signal, and the Perceptual Evaluation of Speech Qual-ity (PESQ) , which rates the overall perceived quality ofthe output signal.",
  ". Implementation details": "Our network is implemented and trained in Pytorch. Thefaces in the video recordings are cropped and resembledinto 25 FPS. The audio input is converted into mono bytaking the mean of both channels and resampled to havea rate of 16kHz, and the signals are upsampled by 3.2 timesbefore feeding into the network. The audio output of themodel is down-sampled by the same ratio. For the audioU-Net, we use the Denoiser implementation of with-out any changes to the architecture. For the visual back-bone, we follow and use a VTP network consisting oftransformer layers on top of a 3D/2D residual CNN ,pretrained on a word-level lip reading task. To speed uptraining, the backbone is frozen and visual features are pre-extracted and saved on a hard drive. For the Transformerencoder, we use N = 3 layers and h = 8 heads, with amodel size of 532. The embedding dimensions across allthe modalities is set to 768 to match the channel dimensionof the audio features from the output of the U-Net encoder layer.We obtain models that condition on a single modality(e.g. only video or only text), by simply not including thecorresponding input in Eq. 4.Training started with a network that includes an LSTMon synchronised audio and corresponding visual sequencesas a pre-training stage for the Transformer model. For train-ing the transformer the learning rate was set to 5 105. Inall cases the Adam optimizer was used with a weight de-cay of 0.0001, batch size of 64 and the learning rate wasreduced linearly after each epoch on plateau. The modelsare trained on the LRS2 and LRS3 datasets separately start-ing with the LRS2 mixtures. The training curriculum forthe speech enhancement models started with mixtures of2 speakers before training on one speaker and backgroundnoise. When experimenting with more than two transformerlayers, all the trainable parameters in the model were frozenapart from the additional encoder layers, which helped tostabilize and accelerate the training.",
  ". Results": "In this section, we give a detailed evaluation of the pro-posed method, including robustness analyses, ablations andcomparison to baselines. We first compare the performanceof our models when they are conditioned on different inputmodality combinations; we then perform robustness tests insettings where parts of the modalities are missing as well aswhen there is a misalignment between video and audio; wefinally compare to the state-of-the-art on the speaker sepa-ration and speech enhancement tasks.Modalities comparison. In order to assess the effect ofusing different modalities as conditioning input, we com-pare models using only video (A+V), only text (A+T) orboth (A+V+T) in . We observe that the text-onlymodel successfully separates the speakers, obtaining rea-sonable performance. This result is evidence that (a) in-deed text can be used to separate speech in cocktail partyscenarios, and (b) that our proposed architecture is flexibleenough to capture information from different conditioningsources without any changes and can successfully solve thenovel text-conditioned speaker separation task without anyhelp from other modalities. Observing the performance ofthe video-conditioned models shows that video obtains astronger performance than text. This is an interesting find-ing, suggesting that lip movements are stronger cues for theseparation task, presumably because they carry more in-formation than the language content of the utterance (e.g.speaker mood, accent etc). We refer to the project webpagefor qualitative separation results using the different models.Cross-modal attention. To assess the effectiveness ofour design for cross-modal attention through a concate-nation of the modalities, we examine the attention in thetransformer bottleneck. The attention maps in re-",
  "A+T13.189.72.1614.191.42.37A+V14.191.32.3615.593.42.62A+V+T14.291.72.4115.593.52.63": ".Speaker separation performance using differentmodalities. We compare VoiceFormer models conditioning ondifferent modality combinations. We observe that the text-onlymodel (A+T) performs reasonably well for this task, although theperformance is lower compared to when conditioning on lip move-ments from the visual stream. The full A+V+T model that condi-tions on both text and video obtains only a slight improvementover the A+V model. denotes higher is better veal the correspondence between the audio tokens and theother modalities. attend to the features in the correspond-ing modalities. This indicates that the model is able to im-plicitly learn the audio-visual and audio-text alignments andconfirms our intuition that it does not require manual align-ment or common temporal rates. . Attention map visualisations of the first Transformerlayer.The visualisations show the average score of the atten-tion heads in the first multi-head attention layer of the transformer.Brighter colours indicate higher scores and brighter pixels on thesame row indicate correspondence between modalities. Left: au-dio and video correlation; and Right: audio and text correlation.Higher scores are given to the audio token and its correspondingtoken in the other modality at each timestep. This indicates thatthe model is able to elegantly fuses the mixed/noisy audio streamwith the conditioning vectors from different modalities, withoutthe need for explicit alignment between the signals, or requiringthem to be operating at the same temporal rate. Architecture component contribution. To analyse thecontribution of our architecture components on the perfor-mance of the model, we examine various architecture con-figurations. The results are shown in Robustness to missing information. As can be seenfrom the experiments presented so far, although text canbe sufficient for performing speech separation, it providesa very limited performance boost when strong visual ev-idence (i.e. clear, unoccluded lip movements) is already",
  "LSTM (A+V) S9.2584.01.91UNet + LSTM (A+V)W12.889.92.17UNet + Transformer (A+V)W14.191.32.36": ". Performance based on various architecture configura-tions. We observe that the UNet module, which digests audio as araw waveform, improves the SDR by 3.55dB in comparison to thespectrograms based baseline model . Replacing the LSTM witha transformer bottleneck further increases the SDR by 1.3dB. Notethat the UNet removes the necessity to calculate spectrograms andpredict phase, while the transformer offers robustness to audio vi-sual misalignment. denotes higher is better. S indicates the audioinput is in mel-spectrogram form and W indicates raw waveformis used. available. However, we emphasise that adding text in thissetting improves the robustness of the model against miss-ing information. We therefore conduct further experimentswhere we evaluate the same models, but artificially limit theamount of information from one of the sources. For simu-lating missing video information we mask out (by zeroing)the visual features for a percentage of video frames. Weshow the results of these experiments in a. It isclear that the A+T+V model that conditions on text in addi-tion to video is much more robust to distortions in the videoinput.Similarly, for simulating missing text information, we re-move a variable number of words from the text input. Theperformance against the number of removed words is shownin b. We make two observations. First, the perfor-mance of the A+T model sharply deteriorates when an in-creasing number of words are removed from the text input.This indicates that the model relies on the text input to per-form the separation of the whole utterance, rather than forjust disambiguation between two separated streams. Sec-ond, the A+V+T model is, as expected, very robust to themissing text information.Robustness to inconsistent modalities.To furtherprobe into our models and gain a better understanding ofthem, we perform a series of experiments where we replacethe conditioning video or text input with input from a differ-ent, irrelevant video clip. The results are shown in .We observe that the models that condition on a single source(A+T, A+V) completely fail when presented with wrong in-puts. This is expected as the visual/textual evidence in thosecases is not consistent with any speech components con-tained in the audio. From a similar analysis of the A+V+Tmodel we observe that, although the model performs a lotworse when supplied with inconsistent video, but consistenttext, input, it does not completely fail (e.g.obtains 5.1 SDR).On the other hand, the performance drops only marginallywhen the model is presented with inconsistent text but con- 0%10%20%30%40%50%60%70%80%90% Percentage of the video removed SDR A+V modelA+T+V model (a) Robustness to missing video information. Conditioning ontext in combination with video clearly improves the robustness tomissing video information (e.g. due to occlusions), compared toonly using video. Number of words removed SDR A+T modelA+T+V model (b) Robustness to missing text information. The performance ofthe text-only conditioned model quickly deteriorates when wordsare removed from the text input. This demonstrates that the sepa-ration is indeed reliant on the text content throughout the utterance,and not just for disambiguation between two separated signals. Asexpected, combining video and text provides robustness to missingtext input.",
  ". Experiments with missing information": "sistent video input.To sum up our analysis of the behaviour of VoiceFormer,we conclude that the A+V+T model provides good robust-ness to disruptions in the video inputs, which comes withvirtually no risk; even if for some reason the textual inputprovided is missing or inconsistent with the video (e.g. if weuse noisy ASR approximations), the model still performs onpar with the video-only model.Bottleneck ablation and robustness to AV misalign-ment. To assess our choice of the Transformer encoder inthe U-Net bottleneck we conduct an experiment replacingthe Transformer with a 2 layer LSTM (similar to the archi-tecture used by ), where the audio and video featuresare concatenated in the channel dimension.We argue that using a Transformer to fuse the differentmodalities offers the advantage of not requiring synchro-nised input streams (e.g. video and audio). To verify thishypothesis, we experiment with artificially shifting the au-dio input by a random offset within the range from -200 to200 ms. We train and evaluate both the LSTM and Trans-former models under those conditions. The results of this",
  "A+V+T14.291.72.41A+V+T5.1170.51.69A+V+T10.983.62.12A+V+T14.191.32.36A+V+T14.191.22.34": ". Missing or inconsistent modalities. indicates that thecorrect signal for the corresponding modality is in input, thatthe input for this modality is supplied from a different sentenceor video, and that this input is not provided. The results arereported for the speaker separation task on the LRS2 test set. Weobserve that the models that use a single modality completely failto solve the task when the conditioning input is wrong. On theother hand, the A+V+T model that uses both video and text is fullyrobust to inconsistent text inputs, and can even partially recoverfrom inconsistent video inputs. denotes higher is better Number of frame shifts SDR A+V (LSTM)A+V (VoiceFormer) . Robustness to audio-visual misalignment. We com-pare our proposed model with a baseline using an LSTM bot-tleneck in the audio-visual speaker separation setting. It is clearthat while the LSTM baseline struggles when the video and audiostreams are misaligned, VoiceFormer is robust to synchronisationoffsets. A five-frame offset corresponds to 200 ms. comparison are shown in . It is clear that the per-formance of the LSTM baseline steeply deteriorates whenthe audio and video inputs are not properly synchronised.VoiceFormer on the other hand is very robust to synchroni-sation issues, retaining high SDR scores (> 12), even whenthe two modalities are offset by up to 200ms.Comparison to the state-of-the-art.We report ourmethods performance on speaker separation and compareit to previous methods in . As baselines, we use thestate-of-the-art speech enhancement method of (audio-only), as well as the recently proposed audio-visual meth-ods . It is clear that VoiceFormer outperforms theprevious works on all metrics, obtaining state-of-the-art per-",
  "Ours A+V+T 14.291.72.4115.593.52.63": ". Comparison to the state-of-the-art on the speaker sep-aration task. We evaluate our best models on the synthetic LRS2and LRS3 test sets. The V and T columns denote which modal-ities are used for conditioning by each model. A+V+T indicatesour full model and A+V for a version conditioning only on videoand not on text. Our A+V model clearly outperforms the previouswork on all the measures, obtaining state-of-the-art performance inthis setting. We note that the state-of-the-art speech enhancementmodel of cannot deal with a mixture of different speakers andoutputs the mixed signal, even after we attempted to fine-tune iton this task.not fine-tuned on the synthetic two-speaker LRS2training set. The comparison is on a different test set publishedby Lee et al. . Higher is better for all metrics. formance in the (comparable) audio-visual setting A+V.Comparison on speech enhancement (denoising). Forcompleteness, we also assess our models performance onthe denoising task. We show the results in . Ourproposed models perform on par with the state-of-the-artDenoiser model . We note that this is expected as thespeech enhancement task is easier than speaker separationand can be solved by using only the audio modality. In-deed the high numbers obtained on all metrics indicate thatthe performance in this setting is potentially saturated. Thisexperiment was included to demonstrate that denoising canalso be well handled by the proposed method. We leavestress-testing of our models on more challenging enhance-ment settings to future work.",
  ". Limitations": "The main limitation of the proposed method is the strongassumption that the textual content of the target spoken ut-terance is available as input to the model during inference.As we have argued in the introduction there are many prac-tical uses where this is a valid assumption (e.g. a preparedconference speech or song lyrics). In our qualitative exam-ples, we also show that it is possible to use related tech-nologies such as ASR to obtain approximate transcriptionswhich can be used in our method for target speaker separa-tion.",
  ". Societal impact": "The development of strong multi-modal speech enhance-ment models opens up exciting opportunities for useful ap-plications, as noted in the introduction. The new methodcan be used without requiring synchronisation while achiev-ing improved performance. It also provides the opportunityfor novel applications, for example, it can use subtitles infilms to suppress all background music. However, there arepossible malign uses of providing a new method to isolate aspeaker from others in terms of surveillance.In the novel setting that we consider in this paper, how-ever, the natural language content is presumed to be al-ready known (or can be obtained beforehand through othermeans). We therefore argue that this concern does not ap-ply in our setting and that overall the potential benefits frombenevolent uses (e.g. medical applications and smart hear-ing aids) outweigh the limited risks.",
  ". Conclusion": "We have presented a multi-modal speech enhancementmethod that can condition on multiple non-aligned modal-ities. We also introduced text-conditioned speech enhance-ment as a new task and showed how our proposed archi-tecture can efficiently solve it. Our trained models demon-strated state-of-the-art performance in a variety of settingsas well as robustness to synchronisation issues and missinginformation. In future work, we will consider extending ourframework by adding new types of embeddings to conditionon modalities such as (i) a particular person (expanding onprevious work in )(ii) the language spoken (e.g. to picka French speaker out of English ones).",
  "Ruohan Gao and Kristen Grauman.VisualVoice: Audio-Visual Speech Separation with Cross-Modal Consistency. InProc. CVPR, 2021. 1, 2, 3, 7, 8": "Rongzhi Gu, Shi-Xiong Zhang, Yong Xu, Lianwu Chen,Yuexian Zou, and Dong Yu. Multi-modal multi-channel tar-get speech separation. IEEE Journal of Selected Topics inSignal Processing, 14(3):530541, 2020. 3 John R. Hershey, Zhuo Chen, Jonathan Le Roux, and ShinjiWatanabe.Deep clustering: Discriminative embeddingsfor segmentation and separation.In 2016 IEEE Interna-tional Conference on Acoustics, Speech and Signal Process-ing (ICASSP), pages 3135, 2016. 2",
  "Andrew Owens and Alexei A. Efros.Audio-visual sceneanalysis with self-supervised multisensory features. Proc.ECCV, 2018. 1, 3": "Sanjeel Parekh, Slim Essid, Alexey Ozerov, Ngoc Q. K.Duong, Patrick Perez, and Gael Richard. Motion informedaudio source separation. In 2017 IEEE International Confer-ence on Acoustics, Speech and Signal Processing (ICASSP),pages 610, 2017. 3 Jie Pu, Yannis Panagakis, Stavros Petridis, and Maja Pantic.Audio-visual object localization and separation using low-rank and sparsity. In 2017 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pages29012905, 2017. 2 Colin Raffel, Brian Mcfee, Eric Humphrey, Justin Salamon,Oriol Nieto, Dawen Liang, and Daniel Ellis. mir eval: Atransparent implementation of common mir metrics. In Pro-ceedings - 15th International Society for Music InformationRetrieval Conference (ISMIR 2014), 10 2014. 5 Chandan KA Reddy,Harishchandra Dubey,KazuhitoKoishida, Arun Nair, Vishak Gopal, Ross Cutler, SebastianBraun, Hannes Gamper, Robert Aichner, and Sriram Srini-vasan. Interspeech 2021 deep noise suppression challenge.In INTERSPEECH, 2021. 4",
  "Prajwal K Renukanand, Liliane Momeni, TriantafyllosAfouras, and Andrew Zisserman. Visual keyword spottingwith attention. In Proc. BMVC, 2021. 3, 5": "Antony W Rix, John G Beerends, Michael P Hollier, andAndries P Hekstra. Perceptual evaluation of speech quality(pesq)-a new method for speech quality assessment of tele-phone networks and codecs. In Proc. ICASSP, volume 2,pages 749752. IEEE, 2001. 5 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In International Conference on Medical image com-puting and computer-assisted intervention, pages 234241.Springer, 2015. 2",
  "DeLiang Wang and Jitong Chen.Supervised speech sep-aration based on deep learning: An overview. IEEE/ACMTransactions on Audio, Speech, and Language Processing,26(10):17021726, 2018. 2": "Quan Wang, Hannah Muckenhirn, Kevin Wilson, PrashantSridhar, Zelin Wu, John Hershey, Rif A Saurous, Ron JWeiss, Ye Jia, and Ignacio Lopez Moreno. Voicefilter: Tar-geted voice separation by speaker-conditioned spectrogrammasking. In Interspeech, 2018. 1, 2 Zhong-Qiu Wang and DeLiang Wang. Combining spectraland spatial features for deep learning based blind speakerseparation. IEEE/ACM Transactions on Audio, Speech, andLanguage Processing, 27(2):457468, 2019. 2 Jian Wu, Yong Xu, Shi-Xiong Zhang, Lian-Wu Chen, MengYu, Lei Xie, and Dong Yu. Time domain audio visual speechseparation. In 2019 IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU), pages 667673, 2019. 3"
}