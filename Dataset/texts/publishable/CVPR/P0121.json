{
  "Abstract": "Diffusion Models (DMs) utilize an iterative denoisingprocess to transform random noise into synthetic data. Ini-tally proposed with a UNet structure, DMs excel at produc-ing images that are virtually indistinguishable with or with-out conditioned text prompts. Later transformer-only struc-ture is composed with DMs to achieve better performance.Though Latent Diffusion Models (LDMs) reduce the com-putational requirement by denoising in a latent space, it isextremely expensive to inference images for any operatingdevices due to the shear volume of parameters and featuresizes. Post Training Quantization (PTQ) offers an imme-diate remedy for a smaller storage size and more memory-efficient computation during inferencing. Prior works ad-dress PTQ of DMs on UNet structures have addressed thechallenges in calibrating parameters for both activationsand weights via moderate optimization. In this work, we pi-oneer an efficient PTQ on transformer-only structure with-out any optimization. By analysing challenges in quantizingactivations and weights for diffusion transformers, we pro-pose a single-step sampling calibration on activations andadapt group-wise quantization on weights for low-bit quan-tization. We demonstrate the efficiency and effectiveness ofproposed methods with preliminary experiments on condi-tional image generation.",
  ". Introduction": "Diffusion Models (DMs) represent a powerful classof generative models that have gained significant attentionin recent years due to their ability to generate high-qualitysynthetic data.These models operate by iteratively de-noising random noise to generate synthetic data, offeringpromising applications in various fields, including computervision , natural language processing ,and image generation . The UNet archi-tecture is a popular variant of diffusion models that in-corporates a U-shaped network topology.This architec-ture is characterized by a contracting path, which capturescoarse-grained features, and an expansive path, which fa- cilitates the reconstruction of detailed structures. Dms withUNet structure have demonstrated efficacy in various im-age generation tasks , offering a balance be-tween efficiency and performance .In contrast,the transformer-only structure eschews convolutional lay-ers in favor of a transformer architecture, which excelsin capturing long-range dependencies and global context.Transformer-only diffusion models are a versatileand potent approach to generative modeling, boasting effec-tive long-range dependency modeling, parallelizable com-putation, adaptability across data modalities and resolu-tions, alongside inherent interpretability. Nonetheless, thecomputational demand is inherently high because of the ex-tensive storage space needed for parameters and the opulentmemory required for inferencing features. Post-TrainingQuantization (PTQ) mitigates this demand by compress-ing model parameters after training, thereby reducing com-putational operations during inference.Previous studies have tackled the challenge of quantizing UNet-structured DMs concerning both activations and weights.While some calibration techniques can be extended to dif-fusion transformers, these approaches necessitate moderateoptimization efforts and rely on UNet-specific structures,such as shortcut connections.In this research, we pio-neer the investigation of quantizing a transformer-only dif-fusion model without any optimizations. By addressing theactivation quantization challenge through calibration withsingle-step sampling and tackling the weight quantizationchallenge through group-wise quantization adaptation, weshowcase the effectiveness and efficiency of our proposedmethod in the text-to-image generation task.",
  ". Related Work": "PTQ for Diffusion Models Quantizing DMs involves re-ducing numerical precision to enhance model efficiencyand minimize operational size, with PTQ refining themodel post-training using calibration data. While quanti-zation shows promise for Large Language Models (LLMs), adapting it to DMs presents unique challengesdue to their iterative and dynamic denoising steps, exacer-bating quantization of activations containing dynamic out-",
  "arXiv:2406.11100v1 [cs.CV] 16 Jun 2024": "liers through simple linear quantization. Previous works address these challenges by selecting uniformlydistributed calibration data across inference timesteps andoptimizing quantization parameters for dynamic activationranges. While all aforementioned works focusing on UNet-structured DMs, in this paper, we present an analysis fo-cusing on transformer-only DMs to propose an efficient andeffective quantization strategy without any optimizations.Quantization of Diffusion Transformers To the best ofour knowledge, no prior research has specifically exploredthe quantization of diffusion transformers. However, ex-isting studies have investigated challenges related to quan-tizing transformers for vision tasks.For instance, proposes a method that distills knowledge from the par-ent model to correct query information distortion. Simi-larly, employ Quantization-Aware Training (QAT)to mitigate information distortion in self-attention maps.These approaches primarily target distortions in the atten-tion mechanism and typically require substantial retrainingefforts. Moreover, the dynamic activation ranges observedduring multiple sampling steps exacerbate quantization er-rors cumulatively. In this study, we direct our attention to-ward addressing the challenge of quantizing dynamic acti-vations and dispersed weights without the need for retrain-ing, using Post-Training Quantization (PTQ).",
  "v = s clip(round(v/s), cmin, cmax)(1)": "where s denotes the quantization scale parameters. round()represents a rounding function . cmin and cmax arethe lower and upper bounds for the clipping function clip().Calibrating parameters through weight and activation distri-bution estimation is crucial. The quantization error can bemeasured as the L2 norm of the Euclidean distance betweenquantized and unquantized elements:",
  ". Diffusion Models": "DMs involve two processes: forward diffusion andreverse diffusion.Forward diffusion process adds noiseto input image, x0 to xT iteratively and reverse diffu-sion process denoises the corrupted data, xT to x0 it-eratively.The forward process adds a Gaussian noise,N(xt1; 1 txt1, tI), to the example at the previoustime step, xt1. The noise-adding process is controlled by a noise scheduler, t. The reverse process aims to learn amodel to align the data distributions between denoised ex-amples and uncorrupted examples at time step t1 with theknowledge of corrupted examples at time step t. To simplifythe optimization, proposes only approximate the meannoise, (xt, t) N(xt1; 1 txt, tI), to be denoisedat time step t by assuming that the variance is fixed. So thereverse process or the inference process is modeled as:",
  "conditional : p(xt1|xt) = (xt, t, (y))(2)": "where y is a conditional context (i.e. class labels or textprompts).Initially, UNet was a popular choice for , but later stud-ies replaced it with a transformer-only structure.Diffusion transformers differ from UNet models by hav-ing fewer convolutional layers, heavily utilizing linear lay-ers, and lacking shortcut connections between downsam-pling and upsampling stages. Because of these differences,prior research on quantizing diffusion UNet may not adaptwell to diffusion transformers, particularly when quantizingweights to lower bits.",
  "Calibrate Activation with 1-step Calibration": "Fig 1 highlights the substantial variation in activationsacross sampling steps, a common issue also acknowledgedin prior UNet DM quantization studies. Without optimiza-tion, calibrated parameters (cmin and cmax) display signif-icant fluctuation between initial and final sampling steps,leading to inconsistent stability throughout different stages.This phenomenon results from incrementally introducednoise during forward diffusion, which triggers estimatednoise to adapt to gradual changes. We assess the robustnessof DiT parameterized components against an 8A4W lowbit quantization setting (8-bit activations and 4-bit weights).Fig 2 displays the SQNR (3) based quantization error be-tween unquantized and quantized features, showing betterresilience for calibrated parameters at the initial reverse dif-fusion step where noise is highest. Thus, we recommendusing a single sampling step with the noise scheduler forquantization parameter calibration due to less varying ac-tivations and maximized noise, leading to more robust pa-rameters overall. See Fig 3 for examples contrasting 1-stepand 50-step calibrations. . Without optimizations, data ranges pose challenges to quantize both activations and weights especially at a lower bit-width.(Above) Activation range varies dynamically across sampling steps and significant outliers persist. (Below) Weights are quantized channel-wise, but dispearsed outliers for each channel introduces high quantization loss when compressed to a lower bit.",
  "Calibrate Weights with Group Quantization": "In Fig 1, we also examine the ranges for weights of a lin-ear layer in all output channels. Although weight quantiza-tion is performed per channel, many channels depicted in display a substantial number of outliers scatteredacross them. This spread creates difficulties when attempt-ing to represent all these weights using a reduced bit width.Indeed, directly applying 4-bit weights leads to severelydistorted image outputs. Prior works have addressed this",
  ". Calibrating through 50 steps produces visible imagenoise. 1-step calibration generates quality closer to the full pre-cision output. Outputs from conditional DiT 8A8W": "problem with QAT and finetuning, but significantretraining and amount of data are required. Since we iden-tified the dispersion of weight as the root cause of the prob-lem, we propose to remedy the quantization difficulty withgroup-wise quantization. Previous works demonstrate thatgroup-wise quantization is adaptable to transformers and isvalid for hardware . In each channel, the weightscan be further divided into groups. Each group will be cali-brated individually. In this manner, the dispersed weightsare divided into smaller ranges and therefore reduce thequantization difficulty for each group. It is simple to imple-ment this group-wise quantization with just two lines. SeeAlgorithm 1 for detailed implementation.",
  "end if": "dation dataset generations following prior methodolo-gies .Classifier-free guidance is fixed at 3.0, andquantization is only applied to parameterized components(e.g., convolutional and fully connected layers). The defaultgroup size is 128 as in . Evaluation Metrics For each experiment, two key metricsare provided: FID and SQNR averaged over sam-pling steps at the output to assess the generation of 512x512images using Pixart Alpha. In addition to FID to COCO, wealso include FID to full-precision model.",
  ". Discussions": "shows that our proposed method significantly sur-passes baselines in both FID to FP and SQNR improve-ments. While FID is commonly used to assess generativemodels, its reliability has been doubted due to potential bi-ases and limitations. Since the COCO datasets are photo-realistic and generated images diverge from this distribu-tion, FID to COCO differences do not convey meaningfuldifference. Additional examples in Fig 4 reveal that the en-hanced models produce more realistic images with reducedquantization noise compared to baseline models.",
  ". Conclusion": "In conclusion, this work represents a pioneering effort in ex-ploring the quantization of transformer-only diffusion mod-els without relying on any optimizations. By addressing ac-tivation quantization challenges via 1-step sampling calibra-tion and overcoming weight quantization hurdles throughgroup-wise adaptation, the proposed approach demonstratesboth effectiveness and efficiency in the text-to-image gener-ation task, providing valuable insights for further advance-ments in this area. Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King,and Michael R Lyu. Towards efficient post-training quanti-zation of pre-trained language models. Advances in NeuralInformation Processing Systems, 35:14051418, 2022. 1 Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,Michael W Mahoney, and Kurt Keutzer.Zeroq: A novelzero shot quantization framework.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1316913178, 2020. 2 Yue Cao, Bin Liu, Mingsheng Long, and Jianmin Wang.Hashgan:Deep learning to hash with pair conditionalwasserstein gan. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 12871296,2018. 4 Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, EnzeXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,Huchuan Lu, and Zhenguo Li. Pixart-: Fast training of dif-fusion transformer for photorealistic text-to-image synthesis,2023. 1, 2 Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Dif-fusiondet: Diffusion model for object detection. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 1983019843, 2023. 1",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural informationprocessing systems, 33:68406851, 2020. 1, 2": "Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang,and Tatsunori B Hashimoto. Diffusion-lm improves control-lable text generation. Advances in Neural Information Pro-cessing Systems, 35:43284343, 2022. 1 Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, ZhenDong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.Q-diffusion: Quantizing diffusion models. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1753517545, 2023. 1, 2 Yanjing Li, Sheng Xu, Mingbao Lin, Xianbin Cao, Chuan-jian Liu, Xiao Sun, and Baochang Zhang. Bi-vit: Pushing thelimit of vision transformer quantization. In Proceedings ofthe AAAI Conference on Artificial Intelligence, pages 32433251, 2024. 2 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 4 Daochang Liu, Qiyue Li, Anh-Dung Dinh, Tingting Jiang,Mubarak Shah, and Chang Xu. Diffusion action segmenta-tion. In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision, pages 1013910149, 2023. 1 Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang, XinggangWang, and Wenyu Liu. Pd-quant: Post-training quantiza-tion based on prediction difference metric. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2442724437, 2023. 3",
  "William Peebles and Saining Xie. Scalable diffusion modelswith transformers. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 41954205,2023. 1, 2": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In Medical image computing and computer-assistedinterventionMICCAI 2015: 18th international conference,Munich, Germany, October 5-9, 2015, proceedings, part III18, pages 234241. Springer, 2015. 1",
  "tuning text-to-image diffusion models for subject-drivengeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2250022510, 2023. 1": "Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in neural informationprocessing systems, 35:3647936494, 2022. 1 Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, andYan Yan. Post-training quantization on diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 19721981, 2023. 1 Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert.In Proceedings of the AAAI Conference on Artificial Intelli-gence, pages 88158821, 2020. 3, 4 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,and Surya Ganguli.Deep unsupervised learning usingnonequilibrium thermodynamics.In International confer-ence on machine learning, pages 22562265. PMLR, 2015.1, 2",
  "Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, andDebing Zhang.Easyquant: Post-training quantization viascale optimization. arXiv preprint arXiv:2006.16669, 2020.2": "Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, YeyunGong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, WeizhuChen, et al. Ar-diffusion: Auto-regressive diffusion modelfor text generation. Advances in Neural Information Pro-cessing Systems, 36, 2024. 1 Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, JulienDemouth, and Song Han. Smoothquant: Accurate and effi-cient post-training quantization for large language models.In International Conference on Machine Learning, pages3808738099. PMLR, 2023. 1 Sheng Xu, Yanjing Li, Mingbao Lin, Peng Gao, GuodongGuo, Jinhu Lu, and Baochang Zhang. Q-detr: An efficientlow-bit quantized detection transformer. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 38423851, 2023. 2, 3 Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey ofmethods and applications. ACM Computing Surveys, 56(4):139, 2023. 1 Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Addingconditional control to text-to-image diffusion models.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 38363847, 2023. 1"
}