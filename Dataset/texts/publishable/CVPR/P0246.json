{
  "Abstract": "Self-supervised landmark estimation is a challengingtask that demands the formation of locally distinct fea-ture representations to identify sparse facial landmarks inthe absence of annotated data. To tackle this task, exist-ing state-of-the-art (SOTA) methods (1) extract coarse fea-tures from backbones that are trained with instance-levelself-supervised learning (SSL) paradigms, which neglectthe dense prediction nature of the task, (2) aggregate theminto memory-intensive hypercolumn formations, and (3) su-pervise lightweight projector networks to navely establishfull local correspondences among all pairs of spatial fea-tures. In this paper, we introduce SCE-MAE, a frameworkthat (1) leverages the MAE , a region-level SSL methodthat naturally better suits the landmark prediction task, (2)operates on the vanilla feature map instead of on expen-sive hypercolumns, and (3) employs a Correspondence Ap-proximation and Refinement Block (CARB) that utilizes asimple density peak clustering algorithm and our proposedLocality-Constrained Repellence Loss to directly hone onlyselect local correspondences. We demonstrate through ex-tensive experiments that SCE-MAE is highly effective androbust, outperforming existing SOTA methods by large mar-gins of 20%-44% on the landmark matching and 9%-15% on the landmark detection tasks.",
  ". Introduction": "Facial landmark detection is a computer vision task involv-ing the identification and localization of specific keypointscorresponding to particular positions on a human face. Fa-cial landmarks form the crux for many classical downstreamtasks such as 3D face reconstruction , face recogni-tion , face emotion/expression recognition ,and more contemporary applications such as facial beautyprediction and face make-up try on .Albeit extremely useful, training facial landmark de-",
  "Net": ". SCE-MAE vs prior self-supervised facial landmark de-tection methods. Stage 1: Prior works (top) use instance-levelmulti-view SSL paradigms that output less distinct initial localfeatures. Our framework (bottom) leverages MAE to naturallyform better initial features that result in well-defined boundariesbetween facial landmarks (see t-SNE plots). Stage 2: Prior worksoperate on memory-intensive hypercolumns and supervise eachfeature pair to achieve correspondence. Our framework employsa Correspondence Approximation and Refinement Block (CARB)that operates on the original MAE output and directly hones onlythe selected correspondence pairs. For the example query, SCE-MAE outputs a more-focused and sharper similarity map, demon-strating the superiority of the final features. tectors requires numerous precise annotations per sample,making it a laborious and expensive ordeal. Furthermore,landmarks are not always semantically well-defined, mak-ing their annotations prone to inconsistencies and biases, which can severely limit the development ofaccurate landmark models. Motivated to avoid these de-merits, recent works have incorporated the unsu-pervised and self-supervised learning (SSL) paradigms into their methods. SSL-pretrained models haveshown to yield highly effective feature representations with-out the use of labeled data and, at many times, outperformtheir supervised counterparts on the target tasks .Facial landmark detection and matching tasks rely onthe formation of locally distinct features to differentiate be-tween (1) the facial regions (e.g., eye vs. lip), (2) the com-ponents of face parts (e.g., left vs. right corners of the lip),",
  "arXiv:2405.18322v1 [cs.CV] 28 May 2024": "and finally, (3) the specific pixels of each landmark. In thesetting where annotations are severely limited, the recentmethods follow a two-stage training protocol. Dur-ing the first stage, the backbone is trained with a typical SSLobjective. In the second stage, the backbone is frozen anda separate light-weight projector network is trained to en-code local correspondences, i.e., the relationships betweenthe different regions within the same image.Prior work adopted multi-view SSL protocols ,which may be less effective on the landmark estimationtasks due to several factors.Firstly, these augment-and-compare pretext tasks prompt the network to learn category-specific signals, but we operate only on a single category,i.e., the human face. Secondly, contrastive learning requiresa large and diverse set of negative samples to avoid collapse. Lastly, the training objectives might not directlyencourage the model to learn the intricate facial cues withinthe positive face samples to differentiate between facial re-gions, which are required for dense tasks such aslandmark detection and matching.On the other hand, the Masked Image Modeling (MIM)protocol , which requires the network to re-construct the masked regions from limited context, intrin-sically suits our downstream task objective. Based on theobservation that the non-landmark regions (e.g., cheeks andforeheads) are larger and more uniform than the sparse anddistinctive landmark regions (e.g., the eyes and lip corners),we hypothesize that the reconstruction of the masked land-mark regions leads to the formation of effective representa-tions of the facial landmarks. Hence, we choose to adoptthe Masked Autoencoder (MAE) as our backbone inthe first stage of our framework.For the second stage, both CL and LEAD uti-lize objectives to establish correspondences between eachpair of feature descriptors within the same image. Based onthe earlier observation that non-landmark regions are largerand more uniform, we ask the question: is it necessary toestablish correspondences between all feature descriptorpairs? We hypothesize that the selective refinement of theimportant correspondences utilizes the networks parame-ters more effectively. To this end, we employ a novel Corre-spondence Approximation and Refinement Block. Here, wefirst differentiate the MAEs output into attentive (landmarkand important facial regions) and inattentive (insignificantfacial regions or background) tokens using the first-stagecorrespondence signals. Next, a clustering algorithm oper-ates on the inattentive tokens and approximates the mem-ber tokens using the cluster center. Finally, we supervisea light-weight projector network using a novel Locality-Constrained Repellence Loss that penalizes the erroneousstrong correspondences between the different token typesweighted by spatial proximity. Here, only the select corre-spondences are directly refined since the loss operates only on the attentive tokens and inattentive cluster center proxies.In order to highlight the above stage-wise merits of ourapproach, we visually compare, at a high level, our frame-work, which we term Selective Correspondence Enhance-ment with MAE (SCE-MAE), with prior works in .Our approach not only produces more distinguishable first-stage features but also outputs sharper similarity maps cor-responding to the example query, testifying to superior finallandmark representations.In this paper, we show that by leveraging MAE during the first stage and systematically eliminating redun-dant correspondence learning during the second stage, SCE-MAE can output locally distinct facial landmark represen-tations without the use of labeled data. As a result, it out-performs the previous SOTA methods by large margins. Wesummarize our contributions below:1. We are the first to adopt an MIM-trained SSL backbonefor the first-stage training of self-supervised facial land-mark detection and matching methods. We demonstrateusing MAE that the mask-and-predict pretext taskmore naturally suits the downstream objective and deliv-ers highly potent initial landmark representations. 2. We introduce the Correspondence Approximation andRefinement Block (CARB) during the second-stage toidentify and approximate the features of unimportantnon-landmark regions, and subsequently operate a novelLocality-Constrained Repellence (LCR) Loss to directlyhone only the salient correspondences. 3. We demonstrate the effectiveness and robustness of ourframework, SCE-MAE, as it surpasses existing SOTAmethods on the landmark matching (20%-44%) anddetection (9%-15%) tasks under various annotationconditions for several challenging datasets.",
  ". Related Works": "Self-Supervised Learning (SSL). By solving unique pre-text tasks, SSL methods are able to learn discriminativefeature representations from unlabeled data. Early worksexplored pretext tasks such as predicting the rotation angle and recovering the original image from random per-muted patches . Recently, invariant and contrastivelearning based SSL methods have gainedpopularity due to their ability to capture high-level seman-tic concepts from the data. Invariant learning aims to learntransformation invariant features by forcing the representa-tions of two randomly augmented views of the same imageto be similar. Contrastive learning defines different views ofan anchor image as positives and views of different imagesas negatives. Here, the objective is to pull the represen-tations of the anchor and positives together while pushingapart those of the anchor and negatives. To achieve this,MOCO and SimCLR adopted the InfoNCE andNT-Xent losses respectively. These methods operate at the encoded image or instance-level and can be categorizedas augment-and-compare SSL methods .Recently, the Masked Image Modeling (MIM) protocolhas gained significant momentum . Thesemethods operate at the region-level and learn to recover themasked regions from the contextual information containedin the unmasked patches. It has been empirically shownthat by using non-extreme masking ratios or patch sizes inMasked Autoencoders (MAE) , the representation ab-stractions capture robust high-level information, while ex-treme masking ratios capture more low-level information. With higher masking ratios as the norm, MAE exe-cutes dense reconstruction, making them intrinsically suit-able for dense prediction tasks .For the first stage of self-supervised face landmark de-tectors, ContrastLandmark (CL) and LEAD utilizeMOCO and BYOL pretrained backbones respec-tively. Since neither MOCO nor BYOL operate explicitly atthe sub-image (region/pixel) level, the representations usedfor the second stage of CL and LEAD are potentially sub-optimal. On the other hand, the sparse nature of facial land-marks perfectly matches the MIM objective to reconstructthe whole view from unmasked patches, resulting in higherfidelity coarse local features. Hence, in our framework, weadopt the MAE as our first stage SSL protocol.Unsupervised Landmark Prediction. To tackle landmarkprediction without annotated data, there have been severalapproaches. Equivalence learning leverages transformationequivalence as a free supervision signal to learn landmarkembeddings . Since an undesirable constant vector out-put would satisfy the objective, adding a diversity loss orenabling similarity enforcement through intermediate aux-iliary images are proposed to tackle the issue . An-other approach is through generative modeling where land-marks are discovered by training networks with a recon-struction objective such as recon-structing the human image with a different pose .Recent works such as ContrastLandmark (CL) andLEAD have adopted SSL methods to extract coarsefeatures that capture the broad semantic concept and furtherprocess them to establish regional/local correspondences.CL and LEAD construct hypercolumns and compact themusing proximity-guided and correspondence guided reduc-tion objectives respectively. While both methods reduce thefinal representation size, hypercolumns are memory-wiseenormous structures and operating on them is a computa-tionally intensive process. Furthermore, each spatial fea-ture pair is subject to the optimization objective, neglectingthe possibility that some local correspondences do not con-tribute as much to the downstream task. On the contrary,using our SCE-MAE framework, we do not operate on ex-pensive hypercolumns, and we identify and directly processonly salient local correspondences.",
  ". Method": "We depict our proposed Selective Correspondence En-hancement with MAE (SCE-MAE) framework in anddetail each component in the following subsections. In Sec.3.1 we revisit Masked Image Modeling to introduce theMAE as a more suitable and potent first stage proto-col. In Sec. 3.2, we elaborate on the setup to execute se-lective correspondence through the process of reducing theeffective number of final correspondence pairs. In Sec. 3.3,we introduce our Correspondence Approximation and Re-finement Block, wherein we explain the components of ournovel Locality-Constrained Repellence Loss and how it di-rectly hones only the selected correspondences.",
  ". A Revisit of Masked Image Modeling": "Masked Image Modeling (MIM) is an SSLparadigm that involves the reconstruction of the original im-age from the unmasked patches. Taking MAE as anexample, given an input image x, the encoder first dividesthe image into non-overlapping patches xp with positionalembedding added to them. A class token is appended tothe patch tokens but will not be affected by the followingmasking procedure. A binary mask M is randomly sam-pled to determine the masked out regions. The unmaskedpatches are denoted by xp = xp M, where symbolizesthe Hadamard product, and are processed by the encoderto output the patch embeddings f p. Finally, MAE uses aspecial embedding [MASK] to fill in the masked positions,f p = f p + [MASK] (1 M), and reconstruct x fromf p by minimizing the pixel-level mean squared error via alight-weight decoder. The reconstruction task requires thenetwork to capitalize on the limited semantic context pro-vided by the unmasked patches and the supplied positionalinformation. This encourages the network to forge discrim-inative features that are optimal for differentiating and lo-calizing the important landmark regions.",
  ". Setup for Selective Correspondence": "Attentive-Inattentive Separation. The second stage of theframework aims to establish local correspondences effec-tively to ensure that the representations reflect the extent ofsimilarity and dissimilarity between the different facial re-gions. To achieve this, we propose to execute selective cor-respondence, i.e., the elimination of the direct refinement ofunimportant non-landmark correspondences, and focus onoptimizing those that are critical for landmark disambigua-tion. The first step in this endeavor is to identify potentiallandmark and non-landmark regions. Due to the observableopposing nature of facial landmarks (sparse and distinct)and non-landmark regions (dense and uniform), we hypoth-esize that they are coarsely distinguishable using the firststage backbone features.",
  "PositionalEmbedding": ". An overview of the second stage of our proposed SCE-MAE. We first split the MAE patch tokens into attentive (blue) andinattentive (yellow) tokens based on CLS token similarity. The inattentive tokens are clustered into K cluster centers. In the Correspon-dence Approximation and Refinement Block (CARB), we first substitute the inattentive tokens using the cluster centers (square symbols)and then refine the local features using our novel Locality-Constrained Repellence (LCR) Loss. The LCR loss weakens existing erroneouscorrespondences in a weighted manner by considering the token-pair proximity (locality) and correspondence type (repellence) constraints. Following MAE , we adopt the ViT as our back-bone architecture. The class (CLS) token represents the im-age and is obtained by aggregating information from theother patch tokens over several layers. Since landmarks aresparse and have more distinct texture, we expect the corre-sponding tokens to have a large influence on the CLS tokenrepresentation. After pretraining, we compute a similarityvector between the CLS token and all patch tokens as:",
  "d) RN,(1)": "where qcls, K, d, and N denote the CLS token query vector,the patch token key matrix, latent dimension, and number ofpatch tokens respectively. Here, qcls Rd and K RNd.We then split the N patch tokens into two groups: (1) atten-tive group, consisting of the N tokens that have the high-est similarity score with the CLS token, and (2) inattentivegroup, consisting of the remaining (1)N tokens. Here, is a hyperparameter between 0 and 1. We observe that theinattentive tokens mostly cover non-landmark face regions(See ), such as cheeks and forehead, as well as back-ground. Henceforth, we presume that attentive tokens coverthe landmark and important facial regions, while inattentivetokens correspond to unimportant non-landmark regions.Inattentive Token Clustering. Since several inattentive to-kens often correspond to the same facial region (e.g., cheek,forehead, etc.), the downstream correspondence objectivesassociated with them would likely be redundant. By ap-plying a clustering algorithm on the inattentive tokens, wecan represent numerous non-landmark regions with only ahandful of cluster centers. Selective correspondence canthen be set up by discarding all non-cluster center tokens,ensuring that no correspondence is established with them. Specifically, we adopt a simple density peak clustering al-gorithm , wherein two variables and are defined foreach inattentive token. Here, i measures the density of thei-th token and i computes the minimum distance from thei-th token to any other inattentive token which has a higherdensity. Mathematically, they are defined as:",
  "i = minj:j>iti tj2,if j s.t. j > imaxjti tj2,otherwise, (3)": "where ti, tj Tinatt, and Tinatt denotes all inattentive to-kens. Since the cluster center should have higher densitythan neighbouring tokens and should also be distant to othercluster centers, the cluster center score of the i-th token iscomputed by i i. We select the top-Kc scoring tokens ascluster centers, where Kc is a hyperparameter. The remain-ing inattentive tokens are discarded and the cluster centertokens subsequently act as representative proxies for them.",
  ". Selective Correspondence using CARB": "In our Correspondence Approximation and RefinementBlock (CARB), we first substitute the discarded inattentivetokens with their corresponding cluster centers and aggre-gate the relevant visual features to obtain a complete 2D fea-ture map as illustrated in . With the backbone frozen,the feature map is passed through a light-weight projector,which is supervised by our novel Locality-Constrained Re-pellence (LCR) Loss.As the LCR loss operates on thefeatures of attentive tokens and inattentive cluster centers,we directly refine only the most important correspondences,thereby achieving selective correspondence. Locality-Constrained Repellence (LCR) Loss. We de-sign and operate the LCR loss to yield high-fidelity fine-grained features by optimally refining local correspon-dences. Henceforth, we use Tatt and Tinatt to denote theattentive and the approximated inattentive tokens (clustercenters) respectively, and define T= Tatt Tinatt, as theset of all considered tokens.We begin by formally defining correspondence, i.e., theprobability that a patch token tj corresponds to a patch to-ken ti in the image x, which is expressed as:",
  "tkT exp(ti(x), tk(x)/),(4)": "where ti(x) is the final projected feature representation ofpatch ti, and is the temperature parameter.We observe that image patches that are spatially distantfrom each other often correspond to different facial regions.Hence, it should follow that strong correspondences be-tween distant patches are likely to be erroneous and shouldbe discouraged. We compute a locality constraint to formal-ize this idea using the following function:",
  "floc(ti, tj) = log(ti tj + 1),(5)": "where ti, tj T, and computes the spatial distance. Thelog function saturates the coefficient in order to discouragethe network from excessively focusing on separating verydistant correspondences. Although a similar constraint wasintroduced in , the primary motive was to avoid collapseduring equivalence learning.Considering the attentive (Tatt) and the approximatedinattentive (Tinatt) token sets, there are three types ofcorrespondences: attentive-attentive (att att), attentive-inattentive (attinatt), and inattentive-inattentive (inattinatt). We introduce a repellence coefficient to quantify theimportance of each correspondence type:",
  ". Comparison between the original (red) and re-annotated(green) landmarks in AFLWR test set. We denote the original andcorrected test sets as AFLWRO and AFLWRC respectively": "Inference. After training, we obtain optimized representa-tions for all image regions. Hence, during inference, we by-pass the clustering and inattentive token approximation pro-cedures and only utilize the original features for the down-stream tasks. Additionally, we require to spatially expandthe size of the feature input to the projector for fair com-parison against prior works. Navely decreasing the patchsize to expand the output not only quadratically increasesthe computation and memory costs but also may lead to theformation of inferior feature representations . In-stead, we adopt the cover-and-stride technique to pro-duce more fine-grained and rich expanded representations.",
  ". Experiments": "Datasets. Following prior works , we pretrainour backbone on the CelebA dataset, which contains162,770 images. Face landmark detection is evaluated onfour datasets: MAFL , 300W and two variantsof AFLW . MAFL consists of 19,000 training imagesand 1,000 test images. 300W has 3148 training images and689 test images. AFLWM contains 10,122 training imagesand 2995 testing images, which are crops from MTFL.AFLWR contains tighter crops of face images where thetraining and test set has 10,122 and 2,991 images respec-tively. Note that 300W provides 68 annotations per imagewhile the other three datasets only provides 5 annotations.Re-annotation. Although the AFLWR has been used inprior works, the fidelity of the annotations are question-able. In , we visualize several annotation errors inthe AFLWR test set using red dots. These include errorsarising due to semantic mismatches, translations, and ran-dom shifts. For a more consistent and reliable evaluation,we re-annotate the AFLWR test set and illustrate a few ofthese corrections using green dots in . In the follow-ing sections, we use AFLWRO and AFLWRC to denote theoriginal and corrected dataset respectively.Implementation Details. We pretrain our models on theCelebA dataset using MAE with three backbones: DeiT-T,DeiT-S and DeiT-B. All models were trained for 400 epochs",
  "GT": ". Qualitative results on landmark matching. The reference/ground-truth are shown in the top/bottom row. The middle rows showthe matching results of our method and prior works, grouped column-wise by errors occurring with the eyes, nose and lip corner landmarksrespectively. Our method outputs consistently more accurate matching resulting from leveraging higher fidelity projected features. with a batch size of 512, a learning rate of 3e-4 and patchsize of 8.Following DVE , we resize the image to136136 and crop the center 9696 as input for both land-mark matching and regression. We set the attentive rate to0.25 for DeiT-B and 0.1 for DeiT-T and DeiT-S. We applyclustering after the third encoder layer and set the numberof clusters Kc to 4. For the LCR loss, the three repellencehyperparameters are set to rattnattn = 5, rattninattn =5, rinattninattn = 2. Ablation studies on the various hy-perparameters are included in the Supplementary Material.",
  ". Landmark Matching": "Evaluation Protocol. Following , 1000 reference-and-test image pairs are generated from MAFL test set for eval-uation. The first 500 pairs serve as the benchmark for land-mark matching between same identities, which containsthe original image and its thin-plate-spline (TPS) deformedcounterpart.The other 500 pairs are of different identi-ties. During evaluation, all feature maps are bi-linearly up-sampled to the image resolution. Landmark representationsof the reference image are used to query the test image.The location with the highest cosine similarity is consid-ered as the matched prediction. Finally, we compute theMean Pixel Error between the prediction and ground-truth.Quantitative Results. We compare our method with exist-ing SOTA methods in by grouping the results basedon the final feature size. We use three different backbonesto control the number of parameters for a fair comparison.In the first group, our model with DeiT-T, being a fraction . Quantitative evaluations on landmark matching. Wereport the mean pixel error between the prediction and ground-truth on 1000 image pairs sampled from MAFL. The best and sec-ond best results are shown in bold and underline respectively. Wegroup the results by the projected feature dimension. Our methodoutperforms all prior works by large margins within each groupfor both the same and different identity settings.",
  "Ours DeiT-B85.32560.271.61": "of the size of prior works, already outperforms the SOTA. Inthe second and third group, our method visibly outperformsprior works by large margins of 20% and 44% for thesame and different identities respectively. We attribute thisto the highly potent initial features from the MAE pretrain-ing, which, when strategically refined through selective cor-respondence using CARB, yields distinctive final featuresthat were vital for successful landmark matching. . Quantitative evaluations on landmark detection with all annotated samples. We compare our method with existing SOTA andreport the error as the percentage of inter-ocular distance on four human face datasets: MAFL, AFLWM, AFLWR and 300W. For AFLWR,we report the results on both the original (AFLWRO) and corrected (AFLWRC) datasets. Our method, despite using significantly smallerfeatures by avoiding expensive hypercolumns, outperforms prior works on all four datasets, even with our smallest backbone, DeiT-T.",
  "Ours DeiT-T5.42562.205.895.544.864.22Ours DeiT-S21.45122.085.335.404.693.94Ours DeiT-B85.310242.075.235.334.603.95": "Qualitative Results. We visualize our landmark matchingresults between different identities and compare our resultswith existing SOTA methods in . The mismatcheson different landmarks when using CL and LEAD are shown in different columnar groups, e.g., the first groupof three columns contain eye-related mismatches.Ourmethod clearly achieves a more accurate matching perfor-mance across all landmarks even on difficult examples suchas those wearing eye-glasses. Admittedly, our method ex-periences some failures when the poses of reference and testsamples are vastly dissimilar or when landmark regions areseverely occluded. Some of the failure cases are shown inthe Supplementary Material.",
  ". Landmark Detection": "Evaluation Protocol. Following prior works , wefreeze the pretrained backbone and projector, and only traina light-weight regressor. Our regressor consists of a con-volution block (instead of a linear layer) and a linear layerwhen training on all annotated samples. The convolutionblock utilizes the spatial context to produce I intermediateheatmaps for each landmark which are converted to I pairsof 2D coordinates by a soft-argmax operation and fed to alinear layer that outputs the final landmark prediction. Weset I = 50 for all experiments . We leverage thefirst and second stage concatenated features as a more ro-bust input to the regressor as we expect the backbone to pro-vide rich task-agnostic representations during the first stageand the projector to supplement task-specific cues criticalfor landmark detection during the second stage. For a faircomparison, the reported results are produced with their of-ficial implementation and checkpoints, if available.All Annotated Samples. We compare our method withprior works on landmark detection benchmarks in .Once again, even with our smallest backbone DeiT-T, beinga fraction of the size of prior works, outperforms existingSOTA. Considering our best results, our method achieves9%-15% performance gain across different benchmarks. Such a compelling performance is an attestation to the ex-cellent discriminative ability of our features, which pro-vide intricate disambiguation cues to the regressor for locat-ing landmarks. We also highlight that all methods achievelower error with our re-annotated AFLWRC test set, henceconfirming the higher annotation quality.Limited Annotated Samples.We compare our methodwith prior works on landmark detection under limited anno-tation in . Our method outperforms all existing SSL-based methods with significant performance gain under allannotation and feature dimension settings. Specifically, weachieved a relative gain of 8.6% on average and as highas 20.1% compared to the existing SOTA. Furthermore, weobserve a smaller standard deviation with repeated experi-ments, indicating that our method produces optimal featuresmore consistently, hence attesting to its robustness.",
  ". Ablation Studies": "Importance of Each Component. To better understand ourproposed SCE-MAE framework, we report the component-wise ablation analysis on the landmark matching task in Ta-ble 4. The first three rows indicate the usage of only thefirst-stage backbone features, while the last two rows, re-spectively, indicate the inclusion of clustering and LCR lossin our framework. CL and LEAD utilize hyper-columns whereas we leverage the vanilla last layer featuresof the pretrained MAE, which we indicate as Baseline. Us-ing the backbone alone, our baseline outperforms CL andLEAD, validating our motivation that MIM is a more suit-able pretext task for landmark representation learning. Weobserve that the clustering assists the matching between thesame identity while the LCR loss boosts the matching per-formance between different identities. Overall, these trendsalign with our expectations: initially, the region-level first-stage MAE features capture local intricacies but are too rawto generalize the landmarks across different identities; theclustering disambiguates the landmarks from the unimpor-tant regions, which improves the same identity matching . Quantitative evaluations on landmark detection with limited annotated samples. We compare our method with existingSOTA under different annotation settings on the AFLWM dataset and report the error as the percentage of inter-ocular distance. Ourmethod proves to be more effective by offering notably lower errors and more robust by yielding a lower std of errors than prior works.",
  "Dim.15102050100": "DVE6414.23 1.4512.04 2.0312.25 2.4211.46 0.8312.76 0.5311.88 0.16CL6424.87 2.6715.15 0.5313.52 1.0811.77 0.6811.57 0.0310.06 0.45LEAD6421.80 2.5413.34 0.4311.50 0.3410.13 0.459.29 0.419.11 0.25SCE-MAE (Ours)6418.41 1.2111.79 0.4410.57 0.249.65 0.148.60 0.178.31 0.06 CL12827.31 1.3918.66 4.5913.39 0.3011.77 0.8510.25 0.229.46 0.05LEAD12821.20 1.6713.22 1.4310.83 0.659.69 0.418.89 0.208.83 0.33SCE-MAE (Ours)12820.14 1.7611.99 0.7110.40 0.229.25 0.148.49 0.197.96 0.21 CL25628.00 1.3915.85 0.8612.98 0.1611.18 0.199.56 0.449.30 0.20LEAD25621.39 0.7412.38 1.2811.01 0.4810.06 0.598.51 0.098.56 0.21SCE-MAE (Ours)25617.08 1.3511.28 0.5410.30 0.098.95 0.088.20 0.207.58 0.09 . Component-wise ablation on landmark matching. Thefirst three rows compare the results using backbone features only.Our raw MAE features (Baseline) are compared against the hyper-columns used in CL and LEAD. The last two rows indicate theinclusion of the clustering and the LCR loss in our framework.",
  "Left EyeRight EyeNoseLeft Lip CornerRight Lip Corner": ". t-SNE plot of the landmark representations. denotesthe usage of the stage 1 hypercolumn representations. SC denotesthe Silhouette Coefficient , a score (higher is better) whichmeasures the quality of the clustering. Our method results in botha clear separation between the landmarks and the densest landmarkclusters, resulting in the highest Silhouette Coefficient.",
  ". Conclusion": "In this work, we present SCE-MAE, a two-stage frame-work to tackle the self-supervised face landmark estimationtasks. To learn effective and locally distinct representations,we target structured improvements on both stages. For thefirst stage, we leverage the region-level MAE instead ofinstance-level SSL methods to derive more potent initialrepresentations. For the second stage, we demonstrate thatit is beneficial to identify important facial regions and di-rectly hone only the salient correspondences. Our approachyields discriminative and high-quality landmark represen-tations that result in superior performance over prior SOTAworks on both the landmark matching and detection tasks.Due to the nature of facial data, we believe that further re-search on the sparsification of the correspondence computa-tion through the systematic elimination of insignificant cor-respondences could allow future self-supervised landmarkestimation methods to better exploit inter-landmark depen-dencies and form higher-caliber landmark representations.",
  "Aniwat Juhong and C. Pintavirooj. Face recognition basedon facial landmark detection. In 2017 10th Biomedical En-gineering International Conference (BMEiCON), 2017. 1": "Tejan Karmali, Abhinav Atrishi, Sai Sree Harsha, SusmitAgrawal, Varun Jampani, and R Venkatesh Babu. LEAD:Self-supervised landmark estimation by aligning distribu-tions of feature similarity. In WACV, 2022. 1, 2, 3, 5, 6,7, 8 Robin Kips, Ruowei Jiang, Sileye Ba, Edmund Phung,Parham Aarabi, Pietro Gori, Matthieu Perrot, and IsabelleBloch. Deep graphics encoder for real-time video makeupsynthesis from example. In CVPRW, 2021. 1",
  "Supplementary Material": "Figure S6. Visualization of inattentive regions and clustering. The original images are shown in the first row and the visualizationresults are shown in the second row. The inattentive regions are represented by colored patches and each color represents one cluster. Wecan see none of the landmark regions are classified as inattentive regions and semantically similar inattentive regions are grouped together.",
  ". Qualitative Studies on Inattentive Regions": "In of the main paper, we highlighted the observa-tion that the non-landmark regions (e.g., cheeks and fore-heads) are larger and more uniform than the sparse and dis-tinctive landmark regions. In .2, we explained thatin order to setup selective correspondence, we first target theseparation of the critical facial and the insignificant regionsusing the CLS token output of the MAE, and then run a sim-ple clustering algorithm on the insignificant regions. To bet-ter understand this attentive-inattentive separation and howthe clustering works, we visualize a few examples in FigureS6. The inattentive regions are denoted by colorful patchesand the patches with the same color belong to the same clus-ter. We observe that semantically similar regions are clus-tered together and none of the landmark regions are classi-fied as inattentive regions, hence corroborating our earlierhypothesis.",
  ". Effects of Changing Backbone Architecture": "In this line of work, we are the first to adopt the Vision-Transformer as the backbone architecture. In this sec-tion, we evaluate whether prior works can benefit by simplychanging the backbone to ViT architectures. We switch thebackbone of CL and LEAD and report the quan-titative results on landmark matching in Table S2, and onlandmark detection in Table S3. Note that both CL andLEAD rely on the extraction of hypercolumns which re-quire feature map of different spatial resolution. However,the hypercolumns are not compatible with our backbones asDeiTs are columnar (patch-based) architectures whichcan only output feature maps at the same spatial size. Forthis reason and for a fair comparison with our work, we evaluate the previous methods using the last layer featurefrom DeiTs. For landmark matching, the mean pixel errorincreases dramatically after changing the backbone for boththe matching between same and different identities.Weobserve similar phenomenon on landmark detection wherethe performance drops on all evaluated datasets except forMAFL. Additionally, we find that the performance of CLand LEAD does not improve with a larger backbone (DeiT-S compared to DeiT-T). We attribute the performance dropto two main reasons: (1) the first stage SSL protocols ofCL and LEAD, namely MoCo and BYOL , arenot designed to accommodate the requirements of vision-transformer backbone. This also explains why the perfor-mance doesnt improve after applied a larger backbone. Al-though integration of ViT to the MoCo framework has beenaddressed in MoCov3 , integrating MoCov3 to CL is be-yond the scope of our work. (2) The use of hypercolumns isessential for CL and LEAD, however they are not availablewhen using the DeiTs. In conclusion, navely switching thebackbone architecture does not necessarily yield better re-sults. The performance gain of our SCE-MAE frameworkover existing SOTA originates due to the intrinsic compati-bility of the first-stage MAE protocol (with ViT backbones)and the ability to leverage the ViT output during the secondstage.",
  ". Attentive Rate": "We use attentive rate to decide the portion of attentive andinattentive tokens. We study the best choice of this hyperpa-rameter by directly dropping a certain portion of the patchtokens. The idea is that the inattentive tokens are not critical Table S2. Quantitative evaluations on landmark matching using different backbone architectures. We report the mean pixel errorbetween the prediction and ground-truth on 1000 image pairs sampled from MAFL. The best and second best results are shown in bold andunderline respectively. We group the results by backbone architecture. The error of previous SOTA methods increase dramatically whenswitching the backbone from (ResNet-50 + Hypercolumn) to DeiTs. This demonstrates that navely changing the backbone architecturedoes not yield better performance.",
  "CLDeiT-S21.43.317.32LEADDeiT-S21.40.918.64OursDeiT-S21.40.311.69": "Table S3. Quantitative evaluations on landmark detection using different backbone architectures. We report the error as the percent-age of inter-ocular distance on four human face datasets: MAFL, AFLWM, AFLWR and 300W. For AFLWR, we report the results on boththe original (AFLWRO) and corrected (AFLWRC) datasets. We group the results by backbone architecture. We can see the performanceof CL and LEAD drops when using DeiTs on all datasets except for MAFL, which demonstrates that navely changing the backbonearchitecture cannot necessarily yield better performance.",
  ". Clustering": "As clustering is a critical step of our proposed method, weoffer some quantitative ablations in this section. There aretwo hyperparameters for clustering the layer to applyclustering and the number of clusters Kc. As shown in Ta-ble S4, we first experiment with the first hyperparameterand find applying clustering after the third layer to be thebest. Then we search for the best number of clusters andreport the results in Table S5. We find the best choice of thecluster number to be 4.",
  ". Influence of the Correspondence Types": "After attentive-inattentive separation, there are three pos-sible correspondence types between the token pairs:attentive-attentive,attentive-inattentive and inattentive-inattentive. We study the importance of each type by set-ting the respective repellence hyperparameter to zero andevaluate how much the performance drops in Table S6. Wefind that the relationship between attentive-attentive tokensis the most important as the error increases the most whenwe dont enforce any repellence. This is expected as theattentive tokens covers most of the landmark regions andto distinguish between the different facial landmarks, theattentive-attentive relationship should be given more im-portance. We also find that the relation between attentive-inattentive to be more important than inattentive-inattentive.This is expected since the former may deliver intricate cuesregarding the dependencies between the landmark and criti-cal non-landmark regions such as landmark orientation (left",
  ". Visualization of Landmark Similarity Map": "We visualize some of the landmark similarity maps in Fig-ure S8. We first obtain the dense feature map from eachcompared method, and then computes the cosine similaritybetween the landmark representation and the entire featuremap. We also group the results based on the property of theoriginal image front faces are shown in the upper rows,side faces are shown in the middle and the occluded facesare shown in the bottom. When there is occlusion or wecan only see one side of the face, it is visibly difficult forthe network to output discriminative representations for theoccluded landmarks. As shown in Figure S8, our methodgenerates sharper and more localized similarity map thanprior arts.",
  ". Failure Cases of Landmark Matching": "Here we visualize some failure cases of landmark matchingin Figure S10. We find the main reason for these failurecases is occlusion. In some cases we can only see one sideof the persons face in the image, thus the query or ground-truth landmark is occluded by other face parts. There arealso cases where the landmarks are directly occluded byhand or cloth. In these cases, the query/test pixel represen-tation at the landmark location may not effectively representthe landmark which leads to failure matching results.",
  ". Trainable Components for Each Stage": "Our proposed method involves two training stages and theevaluation protocols for two downstream tasks are differ-ent as well. To offer a better understanding of how ourframework is trained and evaluated, we detailed the train-able components for each stage and task in Table S7. Notethat only the component listed in the table is trained inthe corresponding stage, e.g., the Backbone (DeiT) is onlytrained in stage 1 and is frozen in all other stages.",
  ". Limitations and Future Work": "In this work, we presented a two-stage framework to ad-dress self-supervised face landmark estimation tasks. De-spite the significant performance gain, there are still somelimitations of the proposed method. Firstly, the use of thecover-and-stride technique to expand feature map resolu-tion and produce more fine-grained representations requiresadditional forward passes during inference. Secondly, oursecond stage refining relies on the similarity map generatedby the CLS token. The CLS token may be distracted whenthere are other salient objects in the given image. However,since our method operates on face crops, the dependency on the CLS token is mostly offloaded onto the face cropgeneration algorithm. Considering the limitations above,future work may involve exploring more efficient methodsto gain high-resolution fine-grained feature representationand more reliable algorithms to separate the landmark andnon-landmark regions. We hope our work will inspire moreresearch in this field."
}