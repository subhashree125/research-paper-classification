{
  "Abstract": "Transformers have revolutionized machine learning withtheir simple yet effective architecture. Pre-training Trans-formers on massive text datasets from the Internet has ledto unmatched generalization for natural language under-standing (NLU) tasks. However, such language models re-main fragile when tasked with algorithmic forms of reason-ing, where computations must be precise and robust. Toaddress this limitation, we propose a novel approach thatcombines the Transformers language understanding withthe robustness of graph neural network (GNN)-based neu-ral algorithmic reasoners (NARs). Such NARs proved effec-tive as generic solvers for algorithmic tasks, when specifiedin graph form. To make their embeddings accessible to aTransformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the lan-guage model to cross-attend to the node embeddings fromthe NAR. We evaluate our resulting TransNAR model onCLRS-Text, the text-based version of the CLRS-30 bench-mark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out ofdistribution.",
  ". Introduction": "Recent work motivated and showcased the effec-tiveness of graph neural networks [31, GNNs] at robustlysolving algorithmic tasks of various input sizes, both in andout of distributionsuch systems are often referred to asneural algorithmic reasoners [34, NARs]. Provided appro-priate inductive biases are used, NARs are capable of hold-ing perfect generalisation even on 6 larger inputs thanones seen in the training set, for highly complex algorith-mic tasks with long rollouts . NARs are, however, stillrelatively narrow forms of AI, as they require rigidly struc-tured formatting of inputs, and they hence cannot be directly . Our TransNAR architecture, with its direct synergy ofTransformers and Neural Algorithmic Reasoners, yields clear im-provements in out-of-distribution reasoning across wide categoriesof algorithmic tasks in CLRS-Text , a textual version of theCLRS-30 benchmark . Here, the x-axis indicates one of theeight algorithmic families of CLRS-30, and the y-axis spans theaverage execution accuracy across a dataset of out-of-distributionexamples. TransNAR enables emerging capabilities in the partic-ular out-of-distribution regime depicted here, with over 20% abso-lute improvement in several of the algorithm classes.",
  "arXiv:2406.09308v1 [cs.CL] 13 Jun 2024": ". Augmenting LLMs with algorithmic reasoning: a birds eye view of TransNAR. A large language model (LLM) consumesinput tokens and produces output tokens, as common for a unimodal Transformer. The neural algorithmic reasoner (NAR) module is agraph neural network (GNN) pre-trained to execute various algorithmic computation on a collection of graph-based inputs the pre-training pipeline is denoted by faded arrows. Throughout its forward pass, the Transformer may access the embeddings computed by theNAR, by leveraging cross-attention (trained by learnable glue weights).",
  "ContributionsOur exploration proved fruitful. The keytakeaways we present in this work are as follows:": "1. We propose a hybrid architecture combining the lan-guage understanding of a Transformer with the robust-ness of reasoning of a pre-trained GNN-based NAR. TheTransformer uses the NAR as a high-dimensional toolthat will modulate its tokens embeddings. 2. We show, through an evaluation on CLRS-Text , thetext-based version of the CLRS-30 benchmark, that sucha NAR-augmented large language model (LLM) exhibitsimproved and more robust reasoning capabilities out-of-distribution ().",
  ". Related work": "Our work sits at the intersection of several areas: neu-ral algorithmic reasoning, length generalisation in languagemodels, tool use, and multimodality. Here, we briefly sur-vey various relevant works in each area. Due to the diversityof perspectives, to preserve brevity, we do not offer a com-prehensive review of related work, but rather aim to providean indication of specific works that inspired ours the most. Neural algorithmic reasoningNAR is, in general terms,the art of building neural networks that are capable of cap-turing algorithmic computation. Such capabilities can beamplified by careful choices in algorithmic alignment ,step-wise training or contrastive objectives .Recently, it was demonstrated that:(1) it is possi-ble to learn an NAR capable of executing multiple algo-rithms simultaneously in its latent space with theTriplet-GMPNN skillfully doing so for a collectionof thirty algorithms across the CLRS benchmark ; (2)Once trained, such NARs can be usefully deployed in var-ious downstream tasks:reinforcement learning ,self-supervised learning , combinatorial optimisation, computational biology and neuroscience .Our works use of NAR is mostly motivated by two ofthe works listed before: we use a relatively small, pre-trained, multi-task NAR , and deploy it in a far morescaled environment: as shown by Numeroso et al. ,NAR should in principle be scalable to systems that areorders-of-magnitude greater than the NARs training distri-bution (180, 000 in that particular case). Length generalisation in LLMsWhile NARs can oftenstrongly generalise to far greater test inputs , LLMshave seen significantly less success in such scenarios. Weattribute this to their autoregressive, causally-masked objec-",
  "t(t+1)5": ". TransNAR hybrid architecture. Similar to Alayrac et al. , we interleave existing Transformer layers with gated cross-attention layers which enable information to flow from the NAR to the Transformer. We generate queries from tokens while we obtain keysand values from nodes and edges of the graph. The node and edge embeddings are obtained by running the NAR on the graph version ofthe reasoning task to be solved. When experimenting with pre-trained Transformers, we initially close the cross-attention gate, in order tofully preserve the language models internal knowledge at the beginning of training. tive, which may not always correspond to the most logicalorder in which outputs of algorithms should be predicted.Just as a simple example, performance of various LLMs onmultiplication can be significantly improved by predictingthe result in reverse order . Of course, on more compli-cated algorithms, it may be much harder to determine thebest way to permute the input, and it may not be the mosthuman-readable.Knowledge of the above issues has led to a significantamount of effort being invested in building Transformersthat can generalise in length. While length generalisationis not the only kind of distribution shift of interest to OODreasoning, it is among the most easy such shifts to simulate.Accordingly, various works have attempted to induce lengthgeneralisation in LLMs, through the use of careful prompt-ing , randomised positional encoding , curricula or scratchpads . We firmly believe that an impor-tant trait of reasoning is robustness with respect to promptqualityso long as the prompt unambiguously specifies theproblemand hence deliberately do not explore promptmodification approaches here; only randomised positions are leveraged out of the works above in our model. Tool use and multimodalityAnother way to obtain ro-bust generalisation performance is to leverage a hard-codedalgorithm (also known as a tool) by teaching an LLM to in-voke its API . Arguably, most of the major successesof reasoning with LLMs can primarily be at-tributed to an LLMs clever usage of a tool rather than theLLM itself, as a tool will by definition not have issues ingeneralising to diverse inputs. Since our aim is to directly evaluate reasoning capabil-ities of LLMs, we explicitly do not permit tool use in ourbaselines. That being said, we envision the pre-trained NARas a modulator for the Transformers embeddings which ismore robust to OOD noise. Hence, we may observe theNAR as an internal tool: rather than using raw tokens,the Transformer and NAR can communicate using their em-beddings, breaking the associated algorithmic bottlenecks.How to actually realise this communication and embed-ding exchange? For this, we turn to multimodal LLMs for inspiration, since we need to integrate signals comingfrom two different representations of algorithmic problems(text and graph). Specifically, our exchange operator is di-rectly inspired by vision language models (VLMs) and thecross-attention operator used in Flamingo , which of-fered a principled way of fusing information from text andimage modalities.",
  ". TransNAR: Augmenting Transformers witha pre-trained GNN-based NAR": "This section describes our hybrid TransNAR architecture(refer to ). TransNAR accepts a dual input con-sisting of a textual algorithmic problem specification (of Ttokens) and its corresponding CLRS-30-specific graph rep-resentation (of N nodes) and outputs a textual response tothe problem. We can assume that, once encoded, the textualinput is stored in T RT k, and the graph input is storedin G RNl. Note that, for simplifying the equations tofollow, we make an assumption that all of the informationrelevant to the graph version of the problem is stored in the nodeswhich is often not true in CLRS-30 (there may beedge- and graph-level inputs as well) but it doesnt changethe underlying dataflow presented below.The forward pass of TransNAR unfolds as follows. First,we properly initialise the inputs by setting T(0) = T andG(0) = G. Next, to compute the representation of a step(t+1), the text (token) representations are fed to the currentlayer of the Transformer :",
  "g(t+1)u= g(t)u , max1vN g(t)u , g(t)v(2)": "where , : Rk Rk Rk are learnable message andupdate functions, respectively, and max is the elementwise-max aggregation. Note that Equation 2 only provides pair-wise interactions between nodes for brevityin reality, ourNAR is a Triplet-GMPNN , which also contains tripletinteractions and a gating mechanism.Further, note thatthere is no timestep index on the learnable parts of theNARat each step, a shared function is applied.Thisaligns well with the iterative, repeated nature of algorithmiccomputation on graphs.Once both streams have prepared their representations,(t+1) and G(t+1), the node embeddings in the graph con-dition the Transformers token embeddings to produce thefinal outcome of the TransNAR block in the Transformerstream, inspired by Flamingo :",
  "G(t)Vt": "(3)where Qt , Kt Rkdk, Vt Rkk are the key, queryand value transformations of the cross-attention, respec-tively.No additional transformations are performed onG(t+1) before concluding this layer.This process repeats until the final, Nl-th layer, when thefinal text output is read out from T(Nl). The final output isconverted into token logits by a prediction head produced bythe final layer, which we supervise by means of a standardnext-token prediction objective.Prior to the start of TransNAR fine-tuning, we pre-trainthe NAR to robustly execute the thirty algorithms spannedby CLRS-30 , in a manner similar to Ibarz et al. .Such procedures are known to yield out-of-distribution gen-eralisation at up-to-4 larger inputs in graph space. The pa- rameters of the NAR are generally kept frozen during fine-tuning, as additional gradients would eliminate the modelsoriginal robustness properties. This is also, similarly, thereason why no cross-attention is performed by the graphembeddings. The LLM itself may be pre-trained over large-scale datasets , to establish its general language priors,though we recover the same experimental findings even ifthe LM is randomly initialised at the onset.",
  ". Experiments": "In our experimentation, we will demonstrate that the recipeoffered by TransNAR admits significant benefits to out-of-distribution reasoning in language model architectures. Inthis section we provide details of our experimental setup.Transformer architecture and initialisation. We use adecoder-only, 6 layers, transformer model from the Chin-chilla family pretrained on MassiveText . In par-ticular we use a model of 70 million parameters with a con-text size 2, 048. To showcase the suitability of our approachregardless of the starting point of training, we run two ab-lative variants. In the first, the Transformer weights are ini-tialised with the outcome of the pre-trainingemulating afine-tuning scenarioand in the second, we use a fully ran-dom initialisation. In our figures and tables of results thatfollow, we will refer to these two setups as Pretrainedand Untrained.Randomized positional encoding. Previous work hasemphasised the significant relevance of randomised posi-tional embeddings in Transformers, especially for enablingmore robust reasoning .Corresponding to previousstudies on the generalization capabilities of language mod-els, randomised positional embeddings have indeed led tosignificant gains on both our baselines and TransNAR, al-lowing more interesting reasoning behaviour to emerge inboth. As such, all our experiments in this paper will userandomised positional embeddings. We provide more de-tails in Appendix 6.Pre-training the NAR. Following Ibarz et al. , wepre-train a multi-task MPNN-based NAR on input problemsizes of up to 16, from the CLRS-30 benchmark . Ow-ing to its graph structure formulation, such NARs are capa-ble of significant OOD generalisationsometimes stayingcompetitive on graphs that are 4 the size. We will attemptto utilise such models through TransNAR, to convey thisrich representational knowledge into text.Combining cross-attention contributions from nodesand edges. The NAR pre-trained by the method presentedin Ibarz et al. produces both node and edge latent repre-sentations, and we cross-attend to both of them, as they maycontain complementary useful information. To cross-attendover the edge features, E(t) RNNk, we apply Equation3 one more time (with (t) cross-attending over E(t)), withthe caveat that we need to flatten the first and second axis",
  "selected:": ". Samples from CLRS-Text for various algorithmic tasks of problem size 4. Input and target parts of the examples are clearlyspecified. Note that variability is limited at size 4, meaning that some algorithms may have trivial answers for the given inputs. Such effectstend to quickly disappear with scaling the problem size. of E into one, to make sure the dimensionalities match. Wecombine the cross-attention contribution from the node andedge embeddings provided by the pre-trained NAR by con-catenation, followed by the application of a linear layer. Wehave attempted to use other reduction schemes such as sum-ming the vectors, or applying a 2-layer MLP. We have alsoattempted different preprocessing schemes such as orthogo-nalising the contributions using the Gram-Schmidt processto ensure their algebraic complementarity before combin-ing them. However, none of these variations have broughtimprovements over our original approach. Datasets. We use the CLRS-Text benchmark , thetext version of the CLRS-30 benchmark . show-cases several samples from this dataset, along with their in-put size and number of tokens. Note that the textual repre-sentation is directly derived from the graph-based CLRS-30in a deterministic manner, so the two datasets convey ex-actly the same information. However, due to the tokenisedrepresentation, there are stringent limitations on how largeof a problem size we can evaluate on without running out ofcontext length for the Chinchilla models.",
  "Accordingly, we train our algorithms on smaller prob-lem sizes and 12, and evaluate on problem sizes 10(out-of-distributioninterpolation), 12 (in-distribution), 14": "(out-of-distributionextrapolation).It is worth noting that CLRS-Text is among the mostchallenging long-range reasoning tasks for language mod-els, compared to the present evaluation landscapea clearstep-up in complexity from grade school math, mainlybecause it allows for explicitly controlling for out-of-distribution generalisation.Yet, there exists a clearpolynomial-time-algorithmic description for each of them,meaning that they can be explained in relatively littleparameterscertainly way less than a typical large lan-guage model of today!The dataset comprises 10, 000 samples per algorithm perinput size, making up a total of 2, 400, 000 data points, splitas per above into 70% for training and 30% for validation. Training details.We train all models over seven epochsof the training data with a batch size of 256 and employ anAdam optimizer with a learning rate of 104. We ap-ply randomized positional encoding with a maximal lengthof 8, 192 on top of Rotary Positional Encoding (RoPE) usedin the base Chinchilla transformer . As previously men-tioned, for all TransNAR models, we keep the NAR frozenduring training.Evaluation metrics. We refrain from computing the ac- . TransNAR significantly outperforms the baseline Transformer. We compare TransNAR to its corresponding Transformerbaseline on various algorithms and for various input sizes: 12 is the largest size in-distribution. The other two sizes tested10 and 14areout-of-distribution, with the former testing interpolation and the latter extrapolation. Note that in-distribution generalisation is much easierfor Transformers, and as such, we have modified the y-axis for this setting only to the [0.7, 1.0] range. It is evident that, on most algorithmictasks of interest, the TransNAR is capable of outperforming its baseline Transformer. Additionally, we see that this advantage is consistentacross both training regimes: initial training and finetuning. The metric used is the CLRS score. Each model was trained with 4 randomseeds. Error bars indicate 1 standard deviation. curacy of each model using exact string matching, on thegrounds that this metric does not provide insights as to thecauses of failure on a particular datapoint, and more crit-ically, it fails to capture how close to correctness a givenmodel output is (as observed by Velickovic et al. ). In-stead, we evaluate the performance of each model accordingto three metrics measuring capabilities of increasing com-plexity over the generated text: 1. The shape score:a binary-valued metric capturingwhether the output has the right shape. For example,if we consider a sorting task, the output should have ex-actly the same number of elements as the input. Sim-ilarly, if the output is a matrix, we ensure its shape isconsistent with both the input and the task.",
  "whether the output is free from any illicit characters,for example, considering again a sorting task on a listof numbers, the output shouldnt contain any letters ofthe alphabet": "3. The CLRS score: The percentage of elements in the out-put that match the ground truth answer. This score isthe one traditionally used in CLRS-30 , hence itsname. Note that we automatically assign a CLRS scoreof 0 if the shape score is 0, as there is no clear correspon-dence between output indices. These multi-faceted scores are explicitly designed to cap-ture the various failure modes of LLMs when learning toreason over text inputs: they may overly specialise to thetraining problem sizes (leading to incorrect shapes at testtime), fail to cope with unseen number combinations (lead- . Shape Score: The TransNAR significantly outperforms its baseline in terms of producing correct shapes. This score sheds lighton an obvious failure model of regular Transformers out-of-distribution: they fail to capture the seemingly trivial dependency betweeninput size and output size, and so irrespective of the complexity of the algorithm itself. The TransNAR model manages to considerablyalleviate this problem (with many emerging gains), albeit, these gains do not always lead to perfect scores, implying a fruitful direciton forfuture research.",
  ". Results": "We summarize our findings in (for CLRS score).Our results show that our TransNAR significantly outper-forms the baseline Transformer overall, and on most indi-vidual algorithms, both in- and out-of-distribution. In par-ticular, we see that our approach not only enhances exist-ing out-of-distribution generalisation capabilities, but alsocauses the emergence of these capabilities when there wasa complete lack thereofreflected in the figure by zero ornear-zero performance of the baseline .The analysis of shape score () provides an ad-ditional way to shed light on why TransNAR performed aswell as it did. Recall, first, that CLRS score is necessarilyzero if shapes do not match. Observing the shape scores achieved, it appears that grounding Transformer outputs inNAR embeddings significantly increases the proportion ofinputs for which a Transformer will produce an output ofthe correct shapeindicating that this is one very specificfailure mode that TransNAR helps alleviate. We note, however, that there remain a few algorithmsfor which TransNAR is not able to outperform the base-line. A closer look at the results indicates that such tasks(Binary Search, Find Maximum Subarray, Minimum, andQuickselect) all involve an element of searching for a par-ticular index in an input list. This hints at a unified fail-ure mode: as these failures persist both when interpolatingand extrapolating, the model as implemented is not able togeneralise to novel index boundaries unseen in the trainingdata. We therefore suspect that the use of index hintsas already demonstrated by Zhou et al. is a promis-ing avenue for ameliorating this behaviour. Alternatively, it might be the case that the final NAR-computed hiddenstates are harder to decode by the cross-attention layersin a generalisable way, and therefore might require eithergiving an additional capacity to the cross-attention and/orperforming a more progressive decoding in that: insteadof having all cross-attention layers decoding from the finalNAR-computed hidden states, s, we could have early cross-attention layers decode from hidden states coming from ear-lier message passing steps, and later cross-attention layersdecode from the later message passing steps.Lastly, we provide parse scores in Appendix 7omittingthem from the main text because, in most cases, parsing canbe done at full accuracy.",
  ". Limitations": "While our approach demonstrates favourable average per-formance under all out-of-distribution regimes we haveevaluated, we highlight that TransNAR requires access toboth textual and graph-representation inputs to be effi-ciently trainable and usable. While this limits TransNARto cases where a particular ground-truth executor or simu-lator (or prior belief about one) is available, now that weknow that TransNAR-like ideas are beneficial, future re-search can enable the deployment of such ideas into purelyunimodal Transformers. For example, lifting the need fora second data stream can be done by distilling the knowl-edge acquired by the trained TransNAR model into a vanillaTransformer model.",
  ". Conclusions": "We presented a Transformer-NAR hybrid architecture: alanguage model that combines the language understandingskills of a Transformer with the robust algorithmic reason-ing capabilities of a pre-trained graph neural network-basedneural algorithmic reasoner, to solve algorithmic tasks spec-ified in natural language. We have demonstrated the superi-ority of our model over its Transformer-only counterpart onthe CLRS-text benchmark, in the in-distribution, and moreimportantly, in two out-of-distribution regimes, with respectto the input problem size. We hope that future work willdraw on our results and insights shared here, and further in-vestigate expansions of interest, notably, datasets with moreambiguous problem specifications (as often encountered inthe real world), and for which their corresponding equiva-lent solver-ready symbolic inputs are not given in advance.",
  "Gpt-4 technical report.arXiv preprint arXiv:2303.08774,2023. 1": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-toine Miech, Iain Barr, Yana Hasson, Karel Lenc, ArthurMensch, Katie Millican, Malcolm Reynolds, Roman Ring,Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.Flamingo: a visual language model for few-shot learning,2022. 3, 4 CemAnil,YuhuaiWu,AndersAndreassen,AitorLewkowycz, Vedant Misra, Vinay Ramasesh, AmbroseSlone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.Exploring length generalization in large language models.Advances in Neural Information Processing Systems, 35:3854638556, 2022. 1, 3 Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalk-wyk, Andrew M Dai, Anja Hauth, et al. Gemini: a fam-ily of highly capable multimodal models.arXiv preprintarXiv:2312.11805, 2023. 1 Beatrice Bevilacqua, Kyriacos Nikiforou, Borja Ibarz, IoanaBica, Michela Paganini, Charles Blundell, Jovana Mitrovic,and Petar Velickovic.Neural algorithmic reasoning withcausal regularisation. In International Conference on Ma-chine Learning, 2023. 1, 2 Andreea-Ioana Deac, Petar Velickovic, Ognjen Milinkovic,Pierre-Luc Bacon, Jian Tang, and Mladen Nikolic. Neural al-gorithmic reasoners are implicit planners. Advances in Neu-ral Information Processing Systems, 34:1552915542, 2021.2, 3",
  "Yu He, Petar Velickovic, Pietro Li`o, and Andreea Deac. Con-tinuous neural algorithmic planners. In Learning on GraphsConference, pages 541. PMLR, 2022. 2": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego deLas Casas, Lisa Anne Hendricks, Johannes Welbl, AidanClark, Tom Hennigan, Eric Noland, Katie Millican, Georgevan den Driessche, Bogdan Damoc, Aurelia Guy, SimonOsindero, Karen Simonyan, Erich Elsen, Jack W. Rae, OriolVinyals, and L. Sifre. Training compute-optimal large lan-guage models. ArXiv, abs/2203.15556, 2022. 4, 5 Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyria-cos Nikiforou, Mehdi Abbana Bennani, R. Csordas, An-drew Dudzik, Matko Bovsnjak, Alex Vitvitskyi, YuliaRubanova, Andreea Deac, Beatrice Bevilacqua, YaroslavGanin, Charles Blundell, and Petar Velickovic. A general-ist neural algorithmic learner. In LOG IN, 2022. 1, 2, 4,6 Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,Carl Doersch, Catalin Ionescu, David Ding, Skanda Kop-pula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al.Perceiver io: A general architecture for structured inputs &outputs. arXiv preprint arXiv:2107.14795, 2021. 3",
  "Diederik P. Kingma and Jimmy Ba. Adam: A method forstochastic optimization, 2017. 5": "Remi Leblond et al. AlphaCode 2 Technical Report. 2023. 3 Nayoung Lee, Kartik Sreenivasan, Jason D Lee, KangwookLee, and Dimitris Papailiopoulos.Teaching arithmetic tosmall transformers. arXiv preprint arXiv:2307.03381, 2023.3 Larisa Markeeva, Sean McLeish, Borja Ibarz, WilfriedBounsi, Olga Kozlova, Alex Vitvitskyi, Charles Blundell,Tom Goldstein, Avi Schwarzschild, and Petar Velickovic.The clrs-text algorithmic reasoning language benchmark,2024. 1, 2, 5",
  "Chendi Qian, Didier Chetelat, and Christopher Morris. Ex-ploring the power of graph neural networks in solving linearoptimization problems.arXiv preprint arXiv:2310.10603,2023. 2": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Milli-can, Jordan Hoffmann, Francis Song, John Aslanides, SarahHenderson, Roman Ring, Susannah Young, Eliza Ruther-ford, Tom Hennigan, Jacob Menick, Albin Cassirer, RichardPowell, George van den Driessche, Lisa Anne Hendricks,Maribeth Rauh, Po-Sen Huang, Amelia Glaese, JohannesWelbl, Sumanth Dathathri, Saffron Huang, Jonathan Ue-sato, John Mellor, Irina Higgins, Antonia Creswell, NatMcAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar,Elena Buchatskaya, David Budden, Esme Sutherland, KarenSimonyan, Michela Paganini, Laurent Sifre, Lena Martens,Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh,Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou,Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli,Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pa-jarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cy-prien de Masson dAutume, Yujia Li, Tayfun Terzi, VladimirMikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,Aurelia Guy, Chris Jones, James Bradbury, Matthew John-son, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell,Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway,Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,and Geoffrey Irving. Scaling language models: Methods,analysis & insights from training gopher, 2022. 4 Bernardino Romera-Paredes, Mohammadamin Barekatain,Alexander Novikov, Matej Balog, M Pawan Kumar, EmilienDupont, Francisco JR Ruiz, Jordan S Ellenberg, PengmingWang, Omar Fawzi, et al. Mathematical discoveries fromprogram search with large language models. Nature, pages13, 2023. 3 Anian Ruoss, Gregoire Deletang, Tim Genewein, JordiGrau-Moya, R. Csordas, Mehdi Abbana Bennani, ShaneLegg, and Joel Veness.Randomized positional encod-ings boost length generalization of transformers.ArXiv,abs/2305.16843, 2023. 3, 4 Timo Schick, Jane Dwivedi-Yu, Roberto Dess`, RobertaRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-cedda, and Thomas Scialom. Toolformer: Language mod-els can teach themselves to use tools.arXiv preprintarXiv:2302.04761, 2023. 3",
  "Petar Velickovic and Charles Blundell. Neural algorithmicreasoning. Patterns, 2, 2021. 1": "Petar Velickovic, Adri`a Puigdom`enech Badia, David Bud-den, Razvan Pascanu, Andrea Banino, Mikhail Dashevskiy,Raia Hadsell, and Charles Blundell. The clrs algorithmic rea-soning benchmark. In International Conference on MachineLearning, 2022. 1, 2, 4, 5, 6 Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, BarretZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,Denny Zhou, Donald Metzler, et al. Emergent abilities oflarge language models.arXiv preprint arXiv:2206.07682,2022. 7",
  ". Effect of Randomized Positional Encoding": "Using randomized positional encoding has benefitted bothour model and the baseline. In particular, combining themwith NAR hiddens led to improvements OOD, most preva-lently in the interpoloation regime (at length 10), but also,to some extent, in the extrapoloation regime (at length 14).One result we found interesting, was that before instat-ing randomized positional encoding, the OOD performanceof our hybrid models was limited (in fact thresholded) bythe performance of the base LLM. Concretely, if the baseLLM achieved near-zero performance, the hybrid architec-ture would fatally share the same fate. We can see that this isno longer the case: if the base LLM uses randomized posi-tional encoding, even if its performance is near-zero, that ofthe hybrid architecture can still be reasonably good. This isillustrated in the second column of the figure 4, for exampleon the Graham Scan, Jarvis March, MST Prim algorithms."
}