{
  "Technique Report of CVPR 2024 PBDL Challenges": "Ying FuYu LiShaodi YouBoxin ShiLinwei ChenYunhao ZouZichun WangYichen LiYuze HanYingkai ZhangJianan WangQinglin LiuWei YuXiaoqian LvJianing LiShengping ZhangXiangyang JiYuanpei ChenYuhan ZhangWeihang PengLiwen ZhangZhe XuDingyong GouCong LiSenyan XuYunkang ZhangSiyuan JiangXiaoqiang LuLicheng JiaoFang LiuXu LiuLingling LiWenping MaShuyuan YangHaiyang XieJian ZhaoShihua HuangPeng ChengXi ShenZheng WangShuai AnCaizhi ZhuXuelong LiTao ZhangLiang LiYu LiuChenggang YanGengchen ZhangLinyan JiangBingyi SongZhuoyu AnHaibo LeiQing LuoJie SongYuan LiuQihang LiHaoyuan ZhangLingfeng WangWei ChenAling LuoCheng LiJun CaoShu ChenZifei DouXinyu LiuJing ZhangKexin ZhangYuting YangXuejian GouQinliang WangYang LiuShizhan ZhaoYanzhao ZhangLibo YanYuwei GuoGuoxin LiQiong GaoChenyue CheLong SunXiang ChenHao LiJinshan PanChuanlong XieHongming ChenMingrui LiTianchen DengJingwei HuangYufeng LiFei WanBingxin XuJian ChengHongzhe LiuCheng XuYuxiang ZouWeiguo PanSongyin DaiSen JiaJunpei ZhangPuhua Chen",
  "Abstract": "The intersection of physics-based vision and deep learn-ing presents an exciting frontier for advancing computer vi-sion technologies. By leveraging the principles of physicsto inform and enhance deep learning models, we can de-velop more robust and accurate vision systems. Physics-based vision aims to invert the processes to recover sceneproperties such as shape, reflectance, light distribution,and medium properties from images. In recent years, deeplearning has shown promising improvements for various vi-sion tasks, and when combined with physics-based vision,these approaches can enhance the robustness and accuracyof vision systems.This technical report summarizes theoutcomes of the Physics-Based Vision Meets Deep Learn-ing (PBDL) 2024 challenge, held in CVPR 2024 workshop.The challenge consisted of eight tracks, focusing on Low-Light Enhancement and Detection as well as High DynamicRange (HDR) Imaging. This report details the objectives,methodologies, and results of each track, highlighting the Ying Fu, Yu Li, Shaodi You and Boxin Shi are the challenge or-ganizers. Ying Fu is with Beijing Institute of Technology, Yu Li is withInternational Digital Economy Academy, Shaodi You is with University ofAmsterdam, Boxin Shi is with Peking University.",
  ". Introduction": "The integration of physics-based vision with deep learningoffers a powerful paradigm for addressing complex com-puter vision problems. Physics-based vision seeks to modeland invert physical processes to recover scene propertiessuch as shape , reflectance , and light dis-tribution from images.Deep learning, on the otherhand, excels at learning representations and patterns fromlarge datasets. Combining these approaches allows for thedevelopment of models that are not only data-driven butalso grounded in physical principles, leading to enhancedperformance in various vision tasks such as object recog-nition , scene understanding , and image restora-tion .To explore the potential of this integrated approach, weorganized a comprehensive challenge at CVPR 2024, heldin conjunction with the Physics-Based Vision Meets DeepLearning (PBDL) workshop.The challenge comprisedeight tracks, divided into two main categories: Low-LightEnhancement and Detection, and High Dynamic Range(HDR) Imaging. Each track was designed to address spe-cific challenges in the field and to stimulate innovation",
  "arXiv:2406.10744v3 [cs.CV] 12 Jul 2024": "in both theoretical and practical aspects.For instance,low-light enhancement aims to improve image visibility inpoorly lit environments, which is crucial for applicationslike autonomous driving and surveillance . HDR imag-ing, on the other hand, focuses on capturing a wider range ofluminance levels to produce more realistic and detailed im-ages, which is essential for photography and cinematogra-phy . This report details the objectives, methodologies,and results of each track, highlighting the top-performingsolutions and their innovative approaches. In the following,we present an overview of the individual tracks.",
  ". Low-Light Enhancement and Detection Chal-lenge": "1. Low-light Object Detection and Instance Segmenta-tion: This track aimed to improve the robustness of ob-ject detection and instance segmentation algorithms inlow-light conditions. Participants developed methods tohandle noise, color distortion, and detail loss, commonissues in low-light environments. 2. Low-light Raw Video Denoising with Realistic Mo-tion: Focusing on enhancing video quality in low-lightconditions, this track involved denoising raw video se-quences with realistic motion. The goal was to reducenoise while preserving motion integrity. 3. Low-light SRGB Image Enhancement: This track tar-geted the enhancement of SRGB images captured inlow-light conditions. Participants worked on methodsto recover normal-light images from very dim environ-ments, addressing noise, color bias, and over-exposureissues. 4. Extreme Low-Light Image Denoising: Participants inthis track aimed to develop algorithms capable of de-noising images captured under extremely low-light con-ditions, pushing the boundaries of what is achievable interms of noise reduction and detail preservation. 5. Low-light Raw Image Enhancement: This track fo-cused on enhancing raw images captured in low-lightscenarios.By leveraging the higher bit-depth of rawdata, participants aimed to improve the overall imagequality significantly.",
  ". High Dynamic Range Imaging Challenge": "1. HDR Reconstruction from a Single Raw Image: Thistrack aimed at reconstructing high dynamic range im-ages from single raw images. The challenge was to avoidpotential misalignments common in multi-image fusiontechniques while capturing a broad spectrum of intensitylevels. 2. Highspeed HDR Video Reconstruction from Events:Participants developed methods to reconstruct HDRvideos from event-based camera data. The goal was tocombine the high temporal resolution of event cameras",
  ". Summary of Challenge Outcomes": "The challenge attracted numerous teams from around theworld, each bringing innovative approaches to tackle thesecomplex problems. This report provides a comprehensivereview of the methodologies and results for each track,highlighting the top-performing solutions. The participat-ing teams demonstrated significant advancements in low-light enhancement and HDR imaging, showcasing the po-tential of combining physics-based vision with deep learn-ing. The top three methods for each track are detailed, of-fering insights into the state-of-the-art techniques and theirpractical applications.Through this challenge, we have not only advanced thefield of computer vision but also demonstrated the mu-tual benefits of integrating physics-based models with deeplearning. The results of this challenge pave the way for fu-ture research and development in this exciting interdisci-plinary area. The following sections will delve into eachtrack individually, presenting the objectives, methodolo-gies, and outcomes in detail.",
  ". Low-light Object Detection and InstanceSegmentation": "Performing object detection and instance segmentation under low-light conditions poses several challenges. e.g.,images captured in low-light environments often sufferfrom poor quality, leading to loss of detail, color distortion,and prominent noise. These factors significantly hinder theperformance of downstream vision tasks, particularly objectdetection and instance segmentation.To address this challenge, the CVPR 2024 PBDL Chal-lenge on Low-light Object Detection and Instance Segmen-tation aims to assess and enhance the robustness of objectdetection and instance segmentation algorithms on imagescaptured in low-light environmental conditions.In the low-light object detection track (), the topthree teams demonstrated exceptional performance. BothGroundTruth and Xocean secured the 1st rank, achievingan average precision (AP) score of 0.76. They displayed re-markable accuracy in detecting objects under low-light con-ditions, with AP scores of 0.89 and 0.81 at IoU thresholdsof 0.50 and 0.75, respectively. UnoWhoiam secured the 3rdrank with an AP score of 0.75, showcasing their strong per-formance in this challenging task.For low-light instance segmentation (), the com-petition was equally intense. GroundTruth achieved the 1st",
  "GroundTruth0.620.820.652UnoWhoiam0.590.870.613Xocean0.580.790.61": "rank with a mask AP score of 0.62, demonstrating their ex-cellent ability to segment instances accurately in low-lightimages. UnoWhoiam secured the 2nd rank with an maskAP score of 0.59, while Xocean secured the 3rd rank withan mask AP score of 0.58. Both teams exhibited impres-sive performance in low-light instance segmentation, fur-ther emphasizing the significance of their contributions.These results highlight the remarkable advancementsmade by the participating teams in addressing the chal-lenges of low-light object detection and instance segmenta-tion. The top-ranking teams have showcased their expertiseand innovation in developing robust algorithms that excelin low-light conditions, paving the way for future advance-ments in computer vision research.",
  ". Low-light Instance Segmentation Dataset": "To systematically investigate the effectiveness of the pro-posed method in real-world conditions, a real low-lightimage dataset for instance segmentation is necessary andfoundamental.The challenge utilizes the Low-light In-stance Segmentation (LIS) dataset, introduced by .It is collected using a Canon EOS 5D Mark IV cam-era. shows examples of annotated images fromLIS dataset. The LIS dataset exhibits the following charac-teristics: Paired samples.The LIS dataset includes images inboth sRGB-JPEG (typical camera output) and RAW for-mats. Each format consists of paired short-exposure low-light and corresponding long-exposure normal-light im-ages. We term these four types of images sRGB-dark,sRGB-normal, RAW-dark, and RAW-normal. To ensurepixel-wise alignment, we mounted the camera on a sturdytripod and used remote control via a mobile app to avoidvibrations. Diverse scenes. The LIS dataset consists of 2230 im-age pairs collected in various indoor and outdoor scenes.To increase the diversity of low-light conditions, we useda series of ISO levels (e.g., 800, 1600, 3200, 6400) to capture long-exposure reference images and deliberatelydecreased the exposure time by various low-light factors(e.g., 10, 20, 30, 40, 50, 100) to capture short-exposureimages, simulating very low-light conditions. Instance-level pixel-wise labels. For each image pair, weprovide precise instance-level pixel-wise labels annotatedby professional annotators. This results in 10,504 labeledinstances across eight common object classes: bicycle,car, motorcycle, bus, bottle, chair, dining table, and TV.The LIS dataset includes images captured in differentscenes (indoor and outdoor) and under varying illumina-tion conditions. As shown in , object occlusion anddensely distributed objects add to the challenges presentedby the low-light conditions.",
  "Network Architecture": "Our training process for the entire task is shown in .For the original training dataset, it is first converted fromCR2 to RGGB four-channel PNG. Then the data is inputinto the CGNet model and supervised learning is car-ried out by the GroundTruth of the RGB three-channel. Fi-nally, the overexposed image can be corrected to make theoverexposed image return to normal.Our test process for the entire task is shown in .For the original validation and test data set, it is first con-verted from mat format to RGGB four-channel PNG. Thenthe data is input into the CGNet model, the trained weightsare loaded for model inference, and the inference results ofRGB three-channel are obtained.Most existing methods of overexposure in image correc-tion have been developed based on sRGB images, whichcan lead to complex and non-linear degradation due to the",
  ". Framework of FocalNet": "tains 892 labeled images as the training set and 669 imagesas the testing set. The LIS dataset comprises paired imagescollected across various scenes, encompassing both indoorand outdoor environments. We utilize all labeled data fortraining and do not perform online evaluations during train-ing. After training, we directly use the last checkpoint to",
  "predict the testing data": "Training details.During training, we take the modelpre-trained on the Object365 dataset and finetuned on theCOCO dataset as the pre-trained model. Specifically, ourmodel is trained on 8 NVIDIA Tesla V100-32G with a totalbatch size of 8, numbers of queries of 900, and numbers ofproposals of 100. Since the training set is small, we train thedetector using the AdamW optimizer with an initial learn-ing rate of 0.0001 and weight decay of 0.0001, to alleviateoverfitting. We employ the standard 1 schedule to trainthe model, and random horizontal flipping with a probabil-ity of 0.5 and random resize-crop-resize are introduced asweak augmentation. Testing details. During testing, simple test-time augmenta-tion like horizontal flipping and multi-scale testing are ex-ploited, in which the scales include 1.0, 1.125, 1.25,1.375, and 1.5. The NMS is not adopted and the detec-tor directly outputs 100 box predictions end to end. Specifi-cally, the initial test image size is 1333x800, and horizontalflipping is adopted to boost model performance. After ob-taining ten predictions with different scale augmentation,we further use weighted boxed fusion (WBF) to en-semble them as our final submission, which achieves an APof 0.76 in the test phase.In addition, we attempt to introduce some advanced low-light image enhancement methods, such as CIDNet ,GlobalDiff , and Retinexformer , to enhance thechallenge data, and perform detection algorithm on the en-hanced images.Unfortunately, the performance has notbeen improved or even decreased. We argue that since thechallenge dataset does not have pairs of low-light and nor-",
  "Implement Details": "Dataset usage.We solely utilized the challenge datasetfor training. Additionally, we attempted to augment ourtraining data by incorporating the COCO dataset which wasunprocessed according to , preserving annotations withcommon classes. However, this augmentation did not yieldimproved results. It is necessary to point out that we still uti-lized the pretrained weights on the unprocessed COCOdataset to initialize some of the models, aiming to enhancethe diversity of our model zoo, which proves advantageousfor ensemble methods.During the initial phase of the challenge, only annota-tions for the training set were available. Initially, we ran-domly divided the training set into a proxy training set anda validation set using an 8:2 ratio. Subsequently, we trainedthe models and optimized the training settings to enhanceperformance. These settings were then uniformly appliedfor training on the original complete training dataset, ensur-ing full utilization of the available data.Training. To achieve higher performance, we initializedthe model weights using pretrained weights from the COCO Dataset. However, Co-DETR was an exception, as we found that the pretrained weights obtained by trainingfirst on Object365 and then on COCO performedbetter than those from COCO .During the validation and test phases, we retained theweights from the last epoch for evaluation on the officialvalidation and test sets.We utilized the MMDetection framework to con-duct all experiments on 4 machines, each equipped with 8NVIDIA RTX 3090/4090 GPUs.Due to the extensive nature of our training process,which involved training over 18 models for ensemble, pro-viding detailed training configurations in this paper may notbe feasible. We recommend referring to the config files inour code repository for more comprehensive information. Ensemble. briefly describes the type of modeland any specific strategies employed. For example, Dino-Swin-L signifies the use of the Dino model with the Swin-L Backbone, while Dino-Swin-L with TTA indicates thesame model enhanced by test-time augmentation (TTA).Additionally, the descriptions encompass different versionsof the RTMDet and Co-DETR models, which may incorpo-rate varying parameters like dropout rates or random seedsduring the training phase. obj2coco indicates that we usepretrained weights obtained by training first on Object365 and then on COCO to initialize the parameters ofthe model.These predictions were then utilized in the weighted boxfusion to ensemble predictions. The weight of each predic-tion was determined using a grid search algorithm on theproxy validation set described in 2.3.2.For more details about the ensemble process, please referto the configuration files in our code project.As shown in , our ensemble method for the ObjectDetection track attained a mean Average Precision (mAP)of 0.76. Additionally, our RTMDet model for the InstanceSegmentation track achieved an mAP of 0.58.",
  "Instance segmentation. We utilized Mask DINO as": ". Ensemble Strategy. Dino-Swin-L signifies the use ofthe Dino model with the Swin-L Backbone, while Dino-Swin-Lwith TTA indicates the same model enhanced by test-time aug-mentation (TTA). Additionally, the descriptions encompass differ-ent versions of the RTMDet and Co-DETR models, which mayincorporate varying parameters like dropout rates or random seedsduring the training phase. obj2coco indicates that we use pre-trained weights obtained by training first on Object365 andthen on COCO to initialize the parameters of the model.",
  "The encoder output features contain dense features, which": "can serve as better priors for the decoder. Therefore, weadopt three prediction heads (classication, detection, andsegmentation) in the encoder output. Note that the threeheads are identical to the decoder heads. The classicationscore of each token is considered as the condence to selecttop-ranked features and feed them to the decoder as con-tent queries. The selected features also regress boxes anddot-product with the high-resolution feature map to predictmasks. The predicted boxes and masks will be supervised bythe ground truth and are considered as initial anchors for thedecoder. Note that we initialize both the content and anchorbox queries in Mask DINO whereas DINO only initializes the box coordinates in an image. Therefore, in the initialstage after unied query selection, mask prediction is muchmore accurate than box (the qualitative AP comparison be-tween mask prediction and box prediction in different stagesis also shown in and 9). Therefore, after uniedquery selection, we derive boxes from the predicted masksas better anchor box initialization for the decoder. By this ef-fective task cooperation, the enhanced box initialization canbring in a large improvement to the detection performance.",
  ". Segmentation Micro Design": "Unied denoising for mask: Query denoising in objectdetection has shown effective to accelerate conver-gence and improve performance. It adds noises to ground-truth boxes and labels and feed them to the Transformerdecoder as noised positional queries and content queries.The model is trained to reconstruct ground truth objectsgiven their noised versions. We also extend this technique tosegmentation tasks. As masks can be viewed as a more ne-grained representation of boxes, box and mask are naturallyconnected. Therefore, we can treat boxes as a noised versionof masks, and train the model to predict masks given boxesas a denoising task. The given boxes for mask prediction",
  ". Framework of Disturbance Suppression Learning": "veloped for detection, and adapts it to handle segmenta-tion tasks with minimal modifications to key components.Mask DINO stands out due to its superior performance, out-performing previous specialized models and achieving thebest results in instance, panoptic, and semantic segmenta-tion tasks among models with fewer than one billion pa-rameters. One of the critical advantages of Mask DINO is its abil-ity to enable task cooperation, demonstrating that detectionand segmentation can mutually enhance each other withinquery-based models. Additionally, Mask DINO leveragesbetter visual representations pre-trained on large-scale de-tection datasets to improve semantic and panoptic segmen-tation. This synergistic approach not only enhances the per-formance but also provides a robust and versatile frameworkcapable of handling multiple vision tasks effectively. Byemploying Mask DINO, we aim to leverage these strengthsto achieve superior results in the Low-light Object Detec-tion and Instance Segmentation competition. Feature alignment.We integrated the Feature-alignedPyramid Network (FaPN) to enhance our network forboth object detection and instance segmentation. FaPN isa simple yet effective top-down pyramidal architecture de-signed to generate multi-scale features for dense image pre-diction. FaPN comprises two key modules: a feature align-ment module and a feature selection module. The featurealignment module learns transformation offsets of pixels tocontextually align upsampled higher-level features, whilethe feature selection module emphasizes lower-level fea-tures rich in spatial details.Empirical results show thatFaPN consistently and substantially improves performanceover the original FPN across four dense prediction tasks andthree datasets. We chose FaPN for our competition due to its demon-strated ability to improve multi-scale feature generation.Its integration into our network aims to leverage thesestrengths, thereby enhancing our models accuracy in thecompetition.",
  "Training and Testing Details": "Training details.During training, we use a model pre-trained on the Object365 dataset and fine-tuned on theCOCO dataset as our base. Our training setup includes 8RTX 3090 GPUs, with a total batch size of 8. All other set-tings are kept the same as in the original paper. We followthe standard 1 training schedule and apply weak data aug-mentation techniques, including random horizontal flippingwith a probability of 0.5 and random resize-crop-resize.Disturbance suppression learning. When fine-tuned onCOCO, we utilize the low-light RAW synthetic pipelinefrom , which consists of two steps, namely, unpro-cessing and noise injection, to obtain synthetic low-lightclean/noisy RAW images. We adopt disturbance suppres-sion learning from previous work . Ideally, a robustnetwork should extract similar features whether the inputimage is corrupted by noise or not. To achieve this, weintroduce disturbance suppression learning, which encour-ages the network to learn disturbance-invariant features dur-ing training. This approach is independent of architecturalconsiderations.The total loss for learning is defined as:",
  "L() = LIS(x; ) + LIS(x; ) + LDS(x, x; ),(1)": "where x is the clean synthetic RAW image, x is its noisyversion, and and are the weights of the respective losses.We empirically set = 1 and = 0.01.The loss LIS is the task loss, e.g., instance segmenta-tion loss, which consists of classification loss, bounding boxregression loss, and segmentation (per-pixel classification)loss. The specific formula for LIS is related to the model,we employ the same loss as the origianl model. This loss isapplied to both the clean image x and the noisy image x toensure the model performs consistently regardless of noise.The loss LDS is the feature disturbance suppression loss,defined as:",
  "i=1f (i)(x; ) f (i)(x; )22,(2)": "where f (i)(x; ) represents the i-th stage of feature mapsof the model. By minimizing the Euclidean distance be-tween the clean features f (i)(x; ) and the noisy featuresf (i)(x; ), the disturbance suppression loss encourages themodel to learn disturbance-invariant features. This reducesfeature disturbance caused by image noise and improves themodels robustness to corrupted low-light images.Unlike perceptual loss , our approach does not re-quire pretraining a teacher model, making our training pro-cess simpler and faster. With LIS(x; ) and LIS(x; ), ourmodel can learn discriminative features from both cleanand noisy images, maintaining stable accuracy regardless of noise.In contrast, the student model in perceptualloss only sees noisy images, which can degrade per-formance on clean images and limit robustness. Addition-ally, the domain gap between the feature distributions of theteacher and student models can harm the learning process.By minimizing the distance between clean and noisy fea-tures predicted by the same model, we avoid this problem. Testing details. During testing, we employ simple test-time augmentation techniques such as horizontal flippingand multi-scale testing. The multi-scale testing involves re-sizing the shorter side of the image to various sizes: 400,500, 600, 700, 800, 900, 1000, 1100, and 1200 pixels.Horizontal flipping is also used to enhance model perfor-mance. For detection, after obtaining ten predictions withdifferent scale augmentations, we use Weighted Box Fusion(WBF) to ensemble them for our final submission.",
  ". Low-light raw video denoising with realisticmotion": "Supervised deep-learning methods have shown their effec-tiveness on raw video denoising in low-light. However, ex-isting training datasets have specific drawbacks, e.g., inac-curate noise modeling in synthetic datasets, simple motioncreated by hand or fixed motion, and limited-quality groundtruth caused by the beam splitter in real captured datasets.These defects significantly decline the performance of net-work when tackling real low-light video sequences, wherenoise distribution and motion patterns are extremely com-plex.To address this challenge, the CVPR 2024 PBDLChallenge on low-light raw video denoising with realisticmotion aims to improve the recovery quality of realisticvideos with complex motion.As shown in the , in this TRACK, all the teamsachieved great denoising performance. The first place teamis ZichunWang, with PSNR and SSIM metrics of 45.47 and0.99. The second place team is wql, with PSNR and SSIMmetrics of 39.06 and 0.96. The third place team is mm-mmmm, with PSNR and SSIM metrics of 33.64 and 0.88.These results show excellent denoising capabilities for real-world videos, and also demonstrate that the participantsexcellent ability in designing algorithms for the denoisingtask, making an important contribution to the future devel-opment of video denoising.",
  ". Low-light Raw Video Denoising Dataset": "In this competition, we first collect70 high-quality 4kvideos from the internet, then play them on the DELLU2720QM monitor.We use a Sony Alpha 7R IV full-frame mirrorless camera. The size of the Bayer image is95046336. The scenes of the video clips contain indoorand outdoor, ranging from natural landscapes to extremesports. This relatively large range of scenes also has an ad-vantage compared to previous datasets. Examples of ourdata are in .",
  ". Several representative examples for low/normal-light images in the LLRVD dataset": "Finally, the extracted multi-frame features are temporallyfused to handle the misalignment.3D Swin Transformer Block. Since vanilla self-attention is computationally consuming, directly adopting it tovideo denoising is not affordable due to the extra tempo-ral dimension. Besides, Transformer has strong long-range modeling ability but neglects local features, whichis vital for recovering details. To extract locality with lesscomputational effort, we apply 3D shifted window-basedmulti-head self-attention (3DSW-MSA) and 3D window-based multi-head self-attention (3DW-MSA) , togetherwith depth-wise convolution in the feed-forward layer. Inthis way, we can effectively extract the local features byconvolution, at the same time fully taking advantage ofintrinsic temporal-spatial self-similarity by the long-rangemodeling ability of the Transformer.Two consecutive 3D shifted window-based Transformerblocks are computed as:",
  "Attention (Q, K, V ) = SoftMaxQKT /": "d + BV,(4)where Q, K, V RT M 2d are the query, key and valuematrices.d is the dimension of the query and key fea-tures. TM 2 is the number of tokens per window. And,the values of B are taken from the 3D bias matrix B R(2T 1)(2M1)(2M1), corresponding to the temporalrange of [T + 1, T 1] and the spatial range of [M +1, M 1].Temporal Fusion.After the exploitation of spatial-temporal self-similarity, features in neighbor frames arefused for the recovery of the reference frame. However, itis not appropriate to simply combine these frames, since thecomplex motion in real videos makes each neighbor framecontribute variously to the central reference frame. Intu-itively, the closer between the features in the neighbor frameand reference frame, the more information a neighbor framecan provide for recovery. Therefore, we first extract the fea-tures by embedding, then compute the similarity betweenthe features of each neighbor and the reference features inan embedded space:",
  "S (Ft+i, Ft) = Sim (Ft+i)T , (Ft),(5)": "where and are embedding functions. Sim denotes thesimilarity calculation function. Here we also adopt the dotproduct following previous work for similarity calcu-lation. Ft refers to the reference frame and Fi+t refers to the . Overview of network architecture. Swin Transformer denotes Shifted Window-based Transformer. 3D (S)W-MSA denotes3D (Shifted)Window-based Multi-head Self-attention. LN denotes Layer Normalization. Convolutional Attention denotes our final fusionblock.",
  ". Wql Teams Method": "Our team, with the username wql on Codalab, achieved afinal score of 38.82 on the leaderboard, ranking second.In this report, we will present all the technical details forsolving this task. The task of this competition is to de-noise and restore low-light raw videos.Considering thelow-light characteristics of the data, we divide the task intotwo subtasks: low-light restoration and denoising. The keyto video restoration lies in fully utilizing inter-frame infor-mation. After extensive experiments, we determined to usethe Shift-Net model for low-light restoration. To avoid com-promising the performance of the model, we converted the original data to the RGB format for training. Given thatthe restored videos still contain a large amount of noise, weapplied the RVRT model for denoising again, resulting inhigh-quality output. Experimental results demonstrate thatour strategy is effective, achieving an outstanding score of38.82 on the LLRVD dataset. Our entire task solution is illustrated in . For theoriginal low-light video, it is first converted into an RGBformat video.Then, it undergoes restoration to normallighting conditions through the Shift-Net model. Atthis point, the videos lighting level is normal, but there stillexists a significant amount of noise. Subsequently, it un-dergoes denoising through RVRT , resulting in the finalvideo restoration and denoising outcome. The key to this track relies on the utilization of inter-frame information.Existing deep learning methods of-ten depend on complex network architectures such as op-tical flow estimation, deformable convolutions, and cross-frame self-attention layers, leading to high computationalcosts. After extensive literature review, our team ultimatelychose Shift-Net as the main model. This model proposes asimple yet effective video restoration and denoising frame-work, surpassing existing state-of-the-art methods not onlyin accuracy but also with its parameter count of only twoversions, 4.1M and 12.3M, much smaller than existingadvanced models.The model is based on grouped spa-",
  ". The overall experimental architecture diagram": "tiotemporal displacements, a lightweight and direct tech-nique that implicitly captures inter-frame correspondencesthrough multi-frame aggregation. By introducing groupedspatial displacements, a broad effective receptive field isobtained, and combined with basic 2D convolutions, thissimple framework can effectively aggregate inter-frame in-formation. Despite the restoration of low-light images us-ing Shift-Net, the images still contain a significant amountof noise. Therefore, our team chose the RVRT model todenoise the restored images, aiming for high-quality videorestoration.",
  "Feature extraction. Each frame Ii typically suffers fromdifferent types of degradation (such as noise or blur),": ". Overview of the Group Shift-Net. It adopts a three-stage design: feature extraction, multi-frame fusion, and finalrestoration. Grouped spatial-temporal shift blocks are proposedto achieve multi-frame aggregation. which affects temporal correspondence modeling. A two-dimensional U-Net-like structure is adopted to mitigate thenegative impact of degradation and extract frame-level fea-tures.Multi-frame feature fusion. At this stage, a grouped spa-tiotemporal displacement block is proposed to move differ-ent features from adjacent frames to the reference frame,implicitly establishing temporal correspondence. Keyframefeatures are fully aggregated with features from neighbor-ing frames to obtain corresponding aggregate features. Byemploying spatiotemporal displacements in different di-rections and distances, multiple candidate displacementsare provided for frame matching.By stacking multiplegrouped spatiotemporal displacement blocks, our frame-work achieves long-term aggregation.Final restoration. Finally, similar to the U-Net structure,taking low-quality input frames and corresponding aggre-gate features as input, the model generates the final resultfor each frame.In multi-frame fusion, frame features are aggregated .The operations of Grouped Spatial-temporal Shift(GSTS). We stack the forward temporal shift (FTS) blocks (Left)and backward temporal shift (BTS) blocks (Right) alternatively toachieve bi-directional propagation. Grouped spatial shift providesmultiple candidate displacements within large spatial fields and es-tablish temporal correspondences implicitly. with adjacent features to obtain temporally fused features.We adopt a two-dimensional U-Net structure for multi-frame fusion, maintaining skip connections within the U-Net.Instead of multiple 2D convolutional blocks, wereplace them with stacked Grouped Spatiotemporal Shift(GSTS) blocks to effectively establish temporal correspon-dence and perform multi-frame fusion. GSTS blocks arenot applied at the finest scale to save computational costs.The GSTS block consists of three parts: 1) temporal dis-placement, 2) spatial shift, 3) lightweight fusion layer, asillustrated in . RVRT. RVRT demonstrates excellent performance in thefield of video denoising, as shown in . The frame-work consists of three parts: shallow feature extraction, re-current feature refinement, and frame reconstruction. Shal-low feature extraction utilizes convolutional layers and mul-tiple RSTB blocks from SwinIR to extract features fromlow-quality videos (LQ). Subsequently, the recurrent fea-ture refinement module performs temporal modeling, andguided deformable attention is employed for video align-ment. Finally, multiple RSTB blocks are fed to generatethe final features, followed by HQ reconstruction using pix-elShuffle.",
  "Implementation Detail": "Shift-Net. We opted for the standard version of the Shift-Net model for training. Since each video in the trainingset randomly contains three levels of noise, the data read-ing strategy during training also involves randomly select-ing one noise level. The training parameters include a batchsize of 4, a learning rate of 4e-4, and 120,000 iterations.The training was conducted using a single NVIDIA RTX3090 GPU, lasting for 48 hours, without loading pretrainedweights. RVRT. For denoising training with RVRT, only the labelsare retrieved during data loading, with a certain amount ofnoise added. The training parameters include a batch sizeof 4, a learning rate of 1e-5, and 40,000 iterations. Train-ing was conducted using a single NVIDIA RTX 3090 GPU,lasting for 7 hours, without loading pretrained weights.",
  "Super-Resolution, and Image Enhancement. Among them,I employed the Dual-Pixel Defocus Deblurring module formy task": "Dual-Pixel Defocus Deblurring. Images captured with awide aperture have a shallow depth of field, meaning that re-gions outside the depth of field become out of focus. Givenan image with defocus blur, the goal of defocus deblurringis to generate a globally sharp image. Existing defocus de-blurring methods either directly deblur images or first es-timate the defocus disparity map and then use it to guidethe deblurring process. Modern cameras are equipped withdual-pixel sensors, where each pixel location has two pho-todiodes, thereby generating two sub-aperture views. Thephase difference between these views is useful for measur-ing the amount of defocus blur at each scene point. Re-cently, Abuolaim et al.introduced a dual-pixel deblur-ring dataset (DPDD) and a new method based on encoder-decoder design. In this paper, our focus is also on directlyusing dual-pixel data to deblur images. Previous defocusdeblurring works have employed encoder-decoder architec-tures that repeatedly use downsampling operations, result-ing in significant loss of important details. In contrast, thearchitectural design of our method enables the preservationof texture details required for the restored image. Visualization. According to the ISP process provided onthe official competition website, we processed the TIFF im-ages in RGGB order, performed grayscale balancing correc-tion, and added an additional step of normalization beforeoutputting PNG images to make the RGB images appearclearer and brighter. The specific process is illustrated inthe following .",
  "Implementation Details": "The training dataset consists of 300 ground truth (gt) andcorresponding overexposed images (ratios = 3, 5, 8, 10).The resolutions of RAW images and corresponding sRGBimages are both 6744 x 4502.The validation set processes RAW images into four-channel (RGGB) images, crops them, and saves them as.mat files. Unlike the training set, the validation set onlyincludes input files and does not have ground truth.For the test data, we converted the .mat file into a .pngfile and adjusted the channel order of the image. We processthe exposure images using a pre-trained model that is pre-",
  "Training setup. Total iteration number is set to 300,000,and cosine annealing restarts learning rate scheduler is used.Adam optimizer is employed with a learning rate of 2e-4": "Validation setup.: Validation is performed every 2000 iter-ations, and the PSNR validation metric is calculated. Vali-dation images are not saved.With these training strategies, I trained on the train-ing set for 300,000 iterations and achieved good perfor-mance.showing in :From the perspective of input image types, directly train-ing with raw images yields lower scores, as shown in.This could be due to the resulting test imageshaving blurry details, unclear textures, and a greenish hue.However, when input images are RGB images, the modelMIRNetv2 is used for deblurring tasks. When our input im-ages are not normalized, the test results tend to be darkerwith heavier colors. Then, after normalization, the colorchanges in the resulting test images are smaller, and the de-tails and textures become clearer and more visible.",
  ". Unnormalized MIRNetv2": "During the early stages of training, we also utilized theRViDeNet model, undergoing training in three phases: pre-denoising, pretraining, and finetuning. However, as trainingrequired four different noise levels while each scene in ourtraining set only had three noise levels, our strategy was todirectly duplicate the highest noise level from the training",
  ". Low-light SRGB Image Enhancement": "Compared with normal-light images, quality degradationof low-light images captured under terrible lighting condi-tions is serious due to inevitable environmental or technicalconstraints, leading to unpleasant visual perception includ-ing details degradation, color distortion, and severe noise.These phenomena have a significant impact on the perfor-mance of advanced downstream visual tasks, such as im-age classification, object detection, semantic segmentation, etc. To mitigate the degradation ofimage quality, low-light image enhancement has be-come an important topic in the low-level image processingcommunity to effectively improve visual quality and restoreimage details.To address this challenge, the CVPR 2024 PBDL Low-light sRGB Image Enhancement Challenge aims to evaluateand improve the visual quality of image enhancement algo-rithms in the field of low-light image enhancement.In the low-light sRGB image enhancement track (Table 7), the top three teams demonstrated exceptional perfor-mance. The IMAGCX team secured the first place, achiev-ing a PSNR score of 22.70 and an SSIM score of 0.82. Thechm team came in second, with a PSNR score of 22.62 andan SSIM score of 0.82.The WanFly team achieved thethird place with a PSNR score of 21.82 and an SSIM scoreof 0.81. Their enhanced images achieved excellent visualquality, showcasing their strong performance in this chal-lenging task.These results highlight the significant progress made bythe participating teams in addressing the challenges of low-light sRGB image enhancement.The top-ranking teamsdemonstrated their expertise and innovative capabilities indeveloping image enhancement algorithms that excel inlow-light conditions, paving the way for future advance-ments in computer vision research.",
  ". WanFly Teams Method": "Diffusion models are increasingly applied in low-light im-age enhancement tasks due to their exceptional capability tomodel data distributions, but an inherent drawback of diffu-sion models in image restoration tasks is that starting thereverse process from pure Gaussian noise can lead to arti-facts . Therefore, as illustrated in , weadopt the Mean-Reverting Stochastic Differential Equation(SDE) as the base diffusion framework, directly im-plementing the mapping from low-quality to high qualityimages.The fundamental idea of diffusion models is to gradu-ally corrupt images by injecting noise, and then learn howto progressively remove this noise to reconstruct the orig-inal image. U-Net plays a crucial role in this denoisingprocess. It is trained to predict the noise injected at eachstep, thereby methodically eliminating the noise and restor-ing the image. The U-Net used in diffusion models typicallyconsists of residual blocks, upsampling and downsamplingoperations, and attention mechanisms. While the stackingof multiple residual blocks is beneficial for feature extrac-tion, it increases the computational load, and the extensiveconvolutional operations are not friendly to low pixel valuesin low-light images.Our motivation is to reduce multiplication operations inU-Net, protect low pixel values, and lighten the computa-tional load. The simplified U-Net designed in this paper, asillustrated in (a), is only constructed from the feature",
  ". (a) Conditional diffusion; (b) Mean-Reverting SDE diffusion": "sitions, facilitating the learning of local image structures.Since the activation function requires multiple multipli-cation operations, we use SimpleGate to replace complexnonlinear activation functions. SimpleGate can achieve theeffect of nonlinear mapping through a single multiplicationoperation, which is particularly beneficial for preserving in-formation in low pixel values, as complex functions like thecubic operations required in the GELU activation functioncan be detrimental to such information. The computation ofSimpleGate is illustrated in Equation (9):",
  "X and Y represent the division of a feature map withchannels C, height H, and width W along the channel di-mension into two parts of ( C": "2 , H, W). The essence of thismultiplication operation is a type of nonlinear mapping thatcan substitute for an activation function.After the feature matrix has been given weights throughParameter-Free Attention Mechanism (PFAM), a 11 con-volution is used to aggregate pixel-level cross-channel con-text information. The subsequent two 1 1 convolutionsserve to facilitate interaction and combination among fea-tures across different channels, creating more complex andeffective feature representations. In order to apply to thediffusion model, we have incorporated a time embeddingblock, which takes the current diffusion time step t as inputand encodes t into the feature matrix, enabling the modelto perceive noise at different time steps t. Overall, the de-sign of SimPF block, while minimizing multiplication op-erations, maintains robust feature extraction capabilities.",
  ". Extremely Low-light Image Denoising": "Light is crucial for photography. Nighttime and low-lightconditions impose significant challenges due to the limitednumber of photons and unavoidable noise. The typical re-sponse is to increase light capture by, for example, enlarg-ing the aperture, lengthening the exposure time, or using aflash. However, each approach has its drawbacks: a largeraperture results in a shallow depth of field and is not fea-sible for smartphone cameras; extended exposure times canlead to blurriness from scene changes or camera movement;and flash can cause color distortions and is effective only forobjects close to the camera.A practical solution for low-light imaging is burst pho-tography , which aligns and fuses multipleimages to increase the signal-to-noise ratio (SNR). How-ever, burst photography is prone to ghosting effects when capturing dynamic scenes involving vehicles, people,etc. An emerging alternative is using neural networks to au-tomatically learn the mapping from a low-light noisy imageto its long-exposure counterpart . This deep learningapproach typically requires a large amount of labeled train-ing data resembling real-world low-light photographs. Col-",
  "lecting extensive high-quality training samples from variousmodern camera devices is extremely labor-intensive and ex-pensive": "To bridge the domain gap between synthetic images andreal photos, some works have collected paired real datafor both evaluation and training . De-spite promising results, gathering sufficient real data withtrue labels to prevent overfitting is very costly and time-consuming. Recent works use paired or single noisyimages as training data instead of paired noisy andclean images. However, they do not significantly reduce thelabor required to capture a large volume of real-world train-ing data. Another research direction focuses on enhancing the re-alism of synthetic training data to avoid the challenges ofobtaining real data from cameras. By considering photonarrival statistics (shot noise) and sensor readout effects(read noise), works like use a signal-dependentheteroscedastic Gaussian model to characterize noisein raw sensor data.Recently, Wang et al. pro-posed a noise model that accounts for dynamic stripe noise,color channel heterogeneity, and clipping effects to simu-late high-sensitivity noise in real low-light color images.Additionally, a flow-based generative model called Noise-Flow was proposed to describe the distribution of realnoise using latent variables with a density of one. However,these methods often oversimplify the imaging pipeline ofmodern sensors, especially the noise sources introduced bycamera electronics, which have been extensively studied inthe electronic imaging community.",
  ". Capture setup and example images from our dataset": "lenge is to use deep learning methods for denoising realextremely low-light images, optimizing denoising perfor-mance and model robustness. Participants are tasked withenhancing model robustness and denoising effectiveness bymodeling noise in real imaging processes and using syn-thetic datasets for training.This challenge aims to ex-plore realistic low-light noise models and efficient denois-ing models for extremely low-light images. We aim to rig-orously assess their effectiveness and identify key trendsin network design. We welcome participants to push theboundaries of innovation and advance the technology of ex-tremely low-light image denoising.",
  "The dataset": "To systematically study the generality of the proposed noiseformation model, we collect an extreme low-light denoising(ELD) dataset that covers 10 indoor scenes and 4 cam-era devices from multiple brands (SonyA7S2, NikonD850,CanonEOS70D, CanonEOS700D). We also record bias andflat field frames for each camera to calibrate our noisemodel. The data capture setup is shown in For eachscene and each camera, a reference image at the base ISOwas firstly taken, followed by noisy images whose expo-sure time was deliberately decreased by low light factors fto simulate extreme low light conditions. Another referenceimage then was taken akin to the first one, to ensure no acci-dental error (e.g. drastic illumination change or accidentalcamera/scene motion) occurred. We choose three ISO lev-els (800, 1600, 3200) and two low light factors (100, 200)for noisy images to capture our dataset, resulting in 240(32104) raw image pairs in total. The hardest examplein our dataset resembles the image captured at a pseudoISO up to 640000 (3200200).",
  "1jly72421528843.8043.890.992yuxiaoxi43.0743.150.99": "and accuracy in testing, we use the Codalab platform( for re-sult evaluation. Due to the storage limitations of the Co-dalab platform, we have cropped the original raw images toa size of 10241024 and saved the cropped images alongwith relevant camera parameters in mat files provided toparticipants. During testing, participants are also requiredto save the denoised images in mat files for submission. TheCodalab platform will then compute the evaluation metricsbased on the ground truth. The final competition score is : Score = logk(SSIMkP SNR) = PSNR+logk(SSIM),(10)where k=1.2.As shown in Tab. 8, in the extremely low-light detec-tion track, jly724215288 achieved first place with a score of43.80, a PSNR of 43.89, and an SSIM of 0.99, demonstrat-ing their exceptional denoising capabilities for extremelylow-light images. Yuxiaoxi secured second place with ascore of 43.07, a PSNR of 43.15, and an SSIM of 0.99.Both teams performed excellently in low-light instance seg-mentation, further highlighting the significance of their con-tributions.These results highlight the remarkable progress made bythe participating teams in addressing the challenges of noisein extremely low-light images. The top-ranked teams show-cased their expertise and innovation in developing robustalgorithms adapted to low-light conditions, paving the wayfor future advancements in computer vision research.",
  "(x, y) = arg max(x,y){r}": "The authors acknowledge that due to the settings and ex-posure time of different brands of sensors, as well as thesensitivity of the expected exposure value (EV) to the finalresult, they introduce a variable , ranging from 0.1 to 10,to reduce reliance on precise exposure accuracy. The digitalgain (ISO) and exposure time in seconds are extracted fromEXIF metadata and calculated into the exposure value (EV)for ratio estimation.",
  "ISOin TIMEin": "Theauthorscarefullyperformdataaugmentationthrough random size cropping and rotation while maintain-ing Bayer pixel alignment. They avoid any scale-like re-sampling augmentation to preserve the sensor noise proper-ties.Network Architecture. The authors use a two-stage train-ing strategy for Bayer raw denoising, employing slightlydifferent networks for each stage. In the first stage, theyuse a U-Net with residual blocks as the denoising network,utilizing the L1 loss function for faster convergence. In thesecond stage, they add attention blocks after the residual",
  "Training strategy": "The authors utilize the L1 loss function as the training ob-jective, similar to most denoising methods. They employthe same data preprocessing and optimization strategy asELD during pre-training. The raw images with long expo-sure times in the SID train subset are used for noise synthe-sis. For data preprocessing, they pack the Bayer images into4 channels, then crop the long exposure data into patches ofsize 512512 with a non-overlapping step of 256. The mod-els are trained for 300 epochs using the Adam optimizerwith 1 = 0.9 and 2 = 0.999, without applying weightdecay. The initial learning rate is set to 104, halved atthe 150th epoch, and further reduced to 105 at the 220thepoch.The inference code and the pre-trained models are re-leased at here.",
  ". Low-light RAW Image Enhancement": "Performing image enhancement under low-light conditionsposes several challenges, such as degradation of details,color distortion, and severe noise, which significantly affectthe quality of images . Meanwhile, com-pared to the 8-bit cameras sRGB output, the RAW data hasnot been processed by the Image Signal Processor (ISP);thus, it can retain the linearity with the scene and more un-quantified information . Based on the advantagesof RAW data, the CVPR 2024 PBDL Challenge Low-lightRAW Image Enhancement aims to assess and enhance algo-rithms robustness on images captured in low-light environ-mental conditions to address the challenge of image qualitydegradation.In the low-light RAW image enhancement track (Ta-ble 9), the top two teams demonstrated exceptional per-formance. Miers achieved total scores of 30.11, 31.13 dBin PSNR, and 0.84 in SSIM. ISS achieved total scores of25.95, 27.09 dB in PSNR, and 0.82 in SSIM. These resultshighlight the remarkable advancements made by the partici-pating teams in addressing the challenges of low-light RAWimage enhancement.",
  ". Low-light RAW Image Dataset": "To systematically investigate the effectiveness of the pro-posed method in real-world conditions, a real low-light im-age dataset for enhancement is necessary and fundamental.We use Canon EOS 5D Mark IV to capture the data.To capture low/normal-light image pairs, the camera wasmounted on a sturdy tripod and controlled remotely via amobile APP. The camera was not touched between the cap-ture process of normal-light and low-light images to avoidvibration. For each pair, we first take the normal-light im-age and fix ISO and aperture. Then the low-light images arecaptured by changing the shutter (exposure time) to simu-late low-light conditions. We capture our dataset indoor andoutdoor to increase the richness of the scene, where includeboth natural scenarios and manual builds. The dataset ex-hibits the following characteristics: Paired samples. The dataset includes images in RAWformat, which consists of a normal-light reference imageand four low-light images at different ratios (8,16,32,64). Diverse scenes. The dataset contains 832 image pairs in208 scenes. Our dataset stands out with its high resolu-tion of 6720 4480, surpassing the common resolutions(below 1920 1080) found in other datasets. This higherresolution captures finer details, offering a more compre-hensive analysis for low-light enhancement.The dataset includes images captured in indoor and out-door scenes under varying lighting conditions as shown in.",
  ". The structure of the SABlock": "tion, the model is not easy to fit for natural state. This teamintroduced a learnable adaptive vector in SABlock to con-trol the gap between the input RAW and the target. Thisallows the model to be effectively fitted to the direction thatcontributes to the correct output.It is also worth noting that downsampling in SANet isimplemented using 4 4 convolution with stride = 2 andthe UP Block consists of a 3 3 depthwise separable con-volution and pixelshuffle.",
  "white level - black level)": "In the training process, the batch size is 4, total iterationsis set to 500,000. This team uses L1 loss as the trainingloss and MultiStepLR for learning rate decay. In addition,the model weight uses exponential moving average (EMA),and the model with the highest PSNR on the validation setis finally selected for testing.",
  ". Overview of network architecture": "RAW Denoising , which can adapt to the target camerawithout calibrating noise parameters and repeated training,requiring only a small amount of lens pairing data and fine-tuning, eliminating the complicated calibration steps, andachieved good performance.As shown in , the whole network adopts the macroarchitecture of Unet , in which the convolution blocksof the Unet network itself is replaced with the reparame-terized noise removal (RepNR) block . In the Pre-trainstage, In RepNR Block has k branches of Camera-SpecificAlignment (CSA) module , Each of these branches isfitted to a class of camera noise, In the Fine-tune phase, Byaveraging the k CSA module, Equivalent to the model in-tegration of noise from multiple classes of cameras, At thistime the RepNR block consists of two branches, Where theupper 3x3 convolution is designed to fit the out-of-modelnoise, Lower Camera-Specific Alignment (CSA) module,The main role is to adjust the distribution of the input fea-tures.",
  ". HDR Reconstruction from a Single Raw Im-age": "The dynamic range of real-world scenes frequently ex-ceeds the capture capabilities of standard consumer cam-era sensors, often resulting in loss of detail in both overlybright and dark areas. In underexposed regions, noise be-comes significant and affects the visual quality , while in overexposed regions, information isoften clipped . To address this, the computa-tional imaging community has extensively explored HighDynamic Range (HDR) imaging, which records a broaderspectrum of intensity levels and captures more scene infor-mation. Unlike conventional Low Dynamic Range (LDR)images, HDR preserves greater detail in both over- andunder-exposed areas. This enhancement not only benefitsvarious vision tasks, such as segmentation and objectdetection , but also produces more visually pleas-ing imagesa goal long pursued by computer vision re-searchers.To advance HDR reconstruction research, we are launch-ing a challenge focused on reconstructing HDR imagesfrom single Raw images. This approach specifically targetssingle Raw image HDR reconstruction, avoiding potentialmisalignments that can occur in multi-image fusion. Wewill utilize a Raw-to-HDR dataset that focus on HDR re-construction from a single Raw image, as shown in ,which contains pairs of Raw and HDR images. The Rawinput is captured under challenging lighting conditions, rep-resenting the over- and under-exposed regions of a high dy-namic range scene. The corresponding ground truth HDRimages in the dataset are produced through bracketed expo-sures of each scene, subsequently merged using basic HDRfusion algorithms .",
  ". Dataset": "The dataset for this challenge is shown in . Thisdataset provides real paired Event-to-HDR data for the pur-pose of high-speed HDR video reconstruction from eventstreams. The collection process involves an integrated sys-tem designed to simultaneously capture high-speed HDRvideos and corresponding event streams. This is achievedby utilizing an event camera to record the event streams,alongside two high-speed cameras that capture synchro- nized Low Dynamic Range (LDR) frames.These LDRframes are later fused to create High Dynamic Range(HDR) frames.The careful alignment of these cameraswithin the system ensures the accurate synchronization ofthe high-speed HDR videos with the event streams, offer-ing a robust dataset for the challenge participants. The chal-lenge dataset has the following characteristics Real high-bit HDR. Unlike existing methods that pri-marily leverage the HDR feature of event data, our datasetincludes real high-bit HDR data.This data is createdby fusing two images with different exposures using anHDR fusion strategy. This inclusion is crucial as mostcurrent methods do not use real high-bit depth HDR datafor training, limiting their ability to generate such HDRformats. Paired Event-to-HDR dataset. While existing datasetsoften contain only paired testing data created by simu-lating a virtual cameras trajectory, this dataset providesreal paired training data. This approach overcomes thedomain gap that synthetic training data typically has withreal-world testing scenarios. This dataset captures gen-uine paired training data, offering a more realistic and ap-plicable training environment. Highspeed. In alignment with the high-speed nature ofevent streams, our videos are captured with a high-speedcamera at a frame rate of 500fps. This speed significantlyexceeds that of APS or any other event-to-HDR dataset,making our dataset uniquely suited for applications re-quiring high temporal resolution.",
  ". Implementation details": "We utilized PyTorch 1.8 within an NVIDIA 3090 GPU en-vironment, equipped with 24GB of memory, to train ourmodel on official datasets with a batch size of 4. The in-put images were standardized to an 80 80 resolution. Thetraining spanned approximately 23 hours, with a learningrate that started at 3104, reduced to 1107 over 75,000iterations using a Cosine Annealing schedule. This was fol-lowed by a second phase with a learning rate of 6 105,also reduced to 1 107 over an additional 60,000 itera-tions. Notably, no special efficiency optimization strategieswere applied during this process.",
  ". Highspeed HDR Video Reconstruction fromEvents": "Event cameras, differing from conventional cameras thatcapture scene intensities at a fixed frame rate, use a uniqueapproach by detecting pixel-wise intensity changes asyn-chronously. This is triggered whenever a pixels intensitychange surpasses a certain contrast threshold. Unlike tradi-tional frame-based cameras, event cameras have several ad-vantages: low latency, low power consumption, high tem-poral resolution, and high dynamic range (HDR). Thesequalities make them particularly useful for a range of vi-sion tasks, including real-time object tracking ,high-speed motion estimation , optical flow estimation, ego motion analysis , and so on.However, the distinct triggering mechanism of eventcameras presents a challenge. The event data they capture,which lacks absolute intensity values and is represented as4-tuples, is incompatible with standard frame-based visionalgorithms. This discrepancy necessitates specialized pro-cessing pipelines, different from traditional image process-ing methods. Consequently, there is a growing interest intransforming event data into intensity images to leverage",
  "IVISLAB16.5618.520.730.132Jackzou16.2118.500.700.133apolloUI16.2118.390.700.13": "the high-speed and HDR capabilities of event cameras inpractical applications .To this end, we are launching a challenge focused on re-constructing high-speed HDR videos from event streams.We will utilize the high-quality Event-to-HDR dataset, cap-tured by a co-axis system and developed by . Thisdataset includes aligned pairs of event streams and HDRvideos in both spatial and temporal dimensions.In the challenge evaluation,three evaluation met-rics are used for assessment: Peak signal-to-noise ratio(PSNR), tone-mapped PSNR (PSNR-), Structural Similar-ity (SSIM) and multi-scale SSIM (MS-SSIM). The trainingdataset consists of 300 paired LDR/HDR images. The inputimages of validation and testing sets are provided, while theGT are not available to participants. The final leaderboardof top-3 participants are shown in .",
  ". IVISLAB Teams Method": "To achieve high-speed HDR video reconstruction fromevents, our team introduces the Dual Event-stream Recon-struction Network (DERNet). As depicted in ,DERNet uses long-time and short-time event voxels to re-construct the low-frequency brightness and high-frequencytexture of HDR video.Furthermore, DERNet integratesSwin Transformer and Conv-GRU blocks to capture spa-tial and temporal contexts, thereby enhancing reconstruc-tion accuracy.",
  "+ 3L2(It, Igtt ) + 4L2(M(It), M(Igtt ))(14)": "where 1, 2, 3, and 4 coefficients balancing the lossterms, L1(, ) is the absolute loss function, L2(, ) is themean squared error loss function, Igttis the ground truth t-th frame HDR image, and M() is the HDR to SDR functiondefined as M(x) = log(1+5000x) log(5001).DERNet is implemented using PyTorch. During train-ing, a batch size of 2 is utilized, with a video sequencelength of 10 and a data size of 224 224. An AdamWoptimizer is adopted with a learning rate of 4 105 and weight decay of 106 to optimize the network weightsfor 60 epochs. A cosine annealing scheduler is adopted todecay the learning rate. To prevent overfitting, random flip-ping, rotation, and cropping are applied to the event vox-els for data augmentation. The coefficients are defined asb = 6, Tl = 16, Ts = 5, 1 = 1, 2 = 0.1,3 = 500, and4 = 10.",
  ". Architecture of DERNet": "convolution layers, producing 2N + 1 output feature maps,{FtN, . . . , Ft+N}. This shared encoding facilitates sub-sequent alignment by transforming the input data into a con-sistent feature space.We then employ a deformable convolution-based align-ment module , which uses pyramidal deformable con-volutions to align features of different event frames withthe central frame feature Ft. This approach predicts offsetsfor the convolution kernels through a pyramidal processingstructure, allowing the network to handle larger movementsand align features accurately, thereby avoiding the pitfallsof inaccurate optical flow estimation.The aligned features are combined in the attentive fusionand reconstruction module. Here, the features are stackedand processed by attention mechanisms that independentlyfocus on height, width, and temporal/channel correlations.The fused features are passed through a recurrent residualnetwork and a ConvLSTM module, which help main-tain temporal continuity by remembering information fromsuccessive sequences.To enhance temporal consistency, we introduce a novel temporal consistency loss based on the integral relation-ship between consecutive frames and events, modeled us-ing a pre-trained UNet-like network.This loss en-sures smooth transitions between frames, mitigating issuesrelated to temporal discontinuity.",
  ". The overview of recurrent convolutional neural network for HDR video reconstruction from events": "The network is initialized using Kaiming initializa-tion , and trained with the Adam optimizer (momentumset to 0.9). The initial learning rate is 104, reduced by afactor of 10 every 50 epochs. We set the batch size to 4,and train the model for 100 epochs. The implementationuses the PyTorch framework, and training is performed onNVIDIA TITAN V GPUs.In summary, our network architecture and training strat-egy effectively reconstruct high-quality HDR videos fromevent data, ensuring both spatial and temporal coherence.",
  ". The network architecture": "noteworthy that the reference frames generated by E2VIDare often affected by the actual data distribution density,leading to occurrences of blank spaces or excessive noise.Regarding the network itself, data augmentation tech-niques such as random horizontal flipping and the additionof Gaussian noise with a standard deviation of 0.001 are ap-plied to the input data. Additionally, to better align withthe evaluation metrics of this challenge, four types of noiseare introduced, including KL divergence noise (to ensurealignment between HDR ground truth images and gener-ated images), L1 noise, L2 noise, and SSIM noise. Dueto time constraints, rigorous ablation experiments were notconducted. However, from a holistic analysis of the results,KL divergence noise yielded relatively favorable gains.Other training parameters include the Adam optimizerwith a learning rate of 0.001, cosine annealing for learn-ing rate scheduling, a batch size of 48, and iterative train-ing conducted using four NVIDIA 4090 GPUs. Trainingis performed for 1000 epochs, with the overall training andtesting dataset split in an 8:2 ratio. To address challengingsamples, this method manually removes data with poor dis-tributions (some data lack original event information due tobandwidth congestion, rendering them unsuitable for train-ing). Finally, inference can be performed, followed by trun-",
  ". Overexposure Image Correction": "Over-exposure is a prevalent issue in digital camera sen-sor systems, caused by automatic exposure errors duringimage processing. This problem particularly arises in dy-namic scenes with fluctuating brightness levels, i.e., a carexiting a tunnel or the sudden illumination of a dark envi-ronment. Exposure correction aims to correct the brightnesserrors that occur during the image capture process .However, most CCD or CMOS cameras can only capturea limited illumination range and will produce clipped orover-exposed pixels when sensor elements are saturateddue to improper settings or physical constraints in sensors.This largely degrades the essential details in bright areas ofphotographs as well as the image quality . Therefore,correcting the brightness and texture details of the over-exposed images becomes a crucial task to improve the vi-sual aesthetics of captured images and the performance ofdownstream image processing applications.On the overexposure correction track (), thetop three teams have shown outstanding performance. Gxjranked first with a comprehensive score of 21.58. Specif-ically, gxj achieved PSNR of 21.58 and SSIM of 0.95.CVCV achieved PSNR of 20.56 and SSIM of 0.94. LiGoxinachieved PSNR of 19.45 and SSIM of 0.92.These results highlight the remarkable advancementsmade by the participating teams in addressing the chal-",
  ". RAW based Over-Exposure Correction dataset": "To propel research in this field forward, it is essential toassess proposed methods in real-world scenarios.Con-sequently, we will utilize the RAW image-based Real-world Paired Over-exposure (RPO) dataset, introducedby Prof.Fus team in , captured using a CanonEOS 5D Mark IV camera.The RPO dataset comprisespaired images collected across various scenes. Each short-exposure (normal-exposure) image is paired with long-exposure (over-exposure) images with 4 ratios (x3, x5, x8,x10). Some representative examples of RPO dataset areshown in .The RPO dataset exhibits the following characteristics: Short Exposure Images (Normal, GT): Captured ineach scene using a tripod-mounted camera. The camerawas set to automatic mode to find optimal aperture andexposure time settings, then switched to manual mode tolock these settings. Images were taken using a remotemobile app to control the shutter, minimizing lens vibra-tion. Long Exposure Images (Over-exposure, OE): Follow-ing the capture of short exposure GT images, only theexposure time setting was adjusted using the mobileapp to simulate real over-exposure caused by incorrectsettings. Four predetermined over-exposure ratios wereused (3, 5, 8, 10). It was ensured that the camerawas not touched during both long and short exposure cap-tures to prevent any misalignment due to lens vibration.",
  ". Framework diagram of the model": "The key of the task is for the color correction of expo-sure area, but because the test file format directly with ex-isting models, so we first convert the existing data set for-mat, then choose compatible with low exposure and exces-sive exposure area perception exposure correction network(RECNet), through adaptive learning and bridging differentarea exposure representation to handle the mixed exposure,then the super resolution model (OmniSR) for the final re-sult, finally achieve high excessive exposure image qualityrecovery. Data preprocessingSince the final test stage of this taskprovides the processed mat format files, in order to make themodel better correct the test image, we further converted theimage in mat format based on the existing RAW format dataset and then converted the JPG format file which is moreacceptable to the model. In general, the data set used fortraining is transformed into an image distribution file simi-lar to the input of the test image. In this process, because theoriginal image size is too large, we modified all the imagesto unify the size of the test set as the data for the trainingmodel. Exposure correctionCorrection for image exposure hasbeen studied for a long time. Traditional methods will relymainly on manual adjustment of models, such as histogramequilibria and gamma correction. Although existing meth-ods achieve commendable results in exposure correction, many of them rely on complex manual designs or strugglewith excessive limitations that ultimately lead to suboptimalresults.After investigating the existing model and analyzing thedata set, and also being inspired by the RECNet model,the exposure correction model used in this task was finallyselected. When processing single images with mixed ex-posures, the network is difficult to stably converge, due tothe large difference in over-and under-exposed regions, re-sulting in unbalanced performance for different exposures.To this end, the model takes into account the locality ofdifferent exposures to reduce the adverse effects of incon-sistent optimization.To achieve this, the model adopt theidea of the divide and conquer strategy, and design aregion-aware exposure correction framework consisting of twowell-designed modules concatenated in a chain of consecu-tive RMBs.The model mainly contains a series of Blocks (RMB)with Region-aware De-exposure Module (RDM) andMixed-scale Restoration Unit (MRU). The RDM maps ex-posure features Fin to a three-branched exposure-invariantfeature Fn, while the MRU integrates the features Fs andFc by the spatial-wise and channel-wise restoration, respec-tively. The exposure mask predictor (EMP) assists in gen-erating the underexposure feature Fu and overexposure fea-ture.It optimize the model with Exposure Contrastive Reg-ularization (ECR). Image super resolutionTo match the results after expo-sure correction with the size of the resulting images requiredfor the task, we used the Omni-SR model to achieve a2x super-resolution of the resulting images. Specifically, themodel proposes a Omni Self-Attention (OSA) block basedon the principle of dense interaction, which can model thepixel interaction from both the dimensions of space andchannel, and mine the potential correlation between theglobal axis (i. e., space and channel). Combined with main-stream window partition strategies, OSA can achieve supe-rior performance with a compelling computational budget.Second, a multi-scale interaction scheme is proposed to al-leviate suboptimal ERFs in the shallow model, promotinglocal propagation and meso global scale interactions to formfull-scale aggregate blocks.",
  ". Different ratio result image examples and scores": "iterations, using a single NVIDIA RTX 4090 GPU. ForOmni-SR, the pre-trained model of epoch885 with Om-niSR on the DF2K dataset was used to treat the exposure-corrected images for 2x super-resolution. The experimentalresults are shown in , including the results and scoresobtained by the original data after the recovery of the pro-cess structure.The title is dedicated to the correction task of overex-posed images. In this report, we detail our teams data pro-cessing methods and the use of models in this task. Forthe recovered images, the image quality is improved againthrough the super-resolution model, yielding better results.The experiments proved that our strategy to solve this taskis reasonable and effective, ultimately achieving a score of21.58 in the Ratio = 3 track of this task dataset.",
  ". Test process diagram": "image signal processing pipeline.Compared to sRGB-based technologies, RAW images are characterized by anear-linear correlation with scene brightness and exhibit su-perior performance due to the rich information content dueto higher bit depth. Traditional digital camera sensors aredesigned to have a higher response ratio and relative spec-tral sensitivity to green channels. Therefore, in RAW im-ages captured by most digital camera systems, the greenchannel is usually more likely to be overexposed in brightscenes than the red or blue channel. The red and blue chan-nels of RAW images show more appropriate brightness andricher texture details than the green channels. This indicatesthat the green channel in the RGGB RAW image is moresaturated than the red or blue channel and requires strongercorrection.Channel-Guidance Network(CGNet), which takes ad-vantage of RAW images for overexposure correction.CGNet estimates correctly exposed sRGB images directlyfrom overexposed RAW images in an end-to-end manner.Specifically, they introduce a RAW based channel guidebranch into the U-Net-based backbone, which utilizes colorchannel intensity priors of RAW images to achieve superioroverexposure correction performance. Data preprocessing. Our team chose CGNet model foroverexposure image correction, and the model default in-put format is RGGB four-channel. In order to maintain theperformance of the model, we decided to convert the orig-inal images in the dataset into RGGB format for training.The original training image is stored in CR2 format, andthe rawpy library is directly called to batch convert CR2files to RGB three-channel format. Then copy the greenchannel and convert it to RGGB four-channel format. Then,the overexposed images of four ratios (3,5,8,10) were inputinto CGNet together and divided into the training set andvalidation set according to the ratio of 8:2.The storage format of the original test image is mat. Af-ter reading the mat file, it is found that the pixel value is",
  "between 0 and 1, and the four-channel is RGBG. Therefore,the pixels are multiplied by 255, and the four channels areconverted to RGGB, and the processed PNG image is ob-tained": "CGNet. As shown in , the main branch is basedon a basic U-net with four encoder (downsample) anddecoder (upsample) stages. Specifically, they first extractthe initial features from the four-channel RAW images us-ing a standard 33 convolution. In the encoder section, theHIN blocks are utilized to broaden the receptive field andenhance the robustness of features at various scales. Dur-ing the downsampling operation, they double the numberof channels in the feature maps. Moving on to the decoderpart, residual blocks are employed to capture high-level fea-tures more effectively. For the skip connection, they intro-duce a novel Cascaded Dilated Residual (CDR) block toextract multi-scale features, which are then merged with theencoders features to mitigate the loss of detail informa-tion resulting from downsampling. The proposed NGCGbranch integrates the prior knowledge of blue and red chan-nels into each scale of the main branch encoder, aiding themain branch in recovering over-exposed areas more effec-tively. Non-Green Channel Guidance. Firstly, the pixels pertain-ing to the red (or blue) channel are extracted from their re-spective positions within each 22 block of a Bayer image.Subsequently, these red and blue channels are input intothe NGCG branch, which then generates an initial predic-tion of the corresponding components in the output sRGBimage. The NGCG branch is structured with a Guidance-Enhanced Block (GEB) and four downsampling blocks,serving to guide the five corresponding encoder blocks.Within the GEB, there are two 33 convolutional layers,with an Instance Normalization and LeakyReLU fol-lowing each layer, as well as a 33 convolutional opera-tion in between. It is worth noting that depth-wise sepa-rable convolution is utilized instead of the traditional 33convolution to efficiently capture local information. Thedownsampling blocks comprise a maxpooling layer, a mod-ified self-attention structure, and two depth-wise separableconvolution layers.This arrangement allows the NGCGbranch to aid the primary backbone network in restoringover-exposed RAW images in a multi-scale manner. Cascaded Dilated Residual Block. In detail, each CDRblock incorporates three residual connections that featuredilated convolution and the LeakyReLU activation function.Subsequently, a 11 convolutional layer follows, which en-ables the CDR block to effectively utilize the features ex-tracted from each stage of the encoder and adequately ex-plore local texture information. Additionally, it is worthnoting that the dilated convolution employed in this config-uration effectively enhances the receptive field of the CDR",
  "ratio=32.2020.56ratio=51.6520.49ratio=81.2720.54ratio=101.1020.03": "The main task of this competition is to correct overex-posed images. This report details our teams approach todata processing and the details of model training and pre-dictions. The experiment verifies that the data convertedfrom CR2 or mat format to four-channel RGGB will not beaffected by performance degradation when using the CGNetmodel for training. Additionally, by multiplying the pixelsof the predicted results, we improved the quality of the im-ages and achieved higher scores. The experiment verifiesthat our strategy to solve this problem is reasonable and ef-fective. In the end, we scored 20.56 on the RPO dataset,placing us in second place.",
  "We use CGNet as a solution to the problem.CGNetcontains two branches, namely a main branch based on U-net and a non-green channel guided (NGCG) branch, asshown in": ". Architecture of our Channel-Guidance Network (CGNet ) for image over-exposure correction. Given an over-exposedRAW image, they pad it into a four-channel RGGB image, and then feed it into their CGNet. Their CGNet is based on a U-Net backbone.The encoder consists of Half-Instance Normalization, while the decoders are Residual blocks. They replace the original skip connectionwith Cascaded Dilated Residual (CDR) blocks. The red and blue channels are input to a Non-Green Channel Guidance (NGCG) branch fortexture detail reconstruction. Their CGNet is pre-trained on their synthetic RAW image-based dataset, and fine-tuned on their Real-worldPaired Over-exposure dataset. The main branch is based on a basic U-net with fourencoder (downsampling) and decoder (upsampling) stages.Specifically, the model first extracts initial features from afour-channel RAW image through a standard 3 3 convo-lution. In the encoder part, the model uses a HIN block toexpand the receptive field and improve the robustness of thefeatures at each scale. During the downsampling operation,the number of channels in the feature map is doubled. Inthe decoder part, the model uses a residual block to betterextract high-level features. For skip connections, the modeluses a novel cascaded dilated residual (CDR) block to ex-tract multi-scale features and fuse them with features fromthe encoder part to compensate for the loss of detail infor-mation caused by downsampling. Specifically, each CDRblock contains three residual connections with dilated con-volutions and LeakyReLU activation functions, followed bya 1 1 convolution layer. This allows the CDR block tomake good use of features from each stage of the encoderand fully explore local texture information. In addition, thedilated convolution used here can effectively expand the re-ceptive field of the CDR block for multi-scale context fea-ture extraction. For the NGCG branch, the pixels belonging to the corre-sponding position of the red (or blue) channel are first ex-tracted in each 2 2 block of the Bayer image. Then, thered and blue channels are input into the NGCG branch to produce an initial estimate of the corresponding elements inthe output sRGB image. The NGCG branch consists of aGuidance-Enhanced Block (GEB) and four downsamplingblocks, guiding 5 corresponding encoder blocks. GEB con-tains 2 3 3 convolutional layers (followed by instance nor-malization and LeakyReLU) and 3 3 convolution opera-tions between them. It is worth noting that the model usesdepthwise separable convolution instead of standard 3 3convolution to fully extract local information. The down-sampling block consists of a maximum pooling layer, animproved self-attention structure, and two depthwise sepa-rable convolutional layers.",
  ". Different ratio examples and scores": "trained on the SOF dataset and fine-tuned on the RPOdataset. After processing, we adjust the images at differentratios and upscale the corrected images. The experimentalresults are shown in .This report details our data processing methods andmodel usage in this task. Experiments have proven that ourstrategy for solving this task is reasonable and effective, andwe ultimately achieved a score of 18.95 on the Ratio = 3track of this task dataset, ranking third.",
  ". Teams and Affiliations": "gxjTitle: 1st Solution Places for PBDL2024 Raw Image BasedOver-Exposure Correction ChallengeMembers: Xuejian Gou (), QinliangWang, Yang Liu, Fang Liu, Lingling Li, Wenping MaAffiliations:School of Artificial Intelligence, XidianUniversity CVCVTitle: Raw Image Based Over-Exposure CorrectionMembers:Shizhan Zhao (),Yanzhao Zhang, Libo Yan, Xiaoqiang Lu, Licheng Jiao,Yuwei GuoAffiliations: Intelligent Perception and Image Understand-ing Lab, Xidian University",
  ". Conclusion": "The three-month-long competition attracted over 300 par-ticipants, with more than 500 submissions from both indus-try and academic institutions. This high level of participa-tion underscores the growing interest and investment in thefield of computer vision, particularly in the integration ofphysics-based approaches with deep learning.Looking forward, we anticipate continued advancementsand breakthroughs in this interdisciplinary area. The suc-cess of this challenge has set a strong foundation for fu-ture research and development, encouraging more collab-oration between academia and industry to solve complexvision problems. We are excited to see the future innova-tions and practical applications that will emerge from theseefforts.",
  "Conf. Comput. Vis. Pattern Recog., pages 1103611045, 2019. 6, 20": "Michael Broxton, John Flynn, Ryan Overbeck,Daniel Erickson, Peter Hedman, Matthew Duvall, Ja-son Dourgarian, Jay Busch, Matt Whalen, and PaulDebevec. Immersive light field video with a layeredmesh representation. ACM Trans. Graph., 39(4):861, 2020. 1 Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang,Radu Timofte, and Yulun Zhang.Retinexformer:One-stage retinex-based transformer for low-lightimage enhancement.In Int. Conf. Comput. Vis.,pages 1250412513, 2023. 4 Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang,Radu Timofte, and Yulun Zhang.Retinexformer:One-stage retinex-based transformer for low-lightimage enhancement.In Proceedings of theIEEE/CVF international conference on computer vi-sion (ICCV), pages 1250412513, 2023. 24",
  "Chen Chen, Qifeng Chen, Minh N Do, and VladlenKoltun. Seeing motion in the dark. In Int. Conf. Com-put. Vis., pages 31853194, 2019. 20": "Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong,Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,Jianping Shi, Wanli Ouyang, et al. Hybrid task cas-cade for instance segmentation. In IEEE Conf. Com-put. Vis. Pattern Recog., pages 49744983, 2019. 3,5 Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao,Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng,Chenchen Zhu, Tianheng Cheng, Qijie Zhao, BuyuLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, JingdongWang, Jianping Shi, Wanli Ouyang, Chen ChangeLoy, and Dahua Lin. MMDetection: Open mmlabdetection toolbox and benchmark.arXiv preprintarXiv:1906.07155, 2019. 6",
  "Paul E Debevec and Jitendra Malik. Recovering highdynamic range radiance maps from photographs.Proc. of ACM SIGGRAPH, pages 110, 2008. 2, 26": "Zongyuan Ding, Tao Wang, Quansen Sun, QiongjieCui, and Fuhua Chen.A dual-stream frameworkguided by adaptive gaussian maps for interactive im-age segmentation. Knowledge-Based Systems, 223:107033, 2021. 16 AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby.An image is worth16x16 words:Transformers for image recogni-tion at scale.In Proceedings of the InternationalConference on Learning Representations, 2021. 10 AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas Unterthiner, Mostafa Dehghani, MatthiasMinderer, Georg Heigold, Sylvain Gelly, et al. Animage is worth 16x16 words:Transformers forimage recognition at scale.In Int. Conf. Learn.Represent., 2021. 3",
  "noisy images. In IEEE Conf. Comput. Vis. PatternRecog., pages 21292137, 2019. 20": "Jun Haeng Lee, Kyoobin Lee, Hyunsurk Ryu,Paul KJ Park, Chang-Woo Shin, Jooyeon Woo, andJun-Seok Kim. Real-time motion estimation basedon event-based vision sensor. In IEEE Int. Conf. Im-age Process., pages 204208. IEEE, 2014. 29 Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren,Samuli Laine, Tero Karras, Miika Aittala, and TimoAila. Noise2noise: Learning image restoration with-out clean data.In Int. Conf. Mach. Learn., pages29652974. PMLR, 2018. 20 Chongyi Li, Chun-Le Guo, Man Zhou, ZhexinLiang,Shangchen Zhou,Ruicheng Feng,andChen Change Loy. Embedding fourier for ultra-high-definition low-light image enhancement. In Int. Conf.Learn. Represent., 2023. 16, 17 Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Che-ung, Simon See, Xiaogang Wang, Hongwei Qin, andHongsheng Li. A simple baseline for video restora-tion with grouped spatial-temporal shift.In IEEEConf. Comput. Vis. Pattern Recog., pages 98229832, 2023. 11 Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, LeiZhang, Lionel M Ni, and Heung-Yeung Shum. Maskdino: Towards a unified transformer-based frame-work for object detection and segmentation. In IEEEConf. Comput. Vis. Pattern Recog., pages 30413050, 2023. 6, 7 Mading Li, Jiaying Liu, Wenhan Yang, XiaoyanSun, and Zongming Guo. Structure-revealing low-light image enhancement via robust retinex model.IEEE Transactions on Image Processing, 27(6):28282841, 2018. 24 Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, RakeshRanjan, Eddy Ilg, Simon Green, Jiezhang Cao, KaiZhang, Radu Timofte, and Luc V Gool.Recur-rent video restoration transformer with guided de-formable attention.Adv. Neural Inform. Process.Syst., 35:378393, 2022. 11 Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks,Tianfan Xue, Nikhil Karnad, Qiurui He, Jonathan TBarron, Dillon Sharlet, Ryan Geiss, et al. Handheldmobile photography in very low light. ACM Trans.Graph., 38(6):1641, 2019. 20 Tsung-Yi Lin, Michael Maire, Serge Belongie,James Hays, Pietro Perona, Deva Ramanan, PiotrDollar, and C Lawrence Zitnick.Microsoft coco:Common objects in context. In Eur. Conf. Comput.Vis., pages 740755. Springer, 2014. 6, 7",
  "Feifan Lv, Yu Li, and Feng Lu.Attention guidedlow-light image enhancement with a large scale low-light simulation dataset. Int. J. Comput. Vis., 129(7):21752193, 2021. 16": "Chengqi Lyu, Wenwei Zhang, Haian Huang, YueZhou, Yudong Wang, Yanyi Liu, Shilong Zhang,and Kai Chen. Rtmdet: An empirical study of de-signing real-time object detectors.arXiv preprintarXiv:2212.07784, 2022. 5, 6 Matteo Maggioni, Yibin Huang, Cheng Li, ShuaiXiao, Zhongqian Fu, and Fenglong Song. Efficientmulti-stage video denoising with recurrent spatio-temporal fusion. In IEEE Conf. Comput. Vis. PatternRecog., pages 34663475, 2021. 13 Miguel Angel Martnez-Domingo, Eva M Valero,Javier Hernandez-Andres, Shoji Tominaga, TakahikoHoriuchi, and Keita Hirai. Image processing pipelinefor segmentation and material classification based onmultispectral high dynamic range polarimetric im-ages. Opt. Express, 25(24):3007330090, 2017. 26 Ben Mildenhall, Jonathan T Barron, Jiawen Chen,Dillon Sharlet, Ren Ng, and Robert Carroll. Burstdenoising with kernel prediction networks. In IEEEConf. Comput. Vis. Pattern Recog., pages 25022510, 2018. 20",
  "Olaf Ronneberger, Philipp Fischer, and ThomasBrox. U-net: Convolutional networks for biomedicalimage segmentation. In Med. Image Comput. Com-put. Assist. Interv., pages 234241, 2015. 31": "Daniel Saner, Oliver Wang, Simon Heinzle, YaelPritch,AljoschaSmolic,AlexanderSorkine-Hornung, and Markus H Gross. High-speed objecttracking using an asynchronous temporal contrastsensor. In Vision, Modeling, and Visualization, pages8794. Citeseer, 2014. 29 Paul-Edouard Sarlin,Ajaykumar Unagar,MansLarsson, Hugo Germain, Carl Toft, Viktor Lars-son, Marc Pollefeys, Vincent Lepetit, Lars Ham-marstrand, Fredrik Kahl, et al. Back to the feature:Learning robust camera localization from pixels topose. In IEEE Conf. Comput. Vis. Pattern Recog.,pages 32473257, 2021. 1",
  "Wei Wang, Xin Chen, Cheng Yang, Xiang Li, Xue-mei Hu, and Tao Yue. Enhancing low light videos byexploring high sensitivity camera noise. In Int. Conf.Comput. Vis., pages 41114119, 2019. 20": "Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong,and Chen Change Loy.Edvr: Video restorationwith enhanced deformable convolutional networks.In IEEE Conf. Comput. Vis. Pattern Recog. Worksh.,pages 00, 2019. 10, 31 Xintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong,and Chen Change Loy.Edvr: Video restorationwith enhanced deformable convolutional networks.In IEEE Conf. Comput. Vis. Pattern Recog. Worksh.,pages 19541963, 2019. 28",
  "Yixin Yang, Jin Han, Jinxiu Liang, Imari Sato, andBoxin Shi.Learning event guided high dynamicrange video reconstruction. In IEEE Conf. Comput.Vis. Pattern Recog., pages 1392413934, 2023. 29": "Zongyuan Yang, Baolin Liu, Yongping Xxiong, LanYi, Guibin Wu, Xiaojun Tang, Ziqi Liu, Junjie Zhou,and Xing Zhang. Docdiff: Document enhancementvia residual diffusion models.In ACM Int. Conf.Multimedia, pages 27952806, 2023. 18 Chengxi Ye, Anton Mitrokhin, Cornelia Fermuller,James A Yorke, and Yiannis Aloimonos. Unsuper-vised learning of dense optical flow, depth and ego-motion with event-based sensors. pages 58315838.IEEE, 2020. 29 Xunpeng Yi, Han Xu, Hao Zhang, Linfeng Tang,and Jiayi Ma. Diff-retinex: Rethinking low-light im-age enhancement with a generative diffusion model.In Proceedings of the IEEE/CVF international con-ference on computer vision (ICCV), pages 1230212311, 2023. 24",
  "Jin Yuan, Xingxing Hou, Yaoqiang Xiao, Da Cao,Weili Guan, and Liqiang Nie. Multi-criteria activedeep learning for image classification. Knowledge-Based Systems, 172:8694, 2019. 16": "Huanjing Yue, Cong Cao, Lei Liao, Ronghe Chu,and Jingyu Yang.Supervised raw video denois-ing with a benchmark dataset on dynamic scenes.In IEEE Conf. Comput. Vis. Pattern Recog., pages23012310, 2020. 9, 13, 26 Syed Waqas Zamir, Aditya Arora, Salman Khan,Munawar Hayat, Fahad Shahbaz Khan, Ming-HsuanYang, and Ling Shao. Multi-stage progressive im-age restoration. In IEEE Conf. Comput. Vis. PatternRecog., pages 1482114831, 2021. 18 Syed Waqas Zamir, Aditya Arora, Salman Khan,Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer forhigh-resolution image restoration.In IEEE Conf.Comput. Vis. Pattern Recog., pages 57285739,2022. 28, 29 Syed Waqas Zamir, Aditya Arora, Salman Khan,Munawar Hayat, Fahad Shahbaz Khan, Ming-HsuanYang, and Ling Shao. Learning enriched features forfast image restoration and enhancement. IEEE Trans.Pattern Anal. Mach. Intell., 2022. 13",
  "Jiqing Zhang, Xin Yang, Yingkai Fu, Xiaopeng Wei,Baocai Yin, and Bo Dong. Object tracking by jointlyexploiting frame and event domain.In Int. Conf.Comput. Vis., pages 1304313052, 2021. 29": "Richard Zhang, Phillip Isola, Alexei A Efros, EliShechtman, and Oliver Wang. The unreasonable ef-fectiveness of deep features as a perceptual metric.In IEEE Conf. Comput. Vis. Pattern Recog., 2018. 31 Yunliang Zhuang, Zhuoran Zheng, Yuang Zhang, LeiLyu, Xiuyi Jia, and Chen Lyu. Dimensional trans-formation mixer for ultra-high-definition industrialcamera dehazing. IEEE Transactions on IndustrialInformatics, 2023. 18"
}