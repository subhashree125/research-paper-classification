{
  "Abstract": "This paper reviews the AIS 2024 Video Quality Assess-ment (VQA) Challenge, focused on User-Generated Con-tent (UGC). The aim of this challenge is to gather deeplearning-based methods capable of estimating the percep-tual quality of UGC videos. The user-generated videos fromthe YouTube UGC Dataset include diverse content (sports,games, lyrics, anime, etc.), quality and resolutions. Theproposed methods must process 30 FHD frames under 1second. In the challenge, a total of 102 participants regis-tered, and 15 submitted code and models. The performanceof the top-5 submissions is reviewed and provided here asa survey of diverse deep models for efficient video qualityassessment of user-generated content.",
  ". Introduction": "Past two decades have seen a massive increase in popularityand demand for online video streaming applications such asNetflix and YouTube . This has been made possible dueto improvements in network capacity, improved end-userdevices, and increased computational efficiency, allowingusers to stream and watch content for hours over the interneteveryday . In order to optimize the end-user experienceand provide them with an improved quality of experience, Marcos V. Conde ( corresponding author, project lead) and RaduTimofte are the challenge organizers, while the other authors participatedin the challenge and survey.Marcos V. Conde and Radu Timofte are with University of Wurzburg,CAIDAS & IFI, Computer Vision Lab.Saman Zadtootaghaj, Marcos V. Conde and Nabajeet Barman are withSony Interactive Entertainment, FTG.AIS2024webpage:https : / / ai4streaming - workshop .github . io/.Code: https : / / github . com / mv - lab /VideoAI-Speedrun",
  ". Samples from the videos in the YT-UGC Dataset": "the service provider must measure the perceptual quality ofthe videos being delivered to them.Image quality assessment (IQA) or video quality assess-ment (VQA) can be assessed either subjectively or objec-tively. In subjective quality assessments, the users directlyassess the image/video quality and provide a rating forthat . However, such assessment processesare time consuming, costly, and often not realistic in real-world applications. Objective quality models help bridgethis gap by using mathematical/statistical models to predictthe quality as would be subjectively judged by human ob-servers . In recent years, deep learning techniques haveenabled us to learn objective quality metrics from visualcontent and the corresponding ratings. Depending on theavailability of a reference, QA models can broadly be clas-sified into Full-Reference and No-Reference (Blind) .This challenge deals with the design of deep learning-based methods for blind video quality metrics, targetinguser-generated content. Given a short video of an arbitraryresolution, the method will predict the overall quality.In this context, user-generated content refers to content",
  "arXiv:2404.16205v1 [cs.CV] 24 Apr 2024": "that is captured by users using consumer-grade devices,such as (primarily) smartphones, tablets, GoPros, etc. (see), and often shared via platforms such as Instagram,YouTube, TikTok, etc . Unlike professionallygenerated content, they are usually captured under verychallenging conditions, and hence, these can suffer frommany artifacts (camera capture impairments, lightning con-ditions, formats (resolution, fps), etc.).Recent works focus on designing NR models using deeplearning approaches on large-scale datasets to tackle thisproblem . Deep learning methods are betterable to capture and model various factors such as content,distortion, compression, and blur artifacts while also takinginto account the temporal aspect for video quality predic-tion. However, these demand large amounts of annotateddata, this has led to the creation of larger, more realisticdatasets such as KonVid-1k , YouTube YT-UGC ,and more recently, KonViD-150k VQA Database .",
  ". Dataset": "The challenge uses the YouTube User-Generated-Content(YT-UGC) dataset that consists of around 1000 videoclips with a duration of 20 seconds.The dataset includes several perceptual artifacts such asblockiness, blur, banding, noise, and jerkiness. In addition,the dataset has a wide range of content types with 15 dis-tinct categories, including animation, gaming, cover songs,music videos, and vlogs, among others.Moreover, a wide range of resolutions is considered inthe dataset, including 360p, 480p, 720p, 1080p, and 2160p.The clips are annotated with subjective ratings in the 5-point categorical Absolute Rating (ACR) Scale. All videoswere rated by more than 100 subjects using crowdsourc-ing. The Mean Opinion Score (MOS) is obtained on a rat-ing scale of 1 to 5, where 1 is the lowest perceived quality(bad) and 5 is the highest perceived quality (excellent).For this AIS UGC Video Quality Assessment Challenge,the dataset is split into two sets, training and test.Thelarger portion of the dataset consisting of 900 clips is usedfor training, while the test set includes 126 clips, selectedcarefully considering a balanced range of resolution, con-tent type, and distortion. We show samples in .",
  ". Baselines": "We consider two Baseline models for benchmarking whichare discussed next.NDNetGaming is a CNN-based quality metric thatis designed to assess gaming video quality. NDNetGamingis designed to predict quality in an interpretable range ofone to five, where one is the lowest quality, and five is thehighest quality score. NDNetGaming uses DenseNet-121as the backbone and is pre-trained on a large-scale gam-ing video dataset annotated with VMAF and fine-tuned bya public gaming video dataset. Since NDNetGaming wastailored for images, we used a sampling rate of 5 frames persecond and averaged the resultant quality estimation.We additionally used MobileNet v2 as the second base-line model, which allows us to compare the efficiencyof proposed models with a lightweight CNN image en-coder architecture. We first process each frame using Mo-bileNet . Next, we average the encoded features for allthe frames obtaining a single deep encoded representation,and finally, we predict the quality using a single linear layer.Thus, no frame sampling is applied to the MobileNet result.This represents a naive solution for benchmarking purposes.The baselines are highlighted in blue in Tab. 1.",
  ". Architectures and main ideas": "1. Frame Sampling: Given a clip of N frames, most meth-ods apply temporal (down)sampling i.e. process 1 (or 2)frames of every 30. This allows to increase efficiencywithout harming performance. Note that this is the rea-son why we report clip-based metrics instead of frame-based metrics. For instance, a model can virtually pro-cess a 30-frame clip in 100 ms, yet it does not imply a330 FPS performance.",
  ". Spatial Downsampling: Besides pooling in the tempo-ral domain, most approaches resize the frames to lowerresolutions (e.g. 512px) to reduce memory requirementsand operations": "3. Ensembles: The best solutions such as COVER and TVQE use multiple image processing models to ex-tract diverse features . Each model is trained to fo-cus on predicting specific properties such as aestheticsor compression. Although combining multiple modelsmight increase training and inference complexity, thisapproach provides the best performance while being sur-prisingly efficient.",
  ". Efficiency Study": "In Tab. 1 we present the summary of quantitative resultsand efficiency metrics for each method. The efficiency met-rics are calculated using: The runtime is the averageof 10 independent runs (after GPU warmup).TVQE and Q-Align use novel LLM-based VQA ap-proaches, thus the number of parameters is considerablyhigh (8 Billion). These approaches leverage video descrip-tions and visual information to provide accurate ratings. Al-though the number of parameters and operations is consid-",
  ". High-Resolution Efficiency study using as input a clipof 30 frames of 4K resolution 38402160. We report the runtimeand MACs for the complete clip of 30 frames": "erably high, the models can process 30 frames under a sec-ond, even at high resolution (FHD, 4K).As we show in Tab. 1 and Tab. 3, all the proposed meth-ods can process 30 FHD frames in under 1 second, and60 HD frames in under 0.5 seconds. Moreover, most ap-proaches can process 30 4K frames under 1 second. Discussion on frame-wise metricsWe report clip-basedmetrics. Since each method uses different frame samplingtechniques, it is difficult to calculate FPS or frame-wisemetrics. We instead focus on 30-frame and 60-frame clips.We can appreciate in Tab. 1 that COVER , TVQE andQ-Align have almost constant runtime (or operations)independently of the input resolution or number of frames.The reason is the constant temporal-spatial downsamplingon the input video i.e. FHD, HD, and 4K frames are alwaysdownsampled to the same resolution and fed into the model. Related ChallengesThis challenge is one of the AIS2024 Workshop associated challenges on:Event-basedEye-Tracking , Video Quality Assessment of user-generated content , Real-time compressed image super-resolution , Mobile Video SR, and Depth Upscaling.",
  "Contact:": "The team uses FasterVQA as backbone, training ina siamese manner. During training, the siamese networktakes a pair of videos as input and tries to predict which oneis in better quality. This training strategy, following a simi-lar methodology proposed in previous works , makesit possible to train our model on multiple datasets with var-ious scoring scale (YouTube-UGC , LIVE-VQC ,KoNVid-1k). After trained in siamese manner, the Faster-VQA model is then fine-tuned on YouTube-UGC.",
  "Method": "The network architecture of our proposed COmprehensiveVideo quality EvaluatoR (COVER) is illustrated in .This network accepts videos that have been subjected totemporal-spatial sampling as its input. Its architecture is di-vided into three branches: a CLIP-based semantic branch,an aesthetic branch and a technical branch, each consistingof a feature extraction module and a quality regression mod-ule. Notably, aesthetic and technical branches additionallyincorporate a feature fusion module to integrate featuresfrom the semantic branch. The input video is processedthrough these branches to generate three scores, reflectingthe videos quality across the respective dimensions. Thefinal score is the average of scores from three dimensions.",
  "As shown in , before serving as input to each branchsfeature extraction module, the input videos first undergo": "temporal-spatio sampling. To enhance the real-time perfor-mance of the network, temporal sampling is designed to bevery sparse. In the temporal sampling process for the inputvideo, the semantic branch samples one frame out of ev-ery thirty frames, while the aesthetic and technical branchessample two frames out of every thirty frames.For spatial sampling,the semantic and aestheticbranches resize the video resolution to 512x512 and224x224, respectively. The technical branch, however, em-ploys a fragment operation, where a frame from the videois divided into 7x7 sub-blocks. These sub-blocks are thenrandomly sampled and reassembled into a frame with a res-olution of 224x224.",
  "Feature Extraction": "Several studies have demonstrated the effectiveness ofCLIP , a foundation model, in both IQA andVQA tasks. By extracting semantic information fromimages and videos, CLIP can accurately assess their sub-jective quality. However, the aforementioned studies didnot address the more challenging task of UGC-VQA. Thismotivates us to employ the Image Encoder of CLIP as thebackbone of the feature extraction module for the semanticbranch. The pretrained weights (ViT-L/14) on OpenAI isimported and frozen.For the technical branch, the Swin Transformer isutilized as the backbone of the feature extraction module. ACNN network, specifically the ConvNet , is used as thebackbone of the feature extraction module for the aestheticbranch. These two branches are initialized with weightspretrained on the LSVQ from DOVER , and it willbe fine-tuned during subsequent training.",
  "Feature Fusion": "CLIPs image encoder is endowed with robust capabili-ties in representing image semantics by its numerous train-ing samples.Thus, the abundant information containedin CLIPs output features may inherently correlate withthe features of the other branches.To fully harness therepresentative features generated by the semantic branchand let it modulate the other branches, we propose a fea-ture fusion block. More specifically, we modify the cross-gating block , and name it Simple Cross-Gating Block(SCGB), for feature fusion between the semantic-aestheticand semantic-technical feature pairs.As illustrated in, The fused features from the aesthetic and technicalbranches, along with the features from the semantic branch,are then fed into their respective quality regression modules.The detailed architecture of SCGB is depicted in .The input of the block are two tensors X and Y . X is thefeature from the technical or aethetic branch, while Y is CLIP",
  "Y2": "SCGB . The architecture of our proposed COmprehensive Video quality EvaluatoR (COVER). COVER processes a video clip in threeparallel branches: 1) a semantic branch that extracts high-level object-semantics-related information using a pre-trained CLIP imageEncoder; 2) an aesthetic branch that leverages a ConvNet run on subsampled image thumbnails to analyze their looking; 3) a technicalbranch utilizing Swin Transformer to execute on fragments. We also devise a simplified cross-gating block (SCGB) to fuse multi-branchfeatures together, yielding the final quality score. from the CLIP-based semantic branch. After the input chan-nel projections are applied, the projected CLIP features arefed to a gating pathway to yield the gating weights, whichare then multiplied by the features from the other branch.Finally, the output projection and residual connection areapplied.",
  "Quality Regression": "The features from each branch are individually fed intoa multi-layer perceptron (MLP) Header to predict qualityscores, i.e., QS, QA, and QT , as shown in , and thefinal predicted quality, QP = (QS + QA + QT )/3. Toenforce that each branch can independently capture the fea-tures of its focused dimension and accurately predict videoquality, we adopted the limited view biased supervisionscheme , which minimizes the relative loss betweenpredictions in each branch with the overall opinion MOS,as formulated below:",
  "Inference Time": "VQA models are highly practical tools potentially deployedon large-scale video streaming platforms to process millionsof video streams every day. Therefore, the actual inferencecost per video is highly significant to the systems total per-formance and revenue. We have imbued efficient modu-lar design in every aspect of the COVER model, leadingto highly efficient inference speed. We benchmarked themodel inference time required by COVER on a video clip of30 frames of 1080p resolution using a TITAN RTX graphiccard. As shown in , COVERs semantic, aesthetic,and technical branch demands 191, 96, and 23 milliseconds to complete, together adding up to a total inference timeof 311 milliseconds. In other words, this inference latencytranslates to a highly efficient VQA metric that attains state-of-the-art performance with explainable properties and in-ferences at 96 fps, almost 3x faster than real-time process-ing speed.",
  "Time (ms)1919623311": "Implementation detailsThe hyper-parameter settingswithin COVER for its various components are outlined asfollows: i) the backbone of the feature extraction modulefor semantic branch is the Image Encoder from CLIP of type ViT-L/14; ii) the feature extraction backbone of aes-thetic branch is a ConvNet , structured into 4 stages.The configuration of each stage, defined by the number ofblocks and feature dimensions, is as follows: (3, 96), (192,3), (384, 9), and (768, 3); iii) the feature extraction back-bone of technical branch is a Swin Transformer , whichalso comprises 4 stages. Within each stage, the number ofheads is set to 3, 6, 12, and 24, respectively, with the num-ber of projection output channels being 96; iv) the SCGBmodule operates with input and output feature dimensionsboth set to 768, and its dropout layer has a drop ratio of 0.1;v) the input feature dimension for the MLP Header moduleis 768. It includes two dropout layers, both with a drop ratioof 0.5.The training process for our model is structured intothree distinct stages: 1. Initial Training of Technical and Aesthetic Branches:Initially, we train the entire network for both the techni-cal and aesthetic branches. During this stage, the weightsof both backbones and MLP Headers for all branches arefine-tuned. 2. Integrating Semantic Branch and Further Fine-tuning: Building on the best weights obtained fromstage 1, we integrate the semantic branch into model.Then MLP Headers of all branches, along with back-bones of both technical and aesthetic branches are fine-tuned. 3. Incorporation of SCGB and Final Fine-tuning: Basedon the optimal weights from stage 2, we add two SCGBsto model. Subsequent fine-tuning of both SCGBs alongwith all MLP Headers is conducted.Given the specific validation set of YouTube-UGC, ourmulti-stage training approach maintains the same data splitacross each step, allowing for incremental improvements intraining effectiveness.Throughout different training stages, only the specifictraining set of YouTube-UGC is used. For training strate-gies. we employ the ADAM optimizer with an initial learn-ing rate of 1 103 and a cosine learning rate decay strat-egy with a decay weight of 0.05, over a total of 20 epochs.However, the batch size varies across different stages, beingset to 10, 8, and 24 respectively. Our network, implementedin the Pytorch framework and running on an A6000 GPUcard, approximately requires one day to complete the entiretraining process.",
  "Tencent2 Wuhan University": "TVQE is a hybrid model trained for VQA tasks. Theproposed method fully takes into account several aspectsof video quality subjective assessment: 1. Humans makejudgments with attention to both global semantic and localvisual information; 2. Subjective evaluation experimentsusually require observers to learn and judge in discrete text-defined levels. Therefore, it combines three networks, i.e.,IQA network, DOVER , and Q-Align model, to ex-tract visual information and semantic information and pre-dicts the subjective quality more accurately via weightedfusion operation. The framework of the proposed method isshown in .First, considering that humans have a strong perceptionof visual information in the spatial dimension when mak-",
  "DOVER+Q-Align57:80.9130.915": ". Performance of Different TVQE Variants. DOVER (v0)represents the pre-trained model, and (v1) the fine-tuned model.Q-Align5 (v0) represents the pre-trained model, (v1) representsthe results by finetuning Visual Abstractor, and (v2) represents theresults by finetuning the last 5 transformer layers in Visual En-coder and Visual Abstractor. ing the judgment, we introduce a feature pyramid aggrega-tion mechanism on the backbone, i.e., the ConvNeXt, to ex-tract visual representations of the key frame. The pyramidstructure facilitates the full utilization of the extracted infor-mation as well as better exploitation of the shallow visualfeatures. Then, considering the influence of video contenton subjective assessment, we use the DOVER model with 3D convolution to assess video quality through aes-thetic and technical branches.Finally, we adopt a large multi-modality model, i.e., Q-align , to fit the fact that subjective judgment is usu-ally in discrete text-defined levels. The purpose is to stimu-late the behavior of the human annotation process by tuningLLMs (Large Language Models).These three models were trained independently on theofficial YT-UGC dataset following the challenge splits.During the inference stage, the final predicted score couldbe obtained by heuristically fusing the prediction results ofthese models. Ablation Study gives the ablation study of sub-mitted solution. We finetuned the SOTA DOVER and Q-align model on the give YT-UGC dataset. We take a smallsplit from the training set as the second validation set formodel selection.For the DOVER architecture, it could be seen that theSROCC value increases from 0.822 to 0.881 after carefullyfinetuning parts of the original network. For the Q-alignarchitecture, we tried different finetune strategy. Empiri-cally, we found that finetuning the last 5 layers of the visualencoder and the visual abstractor block gives the best per-formance gain, i.e., 0.07 in terms of SROCC.Then, thanks to the ensemble strategy, the performanceis further boosted by 0.005 in terms of SROCC and 0.44 interms of PLCC, respectively.",
  "We convert the traditional mean opinion scores (MOS)": "and the corresponding video into question-answer pairs toteach LMM VQA knowledge. Then we acquire the proba-bilities of the video quality from LMM response and obtainthe final quality values via weighted average. Q-Align is based on large multi-modality mod-els (LMMs).During the training stage, we divide thequality labels into specific rating categories.Given thatthe human-assigned ratings are evenly spaced, we utilizeequally spaced intervals for transforming scores into thesecategories. We achieve this by evenly dividing the rangefrom the maximum score (M) to the minimum score (m)into five separate intervals, assigning scores within each in-terval to corresponding categories:",
  "(Mm)": "(2)where the set li|5i=1 = {bad, poor, fair, good, excellent}denotes the established textual rating categories as de-fined by the ITU. We convert the videos into sequencesof keyframes, which are sampled as the first frame ofevery second.Then we form the question-answer pairslike How would you rate the quality of the video?|keyframe1||keyframe2| ... The quality of the video isbad/poor/fair/good/excellent to fine-tune the LMM.",
  "During the efficiency test, we find the Q-Align takes upabout 8,179M parameters and 991G MACs. Q-Align dealswith every 30fps video clip for about 533ms on GPU 3090": "Implementation detailsWe use the PyTorch framework.In experiments, we set batch sizes as 64 and the learningrate is set as 2e 5. We select mPLUG-Owl-2 as the LMMmodel. We only train the model on the training set of YT-UGC. We train for 2 epochs for all variants, which takes upabout 50 minutes. We conduct training on 4*NVIDIA A10080G GPUs, and report inference latency on one RTX309024G GPU. For videos, we sample at rate 1fps. The sampledframes are padded to square and then resized to 448 448.",
  ". The framework of SimpleVQA+ proposed by Team SJTU MMLab": "several one-second length video chunks to extract the cor-responding temporal features.We train the proposed model on 2 Nvidia RTX 3090GPUs with a batch size 6 for 30 epochs ( 3hrs). Thelearning rate is set as 105. During the inference phase,we feed the video into two models which are trained on theLSVQ and YT-UGC datasets respectively, to obtain predic-tion scores. Then, we average two scores to obtain the finalprediction score. Our proposed model is trained efficientlyand can take advantage of other quality-aware pre-trainedfeatures, which can help decrease the risk of overfitting.",
  ": / / github . com / google - research / google -research/tree/master/vila": "The subset of processed frames is done in two steps, thefirst samples for each second of the video the first frame.The second step takes the sampled frames and reduces themwith more importance to the end of the video. That meansfor a 20 s 30 fps video, 20 frames are sampled, and then outof them, the following 5 frames are used: .All features are extracted in separate threads to make themodel faster.Afterwards, the Frankenstone model com-bines the mentioned features and scores using a RandomForest Regression model. AVT uses DOVER for user-generated video quality prediction, and VILA for per-frameimage appeal prediction. Only the YouTube UGC training data was used.In an overview of the model structure is provided.The video is fed into the model and then several featuresare calculated in threads (parallel computation), dover andnvencc features (height, aspect ratio, bitrate for a specificencoding) are calculated for the full video, while pixel (SI,TI, colorfulness, average luminance, sharpness, nima ap-peal/quality , TI calculations to the first frame, SSIMpairwise and to the first frame) and vila features are onlycalculated for a subset of the video frames (because other-wise, the model would not hit the runtime requirements).The extracted features are combined using a random forestregression (during model development with a varying num-ber of trees, the submitted model uses 300 trees).The runtime of the model has been evaluated ex-emplarily with various videos,in the following theSports 2160P-210c.mkv (30 fps, UHD-1, 20s dura-tion) video is used. The 24 time measurements result in anaverage runtime of 19.616 s, with a standard derivationof 0.138 s. However, this may vary, depending on a warmstart of the model (and corresponding file-system caches).The model may not be fast enough for smaller videos, be-cause the data must be transferred to the GPU first.",
  "Optimizer and Learning Rate: A random forest modelwith a variable number of trees (10, 20, 100, and 300)have been used, there was no improvement using moretrees, the final model has 300 trees": "GPU: NVIDIA GeForce RTX 3090 Ti (24 GB) Datasets: Youtube UGC training data, no augmentation. Training Time: Extraction of features for each video 20 s max, thus 892 training videos, 12 h extraction time(was performed with 3-4 parallel processes to reduce thetime, overall on one PC), training the random forest re-gression model takes < 1 min (part of the Tensorflow De-cision Forests package).",
  ". Ablation study on the testing set by Team BVI-VQA": "Based on the intuition to train our model over multi-ple datasets, we proposed a ranking-based training strategyto train an existing SOTA network, FasterVQA , in asiamese manner.A common challenge when training on multiple datasetsis: different datasets usually have inconsistent scoring scaleand crowdsourcing protocol.To solve this problem, wetrained our model using a siamese structure, consisting oftwo FasterVQA networks sharing the same weights.Ateach time, the siamese network takes a random pair ofvideos from the same dataset as input and learns to pre-dict which one is of the better quality (with higher MOSground-truth value). Because the network does not directlytake MOS as training labels, it avoids the problem that MOSfrom different datasets may have different scoring scale.This ranking-based training strategy shares a similar insightas previous works . Pre-trained model from Faster-VQA has been used to initialize the training. After trained20 epoches over 3 datasets (YouTube-UGC , LIVE-VQC , KoNVid-1k) in siamese manner, the model isthen finetuned on YouTube-UGC. The training frameworkis illustrated in .",
  "Nabajeet Barman and Maria G. Martini. QoE Modeling forHTTP Adaptive Video StreamingA Survey and Open Chal-lenges. IEEE Access, 7:3083130859, 2019. 1": "Marcos V. Conde, Zhijun Lei, Wen Li, Ioannis Katsavouni-dis, Radu Timofte, et al. Real-time 4k super-resolution ofcompressed AVIF images. AIS 2024 challenge survey. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition Workshops, 2024. 3 Marcos V. Conde, Saman Zadtootaghaj, Nabajeet Barman,Radu Timofte, et al. AIS 2024 challenge on video qualityassessment of user-generated content: Methods and results.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, 2024. 3",
  "Franz Gotz-Hahn, Vlad Hosu, Hanhe Lin, and DietmarSaupe. KonVid-150k: A Dataset for No-Reference VideoQuality Assessment of Videos in-the-Wild. In IEEE Access9, pages 7213972160. IEEE, 2021. 2": "Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S Ren, RaduTimofte, Yuan Gong, Shanshan Lao, Shuwei Shi, JiahaoWang, Sidi Yang, et al. Ntire 2022 challenge on perceptualimage quality assessment. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition,pages 951967, 2022. 1 Chenlong He, Chenlong He, Ruoxi Zhu, Xiaoyang Zeng,Yibo Fan, and Zhengzhong Tu. COVER: A comprehensivevideo quality evaluator.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern RecognitionWorkshops, 2024. 2, 3, 4 Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, HuiMen, Tamas Sziranyi, Shujun Li, and Dietmar Saupe. Thekonstanz natural video database (konvid-1k). In 2017 Ninthinternational conference on quality of multimedia experience(QoMEX), pages 16. IEEE, 2017. 1, 2 Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Mi-lanfar, and Feng Yang. Vila: Learning image aesthetics fromuser comments with vision-language pretraining.In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1004110051, 2023. 9",
  "Christopher Lennan, Hao Nguyen, and Dat Tran.Imagequality assessment. 2018. 9": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 4, 5, 8 Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-enhofer, Trevor Darrell, and Saining Xie. A convnet for the2020s. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1197611986,2022. 4, 5",
  "Statista.Number of users of OTT video worldwidefrom 2020 to 2029 (in millions) [Graph]. . 1": "Statista.Daily time spent on social networking byinternet users worldwide from 2012 to 2024 (in min-utes) [Graph].https : / / www . statista . com /statistics / 433871 / daily - social - media -usage-worldwide/, . 1 Wei Sun, Xiongkuo Min, Wei Lu, and Guangtao Zhai. Adeep learning based no-reference quality assessment modelfor ugc videos. In Proceedings of the 30th ACM Interna-tional Conference on Multimedia, pages 856865, 2022. 8,9",
  "Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, GuangtaoZhai, and Kede Ma. Analysis of video quality datasets viadesign of minimalistic video quality models. arXiv preprintarXiv:2307.13981, 2023. 3, 8, 9": "Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli,and Alan C. Bovik. UGC-VQA: Benchmarking blind videoquality assessment for user generated content. IEEE Trans.Image Process., 30:44494464, 2021. 3 Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck,Balu Adsumilli, and Alan C Bovik. RAPIQUE: Rapid andaccurate video quality prediction of user generated content.IEEE Open Journal of Signal Processing, 2:425440, 2021.3 Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,Peyman Milanfar, Alan Bovik, and Yinxiao Li.Maxim:Multi-axis mlp for image processing.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 57695780, 2022. 4 Markus Utke, Saman Zadtootaghaj, Steven Schmidt, Se-bastian Bosse,and Sebastian Moller.Ndnetgaming-development of a no-reference deep cnn for gaming videoquality prediction. Multimedia Tools and Applications, pages123, 2022. 2, 3 Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-ploring clip for assessing the look and feel of images. In Pro-ceedings of the AAAI Conference on Artificial Intelligence,pages 25552563, 2023. 3, 4 Yilin Wang, Sasi Inguva, and Balu Adsumilli. Youtube ugcdataset for video compression research. In 2019 IEEE 21stInternational Workshop on Multimedia Signal Processing(MMSP), pages 15. IEEE, 2019. 1, 2, 6, 9, 10",
  "Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, LiZhang, and Kede Ma. Modular Blind Video Quality Assess-ment, 2024. 2": "Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao,Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Fast-vqa: Efficient end-to-end video quality assessment with frag-ment sampling. In European conference on computer vision,pages 538554. Springer, 2022. 3 Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou,Wenxiu Sun, Qiong Yan, Jinwei Gu, and Weisi Lin. Neigh-bourhood representative sampling for efficient end-to-endvideo quality assessment.IEEE Transactions on PatternAnalysis and Machine Intelligence, 2023. 3, 10 Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-wen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan,and Weisi Lin. Exploring video quality assessment on usergenerated contents from aesthetic and technical perspectives.In International Conference on Computer Vision (ICCV),2023. 2, 3, 4, 5, 6, 9 Haoning Wu, Zicheng Zhang, Weixia Zhang, ChaofengChen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang,Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms forvisual scoring via discrete text-defined levels. arXiv preprintarXiv:2312.17090, 2023. 3, 6, 7, 8 Wufeng Xue, Xuanqin Mou, Lei Zhang, Alan C Bovik, andXiangchu Feng. Blind image quality assessment using jointstatistics of gradient magnitude and laplacian features. IEEETrans. Image Process., 23(11):48504862, 2014. 3 Peng Ye, Jayant Kumar, Le Kang, and David Doermann. Un-supervised feature learning framework for no-reference im-age quality assessment. In Proc. IEEE Conf. Comput. Vis.Pattern Recognit. (CVPR), pages 10981105, 2012. 3",
  "Z Ying, M Mandal, D Ghadiyaram, and AC Bovik. Livelarge-scale social video quality (lsvq) database.Online: com/baidut/PatchVQ, 2020. 8": "Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram,and Alan Bovik.Patch-vq:patching upthe video qual-ity problem. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1401914029, 2021. 4 Qi Zheng, Zhengzhong Tu, Pavan C Madhusudana, Xi-aoyang Zeng, Alan C Bovik, and Yibo Fan. Faver: Blindquality prediction of variable frame rate videos. Signal Pro-cessing: Image Communication, 122:117101, 2024. 3"
}