{
  "BackBuildRoadCarPoleVegWall": ". Open-vocabulary event-based semantic segmentation (OpenESS). Our framework is capable of performing zero-shot seman-tic segmentation of event data streams with open vocabularies. Given raw events and text prompts as inputs, OpenESS outputs semanticallycoherent open-world predictions across various adjective, fine-grained, and coarse categories. The last three columns show the language-guided attention maps where regions of a high similarity score to the given text prompts are highlighted. Best viewed in colors.",
  "Abstract": "Event-based semantic segmentation (ESS) is a funda-mental yet challenging task for event camera sensing. Thedifficulties in interpreting and annotating event data limitits scalability.While domain adaptation from images toevent data can help to mitigate this issue, there exist datarepresentational differences that require additional effort toresolve. In this work, for the first time, we synergize infor-mation from image, text, and event-data domains and intro-duce OpenESS to enable scalable ESS in an open-world,annotation-efficient manner. We achieve this goal by trans-ferring the semantically rich CLIP knowledge from image-text pairs to event streams. To pursue better cross-modalityadaptation, we propose a frame-to-event contrastive dis-tillation and a text-to-event semantic consistency regular-ization. Experimental results on popular ESS benchmarksshowed our approach outperforms existing methods. No-",
  ". Introduction": "Event cameras, often termed bio-inspired vision sensors,stand distinctively apart from traditional frame-based cam-eras and are often merited by their low latency, high dy-namic range, and low power consumption . Therealm of event-based vision perception, though nascent, hasrapidly evolved into a focal point of contemporary research. Drawing parallels with frame-based perception andrecognition methodologies, a plethora of task-specific ap-plications leveraging event cameras have burgeoned .Event-based semantic segmentation (ESS) emerges asone of the core event perception tasks and has gained in-creasing attention . ESS inherits the challengesof traditional image segmentation , while",
  "arXiv:2405.05259v1 [cs.CV] 8 May 2024": "also contending with the unique properties of event data, which opens up a plethora of opportunities for explo-ration. Although accurate and efficient dense predictionsfrom event cameras are desirable for practical applications,the learning and annotation of the sparse, asynchronous, andhigh-temporal-resolution event streams pose several chal-lenges . Stemming from the image segmentationcommunity, existing ESS models are trained on densely an-notated events within a fixed and limited set of label map-ping . Such closed-set learning from expensive anno-tations inevitably constrains the scalability of ESS systems.An obvious approach will be to make use of the imagedomain and transfer knowledge to event data for the samevision tasks. Several recent attempts resort tounsupervised domain adaptation to avoid the need for pairedimage and event data annotations for training. These meth-ods demonstrate the potential of leveraging frame annota-tions to train a segmentation model for event data. How-ever, transferring knowledge across frames and events isnot straightforward and requires intermediate representa-tions such as voxel grids, frame-like reconstructions, andbio-inspired spikes. Meanwhile, it is also costly to annotatedense frame labels for training, which limits their usage.A recent trend inclines to the use of multimodal founda-tion models to train task-specific mod-els in an open-vocabulary and zero-shot manner, removingdependencies on human annotations. This paper continuessuch a trend. We propose a novel open-vocabulary frame-work for ESS, aiming at transferring pre-trained knowledgefrom both image and text domains to learn better represen-tations of event data for the dense scene understanding task.Observing the large domain gap in between heterogeneousinputs, we design two cross-modality representation learn-ing objectives that gradually align the event streams withimages and texts. As shown in , given raw eventsand text prompts as the input, the learned feature represen-tations from our OpenESS framework exhibit promising re-sults for known and unknown class segmentation and canbe extended to more open-ended texts such as adjectives,fine-grained, and coarse-grained descriptions.To sum up, this work poses key contributions as follows:",
  ". Related Work": "Event-based Vision. The microsecond-level temporal res-olution, high dynamic range (typically 140 dB vs. 60 dBof standard cameras), and power consumption efficiency ofevent cameras have posed a paradigm shift from traditionalframe-based imaging . A large variety ofevent-based recognition, perception, localization, and re-construction tasks have been established, encompassing ob-ject recognition , object detection , depth estimation , opti-cal flow , intensity-image recon-struction , visual odometry and SLAM, stereoscopic panoramic imaging , etc. Inthis work, we focus on the recently-emerged task of event-based semantic scene understanding . Such a pur-suit is anticipated to tackle sparse, asynchronous, and high-temporal-resolution events for dense predictions, which iscrucial for safety-critical in-drone or in-vehicle perceptions.Event-based Semantic Segmentation. The focus of ESS ison categorizing events into semantic classes for enhancingscene interpretation. Alonso et al. contributed the firstbenchmark based on DDD17 . Subsequent works are tai-lored to improve the accuracy while mitigating the need forextensive event annotations . EvDistill and DTL utilized aligned frames to enhance event-based learn-ing. EV-Transfer and ESS leveraged domain adap-tation to transfer knowledge from existing image datasetsto events. Recently, HALSIE and HMNet inno-vated ESS in cross-domain feature synthesis and memory-based event encoding.Another line of research pursuesto use of spiking neural networks for energy-efficient ESS. In this work, different from previous pur-suits, we aim to train ESS models in an annotation-freemanner by distilling pre-trained vision-language models,hoping to address scalability and annotation challenges.Open-Vocabulary Learning. Recent advances in vision-language models open up new possibilities for visual per-ceptions . Such trends encompass image-basedzero-shot and open-vocabulary detection , aswell as semantic , instance , andpanoptic segmentation. As far as we know, onlythree works studied the adaptation of CLIP for event-basedrecognition. EventCLIP proposed to convert eventsto a 2D grid map and use an adapter to align event fea-tures with CLIPs knowledge. E-CLIP uses a hier-archical triple contrastive alignment that jointly unifies theevent, image, and text feature embedding. Ev-LaFOR designed category-guided attraction and category-agnosticrepulsion losses to bridge event with CLIP. Differently, wepresent the first attempt at adapting CLIP for dense pre-dictions on sparse and asynchronous event streams. Ourwork is also close to superpixel-driven contrastive learn-ing , where pre-processed superpixels are used to",
  "$-$": ". Architecture overview of the OpenESS framework. We distill off-the-shelf knowledge from vision-languages models to eventrepresentations (cf. Sec. 3.1). Given a calibrated event Ievt and a frame Iimg, we extract their features from the event network F evte and thedensified CLIPs image encoder F clipc , which are then combined with the text embedding from CLIPs text encoder F txttfor open-worldprediction (cf. Sec. 3.2). To better serve for cross-modality knowledge transfer, we propose a frame-to-event (F2E) contrastive objective(cf. Sec. 3.3) via superpixel-driven distillation and a text-to-event (T2E) consistency objective (cf. Sec. 3.4) via scene-level regularization. establish contrastive objectives with modalities from othertasks, e.g., point cloud understanding , remote sensing, medical imaging , and so on. In this work, wepropose OpenESS to explore superpixel-to-event represen-tation learning. Extensive experiments verify that such anapproach is promising for annotation-efficient ESS.",
  ". Methodology": "Our study serves as an early attempt at leveraging vision-language foundation models like CLIP to learn mean-ingful event representations without accessing ground-truthlabels. We start with a brief introduction of the CLIP model(cf. Sec. 3.1), followed by a detailed elaboration on our pro-posed open-vocabulary ESS (cf. Sec. 3.2). To encourage ef-fective cross-modal event representation learning, we intro-duce a frame-to-event contrastive distillation (cf. Sec. 3.3)and a text-to-event consistency regularization (cf. Sec. 3.4).An overview of the OpenESS framework is shown in .",
  ". Revisiting CLIP": "CLIP learns to associate images with textual descrip-tions through a contrastive learning framework. It leveragesa dataset of 400 million image-text pairs, training an im-age encoder (based on a ResNet or Vision Transformer) and a text encoder (using a Transformer architecture) to project images and texts into a shared embeddingspace. Such a training paradigm enables CLIP to performzero-shot classification tasks, identifying images based on textual descriptions without specific training on those cate-gories. To achieve annotation-free classification on a cus-tom dataset, one needs to combine class label mappingswith hand-crafted text prompts as the input to generate thetext embedding. In this work, we aim to leverage the seman-tically rich CLIP feature space to assist open-vocabularydense prediction on sparse and asynchronous event streams.",
  ". Open-Vocabulary ESS": "Inputs. Given a set of N event data acquired by an eventcamera, we aim to segment each event ei among the tem-porally ordered event streams i, which are encoded bythe pixel coordinates (xi, yi), microsecond-level timestampti, and the polarity pi {1, +1} which indicates ei-ther an increase or decrease of the brightness. Each eventcamera pixel generates a spike whenever it perceives achange in logarithmic brightness that surpasses a predeter-mined threshold. Meanwhile, a conventional camera cap-tures gray-scale or color frames Iimgi R3HW whichare spatially aligned and temporally synchronized with theevents or can be aligned and synchronized to events via sen-sor calibration, where H and W are the spatial resolutions.Event Representations. Due to the sparsity, high tempo-ral resolution, and asynchronous nature of event streams, itis common to convert raw events i into more regular rep-resentations Ievti RCHW as the input to the neuralnetwork , where C denotes the number of embeddingchannels which is depended on the event representations themselves. Some popular choices of such embedding in-clude spatiotemporal voxel grids , frame-likereconstructions , and bio-inspired spikes . We in-vestigate these three methods and show an example of tak-ing voxel grids as the input in . More analyses andcomparisons using reconstructions and spikes are in latersections. Specifically, with a predefined number of events,each voxel grid is built from non-overlapping windows as:",
  "(1)where is the Kronecker delta function; tj = (B 1) tjt0": "Tis the normalized event timestamp with B as the number oftemporal bins in an event stream; T is the time windowand t0 denotes the time of the first event in the window.Cross-Modality Encoding.Let Fevte: RCHWRD1H1W1 be an event-based segmentation network withtrainable parameters e, which takes as input an eventembedding Ievtiand outputs a D1-dimensional feature ofdownsampled spatial sizes H1 and W1.Meanwhile, weintegrate CLIPs image encoder Fclipc: R3HWRD2H2W2 into our framework and keep the parametersc fixed. The output is a D2-dimensional feature of sizesH2 and W2. Our motivation is to transfer general knowl-edge from Fclipcto Fevte , such that the event branch canlearn useful representations without using dense event an-notations. To enable open-vocabulary ESS predictions, weleverage CLIPs text encoder Ftxttwith pre-trained param-eters t.The input of Ftxttcomes from predefined textprompt templates and the output will be a text embeddingextracted from CLIPs rich semantic space.Densifications. CLIP was originally designed for image-based recognition tasks and does not provide per-pixel out-puts for dense predictions. Several recent attempts exploredthe adaptation from global, image-level recognition to local,pixel-level prediction, via either model structure modifica-tion or fine-tuning . The former directlyreformulates the value-embedding layer in CLIPs imageencoder, while the latter uses semantic labels to graduallyadapt the pre-trained weights to generate dense predictions.In this work, we implement both solutions to densify CLIPsoutputs and compare their performances in our experiments.Up until now, we have presented a preliminary frame-work capable of conducting open-vocabulary ESS by lever-aging knowledge from the CLIP model. However, due tothe large domain gap between the event and image modali-ties, a nave adaptation is sub-par in tackling the challengingevent-based semantic scene understanding task.",
  "Since our objective is to encourage effective cross-modalityknowledge transfer for holistic event scene perception, it": "thus becomes crucial to learn meaningful representationsfor both thing and stuff classes, especially their boundaryinformation. However, the sparsity and asynchronous na-ture of event streams inevitably impede such objectives.Superpixel-Driven Knowledge Distillation. To pursue amore informative event representation learning at highergranularity, we propose to first leverage calibrated framesto generate coarse, instance-level superpixels and then dis-till knowledge from a pre-trained image backbone to theevent segmentation network. Superpixel groups pixels intoconceptually meaningful atomic regions, which can be usedas the basis for higher-level perceptions . Thesemantically coherent frame-to-event correspondences canthus be found using pre-processed or online-generated su-perpixels. Such correspondences tend to bridge the sparseevents to dense frame pixels in a holistic manner withoutinvolving extra training or annotation efforts.Superpixel & Superevent Generation. We resort to thefollowing two ways of generating the superpixels. The firstway is to leverage heuristic methods, e.g. SLIC , to effi-ciently groups pixels from frame Iimgiinto a total of Mslicsegments with good boundary adherence and regularity asIspi= {I1i , I2i , ..., IMslici}, where Mslic is a hyperparame-ter that needs to be adjusted based on the inputs. The gener-ated superpixels satisfy I1i I2i ...IMslici= {1, 2, ..., HW}. For the second option, we use the recent Segment Any-thing Model (SAM) which takes Iimgias the input andoutputs Msam class-agnostic masks. For simplicity, we useM to denote the number of superpixels used during knowl-edge distillation, i.e., {Ispi= {I1i , ..., Iki }|k = 1, ..., M}and show more comparisons between SLIC and SAM in later sections. Since Ievtiand Iimgihave been alignedand synchronized, we can group events from Ievtiinto su-perevents {V spi= {V1i , ..., Vli}|l = 1, ..., M} by using theknown event-pixel correspondences.Frame-to-Event Contrastive Learning. To encourage bet-ter superpixel-level knowledge transfer, we leverage a pre-trained image network Fimgf: R3HW RD3H3W3as the teacher and distill information from it to the eventbranch Fevte . The parameters of Fimgf, which can comefrom either CLIP or other pretext task pre-trained back-bones such as , are kept frozen during the distilla-tion. With Fevteand Fimgf, we generate the superevent andsuperpixel features as follows:",
  "(4)": "where , denotes the scalar product between the su-perevent and superpixel embedding; 1 > 0 is a temperaturecoefficient that controls the pace of knowledge transfer.Role in Our Framework. Our F2E contrastive distillationestablishes an effective pipeline for transferring superpixel-level knowledge from dense, visual informative frame pix-els to sparse, irregular event streams. Since we are targetingthe semantic segmentation task, the learned event represen-tations should be able to reason in terms of instances andinstance parts at and in between semantic boundaries.",
  ". T2E: Text-to-Event Consistency Regularization": "Although the aforementioned frame-to-event knowledgetransfer provides a simple yet effective way of transferringoff-the-shelf knowledge from frames to events, the opti-mization objective might encounter unwanted conflicts.Intra-Class Optimization Conflict. During the model pre-training, the superpixel-driven contrastive loss takes the cor-responding superevent and superpixel pair in a batch as thepositive pair, while treating all remaining pairs as negativesamples. Since heuristic superpixels only provide a coarsegrouping of conceptually coherent segments (kindly referto our Appendix for more detailed analysis), it is thus in-evitable to encounter self-conflict during the optimization.That is to say, from hindsight, there is a chance that thesuperpixels belonging to the same semantic class could beinvolved in both positive and negative samples.Text-Guided Semantic Regularization. To mitigate thepossible self-conflict in Eq. (4), we propose a text-to-eventsemantic consistency regularization mechanism that lever-ages CLIPs text encoder to generate semantically moreconsistent text-frame pairs {Iimgi, Ti}, where Ti denotes thetext embedding extracted from Ftxtt . Such a paired relation-ship can be leveraged via CLIP without additional training.We then construct event-text pairs {Ievti, Ti} by propagat-ing the alignment between events and frames. Specifically,the paired event and text features are extracted as follows:",
  ",(7)": "where 2 > 0 is a temperature coefficient that controls thepace of knowledge transfer. The overall optimization ob-jective of our OpenESS framework is to minimize L =LF 2E + LT 2E, where is a weight balancing coefficient.Role in Our Framework. Our T2E semantic consistencyregularization provides a global-level alignment to compen-sate for the possible self-conflict in the superpixel-drivenframe-to-event contrastive learning. As we will show in thefollowing sections, the two objectives work synergisticallyin improving the performance of open-vocabulary ESS.Inference-Time Configuration. Our OpenESS frameworkis designed to pursue segmentation accuracy in annotation-free and annotation-efficient manners, without sacrificingevent processing efficiency. As can be seen from ,after the cross-modality knowledge transfer, only the eventbranch will be kept. This guarantees that there will be noextra latency or power consumption added during the infer-ence, which is in line with the practical requirements.",
  ". Settings": "Datasets. We conduct experiments on two popular ESSdatasets. DDD17-Seg is a widely used ESS benchmarkconsisting of 40 sequences acquired by a DAVIS346B. Intotal, 15950 training and 3890 testing events of spatial size352 200 are used, along with synchronized gray-scaleframes provided by the DAVIS camera. DSEC-Semantic provides semantic labels for 11 sequences in the DSEC dataset. The training and testing splits contain 8082and 2809 events of spatial size 640 440, accompanied bycolor frames (with sensor calibration parameters available)recorded at 20Hz. More details are in the Appendix.Benchmark Setup. In addition to the conventional fully-supervised ESS, we establish two open-vocabulary ESS set-tings for annotation-free and annotation-efficient learning,respectively. The former aims to train an ESS model with-out using any dense event labels, while the latter assumesan annotation budget of 1%, 5%, 10%, or 20% of events inthe training set. We treat the first few samples from eachsequence as labeled and the remaining ones as unlabeled.Implementation Details. Our framework is implementedusing PyTorch . Based on the use of event represen-tations, we form frame2voxel, frame2recon, andframe2spike settings, where the event branch will adoptE2VID , ResNet-50 , and SpikingFCN , respec-tively, with an AdamW optimizer with cosine learningrate scheduler. The frame branch uses a pre-trained ResNet-50 and is kept frozen. The number of superpixels . Comparative study of existing ESS approaches underthe annotation-free, fully-supervised, and open-vocabulary ESSsettings, respectively, on the test sets of the DDD17-Seg andDSEC-Semantic datasets. All scores are in percentage (%).The best score from each learning setting is highlighted in bold.",
  "Open-Vocabulary ESSMaskCLIP ECCV2290.5061.2789.8155.01FC-CLIP NeurIPS2390.6862.0189.9755.67OpenESSOurs91.0563.0090.2157.21": "involved in the calculation of F2E contrastive loss is set to100 for DSEC-Semantic and 25 for DDD17-Seg .For evaluation, we extract the feature embedding for eachtext prompt offline from a frozen CLIP text encoder usingpre-defined templates. For linear probing, the pre-trainedevent network Fevteis kept frozen, followed by a trainablepoint-wise linear classification head. Due to space limits,kindly refer to our Appendix for additional details.",
  ". Comparative Study": "Annotation-Free ESS. In Tab. 1, we compare OpenESSwith MaskCLIP and FC-CLIP in the absenceof event labels.Our approach achieves zero-shot ESSresults of 53.93% and 43.31% on DDD17-Seg andDSEC-Semantic , much higher than the two competi-tors and even comparable to some fully-supervised meth-ods. This validates the effectiveness of conducting ESS inan annotation-free manner for practical usage. Meanwhile,we observe that a fine-tuned CLIP encoder could gen-erate much better semantic predictions than the structureadaptation method , as mentioned in Sec. 3.2.Comparisons to State-of-the-Art Methods. As shown inTab. 1, the proposed OpenESS sets up several new state-of-the-art results in the two ESS benchmarks. Compared to the",
  "MoCoV2": ". Ablation study on the number of superpixels (providedby either SAM or SLIC ) involved in calculating the frame-to-event contrastive loss. Models after pre-training are fine-tunedwith 1% annotations. All mIoU scores are in percentage (%). previously best-performing methods, OpenESS is 1.63%and 2.21% better in terms of mIoU scores on DDD17-Seg and DSEC-Semantic , respectively. It is worth men-tioning that in addition to the performance improvements,our approach can generate open-vocabulary predictions thatare beyond the closed sets of predictions of existing meth-ods, which is more in line with the practical usage.Annotation-Efficient Learning. We establish a compre-hensive benchmark for ESS under limited annotation sce-narios and show the results in Tab. 3. As can be seen, theproposed OpenESS contributes significant performance im-provements over random initialization under linear probing,few-shot fine-tuning, and fully-supervised learning settings.Specifically, using either voxel grid or event reconstructionrepresentation, our approach achieves > 30% relative gainsin mIoU on both datasets under liner probing and around2% higher than prior art in mIoU with full supervisions. Wealso observe that using voxel grids to represent raw eventstreams tends to yield overall better ESS performance.Qualitative Assessment. provides visual compar-isons between OpenESS and other approaches on DSEC-Semantic . We find that OpenESS tends to predict moreconsistent semantic information from sparse and irregularevent inputs, especially at instance boundaries. We includemore visual examples and failure cases in the Appendix.Open-World Predictions. One of the core advantages ofOpenESS is the ability to predict beyond the fixed label setfrom the original training sets. As shown in , our ap-proach can take arbitrary text prompts as inputs and gen-erate semantically coherent event predictions without usingevent labels. This is credited to the alignment between eventfeatures and CLIPs knowledge in T2E. Such a flexible wayof prediction enables a more holistic event understanding.Other Representation Learning Approaches. In Tab. 2,we compare OpenESS with recent reconstruction-based [3,",
  "EventReconstructionMaskCLIPESS-SupOpenESSFC-CLIPGT": ". Qualitative comparisons of state-of-the-art ESS approaches on the test set of DSEC-Semantic . Each color corresponds toa distinct semantic category. GT denotes the ground truth semantic maps. Best viewed in colors and zoomed-in for additional details. . Comparative study of different representation learningmethods applied on event data. OV denotes whether supportingopen-vocabulary predictions. All mIoU scores are in percentage(%). The best score from each dataset is highlighted in bold.",
  "OOD": ". Cross-dataset representation learning results of com-paring OpenESS pre-training using in-distribution (ID) and out-of-distribution (OOD) data in-between the DDD17-Seg andDSEC-Semantic datasets. Models after pre-training are fine-tuned with 1%, 5%, 10%, and 20% annotations, respectively. both F2E and T2E contribute to an overt improvement overrandom initialization under linear probing and few-shotfine-tuning settings, which verifies the effectiveness of ourproposed approach. Once again, we find that the voxel gridstend to achieve better performance than other representa-tions. The spike-based methods , albeit being compu-tationally more efficient, show sub-par performance com-pared to voxel grids and reconstructions.",
  "Superpixel Generation. We study the utilization of SLIC and SAM in our frame-to-event contrastive distilla-tion and show the results in . Using either frame net-": ". Comparative study of different open-vocabulary semantic segmentation methods under the linear probing (LP) andfew-shot fine-tuning, and full supervision (Full) settings, respectively, on the test sets of the DDD17-Seg and DSEC-Semantic datasets. All mIoU scores are given in percentage (%). The best mIoU scores from each learning configuration are highlighted in bold.",
  "mIoU (%)": ". Single-modality OpenESS representation learningstudy on the DSEC-Semantic dataset. The results are frommodels of random initialization (), recon2voxel pre-training(), and frame2voxel pre-training (), respectively, after lin-ear probing (LP) and annotation-efficient fine-tuning. Framework with Event Camera Only. Lastly, we studythe scenario where the frame camera becomes unavailable.We replace the input to the frame branch with event recon-structions and show the results in . Since the lim-ited visual cues from the reconstruction tend to degrade thequality of representation learning, its performance is sub-par compared to the frame-based knowledge transfer.",
  ". Conclusion": "In this work, we introduced OpenESS, an open-vocabularyevent-based semantic segmentation framework tailored toperform open-vocabulary ESS in an annotation-efficientmanner. We proposed to encourage cross-modality repre-sentation learning between events and frames using frame-to-event contrastive distillation and text-to-event semanticconsistency regularization. Through extensive experiments,we validated the effectiveness of OpenESS in tackling denseevent-based predictions. We hope this work could shed lighton the future development of more scalable ESS systems. Acknowledgement. This work is under the programme DesCartesand is supported by the National Research Foundation, Prime Min-isters Office, Singapore, under its Campus for Research Excel-lence and Technological Enterprise (CREATE) programme.",
  "A.1. Datasets": "In this study, we follow prior works by usingthe DDD17-Seg and DSEC-Semantic datasets forevaluating and validating the baselines, prior methods, andthe proposed OpenESS framework. Some specifications re-lated to these two datasets are listed as follows. DDD17-Seg serves as the first benchmark for ESS. Itis a semantic segmentation extension of the DDD17 dataset, which includes hours of driving data, capturinga variety of driving conditions such as different times ofday, traffic scenarios, and weather conditions. Alonso andMurillo provide the semantic labels on top of DDD17to enable event-based semantic segmentation.Specifi-cally, they proposed to use the corresponding gray-scaleimages along with the event streams to generate an ap-proximated set of semantic labels for training, which wasproven effective in training models to segment directlyon event-based data. A three-step procedure is applied:i) train a semantic segmentation model on the gray-scaleimages in the Cityscapes dataset ; ii) Use the trainedmodel to label the gray-scale images in DDD17; and iii) Conduct a post-processing step on the generated pseudolabels, including class merging and image cropping. Thedataset specification is shown in Tab. 5. In total, thereare 15950 training and 3890 test samples in the DDD17-Seg dataset.Each pixel is labeled across six seman-tic classes, including flat, background, object,vegetation, human, and vehicle. For each sam-ple, we convert the event streams into a sequence of 20voxel grids, each consisting of 32000 events and with aspatial resolution of 352 200. For additional details ofthis dataset, kindly refer to DSEC-Semantic is a semantic segmentation exten-sion of the DSEC (Driving Stereo Event Camera) dataset. DSEC is an extensive dataset designed for advanceddriver-assistance systems (ADAS) and autonomous driv-ing research, with a particular focus on event-based visionand stereo vision. Different from DDD17 , the DSECdataset combines data from event-based cameras and tra-ditional RGB cameras.The inclusion of event-basedcameras (which capture changes in light intensity) along-side regular cameras provides a rich, complementary datasource for perception tasks. The dataset typically featureshigh-resolution images and event data, providing detailedvisual information from a wide range of driving condi-tions, including urban, suburban, and highway environ-ments, various weather conditions, and different times ofthe day. This diversity is crucial for developing systemsthat can operate reliably in real-world conditions. Basedon such a rich collection, Sun et al. adopted a similarpseudo labeling procedure as DDD17-Seg and gener-ated the semantic labels for eleven sequences in DSEC,dubbed as DSEC-Semantic. The dataset specification isshown in Tab. 6. In total, there are 8082 training and2809 test samples in the DSEC-Semantic dataset. Eachpixel is labeled across eleven semantic classes, includingbackground, building, fence, person, pole,road, sidewalk, vegetation, car, wall, andtraffic-sign. For each sample, we convert the eventstreams into a sequence of 20 voxel grids, each consistingof 100000 events and with a spatial resolution of 640 440. For additional details of this dataset, kindly refer to",
  "A.2. Text Prompts": "To enable the conventional evaluation of our proposedopen-vocabulary approach on an event-based semanticsegmentation dataset, we need to use the pre-definedclass names as text prompts to generate the text embed-ding. Specifically, we follow the standard templates when generating the embedding. The dataset-specific textprompts defined in our framework are listed as follows.",
  "# Classes11 Classes11 Classes": "the DDD17-Seg dataset , with static and dynamic com-ponents of driving scenes. Our defined text prompts ofthis dataset are summarized in Tab. 7. For each semanticclass, we generate for each text prompt the text embed-ding using the CLIP text encoder and then average thetext embedding of all text prompts as the final embeddingof this class. DSEC-Semantic.There is a total of eleven semanticclasses in the DSEC-Semantic dataset , ranging fromstatic and dynamic components of driving scenes. Ourdefined text prompts of this dataset are summarized inTab. 8. For each semantic class, we generate for each textprompt the text embedding using the CLIP text encoderand then average the text embedding of all text promptsas the final embedding of this class.",
  "A.3. Superpixels": "In image processing and computer vision, superpixels canbe defined as a scheme that groups pixels in an image intoperceptually meaningful atomic regions, which are used toreplace the rigid structure of the pixel grid . Superpixelsprovide a more natural representation of the image struc-ture, often leading to more efficient and effective image pro-cessing. Here are some of their key aspects: Grouping Pixels. Superpixels are often formed by clus-tering pixels based on certain criteria like color similarity,brightness, texture, and other low-level patterns , ormore recently, semantics . This results in contiguousregions in the image that are more meaningful than indi-vidual pixels for many applications . Reducing Complexity. By aggregating pixels into su-perpixels, the complexity of image data is significantlyreduced . This reduction helps in speeding up subse-quent image processing tasks, as algorithms have fewerelements (superpixels) to process compared to the poten-tially millions of pixels in an image. Preserving Edges. One of the primary goals of super-pixel segmentation is to preserve important image edges.Superpixels often adhere closely to the boundaries of ob-jects in the image, making them useful for tasks that relyon accurate edge information, like object recognition andscene understanding.In this work, we propose to first leverage calibratedframes to generate coarse, instance-level superpixels andthen distill knowledge from a pre-trained image backboneto the event segmentation network. Specifically, we resortto the following two ways to generate the superpixels. SLIC. The first way is to leverage the heuristic Sim-ple Linear Iterative Clustering (SLIC) approach toefficiently group pixels from frame Iimgiinto a to-tal of Mslic segments with good boundary adherenceand regularity.The superpixels are defined as Ispi={I1i , I2i , ..., IMslici}, where Mslic is a hyperparameterthat needs to be adjusted based on the inputs.Thegenerated superpixels satisfy I1i I2i ... IMslici={1, 2, ..., H W}.Several examples of the SLIC-generated superpixels are shown in the second row of, where each of the color-coded patches representsone distinct and semantically coherent superpixel.",
  "10traffic-signtraffic-sign, parking-sign, direction-sign, traffic-sign without pole, trafficlight box": "ment Anything Model (SAM) which takes Iimgiasthe input and outputs Msam class-agnostic masks. Forsimplicity, we use M to denote the number of super-pixels used during knowledge distillation, i.e., {Ispi={I1i , ..., Iki }|k = 1, ..., M}.Several examples of theSAM-generated superpixels are shown in the third row of , where each of the color-coded patches representsone distinct and semantically coherent superpixel.We calculate the SLIC and SAM superpixel distribu-tions on the training set of the DSEC-Semantic dataset and show the corresponding statistics in .As canbe observed, the SLIC-generated superpixels often contain",
  ". The statistical distributions of superpixels generated by SLIC (subfigure a) and SAM (subfigure b)": "more low-level visual cues, such as color similarity, bright-ness, and texture. On the contrary, superpixels generatedby SAM exhibit clear semantic coherence and often depictthe boundaries of objects and backgrounds. As verified inthe main body of this paper, the semantically richer SAMsuperpixels bring higher performance gains in our Frame-to-Event Contrastive Learning framework.",
  "Meanwhile, we provide more fine-grained examples ofthe SLIC algorithm using different Mslic, i.e., 25, 50, 100,150, and 200. The results are shown in . Specifically,": "the number of superpixels Mslic should reflect the complex-ity and detail of the image. For images with high detailor complexity (like those with many objects or textures), alarger Mslic can capture more of this detail. Conversely, forsimpler images, fewer superpixels might be sufficient. Usu-ally, more superpixels mean smaller superpixels. Smallersuperpixels can adhere more closely to object boundariesand capture finer details, but they might also capture morenoise. Fewer superpixels result in larger, more homoge-neous regions but may lead to a loss of detail, especially",
  "SLIC = 150SLIC = 200": ". Examples of superpixels generated by SLIC with different numbers of superpixels Mslic (25, 50, 100, 150, and 200). Eachcolored patch represents one distinct and semantically coherent superpixel. Best viewed in colors. at the edges of objects. The choice also depends on thespecific application.For instance, in object detection orsegmentation tasks where boundary adherence is crucial,a higher number of superpixels might be preferable.Incontrast, for tasks like image compression or abstraction,fewer superpixels might be more appropriate. Often, theoptimal number of superpixels is determined empirically. This involves experimenting with different values and eval-uating the results based on the specific criteria of the taskor application. In our event-based semantic segmentationtask, we choose Mslic = 100 for our Frame-to-Event Con-trastive Learning on the DSEC-Semantic dataset , andMslic = 25 on the DDD17-Seg dataset .",
  "A.4. Backbones": "As mentioned in the main body of this paper, we estab-lish three open-vocabulary event-based semantic segmenta-tion settings based on the use of three different event rep-resentations, i.e., frame2voxel, frame2recon, andframe2spike. It is worth noting that these three eventrepresentations tend to have their own advantages.We supplement additional implementation details re-garding the used event representations as follows. Frame2Voxel. For the use of voxel grids as the event em-bedding, we follow Sun et al. by converting the rawevents i into the regular voxel grids Ivoxi RCHW as the input to the event-based semantic segmentation net-work. This representation is intuitive and aligns well withconventional event camera data processing techniques. Itis suitable for convolutional neural networks as it main-tains spatial and temporal relationships. Specifically, witha predefined number of events, each voxel grid is builtfrom non-overlapping windows as follows:Ivoxi=",
  "(8)where is the Kronecker delta function; tj = (B 1) tjt0": "Tis the normalized event timestamp with B as thenumber of temporal bins in an event stream; T is thetime window and t0 denotes the time of the first event inthe window. It is worth noting that voxel grids can bememory-intensive, especially for high-resolution sensorsor long-time windows. They might also introduce quan-tization errors due to the discretization of space and time.For additional details on the use of voxel grids, kindlyrefer to Frame2Recon. For the use of event reconstructions asthe event embedding, we follow Sun et al. and Re-becq et al. by converting the raw events i into theregular frame-like event reconstructions Ireci RHW as the input to the event-based semantic segmentation net-work. This can be done by accumulating events over shorttime intervals or by using algorithms to interpolate or sim-ulate frames. This approach is compatible with standardimage processing techniques and algorithms developedfor frame-based vision. It is more familiar to practitionersused to working with conventional cameras. In this work,we adopt the E2VID model to generate the event re-constructions. This process can be described as follows:",
  "where Ivoxkdenotes the voxel grids as defined in Eq. (8);": "Ee2vid and De2vid are the encoder of decoder of the E2VIDmodel , respectively. It is worth noting that event re-constructions can lose the fine temporal resolution thatevent cameras provide. They might also introduce arti-facts or noise, especially in scenes with fast-moving ob-jects or low event rates.For additional details on theuse of event reconstructions, kindly refer to Frame2Spike. For the use of spikes as the event embed-ding, we follow Kim et al. by converting the rawevents i into spikes Ispki RHW as the input to theevent-based semantic segmentation network. The spikerepresentation keeps the data in its raw form as indi-vidual spikes or events. This representation preserves thehigh temporal resolution of the event data and is highlyefficient in terms of memory and computation, especiallyfor sparse scenes. The rate coding is used as the spikeencoding scheme due to its reliable performance acrossvarious tasks.Each pixel value with a random num-ber ranging between [smin, smax] at every time step isrecorded, where smin and smax are the minimum andmaximum possible pixel intensities, respectively. If therandom number is greater than the pixel intensity, thePoisson spike generator outputs a spike with amplitude1. Otherwise, the Poisson spike generator does not yieldany spikes. The spikes in a certain time window are ac-cumulated to generate a frame, where such frames willserve as the input to the event-based semantic segmen-tation network. It is worth noting that processing rawspike data requires specialized algorithms, often inspiredby neuromorphic computing.It might not be suitablefor traditional image processing techniques and can bechallenging to interpret and visualize. For additional de-tails on the use of spikes, kindly refer to To sum up, each event representation has its unique char-acteristics and is suitable for different applications or pro-cessing techniques. Our proposed OpenESS framework iscapable of leveraging each of the above event representa-tions for efficient and accurate event-based semantic seg-mentation in an annotation-free and open-vocabulary man-ner. Such a versatile and flexible way of learning verifies thebroader application potential of our proposed framework.",
  "Acc": "Annotation-Free ESSMaskCLIP 21.9726.4552.590.200.044.1965.762.9648.0240.670.670.0858.96FC-CLIP 39.4287.4969.6814.3917.530.2971.7634.5671.3063.192.980.5079.20OpenESS (Ours)43.3192.5374.2211.960.000.4187.3255.0974.2364.257.988.4786.18 . The per-class segmentation results of annotation-efficient event-based semantic segmentation approaches on the test set ofDSEC-Semantic . All approaches adopted the frame2voxel representation. Scores reported are IoUs in percentage (%). For eachsemantic class under each experimental setting, the best score in each column is highlighted in bold.",
  "i=1IoUi ,(12)": "where C is the number of classes and IoUi denotes thescore of class i. mIoU provides a balanced measure sinceeach class contributes equally to the final score, regardlessof its size or frequency in the dataset. A higher mIoU indi-cates better semantic segmentation performance. A score of1 would indicate perfect segmentation for all classes, whilea score of 0 would imply an absence of correct predictions.In this work, all the compared approaches adopt the samemIoU calculation as in the ESS benchmarks . Addi-tionally, we also report the semantic segmentation accuracy(Acc) for the baselines and the proposed framework.",
  "B.1. Annotation-Free ESS": "The per-class zero-shot event-based semantic segmentationresults are shown in Tab. 9.For almost every seman-tic class, we observe that the proposed OpenESS achievesmuch higher IoU scores than MaskCLIP and FC-CLIP . This validates the effectiveness of OpenESS forconducting efficient and accurate event-based semantic seg-mentation without using either the event or frame labels.",
  "B.2. Annotation-Efficient ESS": "The per-class linear probing event-based semantic segmen-tation results are shown in the first block of Tab. 10 andTab. 11. Specifically, compared to the random initializationbaseline, a self-supervised pre-trained network always pro-vides better features. The quality of representation learningoften determines the linear probing performance. The net-work pre-trained using our frame-to-event contrastive distil-lation and text-to-event consistency regularization tends toachieve higher event-based semantic segmentation resultsthan MaskCLIP and FC-CLIP . Notably, suchimprovements are holistic across almost all eleven semanticclasses in the dataset. These results validate the effective-ness of the proposed OpenESS framework in tackling thechallenging event-based semantic segmentation task.The per-class annotation-efficient event-based seman-tic segmentation results of the frame2vodel andframe2recon settings under 1%, 5%, 10%, and 20%annotation budgets are shown in Tab. 10 and Tab. 11, re-spectively. Similar to the findings and conclusions drawnabove, we observe clear superiority of the proposed Ope-nESS framework over the random initialization, MaskCLIP, and FC-CLIP approaches.Such consistentperformance improvements validate again the effectivenessand superiority of the proposed frame-to-event contrastivedistillation and text-to-event consistency regularization. Wehope our framework can lay a solid foundation for future",
  "C.1. Open-Vocabulary Examples": "The key advantage of our proposed OpenESS framework isits capability to leverage open-world vocabularies from theCLIP text embedding space. Unlike prior event-based se-mantic segmentation, which relies on pre-defined and fixedcategories, our open-vocabulary segmentation aims to un-derstand and categorize image regions into a broader, poten-tially unlimited range of categories. We provide more open-vocabulary examples in . As can be observed, givenproper text prompts like road, sidewalk, and build-ing, our proposed OpenESS framework is capable of gen-erating semantically meaningful attention maps for depict-ing the corresponding regions. Such a flexible frameworkcan be further adapted to new or unseen categories withoutthe need for extensive retraining, which is particularly bene-ficial in dynamic environments where new objects or classesmight frequently appear. Additionally, the open-vocabularysegmentation pipeline allows users to work with a more ex-tensive range of objects and concepts, enhancing the userexperience and interaction capabilities.",
  "C.2. Visual Comparisons": "In this section, we provide more qualitative comparisonsof our proposed OpenESS framework over prior works on the DSEC-Semantic dataset. Specifically, thevisual comparisons are shown in and . Ascan be observed, OpenESS shows superior event-based se-mantic segmentation performance over prior works acrossa wide range of event scenes under different lighting andweather conditions. Such consistent segmentation perfor-mance improvements provide a solid foundation to validatethe effectiveness and superiority of the proposed frame-to-event contrastive distillation and text-to-event consis-tency regularization.For additional qualitative compar-isons, kindly refer to Appendix C.4.",
  "C.3. Failure Cases": "As can be observed from , , and ,the existing event-based semantic segmentation approachesstill have room for further improvements. Similar to theconventional semantic segmentation task, it is often hardto accurately segment the boundaries between the seman-tic objects and backgrounds. In the context of event-basedsemantic segmentation, such a problem tends to be partic-ularly overt. Unlike traditional cameras that capture dense, . The per-class segmentation results of annotation-efficient event-based semantic segmentation approaches on the test set ofDSEC-Semantic . All approaches adopted the frame2recon representation. Scores reported are IoUs in percentage (%). For eachsemantic class under each experimental setting, the best score in each column is highlighted in bold.",
  "Random39.2587.1461.806.773.5113.1988.5356.1261.9544.651.296.8482.51": "MaskCLIP 43.3789.8369.807.078.9310.6788.8852.6570.7160.033.1015.3985.69FC-CLIP 47.1891.2071.3911.5324.929.6091.5863.8871.5263.447.5512.3687.07OpenESS (Ours)49.7491.2873.4310.6927.1813.8592.8467.5974.2069.2210.6216.2188.26 synchronous frames, event cameras generate sparse, asyn-chronous events, which brings extra difficulties for accurateboundary segmentation. Meanwhile, the current frameworkfinds it hard to accurately predict the minor classes, suchas fence, pole, wall, and traffic-sign. We believe these arepotential directions that future works can explore to fur-ther improve the event-based semantic segmentation perfor-mance on top of existing frameworks.",
  "C.4. Video Demos": "In addition to the qualitative examples shown in the mainbody and this supplementary file, we also provide severalvideo clips to further validate the effectiveness and supe-riority of the proposed approach. Specifically, we providethree video demos in the attachment, named demo1.mp4,demo2.mp4, and demo3.mp4. The first two video de-mos show open-vocabulary event-based semantic segmen-tation examples using the class names and open-world vo-cabularies as the input text prompts, respectively. The thirdvideo demo contains qualitative comparisons of the seman- tic segmentation predictions among our proposed OpenESSand prior works.All the provided video sequences val-idate again the unique advantage of the proposed open-vocabulary event-based semantic segmentation framework.Kindly refer to our GitHub repository1 for additional detailson accessing these video demos.",
  "D.1. Positive Societal influence": "Event-based cameras can capture extremely fast motionsthat traditional cameras might miss, making them ideal fordynamic environments. In robotics, this leads to better ob-ject detection and scene understanding, enhancing the ca-pabilities of robots in the manufacturing, healthcare, and service industries. In autonomous driving, event-based se-mantic segmentation provides high temporal resolution andlow latency, which is crucial for detecting sudden changesin the environment. This can lead to faster and more accu-rate responses, potentially reducing accidents and enhanc-ing road safety. Our proposed OpenESS is designed to re-duce the annotation budget and training burden of existingevent-based semantic segmentation approaches. We believesuch an efficient way of learning helps increase the scala-bility of event-based semantic segmentation systems and inturn contributes positively to impact society by enhancingsafety, efficiency, and performance in various aspects.",
  "D.2. Potential Limitation": "Although our proposed framework is capable of conductingannotation-free and open-vocabulary event-based semanticsegmentation and achieves promising performance, theretend to exist several potential limitations. Firstly, our cur-rent framework requires the existence of synchronized eventand RGB cameras, which might not be maintained by someolder event camera systems. Secondly, we directly adoptthe standard text prompt templates to generate the text em-bedding, where a more sophisticated design could furtherimprove the open-vocabulary learning ability of the existingframework. Thirdly, there might still be some self-conflictproblems in our frame-to-event contrastive distillation andtext-to-event consistency regularization. The design of abetter representation learning paradigm on the event-baseddata could further resolve these issues. We believe these arepromising directions that future works can explore to fur-ther improve the current framework.",
  "E.1. Public Datasets Used": "We acknowledge the use of the following public datasets,during the course of this work: DSEC2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY-SA 4.0 DSEC-Semantic3 . . . . . . . . . . . . . . . . . . . . . . CC BY-SA 4.0 DDD174 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY-SA 4.0 DDD17-Seg5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown E2VID-Driving6 . . . . . GNU General Public License v3.0",
  "We acknowledge the use of the following public implemen-tations, during the course of this work:": "ESS7 . . . . . . . . . . . . . . . GNU General Public License v3.0 E2VID8 . . . . . . . . . . . . .GNU General Public License v3.0 HMNet9 . . . . . . . . . . . . . . . . . . . . . . . BSD 3-Clause License EV-SegNet10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Unknown SNN-Segmentation11 . . . . . . . . . . . . . . . . . . . . . . . Unknown CLIP12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License MaskCLIP13 . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 FC-CLIP14 . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 SLIC-Superpixels15 . . . . . . . . . . . . . . . . . . . . . . . . Unknown Segment-Anything16 . . . . . . . . . . . . . . .Apache License 2.0",
  ". Qualitative comparisons (2/2) among different ESS approaches on the test set of DSEC-Semantic . Best viewed in colors": "Radhakrishna Achanta, Appu Shaji, Kevin Smith, AurelienLucchi, Pascal Fua, and Sabine Susstrunk. Slic superpix-els compared to state-of-the-art superpixel methods. IEEETransactions on Pattern Analysis and Machine Intelligence,34(11):22742282, 2012. 4, 6, 7, 10, 12, 13 Inigo Alonso and Ana C. Murillo. Ev-segnet: Semanticsegmentation for event-based cameras. In IEEE/CVF Con-ference on Computer Vision and Pattern Recognition Work-shops, pages 110, 2019. 1, 2, 5, 6, 8, 9, 10, 11, 13, 16",
  "Jonathan Binas, Daniel Neil, Shih-Chii Liu, and Tobi Del-bruck. Ddd17: End-to-end davis driving dataset. In In-ternational Conference on Machine Learning Workshops,pages 19, 2017. 2, 6, 7, 8, 9": "Shristi Das Biswas, Adarsh Kosta, Chamika Liyanagedera,Marco Apolinario, and Kaushik Roy. Halsie: Hybrid ap-proach to learning segmentation by simultaneously exploit-ing image and event modalities. In IEEE/CVF Winter Con-ference on Applications of Computer Vision, 2024. 1, 2,6 Vincent Brebion, Julien Moreau, and Franck Davoine.Real-time optical flow for vehicular perception with low-and high-resolution event cameras.IEEE Transactionson Intelligent Transportation Systems, 23(9):1506615078,2021. 2 Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,Piotr Bojanowski, and Armand Joulin. Unsupervised learn-ing of visual features by contrasting cluster assignments. InAdvances in Neural Information Processing Systems, pages99129924, 2020. 4, 5, 8 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin.Emerging properties in self-supervised vision transformers.In IEEE/CVF International Conference on Computer Vi-sion, pages 96509660, 2021. 5, 8 Kaiwei Che, Luziwei Leng, Kaixuan Zhang, JianguoZhang, Qinghu Meng, Jie Cheng, Qinghai Guo, and Jianx-ing Liao. Differentiable hierarchical and surrogate gradientsearch for spiking neural networks. In Advances in Neu-ral Information Processing Systems, pages 2497524990,2022. 2 Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-age segmentation with deep convolutional nets, atrous con-volution, and fully connected crfs. IEEE Transactions onPattern Analysis and Machine Intelligence, 40(4):834848,2017. 1",
  "atrous separable convolution for semantic image segmenta-tion. In European Conference on Computer Vision, pages801818, 2018. 1": "Runnan Chen, Youquan Liu, Lingdong Kong, NenglunChen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wen-ping Wang. Towards label-free scene understanding by vi-sion foundation models. In Advances in Neural InformationProcessing Systems, 2023. 2, 10 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-offrey Hinton. A simple framework for contrastive learningof visual representations. In International Conference onMachine Learning, pages 15971607, 2020. 7",
  "Hoonhee Cho, Jegyeong Cho, and Kuk-Jin Yoon. Learn-ing adaptive dense event stereo from the image domain.In IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1779717807, 2023. 2": "Hoonhee Cho, Hyeonseong Kim, Yujeong Chae, and Kuk-Jin Yoon.Label-free event-based object recognition viajoint learning with image reconstruction from events. InIEEE/CVF International Conference on Computer Vision,pages 1986619877, 2023. 2 Marius Cordts, Mohamed Omran, Sebastian Ramos, TimoRehfeld, Markus Enzweiler, Rodrigo Benenson, UweFranke, Stefan Roth, and Bernt Schiele.The cityscapesdataset for semantic urban scene understanding.InIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 32133223, 2016. 1, 9 Javier Cuadrado, Ulysse Rancon, Benoit R. Cottereau,Francisco Barranco, and Timothee Masquelier.Opticalflow estimation from event-based cameras and spiking neu-ral networks. Frontiers in Neuroscience, 17:1160034, 2023.2",
  "Zheng Ding, Jieke Wang, and Zhuowen Tu.Open-vocabulary panoptic segmentation with maskclip.arXivpreprint arXiv:2208.08984, 2022. 2": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold,Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-age is worth 16x16 words: Transformers for image recog-nition at scale. In International Conference on LearningRepresentations, 2021. 3 Burak Ercan, Onur Eker, Aykut Erdem, and Erkut Erdem.Evreal: Towards a comprehensive benchmark and analysissuite for event-based video reconstruction. In IEEE/CVFConference on Computer Vision and Pattern RecognitionWorkshops, pages 39423951, 2023. 2",
  "Burak Ercan, Onur Eker, Canberk Saglam, Aykut Erdem,and Erkut Erdem.Hypere2vid: Improving event-basedvideo reconstruction via hypernetworks.arXiv preprintarXiv:2305.06382, 2023. 2": "Guillermo Gallego,Tobi Delbruck,Garrick Orchard,Chiara Bartolozzi, Brian Taba, Andrea Censi, StefanLeutenegger, Andrew J. Davison, Jorg Conradt, KostasDaniilidis, and Davide Scaramuzza.Event-based vision:A survey. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 44(1):154180, 2022. 1, 2, 3 Mingfei Gao, Chen Xing, Juan Carlos Niebles, Junnan Li,Ran Xu, Wenhao Liu, and Caiming Xiong. Open vocab-ulary object detection with pseudo bounding-box labels.In European Conference on Computer Vision Workshops,pages 266282, 2022. 2",
  "Daniel Gehrig and Davide Scaramuzza.Are high-resolution event cameras really needed?arXiv preprintarXiv:2203.14672, 2022. 1": "Daniel Gehrig, Antonio Loquercio, Konstantinos G. Der-panis, and Davide Scaramuzza.End-to-end learning ofrepresentations for asynchronous event-based data.InIEEE/CVF International Conference on Computer Vision,pages 56335643, 2019. 2, 4 Daniel Gehrig, Mathias Gehrig, Javier Hidalgo-Carrio, andDavide Scaramuzza.Video to events: Recycling videodatasets for event cameras. In IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 35863595, 2020. 2, 6",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 770778, 2016. 1, 3, 5": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, PiotrDollar, and Ross Girshick. Masked autoencoders are scal-able vision learners.In IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1600016009,2022. 7 Shuting He, Henghui Ding, and Wei Jiang. Primitive gen-eration and semantic-related alignment for universal zero-shot segmentation. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1123811247, 2023.2",
  "Kunping Huang, Sen Zhang, Jing Zhang, and DachengTao. Event-based simultaneous localization and mapping:A comprehensive survey. arXiv preprint arXiv:2304.09793,2023. 1": "Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and EhsanElhamifar. Open-vocabulary instance segmentation via ro-bust cross-modal pseudo-labeling. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pages70207031, 2022. 2 Olivier J. Henaff, Skanda Koppula, Jean-Baptiste Alayrac,Aaron Van den Oord, Oriol Vinyals, and Joao Carreira.Efficient visual pretraining with contrastive detection. InIEEE/CVF International Conference on Computer Vision,pages 1008610096, 2021. 2 Zexi Jia, Kaichao You, Weihua He, Yang Tian, YongxiangFeng, Yaoyuan Wang, Xu Jia, Yihang Lou, Jingyi Zhang,Guoqi Li, and Ziyang Zhang. Event-based semantic seg-mentation with posterior attentio. IEEE Transactions onImage Processing, 32:18291842, 2023. 2, 6, 9 Junho Kim, Jaehyeok Bae, Gangin Park, Dongsu Zhang,and Young Min Kim.N-imagenet:Towards robust,fine-grained object recognition with event cameras.InIEEE/CVF International Conference on Computer Vision,pages 21462156, 2021. 2 Youngeun Kim, Joshua Chough, and Priyadarshini Panda.Beyond classification: Directly training spiking neural net-works for semantic segmentation. Neuromorphic Comput-ing and Engineering, 2(4):044015, 2022. 2, 4, 5, 6, 7, 14 Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao, SpencerWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar,and Ross Girshick. Segment anything. In IEEE/CVF In-ternational Conference on Computer Vision, pages 40154026, 2023. 2, 4, 6, 7, 10, 11, 12",
  "Boyi Li, Kilian Q Weinberger, Serge Belongie, VladlenKoltun, and Rene Ranftl. Language-driven semantic seg-mentation. In International Conference on Learning Rep-resentations, 2022. 2, 4": "Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, LuYuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, andJianfeng Gao. Grounded language-image pre-training. InIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1096510975, 2022. 2 Yijin Li, Zhaoyang Huang, Shuo Chen, Xiaoyu Shi, Hong-sheng Li, Hujun Bao, Zhaopeng Cui, and Guofeng Zhang.Blinkflow: A dataset to push the limits of event-based op-tical flow estimation.arXiv preprint arXiv:2303.07716,2023. 2",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight de-cay regularization. In International Conference on Learn-ing Representations, 2019. 5": "Ana I. Maqueda, Antonio Loquercio, Guillermo Gallego,Narciso Garca, and Davide Scaramuzza. Event-based vi-sion meets deep learning on steering prediction for self-driving cars. In IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 54195427, 2018. 2 Nico Messikommer, Daniel Gehrig, Mathias Gehrig, andDavide Scaramuzza. Bridging the gap between events andframes through unsupervised domain adaptation.IEEERobotics and Automation Letters, 7(2):35153522, 2022.2, 6 Mohammad Mostafavi, Kuk-Jin Yoon, and Jonghyun Choi.Event-intensity stereo: Estimating depth by the best of bothworlds. In IEEE/CVF International Conference on Com-puter Vision, pages 42584267, 2021. 2",
  "ing neural networks. IEEE Signal Processing Magazine, 36(6):5163, 2019. 2": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Rus-sell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, HuXu, Herve Jegou, Julien Mairal, Patrick Labatut, ArmandJoulin, and Piotr Bojanowski.Dinov2:Learning ro-bust visual features without supervision.arXiv preprintarXiv:2304.07193, 2023. 4 Tianbo Pan, Zidong Cao, and Lin Wang. Srfnet: Monoc-ular depth estimation with fine-grained structure via spa-tial reliability-oriented fusion of frames and events. arXivpreprint arXiv:2309.12842, 2023. 2 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: Animperative style, high-performance deep learning library. InAdvances in Neural Information Processing Systems, 2019.5 Xidong Peng, Runnan Chen, Feng Qiao, Lingdong Kong,Youquan Liu, Tai Wang, Xinge Zhu, and Yuexin Ma. Learn-ing to adapt sam for segmenting cross-domain point clouds.arXiv preprint arXiv:2310.08820, 2023. 2, 10",
  "Ulysse Rancon, Javier Cuadrado-Anibarro, Benoit R. Cot-tereau, and Timothee Masquelier. Stereospike: Depth learn-ing with a spiking neural network.IEEE Access, 10:127428127439, 2022. 2": "Yongming Rao, Wenliang Zhao, Guangyi Chen, YansongTang, Zheng Zhu, Guan Huang, Jie Zhou, and JiwenLu.Denseclip: Language-guided dense prediction withcontext-aware prompting.In IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1808218091, 2022. 4 Henri Rebecq, Timo Horstschafer, Guillermo Gallego, andDavide Scaramuzza.Evo:A geometric approach toevent-based 6-dof parallel tracking and mapping in realtime.IEEE Robotics and Automation Letters, 2(2):593600, 2016. 2",
  "with an event camera. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 43(6):19641980, 2019. 2,4, 5, 6, 8, 14": "Corentin Sautier, Gilles Puy, Spyros Gidaris, AlexandreBoulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidarself-supervised distillation for autonomous driving data. InIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 98919901, 2022. 2 Stephan Schraml, Ahmed Nabil Belbachir, and HorstBischof.Event-driven stereo matching for real-time 3dpanoramic vision. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 466474, 2015. 2 Bongki Son, Yunjae Suh, Sungho Kim, Heejae Jung, Jun-Seok Kim, Changwoo Shin, Keunju Park, Kyoobin Lee,Jinman Park, Jooyeon Woo, Yohan Roh, Hyunku Lee, Yib-ing Wang, Ilia Ovsiannikov, and Hyunsurk Ryu. A 640480 dynamic vision sensor with a 9m pixel and 300mepsaddress-event representation. In IEEE International Solid-State Circuits Conference, 2017. 1 Lea Steffen, Daniel Reichard, Jakob Weinland, JacquesKaiser, Arne Roennau, and Rudiger Dillmann. Neuromor-phic stereo vision: A survey of bio-inspired sensors andalgorithms. Frontiers in Neuroscience, 13:28, 2019. 2",
  "David Stutz, Alexander Hermans, and Bastian Leibe. Su-perpixels: An evaluation of the state-of-the-art. ComputerVision and Image Understanding, 166:127, 2018. 10": "Zhaoning Sun, Nico Messikommer, Daniel Gehrig, and Da-vide Scaramuzza. Ess: Learning event-based semantic seg-mentation from still images. In European Conference onComputer Vision, pages 341357, 2022. 1, 2, 5, 6, 7, 8, 9,10, 11, 13, 14, 15, 16, 17, 20, 21 Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In Advancesin Neural Information Processing Systems, 2017. 3",
  "Zhexiong Wan, Yuchao Dai, and Yuxin Mao.Learningdense and continuous optical flow from an event camera.IEEE Transactions on Image Processing, 31:72377251,2022. 2": "Jiacheng Wang, Xiaomeng Li, Yiming Han, Jing Qin, Lian-sheng Wang, and Zhou Qichao. Separated contrastive learn-ing for organ-at-risk and gross-tumor-volume segmentationwith limited annotation. In AAAI Conference on ArtificialIntelligence, pages 24592467, 2022. 3 Lin Wang, Yujeong Chae, and Kuk-Jin Yoon. Dual trans-fer learning for event-based end-task prediction via plug-gable event to image translation.In IEEE/CVF Interna-tional Conference on Computer Vision, pages 21352145,2021. 2, 6 Lin Wang, Yujeong Chae, Sung-Hoon Yoon, Tae-KyunKim, and Kuk-Jin Yoon. Evdistill: Asynchronous eventsto end-task learning via bidirectional reconstruction-guidedcross-modal knowledge distillation. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pages608619, 2021. 2, 6",
  "Shu Wang, Huchuan Lu, Fan Yang, and Ming-Hsuan Yang.Superpixel tracking. In IEEE/CVF International Confer-ence on Computer Vision, pages 13231330, 2011. 4": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.Pyramid vision transformer: A versatile backbone for denseprediction without convolutions.In IEEE/CVF Interna-tional Conference on Computer Vision, pages 568578,2021. 6 Jianzong Wu, Xiangtai Li, Henghui Ding, Xia Li, Guan-gliang Cheng, Yunhai Tong, and Chen Change Loy. Be-trayed by captions: Joint caption grounding and generationfor open vocabulary instance segmentation. arXiv preprintarXiv:2301.00805, 2023. 2 Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, HenghuiDing, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong,Xudong Jiang, Bernard Ghanem, and Dacheng Tao. To-wards open vocabulary learning: A survey. arXiv preprintarXiv:2306.15880, 2023. 2 Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, andChen Change Loy.Aligning bag of regions for open-vocabulary object detection. In IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1525415264, 2023. 2",
  "Ziyi Wu, Xudong Liu, and Igor Gilitschenski. Eventclip:Adapting clip for event-based object recognition.arXivpreprint arXiv:2306.06354, 2023. 2": "Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xi-aolong Wang, and Shalini De Mello.Open-vocabularypanoptic segmentation with text-to-image diffusion models.In IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 29552966, 2023. 2 Jingyi Xu, Weidong Yang, Lingdong Kong, Youquan Liu,Rui Zhang, Qingyuan Zhou, and Ben Fei.Visual foun-dation models boost cross-modal unsupervised domainadaptation for 3d semantic segmentation. arXiv preprintarXiv:2403.10001, 2024. 2, 10",
  "Yan Yang, Liyuan Pan, and Liu Liu. Event camera data pre-training. In IEEE/CVF International Conference on Com-puter Vision, pages 1069910709, 2023. 7": "Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, WeiZhang, Zhenguo Li, and Hang Xu.Detclipv2: Scalableopen-vocabulary object detection pre-training via word-region alignment. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 2349723506, 2023.2 Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, andLiang-Chieh Chen.Convolutions die hard:Open-vocabulary segmentation with single frozen convolutionalclip. In Advances in Neural Information Processing Sys-tems, 2023. 2, 4, 6, 8, 15, 16, 17"
}