{
  "Abstract": "Text-conditionedstyletransferenablesuserstocommunicate their desired artistic styles through textdescriptions, offering a new and expressive means ofachieving stylization. In this work, we evaluate the text-conditioned image editing and style transfer techniqueson their fine-grained understanding of user prompts forprecise local style transfer. We find that current methodsfail to accomplish localized style transfers effectively, eitherfailing to localize style transfer to certain regions in theimage, or distorting the content and structure of the inputimage. To this end, we develop an end-to-end pipeline forlocal style transfer tailored to align with users intent.Further, we substantiate the effectiveness of our approachthrough quantitative and qualitative analysis. The projectcode is available at:",
  ". Introduction": "Text-conditioned style transfer is an exciting area ofresearch, with the potential to provide end users the creativefreedom to express abstract, artistic, or texture styles in free-form text. There has been a considerable progress on thisfront , however existing methods tend to imposea uniform style on the entire image based on a target styledescription.Users might seek distinct styles for variousregions within an image, for e.g., by providing a descriptionlike apply cubism style to the building in the image, theuser indicates the preference for building in cubism style,while keeping the rest of the image unchanged.Thus, we introduce a framework that integrates spatialnuances from user-provided style descriptions into thestyle transfer process. Our proposed end-to-end pipelinecomprises two main stages: First, we extract the region inthe image that the user wants to stylize (for e.g., building),and the corresponding style (for e.g., cubism).Next,we ground the region in the input image, resulting in aprecise segmentation mask. Finally, we adapt CLIP-based loss functions to constrain the style transfer to the identifiedregion in the image, which we call local style transfer.Our method enables fine-grained style transfer by aligningwith the users intent. We corroborate the effectiveness ofour method through quantitative and qualitative analysis.Our human preference study indicates that users prefer ourmethod over CLIPstyler 97% of the time.",
  ". Related Work": "Recent research on text-conditioned style transfer harnesses the multimodal capabilities of VLMs likeCLIP to stylize an image according to a user-provideddescription of the desired style. CLVA leverages a patch-wise style discriminator to jointly embed style images andstyle instructions.CLIPstyler proposes to align thestyled image with the style text in CLIP embedding spaceat both patch and global level. ZeCon is a diffusion-based extension of CLIPstyler.Several modifications toCLIPstyler have been proposed to alleviate itslimitations, with only focusing on artistic styles,and Sem-CS solving the over-stylization problem.MOSAIC is an object-level style transfer method basedon CLIPstyler. Additionally, it requires training a BERT-based text segmentation model.In contrast, ours isan end-to-end inference time optimization with no trainingrequired.Following conventional neural style transfer,some methods require a style image as anadditional input.Althoughtext-conditionedimageeditingapproaches have been quitesuccessful in handling general edit instructions fromusers, they are not as adept at style transfer. Text2live relies on explicit region disambiguation by the user toproduce alpha masks, which are then used to localize thedesired edits.Instruct-pix2pix is a diffusion-basedsupervised model that follows users edit instructions.InstructDiffusion further aligns computer vision taskswith human instructions.In practice, these approachesface difficulties in isolating the specified object or region,resulting in unnecessary changes applied to the image.",
  "(c) Multi-region style transfer": ". Overview of our approach. a) Text-grounding in the content image: We first use LLaVA and SAM to obtain a precise segmentationmask of specified region in the image, and the corresponding desired style. b) Local style transfer: We use the region-style correspondencesto constrain style transfer to the specified local region. c) This process can be repeated several times to achieve multi-region style transfer.",
  ". Proposed Methodology": "For a given input image Ic (called content image) and a user-provided style description tsty (for e.g., apply cubism styleto the sky), we wish to obtain a stylized image Ics thatrespects the semantics of tsty. We optimize a style networkf that takes Ic as input, and outputs Ics := f(Ic). In thefollowing sections, we elaborate on the building blocks ofour method: 1) text grounding in the content image, and 2)local style transfer. Our approach is summarized in .",
  ". Text Grounding in the content image": "Large vision-language models (VLMs), have been shownto exhibit the remarkable ability to comprehend both imageand natural language, and generate contextually relevantresponses for a given user query.Building upon itsconsiderable achievements across diverse applications, wepropose to query LLaVA , a large multimodalmodel, to extract the region and its corresponding stylefrom the users target style description, tsty.In thistask, the region, denoted as R, refers to a specific areawithin the content image, such as building or sky. Thecorresponding style, represented as S, is the artistic orvisual characteristics intended by the user for this region,such as Starry night by Van Gogh or cubism.In particular, we use LLaVA to extract R (in the form ofbounding box coordinates) and S, given the content image(Ic) and user-provided style description (tsty). To achievethis, we query LLaVA using the following prompt:",
  ". Local style transfer": "In the context of our text grounding approach, we haveacquired segmentation mask (M) as detailed in 3.1. Ourobjective is to confine the stylization solely to the identifiedregion in the final output, Ics.It is imperative thatthe regions outside of R remain unaffected and preservetheir original appearance.Conventional style transfermethods predominantly concentrate on stylizing the entireimage and are unsuitable for local style transfer.Toalleviate these limitations, we propose to incorporate M inCLIPstylers inference-time optimization: 1. Masked CLIP Directional loss (Ldir). As our focusis to stylize only the region R, we use segmentationmask M to mask both the content and stylizedimages, and align the direction of CLIP text andimage embeddings (ET , EI are CLIP text and imageencoders respectively). Mathematically, our adapteddirectional loss can be defined as follows:",
  "(1)": "2. Masked Patch CLIP loss (Lpatch). To achieve localstylization within the segmentation mask, we employa heuristic approach that involves sampling patchesfrom the compact bounding box (say, B) around theforeground in M. We randomly select C patches ofsize ps ps from within this bounding box, for boththe content Ic,j and the stylized image Ics,j, j {1, 2, 3, ..., C}. The directional CLIP loss (Ldir) iscomputed on each patch as follows:",
  "Ics := Ics M + Ic (1 M)(6)": "Multi-region style transfer.Our end-to-end local styletransfer process can be systematically iterated for asequence of regions R1, R2, ..., RN.The final stylizedoutput Ics is derived from the last local style transferoperation corresponding to region RN.This iterativeapproach ensures that each region is stylized individually,and the resulting image reflects the cumulative effect of allthe local style transfers. Refer",
  ". Experiments and Results": "Dataset. We curate a set of 25 natural images, spanningbroad categories including interior, outdoors/street, naturalscenery, animals, inanimate objects. For each image in ourdataset, we manually design 10 prompts with a format ofapply < style > style to < region > in the image, where< style > is a placeholder for artistic or texture styles,for e.g., oil painting of roses, white wool, watercolorpainting, Starry Night by Van Gogh etc. and < region >is a placeholder for an object/region in the image. In case ofmultiple objects of same category, we use prepositions likethe leftmost, at the center etc. to resolve ambiguities.Note that there is no standard dataset for our task, and sinceour approach is an end-to-end inference-time optimization,we only use this dataset for evaluation purposes. Model architecture & hyperparameters.The neuralnetwork f is a U-Net architecture with 3 downsampleand 3 upsample layers, with channel dimensions of 16,32, 64 respectively. Following , we use the VGG features from layers conv4 2 and conv5 2 to computethe content loss. The number of crops for Patch CLIP loss,C is set to 64 and patch size ps = 100. We interpolate thecontent images to a resolution of 512 512 for stylization,except ZeCon where the resolution is 256 256. The losscoefficients are set as follows: d = 500, p = 103, c =150, tv = 2 103. We use an Adam optimizer with",
  "a learning rate of 5 104. Empirically, the network fconverges in 200 iterations": "Baselines.We compare our approach against twobroad sets of baselines:text-conditioned image editing(Instruct-pix2pix , InstructDiffusion ), and text-conditioned image style transfer (CLIPstyler , ZeCon) techniques. Evaluation.CLIP score is a common evaluationmetric to quantify the alignment of an image with a giventext. To effectively compute CLIP scores for local styletransfer task, we first mask the local region R using M asfollows: IcsM, and then crop using the compact boundingbox B: cropB(Ics M). We report the CLIP score betweenthis crop and style S in Tab 1. In practice, CLIP score doesnot correlate well with human preferences . We conducta user study to circumvent this issue. We ask 20 users tochoose between CLIPstyler and our outputs for given pairsof content images (Ic) and style descriptions (tsty). Theresults are averaged across users and reported in Tab 2. Results.While shows that our method iscomparable to baselines in CLIP score, it outperforms thehighest scoring CLIPstyler in the user study () by alarge margin. CLIPstyler and other style transfer techniquestend to stylize the entire image with no localization (refer). On the other hand, image editing methods tend todistort the contents of the input image, with little to no styletransfer. Qualitative results are shown in .",
  "Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang,Yinfei Yang, and Zhe Gan.Guiding instruction-basedimage editing via multimodal large language models. arXivpreprint arXiv:2309.17102, 2023. i": "Tsu-Jui Fu, Xin Eric Wang, and William Yang Wang.Language-driven artistic style transfer. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel,October 2327, 2022, Proceedings, Part XXXVI, pages 717734. Springer, 2022. i Prajwal Ganugula, YSSS Kumar, NK Reddy, PrabhathChellingi, Avinash Thakur, Neeraj Kasera, and C ShyamAnand. Mosaic: Multi-object segmented arbitrary stylizationusing clip. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 892903, 2023. i Leon A Gatys, Alexander S Ecker, and Matthias Bethge.Image style transfer using convolutional neural networks. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 24142423, 2016. iv",
  "Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-BinHuang.Expressive text-to-image generation with richtext. In IEEE International Conference on Computer Vision(ICCV), 2023. i": "Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, ShuyangGu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, DongChen, and Baining Guo.Instructdiffusion: A generalistmodeling interface for vision tasks. CoRR, abs/2309.03895,2023. i, iv Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, NicuSebe, Trevor Darrell, Zhangyang Wang, and Humphrey Shi.Pair-diffusion: Object-level image editing with structure-and-appearance paired diffusion models.arXiv preprintarXiv:2303.17546, 2023. i",
  "Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,Yael Pritch, and Daniel Cohen-Or.Prompt-to-promptimage editing with cross attention control. arXiv preprintarXiv:2208.01626, 2022. i": "Chanda Grover Kamra, Indra Deep Mastan, and DebayanGupta.Sem-cs: Semantic clipstyler for text-based imagestyle transfer.In 2023 IEEE International Conference onImage Processing (ICIP), pages 395399. IEEE, 2023. i Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, HuiwenChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models.InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 60076017, 2023. i Alexander Kirillov, Eric Mintun, Nikhila Ravi, HanziMao, Chloe Rolland, Laura Gustafson, Tete Xiao, SpencerWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segmentanything. arXiv preprint arXiv:2304.02643, 2023. ii Gihyun Kwon and Jong Chul Ye. Clipstyler: Image styletransfer with a single text condition.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1806218071, 2022. i, ii, iv",
  "Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, YijunLi, Jingwan Lu, and Jun-Yan Zhu.Zero-shot image-to-image translation.In ACM SIGGRAPH 2023 ConferenceProceedings, pages 111, 2023. i": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,and Dani Lischinski.Styleclip: Text-driven manipulationof stylegan imagery.In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 20852094, 2021. i Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell,Pamela Mishkin,Jack Clark,et al.Learning transferable visual models from natural languagesupervision.In International conference on machinelearning, pages 87488763. PMLR, 2021. i, iv Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:Convolutional networks for biomedical image segmentation.InMedicalImageComputingandComputer-AssistedInterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III18, pages 234241. Springer, 2015. iv",
  "generation in any style. arXiv preprint arXiv:2306.00983,2023. i": "Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion:Controllable disentangled style transfer via diffusion models.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 76777689, 2023. i Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, andHongshengLi.Humanpreferencescore:Betteraligning text-to-image models with human preference.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 20962105, 2023. iv Zipeng Xu, Enver Sangineto, and Nicu Sebe. Stylerdalle:Language-guided style transfer using a vector-quantizedtokenizer of a large-scale generative model. In Proceedingsof the IEEE/CVF International Conference on ComputerVision, pages 76017611, 2023. i Serin Yang, Hyunmin Hwang, and Jong Chul Ye. Zero-shotcontrastive loss for text-guided diffusion image style transfer.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 2287322882, 2023. i, iv Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang,Chongyang Ma, Weiming Dong, and Changsheng Xu.Inversion-based style transfer with diffusion models.InProceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1014610156, 2023.i"
}