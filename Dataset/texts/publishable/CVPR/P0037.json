{
  "Abstract": "Large conferences such as NeurIPS and AAAI serve as crossroads of various AIelds, since they attract submissions from a vast number of communities. How-ever, in some cases, this has resulted in a poor reviewing experience for somecommunities, whose submissions get assigned to less qualied reviewers outsideof their communities. An often-advocated solution is to break up any such largeconference into smaller conferences, but this can lead to isolation of communitiesand harm interdisciplinary research. We tackle this challenge by introducing anotion of group fairness, called the core, which requires that every possible com-munity (subset of researchers) to be treated in a way that prevents them fromunilaterally beneting by withdrawing from a large conference.We study a simple peer review model, prove that it always admits a reviewing as-signment in the core, and design an efcient algorithm to nd one such assignment.We use real data from CVPR and ICLR conferences to compare our algorithm toexisting reviewing assignment algorithms on a number of metrics.",
  "Introduction": "Due to their large scale, conferences like NeurIPS and AAAI use an automated procedure to assignsubmitted papers to reviewers. Popular such systems include the Toronto Paper Matching System ,Microsoft CMT2, and OpenReview3. The authors submitting their works are often very interestedin receiving meaningful and helpful feedback from their peers . Thus, their overall experiencewith the conference heavily depends on the quality of reviews that their submissions receive. The typical procedure of assigning papers to reviewers is as follows. First, for each paper-reviewerpair, a similarity score is calculated based on various parameters such as the subject area of thepaper and the reviewer, the bids placed by the reviewer, etc. . Then, an assignment iscalculated through an optimization problem, where the usual objectives are to maximize either theutilitarian social welfare, which is the total similarity score of all matched paper-reviewer pairs, orthe egalitarian social welfare, which is the least total score of reviewers assigned to any submission.Relevant constraints are imposed to ensure that each submission receives an appropriate number ofreviews, reviewer workloads are respected, and any conicts of interest are avoided. Peng et al. recently mentioned that a major problem with the prestigious mega conferences is thatthey constitute the main venues for several communities, and as a result, in some cases, people areasked to review submissions that are beyond their main areas of work. They claim that a reasonablesolution is to move to a de-centralized publication process by creating more specialized conferencesappropriate for different communities. While specialized conferences denitely have their advan-tages, the maintenance of large conferences that attract multiple communities is also crucial for theemergence of interdisciplinary ideas that can be reviewed by diverse subject experts. Therefore, it isimportant to ensure that no group has an incentive to break off due to a feeling of being mistreated bythe reviewing procedure of a large conference. In this work, we ask whether it is possible to modify",
  "A preliminary version was published in the proceedings of NeurIPS 2023.2": "the existing reviewing processes to resolve this issue by treating the various communities satisfac-torily. We clarify that the goal is not to build roadblocks to the creation of specialized conferences,but rather to mitigate the harm imposed on communities in large conferences. To answer this, we look toward the literature on algorithmic fairness. Specically, we adapt thegroup fairness notion known as the core ; to the best of our knowledge, we are the rst tointroduce it to the peer review setting. For a reviewing assignment to be in the core, it must ensurethat no community (subset of researchers) can deviate by setting up its own conference in which (a)no author reviews her own submission, (b) each submission from within the community is reviewedby just as many reviewers as before, but now from within the community, (c) each reviewer reviewsno more papers than before, and (d) the submissions are assigned to better reviewers, making thecommunity happier. Intuitively, this is a notion of group fairness because it ensure that the treatmentprovided to every group of participants meets their entitlement (which is dened by what thegroup can achieve on its own). It is also a notion of stability (often also known as core stability)because it provides no incentives for any community to break off and get isolated by setting up theirown conference instead. Note that this denition provides fairness to every possible community, andnot only to predened groups, as is the case for fairness denitions such as demographic parity andequalized odds that are popular in the machine learning literature . In particular, it ensures fairtreatment to emerging interdisciplinary communities even before they become visible.",
  "Our Contribution": "We consider a simple peer review model in which each agent submits (as the sole author) a numberof papers to a conference and also serves as a potential reviewer. A reviewing assignment is validif each paper is reviewed by kp reviewers, each reviewer reviews no more than ka papers, and noagent reviews her own submissions. To ensure that a valid assignment always exists, we assume thatthe maximum number of papers that each agent is allowed to submit is at most ka/kp. In , we present an efcient algorithm that always returns a valid assignment in the coreunder minor conditions on the preferences of the authors. Specically, our algorithm takes as inputonly the preference ranking of each author over individual potential reviewers for each of her submis-sions. Then, it produces an assignment that we prove to be in the core for any manner in which theagents submission-specic preferences over individual reviewers may be extended to preferencesover a set of kp reviewers assigned to each submission, aggregated over submissions, subject to twomild conditions being satised. In , we conduct experiments with real data from CVPR and ICLR conferences, and evaluatethe price that our algorithm must pay in lost utilitarian and egalitarian welfare in order to satisfythe core and prevent communities from having an incentive to deviate. We also observe that reviewerassignment methods currently used in practice generate such adverse incentives quite often.",
  "Related Work": "As we mentioned above, usually the rst step of a review assignment procedure is to calculate asimilarity score for each pair of submission and reviewer which aims to capture the expertise of thereviewer for this submission. The problem of identifying the similarity scores has been extensivelystudied in the literature . In this work, we assume that the similarity scores are givenas an input to our algorithm after they have been calculated from a procedure that is considered asa black box. Importantly, our algorithm does not need the exact values of the similarity scores, butit only requires a ranking of the reviewers for each paper, indicating their relative expertise for thispaper. Given, the similarities scores various methods have been proposed for nding a reviewing assign-ment. The most famous algorithm is the Toronto Paper Matching System which is a very broadlyapplied method and focuses on maximizing the utilitarian welfare, i.e., the sum of the similaritiesacross all assigned reviewers and all papers. This approach has been adopted by other popular con-ference management systems such as EasyChair4 and HotCRP 5 . While this approach optimizes",
  "the total welfare, it is possible to discriminate against some papers. Therefore, other methods havefocused on nding reviewing assignments that are (also) fair across all papers": "ODell et al. suggest a method where the goal is to maximize the total score that a paper gets,while Stelmakh et al. generalized this method by maximizing the minimum paper score, thenmaximizing the next smallest paper score, etc. Hartvigsen et al. ensure fairness by requiring thateach paper is assigned at least one qualied reviewer. Kobren et al. proposed two algorithmsthat maximize that total utilitarian under the constraint that each paper should receive a score thatexceeds a particular threshold. Payan and Zick used the idea of envy-freeness from thefair division literature to ensure fairness over the submissions. Moreover, some other works havefocused on being fair over the reviewers rather than the papers ). The core property that weconsider in this work can be viewed as a fairness requirement over groups of authors. The readercan nd more details about the challenges of the peer review problem in the recent survey of Shah. In our model, the review assignment problem is related to exchange problems with endowments ,since authors can be viewed as being endowed by their own papers which they wish to exchange withother authors that also serve as reviewers. For the basic exchange problem of housing reallocation,Shapley and Scarf showed that an algorithm called Top-Trading-Cycle (TTC) nds an allocationwhich is in the core. The rst part of our algorithm uses a variation of TTC where the agents(authors) are incorporated with multiple items (submissions), and constraints related to how manyitems each agent can get and to how many agents one item should be assigned should be satised.In contrast to classical exchange problem with endowments, our model has a distinctive requirementthat agents/authors need to give away all their items/papers as the papers need to be reviewed by theagent who gets the paper. As we further explain in , this difference is crucial and requiresfurther action from our algorithm than simply executing this variation of TTC. Various variations ofTTC have been considered in the literature, tailored for different variations of the basic problem, butto the best of our knowledge, none of them can be directly applied in our model. To give an example,Suzuki et al. consider the case that there are multiple copies of the same object and there aresome quotas that should be satised, but they assume that each agent gets just one object while hereeach paper is assigned to multiple distinct reviewers.",
  "Model": "For q N, dene [q] {1, . . ., q}. There is a set of agents N = [n]. Each agent i submits a setof papers Pi = {pi,1, . . . , pi,mi} for review by her peers, where mi N, and is available to reviewthe submissions of her peers. We refer to pi, as the -th submission of agent i; when consideringthe special case of each agent i having a single submission, we will drop and simply write pi. LetP = iNPi be the set of all submissions and m =",
  "iN mi be the total number of submissions": "Assignment.Our goal is to produce a (reviewing) assignment R : N P {0, 1}, whereR(i, j) = 1 if agent i N is assigned to review submission j P. With slight abuse of notation,let Rai = {j P : R(i, j) = 1} be the set of submissions assigned to agent i and Rpj = {i N :R(i, j) = 1} be the set of agents assigned to review submission j. We want the assignment to bevalid, i.e., satisfy the following constraints: Each agent must be assigned at most ka submissions for review, i.e., |Rai | ka, i N. Each submission must be assigned to kp agents, i.e., |Rpj| = kp, j P. No agent should review one of her own submissions, i.e., R(i, pi,) = 0, i N, [mi]. To ensure that a valid assignment always exists, we impose the constraint that mi kp ka for eachi N, which implies that m kp n ka. Intuitively, this demands that each agent submittingpapers be willing to provide as many reviews as the number of reviews assigned to the submissionsof any single agent. For further discussion on this condition, see . Note that given N N and P i Pi for each i N with P = iN P i , the validity requirementsabove can also be extended to a restricted assignment R : N P {0, 1}. Hereinafter, we willassume validity unless specied otherwise or during the process of building an assignment. Preferences. Each agent i N has a preference ranking, denoted i,, over the agents in N \\{i} forreviewing her -th submission pi,.6 These preferences can be based on a mixture of many factors,such as how qualied the other agents are to review submission pi,, how likely they are to providea positive review for it, etc. Let i,(i) be the position of agent i N \\ {i} in the ranking. Wesay that agent i prefers agent i to agent i as a reviewer for pi, if i,(i) < i,(i). Again, inthe special case where the agents have a single submission each, we drop and just write i. Let = (1,1, . . . , 1,m1, . . . , n,1, . . . , n,mn). While our algorithm takes as input, to reason about its guarantees, we need to dene agent prefer-ences over assignments by extending . In particular, an agent is assigned a set of reviewers for eachof her submissions, so we need to dene her preferences over sets of sets of reviewers. First, weextend to preferences over sets of reviewers for a given submission, and then aggregate preferencesacross different submissions. Instead of assuming a specic parametric extension (e.g., additivepreferences), we allow all possible extensions that satisfy two mild constraints; the group fairnessguarantee of our algorithm holds with respect to any such extension. Extension to a set of reviewers for one submission: Let S i, S (resp., S i, S) denote that agenti strictly (resp., weakly) prefers the set of agents S to the set of agents S for her -th submissionpi,. We require only that these preferences satisfy the following mild axiom.Denition 1 (Order Separability). For every disjoint S1, S2, S3 N with |S1| = |S2| > 0, if itholds that i,(i) < i,(i) for each i S1 and i S2, then we must have S1 S3 i, S2 S3. An equivalent reformulation is that between any two sets of reviewers S and T with |S| = |T |,ignoring the common reviewers in S T , if the agent strictly prefers every (even the worst) reviewerin S \\ T to every (even the best) reviewer in T \\ S, then the agent must strictly prefer S to T .Example 1. Consider the common example of additive preferences, where each agent i has a utilityfunction ui, : N \\ {i} R0 over individual reviewers for her -th submission, inducing herpreference ranking i,. In practice, these utilities are sometimes called similarity scores. Her pref-erences over sets of reviewers are dened via the additive utility function ui,(S) iS ui,(i).It is easy to check that for any disjoint S1, S2, S3 with |S1| = |S2| > 0, ui,(i) > ui,(i) for alli S1 and i S2 would indeed imply ui,(S1 S3) > ui,(S2 S3). Additive preferences arejust one example from a broad class of extensions satisfying order separability. Extension to assignments. Let us now consider agent preferences over sets of sets of reviewers, orequivalently, over assignments. Let R i R (resp., R i R) denote that agent i strictly (resp.,weakly) prefers assignment R to assignment R. Note that these preferences collate the submission-wise preferences i, across all submissions of the agent. We require only that the preferenceextension satises the following natural property. Denition 2 (Consistency). For any assignment R, restricted assignment R over any N N andP = iN P i (where P i Pi for each i N ), and agent i N , if it holds that Rppi, i,Rppi, for each pi, P i, then we must have R i R. In words, if an agent weakly prefers R to R for the set of reviewers assigned to each of her submis-sions individually, then she must prefer R to R overall.Example 2. Let us continue with the previous example of additive utility functions. The pref-erences of agent i can be extended additively to assignments using the utility function ui(R) =pi,P ui,(Rppi,). It is again easy to check that if ui,(Rppi,) ui,( Rppi,) for each pi,, then",
  "ui(R) ui( R). Hence, additive preferences are again one example out of a broad class of prefer-ence extensions that satisfy consistency": "Core.Our goal is to nd a group-fair assignment which treats every possible group of agents atleast as well as they could be on their own, thus ensuring that no subset of agents has an incentive todeviate and set up their own separate conference. Formally:Denition 3 (Core). An assignment R is in the core if there is no N N, P i Pi for each i N ,and restricted assignment R over N and P = iN P i such that R i R for each i N .",
  "L the last kp |U| + 1 agents in N \\ U to have all their submissions completely assigned ;": "In words, if any subset of agents deviate with any subset of their submissions and implement anyrestricted reviewing assignment, at least one deviating agent would not be strictly better off, thuseliminating the incentive for such a deviation. We also remark that our algorithm takes only thepreference rankings over individual reviewers as input and produces an assignment R that isguaranteed to be in the core according to every preference extension of satisfying order separabilityand consistency.",
  "In this section, we prove our main result: when agent preferences are order separable and consistent,an assignment in the core always exists and can be found in polynomial time": "Techniques and key challenges: The main algorithm CoBRA (Core-Based Reviewer Assignment),presented as Algorithm 1, uses two other algorithms, PRA-TTC and Filling-Gaps, presented asAlgorithm 2 and Algorithm 3, respectively. We remark that PRA-TTC is an adaptation of the pop-ular Top-Trading-Cycles (TTC) mechanism, which is known to produce an assignment in the corefor the house reallocation problem (and its variants) . The adaptation mainly incorporates theconstraints related to how many papers each reviewer can review and how many reviewers shouldreview each paper. While for kp = ka = 1, PRA-TTC is identical with the classic TTC that is usedfor the house reallocation problem, the main difference of this problem with the review assignmentproblem is that in the latter each agent should give away her item (i.e., her submission) and obtainthe item of another agent. Therefore, by simply executing TTC in the review assignment problem,one can get into a deadlock before producing a valid assignment. For example, consider the caseof three agents, each with one submission. Each submission must receive one review (kp = 1) andeach agent provides one review (ka = 1). The TTC mechanism may start by assigning agents 1 and2 to review each others submission, but this cannot be extended into a valid assignment becausethere is no one left to review the submission of agent 3. This is where Filling-Gaps comes in; itmakes careful edits to the partial assignment produced by the PRA-TTC, and the key difculty is toprove that this produces a valid assignment while still satisfying the core.",
  "Description of CoBRA": "Before we describe CoBRA in detail, let us introduce some more notation. Let m = maxiN mi.For reasons that will become clear later, we want to ensure that mi = m, for each i N. Toachieve that, we add m mi dummy submissions to agent i, and the rankings over reviewers withrespect to these submissions are arbitrarily. An assignment is called partial if there are submissionsthat are reviewed by less than kp agents. A submission that is reviewed by kp agents under a partialassignment is called completely assigned. Otherwise, it is called incompletely assigned. We denote",
  "P i = , we add an edge from i to i, where i is an arbitrary agent with": "P i = . PRA-TTC startswith an empty assignment, constructs the preference graph and searches for a directed cycle. Ifsuch a cycle exists, the algorithm eliminates it as following: For each (i, i) that is included in thecycle, it assigns submission pi, to i (if is submissions are already completely assigned, it doesnothing) and removes pi, from P i, if it is now completely assigned. Then, the algorithm updatesthe preference graph and continues to eliminate cycles in the same way. When there are no leftcycles in the preference graph, the algorithm terminates and returns two sets, U and L. The rstset contains all the agents that some of their submissions are incompletely assigned and the set Lcontains the last kp |U| + 1 agents whose all submissions became completely assigned. Filling-Gaps. CoBRA calls Filling-Gaps, if the U that returned from PRA-TTC is non empty. Be-fore we describe the Filling-Gaps, we also need to introduce the notion of a greedy graph. Supposethat we have a partial assignment R which indicates a set U that contains all the agents whose atleast one submission is incompletely assigned. We dene the directed greedy graph G R = (U, E R)where (i, i) E R if R(i, pi,) = 0 for some pi, P i. In other words, while in the preferencegraph, agent i points only to her favourite potential reviewer with respect to one of her incompletesubmissions, in the greedy graph agent i points to any agent in U \\ {i} that could review at leastone of her submissions that is incompletely assigned. Filling-Gaps consists of two phases. In therst phase, starting from the partial assignment R that was created from PRA-TTC, it constructs thegreedy graph, searches for cycles and eliminates a cycle by assigning pi, to agent i for each (i, i)in the cycle that exists due to pi, (when an edge exists due to multiple submissions, the algorithmchooses one of them arbitrary). Then, it updates",
  "(b) Execution of Filling-Gaps": ": Execution of CoBRA when n = 6, kp = ka = 3, 1 = 2 3 4 . . ., 2 = 3 1 5 . . ., 3 = 1 2 5 . . ., 4 = 1 3 5 . . ., 5 = 6 4 . . . and 6 = 2 . . .. Onthe left table, we see the assignments that are established in each round of PRA-TTC by eliminatingcycles. After the execution of PRA-TTC, three papers, p4, p5, p6 are not completely assigned. Thus,U = {4, 5, 6} and L = {3}. On the right table, we see the execution of Filling-Gaps. There isa cycle in the greedy graph which is eliminated at the rst round of Phase 1. In Phase 2, where = (6, 5), at the rst round, since p3 is authored by an agent in U L \\ {6}, is not reviewed by6 and is completely assigned, p3 is assigned to 6 while it is removed form 1 in which p6 is nowassigned. At the second round, since p4 is authored by an agent in U L \\ {5}, is not reviewedby 5 and is completely assigned, p4 is assigned to 5 while it is removed form 1 in which p5 is nowassigned.",
  "shows an execution of CoBRA on a small instance. Let us briey describe how we get thisassignment": "In the preference graph, we see that 1 has an outgoing edge to agent 2, agent 2 has an outgoing edgeto agent 3 and agent 3 has an outgoing edge to agent 1. By eliminating this cycle we assign p3 to1, p2 to 3 and p1 to 2. By continuing to detect cycles in the preference graph given the preferences,we conclude on the partial assignment shown in the table on the left. For the table on the right, atthe rst phase of Filling-Gaps, when we create the greedy graph, we see that it consists of 3 nodes,since U = {4, 5, 6}, 4 has outgoing edges to 5 and 6 (since none of them review p4), 6 has outgoingedges to 4 and 5, and 5 has no outgoing edge. Hence there is only one cycle and that is why p6 isassigned to 4 and p4 is assigned to 6. Then, 4 is moved to L, since p4 becomes completely assigned.Then, there is no cycle in the greedy graph as while 6 has an outgoing edge to 5, 5 has no outgoingedges. Hence, we proceed to the second phase where there are two non-completely assigned papers,p5 and p6. = (5, 6) as agent 6 reviews p5, but 5 does not review p6. Then, the algorithm makessure that p6 and then p5 become completely assigned as it is described in the caption of the gure.",
  "Lemma 1. CoBRA returns a valid assignment": "Now, we show that the nal assignment R that CoBRA returns is in the core. Note that while itis possible that an assignment of a submission of an agent in U L, that was established duringthe execution of PRA-TTC, to be removed in the execution of Filling-Gaps, this never happens forsubmissions that belong to some agent in N \\ (U L). For the sake of contradiction, assume thatN N, with P i Pi for each i N , deviate to a restricted assignment R over N and iN P i .Note that R is valid only if |N | > kp, as otherwise there is no way each submission in iN P i tobe completely assigned, since no agent can review her own submissions.",
  "We distinguish into two cases and we show that in both cases the assignment is in the core": "Case I: i N : i L U. Let i N be the rst agent in N whose all submissions becamecompletely assigned in the execution of PRA-TTC. Note that since there exists i U L, we getthat i U L from the denitions of U and L. Now, consider any pi,. Let Q1 = Rppi, \\",
  "(Rppi, Rppi,) and Q2 = Rppi, \\ (Rppi, Rppi,). If Q1 = , then we have that Rppi, = Rppi,which means that Rppi, i, Rppi,. Otherwise, let i = argmaxiQ1 i,(i), i.e., i is ranked at": "the lowest position in i, among the agents that review pi, under R but not under R. Moreover,let i = argminiQ2 i,(i), i.e., i is ranked at the highest position in i, among the agents thatreview pi, under R but not under R. We have R(i, pi,) = 1, if and only if i has an outgoing edgeto i at some round of PRA-TTC. At the same round, we get that i can review more submissions,since i N and if i has incompletely assigned submissions, then any i N has incompletelyassigned submissions, and hence |Rai| < kpm ka. This means that if i,(i) > i,(i), theni would point i instead of i. We conclude that i,(i) < i,(i). Then, from the denitionof i and i and from the order separability property we have that Rppi, i, Rppi,. Thus, either",
  "consistency, we get that R i R which is a contradiction": "Case II: i N : i L U. In this case we have that N = U L, as |U L| = kp + 1.This means that for each i U L and [m], Rppi, = (U L) \\ {i}. Let i L be therst agent in L whose all submissions became completely assigned in the execution of PRA-TTC.Consider any pi,. Note that it is probable that while pi, was assigned to some agent i in PRA-TTC, it was moved to another agent i during the execution of Filling-Gaps. But, i belongs to Uand we can conclude that if pi, is assigned to some i N \\ U at the output of CoBRA, thisassignment took place during the execution of PRA-TTC. Now, let Q1 = Rppi, \\ (Rppi, Rppi,)",
  "and Q2 = Rppi, \\ (Rppi, Rppi,). If Q1 = , then we have that Rppi, = Rppi, which means": "that Rppi, i, Rppi,. If Q1 = , then Q1 N \\ (U L) and Q2 U L since Rppi, = U L.Let i = argmaxiQ1 i,(i), i.e., i is ranked at the lowest position in i, among the agents thatreview pi, under R but not under R. Moreover, let i = argminiQ2 i,(i), i.e., i is rankedat the highest position in i, among the agents that review pi, under R but not under R. Fromabove, we know that the assignment of pi, to i was implemented during the execution of PRA-TTC, since i N \\ (U L). Hence, with very similar arguments as in the previous case, we willconclude that i,(i) < i,(i). We have R(i, pi,) = 1 if and only if i has an outgoing edgeto i at some round of PRA-TTC. At this round, we know that i can review more submissions,since i N and if i has incompletely assigned submissions, then any i N has incompletelyassigned submissions. This means that if i,(i) > i,(i), then i would point i instead ofi. Hence, we conclude that i,(i) < i,(i). Then, from the denition of i and i and fromthe order separability property we have that Rppi, i, Rppi,. Thus, either if Q1 is empty or not,",
  "R i R which is a contradiction": "Lastly, we analyze the time complexity of CoBRA. First, we consider the time complexity of PRA-TTC. In each iteration, the algorithm assigns at least one extra reviewer to at least one incompletely-assigned submission. This can continue for at most mkp nka iterations, since each submissionshould be reviewed by kp reviewers. In each iteration, it takes O(n) time to nd and eliminate acycle in the preference graph. Then, it takes O(n2) time to update the preference graph, since for each arbitrarily-picked incompletely-assigned submission of each agent, we need to nd the mostqualied reviewer who can be additionally assigned to it. By all the above, we conclude that theruntime of PRA-TTC is O(n3), by ignoring ka which is a small constant in practice. After PRA-TTC terminates, CoBRA calls the Filling-Gaps algorithm. However, Lemma 3 ensures that at theend of PRA-TTC, |L U| kp + 1, which is also a small constant. And Filling-Gaps only makeslocal changes that affect these constantly many agents. As such, the running time of Filling-Gaps isconstant as well. Therefore, the time complexity of CoBRA is O(n3).",
  "Experiments": "In this section, we empirically compare CoBRA to TPMS , which is widely used (for example,it was used by NeurIPS for many years), and PR4A , which was used in ICML 2020 . Asmentioned in the introduction, these algorithms assume the existence of a similarity or afnity scorefor each pair of reviewer i and paper j, denoted by S(i, j). The score (or utility) of a paper underan assignment R, denoted by upj, is computed as upj iRpj S(i, j). TMPS nds an assignment R that maximizes the utilitarian social welfare (USW), i.e., the total paper score jP upj, whereasPR4A nds an assignment that maximizes the egalitarian social welfare (ESW), i.e., the minimumpaper score minjP upj.7 We use ka = kp = 3 in these experiments.8 Datasets. We use three conference datasets: from the Conference on Computer Vision and PatternRecognition (CVPR) in 2017 and 2018, which were both used by Kobren et al. , and from theInternational Conference on Learning Representations (ICLR) in 2018, which was used by Xu et al.. In the ICLR 2018 dataset, similarity scores and conicts of interest are also available. Whilea conict between a reviewer and a paper does not necessarily indicate authorship, it is the bestindication we have available, so, following Xu et al. , we use the conict information to deduceauthorship. Since in our model each submission has one author, and no author can submit morethan ka/kp = 1 papers, we compute a maximum cardinality matching on the conict matrix tond author-paper pairs, similarly to what Dhull et al. did. In this way, we were able to match883 out of the 911 papers. We disregard any reviewer who does not author any submissions, butnote that the addition of more reviewers can only improve the results of our algorithm since theseadditional reviewers have no incentive to deviate. For the CVPR 2017 and CVPR 2018 datasets,similarity scores was available, but not the conict information. In both these datasets, there arefewer reviewers than papers. Thus, we constructed articial authorship relations by sequentiallyprocessing papers and matching each paper to the reviewer with the highest score for it, if thisreviewer is still unmatched. In this way, we were able to match 1373 out of 2623 papers from CVPR2017 and 2840 out of 5062 papers from CVPR 2018. In the ICLR 2018 and CVPR 2017 datasets,the similarity scores take values in , so we accordingly normalized the CVPR 2018 scores aswell. Measures. We are most interested in measuring the extent to which the existing algorithms provideincentives for communities of researchers to deviate. To quantify this, we need to specify the utilitiesof the authors. We assume that they are additive, i.e., the utility of each author in an assignment isthe total similarity score of the kp = 3 reviewers assigned to their submission. Core violation factor: Following the literature , we measure the multiplicative violation of thecore (if any) that is incurred by TPMS and PR4A (CoBRA provably does not incur any). This isdone by computing the maximum value of 1 for which there exists a subset of authors such thatby deviating and implementing some valid reviewing assignment of their papers among themselves,they can each improve their utility by a factor of at least . This can easily be formulated as abinary integer linear program (BILP). Because this optimization is computationally expensive (themost time-consuming component of our experiments), we subsample 100 papers9 from each datasetin each run, and report results averaged over 100 runs. Note that whenever there exists a subset of 7Technically, subject to this, it maximizes the second minimum paper score, and then the third minimumpaper score, etc. This renement is also known as leximin in the literature.8Note that kp = 3 reviews per submission is quite common, although the reviewer load ka is typicallyhigher in many conferences, often closer to 6. However, the differences between different algorithms diminishwith higher values of ka.9In in the appendix, we report USW and ESW without any subsampling and we note that thequalitative relationships between the different algorithms according to each metric remain the same.",
  "CoBRA0.166 0.0010.028 0.0010%1.000 0.0000%ICLR 18TPMS0.184 0.0010.048 0.0020%1.048 0.00890%PR4A0.179 0.0010.082 0.0010%1.087 0.009100%: Results on CVPR 2017 and 2018, and ICLR 2018": "authors with zero utility each in the current assignment who can deviate and receive a positive utilityeach, the core deviation becomes innite. We separately measure the percentage of runs in whichthis happens (in the column #unb-), and report the average among the remaining runs in the column. Core violation probability: We also report the percentage of runs in which a core violation exists(i.e., there exists at least one subset of authors who can all strictly improve by deviating from thecurrent assignment). We refer to this as the core violation probability (CV-Pr).",
  "Social welfare: Finally, we also measure the utilitarian and egalitarian social welfare (USW andESW) dened above, which are the objectives maximized by TPMS and PR4A, respectively": "Results. The results are presented in . As expected, TPMS and PR4A achieve the highestUSW and ESW, respectively, on all datasets because they are designed to optimize these objectives.In CVPR 2017, CoBRA and TPMS always end up with zero ESW because this dataset includesmany zero similarity scores, but PR4A is able to achieve positive ESW. In all datasets, CoBRAachieves a relatively good approximation with respect to USW, but this is not always the case withrespect to ESW. For example, in CVPR 2018, CoBRA achieves 0.004 ESW on average whereasPR4A achieves 0.099 ESW on average. This may be due to the fact that this dataset also containsmany zero similarity scores, and the myopic process of CoBRA locks itself into a bad assignment,which the global optimization performed by PR4A avoids. While CoBRA suffers some loss in welfare, TPMS and PR4A also generate signicant adverseincentives. They incentivize at least one community to deviate in almost every run of each dataset(CV-Pr). While the magnitude of this violation is relatively small when it is nite (except for TPMSin CVPR 2017), TPMS and PR4A also suffer from unbounded core violations in more than half ofthe runs for CVPR 2017; this may again be due to the fact that many zero similarity scores leadto deviations by groups where each agent has zero utility under the assignments produced by thesealgorithms. Of all these results, the high probability of core violation under TPMS and PR4A is perhaps themost shocking result; when communities regularly face adverse incentives, occasional deviationsmay happen, which can endanger the stability of the conference. That said, CoBRA resolves thisissue at a signicant loss in fairness (measured by ESW). This points to the need for nding a middleground where adverse incentives can be minimized without signicant loss in fairness or welfare.",
  "Discussion": "In this work, we propose a way for tackling the poor reviewing problem in large conferences byintroducing the concept of core as a notion of group fairness in the peer review process. Thisfairness principle ensures that each subcommunity is treated at least as well as it would be if it wasnot part of the larger conference community.",
  "that peer review assignment procedures that are currently used in practice, quite often motivatesubcommunities to deviate and build their own conferences": "Our theoretical results serve merely as the rst step toward using it to nd reviewer assignments thattreat communities fairly and prevent them from deviating. As such, our algorithm has signicantlimitations that must be countered before it is ready for deployment in practice. A key limitation isthat it only works for single-author submissions, which may be somewhat more realistic for peer re-view of grant proposals, but unrealistic for computer science conferences. We also assume that eachauthor serves as a potential reviewer; while many conferences require this nowadays, exceptionsmust be allowed in special circumstances. We also limit the number of submissions by any author tobe at most ka/kp, which is a rather small value in practice, and some authors ought to submit morepapers than this. We need to make this assumption to theoretically guarantee that a valid assignmentexists. An interesting direction is to design an algorithm that can produce a valid assignment in the(approximate) core whenever it exists. Finally, deploying group fairness in real-world peer reviewprocesses may require designing algorithms that satisfy it approximately at minimal loss in welfare,as indicated by our experimental results.",
  "David Hartvigsen, Jerry C Wei, and Richard Czuchlewski. The conference paper-reviewerassignment problem. Decision Sciences, 30(3):865876, 1999": "Ari Kobren, Barna Saha, and Andrew McCallum. Paper matching with local fairness con-straints. In Proceedings of the 25th International Conference on Knowledge Discovery andData Mining (KDD), pages 12471257, 2019. Justin Payan and Yair Zick. I will have order! optimizing orders for fair reviewer assignment.In Proceedings of the 31st International Joint Conference on Articial Intelligence (IJCAI),pages 440446, 2022.",
  "Ivan Stelmakh. Towards fair, equitable, and efcient peer review. In Proceedings of the 35thAAAI Conference on Articial Intelligence (AAAI), pages 1573615737, 2021": "Yichong Xu, Xiaofei Zhao, Han Shi, and Nihar B. Shah. On strategyproof conference peerreview. In Proceedings of the 28th International Joint Conference on Articial Intelligence(IJCAI), pages 616622, 2019. Komal Dhull, Steven Jecmen, Pravesh Kothari, and Nihar B. Shah. Strategyproong peerassessment via partitioning: The price in terms of evaluators expertise. In Proceedings of the10th AAAI Conference on Human Computation and Crowdsourcing (HCOMP), pages 5363,2022.",
  "j[m]|Rppi,j| < m kp ka": "where the rst inequality follows since there exists at least one submission of i that is assigned toless than kp agents. Hence, we get that each i U can review more submissions, when PRA-TTCterminates. Now, suppose for contradiction that at the last iteration of PRA-TTC, each agent i Uhas an outgoing edge in the preference graph. In this case, we claim that there exists a directed cyclein the preference graph which is a contradiction since PRA-TTC would not have been terminatedyet. To see that, note that each outgoing edge of an agent i U either goes to another agenti U, since i can review more submissions, or goes to an agent i U whose all submissions arecompletely assigned. In the latter case, i has an outgoing edge to an agent in U by the denitionof the preference graph. Thus, starting from any agent in U, we conclude in an agent in U andeventually we would found a cycle. Therefore, we have that there exists an agent i U thatat the last iteration of the algorithm arbitrary picks her incomplete submission pi, and does nothave any outgoing edge to any other agent. This means that all the agents that can review moresubmissions, already review pi,. Since all the agents in U \\ {i} can review more submissions,we get that all of them are assigned pi,. But since pi, is not completely assigned, we concludethat |U \\ {i}| < kp, which means that |U| kp.",
  "Proposition 1. For each t [|U|], (t) reviews all the incompletely assigned submissions of eachi U \\ {(1), . . . , (t 1), (t)}": "Proof. Since is the topological ordering of the greedy graph, we have that no i U \\{(1), . . . , (t 1), (t)} has an outgoing edge to (t). But from Lemma 2, we get that (t) canreview more submissions, since (t) has submissions that are incompletely assigned which meansthat",
  "where the rst equality follows from the assumption that |Ra(1)| =": "[m] |Rpp(1),| and the sec-ond inequality follows from the facts that |U L\\{(1)}| = kp and each agent has m submissions.But then we would conclude that all the submissions of (1) are completely assigned since (1)has m submissions and each of them should be assigned to kp reviewers which is a contradiction.Moreover, from Proposition 1 we know that (1) reviews all the incomplete assigned submissionsthat belongs to some agent i U\\{(1)}. Hence, we get that since (1) reviews all the incompletelyassigned submissions but she cannot review all the submissions of all agents in i U L \\ {(1)},there exists a completely assigned submission pi, that belongs to some i U L \\ {(1)} andis not reviewed by (1). In addition, since pi, is reviewed by kp agents and not from (1), whilep(1), P (1) is reviewed by strictly less than kp agents, it exists an agent i that reviews the for-mer submission but not the latter. It remains to show that during the execution of the 1-th iterationof the for loop in the second phase of Filling-Gaps, it holds that |Ra(1)| = [m] |Rpp(1),|. Notethat if every time that the algorithm enters the second while loop of the algorithm, this property issatised, then the property remains true at the end of this execution, since as we show above, in thiscase there are pi, and i with the desired properties, and therefore one incompletely assigned sub-mission of (1) is assigned to a new reviewer and concurrently (1) is assigned a new submission toreview. We get that |Ra(1)| = [m] |Rpp(1),| is true during the execution of the 1-st ieration ofthe for loop by noticing that from Lemma 2, we know that this is true when we rst enter the whileloop of the second phase. Suppose that the hypothesis holds for t 1. Note that from the base case and the hypothesis, atiteration t, all the submissions of each agent in i L{(1), . . . , (t1)} are completely assigned.Thus, any incompletely assigned submission, that does not belong to (t), belongs to some agenti U \\ {(1), . . . , (t 1), (t)}. But, from Proposition 1 we already know that (t) reviews anysuch submission. Moreover, we note that (t) cannot review all the submissions of all the agents inU L \\ {(t)}. Indeed, if we assume for contradiction that (t) reviews all the submissions of allthe agents in U L \\ {(t)}, then we have that",
  "where the rst inequality follows from the assumption that |Ra(t)| =": "[m] |Rpp(t),| and thesecond follows from the facts that |U L \\ {(t)}| = kp and each of them has m submissions,which would imply that all the submissions of (t) are completely assigned. Hence, we get thatsince (t) reviews all the incompletely assigned submissions but cannot review all the submissionsof all agents in i U L \\ {(t)}, there exists a completely assigned submission that belongs",
  "to some i U L \\ {(t)} and is not reviewed by (t). Moreover, we show that there existsi that reviews pi,, but does not review p(t),": "P (t). Indeed, since pi, is reviewed by kpagents and not from (t), while p(t), is reviewed by strictly less than kp agents, it exists an agentthat reviews the former submission but not the latter. It remains to show that during the executionof the t-th iteration of the for loop in the second phase of Filling-Gaps, it holds that |Ra(t)| =[m] |Rpp(t),|. Note that if when we rst enter the while loop of the t-th iteration it is indeed truethat |Ra(t)| = [m] |Rpp(t),|, then during the whole execution of the while loop this propertyremains true, since as we show above, in this case there are pi, and i with the desired properties.From Lemma 2, we know that this is true when we enter the second phase of Filling-Gaps. Before,round t, if (t) is assigned a new submission to review, she is removed one of the old assignedsubmissions, while none of her incompletely assigned submissions is assigned to any agent. Hence,indeed we have the desired property, when we rst enter the t-th round.",
  "We are now ready to prove Lemma 1": "Proof of Lemma 1. We partition the agents in N, into two parts N1 and N2, where N1 containsall the agents that do not belong in U that PRA-TTC returns and N2 = N \\ N1. We proceed byseparately showing that the assignment that CoBRA returns is valid over the agents in N1 and overthe agent in N2. Valid Assignment over the agents in N1.Note that in PRA-TTC, each submission is assignedto at most kp reviewers and therefore, during the execution of PRA-TTC, for each i N, it holdsthat j[m] |Rppi,j| kp m. From Lemma 2, since kp m ka, we get that in PRA-TTC, eachi N1 is not assigned more than ka papers to review until the point where all of her submissionsbecome completely assigned. After that point, an agent may still participate in a cycle as long as shereviews strictly less than ka submissions. Therefore, when we exit PRA-TTC, each agent in N1 doesnot review more than ka submissions and all her submissions are completely assigned. In Filling-Gaps, from Lemma 4, we get that if an agent in N1 is assigned a new submission to review, she isremoved one of the submissions that she already reviews. Moreover, the assignments of submissionsthat belong to agents in N1 do not change. Hence, we can conclude that the assignment that CoBRAreturns is valid with respect to the agents in N1. Valid Assignment over the agents in N2.From Lemma 2, we get that during the execution ofPRA-TTC each agent i that is included in U that PRA-TTC returns reviews less than ka submissions,since some of her submissions are not completely assigned (which means that [m] |Rppi,| <kp m). From the same lemma, we have that after the execution of the rst phase of Filling-Gaps,it holds that |Rai | = [m] |Rppi,|. Next, we show that this property remains true after the secondphase of Filling-Gaps. Indeed, from Lemma 4, we have that during the second phase of Filling-Gaps, if i U is assigned a new submission to review without one of her incompletely assignedsubmissions is assigned to a new reviewer, she is removed one of her assigned submissions; on theother hand, if one of her incompletely assigned submissions is assigned to a new reviewer, she is alsoassigned to review a new submission. Lastly, from Lemma 4, we conclude that in the second phaseof Filling-Gaps, all the submissions become eventually completely assigned since in each iterationof the while loop, an incompletely assigned submission is assigned to one more reviewer. Thereforein the assignment that Filling-Gaps returns, no agent in N2 reviews more than ka submissions andall the submissions of the agents in N2 are completely assigned, which means that the assignment isvalid with respect to the agents in N2 as well.",
  "BSupplementary Experiments": "In , we show that TPMS and PR4A often motivate group of authors to deviate and redis-tribute their submissions among themselves. The size of a deviating groups is also an interestingmeasure, for evaluating if such a group indeed consists a distinct subcommunity of researchers thathas incentives to build its own conference rather than an extremely tiny group of authors that couldlocally benet by exchanging their papers for reviewing. In , we can see the maximum sizeof a successfully deviating coalition, averaged across 100 runs, together with the standard error. Asbefore, each run is a subsampled dataset of size 100, so these can be interpreted as percentages. It",
  "ICLR 18TPMS11.25 1.76PR4A15.01 1.76: Largest Size of Deviating Group on CVPR 2017 and 2018, and ICLR 2018": "seems that under both TPMS and PR4A across all three datasets, the largest deviating communitiesare 6-15% of the conference size, which we can indeed reect the sizes of some of the largest sub-communities at CVPR and ICLR. Of course, there are deviating groups with smaller size as wellwhich can consist smaller communities."
}