{
  "Abstract": "This research introduces MLB-YouTube, a new and complex dataset created fornuanced activity recognition in baseball videos. This dataset is structured tosupport two types of analysis: one for classifying activities in segmented videosand another for detecting activities in unsegmented, continuous video streams. Thisstudy evaluates several methods for recognizing activities, focusing on how theycapture the temporal organization of activities in videos. This evaluation startswith categorizing segmented videos and progresses to applying these methodsto continuous video feeds. Additionally, this paper assesses the effectiveness ofdifferent models in the challenging task of forecasting pitch velocity and typeusing baseball broadcast videos. The findings indicate that incorporating temporaldynamics into models is beneficial for detailed activity recognition.",
  "Introduction": "Action recognition, a significant problem in computer vision, finds extensive use in sports. Profes-sional sporting events are extensively recorded for entertainment, and these recordings are invaluablefor subsequent analysis by coaches, scouts, and media analysts. While numerous game statisticsare currently gathered manually, the potential exists for these to be replaced by computer visionsystems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)to automatically record pitch speed and movement, utilizing a network of high-speed cameras andradar to collect detailed data on each player. Access to much of this data is restricted from the publicdomain. This paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-ties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition ordetection, our dataset emphasizes fine-grained activity recognition. The differences between activitiesare often minimal, primarily involving the movement of a single individual, with a consistent scenestructure across activities. The determination of activity is based on a single camera perspective. Thisstudy compares various methods for temporal feature aggregation, both for classifying activities insegmented videos and for detecting them in continuous video streams.",
  "Related Work": "The field of activity recognition has garnered substantial attention in computer vision research. Initialsuccesses were achieved with hand-engineered features such as dense trajectories. The focus of morerecent studies has shifted towards the application of Convolutional Neural Networks (CNNs) foractivity recognition. Two-stream CNN architectures utilize both spatial RGB frames and opticalflow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have beendeveloped. The development of these advanced CNN models has been supported by large datasetssuch as Kinetics, THUMOS, and ActivityNet.",
  "MLB-YouTube Dataset": "We have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, availableon YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videosintended for activity recognition and continuous videos designed for activity classification. Thedatasets complexity is amplified by the fact that it originates from televised baseball games, where asingle camera perspective is shared among various activities. Additionally, there is minimal variancein motion and appearance among different activities, such as swinging a bat versus bunting. Incontrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activitieswith diverse settings, scales, and camera angles, our dataset features activities where a single framemight not be adequate to determine the activity. The minor differences between a ball and a strike are illustrated in . Differentiating betweenthese actions requires identifying whether the batter swings or not, detecting the umpires signal() for a strike, or noting the absence of a signal for a ball. This is further complicated becausethe batter or catcher can obstruct the umpire, and each umpire has their unique style of signaling astrike. Our dataset for segmented video analysis comprises 4,290 clips. Each clip is annotated for multiplebaseball actions, including swing, hit, ball, strike, and foul. Given that a single clip may containseveral activities, this is considered a multi-label classification task. presents the completelist of activities and their respective counts within the dataset. Additionally, clips featuring a pitchwere annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. Furthermore, acollection of 2,983 hard negative examples, where no action is present, was gathered. These instancesinclude views of the crowd, the field, or players standing idly before or after a pitch. Examples ofactivities and hard negatives are depicted in . Our continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Everyframe in these videos is annotated with the baseball activities that occur. On average, each continuousclip contains 7.2 activities, amounting to over 15,000 activity instances in total.",
  "Segmented Video Recognition Approach": "We investigate different techniques for aggregating temporal features in segmented video activityrecognition. In segmented videos, the classification task is simpler because each frame corresponds toan activity, eliminating the need for the model to identify the start and end of activities. Our methodsare based on a CNN that generates a per-frame or per-segment representation, derived from standardtwo-stream CNNs using deep CNNs like I3D or InceptionV3. Given video features v of dimensions T D, where T represents the videos temporal length and Dis the features dimensionality, the usual approach for feature pooling involves max- or mean-poolingacross the temporal dimension, followed by a fully-connected layer for video clip classification, asdepicted in (a). This approach, however, yields a single representation for the entire video,losing temporal information. An alternative is to employ a fixed temporal pyramid with variouslengths, as shown in Fig 5(b), dividing the video into intervals of lengths 1/2, 1/4, and 1/8, andmax-pooling each. The pooled features are concatenated, creating a K D representation, where Kis the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip. We also explore learning temporal convolution filters to aggregate local temporal structures. A kernelof size L1 is applied to each frame, enabling each timestep representation to incorporate informationfrom adjacent frames. After applying max-pooling to the output of the temporal convolution, a fully-connected layer is used for classification, as illustrated in (c). While temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.Previous studies have shown that learning the sub-interval to pool is beneficial for activity recognition.These learned intervals are defined by three parameters: a center g, a width , and a stride ,parameterizing N Gaussians. Given the video length T, the positions of the strided Gaussians arefirst calculated as:",
  "where Zm is a normalization constant": "We apply these filters F to the T D video representation through matrix multiplication, yielding anN D representation that serves as input to a fully-connected layer for classification. This methodis shown in Fig 5(d). Additionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden stateas input to a fully-connected layer for classification. We frame our tasks as multi-label classificationand train these models to minimize binary cross-entropy:",
  "Activity Detection in Continuous Videos": "Detecting activities in continuous videos poses a greater challenge. The goal here is to classify eachframe according to the activities occurring. Unlike segmented videos, continuous videos featuremultiple sequential activities, often interspersed with frames of inactivity. This necessitates thatthe model learn to identify the start and end points of activities. As a baseline, we train a singlefully-connected layer to serve as a per-frame classifier, which does not utilize temporal informationbeyond that contained in the features. We adapt the methods developed for segmented video classification to continuous videos by imple-menting a temporal sliding window approach. We select a fixed window duration of L features, applymax-pooling to each window (similar to (a)), and classify each pooled segment. This approachis extended to temporal pyramid pooling by dividing the window of length L into segments of lengthsL/2, L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,and the pooled features are concatenated, yielding a 14 D-dimensional representation for eachwindow, which is then used as input to the classifier. For temporal convolutional models in continuous videos, we modify the segmented video approach bylearning a temporal convolutional kernel of length L and convolving it with the input video features.This operation transforms input of size T D into output of size T D, followed by a per-frameclassifier. This enables the model to aggregate local temporal information. To extend the sub-event model to continuous videos, we follow a similar approach but set T = L inEq. 1, resulting in filters of length L. The T D video representation is convolved with the sub-eventfilters F, producing an N D T-dimensional representation used as input to a fully-connectedlayer for frame classification.",
  "where vt is the per-frame or per-segment feature at time t, H(vt) is the sliding window application ofone of the feature pooling methods, and zt,c is the ground truth class at time t": "A method to learn super-events (i.e., global video context) has been introduced and shown to beeffective for activity detection in continuous videos. This approach involves learning a set of temporalstructure filters modeled as N Cauchy distributions. Each distribution is defined by a center xn and awidth n. Given the video length T, the filters are constructed by:",
  "Implementation Details": "For our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kineticsdatasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliablefeature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenetand Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depthcompared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flowwas computed and clipped to . For InceptionV3, features were computed every 3 frames(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adamoptimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50epochs.",
  "Multi-label Classification": "We assessed various temporal feature aggregation methods by calculating the mean average precision(mAP) for each video clip, a standard metric for multi-label classification. compares theperformance of these methods. All methods surpass mean/max-pooling, highlighting the importanceof preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMsshow some improvement. Temporal convolution offers a more significant performance boost butrequires substantially more parameters (see ). Learning sub-events, as per previous research,yields the best results. While LSTMs and temporal convolutions have been used before, they needmore parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitatesequential processing of video features, whereas other methods can be fully parallelized.",
  "shows the average precision for each activity class. Learning temporal structure is particularlybeneficial for frame-based features (e.g., InceptionV3), which capture less temporal information": "compared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detectingstrikes, hits, foul balls, and hit-by-pitch events, which exhibit changes in video features post-event.For instance, after a hit, the camera often tracks the balls trajectory, while after a hit-by-pitch, itfollows the player to first base, as illustrated in and . : Per-class average precision for segmented videos using two-stream features in multi-label activity classification. Utilizing sub-events to discern temporal intervals of interest provesadvantageous for activity recognition.",
  "Pitch Speed Regression": "Estimating pitch speed from video frames is an exceptionally difficult problem, as it requires thenetwork to pinpoint the pitchs start and end, and derive the speed from a minimal signal. The baseball,often obscured by the pitcher, travels at speeds over 100mph and covers 60.5 feet in approximately 0.5seconds. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,proving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos, werecalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connectedlayer with a single output for pitch speed prediction and minimizing the L1 loss between predictedand actual speeds, we achieved an average error of 3.6mph. compares different models, and illustrates the sub-events learned for various speeds.",
  "Pitch Type Classification": "We conducted experiments to determine the feasibility of predicting pitch types from video, a taskmade challenging by pitchers efforts to disguise their pitches from batters and the subtle differencesbetween pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,utilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.Pose features were considered due to variations in body mechanics between different pitches. Ourdataset includes six pitch types, with results presented in . LSTMs performed worse than thebaseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were theeasiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult(12%).",
  "Continuous Video Activity Detection": "We evaluate models extended for continuous videos using per-frame mean average precision (mAP),with results shown in . This setting is more challenging than segmented videos, requiringthe model to identify activity start and end times and handle ambiguous negative examples. Allmodels improve upon the baseline per-frame classification, confirming the importance of temporalinformation. Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal",
  "Conclusion": "This paper introduces MLB-YouTube, a novel and challenging dataset designed for detailed activityrecognition in videos. We conduct a comparative analysis of various recognition techniques thatemploy temporal feature pooling for both segmented and continuous videos. Our findings reveal thatlearning sub-events to pinpoint temporal regions of interest significantly enhances performance insegmented video classification. In the context of activity detection in continuous videos, we establishthat incorporating convolutional sub-events with a super-event representation, creating a three-levelactivity hierarchy, yields the most favorable outcomes."
}