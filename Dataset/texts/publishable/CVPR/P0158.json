{
  "Abstract": "Data-free knowledge distillation is able to utilize theknowledge learned by a large teacher network to augmentthe training of a smaller student network without access-ing the original training data, avoiding privacy, security,and proprietary risks in real applications. In this line of re-search, existing methods typically follow an inversion-and-distillation paradigm in which a generative adversarial net-work on-the-fly trained with the guidance of the pre-trainedteacher network is used to synthesize a large-scale sampleset for knowledge distillation. In this paper, we reexam-ine this common data-free knowledge distillation paradigm,showing that there is considerable room to improve theoverall training efficiency through a lens of small-scaleinverted data for knowledge distillation. In light of threeempirical observations indicating the importance of how tobalance class distributions in terms of synthetic sample di-versity and difficulty during both data inversion and distil-lation processes, we propose Small Scale Data-free Knowl-edge Distillation (SSD-KD). In formulation, SSD-KD intro-duces a modulating function to balance synthetic samplesand a priority sampling function to select proper samples,facilitated by a dynamic replay buffer and a reinforcementlearning strategy. As a result, SSD-KD can perform dis-tillation training conditioned on an extremely small scaleof synthetic samples (e.g., 10 less than the original train-ing data scale), making the overall training efficiency oneor two orders of magnitude faster than many mainstreammethods while retaining superior or competitive model per-formance, as demonstrated on popular image classificationand semantic segmentation benchmarks. The code is avail-able at",
  ". Introduction": "For computer vision applications on resource-constraineddevices, how to learn portable neural networks yet with sat-isfied prediction accuracy is the key problem. Knowledgedistillation (KD) , which leveragesthe information of a pre-trained large teacher network topromote the training of a smaller target student network onthe same training data, has become a mainstream solution.Conventional KD methods assume that the original train-ing data is always available. However, accessing the sourcedataset on which the teacher network was trained is usu-ally not feasible in practice, due to its potential privacy orsecurity or proprietary or huge-size concerns. To relax theconstraint on training data, knowledge distillation under adata-free regime has recently attracted increasing attention.The basic idea of Data-free Knowledge Distillation (D-KD) is to construct synthetic samples for knowl-edge distillation conditioned on the pre-trained teacher net-work, which would match the underlying distribution ofthe original training data. Existing top-performing D-KDmethods generally adopt an ad-versarial inversion-and-distillation paradigm.Under thisparadigm, (1) during the inversion process, a generator istrained by taking the pre-trained teacher network as the dis-criminator; (2) during the subsequent knowledge distilla-tion process, the on-the-fly learned generator will synthe-size pseudo samples for training the student network. How-ever, adversarial D-KD methods usually require generatinga large number of synthetic samples (compared to the orig-inal training dataset size) in order to guarantee trustworthyknowledge distillation. This poses a heavy burden on train-ing resource consumption, suppressing their use in real ap-plications. In the recent work of , the authors present aneffective meta-learning strategy that seeks common featuresand reuses them as initial priors to reduce the number of it-eration steps required to reach the convergence of the gen-erator. Although faster data synthesis can be attained, still needs to generate a sufficiently large number of syn-",
  "Category": "Accuracy (%) CIFAR-10 (T:WRN40-2, S:WRN16-1) Vanilla KDDeepInvSSD-KD . Comparison of knowledge distillation (KD) using original samples vs. synthetic samples, under the same training data scale:5000 samples (10% of the CIFAR-10 training dataset size). In such small-scale KD regime, the student models (DeepInv and SSD-KD) trained on synthetic samples always show much better accuracy than the counterparts (Vanilla KD ) trained on original samples.",
  "(a) Difficulty distribution.(b) Diversity distribution": ". Comparison of synthetic sample distributions collected from two top-performing adversarial D-KD methods (DeepInv andFast10 ) and our SSD-KD on CIFAR-100 dataset. Comparatively, a shows that our method can better balance the difficultydistribution of synthetic samples while encouraging the generator to invert more hard samples, and b further shows that our methodcan better balance the diversity distribution of synthetic samples across different categories. thetic samples to ensure effective knowledge distillation,neglecting the efficiency of the following knowledge dis-tillation process which will become the major bottleneck tothe overall training efficiency. In a nutshell, there is no re-search effort made to improve the overall training efficiencyof D-KD via jointly considering data inversion and knowl-edge distillation processes, to the best of our knowledge. To remedy this critical gap, this paper presents the firstfully efficient D-KD approach termed Small Scale Data-freeKnowledge Distillation (SSD-KD). Our SSD-KD improvesthe overall training efficiency of the adversarial inversion-and-distillation paradigm from a novel small data scaleperspective. In this work, data scale refers to the totalnumber of inverted samples used in knowledge distillationduring a training epoch. The formulation of SSD-KD is in-spired by three empirical observations. On different pairsof teacher-student networks, we first observe that the stu-dent networks trained on synthetic samples tend to showmuch better performance than their corresponding counter-parts trained on original samples when significantly reduc-ing data scales of synthetic samples and original samples tothe same (e.g., 10% of the source training dataset size), asillustrated in . Note that synthetic samples are gen-erated with the guidance of the teacher network pre-trainedon the whole source dataset, which naturally reflect differ-ent views of the original data distribution. Under a smallenough data scale, this makes synthetic samples have supe-rior capability to original samples in fitting the underlying distribution of the whole source dataset. This inspiring ob-servation indicates that if we can construct a small-scale setof high-quality synthetic samples, a promising way towardfully efficient D-KD would be created. In principle, webelieve a high-quality small-scale synthetic dataset shouldhave well-balanced class distributions in terms of both syn-thetic sample diversity and difficulty. However, our othertwo observations indicate that existing D-KD methods in-cluding both conventional and the most efficient designs donot have good capabilities to balance the aforementionedtwo class distributions of synthetic samples under a smalldata scale, as illustrated in . Note that there alreadyexist a few D-KD methods to enhance the diversity of syn-thetic samples , but the diversity of syntheticsamples in terms of sample difficulty is not explored yet. Driven by the above observations and analysis, we comeup with our SSD-KD which introduces two interdepen-dent modules to significantly accelerate the overall train-ing efficiency of the predominant adversarial inversion-and-distillation paradigm. The first module of SSD-KD relies ona novel modulating function that defines a diversity-awareterm and a difficulty-aware term to jointly balance the classdistributions of synthetic samples during both data synthe-sis and knowledge distillation processes in an explicit man-ner. The second module defines a novel priority samplingfunction facilitated by a reinforcement learning strategy thatselects a small portion of proper synthetic samples fromcandidates stored in a dynamic replay buffer for knowledge distillation, further improving the end-to-end training effi-ciency. Benefiting from these two modules, SSD-KD hastwo appealing merits. On one side, SSD-KD can performdistillation training conditioned on an extremely small scaleof synthetic samples (10 less than original training datascale), making the overall training efficiency one or twoorders of magnitude faster than many mainstream D-KDmethods while retaining competitive model performance.On the other side, SSD-KD attains largely improved per-formance in student model accuracy and maintains overalltraining efficiency when relaxing the data scale of syntheticsamples to a relatively large number (which is still smallerthan those for existing D-KD methods). We validate thesemerits of our method by lots of experiments on popular im-age classification and semantic segmentation benchmarks.",
  ". Related Work": "Data-free knowledge distillation. D-KD is originally ex-plored by which assumes that layer-wise activationrecords of a well-trained teacher network pre-computed ontraining samples is available. The prevailing inversion-and-distillation paradigm for D-KD without reliance on the orig-inal training data or the recorded metadata is introducedin . In the inversion process, synthetic training samplesare generated by leveraging the information learned by theteacher network, whose distribution is expected to fit the un-derlying distribution of the original training dataset. In thedistillation process, the student network is trained on syn-thetic samples by forcing it to match the predictions of theteacher network. Based on this paradigm, use gener-ative adversarial networks for data inversion. SubsequentD-KD methods mostly follow this adversarial inversion-and-distillation paradigm.They try to improve the datainversion process from different aspects, such as enhanc-ing synthetic sample discrimination with contrastive learn-ing or an ensemble of generators , combating dis-tribution shift with momentum adversarial learning ormeta learning , and promoting adversarial learning withdata augmentation . Our method intends to improvethe overall training efficiency of the adversarial D-KD, anddiffers from these methods both in focus and formulation.Efficient synthetic data sampling. How to select propersynthetic samples for knowledge distillation is essential inD-KD research.Existing methods com-monly rely on a memory bank to store synthetic samples,and directly update synthetic samples without consideringthe efficiency of the following knowledge distillation pro-cess.In sharp contrast, our method introduces a rein-forcement learning strategy that adaptively selects appropri-ate synthetic samples to update a portion of existing sam-ples in a dynamic replay buffer by explicitly measuringtheir priorities in terms of jointly balancing sample diver-sity and difficulty, significantly improving knowledge dis-",
  ". Preliminaries: D-KD": "Let ft(; t) be a teacher model pre-trained on the originaltask dataset that is no longer accessible, the goal of D-KDis to first construct a set of synthetic training samples x viainverting the data distribution information learned by theteacher model, on which a target student model fs(; s)then can be trained by forcing it to mimic the teachersfunction. Existing D-KD methods mostly use a generativeadversarial network g(; g) for producing synthetic trainingsamples x = g(z; g) from the latent noise input z, whichis trained by taking the teacher model as the discriminator.The optimization of D-KD contains a common distilla-tion regularization to minimize the teacher-student functiondiscrepancy LKD(x) = DKD(ft(x; t)fs(x; s))1 basedon the KL-divergence, and a task-oriented regularizationLTask(x), e.g., the cross-entropy loss using the teacherspredication as the ground truth. Besides, since D-KD isprimarily based on the assumption that the teacher modelhas been optimized to be capable of capturing the sourcetraining data distribution after pre-training, recent D-KDmethods introduce an extra loss to regularizethe statistics ( Batch-Normalization (BN) parameters) of thetraining data distribution during the data inversion process,",
  "l(x)E(l)2+2l (x)E(2l )2, (1)": "where l() and l() denote the batch-wise mean and vari-ance estimates of feature maps at the l-th layer, respectively;E() over the BN statistics can be approximately substitutedby running mean or variance.The effectiveness of D-KD heavily depends on the qual-ity of synthetic samples which are inverted from leverag-ing the knowledge of the pre-trained teacher model. Theprevailing adversarial D-KD paradigm consists of two pro-cesses, namely data inversion and knowledge distillation.From both perspectives of efficiency and efficacy, on theone hand, the data inversion process largely affects theoptimization performance of the student model; on theother hand, the training time cost of knowledge distillationemerges as a significant constraint to the overall training ef-ficiency of D-KD.",
  "D-KD with memory bank": ". Comparison of optimization pipelines for existing adversarial D-KD methods including both conventional family (left) and more efficient family (middle), and our SSD-KD (right). Our SSD-KD formulates a reinforcement learning strategy thatcan flexibly seek appropriate synthetic samples to update a portion of existing samples in a dynamic replay buffer by explicitly measuringtheir priorities in terms of jointly balancing sample diversity and difficulty distributions. See the method section for notation definitions. inverted data for knowledge distillation. SSD-KD laysemphasis on instructing the data inversion process with thefeedback of both the pre-trained teacher and the knowl-edge distillation process, significantly accelerating the over-all training efficiency. Following the notations in the previ-sion subsection, the optimization goal of our SSD-KD isdefined as",
  "Under the constraint of BN estimates, with (x), we en-courage the generator to explore as tough synthetic sam-ples (w.r.t. the teacher model) as possible, as introducedin Sec. 3.3": "Instead of applying a random sampling strategy to se-lect samples for knowledge distillation, we adopt a re-weighting strategy to control the sampling process. Weabuse notation slightly with to represent applying thestrategy based on priority sampling function , with moredetails in Sec. 3.4. Each synthetic sample is not only prioritized by its modu-lating function (x) but also is reweighted at the samplingstage that reuses the same intermediate values as (x).Although the D-KD pipeline allows training samples tobe synthesized and served for training the student modelon the same task. However, there is a large extent of dataredundancy that hinders the training efficiency of D-KDmethods. In the following sections, we detail our SSD-KD,a fully efficient D-KD method that is capable of using anextremely small scale of synthetic data yet achieving com-petitive performance compared to existing D-KD methods.The pipeline of SSD-KD is summarized in Alg. 1 andthe comparison of optimization pipelines for existing ad-versarial D-KD methods including both conventional fam-ily and more efficient family , and our",
  ". Data Inversion with Distribution Balancing": "We provide to demonstrate the data redundancy ofD-KD that results from a large imbalance of the syntheticdata. The left two sub-figures of depict the distribu-tion of categories predicted by the teacher model, indicatinga significant imbalance in data categories. The right twosub-figures of show sample accounts of different barsthat correspond to different prediction difficulties (the diffi-culty is measured by the predicted probability by the teachermodel). For D-KD, it indicates that generating samples withonly the instruction of teacher-student discrepancy resultsin a sharp distribution over the sample difficulty and tendsto obtain easily predicted data samples. We argue that forthe D-KD task where data samples are all synthesized, thedata-generating process needs to consider both the teacher-student discrepancy and the pre-trained knowledge of theteacher itself, by which our SSD-KD proposes diversity-aware and difficulty-aware data synthesis, as detailed below.Diversity-aware balancing. We first propose to addressthe issue of imbalanced sample difficulty in the data inver-sion process. Specifically, we maintain a replay buffer Bthat stores a constant amount (denoted as |B|) of syntheticdata samples. For each data sample x in B, we penalize itstotal amount of samples that share the same predicted cat-egory (by the teacher model) with x. To realize this, weadopt a diversity-aware balancing term that encourages thegenerator to synthesize samples with infrequent categories,which will be shown in Eq. (3).Difficulty-aware balancing. Drawing inspiration fromthe field of object detection that utilizes focal loss for largelyimbalanced samples , for each sample x, we fur-ther introduce a difficulty-aware balancing term on the pre-dicted probability pT (x). Here, difficult synthetic samplesare considered as those with low-confidence predictions bythe teacher model, which are encouraged by the difficulty-aware balancing term as will be given in Eq. (3).In summary, we introduce a modulating function (x) toadjust the optimization of the generator based on the pre- diction feedback from the pre-trained teacher model. (x)is expected to balance the category distribution and dynam-ically distinguish between easy and hard synthetic samples,by which easy samples no longer overwhelm the distilla-tion process. Formally, for a synthetic data sample x B,its modulating function (x) is computed by",
  ",(3)": "where cT (x) and pT (x) refer to the predicted categoryindex and probability (or confidence) by the pre-trainedteacher model, respectively; IcT (x)=cT (x) denotes an indi-cator function that equals to 1 if the predicted category of x is the same with that of x, while otherwise 0; is a hyper-parameter.We highlight two properties of the modulating function(x). Firstly, for the data sample x with a high predic-tion certainty w.r.t. the teacher model (i.e., considered asan easy sample), (x) approaches to a low value and thusresults in low impact on the task-oriented loss LTask(x) inEq. (2). Secondly, when the category distribution of syn-thetic data samples in B is largely imbalanced as predictedby the teacher network, the sample x of which the categoryshares with more samples in B is penalized and thus thecorresponding LTask(x) is weakened by (x).Although Eq. (3) implies that the value of the modulatingfunction (x) is partially determined by the current replaybuffer B, note that B changes dynamically and is also af-fected by (x). This is due to that the term (x)LTask(x)in Eq. (2) directly optimizes the generator that synthesizesdata samples to compose B. In this sense, the balancing incategory diversity is maintained during training given themutual effects of B and (x). We find it particularly impor-tant in the data inversion process for balancing categories ofdata samples.With both balancing terms, as shown in , ourmethod (SSD-KD) generates mild sample distributions interms of both sample category and difficulty.",
  ". Distillation with Priority Sampling": "The original prioritized experience replay method reuses important transitions more frequently and learnsmore efficiently.Differently, rather than obtaining thereward from the environment, our prioritized samplingmethod is designed to fit in data-free knowledge distillationand get feedback from the framework itself. In other words,the prioritized sampling method performs the opposite roleof the previous data-free knowledge distillation methods: itfocuses on training a sparse set of highly prioritized samplesinstead of uniform sampling to speed up training.By Eq. (3), we sample the synthetic data x from the cur-rent replay buffer B. Instead of uniformly sampling x, we",
  "i(x) = wi1(x)KL(ft(x; t)||fs(x; s)),(4)": "where as mentioned in Sec. 3.1, KL denotes the KL-divergence between the softmax outputs of logits ft(x; t)and fs(x; s); t, s depend on the training step i; wi(x)is the calibration term for normalizing samples in B,as will be formalized in Eq. (5), especially, when i = 0,w1(x) = 1.The training of knowledge distillation with random up-dates relies on those updates corresponding to the same dis- . Performance comparison of Fast2 (the current most efficient D-KD method) and our SSD-KD, in terms of top-1 classificationaccuracy (%) and overall training time cost (hours). Our SSD-KD performs with a very small training data scale: 5000 synthetic samples,i.e., 10% of the original training dataset size. All image classification results in this and the other tables are averaged over three independentruns, and the methods Teacher and Student are performed on the whole original training dataset.",
  "xB|i(x)| + ,(6)": "where is a small positive constant which prevents theedge-case of transitions not being selected once their pri-ority is zero.The priority sampling function (x) has two noteworthyproperties. Firstly, as the delta value increases, (x) reflectsa greater information discrepancy between the teacher andstudent models for the synthetic samples in the current B.The student model should therefore be optimized from sam-ples with greater information discrepancy, as this facilitatesthe faster acquisition of the teacher model. Secondly, (x)dynamically changes with each update iteration of the stu-dent and generative models. Consequently, when the stu-dent model acquires the teacher models capabilities on cer-tain samples, it continues to learn from samples with largerdifferences relative to the teacher model based on the newsample distribution. This further enhances the performanceof the student model.",
  ". Experimental Details": "We conduct comprehensive experiments on image classifi-cation and semantic segmentation tasks to evaluate the ef-fectiveness and study the design of our SSD-KD.Datasets. For image classification experiments, we useCIFAR-10 and CIFAR-100 , two most popular datasets for D-KD research.For semantic segmentation experi-ments, we use the NYUv2 dataset . On each dataset,the baseline models are trained with its standard data split.Training Setups. We follow the settings of for ba-sic experiments and comparisons. Regarding experimentson the CIFAR-10 and CIFAR-100 datasets, we use 5 dif-ferent teacher-student model pairs having either the sametype or different type network architectures (see ,2). Regarding experiments on the NYUv2 dataset, we usetwo Deeplabv3 models as a teacher-student model pair(see ). Unless otherwise stated, we always adopt thesame basic settings as in for experiments, including thenumber of training epochs, the optimizer, the weight decay,etc.Evaluation Metrics.Besides comparing the studentmodel accuracy (top-1 accuracy for image classification andmean Intersection over Union (IoU) for semantic segmen-tation), we also compare the overall training time cost ofexisting mainstream D-KD methods and our SSD-KD. For each teacher-student model pair, wemeticulously record the total time cost (hours) for each runof the end-to-end training by all methods. In order to guar-antee a fair comparison, all training speed assessments areperformed on 1 NVIDIA V100 GPU using 12 cores of IntelXeon Gold 6240R CPU. Our experiments are implementedwith the PyTorch library.For each teacher-studentmodel pair, the experiment is conducted with three inde-pendent runs for all methods, and in comparison we reportthe averaged results, unless otherwise stated.",
  ". Experimental Results": "Results on image classification task. As the main focusof our work is to improve the overall training efficiency ofthe adversarial data-free knowledge distillation paradigm,we first compare the proposed SSD-KD with the currentmost efficient D-KD method Fast2 on the CIFAR-10and CIFAR-100 datasets. From the results shown in , we can observe: (1) on the CIFAR-10 dataset, our SSD-KD (using a very small data scale: 5000 synthetic samples, . Performance comparison of existing top-performing D-KD methods and our SSD-KD, in terms of top-1 classification accuracy(%) and overall training time cost (hours). For our SSD-KD, we relax the data scale of synthetic samples to be similar to that for Fast5.",
  "CIFAR-100": "Teacher78.0571.3275.8375.8375.83Student77.1077.1065.3172.1973.56DeepInv 61.32(25.88h)54.13(13.13h)53.77(10.62h)61.33(13.86h)61.34(11.68h)CMI 77.04(15.42h)70.56(11.28h)57.91(10.01h)68.88(11.57h)68.75(10.53h)DAFL 74.47(303.54h)54.16(209.82h)20.88(96.62h)42.83(157.49h)43.70(118.67h)DFQ 77.01(152.18h)66.21(105.14h)51.27(48.47h)54.43(78.83h)64.79(59.48h)ZSKT 67.74(304.13h)54.31(209.63h)36.66(96.87h)53.60(157.68h)54.59(118.84h)Fast5 72.82(4.35h)65.28(3.06h)52.90(1.56h)61.80(2.35h)63.83(1.86h)Fast10 74.34(4.50h)67.44(3.12h)54.02(1.65h)63.91(2.42h)65.12(1.95h)SSD-KD75.16(4.22h)68.77(2.94h)55.61(1.52h)64.57(2.27h)65.28(1.78h) i.e., 10% of the original training dataset size) shows largeimprovements in training efficiency to Fast2 (at least 1.90and at most 3.92 training speedup, on 5 teacher-studentmodel pairs), and gets better student models on 4 out of5 teacher-student model pairs; (2) on the more challeng-ing CIFAR-100 dataset, our SSD-KD shows a very simi-lar training speedup trend against Fast2 as on the CIFAR-10 dataset, but gets significantly better student models onall 5 teacher-student model pairs (at least 3.29% and atmost 10.19% absolute top-1 accuracy gain to Fast2). Thesuperior performance of SSD-KD against Fast2 validatesthe efficacy of our small-scale data inversion and samplingmechanism which can flexibly balance class distributionsin terms of synthetic sample diversity and difficulty duringboth data inversion and distillation processes.Next, we compare the proposed SSD-KD with exist-ing top-performing D-KD methods including DeepInv ,CMI , DAFL , DFQ , ZSKT , Fast5 andFast10 . In order to get improved student model per-formance, we relax the data scale of synthetic samples inour SSD-KD to be similar to that for Fast5. Experimentalresults are summarized in . Compared to these main-stream D-KD methods, our SSD-KD gets very competitiveperformance in student model accuracy, and significantlybetter performance in overall training efficiency, on all 5teacher-student model pairs.In light of the results in , 2, it can be seen that ourSSD-KD makes the overall training efficiency one or twoorders of magnitude faster than most D-KD methods whileretaining competitive student model performance. . Performance comparison on the NYUv2 dataset. Theteacher model is pre-trained on the ImageNet dataset and fine-tuned on the NYUv2 dataset, and the student model is trained fromscratch. The results of reference methods are collected from .",
  "DFAD 960K (synthetic)0.364DAFL 960K (synthetic)0.105Fast10 17K (synthetic)0.366SSD-KD16K (synthetic)0.384": "Results on semantic segmentation task. To further val-idate the generalization ability of our method, we comparethe performance of SSD-KD with existing D-KD methodson the NYUv2 dataset. summarizes the results. Wecan see that our SSD-KD not only achieves state-of-the-artperformance in terms of model accuracy, but also is signif-icantly more efficient than other D-KD methods in termsof the training data scale. The overall training time costfor our SSD-KD and Fast10 is 8.9 hours and 9.5 hours, re-spectively. Besides, the student model trained by SSD-KDeven outperforms the baseline model trained on the originaltraining data, as NYUv2 is a small-scale dataset.",
  ". Ablation Studies": "Effect of the synthetic data scale. So far, we have al-ready demonstrated that SSD-KD has the appealing capa-bility to attain efficient and effective end-to-end distillationtraining using a small amount of synthetic samples. To bet- 0.0%5.0%10.0%15.0%20.0% Data scale Accuracy (%) CIFAR-10 T:ResNet-34, S:ResNet-18T:VGG-11, S:ResNet-18T:WRN40-2, S:WRN16-1 0.0%5.0%10.0%15.0%20.0% Data scale 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Training time (h) CIFAR-10 T:ResNet-34, S:ResNet-18T:VGG-11, S:ResNet-18T:WRN40-2, S:WRN16-1 0.0%5.0%10.0%15.0%20.0% Data scale Accuracy (%) CIFAR-100 T:ResNet-34, S:ResNet-18T:VGG-11, S:ResNet-18T:WRN40-2, S:WRN16-1 0.0%5.0%10.0%15.0%20.0% Data scale 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Training time (h) CIFAR-100 T:ResNet-34, S:ResNet-18T:VGG-11, S:ResNet-18T:WRN40-2, S:WRN16-1 . Performance comparison of SSD-KD under different synthetic data scales against the original training dataset size, in terms oftop-1 classification accuracy (%) and overall training time cost (hour).",
  ". Visualization examples of synthetic image samples gen-erated by Fast10 and our SSD-KD for the NYUv2 dataset": "ter explore the boundary of this capability, we conduct anablation with three teacher-student model pairs, includingResNet34ResNet18, VGG11ResNet18, and WRN40-2WRN16-1. In the experiments, we decrease the syn-thetic data scale used in SSD-KD from 50,000(100%relative to the original training data size) to {10000(20%),5000(10%), 2500(%5), 500(1%)} for the CIFAR-10 andCIFAR-100 datasets.As shown in , the accuracyof the student model trained by SSD-KD remains stableacross a relatively large synthetic data scale range, for allteacher-student model pairs. When decreasing the syntheticdata scale, the overall training time cost appears to decreasenearly linearly, while the student model accuracy drop ismild (less than 10% even for the synthetic data scale 500).Effect of the core modules.In , we providean ablation to scrutinize our SSD-KD systematically. We observe that: (1) the two modulating functions (consistingof a diversity-aware term and a difficulty-aware term, seeEq. (3)) and the priority sampling function, are both criticalto our SSD-KD; (2) the combination of them strikes a goodtradeoff between model accuracy and training efficiency.Visualization of data inversion. shows examplesof synthetic images inverted by Fast10 and our SSD-KD forthe NYUv2 dataset. Compared to Fast10, our method canbetter invert texture information and has less noise. 5. ConclusionIn this paper, we presented SSD-KD, the first fully efficientmethod to advance adversarial data-free knowledge distilla-tion research. Benefiting from a small-scale data inversionand sampling mechanism based on a modulating functionand a priority sampling function, SSD-KD can flexibly bal-ance class distributions in terms of synthetic sample diver-sity and difficulty during both data inversion and distillationprocesses, attaining efficient and effective data-free knowl-edge distillation. Extensive experiments on image classifi-cation and semantic segmentation benchmarks validate theefficacy of SSD-KD. We hope our work can inspire futureresearch on efficient D-KD designs."
}