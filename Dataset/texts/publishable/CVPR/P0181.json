{
  "Abstract": "Learning to infer labels in an open world, i.e., in anenvironment where the target labels are unknown, is animportant characteristic for achieving autonomy. Founda-tion models pre-trained on enormous amounts of data haveshown remarkable generalization skills through prompting,particularly in zero-shot inference. However, their perfor-mance is restricted to the correctness of the target labelssearch space. In an open world, this target search space canbe unknown or exceptionally large, which severely restrictsthe performance of such models. To tackle this challengingproblem, we propose a neuro-symbolic framework calledALGO - Action Learning with Grounded Object recogni-tion that uses symbolic knowledge stored in large-scaleknowledge bases to infer activities in egocentric videoswith limited supervision using two steps. First, we proposea neuro-symbolic prompting approach that uses object-centric vision-language models as a noisy oracle to groundobjects in the video through evidence-based reasoning. Sec-ond, driven by prior commonsense knowledge, we discoverplausible activities through an energy-based symbolic pat-tern theory framework and learn to ground knowledge-based action (verb) concepts in the video. Extensive exper-iments on four publicly available datasets (EPIC-Kitchens,GTEA Gaze, GTEA Gaze Plus) demonstrate its perfor-mance on open-world activity inference.",
  ". Introduction": "Humans display a remarkable ability to recognize unseenconcepts (actions, objects, etc.)by associating knownconcepts gained through prior experience and reasoningover their attributes.Key to this ability is the notionof grounded reasoning, where abstract concepts can bemapped to the perceived sensory signals to provide evidenceto confirm or reject hypotheses. In this work, we aim tocreate a computational framework that tackles open-world egocentric activity understanding.We define an activityas a complex structure whose semantics are expressed bya combination of actions (verbs) and objects (nouns). Torecognize an activity, one must be cognizant of the objectlabel, action label, and the possibility of any combinationsince not all actions are plausible for an object. Supervisedlearning approaches have been the domi-nant approach to activity understanding but are trained in aclosed world, where there is an implicit assumption aboutthe target labels. The videos during inference will alwaysbelong to the label space seen during training. Zero-shotlearning approaches relax this assumptionby considering disjoint seen and unseen label spaceswhere all labels are not necessarily represented in the train-ing data. This setup is a known world, where the target la-bels are pre-defined and aware during training. In this work,we define an open world to be one where the target labelsare unknown during both training and inference. The goalis to recognize elementary concepts and infer the activity.We propose to tackle this problem using a neuro-symbolicframework that leverages advances in multi-modal founda-tion models to ground concepts from symbolic knowledgebases, such as ConceptNet , in visual data. The over-all approach is shown in . Using the energy-basedpattern theory formalism to represent symbolicknowledge, we ground objects (nouns) using CLIP asa noisy oracle. Driven by prior knowledge, novel activities(verb+noun) are inferred, and the associated action (verb) isgrounded in the video to learn visual-semantic associationsfor novel, unseen actions.The contributions of this work are three-fold: (i) Wepresent a neuro-symbolic framework to leverage composi-tional properties of objects to prompt CLIP for evidence-based grounding.(ii) We propose object-driven activitydiscovery as a mechanism to reason over prior knowledgeand provide action-object affinities to constrain the searchspace. (iii) We demonstrate that the inferred activities canbe used to ground unseen actions (verbs) from symbolicknowledge in egocentric videos, which can generalize to",
  "HasProperty": ". Overall architecture of the proposed approach (ALGO) is illustrated here. Using a two-step process, we first ground the objectswithin a gaze-driven ROI using CLIP as a noisy oracle before reasoning over the plausible activities performed in the video. unseen and unknown action spaces.Egocentric video analysis has been extensively ex-plored in computer vision literature, having applications invirtual reality and human-machine interaction. WhileSupervised learning has been the dominant approach suchas, , , along with some zero-shotlearning approaches , KGL is one of the firstworks to address the problem of open-world understand-ing.They represent knowledge elements derived fromConceptNet , using pattern theory .Theirmethod depends on an object detector to link objects ina source domain before translating concepts to the targetspace via ConceptNet-based semantic connections. How-ever, this approach has drawbacks: (i) false alarms mayarise if the initial object detector misses the object, resort-ing to the closest object instead, and (ii) it relies on Con-ceptNet for correspondences, potentially disregarding ob-jects with zero corresponding probabilities. The develop-ment of object-centric foundation models has enabled im-pressive capabilities in zero-shot object recognition in im-ages, as demonstrated by CLIP , DeCLIP , andALIGN . Recent works, such as EGO-VLP , Hier-VL , LAVILLA , and CoCa have expanded thescope of multimodal foundation models to include egocen-tric videos and have achieved impressive performance inzero-shot generalization which requires substantial amounts of curated pre-training data to learn semantic associationsamong concepts. Neuro-symbolic models show promise in reducing the increasing dependency ondata. We extend the idea of neuro-symbolic reasoning toaddress egocentric, open-world activity recognition.",
  ". Proposed Framework: ALGO": "Problem Formulation.Our task is to recognize unknownactivities in egocentric videos within an open-world setting.We aim to develop a framework that identifies elementaryconcepts, establishes semantic associations, and effectivelycombines these to interpret the observed activity. Activitiesare formed by combining concepts from two distinct sets:an object (nouns) (Gobj) and an action (verbs) (Gact) drawnfrom a predefined search space.Overview.Our proposed framework ALGO (ActionLearning with Grounded Object recognition), as illustratedin tackles the problem of discovering novel ac-tions in an open world. It starts by hypothesizing the plausi-ble objects through evidence-based object grounding (Sec-tion 2.1) by exploring prior knowledge from a symbolicknowledge base.An energy-based inference mechanism(.2) then identifies the plausible actions on theseobjects. We leverage visual-semantic action grounding todiscover activities without explicit supervision by employ- ing tools like CLIP and ConceptNet , respectively.Knowledge Representation. We use Grenanders pat-tern theory to represent the knowledge, integratingneural and symbolic elements in a unified, energy-basedrepresentation. We refer the reader to Aakur et al. andde Souza et al. for a deeper exploration of knowledgerepresentation in pattern theory.",
  ". Evidence-based Object Grounding": "The first step to assess the plausibility of object concepts(generators {go1, go2, . . . goi } Gobj) by grounding them inthe input video Vi. Grounding gathers evidence to supportor reject a concepts presence. To enhance object recog-nition accuracy, we propose a neuro-symbolic mechanismthat leverages compositional properties from ConceptNet tocompute the likelihood of an objects presence. This in-volves constructing an ego-graph for each object and usingCLIP to evaluate the likelihood of ungrounded generators,using prior knowledge to assess the presence of groundedobject generators.Given this set of ungrounded generators ({goi }goiGobj), we then prompt CLIP to provide likelihoods for eachungrounded generator p(goi |It) to compute the evidence-based likelihood for each grounded object generator goi asdefined by the probability p(goi |goi , It, KCS) = p(goi |It) goi p(goi , goi |Egoi ) p(goi )|It)2, where p(goi , goi |Egoi )is the edge weight from the edge graph Egoi (sampled froma knowledge graph KCS) that acts as a prior for each un-grounded evidence generator goi , and p(goi )|It) is the like-lihood from CLIP for its presence in each frame It. To fo-cus on relevant objects, we use the human gaze to selecta specific region for analysis, leveraging object-groundinginsights.",
  ". Object-driven Activity Discovery": "The next step focuses on identifying plausible activ-ities in the video by considering object affordancesandthecompatibilityofaction-objectpairsusingprior knowledge.The probability of an activity (de-finedbyanactiongeneratorgaiandagroundedobject generator goj) is given by p(gai , goj|KCS)=arg maxEKCS (gm,gn)E wk KCS(gm, gn).whereE is the collection of all paths between gai and goj in acommonsense knowledge graph KCS, wi is a weight drawnfrom an exponential decay function based on the distanceof the node gn from gai . After filtering for compositionalproperties, the path with the maximum weight is chosenwith the optimal action-object affinity.Energy-based Activity Inference. To infer activities,we assign an energy term to each label using configurationscomposed of generators connected by affinity-based bonds.Each configuration includes a grounded object generator (goi ), its ungrounded evidence generators (goj ), an actiongenerator (gak), and related ungrounded generators, struc-tured by a graph derived from ConceptNet. The energy of aconfiguration ci is expressed as:",
  ". Visual-Semantic Action Grounding": "In this step we aim to map inferred action verbs into asemantic embedding space provided by ConceptNet Num-berbatch, using a linear projection to translate visual fea-tures from the video to 300-dimensional semantic vectors(R1300). This process involves training a mapping func-tion (gai , fV ), primarily using a mean squared error (MSE)loss, to ground actions recognized in the video within thebroader semantic context of ConceptNet.Temporal Smoothing We implement temporal smooth-ing by first aggregating action predictions at the frame level.For each frame, we compute the top five actions based ontheir energy levels, then average these across the clip to sta-bilize the learning process. This aggregated data forms thebasis for training the mapping function (gai , fV ), focusingon the most frequent and energetically consistent actions.Posterior-based Activity Refinement. The final step in-volves an iterative refinement process that updates the ac-tion concept priors based on predictions from the visual-semantic grounding mechanism (.3). We adjustthe action priors in the energy computation (Equation 1), re-ranking activity interpretations to reflect clip-level dynam-ics better. The refinement cycle alternates between updatingposterior probabilities and re-training the action groundingmodel until generalization error saturates.",
  ". Experimental Evaluation": "Data.We evaluate the approach on GTEA Gaze ,GTEA GazePlus , and EPIC-Kitchens-100 datasets, which contain egocentric, multi-subject videos ofmeal preparation activities. The GTEA Gaze dataset has 10verbs and 38 nouns (search space of 380 activities), whileGTEA GazePlus has 15 verbs and 27 nouns (search spaceof 405), Charades-Ego has 33 verbs and 38 nouns (searchspace of 1254), and Epic-Kitchens has 97 verbs and 300nouns (search space of 29100).Baselines. We compare against both closed-world learn-ing and open-world setup (KGL) . We also create a base-line called KGL+CLIP by augmenting KGL with CLIP-based grounding by including CLIPs similarity score forestablishing semantic correspondences. We compare with",
  "ALGO+LaViLaOpen17.5026.6022.0530.7427.0028.87": ". Open-world activity recognition performance on the GTEA Gaze and GTEA Gaze Plus datasets. We compare approaches witha closed search space, those with a known search space, and those with a partially open one. Accuracy is reported for predicted objects,actions, and activities. VLM: Vision-Language Model pre-trained on egocentric video data. * indicates training on seen classes from thesame dataset(s) and leave-one-action-out evaluation.",
  ". Open World Activity Recognition": "summarizes the evaluation results under the open-world inference setting.Top-1 prediction results are re-ported for all approaches.As can be seen, CLIP-basedgrounding significantly improves the performance of objectrecognition for KGL, as opposed to the originally proposed,prior-only correspondence function. However, our neuro-symbolic grounding mechanism (.1) improves itfurther, achieving an object recognition performance of13.07% on Gaze and 26.23% on Gaze Plus.Similarly,the posterior-based action refinement module (.3)helps achieve a top-1 action recognition performance of 17.05% on Gaze and 11.44% on Gaze Plus, outperform-ing KGL (8.04% and 6.73%). Adding action priors fromLaViLa ((p(gak|It)) in Equation 1) allows us to improvethe performance further, as indicated by ALGO+LaViLa.We also evaluate our approach on the Epic-Kitchens-100dataset, a larger-scale dataset with a significantly highernumber of concepts (actions, verbs, and activities).Ta-ble 2 summarizes the results. We significantly outperformnon-VLM models while offering competitive performanceto the VLM-based models. We see that even without anyvideo-based training data, we achieve an action accuracyof 10.21% and object accuracy of 6.76%, indicating thatwe can learn affordance-based relationships for discoveringand grounding novel actions in egocentric data.",
  ". Discussion, Limitations, and Future Work": "In this work, we proposed ALGO, a neuro-symbolic frame-work for open-world egocentric activity recognition thataims to learn novel action and activity classes withoutexplicit supervision.While showing competitive perfor-mance, there are two key limitations: (i) it is restrictedto ego-centric videos due to the need to navigate clutterby using human attention as a contextual cue for objectgrounding, and (ii) it requires a knowledge base such asConceptNet to learn associations between actions and ob-jects.In the future, we aim to explore attention-basedmechanisms to extend the framework to third-personvideos and using abductive reasoning with neuralknowledgebase completion models to integrate visualcommonsense into the reasoning. Acknowledgements.This research was supported inpart by the US National Science Foundation grants IIS2348689, and IIS 2348690. We thank Dr. Anuj Srivastava(FSU) and Dr. Sudeep Sarkar (USF) for their thoughtfulfeedback during the discussion about the projects problemformulation and experimental analysis phase. Sathyanarayanan Aakur and Sudeep Sarkar. Actor-centeredrepresentations for action localization in streaming videos.In Computer VisionECCV 2022: 17th European Confer-ence, Tel Aviv, Israel, October 2327, 2022, Proceedings,Part XXXVIII, pages 7087. Springer, 2022. 4 Sathyanarayanan Aakur, Fillipe de Souza, and SudeepSarkar. Generating open world descriptions of video usingcommon sense knowledge in a pattern theory framework.Quarterly of Applied Mathematics, 77(2):323356, 2019. 1,2, 3 Sathyanarayanan N Aakur and Sudeep Sarkar. Leveragingsymbolic knowledge bases for commonsense natural lan-guage inference using pattern theory. IEEE Transactions onPattern Analysis and Machine Intelligence, 2023. 4 Sathyanarayanan N Aakur, Sanjoy Kundu, and Nikhil Gunti.Knowledge guided learning: Open world egocentric actionrecognition with zero supervision. Pattern Recognition Let-ters, 156:3845, 2022. 1, 2, 3, 4",
  "Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, andKristen Grauman.Hiervl:Learning hierarchical video-language embeddings, 2023. 1, 2, 4": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, ChaitanyaMalaviya, Asli Celikyilmaz, and Yejin Choi. Comet: Com-monsense transformers for automatic knowledge graph con-struction.In Proceedings of the 57th Annual Meeting ofthe Association for Computational Linguistics, pages 47624779, 2019. 4 Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Sanja Fidler, Antonino Furnari, Evangelos Kazakos, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, andMichael Wray. The epic-kitchens dataset: Collection, chal-lenges and baselines. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence (TPAMI), 43(11):41254141,2021. 3 Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Antonino Furnari, Jian Ma, Evangelos Kazakos, DavideMoltisanti, Jonathan Munro, Toby Perrett, Will Price, andMichael Wray.Rescaling egocentric vision: Collection,pipeline and challenges for epic-kitchens-100. InternationalJournal of Computer Vision (IJCV), 130:3355, 2022. 3",
  ", 2, 3": "Shangchen Han, Beibei Liu, Randi Cabezas, Christopher DTwigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho Yu, Chun-JungTai, Muzaffer Akbay, Zheng Wang, et al.Megatrack:monochrome egocentric articulated hand-tracking for virtualreality. ACM Transactions on Graphics (ToG), 39(4):871,2020. 2 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representationlearning with noisy text supervision, 2021. 2",
  "Yin Li, Alireza Fathi, and James M Rehg. Learning to predictgaze in egocentric video. In Proceedings of the IEEE Inter-national Conference on Computer Vision, pages 32163223,2013. 3": "Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, WanliOuyang, Jing Shao, Fengwei Yu, and Junjie Yan.Su-pervision exists everywhere: A data efficient contrastivelanguage-image pre-training paradigm, 2022. 2 Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, MichaelWray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wen-zhe Zhao, Weijie Kong, et al.Egocentric video-languagepretraining. Advances in Neural Information Processing Sys-tems, 35:75757586, 2022. 1, 2, 4",
  "Minghuang Ma, Haoqi Fan, and Kris M Kitani. Going deeperinto first-person activity recognition. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 18941903, 2016. 1, 2": "Ramy Mounir,Ahmed Shahabaz,Roman Gula,JornTheuerkauf, and Sudeep Sarkar.Towards automatedethogramming: Cognitively-inspired event segmentation forstreaming wildlife video monitoring. International Journalof Computer Vision, pages 131, 2023. 4 Maxwell Nye, Michael Tessler, Josh Tenenbaum, and Bren-den M Lake. Improving coherence and consistency in neuralsequence models with dual-system, neuro-symbolic reason-ing. Advances in Neural Information Processing Systems,34:2519225204, 2021. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International Conference on Machine Learning,pages 87488763. PMLR, 2021. 1, 2, 3",
  "Heng Wang and Cordelia Schmid. Action recognition withimproved trajectories. In IEEE/CVF International Confer-ence on Computer Vision (ICCV), pages 35513558, 2013.1, 4": "Xiaohan Wang, Linchao Zhu, Heng Wang, and Yi Yang. In-teractive prototype learning for egocentric action recogni-tion. In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision (ICCV), pages 81688177, 2021.2 Tailin Wu, Megan Tjandrasuwita, Zhengxuan Wu, XuelinYang, Kevin Liu, Rok Sosic, and Jure Leskovec. Zeroc: Aneuro-symbolic model for zero-shot concept recognition andacquisition at inference time. In Advances in Neural Infor-mation Processing Systems, pages 98289840. Curran Asso-ciates, Inc., 2022. 2"
}