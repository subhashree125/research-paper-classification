{
  "S*, Pixar movie": ". Addressing Content Ignorance. Given user-provided subject images, a part of the content specified in the text prompt (high-lighted in blue) are overlooked. Our Subject-Agnostic Guidance (SAG) aligns the output more closely with both the target subject and textprompt. Here S denotes a pseudo-word, with its text embedding replaced by a learnable subject embedding.",
  "Abstract": "In subject-driven text-to-image synthesis, the synthesisprocess tends to be heavily influenced by the reference im-ages provided by users, often overlooking crucial attributesdetailed in the text prompt.In this work, we proposeSubject-Agnostic Guidance (SAG), a simple yet effectivesolution to remedy the problem.We show that throughconstructing a subject-agnostic condition and applying ourproposed dual classifier-free guidance, one could obtainoutputs consistent with both the given subject and input textprompts. We validate the efficacy of our approach throughboth optimization-based and encoder-based methods. Ad-ditionally, we demonstrate its applicability in second-ordercustomization methods, where an encoder-based model isfine-tuned with DreamBooth. Our approach is conceptuallysimple and requires only minimal code modifications, butleads to substantial quality improvements, as evidenced byour evaluations and user studies.",
  ". Introduction": "Subject-driven text-to-image synthesis focuses on generat-ing diverse image samples, conditioned on user-given textdescriptions and subject images. This domain has witnesseda surge of interest and significant advancements in recentyears. Optimization-based methods tackle theproblem by overfitting pre-trained text-to-image synthesismodels and text tokens to the given subject. Re-cently, encoder-based approaches propose totrain auxiliary encoders to generate subject embeddings, by-passing the necessity of per-subject optimization.In the aforementioned approaches, both the embeddingsand networks are intentionally tailored to closely fit the tar-get subject. As a consequence, these learnable conditionstend to dominate the synthesis process, often obscuring theattributes specified in the text prompt.For instance, asshown in , when employing S1 alongside the style",
  "arXiv:2405.01356v1 [cs.CV] 2 May 2024": "description Monet style, the desired style is not appro-priately synthesized. Such observations underscore that thenetwork struggles to prioritize key content in the existenceof learnable components. To address the content ignoranceissue, existing solutions modify the training process throughadditional regularization , leading to improved per-formance.In this work, we present Subject-Agnostic Guidance(SAG), an approach that diverges from traditional method-ologies.Our strategy emphasizes attending to subject-agnostic attributes by diminishing the influence of subject-specific attributes, accomplished using classifier-free guid-ance. Differing from standard classifier-free guidance ,our method incorporates a subject-agnostic condition2.Subsequently, our proposed Dual Classifier-Free Guidance(DCFG) is employed to enhance attention directed towardssubject-agnostic attributes. Crucially, motivated by the ob-servation that structures are constructed during early iter-ations , we temporarily replace the subject-awarecondition with a subject-agnostic condition at the begin-ning of the iteration process. Following the constructionof coarse image structures, the original subject-aware con-dition is reintroduced to refine customized details.Our SAG is elegant in both design and implementa-tion, seamlessly blending with existing methods. We show-case the efficacy of SAG using both optimization-basedand encoder-based approaches.Furthermore, we delveinto its applicability in second-order customization, withan encoder-based model fine-tuned via DreamBooth .Qualitative and quantitative evaluations as well as user feed-back verify our robustness, succinctness, and versatility.In the evolving realm of subject-driven text-to-imagesynthesis, challenges have emerged due to over-tailoredembeddings and networks. These often inherit crucial at-tributes. While existing solutions modify training to ad-dress these issues, our novel Subject-Agnostic Guidance(SAG) provides a distinct approach. Seamlessly integratingwith prevalent methods, SAG emphasizes a more balancedsynthesis process. Its effectiveness is demonstrated throughvarious methodologies and supported by user feedback.",
  ". Related Work": "Diffusion Model for Text-To-Image Synthesis. Typically,given natural language descriptions, a text encoder such asCLIP or T5 is employed to derive the text embed-ding. This embedding is then fed into the diffusion modelfor the generation phase. Earlier approaches operateddirectly within the high-resolution image space for gener-ation. While these methods yielded promising outcomes,the direct iteration in high-resolution space poses signifi-cant computational challenges. In light of these constraints,",
  "The construction of this condition varies based on the specific cus-tomization approach used": "considerable efforts have been devoted to enhancing gener-ation efficiency. For instance, Imagen employs a multi-stage diffusion model. It starts by synthesizing a 64 64resolution image based on the input text prompt and sub-sequently employs a series of super-resolution modules toincrease the resolution to 10241024. Benefiting from op-timized architectures in the super-resolution stages, this cas-caded approach considerably reduces computational over-head compared to direct high-resolution image synthesis.Latent Diffusion transitions the generation process to alow-resolution feature space to improve efficiency. Initially,a VAE or VQGAN is pre-trained. During train-ing, images are encoded into low-resolution features usingthe pre-trained encoder, and the diffusion model aims to re-construct these encoded features. In the inference stage, thetrained diffusion model produces a feature which is subse-quently decoded using the pre-trained module to render thefinal output image. Subject-Driven Image Synthesis. Subject-driven text-to-image synthesis isa sub-branch of text-to-image synthesis with an additional requirement that the primary at-tributes in the output aligns with the subjects provided bythe user.Existing research has demon-strated that subject information can be encoded as a subject-aware embedding through test-time optimization, given sev-eral reference images. For instance, Textual Inversion leverages pre-trained synthesis networks and optimizes aspecial token while keeping the network static.Dream-Booth shares a similar premise but also fine-tunes thenetwork to enhance subject consistency. To bypass test-time optimization, which restricts instant feedback, recentstudies advocate the use of an encoder to encap-sulate subject information. However, despite advancementsin both quality and speed, the encoded subject informationoften dominates the synthesis process, resulting in inade-quately capture of subject information . In this study, weintroduce Subject-Agnostic Guidance (SAG) to rectify thischallenge. Our SAG focuses on enhancing subject-agnosticattributes, diminishing the influence of subject-specific ele-ments through our dual classifier-free guidance. We illus-trate that SAG not only enhances consistency to the inputcaptions but also maintains fidelity to the subject.",
  "Diffusion Model": "The diffusion process transforms a data distribution to aGaussian noise distribution by iteratively adding noise. Dif-fusion model is a class of generative models that invertthe diffusion process through iterative denoising. Extendedfrom the original unconditional model , recent worksdemonstrate huge success by conditioning diffusion modelson various modalities, including text , segmenta-tion , and many more .Let x0 be the input image, and c be the condition. Duringtraining, a noisy image xt is obtained by adding Gaussiannoise t to x0. The network is trained to predict the addednoise, given the noisy image and condition as input. It isgenerally optimized with a single denoising objective:",
  "Ld = ||(xt, c) t||22,(1)": "where t the noise added to the input image, and (xt, c)corresponds to the noise estimated by the network. Herext and c refer to the noisy image and condition, re-spectively.During inference, the process starts with apure Gaussian noise xT0, and the trained network is iter-atively applied to obtain a series of intermediate outputs{xT01, xT02, , x0}, where x0 is the final output.",
  "Classifier-Free Guidance": "Similar to classifier guidance , classifier-free guidanceis designed to trade between image quality and diversity,but without the need of a classifier. It is widely adopted inexisting works .During training, an unconditional diffusion model isjointly trained by randomly replacing the input conditionc by a null condition . Once trained, during each iteration",
  ". Subject-Agnostic Guidance": "In this section, we introduce the concept of Subject-Agnostic Guidance (SAG). The essence of SAG is anchoredin formulating a subject-agnostic embedding based on theinputs provided by users. The embedding is then used in ourdual classifier-free guidance (DCFG) in generating outputsthat align with both the subject and text prompt. We delveinto the details of constructing subject-agnostic embeddingsin Sec. 3.2.1, and discuss our dual classifier-free guidancein Sec. 3.2.2.",
  "Subject-Agnostic Embeddings": "The construction of subject-agnostic embeddings dependson the choice of methods. Existing approaches generallyfall into two categories: Learnable Text Token and Sepa-rate Subject Embedding. In this section, we discuss theconstruction of subject-agnostic embeddings in these twoapproaches. Learnable Text Token. Given images of a reference sub-ject, the learnable text token approach derives a token em-bedding that captures the identity of the subject, eitherthrough fine-tuning or by using an encoder .The resultant token embedding, combined with the tokenembedding of the text description, is processed by text en-coders such as CLIP and T5 to produce a subject-aware embedding.To construct a subject-agnostic embedding, we replacethe derived token embedding with one from a general de-scription of the subject. This strategy ensures that the syn-thesis process is not dominated by any adaptable compo-nents, thereby allowing the model to focus attention on theattributes specified in the text prompt.Let c be the text condition containing the learnable tokenS. We define a subject-agnostic condition c0 by replacingthe token S by a generic descriptor. For example, assumingthe target subject is a dog and",
  "The generic descriptor is chosen as a noun describing thesubject": "Separate Subject Embedding.Instead of encoding thesubject identity to a learnable text token, the separate sub-ject embedding approach adopts an independentembedding. This embedding is then integrated into the net-work via auxiliary operations. For instance, Jia et al. employ the CLIP image encoder to encapsulate the subjectinformation into an embedding, which is then injected toImagen using cross attention.To construct the subject-agnostic embedding, we opt fora direct method setting both the subject embedding and itscorresponding attention mask to zero. This disables atten-tion to the subject, directing focus towards subject-agnosticinformation.",
  "Dual Classifier-Free Guidance": "In this section, we introduce the Dual Classifier-Free Guid-ance (DCFG), designed primarily to address the issue ofcontent ignorance by attenuating the subject-aware condi-tion. Our DCFG requires no modifications of the trainingprocess. It simply requires the application of an additionalclassifier-free guidance using the subject-aware condition cand the subject-agnostic condition c0. The derived featureis subsequently merged with the null condition within aconventional classifier-free guidance.Weak Classifier-Free Guidance. Given the subject-awarecondition c and the subject-agnostic condition c0, we firstperform classifier-free guidance using c and c0. Incorporat-ing c0 into the synthesis process directs the generation to-wards subject-agnostic content, representing a weaker ver-sion of the desired generation. When subject information isabsent, the model more effectively creates the correct out-line and structure, generating outputs that align with boththe subject and text description.Differing from the conventional classifier-free guidance,where the guidance weight w often remains constant dur-ing the denoising process, we implement a time-varyingscheme to enhance performance. Building on the obser-vation that earlier iterations emphasize structure construc-tion , we highlight the subject-agnostic conditionduring the initial phases.Specifically, we adopt a time-varying weighting strategy, suppressing subject informationin the early stages:",
  "rif 0 t T,1if T < t 1.(4)": "Here 0 T 1 and r 1 are pre-determined constants,which will be ablated in Sec. 5. Essentially, in the earlystages (i.e., when t 1), we use solely the subject-agnosticcondition to establish the structure and outline of the out-put. The subject information is integrated in the subsequentstages. Null Classifier-Free Guidance.The null classifier-freeguidance is identical to the conventional classifier-free guid-ance, leveraging the null condition to encourage diversity.We adopt a constant guidance weight throughout itera-tions. Specifically, the output t of the weak-classifier-freeguidance is used in place of (xt, c) in the conventionalclassifier-free guidance (Eqn. 2):",
  "First, we examine the performance improvement when ap-plying SAG to ELITE . In this study, we simplify itsarchitecture by using only the global mapping branch. Thesettings are as follows:": "Training.To promote the learning of subject informa-tion, we create a domain-specific (e.g., animals) text-imagedataset where the text caption incorporates the specializedtoken. Specifically, we gather images from a pre-definedcategory and employ straightforward templates such asA photo of S for the corresponding captions. Duringtraining, the token corresponding to S is substituted withthe output of the encoder. The condition is subsequently fedinto the text encoder.As discussed in concurrent work , text prompts gen-erated using templates and captioning models have in-herent limits to their diversity. Moreover, training withinnarrow domains may harm generation diversity. To coun-teract this, we employ a general-domain dataset containingdetailed text descriptions for regularization. Training on abroad array of text captions ensures the model retains itstext-understanding abilities.During the training phase, the domain-specific andgeneral-domain datasets are sampled with probabilitiesp 1 and (1 p), respectively. Given that the general-domain dataset serves primarily for regularization, we al-locate a higher value to p, greater than 0.5, emphasizingsubject encoding.",
  "S* next to Tokyo tower, oil painting": ". SAG on ELITE . Our ELITE-SAG produces outputs that are more faithful to text prompts while still preserving subjectidentity. For Stable Diffusion, we generate pure text-to-image results by substituting S with A dog or A cat. Since the subject-agnostic condition c0 is also naturallanguage, no modification to the original denoising objec-tive (Eqn. 1) is needed. Additionally, we adopt a regulariza-tion to the learnable token by constraining its 2-norm.The effective training loss is:",
  "L = Ld + ||s||2 ,(6)": "where s denotes the output of the subject encoder. The re-maining part of the training is identical to the training ofconventional text-to-image networks.Inference. For each input image, we use the encoder to mapthe target subject into a text token. This learnable token isthen combined with the text description to form the inputcondition c. The subject-agnostic condition is c0 is thenconstructed following the process discussed in Sec. 3.2.1.Starting from random Gaussian noise xT , the fine-tunednetwork iteratively denoises the intermediate outputs. In-",
  "stead of applying the conventional classifier-free guidance,our SAG is employed": "Implementation. We adopt the pre-trained Stable Diffu-sion as the synthesis network, which uses CLIP as the text encoder. For the subject encoder, we use theCLIP image encoder and a three-layer MLP to obtain thelearnable token. During training, only the cross-attentionlayers in Stable Diffusion and the MLP are trained, all otherweights are being fixed.We use an internal text-image dataset for training. Toconstruct the domain-specific dataset, we extract imagescontaining dogs and cats from the meta-dataset. The re-maining part is used as our general-domain dataset. Thedataset mixing ratio is 0.1. The proposed method is imple-mented in JAX . The detailed experimental settings willbe discussed in the supplementary material.",
  "DreamBooth 52%68%60%Textual Inversion 64%76%84%ELITE 56%80%76%": "SAG, with three existing works: DreamBooth , TextualInversion , and ELITE . In this section, we assumethe existence of only one reference image. As illustrated in, while Stable Diffusion exhibits high text alignment,the compared methods often fall short in generating resultsfaithful to text prompts in the presence of additional sub-ject images. In contrast, with our SAG, outputs adheringto both text captions and reference subjects are consistentlygenerated.We also conduct a quantitative comparison as presentedin , utilizing CLIP and DINO scores. Specif-ically, the image feature similarities of CLIP and DINOunderscore that SAG enhances subject fidelity, while thetext feature similarity indicates that SAG improves textalignment. Furthermore, our user study depicted in reveals that more than half of the raters prefer our methodwhen compared to the aforementioned methods, therebycorroborating the effectiveness of SAG.",
  ". SuTI": "Unlike ELITE, which encodes subject information into atext token, SuTI employs an encoder-based approachthat leverages a distinct subject embedding. This embed-ding is then fed to the generation network through indepen-dent cross-attention layers. As discussed in Sec. 3.2.1, thesubject-agnostic condition, denoted as c0, is simply con-structed by setting the subject embedding to zero. As illustrated in , without SAG, the model suc-cessfully preserves the identity of the individual provided inthe reference images, yet the text alignment is inadequate.Specifically, the styles are unsatisfactory across all outputs.In contrast, employing SAG and suppressing the subjectinformation during initial iterations significantly enhancestext alignment. Consequently, the outputs exhibit both highidentity preservation and improved text alignment.",
  ". DreamSuTI": "DreamSuTI is a second-order method that fine-tunesSuTI using DreamBooth for compositional customiza-tion. In this section, we fine-tune SuTI with a provided styleimage to achieve simultaneous customization of style andsubject. The subject-agnostic embedding is generated usingthe same method as in SuTI. As depicted in , in the presence of subject images,the outputs are dominated by the subject, resulting in a lackof style fidelity. In contrast, when applying SAG, the sub-ject is suppressed during the early stages of generation, ef-fectively leading to enhanced style generation.",
  "w/ SAG": ". SAG on DreamSuTI . Even after fine-tuning with DreamBooth to adapt to the specified style, the generated results tendto be dominated by the subjects, leading to an inadequate style-alignment. Our SAG addresses this issue by diminishing the influence ofsubjects, thereby ensuring outputs that are well-aligned with both the text, subject, and style.",
  ". Ablations": "Guidance Timing. The hyper-parameter T plays an im-portant role in controlling the contribution of the subjectembedding. An illustration employing DreamSuTI is pro-vided in . With r = 0, adopting a smaller T resultsin a stronger suppression of the subject embedding, therebypromoting a better text-alignment (i.e., style-alignment inthis example). A gradual increment in T facilitates a transi-tion from style alignment to subject alignment. Guidance Weight. While a default value of r = 0 (i.e., em-ploying only the subject-aware condition in later iterations)performs well generally, decreasing r facilitates the utiliza-tion of the subject-agnostic condition in subsequent itera-tions, thereby further enhancing content faithfulness. Asdepicted in , the inclusion of subject-agnostic condi-tions significantly improves the style alignment of Dream-SuTI. Since no re-training is required, the values of T and rcan be dynamically adjusted based on user preference.",
  ". Limitation and Societal Impact": "Limitation. While our SAG significantly enhances con-tent alignment compared to existing methods, the qualityof outputs is inherently constrained by the underlying gen-eration model. Hence, it may still exhibit suboptimal per-formance for uncommon content that challenges the gener-ation model. However, this limitation can be mitigated byincorporating a more robust synthesis network, a directionwe aim to explore in our future work.Societal Impact. This project targets at improving contentalignment in customized synthesis, which holds the poten-tial for misuse by malicious entities aiming to mislead thepublic. Future investigations in this domain should dulyconsider these ethical implications. Moreover, ensuing ef-forts to develop mechanisms for detecting images generatedby such models emerge as a critical avenue to foster the safe",
  ". Conclusion": "Subject-driven text-to-image synthesis has witnessed no-table progress in recent years. However, overcoming theproblem of content ignorance remains a significant chal-lenge. As shown in this work, this problem significantlylimits the diversity of the generation. Rather than introduc-ing complex modules, we propose a straightforward yet ef-fective method to address this issue. Our Subject-AgnosticGuidance demonstrates how a balance between content con-sistency and subject fidelity can be achieved using a subject-agnostic condition. The proposed method enables users togenerate customized and diverse scenes without modifyingthe training process, making it adaptable across various ex-isting approaches. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, DanielCohen-Or, Ariel Shamir, and Amit H Bermano. Domain-agnostic tuning-encoder for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06925, 2023. 2,3",
  "Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.MultiDiffusion: Fusing diffusion paths for controlled imagegeneration. In ICML, 2023. 3": "James Betker, Gabriel Goh, Li Jing, Brooks Tim, JianfengWan, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,and Yunxin Jiao. Improving image generation with bettercaptions. Technical Report, 2023. 2 JamesBradbury,RoyFrostig,PeterHawkins,Matthew James Johnson, Chris Leary, Dougal Maclau-rin, George Necula, Adam Paszke, Jake VanderPlas, SkyeWanderman-Milne, and Qiao Zhang.JAX: composabletransformations of Python+NumPy programs, 2018. 5, 11",
  "Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.GLIGEN: Open-set grounded text-to-image generation. InCVPR, 2023. 3": "Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, KaiZhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and YangCao. Cones 2: Customizable image synthesis with multiplesubjects. arXiv preprint arXiv:2305.19327, 2023. 2 Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-gang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learningadapters to dig out more controllable ability for text-to-imagediffusion models. arXiv preprint arXiv:2302.08453, 2023. 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision. In ICML, 2021.2, 3, 5, 6 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J Liu. Exploring the limits of transfer learning with aunified text-to-text transformer. JMLR, 2020. 2, 3",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution image syn-thesis with latent diffusion models. In CVPR, 2022. 1, 2, 5": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. DreamBooth: Finetuning text-to-image diffusion models for subject-drivengeneration. In CVPR, 2023. 1, 2, 6, 8, 11 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, RaphaGontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet,and Mohammad Norouzi. Photorealistic text-to-image diffu-sion models with deep language understanding. In NeruIPS,2022. 1, 2, 4",
  "Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K.Chan, and Chen Change Loy.Exploiting diffusion priorfor real-world image super-resolution.arXiv preprintarXiv:2305.07015, 2023. 3": "Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, LeiZhang, and Wangmeng Zuo. ELITE: Encoding visual con-cepts into textual embeddings for customized text-to-imagegeneration. In ICCV, 2023. 1, 2, 3, 4, 5, 6, 11 Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-fei Yang, Burcu Karagol Ayan, et al. Scaling autoregressivemodels for content-rich text-to-image generation.TMLR,2022. 2",
  "A. Experimental Settings": "To train ELITE-SAG, we use a subset of the the WebLI dataset for training. We randomly select 70M data fromthe master dataset. We further extract 1M data containingdogs and cats with their face size greater than 128 128 asour domain-specific dataset. The remaining data is used asour general-domain dataset. The dataset mixing ratio is 0.1.In this work, the weak condition c0 is obtained simply byreplacing the special token with the class of the subject (e.g.,dog or cat). We train our models with 8 TPUv4 chipsfor 300,000 iterations. The learning rate is set to 104. Themethod is implemented in JAX .",
  "E. Additional Comparison Using ELITE-SAG": "We provide additional comparison with DreamBooth ,Textual Inversion , and ELITE .As shown in to , while existing works generally producereasonable results, they often experience content ignoranceor insufficient subject fidelity.This observation is espe-cially obvious in complicated prompts, where the modelhas to comprehend complex relation between subjects. Incontrast, ELITE-SAG achieves a balance between promptconsistency and subject fidelity."
}