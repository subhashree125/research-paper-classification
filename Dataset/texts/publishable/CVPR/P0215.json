{
  "Abstract": "Recent advances in neural network pruning have shownhow it is possible to reduce the computational costs andmemory demands of deep learning models before training.We focus on this framework and propose a new pruningat initialization algorithm that leverages the Neural Tan-gent Kernel (NTK) theory to align the training dynamicsof the sparse network with that of the dense one. Specif-ically, we show how the usually neglected data-dependentcomponent in the NTKs spectrum can be taken into ac-count by providing an analytical upper bound to the NTKstrace obtained by decomposing neural networks into indi-vidual paths. This leads to our Path eXclusion (PX), a fore-sight pruning method designed to preserve the parametersthat mostly influence the NTKs trace. PX is able to findlottery tickets (i.e. good paths) even at high sparsity lev-els and largely reduces the need for additional training.When applied to pre-trained models it extracts subnetworksdirectly usable for several downstream tasks, resulting inperformance comparable to those of the dense counterpartbut with substantial cost and computational savings. Codeavailable at:",
  ". Introduction": "Almost daily we hear about new breakthroughs achievedby artificial intelligence.Most of them are obtained bypowerful foundational models that however re-quire prohibitively high computational resources and en-ergy costs.These issues raise critical concerns in termsof financial and environmental sustainability andpose significant challenges for future applications requir-ing lightweight and efficient models embedded in always-on devices and the Internet of Things (IoT).Given the over-parameterized nature of modern deepneural networks, one solution to alleviate their resource de-mands involves removing a significant number of less im- . Our Path eXclusion (PX) involves two copies of the orig-inal dense network. One copy (bottom left) estimates data-relevantpaths, depicted by blue arrows, and injects the extracted informa-tion into the other network (blue shading). The other copy (bottomright) evaluates path relevance in terms of parameter connectionsin the network, illustrated by black connections. These estimationsare then combined to score each parameter, finding a subnetworkby retaining only the most relevant paths based on data, architec-ture, and initialization. The identified sparse subnetwork closelymimics the training dynamics of the original dense network. portant neurons or connections. Several pruning approacheshave been developed with the goal of lowering networkscomplexity without sacrificing accuracy ,and they can further benefit from efficient implementationsof sparse primitives and hardware designed to ex-ploit sparsity . These methods are traditionally appliedlate in training or post-training with the goal of reducing in-ference time, but recent findings suggest that pruning canalso be performed in advance .Specifically, Pruning at Initialization (PaI) searches forrandomly initialized subnetworks that once trained canmatch the test accuracy of the original dense networks witha largely reduced learning cost. Prior works have proposedPaI strategies based on the impact of each parameter on theloss or on different saliency metrics that estimate theinformation flow in the network . Some recent publica-",
  "arXiv:2406.01820v1 [cs.CV] 3 Jun 2024": "tions have targeted the evaluation of the training dynamicsbased on the Neural Tangent Kernel Theory (NTK, ) todefine the parameters scores. Although showing promis-ing results, they usually neglect or loosely approxi-mate the data contribution to the NTK spectrum asthey claim that data has a minimal impact on finding lot-tery tickets i.e. good paths in the network . Some ofthese approaches also suffer for layer collapse i.e. the pre-mature pruning of an entire layer that would make the net-work untrainable. As discussed in , its occurrence canbe avoided under specific conditions.One question that remains open is whether pruning canbe applied to pre-trained networks before their downstreamtransfer . This is a crucial and timely problem as pre-trained models continue to grow in size, and pruning couldbe used to reduce the cost of fine-tuning on downstreamtasks. This still defines a proper PaI setting where the initialmodel is trained on huge corpora and the goal is not only tocompress it but also to preserve its transferability capabili-ties in the obtained subnetworks.With our work we advance PaI research by proposingthe following contributions: We present Path eXclusion (PX, see .)a PaImethod that estimates the relevance of each networks pa-rameter to the training dynamic through a newly definedbound on the trace of the NTK. The saliency function formulated from the bound guar-antees that the network parameters have only positivescores. Together with the iterative nature of PX, thisprovides guarantees on avoiding layer collapse.",
  ". Related Works": "The question of how to significantly reduce the num-ber of parameters of a neural network while maintainingits performance dates back to the 1980s .Severalstrategies include matrix and tensor factorization , gen-eralized dropout , and adding regularization terms inthe learning objective to enforce sparse networks.Other approaches identify parameters with low magnitudeafter training and discard the corresponding connections. For all these methods, the main goalis to improve test efficiency while the computational cost oftraining remains the same as that of a dense network.In the last years, the focus has moved towards effi-cient training with one milestone provided by the Lottery Ticket Hypothesis . It demonstrated that within overlylarge networks it is possible to identify winning tickets,i.e. smaller subnetworks that once trained perform nearlyas well as their dense counterpart. The Iterative Magni-tude Pruning (IMP) algorithm discovers these subnetworksthrough several rounds of alternated training and progres-sive pruning guided by the magnitude of the surviving pa-rameters. Despite its effectiveness, the high computationalcosts of IMP led to the development of alternative cheapermethods for finding sparse networks. They are usually iden-tified as Pruning-at-Initialization (PaI), or foresight pruningalgorithms and can be organized into two main families.The data-agnostic methods exploit either random or con-stant mini-batches to probe the network and score each pa-rameter on the basis of its relevance to some networksproperty.Then, only a small fraction of the parameterswith top scores is kept for training. SynFlow buildson the hypothesis that the synaptic saliency for the incom-ing parameters to a hidden neuron is equal to the sum ofthe synaptic saliency for the outgoing ones. Thus it eval-uates the importance of each parameter on the basis ofits relation to those in the previous and following layers.SynFlow-L2 scores each parameter by considering itscontribution to the networks training dynamics estimatedvia the Neural Tangent Kernel theory . LogSynFlow rescales the scores of SynFlow to account for the possibleissue of exploding gradients. NTK-SAP improves theprevious methods by exploiting a more precise estimate ofthe training dynamics defined from the full spectrum of theNeural Tangent Kernel and then discards parameters thatcontribute the least to it. All these approaches compute im-portance scores iteratively with multiple forward-backwardpasses over the network, while PHEW introduces ran-dom walks biased towards higher parameter magnitudesand requires a single pruning round.The data-driven methods assert the relevance of the dataand of the learning task in evaluating the importance ofeach networks parameter when pruning and avoiding largedegradation in model performance.SNIP defines asaliency score for the parameters based on how they con-tribute to changing the initial loss. ProsPR combines anestimate of the effect of pruning on the loss and on the meta-gradients that define the optimization trajectory. GraSP takes the gradient norm after pruning as a reference crite-rion, and drops the parameters that result in its least de-crease. These methods are single-shot, while some variantsof SNIP such as IterSNIP and FORCE exploit iter-ative solutions to avoid layer collapse.Our work falls in between the two families described.We build on the NTK theory already used by the data-agnostic approaches and we show how information fromthe data can be used to guide the pruning process with sig-nificant advantages in training efficiency.Indeed, as al- ready discussed in data independence can beconsidered as a limitation rather than a benefit. An intu-itive reason is that data statistics have a crucial effect onsome network components as batch normalization that con-tributes to the overall network behavior and parameter rele-vance . Moreover, by focusing on the network trainingdynamics rather than on the loss, as most of the data-drivenapproaches do, our method proves to be task-independent,with the obtained sparse network remaining effective evenwhen transferred to new downstream tasks.",
  ". Method": "In this section, we start by describing the standard frame-work adopted by Pruning-at-Initialization (PaI) methodsand the intuition of our foresight pruning algorithm de-signed to calculate and preserve the trace of the Neural Tan-gent Kernel (NTK). Afterward, we provide a brief overviewof the theory underpinning NTK and present how to expressits trace by exploiting the notion of network paths. Finally,we introduce our Path eXclusion (PX) algorithm that dropsthose network weights that minimally change the trace ofthe NTK, so that the obtained sparse network retains onlythe most relevant paths of the original dense network.",
  "(1)": "where L is a suitable loss function for the downstream task,q is the desired sparsity of the resulting subnetwork and 0are the initial parameters. A is an optimization algorithm(e.g. SGD , Adam ) that takes as input the mask Mand the initialization 0 and returns the trained parametersat convergence final M, where denotes the element-wise (Hadamard) product.Due to the practical intractability of the described opti-mization problem, recent PaI algorithms focus on the notionof saliency, which is used as a score to assess the signifi-cance of network parameters regarding some property F ofthe network. After ranking the parameters scores, only thetop-S mask elements are retained and the final mask is usedto approximate a solution for Eq. (1). Formally, the saliencytakes the following form",
  ".(2)": "For instance, in SNIP the saliency is the loss function:F = L(M; (X, Y )). Thus, that method assigns to eachparameter a score which reflects how the loss would changewhen removing that specific parameter from the network.In this work, our goal is to devise a suitable saliencyscore that correctly reflects how much each weight con-tributes to the trace of the NTK. As described in detail inthe next subsection, the NTK approximates the training dy-namics of the network , so removing those weights thatminimally change its trace will result in small variations inthe NTK spectrum, producing a subnetwork with similarpredictive potential to the original larger network.",
  "f(X, t+1) = f(X, t) t(X, X)fL .(3)": "The matrix t(X, X) = f(X, t)f(X, t)T RNKNK is the Neural Tangent Kernel at time step t .For infinitely wide networks, the exact training dynamics isdescribed by the NTK which is a constant matrix that de-pends only on data, initialization, and architecture. It holdst(X, X) = 0(X, X), thus we can drop the subscripts.Further works observed that the NTK can approximatethe training dynamics of networks of any depth without nec-essarily being infinitely wide by rendering its theory usablein practice. Additionally, it has been shown that a fasterconvergence is correlated with the direction in the parame-ter space pointed by the eigenvector with the largest corre-sponding eigenvalue of the NTK .It is apparent that the NTK and its spectrum encapsulatecrucial information about their model and offer an appeal-ing way to evaluate the alignment between two networks.Models sharing the same NTK exhibit similar training dy-namics , even with different parameter counts. Empiri-cal results indicate that sparse subnetworks maintaining theNTKs largest eigenvalues of their dense counterpart con-verge more rapidly and better replicate the trainingdynamics of denser networks .While the strategy of using the NTK for network prun-ing seems promising, calculating the entire NTK spectrumis only feasible for very small neural networks with limiteddata . For context, recent results on the NTK computa-tion state a time complexity of N 2K[FP] , where Nis the size of the dataset, K is the output size of the net-work, and [FP] is the cost of a single forward pass. Indeedprevious pruning methods that exploited the NTK theory ei-ther resorted to different approximations of the NTK spec- trum or indirectly tried to preserve it by main-taining the gradient flow in the network .The next subsection explains how, by analyzing the acti-vation paths within a neural network, it is possible to obtainan analytical decomposition of the trace of the NTK that isinstrumental for PaI.",
  ". Neural Tangent Kernel and Activation Paths": "Let P be the set of all paths connecting any input neu-ron to any output neuron of the network f, where the edgesof those paths are the weights1 of the network. Each spe-cific path can be referred to by its index p = 1, ..., P in theset P. The presence of weight i in path p is denoted aspi = I[i p]. We can now define the product of weightswithin a path p as vp() = mi=1 pii , where pi is the expo-nent. Given an input example x X, the activation statusof a path is ap(x, ) = {i|ip} I[zi > 0], where zi isthe activation of the neuron connected to the previous layerthrough i. Thus, we can describe the k-th component ofthe output function of the network as:",
  "= Jfv (X)(Jfv (X))T .(5)": "Here Jv RP m compose the so-called Path Kernel ma-trix RP P which is symmetric positive semi-definiteand depends solely on the initialization and the networksarchitecture. The eigenvectors of the Path Kernel can be de-scribed as a collection of paths where the eigenvector as-sociated with the largest eigenvalue represents the set ofpaths that maximize the flow within the network . Onthe other hand, the matrix Jfv (X) RNKP , which werenamed, Path Activation Matrix, represents the change inoutput with respect to path values and entirely captures thedependence of f on the inputs by reweighting the pathswithin the network based on the training data.",
  "We use the terms parameters and weights interchangeably to re-fer to the networks parameters , as paths within a neural network areweighted by the value of each parameter": "Considering the eigenvalues i, i and i respectivelyof , Jfv (X) and (X, X), it was demonstrated thatTr[(X, X)] = NKii NKiii . Previousworks mentioning this upper bound , end up focus-ing only on the Path Kernel of the pruned networks andmaximize its trace Tr() = i i to preserve the largestNTK eigenvalue of the original network, which producesthe highest flow through the network and hence, similartraining dynamics. However, this might be misleading asthe data-dependent term Jfv (X) is neglected.In the following, we present a new upper bound forthe NTKs trace that considers both the Path Kernel andthe Path Activation Matrix, along with an exact calculationmethod, forming the core of our novel Path eXclusion ap-proach for pruning.",
  "p=1ap(xn, )x2ns|sp ,(7)": "where xns is the s-th component of the n-th sample vectorx. This term captures the dependence of the NTKs traceon the input data by choosing which paths are active andre-weighting by the input activations. The second term ofthe upper bound relates to the Path Kernel and as alreadydiscussed in , it holds",
  ".(8)": "Both Eq. (7) and (8) can be calculated efficiently by exploit-ing the implicit computation of the networks gradients viaautomatic differentiation. To do that we introduce two aux-iliary networks h and g which have the same architectureas the original f and are described by the input data x, theirparameters , and the status a of their ReLU activations.",
  "p=1ap(x, )x2s|sp": "Here h takes simplified data 1 as input, with squared param-eters, and a vector of activations that are all one. Instead gtakes the squared data as input, the parameters are all one,and the activations status is an exact copy of that of f. Fi-nally, we consider these two networks working jointly withthe overall behavior described by",
  "= Jfv (X)2F Jv 2F": "Thus we are able to explicitly compute the upper bound inEq. (6) and the value of each of the m components of thegradient R(x, , a)/2j is our saliency score indicatingthe importance of each parameter j in composing the traceof the NTK. To summarize, the final PX saliency score is:",
  "2.(9)": "As we perform global masking, we can observe from Eq.(7), (8) and (9) that our saliency function yields only pos-itive scores which means that the saliency among layers isconserved. Combined with the iterative application of ourpruning procedure, we satisfy the hypotheses of the Theo-rem of Maximal Critical Compression , which allows usto avoid layer collapse, namely pruning all neurons withinone layer and preventing the information flow.The full PX algorithm pseudocode is provided in the sup-plementary material.",
  ". Experiments": "In this section, we describe the results of our experi-mental analysis that thoroughly compares our PX with sev-eral baseline methods.In terms of datasets, tasks, andarchitectures we align with the literature and adopt well-established setups that are briefly summarized in the fol-lowing . Moreover, we investigate whetherPaI can be applied to pre-trained models without damagingtheir downstream transferability. Our empirical evaluationprovides a positive answer to this new research question.Datasets & Tasks. For the classification experiments, weuse CIFAR-10, CIFAR-100 , Tiny-ImageNet and Ima-geNet . For the segmentation experiments we follow and use the training and validation splits of Pascal VOC2012 for model learning and evaluation.Architectures. As done by , for the classificationexperiments we use ResNet-20 on CIFAR-10, VGG-16 on CIFAR-100. ResNet-18 on Tiny-ImageNet andResNet50 on the ImageNet dataset. By following , on thesegmentation task we use DeepLabV3+ with ResNet-50.Initialization. As in we initialize each model usingKaiming normal initialization . Furthermore, we assesshow pre-trained parameters affect the foresight pruning pro-cedure. For this analysis we adopt a setting analogous tothat in that originally considered only iterative unstruc-tured magnitude pruning. Specifically, we use a ResNet-50 pre-trained on ImageNet as well as two self-supervisedmodels obtained with MoCov2 and CLIP .Implementation details. Regarding the training procedurewe follow and when assessing respectively ourPX with respect to the PaI state-of-the-art methods and thepre-training transferability. We evaluate each algorithm ontrivial (36.00%, 59.04%, 73.80%), mild (83.22%, 89.30%,93.12%) and extreme (95.60%, 97.17%, 98.20%) sparsityratios as . We use 100 rounds for iterative PaI methodsadopting an exponential schedule as . Full imple-mentation details can be found in the supplementary.",
  ". Classification with Random Initialization": "To provide an empirical evaluation of the strengths andeffectiveness of our method, we compare PX with state-of-the-art foresight pruning algorithms. These include bothdata-driven methods like SNIP and GraSP , as wellas data-agnostic techniques such as SynFlow and NTK-SAP . We also include two common baselines in PaIwhich are Random pruning and Magnitude-based pruning.CIFAR-10, CIFAR-100 & Tiny-ImageNet.In .we report the classification results when using ResNet-20, VGG-16 and ResNet-18 on CIFAR-10, CIFAR-100and Tiny-Imagenet, respectively. For low sparsity levels,most techniques exhibit strong performance. On ResNet-20 (CIFAR-10), the performance gap across techniques is . Average classification accuracy at different sparsity levels on CIFAR-10 using ResNet-20, CIFAR-100 using VGG-16 and Tiny-ImageNet using ResNet-18, respectively. Each experiment is repeated three times. We report in shaded colors the standard deviation. low, albeit being slightly higher for random pruning andGraSP. On VGG-16 (CIFAR-100) and ResNet-18 (Tiny-ImageNet), a comparable pattern emerges, but the disparityin performance widens noticeably. In all three cases, our ap-proach consistently delivers the highest results. At moder-ate sparsity levels, PX and NTK-SAP start to emerge as thetop performers across all three experiments. Specifically,on VGG-16 (CIFAR-100), these two methods showcasecompetitive performance, with NTK-SAP being slightlyahead. The ranking of the other techniques remains con-sistent at these levels. Finally, at extreme levels of spar-sity, PX clearly outperforms all the competitors. In partic-ular, on VGG-16 (CIFAR-100), there is a considerable dis-parity between PX and all other methods, including NTK-SAP. Notably, theres a substantial decline in NTK-SAPsperformance at 98.20% sparsity on ResNet-20 (CIFAR-10). Across all three experiments, GraSP, despite initiallyexhibiting lower performance, demonstrates commendableconsistency in maintaining its results. ImageNet. We conducted a comprehensive assessment ofPX on larger-scale datasets, specifically ImageNet, employ-ing ResNet-50 as the backbone model. In line with , weexamined two sparsity levels (89.26% and 95.60%). Ourfindings, detailed in ., reveal that PX, NTK-SAP, andGraSP get top results, with PX exhibiting a slight advan-tage. Magnitude pruning surprisingly demonstrates greatercompetitiveness compared to SNIP, which performed wellon smaller-scale datasets but ranks last in this evaluation.",
  ". Starting From Pre-Trained Parameters": "Within this section, we examine the impact of initializa-tion from pre-trained models: we aim at gaining insightsinto how we can leverage PaI algorithms to efficiently trans-fer knowledge to downstream tasks. Classification. . presents our findings on classi-fication results across CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet-50 initialized from ImageNet, Mo-Cov2 on ImageNet, and CLIP pre-trainings. We do not re-port SynFlow here as it produces exploding gradients while",
  "SynFlow 66.48 0.1259.41 0.19SNIP 60.50 0.3445.82 0.35NTK-SAP 67.98 0.3159.84 0.30GraSP 67.21 0.5260.01 0.16Random64.97 0.2756.79 0.44Magnitude66.56 0.2347.80 0.21PX (Ours)68.11 0.2960.28 0.32": ". Average classification accuracy at different sparsity ra-tios on the ImageNet dataset, using Kaiming normal initializedResNet-50 as backbone. Each experiment is repeated three times.We report also the standard deviation. Bold indicates the best re-sult. Underline the second best. estimating saliency scores. This issue has been also ob-served in contexts like Neural Architecture Search .Until reaching extreme sparsity, most methods closelyalign with the performance of the dense baseline, underlin-ing that employing PaI in this context serves as a viable,cost-free alternative to Iterative Magnitude Pruning (IMP).At extreme sparsity levels across all experiments, PX con-sistently outperforms other methods maintaining proxim-ity to the dense baseline on simpler tasks (CIFAR-10) andshowing a clear advantage over the competitors on morecomplex tasks (CIFAR-100 and TinyImageNet).We remark that, while NTK-SAP stands as a state-of-the-art method, its performance drastically diminishes at ex-treme sparsity levels when initialized from pre-trained pa-rameters. We attribute this decline to the interference ofrandom mini-batches with the batch normalization statis-tics of the pre-trained model during its saliency estimation.This provides clear evidence of the limitations of data-freePaI methods. Segmentation. In ., we present the semantic seg-mentation results on the Pascal VOC2012 datasets, em-ploying DeepLabV3+ on ResNet-50 initialized with Mo-Cov2 on ImageNet (further results with other initializationin the supplementary). Here we report only the results forour method, SNIP, Random, and Magnitude-based pruning.This selective reporting stems from issues encountered with . Average classification accuracy at different sparsity levels on CIFAR-10, CIFAR-100 and Tiny-ImageNet using pre-trainedResNet-50 as architecture. The first column reports the results of starting from the supervised ImageNet pre-training. The second columnreports the performance when starting from the MoCov2 pre-training on ImageNet. Finally, in the third column we report the results whenstarting from CLIP. Each experiment is repeated three times. We report in shaded colors the standard deviation. . Average mean Intersection over Union (mIoU) at differ-ent sparsity levels on Pascal VOC2012 using DeepLabV3+ withpre-trained ResNet-50 as the backbone. Each experiment is re-peated three times. Standard deviations are in shaded colors. other methodologies: SynFlow faced again challenges withexploding gradients, NTK-SAP resulted in layer collapsewithin the segmentation head due to the potential absence ofpositive saliency scores, a crucial factor in preventing suchcollapses. Similarly, GraSP, relying on a single round ofpruning, encountered limitations in its applicability. PX confirms its superiority to the other methods evenfor semantic segmentation. We also note that SNIP consis-tently demonstrates good performance and it appears as aremarkable result in comparison to the failure of NTK-SAP.Overall, despite the encouraging results, it is apparent thatsignificant effort needs to be directed toward PaI techniquesspecifically tailored for more complex vision tasks such assemantic segmentation. As of now, PaI methods only ap-proximate the dense network results at trivial sparsity lev-els. Surprisingly, Magnitude-based pruning also finds com-petitive subnetworks, comparable to other PaI methods, butonly at trivial and moderate sparsity levels.",
  ". Active output units at 98.20% sparsity in VGG-16. ForSNIP and PX data mini-batches are sampled from CIFAR-100": "sparsity.Notably, even at this substantial level of spar-sity, our approach closely mirrors the eigenspectrum of theoriginal dense network confirming the expectations.Tomake our argument even more solid, we disregarded thedata-dependent term in PX which implies falling back toSynFlow-L2. As can be observed, data play a central rolewhen preserving the eigenspectrum of the NTK. This abla-tion study further reinforces our claims.",
  ". Layer Width": "As discussed earlier, the integration of iterative pruningrounds and the layer-wise preservation of saliency are piv-otal in preventing layer collapse using saliency functions. However, observed that iterative PaI methods suf-fer a sudden reduction in the number of output units, leadingto narrow layers and bottlenecks. Recent investigations highlighted a correlation between network performance andhigher output width when fixing a certain number of param-eters in an architecture. Motivated by this, we investigatemore in-depth the behavior of PX.For fully connected layers every neuron constitutes anoutput unit. For convolutional layers, we follow andconsider each kernel as an output unit. If all parameterswithin a convolutional kernel are pruned, the layers outputunit count is reduced by one. In , we conduct theanalysis on the number of output units that each iterativePaI method preserves after pruning. Despite being iterative,PX is not affected by the issues mentioned in and con-sistently preserves the output width of each layer.",
  ". Execution Time Analysis": "In , we show the effect of changing the numberof pruning rounds T, presenting accuracy and total execu-tion time of the pruning procedures in seconds. Here weinclude IMP to offer a broader context to our study. We ranour evaluation on NVIDIA Titan Xp GPU, Intel i7-9800XCPU, and using the perf counter clock from Pythonstime module. IMP (Epochs = 960) takes nearly 4 hoursto outperform PX (T = 10). GraSP leads at T = 1, butPX surpasses GraSP at T = 2 without exceeding its timecost. Increasing T beyond 100 marginally improves resultsbut does not alter conclusions, in line with . Morediscussions about the computational cost of PX and its com-petitors are provided in the supplementary.",
  ". Conclusion": "Pruning at initialization offers the attractive possibility ofreducing the number of parameters in a neural network, by-passing the need for training to identify the pruning mask.The NTK and its pathwise decomposition provide a pow-erful proxy for identifying parameters that are importantfor preserving training dynamics after pruning. While mostmethods consider only the data-independent component, wepropose a new upper bound on the trace of the NTK whichled to Path eXclusion (PX), that allows us to preserve itsspectrum and consider the data-dependent component aswell. We show experimentally that PX is not only robust todifferent architectures and tasks but can also be effectivelyused to search for subnetworks in large pre-trained modelsthat retain almost intact transferability.Acknowledgements.L.I. acknowledges the grant receivedfrom the European Union Next-GenerationEU (Piano Nazionaledi Ripresa E Resilienza (PNRR)) DM 351 on Trustworthy AI. T.T.acknowledges the EU project ELSA - European Lighthouse on Se-cure and Safe AI. This study was carried out within the FAIR -Future Artificial Intelligence Research and received funding fromthe European Union Next-GenerationEU (PIANO NAZIONALEDI RIPRESA E RESILIENZA (PNRR) MISSIONE 4 COM-PONENTE 2, INVESTIMENTO 1.3 D.D. 1555 11/10/2022,PE00000013). This manuscript reflects only the authors viewsand opinions, neither the European Union nor the European Com-mission can be considered responsible for them. Milad Alizadeh, Shyam A Tailor, Luisa M Zintgraf, Joostvan Amersfoort, Sebastian Farquhar, Nicholas Donald Lane,and Yarin Gal. Prospect pruning: Finding trainable weightsat initialization using meta-gradients. In ICLR, 2022. 1, 2",
  "A. Implementation Details": "In you can find the training details used in this work.We evaluate each algorithm on trivial (36.00%, 59.04%,73.80%), mild (83.22%, 89.30%, 93.12%) and extreme(95.60%, 97.17%, 98.20%) sparsity ratios as . In each ex-periment, we use 100 rounds for iterative PaI methods adopting anexponential schedule as . We train and test on the respec-tive official splits of each dataset, repeating each experiment threetimes. Classification - Random initialization. For the classification ex-periments starting from Kaiming Normal initialization , wefollow . The augmentations used when training on CIFAR-10 and CIFAR-100 are Random Crop to 3232 with padding4 followed by Random Horizontal Flipping with 0.5 probability.For the experiments on Tiny-ImageNet , we augment the train-ing images with Random Resized Crop to 6464 with scaling go-ing from 0.1 to 1.0 using 0.8 x-ratio and 1.25 y-ratio. Then, weapply Random Horizontal Flipping with 0.5 probability. On Im-ageNet , we apply Random Resized Crop to 224224 withscaling going from 0.2 to 1.0 using 3/4 x-ratio and 4/3 y-ratio.Then, we apply Random Grayscaling with 0.2 probability, ColorJitter with brightness, contrast, saturation and hue all set to 0.4. Fi-nally, we apply Random Horizontal Flipping with 0.5 probability. Classification - Pre-trained models. Regarding the classificationexperiments when starting from ImageNet , MoCov2 on Im-ageNet and CLIP pre-trained models, we align with .Specifically, we use the same augmentations detailed in the previ-ous paragraph but we adjust the cropping and rescaling transfor-mations to ensure that the resultant image size is set at 224224pixels, aligning with the dimensions of the images used in obtain-ing the pre-trained models. Segmentation. For the semantic segmentation experiments weagain align with . We employ the following augmentations dur-ing training: Random Scale with a range between 0.5 and 2.0,Random Crop to 513513, followed by Random Horizontal Flip-ping with 0.5 probability. Pre-trained models & Architectures. Regarding the pre-trainedmodels used in our experiments, we employed the official Im-ageNet pre-trained model from the PyTorch torchVision pack-age . The MoCov2 ImageNet model we used is the officialone from Facebook research3. The CLIP pre-trained model is theofficial one from OpenAI4. Finally, we base our experiments onDINO from its officially released pre-trained model5.Our code is based on the framework for Pruning-at-Initialization provided by . Moreover, we used their imple-mentations for the architectures used in our classification experi-ments. For the segmentation experiments, we align with anduse the same implementation of DeepLabV3+6.",
  "B. Additional Discussions": "In this section we provide the derivations and intuitions aboutthe mathematics used in the main submission, to make it clear howthe path-wise perspective can be studied via forward and backwardpasses on any architecture. Note that in all of our derivations andformulas, we skip bias terms as we embed them in the weight ma-trix by adding an additional input set to 1 to each neuron.Frobenius norm of the Path Activation matrix. In Eq. (7) of themain submission we applied the definition of the Frobenius normon the Path Activation matrix",
  "s=1I[p Psk]ap(xn, )xns": "We observe that by fixing a path p, the inner sum from s = 1 to dwill have only one non-zero term (given by the indicator function).Specifically, the one for which s is the starter input node: the otherinput nodes cannot take part in path p as the first connection willdefine a different path. This means that the equality reported inEq. (7) of the main submission holds, provided that we take thecorrect input node s|s p.From layer-wise to path-wise. Here we provide the key idea onhow to pass from the layer-wise perspective to the path-wise per-spective when considering activations a Rm and parameters Rm. First of all, the relationship between a and ap is given bythe definition of the latter. According to what we reported in Sec-tion 3.3 of the main submission, ap(x, ) = {i|ip} I[zi > 0]where zi is the activation of the neuron connected to the previouslayer through parameter i. Thus, ap is a binary value represent-ing the activation status of path p. This is equivalent to assigning",
  "multiplications between the inputs and parameters of each layer,followed by an element-wise multiplication with the activations": "Computational cost analysis. This analysis, detailed in , focuses on understanding the computational complexity of ourmethod in contrast to that of our competitors. Notably, we in-clude IMP in this assessment to offer a broader context to ourstudy. In the second column of the table, we present the com-putational cost of invoking each pruning procedure, measured innumbers of macro-operations performed to obtain the final prun-ing scores. Where we report O(1) complexity, it means that thescores can be obtained immediately by simply looking at someintrinsic property of the network, such as the magnitude of theweights, which does not require any additional processing. Here,T represents the required number of pruning iterations, and B in-dicates the number of mini-batches processed by each algorithmduring this procedure. Additionally, we denote the costs of a sin-gle forward pass with [FP] and of a single backward pass with[BP]. Columns three and four illustrate the training epochs nec-essary for the pruning algorithm to attain approximately equiva-lent accuracy at 98.20% sparsity when starting from ResNet-20 onCIFAR-10. This metric trivially stands at zero for PaI methods,given that the procedure is executed prior to training. However,IMP demands a minimum of 6 iterative rounds of pruning and sub-sequent re-training (each full training cycle spans 160 epochs) tosurpass the accuracy reached by our method. Within this analysis,it becomes apparent that PXs computational complexity mirrorsthat of NTK-SAP , aligning generally with iterative PaI meth-ods, which are explicitly constructed to iterate across T roundspreceding training.",
  "Procedure clarifications and pseudocode. In Algorithm 1, weprovide the pseudocode to further clarify the role of T and B in the": ". Average classification accuracy at different sparsity levels on CIFAR-10 using ResNet-20, CIFAR-100 using VGG-16 and Tiny-ImageNet using ResNet-18, respectively. Each experiment is repeated three times. We report in shaded colors the standard deviation. . Average classification accuracy at different sparsity levels on CIFAR-10, CIFAR-100 and Tiny-ImageNet using pre-trainedResNet-50 as architecture. The first column reports the results of starting from the supervised ImageNet pre-training. The second columnreports the performance when starting from the MoCov2 pre-training on ImageNet. Finally, in the third column we report the results whenstarting from CLIP. Each experiment is repeated three times. We report in shaded colors the standard deviation. implementation of PX. The functions g, h are copies of f definedfor clarity, but the memory usage does not double as PX only stores and the derivatives w.r.t. 2 are computed in a single pass (lines13-15). PX is an iterative PaI method but differs from the standardframework in lines 3, 9-15. We remark that iteratively refiningthe pruning mask M in T rounds while yielding positive saliencyscores guarantees to avoid layer collapse .",
  "C. Additional Experiments": "Segmentation experiments.In we report the full re-sults of the semantic segmentation experiments on the PascalVOC2012 dataset. In each experiment the architecture used isDeepLabV3+ on a ResNet-50 backbone, starting from Im-ageNet , MoCov2 on ImageNet and DINO pre-trainedmodels.The general trend reported in the main paper is confirmed alsoin this setting, where our method is able to retain the accuracy of . Average mean Intersection over Union (mIoU) at different sparsity levels on Pascal VOC2012 using DeepLabV3+ with pre-trained ResNet-50 as the backbone. Each experiment is repeated three times. Standard deviations are in shaded colors.",
  "the dense baseline at trivial sparsities": "SynFlow-L2 ablation study. As mentioned in the main paper, ourmethod can be interpreted as an extension of SynFlow-L2 .The core difference is that PX reweights the networks outputs onthe basis of the information provided by the data: that informationindicates how much each weight contributes to the upper bound onthe trace of the NTK reported in Eq. (6) of the main submission.Thus, by comparing PX with SynFlow-L2 we conduct an ablation study to provide further evidence regarding the soundness of thehypotheses underpinning our algorithm, proving the importance ofthe data-dependent component. In , 9 and 10 we observe thatour method is always able to improve over SynFlow-L2. Further-more, the latter exhibits a drastic decrease in performance whenthe cardinality of the networks output increases. As already no-ticed in , this is attributed to the combined effect of reducingthe layer width while keeping a high path count in the architecture. Layer widths. In , we present additional plots on the layerwidth to confirm the trend reported in the main submission regard-ing the number of output units preserved by PX at each layer. Weobserve again that PX is able to preserve the output width despitethe very high sparsity ratios under exam and the different modelsizes."
}