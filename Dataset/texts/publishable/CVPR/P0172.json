{
  "Abstract": "Large multimodal language models have shown remark-able proficiency in understanding and editing images. How-ever, a majority of these visually-tuned models struggle tocomprehend the textual content embedded in images, primar-ily due to the limitation of training data. In this work, weintroduce TRINS: a Text-Rich image1 INStruction dataset,with the objective of enhancing the reading ability of themultimodal large language model. TRINS is built uponLAION 2 using hybrid data annotation strategies that includemachine-assisted and human-assisted annotation processes.It contains 39,153 text-rich images, captions, and 102,437questions. Specifically, we show that the number of wordsper annotation in TRINS is significantly longer than that ofrelated datasets, providing new challenges. Furthermore,we introduce a simple and effective architecture, called aLanguage-vision Reading Assistant (LaRA), which is good atunderstanding textual content within images. LaRA outper-forms existing state-of-the-art multimodal large languagemodels on the TRINS dataset, as well as other classicalbenchmarks. Lastly, we conducted a comprehensive evalua-tion with TRINS on various text-rich image understandingand generation tasks, demonstrating its effectiveness.",
  ". Introduction": "Instruction tuning has shown a great generalizationability on unseen tasks and has contributed to the grow-ing popularity of large language models (LLMs), such asChatGPT . Recently, multimodal language models ben-efit from visual instruction finetuning ,and have shown great success in real-world applications.These models leverage visual encoders such as CLIP-ViT to empower LLMs with image comprehension abil-ity. However, challenges arise in comprehension of textualinformation within images, which may stem from the preva-",
  "In this work, we use the phrase text-rich images to describe imageswith rich textual information, such as posters and book covers.2Work done during Q3 2023": "lence of natural images in training datasets, such as Concep-tual Captions and COCO ), as highlighted by Liuet al. . Recognizing the importance of visual textual un-derstanding for effective collaboration between agents andhumans, Zhang et al. proposed enhancing end-to-endvisual instruction-tuned models by introducing noisy Opti-cal Character Recognition (OCR) annotations to improvevision-language alignment. In this work, we surpass existingachievements and collect a new Text-Rich image INStruc-tion dataset named TRINS, which contains 39,153 text-richimages, captions, and 102,437 questions. TRINS is created in a semi-automatic manner for a morecontrollable and faithful collection. Specifically, we ex-ploited large-scale pre-trained models such as CLIP and GPT-4 in the annotation process. This semi-automaticprocess significantly reduces the time and resources requiredfor manual annotation and surprisingly improves the overallquality of annotations. TRINS dataset is composed of threedatasets for captioning, visual question answering (VQA),and image generation, respectively. Specifically, human-annotated captions for text-rich images are first collectedbecause they can best translate text-rich images into texts.During this process, extracted OCR words and recognize-anything model tags are provided to the annotators for betterand efficient annotations. With detailed image descriptions,VAQ data is built and fulfilled by large language models,such as GPT-4 and LLaMA-2 . In detailed statisticsand analysis, we found that both annotated captions and col-lected question-answer pairs are more comprehensive andcontain significantly more details than the existing dataset.Therefore, we show the superior advantage of TRINS com-pared to existing instruction fine-tuning datasets. As a by-product, high-quality image-caption pairs can serve as a goodbenchmark for text-rich image generation, which is still avery challenging task . At the same time, we proposea new, simple, and effective multimodal language modelarchitecture that includes OCR as a component. We call itLanguage-vision Reading Assistant (LaRA) and show thatLaRA fine-tuned on TRINS brings the best text-rich imageunderstanding ability. Our contributions are as follows:",
  ". Related Work": "Multimodal Instruction TuningMulti-modal instructiontuning, including image, video , and audio settings, has been an active research topic. MiniGPT-4 uses ChatGPT to generate high-quality instruction-followingdata, while LLaVA generates such data by promptingGPT-4 with captions and bounding boxes. LLaMA-Adapter aligns text-image features using COCO data, andmPLUG-owl combines extensive image-text pairs forpretraining and a mixture of data for finetuning. Despitethis, many models, according to Liu et al. , struggle withOCR tasks. InstructBLIP addresses this by transforming13 vision language tasks into an instruction-following format.mPLUGOwl apply multitask instruction funetuingusing existing document datasets. A comprehensive surveyis available in Li et al. . LLaVAR exploits GPT-4to collect fine-tuning data without human annotations us-ing OCR and captioning tools. It discovered that resolutionplays a significant role in recognizing textual informationand explored several options. Monkey performed asurgery between simple text labels and high input resolution,enabling remarkable performance in visually-rich documentimages with dense text. TRINS exploits human-machine col-laboration for data annotation and can provide more accurateinformation, reducing the problem of hallucination. Text-Rich Image DatasetsVisual question answering orcaptioning datasets are widely used in task-specific fine-tuning and large multimodal model evaluation. TextCap is the first text-rich image captioning dataset. Compared toTextCap, TRINS-Cap provides more detailed annotationsthat can fulfill the requirement of instruction finetuing. Text-OCR aims to comprehend text in the context of animage, which is similar to our motivation but focuses moreon text recognition in images instead of understanding. ST-VQA uses spatial and textual information to answervisually grounded questions, effectively integrating visual",
  ". Comparison between TRINS and other related datasets": "and textual cues. OCR-VQA focuses on incorporat-ing optical character recognition (OCR) into visual questionanswering (VQA), which operates primarily on text withinimages. TextVQA also takes advantage of the textualinformation present in the images to answer questions, butwith an emphasis on open questions. DocVQA takes thisone step further by applying VQA to document images,handling a variety of layouts and formats. InfoVQA and ChartQA focus on specific subdomains and aim toanswer questions about information graphics and chart im-ages, respectively. In summary, these related works providedatasets for leveraging spatial and textual cues. TRINS-VQAis a dataset that exploits the semi-automatic annotation pro-cess. It can be used for general domain instruction finetuningand model evaluations.",
  ". Text-Rich Image Instruction Dataset": "To equip multimodal language models with the ability to rec-ognize text and relate it to its visual context, we have curateda new dataset named Text-Rich Instruction (TRINS). Theultimate goal is to enable these models to have spatial, seman-tic, and visual reasoning between multiple text tokens andvisual entities. In this section, we present TRINS, a datasetcrafted through a semi-automatic process. Specifically, weleverage large-scale pre-trained models like CLIP andGPT-4 in the annotation process, offering potential advan-tages: (i) Significant reduction in annotation time and re-sources: using these models significantly reduces the timeand resources required for manual annotation. (ii) Enhance-ment of annotation data quality through post-processing: theinvolvement of large models contributes to improving theoverall quality of annotation data through subsequent post-processing. (iii) Functionality of large models as knowledgebases: large models can serve as effective knowledge bases,aiding in the annotation process by virtue of their extensivetraining in diverse datasets.We first outline the document image collection process forTRINS, utilizing CLIP models. Then, we present data statis-tics to facilitate a comprehensive understanding. We delveinto three distinct tasks derived from the TRINS dataset indetail: i) TRINS-Cap: Visual Captioning, ii) TRINS-VQA: . Overview of the TRINS data collection process, which consists of three datasets. Text-rich images are first selected from webimages and then ask annotators to describe the image in detail. i) TRINS-Cap is extracted from human annotations with heuristic dataprocessing for text-rich image captioning tasks. ii) TRINS-VQA is built upon human annotations and generates question-answer pairs fortraining by prompting text-only LLMs. iii) TRINS-Gen combined human annotations and text boxes for text-rich image generation.",
  ". CLIP-based categorization of our collected images and selected representative data samples from each category": "Visual Question Answering and iii) TRINS-Gen: Text-to-Image Generation. The overview of the TRINS data collec-tion process is illustrated in . To provide a succinctoverview, TRINS-Cap undergoes full annotation by humanannotators, while TRINS-Gen and TRINS-VQA are con-structed based on TRINS-Cap with the help of pre-trainedmodels. A summary of the various TRINS datasets is pre-sented in of Appendix A.",
  ". Machine-Assisted Text-rich Image Selections": "Beginning with the LAION-5B dataset3 , our objectiveis to selectively retain images that exhibit a significant pres-ence of text. Recognizing that document images typicallycontain substantial textual content, we initially formed abinary classification dataset by combining natural imageswith document data. Subsequently, we trained an imageclassifier using a DiT base backbone, fine-tuned on theRVL-CDIP dataset . The purpose of this classifier wasto predict whether an image contains text. Then a subsetwas constructed by selecting images with a predicted prob-ability greater than 0.8, while also adhering to the criteriap(watermark) < 0.8 and p(unsafe) < 0.5, where both prob-abilities are derived from the metadata of the LAION dataset.Acknowledging the noise introduced due to the classifierslimitations, we further refined the dataset by incorporat-",
  ". Word clouds of (a) predicted tags and (b) detected wordsfrom the text-rich images of TRINS": "ing human judgment. A random sample of 20,000 imagesfrom the filtered LAION-5B was clustered into 50 groupsbased on CLIP-ViT-B/32 visual features. After inspectingthe clustering results, one cluster was meticulously chosen,encompassing diverse text-rich images such as posters, cov-ers, advertisements, and educational documents. This clustermodel then served as the filtering mechanism for collectingimages that comprise the TRINS dataset. For reference, wepresent a CLIP-based categorization in to de-pict the distribution of images in the collected data. Themajor class is book cover images, further categorized onthe basis of book themes and contents. To enhance our un-derstanding of text-rich images, we employed a RecognizeAnything Model (RAM) to extract tags from TRINSimages. a displays word clouds of RAM tags, wherebook and poster emerge as major keywords. Additionally,we utilize the Azure Read API and PaddleOCR to extracttext within TRINS images. The word cloud of the extracted",
  ". OCR word (a), Caption (b), Question (c) and Answer (d) statistics for TRINS": "texts is presented in b. a illustrates the dis-tribution of OCR words per image, indicating that most ofthe images in TextCap have fewer than 10 words, while theTRINS images average 31.4 OCR words. Recognizing textswithin TRINS images is more challenging because of thepresence of numerous small words. In summary, TRINS im-ages encompass rich visual content, seamlessly integratingtextual information into the image context.",
  ". Annotation Details": "Annotator SelectionsAll annotators (with the tag 100%Job Success and Top Rated Plus) are native English speakersand have experience with document annotations. We firstasked all annotators to annotate a 200-image set and pro-vided them with detailed annotation guidelines with multipleexamples. In addition, we use Labelbox as an annotationtool and set quality control questions.Heuristic FiltersWe first use EasyOCR to extract textsfrom images and retain text phrases with more than threecharacters, a height greater than 5% of the canvas height, anda confidence score greater than 0.1. For each phrase retrieved,we employ an edit distance-based string matching algorithm(due to potentially erroneous OCR results) to search for itsoptimal matching substrings within the human-generatedcaption. The average score for all extracted phrases servesas a metric.Manual Reviews We accept annotations with high metricscores and reject the lowest for rework. We manually reviewother annotations.Sensitive ImagesWe combined neural models with hu- man efforts to filter the images. The first step involves aninitial data filtering by 2-3 individuals to filter out sensitiveimages for training. The second step involves hiring ad-ditional people to perform a further screening on the data.We engaged annotators from various countries to check theselected images.",
  ". TRINS-Cap: Text-Rich Image Captioning": "Annotation ProcessTRINS-Cap is a dataset fully anno-tated by human annotators. We hired 20 native English speak-ers with experience in document annotation through Upwork.The annotation process, conducted in LabelBox, involveda total of 2,079 hours to annotate 40,576 text-rich images,with an additional 159 hours allocated for result review. Af-ter filtering low-quality annotations and addressing missingimages, we obtained a final set of 39,153 image-annotationpairs. The dataset is partitioned into train, validation, andtest splits with sizes of 29,153, 5,000, and 5,000, respec-tively. All annotations undergo an initial automated reviewthat involves matching the OCR words with the annotations.Subsequently, human evaluators conduct a thorough review,rejecting annotations with errors, and prompting annotatorsto rework them for enhancement. We provide comprehen-sive annotation instructions to all annotators to ensure thateach annotation includes: (i) detailed descriptions of visualcomponents. (ii) describe texts location, attributes, and puttexts into annotations. (iii) optional insights or abstract de-scriptions.Statistics and AnalysisThe primary objective of the an-notation process is to facilitate a human or machines fullcomprehension of the information conveyed in the imagewithout direct viewing. Consequently, the average annota-tion length for TRINS is 65.1 words, significantly exceedingthat of COCO (10.6 words) and TextCaps (12.4 words). Fig-ure 4b shows the caption length distributions for TRINS-Cap,COCO, and TextCap, demonstrating the comprehensive na-ture of the TRINS-Cap annotations. TRINS with more con-texts can generally provide a better description of compleximages, where short captions are insufficient. Hence, LLMsfine-tuned on TRINS can better understand images with com-plex texts and layouts, which has been further verified in.",
  ". TRINS-VQA: Multimodal Question Answering": "The annotation process for question answering is inherentlycomplex, primarily due to the necessity for annotators togenerate high-quality questions. Creating an effective ques-tion is more challenging than providing an answer. As aresult, annotators frequently gravitate toward formulatingconcrete and extractive questions (e.g., Who is the author ofthis book?) rather than abstract ones (e.g., How does thedesign of the book cover reflect the content of the book?).We introduce semi-automatic annotation methods to gener-ate high-quality visual question-answering data for TRINS-VQA. This dataset is designed to train general vision lan-guage assistants through instruction fine-tuning, and its ben-efits on model performance are evaluated in .1.TRINS-Cap, on the other hand, serves as human-assistedannotations, offering a comprehensive but non-instructivedataset for fine-tuning. To utilize the wealth of high-qualityannotations available, we incorporated semi-automatic an-notation by using large language models (LLMs) such asOpenAIs GPT-4 and Llama-70B to enhanceour data annotation pipeline. OCR results and detailed de-scriptions of each image are provided to LLMs. Furthermore,high-quality human-crafted demonstrations and detailed an-notation rules are provided to LLMs. One demonstrationfocused on extract questions, while the other emphasizedabstract questions, creating a more balanced dataset. Human AnnotationsTo facilitate a robust evaluation ofmodel performance, we hired 10 Upwork annotators, whosenative language is English, to annotate the test dataset, fol-lowing a methodology similar to previous work .The test dataset comprises 5,000 images with 18,764question-answer pairs. These data collected are used ex-clusively for evaluation purposes. Statistics and AnalysisBuilding upon prior research, we provide visualizations of instructions in based on question keywords. The inner cycle illus-trates the distribution of the first word in the questions, whilethe outer cycle presents extracted keywords determined bycarefully designed heuristics. Types of questions are catego-rized according to keywords found in questions.In c and 4d, we present statistics on the numberof words per question-answer pair, comparing them withprevious work. Generally, the average length of questionsfor TRINS-VQA is 10.5, surpassing that of DocVQA (8.3),OCR-VQA (6.5), and TextVQA (7.1). Surprisingly, the av-erage answer length for TRINS is 23.9, significantly longerthan related datasets (all less than 4). This discrepancy arisesfrom TRINS containing more abstract questions that typi-cally have longer answers. Similarly, the dataset is dividedinto train, validation, and test splits. For extract questions,the accuracy of the answers is calculated similarly to Liuet al. , while for abstract questions, generation metrics",
  "What is the title of the book shown in the image?": ". Model overview of the LaRA. The CLIP model processesthe input image to generate patch-wise features. These featuresthen serve as input to a projection layer, yielding visual tokens.Concurrently, an OCR tool extracts textual data from the image,which is then merged with the user instruction. such as BLEU scores are used to assess the quality of theanswers.The question-answering and instruction data we obtainare extensive, encompassing a balanced mix of extract andabstract questions. This includes detailed descriptions, sum-maries, question-answer pairs, tasks that promote creativityand the generation of novel thoughts, and conversationaltasks. The dataset spans a diverse range of concepts, rangingfrom visual presentation and visual language relations to in-tricate reasoning tasks. Compared to previous methods ,captions generated by models such as BLIP-2 exhibitless informativeness than detailed human annotations, asdemonstrated in .2. Additionally, captioning mod-els may be susceptible to hallucinations , a concernmitigated in TRINS, which provides more comprehensiveand reliable visual descriptions. Although OCR tools arerobust, they can introduce noisy information. To addressthis, we utilize both the Azure Read API and PaddleOCR toextract text information. We added the potential unreliabilityin our system prompts to LLMs, instructing them to gener-ate questions with assured answers. Ultimately, we directlyleverage the responses from large language models (LLMs)to construct TRINS-VQA. The quality of the instruction datacan be further enhanced through self-alignment or byseeking verification from human annotators. Although thishas not been explored in this work, we leave it as a potentialdirection for future research.",
  ". Examples generated by different multimodal language models on the abstract TRINS-VQA benchamrk": "coder generates an excessive number of patches, drasticallyimpacting model efficiency. Our conviction is that a low-resolution visual encoder is designed to capture visual infor-mation more effectively, including layouts. In contrast, a con-siderably smaller OCR tool is employed to extract text fromhigh-resolution images. Instruction-tuning on TRINS couldserve as an effective method of training LLMs to better alignOCR texts and extract visual features. In addition, LLMs canautonomously rectify errors generated by OCR tools. Forthe visual encoder V , we employ CLIP-ViT-L/14-336at a resolution of 336336. The grid features before the lasttransformer layer are then mapped into the word embeddingspace of the language decoder using a trainable projectionmatrix W. Regarding the language decoder D, we utilizeVicuna-1.5-13B , a language model tuned for instructionsthrough LLaMA 2 .We follow the two-stage training design of LLaVA butadopt the pre-trained projection layer of LLaVAR .Training targets remain similar: generating output responses(<res>) for the input instructions (<ins>), alongside OCRresults (<ocr>). The transformed image tokens (<img>)are introduced before or after the first input instruction ran-domly when building the instruction finetuning data. Dur-ing the finetuning stage, both the projection matrix W andthe language decoder D are trained. We consolidate ournearly 90K visual question-answering data with the 158Kinstruction-following data from LLaVA to form the trainingset. It should be noted that the visual encoder remains frozenthroughout the training period. Compared with previousapproaches, LaRA incorporates OCR words as part of theinput, a simple way to enhance visual text understanding.",
  ". TRINS-VQA: Text-Rich Image Visual Ques-tion Answering": "We first performed experiments to evaluate the zero-shotperformance of LaRA on classical benchmarks . Theresults are reported in and . The proposedLaRA model exhibits significant performance improvementacross all the datasets compared to other models. Even with-out OCR, LaRA outperforms other models in most cases,highlighting its robustness and effectiveness in handling vi-sual question-answering tasks. The inclusion of OCR inLaRA further enhances performance, suggesting that LargeLanguage Models (LLMs) can proficiently utilize textual in-formation. However, the ability to directly extract text fromimages remains limited. LaRA, with its simple structure,significantly boosts model performance on text-rich images,offering an alternative solution to overcome the limitationsof pre-trained image encoders. We further evaluate different",
  ". Results of different models on TRINS-VQA for text-rich image question-answering tasks": "methods on the TRINS-VQA dataset, as shown in .For extraction questions, we use the same metric as Wu et al.. For abstract questions, where the answer is typically alonger sentence, we evaluate them based on text similaritymetrics such as BLEU , ROUGE , and CIDEr .In zero-shot inference, LLaVAR with OCR exhibits the bestperformance, reinforcing the importance of extracting tex-tual information. Furthermore, mPLUG-Owl2 and Qwen-VL perform well and represent the best methods in extractquestion evaluations, showing that a high-resolution encodercan significantly improve model performance. Instruct-BLIPdemonstrates good performance on extract questions, butdid not fare as well on abstract questions, given that theanswers provided are usually short and concise. shows an example of the responses of different methods onthe abstract TRINS-VQA dataset, and more examples can befound in the Appendix D. Qwen-VL includes all details butdoes not provide high-level insights, such as the ground-truthannotation. Both mPLUG-Owl2 and GPT-4V suffer fromhallucination issues.",
  ". TRINS-Cap: Text-rich image Captioning": "In our experiments on TRINS-Cap, we ask large multi-modal models to generate summaries based on text-richimages. The data set was divided into train, validation, andtest sets. We compared LaRA with popular baselines, in-cluding InstructBLIP , Mini-GPT4 , LLaVA , LLaVAR , mPLUG-Owl2 and Qwen-VL . Giventhat BLIP-2 faced challenges in generating comprehensiveand meaningful results for text-rich images, we consideredInstructBLIP as an alternative. For all methods, we randomlyselected three prompts from ten as instructions for the model(details provided in the Appendix C.1). presents the results of different methods in termsof classical captioning metrics.Models with enhancedvisual text understanding generally outperform generalmultimodal models, such as LLaVA, Mini-GPT4, andInstruct-BLIP. LaRA (zero-shot) refers to the LaRA modelfine-tuned on the TRINS-QA dataset and demonstratesimproved performance. Comparison of fine-tuned LaRAvariants indicates that text recognition ability is still limitedfor OCR-free methods, suggesting that the CLIP encoderor feature projection process may cause visual informationloss. Addressing this limitation may involve employing abetter trained encoder on text-rich images or designing amore carefully crafted architecture, a direction we leavefor future exploration. When fine-tuned with TRINS-Cap,LaRA exhibits much better performance, underscoring theimportance of high-quality human-annotated data. shows examples of different models on the TRINS-Capbenchmark.It shows the great capability of LaRA inrecognizing text and relating it to its visual contexts,demonstrating the effectiveness of the TRINS dataset.",
  ". Additional Experiments": "Performance on general visual tasks after TRINS fine-tuning.We adopted the evaluation protocols of MiniGPT-v2 and compared LaRA with LLaVA on tradi-tional visual question answering benchmarks in table 5.LaRA shows a comparable performance on knowledgeabilityand better performance on reasoning and spatial awareness.This further verifies the effectiveness of the TRINS dataset,demonstrating that fine-tuning on text-rich images does notdegrade performance on natural images, but instead enhancesthe results.",
  ". Empirical Results on TRINS-Gen (easy) benchmark": "Text-to-document generationDiffusion-based text-to-image generation has shown great success, while precisetextual renderings remain a big challenge. TextDiffuser in-troduced the MARIO-Eval benchmark, drawing from workssuch as DrawBench and DrawTextCreative . How-ever, most text prompts in MARIO-Eval are short and cannotserve as a good evaluation dataset to handle complex real-world human instructions. We take advantage of humanannotations from TRINS-Cap and build the TRINS-Genbenchmark. It is still difficult to render too many words ina single image . In response to this, we filter out imageswith more than 20 OCR words, resulting in a curated set of2,104 images. We divide these images into two sets (easy and difficult) based on the number of OCR words and thelength of the longest OCR string per annotation, where alltext prompts in the easy set have less than 9 OCR words. Weevaluated existing methods using their public checkpointsand reported the results in and detailed results in (Appendix C.3).",
  ". Conclusions": "Despite the challenges posed by the prevalence of natural im-ages in training data, the significance of visual textual under-standing cannot be understated. In this paper, we introduceTRINS, a Text-Rich Image INStruction dataset, comprisinga diverse collection of text-rich images, captions, and ques-tions. This dataset, created through a semi-automatic processleveraging large-scale pre-trained models, not only signifi-cantly reduces annotation time but also elevates annotationquality. Furthermore, we propose a novel multimodal lan-guage model architecture, LaRA, which incorporates OCRas a pivotal enhancement for text-rich image understand-ing. We anticipate that continued progress in multimodallanguage model architectures, fine-tuning techniques, andthe expansion of diverse, text-rich datasets like TRINS willpush the boundaries of visual textual understanding. This,in turn, will facilitate more efficient collaboration betweenhumans and agents, potentially revolutionizing numerousreal-world applications. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. Advances inNeural Information Processing Systems, 35:2371623736,2022. 1 Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel,Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bit-ton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang WeiKoh, Gabriel Ilharco, Mitchell Wortsman, and LudwigSchmidt. Openflamingo, 2023. 22 Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, SinanTan, Peng Wang, Junyang Lin, Chang Zhou, and JingrenZhou. Qwen-vl: A frontier large vision-language model withversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 7,8 Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-aming Song, Karsten Kreis, Miika Aittala, Timo Aila, SamuliLaine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusionmodels with an ensemble of expert denoisers. arXiv preprintarXiv:2211.01324, 2022. 12",
  "Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, QifengChen, and Furu Wei. Textdiffuser: Diffusion models as textpainters. arXiv preprint arXiv:2305.10855, 2023. 1, 8, 12": "Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, ZechunLiu,Pengchuan Zhang,Raghuraman Krishnamoorthi,Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.Minigpt-v2: large language model as a unified interfacefor vision-language multi-task learning.arXiv preprintarXiv:2310.09478, 2023. 8 Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, ZhanghaoWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, YonghaoZhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.Vicuna: An open-source chatbot impressing gpt-4 with 90%*chatgpt quality, 2023. 6 Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shix-iang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, KevinRobinson, Dasha Valter, Sharan Narang, Gaurav Mishra,Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.Scaling instruction-finetuned language models, 2022. 1 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. 2",
  "general-purpose vision-language models with instruction tun-ing. ArXiv, abs/2305.06500, 2023. 7, 8": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale, 2020. 1 Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,Marcal Rusinol, Minesh Mathew, C.V. Jawahar, Ernest Val-veny, and Dimosthenis Karatzas. Icdar 2019 competitionon scene text visual question answering. 2019 InternationalConference on Document Analysis and Recognition (ICDAR),2019. 2, 5 Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue,Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model, 2023. 2",
  "Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis.Evaluation of deep convolutional nets for document imageclassification and retrieval, 2015. 3": "Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi,Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, and ShinjiWatanabe. Audiogpt: Understanding and generating speech,music, sound, and talking head. ArXiv, abs/2304.12995, 2023.2 Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, RuiFeng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei Zhang.Tag2text: Guiding vision-language model via image tagging.arXiv preprint arXiv:2303.05657, 2023. 3",
  "Chunyuan Li. Large multimodal models: Notes on cvpr 2023tutorial. ArXiv, abs/2306.14895, 2023. 1": "Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Lin-jie Li, Lijuan Wang, and Jianfeng Gao. Multimodal founda-tion models: From specialists to general-purpose assistants.arXiv preprint arXiv:2309.10020, 1, 2023. 2 Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang,and Furu Wei. Dit: Self-supervised pre-training for documentimage transformer. Proceedings of the 30th ACM Interna-tional Conference on Multimedia, 2022. 3",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.Visual instruction tuning, 2023. 1, 2, 5, 6, 7, 8, 22": "Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan,Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mo-hammad Norouzi, and Noah Constant.Character-awaremodels improve visual text rendering.arXiv preprintarXiv:2212.10562, 2022. 8, 12 Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, MingxinHuang, Dezhi Peng, Mingyu Liu, Mingrui Chen, ChunyuanLi, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the hiddenmystery of ocr in large multimodal models, 2023. 1, 2, 22",
  "Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar.Docvqa: A dataset for vqa on document images, 2020. 5": "Minesh Mathew, Viraj Bagal, Rub`en Tito, DimosthenisKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa.In Proceedings of the IEEE/CVF Winter Conference on Ap-plications of Computer Vision, pages 16971706, 2022. 2 Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, andAnirban Chakraborty. Ocr-vqa: Visual question answering byreading text in images. In 2019 international conference ondocument analysis and recognition (ICDAR), pages 947952.IEEE, 2019. 2 OpenAI. Gpt-4 technical report, 2023. 1, 5 Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agar-wal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.Training language models to follow instructions with humanfeedback, 2022. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever. Learning transferable visualmodels from natural language supervision, 2021. 1, 3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 1, 2 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J Liu. Exploring the limits of transfer learning witha unified text-to-text transformer. The Journal of MachineLearning Research, 21(1):54855551, 2020. 12",
  "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, TrevorDarrell, and Kate Saenko. Object hallucination in imagecaptioning. arXiv preprint arXiv:1809.02156, 2018. 5": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,Jay Whang, Emily L Denton, Kamyar Ghasemipour, RaphaelGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-torealistic text-to-image diffusion models with deep languageunderstanding. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022. 8, 12 Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.Laion-5b: An open large-scale dataset for training next gener-ation image-text models. arXiv preprint arXiv:2210.08402,2022. 3",
  "Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva,Christoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova.Deepfloyd. 12": "Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Aman-preet Singh. Textcaps: a dataset for image captioning withreading comprehension. In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part II 16, pages 742758. Springer, 2020.2 Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xin-lei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.Towards vqa models that can read. 2019 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),2019. 2, 5 Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Woj-ciech Galuba, and Tal Hassner. Textocr: Towards large-scaleend-to-end reasoning for arbitrary-shaped scene text. In Pro-ceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 88028812, 2021. 2",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.Hashimoto.Stanford alpaca:An instruction-followingllama model. 2023. 5": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-tinet, Marie-Anne Lachaux, Timothee Lacroix, BaptisteRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, AurelienRodriguez, Armand Joulin, Edouard Grave, and GuillaumeLample. Llama: Open and efficient foundation languagemodels, 2023. 5, 6 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-jad Almahairi, Yasmine Babaei, Nikolay Bashlykov, SoumyaBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:Open foundation and fine-tuned chat models. arXiv preprintarXiv:2307.09288, 2023. 1, 5 Ramakrishna Vedantam, C Lawrence Zitnick, and DeviParikh. Cider: Consensus-based image description evalu-ation. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 45664575, 2015. 7",
  "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.Self-instruct: Aligning language models with self-generatedinstructions, 2022. 5": "Zizhang Wu, Xinyuan Chen, Jizheng Wang, Xiaoquan Wang,Yuanzhu Gan, Muqing Fang, and Tianhao Xu. Ocr-rtps: anocr-based real-time positioning system for the valet parking.Applied Intelligence, pages 115, 2023. 6, 7 Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang,et al. Ureader: Universal ocr-free visually-situated languageunderstanding with multimodal large language model. arXivpreprint arXiv:2310.05126, 2023. 2 Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Jun-feng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl:Modularization empowers large language models with multi-modality, 2023. 2, 7, 22 Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu,Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2:Revolutionizing multi-modal large language model withmodality collaboration. arXiv preprint arXiv:2311.04257,2023. 7, 8, 22",
  "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: Aninstruction-tuned audio-visual language model for video un-derstanding. arXiv preprint arXiv:2306.02858, 2023. 2": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, AojunZhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, andYu Qiao. Llama-adapter: Efficient fine-tuning of languagemodels with zero-init attention, 2023. 2 Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, YaqianLi, Shilong Liu, et al. Recognize anything: A strong imagetagging model. arXiv preprint arXiv:2306.03514, 2023. 3 Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhancedvisual instruction tuning for text-rich image understanding.arXiv preprint arXiv:2306.17107, 2023. 1, 2, 5, 6, 7, 8, 22",
  "B. TRINS-Gen: Text-to-Image Generation": "Text-Rich Image GenerationDiffusion-based Text-to-image Generation has shown great success, while precisetextual renderings remain a big challenge. Imagen ,eDiff-I , and DeepFolyd exploit large T5 models for better precision. The study by Liu et al. proposes acharacter-sensitive alternative to traditional token length han-dling. GlyphDraw merges superior images with Chinesetexts, while Textdiffuser uses the Transformer model formultiline text generation and segmentation masks for usercontrol. However, manual input is still needed for key-termrecognition. TextDiffuser also introduced the MARIO-Evalbenchmark, drawing from works such as DrawBench and DrawTextCreative . However, most text prompts inMARIO-Eval are short and cannot serve as a good evaluationdataset to handle complex real-world human instructions. InTRINS-Gen, we introduce a new human-annotated imagegeneration benchmark for existing text-rendering methods,together with a new training dataset.Text rendering is a main challenge for text-rich imagegeneration, and we do an extensive evaluation of existingmethods on the TRINS-Gen benchmark. We take advantageof human annotations and build the TRINS-Gen benchmark.We have chosen data examples with fewer words, as ex-isting methods cannot yet render too many texts within asingle image. Following Chen et al. , we consider thecalculated FID score between the generated images and theTRINS-Gen ground truth images. In addition, we calculatethe CLIP scores to measure whether the visual componentswithin the generated image followed the text instructions,and OCR accuracy to measure whether the desired texts aregenerated correctly. We evaluated existing methods usingtheir public checkpoints and reported the results in Table ??.We can see that DeepfFloyd has the best image quality andaligns well with the user-text prompts. ControlNet, whichtakes a rendered black-white text image as a condition, hasan image quality similar to that of TextDiffuser. In termsof text rendering ability, TextDiffuser is best compared toother baselines. In , we show two examples ofimages generated by different methods. Stable Diffusion(SD) models cannot render any meaningful text. Deepfloydand ControlNet may miss some words or render texts withspelling errors. TextDiffuser can generally render text well when the length of words is short. However, all methodsdo not render the desired texts when the number of wordsincreases, as described in the Appendix C.3. The main causein TextDiffuser is that its layout models cannot parsecomplex prompts, and an LLM-based layout model mayhelp.",
  "B.1. DocGen: Text-rich Image Generation": "Given high-quality text-rich image descriptions, it is natu-ral to consider their application in training and evaluatingtext-rich image generation models . As highlighted in Sec-tion 3.3, the average number of OCR words is 65.1. How-ever, generating such a substantial amount of text withina single image poses a significant challenge for diffusionmodels. In response to this, we have established the firsthuman-annotated text-rich image generation benchmark.This benchmark involves filtering out images with more than20 OCR words, resulting in a curated set of 2,104 images.We divide these images into two sets based on the numberof OCR words and the length of the longest OCR string perannotation, where all text prompts in the easy set have lessthan 9 OCR words with the longest OCR string of 5. Visualexamples of both data sets are included in Appendix A.",
  "C.3. More TRINS-Gen Results": "shows the results of different methods on theTRINS-Gen difficult benchmark. Deeploy model needsmore than two A100-80GB GPUs and we omit its resultdue to the resource limitation. SD-2.0 model demonstratedthe best performance in image quality and alignment withtext descriptions, outperforming SD-1.5, ControlNet, andTextDiffuser. TextDiffuser, however, showed its strengthin accurately generating text within images, leading in met-rics related to text recognition. ControlNet exhibited bal-anced performance in these text-related tasks, making it anotable contender in terms of precision and recall balance.In general, when text prompts become complex, SD-2.0 andSD-1.5 can still generate meaningful images related to theprompts. Textdiffuser struggle to place to many words in theimage and fail to generate good images. In addition, its textgeneration quality is much lower as the number of generatedwords increases. Based on the results, it is obvious to see",
  ". TRINS datasets annotation statistics": "This is the cover of the book titled \"Triple Love Score.\" The title is centrally positioned, written in a cursive black font with a prominent size, leaning towards big. Right above the title, the author's name \"Brandi Megan Granett\" is presented in a smaller uppercase font. Directly below the title, the phrase \"a novel\" is subtly written, indicating the genre of the book. At the bottom, there's a quote that reads, A guilty pleasure. attributed to Jacquelyn Mitchard, with an additional mention: \"Author, The Deep End of the Ocean.\" The most captivating aspect of the background is an inviting and warm scene. It portrays a bright yellow bowl filled with liquid, with the word \"LOVE\" spelt out using tile letters on a table. The image is of a book cover for a biography of Martin Luther King Jr. written in Spanish. The book cover is predominantly yellow with a purple border. The title of the book is \"Martin Luther King Jr. Una vida de determinacin\" written in white and black text. The author's name, \"por Sheila Rivera\", is written in smaller yellow text at the bottom of the cover. The cover features a photo of Martin Luther King Jr. The bottom right corner of the cover has a small logo for \"Lerner\" publishing.",
  "shows the annotation statistics from the LabelBox": "This is an image of four DVD plates with the title \"LAND OF PROMISE: THE BRITISH DOCUMENTARY MOVEMENT 1930-1950\". At the left side of the plate is an acronym \"BFI\", at the right side of the plate is written, 'Disk 4'. On the plate is a picture of a man carrying stick on his shoulder. The plates is flagged at the left and right sides with the cover of the plates. On the left cover is photo of people sitting and looking up,. On the right cover is an image of two children sitting and reading. The texts on the plate is the replica of that on the covers. The background colour of the plates and covers is black. This is a book cover. The book cover is pictured next to the portrait of the author who is a man with a beard wearing a black shirt and black glasses. Text in the upper right of the cover reads scared the living hell out of me, and I'm pretty hard to scare. Stephen King on a head full of ghosts. The title survivor song is in large orange and black radiant text in the center of the cover with subtext below in the bottom center that reads Paul tremblayBram Stoker award-winning author of the cabin at the end of the world. The cover image is a black line illustration of a neuron turning into a dandelion with seeds blowing away in the top left. This image appears to be a visual representation discussing the relationship between humans and horses. The title \"Do horses like humans?\" stands out prominently, positioned at the bottom. The font color is black, and the size of the title is big. Below the title, there's an additional line that says, \"Studies show they understand our emotions,\" in a smaller font, also in black and located at the very bottom of the image. The prime focus in the background showcases a close-up of a young girl giving a gentle kiss to a horse's snout This is a book cover. The title does kangaroo have a mother too? Is enlarged colorful text in the top with subtext under it that reads by eric Carle. The cover image is a mother kangaroo with a Joey in her pouch standing on green grass and behind a yellow sun done in a watercolor paper cut style.",
  ". TRINS-Cap Human Annotated Examples": "This is a book cover. Text in the upper left reads A Golden Book. The title The Big Golden Book of Backyard Birds is in large blue and red text on the upper center with subtext underneath that reads text by Kathleen N. Daly. The cover image is an illustration of a variety of birds in a backyard scene with sparrows and white flowers in the foreground in the left and a cardinal sitting on top of a wooden post with Ivy growing on it in the foreground on the lower right. This is a book cover. Text at the top reads Lonely Planet. The title Western USA is in large white text in the upper center. Blue rectangle in the bottom left contains subtext that reads Easy to Use Maps New- look guide comprehensive listings. The cover image is a photograph of brown and black and red cowboy boots hanging on a hook on a red distressed wooden wall. The image is of a book cover for \"The Adventures of Tintin and the Picaros\" by Herg. The cover is illustrated in a comic book style and features Tintin, his dog Snowy, and two other characters in a jungle setting. The background is a pyramid temple with red flowers and green foliage. The title of the book is in a yellow speech bubble at the top of the cover, and the publisher's logo is at the bottom of the cover. The image is of a book cover for the novel \"The Damsel Fly\" by SJI Holliday. The cover is black with a large blue-green dragonfly on the right side. The title of the book is written in large blue letters, and the author's name is written in smaller white letters at the bottom of the cover. The top of the cover has a quote in white letters that reads \"An unsolved murder. A community turned against each other. A killer close to home...\". The cover has a distressed and torn look around the edges.",
  ". Empirical Results of different methods on TRINS-Gen (easy) benchmark": "GT: The image is of a audio book cover for \"Just One Kiss\" by Susan Mallery. The cover is a photo-realistic image of a couple laying on a grassy field in a romantic embrace. The title of the book is written ina large blue font at the center right of the cover, and the author's name is in a larger green font at the top.The cover also includes a quote from #1 New York Times bestselling author Debbie Macomber \"SusanMallery is one of my favorites.\" and a tagline for the book \"A fool's Gold Romance\". There is alsoBrillianceAudio logo at the top left and Unabridged at the top right. LLaVA: The image depicts a man and a woman sitting in a grassy field, enjoying a summer day. They arepositioned close to each other, creating an intimate atmosphere. The woman has her head resting on theman's shoulder, and both seem to be relaxed in each other's company. A book, likely a paperback copy ofSusan Mallery's \"Just One Kiss,\" is visible in the scene, suggesting that they might be spending their dayreading and bonding outdoors. There is also a clock in the background, indicating they are keeping track ofthe time during their picturesque summer day. LLaVAR: In the image, a couple is seen lying down in the grass, sharing a intimate moment with eachother. They are positioned next to each other, and it seems like they are about to kiss. The scene is set witha backdrop of grass and flowers, making it a romantic and serene environment. Additionally, there are afew words scattered throughout the scene, likely related to the quote or description of the book. There isalso a small cat situated at the top right corner of the image, possibly observing the couple or enjoying theoutdoors. LaRA: This is the cover of the audiobook named Just One Kiss: A Fools Gold Romance. Here the title is written at the lower part with large fonts. At the upper part it mention theauthor Susan Mallery. Just before that it says, New York Times Bestselling Author. Under the authors name it include an endorsement. At the top right it include a logo ofUnabridged and at the top left it include the logo of Brilliance audio. The cover include an image of an intimate couple as the backdrop. Instruct-BLIP: Just One Kiss is a romance novel written by Susan Mallery. In this image, the cover of thebook is prominently displayed in the foreground, with the title \"Just One Kiss\" and the author's name,Susan Mallery, clearly visible. The couple on the cover is shown in an intimate embrace, with the womanleaning over the man's shoulder. There are several other books scattered throughout the image, but they arenot as prominent as the cover of Just One Kiss. LaRA w/o OCR (zero-shot): The image displays the cover of a romance novel called \"Just One Kiss\" by Susan Mallery. The book is a New York Times Bestseller, and the author isrecognized as a #1 New York Times Bestselling Author. The text on the image is a quote from Debbie Macomber, another renowned author, who states that Susan Mallery is one ofher favorite authors. The quote is attributed to the New York Times bestselling book \"Just One Kiss\". The cover also features an image of a couple engaged in an intimate moment. Qwen-VL: The image shows a couple lying on the grass, facing each other and kissing. The man is wearing a yellow shirt and is in the process of removing his sunglasses. Thewoman has long dark hair and is wearing a light blue dress. Her left hand is placed on the man's chest, while his right hand is holding hers. They both have light skin and are smiling. mPLUG-Owl2: The image features a man and a woman sitting on a beach. They are both wearing swimsuits, and the woman has a handbag beside her. The scene appears to be aromantic moment between the two individuals, enjoying their time together on the beach. GT: This is a book cover. Text in the upper left reads A Golden Book. The title The Big Golden Book ofBackyard Birds is in large blue and red text on the upper center with subtext underneath that reads text byKathleen N. Daly. The cover image is an illustration of a variety of birds in a backyard scene with sparrows andwhite flowers in the foreground in the left and a cardinal sitting on top of a wooden post with Ivy growing on itin the foreground on the lower right. LLaVA: The image features a beautifully illustrated children's book about birds. The book cover depicts awooden birdhouse hanging on a tree, surrounded by various birds perched around it. Some birds are sitting onthe birdhouse, while others are scattered across the scene, showing the lively nature of the illustration. The bookitself likely contains stories or information about these birds and their habitats. The illustration is bright andcolorful, capturing the essence of nature and attracting the attention of young readers. LLaVAR: The image features a wooden birdhouse mounted on a wooden post. A bird can be seen perched onthe bird feeder, making it an inviting sight for birds to visit. There are a few books in the scene, with one largebook taking up much of the space on the right side, and another smaller book positioned on the left side. Thebooks seem to be related to birds or nature, possibly providing information about the birds that might visit thebirdhouse. LaRA: This is a cover of a book titled \"THE BIG GOLDEN BOOK OF BACKYARD BIRDS\" The text on the cover is written in a light blue color, with a black outline. At the center ofthe cover is written \"Text by Kathleen M. Daly\" At the top of the cover is a logo of a tree with branches and goldens. \"A Golden Book\" is written in small letters beneath it The bookcover has a picture of a bird house with birds flying in and out in the background. The background color of the book cover is green. The book is placed on a blue background. Instruct-BLIP: A book titled \"The Big Golden Book of Backyard Birds\" is displayed on a blue background. Thebook showcases various types of birds, including cardinals, robins, blue jays, and sparrows, among others. LaRA w/o OCR (zero-shot): The image features the cover of a book titled \"The Big Golden Book of Backyard Birds\". The book has a blue cover with a white border. The title iswritten in a mix of blue and red text. There is a bluebird illustration on the cover, and it is sitting on a birdhouse. The birdhouse is mounted on top of a wooden post. The background ofthe cover shows a backyard scene with trees and grass.",
  "mPLUG-Owl2: The image features a book titled \"The Big Golden Book of Backyard Birds.\" The book is placedon a blue background, and there is a bird on a post in the foreground": "Qwen-VL: The image features a blue book cover with the title \"The Big Golden Book of Backyard Birds\"written in red and gold letters. There are several birds mentioned in the book, as illustrated by the various birdsfeatured on the cover. A cardinal is perched on a wooden post, while a black-capped chickadee is at the left sideof the cover. A blue jay can be seen flying above a birdhouse, and a sparrow is standing on the ground near thebottom of the cover. The book is placed on a blue surface with green plants on the right side of the cover, addinga touch of nature to the scene..",
  "Question: What is the title of the book shown in the image? Answer: The Cajun Doctor": "InstructBLIP: The Cajun Doctor LLaVAR: The title of the book is \"The Cajun Doctor\" by Sandra Hill. mPLUG-Owl2: The title of the book is \"The Cajun Doctor\" by Sandra Hill. Qwen-VL: The title of the book shown in the image is \"The Cajun Doctor\". LaRA: The title of the book is \"The Cajun Doctor\".",
  "Question: What is the title of the book? Answer: The Adventures of Tintin and the Picaros": "InstructBLIP: The Adventures of Tintin and the Picaros LLaVAR: The title of the book is \"The Adventures of Tintin and the Picaros\" by Herge. mPLUG-Owl2: The title of the book is \"The Adventures of Tintinand the Picaros.\" Qwen: The title of the book is \"The Adventures of Tintin and the Picaros\". LaRA: The title of the book is \"The Adventures of Tintin and the Picaros.\""
}