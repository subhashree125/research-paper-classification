{
  "Abstract": "The third Pixel-level Video Understanding in the Wild(PVUW CVPR 2024) challenge aims to advance the stateof art in video understanding through benchmarking VideoPanoptic Segmentation (VPS) and Video Semantic Segmen-tation (VSS) on challenging videos and scenes introducedin the large-scale Video Panoptic Segmentation in the Wild(VIPSeg) test set and the large-scale Video Scene Parsingin the Wild (VSPW) test set, respectively. This paper de-tails our research work that achieved the 1st place winnerin the PVUW24 VPS challenge, establishing state of artresults in all metrics, including the Video Panoptic Quality(VPQ) and Segmentation and Tracking Quality (STQ). Withminor fine-tuning our approach also achieved the 3rd placein the PVUW24 VSS challenge ranked by the mIoU (meanintersection over union) metric and the first place rankedby the VC16 (16-frame video consistency) metric.Ourwinning solution stands on the shoulders of giant founda-tional vision transformer model (DINOv2 ViT-g) and provenmulti-stage Decoupled Video Instance Segmentation (DVIS)frameworks for video understanding.",
  ". Introduction": "Video Panoptic Segmentation (VPS) is an importantcomputer vision task for scene understanding that is likelyto be widely deployed for autonomous driving, virtual real-ity, and mobile phone intelligent cameras. It aims at identi-fying, segmenting, classify, and simultaneously tracking allinstances for classes of interest and background stuff classesacross all frames in a video.Whereas image panoptic segmentation segments andclassifies instances of interest and background stuff classes,VPS is more complex since it must provide consistent seg-mentation and accurate tracking of each segmented instance",
  "*Corresponding author": "across all consecutive video frames it appears in. This be-comes particularly challenging with complex backgroundscenes, multiple moving subjects, occlusions, changed cam-era angles, and longer videos. In particular for videos withlong duration, the same instance or background class mayexhibit significant variations in positions, poses, shapes, oc-clusions, and view angles, which complicates the challengeof accurately associating the same ID across frames that arefar apart. Earlier work of VPS extended the models of imagepanoptic segmentation to video domain . Recent work focuses on developing univer-sal models that be applied to all video segmentation tasks,including Video Semantic Segmentation (VSS) and VideoInstance Segmentation (VIS). State-of-the-art (SOTA) VPSmethods have demonstrated promising per-formance by refining the embeddings generated from SOTAimage segmentation models using the temporal in-formation. One important work is DVIS that decoupled the taskof video panoptic segmentation into three independent sub-tasks: image segmentation, online tracker, and offline re-finer. DVIS++ further proposed a Noiser module toenhance the online tracker to acquire more stable and pow-erful tracking capabilities. Recent trend is to investigatethe performance of large visual foundation models on thedown-streaming tasks. DVIS++ used a medium sizefoundation model DINOv2-L (the large version of ViTtrained by DINOv2) that improves the performance uponSwin-L significantly. InternVL has shown impres-sive results on semantic segmentation using its 6b parametermodel InternViT-6B. Inspired by state-of-the-art methods, we conducted ex-tensive experiments and proposed to integrate the ViT-Adapter with DINOv2-g (the giant version ViT trained byDINOv2) into the DVIS framework. As a result, using this1.2 billion parameter model, we achieved the first placein the VPS track of the PVUW challenge in CVPR 2024.",
  "arXiv:2406.05352v1 [cs.CV] 8 Jun 2024": ". The System architecture overview. It consists of three independent components: a segmenter, an online tracker, and an offlinetemporal refiner. We mainly focus on improve the performance of segmenter by applying ViT-Adapter with ViT-g pretrained by DINOv2. Specifically, our model achieved 58.26% VPQ at the finaltest phase using single scale inference without any test timeaugmentation and model ensemble, as well as without usingany additional training data (including the validation set ofVIPSeg). In addition, we also achieved mIoU 63.92%, VC894.84% and VC16 93.25% in the test phase of VSS track,and ranked third place by finetuning our model trained onVPS track on VSS dataset.",
  ". Method": "DVIS proposed a novel decoupled framework for VideoPanoptic Segmentation that consists of three independentcomponents: an image segmenter, an online tracker, andan offline temporal refiner, as illustrated in 1. For moredetails, one can refer the DVIS paper .DVIS em-ployed Mask2Former as the segmenter in our frame-work. Mask2Former is constructed upon a straightforwardmeta-architecture comprising a backbone, a pixel decoder,and a transformer decoder.We found that DVIS already showed impressive resultsin terms of refining the embeddings, and marginal gain or no gain are observed by increasing the number of blocksin the online tracker and offline temporal refiner in our ex-periments. DVIS++ used a medium size foundation modelDINOv2-L (the large version of ViT trained by DINOv2)and a reduced ViT-Adapter as the backbone that improvesthe performance upon Swin-L significantly.Therefore, we further scale up the size of backboneby applying the ViT-Adapter using DINOv2-g inspired byDVIS++, for performance improvement. Please note thatwe neither used the Noiser module in DVIS++ as the train-ing performance is not stable in our case, nor used the con-trastive loss used in DVIS++ since it consumes a lot of GPUmemory.",
  "DVISMobileViTv236.94DVISSwin-L57.81DVISDINOv2-L58.80DVISDINOv2-g60.42": "bone MobileViTv2 , medium size backbone Swin-L , DINOv2-L and large visual foundation back-bone DINOv2-g .we also considered a even largersize backbone such as the InternViT-6B. However, for thechallenge we limited our submissions to results using theDINOv2 ViT-g foundational model as the backbone. Wecan observe that With large visual foundation backboneDINOv2-g, we can achieve the state-of-the-art performance60.42% VPQ on VIPSeg validation dataset.",
  ". ViT-Adapter design": "Current large visual foundation models are normally plainViT, that is pretrained on massive scale dataset to learnsemantic-rich representations. However, unlike specificallydesigned backbone like Swin, the plain ViT normally hasinferior performance on dense prediction tasks due to thelack of multiple scale features that are very important toperformance.We reference the design of the ViT-Adapter used inDVIS++ , as demonstrated in . We adapt theembedding from the large foundation model through theViT-Adapter, that is implemented as an additional branchthat interacts with the plain ViT blocks with a spatial priormodule and several Extractor modules. Note that originalViT-Adapter also has several Injector modules, whichwe removed, since, in our approach, the large visual foun-dational backbone ViT-g is frozen during training.The input image is input into the Spatial Prior module,where multiple scales of feature maps (e.g. 1/4, 1/8, 1/16,and 1/32) will be extracted and concatenated as input forExtractors. The role of the ViT-Adapter becomes evident, aswe then obtain multiple-scale features by splitting the ViT-Adapter output and providing them as the input for the pixeldecoder. The ViT-Adapter is trained together with the pixeldecoder and transformer decoder in an end to end fashion,",
  "while freezing the DINOv2 ViT-g (DINOv2-g) backbone": "In our experiments, the DINOv2-g consists of 40 blocks,we divide them evenly into four stages by indices [,, , ], where the output of each stagewill interact with the ViT-Adapter by using multi-scale de-formable attention, where we set the number of heads as24. We also considered another design choice that we callViT-CoMer-Adapter, which is a GPU memory friendlycombined version of ViT-CoMer and ViT-Adapter,where the Multi-Receptive Field Feature Pyramid (MRFP)module is applied before each Extractor module forbetter multiple scale feature interactions.However, inour preliminary experiments, ViT-CoMer-Adapter achievedsimilar performance as ViT-Adapter shown in",
  ". Finetune for Video Semantic Segmentation": "To test the generalization capability, we tested the modelpretrained on VIPSeg dataset for the VPS task directly onthe VSPW dataset for the VSS task without any finetun-ing. For this, we simply merged all the predicted instancesfrom the same class into a single class segmentation maskfor the VSS task. Next, we also finetuned the model pre-trained on VIPSeg dataset on VSPW dataset. It is inter-esting to observe that the pretrained model achieved bettertemporal consistency VC8 and VC16 on VSPW test datasetwithout any finetuning, while the finetuned model achievedbetter mIoU, as shown in . This can be attributedto the fact that video panoptic segmentation puts more em-phasis on video consistency by the extensive training foraccurately tracking multiple instances across frames. How-ever, video semantic segmentation training puts more em-phasis on the boundaries of each class, hence improves theper-frame mean intersection over union (mIoU) metric.",
  ". Dataset": "The VIPseg and VSPW datasets are used for the VideoPanoptic Segmentation track and Video Semantic Segmen-tation track in PVUW challenge 2024.VIPSeg VIPSeg dataset is a large-scale dataset forVideo Panoptic Segmentation dataset in the wild. Thereare 124 classes including 58 things and 66 stuffs classes.It consists of 3,536 videos and 84,750 frames, where2806/343/387 videos are for training/validation/testing, re-spectively.VSPW VSPW dataset has the same set of imagesfor training/validation/testing as VIPSeg dataset, while theannotation is just for video semantic segmentation.",
  ". Implementation Details": "For VPS, we follow the same training pipeline as the De-coupled Video Instance Segmentation (DVIS) frameowrk.The segmenter, referring tracker, and temporal refiner aretrained separately.We first pretrained the Mask2formermodel as our segmenter on MSCOCO panoptic segmen-tation dataset using batch size 16 and learning rate 1e-4for 50 epochs. Since we freeze the weights of the large vi-sual foundation model DINOv2-g, as the backbone for fea-ture embeddding, we only train the ViT-adapter part and thetransformer decoder part in this stage. Then, we finetunedthe segmenter using image-level annotations from the train-ing set of VIPSeg, following the training strategy of Min-VIS with batch size 16 and learning rate 1e-4 for 20000iterations, with a learning-rate decay rate of 0.1 at 14000iteration. With the initialization of segmenter, we furthertrained the DVIS online tracker with batch size 8 and learn-ing rate 1e-4 for 40000 iterations while reducing the learn-ing rate by half at 28000 iteration. In the last training stage, the DVIS offline temporal refiner is trained using the samesetting on VIPSeg training datasets.We also explored to increase the crop size of the DVISoffline training from (604, 604) to (840, 840), so we fine-tuned our model using batch size 8 and learning rate 2.5e-5for 10000 iterations and decayed at 7000 iteration using themodel trained with crop size (604, 604) as initialization.All the trainings are conducted on single machine of 8xA100 Nvidia GPUs. The DVIS online model used a con-secutive 5-frame clip from the video as input. The DVISoffline model used a window size of 21 frames as input.Multi-scale training is used to randomly scale the short sideof input video clips from 480 to 1080 during training. Bydefault, a random crop size (604, 604) is used except thelast stage of finetuning using crop size (840, 840).For VSS, we used the best model pretrained on VIPSegdataset as initialization and finetuned it on the VSPWdataset using a batch size of 8 and learning rate 2.5e-5 for10000 iterations and decayed at 7000 iteration.",
  ". Results for Video Panoptic Segmentation Track": "In the VPS track of PVUW Challenge 2024, we ranked firstin the test phase. The leaderboards for the development andtest phases are displayed in and , respec-tively. Our method achieved a VPQ of 55.69 in the devel-opment phase and 58.26 in the test phase.With the help of large visual foundation model as thebackbone, our method surpassed all other methods in allmetrics in the test phase. Additionally, our method alsoshowed its tracking stability when comparing the perfor-mance drop between VPQ6 and VPQ1. One thing to notethat we only used single scale inference to achieve the resultwithout using any test time augmentation or model ensem-ble.",
  ". Results for Video Semantic Segmentation Track": "In the VSS track of PVUW Challenge 2024, we ranked 3rdin the test phase, ranked by the mIoU metric and first rankedby the VC16 (16-frame video-consistency) metric.Theleaderboard for the test phase is displayed in . Ourmethod achieved a mIoU of 55.69 i and VC16 of 0.9325, inthe PVUW VSS test phase.In particular, our model tends to have better temporalconsistency in terms of metrics VC8 and VC16. We ranked2nd in VC8 and ranked 1st in VC16, which means ourmodel tends to have better tracking reliability since it isfinetuned from a VIPSeg pretrained model. We believe thatbetter mIoU can be achieved if we follow the same trainingpipeline as VIPSeg dataset, rather than simply finetuning aVIPSeg pretrained model on VSPW dataset.",
  ". Ablation Studies": "We also carefully tuned the hyper-parameters for several as-pects using the development stage dataset. As shown in Ta-ble 7, a finetuned model with crop size (840, 840) achievedslightly better performance than baseline (604, 604) andalso (630, 1120) with a different aspect ratio 16:9. We alsocompared different query sizes and found that 300 performssimilar as 200, so we still keep 200 query size as our choice,as shown in .",
  "This work demonstrates how the embeddings from largefoundational vision models such as the 1.1B-parameter DI-NOv2 ViT-g model can be adapted for very complex vi-": "sion understanding tasks such as Video Panoptic Segmen-tation (VPS) and Video Semantic Segmentation (VSS). Byadapting the foundational vision model embeddings to themultistage decoupled video instance segmentation (DVIS)framework, our solution achieved the 1st place winner inthe CVPR24 PVUW VPS challenge, establishing new stateof the art 58.26 VPQ (Video Panoptic Quality) and 0.543STQ (Segmentation and Tracking Quality) on the VIPSegtest set. For the CVPR24 PVUW VSS challenge, by fur-ther fine-tuning, our same solution achieved 3rd place inthe mIoU metric with 0.64 mIoU and 1st place in 16-framevideo consistency metric with 0.933 VC16, on the challeng-ing VSS test set.The crux of our solution is carefully designing and train-ing a vision transformer adapter (ViT-Adpater) that adaptsthe embedding from large foundational models for com-plex downstream vision tasks, while freezing the large foun-dational models.We showed by only training the ViT-adapter with the multistage down-stream vision models, thismethod generalizes well to multiple tasks.We demon-strated that the same embeddings from the frozen giantViT-g can be adapted for either the Video Panoptic Seg-mentation task or the Video Semantic Segmentation task,while achieving the state of art results in both tasks. Thisapproach of sharing a foundational backbone across mul-tiple vision tasks, has important implications in the de-sign of deep-learning based vision systems, such as the re-duction in training costs, simplified deep-learning visionmodel design, as well as reduced inference computationalcosts and inference latency for most systems that need torun multiple vision understanding tasks on the same videostream.",
  "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, TongLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter fordense predictions. arXiv preprint arXiv:2205.08534, 2022.3": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and JifengDai.Internvl: Scaling up vision foundation models andaligning for generic visual-linguistic tasks. arXiv preprintarXiv:2312.14238, 2023. 1 Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.Panoptic-deeplab:A simple, strong, and fast baselinefor bottom-up panoptic segmentation.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1247512485, 2020. 1",
  "Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In SoKweon. Video panoptic segmentation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 98599868, 2020. 1": "Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen,Guangliang Cheng, Yunhai Tong, and Chen Change Loy.Video k-net: A simple, strong, and unified baseline for videosegmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1884718857, 2022. 3 Xiangtai Li, Haobo Yuan, Wenwei Zhang, GuangliangCheng, Jiangmiao Pang, and Chen Change Loy. Tube-link:A flexible cross tube baseline for universal video segmenta-tion. arXiv preprint arXiv:2303.12782, 2023. 1, 3 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 4 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 1, 3",
  "Sachin Mehta and Mohammad Rastegari.Separable self-attention for mobile vision transformers.arXiv preprintarXiv:2206.02680, 2022. 3": "Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guangrui Li,and Yi Yang. Vspw: A large-scale dataset for video sceneparsing in the wild. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages41334143, 2021. 4 Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yun-chao Wei, and Yi Yang.Large-scale video panoptic seg-mentation in the wild: A benchmark.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2103321043, 2022. 4 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V.Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-janowski. Dinov2: Learning robust visual features withoutsupervision, 2023. 1, 3 Siyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, andLiang-Chieh Chen. Vip-deeplab: Learning visual perceptionwith depth-aware video panoptic segmentation. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 39974008, 2021. 1 Inkyu Shin, Dahun Kim, Qihang Yu, Jun Xie, Hong-SeokKim, Bradley Green, In So Kweon, Kuk-Jin Yoon, andLiang-Chieh Chen.Video-kmax:A simple unified ap-proach for online and near-online video panoptic segmen-tation. arXiv preprint arXiv:2304.04694, 2023. 1, 3 Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, andYifeng Shi.Vit-comer: Vision transformer with convolu-tional multi-scale feature interaction for dense predictions.arXiv preprint arXiv:2403.07392, 2024. 3 Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-ChiehChen. k-means mask transformer. In European Conferenceon Computer Vision, pages 288307. Springer, 2022. 1 Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang,Yuan Zhang, and Pengfei Wan.Dvis: Decoupled videoinstance segmentation framework.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 12821291, 2023. 1, 2, 3 Tao Zhang, Xingye Tian, Yikang Zhou, Shunping Ji, XueboWang, Xin Tao, Yuan Zhang, Pengfei Wan, ZhongyuanWang, and Yu Wu.Dvis++: Improved decoupled frame-work for universal video segmentation.arXiv preprintarXiv:2312.13305, 2023. 1, 3"
}