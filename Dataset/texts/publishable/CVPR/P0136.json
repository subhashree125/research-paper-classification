{
  "Abstract": "Generic Face Image Quality Assessment (GFIQA) evalu-ates the perceptual quality of facial images, which is crucialin improving image restoration algorithms and selectinghigh-quality face images for downstream tasks. We presenta novel transformer-based method for GFIQA, which isaided by two unique mechanisms.First, a Dual-SetDegradation Representation Learning (DSL) mechanismuses facial images with both synthetic and real degrada-tions to decouple degradation from content, ensuring gen-eralizability to real-world scenarios. This self-supervisedmethod learns degradation features on a global scale, pro-viding a robust alternative to conventional methods that uselocal patch information in degradation learning. Second,our transformer leverages facial landmarks to emphasizevisually salient parts of a face image in evaluating its per-ceptual quality. We also introduce a balanced and diverseComprehensive Generic Face IQA (CGFIQA-40k) datasetof 40K images carefully designed to overcome the biases, inparticular the imbalances in skin tone and gender represen-tation, in existing datasets. Extensive analysis and evalua-tion demonstrate the robustness of our method, marking asignificant improvement over prior methods.",
  ". Introduction": "In the digital era, face images hold a central role in our vi-sual experiences, necessitating a robust metric for assessingtheir perceptual quality. This metric is crucial for not onlyevaluating and improving the performance of face restora-tion algorithms but also for assuring the quality of trainingdatasets for generative models . Designing an ef-fective metric for face image quality assessment presentssignificant challenges. The inherent complexity of human",
  "(b) GFIQA-20k": ". PLCC vs. SRCC Comparison on CGFIQA-40k andGFIQA-20k datasets. DSL-FIQA, denoted by red triangu-lar points, outperforms other methods (ReIQA , StyleGAN-IQA , MANIQA , TRIQ ) and can provide a superiorimage quality assessment of facial images. faces, characterized by nuanced visual features and expres-sions, greatly impacts perceived quality . Additionally,obtaining subjective scores such as Mean Opinion Scores(MOS) is difficult due to the limited availability of licensedface images and the inherent ambiguity in subjective evalu-ations. Compounding these challenges are facial occlusionscaused by masks and accessories, which add another layerof complexity to the assessment process.Decades of research on image quality assessment (IQA)on general images , or general IQA(GIQA), has demonstrated reliable performance across var-ious generic IQA datasets . However,when such methods are applied to faces, they often overlookthe distinct features and subtleties inherent to faces, makingthem less effective for face images.Another thread of research focuses on biometric facequality assessment (BFIQA) ,where the goal is to ensure the quality of a given face im-age for robust biometric recognition. While recognizabilityis achieved by including factors unique to faces like clarity,pose, and lighting, it does not guarantee accurate assess-ment of perceptual degradation.A significant stride forward is made by , which",
  "arXiv:2406.09622v1 [cs.CV] 13 Jun 2024": "clearly defines the problem of generic face IQA (GFIQA):GFIQA focuses exclusively on the perceptual quality offace images, as opposed to BFIQA. Their approach lever-ages pre-trained generative models (StyleGAN2 ) to ex-tract latent codes from input images, which are then used asreferences for quality assessment. Although their methodshows promising prediction performance, its effectivenessreduces when input images deviate significantly in shootingangles or quality from the StyleGAN2 trainingdata, limiting its applicability and accuracy to real-worldscenarios.In this paper, we tackle the challenge of GFIQA byproposing a transformer-based method specifically de-signed to address the limitations of the aforementionedmethods. Inspired by modern GIQA techniques , wepropose a degradation extraction module that obtains degra-dation representations from input images as intermediatefeatures to aid the regression of quality scores, which ispre-trained via self-supervised learning. However, the ex-isting degradation representation learning scheme of-ten makes an oversimplified assumption that the degrada-tion is uniform across different patches of an image whilebeing distinct from those of other images. This assumptiondoes not hold for real-world data, where diverse degrada-tions within a single image exist due to variations in light-ing, motion, camera focus and so on. These inconsistenciesmay impair the effectiveness of degradation extraction andsubsequently hinder the accuracy of quality score predic-tion.To this end, we introduce a strategy termed Dual-Set Degradation Representation Learning (DSL), whichbreaks the limits of traditional patch-based learning and ex-tracts degradation representations from a global perspectivein degradation learning. This approach is enabled by es-tablishing correspondences between a controlled dataset offace images with synthetic degradations and a comprehen-sive in-the-wild dataset with realistic degradations, offeringa comprehensive framework for degradation learning. Thisdegradation representation is injected into the transformerdecoder via cross-attention , enhancing the overallsensitivity to various kinds of challenging real-world imagedegradations.Furthermore, inspired by , we utilize the strong cor-relation between facial image quality and salient facial com-ponents such as mouth and eyes. We incorporate landmarkdetection to localize and feed them as input to our model.This extra module allows our model to autonomously learnto focus on these critical facial components and understandtheir correlation with the perceptual quality of faces, whichhelps predict a regional confidence map that aggregates lo-cal quality evaluations across the entire face.Existing datasets such as GFIQA-20k and PIQ23 suffer from limited size or unbalanced distribution.To bridge this gap, we introduce the Comprehensive GenericFace IQA Dataset (CGFIQA-40k), which comprises 40Kimages with more balanced diversity in gender and skintone. We also include face images with facial occlusions.We believe this dataset will be a valuable resource to fueland inspire future research.To summarize, our contributions are as follows: We design a transformer-based method specifically de-signed for GFIQA, predicting perceptual scores forface images. We propose Dual-set Degradation RepresentationLearning (DSL), a self-supervised approach for learn-ing degradation features globally.This method ef-fectively captures global degradation representationsfrom both synthetically and naturally degraded images,enhancing the learning process of degradation charac-teristics. We enhance our models attention to salient facialcomponents by integrating facial landmark detection,enabling a holistic quality evaluation that adaptivelyaggregates local quality assessment across the face.",
  ". Quality Assessment of Face Images": "Recent work on Face Image Quality Assessment (FIQA)can be categorized into two major branches : BFIQAand GFIQA. BFIQA originates from biometric studies, fo-cusing on the quality of face images for recognition sys-tems.On the other hand, GFIQA encompasses a widerscope, concentrating on the perceptual degradation of im-age quality.Biometric Face Image Quality Assessment (BFIQA):BFIQA evaluates the quality of face images for biometricapplications such as face recognition, which often assessimages based on established standards . Recentprogress in the field has been around learning-based strate-gies, assessing quality via performance of a recognition sys-tem . Some studies have adopted manuallabeling, using a set of predefined characteristics as binaryconstraints , while others have investigated subjectiveaspects of image quality . However, adopting BFIQAmethods does not give the best performance when the em-phasis is on perceptual quality, which will be demonstratedin the results section.Generic Face Image Quality Assessment (GFIQA):GFIQA is a recently defined task , which priori-tizes perceptual degradation in face images instead. Initia-tives like Chahine et al. highlight the relevance of so-",
  "Extraction": ". Overview of our proposed model. The model contains a core GFIQA network, a degradation extraction network, and a landmarkdetection network. In our approach, face images are cropped into several patches to fit the input size requirements of the pre-trained ViTfeature extractor (See Sec. 3.1). Each patch is then processed individually, and their Mean Opinion Scores (MOS) are averaged to determinethe final quality score. For clarity in the figure, the segmentation of the image into patches is not shown. cial media-driven portrait photography, though their PIQ23dataset remains restricted in size. Su et al. bridges thisgap with the larger GFIQA-20k dataset, but it falls short indiversity, lacking in long-tail samples and balanced repre-sentation. Their generative prior-based method, while ef-fective, struggles with non-standard images, showing limi-tations of StyleGAN2-dependent models . This demon-strates the necessity for a more comprehensive dataset androbust GFIQA solution.",
  ". General Image Quality Assessment": "Traditional General Image Quality Assessment (GIQA) methods like BRISQUE , NIQE , and DI-IVINE are built upon traditional statistical models,which work decently on datasets with constrained size buthave faced limitations with complex real-world images.The advent of deep learning has given rise to groundbreak-ing GIQA methods.RAPIQUE and DB-CNN set new standards in adaptability and accuracy. A furtherinnovation was seen in transformer-based models, includ-ing MUSIQ and MANIQA , with significantly im-proved prediction precision. The domain was expanded byCONTRIQUEs self-supervised paradigm and Zhanget al.s vision-language multitask learning.Saha etal. uniquely integrated low and high-level features inan unsupervised manner, emphasizing perceptually relevantquality features.However, the applicability of these advancements to faceimages is debatable, as they often overlook facial features",
  ". Model Overview": "illustrates our method: Given an input image I RHW 3, our model estimates its perceptual quality score.In the following, we briefly summarize its components.Feature Extraction and Refinement: The image initiallyundergoes feature extraction via a pre-trained VisionTransformer (ViT) , followed by a Channel AttentionBlock that emphasizes relevant inter-channel depen-dencies. Subsequently, a Swin Transformer Block re-fines these features, capturing subtle image details.Degradation Extraction: In parallel, a dedicated moduleidentifies and isolates perceptual degradations within theimage, providing a nuanced representation of image qual-ity degradations.Feature Integration and Quality Estimation: The degra-dation features, once extracted, are integrated with the out-puts from the Swin Transformer within a transformer de-coder. This integration employs cross-attention, a techniqueinspired by Stable Diffusion , to enhance the modelssensitivity to degradation. The combined features are thendirected into two MLP branches. The first branch predictsthe regional confidence, while the second estimates the re-gional quality score. Finally, these outputs are combined",
  "m": ". Dual-Set Degradation Representation Learning (DSL) Illustrated. On the left, the process of contrastive optimization isdepicted, utilizing two unique image sets. Degradation representations are extracted, followed by soft proximity mapping (SPM) calcula-tions and contrastive optimization, compelling the degradation encoder to focus on learning specific degradation features. The right sideemphasizes the bidirectional characteristic of our approach, highlighting the comprehensive strategy for identifying and understandingimage degradations through contrastive learning. through a weighted sum to determine the overall qualityscore of the image.Landmark-Guided Mechanism: A landmark detectionnetwork identifies facial key points, influencing the regionalconfidence evaluation and ensuring that essential facial fea-tures improve the final quality score.During the training of the core GFIQA network, thelandmark and degradation modules remain fixed, leverag-ing their pre-trained knowledge. Notably, we avoid resizinginput images to fit the fixed input dimensions of the pre-trained ViT, which could distort quality predictions . In-stead, we crop the image, process each part independently,and average the resulting MOS predictions for a consoli-dated image quality score.This approach maintains theoriginal dimensions of the image and, consequently, the cor-rectness of perceptual quality assessment.In the following subsections, we highlight the main tech-nical contribution of our model design: our degradation ex-traction module and landmark-guided mechanism.",
  "Patch-based Degradation Learning": "Existing degradation extraction methods () as-sume that patches from the same image share similar degra-dation for contrastive learning. In this framework, patchesextracted from the same image are positive samples, whilethose from different images are negative samples.The patches are encoded into degradation representations (x,x+, and x) for the query, positive, and negative samples.The contrastive loss function is designed to enhance thesimilarity between x and x+ and dissimilarity between xand x, which is given by:",
  "LP atch(x, x+, x) = logexp(xx+/)Nn=1 exp(xxn /),(1)": "where N is the number of negative samples and is a tem-perature hyper-parameter.However, the assumption of uniform degradation acrossthe image does not always hold due to lighting, local mo-tion, defocus, and other factors. For example, it is possibleto have a moving face with a static background in an im-age, which means that only some patches suffer from mo-tion blur. This oversimplified assumption often leads to sub-optimal and inconsistent results for degradation learning.",
  "Our Solution": "To bridge this gap, we propose Dual-Set Degradation Rep-resentation Learning (DSL), which considers entire face im-ages. To make this challenging setting compatible with con-trastive learning approaches, we carefully construct two setsof images, S and R, each serving a unique purpose in thedegradation learning process, as shown in .Set S consists of a collection of images derived from asingle high-quality face image, with each image undergoingdifferent types of Synthetic degradation including but notlimited to blurring, noise, resizing, JPEG compression, andextreme lighting conditions. This set acts as a controlled en-vironment, enabling in-depth exploration of a wide varietyof degradations against constant content. In contrast, Set R encompasses a compilation of imagesfrom GFIQA datasets, each having different content underReal-world degradation. This set reflects the unpredictabil-ity and diversity of realistic degradations, which are hard tomodel by synthetic data.Formally, let S = {s1, . . . , sm} and R = {r1, . . . , rn},where m and n represent the number of images in S and R,respectively. Each image from the two sets is mapped to itsdegradation representation by a function defined by thedegradation extraction module with weights z:",
  "(si) = nj=1 sim((si), (rj)) (rj)(4)": "where (si) denotes soft proximity mapping of (si).sim(, ) denotes the similarity between two representations.We use L2 distance as our similarity metric in our imple-mentation. z is omitted for brevity.This construction allows us to define positive and neg-ative pairs for contrastive learning. Intuitively, a degrada-tion representation (si) should be attracted to its own softproximity mapping (si), while any other representations(sj) where j = i should be repelled from this soft proxim-ity mapping because si and sj have different degradationsby the dedicated construction of Set S. Then, we adopt thecontrastive loss :",
  "mmi=1 logexp(sim((si), (si))/)nj=i exp(sim((sj), (si))/) (5)": "This loss function leverages the nature that within S, im-ages share the same content but differ in degradations, con-trasting with R, which varies in both aspects. By drawingthe extracted degradation representation closer to its cor-responding soft proximity mapping and distancing it fromother soft proximity mappings, the degradation extractionmodule is trained to learn a global degradation representa-tion that is independent of the image content.Furthermore, the self-supervised dual-set contrastivelearning strategy is essential for understanding variousdegradations, particularly in real-world scenarios. This ap-proach is vital as it involves accurately extracting degra-dation representations from real-world images to approxi-mate those in the synthetic set S. It is feasible to employcontrastive learning solely on the synthetic set S to capturedegradation patterns: Positive pairs consist of images with the same degradation, and negative pairs otherwise. How-ever, this naive approach does not generalize well to real-world images. In contrast, our dual-set design can bring to-gether the benefits of both the synthetic set with controllabledegradations and the real-world set with realistic degrada-tions, achieving better generalization.Notice that the roles of S and R are symmetric:Just as we utilize representations from S to seek corre-sponding features within R, empirically, we found the re-verse is also viable and informative. Thus, we define ourDegradation Extraction Loss LDE as a bidirectional loss:",
  "LDE = LCon(S, R) + LCon(R, S)(6)": "This bidirectional loss reinforces the mutual learning andalignment between the synthetic and real-world sets, en-suring a comprehensive understanding and representationof realistic degradations. Moreover, it is worth mentioningthat the high-quality image in set S is resampled for ev-ery iteration, where this image undergoes random syntheticdegradations of varying intensities. Concurrently, images inset R are also resampled randomly in each iteration.In summary, DSL gets rid of the uniformity assumptionof degradation in patches across the entire image for degra-dation learning. Instead, it relies on the soft proximity map-ping between two constructed sets of images to calculate thecontrastive loss, which allows for more precise degradationrepresentation (this mechanism is kind of similar in spiritto ). Furthermore, since the entire image is considered,DSL can capture a holistic view of the degradation uniqueto each image, further boosting the performance.",
  ". Landmark-guided GFIQA": "Face images are uniquely challenging in image processing.This is because human eyes are especially sensitive to facialartifacts, raising the importance of nuanced quality assess-ment . Thus, it is important to design an approach thatdoes not treat each pixel equally; it should acknowledge theperceptual significance of salient facial features. Further-more, as stated in Sec. 3.1, considering that our networkcrops the face into various patches to compute the averageMOS score, it is crucial to provide landmark information togive the spatial context on which part of the face each patchcovers, ensuring a holistic and perceptually consistent eval-uation.As shown in , our approach begins with utiliz-ing an existing landmark detection algorithm (i.e., 3DMMmodel ) to identify key facial landmarks. Inspired byNeural Radiance Fields (NeRF) , we apply positionalencoding to these unique landmark identifiers. By apply-ing a series of sinusoidal functions to the raw identifiers,positional encoding enhances the representational capacityof the network, allowing the network to capture and learn more intricate relationships and patterns associated witheach landmark identifier.The encoded information is subsequently concatenatedwith the features processed by the Transformer Decoder,feeding into the regional confidence branch. The humanvisual system is particularly sensitive to high-frequency de-tails, which are often associated with facial landmarks suchas the eyes, nose, and mouth.Providing this landmark-based information to the confidence head can help generatea more precise confidence map, emphasizing regions thathumans naturally prioritize in their perception.In our approach, we deliberately avoid relying on encod-ing landmark coordinates (x, y) in an image as positions, asit can introduce ambiguity during learning, especially whenfaces are unaligned, or images are cropped into patches.In such scenarios, specific coordinates may inconsistentlycorrespond to different facial features on different trainingsamples, therefore muddling the learning process. To avoidthis, our network employs a fixed encoding scheme for eachfacial landmark, assigning a unique identifier to every crit-ical feature regardless of its position in the image. Thismethodology proves particularly advantageous for our ViT,which takes fixed-size crops from the input image, poten-tially capturing only portions of the face.Given the diverse range of degradations encountered inGFIQA, off-the-shelf landmark detectors often fail on im-ages with challenging degradations. We observed that fine-tuning existing landmark detectors like on de-graded images leads to more accurate landmark detection.In summary, by adopting landmark-guided cues, ourmethod maintains a consistent awareness of crucial facialfeatures within each crop, which effectively encourages themodel to focus on salient facial features when aggregatingthe regional quality scores.",
  "(p p)2 + 2(7)": "where p is the predicted MOS, p is the ground truth MOS,and is a small constant to ensure differentiability.Unlike existing GIQA or GFIQA mod-els that typically rely on L2 losses, we opt for the Char-bonnier loss as it is less sensitive to outliers, which in thecontext of GFIQA can arise from rare face quality degra-dations, dataset annotation discrepancies, or occasional ex-treme scores predicted by the model during training. By im-",
  ". Comprehensive Generic Face IQA Dataset": "Existing GFIQA models are evaluated on datasets such asPIQ23 and GFIQA-20k . While PIQ23 contains avariety of in-the-wild images with uncropped faces, its con-strained dataset size limits its efficacy for training robustmodels. Moreover, both datasets exhibit biases in genderand skin tone representation. This disproportion can intro-duce biases during model training, decreasing the perfor-mance and reliability of models in face image quality as-sessment tasks. Prior research has shownthat this imbalance in data distribution has a significant neg-ative impact on model performance in various face-relatedapplications.To tackle these challenges, we introduce a new datasetnamed Comprehensive Generic Face Image Quality Assess-ment (CGFIQA-40k), which includes approximately 40Kimages, each with a resolution of 512x512. Each imageis annotated by 20 labelers, and each labeler spends about30 seconds to give a score. From an initial pool of 40,000images, we filtered out a small number of images with un-usable content or incomplete labels, resulting in a total of39,312 valid images. This dataset is specifically curated toinclude an extensive collection of face images with diversedistribution on skin tone, gender, and facial occlusions suchas masks and accessories.A comparative overview of our dataset with existingGFIQA datasets is provided in . We hope this datasetwill offer a more comprehensive benchmark for GFIQA,pushing the generalization and robustness of state-of-the-artmethods.",
  "--------0.96820.9679-------0.96950.9687-------0.97030.9701-------0.97130.9711-------0.97090.9707-------0.97210.9719------0.97310.9728-----0.97350.9731----0.97450.9740": ". Ablation study on proposed techniques. We demon-strate that the proposed modules can effectively improve GFIQAsperformance.PE denotes the positional encoding operation.DSL-S and DSL-R represent Dual-Set Degradation Represen-tation Learning. In this framework, the degradation encoder istrained using a one-sided loss approach, specifically LCon(S, R)for DSL-S and LCon(R, S) for DSL-R. DSL-cat refers todirectly concatenating the degradation representations with im-age features instead of cross-attention, while DSL indicates theadoption of a dual-sided loss function as defined in Eq. (6). 512 for validation, and 1,023 for testing. Our CGFIQA-40k dataset includes a more extensive collection of 39312images, with a division of 27518 for training, 3931 for val-idation, and 7863 for testing.The evaluation metrics employed are the Pearson LinearCorrelation Coefficient (PLCC) and Spearmans Rank Or-der Correlation Coefficient (SRCC).",
  ". Comparative retrieval accuracy using DSL and patch-based methods under real-world degradations, quantified bymAP scores": "maintain a fair comparison, all models were trained and val-idated under the identical conditions specified in Sec. 5.1.We compare with a wide range of generic IQA mod-els, including Koncept512 , MUSIQ , ReIQA ,CONTRIQUE ,UNIQUE ,MANIQA ,TReS , HyperIQA , LIQE , MetaIQA ,TRIQ , VCRNet , and GraphIQA .Wealso compare with recent GFIQA methods, includingStyleGAN-IQA and IFQA . For completeness, weinclude three representative BFIQA methods, ArcFace ,MegaFace , and CR-FIQA . clearly shows the robust performance of ourmethod, which outperforms existing models across all met-rics on GFIQA-20k, PIQ23, and CGFIQA-40k. The consis-tent results across diverse datasets validate our approachsstrength and adaptability, establishing it as a robust genericface image quality assessment solution.",
  "(c) w landmark": ". Comparison of using landmark mechanism to guidethe GFIQA network. We present the regional confidence mapsand the corresponding input. With landmark guidance, the confi-dence maps focus more on key facial landmarks, providing a morediscriminative assessment. In contrast, without landmark guid-ance, the confidence maps tend to cover the entire face, often lack-ing specificity and even assigning higher confidence to irrelevantareas (e.g., background). and Row 6), highlighting its critical role in boosting GFIQAaccuracy. We also substituted the DSL learning techniquefor the existing patch-based degradation learning in (Row 2 and Row 6), and the results indicate that the pro-posed DSL can improve GFIQA performance more than thepatch-based strategy (Sec. 3.2.1). We also validated the sig-nificance of cross-attention for integrating degradation in-formation (Row 3 and Row 6). The results indicated thatemploying cross-attention for this integration yielded supe-rior outcomes. Comparison between DSL and patch-based methods. Tofurther evaluate the advantage of our DSL over patch-baseddegradation extraction methods, we conducted two addi-tional experiments. Both methods were trained on the samedataset for fair and direct comparison. The testing data usedwas completely independent of the training set, guarantee-ing the validity of our assessment.The first experiment involved synthetic data.Werandomly selected 1,000 face images from the FFHQdataset , subjecting each to six types of synthetic degra-dations. We then employed DSL and patch-based featureextraction, subsequently visualizing these features usingt-SNE. The results, illustrated in , demonstratedthat DSL could effectively separate the images based ontheir specific degradations, while the patch-based methodshowed considerable overlap. Furthermore, we extended our exploration to real-worldconditions, using images from the GFIQA-20k dataset.This second experiment was designed to verify if the dis-tinct degradation representations learned by DSL couldenhance image degradation retrieval accuracy under real-world degradations. To this end, we synthesized six types ofdegradations on 100 images from the FFHQ dataset. Thesesynthetically degraded images were used as queries to probethe GFIQA-20k test set, selecting the top 5 images with thesmallest distance. We then verify whether the five imagesfall under the same degradation category by human inspec-tion. We quantified our methods precision by calculatingthe mean average precision (mAP) for these retrieval tasks,as shown in . The results confirmed DSLs enhancedaccuracy in identifying images with similar degradation at-tributes.In conclusion, these experiments emphasized the effec-tiveness of DSL in crafting distinct degradation represen-tations and its practical superiority in real-world scenarios,bolstering its value in improving GFIQA outcomes. Effectiveness of Landmark-guided GFIQA. Integratingfacial landmarks into GFIQA significantly improves qual-ity assessment accuracy, addressing the complexity of facialfeatures often ignored in traditional methods, which is vali-dated in (Row 6 and 7). To understand how the land-mark guidance works, visualizes the regional con-fidences predicted with and without landmarks guidance.When landmarks are not used, the model indiscriminatelyoveremphasizes the entire face and even background areas.In contrast, the model with landmark guidance focuses oncrucial facial regions, which are more aligned with humanperception. In addition, (Row 7 and 8) substanti-ate the benefit of applying positional encoding to the land-mark identifiers, showing that positional encoding can in-deed enhance the model capacity to capture more complexrelationships inherent in facial features, thereby improvingthe overall prediction accuracy.",
  ". Conclusion": "In this paper, we tackle the inherent complexities in GFIQAwith a transformer-based method.Our Dual-Set Degra-dation Representation Learning improves degradation ex-traction, and the additional guidance from facial land-marks further improves the assessment accuracy. Further-more, we curate the CGFIQA-40k Dataset, rectifying im-balances in skin tones and gender ratios prevalent in previ-ous datasets. Extensive experimental results show that theproposed method performs favorably against state-of-the-art methods across several GFIQA datasets.",
  "A. Social Impact and Ethical Considerations": "This paper contributes to the advancement of Face IQAtechnology, which has widespread applications in digitalmedia and social networking platforms. By ensuring a bal-anced representation of gender and skin tones in the dataset,this paper addresses critical issues of fairness and bias in AI,promoting more equitable facial analysis technologies.However, if the facial image quality assessment (IQA)method fails, it could lead to the selection of incorrect fa-cial quality images for training, subsequently affecting theaccuracy of facial-related algorithms trained on these im-ages. This situation might result in biases or errors in facialrecognition, emotion analysis, or other applications basedon facial images.To address this issue, an effective approach is to double-check the images filtered through the Face IQA methodto ensure their quality meets the expected standards. Thiscan be achieved through manual review or by employ-ing additional verification mechanisms.Such a double-checking mechanism helps reduce the risk of erroneouslyselected images, ensuring the quality of training data,thereby enhancing the reliability and effectiveness of algo-rithms trained on these data.",
  "B.1. Data Collection": "To create a diverse and comprehensive dataset, we initiallycollected face images from the CelebA dataset. We utilizedskin tone and gender detectors to analyze these im-ages, ensuring a balanced representation of both gender andskin tones. This careful sampling approach was comple-mented by the addition of 1,028 images from Flickr, specif-ically chosen to enhance the diversity in terms of skin tonesand occlusion.The combined dataset consists of about40,000 images. Each image was aligned using Dlibs facelandmark detection according to FFHQ dataset and subsequently rescaled to a uniform resolution of512 512 pixels, ensuring consistency across the dataset.",
  "B.2. Annotation Procedure": "To ensure accurate and consistent subjective quality assess-ment of facial images, we provided annotators with a user-friendly and intuitive interface. This interface was designedto display one facial image at a time, accompanied by aninput field for annotators to enter their Mean Opinion Score(MOS) for the image. To assist annotators in making accu-rate judgments, we included example images for each qual-ity level alongside the interface. These examples served asreferences, aiding annotators in better discerning and as-sessing the quality of each image. Additionally, our system supports arbitrary zoom-in and out features for each image,allowing annotators to better assess the details. An illus-tration of the user interface used in our study is shown inFigure S.1.For the subjective scoring process, we adopted the stan-dard 5-interval Absolute Category Rating (ACR) scale,comprising levels: Bad, Poor, Fair, Good, and Excellent.This scale was linearly mapped to a range of [0, 1.0], corre-sponding to the ACR scale as follows: Bad at 0-20%, Poorat 20-40%, Fair at 40-60%, Good at 60-80%, and Excellentat 80-100%.To elevate the precision and uniformity of the evalua-tions, we crafted detailed guidelines alongside a collectionof definitive gold-standard principles. These encompassedseveral facets of image analysis, such as the visibility ofeyelashes, articulated through specific classification tiers:",
  "Excellent:No visible artifacts, whether viewed asthumbnails or in original size": "Good: Artifacts discernible solely at original size. Fair: Minor artifacts noticeable in thumbnail views.Reference images from the GFIQA-20k dataset were instru-mental in guiding the annotators.Additionally, our guidance provides a structure for us-ing midpoint scores when an image does not clearly fit intoa single category. For instance, if an image falls betweenthe Poor and Fair categories, a midpoint score of 40 isrecommended.We curated a collection of 35 images carefully selectedby experts, where each of the five quality intervals is repre-sented by seven images. Three images from each level wereused as golden samples, which were provided to guide eachannotator along with the rating guidelines. Additionally, weconducted a pre-annotation training using the remaining 20images, with four images from each quality level (It is un-known to the annotators that they were evenly distributed).Annotators were required to achieve an accuracy of at least80% in this test to complete their training. To clarify, anannotators assessment was considered correct if their as-signed Mean Opinion Score (MOS) was within a marginof 15 points from the ground truth MOS score. If thiscriterion was not met, they were asked to revisit the guide-lines and 15 example images and then retake the test untilthey reached the accuracy threshold. Importantly, annota-tors were not informed of the correct answers to the testquestions throughout the process.In total, we engaged 20 annotators for this study. Onaverage, each annotator spent approximately 30 seconds as-sessing the quality of each image. This arrangement en-sured both the ratings efficiency, quality, and consistency.These detailed guidelines and scoring mechanisms ensuredthat participants could accurately and consistently assessimage quality, thereby enhancing our datasets overall qual-ity and reliability.",
  "dataset represents a comprehensive collection, covering abroad spectrum of image quality with MOS values rangingfrom 0 to 1": "The CGFIQA-40k dataset is specifically curated to fo-cus on facial images, showcasing various visual qualities,including several images with occlusions. As illustrated inFigure S.2, these occluded images are integral to the dataset,contributing to its diversity and providing edge cases for ro-bust model training. We have included image samples fromdifferent categories - Excellent, Good, Fair, Poor, and Badto demonstrate the overall diversity. From each category,as shown in Figure S.3, six images have been carefully se-lected to represent the range of qualities within that cate-gory. These images and their respective MOS values aredisplayed in the accompanying figures, illustrating the per-ceptual quality differences across categories. Furthermore, we present a histogram of the MOS distri-bution in Figure S.4 for the entire dataset. This histogramprovides a clear overview of the quality distribution of theimages, highlighting the frequency and range of differentquality levels within the dataset.",
  "C.1. Evaluation Criteria": "In our evaluation, we use two well-established metrics toassess the performance of our model: Spearmans Rank-Order Correlation Coefficient (SRCC) and Pearsons LinearCorrelation Coefficient (PLCC).PLCC measures the linear correlation between actualand predicted quality scores, indicating how closely the pre-dictions align with real values on a linear scale. It is sensi-tive to numerical differences between scores. SRCC, in contrast, evaluates the monotonic relationshipbetween two datasets. It focuses on rank order rather thannumerical values, offering robustness against outliers andskewed distributions.Both metrics range from -1 to 1,where 1 signifies perfect correlation, -1 indicates perfect in-verse correlation, and 0 means no linear correlation. Higherabsolute values indicate better performance, with positivevalues showing consistency with the ground truth.For the PLCC, given si and si as the actual and predictedquality scores for the i-th image, and si and si as theirmeans, with N as the number of test images, it is defined 0.00.20.40.60.81.0 MOS",
  "C.2. Training Details": "Degradation Encoder. The Degradation Encoder is tai-lored to extract and encode degradation features inherentin the input face images. Our architecture employs a CNNcomprising six 3 3 convolution blocks. Each block in-corporates batch normalization and is succeeded by a leakyReLU activation. After feature extraction, these featuresare processed through a two-layer MLP to produce the finaldegradation representation vector. We use the Adam opti-mizer with a learning rate of 3 105 across 300 epochsfor training. Our training data is divided into two distinctsets.The first set, labeled as Set S, consists of m im-ages, as mentioned in .2 of the main paper. Theseare derived from 5000 high-quality images from the FFHQdataset , resized to 512 512.The images in thisset are subjected to 15 different synthesized degradations,while one image remains undegraded, resulting in a total of16 images (i.e., m = 16). The synthesized degradations encompass a variety of conditions such as low-light, high-light, blur, defocus, 2x downsample, Gaussian noise, Gaus-sian blur with kernel sizes from 3 to 31, JPEG compressionquality ranging from 1 to 30, motion blur, sun flare, ISOnoise, shadow, and zoom blur. The low-light and high-lightdegradations are implemented using the torchvision library,whereas the other degradations are applied using albumen-tations . The second set, designated as Set R, includesn images, amounting to 256 as specified in .2.2 ofthe main paper. This set is dynamically curated by select-ing from the GFIQA-20k dataset, ensuring that each subsetof 256 images contains at least one high-quality face imagewith a Mean Opinion Score (MOS) greater than 0.9. Thetemperature parameter is 1.0. Notably, both sets undergoresampling in each iteration to ensure a diverse training ex-perience. This module comprises a total of 1.27 106 pa-rameters. The training process was conducted on a singleNVIDIA A100 GPU, equipped with 80GB of memory, us-ing the PyTorch framework. The entire training was com-pleted in roughly 12 hours. Landmark Detection Network. We used a commercialimplementation of which outputs 1313 landmarks byfitting the 3DMM model on the initially detected 68landmarks. We have observed that the original face land-mark detection algorithm does not perform well on low-quality images. However, when fine-tuned specifically forlow-quality images, it significantly improves performance,as shown in Figure S.5. These low-quality images are syn-thesized based on the image degradation model on thecurrent landmark detection dataset. GFIQA Network. The GFIQA Network, informed by thefeatures extracted by the Degradation Encoder, endeavorsto predict the Mean Opinion Score (MOS) for input faceimages. Our network architecture combines a hybrid CNN-Transformer backbone, comprising a VGG-19 model pre-trained on ImageNet , and a Vision Transformer (ViT)backbone , also pre-trained on ImageNet. This setup isfurther enhanced with two Swin Transformer blocks ,a channel attention layer , a transformer decoder, andtwo MLP regression layers. The ViT backbone, tailoredfor an input size of 384 384, processes the image by di-viding it into multiple 16 16 pixel patches, ensuring de-tailed and comprehensive image analysis. During training,we employ a batch size of 16, and all input images undergorandom cropping from 512512 to 384384. Additionally,data augmentation in the form of random horizontal flippingis applied to enhance the models generalization capability.The learning rate is set at 105 across 100 epochs, and weuse the Adam optimizer. The in Lchar is 103. The mod-ule consists of 2.51 108 parameters in total. The networkwas trained on an Nvidia A100 GPU, which has 80GB ofmemory, using the PyTorch framework. The entire training",
  "Fine-tunedlandmarks": "Figure S.5. Evaluating the Impact of Fine-Tuning on Land-mark Detection in Poor-Quality Images. The fine-tuned land-mark detection algorithm can handle low-quality inputs (first col-umn), as demonstrated in the third column of results. In contrast,the unfine-tuned algorithm has large errors, as evidenced in thesecond column (highlighted by the red crosses).The detectedlandmarks have been overlaid on the high-quality version of theinput for better visualization. The basic 68 landmarks are repre-sented by green dots, while the expanded set of 1313 landmarks isdenoted by small red dots.",
  "process was completed within 20 hours": "Clarification. To clarify, in our system, both the degra-dation extraction network and the landmark detection net-work process the entire image (512 512 pixels) to predictlandmarks and extract degradation representations. How-ever, for the GFIQA network, we adapt to the input sizerequirements of the pre-trained Vision Transformer (ViT),which is 384 384 pixels in our implementation. To ac-commodate this, we crop the facial image into several over-lapping 384 384 patches, each serving as an individualinput for the ViT. This ensures that the total coverage areaof all patches encompasses the original input image.",
  "GFIQA-20k/StyleGAN-IQA0.33230.3421CGFIQA-40k/StyleGAN-IQA0.35410.3643GFIQA-20k/Ours0.39470.4165CGFIQA-40k/Ours0.42290.4653": "Table S.1. Performance Comparison of Zero-shot GFIQA onPIQ23 Dataset. This table compares the effectiveness of mod-els trained on CGFIQA-40k and GFIQA-20k datasets. The resultshighlight the superior performance of models using CGFIQA-40k,underscoring its larger scale and balanced diversity in gender andskin tones. In the main paper, particularly in , we simplifiedthe explanation by omitting the step of cropping the facialimage into multiple patches. Moreover, the images outlinedin red in the GFIQA Network section are intended to illus-trate how the ViT divides the input image (384 384) intoseveral patches for feature extraction between patches.",
  "D.1. Cross-Dataset Validation": "To explore the quality attributes of facial data, we conductedan experiment using our newly proposed CGFIQA-40kdataset and the existing GFIQA-20k dataset to trainmodels. In this experiment, we employed the StyleGAN-IQA model and our method for training. The effec-tiveness of these models was then verified on the PIQ23dataset , a benchmark for unseen face image quality as-sessment.As shown in Table S.1, we observed that models trainedon our datasets, particularly the CGFIQA-40k, demon-strated superior performance on the PIQ23 dataset1, an un-seen face image quality dataset.This enhanced perfor-mance can be attributed to several key factors. Firstly, theCGFIQA-40k dataset is extensive in scale, encompassinga wide range of image qualities and scenarios. Secondly,and crucially, it offers a more balanced representation interms of gender and skin tone compared to the GFIQA-20kdataset. This balanced representation ensures a more com-prehensive and unbiased training process, leading to modelsthat are better equipped to handle a diverse array of facialimages in real-world applications. The results clearly high-light the advantages of our dataset, underscoring its poten-tial in advancing the field of facial image quality assess-ment.",
  "Table S.3.Impact of Channel Attention on Model Perfor-mance": "refer to as the Naive method, involves training a modelexclusively on the synthetic set (Set S). In this method,positive pairs are formed from images with identical syn-thetic degradations, while negative pairs are composed ofimages with different degradations. This approach, how-ever, showed limitations in generalizing to real-world im-ages due to its sole reliance on synthetic degradations.In contrast, our dual-set model integrates both synthetic(Set S) and real-world (Set R) degradations. This model istrained to recognize and adapt to a broader range of degra-dation patterns, encompassing both controlled synthetic andnaturally occurring real-world degradations. As a result, itdemonstrated superior generalization capabilities, particu-larly in diverse real-world scenarios. The comparative per-formance of these two approaches is detailed in Table S.2,highlighting the significant advantage of our dual-set ap-proach in achieving more effective generalization in extract-ing degradation representation.Effectiveness of Channel Attention.By integrating achannel attention block, our method achieves a more pre-cise feature focus, enhancing face quality assessment. Thisimprovement leverages the well-documented advantages ofattention mechanisms within the domain of image analysis,effectively emphasizing crucial channels. The comparativeresults, demonstrating the impact of incorporating channelattention into our approach, are detailed in Table S.3.Effectiveness of Landmark Guidance. We examine theimpact of landmark guidance by conducting an experimentin which we omit the landmark detection component fromDSL-FIQA. We then assess the performance on the GFIQA-20k and CGFIQA-40k datasets, with the results de-tailed in Table S.4. This evaluation demonstrates that in-corporating landmark guidance improves the effectivenessof our method.",
  "Table S.4. Impact of Landmark Guidance on Model Perfor-mance": "retically, it is possible for two or more images in set R tohave identical degradation representations.However, it is important to note that the likelihood of thisoccurrence is extremely low due to the complex and vari-able nature of image degradation in real-world scenarios.In practice, even degradations that appear visually similarcan have distinct characteristics influenced by various fac-tors such as environmental conditions, lighting, and camerasettings. Therefore, while the theoretical possibility of iden-tical degradation representations in two images exists, it ispractically negligible.Additionally, we examine the t-SNE results presented in of the main paper. Initially, we observe that Gaus-sian noise, which is random and impacts the entire image,fundamentally contrasts with blurs and compressions thatspecifically affect image structure. This distinction likelycauses Gaussian noise to appear separate from other degra-dations in t-SNE visualizations. Furthermore, JPEG com-pression and low resolution both lead to a loss of imagedetail, with the former eliminating high-frequency informa-tion and the latter decreasing the pixel count. This common-ality in their impact on image clarity might result in similarpatterns within the t-SNE visualizations.",
  "Jiankang Deng, Jia Guo, Niannan Xue, and StefanosZafeiriou. Arcface: Additive angular margin loss for deepface recognition. In CVPR, 2019. 7": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 2, 3, 12 Bernhard Egger, William AP Smith, Ayush Tewari, StefanieWuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard,Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al.3d morphable face modelspast, present, and future. ACMToG, 2020. 5, 12",
  "Simen Sun, Tao Yu, Jiahua Xu, Wei Zhou, and ZhiboChen. Graphiqa: Learning distortion graph representationsfor blind image quality assessment. TMM, 2022. 7": "Philipp Terhorst, Jan Niklas Kolf, Naser Damer, FlorianKirchbuchner, and Arjan Kuijper. Ser-fiq: Unsupervised esti-mation of face image quality based on stochastic embeddingrobustness. In CVPR, 2020. 2 Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck,Balu Adsumilli, and Alan C Bovik.Rapique: Rapid andaccurate video quality prediction of user generated content.IEEE Open Journal of Signal Processing, 2021. 1, 3 Tao Wang, Kaihao Zhang, Xuanxi Chen, Wenhan Luo,Jiankang Deng, Tong Lu, Xiaochun Cao, Wei Liu, Hong-dong Li, and Stefanos Zafeiriou.A survey of deep facerestoration: Denoise, super-resolution, deblur, artifact re-moval. arXiv preprint arXiv:2211.02831, 2022. 2, 5"
}