{
  "Abstract": "This report presents the ECO (Ensembled Clip score andcOnsensus score) pipeline from team DSBA LAB, which isa new framework used to evaluate and rank captions fora given image. ECO selects the most accurate caption de-scribing image. It is made possible by combining an Ensem-bled CLIP score, which considers the semantic alignmentbetween the image and captions, with a Consensus scorethat accounts for the essentialness of the captions. Usingthis framework, we achieved notable success in the CVPR2024 Workshop Challenge on Caption Re-ranking Evalu-ation at the New Frontiers for Zero-Shot Image Caption-ing Evaluation (NICE). Specifically, we secured third placebased on the CIDEr metric, second in both the SPICE andMETEOR metrics, and first in the ROUGE-L and all BLEUScore metrics. The code and configuration for the ECOframework are available at .",
  ". Introduction": "The NICE 2024 Challenge Caption Re-ranking track is acompetition that challenges participants to identify the mostaccurate and comprehensive caption from a set of candidatecaptions for a given image. The goal is to select caption thataccurately and thoroughly describe an image. The NICEdataset provided for this challenge comprises 20,000 im-ages, and about 60 captions for each image, forming a zero-shot evaluation dataset. It includes various images, can-didate captions generated by different models, and undis-closed answer captions created by human annotators.Participants in the Image Caption Re-ranking taskshould choose and submit the caption they consider mostappropriate for each image. Their submissions are evalu-ated against five undisclosed correct captions written by dif-ferent human annotators, based on five metrics: CIDEr ,SPICE , METEOR, ROGUE-L, and BLEU.The aim is to encourage innovative approaches to select-ing captions that enhance the accuracy and depth of imagedescriptions.Upon first glance, one may assume this task is similar tothe image-to-text retrieval task. However, there is a distinc-tion as the Retrieval Task involves selecting the clear an-",
  "arXiv:2405.01028v2 [cs.CV] 13 Jun 2024": "swer caption among multiple candidate captions, some ofwhich may significantly deviate from the answer. This taskis evaluated using recall metrics such as recall@1, 5, and 10. On the other hand, Image Caption Re-ranking task lit-erally focuses on re-ranking candidate captions, which areclosely aligned with the correct answers. These differencescall for evaluating captions in more detail.To develop an algorithm capable of re-ranking captionsbased on the quality of their descriptions of images, it wasimportant first to establish a clear definition of what consti-tutes an accurate and thorough caption. To achieve this,we determined that an ideal caption must meet two key cri-teria:",
  ". An ideal caption should have a high semanticalignment with the associated image.2. An ideal caption should have a high degree ofessentialness": "The first criterion for a caption is that it should accuratelyreflect the context of the image and not include any con-tent that is not present in the image. In other words, thecaption should be semantically aligned with the image, andthe more alignment there is, the better it will meet the firstcriterion.The second criterion requires that the caption avoids us-ing overly elaborate language and instead focuses on usingessential expressions. This means that the caption shouldonly include expressions that are necessary to describe theimage. The more indispensable each expression is for ac-curately depicting the image, the better it meets the secondcriterion.It is important to note that meeting only one of the crite-ria does not guarantee the caption is ideal. A caption withhigh semantic alignment might still have non-essential el-ements, while focusing solely on essential elements mightnot adequately represent the image. To address this issue,we propose the ECO framework, which uses scoring meth-ods to evaluate both the degree of semantic alignment be-tween the image and caption, and how essential the terms inthe captions are.To determine how well captions match images, we usedvarious pre-trained CLIP models and BLIP-2 modelto calculate the cosine similarity between image and textfeatures. We then created a robust Ensembled CLIP scoreby combining the results. To measure how essential theterms in the captions are, we used a Consensus score de-rived from comparing candidates within the pool of cap-tions. Finally, we combined the Ensembled CLIP score andConsensus score to calculate the final score. If the differ-ence between the top two captions for an image is negligi-ble, both captions are considered equally good at describingthe image. In this case, we choose the caption with fewerwords as the final caption. The proposed ECO framework is a method for captionevaluation that is easy to understand and doesnt require anyadditional fine-tuning.It can take into account both thealignment of images and text, as well as the essentialnessof captions in a zero-shot setting. The overall frameworkcan be seen in . By using this approach, we were ableto achieve impressive results in the NICE 2024 Challenge.We came in third place based on the CIDEr metric, secondplace in both SPICE and METEOR metrics, and first placein the ROUGE-L and BLEU metrics. These achievementsshow that our method is not limited to excelling in a singlemetric, but is versatile and can be applied to various evalu-ation criteria.",
  ". Proposed Method": "Building on the concept of a well-explained image cap-tion defined in Sec. 1, we introduce ECO, a frameworkdesigned to select the ideal caption by considering both thesemantic alignment between images and captions and theessentialness of captions. ECO comprises two main scor-ing algorithms: 1) the Ensembled CLIP score and 2) theConsensus score. In Sec. 2.1 discusses the integrated CLIPscore derived from the cosine similarity between image andcaption embeddings, utilizing a variety of CLIP models andthe ITC Loss calculation from BLIP-2 to assess the align-ment between images and captions. In Sec. 2.2 covers themethod of measuring essentialness through mutual com-parison between candidate captions, termed the Consensusscore. In Sec. 3.1, we detail the process of integrating thesetwo scores, and Sec. 2.5 explains how the combined scoreis used to select the final caption.",
  ". Ensembled CLIP score": "The CLIP score is a metric that measures the seman-tic alignment between images and captions by comparingthe cosine similarity between the image embedding EI andcaption embedding EC through a pre-trained CLIP model.However, as the training data for the pre-trained CLIPmodel differs from the zero-shot caption re-ranking datasetprovided, the accuracy of the single CLIP score may not bereliable. To address this issue, an ensemble of CLIP scoresfrom various models that have proven to perform well inzero-shot tasks can provide a more robust semantic align-ment than a single CLIP score.",
  "where I = {EVA-CLIP, MetaCLIP, MobileCLIP,": "OpenCLIP, BLIP-2}.(3)The conventional CLIP score determines semantic align-ment using the cosine similarity between EI and EC, sub-stituting any negative values with zero. However, the ECOframework allows negative values to remain to achieve amore refined score distribution for image-caption pairs withlow relevance. To calculate the Ensembled CLIP score, thecosine similarity values between EI and EC were calcu-lated using models such as EVA-CLIP-18B, MetaCLIP,MobileCLIP, OpenCLIP, and BLIP-2.",
  ". Consensus score": "We refer to the extent to which a caption is made up ofessential expressions as its Essentialness. When variousmodels produce different captions, the expressions that ap-pear most often are considered essential to describe the im-age. To measure this essentialness, we use a scoring methodcalled the Consensus score.The Consensus score is a metric derived from the CIDErscore, that calculates the TF-IDF weights for N-Gramsacross candidate and reference captions. It then calculatesthe cosine similarity between the TF-IDF weight vectors of the candidate caption and each reference caption. To as-sess the essentialness of expressions within a caption, wecalculate the Consensus score for each caption, using allremaining candidate captions as reference captions, exceptthe caption under evaluation.However, the effectiveness of the Consensus score is sig-nificantly influenced by the quality of the caption pool usedas references. In other words, if the reference caption setconsists only of high-quality captions, the Consensus scoreis more likely to reliably reflect the degree of essentialness.To enhance the effectiveness of the Consensus score, we usetwo filters to make a high-quality caption pool.",
  "Bad Format Filter": "Based on the insights from the Flickr30k and COCO datasets, we have identified the typical structure of cap-tions. Generally, a caption is a phrase or clause of a singlesentence and includes a sufficient amount of information inan image. To ensure high-quality and relevant captions, wefiltered out captions that contained more than two periodsor more than three commas, or those that had fewer thanfive words. This filtering was done systematically using arule-based algorithm. This process helps to ensure that thecaptions being evaluated adhere to conventional standards.",
  "ITM Filter": "In order to filter out captions that are irrelevant to the con-tent of the image from group of candidates, we implementanother filter which is called ITM filter. Removing captionsthat are not related to the image is important because theycan hinder the consensus scoring. The consensus scoring isbased on the agreement among captions and can be affectedby the inclusion of expressions that are not related to theimage content.To filter out irrelevant captions, the Image-Text Match-ing (ITM) Loss from BLIP-2 is used. The ITM loss is de-signed to classify an image and text pair as either positiveor negative, making it very efficient for filtering out captionsthat are not related to the image.The ITM loss is calculated for each caption associatedwith an image, and the top 50% of captions with the highestITM values are selected. These captions are then used in thecaption pool for consensus scoring, ensuring that the cap-tions considered are more likely to be relevant and alignedwith the image content.",
  ". Score Combination": "In Sec. 2.1 and Sec. 2.2, we defined the Ensembled CLIPscore and the Consensus score.After normalizing thesescores individually, we combine them using a weighted sumto form the final score. This approach allows us to adjust theinfluence of each score differently, ensuring that both the se-mantic alignment between the image and captions and theessentialness of the captions are appropriately considered indetermining the most suitable caption. This method of inte-gration provides a flexible framework that can be tailored toprioritize different aspects of caption quality depending onthe specific requirements of the task at hand.",
  ". Short Caption Selection": "By combining the earlier Ensembled CLIP score and Con-sensus score, we obtained a final score that reflects both thesemantic alignment between the caption and image, and theessentialness of the caption. If there is a clear distinction inthe final score, the caption with the highest score is selectedas the optimal caption. In cases where the difference in thefinal score is not pronounced, meaning the difference be-tween the scores of the top-2 captions is less than a thresh-old , we chose the caption with fewer words as the finalcaption from the perspective of essentialness as shown in.",
  ". Score Combination Setting": "When setting the weights for score combination, we ob-served significant differences in outcomes depending onhow the Consensus score and the Ensembled CLIP scorescore were utilized. When comparing selected captions byusing only the Consensus score to those by using Consen-sus scoreand Ensembled CLIP score equally, We find a dif-ference in 5,396 out of 20,000 captions. Conversely, thediscrepancy reached 18,217 captions when the EnsembledCLIP score was used alone versus when it was combinedwith the Consensus score. To analyze these differences ac-curately, we visualize the distribution of both scores afternormalization and discovered that the maximum value ofthe Consensus score was approximately three times largerthan that of the clip score as shown in . This discrep-ancy suggested that, in situations where the caption withthe highest combined score was selected, the overwhelminginfluence of the Consensus score could skew the results.To ensure a balanced reflection of both scores, we de-cided to set 1 (the weight for the Ensembled CLIP score)larger than 2 (the weight for the Consensus score). Af-ter a few experiments, we confirm that a ratio of 3.52:1 is",
  "MethodCIDErSPICEMETEORROUGE-LBLEUAVG": "(a) Ensembled CLIP Score176.8330.0634.8463.8857.88(b) Consensus Score202.8931.2135.7968.2067.07(c) Ensembled CLIP Score + Consensus Score212.4432.4136.9869.4167.90(d) 3.52*Ensembled CLIP Score + Consensus Score218.4733.4637.9870.1367.58 . Ablations of Score Combination: The caption with the highest score is ultimately selected for submission. This involves (a)combining the CLIP score from models like EVA-CLIP-18b, MetaCLIP, MobileCLIP, and OpenCLIP with the BLIP-2 ITC score, (b) usingconsensus-based scoring alongside Caption Filtering, (c) combining the Ensembled CLIP score with the Consensus score at a 1:1 ratio,and (d) combining the Ensembled CLIP score with the Consensus score at a ratio of 3.52:1.",
  ". Ablations of Short Caption Selection. (f) has the same settings as (d) with the addition of Short Caption Selection. We set thethreshold to 0.39": "the most effective. This decision is supported by experi-mental evidence presented in Tab. 1, where the CIDEr scorefor results combined equally is 212.44, compared to 218.46for combinations using the 3.52:1 ratio. This result confirmthat placing greater weight on the 2 leads to improved out-comes. This weighting strategy aims to balance the influ-ence of both the Consensus score and the Ensembled CLIPscore, ensuring that both semantic alignment and essential-ness are appropriately considered in the final caption selec-tion.",
  ". Consensus Scorings Effectiveness in Identify-ing Essentialness": "We conduct an evaluation of the effectiveness of using theITM Filter and Bad Format Filter, by comparing Consensusscore for captions with and without filtering. Based on theresults presented in Tab. 2, we find that the filtered case hasa Consensus score of 202.89, while the unfiltered case hasa score of 192.98. This indicates that filtering the captionpool improves the quality of captions selected, as measuredby the CIDEr metric.Furthermore, we created a visualization in to showthe length of captions selected from each pool, measured bythe number of words per caption. The visualization showsthat captions chosen from the filtered pool are significantlyshorter on average within their respective pools. These find-ings collectively suggest that filtering the caption pool en-",
  ". Effects of Caption Filtering": "To assess the effectiveness of the ITM Filter and Bad For-mat Filter, we compare the evaluation results of the filteredcases with those of the unfiltered cases. The results, Tab. 2,shows that the CIDEr score increases from 192.98 to 202.89and there are also improvements in every other metrics. Itdemonstrate an enhancement in consensus scoring by refin-ing the pool of captions. Furthermore, we visualized therelative rank of the selected caption within the candidatecaption pool in terms of the number of words. re-veals that, after filtering the pool, the chosen captions aresignificantly shorter than those in their respective pools.These findings collectively suggest that filtering the captionpool enhances the ability of consensus scoring to discern es-sentialness, maximizing its effectiveness in evaluating cap-tions.",
  ". Conclusion": "We propose ECO, a zero-shot caption re-ranking frame-work that incorporates both image-caption semantic align-ment and caption essentialness.Our method selects themost ideal caption for an image from several candidates,without model training. Through , we have verifiedthat our methodology serves as a general caption re-rankingframework that performs well across all metrics, demon-strating its effectiveness and versatility in identifying ideal",
  "captions": "Peter Anderson, Basura Fernando, Mark Johnson, andStephen Gould. Spice: Semantic propositional image cap-tion evaluation.In Computer VisionECCV 2016: 14thEuropean Conference, Amsterdam, The Netherlands, Octo-ber 11-14, 2016, Proceedings, Part V 14, pages 382398.Springer, 2016. 1 Satanjeev Banerjee and Alon Lavie. Meteor: An automaticmetric for mt evaluation with improved correlation with hu-man judgments. In Proceedings of the acl workshop on in-trinsic and extrinsic evaluation measures for machine trans-lation and/or summarization, pages 6572, 2005. 1",
  "Chin-Yew Lin. Rouge: A package for automatic evaluationof summaries. In Text summarization branches out, pages7481, 2004. 1": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 3 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu. Bleu: a method for automatic evaluation of machinetranslation. In Proceedings of the 40th annual meeting of theAssociation for Computational Linguistics, pages 311318,2002. 1 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2",
  "Gerard Salton. The SMART retrieval systemexperiments inautomatic document processing. Prentice-Hall, Inc., 1971. 2": "Ramakrishna Vedantam, C Lawrence Zitnick, and DeviParikh. Cider: Consensus-based image description evalua-tion. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 45664575, 2015. 1 Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-maier. From image descriptions to visual denotations: Newsimilarity metrics for semantic inference over event descrip-tions.Transactions of the Association for ComputationalLinguistics, 2:6778, 2014. 3"
}