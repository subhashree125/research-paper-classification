{
  "Received: 3 April 2023": "Abstract We describe a protocol to study text-to-video retrieval training with unlabeled videos, wherewe assume (i) no access to labels for any videos, i.e., noaccess to the set of ground-truth captions, but (ii) ac-cess to labeled images in the form of text. Using imageexpert models is a realistic scenario given that annotat-ing images is cheaper therefore scalable, in contrast toexpensive video labeling schemes. Recently, zero-shotimage experts such as CLIP have established a newstrong baseline for video understanding tasks. In thispaper, we make use of this progress and instantiate theimage experts from two types of models: a text-to-imageretrieval model to provide an initial backbone, and im-age captioning models to provide supervision signal intounlabeled videos. We show that automatically labelingvideo frames with image captioning allows text-to-videoretrieval training. This process adapts the features tothe target domain at no manual annotation cost, conse-quently outperforming the strong zero-shot CLIP base-line. During training, we sample captions from multi-ple video frames that best match the visual content,and perform a temporal pooling over frame representa-tions by scoring frames according to their relevance toeach caption. We conduct extensive ablations to pro-vide insights and demonstrate the effectiveness of thissimple framework by outperforming the CLIP zero-shotbaselines on text-to-video retrieval on three standarddatasets, namely ActivityNet, MSR-VTT, and MSVD.Code and models will be made publicly available.",
  "Introduction": "The research on automatic video understanding haswitnessed a number of paradigm shifts recently. Follow-ing the rise of neural networks, the initial question washow to design an architecture to input spatio-temporalsignals . Given the limited video training data,the focus then shifted to borrowing parameter initial-ization from image classification pretraining . In anattempt to provide video pretraining, one line of workhas made costly efforts to annotate video classificationdatasets . On the other hand, the research commu-nity is moving away from closed-vocabulary recogni-tion training as the progress in language modeling in-spired advances in retrieval of visual data given open-vocabulary textual input, bridging the gap between",
  "Ventura, Schmid, Varol": "Fig. A.1: CLIPScore kernel density estimate: Weplot the CLIPScore distribution for three datasets, andboth models (ClipCap and BLIP). CLIPScore is higherfor ClipCap than for BLIP, potentially because of theCLIP backbone. with Top 2 of BLIP. It can be seen that only around 7%of the time the top captions from both captioners comefrom the exact two frames. More than 44% of the timethere is a frame in common with the two captioners.Finally, most frequently, 4 different frames are selectedfrom the 10 possible frames: 2 from each captioner. Repetitive captions.One other benefit of filteringthe captions is that we are left with a set of less repet-itive captions. See Figure A.3 for the percentage ofunique captions when using 10 captions and Top 2 cap-tions. We also check that there are less than 1% of over-lapping captions between the two captioners in any ofthe three datasets. This is yet another reason that mo-tivates us to use different captioners and obtain morediverse and rich captions. Fig. A.2: Combining captioners: We compare 4 dif-ferent strategies: selecting 2 from 10 ClipCap captions,selecting 2 from 10 BLIP captions, selecting Top 4 fromthe 20 combined captions, selecting Top 2 from eachcaptioner. We highlight the best performance with ablack border.",
  "Related Work": "We briefly overview relevant works on text-to-videoretrieval, self-supervised learning on unlabeled videos,pseudo-labeling, and captioning.Text-to-video retrieval. Methods for text-to-videoretrieval only recently started to train end-to-end neu-ral network models thanks to (i) the pow-erful initialization from ViT and (ii) large-scalevideo datasets: noisy HowTo100M data with ASR-based text supervision from speech, or more recentlythe cleaner manually annotated WebVid data . Theprogress in text-to-image retrieval then trig-gered advances in text-to-video retrieval. Recent meth-ods employ the CLIP image backbone and ex-plore the possibility of adding temporal modeling(e.g., CLIP2TV , CLIP4Clip , CLIP2Video ,CLIP-ViP , TS2-Net , ViFi-CLIP ). Theirresults suggest that the simple averaging of embed-dings over frames remains to be a strong baseline thatis difficult to improve on. Several works have explored",
  "Learning text-to-video retrieval from image captioning3": "fine-grained contrastive learning for videos ,e.g., considering both frame-word and frame-sentencecomparisons . Bain et al. presents a simple yeteffective method to pool video frame representationswith a weighted averaging based on query-scoring. Inthis work, we extend this method to use multiple cap-tions instead of a single label per video. We also useCLIP as our baseline, as well as our initializa-tion. Similar to other retrieval methods , weemploy a contrastive objective . Unlike these ap-proaches that assume manually annotated video data or noisy speech signal , we obtain oursupervision from automatic captioning annotations. Inour experiments, we show superior zero-shot perfor-mance over prior models trained on video-text pairsfrom HowTo100M or WebVid . Self-supervised learning on unlabeled videos.A relevant line of work is representation learning onunlabeled videos, which is often referred to as self-supervised learning. In this category, several works use instance discrimination for videos ina similar fashion with SimCLR or BYOL inthe image setting. The majority of methods also makeuse of the multimodal nature of videos, e.g., incorporat-ing the audio signal in the training . Apopular approach is to use the noisy speech signal in un-curated instructional videos such as HowTo100M .The text obtained via ASR is directly considered as thecorresponding label, which is then used within a con-trastive objective . designs a multipleinstance training, VideoCLIP performs retrieval-augmented pretraining, and Support-set defines amulti-task captioning objective. These self-supervisedworks may be complementary to our method, but ourfocus in this work is different in that we seek supervisionfrom external image models that provide pseudo-labels,which can be considered as an alternative route to selfsupervision. Pseudo-labeling. Our work is also relevant to pseudo-labeling (or self-labeling) approaches. Unlike the semi-supervised or few-shot setup consideredin these works, our pseudo-labels do not require anyannotations for the problem at hand. In particular, theconcurrent work of utilizes image experts to aidvideo-language learning, however, requiring a small setof labeled videos. In a similar fashion, VideoCC ex-ploits image-text datasets to assign automatic captionsto videos for audiovisual retrieval, but is limited by thefinite image captioning dataset source. Our work dif-fers from by generating captions for multiple videoframes, rather than retrieving from such a finite set.While these two approaches may potentially be comple-mentary, in our Appendix, we show that nearest neigh-",
  "bor retrieved captions perform worse than generatedcaptions": "In text-image pretraining, BLIP and BLIP-2 employ a bootstrapping approach for image cap-tioning, which falls into the semi-supervised category,i.e., they start training with a set of labeled images(whereas we never train on labeled videos). In fact, weemploy BLIP as one of our image captioners to obtainautomatic video labels. In our experiments, we also in-vestigate the impact of using BLIP initialization as op-posed to CLIP. Captioning. There has been increasing interest in thetask of generating text to describe a given visual con-tent . Although manyworks focus on integrating object information as addi-tional guidance (e.g., Oscar , VLP ), such meth-ods perform well on domains similar to that of the ob-ject detection model (e.g., COCO dataset ). Clip-Cap shows robust performance across datasets ofvarious domains without making use of an explicit ob-ject detection module. Instead, makes use of twopowerful pretrained models (CLIP and GPT-2 )and learns a mapping model between the image featuresand the language generation. More recently, BLIP ,BLIP-2 and CoCa extend the contrastive CLIPtraining by jointly learning image captioning. Alignand tell also incorporates a video captioning headinto their text-video retrieval model during training.OFA further supports a variety of image-languagetasks in a unified framework, where captioning can beperformed by prompting the visual question answeringmodel with What does the image describe?. Very re-cently, CapDec attaches a text decoder on top ofthe frozen CLIP image encoder by exploiting text-onlydata to train an autoencoder with the CLIP text en-coder. In our work, we employ ClipCap and BLIP as our image captioning experts, from which we obtainthe supervision signal for unlabeled videos. While bothof them are only image-based models, we find that theirperformance is satisfactory on video frames. The perfor-mance of video captioning models are currently behindthose of image captioning approaches, mainly due tolimited training data . Future work can explorethem as their performance improves. Recent works ofClipVideoCap , Lavander , CLIP4Caption ,HiREST , and TextKG obtain promising re-sults. However, our setup in this work considers no ac-cess to labeled videos.",
  "(b)": ": Caption selection and multi-caption query-scoring (MCQS): (a) To select the best captions for agiven video, we first extract image captions from both ClipCap and BLIP models for M number offrames. We then compute the CLIPScore (gray box), and finally select Top K = 2 captions for each captioner:c1 and c2 for ClipCap (highlighted in green), and c3 and c4 for BLIP (highlighted in blue). (b) MCQS takes acaption embedding cl and weights the frame embeddings v1...vN according to the query-scoring temporal pooolingfunction fp to obtain a video representation vl. Finally, we simply average the four similarities obtained with theirrespective query-scoring.",
  "Training with automatic captions": "In this section, we first describe how we obtain au-tomatic captions for labeling videos, then present ourmulti-caption video retrieval training, and finally, giveimplementation details for our experimental setup. The overview of our method is illustrated in Fig-ure 2. In summary, we start by constructing a set of la-bels for each video, by applying image captioning mod-els on video frames. Given these noisy frame-level cap-tions (from multiple image captioners), we select thehigh-quality ones by sorting them according to their CLIPScore . We adopt a contrastive video-text re-trieval training using a multi-caption query-scoring ap-proach, where we incorporate all the selected captionsinto the objective. Next, we detail these steps. Selecting high-quality captions. Given an unla-beled training video v consisting of F frames, we selectM frames from the video (M F) and extract cap-tions using I image captioners to form an initial set oflabels C = {Ci}Ii=1, where Ci = {ci1, ci2, . . . , ciM}. Wethen obtain I textual descriptions per frame, resultingin a total of M I labels per video.",
  "Learning text-to-video retrieval from image captioning5": "While we investigate several variants of label forma-tion from captions in our experiments, our final strat-egy is the following. We select a subset of the initiallabels, mainly to eliminate noisy captions that do notwell represent the corresponding video frame. To thisend, we employ CLIPScore as a way to measure thecross-modal similarity between a caption and its corre-sponding frame. For each captioner, we keep the top-Kcaptions (K < M) with the highest CLIPScores, whichgives us a remaining L = K I labels per video. Werefer to this subset as C. Note that some captions arerepetitive across frames due to visual similarity withina video; we therefore conjecture that such a subset se-lection does not cause a significant loss in information.Contrastive video retrieval objective with multi-caption query-scoring. In this work, we employ a rel-atively standard vision-language cross-modal training,where the goal is to find a joint space between videosand automatic captions. Given a video v, we computevisual embeddings V = {vn}Nn=1 on N video frames(N F) using a visual encoder fv : vn Rd. Similarly,we compute textual embeddings with the text encoderft from the corresponding set of labels C to obtain pos-itive text representations C = {cl}Ll=1, where cl Rd (with the same embedding dimension as vn). To obtaina single video embedding, we perform temporal pool-ing over video frame representations. Inspired by thequery-scoring introduced by , our pooling dependson the text representation, simply through weighted av-eraging, where frame weights are proportional to theirsimilarity with the text. The pooled video embedding isthen compared against the text to obtain a single sim-ilarity. Differently from , we have multiple texts cl.We therefore apply query-scoring multiple times, andobtain multiple similarities, which we combine by a sim-ple mean operation (experiments with weighted meando not yield improvements; see .2). More for-mally,",
  "L = Lc2v + Lv2c,(5)": "The final loss is the sum of video-to-captions (Lv2c) andcaptions-to-video (Lc2v) retrieval loss terms. Next, wedetail the optimization procedure.Implementation details. We instantiate two imagecaptioners (I = 2) from ClipCap and BLIP models. ClipCap model is pretrained on the 3M im-ages of the Google Conceptual Captions image-textdataset , using a MLP mapping between CLIP image backbone and GPT-2 text generation mod-els. BLIP jointly trains for retrieval and captioning us-ing 129M images (including a subset of LAION )using a bootstrapping approach. We use the publiclyavailable model, which is further finetuned on theCOCO dataset . Given one captioner, we extractM = 10 captions per video from equally spaced frames.We empirically set the number of high-quality captionsto top K = 2 per captioner (i.e., L = K I = 4). On asingle GTX1080 GPU, the captioning cost for ClipCapand BLIP is 0.65 fps and 0.93 fps, respectively.We minimize the loss function in Eq. 5 usingAdam optimizer and a learning rate schedule witha cosine decay as described in . For ActivityNet,we train on 16 Tesla V100 GPUs for 10 epochs, with ini-tial learning rate 105 and mini-batch size B = 64. ForMSR-VTT and MSVD, we train on 4 NVIDIA GeForceGTX 1080 for 10 epochs, with initial learning rate 104 and mini-batch size B = 16.The weights of our dual encoder model are initial-ized from CLIP pretraining in all experiments un-less explicitly stated otherwise, both for the image (fv)and the text (ft) encoders. The image encoder archi-tecture follows ViT-B/16 in all experiments. Thetext encoder architecture follows GPT-2 . Both en-coders are Transformer-based , operating with anembedding dimensionality of d = 512.",
  "Experiments": "We start with .1 by describing the datasetsand evaluation metrics used to report the results ofour experiments. We then present our ablations in Sec-tion 4.2, quantifying the effects of (i) the captioningmodel, (ii) caption selection, (iii) combining caption-ers, (iv) training with multiple captions per video, and(v) combining datasets. Next, we present a state-of-the-art comparison in .3, followed by experimentson BLIP initialization instead of CLIP in .4.Finally, we provide a qualitative analysis in .5,as well as a discussion on limitations in .6.",
  "Datasets and evaluation metrics": "We conduct experiments on three established bench-marksfortext-to-videoretrieval,namelyActivi-tyNet , MSR-VTT , and MSVD datasets.ActivityNetCaptionscontains20kYouTube videos. Videos are segmented into 42k clipswith an average length of 45s. We use the 10,009 videosfrom the training set, and evaluate on the val1 split(4917 videos). Note that we extract equally spacedcaptions per clip, not per video.MSR-VTT is composed of 10k YouTubevideos. The length of the videos varies from 10s to 32s,with an average of 15s. We train with the Training-9ksplit as in , and report results on the 1ksplit with single video text-pairs as in .MSVD consists of 1970 videos split into 1200training, 100 validation, and 670 test videos. Thedataset contains both short videos (1s) and longvideos (60s). Given the small size of the dataset, we",
  "Ours w/ OFA 27.655.633.659.241.167.4Ours w/ ClipCap 26.753.534.759.840.668.9Ours w/ BLIP 27.954.235.860.641.169.1": ": Captioning models: Training with auto-matic captions obtained with OFA , ClipCap ,and BLIP all improve over the zero-shot CLIP base-line on all three text-to-video retrieval benchmarks.BLIP captions result in best performances. train using three different seeds and average the resultson the test split.As previously explained, even though these datasetscontain ground-truth captions, we do not use them dur-ing training (see experiments in Section A on fully-supervised setting). We report the standard evaluationprotocols: text-to-video (T2V) recall at rank 1 and 5for all experiments. Recall at rank k (R@k) quantifiesthe number of times the correct video is among the topk results. Higher recall means better performance.",
  "Ablation study": "This work constitutes an exploratory study to testwhether captions can provide a training signal for un-labeled videos. The answer is yes; however, there arecertain design choices we make. Here, we provide abla-tions to measure the sensitivity to these decisions. Morespecifically, we investigate the effects of the captioningmodel and the quality of the captions provided to themodel, To further improve the results, we make use ofmultiple captions per video during training, and com-bine datasets to train a single model.(i) Captioning models. The first design choice ison the image captioning model to use. In , wepresent a comparative study experimenting with threerecent captioning models: OFA , ClipCap andBLIP . More specifically, we use the best availablemodel checkpoints: OFA-huge trained with 20M pub-licly available image-text pairs, ClipCap trained withConceptual Captions, and BLIP-Large trained with129M images, finetuned on COCO. Best results are ob-tained with BLIP, potentially due to the large amountof pretraining compared to the other two models. Theresults also demonstrate the effectiveness of using cap-tions to improve over the strong CLIP baseline ,where we average video frame embeddings using thefrozen CLIP. Note that this is the same as the meanpooling method used in CLIP4Clip . In this exper-iment, we randomly select one caption out of the two",
  "Rand(10)26.352.734.660.540.568.7Middle 125.752.433.257.840.169.9Top 127.654.634.960.341.868.3Rand(Top 2)27.954.235.860.641.169.1Rand(Top 3)27.854.235.659.540.968.2": ": Caption selection: For both captioners, wecompare training with a random caption at each epoch,training with only the middle frame caption, and train-ing with different number of Top K captions (bestCLIPScore ). Using CLIPScore filtering improvesover using all the 10 captions or only using the middleone on both datasets. Selecting the Top 2 captions re-sults in overall best performance. best captions during training. We next assess the influ-ence of this selection.(ii) Caption selection. Automatically generated cap-tions vary in quality. We select captions with highimage-text compatibility to eliminate potential noise inour training. The above image captioning models donot output a confidence score; therefore, we use CLIP-Score between the generated caption and the cor-responding input video frame as a caption quality mea-sure.In , we evaluate whether such filtering is ben-eficial. In this experimental setup, we train with onecaption as the video label. We experiment with fivedifferent variants per captioner: (a) randomly selectingone of the 10 extracted captions at each epoch, (b) us-ing only the caption corresponding to the middle frame(i.e., same label in all epochs), (c) using only the bestcaption (i.e., top 1 based on the CLIPscore metric), (d)randomly selecting one of the 2 best captions at everyepoch, (e) randomly selecting one of the 3 best cap-tions at every epoch. The results support the idea thatCLIPScore is an effective filtering method to keep thehighest quality captions. On all three datasets, and onboth captioners (ClipCap and BLIP), using the bestcaption(s) slightly improves over using all the captionsor the middle one. Especially for ActivityNet, wherethe videos are relatively long, it is expected that thecaption of the middle frame may not be representativeof the video. However, there exists a trade-off betweenthe number of captions and their quality. With morecaptions per video, we avoid overfitting as this mayserve as data augmentation. On the other hand, thevariance among the caption qualities starts to increase.We empirically find that taking the best two captions",
  ": Combining two captioners: We observeslight improvements when using captions from bothClipCap (C) and BLIP (B) over using them individ-ually": "constitutes a good compromise, yielding a promisingperformance overall. However, the difference betweentop 1, 2, or 3 (last three rows) is not significant.(iii) Combining captioners. One way to increase thenumber of captions per video without decreasing thequality of the captions is to use the best K captionsfrom each captioner to form the label set. In , wetest this hypothesis by taking two captioners ClipCapand BLIP, to then ensemble their labels. The resultsare slightly better than the performance of individualcaptioners on most metrics. One can potentially furtherextend to more captioners I > 2.Note that we could also select the top K from all thecaptions combined from both captioners. This would beequivalent to taking the best 2 captions out of the 20 (10per captioner). However, this leads to poorer results,perhaps due to the different CLIPScore distributions(slight preference for ClipCap potentially because of theCLIP backbone), and the tendency to output repetitivecaptions across frames for a given captioner. We providefurther analysis in Section D.(iv) Multi-caption query-scoring (MCQS). So far,we have only used one caption as a video label dur-ing each training iteration (even if this is randomlyselected from a pool of 4). Here, we explore how toeffectively combine multiple captions to get a richervideo label, potentially capturing more global contentbeyond a single-frame caption. In , we com-pare multi-caption query-scoring (MCQS) with single-caption query-scoring (QS) for the 4 captions from Clip-Cap and BLIP as before.We first evaluate the effect of QS for the uniformmean baselines (i.e., only at test time for the CLIPbaseline, and also at training for one random captionbaseline). Our first observation from is that QSat evaluation marginally improves the baselines (33.9vs 32.8 for CLIP, 37.6 vs 36.5 for Rand on MSR-VTTR@1). Training and evaluating with QS gives a furtherboost (38.3 vs 37.6).In the last three rows of , we then explorethree variants of our approach for using multiple cap-",
  "Concat(4)QSQS29.857.727.350.935.162.6Weighted(4)MCQSQS29.057.038.663.241.570.5Mean(4)MCQSQS29.757.139.064.642.570.1": ": Multi-caption query-scoring: Using all se-lected captions during training increases performanceover only using one caption. The CLIP baseline andthe model trained with randomly choosing one the 4caption labels are evaluated with query-scoring (QS)for fair comparison. All models use Top-2 from bothcaptioners (i.e., 4 captions in total from C+B). tions: a) concatenating captions into a single text andjust using vanilla QS, b) weighted, or c) mean simi-larity pooling in MCQS. Simple concatenation signif-icantly decreases the performance on MSR-VTT andMSVD, probably due to the distribution shift causedby the longer sentences during training (4 sentencesduring training vs 1 sentence at evaluation). On theother hand, ActivityNet results remain similar or evenslightly improve as the standard evaluation protocolalso concatenates ground-truth descriptions at test time. The mean similarity pooling in MCQS obtains anoverall improvement across datasets, over both CLIPand single-caption baselines. We observe a decrease inperformance when dynamically weighting the similari-ties based on the ClipScore (with a softmax tempera-ture of 0.1). We therefore keep the method simple anduse the mean of similarities when jointly training withmultiple captions in MCQS.(v) Training with multiple datasets. Given thatour framework does not require manually annotatedvideos, we are not constrained by the fixed size of adatasets training split, and we can train with moredata. In , we compare how the performancediffers when: (i) training and evaluating on the samedataset (Self) versus (ii) training with more data bycombining multiple datasets (Combined). The result-ing combined training set has the following distribu-tion in terms of number of video clips coming fromeach dataset: 79% ActivityNet, 19% MSR-VTT,and 2% from MSVD. The percentages represent therelative contribution of each dataset to the combinedtraining set, derived from the total number of videosavailable in each dataset, with a uniform sampling ap-proach that leads to a higher representation of Activ-ityNet due to its larger size. Such joint training im-proves performance moderately for the two relativelybigger datasets (ActivityNet and MSR-VTT), and more",
  "Ours (Combined)WiT+PLViT-B/1630.657.939.2 65.1 44.6 71.8": ": Training on the combination of datasets:We compare training and evaluating on the samedataset (Self), and training with the three combineddatasets (Combined = ActivityNet + MSR-VTT +MSVD), and show that combining datasets removesthe need of training three separate models and slightlyimproves the overall performance. We perform favor-ably compared to the state of the art on zero-shotretrieval (i.e., not using ground-truth video labels indownstream datasets). Colored lines are obtained fromour implementation. denotes results we obtained withthe code from . PL is short for pseudo-labels (usingautomatic captions). H: HowTo100M, VCC: VideoCC.B: COCO+VG+CC+SBU+LAION. significantly for the small MSVD dataset. In the Ap-pendix Section C.1, we also report cross-dataset eval-uations (e.g., training with ActivityNet and evaluat-ing on MSR-VTT). This experiment provides addi-tional insights into the generalizability of our approachacross different dataset domains. An additional advan-tage is to obtain a single model instead of multipledataset-specific models. Future work can exploit includ-ing larger scale datasets provided sufficient computingresources.",
  "Comparison with the state of the art": "In , we summarize other zero-shot methods re-porting performances mainly for MSR-VTT, and ourmethod performs favorably against the state of the art.The rows that are colored are from our implementa-tion, in comparable settings (e.g., using QS); uncol-ored rows correspond to other works. Red rows de-note our baselines, green rows show our final models.Note that CLIP4Clip zero-shot version is similarto our CLIP baseline since they both use a frozenCLIP to mean-pool over frame embeddings. One dif-ference is our use of query scoring, which was previ-ously ablated in . Another difference may bedue to different hyperparameters such as the numberof frames (N = 10 in ours vs 12 in ). Note that in",
  "YesBaseline35.160.643.566.340.667.9Ours34.660.643.566.542.268.5": ": Initialization with BLIP: We show the com-parison between the baseline versus our finetuning withautomatic captions across various settings: CLIP/BLIPinitialization, BLIP backbone with/without COCOfinetuning, BLIP backbone with only the dual encoderor with subsequently reranking with its cross-modal en-coder. Our method demonstrates improvements overthe baseline across different initialization settings, butthe gain is reduced as the baseline performance in-creases. For fairness, unlike the original BLIP evalua-tion, we use Query-Scoring (QS) when computing dualencoder similarities. contrast to other works, we have access to the train-ing videos (denoted with PL in ), albeit withouttheir corresponding ground-truth labels. On the otherhand, some of the competitive methods require an ex-ternal large source of videos such as WebVid andVideoCC . Others rely on noisy speech signal fromthe extensive HowTo100M data , buttheir performances remain inferior. Among prior works, BLIP obtains higher per-formance than our method on MSR-VTT and Activi-tyNet. However, the BLIP model fundamentally differsfrom dual encoder approaches in that BLIP also con-tains a cross-modal encoder that is used for an addi-tional image-text matching (ITM in their paper) as aclassification task. The matching score from this classi-fication head is then ensembled with the cosine similar-ity obtained by the dual encoder. Cross-modal encodersare known to perform better than dual encoders; how-ever, they are less efficient . We, therefore, gray outthis line in to highlight this difference. On theother hand, we compute the performance of the BLIPdual encoder, by considering only the cosine similar-ity between the unimodal embeddings (similar in spiritto CLIP). The result is much lower, for example forMSR-VTT 35.7 R@1, i.e., lower than both (i) their en-sembled result 43.3 and (ii) our best model using only adual encoder 39.2. We next extend our investigation toevaluate the applicability of our method on this morerecent cross-modal BLIP encoder as an intialization in-stead of CLIP.",
  "BLIP initialization": "To evaluate the applicability of our method across var-ious model initializations, we experiment with addi-tional backbones beyond the primary CLIP model. Inparticular, we incorporate the BLIP model , whichis available with and without COCO finetuning. Theimplementation details of BLIP, are summarized in Sec-tion E of the Appendix. In , we compare (a) CLIP and BLIP, (b)two versions of BLIP pretraining, (c) both the effi-cient dual encoder version and the expensive rerankingwith the cross-modal version of BLIP as done in ,(d) with/without our finetuning with automatic cap-tions. Across all datasets and model configurations, wefind that our finetuning with automatic captions consis-tently improves over the baselines, with the exceptionof the last two rows. The improvement is more signifi-cant for the CLIP backbone, than for BLIP where thebaseline performance is already close to that of fully-supervised approaches (see Table A.1 of the Appendix).In other words, with greater baseline results of the un-derlying backbone, the more marginal the performancegains become. We further note that the reranking operation withthe cross-modal encoder, while generally leading to im-proved performance, is significantly less efficient thanusing the dual encoder alone. Specifically, in , an ini-tial retrieval is obtained with the dual encoder, and thetop-k (k = 128) retrieved videos are reranked with thecostly cross-modal encoder. Without the cross-modalencoder, the CLIP-based model with our approachdemonstrates superior performance (refer to rows withNo under Cross-modal Encoder in ). Wealso clarify that the BLIP baseline performances forboth dual and cross-modal encoder configurations areslightly different when compared to , due to theincorporation of QS in the evaluation for a fair com-parison; for example, MSR-VTT R@1 shows 37.4 vs35.7 for the dual encoder and 43.5 vs 43.3 for the cross-modal encoder with and without QS, respectively. Forthe cross-modal encoder setup, QS is only used at thedual encoder retrieval stage, but not in reranking as theencoder inputs all frames without needing a temporalpooling as in . We conclude the quantitative experiments by stat-ing that pseudolabeling text-video retrieval datasetswith image captioning allows finetuning text-to-imagebackbones with no manual annotation cost, which inturn substantially improves, for example over the frozenCLIP (e.g., 23.8 vs 30.6 on ActivityNet, 33.9 vs 39.2 onMSR-VTT, and 38.5 vs 44.6 on MSVD in ).",
  "Qualitative analysis": "In , we illustrate text-to-video results on severalexamples on all three datasets. For each test example,we display (a) the textual query, (b) the ground-truthvideo corresponding to the textual query (first columnwith blue border), (c) middle frames of the top 5 re-trieved videos (in order from highest to lowest simi-larity), and (d) highlighted green border if the videomatches the correct video, or a red border otherwise.Note that we only visualize the middle frame, whichmight not be representative for the overall video. Weobserve that most of the retrieved videos contain rele-vant information to the query text. For example, withthe text query: cartoon one women in horse and speakto that calmly, all the retrieved videos show cartoons.Moreover, sometimes even if the correct video is notranked in the first position, there may be more thanone valid option (e.g., the text query: a man is play-ing the flute). We provide more examples in Section F.",
  "Limitations": "Here, we discuss several limitations of this work. First,we note that image captioning does not necessarily cap-ture the dynamic content of videos. In particular, somevideos may only be recognized when observing severalframes. Similarly, our temporal pooling approach re-mains simple, ignoring the order of frames. Temporalmodeling efforts; however, do not yield gains for re-trieval benchmarks . As an attempt to incorporatetemporal information, we performed preliminary anal-ysis using text summarization techniques over the se-quence of captions, but did not obtain consistent im-provements (see Section B). Another limitation of ourexperiments is to train on the videos from the train-ing set of a target dataset. Even if we do not use theirlabels, this setup ensures minimal domain gap. Futurework can leverage large unlabeled video collections toremove this need.",
  "Conclusion": "We showed a simple yet effective framework to uti-lize an image captioning model as a source of super-vision for text-to-video retrieval datasets. We demon-strated significant improvements over the strong zero-shot CLIP baseline with a comprehensive set of exper-iments. There are several promising directions for fu-ture development. One can explore the integration ofmore image experts beyond captioning, such as open-vocabulary object detection. The pseudolabeling ap- proach could be extended to a wider variety of videodata as mentioned in .6. The complementar-ity of self-supervised representation learning methodscould be investigated to increase the supervision sig-nal in unlabeled videos.Another future direction isto explore methods to combine the sequence of imagecaptions into a single video caption.",
  ". Wang, X., Zhu, L., Zheng, Z., Xu, M., Yang, Y.:Align and tell: Boosting text-video retrieval withlocal alignment and fine-grained supervision. IEEETransactions on Multimedia (2022) 3": "76. Wang, Z., Li, M., Xu, R., Zhou, L., Lei, J., Lin, X.,Wang, S., Yang, Z., Zhu, C., Hoiem, D., Chang,S.F., Bansal, M., Ji, H.: Language models with im-age descriptors are strong few-shot video-languagelearners. arXiv (2022) 3 77. Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D.,Aghajanyan, A., Metze, F., Zettlemoyer, L., Feicht-enhofer, C.: VideoCLIP: Contrastive pre-trainingfor zero-shot video-text understanding. In: EMNLP(2021) 3, 8, 9",
  "A.1 Finetuning with ground-truth captions": "We show that our proposed methodology can be usedas a pretraining step. Here, we experiment with ini-tializing a model trained with automatic captions, andfinetuning with ground-truth captions to further im-prove the performance. Table A.1 summarizes the re-sults. The bottom gray lines compare finetuning themodel with ground-truth captions (i) from CLIP ini-tialization (rows with WiT+GT data), or (ii) from pre-training with our method (last row with WiT+GT+PLdata). This comparison highlights the benefits of usingour proposed methodology as a pretraining step, as itleads to further improvement in performance on thetarget datasets. We note that when we train with theground truth, we keep all hyperparameters the samefor both (i) finetuning from CLIP initialization or (ii)finetuning from our pretraining with pseudolabels.",
  "Ours w/ NN-CC35.461.140.266.9": "Table A.3: Retrieving nearest neighbor captionfrom an image-text dataset: Instead of using a cap-tioner, we experiment with retrieving the captions fromthe Conceptual Captions dataset, using the frameembedding as query (NN-CC) and obtain comparableperformance to other captioners. space between images and text through CLIP, an alter-native approach would be to retrieve the closest textembedding from a large image-text gallery by queryingwith the video frame embedding (similar in spirit to). We performed this baseline experiment using theGoogle Conceptual Captions dataset as the image-text gallery source, which was also the ClipCap trainingset . In a similar fashion to our previous experi-ments, (i) we extract 10 frames, (ii) retrieve a captionfor each frame, and (iii) compute CLIPScore and fil-ter them accordingly. In Table A.3, we show that theretrieved captions can also be used to outperform thezero-shot baseline. However, as will be seen in the nextsection, the retrieved captions appear to be less similarto the ground truth text than with ClipCap or BLIP(Table A.3).Captioningbottleneckwithtext-to-textre-trieval. Another baseline we design is to use the cap-tions directly at test time without fine-tuning CLIP.This constitutes an information bottleneck where thevideo is embedded only into a text, as opposed to ahigh-dimensional embedding space. To determine thenearest video given a text query, we use the previouslyextracted captions with ClipCap and BLIP. To repre-sent a given video, (i) we embed the 10 extracted cap-tions with Sentence-BERT (S-BERT), (ii) select",
  "BLIP CLIP10.628.515.833.325.747.6BLIP S-BERT13.132.318.139.028.552.3": "Table A.4: Captioning bottleneck with text-to-text retrieval: We experiment with retrieving videosby representing them with the text embedding of theextracted captions. This results in lower performancethan the CLIP baseline. We present performances withtwo different text encoders, the CLIP text encoderand Sentence-BERT (S-BERT). See text for moredetails. the two with the highest CLIPScore , and (iii) av-erage their embeddings. We then compare a text query(also embedded with S-BERT) with this video repre-sentation using cosine similarity. In Table A.4, we sum-marize the results. Of the two text encodings tested,S-BERT performs better than CLIP text encoder as S-BERT was intentionally trained to detect similar sen-tences. However, even the best performing caption bot-tleneck (i.e., BLIP with S-BERT) obtains worse resultsthan the zero-shot CLIP baseline. The poor perfor-mance of this caption-based retrieval approach suggeststhat captions are not sufficient to be used directly forretrieval, but they can instead provide a supervisionsignal for training.Text summarization. As mentioned in .6of the main paper, we explored using a text sum-marization model to combine multiple captions in agiven video, and our attempts led to inconsistent re-sults, as seen in Table A.5. We experimented withsummarizing the 10 captions from the two captioners,(Summ(10C) for ClipCap and Summ(10B) for BLIP)and summarizing the filtered and combined 4 captions(Summ(2C+2B)). To summarize the captions, we usethe Ada language model hosted in OpenAI. We empiri-cally find that it helps to prepend a randomly sampledraw caption to the summary, potentially because weobtain a longer caption with both local and global in-formation (i.e., results in Table A.5 improve when theprepend column is not empty, e.g., 37.5 vs 35.9).",
  "C.1 Cross-dataset evaluation": "As mentioned in .2 of the main paper, we re-port cross-dataset evaluations. In Table A.6, we use themodels trained with multi-caption query scoring, wherethe diagonal corresponds to the second-last row of Sec-tion 5 (training and evaluating on the same dataset). In-terestingly, the performance of MSR-VTT training andevaluating on ActivityNet is almost as good as trainingwith ActivityNet videos. Furthermore, models trainedonly on MSVD perform poorly on all datasets (includ-ing itself), given its small size.",
  "D Analysis on selecting captions andcombining multiple captioners": "As mentioned in .2 of the main paper, we pro-vide further analysis about the source of captions frommultiple frames and multiple captioners.Quantitative results. One way to check the assump-tion that selecting the best captions is removing noisycaptions is to compare the captions with the groundtruth. In Table A.9, we compare the extracted captionswith the ground truth with two metrics: METEORand CLIPScore. However, unlike in the main paper,here we compute the CLIPScore between the two texts",
  "Max28.080.427.591.931.884.6": "Table A.9: Comparingautomaticcaptionstoground-truth text: We compare the extracted cap-tions from Nearest Neighbour (NN), ClipCap (C) andBLIP (B) approaches to ground-truth video captions,with METEOR (M) and Text CLIPScore (T-CS) metrics. When we evaluate 10 or 2 captions,we compute the metrics individually for each captionand report the average. For the maximum, we com-pute the metrics for all the 10 captions and select theone with the highest score. Retrieving nearest neigh-bour captions have the least similarity with the groundtruth text. Filtering captions with CLIPScore (Top 2)improves all metrics. (extracted and ground-truth captions), rather than be-tween visual and text embeddings. The results motivatethe top-2 selection instead of using all 10 captions. Wealso show the maximum (Max), which corresponds tothe comparison of the ground truth with each of the10 captions individually and selecting the one with thehighest score, as a way to give an upperbound on thisscore assuming a perfect selection method (note thatthis requires access to the ground truth). We observethat retrieving nearest neighbor captions has the leastsimilarity with the ground-truth text.Different CLIPScore distributions. As seen in Fig-ure A.1, ClipCap and BLIP captions have differentCLIPScore distributions, with being higher for Clip-Cap, perhaps due to the CLIP backbone. If we were toselect the best 4 captions out of the 20 available ones,we would be selecting ClipCap captions more often thanBLIP captions.Top 4 of all the captions. We see in Figure A.2 thatcombining captions from different captioners is betterthan using only ClipCap or BLIP. Out of the two al-ternatives: (i) selecting top 4 of the 20 combined set ofcaptions, (ii) selecting top 2 from each captioner, option(ii) leads to better results.Number of different frames. When we select Top2 from one captioner, our captions come from only twoframes. In Table A.10, we see statistics of the amountof different frames when combining Top 2 of ClipCap",
  "ActivityNet47.4%45.4%7.2%MSR-VTT48.5%44.2%7.3%MSVD47.4%44.8%7.8%": "Table A.10: Different frames: When using C+B Top-2 (4 captions), about 47% of the videos have captionsfrom 4 different frames, and around 45% of the videoshave captions from 3 different frames (i.e., the two cap-tioners pick the same one frame in their top rankings).Finally, there are roughly 7% of the videos where bothcaptioners select the same two frames. In these cases,multiple captions can still be useful to provide dataaugmentation.",
  "Learning text-to-video retrieval from image captioning19": "Fig. A.3: Percentage of unique captions: We makestatistics about the percentage of unique extracted cap-tions within a video (top: for all 10 captions, bottom:for the best 2 captions). We observe that BLIP cap-tions are more diverse, and ClipCap ones are a bit morerepetitive. Beyond two captioners. We explore using three dif-ferent captions by combining ClipCap (C), BLIP (B)and OFA (O) in Table A.11. The results do not bringconsistent improvements in both metrics (better R@1,worse R@5), possibly because OFA performance aloneis not as effective compared to BLIP.",
  "E Implementation details for the BLIPinitialization experiment": "We here explain the BLIP implementation details ofthe backbone experiments in . We train usinga method akin to that of BLIP, where the Image-TextContrastive (ITC) loss is denoted as our L in Eq. (5).For the Image-Text Matching (ITM) loss, we extendthe encoder hidden states by the number of frames. Wetrain with 4 frames and evaluate with 8 frames. We adopt the ViT-B/16 backbone for the image encoderand the BERT architecture for the text encoderas in BLIP. We train the model with a single NVIDIARTX A600 using 4 frames, while evaluations are con-ducted using 8 frames as in the original paper.",
  "F Additional qualitative results": "Captioning. Similar to of the main paper, inFigure A.4, we provide more examples of captioning re-sults from both ClipCap and BLIP, together with theircorresponding CLIPScores when compared to the im-age embeddings. In the third picture of the second videoor in the first picture of the third video, we see thatCLIPScore is low when the captions does not matchthe frame. In the last video, we see examples of a shortvideo where all the frames look alike, and the extractedcaptions are the same or almost the same.Retrieval. To complement of the main pa-per, we provide additional qualitative results in Fig-ure A.5 for the three datasets: ActivityNet (first tworows), MSR-VTT (middle two rows) and MSVD (lasttwo rows).",
  "Learning text-to-video retrieval from image captioning21": "Fig. A.5: Qualitative text-to-video retrieval results: Above, video retrieval results for our best model (Com-bined) are shown. The examples belong to the test sets of ActivityNet (first two rows), MSR-VTT (third andfourth rows), and MSVD (last two rows). Each example is shown with the text query, the ground-truth video(first column, blue border), and the top 5 retrieved videos from the gallery. Every video is only displayed usingthe middle frame, with a green border if it matches the ground-truth video, or a red border otherwise. Overall,all the retrieved videos have similar semantic meaning with the text query, even in cases where the correct videois not retrieved at the first rank."
}