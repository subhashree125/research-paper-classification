{
  "- Bauer contacted Meredith Reed.- You're suggesting muzzling thepress?": "- ... Meredith Reed. Call her publisher and demand not to run the story. - ... muzzling press. The freedom of press is constitutionally protected? - You are the President. Tell them this story will create a war. - ... I don't need to tell what damage your criminal prosecution will do. - You're poison. ... never should have let you do this. - Where's Novakovich? What did he say to Bauer? - Nothing ... was bleeding ... wounded by one of the guards. - ... murdered. Bauer learned that Novakovich killed Omar and Renee. - Lunatic was gonna kill me, Yuri. Think of it as sacricing a rook for a king. - ... orders? There's nothing more dangerous than a wounded animal. . We illustrate how TV show recaps can be used to generate labels for multimodal story summarization. The top half featuresthe recap shown at the beginning of the episode S08E23 based on key moments (shots and dialogs) from S08E22 of the TV series 24. Asrecaps help viewers recall essential story events, we extend these aligned segments to create summarization labels (visualized in the bottomhalf where the actual shots and dialogs inherited from recap are marked in deep red). For example, in the sub-story (left), the recap hintsat Jack Bauer relaying classified information to the press, while the summary presents the complete sub-story, including Logan informingPresident Taylor about their failure to catch Jack and their disagreement over muzzling the press.",
  "Abstract": "We introduce multimodal story summarization by lever-aging TV episode recaps short video sequences interweav-ing key story moments from previous episodes to bring view-ers up to speed. We propose PlotSnap, a dataset featur-ing two crime thriller TV shows with rich recaps and longepisodes of 40 minutes.Story summarization labels areunlocked by matching recap shots to corresponding sub-stories in the episode. We propose a hierarchical modelTaleSumm that processes entire episodes by creating com-pact shot and dialog representations, and predicts impor-tance scores for each video shot and dialog utterance by en-abling interactions between local story groups. Unlike tra-ditional summarization, our method extracts multiple plotpoints from long videos. We present a thorough evaluationon story summarization, including promising cross-seriesgeneralization. TaleSumm also shows good results on clas-sic video summarization benchmarks.",
  "Imagine settling in to catch the latest episode of our favoriteTV series. We hit play and the familiar Previously on ...,": "the recap, a smartly edited segment swiftly brings us up tospeed, reminding us of key moments from past episodes.A TV show recap is a concise, under-two-minute se-quence of crucial plot points from previous episodes. Tosatisfy the time constraint, the recap is constructed by edit-ing shots from previous episode with sharp and rapid cutsand selecting/modifying dialog utterances to ensure rele-vance to the sub-story. A good recap sets the stage for themain part of the episode by weaving visual and dialog cuesto spark the viewers memory. Thus, a recap is a great wayto identify sub-stories important to the overall story arc.We use recaps to create story summaries by identifyingand expanding the sub-stories from the episode ().We introduce an innovative shot-matching algorithm(Sec. 3) that associates shots from the recap to their cor-responding shots in the episode. Different from a recap,a story summary consists of entire scenes or sub-storiesthat are essential to the narrative. Thus, a first-time viewermay watch story summaries of each episode serially and un-derstand the main narrative, while watching recaps seriallydoes not help as they are only meant as memory triggers andassume that the viewer has seen the episode before.We propose a novel task of creating multimodal story",
  "DatasetModalities#LengthContentSummary Annotations": "SumMe V2V251-6 minHolidays, events, sportsMultiple set of key fragmentsTVSum V2V501-11 minNews, how-to, user-generated, documentary Multiple fragment-level scoresOVP V2V501-4 minDocumentary, educational, historical, lecture Multiple set of key-frames CNN-DailyMail T2T311672766 wordsNews articles and highlight storiesHuman-generated internet summariesXSum T2T226711431 wordsBBC News articlesSingle-sentence summary by authorTRIPOD T2T9922072 wordsMovies (action, romance, comedy, drama)Synopses level annotationsSummScreen T2T268517013 wordsTV screenplays (wide scope, 21 genres)Human-written internet summaries How2 VT2T791142-3 minInstructional videosYoutube descriptions (and translations)SummScreen3D VT2T45755721 wordsTV Shows (soap operas)Human written internet summariesBLiSS VT2VT1330310.1 min/49 words Livestream VideosHuman text-summaries; Thumbnail animationPlotSnap (Ours)VT2VT21540-45 minTV Shows (crime thriller)Matching recap shots followed by smoothing . Overview of video/text/multimodal summarization datasets. # indicates the size of the dataset (no. of instances). The modalitiescolumn includes: V2V: Video-to-video, T2T: Text-to-text, VT2T: Video-text to text, and VT2VT: Video-text to video-text summarization.Closest to our domain of story summarization are SummScreen and SummScreen3D, however, they produce text summaries. summaries for TV episodes. We introduce PlotSnap, a newdataset for story summarization consisting of two popularcrime thrillers: (i) 24 features Jack Bauer, an agent atthe counter-terrorism unit who relentlessly tackles seem-ingly impossible missions; and (ii) Prison Break fea-tures Michael Scofield who plans and executes daring es-capes from prisons. We choose action thrillers as they areoften more challenging than romantic and situational come-dies with multiple suspenseful story-lines, rapid action se-quences, and complex visual scenes. With excellent recapsin both shows, we can extract important narrative subplotsfrom the recap to create story summaries (see Sec. 3).Our task of story summarization is an instance of mul-timodal long-video understanding where an entire episode(typically 40 minutes) needs to be processed. We formulatestory summarization as an extractive multimodal summa-rization task with multimodal outputs (video-text to video-text, VT2VT). Specifically, we build models that predict theimportance of each video shot and dialog utterance (storyelements) in an episode. Selecting multiple major and con-nected sub-stories is different and challenging from mostsummarization works that promote visual diversity .We also propose a new hierarchical Transformer model,TaleSumm, to perform story summarization. Different fromtypical summarization approaches that usemultimodal inputs to either generate a video (select frames)or a text summary, our model predicts scores for bothmodalities. Recent multimodal approaches, A2Summ and VideoXum , also generates both outputs; butwe differ significantly in video type (stories vs. creativevideos), the duration of the input video, and the model ar-chitecture. The first level of our model encodes shot andutterance representations. At the second level, we fosterinteraction between shots and utterances within local storygroups based on a temporal neighborhood, reducing the im-pact of distant and potentially noisy elements. A dedicatedgroup token enables message-passing across story groups.In summary, our contributions are: (i) We propose story summarization that requires identifying and extracting mul-tiple plot points from narrative content. This is a challeng-ing multimodal long-video understanding task. (ii) We pi-oneer the use of TV show recaps for video understandingand show their application in story summarization. We in-troduce PlotSnap, a new dataset featuring 2 crime thrillerTV series with rich recaps. (iii) We propose a novel hier-archical model that features shot and dialog level encodersthat feed into an episode-level Transformer. The model op-erates on the full episode while being lightweight enough totrain on consumer GPUs. (iv) We present an extensive eval-uation: ablation studies validate our design choices, Tale-Summ obtains SoTA on PlotSnap and performs well onvideo summarization benchmarks. We show generalizationacross seasons and even across TV shows, and evaluate con-sistency of labels obtained from multiple diverse sources.",
  ". Related Work": "Video summarization predates Deep Learning (DL). Pastmethods focused on generating keyframes ,skims , video storyboards , time-lapses ,montages , or video synopses . However, given theeffectiveness of DL methods (e.g. ) overtraditional optimization-based approaches, we will primar-ily discuss learning-based approaches in the following. Summarization modalities.We classify approachesbased on input and output modalities.(i) Video toframes/video (V2V) approaches model temporal rela-tions , preserve diversity , or gen-erate images/videos . On the other, (ii) textto text (T2T) methods are either extractive picking important sentences from a document, or abstrac-tive summarizing the overall meaning by gen-erating new text . Relevant to our work, story screen-play summaries or turning point identification can be seen as T2T summarization.Multimodal approaches typically benefit from additional modalities to enhance model performance. (iii) Video-textto text (VT2T) is popular for screenplays , particu-larly in generating video captions . (iv) Video-text to video (VT2V) covers the field of query-guidedsummarization .Finally, the last option is(v) video-text to video-text (VT2VT) summarization. Ourwork lies here and is different from A2Summ andVideoXum , as we operate on long videos edited to con-vey complex stories. Different from trailer generation that avoids spoilers, we wish to identify all key story events. Summarization datasets. We compare popular summa-rization datasets based on above modalities in .Video-only datasets, TVSum and SumME , con-sist of short duration videos unlike ours.Other videodatasets work with first-person videos , are used fortitle generation , and even feature e-sports audiencereactions .For a nice overview of text-only (T2T)and text-primary (VT2T) datasets, we refer the readerto . Briefly, text datasets include news articles (CNN-DailyMail , XSum ), human dialog (Samsum ),and TV/movie screenplays (SummScreen ). While sim-ilar in spirit to screenplays used for storytelling, PlotSnap isdifferent as it features TV episodes with long videos and di-alogs (without speaker labels or scene descriptions), a sig-nificant challenge in long-form video understanding. Story summarization retrieves multiple sub-stories con-tained within the story-arc of an episode.To our bestknowledge, we are unaware of works on video-text story-summary generation. There are attempts to understand sto-ries in movies/TV shows through various dimensions: per-son identification , question-answering , captioning , situation understand-ing , text alignment ,or scene detection .Recently, Summ-Screen3D extends SummScreen with visual in-puts, but the output summary is still textual. On the otherhand, our goal is multimodal story-summary generation bypredicting both - important video shots and dialogs.",
  ". PlotSnap Dataset": "We introduce the PlotSnap dataset consisting of long-formmultimodal TV episodes with a well-structured underlyingplot spanning multiple seasons and episodes. We considertwo American crime thriller TV shows with rich storylines:24 and Prison Break (PB) . Unlike sitcoms, crimethrillers are recognized for their methodically crafted capti-vating plot lines. Notably, both 24 and Prison Break havegood recaps, and are famous for using the catchphrase Pre-viously on ... at the start of the recap.We present some statistics of PlotSnap in . Witha total of 205 episodes, the large number of shots and di-alogs present in each episode pose interesting challenges",
  ". Mean ( stddev) featuring properties of video shots, dia-log utterances, and the recap in our dataset PlotSnap": "for summarization. The first section of the table presentsoverall size and duration, second shows statistics for shotsand dialog utterances, and the third for recaps. We note thatrecap shots are much shorter (1.9s vs. 3.2s for 24) allowingthe editors to pack more story content in the same duration. Our key idea is to use professionally edited recaps, shownat the beginning of a new episode, as labels for story sum-marization. Let En be the nth episode in a TV series. Rn+1is the recap shown just before the episode En+1 begins andmay contain content from all past episodes {En, . . . , E1}.Thus, we classify the visual content appearing in the recapinto three sources along with their average proportions (for24): (i) shots that are picked (and usually trimmed) fromEn (88%), (ii) shots that are picked from En1 or earlier(5%), and (iii) new shots that did not appear in any previousepisode (7%). As most shots (88%) of a recap are from thepreceding episode, recaps serve as good summary labels.The remaining 12% recap content (from earlier episodes orunseen shots) is ignored. We also remove the last episodeof each season due to the absence of a recap. Recap inspired labels. We present how recap shots and di-alogs can be used to create labels for story summarization.First, we manually extract the recap (Rn+1) from En+1instead of employing automatic detection methods toavoid introducing additional label noise. Second, to local-ize trimmed recap shots in past episodes (E1, . . . , En), wepropose a shot-matching algorithm that conducts pairwisecomparisons of frame-level embeddings, making selectionsbased on a threshold determined by similarity score and fre-quency. Due to shot thread patterns , one recap shotmay match multiple shots in the episode. This is desirableas we want to highlight larger sub-stories as part of the sum-mary. In fact, selecting only one shot in a thread adverselyaffects the model due to conflicting signals as shots withvery similar appearance are assigned opposite labels.We think of recap matched shots as temporal point an-notations . We identify the set of matching shots in the episode, create a binary label vector, and smooth this vec-tor using a triangular filter. We will refer to these smoothedlabels as ground-truth (GT) for story summarization. Ex-tending the supervision helps the model identify meaning-ful, contiguous sub-stories rather than focusing solely onspecific shots highlighted in the recap. For example, it is un-likely that shot si is important to the story while si1 is en-tirely irrelevant (except at scene boundaries). Thus, smooth-ing is essential to clarify the distinction between positive(essential) and negative (unimportant) shots. Please refer toApps. A.1 and A.2 for details on the label creation process.A similar approach can be adopted for dialog utterances.We are able to match 88% of recap utterances to dialogwithin the smoothed video labels. The rest do not appearin episode En or are picked from extra recorded footage.For simplicity, we inherit labels for the dialogs based on thesmoothed label for the temporally co-occurring shot.",
  ". Method: TaleSumm": "We introduce TaleSumm, a two-level hierarchical modelthat identifies important sub-stories in a TV episodes nar-rative (illustrated in ). At the first level, our approachexploits frame-level (word-level) interactions to extract shot(dialog) representations (Sec. 4.2, (B, C)). At the sec-ond level, we capture cross-modal interactions across theentire episode through a Transformer encoder (Sec. 4.3,(A, D)). Before diving into the architecture, we for-malize story summarization and introduce notation.",
  "Our aim is to extract a multimodal story summary (videoand text) from a given episode, typically lasting around 40minutes, and encompassing multiple key events": "Notation. An episode E = (S, U) consists of a set of Nvideo shots S = {si}Ni=1 and a set of dialog utterancesU = {ul}Ml=1. A shot serves as a basic unit of video pro-cessing and comprises temporally contiguous frames takenfrom the same camera, while a dialog utterance typicallyrefers to a sentence uttered by an individual as part of alarger conversation. We denote each shot as si = {fij}Tij=1,where fij are sub-sampled frames, and each utterance asul = {wlp}Tlp=1 with multiple word tokens wlp. Summarization as importance scoring. While humansmay naturally select start and end temporal boundaries toindicate important sub-stories, for ease of computation, wediscretize time and associate an importance score with eachvideo shot or dialog utterance.Thus, given an episodeE = (S, U), we formulate story summarization as a binaryclassification task applied to each element (shot or dialog).The ground-truth labels can be denoted as yS = {ySi }Ni=1and yU = {yUl }Ml=1, where each ySi , yUl , signalingtheir importance to the story summary.",
  ". Level 1: Shot and Dialog Representations": "In narrative video production, shots play an important rolein advancing the storyline and contextualizing neighboringcontent. We obtain shot-level representations from granu-lar frame-level features to determine how well the shot cancontribute to understanding the storyline.Feature extraction. To capture various aspects of the shot,we use three pretrained backbones that capture visual di-versity through people, their actions, objects, places, andscenes: kS(), k = {1, 2, 3}. We extract relevant visual in-formation from frame(s) of a given shot, si as follows:",
  "wlp = FTU ({wlp}) ,wlp RDU .(2)": "Shot CLS pooling. To compute an aggregated shot rep-resentation, we combine frame-level signals into a com-pact representation.An attention-based aggregation ()(inspired by ), effectively weighs the most pertinent in-formation (e.g. action in a motion-heavy shot or sceneryin an establishing shot).First, the frame features fromdifferent backbones are projected to the same space (us-ing WkS RDDkS) and then concatenated to form f 1:3ijR3D (Eq. 3). A linear layer WP R33D followed bytanh and softmax computes scalar importance scores thatare used for weighted fusion:",
  "Fij = 1ijf 1ij + 2ijf 2ij + 3ijf 3ij .(5)": "We omit bias for brevity. We add relative frame positionto Fij through a time-embedding vector, ESj , similar toFourier position encoding .A shot transformer ST is used to encode the frame-level feature sequence {Fij}Tij=1. We tap the output fromthe CLS token appended at the beginning of the sequence(e.g. similar to BERT ) as the final shot representation:",
  "Pooling": ". (A) TaleSumm ingests all video shots and dialogs of the episode and encodes them using (B) and (C). Based on temporal order,we combine tokens into local story groups (illustration shows small groups of 2 shots and 0-2 utterances). To each group, we append agroup token and add multiple embeddings, before feeding them to the the episode-level Transformer (ET). For each shot or dialog token, alinear classifier predicts its importance. (B) Video shot encoder. For each frame, representations from multiple backbones are fused usingattention (). We feed these to a shot Transformer encoder ST, and tap a shot-level representation from the CLS token. (C) Utteranceencoder uses a fine-tuned language model and avg-pooling across all words of the utterance. (D) Self-attention mask illustrates theblock-diagonal self-attention structure across the episode. Group tokens across the episode (purple squares) communicate with each other.(E) Multiple embeddings are added to the tokens to capture modality type, time, and membership to a local story group.",
  ". Level 2: Episode-level Interactions": "We propose an episode-level Transformer encoder, ET, thatmodels interactions across shots and dialog of the entireepisode. Predicting the importance of an element (shot ordialog) requires context in a neighborhood; e.g. shot in ascene, dialog utterance in a conversation.Additional embeddings. We arrange shot and dialog to-kens based on their order in the episode (see (A)).Learnable type embeddings help the model distinguish be-tween shot and dialog modalities (EM R2D). We en-code the real time (in seconds) of appearance of each el-ement (shot or dialog) using a binning strategy. Given anepisode of T seconds, we initialize Fourier position encod-ings ET RT/D where is the bin-size. Based on themid-timestamp of each element t, we add ETt to the repre-sentation, the t/th row in the position encoding matrix.Such time embeddings allow our model to: (i) implicitly en-code shot duration; and (ii) relate co-occurring dialogs withvideo shots without the need for complex attention maps.Local story groups.The total number of video shotsand dialog that make up the sequence length for ET isS=N+M (1500). Self-attention over so many tokens isnot only computationally demanding, but also difficult totrain due to unrelated temporally distant tokens that happento look similar. We adopt a block-diagonal attention maskto constrain the tokens to attend to local story regions:",
  "ASS = diag(1n1n1, . . . , 1ngng, . . . , 1nGnG) ,(8)": "where 1ngng denotes an all one matrix, ng is the # of to-kens in the gth local block, Gg=1 ng = S, and diag(. . .)constructs a block diagonal matrix. We add new learnablegroup index embeddings EG RGD to our tokens to in-form our model about their group membership. Group tokens.While capturing interactions across alltokens may lead to poor performance, self-attention onlywithin the local story groups prohibits the model fromcapturing long-range story dependencies. To enable storygroup interactions, we propose to add a set of group tokensto the input, extending the sequence length to S=S+G. Thegroup tokens qg represent an additional layer of hierarchywithin the episode model as they summarize the story con-tent inside a group and also communicate across groups,providing a way to understand the continuity of the story.(E) shows how group tokens are inserted at the end ofeach local story groups shot and dialog tokens.To facilitate cross-group communication, we make twomodifications to the self-attention mask: (i) The size of eachlocal group ng is extended by 1 to incorporate the grouptoken qg within the block matrix. We also update A toreflect this and is of size S S. (ii) We compute a bi-nary index o {0, 1} S to represent the locations at which agroup token appears in the sequence. The new self-attentionmask A = A + ooT allows group-tokens to communicate.(D) illustrates the attention mask; light blue squarescorrespond to attention within a group, and sparse purplesquares visualize attention across the group tokens.",
  "qg=q + EGg .(11)": "where ti, tl and gi, gl correspond to the mid-timestamp andgroup membership of shot si and dialog ul respectively. qdenotes the learnable shared group type embedding.We feed the updated shot, dialog, and group token rep-resentations to ET post LayerNorm , a HE layer Trans-former encoder with a curated self-attention mask A:",
  "L = BCEyS, yS; wS+ BCEyU, yU; wU.(14)": "Inference. At test time, we follow the procedure outlinedin Sec. 4.3 and generate importance scores for each videoshot and dialog utterance (Eq. 13).Model ablations. As we will see empirically, our model isversatile and well-suited for adding/removing modalities oradditional representations by adjusting the sequence lengthof the Transformer (number of tokens). It can also be mod-ified to act as an unimodal model that applies only to videoor dialog utterances by disregarding other modalities.",
  ". Experiments and Analysis": "We first discuss the experimental setup.Data splits. We adopt 3 settings. (i) IntraCVT: On 24,most experiments (Tabs. 3 to 5) follow an intra-season 5-fold cross-validation-test strategy. (ii) X-Season: On 24,we assess cross-season generalization using a 7-fold cross-validation-test (Tab. 7). (iii) X-Series: shows transfer re-sults from 24 to PB (Tab. 7). More details can be found inApp. A.3.Evaluation metric. We adopt Average Precision (AP, areaunder PR curve) as the metric to compare predicted impor-tance scores of shots or dialogs against ground-truth.",
  "MLP42.3 42.3 42.3 42.4 42.5 35.7 35.735.8woG + FA 51.8 51.9 51.1 51.6 52.0 44.5 44.544.6wG + SA52.4 52.5 53.3 53.3 53.4 46.5 46.547.2": ". Rows demonstrate methods for capturing episode-levelinteractions. In an MLP, tokens are independent. woG+FA is aTransformer encoder that captures full-attention over the entireepisode without grouping; and wG+SA uses the proposed archi-tecture with local story groups and sparse-attention. Columns de-scribe the aggregation method used to combine frame (or token)level features into shot (or utterance) representation. C1 and C7use average pooling. C2 and C6 use max pooling. C3-C5 are vari-ants of ST: C3 concatenates backbone features of each frame, C4uses backbone features as separate tokens, and C5 uses proposed attention fusion. C8 uses the CLS token for dialog. Chosen: for shot, and average pooling for utterance representation.",
  "We present some high-level details here": "Feature backbones.We adopt three visual backbones:DenseNet169 for object understanding; MViT foraction information; and OpenAI CLIP for semantics.To encode dialog, we adapt RoBERTa-large forextractive summarization using parameter-efficient fine-tuning on the text from our dataset. The backboneis frozen when training TaleSumm for story summarization.Additional backbone details are in Apps. B.1 and B.2. Architecture. We find HS=1, HE=6, and ng=20 to workbest. Both ST and ET have the same configuration: 8 atten-tion heads and D=128. We tried several architecture con-figurations whose details can be found at App. B.1. Training details. We randomly sample up to 25 framesper shot during training as a form of data augmentation anduse uniform sampling during inference. Our model is im-plemented in PyTorch , has 1.94 M parameters, and istrained on 4 RTX-2080 Ti GPUs for a batch size of 4 (i.e. 4entire episodes). The optimizer, learning rate, dropout, andother hyperparameters are tuned for best performance onthe validation set and indicated in App. B.1.",
  ". Experiments on 24": "Architecture ablations. Results in Tab. 3 are across two di-mensions: (i) columns span the shot or utterance level and(ii) rows span the episode level. All model variants outper-form a baseline that predicts a random score between :AP 34.2 (video) and 30.4 (dialog), over 1000 trials.Across rows, we observe that the MLP performs worsethan the other two variants by almost 10% AP score be-cause assuming independence between story elements isbad. Our proposed approach with local story groups and",
  ". Comparison against SoTA video-only, text-only, and mul-timodal summarization models. Our approach outperforms previ-ous work by a significant margin": "sparse-attention (wG+SA) outperforms a vanilla encoderwithout groups and full-attention (woG+FA) by 1-2% onthe video model and 2-3% for the dialog model.Across columns, performance changes are minor. How-ever, when using wG+SA at the episode-level, gated atten-tion fusion with a shot transformer () improves resultsover Avg and Max pooling by 1%. For dialog-only, thoughwCLS outperforms Avg and Max by 0.7%, we adopt Avgpooling for its effective performance in a multimodal setup. TaleSumm ablations are presented in Tab. 4. Rows 1 and 2highlight the best video-only and dialog-only models (fromTab. 3). We report mean std dev on the val set. Std devis found to be high due to variation across multiple folds;but low across random seeds. Results for joint predictionof video shot and utterance importance are shown in rows3-6. Our proposed approach in row 6 performs best for bothmodalities, outperforming rows 3-5. SoTA comparison. We compare against SoTA methods:(i) video-only (PGLSUM , MSVA ), (ii) dialog-only(PreSumm ), and (iii) multimodal (A2Summ ) inTab. 5. While none of the above methods are built for pro-cessing 40 minutes of video, we make modifications to themto make them comparable to our work (details in the supple-ment). On both the validation and the test set, TaleSummoutperforms all other baselines in both modalities.",
  ". Analysis and Discussion": "Video summarization benchmarks.We evaluate Tale-Summ on SumMe and TVSum . However, bothdatasets are small (25 and 50 videos) and have short du-ration videos (few minutes). As splits and metrics are notcomparable across previous works, we re-ran the baselines.While MSVA uses three feature sets:i3d-rgb, i3d-flow and GoogleNet with intermediate fusion,PGLSUM uses GoogleNet and captures local and globalfeatures.In contrast, A2Summ aligns cross-modalinformation using dual-contrastive loss between video(GoogleNet features) and text (captions generated usingGPT-2 , embedded by RoBERTa at frame level).Similar to MSVA, we fuse all 3 features. Even thoughTaleSumm is built for long videos (group blocks, sparse at-tention), Tab. 6 shows that we achieve SoTA on SumMe.The drop in performance on TVSum may be due to videodiversity (documentaries, how-to videos, etc.).Generalization to a new season/TV series. Tab. 7 showsresults in two different setups. In X-Season, we see theimpact of evaluating on unseen seasons (in a 7-fold cross-val-test). While TaleSumm outperforms baselines, it is in-teresting that most methods show comparable performanceacross IntraCVT and X-Season setups (see Tabs. 5 and 7).In the X-Series setting, we train our model on 24 andevaluate on Prison Break. Although both series are crimethrillers, there are significant visual and editing differencesbetween the two shows. Our approach obtains good scoreson video summarization, and is a close second on dialog. Get on your feet.You're lying...Cheng attacked us. Cover me now.Milo was abraveman. Info sent toRussians... Lisa was injured...Orders, Jack.Go to CTU...",
  "Phillip Bauer": "is on line... Time (in minutes)010203040 H F GT Ours . TaleSumm predictions on S06E22 of 24 (test set). Ours filled-plot illustrates the importance score profile over time, whereorange patches indicate story segments selected for summarization. Annotations are shown below: ground-truth (GT), fandom (F), andhuman annotated (H). The story: Amid the high-stakes sequence depicted in the selected groups 1-3, Zhou Yongs team captures JoshBauer, leading to a firefight with Jack Bauer, who seeks Joshs location. Negotiations with Phillip Bauer over Joshs return for a vitalcircuit board escalate global tensions between Russia and the USA. Simultaneously, Mike Doyle defies Jacks wishes and departs with Joshby helicopter (segment 7). Parallely, Lisa, backed by Tom Lennox, confronts a Russian agent, leading to her injury (4, 6). Morris attemptsto console Nadia for Milos loss at CTU in 5. Escalating global tensions and the imminent showdown mark the episode.",
  ".Results on labels from 24 fan site (F) and human-annotated story summaries (H) averaged over 17 episodes of 24": "Label consistency. As suggested by , label con-sistency is crucial to evaluate summarization methods. Weassess PlotSnap using Cronbachs , pairwise F1-measure,and another agreement score: Fleiss .We obtain three sets of labels for 17 episodes of 24 (de-tails in App. A.4). (i) GT: obtained from matching recaps;(ii) F: maps plot events from a 24 fan site1 to videos; (iii) H:human response for a summary. Our labels have superiorconsistency compared to SumMe and TVSum (seeTab. 8), indicating that identifying key story events in aTV episode is less subjective than scoring importance forgeneric Youtube videos. Tab. 9 shows the results for thebaselines and TaleSumm on the two other labels F and H.",
  ". Conclusion": "Our work pioneered the use of TV episode recaps for storyunderstanding.We proposed PlotSnap, a dataset of twoTV shows with high-quality recaps, leveraging them forstory summarization labels, while showing high consistencyacross labeling approaches. We introduced TaleSumm, a hi-erarchical summarization approach that captures and com-presses shots and dialog, and enables cross-modal interac-tions across the entire episode, trainable on a single GPUof 12 GB. We performed thorough ablations, establishedSoTA performance, and demonstrated transfer across sea-sons, other series, and even movie genres. For reproducibil-ity and encouraging future work, we will release the codeand share the dataset, as keyframes, features, and labels.Limitations and future work. While our current work fo-cuses on recaps obtained from a limited genre and two TVseries, we believe the approach should be scalable to addi-tional genres and datasets. Early experiments in evaluatingour model on condensed movies (CMD) show limitedimprovements. Our approach to story summarization doesnot explicitly model the presence of characters (e.g. via per-son and face tracks and their emotions ) which are cen-tral to any story and this can be an important direction forfuture work. Additional discussions are provided in the sup-plementary material. Acknowledgments.We thank the Bank of Baroda for partialtravel support, and IIIT-Hs faculty seed grant and Adobe ResearchIndia for funding. Special thanks to Varun Gupta for assisting withexperiments and the Katha-AI group members for user studies.",
  "English Wikipedia. Wikipedia, 2021 Present. 17": "Evlampios Apostolidis,Georgios Balaouras,VasileiosMezaris, and Ioannis Patras. Combining Global and Lo-cal Attention with Positional Encoding for Video Summa-rization. In IEEE International Symposium on Multimedia(ISM), 2021. 7, 16, 21 George Awad, Keith Curtis, Asad A. Butt, Jonathan Fis-cus, Afzal Godil, Yooyoung Lee, Andrew Delgado, JesseZhang, Eliot Godard, Baptiste Chocot, Lukas Diduch,Jeffrey Liu, Yvette Graham, , and Georges Quenot.An overview on the evaluated video retrieval tasks atTRECVID 2022. In Proceedings of TRECVID, 2022. 2,3",
  "Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gim-pel. SummScreen: A Dataset for Abstractive ScreenplaySummarization. In Association of Computational Linguis-tics (ACL), 2022. 2, 3": "Shixing Chen, Chun-Hao Liu, Xiang Hao, Xiaohan Nie,Maxim Arap, and Raffay Hamid. Movies2Scenes: Usingmovie metadata to learn scene representation. In Confer-ence on Computer Vision and Pattern Recognition (CVPR),2023. 3 Guilhem Cheron, Jean-Baptiste Alayrac, Ivan Laptev, andCordelia Schmid.A Flexible Model for Training Ac-tion Localization with Varying Levels of Supervision.In Advances in Neural Information Processing Systems(NeurIPS), 2018. 3 Sandra Eliza Fontes de Avila, Ana Paula Brandao Lopes,Antonio da Luz, and Arnaldo de Albuquerque Araujo.VSUMM: A mechanism designed to produce static videosummaries and a novel evaluation method. Pattern Recog-nition Letters, 32(1):5668, 2011. 2",
  "Tsu-Jui Fu, Shao-Heng Tai, and Hwann-Tzong Chen. At-tentive and adversarial learning for video summarization.In Winter Conference on Applications of Computer Vision(WACV), 2019. 2": "Xiyan Fu, Jun Wang, and Zhenglu Yang. MM-AVS: A Full-Scale Dataset for Multi-modal Summarization. In NorthAmerican Chapter of Association of Computational Lin-guistics: Human Language Technologies (NAACL-HLT),2021. 2 Junaid Ahmed Ghauri, Sherzod Hakimov, and Ralph Ew-erth. Supervised Video Summarization Via Multiple Fea-ture Sets with Parallel Attention. IEEE International Con-ference on Multimedia and Expo (ICME), pages 16s,2021. 7, 16, 20, 22 Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-sander Wawer. SAMSum Corpus: A Human-annotated Di-alogue Dataset for Abstractive Summarization. In Proceed-ings of the 2nd Workshop on New Frontiers in Summariza-tion, 2019. 3",
  "Xiang Hao, Kripa Chettiar, Ben Cheung, Vernon Germano,and Raffay Hamid. Intro and Recap Detection for Moviesand TV Series. In Winter Conference on Applications ofComputer Vision (WACV), 2021. 3": "Monica-Laura Haurilet, Makarand Tapaswi, Ziad Al-Halah, and Rainer Stiefelhagen. Naming TV characters bywatching and analyzing dialogs. In Winter Conference onApplications of Computer Vision (WACV), 2016. 3 Bo He, Jun Wang, Jielin Qiu, Trung Bui, Abhinav Shrivas-tava, and Zhaowen Wang. Align and Attend: MultimodalSummarization with Dual Contrastive Losses. In Confer-ence on Computer Vision and Pattern Recognition (CVPR),2023. 2, 3, 7, 16, 21, 22",
  "Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal.TVQA+: Spatio-Temporal Grounding for Video QuestionAnswering. In Association of Computational Linguistics(ACL), 2020. 3": "Hector J. Levesque, Ernest Davis, and Leora Morgen-stern.The winograd schema challenge.In Proceedingsof the Thirteenth International Conference on Principles ofKnowledge Representation and Reasoning, 2012. 17 MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,Abdelrahman Mohamed,Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. BART: Denois-ing sequence-to-sequence pre-training for natural languagegeneration, translation, and comprehension. In Associationof Computational Linguistics (ACL), 2020. 2 Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.Hoi.BLIP: Bootstrapping Language-Image Pre-trainingfor Unified Vision-Language Understanding and Genera-tion. In International Conference on Machine Learning,2022. 22",
  "Jae Sung Park, Trevor Darrell, and Anna Rohrbach.Identity-Aware Multi-Sentence Video Description. In Eu-ropean Conference on Computer Vision (ECCV), 2020. 3": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,Lu Fang, Junjie Bai, and Soumith Chintala.PyTorch:An Imperative Style, High-Performance Deep Learning Li-brary. In Advances in Neural Information Processing Sys-tems (NeurIPS), 2019. 6",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al.Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9,2019. 7": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In International Conference on Machine Learning(ICML). PMLR, 2021. 6, 17, 18 Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,and Peter J Liu. Exploring the limits of transfer learningwith a unified text-to-text transformer. Journal of MachineLearning Research (JMLR), 2020. 2",
  "Anna Rohrbach, Marcus Rohrbach, Niket Tandon, andBernt Schiele. A dataset for Movie Description. In Confer-ence on Computer Vision and Pattern Recognition (CVPR),2015. 3": "AnnaRohrbach,AtousaTorabi,MarcusRohrbach,Niket Tandon, Christopher Pal, Hugo Larochelle, AaronCourville, and Bernt Schiele. Movie Description. Interna-tional Journal of Computer Vision (IJCV), 123(1):94120,2017. 3 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexander C.Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recog-nition Challenge. International Journal of Computer Vision(IJCV), pages 211252, 2015. 14, 17 Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia,and Aniruddha Kembhavi. Visual Semantic Role Labelingfor Video Understanding. In Conference on Computer Vi-sion and Pattern Recognition (CVPR), 2021. 3 RamonSanabria,OzanCaglayan,ShrutiPalaskar,Desmond Elliott, Loc Barrault, Lucia Specia, and FlorianMetze. How2: A Large-scale Dataset for Multimodal Lan-guage Understanding. In Advances in Neural InformationProcessing Systems-Workshop (NeurIPS-W), 2018. 2, 3",
  "Leslie N. Smith and Nicholay Topin. Super-convergence:very fast training of neural networks using large learningrates. In Defense + Commercial Sensing, 2018. 17": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-YanLiu. MPNet: Masked and Permuted Pre-Training for Lan-guage Understanding. In Advances in Neural InformationProcessing Systems (NeurIPS), 2020. 18, 21 Yale Song, Jordi Vallmitjana, Amanda Stent, and AlejandroJaimes. TVSum: Summarizing web videos using titles. InConference on Computer Vision and Pattern Recognition(CVPR), 2015. 2, 3, 7, 8, 14, 22, 23",
  "Min Sun, Ali Farhadi, Ben Taskar, and Steve Seitz. Salientmontages from unconstrained videos. In European Confer-ence on Computer Vision (ECCV), 2014. 2": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,Scott Reed, Dragomir Anguelov, Dumitru Erhan, VincentVanhoucke, and Andrew Rabinovich. Going deeper withconvolutions. In Conference on Computer Vision and Pat-tern Recognition (CVPR), 2015. 7 Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe Laban,Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, JustinRousseau, and Greg Durrett.Understanding Factual Er-rors in Summarization: Errors, Summarizers, Datasets, Er-ror Detectors. In Association of Computational Linguistics(ACL), 2023. 23",
  "Timeline. In Conference on Computer Vision and PatternRecognition (CVPR), 2014. 3, 14": "Makarand Tapaswi, Martin Bauml, and Rainer Stiefelha-gen. Aligning plot synopses to videos for story-based re-trieval. International Journal of Multimedia InformationRetrieval (IJMIR), 4(1):316, 2015. 3 Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,Antonio Torralba, Raquel Urtasun, and Sanja Fidler.MovieQA: Understanding Stories in Movies throughQuestion-Answering. In Conference on Computer Visionand Pattern Recognition (CVPR), 2016. 3 Tengda Han and Max Bain and Arsha Nagrani and GulVarol and Weidi Xie and Andrew Zisserman. AutoAD II:The Sequel - Who, When, and What in Movie Audio De-scription. In International Conference on Computer Vision(ICCV), 2023. 3",
  "Trieu H. Trinh and Quoc V. Le. A Simple Method for Com-monsense Reasoning. ArXiv, abs/1806.02847, 2018. 17": "Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin.Attention is All you Need.In Advances in Neural Information Processing Systems(NeurIPS), 2017. 4 Paul Vicol, Makarand Tapaswi, Lluis Castrejon, and SanjaFidler.MovieGraphs: Towards Understanding Human-Centric Situations from Videos. In Conference on Com-puter Vision and Pattern Recognition (CVPR), 2018. 3 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-mond, Clement Delangue, Anthony Moi, Pierric Cistac,Tim Rault, Remi Louf, Morgan Funtowicz, et al.Hug-gingfaces transformers: State-of-the-art natural languageprocessing. arXiv preprint arXiv:1910.03771, 2019. 18",
  "Wencheng Zhu, Yucheng Han, Jiwen Lu, and Jie Zhou. Re-lational reasoning over spatial-temporal graphs for videosummarization. IEEE Transactions on Image Processing,31:30173031, 2022. 2": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhut-dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.Aligning Books and Movies: Towards Story-like Visual Ex-planations by Watching Movies and Reading Books.InarXiv preprint arXiv:1506.06724, 2015. 17 Yukun Zhu, Ryan Kiros, Richard S. Zemel, RuslanSalakhutdinov, Raquel Urtasun, Antonio Torralba, andSanja Fidler. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and ReadingBooks. In International Conference on Computer Vision(ICCV), 2015. 3 Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. A RobustlyOptimized BERT Pre-training Approach with Post-training.In Proceedings of the 20th Chinese National Conference onComputational Linguistics, 2021. 6, 7, 17, 18, 21 We provide additional information and experimental resultsto complement the main paper submission. In this work, wepropose a dataset PlotSnap consisting of two crime thrillerTV shows with rich recaps and a new hierarchical modelTaleSumm that selects important local story groups to forma representative story summary. We present details of Plot-Snap in App. A. App. B extends on additional experimen-tation on PlotSnap as well as SumMe and TVSum followed by an extensive qualitative study. Finally, we con-clude this supplement with a discussion of future researchdirections in App. C.",
  "A. Dataset Details": "We start with the essential step of creating soft labels foreach shot and dialog of the episode, presented in App. A.1.We describe how recaps are used to generate binary labelsusing our novel shot-matching algorithm. Next, App. A.2describes how label smoothing is performed. This is anessential step towards capturing the local sub-story in abetter way. Further, App. A.3 comments on the differentdata split creation strategies used to evaluate baselines andour models generalizability. Finally, to conclude App. A.4presents detailed episode-level label reliability scores.",
  "A.1. Shot Matching": "We propose a novel shot-matching algorithm whose work-ing principle involves frame-level similarities to obtainmatches.First,wecomputeframe-levelembeddingsusingDenseNet169 , which were found to work better thanmodels such as ResNet pre-trained on ImageNet based on a qualitative analysis. An example is shown in where we see higher similarity between retrievedepisode frames and the query frame from the recap.Second, we discuss how these embeddings are used toobtain matches is detailed below.",
  ". Retrieval results for Recap from Episode Frames withDenseNet (Top) v/s ResNet (bottom). We observe qualitativelythat DenseNet is able to match to the correct frames from theepisode more often": "Matching. For a given recap shot s in Rn+1 we compareit against multiple frames in the episode En, and compute amatrix dot-product with appropriate normalization (cosinesimilarity) between respective frame representations of therecap and episode as illustrated in the toy-example of . We remove very dark or very bright frames, typical in poorlighting conditions or glares, to avoid spurious matches andnoisy labels.Next, we choose a high threshold to identify matchingframes (0.85 in our case after analysis) and fetch all thetop matching frames along with their shot indices (the shotwhere the frame is sourced from). We compute a set unionover all matched shots obtained by scoring similarities be-tween the recap frame of shot s and denote this set as Ss. Inthe example, we match 3 frames of a recap shot and identifyseveral episode shots shown in the blue box with Ss.Weeding out spurious shot matches. The set Ss may con-tain shots beyond a typical shot thread pattern due to spuri-ous matches. These need to be removed to prevent wrongimportance scores from being obtained from the recap. Todo this, we first find the best matching shot in the episode.We observe that taking top three matched frames for ev-ery recap frame results in strong matches. Subsequently,we pick the maximum similarity score for each unique shotmatched to a recap frame. This allows us to accumulate thescore for an episode shot if multiple frames of the episodeshot match with frames of the recap shot. The shot thatscores the highest (after summing up the scores) is consid-ered the best-matched episode shot for recap shot s.Next, we choose a window size of 21 (10 on either side)and include all shots in Ss that fall within this window to anew matched set, Ns. This is motivated by the typical du-ration of a scene in a movie or TV episode (40-60 seconds)and an average shot duration of 2-3 seconds. We repeat thisprocess until no more shots are added to the set Ns and dis-card the rest in Ss. Thus, for a given shot from the recap, weobtain all matching shots in the episode that are localized toa certain region of high-scoring similarity (see ). Werepeat this process for all frames and shots from the recap.",
  "A.2. Label Smoothing": "The intuition behind extending the recap matched shots ob-tained in the previous section is to include chunks of thesub-story that are important to the storyline. While a shortrecap (intended to bring back memories) only selects a fewshots, a story summary should present the larger sub-story.Selecting only one shot in a thread also adversely af-fects model training due to conflicting signals, as multipleshots with similar appearance can have opposite labels. La-bel smoothing solves both these issues.Triangle Smoother. We hypothesize that the importance ofshots neighboring a matched shot are usually quite high anduse a simple triangle smoother to re-label the importanceof shots. In particular, we slide a window of size w center-ing at positive labels and set the importance of neighboringshots according to height of the triangle. The above processis illustrated in as the first step. In the second step,we add and clip the soft labels of strongly overlapping re-",
  "Similarity Matrix": "{770, 769, 761, 523, 513} Valid frame encodings Shape: 3 x 1664 Valid frame encodings Shape: 18 x 1664 Matrix Dot-Product(Cosine Similarity) Sort and retrieve top matching frames having scores > 0.85 770 770 761 523 769 770 Similarity table of size 3 x 18. Cellconsists of corresponding shot numbersorted on the basis of similarity score.Gray boxes indicates scores < 0.85 Set union of all such selectedshot-indices for all 3 frames. Find the best Episode Shot: For each recap frame:1. Take top 3 matches.2. Choose episode framescorresponding to a unique shot andnote their maximum similarity score. Matched episode shots for recap shot 770 770 761 769 770 For each row unique shots are consideredalong with their maximum similarity score(color=yellowish orange) Sum up the scores in the listscoreshot-indexfor each chosen shot Shot matches 3, score770 769 1, score769 1, scores513513 Summary tuple of candidate shotscompeting for best matching shot. Firstentry for each shot index denotes itsfrequency, while the second is the list ofmatching scores.Best Shot: 770 CHECK? (Statement in dashed box)",
  "Embeddings": "{770} For illustration purpose, n=3 and m=18 7611, score761 The set of all }ltered episodematching shots for recap shot .Initial input is a singleton-set shot-index , collectall satisfying and denote it as . CHECK if ? . Flowchart for identifying shots from the episode that appear in a recap and can be used as weak labels for story summarization.The process involves identifying the list of high-scoring matching frames, indexing the shots, and then preventing spurious matches bylooking for high-scoring matches within a bounded duration. The flowchart presents an example of the process used to identify the set ofshots Ns from the episode that match to the recap shot.",
  "For": ". Triangle smoothing process. Here x-axis denotes binary labels derived from the shot matching process, while y-axis shows softimportance scores used to train our model. Top: The triangular filter is applied at each shot selected from the matching process. Scoresof shots falling within the window are updated. Bottom: In the second step, we add shot importance derived potentially from multipleoverlapping triangle filters. This typically happens when episode shots in close proximity are matched to the recap. gions to prevent any score from going higher than 1. Forthe sake of simplicity, we used triangle smoothing, how-ever, one could also use other filters.We choose w = 17 by analyzing the spread of the shotsand their importance scores and comparing them against afew episodes for which we manually annotated the storysummaries. Dialog labels. The above smoothing procedure generatessoft labels for video shots. For dialog utterances, we sim-ply import the score of the shot that encompasses the mid-timestamp of the dialog. The key assumption here is thatthe dialog utterance associated with the matched shot is alsoimportant.",
  "A.3. Data Splits": "For evaluation of our approach, PlotSnap is split into 4 typesof splits as described below:1. IntraCVT (Intra-Season 5 fold cross-val-test) represents5 different non-overlapping splits from 7 seasons of 24(Season 2 to 8). Intra-season means episodes from eachseason are present in the train/val/test splits. For exam-ple, split-1 uses 5 episodes from the end of each seasonfor val and test (2, 3 respectively). Likewise, Split-5 (thefifth fold) uses episodes from the beginning of the seasonin val/test. We observe that Split-5 is harder. 2. X-Season (Cross-Season 7 fold cross-val-test) involves 7non-overlapping splits with one season entirely kept fortesting (from Season 2 to 8), while the train and val use18 and 5 episodes respectively from each season. Thisstrategy is used to test for the generalization of our modelon different seasons of the same series, 24.",
  "A.4. Label Consistency": "As discussed in Tab. 8 (of the main paper), we show theagreement between story summary labels obtained fromdifferent sources. Consistency is evaluated on 3 kinds oflabels: From recap (GT), Fandom (F) and Human (H). Weassess the consistency of labels via Cronbachs , PairwiseF1, and Fleiss statistics.Tab. 10 expands on the individual scores obtained foreach of the 17 episodes. We observe that Cronbachs isconsistently high, while Fleisss varies typically between0.2-0.5 indicating fair to moderate agreement.",
  "B. Experiments and Results": "In this section we expand our implementation details(App. B.1) and present additional details of the backbonesused for feature extraction (App. B.2). Extensive ablationstudies with regard to feature combinations and model hy-perparameters are presented in App. B.3.In App. B.4, we present details of the modifica-tions that need to be made to adapt SoTA baselinessuch as MSVA , PGL-SUM , PreSumm , andA2Summ for comparison with TaleSumm. App. B.5details how we adapted our model for SumMe and TVSum,along with comprehensive hyperparameter particulars. To",
  "B.1. Implementation Details": "Visual features.We first segment the episode intoshots using .We adopt 3 specialized backbones(for their combined effective performance;shown inTab. 11):(i) DenseNet169 pre-trained on Ima-geNet , SVHN , and CIFAR for object se-mantics; (ii) MViT pre-trained on Kinetics-400 foraction information; and (iii) OpenAI CLIP , pre-trainedon 4M image-text pairs, for semantic information. Utterance features. We adapt RoBERTa-large orig-inally pretrained on the reunion of five datasets: (i) Book-Corpus , a dataset consisting of 11,038 unpublishedbooks; (ii) English Wikipedia (excluding lists, tablesand headers); (iii) CC-News , a dataset containing 63millions English news articles crawled between September2016 and February 2019; (iv) OpenWebText , an open-source recreation of the WebText dataset used to train GPT-2; and (v) Stories a dataset containing a subset ofCommonCrawl data filtered to match the story-like styleof Winograd schemas . Together these datasets weigh160GB of text.Given dialogs from the episode, our fine-tuning ob-",
  "jective is to predict the important dialogs.We extractword/token-level representations (w) from finetuned (butfrozen) RoBERTa-large (FTU ) for the task of dialog storysummarization": "Frame sampling strategy. We randomly sample up to 25frames per shot during training as a form of data augmenta-tion. During inference, we use uniform sampling. We usedfourier position embeddings ESj for indexing video frames. Architecture details. We experiment with the number oflayers for ST, HS[1 : 3] and ET, HE[1 : 9], and findHS=1 and HE=6 to work best. Except the number of lay-ers, ST and ET have the same configuration: 8 attentionheads and D=128. Appropriate padding and masking isused to create batches. We compare multiple local storygroup sizes ng {5 : 30 : 5} and find ng=20 to work best. Training details.Our model is trained on 4 RTX-2080Ti GPUs for a maximum of 65 epochs with a batch sizeof 4 (i.e. 4 entire episodes each GPU handling oneepisode). We adopt the AdamW optimizer with pa-rameters: learning rate=104, weight decay=103.Weuse OneCycleLR as learning rate scheduler with maxlr=103, and multiple dropouts : 0.1 for projection to128 dim inside video and utterance encoder; 0.2 for atten-tion layers; and 0.2 for the classification head. The hyperpa-rameters are tuned for best performance on validation set.",
  "Visual Feature Backbones We present the details for threebackbones capturing different aspects of a video": "DenseNet169 f 1.Feature extraction of salient ob-jects/person in each frame is of utmost priority and isachieved through DenseNet169 pretrained on Ima-geNet , SVHN , and CIFAR . We considerthe frozen backbone without the linear classification headto obtain flattened features, f 1 R1664. Before feeding theimages, we apply a few preprocessing steps to sub-sampleraw images.1. Frames are resized to 256256 resolution along withcenter cropping.",
  "Dialog Features We test three different dialog features andpresent the details as follows": "Fine-tuning Language Models.We fine-tune lan-guage models such as PEGASUSLARGE , RoBERTa-Large , and MPNet-base-v2 for our task of extrac-tive dialog summarization. To account for the small datasetsizes, for all fine-tuning, we use the Adapter modules that add only a few trainable parameters in the form ofdown- and up-projection layers as illustrated in .",
  "RoBERTa-Large and MPNet-base-v2 are fine-tuned for utterance-level binary classification to decidewhether the dialog utterance is important or not": "PEGASUSLARGE is trained originally to generate ab-stractive summaries. Instead, we adapt it to generate sum-mary dialogs. As the number of tokens accepted by thePEGASUSLARGE model does not allow feeding all the di-alog from the episode, we break it into 6 chunks. Word-level embeddings. We use last hidden-state of theencoder to obtain contextualized word-level embeddings,w R1024 (768 for MPNet-base-v2). PEGASUSLARGE,being a generative model (consisting of both encoder anddecoder), we keep only the encoder portion for word-levelfeature extraction.",
  "Layer": ". Architecture of the adapter module and its inte-gration with language models encoder layer. Left: The adaptermodule has fewer parameters compared to the attention and feed-forward layers in a Transformer layer and consists of a bottleneckand skip-connection. Right: We add the adapter module twiceto each encoder layer. Once after the multi-head attention and theother is placed after the two feed-forward layers typical of a Trans-former architecture. For the Transformer decoder, in the case ofPEGASUSLARGE, we add one extra adapter after cross-attention aswell. Adaptation: When adapting the model, the purple and greenlayers illustrated on the right module are trained on the down-stream data and task, while the other blocks are frozen.",
  "All experiments are run on IntraCVT split, and we reportmean and standard deviation": "Visual features for Story summarization. showsthe results for all combinations of the three chosen visualfeature backbones in a multimodal setup.We draw twomain conclusions: (i) The Shot Transformer encoder (ST)shows improvements when compared to simple average ormax pooling. (ii) DMC (DenseNet + MViT + CLIP), thefeature combination that uses all backbones, performs well,while MC shows on-par performance. Importantly, the fea-ture combination is better than using any feature alone. Language backbones for Story summarization. Differentfrom the previous experiment, shows results fordifferent dialog features with different word-to-utterancepooling approaches in a multimodal setting.RoBERTa-",
  "Dialog Average Precision": "# layers = 1# layers = 2# layers = 3# layers = 4# layers = 5# layers = 6# layers = 7# layers = 8# layers = 9 . Performance of TaleSumm for varying local story group size (also referred to as window size) and number of layers in theepisode-level Transformer ET. y-axis denotes the AP score for the Left: Video and Right: Dialog. We observe that a 6 layer modelHE = 6 works well together with a local story group size of ng = 20.",
  "CVid AP53.9 3.753.9 3.954.1 3.7---Dlg AP48.6 4.248.7 4.248.7 4.0---": ". TaleSumm ablations for different feature combination strategies, using both video and dialog modalities. Feature ablations areperformed over the visual modality. Columns describe the Pooling approaches used to form shot-level from frame-level representations;Concatenate corresponds to how backbone feature are combined (concatenate or as separate tokens (NC)). Stack pooling is an alternativeapproach to , where instead of condensing the aggregated (concatenated) visual features via a linear layer WP R33D, we obtainindividual feature importance score (with WP R1D followed by tanh and softmax). D: DenseNet169, M: MViT, and C: CLIP. Allresults are for TaleSumm that captures Episode level interactions.",
  "Time (in minutes)010203040": ". TaleSumm predictions on S07E22 of 24 (test set). Ours filled-plot illustrates the importance score profile over time, whereorange patches indicate story segments selected for summarization. Annotations are shown below: ground-truth (GT), fandom (F) andhuman annotated (H). We number the grouped frames representing the predicted contiguous orange chunks as shot groups (SG-n), e.g. thisepisode has 8 SGs. The story: In a tense sequence, as shown in SG-1, Jack resorts to torture to extract information from Harbinsonabout the impending attack but is left empty-handed. In SG-2, following the murder of Jonas Hodges, Olivia Taylor faces scrutiny fromthe Justice Department. Meeting with Martin Collier, she denies transferring funds, revealing a sinister plot. Meanwhile, SG-3 showsKim Bauers plans are disrupted by a flight delay, leading to a strained father-daughter relationship. SG-4,5 displays how Jack, aided byChloe OBrian and Renee Walker, captures Tony Almeida and interrogates him about a dangerous canister, followed by Renee uncoveringJibraans location, and a high-stakes exchange ensues at the Washington Center station. Jack detonates the canister, succumbing to itseffects. As a consequence (SG-6), Cara Bowden reports Tonys failure to Alan Wilson, adding tension to the unfolding crisis. Oliviareturns to the White House, explaining her absence to Aaron Pierce (SG-7), which beautifully connects back to the SG-2. The narrativetakes a dire turn as Cara blackmails Jack for the safety of Kim (SG-8), introducing a new layer of suspense and complexity to the unfoldingevents. The presence of SG-1,6,7 (absent in GT) clearly highlights our models ability to complete the overall story arc.",
  "In this section, we discuss how we adapt different video-and dialog-only state-of-the-art baselines for our task": "MSVA considers frame-level features from multiplesources and applies aperture-guided attention across allsuch feature streams independently, followed by intermedi-ate fusion and a linear classification head that selects framesbased on predicted importance scores. Since we are model- ing at the level of the entire episode, we feed condensedshot features (after Avg or Max pooling or ST) of eachbackbone: DenseNet169 (Xo), MViT (Xr), and CLIP (Xf)through three different input streams and output shot-levelimportance scores. Our model is different from MSVA as MSVA treats eachfeature separately, while we perform early fusion and con-catenate representations from multiple backbones even be-fore obtaining a compact video shot representation. Tale-",
  "- ...How sorry I am for everything...- If recording of yours leak,...compromise national security.- We'll be ready?": "H F GT Ours . TaleSumm predictions on S05E21 of 24 (test set). Ours filled-plot illustrates the importance score profile over time, whereorange patches indicate story segments selected for summarization. Annotations are shown below: ground-truth (GT), fandom (F), andhuman annotated (H). We number the grouped frames representing the predicted contiguous orange chunks as shot groups (SG-n), e.g. thisepisode has 5 SGs. This episode stands out due to its rapid and significant story advancements, where each sub-story holds apparentimportance. Also, human annotations are a bit off in comparison to the ground-truth (can be verified from reliability scores shownin Tab. 10). Importantly, our model considers opinions from all the sources. The story: In the SG-1,2, President Logan, pretendingsurprised, learns from Admiral Kirkland that Flight 520, now under Jacks control, is a potential threat. Despite Mikes doubts about Jacksintentions, Kirkland urges immediate action, advocating for shooting down the plane. Logan, pretending shock, reluctantly authorizes theattack. Karen alerts Jack to the order, leading to a tense situation. As the plane assumes a landing profile, Kirkland suggests calling offthe strike, but Pres. Logan insists on taking it down. Further, in SG-3, Graem criticizes Logans decision, emphasizing the importance ofcapturing Jack, but Logan assures Graem of recapturing him. Meanwhile, in SG-4, Jack, having secured incriminating evidence, vows tomake Logan pay for President Palmers assassination. In a surprising turn, as shown in SG-5, President Logan contemplates suicide, butan unexpected call from Miles Papazian presents an alternative the destruction of the recording. Encouraging Miles to act, Logan faces acritical juncture in the unfolding crisis. Overall the entire episode sets the stage for a series of dramatic events, stressing the depth of deceitand the potential consequences for key characters.",
  "Summ is also developed for encoding and making predic-tions on multiple modalities, while MSVA is not": "PGL-SUM splits the video in small equal-sized groupof frames. Similar to our work, contextualization is per-formed within local multi-headed self-attention on smallgroups, while another is at global level using global multi-headed attention for the entire video. Later, both are mergedvia addition along the feature axis and subsequently passedthrough an MLP classification head to obtain frame-levelscores. To adapt PGL-SUM to our work, we again think ofshots as the basic unit and concatenate visual features fromall streams (f 1, f 2, and f 3), followed by pooling (ST or Maxor Avg pooling) and then PGL-SUM to generate shot-levelimportance scores.PGL-SUM has some similarities to our approach asboth involve local groups. Interestingly, PGL-SUM creates groups of frames to perform summarization for short videosof a few minutes, while TaleSumm creates groups of shotsand dialog utterances to generate summaries for 40 minutelong episodes. Among technical contributions, we also ex-plore different attention mechanisms such as across the full-episode (FE) or within a local story group (SG). Differentfrom PGL-SUM, we introduce a story group token that al-lows to capture the essence of a local story group. PreSumm is used for text-only extractive summariza-tion which takes word-level inputs and produces sentence-level probability scores. To represent each episode, the ut-terances are concatenated, lower-cased, and separated byCLS and SEP tokens into a single line input. The PreSummmodel leverages word embeddings from pre-trained BERT-base language model. Considering the long inputs inour case, we extend the existing positional embeddings ofBERT from 512 to 10000 by keeping the original embed-dings and replicating the last embeddings for the remain-der. At the sentence level, the corresponding CLS token isfed into two transformer encoder layers for contextualiza-tion, followed by a small MLP and sigmoid operation togenerate per-sentence scores. The model is trained usingthe Adam optimizer with Binary Cross-Entropy loss.PreSumm is very different from our work as it operatesdirectly on tokens, while our model develops a hierarchicalapproach going from words to dialogs to local story groups.",
  "A2Summ is a contemporary multimodal summariza-": "tion (MSMO) baseline, primarily designed to align tempo-ral correspondence between video and text signals througha dual-contrastive loss approach.They also introduce adataset, BLiSS , comprising 13,303 pairs of livestreamvideos and transcripts, each with an average duration of5 minutes, along with multimodal (VT2VT) summaries.A2Summ exploits cross-modality correlations within andbetween videos through dual contrastive losses. These in-clude: (a) inter-sample contrastive loss (operates across dif-ferent sample pairs within a batch, leveraging the intrin-sic correlations between video-text pairs), and (b) an intra-sample contrastive loss (works within each sample pair,emphasizing the similarities between ground-truth videoand text summaries while contrasting positive features withhard-negative features).We adapt A2Summ for PlotSnap, where we work atthe episodic level, by using max-pooling with an MLP forvideo features and average-pooling for dialog (text) fea-tures to derive shot- and utterance-level representations.We create explicit intra-sample attention masks encourag-ing temporal alignment, allowing video shots to attend totheir corresponding utterances and permitting video andutterance tokens to fully attend to their respective coun-terparts. Considering memory constraints, we maintain abatch size of 4 (four entire episodes - inter-sample con-trasting) on a single NVIDIA GeForce RTX-2080 Ti GPU.We adopt CyclicLR with a maximum learning rate of104 and the triangular2 mode. A2Summ model com-prises 6 encoder layers and incorporates multiple dropoutlayers : (a) dropout video =0.1, (b) dropout text=0.2, (c) dropout attn =0.3, and (d) dropout fc =0.5,while keeping rest the of the hyperparameters the same.Training extends to a maximum of 50 epochs, with theAdamW optimizer utilized, featuring a learning rateof 105 and a weight decay =0.01.In PlotSnap, where episodes show related content, theapplication of inter-episode contrastive learning negativelyimpacts the models performance. This is due to the vari-ability in the importance of related story segments acrossepisodes, which depends on the specific context.",
  "continuous-index-based time embeddings for shot-frames.Essentially, we assume that a shot consists of 15 frames asSumMe and TVSum require predictions at every 15 frames": "Hyperparameter configuration. We determine the config-uration based on the best validation score obtained over fiverandom splits (5-RCV). This time our model is trained on asingle NVIDIA GeForce RTX-2080 Ti GPU for a maximumof 300 epochs for SumMe and 100 epochs for TVSum, witha batch size of 1. Additional dataset-specific hyperparame-ters are detailed in . In common, we have AdamWoptimizer with parameters: learning rate =5105,weight decay =103. We use CyclicLR for learn-ing rate scheduling with max lr =5104 and triangular2mode. ReLU is used for classification head and GELU forprojection and attention modules.",
  "B.6. Extended Qualitative Analysis": "In this section, we analyze 3 more episodes and compare themodels prediction against all three labels (GT, F, and H).Recall, the labels denoted F (Fandom) are based on sum-marized plot synopses from the 24 fan site5 that includesthe key events in the story (as a text description). Plot syn-opses are short textual descriptions of the key story events ofan episode. We ask annotators to use the plot synopses andtag the start-end duration for story sequences correspondingto the description. We refer to these labels as Fandom (F)and use them for qualitative evaluation. While the H-labelsare annotations from a human, based on what they feel isrelevant summary as per the narrative. Qualitative evaluation. We present importance scores forthree episodes: S06E206, S07E227, and S05E218, in ,, and , respectively.We observe that themodel predictions are quite good and not only match theground-truth labels (on which the model is trained) but alsothe fandom and human annotations. Please refer to the fig-ure captions for additional comments on episode-specificremarks.",
  "C. Future Work": "Ingesting 40 minute long videos is a challenging prob-lem.Aligning different modalities, such as processinga 40-minute video containing 2,500 frames (at 1fps) and4,500 dialogue words, totaling approximately 8000 tokens,may require a larger GPU memory. And most of the L-VLMs struggle to handle contexts exceeding 5 246 6: 2:00am-3:00am talks aboutthe key story events in S06E20 in a Previously on 24 section (see ).7 7: 6:00am-7:00am talks aboutthe key story events in S07E22 in a Previously on 24 section (see ).8 5: 4:00am-5:00am talks aboutthe key story events in S05E21 in a Previously on 24 section (see ).",
  "SumMe True5120.5/0.2/0.2310.0001r/g/gStackTVSum False7680.7/0.4/0.2610.01r/g/g": ". Hyperparameter configuration for SumMe and TVSum. d model specify the dimension for the transformer modules in-ternal representation, while dec l and enc l denote # of decoder and encoder layers, respectively. Other hyperparameters includedrop fc/trm/proj for dropout at classification head, attention module, and projection module, wd stands for weight decay parameter(used inside AdamW ), act clf/mlp/trm for activation function used in classification head, projection, and attention module, with rfor ReLU and g for GELU. Lastly, ffs stands for feature fusion style, with Stack and depicting the pooling strategy showed in . 8,000 tokens, and even if they do, consumer-grade GPUsmay lack the capacity to accommodate them.Our approach considers coarse-grained visual infor-mation, which we demonstrate is beneficial for story-summarization. Considering more fine-grained visual info,such as person and face tracks across frames, and their emo-tions, would be useful. Our idea of using recap-inspiredstory summary labels or modeling approach are not specificto thrillers and can be easily extended to other genres andshows with recaps. Having speaker information in dialogutterances with mapping to character faces would probablyimprove performance on the summarization task. The localstory groups are a proxy to scene segments of an episode.Replacing them with actual scene segments may improveour models performance for summarization.Finally, further analysis and experiments are required todetermine the quality of these methods , particularlybecause evaluating long video summarization using humanjudgment is very time-consuming.However, we believe that this work provides a windowinto this challenging problem and can help facilitate furtherresearch in this area."
}