{
  "Abstract": "In visual place recognition, accurately identifying andmatching images of locations under varying environmentalconditions and viewpoints remains a significant challenge.In this paper, we introduce a new technique, called Bag-of-Queries (BoQ), which learns a set of global queries, de-signed to capture universal place-specific attributes. Un-like existing techniques that employ self-attention and gen-erate the queries directly from the input, BoQ employ dis-tinct learnable global queries, which probe the input fea-tures via cross-attention, ensuring consistent informationaggregation. In addition, this technique provides an inter-pretable attention mechanism and integrates with both CNNand Vision Transformer backbones.The performance ofBoQ is demonstrated through extensive experiments on 14large-scale benchmarks. It consistently outperforms currentstate-of-the-art techniques including NetVLAD, MixVPRand EigenPlaces.Moreover, despite being a global re-trieval technique (one-stage), BoQ surpasses two-stage re-trieval methods, such as Patch-NetVLAD, TransVPR andR2Former, all while being orders of magnitude faster andmore efficient. The code and model weights are publiclyavailable at",
  ". Introduction": "Visual Place Recognition (VPR) consists of determining thegeographical location of a place depicted in a given im-age, by comparing its visual features to a database of pre-viously visited places. The dynamic and ever-changing na-ture of real-world environments pose significant challengesfor VPR . Factors such as varying lighting condi-tions, seasonal changes and the presence of dynamic ele-ments such as vehicles and pedestrians introduce consider-able variability into the appearance of a place. Additionally,changes in viewpoint and image scale can expose previ-",
  "CosPlaceConv-AP": ". Recall@1 performance comparison between our pro-posed technique, Bag-of-Queries (BoQ), and current state of theart methods, Conv-AP , CosPlace , MixVPR and Eigen-Places . ResNet-50 is used as backbone for all techniques.BoQ consistently achieves better performance in various environ-ment conditions such as viewpoint changes (Pitts-250k , Map-illarySLS ), seasonal changes (Nordland ), historical loca-tions (AmsterTime ) and extreme lightning and weather con-ditions (SVOX ). ously obscured areas, further complicating the recognitionprocess. These challenges are exacerbated by the opera-tional constraints of VPR systems, which often need to op-erate in real-time and under limited memory. Consequently,there is a compelling need for efficient algorithms capableof generating compact yet robust representations. With the rise of deep learning, numerous VPR-specificneural networks have been proposed , leveraging Convolutional Neural Networks (CNNs) toextract high-level features, followed by aggregation layersthat consolidate these features into a single global descrip-tor. Such end-to-end trainable architectures have been in-",
  "arXiv:2405.07364v3 [cs.CV] 13 Nov 2024": "strumental in enhancing the efficiency and performance ofVPR systems.Recently, Vision Transformers (ViT) have demon-strated remarkable performance in various computer vi-sion tasks, including image classification , object de-tection and semantic segmentation .Theirsuccess can be attributed to their self-attention mechanism,which captures global dependencies between distant partsof the image .However, current Transformer-basedVPR techniques , often rely on rerank-ing , a post-processing step aimed at refining the ini-tial set of candidate locations identified through global de-scriptor search.The reranking process is usually donewith geometric verification (e.g. RANSAC) on stored localpatch tokens, which is computationally and memory inten-sive. Moreover, the global retrieval process in Transformer-based methods, whether through specific aggregation of lo-cal patches or using the class token of the ViT ,has yet to reach the performance levels of non-Transformer-based approaches like MixVPR and CosPlace .In this paper, we bridge this performance gap, by intro-ducing a novel Transformer-based aggregation technique,called Bag-of-Queries (BoQ), that learns a set of embed-dings (global queries) and employs a cross-attention mech-anism to probe local features coming from the backbonenetwork. This approach enables each global query to con-sistently seek relevant information uniformly across inputimages. This is in contrast with self-attention-based tech-niques , where the queries are dynamically de-rived from the input itself. Furthermore, BoQ is designedfor end-to-end training, thus seamlessly integrating withboth conventional CNN and ViT backbones. Its effective-ness is validated through extensive experiments on mul-tiple large-scale benchmarks, consistently outperformingstate-of-the-art techniques, including MixVPR , Eigen-Places , and NetVLAD . More importantly, BoQ,a single-stage (global) retrieval method that does not em-ploy reranking, outperforms two-stage retrieval methodslike TransVPR and R2Former , while being ordersof magnitude more efficient in terms of computational andmemory resources.",
  ". Related Works": "Early visual place recognition (VPR) techniques relied onhand-crafted local features such as SIFT , SURF andORB , which were aggregated into global descriptorsusing techniques like Bag-of-Words (BoW) orVector of Locally Aggregated Descriptors (VLAD) .BoW involves learning a visual vocabulary (or set of clus-ters), where each visual word (or cluster) represents a spe-cific visual characteristic. Images are then represented ashistograms, with bins indicating the frequency of each vi-sual word. VLAD extends on this by capturing first-order",
  "statistics by accumulating the differences between local de-scriptors and their respective cluster centers": "CNN-based VPR. The advent of deep learning markeda significant shift in VPR techniques, with various ag-gregation architectures proposed. Arandjelovic et al. introduced NetVLAD, a trainable version of VLADthat integrates with CNN backbones, achieving supe-rior performance over traditional methods.Followingits success, many researchers proposed variants such asCRN , SPE-VLAD , Gated NetVLAD andSSL-VLAD . Other approaches focus on regions of in-terests within feature maps . Another key aggre-gation method in image retrieval is the Generalized Mean(GeM) , which is a learnable form of global pooling.Building on GeM, Berton et al. have recently intro-duced CosPlace, enhancing GeM by integrating it with alinear projection layer, achieving remarkable performancefor VPR. Ali-bey et al. introduced MixVPR, an all-MLPaggregation technique that achieved state-of-the-art perfor-mance on multiple benchmarks. Another facet of VPR isthe training procedure, where the focus is on the loss func-tion . Transformer-based VPR. The Transformer architecturewas initially introduced for natural language process-ing , and later adapted to Vision Transformers (ViT) forcomputer vision applications . In VPR, they have re-cently been used as backbone combined with MixVPR or NetVLAD resulting in a performance boostcompared to CNN backbones. Furthermore, AnyLoc used features extracted from a foundation model, called DI-NOv2 , combined with unsupervised aggregation meth-ods such as VLAD, resulting in notable performance on awide range of benchmarks. Recently, two-stage retrieval strategy has gained promi-nence in Transformers-based VPR. It starts with global re-trieval using global descriptors to identify top candidates,followed by a computationally-intensive reranking phasethat refines the results based on local features. In this con-text, Wang et al. introduced TransVPR, which usesCNNs for feature extraction and Transformers for attentionfusion to create global image descriptors, with additionalpatch-level descriptors for geometric verification. On theother hand, Zhang et al. proposed ETR, a Transformer-based reranking technique that uses a CNN backbone forlocal and global descriptors. Their method leverages cross-attention between the local features of two images to castthe reranking as classification. Recently, Zhu et al. pro-posed a unified framework integrating global retrieval andreranking, solely using transformers. For global retrieval,it employs the class token and utilizes other image tokensas local features for the reranking module.These two-stage techniques showed great performance, but at the ex- . Overall architecture of the Bag-of-Queries (BoQ) model. The input image is first processed by a backbone network to extractits local features, which are then sequentially refined in a cascade of Encoder units. Each BoQ block contains a set of learnable queries Q(Learned Bag of Queries), which undergo self-attention to integrate their shared information. The refined features Xi are then processedthrough cross-attention with Q for selective aggregation. Outputs from all BoQ blocks (O1, O2, . . . , OL) are concatenated and linearlyprojected. The final global descriptor is L2-normalized to optimize it for subsequent similarity search. pense of more computation and memory overheads. How-ever, their global retrieval performance is still very lim-ited compared to one-stage methods such as MixVPR and CosPlace .In this work, we propose, Bag-of-Queries (BoQ), a Transformer-based aggregation techniquefor global retrieval, that outperforms existing state-of-the-art without relying on reranking.This makes BoQ par-ticularly suitable for applications where computational re-sources are limited, yet high accuracy and efficiency are es-sential.",
  ". Methodology": "In visual place recognition, effective aggregation of localfeatures is crucial for accurate and fast global retrieval. Toaddress this, we propose the Bag-of-Queries (BoQ) tech-nique, a novel aggregation architecture that is end-to-endtrainable and surprisingly simple, as depicted in .Our technique rely on Multi-Head Attention (MHA)mechanism , which takes three inputs, queries (q), keys(k) and values (v), and linearly project them into multipleparallel heads. The output, for each head, is computed asfollows:",
  "d)v.(1)": "In this mechanism, the queries q play a crucial role. Theyact as a set of filters, determining which parts of the in-put (represented by keys k and values v) are most relevant.The attention scores, derived from the dot product betweenqueries and keys, essentially indicate to the model the de-gree of attention to give to each part of the input. We referto self-attention when q, k and v are derived from the sameinput, e.g. MHA(x, x, x). In contrast, cross-attention is thescenario where the query comes from a different source thanthe key and value. Given an input image I R3HW , we first process itthrough a backbone network, typically a pre-trained Con-volutional Neural Network (CNN) or Vision Transformer(ViT), to extract its high-level features. For CNN back-bones, we apply a 33 convolution to reduce their dimen-sionality, whereas, in the case of a ViT backbone, we applya linear projection for the same purpose. We regard the re-sult as a sequence of N local features of dimension d, suchas: X0 = [x01, x02, . . . , x0N]. We then process X0 througha sequence of Transformer-Encoder units and BoQ blocks.Each encoder transforms its input features, and passes theresult to its subsequent BoQ block as follows:",
  "Xi = Encoderi(Xi1).(2)": "Here, Xi1 denotes the feature input to the i-th Encoderunit, and Xi represents the transformed output, which be-comes the input for the next block in the pipeline.Each BoQ block contains a fixed set (a bag) of M learn-able queries, denoted as Qi = [qi1, qi2, . . . , qiM]. Thesequeries are trainable parameters of the model, independentof the input features (e.g., nn.Parameter in PyTorchcode), not to be confused with the term query images,which refers to the test images used for benchmarking.Before using Qi to aggregate information in the ith BoQblock, we first apply self-attention between them:",
  "Output = W2(W1O)T .(6)": "This is followed by L2-normalization, to bring the globaldescriptor to the unit hypersphere, which optimizes the sim-ilarity search. The training is performed using pair-based(or triplet-based) loss functions that are widely used in theliterature . Relations to other methods. The Bag-of-Queries (BoQ)approach bears conceptual resemblance to Detection Trans-former (DETR) model in its use of object queries.However, BoQ is fundamentally different, as its globalqueries are exclusively used to extract and aggregate lo-cal feature information from the input; these queries donot contribute directly to the final representation, as demon-strated by the absence of any residual connection betweenthe global queries and the output of the cross-attention. Thisis in contrast to DETR, where the object queries are integralpart to the output, upon which object detection and classifi-cation is done.In comparison to NetVLAD , which employs a set oflearned cluster centers, and aggregates local features by cal-culating the weighted sum of their distances to each center.BoQ, leverages cross-attention between its learned queriesand the input local feature to dynamically assess their rele-vance.",
  "MSLS 74018.9kPitts250k 8.2k84kPitts30k 6.8k10kAmsterTime 12311231Eynsham 24k24kNordland* 276027.6kNordland** 27.6k27.6kSt-Lucia 14641549SVOX 14.3k17.2kSPED 607607": ". Overview of VPR benchmarks used in our experiments,highlighting their number of query images (# quer.) and referenceimages (# ref.), as well as their variations in terms of viewpoint,season, and day/night. The number of marks indicates the de-gree of variations present ( means low, means high). dashcams and features a broad range of viewpoints andlighting changes, testing the systems adaptability to typicalvariations in ground-level VPR. Pitts250k and Pitts30kdatasets, extracted from Google Street View, exhibit signif-icant changes in viewpoint, which tests the systems abil-ity to maintain recognition across varying angles and per-spectives.AmsterTime presents a unique challengewith historical grayscale images as queries and contempo-rary color images as references, covering temporal varia-tions over several decades. Eynsham consists entirelyof grayscale images, adding complexity due to the absenceof color information. Nordland dataset was collectedover four seasons, using a camera mounted on the front ofa train. It encompasses scenes ranging from snowy wintersto sunny summers, presenting the challenge of extreme ap-pearance changes due to seasonal variations. Note that twovariants of Nordland are used in VPR benchmarking: oneuses a subset of 2760 summer images as queries against theentire winter sequence as reference images (marked with *),while the other uses the entire winter sequence as query im-ages against the entire summer sequence as reference im-ages (marked with **). SVOX stands out with its com-prehensive coverage of weather conditions, including over-cast, rainy, snowy, and sunny images, to test adaptabilityto meteorological changes. Additionally, SVOX Night sub-set focuses on nocturnal scenes, a challenging scenario formost VPR systems.",
  ". Implementation details": "Architecture.For our experiments, we primarily em-ploy pre-trained ResNet-50 backbones for feature extrac-tion . Importantly, our proposed Bag-of-Queries (BoQ)technique is architecturally agnostic and seamlessly inte-grates with various CNNs or Vision Transformer (ViT)backbones. When employing a ResNet, we crop it at thesecond last residual to preserve a higher resolution of lo- cal features. Additionally, we freeze all but the last residualblock and add a 33 convolution to reduce the number ofchannels, which decreases the computational and memoryfootprint of the self-attention mechanism.Implementingthe BoQ model is straightforward with popular deep learn-ing frameworks such as PyTorch and TensorFlow ,both providing highly optimized implementations of self-attention and cross-attention, which are the basic blocks ofour technique. Training.We train BoQ following the standard frame-work of GSV-Cities , which provides a highly accuratedataset of 560k images depicting 67k places. For the lossfunction, we use Multi-Similarity loss , as it has beenshown to perform best for visual place recognition . Weuse batches containing 120 200 places, each depictedby 4 images resulting in mini-batches of 480 800 im-ages. We use the AdamW optimizer with a learning rateof 0.0002 0.0004 depending the batch-size, and a weightdecay of 0.001. The learning rate is multiplied by 0.3 af-ter each 5 10 epochs. Finally, we train for a maximumof 40 epochs (including 3 epochs of linear warmup) usingimages resized to 320320. Note that while current tech-niques such as NetVLAD , CosPlace and Eigen-Places train on images of size 480640effectivelytriple the pixel count of our chosen resolutionour BoQmodel still achieves better performance, underscoring its ef-fectiveness. Evaluation metrics. We follow the same evaluation metricof existing literature , wherethe recall@k is measured. Recall@k is defined as the per-centage of query images for which at least one of the top-kpredicted reference images falls within a predefined thresh-old distance, which is typically 25 meters. However, forthe Nordland benchmark, which features aligned sequences,the threshold is 10 frame counts for Nordland** and only 1frame for Nordland*.",
  ". Comparison with previous works": "In this section, we present an extensive comparison of ourproposed method, Bag-of-Queries (BoQ), with a wide rangeof existing state-of-the-art VPR aggregation techniques.This includes global average pooling (AVG) , General-ized Mean (GeM) , NetVLAD alongside its recentvariants, SPE-NetVLAD and Gated NetVLAD .Importantly, we compare against recent cutting-edge tech-niques, such as ConvAP , CosPlace , Eigen-Places and MixVPR , which currently hold the bestscores in most existing benchmarks, setting a high standardfor BoQ to measure up against in the field of VPR.Moreover, despite BoQ being a global retrieval tech-nique which does not employ reranking, we compare it withcurrent state-of-the-art two-stage retrieval techniques . These methods leverage geometric verifi-",
  "cation to enhance recall@k performance at the expense ofincreased computation and memory overhead": "Results analysis. In Tab. 2, we present a comparative anal-ysis across several challenging datasets, focusing on Re-call@1 (R@1), Recall@5 (R@5), and Recall@10 (R@10).We also offer insight into the performance of each methodwith regard to the top-ranked retrieved images. On Pitts250k-test, BoQ outperforms all other methodswith a marginal yet notable improvement as it is the firstglobal technique that reaches the 95% R@1 threshold onthis benchmark.",
  "On MSLS-val and SPED, BoQ outperforms the secondbest methods (EigenPlaces and MixVPR) by 2.0 and 1.3percentage points respectively": "On the challenging Nordland* benchmark, known for itsextreme seasonal variations, BoQ significantly outper-forms all compared methods, indicating its robustness tosevere environmental changes. Surpassing CosPlace andEigenplaces by 16, and MixVPR by 12 percentage points,which translates to over 400 additional accurate retrievalswithin the top-1 results.In Tab. 3, we follow the same evaluation style proposed byBerton et al. , where benchmarks are categorized intomulti-view datasets, where images are oriented in variousangles, and frontal-view dataset where images are mostlyforward facing. Top-1 retrieval performance (R@1) is re-ported for each technique. For the multi-view benchmarks, BoQ achieves highestR@1 on AmsterTime and Eynsham, and second bestcompared to EigenPlaces on Pitts30k-test.Both tech-niques show robustness to viewpoint changes in urban en-vironments, while facing challenges with the AmsterTimedataset, which includes decades-old historical imagery. On frontal-view benchmarks, particularly on Nordland**(which comprises 27.6k queries), BoQ achieves R@1 of83.1%, which is 7 and 11 percentage points more thanMixVPR, and CosPlace respectively. This translates toBoQ correctly retrieving an additional 2400 and 3700 im-ages within the top-1 results. On SVOX, BoQ achieves best results under all conditions,especially on SVOX night, where it achieves 87.1% R@1,outperforming the second best method (MixVPR) by 22.7percentage points. This highlights BoQs potential underlow-light conditions.Furthermore, we compare BoQ to two-stage retrieval tech-niques in Tab. 4, by reporting performance on MSLS-valand Pitts30k-test. Although our technique does not performreranking, its global retrieval performance surpasses that ofexisting two-stage techniques, including Patch-NetVLAD(2021), TransVPR (2022) and R2Former (2023). Impor-tantly, this makes our approach more efficient in terms ofmemory and computation: over 30 faster retrieval timecompared to R2Former.",
  "BoQ (Ours)409695.0 98.499.191.194.895.785.493.195.469.583.487.0BoQ (Ours)1638495.0 98.599.191.2 95.396.186.5 93.495.770.7 84.087.5": ". Comparison of our technique, Bag-of-Queries (BoQ), to existing state-of-the-art methods. Best results are in shown in bold andsecond best are underlined. All methods use a pre-trained ResNet-50 backbone and follow identical training procedures on the GSV-Citiesdataset, except for CosPlace and EigenPlaces where authors pre-trained weights demonstrated superior performance and are thus usedhere.",
  "BoQ (Ours)7091.494.592.496.2": ". Comparing against two-stage retrieval techniques interms of performance and latency of feature extraction and rerank-ing (when applicable). The first 5 techniques use a second refine-ment pass (geometric matching) to re-rank the top 100 candidatesin order to improve retrieval performance. BoQ (ours) does notemploy re-ranking, which makes it orders of magnitude faster thanthe fastest two-stage technique.",
  ". Ablation studies": "Number of learnable queries.We conduct an ablationstudy to investigate the impact of varying the number oflearnable queries, Q, on the performance of BoQ. We use aResNet-50 backbone and two BoQ blocks. The results arepresented in Tab. 5. We observe that performance improvesas the number of queries increases. However, the benefits ofadditional queries vary across datasets. In less diverse en-vironments, such as Pitts30k, adding more queries yieldsmarginal performance improvements (increasing from 8to 64 queries results in a mere 0.2% R@1 performancegain). In contrast, the results on AmsterTime, which fea-tures highly diverse images spanning decades, demonstratethat the model benefits significantly from additional queries.This aligns with the underlying intuition of BoQ, whereeach query implicitly learns a distinct universal pattern.",
  "ResNet-187.188.492.683.592.765.979.3ResNet-3412.589.593.185.592.664.180.1ResNet-5014.691.494.586.294.474.486.1ResNet-10142.990.595.485.693.871.984.9": ". Ablation on the backbone architecture. Each backboneis cropped at the second last residual block. Two BoQ blocks areused, with 64 learnable queries in each. We show the total num-ber of parameters. BoQ achieves state-of-the-art performance withonly a ResNet-34 backbone, which highlights its potential use forreal-time scenarios. Number of BoQ blocks. To assess the influence of thenumber of BoQ blocks within our architecture, we con-duct experiments by varying L, the number BoQ blocksutilized.We use a ResNet-18 backbone followed by LBoQ blocks, each comprising a bag of 32 learnable queries.We report R@1 performance for each setup. The results,presented in Tab. 6, demonstrate that even when paired with a lightweight backbone like ResNet-18, BoQ remainscompetitive against state-of-the-art methods such as Cos-Place and MixVPR , which use ResNet-50 back-bone. Best overall performance is observed when employ-ing 4 BoQ blocks. Ablation on the self-attention. Applying self-attention on(Qi) brings more stability/performance by adding contextbetween the queries. As shown in Tab. 7, we train a BoQmodel with ResNet-18 backbone for 35 epochs. Addingself-attention between the learnable queries brings consis-tent performance improvement across various benchmarks.Note that the self-attentions output can be cached duringeval/test to avoid recomputation at every test iteration. Backbone architecture. In Tab. 8 we present a comparativeperformance of BoQ coupled with different ResNet back-bone architectures. For each backbone, we crop it at the sec-ond last residual block and follow it with two BoQ blocks,each comprising 64 learnable queries. The total numberof parameters (trainable + non trainable) is provided. Theempirical results showcase that when BoQ is coupled withResNet-34, a relatively lightweight backbone, it achievescompetitive performance compared to NetVLAD, CosPlaceand EigenPlaces coupled with ResNet-50 backbone. Inter-estingly, using ResNet-101, a relatively deeper backbone,we did not achieve better performance than ResNet-50,which could be attributed to memory constraints necessi-tating a smaller training batch size.",
  ". Vision Transformer Backbones": "All previous experiments in this paper were conducted us-ing a ResNet backbone to ensure fairness against ex-isting state-of-the-art methods.Recently, vision founda-tion models such as DINOv2 have been introducedand rapidly adopted as the de facto backbone for variouscomputer vision tasks. In this experiment, we evaluate theperformance of BoQ when using a DINOv2 backbone. Weemploy the pre-trained base variant with 86M parameters,freezing all but the last two blocks to enable fine-tuning.We train on GSV-Cities with images resized to 280280and perform testing with images resized to 322322. Theresults presented in Tab. 9, demonstrate that DINOv2 fur-ther enhances the performance of BoQ, achieving new all-time high performance on all benchmarks.",
  ". Visualization of the learned queries": "illustrates the cross-attention weights between the in-put image and a subset of learned queries within the BoQblocks. We highlight four distinct queries (among 64 in to-tal) from the second BoQ block, in order to understand theirindividual aggregation characteristics. Observing vertically,we can see how the input image is seen through the lensesof each learned query. The aggregation is realized through",
  ".Performance comparison of BoQ using DINOv2 andResNet-50 backbone": "the product of the cross-attention weights with the input fea-ture maps, resulting in a single aggregated descriptor perquery. Notice that the role of each query is to scanviacross-attentionthe input features and generate the aggre-gation weights.Observing horizontally, the diversity in attention patternsacross the learned queries becomes apparent. For instance,the first query (top row) appears to concentrate on fine-grained details within the feature maps, as indicated by theintense localized areas. In contrast, the second query (sec-ond row) shows a preference for larger regions in the in-put features, suggesting a more holistic capture of scene at-tributes.",
  ". Conclusion": "In this work, we introduced Bag-of-Queries (BoQ), a novelaggregation technique for visual place recognition, which isbased on the use of learnable global queries to probe localfeatures via a cross-attention mechanism, allowing for ro-bust and discriminative feature aggregation. Our extensiveexperiments on 14 different large-scale benchmarks demon-strate that BoQ consistently outperforms current state-of-the-art techniques, particularly in handling complex varia-tions in viewpoint, lighting, and seasonal changes. Further-more, BoQ being a global (one-stage) retrieval technique, itoutperforms existing two-stage retrieval methods that em-ploy geometric verification for re-ranking, all while beingorders of magnitude faster, setting a new standard for VPRresearch.In future work, building upon the concepts discussedin Sec. 3, the spatial information carried in the output ofthe last encoder, denoted as XL, presents an opportunity . Visualization of the cross attention weights between theinput images and the learned queries. The three examples are fromNordland, Pitts30k and MSLS datasets, respectively. We selectedfour queries (among 64) from the second BoQ block of a trainednetwork. Vertically, we can see how the input image is aggregatedby each query. The aggregation is done through the product of theweight with the input feature maps, resulting in one aggregateddescriptor per query. Horizontally, we can see in each line howeach query spans the input image. For example, the first querylooks more for fine grained details, while the second looks morefor large areas in the input images.",
  "for further enhancement through the application of specialreranking strategies": "Acknowledgement: This work has been supported by TheFonds de Recherche du Quebec Nature et technologies(FRQNT 254912). We gratefully acknowledge the supportof NVIDIA Corporation with the donation of a Quadro RTX8000 GPU used for our experiments. Martn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-mawat, Geoffrey Irving, Michael Isard, et al. Tensorflow:A system for large-scale machine learning. In USENIX Sym-posium on Operating Systems Design and Implementation,pages 265283, 2016. 5",
  "Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF:Speeded up robust features.In European Conference onComputer Vision (ECCV), pages 404417, 2006. 2": "Gabriele Berton, Valerio Paolicelli, Carlo Masone, and Bar-bara Caputo.Adaptive-attentive geolocalization from fewqueries: A hybrid approach.In IEEE Winter Conferenceon Applications of Computer Vision (WACV)n, pages 29182927, 2021. 1, 4, 8 Gabriele Berton, Carlo Masone, and Barbara Caputo. Re-thinking visual geo-localization for large-scale applications.In IEEE/CVF International Conference on Computer Visionand Pattern Recognition (CVPR), pages 48784888, 2022.1, 3, 5, 6, 7 Gabriele Berton, Gabriele Trivigno, Barbara Caputo, andCarlo Masone.Eigenplaces:Training viewpoint robustmodels for visual place recognition.In IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages1108011090, 2023. 1, 2, 5, 6",
  "Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep lo-cal and global features for image search. In European Con-ference on Computer Vision (ECCV), pages 726743, 2020.5, 6": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Confer-ence on Computer Vision (ECCV), pages 213229, 2020. 2,4 Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.CrossViT: Cross-attention multi-scale vision transformer forimage classification. In IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 357366, 2021. 2 David M Chen, Georges Baatz, Kevin Koser, Sam STsai, Ramakrishna Vedantham, Timo Pylvanainen, KimmoRoimela, Xin Chen, Jeff Bach, Marc Pollefeys, et al. City-scale landmark identification on mobile devices. In CVPR2011, pages 737744. IEEE, 2011. 8 Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc VLe. Randaugment: Practical automated data augmentationwith a reduced search space. In IEEE International Con-ference on Computer Vision and Pattern Recognition Work-shops (CVPRW), pages 702703, 2020. 1",
  "Mark Cummins. Highly scalable appearance-only SLAM-FAB-MAP 2.0. Robotics: Science and Systems (RSS), 2009.4, 8": "Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. In IEEE International Conference on Com-puter Vision and Pattern Recognition Workshops (CVPRW),pages 224236, 2018. 5, 6 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. International Con-ference on Learning Representations (ICLR), 2020. 2",
  "Dorian Galvez-Lopez and Juan D Tardos. Bags of binarywords for fast place recognition in image sequences. IEEETransactions on Robotics, 28(5):11881197, 2012. 2": "Yixiao Ge, Haibo Wang, Feng Zhu, Rui Zhao, and Hong-sheng Li. Self-supervising fine-grained region similaritiesfor large-scale image localization. In European Conferenceon Computer Vision (ECCV), pages 369386, 2020. 1, 6 Amin Ghiasi, Hamid Kazemi, Eitan Borgnia, Steven Reich,Manli Shu, Micah Goldblum, Andrew Gordon Wilson, andTom Goldstein. What do vision transformers learn? a visualexploration. arXiv preprint arXiv:2212.06727, 2022. 2 Stephen Hausler, Sourav Garg, Ming Xu, Michael Mil-ford, and Tobias Fischer. Patch-NetVLAD: Multi-scale fu-sion of locally-global descriptors for place recognition. InIEEE/CVF International Conference on Computer Visionand Pattern Recognition (CVPR), pages 1414114152, 2021.5, 6 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 4, 7, 1",
  "Qingfeng Hou, Jun Lu, Haitao Guo, Xiangyun Liu, ZhihuiGong, Kun Zhu, and Yifan Ping. Feature relation guidedcross-view image based geo-localization. Remote Sensing,15(20):5029, 2023. 2": "Gaoshuang Huang, Yang Zhou, Xiaofei Hu, ChenglongZhang, Luying Zhao, Wenjian Gan, and Mingbo Hou.Dino-mix: Enhancing visual place recognition with foun-dational vision model and feature mixing.arXiv preprintarXiv:2311.00230, 2023. 2 Herve Jegou, Florent Perronnin, Matthijs Douze, JorgeSanchez, Patrick Perez, and Cordelia Schmid. Aggregatinglocal image descriptors into compact codes. IEEE Transac-tions on Pattern Analysis and Machine Intelligence, 34(9):17041716, 2011. 2 NikhilKeetha,AvneeshMishra,JayKarhade,Kr-ishna Murthy Jatavallabhula, Sebastian Scherer, MadhavaKrishna, and Sourav Garg. Anyloc: Towards universal visualplace recognition. arXiv preprint arXiv:2308.00688, 2023. 2 Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.Learned contextual feature reweighting for image geo-localization.In IEEE/CVF International Conference onComputer Vision and Pattern Recognition (CVPR), pages32513260, 2017. 1, 2, 5 Mara Leyva-Vallina,Nicola Strisciuglio,and NicolaiPetkov.Data-efficient large scale place recognition withgraded similarity supervision. In IEEE/CVF InternationalConference on Computer Vision and Pattern Recognition(CVPR), pages 2348723496, 2023. 2 Liu Liu, Hongdong Li, and Yuchao Dai.Stochasticattraction-repulsion embedding for large scale image local-ization.In IEEE/CVF International Conference on Com-puter Vision (ICCV), pages 25702579, 2019. 1 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InIEEE/CVF International Conference on Computer Vision(ICCV), pages 1001210022, 2021. 2",
  "Michael Milford and G. Wyeth. Mapping a suburb with a sin-gle camera using a biologically inspired slam system. IEEETransactions on Robotics, 24:10381053, 2008. 4, 8": "Jiwei Nie, Joe-Mei Feng, Dingyu Xue, Feng Pan, Wei Liu,Jun Hu, and Shuai Cheng. A training-free, lightweight globalimage descriptor for long-term visual place recognition to-ward autonomous vehicles. IEEE Transactions on IntelligentTransportation Systems, 2023. 2 Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.Dinov2: Learning robust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023. 2, 7 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, Zem-ing Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:An imperative style, high-performance deep learning li-brary. In Advances in Neural Information Processing Sys-tems (NeurIPS), pages 80268037, 2019. 5 James Philbin, Ondrej Chum, Michael Isard, Josef Sivic,and Andrew Zisserman. Object retrieval with large vocab-ularies and fast spatial matching.In IEEE/CVF Interna-tional Conference on Computer Vision and Pattern Recog-nition (CVPR), pages 18, 2007. 2",
  "Ethan Rublee, Vincent Rabaud, Kurt Konolige, and GaryBradski. ORB: An efficient alternative to sift or surf. InIEEE/CVF International Conference on Computer Vision(ICCV), pages 25642571, 2011. 2": "Zachary Seymour, Karan Sikka, Han-Pang Chiu, SupunSamarasekera, and Rakesh Kumar. Semantically-aware at-tentive neural embeddings for long-term 2D visual localiza-tion. In British Machine Vision Conference (BMVC), 2019.1 Niko Sunderhauf, Peer Neubert, and Peter Protzel. Are wethere yet?challenging SeqSLAM on a 3000 km journeyacross all four seasons. In Workshop on Long-Term Auton-omy, IEEE International Conference on Robotics and Au-tomation (ICRA), 2013. 4, 8",
  "Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob-ject retrieval with integral max-pooling of CNN activations.arXiv preprint arXiv:1511.05879, 2015. 2": "Akihiko Torii, Josef Sivic, Tomas Pajdla, and MasatoshiOkutomi. Visual place recognition with repetitive structures.In IEEE/CVF International Conference on Computer Visionand Pattern Recognition (CVPR), pages 883890, 2013. 1,2, 4, 8 Akihiko Torii, Relja Arandjelovic, Josef Sivic, MasatoshiOkutomi, and Tomas Pajdla.24/7 place recognition byview synthesis. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 18081817,2015. 8 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in NeuralInformation Processing Systems (NeurIPS), 30, 2017. 2, 3 Ruotong Wang, Yanqing Shen, Weiliang Zuo, SanpingZhou, and Nanning Zheng. TransVPR: Transformer-basedplace recognition with multi-level attention aggregation. InIEEE/CVF International Conference on Computer Visionand Pattern Recognition (CVPR), pages 1364813657, 2022.2, 5, 6 Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, andMatthew R Scott.Multi-similarity loss with general pairweighting for deep metric learning. In IEEE/CVF Interna-tional Conference on Computer Vision and Pattern Recogni-tion (CVPR), pages 50225030, 2019. 5 Yuwei Wang, Yuanying Qiu, Peitao Cheng, and JunyuZhang. Hybrid CNN-transformer features for visual placerecognition. IEEE Transactions on Circuits and Systems forVideo Technology, 33(3):11091122, 2023. 2",
  "Burak Yildiz, Seyran Khademi, Ronald Maria Siebes, andJan van Gemert.Amstertime: A visual place recognitionbenchmark dataset for severe domain shift. arXiv preprintarXiv:2203.16291, 2022. 1, 4, 8": "Jun Yu, Chaoyang Zhu, Jian Zhang, Qingming Huang, andDacheng Tao.Spatial pyramid-enhanced NetVLAD withweighted triplet loss for place recognition. IEEE Transac-tions on Neural Networks and Learning Systems, 31(2):661674, 2019. 2, 5, 6 Mubariz Zaffar, Sourav Garg, Michael Milford, Julian Kooij,David Flynn, Klaus McDonald-Maier, and Shoaib Ehsan.VPR-Bench: An open-source visual place recognition evalu-ation framework with quantifiable viewpoint and appearancechange. International Journal of Computer Vision (IJCV),pages 139, 2021. 1, 4, 5, 8 Hao Zhang, Xin Chen, Heming Jing, Yingbin Zheng, YuanWu, and Cheng Jin. ETR: An efficient transformer for re-ranking in visual place recognition. In Proceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision (WACV), pages 56655674, 2023. 2",
  "Xiwu Zhang, Lei Wang, and Yan Su. Visual place recog-nition: A survey from deep learning perspective.PatternRecognition, 113:107760, 2021. 1": "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, TaoXiang, Philip HS Torr, et al. Rethinking semantic segmen-tation from a sequence-to-sequence perspective with trans-formers. In IEEE/CVF International Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 68816890, 2021. 2, 5 Sijie Zhu, Linjie Yang, Chen Chen, Mubarak Shah, Xiao-hui Shen, and Heng Wang. R2Former: Unified retrieval andreranking transformer for place recognition. In IEEE/CVFInternational Conference on Computer Vision and PatternRecognition (CVPR), pages 1937019380, 2023. 2, 5, 6",
  ". More implementation details": "Training image size. For our training implementation, weresized the images to 320320 pixels to maintain consis-tency with the training procedures of . This choice ofspecific resolution directly influences the spatial resolutionof the resulting feature maps, which is 2020 when using acropped ResNet backbone . This choice also allows forthe use of larger batch sizes. Inference image size. Considering that many benchmarkscontain images of varying sizes and aspect ratiosand of-ten at higher resolutions we resize the images to a res-olution slightly higher than 320 pixels while preservingthe original aspect ratio. This approach maintains the in-tegrity of the scenes by keeping the original aspect ratio,and allows the learned queries to interact with bigger featuremaps. In Tab. 10 we perform testing at different image sizes(288, 320, 384, 432 and 480), using a BoQ model trainedwith 320320 images. As we can see, when the images areresized to a height of 384 pixels, there is a consistent im-provement in Recall@1 across almost all datasets. This ex-periment suggests that heights of 384 and 432 may representan optimal balance between image detail and the modelscapacity to extract and utilize informative features. Notethat the performance gains from resizing to these heightsare marginal compared to the baseline size of 320 pixels.",
  ". Interpretability of the learned queries": "In this section, we demonstrate how the learned queries inBoQ can be visually interpreted through their direct interac-tions with the feature maps via cross-attention mechanisms.To do so, we examine their behavior in images of the samelocation under viewpoint changes, occlusions, and varyingweather conditions.The cross-attention mechanisms in our BoQ model havebeen instrumental in achieving fine-grained feature discrim-ination, as demonstrated by , and . Thesefigures provide a visualization of the learned queries atten-tion patterns across diverse urban scenes and under variousenvironmental conditions. demonstrates the models temporal robustness,displaying consistent attention across images of the samelocation captured at different times. The learned queriesreliably focus on distinctive features, such as buildings, fo-liage, and poles, despite variations in viewpoint, lighting,and weather conditions.Moving objects within a scene often pose a challengefor VPR techniques. Nonetheless, as shown in ourlearned queries focus their attention towards static elementsof the environment, avoiding moving objects like vehicles. underscores the specialization of the learnedqueries, showcasing their selective focus on relevant fea-tures, such as vegetation and buildings. This selective atten-tion is indicative of our models ability to interpret complexvisual information within the environment.",
  ". Detailed architecture of our model using ResNet-50 backbone and two BoQ blocks": ". Weather and occlusions. The first row displays four images of the same location captured at different times, illustrating changesin the environment. Subsequent rows reveal the cross-attention scores between one learned query and the feature maps of the respectiveinput image. In these heatmaps, regions with higher attention scores are indicated in warmer colors (red/yellow), signifying areas wherethe query is focusing more intensely. First row shows four images of the same place accross different times. The following four rows showthe cross-attention scores of four selected learned queries on the feature maps of the input image. . Moving objects. The consistency of attention allocation across scenes with different moving objects (cars) underscores ourmodels capability in distinguishing between transient and persistent features (trees, buildings) within an urban environment. . In this example, we can see that each query specializes in identifying particular elements within the scenes. The first query(second row) predominantly activates over big blobs of vegetation, while the second query (third row) demonstrates higher activation overarchitectural structures, such as buildings. These attention patterns suggest a high degree of specialization in the learned queries, enablingprecise feature discrimination within complex environments."
}