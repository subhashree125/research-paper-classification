{
  "Abstract": "Collaborative perception empowers each agent to im-prove its perceptual ability through the exchange of per-ceptual messages with other agents.It inherently re-sults in a fundamental trade-off between perception abil-ity and communication cost.To address this bottleneckissue, our core idea is to optimize the collaborative mes-sages from two key aspects:representation and selec-tion. The proposed codebook-based message representa-tion enables the transmission of integer codes, rather thanhigh-dimensional feature maps. The proposed information-filling-driven message selection optimizes local messages tocollectively fill each agents information demand, prevent-ing information overflow among multiple agents. By in-tegrating these two designs, we propose CodeFilling,a novel communication-efficient collaborative percep-tion system, which significantly advances the perception-communication trade-off and is inclusive to both homo-geneous and heterogeneous collaboration settings.Weevaluate CodeFilling in both a real-world dataset,DAIR-V2X, and a new simulation dataset, OPV2VH+.Results show that CodeFilling outperforms previ-ous SOTA Where2comm on DAIR-V2X/OPV2VH+ with1,333/1,206 lower communication volume.Our codeis available at",
  ". Introduction": "Collaborative perception aims to enhance the perceptualability of each individual agent by facilitating the exchangeof complementary perceptual information among multipleagents .Itfundamentally overcomes the occlusion and long-range is-sues in single-agent perception .As theforefront of autonomous systems, collaborative perceptionshows significant potential in enormous real-world appli- .CodeFilling avoids redundant messages andachieves more complete detections by transmitting more criticalperceptual information with the compact code index message.cations, particularly vehicle-to-everything-communication-aided autonomous driving .In this emerging field, a central challenge lies in opti-mizing the trade-off between perception performance andcommunication cost inherent in agents sharing perceptualdata . Given inevitable practi-cal constraints of communication systems, efficient utiliza-tion of communication resources is the prerequisite for col-laborative perception. To minimize communication over-head, a straightforward solution is late collaboration, whereagents directly exchange the perception outputs. However,numerous previous works indicate that late collaborationyields marginal perception improvements and is vulnera-ble to various noises . To optimize the perception-communication trade-off, most studies consider intermedi-ate collaboration, where the collaborative messages are per-ceptual features . Forexample, When2com proposed the handshake strategyto limit the number of collaborators; and Where2comm proposed a pragmatic strategy that only transmits messagesabout constrained spatial areas. While these methods miti-gate certain communication costs, they still necessitate thetransmission of high-dimensional feature maps, which in-curs substantial communication expenses.To overcome the limitations of intermediate collabora-",
  "arXiv:2405.04966v1 [cs.IT] 8 May 2024": "tion, our core idea is to optimize the collaborative messagesfrom two key perspectives: representation and selection.For message representation, we introduce a codebook tostandardize the communication among agents, where eachcode is analogous to a word in the human language dictio-nary. Based on this shared codebook among all agents, wecan use the codes to approximate perceptual features; con-sequently, only integer code indices need to be exchanged,eliminating the need for transmitting high-dimensional fea-tures comprised of floating-point numbers.For messageselection, we propose an information filling strategy, akinto piecing together a jigsaw puzzle. In this approach, as-suming an agents information demand is upper bounded,each of its collaborators performs a local optimization toselect non-redundant messages to fill its information gap.This strategy prevents information overflow among multipleagents, further significantly reducing communication cost.Following the above spirit, we propose CodeFilling,a novel communication-efficient collaborative 3D detectionsystem; see . The proposed CodeFilling in-cludes four key modules: i) a single-agent detector, provid-ing basic detection capabilities; ii) an novel information-filling-driven message selection, which solves local opti-mizations for choosing pertinent messages to optimally fillother agents information demands without causing infor-mation flow; iii) an novel codebook-based message rep-resentation, which leverages a task-driven codebook toachieve pragmatic approximation of feature maps, enablingthe transmission of integer code indices; and iv) a messagedecoding and fusion module, which integrates the messagesto achieve enhanced collaborative detections.CodeFilling offers two distinct advantages:i)it delivers a substantial advancement in the perception-communication trade-off through the transmission of non-redundant code indices; and ii) it is inclusive to both settingsof homogeneous and heterogeneous agents by leveraging astandardized code representation; that is, feature maps ob-tained from various perception models and diverse sensorscan be aligned to the unified feature space provided by theproposed codebook.To evaluate CodeFilling, we conduct extensive ex-periments on real-world dataset DAIR-V2X, and a new sim-ulation dataset OPV2VH+ under two homogeneous (Li-DAR, camera) and one heterogeneous setting.The re-sults show that i) CodeFilling achieves superior perfor-mance performances than Where2comm, the current SOTA,with 1333/1206 less communication cost on DAIR-V2X/OPV2VH+; and ii) CodeFilling maintains supe-rior trade-off in both homogeneous and heterogeneous set-tings, establishing an inclusive collaboration system.To sum up, our main contributions are three-fold: We propose CodeFilling, a novel communication-efficient collaborative 3D detection system, which signif- icantly improves the perception- communication trade-offand is inclusive to both homogeneous and heterogeneouscollaboration settings; We propose two novel methods to optimize collabora-tive messages: codebook-based message representation andinformation-filling-driven message selection; We conduct comprehensive experiments to vali-date that CodeFilling achieves SOTA perception-communication trade-off across varying communicationbandwidths, on both real-world and simulation datasets inboth homogeneous and heterogeneous settings. 2. Related worksCollaborative Perception. Collaborative perception is an emergingapplication of multi-agent communication systems to per-ception tasks, which promote the crucial perception mod-ule through communication-enabled complementary per-ceptual information sharing. Several high-quality datasetshave emerged to aid in the algo-rithm development. Collaborative perception systems havemade remarkable progress in improving perception perfor-mance and robustness on practical issues, such ascommunication bandwidth constraints, pose error and latency . Here, considering that communicationefficiency is the bottleneck issue for the scale-up of collab-orative perception, we aim to optimize the performance-communication trade-off instead of solely promoting theperception performance regardless of bandwidth costs.Communication efficiency in collaborative perception.To address this bottleneck issue, prior methods have madeefforts in two key aspects: message selection and messagerepresentation. For message selection, When2com andWho2comm employ a handshake mechanism, to se-lect information from all the relevant collaborators. Fur-thermore, Where2comm and CoCa3D expand theselection process to incorporate spatial dimensions.Formessage representation, intermediate feature representa-tion has demonstrated a morebalanced performance-communication trade-off.Sourcecoding and channel compression techniques areused to further enhance feature representation efficiency.However, previous methods accumulate redundant infor-mation from various collaborators and still transmit high-dimensional feature vectors, incurring high communicationcosts. Here, we facilitate essential supportive informationexchange among agents with compact codebook-based rep-resentation, efficiently enhancing detection performance.Codebook compression. Codebook compression, a loss-less compression technique, effectively captures the essenceof high-dimensional vectors through the combination ofthese codes . It has diverse applications, ranging fromdigital image compression to the neural network param-eters compression . Recently, task-adaptive codebooks . CodeFilling is a novel communication-efficient collaborative 3D detection system. The proposed information-filling-drivenmessage selection and codebook-based message representation contribute to optimizing collaborative messages. have emerged. Rather than pursuing lossless compression,it drops task-irrelevant information and focuses on essen-tial information for specific downstream tasks, further im-proving representation efficiency . However, existingtask-adaptive codebooks have largely concentrated on 2Dclassification tasks . Here, we explore the novel realmof collaborative 3D object detection, introducing fresh chal-lenges for codebook compression. This entails preservingan extensive feature set for precise 3D interpretation andadapting to fluctuating communication bandwidths, neces-sitating versatile codebook configurations.",
  ". Problem Formulation": "Consider N homogeneous or heterogeneous agents in thescene, each has its individual perceptual task and uniquesensor setup. To enhance the perception abilities, the agentsexchange complementary perceptual information, forminga decentralized mutually beneficial collaboration network.Each agent concurrently acts as both a supporter and a re-ceiver. In their role as supporters, they contribute perceptualinformation to assist their counterparts. Conversely, as re-ceivers, they gain from the messages provided by others.Such collaborative perception leads to a holistic enhance-ment of perceptual capabilities. Here we focus on 3D objectdetection. Let Xi be the input collected by the ith agentssensor (LiDAR or camera), and O0i be the correspondingground-truth detection. The objective is to maximize thedetection performances of all agents given certain commu-nication budget B; that is,",
  ". CodeFilling: Collaborative 3D Detection": "To optimize the trade-off between perception ability andcommunication cost, we present CodeFilling, a novelcommunication-efficient collaborative 3D detection system;see . It has two parts: i) single-agent 3D detection,which allows an agent to equip basic detection ability, im-plementing in (1), and ii) multi-agent collaboration, en-hancing an agents detection ability through the exchangeof efficient perceptual messages P in (1).4.1. Single-agent 3D detection An agent learns to detect 3D objects based on its sen-sor inputs. It involves an observation encoder and a de-tection decoder. CodeFilling allows agents to acceptmulti-modality inputs, including RGB images and 3D pointclouds. Each agents with its distinct modality projects itsperceptual information to the unified global birds eye view(BEV) coordinate system, better supporting inter-agent col-laboration and more compatible with both homogeneousand heterogeneous settings.Observation encoder. The observation encoder extractsfeature maps from the sensor data. For the ith agent, givenits input Xi, the BEV feature map is Fi = enc(Xi) RHW C, where enc() is the encoder and H, W, C areits height, weight and channel. For image inputs, enc() isfollowed by an additional warping function that transformsthe extracted front-view feature to BEV. The BEV feature isoutput to the decoder, and also message selection and fusionmodules when collaboration is established.Detection decoder. The detection decoder decodes fea-tures into objects, including class and regression output.Given the feature map Fi, the detection decoder dec()generate the detections of ith agent by Oi = dec(Fi) RHW 7, where each location of Oi represents a rotatedbox with class (c, x, y, h, w, cos , sin ), denoting classconfidence, position, size and angle.",
  ". Multi-agent collaboration": "In the proposed multi-agent collaboration, each agent actsin the dual role of supporter and receiver.As a sup-porter, each agent employs two novel modules, includinginformation-filling-driven message selection and codebook-based message representation, to determine compact, yetsupportive collaboration messages to help others.Thesetwo proposed modules enhance communication efficiencyin both spatial and channel dimensions of a feature map, re-spectively. As a receiver, each agent employs a message de-coding and fusion module to integrate supportive messagesfrom other agents, improving its perceptual performance.",
  "Information-filling-driven message selection": "Toefficientlyselectcompactcollaborativemessagesthat support other agents, each agent employs a novelinformation-filling-driven message selection method. Thekey idea is to enable each agent to restrainedly select per-tinent messages to share with other agents; then collec-tively, these pieces of non-redundant messages mutuallyfulfill each others information demands.For example,in occlusion scenarios, extra information from supportershelps an agent detect missed objects. However, overfilledinformation from multiple supporters wastes communica-tion resources. Thus, collective coordination is essential toavoid redundancy and enable more beneficial information.To achieve this, the proposed selection has two key steps:information disclosure, wherein agents mutually share theirawareness of available information within specific spatialareas, and filling-driven optimization, wherein each agentlocally optimizes the supportive messages for others.Information disclosure. In information disclosure, eachagent: i) employs an information score generator to createits information score map from its feature map, reflecting itsavailable information at each spatial area, and ii) broadcaststhis map to all other agents, promoting a mutually thoroughawareness of all the available support.The information score map is implemented with the de-tection confidence map. Intuitively, the areas containing anobject are likely to offer more useful information for reveal-ing missed detections and, therefore, should be assignedhigher information scores. Specifically, given a BEV fea-ture map Fi, its spatial information score map is Ci = generator(Fi) HW ,(2)where generator() is implemented by detection decoder.When information score map is generated, each agentbroadcasts it to other agents. This initial communicationis efficient because of lightweight information score maps.Filling-driven optimization. In the role of a supporter,each agent gathers the other agents information score mapsand determines who needs perceptual information at whichspatial areas by locally solving a filling-driven optimization.Here, filling the information demands means that an agent",
  ". The information-filling-driven message selection fulfillsthe information demand with non-redundant information": "only requires the necessary information at certain spatialareas for precise detection, as extra information no longerprovides significant benefits. This requires each supporterto prioritize non-redundant and informative spatial regionswith higher scores to assist others and halts the selectiononce the receivers information demands are fulfilled.Specifically, the optimization is formulated as a proxy-constrained problem and obtains a binary selection matrixfor each agent to support each receiver.Let Mij {0, 1}HW be the binary selection matrix supported on theBEV map. Each element in the matrix indicates whetherAgent i should send the information to Agent j at a spe-cific spatial location (1 for sending information, and 0 fornot sending). To solve for the binary selection matrix, theproxy-constrained problem is formulated as follows,",
  "i,j=1,j=iMij b, Mij {0, 1}HW .(3b)": "Here denotes element-wise multiplication, and the scalaru is a hyper-parameter to reflect the upper bound of in-formation demand. The function fmin(, ) computes theelement-wise minimum between a matrix and a scalar.In (3a), Cj + Ni=1,i=j Mij Ci indicates that eachreceiver j accumulates the information transmitted from allsupporters, combined with its own information, at each lo-cation, fmin(Cj + Ni=1,i=j Mij Ci, u) denotes theutility for each receiver, linearly increasing with the accu-mulated information scores until reaching the informationdemand u. Note that: i) (3a) is solved at the supporter sidefor preparing messages to a receiver; ii) the sum-based util-ity motivates supporters to collectively meet the receiversdemand and focus on higher-scoring regions, and iii) thecutoff point leads to halting selection to prevent redundancy.Equation (3a) transforms the feature-based collabora-tion utility in (1) as the sum of the information scores.This is based on the assumption that the accumulation ofinformation scores mirrors the benefits of feature aggre-gation. Equation (3b) addresses the bandwidth limitationin (1) by quantifying the total number of selected regions.This approach simplifies the objective in (1) into a proxy-constrained problem in (3a) and (3b). The optimized selec-tion solution derived from (1) is expected to yield a superioroutcome in the final feature-based collaboration.This optimization problem has an analytical solution; seethe theoretical derivation in the appendix. The solving pro- cess incurs a computational cost of O(log(m)), where mdenotes the number of spatial region candidates. By focus-ing on the extremely sparse foreground areas, we effectivelyreduce the cost to a negligible level, enabling each agent toprovide more targeted support for others with minimal cost.Based on the optimized selection matrix {Mij}Nj=1,each agent supports each collaborator with a sparse yet in-formative feature map Zij = Mij Fi, promisingsuperior perception improvements given the limited com-munication budget. These selected sparse feature maps arethen output to the message representation module.The proposed message selection offers two key benefits:i) it avoids redundancy from multiple supporters via col-lective selection, and ii) it adapts to varying communicationconditions by adjusting information demand, lower demandfor efficiency in limited budgets, and higher demand for su-perior performance in ample budgets. Compared to existingselection methods , which are based on indi-vidual supporter-receiver pairs, our collective optimizationfurther reduces redundancy across various supporters.",
  "Codebook-based message representation": "To efficiently transmit the selected feature map Zij, eachagent leverages a novel codebook-based message repre-sentation, reducing communication cost along the chan-nel dimension.The core idea is to approximate a high-dimensional feature vector by the most relevant code from atask-driven codebook; as a result, only integer code indicesneed to be transmitted, rather than the complete feature vec-tors composed of floating-point numbers.Codebook learning. Analogous to a language dictio-nary used by humans, our task-driven codebook is sharedamong all agents to standardize their communication forachieving the detection task. This codebook consists of a setof codes, which are learned to pragmatically approximatepossible perceptual features present in the training dataset.Here the pragmatic approximation refers to each code serv-ing as a lossy approximation of a feature vector, while re-taining essential information necessary for the downstreamdetection task within that vector.Specifically, let ={F(i,s)}N,Si=1,s=1 be the collective set of BEV feature mapsextracted by the observation encoders of all N agents acrossall S training scenes.Let D =d1, d2, , dnLRCnL be the codebook, where D[] = d RC is theth code and nL is the number of codes.The task-driven codebook is learned through feature ap-proximation at each spatial location; that is,",
  ". The codebook-based message representation depicts theoriginal feature vector with the most relevant codes": "task and the second term reflects the reconstruction errorbetween the original feature vector and the code. This ap-proximation is lossy for reconstruction while lossless forthe perceptual task, enabling the reduction of communica-tion cost without sacrificing perceptual capacity.Code index representation. Based on the shared code-book D, each agent can substitute the selected sparse fea-ture map Zij by a series of code indices Iij. For eachBEV location (h, w), the code index is obtained as,",
  ".(5)": "The codebook offers versatility in its configuration by ad-justing both the codebook size nL and the quantity of codesnR used for representing the input vector.Equation (5)demonstrates a specific instance where nR = 1, chosen forsimplicity in notation. When nR is larger, the representationinvolves a combination of multiple codes.Overall, the final message sent from the ith agent to thejth agent is Pij = Iij, conveying the required com-plementary information with compact code indices. Agentsexchange these packed messages with each other.This codebook-based representation offers three advan-tages: i) efficiency for transmitting lightweight code in-dices; ii) adaptability to various communication resourcesvia adjusting code configurations (smaller for efficiency,larger for superior performance), and iii) extensibility byproviding a shared standardized representation. New het-erogeneous agents can easily join the collaboration byadding its effective perceptual feature basis to the codebook.",
  "Message decoding and fusion": "Message decoding reconstructs the supportive featuresbased on the received code indices and the shared codebook.Given the received message Pji = Iji, the decoded fea-ture maps Zji RHW C element located at (h, w) is( Zji)[h,w] = D[Iji[h,w]]. Subsequently, message fusionaggregates these decoded feature maps to augment indi-vidual features, implementing by the non-parametric point-wise maximum fusion. For the ith agent, given the recon-structed feature Zji. The enhanced BEV feature is ob-tained as Hi = maxjNi(Fi, Zji) RHW C where Ni is i-th agents connected collaborators and max() maximizesthe corresponding features from multiple agents at each in-dividual spatial location. The enhanced feature Hi is de-coded to generate the upgraded detection Oi.",
  ". Loss functions": "To train the overall system, we supervise three tasks: infor-mation score map generation, object detection, and code-book learning. The information score map generator reusesthe parameters of the detection decoder. The overall loss isdefined as, L = Ni LdetOi, O0i+Fi Fi22 , where Ldet() denotes the detection loss , O0i and Oi repre-sents the ground-truth and predicted objects, and Fi and Fidenote the i-th agents original feature map and the one ap-proximated by codes. During the optimization, the networkparameters and the codebook are updated simultaneously.",
  ". Experimental Results": "Our experiments cover two datasets, both real-world andsimulation scenarios, two types of sensors (LiDAR andcameras), and both homogeneous and heterogeneous set-tings. Specifically, we conduct 3D object detection in thesetting of V2X-communication-aided autonomous drivingon DAIR-V2X dataset and the extended large-scaleOPV2VH+ dataset. The detection results are evaluated byAverage Precision (AP) at Intersection-over-Union (IoU)thresholds of 0.30 and 0.50. The communication volumefollows the standard setting as that countsthe message size by byte in log scale with base 2.",
  ". Datasets and experimental settings": "DAIR-V2X is a widely-used real-world collabora-tive perception dataset. Each scene contains two agents:a vehicle and a road-side-unit.Each agent is equippedwith a LiDAR and a camera.The perception range is204.8m102.4m. OPV2VH+ is an extended large-scaleversion of the original vehicle-to-vehicle camera-only col- laborative perception dataset OPV2V+ with a largerarray of collaborative agents (a total of 10) and addi-tional LiDAR sensors, co-simulated by OpenCDA andCARLA . Each agent has a LiDAR, 4 cameras, and 4depth sensors. The detection range is 281.6m 80m.Implementation.WeadoptPointPillarandCaDDN for the LiDAR and camera detector, respec-tively. Regarding the heterogeneous setup, agents are ran-domly assigned either LiDAR or camera, resulting in a bal-anced 1:1 ratio of agents across the different modalities.Communication volume. Specifically, for feature repre-sentation, given a selection matrix M, the bandwidth is cal-culated as log2(H W |M|C 32/8). Here, 32 repre-sents the float32 data type and 8 converts bits to bytes. Forcode index representation, given codebook D RCnL,comprised of nL codes and each vector constructed usingnR codes, the bandwidth given the selection matrix M iscalculated as log2(H W |M| log2(nL) nR/8).Here, log2(nL) signifies the data amount required to repre-sent each code index integer, decided by the codebook size. 5.2. Quantitative evaluationBenchmark comparison. and 5 compare the pro-posed CodeFilling with previous methods in termsof the trade-off between detection performance and com-munication bandwidth for DAIR-V2X and OPV2VH+datasets under homogeneous and heterogeneous settings,respectively.Detailed values can also be found inthe appendix.Baselines include no collaboration (Oi),Where2comm , HMViT , V2VNet , Dis-coNet , V2X-ViT , AttFuse and late fusion,where agents exchange the detected 3D boxes directly. Notethat HMViT is specifically designed for heterogeneous set-",
  ". Both the proposed information-filling-driven message selection and codebook-based representation are effective": "tings by using domain adaption, while CodeFilling isnaturally compatible with heterogeneous settings withoutadditional cost. We see that CodeFilling: i) achievesa far-more superior perception-communication trade-offacross all the communication bandwidth choices and var-ious collaborative perception settings, including camera-only, lidar-only, and heterogeneous 3D detection; ii) sig-nificantly improves the detection performance, especiallyunder extremely limited communication bandwidth, im-proves the SOTA performance by 11.093/5.271/38.357%,14.75/28.516/28.372% for LiDAR/camera/heterogeneouson DAIR-V2X and OPV2VH+ even when the bandwidthis constrained by a factor of 100K; and iii) outperforms pre-vious communication-efficient SOTA, Where2comm, withsignificantly reduced communication cost: 1333/115/863,1206/1078/252 times less on DAIR-V2X and OPV2VH+. Furthermore,for inference speed,CodeFilling(36/99ms) is comparable to Where2comm (34/94ms), andsignificantly faster than HMViT (90/1266ms) on DAIR-V2X/OPV2VH+. This communication efficiency ensuresthat agents are able to actively collaborate with each other. Robustness to pose error and communication latency.We validate the robustness against pose error and commu-nication latency on both OPV2VH+ and DAIR-V2X. Thepose error setting follows CoAlign using Gaussiannoise with a mean of 0m and standard deviations rangingfrom 0m to 1.0m. The latency setting follows SyncNet ,varying from 0ms to 500ms. Figs. 7 and 8 show the detec-tion performances as a function of pose error and latency,respectively. We see: i) while perception performance gen-erally declines with increasing levels of pose error and la-tency, CodeFilling consistently outperforms baselinesunder all imperfect conditions; ii) CodeFilling consis- tently surpasses No Collaboration, whereas baselines failwhen pose error exceeds 0.6m and latency surpasses 100ms.5.3. Ablation studiesEffectiveness of our message selection and representa-tion. a compares CodeFilling, the one withoutmessage selection, and the one without codebook and mes-sage selection. We see that: i) applying code index rep-resentation reduces the communication cost by 208 timeswhile maintaining the same detection performance, as it by-passes the high channel dimension typical of feature vec-tors, and an integer index requires less data than the orig-inal floating-point numbers, and ii) applying information-filling-driven message selection achieves 4.8% higher de-tection performance with the same communication cost, asit reallocates the bandwidth wasted in redundant informa-tion to more beneficial information.Ablation of information-filling-driven message selection.b compares different utility designs in information-filling optimization: sum, max, and scenarios without se-lection. The max utility design favors selecting collabora-tors with the highest score, leading to no selection if theego agent has the highest score. We see that the sum-basedutility outperforms both the max and no selection in theperception-communication trade-off across all communica-tion bandwidth conditions. This superiority is due to the op-timized combination of information from different collabo-rators, which has proved to be more effective than relyingsolely on the best-performing single agent. This approachencourages agents to participate in collaboration; even thetop-performing agents can benefit from collaboration.b evaluates three different information demands u(0.5, 1.0, 1.5). We see that: i) lower u demonstrates betterefficiency under limited communication budgets; ii) higher",
  "(e) CodeFilling": ". CodeFilling achieves more accurate detections with 1256 times less communication cost. Green and red boxes denoteground-truth and detection, respectively.u demonstrates superior performance under ample commu-nication budgets.By adjusting u, CodeFilling con-sistently maintains superior performance-communicationtrade-off across all communication conditions.Ablation of codebook-based message representation.d and e explore different codebook configu-rations: codebook size and code quantity.We see that:i) all the codebook configurations demonstrate a supe-rior perception-communication trade-off under highly con-strained communication conditions, showing the effective-ness and robustness of the codebook-based representation;and ii) larger codebook sizes and quantities yield bet-ter performance, while smaller sizes and quantities offergreater communication efficiency. By adapting the config-uration,CodeFilling maintains superior performance-communication trade-off across all communication budgets.",
  ". Qualitative evaluation": "Visualization of message selection and representa-tion. showcases the efficient collaboration inCodeFilling. The scene features one ego agent and twocollaborators - one with LiDAR and the other with a cam-era, with all visualizations presented from the egos per-spective. (a) and (j) compare the detection resultsbefore and after collaboration. We see that through col-laboration, the ego agent successfully uncovers detectionsmissed in its individual view. (d-g) displays the col-laborators information score maps and their correspondingselection matrices. We see that the redundant informationin the overlapped regions is avoided, promoting communi-cation efficiency. (c) and (h) illustrate the evolutionof the information score before and after filling, highlight- ing that the information demands have been met, which isindicative of improved perception performance. (f)presents the codebook-represented message, revealing that:i) it is spatially sparse, and ii) the selected information fromheterogeneous collaborators is unified in a common format,facilitating extensibility.Visualization of detection results. com-pares CodeFilling with previous SOTAs.We seethat CodeFilling qualitatively outperforms previousSOTAs with 1256 times less communication cost. The rea-son is that CodeFilling avoids redundant informationfrom different collaborators and employs efficient code in-dex representation, thereby transmitting more critical per-ceptual information even with less communication cost. 6. ConclusionsWe propose CodeFilling, a novel communication-efficient collaborative 3D detection system with twonovel designs: codebook-based message representation andinformation-filling-driven message selection. Extensive ex-periments covering both real-world and simulation scenar-ios show that CodeFilling not only achieves state-of-the-art perception-communication trade-off under variousmodalities, including LiDAR, camera, and heterogeneoussettings but also is robust to pose error and latency issues.Limitation and future work. We plan to explore the tem-poral dimension and determine critical time stamps.Acknowledgments.This research is supported by theNational Key R&D Program of China under Grant2021ZD0112801, NSFC under Grant 62171276 and theScience and Technology Commission of Shanghai Munici-pal under Grant 21511100900 and 22DZ2229005.",
  "Qi Chen. F-cooper: feature based cooperative perception forautonomous vehicle edge computing system using 3d pointclouds. Proceedings of the 4th ACM/IEEE Symposium onEdge Computing, 2019. 1": "Runjian Chen, Yao Mu, Runsen Xu, Wenqi Shao, ChenhanJiang, Hang Xu, Yu Qiao, Zhenguo Li, and Ping Luo. CO3:Cooperative unsupervised 3d representation learning for au-tonomous driving. In The Eleventh International Conferenceon Learning Representations, 2023. 1 Siheng Chen, Baoan Liu, Chen Feng, Carlos Vallespi-Gonzalez, and Carl K. Wellington. 3d point cloud processingand learning for autonomous driving: Impacting map cre-ation, localization, and perception. IEEE Signal ProcessingMagazine, 38:6886, 2021. 1 Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengy-ong Wu, Yunji Chen, and Olivier Temam.Diannao: asmall-footprint high-throughput accelerator for ubiquitousmachine-learning.Proceedings of the 19th internationalconference on Architectural support for programming lan-guages and operating systems, 2014. 3",
  "Yue Hu, Shaoheng Fang, Weidi Xie, and Siheng Chen.Aerial monocular 3d object detection. IEEE Robotics andAutomation Letters, 8:19591966, 2022. 1": "Yue Hu, Yifan Lu, Runsheng Xu, Weidi Xie, Siheng Chen,and Yanfeng Wang. Collaboration helps camera overtake li-dar in 3d detection. 2023 IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), 2023. 1, 2, 5,6 Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encodersfor object detection from point clouds.2019 IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 1268912697, 2018. 1, 6",
  "Zixing Lei, Shunli Ren, Yue Hu, Wenjun Zhang, and SihengChen. Latency-aware collaborative perception. ECCV, 2022.2, 7": "Jinlong Li, Runsheng Xu, Xinyi Liu, Baolu Li, Qin Zou, Ji-aqi Ma, and Hongkai Yu. S2r-vit for multi-agent coopera-tive perception: Bridging the gap from simulation to reality.ArXiv, abs/2307.07935, 2023. 1 Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, ChenFeng, and Wenjun Zhang. Learning distilled collaborationgraph for multi-agent perception. Advances in Neural Infor-mation Processing Systems, 34:2954129552, 2021. 1, 2, 6",
  "Yiming Li, Qi Fang, Jiamu Bai, Siheng Chen, Felix Juefei-Xu, and Chen Feng. Among us: Adversarially robust collab-orative perception by consensus. ICCV, 2023. 1": "Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and ZsoltKira. When2com: Multi-agent perception via communica-tion graph grouping. In Proceedings of the IEEE/CVF Con-ference on computer vision and pattern recognition, pages41064115, 2020. 1, 2, 5 Yen-Cheng Liu, Junjiao Tian, Chih-Yao Ma, Nathan Glaser,Chia-Wen Kuo, and Zsolt Kira. Who2com: Collaborativeperception via learnable handshake communication. In 2020IEEE International Conference on Robotics and Automation(ICRA), pages 68766883. IEEE, 2020. 1, 2, 5 Yifan Lu, Quanhao Li, Baoan Liu, Mehrdad Dianat, ChenFeng, Siheng Chen, and Yanfeng Wang. Robust collabora-tive 3d object detection in presence of pose errors. IEEE In-ternational Conference on Robotics and Automation (ICRA),2023. 1, 2, 7 Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Siheng Chen,and Yanfeng Wang. An extensible framework for open het-erogeneous collaborative perception. In The Twelfth Interna-tional Conference on Learning Representations, 2024. 1 Cody Reading, Ali Harakeh, Julia Chae, and Steven L.Waslander.Categorical depth distribution network formonocular 3d object detection. 2021 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages85518560, 2021. 6 Jurgen Scherer, Saeed Yahyanejad, Samira Hayat, EvsenYanmaz, Torsten Andre, Asif Khan, Vladimir Vukadinovic,Christian Bettstetter, Hermann Hellwagner, and BernhardRinner. An autonomous multi-uav system for search and res-cue. In Proceedings of the First Workshop on Micro AerialVehicle Networks, Systems, and Applications for CivilianUse, pages 3338, 2015. 1 Saurabh Singh, Sami Abu-El-Haija, Nick Johnston, Jo-hannes Balle, Abhinav Shrivastava, and George Toderici.End-to-end learning of compressible features. In 2020 IEEEInternational Conference on Image Processing (ICIP), pages33493353. IEEE, 2020. 3",
  "Binglu Wang, Lei Zhang, Zhaozhong Wang, YongqiangZhao, and Tianfei Zhou. Core: Cooperative reconstructionfor multi-agent perception. ICCV, 2023. 1": "Tianhang Wang, Guang Chen, Kai Chen, Zhengfa Liu, BoZhang, Alois Knoll, and Changjun Jiang.Umc: A uni-fied bandwidth-efficient and multi-resolution based collab-orative perception framework. 2023 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), 2023.1 Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang,Bin Yang, Wenyuan Zeng, and Raquel Urtasun.V2vnet:Vehicle-to-vehicle communication for joint perception andprediction.In European Conference on Computer Vision,pages 605621. Springer, 2020. 1, 2, 6 Sizhe Wei, Yuxi Wei, Yue Hu, Yifan Lu, Yiqi Zhong, Si-heng Chen, and Ya Zhang. Asynchrony-robust collaborativeperception via birds eye view flow. In Advances in NeuralInformation Processing Systems, 2023. 1, 2 Hao Xiang, Runsheng Xu, Xin Xia, Zhaoliang Zheng, BoleiZhou, and Jiaqi Ma.V2xp-asg: Generating adversarialscenes for vehicle-to-everything perception. 2023 IEEE In-ternational Conference on Robotics and Automation (ICRA),pages 35843591, 2022. 2",
  "Hao Xiang, Runsheng Xu, and Jiaqi Ma. Hm-vit: Hetero-modal vehicle-to-vehicle cooperative perception with visiontransformer. ICCV, 2023. 1, 6": "Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Hao Xiang, andJiaqi Ma. Opencda: An open cooperative driving automa-tion framework integrated with co-simulation. 2021 IEEEInternational Intelligent Transportation Systems Conference(ITSC), pages 11551162, 2021. 6 Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Liu,and Jiaqi Ma. Opv2v: An open benchmark dataset and fu-sion pipeline for perception with vehicle-to-vehicle commu-nication.2022 International Conference on Robotics andAutomation (ICRA), pages 25832589, 2021. 1, 2, 6",
  "Communication-efficient and collaboration-pragmatic multi-agent perception. Advances in Neural Information Process-ing Systems, 2023. 1": "Kun Yang, Dingkang Yang, Jingyu Zhang, Mingcheng Li,Y. Liu, Jing Liu, Hanqi Wang, Peng Sun, and Liang Song.Spatio-temporal domain awareness for multi-agent collabo-rative perception. Proceedings of the 31st ACM InternationalConference on Multimedia, 2023. 1 Kun Yang, Dingkang Yang, Jingyu Zhang, Hanqi Wang,Peng Sun, and Liang Song.What2comm:Towardscommunication-efficient collaborative perception via featuredecoupling. Proceedings of the 31st ACM International Con-ference on Multimedia, 2023. 1 Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang,Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, JiruiYuan, et al. DAIR-V2X: A large-scale dataset for vehicle-infrastructure cooperative 3d object detection. In Proceed-ings of the IEEE/CVF Conference on computer vision andpattern recognition (CVPR), 2022. 1, 2, 6 Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang,Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan,Ning Sun, Juan Song, Jirui Yuan, Ping Luo, and ZaiqingNie. V2x-seq: A large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, 2023. 2"
}